Word Re-order ing  and  DP-based  Search  in S ta t i s t i ca l  Mach ine  
Trans la t ion  
Christoph Til lmann and Hermann Ney 
Lehrstuhl fiir Informatik VI, Computer Science Department 
RWTH Aachen - University of Technology 
D-52056 Aachen, Germany 
{t illmann, ney}@inf ormat ik. rwth-aachen, de 
Abstract 
In this paper, we describe a search procedure for sta- 
tistical machine translation (MT) based on dynmnic 
programming (DP). Starting from a DP-based solu- 
tion to the traveling salesman problem, we present 
a novel technique to restrict the possible word re- 
ordering between source and target language in or- 
der to achieve an efficient search algorithm. A search 
restriction especially useful for tile translation di- 
rection from German to English is presented. The 
experimental tests are carried out on the Verbmo- 
bil task (Germm>English, 8000-word vocabulary), 
which is a limited-domain spoken-language task. 
1 Introduction 
The goal of machine translation is tile translation 
of a text given in some source language into a tar- 
gel: language. We are given a source string f J  = 
fl ...fj.-.f.l of length J, wlfich is to be translated into 
a target string c\[ = cl ...ei...el of length I. Among 
all possible target strings, we will choose the string 
with the highest probability: 
dI = argmax {Pr(e{lff)} q 
-- argmax {Pr (e l ) .P r ( f f le l )  } (1) 
The argmax operation denotes the search problem, 
i.e. the generation of the output sentence in the tar- 
get language. Pr(c~) is the language model of tim 
target language, whereas Pr(fi'le*l) is the transla- 
tion model. Our approach uses word-to-word epen- 
dencies between source and target words. The model 
is often further restricted so that each source word 
is assigned to exactly one target word (Brown et al, 
1993; Ney et al, 2000). These alignment models 
are similar to the concept of hidden Markov models 
(HMM) in speech recognition. The alignment map- 
ping is j --+ i = aj from source position j to target 
position i = aj. The use of this alignment model 
raises major problems if a source word has to be 
aligned to several target words, e.g. when translat- 
ing German compound nouns. A siuLple extension 
will be used to handle this problem. 
In Section 2, we briefly review our approach to sta- 
tistical machine translation. In Section 3, we in- 
troduce our novel concept to word re-ordering and 
a DP-based search, which is especially suitable for 
the translation direction fl'om German to English. 
This approach is compared to another re-ordering 
scheme presented in (Berger et al, 1996). In Sec- 
tion 4, we present the t)erformance measures used 
and give translation results on the Verbmobil task. 
2 Basic Approach 
In this section, we briefly review our translation ap- 
proach. In Eq. (1), Pr(C~l) is the language model, 
which is a trigrain language model in this case. For 
the translation model Pr(fil le{), we go on the as- 
sunlption that each source word is aligned to ex- 
actly one target word. The alignment model uses 
two kinds of parameters: alignment probabilities 
p(ajlaj_l, I, J), where the probability of alignment 
aj for position j deI)ends on tile previous alignment 
position aj-i  (Ney et al, 2000) and lexicon proba.- 
bilities p(\]~le~j). When aligning the words in par- 
allel texts (tbr language pairs like Spanish-English, 
French-Englisll, Italian-German,...), we typically ob- 
serve a strong localization effect. In many cases, 
there is an even stronger estriction: over large por- 
tions of tile source string, the alignment is monotone. 
2.1 Inverted Alignments 
To explicitly handle the word re-ordering between 
words in source and target language, we use the con- 
cept of the so-called inverted aligmnents as given in 
(Ney et al, 2000). An inverted alignment is defined 
as follows: 
inverted alignment: i -+ j = bi. 
Target positions i are mapped to source positions bi. 
What is important and is not expressed by the nota- 
tion is the so-called coverage constraint: each source 
position j should be 'hit' exactly once by the path 
of the inverted aligmnent b~ = bL...bi...bi. Using the 
inverted alignments in the maximum approximation, 
850 
we obtain as search criterion: 
/ f l  i - I  sn} . v ( ,Z l5  ? 
ell k i= l  
' }} ? max  I I  z,.5. v(f,,,le*)\] --- 
hi  i=1 
= max V( J I I ) -max 
I i I el 'hi i=1 
"p(bi\[bi-l,-\[,'\])'P(fbilCi)\] }}, 
where the two products over i have l)een merged into 
) i -1  a single product ()vet i. I (cilei_~) is tim trigram 
language model probability. The inverted alignment 
probability p(bi\[bi-l, I, .1) and the lexicon probabil- 
ity p(J'~,~ led are obtained by relative fl'equency es- 
timal;es frosll the Viterbi alignment path after the 
final training iteration. The details are given in 
(Och art(1 Ney, 2000). The sentence length prob- 
ability p(J\[1) is omitted without any loss in per- 
tbrmance. For the inverted alignment probability 
p(bi\[bi-~, I J), we drop the dependence on the tar- 
get sentence length I. 
2.2  Word Jo in ing  
The baseline alignment model does not pernfit hat a 
source word is aligned to two or more target words, 
e.g. %r the translation direction from German to 
\]?,nglish, the German (:Oml)ound I IOlStl  'Zahnarztter- 
rain' causes ira>blares, because it must be translated 
by the two target words dcntist'.s appoi'ntmcnt. We 
use a solution to this 1)roblenl similar to the one 
presented in (()ell el; al., 1999), where target words 
are joined during training. The word joining is (lotto 
on the basis of a likelihood criterion. An extended 
lexicon model is defined, and its likelihood is com- 
pared to a baseline lexicon model, which takes only 
single-word ependencies into aecollnt. E.g. when 
'Zahnarzttermin' is aligned to dentist'.s, the extended 
lexicon model might learn that 'Zahnarzttcrmin' ac- 
tually has to be aligned to both dentist's and ap- 
pointment. In the following, we assmne that this 
word joining has been carried out. 
"I DP  Algor i thm for Statistical 
Machine Translation 
in order to handle the necessary word re-ordering as 
&n optimization problem within our dynmnic pro- 
gramming approach, we describe a solution to the 
traveling salesnmn problem (TSP) which is based 
on dynamic programming (Held, Karp, 1962). The 
traveling salesman problem is an oi)timization prob- 
lem which is defined as follows: given are a set of 
May 
of 
fourth 
the 
oll 
you 
visit 
not 
can 
colleague 
my 
case 
this 
In 
O O O O O O O O O O ~_ . . . .O  
O O O O O O O O O /O~ O O O 
O O O O O O O O O/O O O O 
O O O O O O O O ? O O O O 
O O O O O O O O// O O O O O 
O O O O O O O/ /O  O O O O O 
o o o o o o 
oo Oo Oo Oo oO oO oO o o o ?
?_... o.... o ...o._...o.....? 
O O O O O/~ O O O O O O O 
O O O9/7 .O  O O O O O O O O 
O O f  O O O O O O O O O O 
 oO oOoOOoOOoOO 
I I I I 1 I I I I I I 1 I 
I d F k rn K S a v M n b 
. i a a e o i m i ~ i e 
e / n i / e e i C s 
s I n n I r h u 
e e t t C 
m g e h 
e I/ a 
n 
Figm'e 1: Re-ordering for the Gerlnan verbgroul). 
cities S = Sl ,--- ,  s ,  and tbr each pair of cities si, sj 
the cost dij > 0 for traveling flom city s: to city 
.sj. We arc.' looking for the shortest tour visiting 
all cities exactly once while starting and ending in 
city sl. A straightforward way to find the short- 
est tour is by trying all possible permutations of the 
n cities. The resulting algorithm has a complexity 
of O(n!). Itowever, dynamic progrmnming can be 
used to find tile shortest our in exponential time, 
namely in O(n 2-2'~), using the algorithm by Ileld and 
Karp. The approach recursively evahlates a quantity 
Q(C,j), where C is the set; of already visited cities 
and sj is the last visited city. Subsets C of increas- 
ing cardinality c are processed. The algorithm works 
due to the fact that not all permutations of cities 
have to be considered explicitly. For a given partial 
hypothesis (C, j), the order in which the cities in (2 
have beast visited cast be ignored (except j) ,  only the 
score for the best path reaching j has to be stored. 
This algorithm can be applied to statistical machine 
translation. Using the concept of inverted align- 
ments, we explicitly take care of the coverage con- 
straint by introducing a coverage set C of source sen- 
tence positions that have been already processed. 
The advantage is that we can recombine search hy- 
potheses by dynmnic programming. The cities of 
the traveling salesman probleIn correspond to source 
851 
Table 1: DP algorithm tbr statistical machine translation. 
input: source string fl..-f j . ..f. l  
initialization 
for each cardinality c = 1, 2,. - ?, J do 
tbr each pair (C,j), where j C C and ICl = c do 
tbr each tat'get word e E E 
max 
5,e  t !  
Q~,(e,C,j) =p(fjle) {p(jlj',J).p(5) .pa(ele',e" ) -O~,,(e',C\ {j},j')} 
j lEC\{j} 
words f j  in the input string of length J. For the 
final translation each source position is considered 
exactly once. Subsets of partial hypotheses with 
coverage sets C of increasing cardinality c are pro- 
cessed. For a trigrmn language model, the partial 
hyl)otheses are of tile form (c', c,(2, j) ,  e', c are the 
last two target words, C is a coverage set for tile al- 
ready covered source positions and j is the last posi- 
tion visited. Each distance in the traveling salesman 
problem now corresponds to the negative logarithm 
of tile product of the translation, alignment and lan- 
guage model probabilities. The following auxiliary 
quantity is defined: 
Qc~((?,~ C~j)  :.~. probability of tile best partial 
hypothesis (c~, b~), where 
C = {bt:\]k = 1, - - . , i} ,  bi = j ,  
(2 i ~- C a ILd  C i _  1 ~ e I. 
The type of alignment we have considered so far re- 
quires the stone length tbr source and target sen- 
tence, i.e. I = J.  Evidently, this is an unrealistic 
assumption, therefore we extend the concept of in- 
verted alignments as follows: Wtmn adding a new 
position to the coverage set C, we might generate i- 
ther 5 = 0 or a = 1 new target words. For 5 = 1, a 
new target language word is generated using the tri- 
gram language model p(e\[e', e"). For 5 = 0, no new 
target word is generated, while an additional source 
sentence position is covered. A modified language 
inodel probability pa(e\[c', c") is defined as follows: 
1.0 i f a=0 
Pa(ele"e") = p(e\]e',e") i fa  = 1 
We associate a distribution p(5) with the two cases 
5 = 0 and 5 = 1 and set p(5 = 1) = 0.7. 
The above auxiliary quantity satisfies tile following 
recursive DP equation: 
Qe, (c ,C , j )  = 
4. mein 5. Kollege 
~ ~  ,Q ~Verb_ .~  F in: l~ 
--_ J J  
1. In 7.nicht 9. Sie 
2. diesem 8. besuehen 10. am 
3. Fall 11. vierten 
6. kann 12. Mai 13.. 
Figure 2: Order in which source positions are visited 
tbr the example given in Fig.1. 
P(fJIe) "max {P(JlJ','/)'P(5)" ~,c I! 
i' ec\ {j} 
? pa( le', e")- Qe,, c \ {j}, j ') }. 
The DP equation is evaluated recursively for each 
hypothesis (e ' ,e,C, j ) .  TILe resulting algorithm is 
depicted in Table 1. The complexity of the algorithm 
is O(E  a ? j2 .2 .1) ,  where E is the size of the target 
language vocabulary. 
3.1 Word  Re-Order ing  w i th  Verbgroup 
Rest r i c t ions :  Quas i -monotone  Search  
The above search space is still too large to allow 
the translation of a inedium length input sentence. 
On the other hand, only very restricted re-orderings 
are necessary, e.g. for the translation direction fi'om 
852 
Table 2: Coverage set hyllothesis extensions for the IBM re-ordering. 
Predecessor eovera~e, set \[ I Successor coverage set 
({1,...,,, ,} \ ,l') ({1,... , , , ,} ,0 
({ \ ] , - -  . , , , t} \ {l,/1 } , l ' )  -- ({1 , ' ' ' ,  'L~, } \ {/l} ,/) 
\ ,l') ({1,.--,,,,,} \ ,0 
({ I , . . . , , , -  J} \ ,l') ({1,--.,,,,} \ ,m) 
Oerlnan to English the monotonicity constraint is 
violated mainly with respect; to the German verb- 
group. In German, the verbgroui) usually consists 
of a left and a right verbal brace., whereas in En- 
glish the words of the verbgroul) usually tbrm a se- 
quence of consecutive words. Our new al)t)roach, 
which is ('alle.d quas i -monotone  search, proce.sses 
the source sentence monotonically, while explicitly 
taking into account the positions of the (-lel'lnan 
verbgroup. 
A typical situation is shown in Figure \]. When 
translating the sentence monotonically fl'om left to 
right, the translation of the German finite verb 
'kmm', which is the left verbal brace in this case, 
is postponed mttil the German noun phrase 'mein 
t(ollege' is translated, which is the subject of the 
sentence. The.n, the. German infinitive 'besuclmn' 
and the negation particle 'nicht;' are translated. The 
trai ls\]at\oi l  of erie posit ion in the source sentence 
nmy be postponed ti)r up to L = 3 source positions, 
and the translation of u I) to two source positions 
l ink be anticittated for at most 1~ = l0 source l)osi- 
tions. To formalize the attl)roach, we introduce four 
verbgroup stat;es S: 
? hfitial (Z): A contiguous, initial block <)f s<mrce 
l)ositions is covered. 
? Skitlped (K;): The translation of up to one word 
may be l)OStl)oned . 
? Verl> (V): The translation of Ul> (;o two words 
may be antMl)ated. 
, Final (Y:): The rest of the sentence is pro- 
cessed monotonically taking accoullt of tit(; al: 
ready covered positions. 
\Vhile processing the source sentence monotonically, 
the iifitial state Z is entere.d whenever there are no 
mmovered positions to the left of the rightmost cov- 
ered position. The sequence of states needed to 
carry out the. word re-ordering example in Fig. 1 
is given in Fig. 2. The 13 positions of the source 
sentence are processed in the order shown. A posi- 
tion is presented by the word at that position. Using 
these states, we define partial hypothesis extensions, 
which are of the following type: 
(S',C \ { j} , j ' )  --9 (S,C, j ) ,  
Not only the cove.rage set C and the posit\oilS j , j ' ,  
but also the verbgroup states S, S' are taken into ac- 
count. ~1~) be short, we omit the target words c, e' in 
the tbrinulation of the search hypotheses. There are 
13 types of extensions needed to describe the verb- 
group re-ordering. The details are given in (Till- 
mann, 2000). For each extension a new position is 
added to the coverage set. Covering the first lul- 
covered position in the source sentence, we use the 
language model probatfility p(e\[$, $). IIere, $ is the 
sentence boundary symbol, which is thought o be at; 
position 0 in the target sentence. Tile search starts 
in the hyl)othesis (Z, {~}, 0). {~} denotes the empty 
set, where, no source sentence t)osition is covered. 
The following recursive quation is evaluated: 
= (2) 
{gj\[j', 3). p(5). ( ld, e"). p(fj lc) ? ntax ~,c II 
? max Oc, (c ' ,S ' ,C  \ {j},j')??. (st,/) ) 
(S t ,C \ t j} , f f )~(? , ,C , j )  
j ' cc \u}  
The search ends in the hypotheses (Z, {1, . - . ,  d}, j). 
{1, . . . ,  d} de.notes a coverage se.t including all posi- 
tions from the starting 1)osition I to position J and 
j C {d-  L , - . - ,  .\]}. The final score is obtaiiled from: 
,nax P($l", c'). G '  (c, Z, { 1, . . - ,  d}, .it, 
c ,c  I 
j c{a  :.,.. . ,a} 
where p($lc, c/) denotes the trigram language model, 
which predicts the sentence boundary $ at tim end 
of the target sentence. The complexity of the quasi- 
monotone search is O(\]'J a-,l- (\[~2-t-L-1~)). The proof 
is given ill (Tilhnann, 2000). 
3.2 Re-order ing  w i th  IBM Style 
Rest r i c t ions  
We compare our new api)roach with tim word re- 
ordering used in the IBM translation approach 
(Berger et al, 1996). A detailed descrit)tion of tile 
search procedure used is given in this patent. Source 
sentence words are aligned with hypothesized target 
sentence words, where the choice of a new source 
word, which has not been aligned with a target word 
yet, is restricted I . A procedural definitioll to restrict 
l In the approach described in (Berger et al, 1996), a mor- 
phological analysis is carried out and word morphenles rather 
thin, full-form words are used during the search, ltere, we 
process only flfll-forin words within the trmmlation proce.durc. 
853 
the number of pernmtations carried out for the word 
re-ordering is given. During the search process, a 
partial hyt)othesis i  extended by choosing a source 
sentence position, which has not been aligned with a 
target sentence t)osition yet. Only one of the first n 
positions which are not already aligned in a partial 
hyt)othesis may be chosen, where n is set to 4. Tile 
restriction can be expressed in terms of the nmn- 
ber of uncovered source sentence positions to the 
left of the rightmost position m in the coverage set. 
This munber must be less than or equal to n - 1. 
Otherwise for the predecessor search hyt)othesis, we 
wonld have chosen a position that would not have 
been among the first n uncovered t)ositions. 
Ignoring the identity of the target language words 
e and c', the possible partial hypothesis extensions 
due to the IBM restrictions are shown in Table 2. 
In general, m, l, l' ~k {/1,12,/3} and in line umber 3 
and 4, l' must be chosen not to violate the above 
re-ordering restriction. Note that in line 4 the last; 
visited position for tile successor hypothesis must 
be m. Otherwise, there will be four uncovered po- 
sitions tbr the t)redecessor hypothesis violating the 
restriction. A dynamic programming recursion sin> 
ilar to the one in Eq. 2 is evaluated. In this case, we 
have no finite-state restrictions for the search space. 
Tile search stm'ts in hyi)othesis ({0}, 0) and ends in 
the hyt)otheses ({1 , . . . , J} , j ) ,  with j C {1 , . . - ,d} .  
This approach leads to a search procedure with com- 
plexity O(E  a . j4). The proof is given in (Tilhnann, 
2000). 
4 Exper imenta l  Resu l t s  
4.1 The  Task and the Corpus  
We have tested tim translation system Oil the Verb- 
mobil task (Wahlster 1993). The Verbmobil task is 
an appointment scheduling task. Two subjects are 
each given a calendar and they are asked to schedule 
a meeting. The translation direction is from Ger- 
man to English. A summary of the corpus used in 
the experiments i given in Table 3. The perplexity 
for the trigrmn language model used is 26.5. Al- 
though the ultimate goal of tile Verbmobil project 
is the translation of spoken language, the input used 
for the translation experinmnts reported on in this 
paper is the (more or less) correct orthographic tran- 
scription of the spoken sentences. Thus, the effects 
of spontaneous speech are t)resent in the corpus, e.g. 
the syntactic structure of tile sentence is rather less 
restricted, however the effect of sl)eech recognition 
errors is not covered. 
For the experiments, we use a simt)le pret)rocessing 
step. German city names are replaced by category 
markers. The translation search is carried out with 
tlm category markers and tlm city names are resub- 
stituted into the target sentence as a postt)rocessing 
step. 
Table 3: Training and test; conditions for the Verb- 
mobil task (*number of words without punctuation 
marks). 
\ [German English 
Training: Selltences 
Words 
Words* 
Vocabulary Size 
Singletons 
Test-147: Sentences 
Words 
Perplexity 
58073 
519523 549921 
418979 453632 
7939 4648 
3454 1699 
147 
1968 2173 
- 26.5 
Table 4: Multi-reference word error rate (roWER) 
and subjective sentence rror rate (SSER) for three 
different search t)rocedures. 
Search CPU time 
Method \[sec\] 
MonS 0.9 
QmS 10.6 
IbnlS 28.6 
roWER SSER 
\[yo\] \[y0\] 
42.0 30.5 
34.4 23.8 
38.2 26.2 
4.2 Per formance Measures  
The following two error criteria are used ill our ex- 
t)erinmnts: 
? roWER: multi-reference WER: 
We use the Levenshtein distance between tile 
automatic translation and several reference 
translations as a measure of tile translation er- 
rors. On average, 6 reference translations per 
automatic translation are availal)le. Tile Lev- 
enshtein distance between the automatic trans- 
lation and each of tile reference translations is 
comt)uted, and the minimum Levenshtein dis- 
tance is taken. This measure has the advantage 
of being completely automatic. 
? SSER: subjective sentence rror rate: 
For a more detailed analysis, tile translations 
are judged i)y a tminan test 1)erson. For the er-- 
ror counts, a range from 0.0 to 1.0 is used. An 
error count of 0.0 is assigned to a perfect rans- 
lation, and an error count of 1.0 is assigned to 
a semantically and syntactically wrong transb> 
tion. 
4.3 Translat ion Exper iments  
For tile translation experiments, Eq. 2 is recursively 
evahlated. We apply a beam search concet)t as in 
st)eech recognition. However there is no global prun- 
ing. Search hypotheses are i)rocessed separately ac- 
cording to their coverage set d. The best scored 
854 
hyi)othesis tbr each coverage set is comlmted: 
Om, , , , , , ( c )  = c,c',6",j 
The hyl)othesis (d, e, $, C, j) is t)1'1151(;(l if:
Q~,(e,S,C, j )  < to.O,~cam(C), 
where to is a threshold to control the mmlber of sur- 
viving hypotheses. Additionally, for a given coverage 
set, at most 250 different hypotheses are kept dur- 
ing the search process, and the number of difl'erent 
words to |)e hyl)othesized by a source word is lim- 
ited. For each source word f ,  the list of its possible 
translations c is sorte(1 according to p(.flc) ? p.,,.,,i(c), 
where Puui(e) is the unigrmn probability of the En- 
glish word c. It is sufficient o consi(ter only the best 
50 words. 
We show translation results for three at)l)roaches: 
tile monotone search (MonS):  where no word re- 
ordering is allowed (Tillmann, 1997), the quasi- 
monotone search (QmS) as 1)resented in this palser 
amt the IBM style ( IbmS) search as described in 
Section 3.2. 
TMsle 4: shows translation results tbr the three ap- 
I)roaches. The eomlsuting time is given in terms of 
CPU time per sentence (on a 450-MIlz l?entimn-III- 
PC). Itere, the printing threshold to = 10.0 is used. 
q_5:anslation errors are reported in terms of multi- 
reference word error rate (roWER) and subjective 
s(mtenee rror rate (SSER). The monotone search 
tserforms worst in terms of both elTror rates 5IsWI~;I~. 
mid SSEIL The (;OSlll)lstislg time is low, sitlce 51o 5e- 
ordering is (:arried o551,. '\]'he quasi-inonotone s arch 
i)e1foI'551s t)est, in ter551s of l)oth error rates roWER 
and SSh;R. Additionally, it; works about 3 times as 
fast as the II3M style sem:eh. For our demonstra- 
tion system, we typically use the pruning threshold 
to = 5.0 to speed Ul) the search by a factor 5 while 
allowing for a sm?fll degradation i  translation accu- 
racy. 
The effect of the pruning threshold to is shown in 
Table 5. The coml)uting time, the number of search 
errors, and the mull;i-reference WEll, (roWER) are 
shown as a flmction of to. The negative logarithm 
of to is reporte(t. The translation scores for the hy- 
i)otheses generated with different threshohl values 
to are compared to the translation scores obtained 
with a conservatively large threshold to = 10.0. For 
each test series, we count tile mlml)er of sentences 
whose score is worse than the corresponding score of 
the tent; series with the conserw~tively large thresh- 
old to = 10.0, and this mm:ber is reported as the 
number of search errors, l)epending on the thresh- 
old to, the search algorithm may miss the globally 
of)timal path which typically results in additional 
translation errors. Decreasing the threshold results 
in higher mWEl l  due to additional search errors. 
Table 5: Effect 
of sere'e\ errors (147 sentences). 
Search to I CPU time #search 
Method \[ \[sec\] error 
QmS 0.0 0.07 108 
1.0 0.13 85 
2.5 0.35 44 
5.0 1.92 4 
10.0 10.6 0 
lbmS 0.0 0.14 108 
1.0 0.3 84 
2.5 0.8 45 
5.0 4.99 7 
10.0 28.52 0 
of the beam threshold on the nmnber 
roWER 
\[%1 
42.6 
37.8 
36.6 
34.6 
34.5 
43.4 
39.5 
39.1 
38.3 
38.2 
Table 6 shows example translations obtained by the 
three, difli;rent appro~mhes. Again, the monotone 
search performs worst. In the second and third 
translation examples, the 1bins word re-ordering 
performs worse than the QmS word re-ordering, 
since it; can not take l)roperly into at:count he word 
re-ordering (115(; to the, German verbgroul). Tile 
German finite verbs 'l)in' (second exmnple) and 
'kSnnten' (third exmnt)le) are too far away from the 
t)ersonal pronouns 'ich' and 'Sic' (6 respectively 5
source sentence positions). In the last example, the 
less restrictive IbmS word re-ordering leads to a bet- 
ter translation, although the QmS translation is still 
aceeptabh'. 
5 Conclusion 
in this pal)er, we have presented a new, efficient 
DP-based search procedure for statistical machine 
translation. The approach assumes that, the word re- 
ordering is restricted to a few positions in the source 
sentence. The approach as been successfiflly tested 
on the 8000-word Verbmobil task. l'hture exten- 
sions of the system might include: 1) An e?tended 
translation model, where we use. more context o pre- 
(lict a source word. 2) An trot)roved language model, 
which takes into at:count syntactic structure, e.g. to 
ensure that a l)roper English verbgrout) in generated. 
3) A tight coupling with the speech recognizer out- 
lint. 
Acknowledgements 
This work has been supported as part of tile Verb- 
mobil project (contract number 01 IV 601 A) by 
the Certain5 Federal Ministry of Education, Science, 
Research and Technology and as part of the Eutrans 
project (ESPRIT project number 30268) by the Eu- 
rot)can Comimmity. 
855 
Table 6: Example Translations for the Verbmobil task. 
Input: Ja ,  wunderbar. KSnnen wir macllen. 
MonS: Yes, wonderflfl. Can we do . 
QmS: Yes, wonderflfl. We can do that . 
IbInS: Yes, wonderflfl. We can do that . 
Input: Das ist zu knapt) , weil ich ab dem dritten in Kaiserslautern bin . Genaugenommen imr atn dritten . 
Wie wSre es denn mn ghln Samstag , dem zehnten Februar ? 
MonS: That is too t ight,  becmlse I froln the third in Kaiserslautern. hi fact only on the third . 
How about 5hm Saturday, the tenth of February "? 
QmS: That is too t ight, because Imn froln tim third in Kaiserslautern. In fact only on the third . 
Ahm how about Saturday, February the tenth ? 
IbmS: That is too tight , froln the third because I will be in Kaiserslautern. In fact only on the third. 
Atlln how about Saturday, February the tenth ? 
Input: Wenn Sie dann noch den siel)zehnten kSnnten , wSre das toll , j a .  
MonS: If you then also the seventeenth could, would be the great , yes . 
QmS: If you could then also the seventeenth , tlmt would be great,  yes . 
1binS: Then if you could even take seventeenth , that would be great, yes . 
Illtmt: .Ja, das kommt mir sehr gelegen. Machen wir es dann am besten so. 
MonS: Yes, that suits me perfectly . Do we should best like that . 
QmS: Yes , that suits me fine . We do it like that then best; . 
IbmS: Yes , that suits me fine . We should best do it like tlmt . 
References  
A. L. Berger, P. F. Brown, S. A. Della Pietra, 
V. J. Della Pietra, J. R. Gillett, A. S. Kehler, 
R. L. Mercer. 1996. Language Translation appa- 
ratus and method of using context-based transla- 
tion models. United States Patent, Patent Num- 
ber 5510981, April. 
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, 
and R. L. Mercer. 1993. The Mathematics of Sta- 
tistical Machine Translation: Parameter Estima- 
tion. Computational Linguistics, vol. 19, no. 2, 
pp. 263-311. 
M. Held, R. M. Karp. 1962. A Dynmnic Progrmn- 
ruing Approach to Sequencing Problems. &SIAM, 
vol. 10, no. 1, pp. 196-210. 
H. Ney, S. Niessen, F. J. Och, H. Sawaf, C. Tilhnmm, 
S. Vogel. 2000. Algorittuns for Statistical %'ansla- 
tion of Spoken Language. IEEE Transactions on 
Speech and Audio Processing, vol. 8, no. 1, pp. 24- 
36. 
F. J. Och, C. Tilhnann, H. Ney. 1999. hnprovcd 
Alignment Models for Statistical Machine ~IYans- 
lation. In Proc. of the ,loint SIGDAT Conference 
on Empirical Methods in Natural Language Pro- 
ccssing and Very Large Corpora (EMNLP99), pp. 
20-28, University of Maryland, College Park, MD, 
USA, June. 
F. J. Och and Ney, H. 2000. A comparison of align- 
ment models for statistical machine translation, hi 
Proceedings of COLING 2000: The 18th Interna- 
tional Conference on Computational Linguistics, 
Saarbr/ieken, Germany, July-August. 
C. Tilltnann. 2000. Complexity of the Dif  
ferent Word Re-ordering Approaches. The 
document can be found under the URL 
http ://www-i6. Informat ik. RWTg-Aachen. de/C 
olleagues/tilli/. Aachen University of' Tech- 
nology, Aachen, Germany, June. 
C. Tilhnann, S. Vogel, H. Ney and A. Zubiaga. 1997. 
A DP based Search Using Monotone Alignments 
in Statistical Translation. In Proc. of the 35th An- 
nual Conf. of the Association for" Computational 
Linguistics, pp. 289 296, Madrid, Spain, July. 
W. Wahlster. 1993. Verbmol)ih Translation of Face- 
to-Face Dialogs. MT Summit IV, pp. 127-135, 
Kobe, Japan. 
856 
Improving SMT quality with morpho-syntactic analysis 
Sonja Nief lcn and Hcrmann Ney 
Lehrstuhl  fiir hf lbr lnatik VI 
Computer  Science Department  
RWTH University of Technology Aachen 
D-52056 Aachen, Germany 
Email: n iessen@in?ormat ik ,  rwth -aachen,  de 
Abst ract  
In the framework of statistical machine transla- 
tion (SMT), correspondences between the words 
in the source and the target language are 
learned from bilingual corpora on the basis of 
so-called alignment mode, Is. Many of the sta- 
tistical systems use little or no linguistic know- 
ledge to structure the underlying models. In 
this paper we argue that training data is typical- 
ly not large enough to sutficiently represent the 
range of different phenomena in natural angua- 
ges and that SMT can take advantage of the ex- 
plicit introduction of some knowledge about the 
lmlgnages under consideration. The improve- 
ment of the translation results is demonstrated 
on two ditferent German-English corpora. 
1 I n t roduct ion  
In this pal)er, we address the question of how 
morl)hological and syntactic analysis can help 
statistical machine translation (SMT). In our 
apl)roach, we introduce several transtbrmations 
to the source string (in our experiments the 
source language is German) to demonstrate how 
linguistic knowledge can improve translation re- 
suits especially in the cases where, the token- 
type ratio (nmnber of training words versus 
nmnber of vocabulary entries) is unthvorable. 
After reviewing the statistical approach to 
machine translation, we first explain our mo- 
tivation for examining additional knowledge 
sources. We then present our approach in detail. 
Ext)erimental results on two bilingual German- 
English tasks are reported, namely the VERB- 
MOBIL  and the EUTRANS task. Finally, we give 
an outlook on our fllture work. 
2 Stat i s t i ca l  Mach ine  Trans la t ion  
The goal of the translation process in statistical 
machine translation can l)e fornmlated as tbl- 
lows: A source language string .f~ = f l . . .  f.! 
is to be translated into a target language string 
c\[ =- e l . . .  el. In the experiments reported in 
this paper, the source language is German and 
the target language is English. Every English 
string is considered as a possible translation for 
the intmt. If we assign a probability P'r(e\[lfi/) 
to each pair of strings (el, fi/), then according to 
Bayes' decision rule, we have to choose the En- 
glish string that maximizes the I)roduct of the 
English language model Pr(c{) and the string 
translation model r'r(fff\[e{). 
Many existing systems tbr SMT (Wang and 
Waibel, 1997; Niefien et al, 1.(/98; Och and We- 
ber, 1998) make use of a special way of structur- 
ing the string translation model (Brown et al, 
1993): 'l?he correspondence b tween the words 
in the source and the target string is described 
by aligmuents that assign one target word posi- 
tion to each source word position. The prob- 
ability of a certain English word to occur in 
the target string is assumed to depend basically 
only on the source word aligned to it. It is clear 
that this assumption is not always valid tbr the 
translation of naturM languages. It turns out 
that even those approaches that relax the word- 
by-word assumption like (Och et al, 1999) have 
problems with lnany phenomena typical of nat- 
ural languages in general and German in par- 
titular like 
? idiomatic expressions; 
? colnpound words that have to be translated 
by more than one word; 
? long range dependencies like prefixes of 
verbs placed at the end of the sentence; 
? ambiguous words with different meanings 
dependent on the context. 
1081 
Tile parameters of the statistical knowledge 
sources nlentioned above are trained on bi- 
lingual corpora. Bearing ill mind that more 
than 40% of the word tbrms have only been seen 
once in training (see q~,bles 1 and 4), it is obvi- 
ous that the phenomena listed above can hardly 
be learned adequately from the data and that 
the explicit introduction of linguistic knowledge 
is expected to improve translation quality. 
The overall architecture of the statistical 
translation approach is depicted in Figure 1. hi 
this figure we already anticipate the t'aet that 
we will transtbrm the source strings in a certain 
manner. If necessary we can also apply the in- 
verse of these transfbrmations on the produced 
output strings. Ill Section 3 we explain in detail 
which kinds of transtbrmations we apply. 
Source Language Text 
1 
QTransformation ) 
1' fl 
Global Search: 
maximize Pr(el). Pr(f~ lel) 
over e I 
1 I 
l 
Target Language Text 
l J I ~ Lexicon Model Pr(l 1 \]e,) \[ 
Alignment Model \] 
Language Model 
Figure 1.: Architecture of the translation 31)- 
preach based on Bwes' decision rule. 
3 Ana lys is  and  Trans format ion  of  
the Input  
As already pointed ouL we used the inethod 
of transforming the inl)ut string in our experi- 
ments. The advantage of this approach is that 
existing training and search procedures did not 
have to be adapted to new nlodels incorporat- 
ing the information under consideration. On the 
other hand, it would be more elegant to leave 
the decision between different readings, tbr in- 
stance, to the overall decision process in search. 
Tile transtbrmation method however is nlore 3(t- 
equate tbr the preliminary identification of those 
phenonmna relevant br improving the transla- 
tion results. 
3.1 Analysis 
We used GERTWOL,  a German Morphologi- 
cal Analyser (Haapalainen and M~@)rin, 1995) 
and the Constraint Grammar Parser Ibr Ger- 
man GERCG tbr lexical analysis and inorpho- 
logical and syntactic dismnbiguation. For a de- 
scription of the Constraint Grammar approach 
we refer the reader to (Karlsson, 1990). Some 
prel)rocessing was necessary to meet the input 
format requirements of the tools, hi the cases 
where the tools returned lnore thalt one reading, 
either simple heuristics based on domain spe- 
cific pretbrence ruh;s where at)plied or a nlore 
general, non-mnbiguous analysis was used. 
In the following subsections we list some 
transtbrmations we have tested. 
3.2 Separated German Verbprefixes 
Sortie verbs in German consist of a main part 
and a detachable prefix which can be shifted 
to the end of the clause, e.g. "losfahren" ("to 
leave") in the sentence "Ich fahre morgen los.". 
We extr~cted all word forms of separable verbs 
fl:om th.e training corl)us. The resulting list con- 
tains entries of the tbrm prefixlmain. The en- 
try "los\[t:'ahre" indicates, fi)r exalnple, that the 
prefix "los" (:an l)e detached flom the word tbrm 
"fahre". In all clauses containing a word match- 
ing a main part and a word matching the corre- 
sponding prefix part occuring at the end of the 
clause, the prefix is prepended to the beginning 
of the main part, as in "Ich losfahre morgen." 
a.a German Compound Words 
German comt)(mnd words pose special 1)roblems 
to the robustness of a translation method, be- 
cause the word itself must be represented in the 
training data: the occurence of each of the coin- 
t)onents is not enough. The word "I~'iichtetee" 
tbr example can not be translated although its 
coml)onents "Friichte" and "Tee" appear in the 
training set of EUTRANS. Besides, even if the 
coml)ound occurs in training, tile training algo- 
r ithm may not be capable of translating it prop- 
erly as two words (in the nlentioned case the 
words "fl'uit" and "tea") due to the word align- 
ment assumption mentioned in Section 2. We 
1082 
therefore split the COml)ound words into their 
(:Oml)onents. 
3,,4 Annotat ion  w i th  POS Tags  
()he way of hell)|rig the disanfl)iguation of gill- 
t)Jguous words is to annotate them with their 
t)m:l; of Sl)eech (POS) inl'()rmation. We (:hose l;he 
tbllowing very ti'equent short words that often 
(:;rased errors in translation fi)r VERBMO\]3IL: 
"aber"  can 1)e adverb or (:onjun('tion. 
"zu"  can l)e adverb, pret)osition , sepnrated 
verb prefix or infinitive marker. 
%ler' ,  "die" and "das" cnn 17e definite m:ti- 
CIos el' \])1"Ol1Ol111S. 
'.\['he difficulties due to l;hese aml)iguities m:e 
i l lustrated by the fi)lh)wing exmnt)les: The sen- 
tence "Das wiird(' mir sehr gut 1)~ssen. '' is often 
trnnslnted 1)y "Th, e would suit me very well." 
iltsl;e;~(l ()\[ "5l'h,at would suit me very well." and 
"Das win: zu s(:lmcll." is trnnsl;~ted by "Th~Lt 
was to t'~lsl;." instea,(t of "Theft; was too f;~st;.". 
We alTpended the POS l;~g in training a,mt 
t(;st corpus fiTr the VERBMOBII, task (see 4.\]). 
3.5 Merg ing  Phrases  
Some multi-word phrases as ~ whole rel)r(;sent 
a distine(; synta.7"tie rob; in (;he s(mtenT:e. The 
17hra.se "irgend ('.t;w;ls" (%nything")  for exa,m- 
t)1(; m~y form ('it, l,('a: a.n in(h'tinit;('. (h'.t;('.rmino.r 
():c an in(lelinil;e pronoun. Like 2\] other mull;i- 
word tThrases "irg(:nd-et;wa.s" is merged in order 
t;o form one single voca,bulary ('nl;ry. 
3.6 Treatment  o f  Unseen Words  
l"or sl;atist;i(::fl ma(:hin(; tr;mslation it is difficult 
1;() handle woi'ds not seen in training. \]~br m> 
kllOWll i)l;O1)el; ll&llIeS~ i\[; is normally ('(TrreT't to 
t)bme the word un(;h~mge(t into th(; transl~fl;ion. 
We have t)(;(;n working on the l;17ea~l;nlenI; of 1111- 
kll()Wll words of other types. As ~flr(;~dy men- 
l;ioned in Se(:l;ion 3.3, the st)litting of eomt)ound 
words cml reduce |;he nmnber of unknown Cl(:r- 
man words. 
In addit ion, we have examined methods of r(> 
pl~('ing a word \['ullform l)y ~ more ;O)stra('l; word 
form nnd (-heek whether this fi)rm is kn()wn and 
(:;~m l)e I;ranslnted. Th(' l;rmlslat, ioll of the sin> 
|)lifted word tbrm is generally not the precis(' 
trmlslai;ion of the original on(', 17ul; sometimes 
the intended semantics is conveyed, e.g.: 
"ka l tes"  is ~m adjective in the singular neuter 
fOl;lll &lid. c3~11 be  t,l'a, nst:'ornled to the less 
specilic form "kalt" ("cold"). 
"Jahre" ("years") (:~m be replaced by the sin- 
gulm: form "J~fln:". 
"bene idest "  (%o envy" in tirst person singu- 
lar): if the infinitive tbnn "beneiden" is not 
known, it might hell).just, to remove tim 
leading t)artiele "be". 
4 Trans la t ion  Resu l ts  
We use the SSER (sul)jectivc sentence error 
rat(') (Ni('fien et al, 2000) as evaluation cri- 
t('rion: E~wh translated senten(:e is judged by 
~ tmmmi exmniner according 1;(7 nn error scale 
ti'om 0.0 (semantical ly and syntaeti(:~flly co l  
reef) to 1.0 ((:onlt)h;l;ely wrong). 
4.1 Trans la t ion  Resu l ts  for VEm~MOmL 
Th(, VEI{BM()BII, corpus consists of st)onttme- 
ously spoken dialogs in t;he al)t)oint;ment sch(> 
(hfling domain (Wtflflster, 1993). German sen- 
t;ences ;~re l;ra.nsl;~lx;d inl;o English. The output  
of the st)ee('h re(:ognizer (Ibr example th(; single- 
best hyl)othesis ) is used as int)ut to the tr;ms- 
lation moduh',s. For resem:eh tmri/oses the orig- 
inal l;(;xt st)oken 1)y th(, users can t)7, t)r(;sented 
t() the translal;ion system t;(7 ev~flm~te the MT 
(:omponent set)er~ti;ely from l;hc, re(:ognizT~r. 
'l'h('. tra.ining set (:onsist;s (Tf d5 680 s(;nl;o.n(:e 
pairs. Testing was carried out on ~t seper~te 
set of 14:7 senl;enees l;h~fl; (to not contain any 
mlseen words, hi Table 1 l;he ehara('teristics of 
the training sets are summarized for l;he original 
eort)ns and after l;he ai)plication of the des(:rit)ed 
tr~Lnsfornlat;ion.s on t;he Gerlll}~tll part of l;he co l  
pus. \[l.'he tM)le shows that  on t;his cou)us Ill(', 
splitting of (:Oml)OUll(ts iinl)roves l;hc l;oken-tyl)e 
rntio t iom 59.7 t(7 65.2, lint th(', mmfl)er of singh;- 
tons (words s(;en only on('e in tt'nhfing) does not 
go down by more than 2.8%. '.l'he oth.er trans- 
fi)rm~tions (i)r(;1)ending separated verb 1)refixe,~ 
"t)ref"; mineral;ion wi|;h 1)OS t~gs "i)os"; merg- 
ing of phrases "merge") do not at\[bet hese co> 
pus st;,l;isl;ies much. 
The translntion l)erformmme results are given 
in rl2~fi)le 2 tbr tra.nslat;ion of text and in 'l~fi)le 
3 for translation of t;he single-best hyl)oth(!sis 
given t)y a sl)eech recognizer (a('(:m:a.('y 69%). 
For t)oth cases, l;r;mslation on text ml(t on 
st)ee(:h int)ut , st)litting (:oml)oml(t words does 
1083 
Table 1: Corpus statistics: VERBMOBIL train- 
ing ( "baseline" =no preprocessing). 
preprocessing 
English 465 143 
Gerlnan 
baseline 
verb prefixes 
split compounds 
pos 
pos+merge 
pos+merge+pref 
no. of no. of single- 
tokens types tons 
4382 37.6% 
437968 7335 44.8% 
435 686 7370 44.3% 
442938 6794 42.0% 
437972 7 344 44.8% 
437330 7363 44.7% 
435055 7397 44.2% 
not iml)rove translation quality, but it is not 
harmful either. The treatment of separable pre- 
fixes helps as does annotating some words with 
part of speech inibrmation. Merging of 1)hrases 
does not improve the quality much further. The 
best translations were adfieved with the combi- 
nation of POS-annotation, phrase merging and 
prepending separated verb prefixes. This holds 
tbr t)oth translation of text and of speech input. 
Table 2: Results on VERBMOBIL text intmt. 
preprocessing SSER \[%\] 
baseline 
verb prefixes 
split compounds 
pos 
pos+merge 
pos+merge+pref 
20.3 
19.4 
20.3 
19.7 
19.5 
18.0 
The fact that these hard-coded transtbrma- 
tions are not only hclpflfl on text input, but 
also on speech input is quite encouraging. As 
an example makes clear this cannot be taken 
for granted: The test sentence "Dann fahren 
wir dann los." is recognized as "Dam1 fahren wir 
dann uns." and the tact that separable verbs do 
not occur in their separated form in the train- 
ing data is mffavorable in this case. The fig- 
ures show that in generM the speech recognizer 
output contains enough information for helpflfl 
preprocessing. 
Table 3: Results on VERBMOBIL speech inlmt. 
preprocessing 
baseline 
verb prefixes 
split compounds 
split+pref 
pos+merge+pref 
ssEa \[%1 
43.4 
41.8 
43.1 
42.3 
41.1 
4.2 Translat ion Results for EUTRANS 
The EUTRANS corpus consists of different 
types of German-English texts belonging to the 
tourism domain: web pages of hotels, touris- 
tic brochures and business correspondence. The 
string translation and language model parame- 
ters were trained on 27 028 sentence pairs. The 
200 test sentences contain 150 words never seen 
in training. 
Table 4 summarizes the corpus statistics of 
the training set for the original corpus, af- 
ter splitting of compound words and after ad- 
ditional prepending of seperated verb prefixes 
("split+prefixes"). The splitting of compounds 
improves the token-type ratio flom 8.6 to 12.3 
and the nmnber of words seen only once in train- 
ing reduces by 8.9%. 
Table 4: Corpus statistics: EUTRANS. 
preprocessing no. of 
tokens 
English 562 264 
German 
baseline 
split compounds 
split+prefixes 
499 217 
535 505 
534 676 
no. of single- 
types tons 
33 823 47.1% 
58317 58.9% 
43 405 50.0% 
43 407 49.8% 
Tile mlmber of words in the test sentences 
never seen in training reduces from 150 to 81 by 
compound splitting and can further be reduced 
to 69 by replacing the unknown word forms by 
more general forms. 80 unknown words are en- 
countered when verb prefixes are treated in ad- 
dition to compound splitting. 
Experiments for POS-annotation have not 
been pertbrmed on this corpus because no small 
set of ambiguous words causing many of the 
1084 
translation errors on this |;ask can be identified: 
Comt)ared to |;it(', VERBMOBIL task, this tort)us 
is less homogeneous. Merging of 1)hrases did not 
help much on VEI/,BMOBIL and is theretbre not 
tested here. 
Tal)le 5 shows that the splitting of comt)ound 
words yields an improvement in the subjective 
sentence rror rate of 4.5% and the treatment 
of unknown words ("unk") improves the trans- 
lation quality by an additional 1%. Treating 
SOl)arable verb 1)refixes in addition to splitting 
compounds gives the be, st result so far with an 
improvement of 7.1% absolute COml)ared to the 
l)aseline. 
Table 5: Results on EUTRANS. 
1)ret)rocessing SSER \[%\] 
1)aseline 57.4 
split comi)ounds 52.9 
sl) l it+lmk 51.8 
split+prefixes 50.3 
5 Conclusion and Future Work  
In this paper, we have presented some methods 
of providing morphological im syntactic intbr- 
mat|on tbr improving the 1)ertbrmance of sta- 
tistical machine trallslation. First ext)eriments 
prove their general aplflicalfility to reMistic and 
comI)lex tasks such as spontaneously spoken di- 
alogs. 
We are. 1)lamfing to integrate the al)t)roach 
into the search process. We are also working 
on language models and translation models that 
use mort)hological categories for smoothing in 
the case  of unseen events. 
Acknowledgement. This work was partly 
supported by the German FederM Ministry of 
Education, Science, Research and Technology 
under the Contract Number 01 IV 701 q_'4 
(VERBMOBIL) and as part of the EUTRANS 
project by the European Comnmnity (ESPRIT 
project number 30268). 
The authors would like to thank Gregor 
Leusch tbr his support in implementation. 
References 
P.F. Brown, S.A. Della Pietra, V.J. 
Della Pietra, and ILL. Mercer. 1993. 
Mathematics of Statistical Machine %'ansla- 
tion: Parameter Estimation. Computational 
Linguistics, 19(2):263 311. 
Mariikka Haapalainen and Ari Majorin. 1995. 
GERTWOL und Morphologische Disambi- 
guierung fiir das Deutsche. URL: 
www.lingsoft.fi/doc/gercg/NODALIDA-poster.html. 
Fred Karlsson. 1990. Constraint Grmnmar as 
a Frainework tbr Parsing Running Text. In 
PTvecedings of th, e 13th, hzternational Confer- 
cnce on Computational Linguistics, volume 3, 
pages 168-173, Helsinki, Finland. 
Sonja Niefien, Stephan Vogel, Hermann Ney, 
and Christoph Tilhnann. 1998. A DP based 
Search Algorithm tbr Statistical Machine 
Translation. In Proceedings of the 36th An- 
nual Con:ferencc of the Association for Com- 
putational Linguistics and the 17th Interna- 
tional Conference on Computational Linguis- 
ties, pages 960 967, Montrdal, P.Q., Canada, 
August. 
Sonja Niefien, Franz loser Oeh, Gregor Leusch, 
and Hermaml Ney. 2000. An Ewfluation Tool 
tbr Machine %'anslation: Fast Evaluation 
for MT Research. In Proceedings of the 2nd 
International Conference on Language Rc- 
so'arccs and Evaluation, pages 39 45, Athens, 
Greece, May. 
Franz .losef Och and Hans Weber. 1998. hn- 
t)roving Statistical Natural Language ~:ans- 
lation with Categories and Rules. In Pro- 
eccdings of the 36th Annual Con.fcrcncc of 
th, e Association for Computational Linguis- 
tics and the 17th international Conference on 
Computational Linguistics, pages 985-989, 
Montrdal, P.Q., Canada, August. 
Iq:anz ,loser Och, Christol)h Tillmmm, aim Her- 
maml Ney. 1999. hnproved Alignment Mod- 
els tbr Statistical Machine Translation. In 
Proceedings of the Co~:ference on Empirical 
Methods in Natu~nl Language Processing and 
Very Large Corpora, pages 20-28, University 
of Maryland, College Park, Maryland, June. 
Wolfgang Wahlster. 1993. Verl)mobih Transla- 
lion of Face-to-Face Dialogs. In Proceedings 
of the MT Summit IV, pages 127-135, Kobe, 
Japan. 
Ye-Yi Wang and Alex Waibel. 1997. Decod- 
ing Algorithm in Statistical %'anslation. In 
Proceedings of the A CL/EA CL '97, Madrid, 
Spain, pages 366 372, July. 
1085 
A Compar i son  of  A l ignment  Mode ls  for S ta t i s t i ca l  Mach ine  
Trans la t ion  
Franz Josef Och and Hermann Ney 
Lehrstuhl fiir Informatik VI, Comlmter Science Department 
RWTH Aachen - University of Technology 
D-52056 Aachen, Germany 
{och, ney}~inf ormat ik. ruth-aachen, de 
Abst ract  
In this paper, we t)resent and compare various align- 
nmnt models for statistical machine translation. We 
propose to measure tile quality of an aligmnent 
model using the quality of the Viterbi alignment 
comt)ared to a manually-produced alignment and de- 
scribe a refined mmotation scheme to produce suit- 
able reference alignments. We also con,pare the im- 
pact of different; alignment models on tile translation 
quality of a statistical machine translation system. 
1 I n t roduct ion  
In statistical machine translation (SMT) it is neces- 
sm'y to model the translation probability P r ( f l  a Ic~). 
Here .fi' = f denotes tile (15'ench) source and e{ = e 
denotes the (English) target string. Most SMT 
models (Brown et al, 1993; Vogel et al, 1996) 
try to model word-to-word corresl)ondences between 
source and target words using an alignment nmpl)ing 
from source l)osition j to target position i = aj. 
We can rewrite tim t)robal)ility Pr(fille~) t) 3, in- 
troducing the 'hidden' alignments ai 1 := al ...aj...a.l 
(aj C {0 , . . . , /} ) :  
Pr(f~lel) = ~Pr(f i ' ,a~le{) 
.1 
? j -1  I~ = E H Pr(fj 'ajlf i '-"al 'e l )  
q, j=l 
To allow fbr French words wlfich do not directly cor- 
respond to any English word an artificial 'empty' 
word c0 is added to the target sentence at position 
i=0.  
The different alignment models we present pro- 
vide different decoInt)ositions of Pr(f~,a~le(). An 
alignnlent 5~ for which holds 
a~ = argmax Pr(fi' , a'l'\[eI) 
at 
for a specific model is called V i terb i  al ignment of" 
this model. 
In this paper we will describe extensions to tile 
Hidden-Markov alignment model froln (Vogel et al, 
1.996) and compare tlmse to Models 1 - 4 of (Brown 
et al, 1993). We t)roI)ose to measure the quality of 
an alignment nlodel using the quality of tlle Viterbi 
alignment compared to a manually-produced align- 
ment. This has the advantage that once having pro- 
duced a reference alignlnent, the evaluation itself can 
be performed automatically. In addition, it results in 
a very precise and relia.ble valuation criterion which 
is well suited to assess various design decisions in 
modeling and training of statistical alignment mod- 
els. 
It, is well known that manually pertbrming a word 
aligmnent is a COlnplicated and ambiguous task 
(Melamed, 1998). Therefore, to produce tlle refer- 
ence alignment we use a relined annotation scheme 
which reduces the complications and mnbiguities oc- 
curring in the immual construction of a word align- 
ment. As we use tile alignment models for machine 
translation purposes, we also evahlate the resulting 
translation quality of different nlodels. 
2 Al ignment  w i th  HMM 
In the Hidden-Markov alignment model we assume 
a first-order dependence for tim aligmnents aj and 
that the translation probability depends Olfly on aj 
and not  Oil (tj_l: 
- ~-' el) =p(ajl. j-,,Z)p(J~l%) Pr(fj,(glf~ ',% , 
Later, we will describe a refinement with a depen- 
dence on e,,j_, iu the alignment model. Putting 
everything together, we have the following basic 
HMM-based modeh 
.1 
*'(flJl~I) = ~ I I  \[~,(-jla~.-,, z). p(fj l%)\] (1) 
at j= l  
with the alignment I)robability p(ili',I ) and the 
translation probability p(fle). To find a Viterbi 
aligninent for the HMM-based model we resort to 
dynamic progralnming (Vogel et al, 1996). 
The training of tlm HMM is done by the EM- 
algorithm. In the E-step the lexical and alignment 
1086 
counts for one sentenee-i)air (f, e) are calculated: 
c(flc; f, e) = E P"(a l f '  e) ~ 5(f, f~)5(e, c~) 
a i,j 
,.:(ill', z; f, e) = E / ' , ' (a i r ,  e) aj) 
a j 
In the M-step the lexicon and translation probabili- 
ties are: 
p(f le) o< ~-~c(fle;f('~),e (~)) 
8 
P( i l i ' , I )  o (Ec ( i l i ' , I ; fO) ,e (~) )  
8 
To avoid the smlunation ov(;r all possible aligmnents 
a, (Vogel et el., 1996) use the maximum apllroxima- 
tion where only the Viterbi alignlnent )ath is used to 
collect counts. We used the Baron-Welch-algorithm 
(Baum, 1972) to train the model parameters in out' 
ext)eriments. Theret/y it is possible to t)erti)rm an 
efl-iciellt training using; all aligmnents. 
To make the alignlnenl; t)arameters indo,1)en(lent 
t'ronl absolute word i)ositions we assmne that the 
alignment i)robabilities p(i\[i', I )  (lel)end only Oil the 
jmnp width (i - i'). Using a set of non-negative 
t)arameters {c(i - i ' )} ,  we can write the alignment 
probabilities ill the fl)rm: 
~'(i - i') (2) p(i l i ' ,  I)  = 
c(,,:" - i ' )  
This form ensures that for eadl word posilion it, 
i' = 1, ..., I , the aligmnent probat)ilities atis(y th(, 
normalization constraint. 
Extension:  refined a l igmnent mode l  
The count table e(i - i') has only 2.1  ......... - 1 en- 
tries. This might be suitable for small corpora, but 
fi)r large corpora it is possil)le to make a more re- 
fine(1 model of Pr (a j  ~i-I  i - I  Ji ,% ,c'~). Est)ecially, we 
analyzed the effect of a det)endence on c,b_ ~ or .fj. 
As a dependence on all English words wouht result 
ill a huge mmflmr of aligmnent 1)arameters we use as 
(Brown et el., 1993) equivalence classes G over tlle 
English and the French words. Here G is a mallping 
of words to (:lasses. This real)ping is trained au- 
tonmtically using a modification of the method de- 
scrilled ill (Kneser and Ney, 1991.). We use 50 classes 
in our exlmriments. The most general form of align- 
ment distribution that we consider in the ItMM is 
p(aj - a.+_, la(%), G(f~), h -  
Extension:  empty  word 
In the original formulation of the HMM alignment 
model there ix no 'empty' word which generates 
Fren(:h words having no directly aligned English 
word. A direct inchlsion of an eml/ty wor(t ill the 
HMM model by adding all c o as in (Brown et al, 
1.993) is not 1)ossit)le if we want to model the j un lp  
distances i - i', as the I)osition i = 0 of tim emt)ty 
word is chosen arbitrarily. Therefore, to introduce 
the eml)ty word we extend the HMM network by I 
empty words ci+ 1.'2I The English word ci has a co l  
rest)onding eml)ty word el+ I. The I)osition of the 
eml)ty word encodes the previously visited English 
word. 
We enforce the following constraints for the tran- 
sitions in the HMM network (i _< I, i' _< I): 
p(i  + I l i ' , I )  = pff . 5( i , i ' )  
V(i + I l l '  + I, I )  = J J .  5( i , i ' )  
p(i l i '  + I, 1) = p(iIi ' ,1) 
The parameter pff is the 1)robability of a transition 
to the emt)ty word. In our extleriments we set pIl = 
0.2. 
Smooth ing  
For a t)etter estimation of infrequent events we in- 
troduce the following smoothing of alignment )rob- 
abilities: 
1 
F(a j I~ j - , ,~)  = ~" ~- + (1 - , , ) .p (a j la j _ l  , I )  
in our exlleriments we use (t = 0.4. 
3 Mode l  1 and  Mode l  2 
l~cl)lacing the (l(~,t)endence on aj - l  in the HMM 
alignment mo(M I)y a del)endence on j, we olltain 
a model wlfich (:an lie seen as a zero-order Hid(l(m- 
Markov Model which is similar to Model 2 1)rot)ose(t 
t/y (Brown et al, 1993). Assmning a mfiform align- 
ment prol)ability p(i l j ,  I )  = 1/1, we obtain Model 
1. 
Assuming that the dominating factor in the align- 
ment model of Model 2 is the distance relative to the 
diagonal line of the (j, i) plane the too(tel p(i l j  , I)  can 
1)e structured as tbllows (Vogel et al, 1996): 
,'(i -, 
- (3) v(ilj, 5 = Ei,=t r ( ' i '  l 
This model will be referred to as diagonal-oriented 
Model 2. 
4 Mode l  3 and  Mode l  4 
Model:  The fertility models of (Brown et el., 1993) 
explicitly model the probability l,(?lc) that the En- 
glish word c~ is aligned to 
4,, = E 
J 
\]~rench words. 
1087 
Model 3 of (Brown et al, 1993) is a zero-order 
alignment model like Model 2 including in addi- 
tion fertility paranmters. Model 4 of (Brown et al, 
1993) is also a first-order alignment model (along 
the source positions) like the HMM, trot includes 
also fertilities. In Model 4 the alignment position 
j of an English word depends on the alignment po- 
sition of tile previous English word (with non-zero 
fertility) j ' . It models a jump distance j - j '  (for con- 
secutive English words) while in the HMM a jump 
distance i - i '  (for consecutive French words) is mod- 
eled. Tile full description of Model 4 (Brown et al, 
1993) is rather complica.ted as there have to be con- 
sidered tile cases that English words have fertility 
larger than one and that English words have fertil- 
ity zero. 
For training of Model 3 and Model 4, we use an 
extension of the program GlZA (A1-Onaizan et al, 
1999). Since there is no efficient way in these mod- 
els to avoid tile explicit summation over all align- 
ments in the EM-algorithin, the counts are collected 
only over a subset of promising alignments. It is not 
known an efficient algorithm to compute the Viterbi 
alignment for the Models 3 and 4. Therefore, the 
Viterbi alignment is comlmted only approximately 
using the method described in (Brown et al, 1993). 
The models 1-4 are trained in succession with the 
tinal parameter values of one model serving as the 
starting point tbr the next. 
A special problein in Model 3 and Model 4 con- 
cerns the deficiency of tile model. This results in 
problems in re-estimation of the parameter which 
describes the fertility of the empty word. In nor- 
real EM-training, this parameter is steadily decreas- 
ing, producing too many aligmnents with tile empty 
word. Therefore we set tile prot)ability for aligning 
a source word with tile emt)ty word at a suitably 
chosen constant value. 
As in tile HMM we easily can extend the depen- 
dencies in the alignment model of Model 4 easily 
using the word class of the previous English word 
E = G(ci,), or the word class of the French word 
F = G(I j)  (Brown et al, 1993). 
5 Inc lud ing  a Manual Dictionary 
We propose here a simple method to make use of 
a bilingual dictionary as an additional knowledge 
source in the training process by extending the train- 
ing corpus with the dictionary entries. Thereby, the 
dictionary is used already in EM-training and can 
improve not only the alignment fox" words which are 
in the dictionary but indirectly also for other words. 
The additional sentences in the training cortms are 
weighted with a factor Fl~x during the EM-training 
of the lexicon probabilities. 
We assign tile dictionary entries which really co- 
occur in the training corpus a high weight Fle.~. and 
the remaining entries a vex'y low weight. In our ex- 
periments we use Flex = 10 for the co-occurring dic- 
tionary entries which is equivalent to adding every 
dictionary entry ten times to the training cortms. 
6 The Al ignment Template  System 
The statistical machine-translation method descri- 
bed in (Och et al, 1999) is based on a word aligned 
traiifing corIms and thereby makes use of single- 
word based alignment models. Tile key element of 
tiffs apt/roach are the alignment emplates which are 
pairs of phrases together with an alignment between 
the words within tile phrases. The advantage of 
the alignment emplate approach over word based 
statistical translation models is that word context 
and local re-orderings are explicitly taken into ac- 
count. We typically observe that this approach pro- 
duces better translations than the single-word based 
models. The alignment templates are automatically 
trailmd using a parallel trailxing corlms. For more 
information about the alignment template approach 
see (Och et at., 1999). 
7 Resu l ts  
We present results on the Verbmobil Task which is 
a speech translation task ill the donmin of appoint- 
nxent scheduling, travel planning, and hotel reserva- 
tion (Wahlster, 1993). 
We measure the quality of tile al)ove inentioned 
aligmnent models with x'espect to alignment quality 
and translation quality. 
To obtain a refereuce aligmnent for evaluating 
alignlnent quality, we manually aligned about 1.4 
percent of onr training corpus. We allowed the hu- 
mans who pertbrmed the alignment o specify two 
different kinds of alignments: an S (sure) a, lignment 
which is used for alignmelxts which are unambigu- 
ously and a P (possible) alignment which is used 
for alignments which might or might not exist. The 
P relation is used especially to align words within 
idiomatic expressions, free translations, and missing 
function words. It is guaranteed that S C P. Figure 
1 shows all example of a manually aligned sentence 
with S and P relations. The hunxan-annotated align- 
ment does not prefer rely translation direction and 
lnay therefore contain many-to-one and one-to-many 
relationships. The mmotation has been performed 
by two annotators, producing sets $1, 1~, S2, P2. 
Tile reference aliglunent is produced by forming the 
intersection of the sure aligmnents (S = $1 rqS2) and 
the ration of the possible atignumnts (P = P1 U P'2). 
Tim quality of an alignment A = { (j, aj) } is mea- 
sured using the following alignment error rate: 
AER(S, P; A) = 1 - IA o Sl + IA o Pl 
IAI + ISl 
1088 
that  . . . . . . . . .  \ [ \ ]  
at  . . . . . . . . .  \ [ \ ]  
. . . . . . .  V1V1.  
l eave  . . . . . . .  \[---'l \ [ - "~ " 
. . . . . . .  l i E \ ] .  
l e t  . . . . . . .  C l l -1  " 
e . . . . . .  ? . . . .  
say  . . . . .  ? . . . . .  
would " ? . . . . . . .  
T . . . .  ? . . . . . .  
then"  " ? . . . . . . . .  
? \ [ \ ]  . . . . . . . .  o 
yes  ? . . . . . . . . . .  
-rn I:I '13 O ? ? -~t ~1 
J~ 
o 
Figure i: Exmnple of a manually annotated align- 
ment with sure (filled dots) and possible commotions. 
Obviously, if we colnpare the sure alignnlents of ev- 
ery sitigle annotator with the reference a.ligmnent we 
obtain an AEI{ of zero percent. 
~\[ifl)le l.: Cort)us characteristics for alignment quality 
experiments. 
Train Sente iH : ( i s  
Words 
Vocalmlary 
Dictionary Entries 
Words 
Test Sentences 
Words 
German I English 
34 446 
329 625 / 343 076 
5 936 \] 3 505 
4 183 
4 533 I 5 324 
354 
3 109 I 3 233 
Tal)le 1 shows the characteristics of training and 
test corlms used in the alignment quality ext)eri- 
inents. The test cortms for these ext)eriments (not 
for the translation exl)eriments) is 1)art of the train- 
ing corpus. 
Table 2 shows the aligmnent quality of different 
alignment models. Here the alignment models of 
IIMM and Model 4 do not include a dependence 
on word classes. We conclude that more sophisti- 
cated alignment lnodels are crtlcial tbr good align- 
ment quality. Consistently, the use of a first-order 
aligmnent model, modeling an elnpty word and fer- 
tilities result in better alignments. Interestingly, the 
siinl)ler HMM aligninent model outt)erforms Model 
3 which shows the importance of first-order align- 
ment models. The best t)erformanee is achieved 
with Model 4. The improvement by using a dictio- 
nary is small eomI)ared to the effect of using 1)etter 
a.lignmellt models. We see a significant dill'erence 
in alignment quality if we exchange source and tar- 
get languages. This is due to the restriction in all 
alignment models that a source language word can 
1)e aligned to at most one target language word. If 
German is source language the t'requelltly occurring 
German word coml)ounds, camlot be aligned cor- 
rectly, as they typically correspond to two or more 
English words. 
WaNe 3 shows the effect of including a det)endence 
on word classes in the aligmnent model of ItMM or 
Model 4. By using word classes the results can be 
Table 3: Eft'cot of including a det)endence on word 
classes in the aligmnent model. 
AER \[%\] 
Det)endencies -IIMM I Model 4 
no 8.0 6.5 
source 7.5 6.0 
target 7.1 6.1 
source ? target 7.6 6.1 
improved by 0.9% when using the ItMM and by 0.5% 
when using Model 4. 
For the translation experiments we used a differ- 
ent training and an illdetmndent test corpus (Table 
4). 
Table 4: Corlms characteristics for translation (tual- 
it;.), exlmriments. 
Train 
S ~e,t 
Sentences  
Words 
Vocabulary 
Se l l te l lees  
Words 
PP (trigram LM) 
I German English 
58332 
519523 549921 
7 940 4 673 
147 
1968 2173 
(40.3) 28.8 
For tile evMuation of the translation quality we 
used the automatically comlmtable Word Error Rate 
(WEll.) and the Subjective Sentence Error Rate 
(SSEll,) (Niefien et al, 2000). The WEll, corre- 
spomls to the edit distance t)etween the produced 
translation and one t)redefined reference translation. 
To obtain the SSER the translations are classified by 
human experts into a small number of quality classes 
ranging from "l)ertbet" to "at)solutely wrong". In 
comparison to the WEll,, this criterion is more mean- 
ingflfl, but it is also very exl)ensive to measure. The 
translations are produced by the aligmnent template 
system mentioned in the previous ection. 
1089 
Table 2: Alignment error rate (AER \[%\]) of ditl~rent alignment models tbr the translations directions English 
into German (German words have fertilities) and German into English. 
English -+ German German -~ English 
Dictionary no yes no yes 
Empty Word no lYes yes no l yes yes 
Model 1 17.8 16.9 16.0 22.9 21.7 20.3 
Model 2 12.8 12.5 11.7 17.5 17.1 15.7 
Model 2(diag) 11.8 10.5 9.8 16.4 15.1 13.3 
Mode l  3 10.5 9.3 8.5 15.7 14.5 12.1 
HMM 10.5 9.2 8.0 14.1 12.9 11.5 
Model 4 9.0 7.8 6.5 14.0 12.5 10.8 
Table 5: Effect of different alignment models on 
translation quality. 
Alignlnent Model 
in Training WER\[%\] SSER\[%\] 
Model 1 49.8 22.2 
HMM 47.7 19.3 
Model 4 48.6 16.8 
The results are shown in Table 5. We see a clear 
improvement in translation quality as measured by 
SSER whereas WER is inore or less the same for all 
models. The imwovement is due to better lexicons 
and better alignment templates extracted from the 
resulting aliglunents. 
8 Conclusion 
We have evaluated vm'ious statistical alignment 
models by conlparing the Viterbi alignment of the 
model with a human-made alignment. We have 
shown that by using inore sophisticated models the 
quality of the alignments improves ignificantly. Fur- 
ther improvements in producing better alignments 
are expected from using the HMM alignment model 
to bootstrap the fertility models, fronl making use of 
cognates, and from statistical lignment models that 
are based on word groups rather than single words. 
Acknowledgment 
This article has been partially supported as 
part of the Verbmobil project (contract nmnber 
01 IV 701 T4) by the German Federal Ministry of 
Education, Science, Research and Technology. 
References 
Y. A1-Onaizan, J. Cur\]n, M. Jahr, K. Knight, J. Laf- 
ferty, I. D. Melamed, F. a. Och, D. Purdy, N. A. 
Smith, and D. Yarowsky. 1999. Statistical ina- 
chine translation, final report, JHU workshop. 
http ://www. clsp. j hu. edu/ws99/proj ects/mt/ 
f inal_report/mr- f inal-report, ps. 
L.E. Baum. 1972. An Inequality and Associated 
Maximization Technique in Statistical Estimation 
for Probabilistie Functions of Markov Processes. 
Inequalities, 3:1 8. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1993. The mathenlatics ofsta- 
tistical machine trmlslation: Parameter estima- 
tion. Computational Linguistics, 19(2):263-311. 
R. Kneser and H. Ney. 1991. Forming Word Classes 
by Statistical Clustering for Statistical Langm~ge 
Modelling. In 1. Quantitative Linguistics Conf. 
I. D. Melamed. 1998. Manual mmotation of transla- 
tional equivalence: The Blinker project. Technical 
Report 98-07, IRCS. 
S. Niegen, F. J. ()ch, G. Leusch, and H. Ney. 
2000. An evaluation tool \]'or machine translation: 
Fast evaluation for mt research. In Proceedings of 
the Second International Conference on Language 
Resources and Evaluation, pages 39-45, Athens, 
Greece, May June. 
F. J. Och, C. Tilhnalm, mid H. Ney. 1999. Improved 
alignment models for statistical machine transla- 
tion. In In Prec. of the Joint SIGDAT Co~? on 
Empirical Methods in Natural Language Process- 
ing and Very LaTye Corpora, pages 20-28, Univer- 
sity of Marylmld, College Park, MD, USA, June. 
S. Vogel, H. Ney, and C. Tilhnann. 1996. HMM- 
based word alignment in statistical translation. 
In COLING '96: The 16th Int. Conf. on Compu- 
tational Linguistics, pages 836-841, Copenhagen, 
August. 
W. Wahlster. 1993. Verbmobil: Translation of face- 
to-face dialogs. In P~vc. of the MT Summit IV, 
pages 127-135, Kobe, Jat)an. 
1090 
Const ruct ion  of  a Hierarchica l  \ ]?ans la t ion  Memory  
S. Vogel, H. Ney 
Lehrs tuh l  flit In tbrmat ik  VI, Computer  Science Depar tment  
1{1~7~111 Aach(',n Univers i ty  of T(~chnology 
1)-52056 Aachen,  Gerinm~y 
Elnaih voge l@in format ik ,  rwth -aachen,  de 
Abst rac t  
_q}:anslation memories are t)ronfising devi('es for 
artt;omati(- translation. Their main weakuess, 
however, is poor coverage, on llllSeell {;ex|;. \]ill 
this t)at)er, l;he use of a hierarchical ;ransla- 
tion memory, (:onsisting of a ('as(:ade of finite 
si;~d;e transducers, is t)rot)os(;d. A mmfl)er of 
tr~nsdu(:e, rs is al)l)\]ied to (;onverl; s(;ni;enee 1)airs 
fl:om a t)ilingual cortms into translat ion pat- 
terns, which are then used as a translat ion me, m- 
ory. Pr(;l iminary results on the (\]erman English 
V ERBMOIIIL ('orl)us a,re given. 
1 In t roduct ion  
In reeenl; years, exa,int)le-1)ased t;l"ansl~l;i<)l~ has 
been 1)rol)osed as an efli<:ient ~n(;t;llo<l for auto- 
m~d;i(: translation (Sal;o and Nag;to, 1990; Ki- 
tan(), 1993; Brown, \]99(i). 'lli'anslations are 
sl;()l;ed il l a t ra l l s la i ; i ( ) l l  l l le. l l lory tloll(t llso, d t;o coi1- 
SI;YllCI; trauslations for new sealten(:e.s. In its sin> 
1)lest version, examl)le-1)ased translat ion boils 
down to l l S i l lg  a (tat;fl)ase of SOllrce sell(;el l(;es 
with their l;rmlslations. For many translat;ion 
tasks, esl)eeially in coml)ul;er assisl;cd |;ransla- 
tion, this at)l)roa(:h works with greal; success. 
For flflly aul;onlat;i(" l;ranslal;ion the main t ) ro t )  - 
\ ]em is t )oor  COVel'a~e oi1 l leW data. To overco l I le  
this weakness, it hierar(:hi(:al translation llleln- 
ory is prot)osed. Al)plying a cascade of tinite 
sti%te |;ra, i ls(hl( 'ers~ a~ SOllrce Se l l te l l ce  is {;ralis- 
laW, d into the tin:get language. 
2 The  Transducers  
2.1 Overv iew 
A translat;ion lnemory is siml)ly a eolle(:|;ion 
of source-l;arge3; string i)airs. As a tirst Stel) ~ 
these translat ion examt)les (:all be (:onverted 
inl;o translat ion 1)atl;(.q:ns t)y lilt;reducing cate- 
gory \]abels, e.g. tbr prol)er nmnes or numbers. 
3.'0 make the translat ion patterns even more use- 
ful, not only single words but comph;x phrases 
can be replace.d by category labels. Which 
phrases t;o select for categorization depends on 
the aplflication, l,br example, the corpus lls0.d 
for this si;udy coal;alas many time and date ex- 
pressions. Therefore, a specialized |;ransduce, r 
was constructed to recognize and translal;e such 
e, xl)ressions. 
Each transducer is a se~ of quadrut)les of the 
tbrm: 
label # source pal;t;ern # l;arget; t)atl;ern # score, 
Som'ce l)al;terns and target patterns may con- 
tain category labels. We call su(:h l)atterns 
~(:ompomldL ~.l.~:ansdueea's working only on the 
word level are (:ailed 'simple'. \]if a la:ans- 
dll(:e,r coal;alas recursive p:tl;terns, e.g. \])ATE # 
\])NPE lind \])ATE # I)AS?I'~ and \])ATI'3 # -3.0, it; 
has |;o be. apl)lied re.cursively t;o t;he input;. 
The scores a,t|;a(:hed to the translation t)at -
terns can be viewed ns translat ion scores. They 
are llse, d to bi~ts towards 1;he selection of lollg(;r 
part;eras and towards lliore likely translations 
in I;hose cases where several targol; patterns are 
associated with ()lie SOl l rce  t)a,l;i;ern. 
'.l'he transducers can be applied in 1)oth di- 
rections, i.e. for a given language pair, each 
language can be viewed as source language. 
Thcrel)y, bil ingual abeling is possilfle. This can 
l)e applied to convert a bilingual corlms into a 
selection of translat ion l)atterns which are. for- 
mulated in terms of words and ('ategory lal)els. 
2.2 Const ruct ion  o f  the  Transducers  
The transducers should t)e selected in such a 
way am to minimize l;he lle, ed tbr recursive ap- 
t)li(:al;ion in order l;o lint)rove efficiency. There- 
tbre, |;11(' l)atl;erns to search tbr are l)artitioned to 
forln a ('as(:ade of t;ransducers. Sonic trans(luc- 
ers analys(,' l)arts of the senten(:e and rel)la(:e it 
1131 
by a category label, which is then used at a later 
step by another transducer. The labeling of the 
days of the week or the names of the months is 
a prerequisite to apply more complex patterns 
for date expressions. The transducers currently 
used are listed in Table 1. 
'Fable 1: List of transducers. 
1. names (persons, towns, places, events, etc) 
2. spelling (e.g. 'D A double L') 
3. numbers (ordinal, cardinal, fractions, etc) 
4. time and date expressions 
5. parts of speech (tbr certain word classes) 
6. grammar (noun phrases, verb phrases) 
Some transducers are general in scope, e.g. 
the transducers for numbers, part of speech tags 
and grammar. Others are costumized towards 
the domain tbr which the translation system is 
developed. In tile VERBMOBIL corpus, which is 
used for the experiments, time and date expres- 
sions are very prominent. To recognize these 
expressions, a small grammar has been devel- 
oped and coded as finite state transducer. Ac- 
tually, two transducers are used. On the first 
level, words are replaced by labels, like DAY- 
OFWEEK = { Montag, Dienstag, ...}. On the 
second level, these labels are used to t'orm com- 
plex time and date expressions. This second 
transducer works recursively, as simpler expres- 
sions are used to build more complex expres- 
sions. 
Finally, a small grammar based on POS (part 
of speech) tags has been crafted mamlally. The 
purpose of this grammar is to recognize simple 
noun phrases. Extensions to handle the differ- 
ent word ordering in the verb phrases arc under 
development. 
2.3 Scoring 
The scores attached to the translation patterns 
can be viewed as a kind of translation scores. 
In the current implementation a rather crude 
heuristic together with some manual tuning in 
the grammar transducer is applied. The idea 
is to give preference to longer translation pat- 
terns as they take more context into account 
and encode word reordering in an explicit man- 
ner. Thus, fbr simple and compound translation 
patterns the score is exponential to the length 
of the source pattern. Tile scores are negative 
by convention: not translating a word gives zero 
cost, translating it gives a benefit, i.e. negative 
costs. In future, scoring will be refined by using 
corpus statistics to assign probabilities to the 
translation patterns. 
2.4 Bilingual Labeling 
The sentence pairs ill the bilingual training cor- 
pus can be segmented into shorter segments 
with the help of an alignment progrmn (Och et 
al., 1999). This collection of segments could be 
used directly as a translation memory. However, 
to improve the coverage on unseen data, these 
segnmnts are labeled. Applying the transducers 
as given in Table 1 transfbrms these segments 
into compound t)hrases. 
The procedure is as follows: 
1. For each transducer taken from the com- 
plete cascade - as given in Table 1 ap- 
lilY the transducer to both, the source and 
tlm target sentences of the bilingual train- 
ing cortms. 
2. Find those sentence pairs which contain 
equal number and types of category labels 
tbr both sentences. 
3. For sentence pairs which do not match in 
mmflmr and type of the category labels 
keep the original sentence pair. 
Table 2 shows examples of some translation 
patterns which resulted flom bilingual abeling. 
3 Applying the Transducers 
The working of the transducers i best described 
as tile construction of a translation graph. That 
is to say, the sentence to be translated is viewed 
as a graph which is traversed fi'om left to right. 
For each matching source pattern, as encoded 
in the transducers, a new edge is added to the 
graph. The edge is labeled with the category la- 
bel of the translation pattern. The translation 
and the translation score are attached to the 
edge. In this way a translation graph is con- 
structed. In those cases, where a source pattern 
has several translations, one edge tbr each trans- 
lation is added to the graph. 
Tim left right search on the graph is orga- 
nized in such a way that all paths are traversed 
1132 
Table 2: Coml)ound translation t)atterns (CTP). 
CTP ~ DATE_DAY ginge es wiedcr 
CTP ~ SURNAME am A1)i)~rat 
CTP ~ NP dauert DATE 
CTP @ nehmen PPER NP DATE 
@ DATE_DAY it is possible again :~ -4.6 
~/: this is SURNAME st)caking @ -3.3 
NP takes DATE :~ -3.3 
let PPER take NP DATE @ -4.6 
in parallel and tile patterns l;ored in the trans- 
ducer are matched synchronously. For each 
~lo(te n and each edge e leading to n, all patterns 
in tile transducer starting with the label of e arc 
attached to n. This gives a mmlber of hypothe- 
ses describing partially matching patterns. Al- 
ready started hypotheses are expanded with tile 
lal)el of the edge running ti'om the l)revious node 
to the current node. This procedure is shown in 
l~'igul'e 1. For a selection of t;rmmlation patterns 
from the siml)le , word-1)ased translation mem- 
ory the hyt)otheses tbr 1)artially matching pat- 
terns generated uring the left--right traversal 
are shown as well as the resulting new edges. 
The result of applying all transducers is a 
graph where each path is a (partial) transla- 
tion of the source sentence. The 1)ath with the 
best overall score is used to construct the fi- 
nal translation. For good result;s, not; only the 
scores from t;he transducers houl(l 1)e used in 
selecting the best t)ath, but a language model 
of the target language should l)e inchlde(l. 
1 llIIl # al, on, at the  
9 Montag# Monday 
17 waere  es so  moeglich # would that  be possilflc 
18 wic ist cs bel lhncn # how about you 
19 wie waerc es # how al)out 
20 wie wacrc cs denn # how about 
21 wie waere es denn am Montag # how about Monday 
22 wie wacrc es am Montag # Imw about Monday 
Figure 1: Ext)ansion of Pattern Hypotheses 
3.1 Error Tolerant Match  
To improve tile coverage on unseen test data, 
it may be avantageous to allow tbr approxima- 
tivc matching. The idea is, to apply longer seg- 
ments tbr syntactically better translations with- 
out loosing to much as far as tile content of the 
sentences i concerned. 
We us(; weighted edit distance, i.e. each er- 
ror (insertion, deletion, substitution) is assici- 
ated with an individual score. Thereby, the 
deletion or insertion of typical filler words can 
be allowed, whereas the deletion or insertion of 
content words is avoided. 
3.2 Translation on Word Lat t i ces  
The approach described so far can be used for 
a tight integration of speech recognition and 
translation. Speech recognition systems typi- 
cally 1)ro(luce wor(l lattices which encode the 
most likely word sequences in an e.flicient lllall- 
net. A direct translation on the lattice has, 
compared to transforming the lattice, into an n- 
best list;, translating each word sequence, mM 
selecting the overall best translation, a nulnber 
of advantages: 
? all the paths can be covered, whereas in 
an n-best approach typically only a small 
fraction of tile paths is considered; 
? partial translation hypotheses are reused; 
? acoustic scores can be taken into account 
when calculating an overall score for each 
translation hypothesis. 
4 Exper iments  and Resu l ts  
In this section, we will report on first expert- 
ments and results obtained with the cascaded 
transducer approach. Experiments were per- 
tbrmed on the VERBMOBIL corpus. This cor- 
pus consists of spontaneously spoken dialogs in 
the appointment scheduling domain (Wahlster, 
1993). The vocabulary comprises 7335 German 
1133 
words and 4382 English words. A test corI)us 
of 147 sentences with a total of 1 968 words was 
used to test the coverage of tile transducers and 
to run preliminary translation experiments. 
In Table 3 the sizes of the transducers are 
given. 
Table 3: Number of translation t)atterns of tile 
transducers. 
Transducer Patterns 
Nalne 
Spell 
Number 
Date 
POS Tags 
~ralnnlar 
442 
60 
342 
334 
671.4 
124 
4.1 Coverage 
In a first series of experiments, the coverage 
of the cascaded transducers was tested. TILe 
sentences pairs Dora the training corpus were 
segmented into shorter segments. This resulted 
in 43609 bilingual phrases running from 1 word 
up to 82 words in length. The longest phrases 
were discarded as it is very unlikely that they 
will match other sentences. Thus, for the ex- 
periments only 40000 sentence pairs were used, 
the longest sentences containing sixteen source 
words. 
Starting fi'om those simple phrases, succes- 
sively more transducers were applied 1lt) to the 
fllll cascade. In Table 4 the coverage for each 
level is shown. As expected, the coverage in- 
creases and nearly flfll coverage on the test 
sentences is reached. In tile final step, the 
POS transducer and the grammer transducer 
are both applied. 
The first cohnnn shows which transducers 
have been applied. In each step, one additional 
transducer is applied tbr bilingual labeling and 
tbr translation. Bilingual labeling reduces the 
number of distinct patterns in the translation 
memory, whereas the immber of compound pat- 
terns increases. The last column shows the 
number of words in the test sentences not cov- 
ered by the patterns ill tile translation mmory. 
As can be seen, the coverage increases which 
each step. The large improvement in the final 
Table 4: Efl'ect of selected transducers oi1 cov- 
erage on test corpus. 
%'ansdncers Patterns Coln- not 
pound covered 
NOlle 
Name 
+ Spell 
+ Number 
+ Date 
+ Gramnlar 
40000 
39624 
39508 
38669 
36118 
35519 
1.259 
1468 
11181 
14684 
15682 
273 
254 
249 
238 
215 
9 
step results froln applying tile POS-tag trans- 
ducer whidl coveres a large part of the vocabu- 
lary. 
4.2 Translat ion 
First experiments have been performed to test 
tile approach tbr translation. So far, no lan- 
guage model tbr the target language is applied 
to score the different ranslations. 
For the sentence 'Samstag und Februar sind 
gut, aber der siebzehnte ware besser' the best 
t)ath through the resnlting translation graph 
gives a structure as shown in Figure 2. IlL Ta- 
ble 5, some translation examples for test sen- 
tences not seen ill the training corpus arc given. 
Table 5: Three translations generated t'rom the 
hierarchical translation memory. 
Ich werde lnit dem Fhlgzeug kolnnmn. 
I will come with the plane. 
Ja, wunderbar. Machen wir das so, und 
dann treflbn wir uns daim ill Hamburg. 
Vielen Dank und auf WiederhSren. 
Well, excellent. Shall we fix this, and 
then we will meet then in Hanfl)urg. 
Thank you very much goodbye. 
Das kann ich nicht einrichten. Ich habe 
eine Chance ab dreimldzwanzigsten 
Oktober. Ist es da bei Ihnen m6glich? 
It can I not arrange. I have 
a chance froln twenty-third of 
October. Is it as for you possible? 
1134 
I C_PHRASE 
the fourth would be better 
-7.4 
~DATE \] 
Saturday and February 
-4,2 
DATE DATE 
Saturday February 
-0.6 -0.6 
DAYWEEK 
Saturday 
-0.5 
Samstag I 
MONTH 
February 
-0.5 
Feb  ruar  4 
I DATE 
the fourth 
-4.1 
~ DATEDAY the fourth 
-4.0 
are   ood but  
-2 1 -0 1 
~a_ere ~ A  
Figure 2: \[\[~'m~slation example 
5 Sun'nnary  and  conc lus ions  
In this t)npcr a translation at)pronch 1)asexl on 
cascaded tin|re state l;ra,nsducers has l)een pre- 
sen|ext. A mm~l\] mm~l)er of simple l;rmlsdut'- 
(;rs is handcrafted and then used to convert; n 
bilingual cortms in|;o a translation memory con- 
sisting of som:(:c l)al;tcrn target; i)a,l;l;(;rn p~tirs, 
which inchuh; category lnlmls. Trmlslni;ion is 
then lmrformcd by applying l;he comtflel;e cas- 
ca(le of l;rans(luce.rs. 
First (;xl)e.rim(mts ha,v(; shown l;lm \])ot,cnl;i;J 
of this ai)l)ro~u:h for m~tchine l;ransla,tion. Good 
coverag(~ on mlse,(m test data ('ould 1)e ol)l;aine(l. 
The. main ditficulty in this nt)l)roach is to (te- 
l|he a (:onsistenl; scoring s('heme thr the (litt'e,r- 
ent transdu(:('rs. Especially, ~ good l)M~m('e t)(;- 
tween the grammm: and th(', word-t)as(;d |,ransb> 
lion m('mory is n(',c(;ssary. 'Phis will t)e th(' main 
focus for futur(', work. 
As Mrea(ty mentioned, ;~ l~tngmtge modal for 
th(; tnrget l~mguag(; has to bc integrated into 
t;h(, scoring of the translation hyl)othes(,s. Fi- 
mflly, the l, rmmdu('er based al)t)roadl to transla- 
tion will 1)e tested on word lattice.s as i)rodu(:ed 
by spee,(:h recognition systeans. 
Acknowledgement .  This work was partly 
SUl)t)orted l)y the German Fede.ral Ministry of 
E(tuc~ttion, S(:ie.n(:e, ll.es(;m:ch mM 3b.(:hnoh)gy 
under the. Contract Nulnl)er 01 IV 701 Td 
(vl m vonu,). 
References  
R. 1). Brown. i\[996. Exmut)lc-1)ase, d machine 
translation in the pangloss system, l"rocc, cd- 
ings of the 16th, international Co~@rencc, on 
Computational Linguistics, 169-174, Copcn- 
tm,ge, n, l)emnark, August. 
It. l(itmJo. 1993. A COml)rehensive mM prn(> 
ti(-M model of memory-ha,seal machine trmls- 
la.tion, l~mcccdi,ng.~ of the 13th, hzl, c'r,natio'nal 
Joint Co'nfere, nce, o'n Art{/icial bl, tclligc,'n, ce, 
vohmm 2. 1276 1282. Morgmt Ka.ufmmm. 
F..\]. Och, C. Tillmmm, mM H. Ney. 1999. lm- 
prove, d aligmnent models for statistical ma- 
chilw, I;ranslation. Procceding,s of the Joint 
SIGDAT Co~@rcncc on Empirical Meth, ods 
in Na, t,wral Language PTwccs.sin9 and Very 
Large, Corpora, 20 28, University of Mm:y~ 
land, College Park, MD, USA, June. 
S. Sato and M. Nagao. 1990. Towmd memory- 
based tnmslation. P'rocc, edings of the 13th 
International Cm@rcnce on Computational 
Lingui,~tics, vol. 3, 24:7 ~252, Hclsinki, Fin- 
land. 
W. Wahlster. 1993. Vert)mobil: %'anslation of 
t'a(:c-to-fac(; dialogs. Proceedings of th, e MT 
Summit IV, 1.27 135, Kol)e, Jal)mL 
1135 
 
	
ffImproved Word Alignment Using a Symmetric Lexicon Model
Richard Zens and Evgeny Matusov and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{zens,matusov,ney}@cs.rwth-aachen.de
Abstract
Word-aligned bilingual corpora are an
important knowledge source for many
tasks in natural language processing. We
improve the well-known IBM alignment
models, as well as the Hidden-Markov
alignment model using a symmetric lex-
icon model. This symmetrization takes
not only the standard translation direc-
tion from source to target into account,
but also the inverse translation direction
from target to source. We present a the-
oretically sound derivation of these tech-
niques. In addition to the symmetriza-
tion, we introduce a smoothed lexicon
model. The standard lexicon model is
based on full-form words only. We propose
a lexicon smoothing method that takes
the word base forms explicitly into ac-
count. Therefore, it is especially useful
for highly inflected languages such as Ger-
man. We evaluate these methods on the
German?English Verbmobil task and the
French?English Canadian Hansards task.
We show statistically significant improve-
ments of the alignment quality compared
to the best system reported so far. For
the Canadian Hansards task, we achieve
an improvement of more than 30% rela-
tive.
1 Introduction
Word-aligned bilingual corpora are an impor-
tant knowledge source for many tasks in nat-
ural language processing. Obvious applica-
tions are the extraction of bilingual word or
phrase lexica (Melamed, 2000; Och and Ney,
2000). These applications depend heavily on
the quality of the word alignment (Och and
Ney, 2000). Word alignment models were first
introduced in statistical machine translation
(Brown et al, 1993). The alignment describes
the mapping from source sentence words to
target sentence words.
Using the IBM translation models IBM-1
to IBM-5 (Brown et al, 1993), as well as
the Hidden-Markov alignment model (Vogel
et al, 1996), we can produce alignments of
good quality. In (Och and Ney, 2003), it is
shown that the statistical approach performs
very well compared to alternative approaches,
e.g. based on the Dice coefficient or the com-
petitive linking algorithm (Melamed, 2000).
A central component of the statistical trans-
lation models is the lexicon. It models the
word translation probabilities. The standard
training procedure of the statistical models
uses the EM algorithm. Typically, the models
are trained for one translation direction only.
Here, we will perform a simultaneous training
of both translation directions, source-to-target
and target-to-source. After each iteration of
the EM algorithm, we combine the two lexica
to a symmetric lexicon. This symmetric lex-
icon is then used in the next iteration of the
EM algorithm for both translation directions.
We will propose and justify linear and loglin-
ear interpolation methods.
Statistical methods often suffer from the
data sparseness problem. In our case, many
words in the bilingual sentence-aligned texts
are singletons, i.e. they occur only once. This
is especially true for the highly inflected lan-
guages such as German. It is hard to obtain
reliable estimations of the translation proba-
bilities for these rarely occurring words. To
overcome this problem (at least partially), we
will smooth the lexicon probabilities of the
full-form words using a probability distribu-
tion that is estimated using the word base
forms. Thus, we exploit that multiple full-
form words share the same base form and have
similar meanings and translations.
We will evaluate these methods on the
German?English Verbmobil task and the
French?English Canadian Hansards task. We
will show statistically significant improve-
ments compared to state-of-the-art results in
(Och and Ney, 2003). On the Canadian
Hansards task, the symmetrization methods
will result in an improvement of more than
30% relative.
2 Statistical Word Alignment Models
In this section, we will give a short description
of the commonly used statistical word align-
ment models. These alignment models stem
from the source-channel approach to statisti-
cal machine translation (Brown et al, 1993).
We are given a source language sentence fJ1 :=
f1...fj ...fJ which has to be translated into
a target language sentence eI1 := e1...ei...eI .
Among all possible target language sentences,
we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources allows for an independent modeling of
target language model Pr(eI1) and translation
model Pr(fJ1 |eI1). Into the translation model,
the word alignment A is introduced as a hid-
den variable:
Pr(fJ1 |eI1) =
?
A
Pr(fJ1 , A|eI1)
Usually, we use restricted alignments in the
sense that each source word is aligned to at
most one target word, i.e. A = aJ1 . A de-
tailed description of the popular translation
models IBM-1 to IBM-5 (Brown et al, 1993),
as well as the Hidden-Markov alignment model
(HMM) (Vogel et al, 1996) can be found in
(Och and Ney, 2003). All these models include
parameters p(f |e) for the single-word based
lexicon. They differ in the alignment model.
A Viterbi alignment A? of a specific model is
an alignment for which the following equation
holds:
A? = argmax
A
{Pr(fJ1 , A|eI1)
}
We measure the quality of an alignment model
using the quality of the Viterbi alignment com-
pared to a manually produced reference align-
ment.
In Section 3, we will apply the lexicon sym-
metrization methods to the models described
previously. Therefore, we will now sketch the
standard training procedure for the lexicon
model. The EM algorithm is used to train
the free lexicon parameters p(f |e).
In the E-step, the lexical counts for each
sentence pair (fJ1 , eI1) are calculated and then
summed over all sentence pairs in the training
corpus:
N(f, e) =
?
(fJ1 ,eI1)
?
aJ1
p(aJ1 |fJ1 , eI1)
?
i,j
?(f, fj)?(e, ei)
In the M-step the lexicon probabilities are:
p(f |e) = N(f, e)?
f?
N(f? , e)
3 Symmetrized Lexicon Model
During the standard training procedure, the
lexicon parameters p(f |e) and p(e|f) were es-
timated independent of each other in strictly
separate trainings. In this section, we present
two symmetrization methods for the lexicon
model. As a starting point, we use the
joint lexicon probability p(f, e) and determine
the conditional probabilities for the source-
to-target direction p(f |e) and the target-to-
source direction p(e|f) as the corresponding
marginal distribution:
p(f |e) = p(f, e)?
f?
p(f? , e) (1)
p(e|f) = p(f, e)?
e?
p(f, e?) (2)
The nonsymmetric auxiliary Q-functions for
reestimating the lexicon probabilities during
the EM algorithm can be represented as fol-
lows. Here, NST (f, e) and NTS(f, e) denote
the lexicon counts for the source-to-target
(ST ) direction and the target-to-source (TS)
direction, respectively.
QST ({p(f |e)}) =
?
f,e
NST (f, e) ? log p(f, e)?
f?
p(f? , e)
QTS({p(e|f)}) =
?
f,e
NTS(f, e) ? log p(f, e)?
e?
p(f, e?)
3.1 Linear Interpolation
To estimate the joint probability using the EM
algorithm, we define the auxiliary Q-function
as a linear interpolation of the Q-functions for
the source-to-target and the target-to-source
direction:
Q?({p(f, e)}) = ? ?QST ({p(f |e)})
+(1? ?) ?QTS({p(e|f)})
= ? ?
?
f,e
NST (f, e) ? log p(f, e)
+(1? ?) ?
?
f,e
NTS(f, e) ? log p(f, e)
?? ?
?
e
NST (e) ? log
?
f?
p(f? , e)
?(1? ?) ?
?
f
NTS(f) ? log
?
e?
p(f, e?)
The unigram counts N(e) and N(f) are deter-
mined, for each of the two translation direc-
tions, by taking a sum of N(f, e) over f and
over e, respectively. We define the combined
lexicon count N?(f, e):
N?(f, e) := ? ?NST (f, e) + (1? ?) ?NTS(f, e)
Now, we derive the symmetrized Q-function
over p(f, e) for a certain word pair (f, e).
Then, we set this derivative to zero to deter-
mine the reestimation formula for p(f, e) and
obtain the following equation:
N?(f, e)
p(f, e) = ? ?
NST (e)?
f?
p(f? , e) + (1? ?) ?
NTS(f)?
e?
p(f, e?)
We do not know a closed form solution for this
equation. As an approximation, we use the
following term:
p?(f, e) = N?(f, e)?
f? ,e?
N?(f? , e?)
This estimate is an exact solution, if the uni-
gram counts for f and e are independent of the
translation direction, i. e. NST (f) = NTS(f)
and NST (e) = NTS(e). We make this approx-
imation and thus we interpolate the lexicon
counts linear after each iteration of the EM
algorithm. Then, we normalize these counts
(according to Equations 1 and 2) to determine
the lexicon probabilities for each of the two
translation directions.
3.2 Loglinear Interpolation
We will show in Section 5 that the linear in-
terpolation results in significant improvements
over the nonsymmetric system. Motivated by
these experiments, we investigated also the
loglinear interpolation of the lexicon counts of
the two translation directions. The combined
lexicon count N?(f, e) is now defined as:
N?(f, e) = NST (f, e)? ?NTS(f, e)1??
The normalization is done in the same way as
for the linear interpolation. The linear inter-
polation resembles more a union of the two lex-
ica whereas the loglinear interpolation is more
similar to an intersection of both lexica. Thus
for the linear interpolation, a word pair (f, e)
obtains a large combined count, if the count in
at least one direction is large. For the loglin-
ear interpolation, the combined count is large
only if both lexicon counts are large.
In the experiments, we will use the interpo-
lation weight ? = 0.5 for both the linear and
the loglinear interpolation, i. e. both transla-
tion directions are weighted equally.
3.3 Evidence Trimming
Initially, the lexicon contains all word pairs
that cooccur in the bilingual training corpus.
The majority of these word pairs are not trans-
lations of each other. Therefore, we would
like to remove those lexicon entries. Evidence
trimming is one way to do this. The evidence
of a word pair (f, e) is the estimated count
N(f, e). Now, we discard a word pair if its ev-
idence is below a certain threshold ? .1 In the
case of the symmetric lexicon, we can further
refine this method. For estimating the lex-
icon in the source-to-target direction p?(f |e),
the idea is to keep all entries from this di-
rection and to boost the entries that have a
high evidence in the target-to-source direction
NTS(f, e). We obtain the following formula:
N?ST (f, e) =
?
?
?
?NST (f, e) + (1? ?)NTS(f, e)
if NST (f, e) > ?
0 else
The count N?ST (f, e) is now used to estimate
the source-to-target lexicon p?(f |e). With this
method, we do not keep entries in the source-
to-target lexicon p?(f |e) if their evidence is low,
even if their evidence in the target-to-source
1Actually, there is always implicit evidence trim-
ming caused by the limited machine precision.
direction NTS(f, e) is high. For the target-to-
source direction, we apply this method in a
similar way.
4 Lexicon Smoothing
The lexicon model described so far is based on
full-form words. For highly inflected languages
such as German this might cause problems,
because many full-form words occur only a few
times in the training corpus. Compared to En-
glish, the token/type ratio for German is usu-
ally much lower (e.g. Verbmobil: English 99.4,
German 56.3). The information that multiple
full-form words share the same base form is
not used in the lexicon model. To take this in-
formation into account, we smooth the lexicon
model with a backing-off lexicon that is based
on word base forms. The smoothing method
we apply is absolute discounting with interpo-
lation:
p(f |e) = max {N(f, e)? d, 0}N(e) + ?(e) ? ?(f, e?)
This method is well known from language
modeling (Ney et al, 1997). Here, e? de-
notes the generalization, i.e. the base form,
of the word e. The nonnegative value d is
the discounting parameter, ?(e) is a normal-
ization constant and ?(f, e?) is the normalized
backing-off distribution.
The formula for ?(e) is:
?(e) = 1N(e)
?
? ?
f :N(f,e)>d
d+
?
f :N(f,e)?d
N(f, e)
?
?
= 1N(e)
?
f
min{d,N(f, e)}
This formula is a generalization of the one
typically used in publications on language
modeling. This generalization is necessary,
because the lexicon counts may be fractional
whereas in language modeling typically inte-
ger counts are used. Additionally, we want
to allow for discounting values d greater than
one. The backing-off distribution ?(f, e?) is es-
timated using relative frequencies:
?(f, e?) = N(f, e?)?
f?
N(f? , e?)
Here, N(f, e?) denotes the count of the event
that the source language word f and the target
language base form e? occur together. These
counts are computed by summing the lexicon
counts N(f, e) over all full-form words e which
share the same base form e?.
5 Results
5.1 Evaluation Criteria
We use the same evaluation criterion as de-
scribed in (Och and Ney, 2000). The gen-
erated word alignment is compared to a ref-
erence alignment which is produced by hu-
man experts. The annotation scheme explic-
itly takes the ambiguity of the word alignment
into account. There are two different kinds
of alignments: sure alignments (S) which are
used for alignments that are unambiguous and
possible alignments (P ) which are used for
alignments that might or might not exist. The
P relation is used especially to align words
within idiomatic expressions, free translations,
and missing function words. It is guaranteed
that the sure alignments are a subset of the
possible alignments (S ? P ). The obtained
reference alignment may contain many-to-one
and one-to-many relationships.
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER), which is derived from the
well-known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER(S, P ;A) = 1? |A ? S|+ |A ? P ||A|+ |S|
With these definitions a recall error can only
occur if a S(ure) alignment is not found and a
precision error can only occur if a found align-
ment is not even P (ossible).
5.2 Experimental Setup
We evaluated the presented lexicon sym-
metrization methods on the Verbmobil and
the Canadian Hansards task. The German?
English Verbmobil task (Wahlster, 2000) is a
speech translation task in the domain of ap-
pointment scheduling, travel planning and ho-
tel reservation. The French?English Canadian
Hansards task consists of the debates in the
Canadian Parliament.
The corpus statistics are shown in Table 1
and Table 2. The number of running words
and the vocabularies are based on full-form
words including punctuation marks. As in
Table 1: Verbmobil: Corpus statistics.
German English
Train Sentences 34K
Words 329 625 343 076
Vocabulary 5 936 3 505
Singletons 2 600 1 305
Test Sentences 354
Words 3 233 3 109
Table 2: Canadian Hansards: Corpus statistics.
French English
Train Sentences 128K
Words 2.12M 1.93M
Vocabulary 37 542 29 414
Singletons 12 986 9 572
Test Sentences 500
Words 8 749 7 946
(Och and Ney, 2003), the first 100 sentences
of the test corpus are used as a development
corpus to optimize model parameters that are
not trained via the EM algorithm, e.g. the
discounting parameter for lexicon smoothing.
The remaining part of the test corpus is used
to evaluate the models.
We use the same training schemes (model
sequences) as presented in (Och and Ney,
2003). As we use the same training and test-
ing conditions as (Och and Ney, 2003), we will
refer to the results presented in that article as
the baseline results. In (Och and Ney, 2003),
the alignment quality of statistical models is
compared to alternative approaches, e.g. us-
ing the Dice coefficient or the competitive
linking algorithm. The statistical approach
showed the best performance and therefore we
report only the results for the statistical sys-
tems.
5.3 Lexicon Symmetrization
In Table 3 and Table 4, we present the follow-
ing experiments performed for both the Verb-
mobil and the Canadian Hansards task:
? Base: the system taken from (Och and
Ney, 2003) that we use as baseline system.
? Lin.: symmetrized lexicon using a lin-
ear interpolation of the lexicon counts af-
ter each training iteration as described in
Section 3.1.
? Log.: symmetrized lexicon using a log-
linear interpolation of the lexicon counts
after each training iteration as described
in Section 3.2.
Table 3: Comparison of alignment perfor-
mance for the Verbmobil task (S?T: source-
to-target direction, T?S: target-to-source di-
rection; all numbers in percent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 93.5 95.3 5.7 91.4 88.7 9.9
Lin. 96.0 95.4 4.3 93.7 89.6 8.2
Log. 93.6 95.6 5.5 94.5 89.4 7.9
 4
 6
 8
 10
 12
 14
 16
 18
 100  1000  10000  100000
A
E
R
Corpus Size
baselinelinearloglinear
Figure 1: AER[%] of different alignment meth-
ods as a function of the training corpus size
for the Verbmobil task (source-to-target direc-
tion).
In Table 3, we compare both interpolation
variants for the Verbmobil task to (Och and
Ney, 2003). We observe notable improvements
in the alignment error rate using the linear in-
terpolation. For the translation direction from
German to English (S?T), an improvement of
about 25% relative is achieved from an align-
ment error rate of 5.7% for the baseline system
to 4.3% using the linear interpolation. Per-
forming the loglinear interpolation, we observe
a substantial reduction of the alignment error
rate as well. The two symmetrization methods
improve both precision and recall of the result-
ing Viterbi alignment in both translation di-
rections for the Verbmobil task. The improve-
ments with the linear interpolation is for both
translation directions statistically significant
at the 99% level. For the loglinear interpo-
lation, the target-to-source translation direc-
tion is statistically significant at the 99% level.
The statistical significance test were done us-
ing boostrap resampling.
We also performed experiments on sub-
corpora of different sizes. For the Verbmo-
bil task, the results are illustrated in Figure 1.
Table 4: Comparison of alignment perfor-
mance for the Canadian Hansards task (S?T:
source-to-target direction, T?S: target-to-
source direction; all numbers in percent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 85.4 90.6 12.6 85.6 90.9 12.4
Lin. 89.3 91.4 9.9 89.0 92.0 9.8
Log. 91.0 92.0 8.6 91.2 92.1 8.4
We observe that both symmetrization variants
result in improvements for all corpus sizes.
With increasing training corpus size the per-
formance of the linear interpolation becomes
superior to the performance of the loglinear
interpolation.
In Table 4, we compare the symmetriza-
tion methods with the baseline system for the
Canadian Hansards task. Here, the loglin-
ear interpolation performs best. We achieve
a relative improvement over the baseline of
more than 30% for both translation directions.
For instance, the alignment error rate for the
translation direction from French to English
(S?T) improves from 12.6% for the baseline
system to 8.6% for the symmetrized system
with loglinear interpolation. Again, the two
symmetrization methods improve both preci-
sion and recall of the Viterbi alignment.
For the Canadian Hansards task, all the im-
provements of the alignment error rate are sta-
tistically significant at the 99% level.
5.4 Generalized Alignments
In (Och and Ney, 2003) generalized alignments
are used, thus the final Viterbi alignments of
both translation directions are combined us-
ing some heuristic. Experimentally, the best
heuristic for the Canadian Hansards task is
the intersection. For the Verbmobil task, the
refined method of (Och and Ney, 2003) is
used. The results are summarized in Table 5.
We see that both the linear and the loglinear
lexicon symmetrization methods yield an im-
provement with respect to the alignment error
rate. For the Verbmobil task, the improve-
ment with the loglinear interpolation is sta-
tistically significant at the 99% level. For the
Canadian Hansards task, both lexicon sym-
metrization methods result in statistically sig-
nificant improvements at the 95% level. Addi-
tionally, we observe that precision and recall
are more balanced for the symmetrized lexicon
variants, especially for the Canadian Hansards
Table 6: Effect of smoothing the lexicon prob-
abilities on the alignment performance for the
Verbmobil task (S?T: source-to-target direc-
tion, smooth English; T?S: target-to-source
direction, smooth German; all numbers in per-
cent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 93.5 95.3 5.7 91.4 88.7 9.9
smooth 94.8 94.8 5.2 93.4 88.2 9.1
task.
5.5 Lexicon Smoothing
In Table 6, we present the results for the lex-
icon smoothing as described in Section 4 on
the Verbmobil corpus2. As expected, a no-
table improvement in the AER is reached if
the lexicon smoothing is performed for Ger-
man (i.e. for the target-to-source direction),
because many full-form words with the same
base form are present in this language. These
improvements are statistically significant at
the 95% level.
6 Related Work
The popular IBM models for statistical ma-
chine translation are described in (Brown et
al., 1993). The HMM-based alignment model
was introduced in (Vogel et al, 1996). A
good overview of these models is given in
(Och and Ney, 2003). In that article Model
6 is introduced as the loglinear interpolation
of the other models. Additionally, state-of-
the-art results are presented for the Verbmo-
bil task and the Canadian Hansards task for
various configurations. Therefore, we chose
them as baseline. Compared to our work,
these publications kept the training of the
two translation directions strictly separate
whereas we integrate both directions into one
symmetrized training. Additional linguistic
knowledge sources such as dependency trees
or parse trees were used in (Cherry and Lin,
2003) and (Gildea, 2003). In (Cherry and
Lin, 2003) a probability model Pr(aJ1 |fJ1 , eI1) is
used, which is symmetric per definition. Bilin-
gual bracketing methods were used to produce
a word alignment in (Wu, 1997). (Melamed,
2000) uses an alignment model that enforces
one-to-one alignments for nonempty words. In
2The base forms were determined using LingSoft
tools.
Table 5: Effect of different lexicon symmetrization methods on alignment performance for the
generalized alignments for the Verbmobil task and the Canadian Hansards task.
task: Verbmobil Canadian Hansards
Precision[%] Recall[%] AER[%] Precision[%] Recall[%] AER[%]
Base 93.3 96.0 5.5 96.6 86.0 8.2
Lin. 96.1 94.0 4.9 95.2 88.5 7.7
Loglin. 95.2 95.3 4.7 93.6 90.8 7.5
(Toutanova et al, 2002), extensions to the
HMM-based alignment model are presented.
7 Conclusions
We have addressed the task of automatically
generating word alignments for bilingual cor-
pora. This problem is of great importance for
many tasks in natural language processing, es-
pecially in the field of machine translation.
We have presented lexicon symmetrization
methods for statistical alignment models that
are trained using the EM algorithm, in par-
ticular the five IBM models, the HMM and
Model 6. We have evaluated these meth-
ods on the Verbmobil task and the Cana-
dian Hansards task and compared our results
to the state-of-the-art system of (Och and
Ney, 2003). We have shown that both the
linear and the loglinear interpolation of lexi-
con counts after each iteration of the EM al-
gorithm result in statistically significant im-
provements of the alignment quality. For the
Canadian Hansards task, the AER improved
by about 30% relative; for the Verbmobil task
the improvement was about 25% relative.
Additionally, we have described lexicon
smoothing using the word base forms. Es-
pecially for highly inflected languages such as
German, this smoothing resulted in statisti-
cally significant improvements.
In the future, we plan to optimize the inter-
polation weights to balance the two transla-
tion directions. We will also investigate the
possibility of generating directly an uncon-
strained alignment based on the symmetrized
lexicon probabilities.
Acknowledgment
This work has been partially funded by the
EU project LC-Star, IST-2001-32216.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, June.
C. Cherry and D. Lin. 2003. A probability model
to improve word alignment. In Proc. of the 41th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 88?95, Sap-
poro, Japan, July.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of the 41th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 80?87, Sapporo,
Japan, July.
I. D. Melamed. 2000. Models of translational
equivalence among words. Computational Lin-
guistics, 26(2):221?249.
H. Ney, S. Martin, and F. Wessel. 1997. Statisti-
cal language modeling using leaving-one-out. In
S. Young and G. Bloothooft, editors, Corpus-
Based Methods in Language and Speech Process-
ing, pages 174?207. Kluwer.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 440?447, Hong Kong,
October.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
K. Toutanova, H. T. Ilhan, and C. D. Manning.
2002. Extensions to hmm-based statistical word
alignment models. In Proc. Conf. on Empirical
Methods for Natural Language Processing, pages
87?94, Philadelphia, PA, July.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation.
In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
W. Wahlster, editor. 2000. Verbmobil: Founda-
tions of speech-to-speech translations. Springer
Verlag, Berlin, Germany, July.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
Reordering Constraints for Phrase-Based
Statistical Machine Translation
Richard Zens1, Hermann Ney1, Taro Watanabe2 and Eiichiro Sumita2
1Lehrstuhl fu?r Informatik VI 2 Spoken Language Translation Research Laboratories
Computer Science Department ATR
RWTH Aachen University, Germany Kyoto, Japan
{zens,ney}@cs.rwth-aachen.de {watanabe,sumita}@slt.atr.co.jp
Abstract
In statistical machine translation, the gen-
eration of a translation hypothesis is com-
putationally expensive. If arbitrary re-
orderings are permitted, the search prob-
lem is NP-hard. On the other hand,
if we restrict the possible reorderings
in an appropriate way, we obtain a
polynomial-time search algorithm. We in-
vestigate different reordering constraints
for phrase-based statistical machine trans-
lation, namely the IBM constraints and
the ITG constraints. We present effi-
cient dynamic programming algorithms
for both constraints. We evaluate the con-
straints with respect to translation quality
on two Japanese?English tasks. We show
that the reordering constraints improve
translation quality compared to an un-
constrained search that permits arbitrary
phrase reorderings. The ITG constraints
preform best on both tasks and yield sta-
tistically significant improvements com-
pared to the unconstrained search.
1 Introduction
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =f1 . . . fj . . . fJ , which is to be translated into
a target language (?English?) sentence eI1 =e1 . . . ei . . . eI . Among all possible target lan-
guage sentences, we will choose the sentence
with the highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources is known as the source-channel ap-
proach to statistical machine translation
(Brown et al, 1990). It allows an independent
modeling of target language model Pr(eI1) and
translation model Pr(fJ1 |eI1). The target lan-guage model describes the well-formedness of
the target language sentence. The translation
model links the source language sentence to
the target language sentence. It can be fur-
ther decomposed into alignment and lexicon
model. The argmax operation denotes the
search problem, i.e. the generation of the out-
put sentence in the target language. We have
to maximize over all possible target language
sentences.
An alternative to the classical source-
channel approach is the direct modeling of the
posterior probability Pr(eI1|fJ1 ). Using a log-linear model (Och and Ney, 2002), we obtain:
Pr(eI1|fJ1 ) = exp
( M?
m=1
?mhm(eI1, fJ1 )
)
? Z(fJ1 )
Here, Z(fJ1 ) denotes the appropriate normal-ization constant. As a decision rule, we obtain:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
This approach is a generalization of the
source-channel approach. It has the advan-
tage that additional models or feature func-
tions can be easily integrated into the over-
all system. The model scaling factors ?M1 aretrained according to the maximum entropy
principle, e.g. using the GIS algorithm. Al-
ternatively, one can train them with respect
to the final translation quality measured by
some error criterion (Och, 2003).
In this paper, we will investigate the re-
ordering problem for phrase-based translation
approaches. As the word order in source and
target language may differ, the search algo-
rithm has to allow certain reorderings. If arbi-
trary reorderings are allowed, the search prob-
lem is NP-hard (Knight, 1999). To obtain an
efficient search algorithm, we can either re-
strict the possible reorderings or we have to
use an approximation algorithm. Note that in
the latter case we cannot guarantee to find an
optimal solution.
The remaining part of this work is struc-
tured as follows: in the next section, we
will review the baseline translation system,
namely the alignment template approach. Af-
terward, we will describe different reordering
constraints. We will begin with the IBM con-
straints for phrase-based translation. Then,
we will describe constraints based on inver-
sion transduction grammars (ITG). In the fol-
lowing, we will call these the ITG constraints.
In Section 4, we will present results for two
Japanese?English translation tasks.
2 Alignment Template Approach
In this section, we give a brief description of
the translation system, namely the alignment
template approach. The key elements of this
translation approach (Och et al, 1999) are the
alignment templates. These are pairs of source
and target language phrases with an alignment
within the phrases. The alignment templates
are build at the level of word classes. This
improves the generalization capability of the
alignment templates.
We use maximum entropy to train the
model scaling factors (Och and Ney, 2002).
As feature functions we use a phrase transla-
tion model as well as a word translation model.
Additionally, we use two language model fea-
ture functions: a word-based trigram model
and a class-based five-gram model. Further-
more, we use two heuristics, namely the word
penalty and the alignment template penalty.
To model the alignment template reorderings,
we use a feature function that penalizes re-
orderings linear in the jump width.
A dynamic programming beam search al-
gorithm is used to generate the translation
hypothesis with maximum probability. This
search algorithm allows for arbitrary reorder-
ings at the level of alignment templates.
Within the alignment templates, the reorder-
ing is learned in training and kept fix during
the search process. There are no constraints
on the reorderings within the alignment tem-
plates.
This is only a brief description of the align-
ment template approach. For further details,
see (Och et al, 1999; Och and Ney, 2002).
3 Reordering Constraints
Although unconstrained reordering looks per-
fect from a theoretical point of view, we find
that in practice constrained reordering shows
J
uncovered position
covered position
uncovered position for extension
1 j
Figure 1: Illustration of the IBM constraints
with k = 3, i.e. up to three positions may be
skipped.
better performance. The possible advantages
of reordering constraints are:
1. The search problem is simplified. As a
result there are fewer search errors.
2. Unconstrained reordering is only helpful
if we are able to estimate the reorder-
ing probabilities reliably, which is unfor-
tunately not the case.
In this section, we will describe two variants
of reordering constraints. The first constraints
are based on the IBM constraints for single-
word based translation models. The second
constraints are based on ITGs. In the follow-
ing, we will use the term ?phrase? to mean ei-
ther a sequence of words or a sequence of word
classes as used in the alignment templates.
3.1 IBM Constraints
In this section, we describe restrictions on the
phrase reordering in spirit of the IBM con-
straints (Berger et al, 1996).
First, we briefly review the IBM constraints
at the word level. The target sentence is pro-
duced word by word. We keep a coverage vec-
tor to mark the already translated (covered)
source positions. The next target word has to
be the translation of one of the first k uncov-
ered, i.e. not translated, source positions. The
IBM constraints are illustrated in Figure 1.
For further details see e.g. (Tillmann and Ney,
2003).
For the phrase-based translation approach,
we use the same idea. The target sentence is
produced phrase by phrase. Now, we allow
skipping of up to k phrases. If we set k = 0,
we obtain a search that is monotone at the
phrase level as a special case.
The search problem can be solved using dy-
namic programming. We define a auxiliary
function Q(j, S, e). Here, the source position
j is the first unprocessed source position; with
unprocessed, we mean this source position is
neither translated nor skipped. We use the
set S = {(jn, ln)|n = 1, ..., N} to keep track
of the skipped source phrases with lengths ln
and starting positions jn. We show the formu-
lae for a bigram language model and use the
target language word e to keep track of the
language model history. The symbol $ is used
to mark the sentence start and the sentence
end. The extension to higher-order n-gram
language models is straightforward. We use
M to denote the maximum phrase length in
the source language. We obtain the following
dynamic programming equations:
Q(1, ?, $) = 1
Q(j, S, e) = max
{
max
e?,e?
{
max
j?M?j?<j
Q(j?, S, e?) ? p(f j?1j? |e?) ? p(e?|e?),
max
(j?,l)?S?
S=S?\{(j?,l)}
Q(j, S?, e?) ? p(f j?+l?1j? |e?) ? p(e?|e?)
}
,
max
j?M?j?<j
S?:S=S??{(j?,j?j?)}?|S?|<k
Q(j?, S?, e)
}
Q(J + 2, ?, $) = maxe Q(J + 1, ?, e) ? p($|e)
In the recursion step, we have distinguished
three cases: in the first case, we translate the
next source phrase. This is the same expan-
sion that is done in monotone search. In the
second case, we translate a previously skipped
phrase and in the third case we skip a source
phrase. For notational convenience, we have
omitted one constraint in the preceding equa-
tions: the final word of the target phrase e? is
the new language model state e (using a bi-
gram language model).
Now, we analyze the complexity of this al-
gorithm. Let E denote the vocabulary size of
the target language and let E? denote the max-
imum number of phrase translation candidates
for a given source phrase. Then, J ?(J ?M)k ?E
is an upper bound for the size of the Q-table.
Once we have fixed a specific element of this
table, the maximization steps can be done in
O(E ? E? ? (M + k ? 1) + (k ? 1)). There-
fore, the complexity of this algorithm is in
O(J ?(J ?M)k ?E ?(E ?E? ?(M+k?1)+(k?1))).
Assuming k < M , this can be simplified to
O((J ?M)k+1 ?E2 ? E?). As already mentioned,
source positions
ta
rg
et
 p
os
it
io
ns
without inversion with inversion
source positions
ta
rg
et
 p
os
it
io
ns
Figure 2: Illustration of monotone and
inverted concatenation of two consecutive
blocks.
setting k = 0 results in a search algorithm that
is monotone at the phrase level.
3.2 ITG Constraints
In this section, we describe the ITG con-
straints (Wu, 1995; Wu, 1997). Here, we inter-
pret the input sentence as a sequence of blocks.
In the beginning, each alignment template is a
block of its own. Then, the reordering process
can be interpreted as follows: we select two
consecutive blocks and merge them to a single
block by choosing between two options: either
keep the target phrases in monotone order or
invert the order. This idea is illustrated in Fig-
ure 2. The dark boxes represent the two blocks
to be merged. Once two blocks are merged,
they are treated as a single block and they can
be only merged further as a whole. It is not
allowed to merge one of the subblocks again.
3.2.1 Dynamic Programming Algorithm
The ITG constraints allow for a polynomial-
time search algorithm. It is based on the fol-
lowing dynamic programming recursion equa-
tions. During the search a table Qjl,jr,eb,etis constructed. Here, Qjl,jr,eb,et denotes theprobability of the best hypothesis translating
the source words from position jl (left) to po-
sition jr (right) which begins with the target
language word eb (bottom) and ends with the
word et (top). This is illustrated in Figure 3.
The initialization is done with the phrase-
based model described in Section 2. We in-
troduce a new parameter pm (m=? monotone),
which denotes the probability of a monotone
combination of two partial hypotheses. Here,
we formulate the recursion equation for a bi-
gram language model, but of course, the same
method can also be applied for a trigram lan-
jl jr
e b
et
Figure 3: Illustration of the Q-table.
guage model.
Qjl,jr,eb,et =
max
jl?k<jr,
e?,e??
{
Q0jl,jr,eb,et ,
Qjl,k,eb,e? ?Qk+1,jr,e??,et ? p(e??|e?) ? pm,
Qk+1,jr,eb,e? ?Qjl,k,e??,et ? p(e??|e?) ? (1? pm)
}
The resulting algorithm is similar to the CYK-
parsing algorithm. It has a worst-case com-
plexity of O(J3 ?E4). Here, J is the length of
the source sentence and E is the vocabulary
size of the target language.
3.2.2 Beam Search Algorithm
For the ITG constraints a dynamic program-
ming search algorithm exists as described in
the previous section. It would be more prac-
tical with respect to language model recom-
bination to have an algorithm that generates
the target sentence word by word or phrase
by phrase. The idea is to start with the beam
search decoder for unconstrained search and
modify it in such a way that it will produce
only reorderings that do not violate the ITG
constraints. Now, we describe one way to ob-
tain such a decoder. It has been pointed out
in (Zens and Ney, 2003) that the ITG con-
straints can be characterized as follows: a re-
ordering violates the ITG constraints if and
only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as
a subsequence. This means, if we select four
columns and the corresponding rows from the
alignment matrix and we obtain one of the two
patterns illustrated in Figure 4, this reordering
cannot be generated with the ITG constraints.
Now, we have to modify the beam search
decoder such that it cannot produce these two
patterns. We implement this in the follow-
ing way. During the search, we have a cover-
age vector cov of the source sentence available
for each partial hypothesis. A coverage vec-
1
2
3
4
a b c d
1
2
3
4
a b c d
Figure 4: Illustration of the two reordering
patterns that violate the ITG constraints.
tor is a binary vector marking the source sen-
tence words that have already been translated
(covered). Additionally, we know the current
source sentence position jc and a candidate
source sentence position jn to be translated
next.
To avoid the patterns in Figure 4, we have
to constrain the placement of the third phrase,
because once we have placed the first three
phrases we also have determined the position
of the fourth phrase as the remaining uncov-
ered position. Thus, we check the following
constraints:
case a) jn < jc (1)
?jn < j < jc : cov[j] ? cov[j + 1]
case b) jc < jn (2)
?jc < j < jn : cov[j] ? cov[j ? 1]
The constraints in Equations 1 and 2 enforce
the following: imagine, we traverse the cover-
age vector cov from the current position jc to
the position to be translated next jn. Then,
it is not allowed to move from an uncovered
position to a covered one.
Now, we sketch the proof that these con-
straints are equivalent to the ITG constraints.
It is easy to see that the constraint in Equa-
tion 1 avoids the pattern on the left-hand side
in Figure 4. To be precise: after placing the
first two phrases at (b,1) and (d,2), it avoids
the placement of the third phrase at (a,3).
Similarly, the constraint in Equation 2 avoid
the pattern on the right-hand side in Fig-
ure 4. Therefore, if we enforce the constraints
in Equation 1 and Equation 2, we cannot vio-
late the ITG constraints.
We still have to show that we can gener-
ate all the reorderings that do not violate the
ITG constraints. Equivalently, we show that
any reordering that violates the constraints in
Equation 1 or Equation 2 will also violate the
ITG constraints. It is rather easy to see that
any reordering that violates the constraint in
Table 1: Statistics of the BTEC corpus.
Japanese English
train Sentences 152 K
Words 1 044 K 893 K
Vocabulary 17 047 12 020
dev sentences 500
words 3 361 2 858
test sentences 510
words 3 498 ?
Table 2: Statistics of the SLDB corpus.
Japanese English
train Sentences 15 K
Words 201 K 190 K
Vocabulary 4 757 3 663
test sentences 330
words 3 940 ?
Equation 1 will generate the pattern on the
left-hand side in Figure 4. The conditions to
violate Equation 1 are the following: the new
candidate position jn is to the left of the cur-
rent position jc, e.g. positions (a) and (d).
Somewhere in between there has to be an cov-
ered position j whose successor position j + 1
is uncovered, e.g. (b) and (c). Therefore, any
reordering that violates Equation 1 generates
the pattern on the left-hand side in Figure 4,
thus it violates the ITG constraints.
4 Results
4.1 Corpus Statistics
To investigate the effect of reordering con-
straints, we have chosen two Japanese?English
tasks, because the word order in Japanese and
English is rather different. The first task is the
Basic Travel Expression Corpus (BTEC) task
(Takezawa et al, 2002). The corpus statistics
are shown in Table 1. This corpus consists of
phrasebook entries.
The second task is the Spoken Language
DataBase (SLDB) task (Morimoto et al,
1994). This task consists of transcription of
spoken dialogs in the domain of hotel reser-
vation. Here, we use domain-specific training
data in addition to the BTEC corpus. The
corpus statistics of this additional corpus are
shown in Table 2. The development corpus is
the same for both tasks.
4.2 Evaluation Criteria
WER (word error rate). The WER is com-
puted as the minimum number of substitution,
insertion and deletion operations that have to
be performed to convert the generated sen-
tence into the reference sentence.
PER (position-independent word er-
ror rate). A shortcoming of the WER is that
it requires a perfect word order. The word or-
der of an acceptable sentence can be different
from that of the target sentence, so that the
WER measure alone could be misleading. The
PER compares the words in the two sentences
ignoring the word order.
BLEU. This score measures the precision
of unigrams, bigrams, trigrams and fourgrams
with respect to a reference translation with a
penalty for too short sentences (Papineni et
al., 2002). The BLEU score measures accu-
racy, i.e. large BLEU scores are better.
NIST. This score is similar to BLEU. It is
a weighted n-gram precision in combination
with a penalty for too short sentences (Dod-
dington, 2002). The NIST score measures ac-
curacy, i.e. large NIST scores are better.
Note that for each source sentence, we have
as many as 16 references available. We com-
pute all the preceding criteria with respect to
multiple references.
4.3 System Comparison
In Table 3 and Table 4, we show the trans-
lation results for the BTEC task. First, we
observe that the overall quality is rather high
on this task. The average length of the used
alignment templates is about five source words
in all systems. The monotone search (mon)
shows already good performance on short sen-
tences with less than 10 words. We conclude
that for short sentences the reordering is cap-
tured within the alignment templates. On the
other hand, the monotone search degrades for
long sentences with at least 10 words resulting
in a WER of 16.6% for these sentences.
We present the results for various nonmono-
tone search variants: the first one is with the
IBM constraints (skip) as described in Sec-
tion 3.1. We allow for skipping one or two
phrases. Our experiments showed that if we
set the maximum number of phrases to be
skipped to three or more the translation re-
sults are equivalent to the search without any
reordering constraints (free). The results for
the ITG constraints as described in Section 3.2
are also presented.
The unconstrained reorderings improve the
total translation quality down to a WER of
11.5%. We see that especially the long sen-
tences benefit from the reorderings resulting in
an improvement from 16.6% to 13.8%. Com-
paring the results for the free reorderings and
Table 3: Translation performance WER[%]
for the BTEC task (510 sentences). Sentence
lengths: short: < 10 words, long: ? 10 words;
times in milliseconds per sentence.
WER[%]
sentence length
reorder short long all time[ms]
mon 11.4 16.6 12.7 73
skip 1 10.8 13.5 11.4 134
2 10.8 13.4 11.4 169
free 10.8 13.8 11.5 194
ITG 10.6 12.2 11.0 164
Table 4: Translation performance for the
BTEC task (510 sentences).
error rates[%] accuracy measures
reorder WER PER BLEU[%] NIST
mon 12.7 10.6 86.8 14.14
skip 1 11.4 10.1 88.0 14.19
2 11.4 10.1 88.1 14.20
free 11.5 10.0 88.0 14.19
ITG 11.0 9.9 88.2 14.25
the ITG reorderings, we see that the ITG
system always outperforms the unconstrained
system. The improvement on the whole test
set is statistically significant at the 95% level.1
In Table 5 and Table 6, we show the re-
sults for the SLDB task. First, we observe
that the overall quality is lower than for the
BTEC task. The SLDB task is a spoken lan-
guage translation task and the training cor-
pus for spoken language is rather small. This
is also reflected in the average length of the
used alignment templates that is about three
source words compared to about five words for
the BTEC task.
The results on this task are similar to the
results on the BTEC task. Again, the ITG
constraints perform best. Here, the improve-
ment compared to the unconstrained search is
statistically significant at the 99% level. Com-
pared to the monotone search, the BLEU score
for the ITG constraints improves from 54.4%
to 57.1%.
5 Related Work
Recently, phrase-based translation approaches
became more and more popular. Marcu and
Wong (2002) present a joint probability model
for phrase-based translation. In (Koehn et
1The statistical significance test were done for the
WER using boostrap resampling.
Table 5: Translation performance WER[%]
for the SLDB task (330 sentences). Sentence
lengths: short: < 10 words, long: ? 10 words;
times in milliseconds per sentence.
WER[%]
sentence length
reorder short long all time[ms]
mon 32.0 52.6 48.1 911
skip 1 31.9 51.1 46.9 3 175
2 32.0 51.4 47.2 4 549
free 32.0 51.4 47.2 4 993
ITG 31.8 50.9 46.7 4 472
Table 6: Translation performance for the
SLDB task (330 sentences).
error rates[%] accuracy measures
reorder WER PER BLEU[%] NIST
mon 48.1 35.5 54.4 9.45
skip 1 46.9 35.0 56.8 9.71
2 47.2 35.1 57.1 9.74
free 47.2 34.9 57.1 9.75
ITG 46.7 34.6 57.1 9.76
al., 2003), various aspects of phrase-based
systems are compared, e.g. the phrase ex-
traction method, the underlying word align-
ment model, or the maximum phrase length.
In (Vogel, 2003), a phrase-based system is
used that allows reordering within a window
of up to three words. Improvements for a
Chinese?English task are reported compared
to a monotone search.
The ITG constraints were introduced in
(Wu, 1995). The applications were, for in-
stance, the segmentation of Chinese character
sequences into Chinese words and the bracket-
ing of the source sentence into sub-sentential
chunks. Investigations on the IBM constraints
(Berger et al, 1996) for single-word based sta-
tistical machine translation can be found e.g.
in (Tillmann and Ney, 2003). A comparison of
the ITG constraints and the IBM constraints
for single-word based models can be found in
(Zens and Ney, 2003). In this work, we investi-
gated these reordering constraints for phrase-
based statistical machine translation.
6 Conclusions
We have presented different reordering con-
straints for phrase-based statistical machine
translation, namely the IBM constraints and
the ITG constraints, as well as efficient dy-
namic programming algorithms. Transla-
tion results were reported for two Japanese?
English translation tasks. Both type of re-
ordering constraints resulted in improvements
compared to a monotone search. Restrict-
ing the reorderings according to the IBM con-
straints resulted already in a translation qual-
ity similar to an unconstrained search. The
translation results with the ITG constraints
even outperformed the unconstrained search
consistently on all error criteria. The improve-
ments have been found statistically significant.
The ITG constraints showed the best per-
formance on both tasks. Therefore we plan to
further improve this method. Currently, the
probability model for the ITG constraints is
very simple. More sophisticated models, such
as phrase dependent inversion probabilities,
might be promising.
Acknowledgments
This work was partially done at the Spoken
Language Translation Research Laboratories
(SLT) at the Advanced Telecommunication
Research Institute International (ATR), Ky-
oto, Japan. This research was supported in
part by the Telecommunications Advancement
Organization of Japan. This work has been
partially funded by the EU project PF-Star,
IST-2001-37599.
References
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D.
Pietra, J. R. Gillett, A. S. Kehler, and R. L.
Mercer. 1996. Language translation apparatus
and method of using context-based translation
models, United States patent, patent number
5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J.
Della Pietra, F. Jelinek, J. D. Lafferty, R. L.
Mercer, and P. S. Roossin. 1990. A statisti-
cal approach to machine translation. Compu-
tational Linguistics, 16(2):79?85, June.
G. Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proc. ARPA Workshop
on Human Language Technology.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615, December.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. of
the Human Language Technology Conf. (HLT-
NAACL), pages 127?133, Edmonton, Canada,
May/June.
D. Marcu and W. Wong. 2002. A phrase-based,
joint probability model for statistical machine
translation. In Proc. Conf. on Empirical Meth-
ods for Natural Language Processing, pages 133?
139, Philadelphia, PA, July.
T. Morimoto, N. Uratani, T. Takezawa, O. Furuse,
Y. Sobashima, H. Iida, A. Nakamura, Y. Sag-
isaka, N. Higuchi, and Y. Yamazaki. 1994. A
speech and language database for speech trans-
lation research. In Proc. of the 3rd Int. Conf. on
Spoken Language Processing (ICSLP?94), pages
1791?1794, Yokohama, Japan, September.
F. J. Och and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statisti-
cal machine translation. In Proc. of the 40th
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302,
Philadelphia, PA, July.
F. J. Och, C. Tillmann, and H. Ney. 1999. Im-
proved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 20?28,
University of Maryland, College Park, MD,
June.
F. J. Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of
the 41th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 160?
167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto,
and S. Yamamoto. 2002. Toward a broad-
coverage bilingual corpus for speech translation
of travel conversations in the real world. In
Proc. of the Third Int. Conf. on Language Re-
sources and Evaluation (LREC), pages 147?152,
Las Palmas, Spain, May.
C. Tillmann and H. Ney. 2003. Word reordering
and a dynamic programming beam search algo-
rithm for statistical machine translation. Com-
putational Linguistics, 29(1):97?133, March.
S. Vogel. 2003. SMT decoder dissected: Word re-
ordering. In Proc. of the Int. Conf. on Natural
Language Processing and Knowledge Engineer-
ing (NLP-KE), pages 561?566, Beijing, China,
October.
D. Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation,
bracketing, and alignment of parallel corpora.
In Proc. of the 14th International Joint Conf.
on Artificial Intelligence (IJCAI), pages 1328?
1334, Montreal, August.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
R. Zens and H. Ney. 2003. A comparative study
on reordering constraints in statistical machine
translation. In Proc. of the 41th Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL), pages 144?151, Sapporo, Japan,
July.
Symmetric Word Alignments for Statistical Machine Translation
Evgeny Matusov and Richard Zens and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{matusov,zens,ney}@cs.rwth-aachen.de
Abstract
In this paper, we address the word
alignment problem for statistical machine
translation. We aim at creating a sym-
metric word alignment allowing for reli-
able one-to-many and many-to-one word
relationships. We perform the iterative
alignment training in the source-to-target
and the target-to-source direction with
the well-known IBM and HMM alignment
models. Using these models, we robustly
estimate the local costs of aligning a source
word and a target word in each sentence
pair. Then, we use efficient graph algo-
rithms to determine the symmetric align-
ment with minimal total costs (i. e. max-
imal alignment probability). We evalu-
ate the automatic alignments created in
this way on the German?English Verb-
mobil task and the French?English Cana-
dian Hansards task. We show statistically
significant improvements of the alignment
quality compared to the best results re-
ported so far. On the Verbmobil task,
we achieve an improvement of more than
1% absolute over the baseline error rate of
4.7%.
1 Introduction
Word-aligned bilingual corpora provide im-
portant knowledge for many natural language
processing tasks, such as the extraction of
bilingual word or phrase lexica (Melamed,
2000; Och and Ney, 2000). The solutions of
these problems depend heavily on the quality
of the word alignment (Och and Ney, 2000).
Word alignment models were first introduced
in statistical machine translation (Brown et
al., 1993). An alignment describes a mapping
from source sentence words to target sentence
words.
Using the IBM translation models IBM-1
to IBM-5 (Brown et al, 1993), as well as
the Hidden-Markov alignment model (Vogel et
al., 1996), we can produce alignments of good
quality. However, all these models constrain
the alignments so that a source word can be
aligned to at most one target word. This con-
straint is useful to reduce the computational
complexity of the model training, but makes
it hard to align phrases in the target lan-
guage (English) such as ?the day after tomor-
row? to one word in the source language (Ger-
man) ?u?bermorgen?. We will present a word
alignment algorithm which avoids this con-
straint and produces symmetric word align-
ments. This algorithm considers the align-
ment problem as a task of finding the edge
cover with minimal costs in a bipartite graph.
The parameters of the IBM models and HMM,
in particular the state occupation probabili-
ties, will be used to determine the costs of
aligning a specific source word to a target
word.
We will evaluate the suggested alignment
methods on the German?English Verbmo-
bil task and the French?English Canadian
Hansards task. We will show statistically sig-
nificant improvements compared to state-of-
the-art results in (Och and Ney, 2003).
2 Statistical Word Alignment Models
In this section, we will give an overview of
the commonly used statistical word alignment
techniques. They are based on the source-
channel approach to statistical machine trans-
lation (Brown et al, 1993). We are given
a source language sentence fJ1 := f1...fj ...fJwhich has to be translated into a target lan-
guage sentence eI1 := e1...ei...eI . Among allpossible target language sentences, we will
choose the sentence with the highest proba-
bility:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources allows for an independent modeling of
target language model Pr(eI1) and translation
model Pr(fJ1 |eI1). Into the translation model,the word alignment A is introduced as a hid-
den variable:
Pr(fJ1 |eI1) =
?
A
Pr(fJ1 , A|eI1)
Usually, the alignment is restricted in the
sense that each source word is aligned to at
most one target word, i.e. A = aJ1 . The align-ment may contain the connection aj = 0 with
the ?empty? word e0 to account for source sen-
tence words that are not aligned to any tar-
get word at all. A detailed description of the
popular translation/alignment models IBM-1
to IBM-5 (Brown et al, 1993), as well as the
Hidden-Markov alignment model (HMM) (Vo-
gel et al, 1996) can be found in (Och and Ney,
2003). Model 6 is a loglinear combination of
the IBM-4, IBM-1, and the HMM alignment
models.
A Viterbi alignment A? of a specific model is
an alignment for which the following equation
holds:
A? = argmax
A
{Pr(fJ1 , A|eI1)
} .
3 State Occupation Probabilities
The training of all alignment models is done
using the EM-algorithm. In the E-step, the
counts for each sentence pair (fJ1 , eI1) are cal-culated. Here, we present this calculation on
the example of the HMM. For its lexicon pa-
rameters, the marginal probability of a target
word ei to occur at the target sentence posi-
tion i as the translation of the source word fj
at the source sentence position j is estimated
with the following sum:
pj(i, fJ1 |eI1) =
?
aJ1 :aj=i
Pr(fJ1 , aJ1 |eI1)
This value represents the likelihood of aligning
fj to ei via every possible alignment A = aJ1that includes the alignment connection aj = i.
By normalizing over the target sentence posi-
tions, we arrive at the state occupation proba-
bility :
pj(i|fJ1 , eI1) =
pj(i, fJ1 |eI1)
I?
i?=1
pj(i?, fJ1 |eI1)
In the M-step of the EM training, the state
occupation probabilities are aggregated for all
words in the source and target vocabularies
by taking the sum over all training sentence
pairs. After proper renormalization the lexi-
con probabilities p(f |e) are determined.
Similarly, the training can be performed
in the inverse (target-to-source) direction,
yielding the state occupation probabilities
pi(j|eI1, fJ1 ).The negated logarithms of the state occu-
pation probabilities
w(i, j; fJ1 , eI1) := ? log pj(i|fJ1 , eI1) (1)
can be viewed as costs of aligning the source
word fj with the target word ei. Thus, the
word alignment task can be formulated as the
task of finding a mapping between the source
and the target words, so that each source and
each target position is covered and the total
costs of the alignment are minimal.
Using state occupation probabilities for
word alignment modeling results in a num-
ber of advantages. First of all, in calculation
of these probabilities with the models IBM-1,
IBM-2 and HMM the EM-algorithm is per-
formed exact, i.e. the summation over all
alignments is efficiently performed in the E-
step. For the HMM this is done using the
Baum-Welch algorithm (Baum, 1972). So far,
an efficient algorithm to compute the sum over
all alignments in the fertility models IBM-3
to IBM-5 is not known. Therefore, this sum
is approximated using a subset of promising
alignments (Och and Ney, 2000). In both
cases, the resulting estimates are more pre-
cise than the ones obtained by the maximum
approximation, i. e. by considering only the
Viterbi alignment.
Instead of using the state occupation prob-
abilities from only one training direction as
costs (Equation 1), we can interpolate the
state occupation probabilities from the source-
to-target and the target-to-source training for
each pair (i,j) of positions in a sentence pair
(fJ1 , eI1). This will improve the estimation ofthe local alignment costs. Having such sym-
metrized costs, we can employ the graph align-
ment algorithms (cf. Section 4) to produce
reliable alignment connections which include
many-to-one and one-to-many alignment re-
lationships. The presence of both relation-
ship types characterizes a symmetric align-
ment that can potentially improve the trans-
lation results (Figure 1 shows an example of a
symmetric alignment).
Another important advantage is the effi-
ciency of the graph algorithms used to deter-
Figure 1: Example of a symmetric alignment
with one-to-many and many-to-one connec-
tions (Verbmobil task, spontaneous speech).
mine the final symmetric alignment. They will
be discussed in Section 4.
4 Alignment Algorithms
In this section, we describe the alignment ex-
traction algorithms. We assume that for each
sentence pair (fJ1 , eI1) we are given a cost ma-trix C.1 The elements of this matrix cij are
the local costs that result from aligning source
word fj to target word ei. For a given align-
ment A ? I ? J , we define the costs of this
alignment c(A) as the sum of the local costs
of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (2)
Now, our task is to find the alignment with the
minimum costs. Obviously, the empty align-
ment has always costs of zero and would be op-
timal. To avoid this, we introduce additional
constraints. The first constraint is source sen-
tence coverage. Thus each source word has
to be aligned to at least one target word or
alternatively to the empty word. The second
constraint is target sentence coverage. Similar
to the source sentence coverage thus each tar-
get word is aligned to at least one source word
or the empty word.
Enforcing only the source sentence cover-
age, the minimum cost alignment is a mapping
from source positions j to target positions aj ,
including zero for the empty word. Each tar-
get position aj can be computed as:
aj = argmin
i
{cij}
This means, in each column we choose the
row with the minimum costs. This method re-
sembles the common IBM models in the sense
1For notational convenience, we omit the depen-
dency on the sentence pair (fJ1 , eI1) in this section.
that the IBM models are also a mapping from
source positions to target positions. There-
fore, this method is comparable to the IBM
models for the source-to-target direction. Sim-
ilarly, if we enforce only the target sentence
coverage, the minimum cost alignment is a
mapping from target positions i to source po-
sitions bi. Here, we have to choose in each
row the column with the minimum costs. The
complexity of these algorithms is in O(I ? J).
The algorithms for determining such a non-
symmetric alignment are rather simple. A
more interesting case arises, if we enforce both
constraints, i.e. each source word as well as
each target word has to be aligned at least
once. Even in this case, we can find the global
optimum in polynomial time.
The task is to find a symmetric alignment
A, for which the costs c(A) are minimal (Equa-
tion 2). This task is equivalent to finding
a minimum-weight edge cover (MWEC) in a
complete bipartite graph2. The two node
sets of this bipartite graph correspond to the
source sentence positions and the target sen-
tence positions, respectively. The costs of an
edge are the elements of the cost matrix C.
To solve the minimum-weight edge cover
problem, we reduce it to the maximum-weight
bipartite matching problem. As described
in (Keijsper and Pendavingh, 1998), this re-
duction is linear in the graph size. For the
maximum-weight bipartite matching problem,
well-known algorithm exist, e.g. the Hungar-
ian method. The complexity of this algorithm
is in O((I + J) ? I ? J). We will call the solu-
tion of the minimum-weight edge cover prob-
lem with the Hungarian method ?the MWEC
algorithm?. In contrary, we will refer to the al-
gorithm enforcing either source sentence cov-
erage or target sentence coverage as the one-
sided minimum-weight edge cover algorithm
(o-MWEC).
The cost matrix of a sentence pair (fJ1 , eI1)can be computed as a weighted linear interpo-
lation of various cost types hm:
cij =
M?
m=1
?m ? hm(i, j)
In our experiments, we will use the negated
logarithm of the state occupation probabilities
as described in Section 3. To obtain a more
symmetric estimate of the costs, we will inter-
polate both the source-to-target direction and
2An edge cover of G is a set of edges E? such that
each node of G is incident to at least one edge in E?.
the target-to-source direction (thus the state
occupation probabilities are interpolated log-
linearly). Because the alignments determined
in the source-to-target training may substan-
tially differ in quality from those produced in
the target-to-source training, we will use an
interpolation weight ?:
cij = ? ?w(i, j; fJ1 , eI1) + (1??) ?w(j, i; eI1, fJ1 )
(3)
Additional feature functions can be included
to compute cij ; for example, one could make
use of a bilingual word or phrase dictionary.
To apply the methods described in this sec-
tion, we made two assumptions: first, the costs
of an alignment can be computed as the sum
of local costs. Second, the features have to be
static in the sense that we have to fix the costs
before aligning any word. Therefore, we can-
not apply dynamic features such as the IBM-
4 distortion model in a straightforward way.
One way to overcome these restrictions lies in
using the state occupation probabilities; e.g.
for IBM-4, they contain the distortion model
to some extent.
5 Results
5.1 Evaluation Criterion
We use the same evaluation criterion as de-
scribed in (Och and Ney, 2000). We compare
the generated word alignment to a reference
alignment produced by human experts. The
annotation scheme explicitly takes the am-
biguity of the word alignment into account.
There are two different kinds of alignments:
sure alignments (S) which are used for unam-
biguous alignments and possible alignments
(P ) which are used for alignments that might
or might not exist. The P relation is used
especially to align words within idiomatic ex-
pressions and free translations. It is guaran-
teed that the sure alignments are a subset of
the possible alignments (S ? P ). The ob-
tained reference alignment may contain many-
to-one and one-to-many relationships.
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER), which is derived from the
well-known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER(S, P ;A) = 1? |A ? S|+ |A ? P ||A|+ |S|
Table 1: Verbmobil task: corpus statistics.
Source/Target: German English
Train Sentences 34 446
Words 329 625 343 076
Vocabulary 5 936 3 505
Singletons 2 600 1 305
Dictionary Entries 4 404
Test Sentences 354
Words 3 233 3 109
S reference relations 2 559
P reference relations 4 596
Table 2: Canadian Hansards: corpus statistics.
Source/Target: French English
Train Sentences 128K
Words 2.12M 1.93M
Vocabulary 37 542 29 414
Singletons 12 986 9 572
Dictionary Entries 28 701
Test Sentences 500
Words 8 749 7 946
S reference relations 4 443
P reference relations 19 779
With these definitions a recall error can only
occur if a S(ure) alignment is not found and a
precision error can only occur if a found align-
ment is not even P (ossible).
5.2 Experimental Setup
We evaluated the presented lexicon sym-
metrization methods on the Verbmobil and
the Canadian Hansards task. The German?
English Verbmobil task (Wahlster, 2000) is a
speech translation task in the domain of ap-
pointment scheduling, travel planning and ho-
tel reservation. The French?English Canadian
Hansards task consists of the debates in the
Canadian Parliament.
The corpus statistics are shown in Table 1
and Table 2. The number of running words
and the vocabularies are based on full-form
words including punctuation marks. As in
(Och and Ney, 2003), the first 100 sentences
of the test corpus are used as a development
corpus to optimize model parameters that are
not trained via the EM algorithm, e.g. the
interpolation weights. The remaining part of
the test corpus is used to evaluate the models.
We use the same training schemes (model
sequences) as presented in (Och and Ney,
2003): 15H5334363 for the Verbmobil Task ,
i.e. 5 iteration of IBM-1, 5 iterations of the
HMM, 3 iteration of IBM-3, etc.; for the Cana-
dian Hansards task, we use 15H10334363. We
refer to these schemes as the Model 6 schemes.
For comparison, we also perform less sophisti-
cated trainings, to which we refer as the HMM
schemes (15H10 and 15H5, respectively), as
well as the IBM Model 4 schemes (15H103343
and 15H53343).
In all training schemes we use a conventional
dictionary (possibly containing phrases) as ad-
ditional training material. Because we use the
same training and testing conditions as (Och
and Ney, 2003), we will refer to the results pre-
sented in that article as the baseline results.
5.3 Non-symmetric Alignments
In the first experiments, we use the state oc-
cupation probabilities from only one transla-
tion direction to determine the word align-
ment. This allows for a fair comparison with
the Viterbi alignment computed as the result
of the training procedure. In the source-to-
target translation direction, we cannot esti-
mate the probability for the target words with
fertility zero and choose to set it to 0. In this
case, the minimum weight edge cover problem
is solved by the one-sided MWEC algorithm.
Like the Viterbi alignments, the alignments
produced by this algorithm satisfy the con-
straint that multiple source (target) words can
only be aligned to one target (source) word.
Tables 3 and 4 show the performance of
the one-sided MWEC algorithm in compar-
ison with the experiment reported by (Och
and Ney, 2003). We report not only the final
alignment error rates, but also the intermedi-
ate results for the HMM and IBM-4 training
schemes.
For IBM-3 to IBM-5, the Viterbi alignment
and a set of promising alignments are used
to determine the state occupation probabili-
ties. Consequently, we observe similar align-
ment quality when comparing the Viterbi and
the one-sided MWEC alignments.
We also evaluated the alignment quality af-
ter applying alignment generalization meth-
ods, i.e. we combine the alignment of both
translation directions. Experimentally, the
best generalization heuristic for the Canadian
Hansards task is the intersection of the source-
to-target and the target-to-source alignments.
For the Verbmobil task, the refined method
of (Och and Ney, 2003) is used. Again, we
observed similar alignment error rates when
merging either the Viterbi alignments or the
o-MWEC alignments.
Table 3: AER [%] for non-symmetric align-
ment methods and for various models (HMM,
IBM-4, Model 6) on the Canadian Hansards
task.
Alignment method HMM IBM4 M6
Baseline T?S 14.1 12.9 11.9
S?T 14.4 12.8 11.7
intersection 8.4 6.9 7.8
o-MWEC T?S 14.0 13.1 11.9
S?T 14.3 13.0 11.7
intersection 8.2 7.1 7.8
Table 4: AER [%] for non-symmetric align-
ment methods and for various models (HMM,
IBM-4, Model 6) on the Verbmobil task.
Alignment method HMM IBM4 M6
Baseline T?S 7.6 4.8 4.6
S?T 12.1 9.3 8.8
refined 7.1 4.7 4.7
o-MWEC T?S 7.3 4.8 4.5
S?T 12.0 9.3 8.5
refined 6.7 4.6 4.6
5.4 Symmetric Alignments
The heuristically generalized Viterbi align-
ments presented in the previous section can
potentially avoid the alignment constraints3.
However, the choice of the optimal general-
ization heuristic may depend on a particular
language pair and may require extensive man-
ual optimization. In contrast, the symmetric
MWEC algorithm is a systematic and theo-
retically well-founded approach to the task of
producing a symmetric alignment.
In the experiments with the symmetric
MWEC algorithm, the optimal interpolation
parameter ? (see Equation 3) for the Verbmo-
bil corpus was empirically determined as 0.8.
This shows that the model parameters can be
estimated more reliably in the direction from
German to English. In the inverse English-
to-German alignment training, the mappings
of many English words to one German word
are not allowed by the modeling constraints,
although such alignment mappings are signif-
icantly more frequent than mappings of many
German words to one English word.
The experimentally best interpolation pa-
rameter for the Canadian Hansards corpus was
? = 0.5. Thus the model parameters esti-
mated in the translation direction from French
to English are as reliable as the ones estimated
3Consequently, we will use them as baseline for the
experiments with symmetric alignments.
in the direction from English to French.
Lines 2a and 2b of Table 5 show the perfor-
mance of the MWEC algorithm. The align-
ment error rates are slightly lower if the HMM
or the full Model 6 training scheme is used
to train the state occupation probabilities on
the Canadian Hansards task. On the Verbmo-
bil task, the improvement is more significant,
yielding an alignment error rate of 4.1%.
Columns 4 and 5 of Table 5 contain the re-
sults of the experiments, in which the costs
cij were determined as the loglinear interpola-
tion of state occupation probabilities obtained
from the HMM training scheme with those
from IBM-4 (column 4) or from Model 6 (col-
umn 5). We set the interpolation parameters
for the two translation directions proportional
to the optimal values determined in the previ-
ous experiments. On the Verbmobil task, we
obtain a further improvement of 19% relative
over the baseline result reported in (Och and
Ney, 2003), reaching an AER as low as 3.8%.
The improvements of the alignment qual-
ity on the Canadian Hansards task are less
significant. The manual reference alignments
for this task contain many possible connec-
tions and only a few sure connections (cf. Ta-
ble 2). Thus automatic alignments consisting
of only a few reliable alignment points are fa-
vored. Because the differences in the number
of words and word order between French and
English are not as dramatic as e.g. between
German and English, the probability of the
empty word alignment is not very high. There-
fore, plenty of alignment points are produced
by the MWEC algorithm, resulting in a high
recall and low precision. To increase the preci-
sion, we replaced the empty word connection
costs (previously trained as state occupation
probabiliities using the EM algorithm) by the
global, word- and position-independent costs
depending only on one of the involved lan-
guages. The alignment error rates for these
experiments are given in lines 3a and 3b of Ta-
ble 5. The global empty word probability for
the Canadian Hansards task was empirically
set to 0.45 for French and for English, and,
for the Verbmobil task, to 0.6 for German and
0.1 for English. On the Canadian Hansards
task, we achieved further significant reduction
of the AER. In particular, we reached an AER
of 6.6% by performing only the HMM training.
In this case the effectiveness of the MWEC al-
gorithm is combined with the efficiency of the
HMM training, resulting in a fast and robust
alignment training procedure.
We also tested the more simple one-sided
MWEC algorithm. In contrast to the exper-
iments presented in Section 5.3, we used the
loglinear interpolated state occupation prob-
abilities (given by the Equation 3) as costs.
Thus, although the algorithm is not able to
produce a symmetric alignment, it operates
with symmetrized costs. In addition, we used
a combination heuristic to obtain a symmetric
alignment. The results of these experiments
are presented in Table 5, lines 4-6 a/b.
The performance of the one-sided MWEC
algorithm turned out to be quite robust on
both tasks. However, the o-MWEC align-
ments are not symmetric and the achieved low
AER depends heavily on the differences be-
tween the involved languages, which may fa-
vor many-to-one alignments in one translation
direction only. That is why on the Verbmobil
task, when determining the mininum weight in
each row for the translation direction from En-
glish to German, the alignment quality deteri-
orates, because the algorithm cannot produce
alignments which map several English words
to one German word (line 5b of Table 5).
Applying the generalization heuristics
(line 6a/b of Table 5), we achieve an AER of
6.0% on the Canadian Hansards task when
interpolating the state occupation probabil-
ities trained with the HMM and with the
IBM-4 schemes. On the Verbmobil task, the
interpolation of the HMM and the Model 6
schemes yields the best result of 3.7% AER.
In the latter experiment, we reached 97.3%
precision and 95.2% recall.
6 Related Work
A description of the IBM models for statistical
machine translation can be found in (Brown et
al., 1993). The HMM-based alignment model
was introduced in (Vogel et al, 1996). An
overview of these models is given in (Och and
Ney, 2003). That article also introduces the
Model 6; additionally, state-of-the-art results
are presented for the Verbmobil task and the
Canadian Hansards task for various configura-
tions. Therefore, we chose them as baseline.
Additional linguistic knowledge sources such
as dependeny trees or parse trees were used in
(Cherry and Lin, 2003; Gildea, 2003). Bilin-
gual bracketing methods were used to produce
a word alignment in (Wu, 1997). (Melamed,
2000) uses an alignment model that enforces
one-to-one alignments for nonempty words.
Table 5: AER[%] for different alignment symmetrization methods and for various alignment
models on the Canadian Hansards and the Verbmobil tasks (MWEC: minimum weight edge
cover, EW: empty word).
Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6
Canadian 1a. Baseline (intersection) 8.4 6.9 7.8 ? ?
Hansards 2a. MWEC 7.9 9.3 7.5 8.2 7.4
3a. MWEC (global EW costs) 6.6 7.4 6.9 6.4 6.4
4a. o-MWEC T?S 7.3 7.9 7.4 6.7 7.0
5a. S?T 7.7 7.6 7.2 6.9 6.9
6a. S?T (intersection) 7.2 6.6 7.6 6.0 7.1
Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6
Verbmobil 1b. Baseline (refined) 7.1 4.7 4.7 ? ?
2b. MWEC 6.4 4.4 4.1 4.3 3.8
3b. MWEC (global EW costs) 5.8 5.8 6.6 6.0 6.7
4b. o-MWEC T?S 6.8 4.4 4.1 4.5 3.7
5b. S?T 9.3 7.2 6.8 7.5 6.9
6b. S?T (refined) 6.7 4.3 4.1 4.6 3.7
7 Conclusions
In this paper, we addressed the task of au-
tomatically generating symmetric word align-
ments for statistical machine translation. We
exploited the state occupation probabilties de-
rived from the IBM and HMM translation
models. We used the negated logarithms of
these probabilities as local alignment costs and
reduced the word alignment problem to find-
ing an edge cover with minimal costs in a
bipartite graph. We presented efficient algo-
rithms for the solution of this problem. We
evaluated the performance of these algorithms
by comparing the alignment quality to man-
ual reference alignments. We showed that in-
terpolating the alignment costs of the source-
to-target and the target-to-source translation
directions can result in a significant improve-
ment of the alignment quality.
In the future, we plan to integrate the graph
algorithms into the iterative training proce-
dure. Investigating the usefulness of addi-
tional feature functions might be interesting
as well.
Acknowledgment
This work has been partially funded by the
EU project TransType 2, IST-2001-32091.
References
L. E. Baum. 1972. An inequality and associated
maximization technique in statistical estimation
for probabilistic functions of markov processes.
Inequalities, 3:1?8.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, June.
C. Cherry and D. Lin. 2003. A probability model
to improve word alignment. In Proc. of the 41th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 88?95, Sap-
poro, Japan, July.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of the 41th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 80?87, Sapporo,
Japan, July.
J. Keijsper and R. Pendavingh. 1998. An effi-
cient algorithm for minimum-weight bibranch-
ing. Journal of Combinatorial Theory Series B,
73(2):130?145, July.
I. D. Melamed. 2000. Models of translational
equivalence among words. Computational Lin-
guistics, 26(2):221?249.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 440?447, Hong Kong,
October.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation.
In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
W. Wahlster, editor. 2000. Verbmobil: Founda-
tions of speech-to-speech translations. Springer
Verlag, Berlin, Germany, July.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
Improving Word Alignment Quality using Morpho-syntactic
Information
Maja Popovic? and Hermann Ney
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
Ahornstrasse 55
52056 Aachen
Germany
{popovic,ney}@cs.rwth-aachen.de
Abstract
In this paper, we present an approach to
include morpho-syntactic dependencies into
the training of the statistical alignment
models. Existing statistical translation sys-
tems usually treat different derivations of
the same base form as they were indepen-
dent of each other. We propose a method
which explicitly takes into account such in-
terdependencies during the EM training of
the statistical alignment models. The eval-
uation is done by comparing the obtained
Viterbi alignments with a manually anno-
tated reference alignment. The improve-
ments of the alignment quality compared
to the, to our knowledge, best system are
reported on the German-English Verbmobil
corpus.
1 Introduction
In statistical machine translation, a translation
model Pr(fJ1 |eI1) describes the correspondences
between the words in the source language sen-
tence fJ1 and the words in the target language
sentence eI1. Statistical alignment models are
created by introducing a hidden variable aJ1
representing a mapping from the source word
fj into the target word eaj . So far, most of
the statistical machine translation systems are
based on the single-word alignment models as
described in (Brown et al, 1993) as well as the
Hidden Markov alignment model (Vogel et al,
1996). The lexicon models used in these systems
typically do not include any linguistic or con-
textual information which often results in inad-
equate alignments between the sentence pairs.
In this work, we propose an approach to im-
prove the quality of the statistical alignments
by taking into account the interdependencies of
different derivations of the words. We are get-
ting use of the hierarchical representation of the
statistical lexicon model as proposed in (Nie?en
and Ney, 2001) for the conventional EM training
procedure. Experimental results are reported
for the German-English Verbmobil corpus and
the evaluation is done by comparing the ob-
tained Viterbi alignments after the training of
conventional models and models which are using
morpho-syntactic information with a manually
annotated reference alignment.
2 Related Work
The popular IBM models for statistical ma-
chine translation are described in (Brown et
al., 1993) and the HMM-based alignment model
was introduced in (Vogel et al, 1996). A good
overview of all these models is given in (Och
and Ney, 2003) where the model IBM-6 is also
introduced as the log-linear interpolation of the
other models.
Context dependencies have been introduced
into the training of alignments in (Varea et al,
2002), but they do not take any linguistic infor-
mation into account.
Some recent publications have proposed the
use of morpho-syntactic knowledge for statisti-
cal machine translation, but mostly only for the
preprocessing step whereas training procedure
of the statistical models remains the same (e.g.
(Nie?en and Ney, 2001a)).
Incorporation of the morpho-syntactic knowl-
egde into statistical models has been dealt
in (Nie?en and Ney, 2001): hierarchical lexi-
con models containing base forms and set of
morpho-syntactic tags are proposed for the
translation from German into English. How-
ever, these lexicon models are not used for the
training but have been created from the Viterbi
alignment obtained after the usual training pro-
cedure.
The use of POS information for improving
statistical alignment quality of the HMM-based
model is described in (Toutanova et al, 2002).
They introduce additional lexicon probability
for POS tags in both languages, but actually
are not going beyond full forms.
3 Statistical Alignment Models
The goal of statistical machine translation is to
translate an input word sequence f1, . . . , fJ in
the source language into a target language word
sequence e1, . . . , eI . Given the source language
sequence, we have to choose the target language
sequence that maximises the product of the lan-
guage model probability Pr(eI1) and the trans-
lation model probability Pr(fJ1 |eI1). The trans-
lation model describes the correspondence be-
tween the words in the source and the target
sequence whereas the language model describes
well-formedness of a produced target sequence.
The translation model can be rewritten in the
following way:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1)
where aJ1 are called alignments and represent
a mapping from the source word position j to
the target word position i = aj . Alignments
are introduced into translation model as a hid-
den variable, similar to the concept of Hidden
Markov Models (HMM) in speech recognition.
The translation probability Pr(fJ1 , aJ1 |eI1) can
be further rewritten as follows:
Pr(fJ1 , aJ1 |eI1) =
J?
j=1
Pr(fj , aj |f j?11 , aj?11 , eI1)
=
J?
j=1
Pr(aj |f j?11 , aj?11 , eI1) ?
?Pr(fj |f j?11 , aj1, eI1)
where Pr(aj |f j?11 , aj?11 , eI1) is called alignment
probability and Pr(fj |f j?11 , aj1, eI1) is lexicon
probability.
In all popular translation models IBM-1 to
IBM-5 as well as in HMM translation model,
the lexicon probability Pr(fj |f j?11 , aj1, eI1) is ap-
proximated with the simple single-word lexi-
con probability p(fj |eaj ) which takes into ac-
count only full forms of the words fj and
eaj . The difference between these models is
based on the definition of alignment model
Pr(aj |f j?11 , aj?11 , eI1). Detailed description of
those models can be found in (Brown et al,
1993), (Vogel et al, 1996) and (Och and Ney,
2003).
4 Hierarchical Representation of the
Lexicon Model
Typically, the statistical lexicon model is based
only on the full forms of the words and does not
have any information about the fact that some
different full forms are actually derivations of
the same base form. For highly inflected lan-
guages like German this might cause problems
because the coverage of the lexicon might be
low since the token/type ratio for German is
typically much lower than for English (e.g. for
Verbmobil: English 99.4, German 56.3).
To take these interdependencies into account,
we use the hierarchical representation of the sta-
tistical lexicon model as proposed in (Nie?en
and Ney, 2001). A constraint grammar parser
GERCG for lexical analysis and morphological
and syntactic disambiguation for German lan-
guage is used to obtain morpho-syntactic infor-
mation. For each German word, this tool pro-
vides its base form and the sequence of morpho-
syntactic tags, and this information is then
added into the original corpus. For example,
the German word ?gehe? (go), a verb in the
indicative mood and present tense which is de-
rived from the base form ?gehen? is annotated
as ?gehe#gehen-V-IND-PRES#gehen?.
This new representation of the corpus where
full word forms are enriched with its base forms
and tags enables gradual accessing of informa-
tion with different levels of abstraction. Con-
sider for example the above mentioned German
word ?gehe? which can be translated into the
English word ?go?. Another derivation of the
same base form ?gehen? is ?gehst? which also
can be translated by ?go?. Existing statistical
translation models cannot handle the fact that
?gehe? and ?gehst? are derivatives of the same
base form and both can be translated into the
same English word ?go?, whereas the hierarchi-
cal representation makes it possible to take such
interdependencies into account.
5 EM Training
5.1 Standard EM training (review)
In this section, we will briefly review the stan-
dard EM algorithm for the training of the lexi-
con model.
In the E-step the lexical counts are collected
over all sentences in the corpus:
C(f, e) =
?
s
?
a
p(a|f s, es)
?
i,j
?(f, fjs)?(e, eis)
In the M-step the lexicon probabilities are cal-
culated:
p(f |e) = C(f, e)?
f?
C(f? , e)
The procedure is similar for the other model
parameters, i.e. alignment and fertility proba-
bilities.
For models IBM-1, IBM-2 and HMM, an ef-
ficient computation of the sum over all align-
ments is possible. For the other models, the
sum is approximated using an appropriately de-
fined neighbourhood of the Viterbi alignment
(see (Och and Ney, 2003) for details).
5.2 EM training using hierarchical
counts
In this section we describe the EM training of
the lexicon model using so-called hierarchical
counts which are collected from the hierarchi-
caly annotated corpus.
In the E-step the following types of counts are
collected:
? full form counts:
C(f, e) =
?
s
?
a
p(a|f s, es) ?
?
?
i,j
?(f, fjs)?(e, eis)
where f is the full form of the word, e.g.
?gehe?;
? base form+tag counts:
C(fbt, e) =
?
s
?
a
p(a|f s, es) ?
?
?
i,j
?(fbt, fbtjs)?(e, eis)
where fbt represents the base form of the
word f with sequence of corresponding
tags, e.g. ?gehen-V-IND-PRES?;
? base form counts:
C(fb, e) =
?
s
?
a
p(a|f s, es) ?
?
?
i,j
?(fb, fbjs)?(e, eis)
where fb is the base form of the word f ,
e.g. ?gehen?.
For each full form, refined hierarchical counts
are obtained in the following way:
Chier(f, e) = C(f, e) + C(fbt, e) + C(fb, e)
and the M-step is then performed using hier-
archical counts:
p(f |e) = Chier(f, e)?
f?
Chier(f? , e)
The training procedure for the other model
parameters remains unchanged.
6 Experimental Results
We performed our experiments on the Verbmo-
bil corpus. The Verbmobil task (W. Wahlster,
editor, 2000) is a speech translation task in the
domain of appointment scheduling, travel plan-
ning and hotel reservation. The corpus statis-
tics is shown in Table 1. The number of sure and
possible alignments in the manual reference is
given as well. We also used a small training cor-
pus consisting of only 500 sentences randomly
chosen from the main corpus.
We carried out the training scheme
14H5334365 using the toolkit GIZA++.
The scheme is defined according to the number
of iterations for each model. For example, 43
means three iterations of the model IBM-4. We
trained the IBM-1 and HMM model using hier-
archical lexicon counts, and the parameters of
the other models were also indirectly improved
thanks to the refined parameters of the initial
models.
German English
Train Sentences 34446
Words 329625 343076
Vocabulary 5936 3505
Singletons 2600 1305
Test Sentences 354
Words 3233 3109
S relations 2559
P relations 4596
Table 1: Corpus statistics for Verbmobil task
6.1 Evaluation Method
We use the evaluation criterion described in
(Och and Ney, 2000). The obtained word
alignment is compared to a reference alignment
produced by human experts. The annotation
scheme explicitly takes into account the ambi-
guity of the word alignment. The unambiguous
alignments are annotated as sure alignments (S)
and the ambiguous ones as possible alignments
(P ). The set of possible alignments P is used
especially for idiomatic expressions, free trans-
lations and missing function words. The set S
is subset of the set P (S ? P ).
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER) which is derived from the well-
known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER = 1? |A ? S|+ |A ? P ||A|+ |S|
Thus, a recall error can only occur if a S(ure)
alignment is not found and a precision error
can only occur if a found alignment is not even
P (ossible).
6.2 Alignment Quality Results
Table 2 shows the alignment quality for the two
corpus sizes of the Verbmobil task. Results
are presented for the Viterbi alignments from
both translation directions (German?English
and English?German) as well as for combina-
tion of those two alignments.
The table shows the baseline AER for dif-
ferent training schemes and the corresponding
AER when the hierarchical counts are used. We
see that there is a consistent decrease in AER
for all training schemes, especially for the small
training corpus. It can be also seen that greater
improvements are yielded for the simpler mod-
els.
7 Conclusions
In this work we have presented an approach
for including morpho-syntactic knowledge into
a maximum likelihood training of statistical
translation models. As can be seen in Section
5, going beyond full forms during the training
by taking into account the interdependencies of
the different derivations of the same base form
results in the improvements of the alignment
corpus size = 0.5k
Training Model D ? E E ? D combined
14 ibm1 27.5 33.4 22.7
+hier 24.8 30.3 20.5
14H5 hmm 18.8 24.0 16.9
+hier 16.9 21.5 14.8
14H533 ibm3 18.4 22.8 17.0
+hier 16.7 22.1 15.5
14H53343 ibm4 16.9 21.5 16.2
+hier 15.8 20.7 14.9
14H5334365 ibm6 16.7 21.1 15.9
+hier 15.6 20.9 14.8
corpus size = 34k
Training Model D ? E E ? D combined
14 ibm1 17.6 24.1 14.1
+hier 16.8 21.8 13.7
14H5 hmm 8.9 14.9 7.9
+hier 8.4 13.7 7.3
14H533 ibm3 8.4 12.8 7.7
+hier 8.2 12.7 7.4
14H53343 ibm4 6.3 10.9 6.0
+hier 6.1 10.8 5.7
14H5334365 ibm6 5.7 10.0 5.5
+hier 5.5 9.7 5.0
Table 2: AER [%] for Verbmobil corpus for the
baseline system (name of the model) and the
system using hierarchical method (+hier)
quality, especially for the small training corpus.
We assume that the method can be very effec-
tive for cases where only small amount of data is
available. We also expect further improvements
by performing a special modelling for the rare
words.
We are planning to investigate possibilities
of improving the alignment quality for different
language pairs using different types of morpho-
syntactic information, like for example to use
word stems and suffixes for morphologicaly rich
languages where some parts of the words have
to be aligned to the whole English words (e.g.
Spanish verbs, Finnish in general, etc.) We are
also planning to use the refined alignments for
the translation process.
References
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra and Robert L. Mercer.
1993. The mathematics of statistical machine
translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311
Ismael Garc??a Varea, Franz Josef Och, Her-
mann Ney and Francisco Casacuberta. 2002.
Improving alignment quality in statistical
machine translation using context-dependent
maximum entropy models. In Proc. of the
19th International Conference on Computa-
tional Linguistics (COLING), pages 1051?
1057, Taipei, Taiwan, August.
Sonja Nie?en and Hermann Ney. 2001a.
Morpho-syntactic analysis for reordering in
statistical machine translation. In Proc. MT
Summit VIII, pages 247?252, Santiago de
Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001. Toward
hierarchical models for statistical machine
translation of inflected languages. In 39th
Annual Meeting of the Assoc. for Computa-
tional Linguistics - joint with EACL 2001:
Proc. Workshop on Data-Driven Machine
Translation, pages 47?54, Toulouse, France,
July.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proc.
of the 38th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
440?447, Hong Kong, October.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguis-
tics, 29(1):19?51
Kristina Toutanova, H. Tolga Ilhan and
Christopher D. Manning. 2002. Extensions
to HMM-based statistical word alignment
models. In Proc. Conf. on Empirical Methods
for Natural Language Processing (EMNLP),
pages 87?94, Philadelphia, PA, July.
Stephan Vogel, Hermann Ney and Cristoph Till-
mann. 1996. HMM-based word alignment
in statistical translation. In Proc. of the
16th International Conference on Computa-
tional Linguistics (COLING) , pages 836?
841, Copenhagen, Denmark, August.
W. Wahlster, editor 2000. Verbmobil: Foun-
dations of speech-to-speech translations.
Springer Verlag, Berlin, Germany, July.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1017?1024
Manchester, August 2008
Bayesian Semi-Supervised Chinese Word Segmentation
for Statistical Machine Translation
Jia Xu
?
, Jianfeng Gao
?
, Kristina Toutanova
?
, Hermann Ney
?
Computer Science 6
?
Microsoft Corporation
?
RWTH Aachen University One Microsoft Way
D-52056 Aachen, Germany Redmond, WA 98052, USA
{xujia,ney}@cs.rwth-aachen.de {jfgao,kristout}@microsoft.com
Abstract
Words in Chinese text are not naturally
separated by delimiters, which poses a
challenge to standard machine translation
(MT) systems. In MT, the widely used
approach is to apply a Chinese word seg-
menter trained from manually annotated
data, using a fixed lexicon. Such word
segmentation is not necessarily optimal
for translation. We propose a Bayesian
semi-supervised Chinese word segmenta-
tion model which uses both monolingual
and bilingual information to derive a seg-
mentation suitable for MT. Experiments
show that our method improves a state-of-
the-art MT system in a small and a large
data environment.
1 Introduction
Chinese sentences are written in the form of a se-
quence of Chinese characters, and words are not
separated by white spaces. This is different from
most European languages and poses difficulty in
many natural language processing tasks, such as
machine translation.
It is difficult to define ?correct? Chinese word
segmentation (CWS) and various definitions have
been proposed. In this work, we explore the idea
that the best segmentation depends on the task, and
concentrate on developing a CWS method for MT,
which leads to better translation performance.
The common solution in Chinese-to-English
translation has been to segment the Chinese text
using an off-the-shelf CWS method, and to apply
a standard translation model given the fixed seg-
mentation. The most widely applied method for
MT is unigram segmentation, such as segmenta-
tion using the LDC (LDC, 2003) tool, which re-
quires a manual lexicon containing a list of Chi-
nese words and their frequencies. The lexicon and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
frequencies are obtained using manually annotated
data. This method is sub-optimal for MT. For ex-
ample, ?(paper) and ](card) can be two words
or composed into one word ?](cards). Since ?
]does not exist in the manual lexicon, it cannot
be generated by this method.
In addition to unigram segmentation, other
methods have been proposed. For example, (Gao
et al, 2005) described an adaptive CWS system,
and (Andrew, 2006) employed a conditional ran-
dom field model for sequence segmentation. How-
ever, these methods are not specifically devel-
oped for the MT application, and significant im-
provements in translation performance need to be
shown.
In (Xu et al, 2004) and (Xu et al, 2005),
word segmentations are integrated into MT sys-
tems during model training and translation. We re-
fine the method in training using a Bayesian semi-
supervised CWS approach motivated by (Goldwa-
ter et al, 2006). We describe a generative model
which consists of a word model and two alignment
models, representing the monolingual and bilin-
gual information, respectively. In our methods, we
first segment Chinese text using a unigram seg-
menter, and then learn new word types and word
distributions, which are suitable for MT.
Our experiments on both large (NIST) and small
(IWSLT) data tracks of Chinese-to-English trans-
lation show that our method improves the per-
formance of a state-of-the-art machine translation
system.
2 Review of the Baseline System
2.1 Word segmentation
In statistical machine translation, we are given a
Chinese sentence in characters c
K
1
= c
1
. . . c
K
which is to be translated into an English sentence
e
I
1
= e
1
. . . e
I
. In order to obtain a more adequate
mapping between Chinese and English words, c
K
1
is usually segmented into words f
J
1
= f
1
. . . f
J
in
preprocessing.
In our baseline system, we apply the commonly
1017
used unigram model to generate the segmenta-
tion. Given a manually compiled lexicon contain-
ing words and their relative frequencies P
s
(f
?
j
),
the best segmentation f
J
1
is the one that maximizes
the joint probability of all words in the sentence,
with the assumption that words are independent of
each other
1
:
f
J
1
= argmax
f
?
J
?
1
Pr(f
?
J
?
1
|c
K
1
)
? argmax
f
?
J
?
1
J
?
?
j=1
P
s
(f
?
j
),
where the maximization is taken over Chinese
word sequences whose character sequence is c
K
1
.
2.2 Translation system
Once we have segmented the Chinese sentences
into words, we train standard alignment models
in both directions with GIZA++ (Och and Ney,
2002) using models of IBM-1 (Brown et al, 1993),
HMM (Vogel et al, 1996) and IBM-4 (Brown et
al., 1993).
Our MT system uses a phrase-based decoder
and the log-linear model described in (Zens and
Ney, 2004). Features in the log-linear model in-
clude translation models in two directions, a lan-
guage model, a distortion model and a sentence
length penalty. The feature weights are tuned on
the development set using a downhill simplex al-
gorithm (Press et al, 2002). The language model
is a statistical ngram model estimated using modi-
fied Kneser-Ney smoothing.
3 Unigram Dirichlet Process Model for
CWS
The simplest version of our model is based on a
unigram Dirichlet Process (DP) model, using only
monolingual information. Different from a stan-
dard unigram model for CWS, our model can in-
troduce new Chinese word types and learn word
distributions automatically from unlabeled data.
According to this model, a corpus of Chinese
words f
1
, . . . f
m
, . . . , f
M
is generated via:
G|?, P
0
? DP (?, P
0
)
f
m
|G ? G
where G is a distribution over words drawn from a
Dirichlet Process prior with base measure P
0
and
concentration parameter ?.
We never explicitly estimate G but instead
integrate over its possible values and perform
Bayesian inference. It is easy to compute the
1
The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol P (?).
probability of a Chinese word given a set of al-
ready generated words, while integrating over G.
This is done by casting Chinese word generation
as a Chinese restaurant process (CRP) (Aldous,
1985), i.e. a restaurant with an infinite num-
ber of tables (approximately corresponding to Chi-
nese word types), each table with infinite number
of seats (approximately corresponding to Chinese
word frequencies).
The Dirichlet Process model can be viewed in-
tuitively as a cache model (Goldwater et al, 2006).
Each word f
j
in the corpus is either retrieved from
a cache or generated anew given the previously ob-
served words f
?j
:
P (f
j
|f
?j
) =
N(f
j
)
+
?P
0
(f
j
)
N + ?
, (1)
whereN(f
j
) is the number of Chinese words f
j
in
the previous context. N is the total number of Chi-
nese words, P
0
is the base probability over words,
and ? influences the probability of introducing a
new word at each step and controls the size of the
lexicon. The probability of generating a word from
the cache increases as more instances of that word
are seen.
For the base distribution P
0
, which governs the
generation of new words, we use the following dis-
tribution (called the spelling model):
P
0
(f) = P (L)P
0
(f |L)
=
?
L
L!
e
??
u
L
(2)
where
1
u
is the number of characters in the docu-
ment, i.e. character vocabulary size, and L is the
number of Chinese characters of word f . We note
that this is a Poisson distribution on word length
and a unigram distribution on characters given the
length. We used ? = 2 and ? = 0.3 in our experi-
ments.
4 CWS Model for MT
As a solution to the problems with the conventional
approach to CWS mentioned in Section 1, we pro-
pose a generative model for CWS in Section 4.1,
and then extend the model to a more general but
deficient model, similar to a maximum entropy
model in which most features are derived from the
submodels of the generative model.
4.1 Generative Model
The generative model assume that a corpus of par-
allel sentences (c
1
K
,e
1
I
) is generated along with a
hidden sequence of Chinese words f
1
J
and a hid-
den word alignment b
1
I
for every sentence. The
alignment indicates the aligned Chinese word f
b
i
for each English word e
i
, where f
0
indicates a spe-
cial null word as in the IBM models.
1018
Without assuming any special form for the prob-
ability of a sentence pair along with hidden vari-
ables, we can factor it into a monolingual Chi-
nese sentence probability and a bilingual transla-
tion probability as follows:
Pr(c
1
K
, e
1
I
, f
1
J
, b
1
I
)
=Pr(c
K
1
, f
J
1
)Pr(e
I
1
, b
I
1
|f
J
1
)
=Pr(f
J
1
)?(f
1
J
, c
1
K
)Pr(e
I
1
, b
I
1
|f
J
1
),
where ?(f
J
1
, c
K
1
) is 1 if the characters of the se-
quence of words f
1
J
are c
1
K
, and to 0 other-
wise. We can drop the conditioning on c
1
K
in
Pr(e
I
1
, b
I
1
|f
J
1
), because the characters are deter-
ministic given the words.
The joint probability of the observations
(c
1
K
, e
I
1
) can be obtained by summing over all
possible values of the hidden variables f
J
1
and b
I
1
.
In Sections 4.1.1 and 4.1.2, we will describe
the modeling assumptions behind the monolingual
Chinese sentence model and the translation model,
respectively.
4.1.1 Monolingual Chinese sentence model
We use the Dirichlet Process unigram word
model introduced in section 3. In this model, the
parameters of a distribution over words G are first
drawn from the Dirichlet prior DP (?, P
0
). Words
are then independently generated according to G.
The probability of a sequence of Chinese words in
a sentence is thus:
Pr(f
J
1
) ?
J
?
j=1
P (f
j
|G) (3)
4.1.2 Translation model
We employ the Dirichlet Process inverse IBM
model 1 to generate English words and alignment
given the Chinese words. In this model, for every
Chinese word f (including the null word), a distri-
bution over English words G
f
is first drawn from
a Dirichlet Process prior DP (?, P
0
(e)), where
P
0
(e) we used the empirical distribution over En-
glish words in the parallel data. Then, given these
parameters, the probability of an English sentence
and alignment given a Chinese sentence (sequence
of words) is given by:
P (e
I
1
, b
I
1
|f
J
1
, G
f
) =
I
?
i=1
1
J + 1
P (e
i
|G
f
b
i
)
This is the same model form as inverse IBM
model 1, except we have placed Dirichlet Process
priors on the Chinese-word specific distributions
over English words.
2
2
f
b
i
is the Chinese word aligned to e
i
and G
f
b
i
is the
distribution over English words conditioned on the word f
b
i
.
Similarly, e
a
j
is the English word aligned to f
j
in the other di-
rection and G
e
a
j
is the distribution over Chinese words con-
ditioned on e
a
j
.
In practice, we observed that using a word-
alignment model in one direction is not sufficient.
We then added a factor to our model which in-
cludes word alignment in the other direction , i.e. a
Dirichlet Process IBM model 1. We ignore the de-
tailed description here, because the calculation is
the same as that of the inverse IBM model 1. Ac-
cording to this model, for every English word e (in-
cluding the null word), a distribution over Chinese
words G
e
is first drawn from a Dirichlet Process
prior DP (?, P
0
(f)). Here, for the base distribu-
tion P
0
(f) we used the same spelling model as for
the monolingual unigram Dirichlet Process prior.
The probability of a sequence of Chinese words
f
J
1
and a word alignment a
J
1
given a sequence of
English words e
I
1
is then:
P (f
J
1
, a
J
1
|e
I
1
, G
e
) =
J
?
j=1
1
I + 1
P (f
j
|G
e
a
j
)
4.2 Final Model
We put the monolingual model and the transla-
tion models in both directions together into a sin-
gle model, where each of the component models
is weighted by a scaling factor. This is similar to
a maximum entropy model. We fit the weights of
the sub-models on a development set by maximiz-
ing the BLEU score of the final translation.
P (c
K
1
, e
I
1
, f
J
1
, a
J
1
, b
I
1
) (4)
?
1
Z
P (f
J
1
)
?
1
? P (e
I
1
, b
I
1
|f
J
1
)
?
2
?P (f
J
1
, a
J
1
|e
I
1
)
?
3
,
where Z is the normalization factor.
In practice we do not re-normalize the proba-
bilities and our model is thus deficient because it
does not sum to 1 over valid observations. How-
ever, we found the model work very good in our
experiments. Similar deficient models have been
used very successfully before, for example, in the
IBM models 3?6 and in the unsupervised grammar
induction model of (Klein and Manning, 2002).
5 Gibbs Sampling Training
It is generally impossible to find the most likely
segmentation according to our Bayesian model us-
ing exact inference, because the hidden variables
do not allow exact computation of the integrals.
Nonetheless, it is possible to define algorithms us-
ing Markov chain Monte Carlo (MCMC) that pro-
duce a stream of samples from the posterior dis-
tribution of the hidden variables given the obser-
vations. We applied the Gibbs sampler (Geman
and Geman, 1984) ? one of the simplest MCMC
methods, in which transitions between states of the
1019
Figure 1: Case I, transition from a no-boundary to
a boundary state, f to f
?
f
??
.
Figure 2: Case II, transition from a boundary to a
no-boundary state, f
?
f
??
to f .
Markov chain result from sampling each compo-
nent of the state conditioned on the current value
of all other variables.
In our problem, the observations are D =
(d
1
, ..d
n
, .., d
N
), where d
n
=(c
K
1
, e
I
1
) indicates a
bilingual sentence pair, the hidden variables are the
word segmentations f
J
1
and the alignments in two
directions a
J
1
and b
I
1
.
To perform Gibbs sampling, we start with
an initial word segmentation and initial word
alignments, and iteratively re-sample the word-
segmentation and alignments according to our
model of Equation 4.
Note that for efficiency, we only allow limited
modifications to the initial word alignments. Thus
we only use models derived from IBM-1 (instead
of IBM-4) for comparing different word segmenta-
tions. On the other hand, re-sampling the segmen-
tation causes re-linking alignment points to parts
or groups of the original words.
Hence, we organize our sampling process
around possible word boundaries. For each char-
acter c
k
in each sentence, we consider two alterna-
tive segmentations: c
k
+
indicates the segmentation
where there is a boundary after c
k
and c
k
?
indi-
cates the segmentation where there is no boundary
after c
k
, keeping all other boundaries fixed. Let f
denote the single word spanning character c
k
when
there is no boundary after it, and f
?
,f
??
denote the
two adjacent words resulting if there is a bound-
ary: f
?
includes c
k
and f
??
starts just to the right,
with character c
k+1
. The introduction of f
?
and
f
??
leads to M new possible alignments in the E-
to-C direction b
+
k1
, . . . , b
+
kM
, such as in Figure 1.
Together with the boundary vs no-boundary state
at each character position, we re-sample a set of
alignment links between English words and any of
the Chinese words f ,f
?
, and f
??
, keeping all other
word alignments in the sentence pair fixed. (See
Figures 1 and 2.)
Table 1: General Algorithm of GS for CWS.
Input: D with an initial segmentation and alignments
Output: D with sampled segmentation and alignments
for n = 1 to
?
N
for k = 1 to K that c
k
? d
n
Create M+1 candidates, cba
+
k,m
and cba
?
k
, where
cba
+
k,m
: there is a word boundary after c
k
cba
?
k
: there is no word boundary after c
k
Compute probabilities
P (cba
+
k,m
|dh
nk
?
)
P (cba
?
k
|dh
nk
?
)
Sample boundary and relevant alignments
Update counts
Thus at each step in the Gibbs sampler, we con-
sider a set of alternatives for the boundary after
c
k
and relevant alignment links, keeping all other
hidden variables fixed. At each step, we need to
compute the probability of each of the alternatives,
given the fixed values of the other hidden variables.
We introduce some notation to make the presen-
tation easier. For every position k in sentence pair
n, we denote by dh
nk
?
the observations and hid-
den variables for all sentences other than sentence
n, and the observations and hidden variables in-
side sentence n, not involving character position
c
k
. The fixed variables inside the sentence are the
words not neighboring position k, and the align-
ments in both directions to these words.
In the process of sampling, we consider a set
of alternatives: segmentation c
k
+
along with the
product space of relevant alignments in both direc-
tions b
+
k1
, . . . , b
+
kM
, and a
+
k
, and segmentation c
?
k
along with relevant alignments b
k
?
and a
?
k
. For
brevity, we denote these alternatives by cba
k,m
+
and cba
k
?
.
We describe how we derive the set of alterna-
tives in section 5.2 and how we compute their
probabilities in section 5.1.
Table 1 shows schematically one iteration of
Gibbs sampling through the whole training corpus
of parallel sentences, where
?
N is the number of
parallel sentences.
5.1 Computing probabilities of alternatives
For the Gibbs sampling algorithm in Table 1, we
need to compute the probability of each alternative
segmentation/alignments, given the fixed values of
the rest of the data dh
nk
?
. The probability of the
hidden variables in the alternatives is proportional
to the joint probability of the hidden variables and
observations, and thus it is sufficient to compute
the probability of the latter. We compute these
probabilities using the Chinese restaurant process
sampling scheme for the Dirichlet Process, thus in-
1020
tegrating over all of the possible values of the dis-
tributions G, G
f
and G
e
.
Let cba
k
denote an alternative hypothesis in-
cluding boundary or no boundary at position k,
and relevant alignments to English words in both
directions of the one or two Chinese words result-
ing from the segmentation at k. The probability of
this configuration given by our model is:
P (cba
k
|dh
nk
?
) ? P
m
(cba
k
|dh
nk
?
)
?
1
(5)
?P
ef
(cba
k
|dh
nk
?
)
?
2
? P
fe
(cba
k
|dh
nk
?
)
?
3
,
where P
m
(cba
k
|dh
nk
?
) is the monolingual
word probability, and P
fe
(cba
k
|dh
nk
?
) and
P
ef
(cba
k
|dh
nk
?
) are the translation probabilities
in the two directions.
We now describe the computations of each of
the component probabilities.
5.1.1 Word model probability
The word model probability P
m
(cab
k
|dh
nk
?
)
in Equation 5 is derived from Equations 3 and 1:
There are two cases, depending on whether the
hypothesis specifies that there is a boundary after
character c
k
, in which case we need the probabili-
ties of the two resulting words f
?
, and f
??
, or there
is no boundary, in which case we need the proba-
bility of the single word f . (See the initial states in
Figures 1 and 2, respectively.)
Let N denote the total number of word tokens
in the rest of the corpus dh
nk
?
, and N(f) denote
the number of instances of word f in dh
nk
?
. The
probabilities in the two cases are
P
m
(c
+
k
|dh
nk
?
) ?
N(f
?
) + ?P
0
(f
?
)
N + ?
?
N(f
??
) + ?P
0
(f
??
)
N + ?
P
m
(c
?
k
|dh
nk
?
) ?
N(f) + ?P
0
(f)
N + ?
Here P
0
(f) is computed using Equation 2.
5.1.2 Translation model probability
The translation model probabilities depend on
whether or not there is a segmentation boundary
at c
k
and which English words are aligned to the
relevant Chinese words.
In the first case, assume that there is a word
boundary in cab
k
, and that English words {e
?
} are
aligned to f
?
and words {e
??
} are aligned to f
??
in
the E-to-C direction according to the alignment b
k
,
and that f
?
is aligned to e
?
?
and f
??
is aligned to e
?
??
in the C-to-E direction according to the alignment
a
k
(see the initial state in Figure 1). Here we over-
loaded notations and use b
k
and a
k
to indicate the
alignments of the relevant Chinese words at posi-
tion k to any English words. Let I denote the total
number of English words in the sentence, and J+1
denote the number of Chinese words according to
this segmentation. We also denote the total num-
ber of English words aligned to either f
?
or f
??
in
the E-to-C direction by P .
The translation model probability in the E-to-C
direction is thus:
P
ef
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
1
(J + 2
)
P
?
e
?
P (e
?
|f
?
, dh
nk
?
)
?
e
??
P (e
??
|f
??
, dh
nk
?
)
Here we compute P (e|f, dh
nk
?
) as:
P (e|f, dh
nk
?
) =
N(e, f) + ?P
0
(e)
N(f) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
The translation probability in the other direction
is similarly computed as:
P
fe
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
(
1
I + 1
)
2
P (f
?
|e
?
, dh
nk
?
)P (f
??
|e
?
, dh
nk
?
)
And P (f |e, dh
nk
?
) is computed as:
P (f |e, dh
nk
?
) =
N(f, e) + ?P
0
(f)
N(e) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
In the second case, if the hypothesis in evalua-
tion does not have a word boundary at position k,
the total number of Chinese words would be one
less, i.e. J instead of J +1 in the equations above,
and there would be a single set of English words
aligned to the word f in the E-to-C direction, and a
single word e
?
aligned to f in the C-to-E direction
(see the initial state in Figure 2. The probability of
this hypothesis is computed analogously.
5.2 Determining the set of alternative
hypotheses
As mentioned earlier, we consider alternative
alignments which deviate minimally from the cur-
rent alignments, and which satisfy the constraints
of the IBM model 1 in both directions. In order
to describe the set of alternatives, we consider two
cases, depending on whether there is a boundary at
the current character before sampling at position k.
Case 1. There was no boundary at c
k
in the previ-
ous state (see Figure 1).
1021
If there is no boundary at c
k
, there is a sin-
gle word f spanning that position. We denote by
{e} the set of English words aligned to f at that
state in the E-to-C direction and by e
?
the En-
glish word aligned to f in the C-to-E direction.
Since every state we consider satisfies the IBM
one-to-many constraints, there is exactly one En-
glish word aligned to f in the C-to-E direction and
the words {e} have no other words aligned to them
in the E-to-C direction.
In this case, we consider as hypothesis cba
k
?
the same segmentation and alignment as in the pre-
vious state. (see Table 1 for an overview of the
alternative hypotheses.)
We consider M different hypotheses which in-
clude a boundary at k in this case, where M de-
pends on the number of words {e} aligned to f
in the previous state. Because we are breaking
the word f into two words f
?
and f
??
by placing
a boundary at c
k
, we need to re-align the words
{e} to either f
?
or f
??
. Additionally we need to
align f
?
and f
??
to English words in the C-to-E
direction. The number of different hypotheses is
equal to 2
P
where P = |{e}|. These alternatives
arise by considering that each of the words in {e}
needs to align to either f
?
or f
??
, and there are 2
P
combinations of these alignments. For example, if
{e} = {e
1
, e
2
}, after splitting the word f there are
four possible alignments, illustrated in Figure 1:
I. (f
?
, e
1
) and (f
??
, e
2
), II. (f
?
, e
2
) and (f
??
, e
1
),
III. (f
?
, e
1
) and (f
?
, e
2
), IV. (f
??
, e
1
) and (f
??
, e
2
).
For the alignment a
k
in the C-to-E direction, we
consider only one option, in which both resulting
words f
?
and f
??
align to e
?
. These alternatives
form cba
k,m
+
in Table 1.
Case 2. There was a boundary at c
k
in the previous
state (see Figure 2).
In this case, for the hypotheses c
+
k
we consider
only one alternative, which is exactly the same as
the assignment of segmentation and alignments in
the previous state. Thus we have M = 1 in Table
1.
Let f
?
and f
??
denote the two words at position
k in the previous state, {e
?
} and {e
??
} denote the
sets of English words aligned to them in the E-to-C
direction, respectively, and e
?
?
and e
?
??
denote the
English words aligned to f
?
and f
??
in the C-to-E
direction.
We consider only one hypothesis cba
k
?
where
there is no boundary at c
k
. In this hypothesis, there
is a single word f = f
?
f
??
spanning position k,
and all words {e
?
} ? {e
??
} align to f in the E-to-
C direction. For the C-to-E direction we consider
the ?better? of the alignments (f, e
?
?
) and (f, e
??
?
)
where the better alignment is defined as the one
having higher probability according to the C-to-E
word translation probabilities.
Table 2: Complete Algorithm of Gibbs Sampler
for CWS including Alignment Models.
Input: D, F
0
Output: A
T
, F
T
for t = 1 to T
Run GIZA++ on (D,F
t?1
) to obtain A
t
Run GS on (D,F
t?1
, A
t
) to obtain F
t
5.3 Complete segmentation algorithm
So far, we have described how we re-sample word
segmentation and alignments according to our
model, starting from an initial segmentation and
alignments from GIZA++. Putting these pieces to-
gether, the algorithm is summarized in Table 1.
We found that we can further improve perfor-
mance by repeatedly aligning the corpus using
GIZA++, after deriving a new segmentation us-
ing our model. The complete algorithm which in-
cludes this step is shown in Table 2, where F
t
in-
dicates the word segmentation at iteration t and A
t
denotes the GIZA++ corpus alignment in both di-
rections. The GS re-segmentation step is done ac-
cording to the algorithm in Table 1.
Using this algorithm, we obtain a new segmen-
tation of the Chinese data and train the translation
models using this segmentation as in the baseline
MT system. To segment the test data for transla-
tion, we use a unigram model, trained with maxi-
mum likelihood estimation off of the final segmen-
tation of the training corpus F
T
.
6 Translation Experiments
We performed experiments using our models for
CWS on a large and a small data track. We evalu-
ated performance by measuring WER (word error
rate), PER (position-independent word error rate),
BLEU (Papineni et al, 2002) and TER (translation
error rate) (Snover et al, 2006) using multiple ref-
erences.
6.1 Translation Task: Large Track NIST
We first report the experiments using our mono-
lingual unigram Dirichlet Process model for word
segmentation on the NIST machine translation task
(NIST, 2005). Because of the computational re-
quirements, we only employed the monolingual
word model for this large data track, i.e. the fea-
ture weights were ?
1
= 1, ?
2
= 0, ?
3
= 0. There-
fore, no alignment information needs to be main-
tained in this case.
The bilingual training corpus is a superset of
corpora in the news domain collected from differ-
ent sources.
We took LDC (LDC, 2003) as a baseline CWS
method (Base). As shown in Table 3, the training
corpus in each language contains more than two
million sentences. There are 56 million Chinese
1022
Table 3: Statistics of corpora in task NIST.
Data Sents. Words[K] Voc.[K]
Cn. En. Cn. En.
Chars 2M 56M 49.5M 65.4 211
Base 39.2M 95.7
GS 40.5M 95.4
02 878 23.1 28.0 2.04 4.34
03 919 24.6 29.2 2.21 4.91
04 1788 49.8 60.7 2.61 6.71
05 1082 30.8 34.2 2.30 5.39
Table 4: Translation performance [% BLEU] with
the baseline(LDC) and GS method on NIST.
MT-eval LDC(Base) GS
2005 32.85 33.26
2002 34.32 34.36
2003 33.41 33.75
2004 33.74 34.06
characters. The LDC and GS word segmentation
methods generated 39.2 and 40.5 million running
words, respectively.
The scaling factors of the translation models de-
scribed in Section 2.2 were optimized on the devel-
opment corpus, MT-eval 05 with 1082 sentences.
The resulting systems were evaluated on the test
corpora MT-eval 02-04. For convenience, we only
list the statistics of the first English reference.
Starting from the baseline LDC output as ini-
tial word segmentation, we performed Gibbs sam-
pling (GS) of word segmentations using 30 itera-
tions over the Chinese training corpus.
Since BLEU is the official NIST measure of
translation performance, we show the translation
results measured in BLEU score only. As shown
in Table 4, on the development data MT-eval 05,
the BLEU score was improved by 0.4% absolute or
more than 1% relative using GS. Similarly, the ab-
solute BLEU scores are also improved on all other
test sets, in the range of 0.04% to 0.4%.
We can see that even a monolingual semi-
supervised word segmentation method can outper-
form a supervised one in MT, probably because the
training/test corpora contain many unknown words
and words have different frequencies in our MT
data from they do in the manually labeled CWS
data.
6.2 Translation Task: Small Track IWSLT
We evaluate our full model, using both monolin-
gual and bilingual information, on the IWSLT data.
As shown in Table 5, the Chinese training
corpus was segmented using the unigram seg-
menter (Base) described in Section 2.1 and our GS
method. Since the unigram segmenter performs
better in our experiments, we took it as the base-
line and the method for initialization in later ex-
periments. We see that the vocabulary size of the
Chinese training corpus was reduced more signif-
icantly by GS than by the baseline method, even
Table 5: Statistics of corpora in task IWSLT.
Test Sents. Words[K] Voc.
Cn. En. Cn. En.
Chars 42.9K 520 420 2780 9930
Base 394 8800
GS 398 6230
Dev2 500 3.74 3.82 1004 821
Dev3 506 4.01 3.90 980 820
Eval 489 3.39 3.72 904 810
Table 6: Translation performance with different
CWS methods on IWSLT[%].
Test Method WER PER BLEU TER
Dev2 Unigram (Base) 38.2 31.2 55.4 37.0
GS 36.8 30.0 56.6 35.5
Dev3 Unigram (Base) 33.5 27.5 60.4 32.1
GS 32.3 26.6 61.0 31.4
Eval Characters 49.3 41.8 35.4 47.5
LDC 46.2 40.0 39.2 45.0
ICT 45.9 40.4 40.1 44.9
Unigram (Base) 46.8 40.2 41.6 45.6
9-gram 46.9 40.4 40.1 45.4
GS 45.9 40.0 41.6 44.8
though they resulted in a similar number of run-
ning words. This shows that the distribution of
Chinese words is more concentrated when using
GS.
The parameter optimizations were performed on
the Dev2 data with 500 sentences, and evaluations
were done both on Dev3 and on Eval data, i.e. the
evaluation corpus of (IWSLT, 2007).
The model weights ? of GS from Section 5.1.2
were optimized using the Powell (Press et al,
2002) algorithm with respect to the BLEU score.
We obtained ?
1
= 1.4, ?
2
= 1 and ?
3
= 0.8 as
optimal values and T = 4 as the optimal number
of iterations of re-alignment with GIZA++.
For a fair comparison, we evaluated on various
CWS methods including translation on characters
, LDC (LDC, 2003), ICT (Zhang et al, 2003), uni-
gram, 9-gram and GS. Improvements using GS can
be seen in Table 6. Under all test sets and evalua-
tion criteria, GS outperforms the baseline method.
The absolute WER decreases with 1.2% on Dev3
and with 1.1% on Eval data over baseline.
We compared the translation outputs using GS
with the baseline method. On the Eval data, 196
sentences are different out of 489 lines, where 64
sentences from GS are better, 33 sentences are
worse, and the rests have similar translation qual-
ities. Table 7 shows two examples from the Eval
corpus. We list segmentations produced by the
baseline and GS methods, as well as the transla-
tions corresponding to these segmentations. The
GS method generates better translation results than
the baseline method in these cases.
1023
Table 7: Segmentation and translation outputs with
baseline and GS methods.
a) Baseline ? ?4 m?
do you have a ?
GS ??4m?
do you have a shorter way ?
REF is there a shorter route ?
b) Baseline >????
please show me the in .
GS >????
please show me the total price .
REF can you tell me the total amount ?
7 Conclusion and future work
We showed that it is possible to learn Chinese word
boundaries such that the translation performance
of Chinese-to-English MT systems is improved.
We presented a Bayesian generative model for
parallel Chinese-English sentences which uses
word segmentation and alignment as hidden vari-
ables, and incorporates both monolingual and
bilingual information to derive a segmentation
suitable for MT.
Starting with an initial word segmentation, our
method learns both new Chinese words and dis-
tributions for these words. In a large and a small
data environment, our method outperformed the
standard Chinese word segmentation approach in
terms of the Chinese to English translation quality.
In future work, we plan to enrich our monolingual
and bilingual models to better represent the true
distribution of the data.
8 Acknowledgments
Jia Xu conducted this research during her intern-
ship at Microsoft Research. This material is also
partly based upon work supported by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023.
References
Aldous, D. 1985. Exchangeability and related topics.
In
?
Ecole d??et?e de probabilit?es de Saint-Flour, XIII-
1983, pages 1?198, Springer, Berlin.
Andrew, G. 2006. A hybrid markov/semi-markov con-
ditional random field for sequence segmentation. In
Proceedings of EMNLP, Sydney, July.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
Gao, J., M. Li, A. Wu, and C. Huang. 2005. Chi-
nese word segmentation and named entity recogni-
tion: A pragmatic approach. Computational Lin-
guistics, 31(4).
Goldwater, S., T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of Coling/ACL, Sydney,
July.
IWSLT. 2007. International workshop on
spoken language translation home page.
http://www.slt.atr.jp/IWSLT2007.
Klein, D. and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of ACL, pages 128?135.
LDC. 2003. Linguistic data consor-
tium Chinese resource home page.
http://www.ldc.upenn.edu/Projects/Chinese.
NIST. 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
Och, F. J. and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302,
Philadelphia, PA, July.
Papineni, K. A., S. Roukos, T. W., and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, July.
Press, W. H., S. A. Teukolsky, W. T. Vetterling, and
B. P. Flannery. 2002. Numerical Recipes in C++.
Cambridge University Press, Cambridge, UK.
Snover, M., B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA, pages 223?231, Cambridge, MA, August.
Vogel, S., H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of COLING.
Xu, J., R. Zens, and H. Ney. 2004. Do we need
Chinese word segmentation for statistical machine
translation? In Proceedings of the SIGHAN Work-
shop on Chinese Language Learning, pages 122?
128, Barcelona, Spain, July.
Xu, J., E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proceedings of IWSLT, pages
141?147, Pittsburgh, PA, October.
Zens, R. and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceedings
of HLT/NAACL, Boston, MA, May.
Zhang, H., H. Yu, D. Xiong, and Q. Liu. 2003.
HHMM-based Chinese lexical analyzer ICTCLAS.
In Proceedings of the Second SIGHAN Workshop on
Chinese Language Learning, pages 184?187, July.
1024
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 524?532, Prague, June 2007. c?2007 Association for Computational Linguistics
A Systematic Comparison of Training Criteria
for Statistical Machine Translation
Richard Zens and Sas?a Hasan and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,hasan,ney}@cs.rwth-aachen.de
Abstract
We address the problem of training the free
parameters of a statistical machine transla-
tion system. We show significant improve-
ments over a state-of-the-art minimum er-
ror rate training baseline on a large Chinese-
English translation task. We present novel
training criteria based on maximum likeli-
hood estimation and expected loss compu-
tation. Additionally, we compare the maxi-
mum a-posteriori decision rule and the min-
imum Bayes risk decision rule. We show
that, not only from a theoretical point of
view but also in terms of translation qual-
ity, the minimum Bayes risk decision rule is
preferable.
1 Introduction
Once we specified the Bayes decision rule for statis-
tical machine translation, we have to address three
problems (Ney, 2001):
? the search problem, i.e. how to find the best
translation candidate among all possible target
language sentences;
? the modeling problem, i.e. how to structure
the dependencies of source and target language
sentences;
? the training problem, i.e. how to estimate the
free parameters of the models from the training
data.
Here, the main focus is on the training problem. We
will compare a variety of training criteria for statisti-
cal machine translation. In particular, we are consid-
ering criteria for the log-linear parameters or model
scaling factors. We will introduce new training cri-
teria based on maximum likelihood estimation and
expected loss computation. We will show that some
achieve significantly better results than the standard
minimum error rate training of (Och, 2003).
Additionally, we will compare two decision rules,
the common maximum a-posteriori (MAP) deci-
sion rule and the minimum Bayes risk (MBR) de-
cision rule (Kumar and Byrne, 2004). We will show
that the minimum Bayes risk decision rule results
in better translation quality than the maximum a-
posteriori decision rule for several training criteria.
The remaining part of this paper is structured
as follows: first, we will describe related work in
Sec. 2. Then, we will briefly review the baseline
system, Bayes decision rule for statistical machine
translation and automatic evaluation metrics for ma-
chine translation in Sec. 3 and Sec. 4, respectively.
The novel training criteria are described in Sec. 5
and Sec. 6. Experimental results are reported in
Sec. 7 and conclusions are given in Sec. 8.
2 Related Work
The most common modeling approach in statistical
machine translation is to use a log-linear combina-
tion of several sub-models (Och and Ney, 2002). In
(Och and Ney, 2002), the log-linear weights were
tuned to maximize the mutual information criterion
(MMI). The current state-of-the-art is to optimize
these parameters with respect to the final evaluation
criterion; this is the so-called minimum error rate
training (Och, 2003).
Minimum Bayes risk decoding for machine trans-
524
lation was introduced in (Kumar and Byrne, 2004).
It was shown that MBR outperforms MAP decoding
for different evaluation criteria. Further experiments
using MBR for Bleu were performed in (Venugopal
et al, 2005; Ehling et al, 2007). Here, we will
present additional evidence that MBR decoding is
preferable over MAP decoding.
Tillmann and Zhang (2006) describe a percep-
tron style algorithm for training millions of features.
Here, we focus on the comparison of different train-
ing criteria.
Shen et al (2004) compared different algorithms
for tuning the log-linear weights in a reranking
framework and achieved results comparable to the
standard minimum error rate training.
An annealed minimum risk approach is presented
in (Smith and Eisner, 2006) which outperforms both
maximum likelihood and minimum error rate train-
ing. The parameters are estimated iteratively using
an annealing technique that minimizes the risk of an
expected-BLEU approximation, which is similar to
the one presented in this paper.
3 Baseline System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Statistical decision the-
ory tells us that among all possible target language
sentences, we should choose the sentence which
minimizes the expected loss, also called Bayes risk:
e?I?1 = argmin
I,eI1
{
?
I?,e?I
?
1
Pr(e?I
?
1 |f
J
1 ) ? L(e
I
1, e
?I?
1 )
}
Here, L(eI1, e
?I?
1 ) denotes the loss function under
consideration. It measures the loss (or errors) of a
candidate translation eI1 assuming the correct trans-
lation is e?I
?
1 . In the following, we will call this de-
cision rule the MBR rule (Kumar and Byrne, 2004).
This decision rule is optimal in the sense that any
other decision rule will result (on average) in at least
as many errors as the MBR rule. Despite this, most
SMT systems do not use theMBR decision rule. The
most common approach is to use the maximum a-
posteriori (MAP) decision rule. Thus, we select the
hypothesis which maximizes the posterior probabil-
ity Pr(eI1|f
J
1 ):
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
}
This is equivalent to the MBR decision rule under
a 0-1 loss function:
L0?1(e
I
1, e
?I?
1 ) =
{
0 if eI1 = e
?I?
1
1 else
Hence, the MAP decision rule is optimal for the
sentence or string error rate. It is not necessarily
optimal for other evaluation metrics such as the Bleu
score. One reason for the popularity of the MAP
decision rule might be that, compared to the MBR
rule, its computation is simpler.
The posterior probability Pr(eI1|f
J
1 ) is modeled
directly using a log-linear combination of several
models (Och and Ney, 2002):
p?M1 (e
I
1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
I?,e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(1)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be easily
integrated into the overall system.
The denominator represents a normalization fac-
tor that depends only on the source sentence fJ1 .
Therefore, we can omit it in case of the MAP de-
cision rule during the search process and obtain:
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, f
J
1 )
}
Note that the denominator affects the results of the
MBR decision rule and, thus, cannot be omitted in
that case.
We use a state-of-the-art phrase-based translation
system similar to (Koehn, 2004; Mauser et al, 2006)
including the following models: an n-gram lan-
guage model, a phrase translation model and a word-
based lexicon model. The latter two models are used
for both directions: p(f |e) and p(e|f). Additionally,
we use a word penalty, phrase penalty and a distor-
tion penalty.
525
In the following, we will discuss the so-called
training problem (Ney, 2001): how do we train the
free parameters ?M1 of the model? The current
state-of-the-art is to use minimum error rate train-
ing (MERT) as described in (Och, 2003). The free
parameters are tuned to directly optimize the evalu-
ation criterion.
Except for the MERT, the training criteria that
we will consider are additive at the sentence-level.
Thus, the training problem for a development set
with S sentences can be formalized as:
??M1 = argmax
?M1
S?
s=1
F (?M1 , (e
I
1, f
J
1 )s) (2)
Here, F (?, ?) denotes the training criterion that we
would like to maximize and (eI1, f
J
1 )s denotes a sen-
tence pair in the development set. The optimization
is done using the Downhill Simplex algorithm from
the Numerical Recipes book (Press et al, 2002).
This is a general purpose optimization procedure
with the advantage that it does not require the deriva-
tive information. Before we will describe the details
of the different training criteria in Sec. 5 and 6, we
will discuss evaluation metrics in the following sec-
tion.
4 Evaluation Metrics
The automatic evaluation of machine translation is
currently an active research area. There exists a
variety of different metrics, e.g., word error rate,
position-independent word error rate, BLEU score
(Papineni et al, 2002), NIST score (Doddington,
2002), METEOR (Banerjee and Lavie, 2005), GTM
(Turian et al, 2003). Each of them has advantages
and shortcomings.
A popular metric for evaluating machine trans-
lation quality is the Bleu score (Papineni et al,
2002). It has certain shortcomings for compar-
ing different machine translation systems, especially
if comparing conceptually different systems, e.g.
phrase-based versus rule-based systems, as shown
in (Callison-Burch et al, 2006). On the other hand,
Callison-Burch concluded that the Bleu score is re-
liable for comparing variants of the same machine
translation system. As this is exactly what we will
need in our experiments and as Bleu is currently the
most popular metric, we have chosen it as our pri-
mary evaluation metric. Nevertheless, most of the
methods we will present can be easily adapted to
other automatic evaluation metrics.
In the following, we will briefly review the com-
putation of the Bleu score as some of the training
criteria are motivated by this. The Bleu score is a
combination of the geometric mean of n-gram pre-
cisions and a brevity penalty for too short translation
hypotheses. The Bleu score for a translation hypoth-
esis eI1 and a reference translation e?
I?
1 is computed as:
Bleu(eI1, e?
I?
1) = BP(I, I?) ?
4?
n=1
Precn(e
I
1, e?
I?
1)
1/4
with
BP(I, I?) =
{
1 if I ? I?
exp (1 ? I/I?) if I < I?
Precn(e
I
1, e?
I?
1) =
?
wn1
min{C(wn1 |e
I
1), C(w
n
1 |e?
I?
1)}
?
wn1
C(wn1 |e
I
1)
(3)
Here, C(wn1 |e
I
1) denotes the number of occur-
rences of an n-gram wn1 in a sentence e
I
1. The de-
nominators of the n-gram precisions evaluate to the
number of n-grams in the hypothesis, i.e. I ?n+1.
The n-gram counts for the Bleu score computa-
tion are usually collected over a whole document.
For our purposes, a sentence-level computation is
preferable. A problem with the sentence-level Bleu
score is that the score is zero if not at least one four-
gram matches. As we would like to avoid this prob-
lem, we use the smoothed sentence-level Bleu score
as suggested in (Lin and Och, 2004). Thus, we in-
crease the nominator and denominator of Precn(?, ?)
by one for n > 1. Note that we will use the
sentence-level Bleu score only during training. The
evaluation on the development and test sets will be
carried out using the standard Bleu score, i.e. at the
corpus level. As the MERT baseline does not require
the use of the sentence-level Bleu score, we use the
standard Bleu score for training the baseline system.
In the following, we will describe several crite-
ria for training the log-linear parameters ?M1 of our
model. For notational convenience, we assume that
there is just one reference translation. Nevertheless,
the methods can be easily adapted to the case of mul-
tiple references.
526
5 Maximum Likelihood
5.1 Sentence-Level Computation
A popular approach for training parameters is max-
imum likelihood estimation (MLE). Here, the goal
is to maximize the joint likelihood of the parameters
and the training data. For log-linear models, this re-
sults in a nice optimization criterion which is con-
vex and has a single optimum. It is equivalent to the
maximum mutual information (MMI) criterion. We
obtain the following training criterion:
FML?S(?
M
1 , (e
I
1, f
J
1 )) = log p?M1 (e
I
1|f
J
1 )
A problem that we often face in practice is that
the correct translation might not be among the can-
didates that our MT system produces. Therefore,
(Och and Ney, 2002; Och, 2003) defined the trans-
lation candidate with the minimum word-error rate
as pseudo reference translation. This has some bias
towards minimizing the word-error rate. Here, we
will use the translation candidate with the maximum
Bleu score as pseudo reference to bias the system
towards the Bleu score. However, as pointed out in
(Och, 2003), there is no reason to believe that the re-
sulting parameters are optimal with respect to trans-
lation quality measured with the Bleu score.
The goal of this sentence-level criterion is to dis-
criminate the single correct translation against all the
other ?incorrect? translations. This is problematic
as, even for human experts, it is very hard to define
a single best translation of a sentence. Furthermore,
the alternative target language sentences are not all
equally bad translations. Some of them might be
very close to the correct translation or even equiva-
lent whereas other sentences may have a completely
different meaning. The sentence-level MLE crite-
rion does not distinguish these cases and is therefore
a rather harsh training criterion.
5.2 N -gram Level Computation
As an alternative to the sentence-level MLE, we
performed experiments with an n-gram level MLE.
Here, we limit the order of the n-grams and assume
conditional independence among the n-gram prob-
abilities. We define the log-likelihood (LLH) of a
target language sentence eI1 given a source language
sentence fJ1 as:
FML?N (?
M
1 , (e
I
1, f
J
1 )) =
N?
n=1
?
wn1?e
I
1
log p?M1 (w
n
1 |f
J
1 )
Here, we use the n-gram posterior probability
p?M1 (w
n
1 |f
J
1 ) as defined in (Zens and Ney, 2006).
The n-gram posterior distribution is smoothed using
a uniform distribution over all possible n-grams.
p?M1 (w
n
1 |f
J
1 ) = ? ?
N?M1 (w
n
1 , f
J
1 )
?
w?n1
N?M1 (w
?n
1 , f
J
1 )
+ (1 ? ?) ?
1
V n
Here, V denotes the vocabulary size of the tar-
get language; thus, V n is the number of possi-
ble n-grams in the target language. We define
N?M1 (w
n
1 , f
J
1 ) as in (Zens and Ney, 2006):
N?M1 (w
n
1 , f
J
1 ) =
?
I,eI1
I?n+1?
i=1
p?M1 (e
I
1|f
J
1 )??(e
i+n?1
i , w
n
1 )
(4)
The sum over the target language sentences is lim-
ited to an N -best list, i.e. the N best translation
candidates according to the baseline model. In this
equation, we use the Kronecker function ?(?, ?), i.e.
the term ?(ei+n?1i , w
n
1 ) evaluates to one if and only
if the n-gram wn1 occurs in the target sentence e
I
1
starting at position i.
An advantage of the n-gram level computation
of the likelihood is that we do not have to define
pseudo-references as for the sentence-level MLE.
We can easily compute the likelihood for the human
reference translation. Furthermore, this criterion has
the desirable property that it takes partial correctness
into account, i.e. it is not as harsh as the sentence-
level criterion.
6 Expected Bleu Score
According to statistical decision theory, one should
maximize the expected gain (or equivalently mini-
mize the expected loss). For machine translation,
this means that we should optimize the expected
Bleu score, or any other preferred evaluation metric.
527
6.1 Sentence-Level Computation
The expected Bleu score for a given source sentence
fJ1 and a reference translation e?
I?
1 is defined as:
E[Bleu|e?I?1, f
J
1 ] =
?
eI1
Pr(eI1|f
J
1 ) ? Bleu(e
I
1, e?
I?
1)
Here, Pr(eI1|f
J
1 ) denotes the true probability dis-
tribution over the possible translations eI1 of the
given source sentence fJ1 . As this probability dis-
tribution is unknown, we approximate it using the
log-linear translation model p?M1 (e
I
1|f
J
1 ) from Eq. 1.
Furthermore, the computation of the expected Bleu
score involves a sum over all possible translations
eI1. This sum is approximated using an N -best list,
i.e. the N best translation hypotheses of the MT sys-
tem. Thus, the training criterion for the sentence-
level expected Bleu computation is:
FEB?S(?
M
1 , (e?
I?
1, f
J
1 )) =
?
eI1
p?M1 (e
I
1|f
J
1 )?Bleu(e
I
1, e?
I?
1)
An advantage of the sentence-level computation is
that it is straightforward to plug in alternative eval-
uation metrics instead of the Bleu score. Note that
the minimum error rate training (Och, 2003) uses
only the target sentence with the maximum posterior
probability whereas, here, the whole probability dis-
tribution is taken into account.
6.2 N -gram Level Computation
In this section, we describe a more fine grained com-
putation of the expected Bleu score by exploiting its
particular structure. Hence, this derivation is spe-
cific for the Bleu score but should be easily adapt-
able to other n-gram based metrics. We can rewrite
the expected Bleu score as:
E[Bleu|e?I?1, f
J
1 ] = E[BP|I? , f
J
1 ]
?
4?
n=1
E[Precn|e?I?1, f
J
1 ]
1/4
We assumed conditional independence between
the brevity penalty BP and the n-gram precisions
Precn. Note that although these independence as-
sumptions do not hold, the resulting parameters
might work well for translation. In fact, we will
show that this criterion is among the best perform-
ing ones in Sec. 7. This type of independence as-
sumption is typical within the naive Bayes classifier
framework. The resulting training criterion that we
will use in Eq. 2 is then:
FEB?N (?
M
1 , (e?
I?
1, f
J
1 )) = E?M1 [BP|I? , f
J
1 ]
?
4?
n=1
E?M1 [Precn|e?
I?
1, f
J
1 ]
1/4
We still have to define the estimators for the ex-
pected brevity penalty as well as the expected n-
gram precision:
E?M1 [BP|I? , f
J
1 ] =
?
I
BP(I, I?) ? p?M1 (I|f
J
1 )
E?M1 [Precn|e?
I?
1, f
J
1 ] = (5)
?
wn1
p?M1 (w
n
1 |f
J
1 )
?
c
min{c, C(wn1 |e?
I?
1)} ? p?M1 (c|w
n
1 , f
J
1 )
?
wn1
p?M1 (w
n
1 |f
J
1 )
?
c
c ? p?M1 (c|w
n
1 , f
J
1 )
Here, we use the sentence length posterior proba-
bility p?M1 (I|f
J
1 ) as defined in (Zens and Ney, 2006)
and the n-gram posterior probability p?M1 (w
n
1 |f
J
1 ) as
described in Sec. 5.2. Additionally, we predict the
number of occurrences c of an n-gram. This infor-
mation is necessary for the so-called clipping in the
Bleu score computation, i.e. the min operator in the
nominator of formulae Eq. 3 and Eq. 5. The denom-
inator of Eq. 5 is the expected number of n-grams in
the target sentence, whereas the nominator denotes
the expected number of correct n-grams.
To predict the number of occurrences within a
translation hypothesis, we use relative frequencies
smoothed with a Poisson distribution. The mean of
the Poisson distribution ?(wn1 , f
J
1 , ?
M
1 ) is chosen to
be the mean of the unsmoothed distribution.
p?M1 (c|w
n
1 , f
J
1 ) = ? ?
N?M1 (c, w
n
1 , f
J
1 )
N?M1 (w
n
1 , f
J
1 )
+ (1 ? ?) ?
?(wn1 , f
J
1 , ?
M
1 )
c ? e?c
c!
528
Table 1: Chinese-English TC-Star task: corpus
statistics.
Chinese English
Train Sentence pairs 8.3M
Running words 197M 238M
Vocabulary size 224K 389K
Dev Sentences 1 019 2 038
Running words 26K 51K
Eval 2006 Sentences 1 232 2 464
Running words 30K 62K
2007 Sentences 917 1 834
Running words 21K 45K
with
?(wn1 , f
J
1 , ?
M
1 ) =
?
c
c ?
N?M1 (c, w
n
1 , f
J
1 )
N?M1 (w
n
1 , f
J
1 )
Note that in case the mean ?(wn1 , f
J
1 , ?
M
1 ) is zero,
we do not need the distribution p?M1 (c|w
n
1 , f
J
1 ). The
smoothing parameters ? and ? are both set to 0.9.
7 Experimental Results
7.1 Task Description
We perform translation experiments on the Chinese-
English TC-Star task. This is a broadcast news
speech translation task used within the European
Union project TC-Star1. The bilingual training
data consists of virtually all publicly available LDC
Chinese-English corpora. The 6-gram language
model was trained on the English part of the bilin-
gual training data and additional monolingual En-
glish parts from the GigaWord corpus. We use the
modified Kneser-Ney discounting as implemented
in the SRILM toolkit (Stolcke, 2002).
Annual public evaluations are carried out for this
task within the TC-Star project. We will report re-
sults on manual transcriptions, i.e. the so-called ver-
batim condition, of the official evaluation test sets of
the years 2006 and 2007. There are two reference
translations available for the development and test
sets. The corpus statistics are shown in Table 1.
7.2 Translation Results
In Table 2, we present the translation results
for different training criteria for the development
1http://www.tc-star.org
set and the two blind test sets. The reported
case-sensitive Bleu scores are computed using
the mteval-v11b.pl2 tool using two reference
translations, i.e. BLEUr2n4c. Note that already the
baseline system (MERT-Bleu) would have achieved
the first rank in the official TC-Star evaluation 2006;
the best Bleu score in that evaluation was 16.1%.
The MBR hypotheses were generated using the
algorithm described in (Ehling et al, 2007) on a
10 000-best list.
On the development data, the MERT-Bleu
achieves the highest Bleu score. This seems reason-
able as it is the objective of this training criterion.
The maximum likelihood (MLE) criteria perform
somewhat worse under MAP decoding. Interest-
ingly, the MBR decoding can compensate this to
a large extent: all criteria achieve a Bleu score of
about 18.9% on the development set. The bene-
fits of MBR decoding become even more evident
on the two test sets. Here, the MAP results for the
sentence-level MLE criterion are rather poor com-
pared to the MERT-Bleu. Nevertheless, using MBR
decoding results in very similar Bleu scores for most
of the criteria on these two test sets. We can there-
fore support the claim of (Smith and Eisner, 2006)
that MBR tends to have better generalization capa-
bilities.
The n-gram level MLE criterion seems to perform
better than the sentence-level MLE criterion, espe-
cially on the test sets. The reasons might be that
there is no need for the use of pseudo references
as described in Sec. 5 and that partial correctness
is taken into account.
The best results are achieved using the expected
Bleu score criteria described in Sec. 6. Here, the sen-
tence level and n-gram level variants achieve more
or less the same results. The overall improvement
on the Eval?06 set is about 1.0% Bleu absolute for
MAP decoding and 0.9% for MBR decoding. On
the Eval?07 set, the improvements are even larger,
about 1.8% Bleu absolute for MAP and 1.1% Bleu
for MBR. All these improvements are statistically
significant at the 99% level using a pairwise signifi-
cance test3.
Given that currently the most popular approach is
to use MERT-Bleu MAP decoding, the overall im-
2http://www.nist.gov/speech/tests/mt/resources/scoring.htm
3The tool for computing the significance test was kindly pro-
vided by the National Research Council Canada.
529
Table 2: Translation results: Bleu scores [%] for the Chinese-English TC-Star task for various training
criteria (MERT: minimum error rate training; MLE: maximum likelihood estimation; E[Bleu]: expected
Bleu score) and the maximum a-posteriori (MAP) as well as the minimum Bayes risk (MBR) decision rule.
Development Eval?06 Eval?07
Decision Rule MAP MBR MAP MBR MAP MBR
Training Criterion MERT-Bleu (baseline) 19.5 19.4 16.7 17.2 22.2 23.0
MLE sentence-level 17.8 18.9 14.8 17.1 18.9 22.7
n-gram level 18.6 18.8 17.0 17.8 22.8 23.5
E[Bleu] sentence-level 19.1 18.9 17.5 18.1 23.5 24.1
n-gram level 18.6 18.8 17.7 17.6 24.0 24.0
provement is about 1.4% absolute for the Eval?06
set and 1.9% absolute on the Eval?07 set.
Note that the MBR decision rule almost always
outperforms theMAP decision rule. In the rare cases
where the MAP decision rule yields better results,
the difference in terms of Bleu score are small and
not statistically significant.
We also investigated the effect of the maximum
n-gram order for the n-gram level maximum like-
lihood estimation (MLE). The results are shown in
Figure 1. We observe an increase of the Bleu score
with increasing maximum n-gram order for the de-
velopment corpus. On the evaluation sets, however,
the maximum is achieved if the maximum n-gram
order is limited to four. This seems intuitive as the
Bleu score uses n-grams up to length four. However,
one should be careful here: the differences are rather
small, so it might be just statistical noise.
Some translation examples from the Eval?07 test
set are shown in Table 3 for different training criteria
under the maximum a-posteriori decision rule.
8 Conclusions
We have presented a systematic comparison of sev-
eral criteria for training the log-linear parameters of
a statistical machine translation system. Addition-
ally, we have compared the maximum a-posteriori
with the minimum Bayes risk decision rule.
We can conclude that the expected Bleu score
is not only a theoretically sound training criterion,
but also achieves the best results in terms of Bleu
score. The improvement over a state-of-the-art
MERT baseline is 1.3% Bleu absolute for the MAP
decision rule and 1.1% Bleu absolute for the MBR
decision rule for the large Chinese-English TC-Star
speech translation task.
1 2 3 4 5 6 7 8 9max. n-gram order
14
16
18
20
22
24
Bleu
 [%]
DevEval'06Eval'07
Figure 1: Effect of the maximum n-gram order on
the Bleu score for the n-gram level maximum like-
lihood estimation under the maximum a-posteriori
decision rule.
We presented two methods for computing the ex-
pected Bleu score: a sentence-level and an n-gram
level approach. Both yield similar results. We think
that the n-gram level computation has certain ad-
vantages: The n-gram posterior probabilities could
be computed from a word graph which would result
in more reliable estimates. Whether this pays off
in terms of translation quality is left open for future
work.
Another interesting result of our experiments is
that the MBR decision rule seems to be less affected
by sub-optimal parameter settings.
Although it is well-known that the MBR decision
rule is more appropriate than the MAP decision rule,
the latter is more popular in the SMT community
(and many other areas of natural language process-
ing). Our results show that it can be beneficial to
530
Table 3: Translation examples from the Eval?07 test set for different training criteria and the maximum a-
posteriori decision rule. (MERT: minimum error rate training, MLE-S: sentence-level maximum likelihood
estimation, E[Bleu]: sentence-level expected Bleu)
Criterion Translation
Reference 1 Saving Private Ryan ranks the third on the box office revenue list which is also a movie that is
possible to win an 1999 Oscar award
2 Saving Private Ryan ranked third in the box office income is likely to compete in the nineteen
ninety-nine Oscar Awards
MERT-Bleu Saving private Ryan in box office income is possible ranked third in 1999 Oscar a film
MLE-S Saving private Ryan box office revenue ranked third is possible in 1999 Oscar a film
E[Bleu]-S Saving private Ryan ranked third in the box office income is also likely to run for the 1999
Academy Awards a film
Reference 1 The following problem is whether people in countries like China and Japan and other countries
will choose Euros rather than US dollars in international business activities in the future
2 The next question is whether China or Japan or other countries will choose to use Euros instead
of US dollars when they conduct international business in the future
MERT-Bleu The next question is in China or Japan international business activities in the future they will not
use the Euro dollar
MLE-S The next question was either in China or Japan international business activities in the future they
will adopt the Euro instead of the dollar
E[Bleu]-S The next question was in China or Japan in the international business activities in the future they
will adopt the Euro instead of the US dollar
Reference 1 The Chairman of the European Commission Jacques Santer pointed out in this September that the
financial crisis that happened in Russia has not affected people?s confidence in adopting the Euro
2 European Commission President Jacques Santer pointed out in September this year that
Russia?s financial crisis did not shake people?s confidence for planning the use of the Euro
MERT-Bleu President of the European Commission Jacques Santer on September this year that the Russian
financial crisis has not shaken people ?s confidence in the introduction of the Euro
MLE-S President of the European Commission Jacques Santer September that the Russian financial crisis
has not affected people ?s confidence in the introduction of the Euro
E[Bleu]-S President of the European Commission Jacques Santer pointed out that Russia ?s financial crisis
last September has not shaken people ?s confidence in the introduction of the Euro
Reference 1 After many years of friction between Dutch and French speaking Belgians all of them now hope
to emphasize their European identities
2 After years of friction between Belgium?s Dutch-speaking and French-speaking people they now
all wish to emphasize their European identity
MERT-Bleu Belgium?s Dutch-speaking and French-speaking after many years of civil strife emphasized that
they now hope that Europeans
MLE-S Belgium?s Dutch-speaking and francophone after years of civil strife that they now hope that
Europeans
E[Bleu]-S Belgium?s Dutch-speaking and French-speaking after many years of civil strife it is now want
to emphasize their European identity
531
use the MBR decision rule. On the other hand, the
computation of the MBR hypotheses is more time
consuming. Therefore, it would be desirable to have
a more efficient algorithm for computing the MBR
hypotheses.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An au-
tomatic metric for MT evaluation with improved correlation
with human judgments. In Proc. Workshop on Intrinsic and
Extrinsic Evaluation Measures for MT and/or Summariza-
tion at the 43th Annual Meeting of the Association of Com-
putational Linguistics (ACL), pages 65?72, Ann Arbor, MI,
June.
Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J.
Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L.
Mercer, and Paul S. Roossin. 1990. A statistical approach to
machine translation. Computational Linguistics, 16(2):79?
85, June.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of BLEU in machine trans-
lation research. In Proc. 11th Conf. of the Europ. Chapter
of the Assoc. for Computational Linguistics (EACL), pages
249?256, Trento, Italy, April.
George Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statistics. In
Proc. ARPA Workshop on Human Language Technology.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007. Mini-
mum Bayes risk decoding for BLEU. In Proc. 45th Annual
Meeting of the Assoc. for Computational Linguistics (ACL):
Poster Session, Prague, Czech Republic, June.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Proc.
6th Conf. of the Assoc. for Machine Translation in the Amer-
icas (AMTA), pages 115?124, Washington DC, Septem-
ber/October.
Shankar Kumar and William Byrne. 2004. Minimum Bayes-
risk decoding for statistical machine translation. In Proc.
Human Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual Meet-
ing (HLT-NAACL), pages 169?176, Boston, MA, May.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method
for evaluating automatic evaluation metrics for machine
translation. In Proc. COLING ?04: The 20th Int. Conf.
on Computational Linguistics, pages 501?507, Geneva,
Switzerland, August.
Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a Hasan,
and Hermann Ney. 2006. The RWTH statistical machine
translation system for the IWSLT 2006 evaluation. In Proc.
Int. Workshop on Spoken Language Translation (IWSLT),
pages 103?110, Kyoto, Japan, November.
Hermann Ney. 2001. Stochastic modelling: from pattern
classification to language translation. In Proc. 39th Annual
Meeting of the Assoc. for Computational Linguistics (ACL):
Workshop on Data-Driven Machine Translation, pages 1?5,
Morristown, NJ, July.
Franz Josef Och and Hermann Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical machine
translation. In Proc. 40th Annual Meeting of the Assoc. for
Computational Linguistics (ACL), pages 295?302, Philadel-
phia, PA, July.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. 41st Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. 40th Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetterling, and
Brian P. Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Dis-
criminative reranking for machine translation. In Proc. Hu-
man Language Technology Conf. / North American Chapter
of the Assoc. for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 177?184, Boston, MA, May.
David A. Smith and Jason Eisner. 2006. Minimum risk anneal-
ing for training log-linear models. In Proc. 21st Int. Conf.
on Computational Linguistics and 44th Annual Meeting of
the Assoc. for Computational Linguistics (COLING/ACL):
Poster Session, pages 787?794, Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Speech and Language
Processing (ICSLP), volume 2, pages 901?904, Denver, CO,
September.
Christoph Tillmann and Tong Zhang. 2006. A discriminative
global training algorithm for statistical MT. In Proc. 21st
Int. Conf. on Computational Linguistics and 44th Annual
Meeting of the Assoc. for Computational Linguistics (COL-
ING/ACL), pages 721?728, Sydney, Australia, July.
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Eval-
uation of machine translation and its evaluation. Technical
Report Proteus technical report 03-005, Computer Science
Department, New York University.
Ashish Venugopal, Andreas Zollmann, and Alex Waibel. 2005.
Training and evaluating error minimization rules for statis-
tical machine translation. In Proc. 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL): Workshop
on Building and Using Parallel Texts: Data-Driven Machine
Translation and Beyond, pages 208?215, Ann Arbor, MI,
June.
Richard Zens and Hermann Ney. 2006. N -gram posterior prob-
abilities for statistical machine translation. In Proc. Human
Language Technology Conf. / North American Chapter of the
Assoc. for Computational Linguistics Annual Meeting (HLT-
NAACL): Proc. Workshop on Statistical Machine Transla-
tion, pages 72?77, New York City, NY, June.
532
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 372?381,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Triplet Lexicon Models for Statistical Machine Translation
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, Jesu?s Andre?s-Ferrer??
Human Language Technology and Pattern Recognition, RWTH Aachen University, Germany
?Universidad Polite?cnica de Valencia, Dept. Sist. Informa?ticos y Computacio?n
{hasan,ganitkevitch,ney}@cs.rwth-aachen.de jandres@dsic.upv.es
Abstract
This paper describes a lexical trigger model
for statistical machine translation. We present
various methods using triplets incorporating
long-distance dependencies that can go be-
yond the local context of phrases or n-gram
based language models. We evaluate the pre-
sented methods on two translation tasks in a
reranking framework and compare it to the re-
lated IBM model 1. We show slightly im-
proved translation quality in terms of BLEU
and TER and address various constraints to
speed up the training based on Expectation-
Maximization and to lower the overall num-
ber of triplets without loss in translation per-
formance.
1 Introduction
Data-driven methods have been applied very suc-
cessfully within the machine translation domain
since the early 90s. Starting from single-word-
based translation approaches, significant improve-
ments have been made through advances in mod-
eling, availability of larger corpora and more pow-
erful computers. Thus, substantial progress made
in the past enables today?s MT systems to achieve
acceptable results in terms of translation quality for
specific language pairs such as Arabic-English. If
sufficient amounts of parallel data are available, sta-
tistical MT systems can be trained on millions of
?The work was carried out while the author was at the Hu-
man Language Technology and Pattern Recognition group at
RWTH Aachen University and partly supported by the Valen-
cian Conselleria d?Empresa, Universitat i Cie`ncia under grants
CTBPRA/2005/ and BEFPI/2007/014.
target
source
e e?
f
Figure 1: Triplet example: a source word f is triggered
by two target words e and e?, where one of the words is
within and the other outside the considered phrase pair
(indicated by the dashed line).
sentence pairs and use an extended level of context
based on bilingual groups of words which denote
the building blocks of state-of-the-art phrase-based
SMT systems.
Due to data sparseness, statistical models are of-
ten trained on local context only. Language mod-
els are derived from n-grams with n ? 5 and bilin-
gual phrase pairs are extracted with lengths up to
10 words on the target side. This captures the local
dependencies of the data in detail and is responsi-
ble for the success of data-driven phrase-based ap-
proaches.
In this work, we will introduce a new statistical
model based on lexicalized triplets (f, e, e?) which
we will also refer to as cross-lingual triggers of
the form (e, e? ? f). This can be understood
as two words in one language triggering one word
in another language. These triplets, modeled by
p(f |e, e?), are closely related to lexical translation
probabilities based on the IBM model 1, i.e. p(f |e).
Several constraints and setups will be described later
on in more detail, but as an introduction one can
372
think of the following interpretation which is de-
picted in Figure 1: Using a phrase-based MT ap-
proach, a source word f is triggered by its trans-
lation e which is part of the phrase being consid-
ered, whereas another target word e? outside this
phrase serves as an additional trigger in order to al-
low for more fine-grained distinction of a specific
word sense. Thus, this cross-lingual trigger model
can be seen as a combination of a lexicon model (i.e.
f and e) and a model similar to monolingual long-
range (i.e. distant bigram) trigger models (i.e. e and
e?, although these dependencies are reflected indi-
rectly via e? ? f ) which uses both local (in-phrase)
and global (in-sentence) information for the scoring.
The motivation behind this approach is to get non-
local information outside the current context (i.e. the
currently considered bilingual phrase pair) into the
translation process. The triplets are trained via the
EM algorithm, as will be shown later in more detail.
2 Related Work
In the past, a significant number of methods has
been presented that try to capture long-distance de-
pendencies, i.e. use dependencies in the data that
reach beyond the local context of n-grams or phrase
pairs. In language modeling, monolingual trigger
approaches have been presented (Rosenfeld, 1996;
Tillmann and Ney, 1997) as well as syntactical meth-
ods that parse the input and model long-range de-
pendencies on the syntactic level by conditioning on
the predecessing words and their corresponding par-
ent nodes (Chelba and Jelinek, 2000; Roark, 2001).
The latter approach was shown to reduce perplex-
ities and improve the WER in speech recognition
systems. One drawback is that the parsing process
might slow down the system significantly and the
approach is complicated to be integrated directly in
the search process. Thus, the effect is often shown
offline in reranking experiments using n-best lists.
One of the simplest models that can be seen in
the context of lexical triggers is the IBM model 1
(Brown et al, 1993) which captures lexical depen-
dencies between source and target words. It can be
seen as a lexicon containing correspondents of trans-
lations of source and target words in a very broad
sense since the pairs are trained on the full sentence
level. The model presented in this work is very close
to the initial IBM model 1 and can be seen as taking
another word into the conditioning part, i.e. the trig-
gering items.1 Furthermore, since the second trig-
ger can come from any part of the sentence, we also
have a link to long-range monolingual triggers as
presented above.
A long-range trigram model is presented in
(Della Pietra et al, 1994) where it is shown how to
derive a probabilistic link grammar in order to cap-
ture long-range dependencies in English using the
EM algorithm. Expectation-Maximization is used
in the presented triplet model as well which is de-
scribed in more detail in Section 3. Instead of deriv-
ing a grammar automatically (based on POS tags of
the words), we rely on a fully lexicalized approach,
i.e. the training is taking place at the word level.
Related work in the context of fine-tuning lan-
guage models by using cross-lingual lexical triggers
is presented in (Kim and Khudanpur, 2003). The
authors show how to use cross-lingual triggers on a
document level in order to extract translation lexi-
cons and domain-specific language models using a
mutual information criterion.
Recently, word-sense disambiguation (WSD)
methods have been shown to improve translation
quality (Chan et al, 2007; Carpuat and Wu, 2007).
Chan et al (2007) use an SVM based classifier for
disambiguating word senses which are directly in-
corporated in the decoder through additional fea-
tures that are part of the log-linear combination of
models. They use local collocations based on sur-
rounding words left and right of an ambiguous word
including the corresponding parts-of-speech. Al-
though no long-range dependencies are modeled, the
approach yields an improvement of +0.6% BLEU on
the NIST Chinese-English task. In Carpuat and Wu
(2007), another state-of-the-art WSD engine (a com-
bination of naive Bayes, maximum entropy, boost-
ing and Kernel PCA models) is used to dynamically
determine the score of a phrase pair under consid-
eration and, thus, let the phrase selection adapt to
the context of the sentence. Although the baseline is
significantly lower than in the work of Chan et al,
this setup reaches an improvement of 0.5% BLEU
on the NIST CE task and up to 1.1% BLEU on the
1Thus, instead of p(f |e) we model p(f |e, e?) with different
additional constraints as explained later on.
373
IWSLT?06 test sets.
The work in this paper tries to complement the
WSD approaches by using long-range dependen-
cies. If triggers from a local context determine dif-
ferent lexical choice for the word being triggered,
the setting is comparable to the mentioned WSD
approaches (although local dependencies might al-
ready be reflected sufficiently in the phrase models).
A distant second trigger, however, might have a ben-
eficial effect for specific languages, e.g. by captur-
ing word splits (as it is the case in German for verbs
with separable prefixes) or, as already mentioned, al-
lowing for a more fine-grained lexical choice of the
word being triggered, namely based on another word
which is not part of the current local, i.e. phrasal,
context.
The basic idea of triplets of the form (e, f ? ? f),
called multi-word extensions, is also mentioned in
(Tillmann, 2001) but neither evaluated nor investi-
gated in further detail.
In the following sections, we will describe the
model proposed in this work. In Section 3, a de-
tailed introduction is given, as well as the EM train-
ing and variations of the model. The different set-
tings will be evaluated in Section 4, where we show
experiments on the IWSLT Chinese-English and
TC-STAR EPPS English-Spanish/Spanish-English
tracks. A discussion of the results and further ex-
amples are given in Section 5. Final remarks and
future work are addressed in Section 6.
3 Model
As an extension to commonly used lexical word
pair probabilities p(f |e) as introduced in (Brown
et al, 1993), we define our model to operate on
word triplets. A triplet (f, e, e?) is assigned a value
?(f |e, e?) ? 0 with the constraint such that
?e, e? :
?
f
?(f |e, e?) = 1.
Throughout this paper, e and e? will be referred to as
the first and the second trigger, respectively. In view
of its triggers f will be termed the effect.
For a given bilingual sentence pair (fJ1 , e
I
1), the
probability of a source word fj given the whole tar-
get sentence eI1 for the triplet model is defined as:
pall (fj |e
I
1) =
1
Z
I?
i=1
I?
k=i+1
?(fj |ei, ek), (1)
where Z denotes a normalization factor based on the
corresponding target sentence length, i.e.
Z =
I(I ? 1)
2
. (2)
The introduction of a second trigger (i.e. ek in
Eq. 1) enables the model to combine local (i.e. word
or phrase level) and global (i.e. sentence level) infor-
mation.
In the following, we will describe the training pro-
cedure of the model via maximum likelihood esti-
mation for the unconstrained case.
3.1 Training
The goal of the training procedure is to maximize the
log-likelihood Fall of the triplet model for a given
bilingual training corpus {(fJ1 , e
I
1)}
N
1 consisting of
N sentence pairs:
Fall :=
N?
n=1
Jn?
j=1
log pall (fj |e
In
1 ),
where Jn and In are the lengths of the nth source
and target sentences, respectively. As there is no
closed form solution for the maximum likelihood es-
timate, we resort to iterative training via the EM al-
gorithm (Dempster et al, 1977). We define the aux-
iliary function Q(?; ??) based on Fall where ?? is the
new estimate within an iteration which is to be de-
rived from the current estimate ?. Here, ? stands for
the entire set of model parameters to be estimated,
i.e. the set of all {?(f |e, e?)}. Thus, we obtain
Q
(
{?(f |e, e?)}; {??(f |e, e?)}
)
=
N?
n=1
Jn?
j=1
In?
i=1
In?
k=i+1
[
Z?1n ?(fj |ei, ek)
pall (fj |e
In
1 )
? (3)
log
(
Z?1n ??(fj |ei, ek)
)
]
,
where Zn is defined as in Eq. 2. Using the
method of Lagrangian multipliers for the normaliza-
tion constraint, we take the derivative with respect to
374
??(f |e, e?) and obtain:
??(f |e, e?) =
A(f, e, e?)
?
f ? A(f
?, e, e?)
(4)
where A(f, e, e?) is a relative weight accumulator
over the parallel corpus:
A(f, e, e?) =
N?
n=1
Jn?
j=1
?(f, fj)
Z?1n ?(f |e, e
?)
pall (fj |e
In
1 )
Cn(e, e
?) (5)
and
Cn(e, e
?) =
In?
i=1
In?
k=i+1
?(e, ei)?(e
?, ek).
The function ?(?, ?) denotes the Kronecker delta.
The resulting training procedure is analogous to the
one presented in (Brown et al, 1993) and (Tillmann
and Ney, 1997).
The next section presents variants of the ba-
sic unconstrained model by putting restrictions on
the valid regions of triggers (in-phrase vs. out-of-
phrase) and using alignments obtained from either
GIZA++ training or forced alignments in order to
reduce the model size and to incorporate knowledge
already obtained in previous training steps.
3.2 Model variations
Based on the unconstrained triplet model presented
in Section 3, we introduce additional constraints,
namely the phrase-bounded and the path-aligned
triplet model in the following. The former reduces
the number of possible triplets by posing constraints
on the position of where valid triggers may originate
from. In order to obtain phrase boundaries on the
training data, we use forced alignments, i.e. translate
the whole training data by constraining the transla-
tion hypotheses to the target sentences of the training
corpus.
Path-aligned triplets use an alignment constraint
from the word alignments that are trained with
GIZA++. Here, we restrict the first trigger pair (f, e)
to the alignment path as based on the alignment ma-
trix produced by IBM model 4.
These variants require information in addition to
the bilingual sentence pair (fJ1 , e
I
1), namely a corre-
sponding phrase segmentation ? = {piij} with
piij =
{
1 ? a phrase pair that covers ei and fj
0 otherwise
for the phrase-bounded method and, similarly, a
word alignment A = {aij} where
aij =
{
1 if ei is aligned to fj
0 otherwise
.
3.2.1 Phrase-bounded triplets
The phrase-bounded triplet model (referred to as
pphr in the following), restricts the first trigger e to
the same phrase as f , whereas the second trigger e?
is set outside the phrase, resulting in
pphr (fj |e
I
1,?) =
1
Zj
I?
i=1
I?
k=1
piij(1 ? pikj)?(fj |ei, ek). (6)
3.2.2 Path-aligned triplet
The path-aligned triplet model (denoted by palign
in the following), restricts the scope of e to words
aligned to f by A, yielding:
palign(fj |e
I
1, A) =
1
Zj
I?
i=1
I?
k=1
aij?(fj |ei, ek) (7)
where the Zj are, again, the appropriate normaliza-
tion terms.
Also, to account for non-aligned words (analo-
gously to the IBM models), the empty word e0 is
considered in all three model variations. We show
the effect of the empty word in the experiments (Sec-
tion 4). Furthermore, we can train the presented
models in the inverse direction, i.e. p(e|f, f ?), and
combine the two directions in the rescoring frame-
work. The next section presents a set of experiments
that evaluate the performance of the presented triplet
model and its variations.
4 Experiments
In this section, we describe the system setup used in
this work, including the translation tasks and the cor-
responding training corpora. The experiments are
based on an n-best list reranking framework.
375
4.1 System
The experiments were carried out using a state-of-
the-art phrase-based SMT system. The dynamic
programming beam search decoder uses several
models during decoding by combining them log-
linearly. We incorporate phrase translation and word
lexicon models in both directions, a language model,
as well as phrase and word penalties including a
distortion model for the reordering. While gener-
ating the hypotheses, a word graph is created which
compactly represents the most likely translation hy-
potheses. Out of this word graph, we generate n-
best lists and use them to test the different setups as
described in Section 3.
In the experiments, we use 10,000-best lists con-
taining unique translation hypotheses, i.e. duplicates
generated due to different phrase segmentations are
reduced to one single entry. The advantage of this
reranking approach is that we can directly test the
obtained models since we already have fully gener-
ated translations. Thus, we can apply the triplet lex-
icon model based on p(f |e, e?) and its inverse coun-
terpart p(e|f, f ?) directly. During decoding, since e?
could be from anywhere outside the current phrase,
i.e. even from a part which lies beyond the current
context which has not yet been generated, we would
have to apply additional constraints during training
(i.e. make further restrictions such as i? < i for a
trigger pair (ei, ei?)).
Optimization of the model scaling factors is car-
ried out using minimum error rate training (MERT)
on the development sets. The optimization criterion
is 100-BLEU since we want to maximize the BLEU
score.
4.2 Tasks
4.2.1 IWSLT
For the first part of the experiments, we use
the corpora that were released for the IWSLT?07
evaluation campaign. The training corpus con-
sists of approximately 43K Chinese-English sen-
tence pairs, mainly coming from the BTEC cor-
pus (Basic Travel Expression Corpus). This is a
multilingual speech corpus which contains tourism-
related material, such as transcribed conversations
about making reservations, asking for directions or
conversations as taking place in restaurants. For the
experiments, we use the clean data track, i.e. tran-
scriptions of read speech. As the development set
which is used for tuning the parameters of the base-
line system and the reranking framework, we use
the IWSLT?04 evaluation set (500 sentence pairs).
The two blind test sets which are used to evaluate
the final performance of the models are the official
evaluation sets from IWSLT?05 (506 sentences) and
IWSLT?07 (489 sentences).
The average sentence length of the training cor-
pus is 10 words. Thus, the task is somewhat lim-
ited and very domain-specific. One of the advan-
tages of this setting is that preliminary experiments
can be carried out quickly in order to analyze the ef-
fects of the different models in detail. This and the
small vocabulary size (12K entries) makes the cor-
pus ideal for first ?rapid application development?-
style setups without having to care about possible
constraints due to memory requirements or CPU
time restrictions.
4.2.2 EPPS
Furthermore, additional experiments are based on
the EPPS corpus (European Parliament Plenary Ses-
sions) as used within the FTE (Final Text Edition)
track of the TC-STAR evaluations. The corpus con-
tains speeches held by politicians at plenary sessions
of the European Parliament that have been tran-
scribed, ?corrected? to make up valid written texts
and translated into several target languages. The lan-
guage pairs considered in the experiments here are
Spanish-English and English-Spanish.
The training corpus consists of roughly 1.3M sen-
tence pairs with 35.5M running words on the En-
glish side. The vocabulary sizes are considerably
larger than for the IWSLT task, namely around 170K
on the target side. As development set, we use
the development data issued for the 2006 evaluation
(1122 sentences), whereas the two blind test sets are
the official evaluation data from 2006 (TC-Star?06,
1117 sentences) and 2007 (TC-Star?07, 1130 sen-
tences).
4.3 Results
4.3.1 IWSLT experiments
One of the first questions that arises is how many
EM iterations should be carried out during training
of the triplet model. Since the IWSLT task is small,
376
 56.6
 56.8
 57
 57.2
 57.4
 57.6
 0  10  20  30  40  50 34.6
 34.8
 35
 35.2
 35.4
 35.6
BL
EU
 sc
ore
TE
R s
cor
e
EM iterations
IWSLT?04 BLEUIWSLT?04 TER
Figure 2: Effect of EM iterations on IWSLT?04, left axis
shows BLEU (higher numbers better), right axis (dashed
graph) shows TER score (lower numbers better).
IWSLT?04 IWSLT?05
BLEU TER BLEU TER
baseline 56.7 35.49 61.1 30.59
pall(e|f, f ?) 57.1 35.03 61.3 30.55
w/ singletons 57.3 35.04 61.3 30.61
w/ empties 57.3 35.00 61.2 30.65
+ pall(f |e, e?) 57.5 34.69 61.7 30.24
Table 1: Different setups showing the effect of singletons
and empty words for IWSLT CE IWSLT?04 (dev) and
IWSLT?05 (test) sets, pall triplets, 20 EM iterations.
we can quickly run the experiments on a full uncon-
strained triplet model without any cutoff or further
constraints. Figure 2 shows the rescoring perfor-
mance for different numbers of EM iterations. The
first 10 iterations significantly improve the triplet
model performance for the IWSLT task. After that,
there are no big changes. The performance even de-
grades a little bit after 30 iterations. For the IWSLT
task, we therefore set a fixed number of 20 EM iter-
ations for the following experiments since it shows a
good performance in terms of both BLEU and TER
score. The oracle TER scores of the 10k-best lists
are 14.18% for IWSLT?04, 11.36% for IWSLT?05
and 18.85% for IWSLT?07, respectively.
The next chain of experiments on the IWSLT task
investigates the impact of changes to the setup of
training an unconstrained triplet model, such as the
addition of the empty word and the inclusion of sin-
gletons (i.e. triplets that were only seen once in the
IWSLT?05 IWSLT?07
BLEU TER BLEU TER
baseline 61.1 30.59 38.9 45.60
IBM model 1 61.5 30.29 39.4 45.31
trip fe+ef pall 61.7 30.24 39.7 45.24
trip fe+ef pphr 61.5 30.32 39.1 45.36
trip fe+ef palign 61.2 30.60 39.7 45.02
Table 2: Comparison of triplet variants on IWSLT CE test
sets, 20 EM iterations, with singletons and empty words.
training data). This might show the importance of
rare events in order to derive strategies when mov-
ing to larger tasks where it is not feasible to train all
possible triplets, such as e.g. on the EPPS task (as
shown later) or the Chinese-English NIST task. The
results for the unconstrained model are shown in Ta-
ble 1, beginning with a full triplet model in reverse
direction, pall (e|f, f ?), that contains no singletons
and no empty words for the triggering side. In this
setting, singletons seem to help on dev but there is no
clear improvement on one of the test sets, whereas
empty words do not make a significant difference but
can be used since they do not harm either. The base-
line can be improved by +0.6% BLEU and around
-0.5% in TER on the IWSLT?04 set. For the vari-
ous setups, there are no big differences in the TER
score which might be an effect of optimization on
BLEU. Therefore, for further experiments using the
constraints from Section 3.2, we use both singletons
and empty words as the default.
Adding the other direction p(f |e, e?) results in an-
other increase, with a total of +0.8% BLEU and
-0.8% TER, which shows that the combination of
both directions helps overall translation quality. The
results on the two test sets are shown in Table 2.
As can be seen, we arrive at similar improvements,
namely +0.6% BLEU and -0.3% TER on IWSLT?05
and +0.8% BLEU and -0.4% TER on IWSLT?07, re-
spectively. The constrained models, i.e. the phrase-
bounded (pphr ) and path-aligned (palign ) triplets are
outperformed by the full unconstrained case, al-
though on IWSLT?07 both unconstrained and path-
aligned models are close.
For a fair comparison, we added a classical IBM
model 1 in the rescoring framework. It can be seen
that the presented triplet models slightly outperform
377
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 52.3 34.57 50.4 36.46
trip fe+ef pall 52.9 34.32 50.6 36.34
+ max dist 10 52.9 34.20 50.8 36.22
Table 3: Effect of using maximum distance constraint for
pall on EPPS Spanish-English test sets, occ3, 4 EM iter-
ations due to time constraints.
the simple IBM model 1. Note that IBM model 1
is a special case of the triplet lexicon model if the
second trigger is the empty word.
4.3.2 EPPS experiments
Since EPPS is a considerably harder task (larger
vocabulary and longer sentences), the training of a
full unconstrained triplet model cannot be done due
to memory restrictions. One possibility to reduce
the number of extracted triplets is to apply a max-
imum distance constraint in the training procedure,
i.e. only trigger pairs are considered where the dis-
tance between first and second trigger is below or
equal to the specified maximum.
Table 3 shows the effect of a maximum distance
constraint for the Spanish-English direction. Due
to the large amount of triplets (we extract roughly
two billion triplets2 for the EPPS data), we drop all
triplets that occur less than 3 times which results in
640 million triplets. Also, due to time restrictions3,
we only train 4 iterations and compare it to 4 itera-
tions of the same setting with the maximum distance
set to 10. The training with the maximum distance
constraints ends with a total of 380 million triplets.
As can be seen (Table 3), the performance is compa-
rable while cutting down the computation time from
9.2 to 3.1 hours. The experiments were carried out
on a 2.2GHz Opteron machine with 16 GB of mem-
ory. The overall gain is +0.4?0.6% BLEU and up to
-0.4% in TER. We even observe a slight increase in
BLEU for the TC-Star?07 set which might be a ran-
dom effect due to optimization on the development
set where the behavior is the same as for TC-Star?06.
2Extraction can be easily done in parallel by splitting the
corpus and merging identical triplets iteratively in a separate
step for two chunks at a time.
3One iteration needs more than 12 hours for the uncon-
strained case.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
trip fe+ef pphr 50.2 37.01 51.5 35.38
+ occ2 50.2 37.06 51.8 35.32
Table 4: Results on EPPS, English-Spanish, pphr com-
bined, occ3, 10 EM iterations.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
using FA 50.0 37.18 51.7 35.52
using IBM4 50.0 37.12 51.7 35.43
+ occ2 50.2 36.84 52.0 35.10
+ max dist 1 50.0 37.10 51.7 35.51
Table 5: Results on EPPS, English-Spanish, maximum
approximation, palign combined, occ3, 10 EM iterations.
Results on EPPS English-Spanish for the phrase-
bounded triplet model are presented in Table 4.
Since the number of triplets is less than for the un-
constrained model, we can lower the cutoff from 3
to 2 (denoted in the table by occ3 and occ2 , respec-
tively). There is a small additional gain on the TC-
Star?07 test set by this step, with a total of +0.7%
BLEU for TC-Star?06 and +0.8% BLEU for TC-
Star?07.
Table 5 shows results for a variation of the path-
aligned triplet model palign that restricts the first trig-
ger to the best aligned word as estimated in the IBM
model 1, thus using a maximum-approximation of
the given word alignment. The model was trained
on two word alignments, firstly the one contained in
the forced alignments on the training data, and sec-
ondly on an IBM-4 word alignment generated using
GIZA++. For this second model we also demon-
strate the improvement obtained when increasing the
triplet lexicon size by using less trimming.
Another experiment was carried out to investigate
the effect of immediate neighboring words used as
triggers within the palign setting. This is equivalent
to using a ?maximum distance of 1? constraint. We
obtained worse results, namely a 0.2-0.3% drop in
BLEU and a 0.3-0.4% raise in TER (cf. Table 5,
last row), although the training is significantly faster
with this setup, namely roughly 30 minutes per it-
378
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
IBM model 1 50.0 37.12 51.8 35.51
pall , occ3 50.0 37.17 51.8 35.43
pphr , occ2 50.2 37.06 51.8 35.32
palign , occ2 50.2 36.84 52.0 35.10
Table 6: Final results on EPPS English-Spanish, con-
strained triplet models, 10 EM iterations, compared to
standard IBM model 1.
eration using less than 2 GB of memory. However,
this shows that triggers outside the immediate con-
text help overall translation quality. Additionally, it
supports the claim that the presented methods are a
complementary alternative to the WSD approaches
mentioned in Section 2 which only consider the im-
mediate context of a single word.
Finally, we compare the constrained models to an
unconstrained setting and, again, to a standard IBM
model 1. Table 6 shows that the palign model con-
strained on using the IBM-4 word alignments yields
+0.7% in BLEU on TC-Star?06 which is +0.2%
more than with a standard IBM model 1. TER de-
creases by -0.3% when compared to model 1. For
the TC-Star?07 set, the observations are similar.
The oracle TER scores of the development n-best
list are 25.16% for English-Spanish and 27.0% for
Spanish-English, respectively.
5 Discussion
From the results of our reranking experiments, we
can conclude that the presented triplet lexicon model
outperforms the baseline single-best hypotheses of
the decoder. When comparing to a standard IBM
model 1, the improvements are significantly smaller
though measurable. So far, since IBM model 1
is considered one of the stronger rescoring mod-
els, these results look promising. An unconstrained
triplet model has the best performance if training is
feasible since it also needs the most memory and
time to be trained, at least for larger tasks.
In order to cut down computational requirements,
we can apply phrase-bounded and path-aligned
training constraints that restrict the possibilities of
selecting triplet candidates (in addition to simple
f e e? ?(f |e, e?)
pagar taxpayer bill 0.76
factura taxpayer bill 0.11
contribuyente taxpayer bill 0.10
f e ? pibm1 (f |e)
contribuyente taxpayer 0.40
contribuyentes taxpayer 0.18
europeo taxpayer 0.08
factura bill 0.19
ley bill 0.18
proyecto bill 0.11
Table 7: Example of triplets and related IBM model 1
lexical probabilities. The triggers ?taxpayer? and ?bill?
have a new effect (?pagar?), previously not seen in the
top ranks of the lexicon.
thresholding). Although no clear effect could be
observed for adding empty words on the trigger-
ing side, it does not harm and, thus, we get a sim-
ilar functionality to IBM model 1 being ?integrated?
in the triplet lexicon model. The phrase-bounded
training variant uses forced alignments computed
on the whole training data (i.e. search constrained
to producing the target sentences of the bilingual
corpus) but could not outperform the path-aligned
model which reuses the alignment path information
obtained in regular GIZA++ training.
Additionally, we observe a positive impact from
triggers lying outside the immediate context of one
predecessor or successor word.
5.1 Examples
Table 7 shows an excerpt of the top entries for
(e, e?) = (taxpayer , bill) and compares it to the top
entries of a lexicon based on IBM model 1. We ob-
serve a triggering effect since the Spanish word pa-
gar (to pay) is triggered at top position by the two
English words taxpayer and bill. The average dis-
tance of taxpayer and bill is 5.4 words. The models
presented in this work try to capture this property
and apply it in the scoring of hypotheses in order to
allow for better lexical choice in specific contexts.
In Table 8, we show an example translation where
rescoring with the triplet model achieves higher n-
gram coverage on the reference translation than the
variant based on IBM model 1 rescoring. The differ-
ing phrases are highlighted.
379
Source sen-
tence
. . . respecto de la Posicio?n Comu?n
del Consejo con vistas a la adopcio?n
del Reglamento del Parlamento Eu-
ropeo y del Consejo relativo al . . .
IBM-1
rescoring
. . . on the Council common position
with a view to the adoption of the
Rules of Procedure of the European
Parliament and of the Council . . .
Triplet
rescoring
. . . on the common position of the
Council with a view to the adop-
tion of the regulation of the Euro-
pean Parliament and of the Council
. . .
Reference
translation
. . . as regards the Common Position
of the Council with a view to the
adoption of a European Parliament
and Council Regulation as regards
the . . .
Table 8: A translation example on TC-Star?07 Spanish-
English comparing the effect of the triplet model to a
standard IBM-1 model.
6 Outlook
We have presented a new lexicon model based on
triplets extracted on a sentence level and trained it-
eratively using the EM algorithm. The motivation of
this approach is to add an additional second trigger
to a translation lexicon component which can come
from a more global context (on a sentence level) and
allow for a more fine-grained lexical choice given a
specific context. Thus, the method is related to word
sense disambiguation approaches.
We showed improvements by rescoring n-best
lists of the IWSLT Chinese-English and EPPS
Spanish-English/English-Spanish task. In total, we
achieve up to +1% BLEU for some of the test sets in
comparison to the decoder baseline and up to +0.3%
BLEU compared to IBM model 1.
Future work will address an integration into the
decoder since the performance of the current rescor-
ing framework is limited by the quality of the n-
best lists. For the inverse model, p(e|f, f ?), an in-
tegration into the search is directly possible. Further
experiments will be conducted, especially on large
tasks such as the NIST Chinese-English and Arabic-
English task. Training on these huge databases will
only be possible with an appropriate selection of
promising triplets.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
The authors would like to thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL 2007),
Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33?40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stephen A. Della Pietra, Vincent J. Della Pietra, John R.
Gillett, John D. Lafferty, Harry Printz, and Lubos?
Ures?. 1994. Inference and estimation of a long-range
trigram model. In J. Oncina and R. C. Carrasco, ed-
itors, Grammatical Inference and Applications, Sec-
ond International Colloquium, ICGI-94, volume 862,
pages 78?92, Alicante, Spain. Springer Verlag.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?22.
Woosung Kim and Sanjeev Khudanpur. 2003. Cross-
lingual lexical triggers in statistical language model-
ing. In Proceedings of the 2003 Conference on Empir-
ical Methods in Natural Language Processing, pages
17?24, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
380
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10(3):187?228.
Christoph Tillmann and Hermann Ney. 1997. Word trig-
gers and the EM algorithm. In Proc. Special Interest
Group Workshop on Computational Natural Language
Learning (ACL), pages 117?124, Madrid, Spain, July.
Christoph Tillmann. 2001. Word Re-Ordering and Dy-
namic Programming based Search Algorithm for Sta-
tistical Machine Translation. Ph.D. thesis, RWTH
Aachen University, Aachen, Germany, May.
381
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 839?847,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Complexity of finding the BLEU-optimal hypothesis in a confusion network
Gregor Leusch and Evgeny Matusov and Hermann Ney
RWTH Aachen University, Germany
{leusch,matusov,ney}@cs.rwth-aachen.de
Abstract
Confusion networks are a simple representa-
tion of multiple speech recognition or transla-
tion hypotheses in a machine translation sys-
tem. A typical operation on a confusion net-
work is to find the path which minimizes or
maximizes a certain evaluation metric. In this
article, we show that this problem is gener-
ally NP-hard for the popular BLEU metric,
as well as for smaller variants of BLEU. This
also holds for more complex representations
like generic word graphs. In addition, we give
an efficient polynomial-time algorithm to cal-
culate unigram BLEU on confusion networks,
but show that even small generalizations of
this data structure render the problem to be
NP-hard again.
Since finding the optimal solution is thus not
always feasible, we introduce an approximat-
ing algorithm based on a multi-stack decoder,
which finds a (not necessarily optimal) solu-
tion for n-gram BLEU in polynomial time.
1 Introduction
In machine translation (MT), confusion networks
(CNs) are commonly used to represent alternative
versions of sentences. Typical applications include
translation of different speech recognition hypothe-
ses (Bertoldi et al, 2007) or system combination
(Fiscus, 1997; Matusov et al, 2006).
A typical operation on a given CN is to find the
path which minimizes or maximizes a certain eval-
uation metric. This operation can be used in ap-
plications like Minimum Error Rate Training (Och,
2003), or optimizing system combination as de-
scribed by Hillard et al (2007). Whereas this is
easily achievable for simple metrics like the Word
Error Rate (WER) as described by Mohri and Riley
(2002), current research in MT uses more sophisti-
cated measures, like the BLEU score (Papineni et
al., 2001). Zens and Ney (2005) first described this
task on general word graphs, and sketched a com-
plete algorithm for calculating the maximum BLEU
score in a word graph. While they do not give an
estimate on the complexity of their algorithm, they
note that already a simpler algorithm for calculating
the Position independent Error Rate (PER) has an
exponential worst-case complexity. The same can
be expected for their BLEU algorithm. Dreyer et
al (2007) examined a special class of word graphs,
namely those that denote constrained reorderings of
single sentences. These word graphs have some
properties which simplify the calculation; for exam-
ple, no edge is labeled with the empty word, and
all paths have the same length and end in the same
node. Even then, their decoder does not optimize
the true BLEU score, but an approximate version
which uses a language-model-like unmodified pre-
cision. We give a very short introduction to CNs and
the BLEU score in Section 2.
In Section 3 we show that finding the best BLEU
score is an NP-hard problem, even for a simplified
variant of BLEU which only scores unigrams and
bigrams. The main reason for this problem to be-
come NP-hard is that by looking at bigrams, we al-
low for one decision to also influence the following
decision, which itself can influence the decisions af-
ter that. We also show that this also holds for uni-
gram BLEU and the position independent error rate
(PER) on a slightly augmented variant of CNs which
allows for edges to carry multiple symbols. The con-
catenation of symbols corresponds to the interde-
pendency of decisions in the case of bigram matches
above.
NP-hard problems are quite common in machine
839
translation; for example, Knight (1999) has shown
that even for a simple form of statistical MT mod-
els, the decoding problem is NP-complete. More
recently, DeNero and Klein (2008) have proven the
NP-completeness of the phrase alignment problem.
But even a simple, common procedure as BLEU
scoring, which can be performed in linear time on
single sentences, becomes a potentially intractable
problem as soon as it has to be performed on a
slightly more powerful representation, such as con-
fusion networks. This rather surprising result is the
motivation of this paper.
The problem of finding the best unigram BLEU
score in an unaugmented variant of CNs is not NP-
complete, as we show in Section 4. We present an
algorithm that finds such a unigram BLEU-best path
in polynomial time.
An important corollary of this work is that calcu-
lating the BLEU-best path on general word graphs
is also NP-complete, as CNs are a true subclass
of word graphs. It is still desirable to calculate a
?good? path in terms of the BLEU score in a CN,
even if calculating the best path is infeasible. In Sec-
tion 5, we present an algorithm which can calculate
?good? solutions for CNs in polynomial time. This
algorithm can easily be extended to handle arbitrary
word graphs. We assess the algorithm experimen-
tally on real-world MT data in Section 6, and draw
some conclusions from the results in this article in
Section 7.
2 Confusion networks
A confusion network (CN) is a word graph where
each edge is labeled with exactly zero or one sym-
bol, and each path from the start node to the end
node visits each node of the graph in canonical or-
der. Usually, we represent unlabeled edges by label-
ing them with the empty word ?.
Within this paper, we represent a CN by a list of
lists of words {wi,j}, where each wi,j corresponds
to a symbol on an edge between nodes i and i + 1.
A path in this CN can be written as a string of inte-
gers, an1 = a1, . . . , an, such that the path is labeled
w1,a1w2,a2 . . . wn,an . Note that there can be a differ-
ent number of possible words, j, for different posi-
tions i.
2.1 BLEU and variants
The BLEU score, as defined by Papineni et al
(2001), is the modified n-gram precision of a hy-
pothesis, with 1 ? n ? N , given a set of reference
translations R. ?Modified precision? here means
that for each n-gram, its maximum number of oc-
currences within the reference sentences is counted,
and only up to that many occurrences in the hypothe-
sis are considered to be correct. The geometric mean
over the precisions for all n is calculated, and mul-
tiplied by a brevity penalty bp. This brevity penalty
is 1.0 if the hypothesis sentence is at least as long as
the reference sentence (special cases occur if multi-
ple reference sentences with different length exists),
and less than 1.0 otherwise. The exact formulation
can be found in the cited paper; for the proofs in
our paper it is enough to note that the BLEU score
is 1.0 exactly if all n-grams in the hypothesis oc-
cur at least that many times in a reference sentence,
and if there is a reference sentence which is as long
as or shorter than the hypothesis. Assuming that
we can always provide a dummy reference sentence
shorter than this length, we do not need to regard
the brevity penalty in these proofs. Within the fol-
lowing proofs of NP-hardness, we will only require
confusion networks (and word graphs) which do not
contain empty words, and where all paths from the
start node to the end node have the same length.
Usually, in the definition of the BLEU score, N is
set to 4; within this article we denote this metric as
4BLEU. We can also restrict the calculations to un-
igrams only, which would be 1BLEU, or to bigrams
and unigrams, which we denote as 2BLEU.
Similar to the 1BLEU metric is the Position in-
dependent Error Rate PER (Tillmann et al, 1997),
which counts the number of substitutions, insertions,
and deletions that have to be performed on the uni-
gram counts to have the hypothesis counts match the
reference counts. Unlike 1BLEU, for PER to be op-
timal (here, 0.0), the reference counts must match
the candidate counts exactly.
Given a CN {wi,j} and a set of reference sen-
tences R, we define the optimization problem
Definition 1 (CN-2BLEU-OPTIMIZE) Among
all paths aI1 through the CN, what is the path with
the highest 2BLEU score?
Related to this is the decision problem
Definition 2 (CN-2BLEU-DECIDE) Among all
paths aI1 through the CN, is there a path with a
2BLEU score of 1.0?
Similarly we define CN-4BLEU-DECIDE, CN-
PER-DECIDE, etc.
840
3 CN-2BLEU-DECIDE is NP-complete
We now show that CN-2BLEU-DECIDE is NP-
complete. It is obvious that the problem is in NP:
Given a path aI1, which is polynomial in size to the
problem, we can decide in polynomial time whether
aI1 is a solution to the problem ? namely by calcu-
lating the BLEU score. We now show that there is
a problem known to be NP-complete which can be
polynomially reduced to CN-2BLEU-DECIDE. For
our proof, we choose 3SAT.
3.1 3SAT
Consider the following problem:
Definition 3 (3SAT) Let X = {x1, . . . , xn}
be a set of Boolean variables, let F =?k
i=1 (Li,1?Li,2?Li,3) be a Boolean formula,
where each literal Li,j is either a variable x or its
negate x. Is there a assignment ? : X ? {0, 1}
such that ? |= F? In other words, if we replace
each x in F by ?(x), and each x by 1? ?(x), does
F become true?
It has been shown by Karp (1972) that 3SAT is
NP-complete. Consequently, if for another problem
in NP there is polynomial-size and -time reduction
of an arbitrary instance of 3SAT to an instance of this
new problem, this new problem is also NP-complete.
3.2 Reduction of 3SAT to
CN-2BLEU-DECIDE
Let F be a Boolean formula in 3CNF, and let k be
its size, as in Definition 3. We will now reduce it to a
corresponding CN-2BLEU-DECIDE problem. This
means that we create an alphabet ?, a confusion net-
work C, and a set of reference sentencesR, such that
there is a path through C with a BLEU score of 1.0
exactly if F is solvable:
Create an alphabet ? based on F as ? :=
{x1, . . . , xn} ? {x1, . . . , xn} ? {}. Here, the xi
and xi symbols will correspond to the variable with
the same name or their negate, respectively, whereas
 will serve as an ?isolator symbol?, to avoid un-
wanted bigram matches or mismatches between sep-
arate parts of the constructed CN or sentences.
Consider the CN C from Figure 1.
Consider the following set of reference sentences:
R := {  (x1)
k(x2)
k . . . (xn)
k
 (x1)
k(x2)
k . . . (xn)
k,
(x1)
k  (x1)
k  . . . (xn)
k  (xn)
k  ()k+n }
where (x)k denotes k subsequent occurrences of x.
Clearly, both C and R are of polynomial size in n
and k, and can be constructed in polynomial time.
Then,
There is an assignment ? such that ? |= F
?
There is a path aI1 through C such that
BLEU(aI1, R) = 1.0.
Proof: ???
Let ? be an assignment under which F becomes
true. Create a path aI1 as follows: Within A, for
each set of edges Li,1, Li,2, Li,3, choose the path
through an x where ?(x) = 1, or through an x
where ?(x) = 0. Note that there must be such an
x, because otherwise the clause Li,1 ? Li,2 ? Li,3
would not be true under ?. Within B, select the path
always through xi if ?(xi) = 0, and through xi if
?(xi) = 1.
Then, aI1 consists of, for each i,
? At most k occurrences of both xi and xi
? At most k occurrences of each of the bigrams
xi, xi, xi, xi, xixi, and xixi
? No other bigram than those listed above.
For all of these unigram and bigram counts, there is
a reference sentence in R which contains at least as
many of those unigrams/bigrams as the path. Thus,
the unigram and bigram precision of aI1 is 1.0. In ad-
dition, there is always a reference sentence whose
length is shorter than that of aI1, such that the brevity
penalty is also 1.0. As a result, BLEU(aI1, R) =
1.0.
???
Let aI1 be a path through C such that
BLEU(aI1, R) = 1.0. Because there is no bi-
gram xixi or xixi in R, we can assume that for each
xi, either only xi edges, or only xi edges appear
in the B part of aI1, each at most k times. As no
unigram xi and xi appears more than k times in R,
we can assume that, if the xi edges are passed in
B, then only the xi edges are passed in A, and vice
versa. Now, create an assignment ? as follows:
? :=
{
0 ifxi edges are passed inB
1 otherwise
Then, ? |= F . Proof: Assume that F? = 0. Then
there must be a clause i such that Li,1?Li,2?Li,3 =
841
A :=          
   




L1,1
L1,2
L1,3
   
   
   
   





   
   
   
   




L2,1
L2,2
L2,3
   
   
   
   




 ? ? ?
   
   
   
   





   
   
   
   




Lk,1
Lk,2
Lk,3
   
   
   
   





   
   
   
   




B :=             
x1
x1
   
   
   
   




x1
x1
? ? ?
   
   
   
   




x1
x1? ?? ?
k times
   
   
   
   





   
   
   
   




x2
x2
? ? ?
? ?? ?
k
   
   
   
   




 ? ? ?
   
   
   
   




xn
xn
? ? ?
? ?? ?
k
   
   
   
   





C := A             

B
Figure 1: CN constructed from a 3SAT formula F . C is the concatenation of the left part A, and the right path B,
separated by an isolating .
0. At least one of the edges Li,j associated with the
literals of this clause must have been passed by aK1
in A. This literal, though, can not have been passed
in B. As a consequence, ?(Li,j) = 1. But this
means that Li,1 ? Li,2 ? Li,3 = 1 ; contra-
diction.
Because CN-2BLEU-DECIDE is in NP, and we
can reduce an NP-complete problem (3SAT) in poly-
nomial time to a CN-2BLEU-DECIDE problem, this
means that CN-2BLEU-DECIDE is NP-complete.
3.3 CN-4BLEU-DECIDE
It is straightforward to modify the construction
above to create an equivalent CN-4BLEU-DECIDE
problem instead: Replace each occurrence of the
isolating symbol  in A,B, C, R by three consecu-
tive isolating symbols . Then, everything said
about unigrams still holds, and bi-, tri- and four-
grams are handled equivalently: Previous unigram
matches on  correspond to uni-, bi-, and trigram
matches on , , . Bigram matches on x corre-
spond to bi-, tri-, and fourgram matches on x, x,
x, and similar holds for bigram matches x, x,
x. Unigram matches x, x, and bigram matches
xx etc. stay the same. Consequently, CN-4BLEU-
DECIDE is also an NP-complete problem.
3.4 CN*-1BLEU-DECIDE
Is it possible to get rid of the necessity for bi-
gram counts in this proof? One possibility might be
to look at slightly more powerful graph structures,
CN*. In these graphs, each edge can be labeled
by arbitrarily many symbols (instead of just zero or
one). Then, consider a CN* graph C? := A            

B?,
B? :=             
(x1)k
(x1)k
   
   
   
   





   
   
   
   




x1
x1
?? ?? ?
k times
? ? ?
   
   
   
   




(xn)k
(xn)k
   
   
   
   




 ? ? ?
Figure 2: Right part of a CN* constructed from a 3SAT
formula F .
with B? as in Figure 2.
With
R? := {(x1)
k(x1)
k . . . (xn)
k(xn)
k()k}
we can again assume that either xi or xi ap-
pears k times in the B?-part of a path aK1 with
1BLEU(aK1 , R
?) = 1.0, and that for every solution
? to F there is a corresponding path aK1 through C
?
and vice versa. In this construction, we also have
exact matches of the counts, so we can also use PER
in the decision problem.
While CN* are generally not word graphs by
themselves due to the multiple symbols on edges,
it is straightforward to create an equivalent word
graph from a given CN*, as demonstrated in Fig-
ure 3. Consequently, deciding unigram BLEU and
unigram PER are NP-complete problems for general
word graphs as well.
4 Solving CN-1BLEU-DECIDE in
polynomial time
It is not a coincidence that we had to resort to
bigrams or to edges with multiple symbols for
NP-completeness: It turns out that CN-1BLEU-
DECIDE, where the order of the words does not
842
CN*: ? ? ?             
(x1)k
(x1)k
   
   
   
   




 ? ? ?
;
WG: ? ? ?          
x1
x1
  
  
  



  
  
  
  




x1
x1
? ? ?   
  
  



  
  
  
  




x1
x1? ?? ?
k times
   
   
   
   




 ? ? ?
Figure 3: Construction of a word graph from a CN* as in
B?.
matter at all, can be decided in polynomial time
using the following algorithm, which disregards a
brevity penalty for the sake of simplicity:
Given a vocabulary X , a CN {wi,j}, and a set of
reference sentences R together with their unigram
BLEU counts c(x) : X ? N and C :=
?
x?X c(x),
1. Remove all parts fromw where there is an edge
labeled with the empty word ?. This step will
always increase unigram precision, and can not
hurt any higher n-gram precision here, because
n = 1. In the example in Figure 4, the edges
labeled very and ? respectively are affected in
this step.
2. Create nodes A0 := {1, . . . , n}, one for each
node with edges in the CN. In the example in
Figure 5, the three leftmost column heads cor-
respond to these nodes.
3. Create nodes B := {x.j |x ? X, 1?j?c(x)}.
In other words, create a unique node for each
?running? word in R ? e.g. if the first and
second reference sentence contain x once each,
and the third reference contains x twice, create
exactly x.1 and x.2. In Figure 5, those are the
row heads to the right.
4. Fill A with empty nodes to match the total
length: A := A0 ? {?.j | 1 ? j ? C ? n}.
If n > C, the BLEU precision can not be 1.0.
The five rightmost columns in Figure 5 corre-
spond to those.
5. Create edges
E := {(i, wi,j .k) | 1? i?n, all j, 1?c(wi,j)}
? {(i, ?.j) | 1 ? i ? n, all j}. These edges are
denoted as ? or ? in Figure 5.
C :=             
on
at
   
   
   
   




the
that
   
   
   
   




very
?
   
   
   
   




day
time
R := { on the same day,
at the time and the day }
Figure 4: Example for CN-1BLEU-DECIDE.
1 2 3 ?.1 ?.2 ?.3 ?.4 ?.5
? ? ? ? ? ? on
? ? ? ? ? ? the.1
? ? ? ? ? ? the.2
? ? ? ? ? same
? ? ? ? ? ? day
? ? ? ? ? ? at
? ? ? ? ? ? time
? ? ? ? ? and
Figure 5: Bipartite graph constructed to find the optimal
1BLEU path in Figure 4. One possible maximum bipar-
tite matching is marked with ?.
6. Find the maximum bipartite matching M be-
tween A and B given E. Figure 5 shows such
a matching with ?.
7. If all nodes in A and B are covered by M ,
then 1BLEU({wi,j}, R) = 1.0. The words that
are matched to A0 then form the solution path
through {wi,j}.
Figure 4 gives an example of a CN and a set of ref-
erences R, for which the best 1BLEU path can be
constructed by the algorithm above. The bipartite
graph constructed in Step 1 to Step 4 for this exam-
ple, given in matrix form, can be found in Figure 5.
Such a solution to Step 6, if found, corresponds
exactly to a path through the confusion network with
1BLEU=1.0, and vice versa: for each position 1 ?
i ? n, the matched word corresponds to the word
that is selected for the position of the path; ?surplus?
counts are matched with ?s.
Step 6 can be performed in polynomial time
(Hopcroft and Karp, 1973) O((C +n)5/2); all other
steps in linear time O(C + n). Consequently, CN-
1BLEU can be decided in polynomial time O((C +
n)5/2). Similarly, an actual optimum 1BLEU score
843
can be calculated in O((C + n)5/2).
It should be noted that the only alterations in the
hypothesis length, and as a result the only alterations
in the brevity penalty, will come from Step 1. Con-
sequently, the brevity penalty can be taken into ac-
count as follows: Consider that there are M nodes
with an empty edge in {wi,j}. Instead of remov-
ing them in Step 1, keep them in, but for each
1 ? m ? M , run through steps 2 to 6, but add
m nodes ?.1, . . . , ?.m to B in Step 3, and add corre-
sponding edges to these nodes to E in Step 5. After
each iteration (which leads to a constant hypothesis
length), calculate precision and brevity penalty. Se-
lect the best product of precision and brevity penalty
in the end. The overall time complexity now is in
M ?O((C + n)5/2).
A PER score can be calculated in a similar fash-
ion.
5 Finding approximating solutions for
CN-4BLEU in polynomial time
Knowing that the problem of finding the BLEU-best
path is an NP-complete problem is an unsatisfactory
answer in practice ? in many cases, having a good,
but not necessarily optimum path is preferable to
having no good path at all.
A simple approach would be to walk the CN from
the start node to the end node, keeping track of n-
grams visited so far, and choosing the word next
which maximizes the n-gram precision up to this
word. Track is kept by keeping n-gram count vec-
tors for the hypothesis path and the reference sen-
tences, and update those in each step.
The main problem with this approach is that of-
ten the local optimum is suboptimal on the global
scale, for example if a word occurs on a later posi-
tion again.
Zens and Ney (2005) on the other hand propose
to keep all n-gram count vectors instead, and only
recombine path hypotheses with identical count vec-
tors. As they suspect, the search space can become
exponentially large.
In this paper, we suggest a compromise between
these two extremes, namely keeping active a suffi-
ciently large number of ?path hypotheses? in terms
of n-gram precision, instead of only the first best,
or of all. But even then, edges with empty words
pose a problem, as stepping along an empty edge
will never decrease the precision of the local path.
In certain cases, steps along empty edges may affect
the n-gram precision for higher n-grams. But this
will only take effect after the next non-empty step, it
does not influence the local decision in a node. Step-
ping along a non-empty edge will often decrease the
local precision, though. As a consequence, a simple
algorithm will prefer paths with shorter hypotheses,
which leads to a suboptimal total BLEU score, be-
cause of the brevity penalty. One can counter this
problem for example by using a brevity penalty al-
ready during the search. But this is problematic as
well, because it is difficult to define a proper partial
reference length in this case.
The approach we propose is to compare only par-
tial path hypotheses with the same number of empty
edges, and ending in the same position in the confu-
sion network. This idea is illustrated in Figure 6: We
compare only the partial precision of path hypothe-
ses ending in the same node. Due to the simple na-
ture of this search graph, it can easily be traversed in
a left-to-right, top-to-bottom manner. With regard to
a node currently being expanded, only the next node
in the same row, and the corresponding columns in
the next row need to be kept active. When imple-
menting this algorithm, Hypotheses should be com-
pared on the modified BLEUS precision by Lin and
Och (2004) because the original BLEU precision
equals zero as long as there are no higher n-gram
matches in the partial hypotheses, which renders
meaningful comparison hard or impossible.
In the rightmost column, all path hypotheses
within a node have the same hypothesis length. Con-
sequently, we can select the hypothesis with the best
(brevity-penalized) BLEU score by multiplying the
appropriate brevity penalty to the precision of the
best path ending in each of these nodes. If we al-
ways expand all possible path hypotheses within the
nodes, and basically run a full search, we will al-
ways find the BLEU-best path this way. From the
proof above, it follows that the number of path hy-
pothesis we would have to keep can become expo-
nentially large. Fortunately, if a ?good? solution is
good enough, we do not have to keep all possible
path hypotheses, but only the S best ones for a given
constant S, or those with a precision not worse than
c times the precision of the best hypothesis within
the node. Assuming that adding and removing an
element to/from a size-limited stack of size S takes
timeO(logS), that we allow at mostE empty edges
in a solution, and that there are j edges in each of the
n positions, this algorithm has a time complexity of
844
Figure 6: Principle of the multi-stack decoder used to find
a path with a good BLEU score. The first row shows
the original confusion network, the following rows show
the search graph. Duplicate edges were removed, but no
word was considered ?unknown?.
O(E ? n ? j ? S logS).
To reduce redundant duplicated path hypotheses,
and by this to speed up the algorithm and reduce the
risk that good path hypotheses are pruned, the confu-
sion network should be simplified before the search,
as shown in Figure 6:
1. Remove all words in the CN which do not ap-
pear in any reference sentence, if there at least
one ?known? non-empty word at the same po-
sition. If there is no such ?known? word, re-
place them all by a single token denoting the
?unknown word?.
2. Remove all duplicate edges in a position, that
is, if there are two or more edges carrying the
same label in one position, remove all but one
for them.
These two steps will keep at least one of the BLEU-
best paths intact. But they can remove the average
branching factor (j) of the CN significantly, which
leads to a significantly lower number of duplicate
path hypotheses during the search.
Table 1: Statistics of the (Chinese?)English MT corpora
used for the experiments
NIST NIST
2003 2006
number of systems 4 4
number of ref. 4 4 per sent.
sentences 919 249
system length 28.4 33.2 words?
ref. length 27.5 34.2 words?
best path 24.4 33.9 words?
CN length 40.7 39.5 nodes?
best single system 29.3 52.5 BLEU
30.5 51.6 BLEUS?
?average per sentence
Our algorithm can easily be extended to handle ar-
bitrary word graphs instead of confusion networks.
In this case, each ?row? in Figure 6 will reflect the
structure of the word graph instead of the ?linear?
structure of the CN.
While this algorithm searches for the best path
for a single sentence only, a common task is to
find the best BLEU score over a whole test set ?
which can mean suboptimal BLEU scores for in-
dividual sentences. This adds an additional com-
binatorial problem over the sentences to the actual
decoding process. Both Zens and Ney (2005) and
Dreyer et al(2007) use a greedy approach here; the
latter estimated the impact of this to be insignifi-
cant in random sampling experiments. In our exper-
iments, we used the per-sentence BLEUS score as
(greedy) decision criterion, as this is also the prun-
ing criterion. One possibility to adapt this approach
to Zens?s/Dreyer?s greedy approach for system-level
BLEU scores might be to initialize n-gram counts
and hypothesis length not to zero at the beginning
of each sentence, but to those of the corpus so far.
But as this diverts from our goal to optimize the
sentence-level scores, we have not implemented it
so far.
6 Experimental assessment of the
algorithm
The question arises how many path hypotheses we
need to retain in each step to obtain optimal paths.
To examine this, we created confusion networks out
of the translations of the four best MT systems of
845
ll l
l l l l l l l l l l l l
5 10 15 20
0.32
0.34
0.36
0.38
0.40
# path hyps
BL
EU
, BL
EU
S
l
sys?BLEU
avg. seg?BLEUS
avg. seg?BLEU
Figure 7: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT03 corpus.
the NIST 2003 and 2006 Chinese?English evalu-
ation campaigns, as available from the Linguistic
Data Consortium (LDC). The hypotheses of the best
single system served as skeleton, those of the three
remaining systems were reordered and aligned to the
skeleton hypothesis. This corpus is described in Ta-
ble 1. Figures 7 and 8 show the measured BLEU
scores in three different definitions, versus the max-
imum number of path hypotheses that are kept in
each node of the search graph. Shown are the av-
erage sentence-wise BLEUS score, which is what
the algorithm actually optimizes, for comparison the
average sentence-wise BLEU score, and the total
document-wise BLEU score.
All scores increase with increasing number of re-
tained hypotheses, but stabilize around a total of 15
hypotheses per node. The difference over a greedy
approach, which corresponds to a maximum of one
hypothesis per node if we leave out the separation by
path length, is quite significant. No further improve-
ments can be expected for a higher number of hy-
potheses, as experiments up to 100 hypotheses show.
7 Conclusions
In this paper, we showed that deciding whether a
given CN contains a path with a BLEU score of 1.0
is an NP-complete problem for n-gram lengths ? 2.
l
l l l
l l l l l l l l l l l l
5 10 15 20 250.
65
0.67
0.69
0.71
# path hyps
BL
EU
, BL
EU
S
l
sys?BLEU
avg. seg?BLEUS
avg. seg?BLEU
Figure 8: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT06 corpus.
The problem is also NP-complete if we only look
at unigram BLEU, but allow for CNs where edges
may contain multiple symbols, or for arbitrary word
graphs. As a corollary, any proposed algorithm to
find the path with an optimal BLEU score in a CN,
even more in an arbitrary word graph, which runs
in worst case polynomial time can only deliver an
approximation1.
We gave an efficient polynomial time algorithm
for the simplest variant, namely deciding on a uni-
gram BLEU score for a CN. This algorithm can eas-
ily be modified to decide on the PER score as well,
or to calculate an actual unigram BLEU score for the
hypothesis CN.
Comparing these results, we conclude that the
ability to take bi- or higher n-grams into account,
be it in the scoring (as in 2BLEU), or in the graph
structure (as in CN*), is the key to render the prob-
lem NP-hard. Doing so creates long-range depen-
dencies, which oppose local decisions.
We also gave an efficient approximating algo-
rithm for higher-order BLEU scores. This algorithm
is based on a multi-stack decoder, taking into ac-
count the empty arcs within a path. Experimental
results on real-world data show that our method is
indeed able to find paths with a significantly better
1provided that P 6= NP , of course.
846
BLEU score than that of a greedy search. The re-
sulting BLEUS score stabilizes already on a quite
restricted search space, showing that despite the
proven NP-hardness of the exact problem, our al-
gorithm can give useful approximations in reason-
able time. It is yet an open problem in how far
the problems of finding the best paths regarding a
sentence-level BLEU score, and regarding a system-
level BLEU score correlate. Our experiments here
suggest a good correspondence.
8 Acknowledgments
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
The proofs and algorithms in this paper emerged
while the first author was visiting researcher at
the Interactive Language Technologies Group of
the National Research Council (NRC) of Canada,
Gatineau. The author wishes to thank NRC and
Aachen University for the opportunity to jointly
work on this project.
References
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 1297?1300,
Honululu, HI, USA, April.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In Human Language
Technologies 2008: The Conference of the Association
for Computational Linguistics, Short Papers, pages
25?28, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing Reordering Constraints for SMT
Using Efficient BLEU Oracle Computation. In AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation (SSST) at the Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT), pages 103?110,
Rochester, NY, USA, April.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recogniser output vot-
ing error reduction (ROVER). In Proceedings 1997
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?352, Santa Barbara, CA.
Dustin Hillard, Bjo?rn Hoffmeister, Mari Ostendorf, Ralf
Schlu?ter, and Hermann Ney. 2007. iROVER: Improv-
ing system combination with classification. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 65?68, Rochester, New York, April.
John E. Hopcroft and Richard M. Karp. 1973. An n5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on Computing, 2(4):225?231.
Richard M. Karp. 1972. Reducibility among combina-
torial problems. In R. E. Miller and J. W. Thatcher,
editors, Complexity of Computer Computations, pages
85?103. Plenum Press.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluation automatic evaluation metrics for
machine translation. In Proc. COLING 2004, pages
501?507, Geneva, Switzerland, August.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 33?40, Trento, Italy, April.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proc. of
the 7th Int. Conf. on Spoken Language Processing (IC-
SLP?02), pages 1313?1316, Denver, CO, September.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the 41th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Research Division,
Thomas J. Watson Research Center, September.
Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex
Zubiaga, and Hassan Sawaf. 1997. Accelerated
DP based search for statistical translation. In Euro-
pean Conf. on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece, September.
Richard Zens and Hermann Ney. 2005. Word graphs
for statistical machine translation. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
191?198, Ann Arbor, MI, June.
847
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 210?218,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Extending Statistical Machine Translation with
Discriminative and Trigger-Based Lexicon Models
Arne Mauser and Sa
?
sa Hasan and Hermann Ney
Human Language Technology and Pattern Recognition Group
Chair of Computer Science 6, RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this work, we propose two extensions of
standard word lexicons in statistical ma-
chine translation: A discriminative word
lexicon that uses sentence-level source in-
formation to predict the target words and
a trigger-based lexicon model that extends
IBM model 1 with a second trigger, allow-
ing for a more fine-grained lexical choice
of target words. The models capture de-
pendencies that go beyond the scope of
conventional SMT models such as phrase-
and language models. We show that the
models improve translation quality by 1%
in BLEU over a competitive baseline on a
large-scale task.
1 Introduction
Lexical dependencies modeled in standard phrase-
based SMT are rather local. Even though the deci-
sion about the best translation is made on sentence
level, phrase models and word lexicons usually do
not take context beyond the phrase boundaries into
account. This is especially problematic since the
average source phrase length used during decod-
ing is small. When translating Chinese to English,
e.g., it is typically close to only two words.
The target language model is the only model
that uses lexical context across phrase boundaries.
It is a very important feature in the log-linear setup
of today?s phrase-based decoders. However, its
context is typically limited to three to six words
and it is not informed about the source sentence.
In the presented models, we explicitly take advan-
tage of sentence-level dependencies including the
source side and make non-local predictions for the
target words. This is an important aspect when
translating from languages like German and Chi-
nese where long-distance dependencies are com-
mon. In Chinese, for example, tenses are often en-
coded by indicator words and particles whose po-
sition is relatively free in the sentence. In German,
prefixes of verbs can be moved over long distances
towards the end of the sentence.
In this work, we propose two models that can
be categorized as extensions of standard word lex-
icons: A discriminative word lexicon that uses
global, i.e. sentence-level source information to
predict the target words using a statistical classi-
fier and a trigger-based lexicon model that extends
the well-known IBM model 1 (Brown et al, 1993)
with a second trigger, allowing for a more fine-
grained lexical choice of target words. The log-
linear framework of the discriminative word lexi-
con offers a high degree of flexibility in the selec-
tion of features. Other sources of information such
as syntax or morphology can be easily integrated.
The trigger-based lexicon model, or simply
triplet model since it is based on word triplets,
is not trained discriminatively but uses the classi-
cal maximum likelihood approach (MLE) instead.
We train the triplets iteratively on a training cor-
pus using the Expectation-Maximization (EM) al-
gorithm. We will present how both models al-
low for a representation of topic-related sentence-
level information which puts them close to word
sense disambiguation (WSD) approaches. As will
be shown later, the experiments indicate that these
models help to ensure translation of content words
that are often omitted by the baseline system. This
is a common problem in Chinese-English transla-
tion. Furthermore, the models are often capable to
produce a better lexical choice of content words.
210
The structure of the paper is as follows: In Sec-
tion 2, we will address related work and briefly
pin down how our models differentiate from pre-
vious work. Section 3 will describe the discrimi-
native lexical selection model and the triplet model
in more detail, explain the training procedures and
show how the models are integrated into the de-
coder. The experimental setup and results will be
given in Section 4. A more detailed discussion
will be presented in Section 5. In the end, we con-
clude our findings and give an outlook for further
research in Section 6.
2 Related Work
Several word lexicon models have emerged in the
context of multilingual natural language process-
ing. Some of them were used as a machine transla-
tion system or as a part of one such system. There
are three major types of models: Heuristic models
as in (Melamed, 2000), generative models as the
IBM models (Brown et al, 1993) and discrimina-
tive models (Varea et al, 2001; Bangalore et al,
2006).
Similar to this work, the authors of (Varea et
al., 2001) try to incorporate a maximum entropy
lexicon model into an SMT system. They use
the words and word classes from the local con-
text as features and show improvements with n-
best rescoring.
The models in this paper are also related to
word sense disambiguation (WSD). For example,
(Chan et al, 2007) trained a discriminative model
for WSD using local but also across-sentence un-
igram collocations of words in order to refine
phrase pair selection dynamically by incorporat-
ing scores from the WSD classifier. They showed
improvements in translation quality in a hierar-
chical phrase-based translation system. Another
WSD approach incorporating context-dependent
phrasal translation lexicons is given in (Carpuat
and Wu, 2007) and has been evaluated on sev-
eral translation tasks. Our model differs from the
latter in three ways. First, our approach mod-
els word selection of the target sentence based on
global sentence-level features of the source sen-
tence. Second, instead of disambiguating phrase
senses as in (Carpuat and Wu, 2007), we model
word selection independently of the phrases used
in the MT models. Finally, the training is done in a
different way as will be presented in Sections 3.1.1
and 3.2.1.
Recently, full translation models using discrim-
inative training criteria emerged as well. They
are designed to generate a translation for a given
source sentence and not only score or disam-
biguate hypotheses given by a translation system.
In (Ittycheriah and Roukos, 2007), the model can
predict 1-to-many translations with gaps and uses
words, morphologic and syntactic features from
the local context.
The authors of (Venkatapathy and Bangalore,
2007) propose three different models. The first
one is a global lexical selection model which in-
cludes all words of the source sentence as features,
regardless of their position. Using these features,
the system predicts the words that should be in-
cluded in the target sentence. Sentence structure is
then reconstructed using permutations of the gen-
erated bag of target words. We will also use this
type of features in our model.
One of the simplest models in the context of
lexical triggers is the IBM model 1 (Brown et
al., 1993) which captures lexical dependencies be-
tween source and target words. It can be seen
as a lexicon containing correspondents of transla-
tions of source and target words in a very broad
sense since the pairs are trained on the full sen-
tence level. The trigger-based lexicon model used
in this work follows the training procedure intro-
duced in (Hasan et al, 2008) and is integrated di-
rectly in the decoder instead of being applied in
n-best list reranking. The model is very close to
the IBM model 1 and can be seen as an extension
of it by taking another word into the condition-
ing part, i.e. the triggering items. Thus, instead
of p(f |e), it models p(f |e, e
?
). Furthermore, since
the second trigger can come from any part of the
sentence, there is a link to long-range monolin-
gual triggers as presented in (Tillmann and Ney,
1997) where a trigger language model was trained
using the EM algorithm and helped to reduce per-
plexities and word error rates in a speech recog-
nition experiment. In (Rosenfeld, 1996), another
approach was chosen to model monolingual trig-
gers using a maximum-entropy based framework.
Again, this adapted LM could improve speech
recognition performance significantly.
A comparison of a variant of the trigger-based
lexicon model applied in decoding and n-best list
reranking can be found in (Hasan and Ney, 2009).
In order to reduce the number of overall triplets,
the authors use the word alignments for fixing the
211
first trigger to the aligned target word. In general,
this constraint performs slightly worse than the un-
constrained variant used in this work, but allows
for faster training and decoding.
3 Extended Lexicon Models
In this section, we present the extended lexicon
models, how they are trained and integrated into
the phrase-based decoder.
3.1 Discriminative Lexicon Model
Discriminative models have been shown to outper-
form generative models on many natural language
processing tasks. For machine translation, how-
ever, the adaptation of these methods is difficult
due to the large space of possible translations and
the size of the training data that has to be used to
achieve significant improvements.
In this section, we propose a discriminative
word lexicon model that follows (Bangalore et al,
2007) and integrate it into the standard phrase-
based machine translation approach.
The core of our model is a classifier that pre-
dicts target words, given the words of the source
sentence. The structure of source as well as tar-
get sentence is neglected in this model. We do
not make any assumptions about the location of
the words in the sentence. This is useful in many
cases, as words and morphology can depend on in-
formation given at other positions in the sentence.
An example would be the character? in Chinese
that indicates a completed or past action and does
not need to appear close to the verb.
We model the probability of the set of target
words in a sentence e given the set of source words
f . For each word in the target vocabulary, we can
calculate a probability for being or not being in-
cluded in the set. The probability of the whole set
then is the product over the entire target vocabu-
lary V
E
:
P (e|f) =
?
e?e
P (e
+
|f) ?
?
e?V
E
\e
P (e
?
|f) (1)
For notational simplicity, we use the event e
+
when the target word e is included in the target
sentence and e
?
if not. We model the individual
factors p(e|f) of the probability in Eq. 1 as a log-
linear model using the source words from f as bi-
nary features
?(f, f) =
{
1 if f ? f
0 else
(2)
and feature weights ?
f,?
:
P (e
+
|f) =
exp
(
?
f?f
?
f,e
+ ?(f, f)
)
?
e?{e
+
,e
?
}
exp
(
?
f?f
?
f,e
?(f, f)
)
(3)
Subsequently, we will call this model discrimina-
tive word lexicon (DWL).
Modeling the lexicon on sets and not on se-
quences has two reasons. Phrase-based MT along
with n-gram language models is strong at predict-
ing sequences but only uses information from a lo-
cal context. By using global features and predict-
ing words in a non-local fashion, we can augment
the strong local decisions from the phrase-based
systems with sentence-level information.
For practical reasons, translating from a set to
a set simplifies the parallelization of the training
procedure. The classifiers for the target words can
be trained separately as explained in the following
section.
3.1.1 Training
Common classification tasks have a relatively
small number of classes. In our case, the num-
ber of classes is the size of the target vocabulary.
For large translation tasks, this is in the range of a
hundred thousand classes. It is far from what con-
ventional out-of-the-box classifiers can handle.
The discriminative word lexicon model has the
convenient property that we can train a separate
model for each target word making paralleliza-
tion straightforward. Discussions about possible
classifiers and the choice of regularization can
be found in (Bangalore et al, 2007). We used
the freely available MegaM Toolkit
1
for training,
which implements the L-BFGS method (Byrd et
al., 1995). Regularization is done using Gaussian
priors. We performed 100 iterations of the train-
ing algorithm for each word in the target vocabu-
lary. This results in a large number of classifiers to
be trained. For the Arabic-English data (cf. Sec-
tion 4), the training took an average of 38 seconds
per word. No feature cutoff was used.
3.1.2 Decoding
In search, we compute the model probabilities as
an additional model in the log-linear model com-
bination of the phrase-based translation approach.
To reduce the memory footprint and startup time
of the decoding process, we reduced the number of
1
http://www.cs.utah.edu/
?
hal/megam/
212
parameters by keeping only large values ?
f,e
since
smaller values tend to have less effect on the over-
all probability. In experiments we determined that
we could safely reduce the size of the final model
by a factor of ten without losing predictive power.
In search, we compute the model probabilities as
an additional model in the log-linear combination.
When scoring hypotheses from the phrase-based
system, we see the translation hypothesis as the
set of target words that are predicted. Words from
the target vocabulary which are not included in
the hypothesis are not part of the set. During the
search process, however, we also have to score in-
complete hypotheses where we do not know which
words will not be included. This problem is cir-
cumvented by rewriting Eq. 1 as
P (e|f) =
?
e?V
E
P (e
?
|f) ?
?
e?e
P (e
+
|f)
P (e
?
|f)
.
The first product is constant given a source sen-
tence and therefore does not affect the search. Us-
ing the model assumption from Eq. 3, we can fur-
ther simplify the computation and compute the
model score entirely in log-space which is numer-
ically stable even for large vocabularies. Exper-
iments showed that using only the first factor of
Eq. 1 is sufficient to obtain good results.
In comparison with the translation model from
(Bangalore et al, 2007) where a threshold on the
probability is used to determine which words are
included in the target sentence, our approach relies
on the phrase model to generate translation candi-
dates. This has several advantages: The length of
the translation is determined by the phrase model.
Words occurring multiple times in the translation
do not have to be explicitly modeled. In (Banga-
lore et al, 2007), repeated target words are treated
as distinct classes.
The main advantage of the integration being
done in a way as presented here is that the phrase
model and the discriminative word lexicon model
are complementary in the way they model the
translation. While the phrase model is good in
predicting translations in a local context, the dis-
criminative word lexicon model is able to predict
global aspects of the sentence like tense or vocabu-
lary changes in questions. While the phrase model
is closely tied to the structure of word and phrase
alignments, the discriminative word lexicon model
completely disregards the structure in source and
target sentences.
3.2 Trigger-based Lexicon Model
The triplets of the trigger-based lexicon model,
i.e. p(e|f, f
?
), are composed of two words in the
source language triggering one target language
word. We chose this inverse direction since it
can be integrated directly into the decoder and,
thus, does not rely on a two-pass approach us-
ing reranking, as it is the case for (Hasan et al,
2008). The triggers can originate from words of
the whole source sentence, also crossing phrase
boundaries of the conventional bilingual phrase
pairs. The model is symmetric though, mean-
ing that the order of the triggers is not relevant,
i.e. (f, f
?
? e) = (f
?
, f ? e). Nevertheless,
the model is able to capture long-distance effects
such as verb splits or adjustments to lexical choice
of the target word given the topic-triggers of the
source sentence. In training, we determine the
probability of a target sentence e
I
1
given the source
sentence f
J
1
within the model by
p(e
I
1
|f
J
1
) =
I
?
i=1
p(e
i
|f
J
1
)
=
I
?
i=1
2
J(J + 1)
J
?
j=0
J
?
j
?
=j+1
p(e
i
|f
j
, f
j
?
), (4)
where f
0
denotes the empty word and, thus, for
f
j
= ?, allows for modeling the conventional (in-
verse) IBM model 1 lexical probabilities as well.
Since the second trigger f
j
?
always starts right of
the current first trigger, the model is symmetric
and does not need to look at all trigger pairs. Eq. 4
is used in the iterative EM training on all sentence
pairs of the training data which is described in
more detail in the following.
3.2.1 Training
For training the trigger-based lexicon model, we
apply the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). The goal is to max-
imize the log-likelihood F
trip
of this model for
a given bilingual training corpus {(f
J
n
1
, e
I
n
1
)}
N
1
consisting of N sentence pairs:
F
trip
:=
N
?
n=1
log p(e
I
n
1
|f
J
n
1
),
where I
n
and J
n
are the lengths of the n-th tar-
get and source sentence, respectively. An aux-
iliary function Q(?; ??) is defined based on F
trip
213
where ?? is the updated estimate within an itera-
tion which is to be derived from the current esti-
mate ?. Here, ? stands for the entire set of model
parameters, i.e. the set of all {?(e|f, f
?
)} with the
constraint
?
e
?(e|f, f
?
) = 1. The accumulators
?(?) are therefore iteratively trained on the train-
ing data by using the current estimate, i.e. deriv-
ing the expected value (E-step), and maximizing
their likelihood afterwards to reestimate the distri-
bution. Thus, the perplexity of the training data is
reduced in each iteration.
3.2.2 Decoding
In search, we can apply this model directly when
scoring bilingual phrase pairs. Given a trained
model for p(e|f, f
?
), we compute the feature score
h
trip
(?) of a phrase pair (e?,
?
f) as
h
trip
(e?,
?
f, f
J
0
) = (5)
?
?
i
log
(
2
J ? (J + 1)
?
j
?
j
?
>j
p(e?
i
|f
j
, f
j
?
)
)
,
where i moves over all target words in the phrase
e?, the second sum selects all source sentence
words f
J
0
including the empty word, and j
?
> j
incorporates the rest of the source sentence right of
the first trigger. We take negative log-probabilities
and normalize to obtain the final score (represent-
ing costs) for the given phrase pair. Note that in
search, we can only use this direction, p(e|f, f
?
),
since the whole source sentence is available for
triggering effects whereas not all target words
have been generated so far, as it would be neces-
sary for the standard direction, p(f |e, e
?
).
Due to the enormous number of triplets, we
trained the model on a subset of the overall train-
ing data. The subcorpus, mainly consisting of
newswire articles, contained 1.4M sentence pairs
with 32.3M running words on the English side.
We trained two versions of the triplet lexicon, one
using 4 EM iterations and another one that was
trained for 10 EM iterations. Due to trimming
of triplets with small probabilities after each it-
eration, the version based on 10 iterations was
slightly smaller, having 164 million triplets but
also performed slightly worse. Thus, for the ex-
periments, we used the version based on 4 itera-
tions which contained 291 million triplets.
Note that decoding with this model can be quite
efficient if caching is applied. Since the given
source sentence does not change, we have to cal-
culate p(e|f, f
?
) for each e only once and can re-
train (C/E) test08 (NW/WT)
Sent. pairs 9.1M 480 490
Run. words 259M/300M 14.8K 12.3K
Vocabulary 357K/627K 3.6K 3.2K
Table 1: GALE Chinese-English corpus statistics
including two test sets: newswire and web text.
train C/E ? A/E nist08 C/A
Sent. pairs 7.3M 4.6M 1357
Words (M) 185/196 142/139 36K/46K
Vocab. (K) 163/265 351/361 6.4K/9.6K
Table 2: NIST Chinese-English and Arabic-
English corpus statistics including the official
2008 test sets.
trieve the probabilities from the cache for consec-
utive scorings of the same target word e. This sig-
nificantly speeds up the decoding process.
4 Experimental Evaluation
In this section we evaluate our lexicon models on
the GALE Chinese-English task for newswire and
web text translation and additionally on the of-
ficial NIST 2008 task for both Chinese-English
and Arabic-English. The baseline system was
built using a state-of-the art phrase-based MT sys-
tem (Zens and Ney, 2008). We use the standard
set of models with phrase translation probabilities
for source-to-target and target-to-source direction,
smoothing with lexical weights, a word and phrase
penalty, distance-based and lexicalized reordering
and a 5-gram (GALE) or 6-gram (NIST) target
language model.
We used training data provided by the Linguis-
tic Data Consortium (LDC) consisting of 9.1M
parallel Chinese-English sentence pairs of vari-
ous domains for GALE (cf. Table 1) and smaller
amounts of data for the NIST systems (cf. Ta-
ble 2). The DWL and Triplet models were inte-
grated into the decoder as presented in Section 3.
For the GALE development and test set, we sep-
arated the newswire and web text parts and did
separate parameter tuning for each genre using
the corresponding development set which consists
of 485 sentences for newswire texts and 533 sen-
tences of web text. The test set has 480 sentences
for newswire and 490 sentences for web text. For
NIST, we tuned on the official 2006 eval set and
used the 2008 evaluation set as a blind test set.
214
GALE NW WT
test08 BLEU TER BLEU TER
[%] [%] [%] [%]
Baseline 32.3 59.38 25.3 64.40
DWL 33.1 58.90 26.2 63.75
Triplet 32.9 58.59 26.2 64.20
DWL+Trip. 33.3 58.23 26.3 63.87
Table 3: Results on the GALE Chinese-English
test set for the newswire and web text setting
(case-insensitive evaluation).
4.1 Translation Results
The translation results on the two GALE test
sets are shown in Table 3 for newswire and web
text. Both the discriminative word lexicon and the
triplet lexicon can individually improve the base-
line by approximately +0.6?0.9% BLEU and -0.5?
0.8% TER. For the combination of both lexicons
on the newswire setting, we observe only a slight
improvement on BLEU but also an additional
boost in TER reduction, arriving at +1% BLEU
and -1.2% TER. For web text, the findings are sim-
ilar: The combination of the discriminative and
trigger-based lexicons yields +1% BLEU and de-
creases TER by -0.5%.
We compared these results against an inverse
IBM model 1 but the results were inconclusive
which is consistent with the results presented in
(Och et al, 2004) where no improvements were
achieved using p(e|f). In our case, inverse IBM1
improves results by 0.2?0.4% BLEU on the devel-
opment set but does not show the same trend on
the test sets. Furthermore, combining IBM1 with
DWL or Triplets often even degraded the transla-
tion results, e.g. only 32.8% BLEU was achieved
on newswire for a combination of the IBM1, DWL
and Triplet model. In contrast, combinations of
the DWL and Triplet model did not degrade per-
formance and could benefit from each other.
In addition to the automatic scoring, we also
did a randomized subjective evaluation where the
hypotheses of the baseline was compared against
the hypotheses generated using the discrimina-
tive word lexicon and triplet models. We evalu-
ated 200 sentences from newswire and web text.
In 80% of the evaluated sentences, the improved
models were judged equal or better than the base-
line.
We tested the presented lexicon models also on
another large-scale system, i.e. NIST, for two lan-
NIST Chinese-Eng. Arabic-Eng.
nist08 BLEU TER BLEU TER
[%] [%] [%] [%]
Baseline 26.8 65.11 42.0 50.55
DWL 27.6 63.56 42.4 50.01
Triplet 27.7 63.60 42.9 49.76
DWL+Trip. 27.9 63.56 43.0 49.15
Table 4: Results on the test sets for the NIST 2008
Chinese-English and Arabic-English task (case-
insensitive evaluation).
guage pairs, namely Chinese-English and Arabic-
English. Interestingly, the results obtained for
Arabic-English are similar to the findings for
Chinese-English, as can be seen in Table 4. The
overall improvements for this language pair are
+1% BLEU and -1.4% TER. In contrast to the
GALE Chinese-English task, the triplet lexicon
model for the Arabic-English language pair per-
forms slightly better than the discriminative word
lexicon.
These results strengthen the claim that the pre-
sented models are capable of improving lexical
choice of the MT system. In the next section, we
discuss the observed effects and analyze our re-
sults in more detail.
5 Discussion
In terms of automatic evaluation measures, the re-
sults indicate that it is helpful to incorporate the
extended lexicon models into the search process.
In this section, we will analyze some more details
of the models and take a look at the lexical choice
they make and what differentiates them from the
baseline models. In Table 5, we picked an ex-
ample sentence from the GALE newswire test set
and show the different hypotheses produced by our
system. As can be seen, the baseline does not
produce the present participle of the verb restore
which makes the sentence somewhat hard to un-
derstand. Both the discriminative and the trigger-
based lexicon approach are capable of generating
this missing information, i.e. the correct use of
restoring. Figure 1 gives an example how discon-
tinuous triggers affect the word choice on the tar-
get side. Two cases are depicted where high proba-
bilities of triplets including emergency and restor-
ing on the target side influence the overall hypoth-
esis selection. The non-local modeling advantages
of the triplet model can be observed as well: The
215
?? , ?? ?? ? ?? ?? ?? ?? ?? .source
target [...] the emergency rescue group is [...] restoring  the ventilation system.
p(restoring | ??,  ? ) = 0.1572p(emergency | ??, ??) = 0.3445
Figure 1: Triggering effect for the example sentence using the triplet lexicon model. The Chinese source
sentence is shown in its segmented form. Two triplets are highlighted that have high probability and
favor the target words emergency and restoring.
Figure 2: Ranking of words for the example sentence for IBM1, Triplet and DWL model. Ranks are
sorted at IBM1, darker colors indicate higher probabilities within the model.
triggering events do not need to be located next
to each other or within a given phrase pair. They
move across the whole source sentence, thus al-
lowing for capturing of long-range dependencies.
Table 6 shows the top ten content words that are
predicted by the two models, discriminative word
lexicon and triplet lexicon model. IBM model 1
ranks are indicated by subscripts in the column
of the triplet model. Although the triplet model
is similar to IBM1, we observe differences in the
word lists. Comparing this to the visualization of
the probability distribution for the example sen-
tence, cf. Figure 2, we argue that, although the
IBM1 and Triplet distributions look similar, the
triplet model is sharper and favors words such as
the ones in Table 6, resulting in different word
choice in the translation process. In contrast, the
DWL approach gives more distinct probabilities,
selecting content words that are not chosen by the
other models.
Table 7 shows an example from the web text
test set. Here, the baseline hypothesis contains
an incorrect word, anna, which might have been
mistaken for the name ying. Interestingly, the hy-
potheses of the DWL lexicon and the combina-
tion of DWL and Triplet contain the correct con-
tent word remarks. The triplet model makes an er-
ror by selecting music, an artifact that might come
from words that co-occur frequently with the cor-
responding Chinese verb to listen, i.e. ? , in the
data. Although the TER score of the baseline is
better than the one for the alternative models for
this particular example, we still think that the ob-
served effects show how our models help produc-
ing different hypotheses that might lead to subjec-
tively better translations.
An Arabic-English translation example is
shown in Table 8. Here, the term incidents of mur-
der in apartments was chosen over the baseline?s
killings inside the flats. Both translations are un-
derstandable and the difference in the wording is
only based on synonyms. The translation using
the discriminative and trigger-based lexicons bet-
ter matches the reference translation and, thus, re-
flects a better lexical choice of the content words.
6 Conclusion
We have presented two lexicon models that use
global source sentence context and are capable
of predicting context-specific target words. The
models have been directly integrated into the de-
coder and have shown to improve the translation
quality of a state-of-the-art phrase-based machine
translation system. The first model was a dis-
criminative word lexicon that uses sentence-level
features to predict if a word from the target vo-
cabulary should be included in the translation or
not. The second model was a trigger-based lexi-
216
Source ?? , ?? ?? ? ?? ??
?????? .
Baseline at present, the accident and rescue
teams are currently emergency re-
covery ventilation systems.
DWL at present, the emergency rescue
teams are currently restoring the
ventilation system.
Triplet at present, the emergency rescue
group is in the process of restoring
the ventilation system.
DWL
+Triplet
at present, the accident emergency
rescue teams are currently restor-
ing the ventilation system.
Reference right now, the accident emergency
rescue team is making emergency
repair on the ventilation system.
Table 5: Translation example from the GALE
newswire test set, comparing the baseline and the
extended lexicon models given a reference trans-
lation. The Chinese source sentence is presented
in its segmented form.
con that uses triplets to model long-range depen-
dencies in the data. The source word triggers can
move across the whole sentence and capture the
topic of the sentence and incorporate more fine-
grained lexical choice of the target words within
the decoder.
Overall improvements are up to +1% in BLEU
and -1.5% in TER on large-scale systems for
Chinese-English and Arabic-English. Compared
to the inverse IBM model 1 which did not yield
consistent improvements, the presented models
are valuable additional features in a phrase-based
statistical machine translation system. We will test
this setup for other language pairs and expect that
languages like German where long-distance ef-
fects are common can benefit from these extended
lexicon models.
In future work, we plan to extend the discrimi-
native word lexicon model in two directions: ex-
tending context to the document level and feature
engineering. For the trigger-based model, we plan
to investigate more model variants. It might be
interesting to look at cross-lingual trigger mod-
els such as p(f |e, f
?
) or constrained variants like
p(f |e, e
?
) with pos(e
?
) < pos(e), i.e. the second
trigger coming from the left context within a sen-
tence which has already been generated. These
DWL Triplet
emergency 0.894 emergency
1
0.048
currently 0.330 system
2
0.032
current 0.175 rescue
8
0.027
emergencies 0.133 accident
3
0.022
present 0.133 ventilation
7
0.021
accident 0.119 work
33
0.021
recovery 0.053 present
5
0.011
group 0.046 currently
9
0.010
dealing 0.042 rush
60
0.010
ventilation 0.034 restoration
31
0.009
Table 6: The top 10 content words predicted by
each model for the GALE newswire example sen-
tence. Original ranks for the related IBM model 1
are given as subscripts for the triplet model.
Source ?? ???? , ?????
? .
Baseline i have listened to anna, happy and
laugh.
DWL i have listened to the remarks,
happy and laugh.
Triplet i have listened to the music, a roar
of laughter.
DWL
+Triplet
i have listened to the remarks,
happy and laugh.
Reference hearing ying?s remark, i laughed
aloud happily.
Table 7: Translation example from the GALE web
text test set. In this case, the baseline has a bet-
ter TER but we can observe a corrected content
word (remark) for the extended lexicon models.
The Chinese source sentence is shown in its seg-
mented form.
extensions could be integrated directly in search
as well and would enable the system to combine
both directions (standard and inverse) to some ex-
tent which was previously shown to help when ap-
plying the standard direction p(f |e, e
?
) as an addi-
tional reranking step, cf. (Hasan and Ney, 2009).
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023, and was partly realized as part of
the Quaero Programme, funded by OSEO, French
State agency for innovation.
The authors would like to thank Christian Buck
217
Source
	
?j
.
??@ ?

I
	
?Q?

K ?



?? @

HBAm
?
'@
	
?? @XY?

HQ?

?
	
 Y

?

?K


X????@
	
?j??@
	
??K
.

I
	
KA? ?
. A?
Q



	
? ?

?

?

??@ ?
	
g@X ?

J

?? @

HX@?k
	
??K
.
??
	
Y? ? P
Q

.
?
	
??X
Baseline some saudi newspapers have published a number of cases that had been subjected to
imprisonment without justification, as well as some killings inside the flats and others.
DWL
+Triplet
some of the saudi newspapers have published a number of cases which were subjected
to imprisonment without justification, as well as some incidents of murder in apartments
and others.
Reference some saudi newspapers have published a number of cases in which people were unjusti-
fiably imprisoned, as well as some incidents of murder in apartments and elsewhere.
Table 8: Translation example from the NIST Arabic-English test set. The DWL and Triplet models
improve lexical word choice by favoring incidents of murder in apartments instead of killings inside the
flats. The Arabic source is shown in its segmented form.
and Juri Ganitkevitch for their help training the ex-
tended lexicon models.
References
S. Bangalore, P. Haffner, and S. Kanthak. 2006. Se-
quence classification for machine translation. In
Ninth International Conf. on Spoken Language Pro-
cessing, Interspeech 2006 ? ICSLP, pages 1722?
1725, Pitsburgh, PA, September.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statis-
tical machine translation through global lexical se-
lection and sentence reconstruction. In 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 152?159, Prague, Czech Republic,
June.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?312, June.
R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. 1995. A
limited memory algorithm for bound constrained op-
timization. SIAM Journal on Scientific Computing,
16(5):1190?1208.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL 2007),
Prague, Czech Republic, June.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In 45th Annual Meeting of the Association
of Computational Linguistics, pages 33?40, Prague,
Czech Republic, June.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1?22.
S. Hasan and H. Ney. 2009. Comparison of extended
lexicon models in search and rescoring for SMT. In
NAACL HLT 2009, Companion Volume: Short Pa-
pers, pages 17?20, Boulder, Colorado, June.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr?es-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In EMNLP, pages 372?381, Honolulu,
Hawaii, October.
A. Ittycheriah and S. Roukos. 2007. Direct translation
model 2. In HLT-NAACL 2007: Main Conference,
pages 57?64, Rochester, New York, April.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. pages 161?168, Boston, MA, May.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10(3):187?228.
C. Tillmann and H. Ney. 1997. Word triggers
and the EM algorithm. In Proc. Special Interest
Group Workshop on Computational Natural Lan-
guage Learning (ACL), pages 117?124, Madrid,
Spain, July.
I. Garc??a Varea, F. J. Och, H. Ney, and F. Casacu-
berta. 2001. Refined lexicon models for statistical
machine translation using a maximum entropy ap-
proach. In ACL ?01: 39th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 204?
211, Morristown, NJ, USA.
S. Venkatapathy and S. Bangalore. 2007. Three
models for discriminative machine translation us-
ing global lexical selection and sentence reconstruc-
tion. In SSST, NAACL-HLT 2007 / AMTA Workshop
on Syntax and Structure in Statistical Translation,
pages 96?102, Rochester, New York, April.
R. Zens and H. Ney. 2008. Improvements in dynamic
programming beam search for phrase-based statis-
tical machine translation. In International Work-
shop on Spoken Language Translation, Honolulu,
Hawaii, October.
218
347
348
349
350
351
352
353
354
387
388
389
390
391
392
393
394
11
12
13
14
15
16
17
18
Computing Consensus Translation from Multiple Machine Translation
Systems Using Enhanced Hypotheses Alignment
Evgeny Matusov, Nicola Ueffing, Hermann Ney
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University, Aachen, Germany.
{matusov,ueffing,ney}@informatik.rwth-aachen.de
Abstract
This paper describes a novel method for
computing a consensus translation from
the outputs of multiple machine trans-
lation (MT) systems. The outputs are
combined and a possibly new transla-
tion hypothesis can be generated. Simi-
larly to the well-established ROVER ap-
proach of (Fiscus, 1997) for combining
speech recognition hypotheses, the con-
sensus translation is computed by voting
on a confusion network. To create the con-
fusion network, we produce pairwise word
alignments of the original machine trans-
lation hypotheses with an enhanced sta-
tistical alignment algorithm that explicitly
models word reordering. The context of a
whole document of translations rather than
a single sentence is taken into account to
produce the alignment.
The proposed alignment and voting ap-
proach was evaluated on several machine
translation tasks, including a large vocab-
ulary task. The method was also tested in
the framework of multi-source and speech
translation. On all tasks and conditions,
we achieved significant improvements in
translation quality, increasing e. g. the
BLEU score by as much as 15% relative.
1 Introduction
In this work we describe a novel technique for
computing a consensus translation from the out-
puts of multiple machine translation systems.
Combining outputs from different systems
was shown to be quite successful in automatic
speech recognition (ASR). Voting schemes like
the ROVER approach of (Fiscus, 1997) use edit
distance alignment and time information to cre-
ate confusion networks from the output of several
ASR systems.
Some research on multi-engine machine trans-
lation has also been performed in recent years.
The most straightforward approaches simply se-
lect, for each sentence, one of the provided hy-
potheses. The selection is made based on the
scores of translation, language, and other mod-
els (Nomoto, 2004; Paul et al, 2005). Other
approaches combine lattices or N -best lists from
several different MT systems (Frederking and
Nirenburg, 1994). To be successful, such ap-
proaches require compatible lattices and compa-
rable scores of the (word) hypotheses in the lat-
tices. However, the scores of most statistical ma-
chine translation (SMT) systems are not normal-
ized and therefore not directly comparable. For
some other MT systems (e.g. knowledge-based
systems), the lattices and/or scores of hypotheses
may not be even available.
(Bangalore et al, 2001) used the edit distance
alignment extended to multiple sequences to con-
struct a confusion network from several transla-
tion hypotheses. This algorithm produces mono-
tone alignments only (i. e. allows insertion, dele-
tion, and substitution of words); it is not able to
align translation hypotheses with significantly dif-
ferent word order. (Jayaraman and Lavie, 2005)
try to overcome this problem. They introduce a
method that allows non-monotone alignments of
words in different translation hypotheses for the
same sentence. However, this approach uses many
heuristics and is based on the alignment that is per-
formed to calculate a specific MT error measure;
the performance improvements are reported only
in terms of this measure.
33
Here, we propose an alignment procedure that
explicitly models reordering of words in the hy-
potheses. In contrast to existing approaches, the
context of the whole document rather than a sin-
gle sentence is considered in this iterative, unsu-
pervised procedure, yielding a more reliable align-
ment.
Based on the alignment, we construct a con-
fusion network from the (possibly reordered)
translation hypotheses, similarly to the approach
of (Bangalore et al, 2001). Using global system
probabilities and other statistical models, the vot-
ing procedure selects the best consensus hypoth-
esis from the confusion network. This consen-
sus translation may be different from the original
translations.
This paper is organized as follows. In Section 2,
we will describe the computation of consensus
translations with our approach. In particular, we
will present details of the enhanced alignment and
reordering procedure. A large set of experimental
results on several machine translation tasks is pre-
sented in Section 3, which is followed by a sum-
mary.
2 Description of the Algorithm
The proposed approach takes advantage of mul-
tiple translations for a whole test corpus to com-
pute a consensus translation for each sentence in
this corpus. Given a single source sentence in the
test corpus, we combine M translation hypothe-
ses E1, . . . , EM from M MT engines. We first
choose one of the hypotheses Em as the primary
one. We consider this primary hypothesis to have
the ?correct? word order. We then align and re-
order the other, secondary hypotheses En(n =
1, ..., M ;n 6= m) to match this word order. Since
each hypothesis may have an acceptable word or-
der, we let every hypothesis play the role of the
primary translation once, and thus align all pairs
of hypotheses (En, Em); n 6= m.
In the following subsections, we will explain
the word alignment procedure, the reordering ap-
proach, and the construction of confusion net-
works.
2.1 Statistical Alignment
The word alignment is performed in analogy to the
training procedure in SMT. The difference is that
the two sentences that have to be aligned are in the
same language. We consider the conditional prob-
ability Pr(En|Em) of the event that, given Em,
another hypothesis En is generated from the Em.
Then, the alignment between the two hypotheses
is introduced as a hidden variable:
Pr(En|Em) =
?
A
Pr(En,A|Em)
This probability is then decomposed into the align-
ment probability Pr(A|Em) and the lexicon prob-
ability Pr(En|A, Em):
Pr(En,A|Em) = Pr(A|Em) ? Pr(En|A, Em)
As in statistical machine translation, we make
modelling assumptions. We use the IBM Model 1
(Brown et al, 1993) (uniform distribution) and the
Hidden Markov Model (HMM, first-order depen-
dency, (Vogel et al, 1996)) to estimate the align-
ment model. The lexicon probability of a sentence
pair is modelled as a product of single-word based
probabilities of the aligned words.
The training corpus for alignment is created
from a test corpus of N sentences (usually a few
hundred) translated by all of the involved MT en-
gines. However, the effective size of the training
corpus is larger than N , since all pairs of different
hypotheses have to be aligned. Thus, the effective
size of the training corpus is M ? (M ?1) ?N . The
single-word based lexicon probabilities p(en|em)
are initialized with normalized lexicon counts col-
lected over the sentence pairs (En, Em) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring equal words, i. e.
if en is the same word as em. In addition, we add
a fraction of a count for words with identical pre-
fixes. The initialization could be furthermore im-
proved by using word classes, part-of-speech tags,
or a list of synonyms.
The model parameters are trained iteratively in
an unsupervised manner with the EM algorithm
using the GIZA++ toolkit (Och and Ney, 2003).
The training is performed in the directions En ?
Em and Em ? En. The updated lexicon tables
from the two directions are interpolated after each
iteration.
The final alignments are determined using cost
matrices defined by the state occupation probabil-
ities of the trained HMM (Matusov et al, 2004).
The alignments are used for reordering each sec-
ondary translation En and for computing the con-
fusion network.
34
Figure 1: Example of creating a confusion network frommonotone one-to-one word alignments (denoted
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an ?-arc in the corresponding part of the confusion network.
1. would you like coffee or tea
original 2. would you have tea or coffee
hypotheses 3. would you like your coffee or
4. I have some coffee tea would you like
alignment would|would you|you have|like coffee|coffee or|or tea|tea
and would|would you|you like|like your|$ coffee|coffee or|or $|tea
reordering I|$ would|would you|you like|like have|$ some|$ coffee|coffee $|or tea|tea
$ would you like $ $ coffee or tea
confusion $ would you have $ $ coffee or tea
network $ would you like your $ coffee or $
I would you like have some coffee $ tea
2.2 Word Reordering
The alignment between En and the primary hy-
pothesis Em used for reordering is computed as a
function of words in the secondary translation En
with minimal costs, with an additional constraint
that identical words in En can not be all aligned to
the same word in Em. This constraint is necessary
to avoid that reordered hypotheses with e. g. multi-
ple consecutive articles ?the? would be produced if
fewer articles were used in the primary hypothesis.
The new word order for En is obtained through
sorting the words in En by the indices of the words
in Em to which they are aligned. Two words in
En which are aligned to the same word in Em are
kept in the original order. After reordering each
secondary hypothesis En, we determine M ? 1
monotone one-to-one alignments between Em and
En, n = 1, . . . ,M ; n 6= m. In case of many-to-
one connections of words in En to a single word in
Em, we only keep the connection with the lowest
alignment costs. The one-to-one alignments are
convenient for constructing a confusion network
in the next step of the algorithm.
2.3 Building Confusion Networks
Given the M?1 monotone one-to-one alignments,
the transformation to a confusion network as de-
scribed by (Bangalore et al, 2001) is straightfor-
ward. It is explained by the example in Figure 1.
Here, the original 4 hypotheses are shown, fol-
lowed by the alignment of the reordered secondary
hypotheses 2-4 with the primary hypothesis 1. The
alignment is shown with the | symbol, and the
words of the primary hypothesis are to the right
of this symbol. The symbol $ denotes a null align-
ment or an ?-arc in the corresponding part of the
confusion network, which is shown at the bottom
of the figure.
Note that the word ?have? in translation 2 is
aligned to the word ?like? in translation 1. This
alignment is acceptable considering the two trans-
lations alone. However, given the presence of the
word ?have? in translation 4, this is not the best
alignment. Yet the problems of this type can in
part be solved by the proposed approach, since ev-
ery translation once plays the role of the primary
translation. For each sentence, we obtain a total of
M confusion networks and unite them in a single
lattice. The consensus translation can be chosen
among different alignment and reordering paths in
this lattice.
The ?voting? on the union of confusion net-
works is straightforward and analogous to the
ROVER system. We sum up the probabilities of
the arcs which are labeled with the same word
and have the same start and the same end state.
These probabilities are the global probabilities as-
signed to the different MT systems. They are man-
ually adjusted based on the performance of the in-
volvedMT systems on a held-out development set.
In general, a better consensus translation can be
produced if the words hypothesized by a better-
performing system get a higher probability. Ad-
ditional scores like word confidence measures can
be used to score the arcs in the lattice.
2.4 Extracting Consensus Translation
In the final step, the consensus translation is ex-
tracted as the best path from the union of confu-
35
Table 1: Corpus statistics of the test corpora.
BTEC IWSLT04 BTEC CSTAR03 EPPS TC-STAR
Chinese Japanese English Italian English Spanish English
Sentences 500 506 1 073
Running Words 3 681 4 131 3 092 3 176 2 942 2 889 18 896 18 289
Distinct Words 893 979 1 125 1 134 1 028 942 3 302 3 742
sion networks. Note that the extracted consensus
translation can be different from the original M
translations. Alternatively, the N -best hypothe-
ses can be extracted for rescoring by additional
models. We performed experiments with both ap-
proaches.
Since M confusion networks are used, the lat-
tice may contain two best paths with the same
probability, the same words, but different word
order. We extended the algorithm to favor more
well-formed word sequences. We assign a higher
probability to each arc of the primary (unre-
ordered) translation in each of the M confusion
networks. Experimentally, this extension im-
proved translation fluency on some tasks.
3 Experimental Results
3.1 Corpus Statistics
The alignment and voting algorithm was evaluated
on both small and large vocabulary tasks. Initial
experiments were performed on the IWSLT 2004
Chinese-English and Japanese-English tasks (Ak-
iba et al, 2004). The data for these tasks come
from the Basic Travel Expression corpus (BTEC),
consisting of tourism-related sentences. We com-
bined the outputs of several MT systems that had
officially been submitted to the IWSLT 2004 eval-
uation. Each system had used 20K sentence pairs
(180K running words) from the BTEC corpus for
training.
Experiments with translations of automatically
recognized speech were performed on the BTEC
Italian-English task (Federico, 2003). Here, the
involved MT systems had used about 60K sen-
tence pairs (420K running words) for training.
Finally, we also computed consensus translation
from some of the submissions to the TC-STAR
2005 evaluation campaign (TC-STAR, 2005). The
TC-STAR participants had submitted translations
of manually transcribed speeches from the Euro-
pean Parliament Plenary Sessions (EPPS). In our
experiments, we used the translations from Span-
Table 2: Improved translation results for the con-
sensus translation computed from 5 translation
outputs on the Chinese-English IWSLT04 task.
BTEC WER PER BLEU
Chinese-English [%] [%] [%]
worst single system ?04 58.3 46.6 34.6
best single system? ?04 54.6 42.6 40.3
consensus of 5 systems
from 2004 47.8 38.0 46.2
system (*) in 2005 50.3 40.5 45.1
ish to English. The MT engines for this task had
been trained on 1.2M sentence pairs (32M running
words).
Table 1 gives an overview of the test corpora,
on which the enhanced hypotheses alignment was
computed, and for which the consensus transla-
tions were determined. The official IWSLT04
test corpus was used for the IWSLT 04 tasks; the
CSTAR03 test corpus was used for the speech
translation task. The March 2005 test corpus of
the TC-STAR evaluation (verbatim condition) was
used for the EPPS task. In Table 1, the number of
running words in English is the average number of
running words in the hypotheses, from which the
consensus translation was computed; the vocabu-
lary of English is the merged vocabulary of these
hypotheses. For the BTEC IWSLT04 corpus, the
statistics for English is given for the experiments
described in Sections 3.3 and 3.5, respectively.
3.2 Evaluation Criteria
Well-established objective evaluation measures
like the word error rate (WER), position-
independent word error rate (PER), and the BLEU
score (Papineni et al, 2002) were used to assess
the translation quality. All measures were com-
puted with respect to multiple reference transla-
tions. The evaluation (as well as the alignment
training) was case-insensitive, without consider-
ing the punctuation marks.
36
3.3 Chinese-English Translation
Different applications of the proposed combina-
tion method have been evaluated. First, we fo-
cused on combining different MT systems which
have the same source and target language. The
initial experiments were performed on the BTEC
Chinese-English task. We combined translations
produced by 5 different MT systems. Table 2
shows the performance of the best and the worst of
these systems in terms of the BLEU score. The re-
sults for the consensus translation show a dramatic
improvement in translation quality. The word er-
ror rate is reduced e. g. from 54.6 to 47.8%. The
research group which had submitted the best trans-
lation in 2004 translated the same test set a year
later with an improved system. We compared
the consensus translation with this new translation
(last line of Table 2). It can be observed that the
consensus translation based on the MT systems
developed in 2004 is still superior to this 2005 sin-
gle system translation in terms of all error mea-
sures.
We also checked how many sentences in the
consensus translation of the test corpus are differ-
ent from the 5 original translations. 185 out of 500
sentences (37%) had new translations. Computing
the error measures on these sentences only, we ob-
served significant improvements in WER and PER
and a small improvement in BLEU with respect
to the original translations. Thus, the quality of
previously unseen consensus translations as gen-
erated from the original translations is acceptable.
In this experiment, the global system proba-
bilities for scoring the confusion networks were
tuned manually on a development set. The distri-
bution was 0.35, 0.25, 0.2, 0.1, 0.1, with 0.35 for
the words of the best single system and 0.1 for the
words of the worst single system. We observed
that the consensus translation did not change sig-
nificantly with small perturbations of these val-
ues. However, the relation between the proba-
bilities is very important for good performance.
No improvement can be achieved with a uniform
probability distribution ? it is necessary to penal-
ize translations of low quality.
3.4 Spanish-English Translation
The improvements in translation quality are
also significant on the TC-STAR EPPS Spanish-
English task. Here, we combined four different
systems which performed best in the TC-STAR
Table 3: Improved translation results for the con-
sensus translation computed from 4 translation
outputs on the Spanish-English TC-STAR task.
EPPS WER PER BLEU
Spanish-English [%] [%] [%]
worst single system 49.1 38.2 39.6
best single system 41.0 30.2 47.7
consensus of 4 systems 39.1 29.1 49.3
+ rescoring 38.8 29.0 50.7
2005 evaluation, see Table 3. Compared to the
best performing single system, the consensus hy-
pothesis reduces the WER from 41.0 to 39.1%.
This result is further improved by rescoring the
N -best lists derived from the confusion networks
(N=1000). For rescoring, a word penalty fea-
ture, the IBM Model 1, and a 4-gram target lan-
guage model were included. The linear interpola-
tion weights of these models and the score from
the confusion network were optimized on a sep-
arate development set with respect to word error
rate.
Table 4 gives examples of improved translation
quality by using the consensus translation as de-
rived from the rescored N -best lists.
3.5 Multi-source Translation
In the IWSLT 2004 evaluation, the English ref-
erence translations for the Chinese-English and
Japanese-English test corpora were the same, ex-
cept for a permutation of the sentences. Thus, we
could combine MT systems which have different
source and the same target language, performing
multi-source machine translation (described e. g.
by (Och and Ney, 2001)). We combined two
Japanese-English and two Chinese-English sys-
tems. The best performing system was a Japanese-
English system with a BLEU score of 44.7%, see
Table 5. By computing the consensus translation,
we improved this score to 49.6%, and also signifi-
cantly reduced the error rates.
To investigate the potential of the proposed ap-
proach, we generated the N -best lists (N = 1000)
of consensus translations. Then, for each sentence,
we selected the hypothesis in the N -best list with
the lowest word error rate with respect to the mul-
tiple reference translations for the sentence. We
then evaluated the quality of these ?oracle? trans-
lations with all error measures. In a contrastive
experiment, for each sentence we simply selected
37
Table 4: Examples of improved translation quality with the consensus translations on the Spanish-English
TC-STAR EPPS task (case-insensitive output).
best system I also authorised to committees to certain reports
consensus I also authorised to certain committees to draw up reports
reference I have also authorised certain committees to prepare reports
best system human rights which therefore has fought the european union
consensus human rights which the european union has fought
reference human rights for which the european union has fought so hard
best system we of the following the agenda
consensus moving on to the next point on the agenda
reference we go on to the next point of the agenda
Table 5: Multi-source translation: improvements
in translation quality when computing consen-
sus translation using the output of two Chinese-
English and two Japanese-English systems on the
IWSLT04 task.
BTEC Chinese-English WER PER BLEU
+ Japanese-English [%] [%] [%]
worst single system 58.0 41.8 39.5
best single system 51.3 38.6 44.7
consensus of 4 systems 44.9 33.9 49.6
Table 6: Consensus-based combination vs. se-
lection: potential for improvement (multi-source
translation, selection/combination of 4 translation
outputs).
BTEC Chinese-English WER PER BLEU
+ Japanese-English [%] [%] [%]
best single system 51.3 38.6 44.7
oracle selection 33.3 29.3 59.2
oracle consensus
(1000-best list) 27.0 22.8 64.2
the translation with the lowest WER from the orig-
inal 4 MT system outputs. Table 6 shows that the
potential for improvement is significantly larger
for the consensus-based combination of transla-
tion outputs than for simple selection of the best
translation1. In our future work, we plan to im-
prove the scoring of hypotheses in the confusion
networks to explore this large potential.
3.6 Speech Translation
Some state-of-the-art speech translation systems
can translate either the first best recognition hy-
1Similar ?oracle? results were observed on other tasks.
potheses or the word lattices of an ASR system. It
has been previously shown that word lattice input
generally improves translation quality. In practice,
however, the translation system may choose, for
some sentences, the paths in the lattice with many
recognition errors and thus produce inferior trans-
lations. These translations can be improved if we
compute a consensus translation from the output
of at least two different speech translation systems.
From each system, we take the translation of the
single best ASR output, and the translation of the
ASR word lattice.
Two different statistical MT systems capable of
translating ASR word lattices have been compared
by (Matusov and Ney, 2005). Both systems pro-
duced translations of better quality on the BTEC
Italian-English speech translation task when using
lattices instead of single best ASR output. We
obtained the output of each of the two systems
under each of these translation scenarios on the
CSTAR03 test corpus. The first-best recognition
word error rate on this corpus is 22.3%. The objec-
tive error measures for the 4 translation hypothe-
ses are given in Table 7. We then computed a con-
sensus translation of the 4 outputs with the pro-
posed method. The better performing word lattice
translations were given higher system probabili-
ties. With the consensus hypothesis, the word er-
ror rate went down from 29.5 to 28.5%. Thus, the
negative effect of recognition errors on the trans-
lation quality was further reduced.
4 Conclusions
In this work, we proposed a novel, theoretically
well-founded procedure for computing a possi-
bly new consensus translation from the outputs of
multiple MT systems. In summary, the main con-
38
Table 7: Improvements in translation quality on
the BTEC Italian-English task through comput-
ing consensus translations from the output of two
speech translation systems with different types of
source language input.
system input WER PER BLEU
[%] [%] [%]
2 correct text 23.3 19.3 65.6
1 a) single best 32.8 28.6 53.9
b) lattice 30.7 26.7 55.9
2 c) single best 31.6 27.5 54.7
d) lattice 29.5 26.1 58.2
consensus a-d 28.5 25.0 58.9
tributions of this work compared to previous ap-
proaches are as follows:
? The words of the original translation hy-
potheses are aligned in order to create a con-
fusion network. The alignment procedure ex-
plicitly models word reordering.
? A test corpus of translations generated by
each of the systems is used for the unsuper-
vised statistical alignment training. Thus, the
decision on how to align two translations of
a sentence takes the whole document context
into account.
? Large and significant gains in translation
quality were obtained on various translation
tasks and conditions.
? A significant improvement of translation
quality was achieved in a multi-source trans-
lation scenario. Here, we combined the
output of MT systems which have different
source and the same target language.
? The proposed method can be effectively ap-
plied in speech translation in order to cope
with the negative impact of speech recogni-
tion errors on translation accuracy.
An important feature of a real-life application of
the proposed alignment technique is that the lex-
icon and alignment probabilities can be updated
with each translated sentence and/or text. Thus,
the correspondence between words in different hy-
potheses and, consequently, the consensus transla-
tion can be improved overtime.
5 Acknowledgement
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-
0023. This work was also in part funded by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa,
M. Paul, and J. Tsujii. 2004. Overview of the
IWSLT04 Evaluation Campaign. Int. Workshop
on Spoken Language Translation, pp. 1?12, Kyoto,
Japan.
S. Bangalore, G. Bordel, G. Riccardi. 2001. Comput-
ing Consensus Translation from Multiple Machine
Translation Systems. IEEE Workshop on Automatic
Speech Recognition and Understanding, Madonna
di Campiglio, Italy.
P. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statisti-
cal Machine Translation. Computational Linguis-
tics, vol. 19(2):263?311.
J. G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). IEEE Workshop on
Automatic Speech Recognition and Understanding.
S. Jayaraman and A. Lavie. 2005. Multi-Engline Ma-
chine Translation Guided by Explicit Word Match-
ing. 10th Conference of the European Association
for Machine Translation, pp. 143-152, Budapest,
Hungary.
M. Federico 2003. Evaluation Frameworks for Speech
Translation Technologies. Proc. of Eurospeech,
pp. 377-380, Geneva, Switzerland.
R. Frederking and S. Nirenburg. 1994. Three Heads
are Better Than One. Fourth Conference on Applied
Natural Language Processing, Stuttgart, Germany.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. 20th Int. Conf. on Computational Linguistics,
pp. 219?225, Geneva, Switzerland.
E. Matusov and H. Ney. 2005. Phrase-based Trans-
lation of Speech Recognizer Word Lattices Using
Loglinear Model Combination. IEEE Workshop on
Automatic Speech Recognition and Understanding,
pp. 110-115, San Juan, Puerto-Rico.
T. Nomoto. 2004. Multi-Engine Machine Transla-
tion with Voted Language Model. 42nd Confer-
ence of the Association for Computational Linguis-
tics (ACL), pp. 494-501, Barcelona, Spain.
39
F. J. Och and H. Ney. 2001. Statistical Multi-Source
Translation. MT Summit VIII, pp. 253-258, Santi-
ago de Compostela, Spain.
F. J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. Annual Meeting of the ACL, pp.
311?318, Philadelphia, PA, USA.
M. Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma,
and E. Sumita. 2005. Nobody is Perfect: ATR?s
Hybrid Approach to Spoken Language Translation.
International Workshop on Spoken Language Trans-
lation, pp. 55-62, Pittsburgh, PA, USA.
TC-STAR Spoken Language Translation Progress Re-
port. 2005. http://www.tc-star.org/documents/
deliverable/Deliv D5 Total 21May05.pdf
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
Word Alignment in Statistical Translation. 16th Int.
Conf. on Computational Linguistics, pp. 836?841,
Copenhagen, Denmark.
40
CDER: Efficient MT Evaluation Using Block Movements
Gregor Leusch and Nicola Ueffing and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{leusch,ueffing,ney}@i6.informatik.rwth-aachen.de
Abstract
Most state-of-the-art evaluation measures
for machine translation assign high costs
to movements of word blocks. In many
cases though such movements still result
in correct or almost correct sentences. In
this paper, we will present a new eval-
uation measure which explicitly models
block reordering as an edit operation.
Our measure can be exactly calculated in
quadratic time.
Furthermore, we will show how some
evaluation measures can be improved
by the introduction of word-dependent
substitution costs. The correlation of the
new measure with human judgment has
been investigated systematically on two
different language pairs. The experimental
results will show that it significantly
outperforms state-of-the-art approaches in
sentence-level correlation. Results from
experiments with word dependent substi-
tution costs will demonstrate an additional
increase of correlation between automatic
evaluation measures and human judgment.
1 Introduction
Research in machine translation (MT) depends
heavily on the evaluation of its results. Espe-
cially for the development of an MT system,
an evaluation measure is needed which reliably
assesses the quality of MT output. Such a measure
will help analyze the strengths and weaknesses of
different translation systems or different versions
of the same system by comparing output at
the sentence level. In most applications of
MT, understandability for humans in terms of
readability as well as semantical correctness
should be the evaluation criterion. But as human
evaluation is tedious and cost-intensive, automatic
evaluation measures are used in most MT research
tasks. A high correlation between these automatic
evaluation measures and human evaluation is thus
desirable.
State-of-the-art measures such as BLEU (Pap-
ineni et al, 2002) or NIST (Doddington, 2002)
aim at measuring the translation quality rather
on the document level1 than on the level of
single sentences. They are thus not well-suited
for sentence-level evaluation. The introduction
of smoothing (Lin and Och, 2004) solves this
problem only partially.
In this paper, we will present a new automatic
error measure for MT ? the CDER ? which is
designed for assessing MT quality on the sentence
level. It is based on edit distance ? such as the
well-known word error rate (WER) ? but allows
for reordering of blocks. Nevertheless, by defining
reordering costs, the ordering of the words in
a sentence is still relevant for the measure. In
this, the new measure differs significantly from
the position independent error rate (PER) by
(Tillmann et al, 1997). Generally, finding an
optimal solution for such a reordering problem is
NP hard, as is shown in (Lopresti and Tomkins,
1997). In previous work, researchers have tried to
reduce the complexity, for example by restricting
the possible permutations on the block-level, or by
approximation or heuristics during the calculation.
Nevertheless, most of the resulting algorithms still
have high run times and are hardly applied in
practice, or give only a rough approximation. An
overview of some better-known measures can be
found in Section 3.1. In contrast to this, our new
measure can be calculated very efficiently. This
is achieved by requiring complete and disjoint
coverage of the blocks only for the reference
sentence, and not for the candidate translation. We
will present an algorithm which computes the new
error measure in quadratic time.
The new evaluation measure will be investi-
gated and compared to state-of-the-art methods
on two translation tasks. The correlation with
human assessment will be measured for several
different statistical MT systems. We will see
that the new measure significantly outperforms the
existing approaches.
1The n-gram precisions are measured at the sentence level
and then combined into a score over the whole document.
241
As a further improvement, we will introduce
word dependent substitution costs. This method
will be applicable to the new measure as well
as to established measures like WER and PER.
Starting from the observation that the substitution
of a word with a similar one is likely to affect
translation quality less than the substitution with
a completely different word, we will show how
the similarity of words can be accounted for in
automatic evaluation measures.
This paper is organized as follows: In Section 2,
we will present the state of the art in MT
evaluation and discuss the problem of block
reordering. Section 3 will introduce the new
error measure CDER and will show how it can
be calculated efficiently. The concept of word-
dependent substitution costs will be explained in
Section 4. In Section 5, experimental results on
the correlation of human judgment with the CDER
and other well-known evaluation measures will be
presented. Section 6 will conclude the paper and
give an outlook on possible future work.
2 MT Evaluation
2.1 Block Reordering and State of the Art
In MT ? as opposed to other natural language
processing tasks like speech recognition ? there
is usually more than one correct outcome of a
task. In many cases, alternative translations of
a sentence differ from each other mostly by the
ordering of blocks of words. Consequently, an
evaluation measure for MT should be able to
detect and allow for block reordering. Neverthe-
less, a higher ?amount? of reordering between a
candidate translation and a reference translation
should still be reflected in a worse evaluation
score. In other words, the more blocks there are
to be reordered between reference and candidate
sentence, the higher we want the measure to
evaluate the distance between these sentences.
State-of-the-art evaluation measures for MT
penalize movement of blocks rather severely: n-
gram based scores such as BLEU or NIST still
yield a high unigram precision if blocks are
reordered. For higher-order n-grams, though, the
precision drops. As a consequence, this affects the
overall score significantly. WER, which is based
on Levenshtein distance, penalizes the reordering
of blocks even more heavily. It measures the
distance by substitution, deletion and insertion
operations for each word in a relocated block.
PER, on the other hand, ignores the ordering
of the words in the sentences completely. This
often leads to an overly optimistic assessment of
translation quality.
2.2 Long Jumps
The approach we pursue in this paper is to
extend the Levenshtein distance by an additional
operation, namely block movement. The number
of blocks in a sentence is equal to the number
of gaps among the blocks plus one. Thus, the
block movements can equivalently be expressed
as long jump operations that jump over the
gaps between two blocks. The costs of a
long jump are constant. The blocks are read
in the order of one of the sentences. These
long jumps are combined with the ?classical?
Levenshtein edit operations, namely insertion,
deletion, substitution, and the zero-cost operation
identity. The resulting long jump distance dLJ
gives the minimum number of operations which
are necessary to transform the candidate sentence
into the reference sentence. Like the Levenshtein
distance, the long jump distance can be depicted
using an alignment grid as shown in Figure 1:
Here, each grid point corresponds to a pair of
inter-word positions in candidate and reference
sentence, respectively. dLJ is the minimum cost of
a path between the lower left (first) and the upper
right (last) alignment grid point which covers all
reference and candidate words. Deletions and
insertions correspond to horizontal and vertical
edges, respectively. Substitutions and identity
operations correspond to diagonal edges. Edges
between arbitrary grid points from the same row
correspond to long jump operations. It is easy to
see that dLJ is symmetrical.
In the example, the best path contains one dele-
tion edge, one substitution edge, and three long
jump edges. Therefore, the long jump distance
between the sentences is five. In contrast, the
best Levenshtein path contains one deletion edge,
four identity and five consecutive substitution
edges; the Levenshtein distance between the two
sentences is six. The effect of reordering on the
BLEU measure is even higher in this example:
Whereas 8 of the 10 unigrams from the candidate
sentence can be found in the reference sentence,
this holds for only 4 bigrams, and 1 trigram. Not a
single one of the 7 candidate four-grams occurs in
the reference sentence.
3 CDER: A New Evaluation Measure
3.1 Approach
(Lopresti and Tomkins, 1997) showed that finding
an optimal path in a long jump alignment grid is
an NP-hard problem. Our experiments showed
that the calculation of exact long jump distances
becomes impractical for sentences longer than 20
words.
242
we
met
at
the
airport
at
seven
o?clock
.
we met
at seven
o?clock
on the
airport
.have
candidate
re
fe
re
nc
e
deletion
insertion
substitution
identity best path
start/
end node
long jump
block
Figure 1: Example of a long jump alignment
grid. All possible deletion, insertion, identity and
substitution operations are depicted. Only long
jump edges from the best path are drawn.
A possible way to achieve polynomial run-
time is to restrict the number of admissible block
permutations. This has been implemented by
(Leusch et al, 2003) in the inversion word error
rate. Alternatively, a heuristic or approximative
distance can be calculated, as in GTM by (Turian et
al., 2003). An implementation of both approaches
at the same time can be found in TER by (Snover
et al, 2005). In this paper, we will present another
approach which has a suitable run-time, while
still maintaining completeness of the calculated
measure. The idea of the proposed method is to
drop some restrictions on the alignment path.
The long jump distance as well as the Lev-
enshtein distance require both reference and
candidate translation to be covered completely
and disjointly. When extending the metric by
block movements, we drop this constraint for the
candidate translation. That is, only the words
in the reference sentence have to be covered
exactly once, whereas those in the candidate
sentence can be covered zero, one, or multiple
times. Dropping the constraints makes an efficient
computation of the distance possible. We drop
the constraints for the candidate sentence and not
for the reference sentence because we do not want
any information contained in the reference to be
omitted. Moreover, the reference translation will
not contain unnecessary repetitions of blocks.
The new measure ? which will be called
CDER in the following ? can thus be seen as a
measure oriented towards recall, while measures
like BLEU are guided by precision. The CDER
is based on the CDCD distance2 introduced
in (Lopresti and Tomkins, 1997). The authors
show there that the problem of finding the optimal
solution can be solved in O(I2 ? L) time, where
I is the length of the candidate sentence and L
the length of the reference sentence. Within this
paper, we will refer to this distance as dCD . In
the next subsection, we will show how it can be
computed in O(I ?L) time using a modification of
the Levenshtein algorithm.
We also studied the reverse direction of the
described measure; that is, we dropped the
coverage constraints for the reference sentence
instead of the candidate sentence. Addition-
ally, the maximum of both directions has been
considered as distance measure. The results in
Section 5.2 will show that the measure using the
originally proposed direction has a significantly
higher correlation with human evaluation than the
other directions.
3.2 Algorithm
Our algorithm for calculating dCD is based
on the dynamic programming algorithm for the
Levenshtein distance (Levenshtein, 1966). The
Levenshtein distance dLev(eI1, e?L1
)
between two
strings eI1 and e?L1 can be calculated in con-
stant time if the Levenshtein distances of the
substrings, dLev(eI?11 , e?L1
)
, dLev(eI1, e?L?11
)
, and
dLev(eI?11 , e?L?11
)
, are known.
Consequently, an auxiliary quantity
DLev(i, l) := dLev
(
ei1, e?l1
)
is stored in an I ?L table. This auxiliary quantity
can then be calculated recursively from DLev(i ?
1, l), DLev(i, l ? 1), and DLev(i ? 1, l ? 1).
Consequently, the Levenshtein distance can be
calculated in time O(I ? L).
This algorithm can easily be extended for the
calculation of dCD as follows: Again we define
an auxiliary quantity D(i, l) as
D(i, l) := dCD
(
ei1, e?l1
)
Insertions, deletions, and substitutions are
handled the same way as in the Levenshtein
algorithm. Now assume that an optimal dCD path
has been found: Then, each long jump edge within
2C stands for cover and D for disjoint. We adopted this
notion for our measures.
243
...
i
l
deletion insertion subst/id long jump
l-1
i-1
Figure 2: Predecessors of a grid point (i, l) in
Equation 1
this path will always start at a node with the lowest
D value in its row3.
Consequently, we use the following modifica-
tion of the Levenshtein recursion:
D(0, 0) = 0
D(i, l) = min
?
?
?
?
?
?
?
D(i?1, l?1) + (1??(ei, e?l)) ,
D(i? 1, l) + 1,
D(i, l ? 1) + 1,
min
i?
D(i?, l) + 1
?
?
?
?
?
?
?
(1)
where ? is the Kronecker delta. Figure 2 shows the
possible predecessors of a grid point.
The calculation of D(i, l) requires all values of
D(i?, l) to be known, even for i? > i. Thus, the
calculation takes three steps for each l:
1. For each i, calculate the minimum of the first
three terms.
2. Calculate min
i?
D(i?, l).
3. For each i, calculate the minimum according
to Equation 1.
Each of these steps can be done in time O(I).
Therefore, this algorithm calculates dCD in time
O(I ? L) and space O(I).
3.3 Hypothesis Length and Penalties
As the CDER does not penalize candidate trans-
lations which are too long, we studied the use
of a length penalty or miscoverage penalty. This
determines the difference in sentence lengths
between candidate and reference. Two definitions
of such a penalty have been studied for this work.
3Proof: Assume that the long jump edge goes from (i?, l)
to (i, l), and that there exists an i?? such that D(i??, l) <
D(i?, l). This means that the path from (0, 0) to (i??, l) is
less expensive than the path from (0, 0) to (i?, l). Thus, the
path from (0, 0) through (i??, l) to (i, l) is less expensive than
the path through (i?, l). This contradicts the assumption.
Length Difference
There is always an optimal dCD alignment path
that does not contain any deletion edges, because
each deletion can be replaced by a long jump, at
the same costs. This is different for a dLJ path,
because here each candidate word must be covered
exactly once. Assume now that the candidate
sentence consists of I words and the reference
sentence consists of L words, with I > L.
Then, at most L candidate words can be covered
by substitution or identity edges. Therefore, the
remaining candidate words (at least I ? L) must
be covered by deletion edges. This means that at
least I?L deletion edges will be found in any dLJ
path, which leads to dLJ ? dCD ? I ? L in this
case.
Consequently, the length difference between
the two sentences gives us a useful miscoverage
penalty lplen:
lplen := max
(
I ? L, 0
)
This penalty is independent of the dCD alignment
path. Thus, an optimal dCD alignment path
is optimal for dCD + lplen as well. Therefore
the search algorithm in Section 3.2 will find the
optimum for this sum.
Absolute Miscoverage
Let coverage(i) be the number of substitution,
identity, and deletion edges that cover a candidate
word ei in a dCD path. If we had a complete and
disjoint alignment for the candidate word (i.e., a
dLJ path), coverage(i) would be 1 for each i.
In general this is not the case. We can use the
absolute miscoverage as a penalty lpmisc for dCD:
lpmisc :=
?
i
|1 ? coverage(i)|
This miscoverage penalty is not independent of
the alignment path. Consequently, the proposed
search algorithm will not necessarily find an
optimal solution for the sum of dCD and lpmisc.
The idea behind the absolute miscoverage is
that one can construct a valid ? but not necessarily
optimal ? dLJ path from a given dCD path. This
procedure is illustrated in Figure 3 and takes place
in two steps:
1. For each block of over-covered candidate
words, replace the aligned substitution and/or
identity edges by insertion edges; move the
long jump at the beginning of the block
accordingly.
2. For each block of under-covered candidate
words, add the corresponding number of
244
candidate
re
fe
re
nc
e
2 2 0coverage
candidate
1 1 1 1 1 1 1 1 1 1 1
dCD dLJ
deletion insertion subst/id long jump
Figure 3: Transformation of a dCD path into a dLJ
path.
deletion edges; move the long jump at the
beginning of the block accordingly.
This also shows that there cannot be4 a
polynomial time algorithm that calculates the
minimum of dCD + lpmisc for arbitrary pairs of
sentences, because this minimum is equal to dLJ.
With these miscoverage penalties, inexpensive
lower and upper bounds for dLJ can be calculated,
because the following inequality holds:
(2) dCD + lplen ? dLJ ? dCD + lpmisc
4 Word-dependent Substitution Costs
4.1 Idea
All automatic error measures which are based
on the edit distance (i.e. WER, PER, and
CDER) apply fixed costs for the substitution
of words. However, this is counter-intuitive,
as replacing a word with another one which
has a similar meaning will rarely change the
meaning of a sentence significantly. On the other
hand, replacing the same word with a completely
different one probably will. Therefore, it seems
advisable to make substitution costs dependent on
the semantical and/or syntactical dissimilarity of
the words.
To avoid awkward case distinctions, we assume
that a substitution cost function cSUB for two
words e, e? meets the following requirements:
1. cSUB depends only on e and e?.
2. cSUB is a metric; especially
(a) The costs are zero if e = e?, and larger
than zero otherwise.
(b) The triangular inequation holds: it is
always cheaper to replace e by e? than to
replace e by e? and then e? by e?.
4provided that P 6= NP , of course.
3. The costs of substituting a word e by e? are
always equal or lower than those of deleting
e and then inserting e?. In short, cSUB ? 2.
Under these conditions the algorithms for
WER and CDER can easily be modified to use
word-dependent substitution costs. For example,
the only necessary modification in the CDER
algorithm in Equation 1 is to replace 1 ? ?(e, e?)
by cSUB(e, e?).
For the PER, it is no longer possible to use a
linear time algorithm in the general case. Instead,
a modification of the Hungarian algorithm (Knuth,
1993) can be used.
The question is now how to define the word-
dependent substitution costs. We have studied two
different approaches.
4.2 Character-based Levenshtein Distance
A pragmatic approach is to compare the spelling
of the words to be substituted with each other.
The more similar the spelling is, the more similar
we consider the words to be, and the lower we
want the substitution costs between them. In
English, this works well with similar tenses of
the same verb, or with genitives or plurals of the
same noun. Nevertheless, a similar spelling is no
guarantee for a similar meaning, because prefixes
such as ?mis-?, ?in-?, or ?un-? can change the
meaning of a word significantly.
An obvious way of comparing the spelling is the
Levenshtein distance. Here, words are compared
on character level. To normalize this distance
into a range from 0 (for identical words) to 1
(for completely different words), we divide the
absolute distance by the length of the Levenshtein
alignment path.
4.3 Common Prefix Length
Another character-based substitution cost function
we studied is based on the common prefix length
of both words. In English, different tenses of
the same verb share the same prefix; which is
usually the stem. The same holds for different
cases, numbers and genders of most nouns and
adjectives. However, it does not hold if verb
prefixes are changed or removed. On the other
hand, the common prefix length is sensitive to
critical prefixes such as ?mis-? for the same
reason. Consequently, the common prefix length,
normalized by the average length of both words,
gives a reasonable measure for the similarity of
two words. To transform the normalized common
prefix length into costs, this fraction is then
subtracted from 1.
Table 1 gives an example of these two word-
dependent substitution costs.
245
Table 1: Example of word-dependent substitution costs.
Levenshtein prefix
e e? distance substitution cost similarity substitution cost
usual unusual 2 27 = 0.29 1 1 ? 16 = 0.83
understanding misunderstanding 3 316 = 0.19 0 1.00
talk talks 1 15 = 0.20 4 1 ? 44.5 = 0.11
4.4 Perspectives
More sophisticated methods could be considered
for word-dependent substitution costs as well.
Examples of such methods are the introduction of
information weights as in the NIST measure or the
comparison of stems or synonyms, as in METEOR
(Banerjee and Lavie, 2005).
5 Experimental Results
5.1 Experimental Setting
The different evaluation measures were assessed
experimentally on data from the Chinese?English
and the Arabic?English task of the NIST 2004
evaluation workshop (Przybocki, 2004). In this
evaluation campaign, 4460 and 1735 candidate
translations, respectively, generated by different
research MT systems were evaluated by human
judges with regard to fluency and adequacy.
Four reference translations are provided for each
candidate translation. Detailed corpus statistics
are listed in Table 2.
For the experiments in this study, the candidate
translations from these tasks were evaluated using
different automatic evaluation measures. Pear-
son?s correlation coefficient r between automatic
evaluation and the sum of fluency and adequacy
was calculated. As it could be arguable whether
Pearson?s r is meaningful for categorical data like
human MT evaluation, we have also calculated
Kendall?s correlation coefficient ? . Because of
the high number of samples (= sentences, 4460)
versus the low number of categories (= out-
comes of adequacy+fluency, 9), we calculated
? separately for each source sentence. These
experiments showed that Kendall?s ? reflects the
same tendencies as Pearson?s r regarding the
ranking of the evaluation measures. But only
the latter allows for an efficient calculation of
confidence intervals. Consequently, figures of ?
are omitted in this paper.
Due to the small number of samples for eval-
uation on system level (10 and 5, respectively),
all correlation coefficients between automatic
and human evaluation on system level are very
close to 1. Therefore, they do not show any
significant differences for the different evaluation
Table 2: Corpus statistics. TIDES corpora,
NIST 2004 evaluation.
Source language Chinese Arabic
Target language English English
Sentences 446 347
Running words 13 016 10 892
Ref. translations 4 4
Avg. ref. length 29.2 31.4
Candidate systems 10 5
measures. Additional experiments on data from
the NIST 2002 and 2003 workshops and from
the IWSLT 2004 evaluation workshop confirm
the findings from the NIST 2004 experiments;
for the sake of clarity they are not included
here. All correlation coefficients presented here
were calculated for sentence level evaluation.
For comparison with state-of-the-art evaluation
measures, we have also calculated the correlation
between human evaluation and WER and BLEU,
which were both measures of choice in several
international MT evaluation campaigns. Further-
more, we included TER (Snover et al, 2005) as
a recent heuristic block movement measure in
some of our experiments for comparison with our
measure. As the BLEU score is unsuitable for
sentence level evaluation in its original definition,
BLEU-S smoothing as described by (Lin and
Och, 2004) is performed. Additionally, we
added sentence boundary symbols for BLEU, and
a different reference length calculation scheme
for TER, because these changes improved the
correlation between human evaluation and the two
automatic measures. Details on this have been
described in (Leusch et al, 2005).
5.2 CDER
Table 3 presents the correlation of BLEU, WER,
and CDER with human assessment. It can be
seen that CDER shows better correlation than
BLEU and WER on both corpora. On the
Chinese?English task, the smoothed BLEU score
has a higher sentence-level correlation than WER.
However, this is not the case for the Arabic?
246
Table 3: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalu-
ation with BLEU, WER, and CDER (NIST 2004
evaluation; sentence level).
Automatic measure Chin.?E. Arab.?E.
BLEU 0.615 0.603
WER 0.559 0.589
CDER 0.625 0.623
CDER reverseda 0.222 0.393
CDER maximumb 0.594 0.599
aCD constraints for candidate instead of reference.
bSentence-wise maximum of normal and reversed CDER
Table 4: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalua-
tion for CDER with different penalties.
Penalty Chin.?E. Arab.?E.
? 0.625 0.623
lplen 0.581 0.567
lpmisc 0.466 0.528
(lplen + lpmisc)/2 0.534 0.557
English task. So none of these two measures
is superior to the other one, but they are both
outperformed by CDER.
If the direction of CDER is reversed (i.e, the
CD constraints are required for the candidate
instead of the reference, such that the measure
has precision instead of recall characteristics), the
correlation with human evaluation is much lower.
Additionally we studied the use of the maxi-
mum of the distances in both directions. This has
a lower correlation than taking the original CDER,
as Table 3 shows. Nevertheless, the maximum still
performs slightly better than BLEU and WER.
5.3 Hypothesis Length and Penalties
The problem of how to avoid a preference of
overly long candidate sentences by CDER remains
unsolved, as can be found in Table 4: Each of
the proposed penalties infers a significant decrease
of correlation between the (extended) CDER and
human evaluation. Future research will aim at
finding a suitable length penalty. Especially
if CDER is applied in system development,
such a penalty will be needed, as preliminary
optimization experiments have shown.
5.4 Substitution Costs
Table 5 reveals that the inclusion of word-
dependent substitution costs yields a raise by more
than 1% absolute in the correlation of CDER
with human evaluation. The same is true for
Table 5: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalu-
ation for WER and CDER with word-dependent
substitution costs.
Measure Subst. costs Chin.?E. Arab.?E.
WER const (1) 0.559 0.589
prefix 0.571 0.605
Levenshtein 0.580 0.611
CDER const (1) 0.625 0.623
prefix 0.637 0.634
Levenshtein 0.638 0.637
WER: the correlation with human judgment is
increased by about 2% absolute on both language
pairs. The Levenshtein-based substitution costs
are better suited for WER than the scheme based
on common prefix length. For CDER, there is
hardly any difference between the two methods.
Experiments on five more corpora did not give any
significant evidence which of the two substitution
costs correlates better with human evaluation. But
as the prefix-based substitution costs improved
correlation more consistently across all corpora,
we employed this method in our next experiment.
5.5 Combination of CDER and PER
An interesting topic in MT evaluation research
is the question whether a linear combination of
two MT evaluation measures can improve the
correlation between automatic and human evalu-
ation. Particularly, we expected the combination
of CDER and PER to have a significantly higher
correlation with human evaluation than the mea-
sures alone. CDER (as opposed to PER) has the
ability to reward correct local ordering, whereas
PER (as opposed to CDER) penalizes overly long
candidate sentences. The two measures were
combined with linear interpolation. In order
to determine the weights, we performed data
analysis on seven different corpora. The result was
consistent across all different data collections and
language pairs: a linear combination of about 60%
CDER and 40% PER has a significantly higher
correlation with human evaluation than each of
the measures alone. For the two corpora studied
here, the results of the combination can be found
in Table 6: On the Chinese?English task, there is
an additional gain of more than 1% absolute in
correlation over CDER alone. The combined error
measure is the best method in both cases.
The last line in Table 6 shows the 95%-
confidence interval for the correlation. We see
that the new measure CDER, combined with PER,
has a significantly higher correlation with human
evaluation than the existing measures BLEU, TER,
247
Table 6: Correlation (r) between human evalua-
tion (adequacy + fluency) and automatic evalua-
tion for different automatic evaluation measures.
Automatic measure Chin.?E. Arab.?E.
BLEU 0.615 0.603
TER 0.548 0.582
WER 0.559 0.589
WER + Lev. subst. 0.580 0.611
CDER 0.625 0.623
CDER +prefix subst. 0.637 0.634
CDER +prefix+PER 0.649 0.635
95%-confidence ?0.018 ?0.028
and WER on both corpora.
6 Conclusion and Outlook
We presented CDER, a new automatic evalua-
tion measure for MT, which is based on edit
distance extended by block movements. CDER
allows for reordering blocks of words at constant
cost. Unlike previous block movement measures,
CDER can be exactly calculated in quadratic
time. Experimental evaluation on two different
translation tasks shows a significantly improved
correlation with human judgment in comparison
with state-of-the-art measures such as BLEU.
Additionally, we showed how word-dependent
substitution costs can be applied to enhance the
new error measure as well as existing approaches.
The highest correlation with human assessment
was achieved through linear interpolation of the
new CDER with PER.
Future work will aim at finding a suitable length
penalty for CDER. In addition, more sophisticated
definitions of the word-dependent substitution
costs will be investigated. Furthermore, it will
be interesting to see how this new error measure
affects system development: We expect it to
allow for a better sentence-wise error analysis.
For system optimization, preliminary experiments
have shown the need for a suitable length penalty.
Acknowledgement
This material is partly based upon work supported
by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023, and was partly funded by the Euro-
pean Union under the integrated project TC-STAR
? Technology and Corpora for Speech to Speech
Translation
References
S. Banerjee and A. Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
65?72, Ann Arbor, MI, Jun.
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. ARPA Workshop on Human
Language Technology.
D. E. Knuth, 1993. The Stanford GraphBase: a
platform for combinatorial computing, pages 74?87.
ACM Press, New York, NY.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications
to machine translation evaluation. MT Summit IX,
pages 240?247, New Orleans, LA, Sep.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic eval-
uation of machine translation. ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
17?24, Ann Arbor, MI, Jun.
V. I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710, Feb.
C.-Y. Lin and F. J. Och. 2004. Orange: a
method for evaluation automatic evaluation metrics
for machine translation. COLING 2004, pages 501?
507, Geneva, Switzerland, Aug.
D. Lopresti and A. Tomkins. 1997. Block edit models
for approximate string matching. Theoretical
Computer Science, 181(1):159?179, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002. BLEU: a method for automatic evaluation
of machine translation. 40th Annual Meeting of the
ACL, pages 311?318, Philadelphia, PA, Jul.
M. Przybocki. 2004. NIST machine translation 2004
evaluation: Summary of results. DARPA Machine
Translation Evaluation Workshop, Alexandria, VA.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul,
L. Micciulla, and R. Weischedel. 2005. A
study of translation error rate with targeted human
annotation. Technical Report LAMP-TR-126,
CS-TR-4755, UMIACS-TR-2005-58, University of
Maryland, College Park, MD.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based search for
statistical translation. European Conf. on Speech
Communication and Technology, pages 2667?2670,
Rhodes, Greece, Sep.
J. P. Turian, L. Shen, and I. D. Melamed. 2003.
Evaluation of machine translation and its evaluation.
MT Summit IX, pages 23?28, New Orleans, LA, Sep.
248
Robust Knowledge Discovery from Parallel Speech and
Text Sources
F. Jelinek, W. Byrne, S. Khudanpur, B. Hladka?. CLSP, Johns Hopkins University, Baltimore, MD.
H. Ney, F. J. Och. RWTH Aachen University, Aachen, Germany
J. Cur???n. Charles University, Prague, Czech Rep.
J. Psutka. University of West Bohemia, Pilsen, Czech Rep.
1. INTRODUCTION
As a by-product of the recent information explosion, the same
basic facts are often available from multiple sources such as the In-
ternet, television, radio and newspapers. We present here a project
currently in its early stages that aims to take advantage of the re-
dundancies in parallel sources to achieve robustness in automatic
knowledge extraction.
Consider, for instance, the following sampling of actual news
from various sources on a particular day:
CNN: James McDougal, President Bill Clinton?s former business
partner in Arkansas and a cooperating witness in the White-
water investigation, died Sunday while serving a federal prison
term. He was 57.
MSNBC: Fort Worth, Texas, March 8. Whitewater figure James
McDougal died of an apparent heart attack in a private com-
munity hospital in Fort Worth, Texas, Sunday. He was 57.
ABC News: Washington, March 8. James McDougal, a key figure
in Independent Counsel Kenneth Starr?s Whitewater investi-
gation, is dead.
The Detroit News: Fort Worth. James McDougal, a key witness
in Kenneth Starr?s Whitewater investigation of President Clin-
ton and First Lady Hillary Rodham Clinton, died of a heart
attack in a prison hospital Sunday. He was 57.
San Jose Mercury News: James McDougal, the wily Arkansas
banking rogue who drew Bill Clinton and Hillary Rodham
Clinton into real estate deals that have come to haunt them,
died Sunday of cardiac arrest just months before he hoped to
be released from prison. He was 57.
The Miami Herald: Washington. James McDougal, the wily
Arkansas financier and land speculator at the center of the
original Whitewater probe against President Clinton, died
Sunday.
.
Story
Alignment
Speech
RecognitionSpeech Sources
Basic Models:
acoustic
lexical
language
Topic specific
acoustic and language
models
stories
Aligned Sentence
retrieval
Ranked
Answers
Query
Text sources
Figure 1: Information Flow in Alignment and Extraction
We propose to align collections of stories, much like the exam-
ple above, from multiple text and speech sources and then develop
methods that exploit the resulting parallelism both as a tool to im-
prove recognition accuracy and to enable the development of sys-
tems that can reliably extract information from parallel sources.
Our goal is to develop systems that align text sources and rec-
ognize parallel speech streams simultaneously in several languages
by making use of all related text and speech. The initial systems
we intend to develop will process each language independently.
However, our ultimate and most ambitious objective is to align text
sources and recognize speech using a single, integrated multilin-
gual ASR system. Of course, if sufficiently accurate automatic ma-
chine translation (MT) techniques ([1]) were available, we could
address multilingual processing and single language systems in the
same way. However MT techniques are not yet reliable enough
that we expect all words and phrases recognized within languages
to contribute to recognition across languages. We intend to develop
methods that identify the particular words and phrases that both can
be translated reliably and also used to improve story recognition.
As MT technology improves it can be incorporated more exten-
sively within the processing paradigm we propose. We consider
this proposal a framework within which successful MT techniques
can eventually be used for multilingual acoustic processing.
2. PROJECT OBJECTIVES
The first objective is to enhance multi-lingual information sys-
tems by exploiting the processing capabilities for resource-rich lan-
guages to enhance the capabilities for resource-impoverished lan-
guage. The second objective is to advance information retrieval and
knowledge information systems by providing them with consider-
ably improved multi-lingual speech recognition capabilities. Our
research plan proceeds in several steps to (i) collect and (ii) align
multi-lingual parallel speech and text sources, (iii) exploit paral-
lelism for improving ASR within a language, and to (iv) exploit
parallelism for improving ASR across languages. The main infor-
mation flows involved in aligning and exploiting parallel sources
are illustrated in Figure 1. We will initially focus on German, En-
glish and Czech language sources. This section summarizes the
major components of our project.
2.1 Parallel Speech and Text Sources
The monolingual speech and text collections that we will use
to develop techniques to exploit parallelism for improving ASR
within a language are readily available. For instance, the North
American News Text corpus of parallel news streams from 16 US
newspapers and newswire is available from LDC. A 3-year period
yields over 350 million words of multi-source news text.
In addition to data developed within the TIDES and other HLT
programs, we are in the process of identifying and creating our own
multilingual parallel speech and text sources.
FBIS TIDES Multilingual Newstext Collection
For the purposes of developing multilingual alignment techniques,
we intend to use the 240 day, contemporaneous, multilingual news
text collection made available for use to TIDES projects by FBIS.
This corpus contains news in our initial target languages of English,
German, and Czech. The collections are highly parallel, in that
much of the stories are direct translations.
Radio Prague Multilingual Speech and Text Corpus
Speech and news text from Radio Prague was collected under the
direction of J. Psutka with the consent of Radio Prague. The col-
lection contains speech and text in 5 languages: Czech, English,
German, French, and Spanish. The collection began June 1, 2000
and continued for approximately 3 months. The text collection con-
tains the news scripts used for the broadcast; the broadcasts more
or less follow the scripts. The speech is about 3 minutes per day
in each language, which should yield a total of about 5 hours of
speech per language.
Our initial analysis of the Radio Prague corpus suggest that only
approximately 5% of the stories coincide in topic, and that there
is little, if any, direct translation of stories. We anticipate that this
sparseness will make this corpus significantly hard to analyze than
another, highly-parallel corpus. However, we expect this is the
sort of difficulty that will likely be encountered in processing ?real-
world? multilingual news sources.
2.2 Story-level Alignment
Once we have the multiple streams of information we must be
able to align them according to story. A story is the description of
one or more events that happened in a single day and that are re-
ported in a single article by a daily news source the next day. We
expect that we will use the same techniques used in the Topic De-
tection (TDT) field ([5]). Independently of the specific details of
the alignment procedure, there is now substantial evidence that re-
lated stories from parallel streams can be identified using standard
statistical Information Retrieval (IR) techniques.
Sentence Alignment As part of the infrastructure needed to in-
corporate cross-lingual information into language models, we are
employing statistical MT systems to generate English/German and
English/Czech alignments of sentences in the FBIS Newstext Col-
lection. For the English/German sentence and single-word based
alignments, we plan to use statistical models ([4]) [3] which gen-
erate both sentence and word alignments. For English/Czech sen-
tence alignment, we will employ the statistical models trained as
part of the Czech-English MT system developed during the 1999
Johns Hopkins Summer Workshop ([2]).
2.3 Multi-Source Automatic Speech
Recognition
The scenario we propose is extraction of information from paral-
lel text followed by repeated recognition of parallel broadcasts, re-
sulting in a gradual lowering the WER. The first pass is performed
in order to find the likely topics discussed in the story and to iden-
tify the topics relevant to the query. In this process, the acoustic
model will be improved by deriving pronunciation specifications
for out-of-vocabulary words and fixed phrases extracted from the
parallel stories. The language model will be improved by extending
the coverage of the underlying word and phrase vocabulary, and by
specializing the model?s statistics to the narrow topic at hand. As
long as a round of recognition yields new information, the corre-
sponding improvement is incorporated into the recognizer modules
and bootstrapping of the system continues.
Story-specific Language Models from Parallel Speech and Text
Our goal is to create language models combining specific but sparse
statistics, derived from relevant parallel material, with reliable but
unspecific statistics obtainable from large general corpora. We will
create special n-gram language models from the available text, re-
lated or parallel to the spoken stories. We can then interpolate
this special model with a larger pre-existing model, possibly de-
rived from training text associated to the topic of the story. Our
recent STIMULATE work demonstrated success in construction of
topic-specific language models on the basis of hierarchically topic-
organized corpora [8].
Unlike building models from parallel texts, the training of story
specific language models from recognized speech is also affected
by recognition errors in the data which will be used for language
modeling. Confidence measures can be used to estimate the cor-
rectness of individual words or phrases on the recognizer output.
Using this information, n-gram statistics can be extracted from the
recognizer output by selecting those events which are likely to be
correct and which can therefore be used to adjust the original lan-
guage model without introducing new errors to the recognition sys-
tem.
Language Models with Cross-Lingual Lexical Triggers
A trigger language model ([6], [7]) will be constructed for the tar-
get language from the text corpus, where the lexical triggers are not
from the word-history in the target language, but from the aligned
recognized stories in the source language. The trigger informa-
tion becomes most important in those cases in which the baseline
n-gram model in the target language does not supply sufficient in-
formation to predict a word. We expect that content words in the
source language are good predictors for content words in the target
language and that these words are difficult to predict using the tar-
get language alone, and the mutual information techniques used to
identify trigger pairs will be useful here.
Once a spoken source-language story has been recognized, the
words found here there will be used as triggers in the language
model for the recognition of the target-language news broadcasts.
3. SUMMARY
Our goal is to align collections of stories from multiple text and
speech sources in more than one language and then develop meth-
ods that exploit the resulting parallelism both as a tool to improve
recognition accuracy and to enable the development of systems that
can reliably extract information from parallel sources. Much like
a teacher rephrases a concept in a variety of ways to help a class
understand it, the multiple sources, we expect, will increase the po-
tential of success in knowledge extraction. We envision techniques
that will operate repeatedly on multilingual sources by incorporat-
ing newly discovered information in one language into the models
used for all the other languages. Applications of these methods ex-
tend beyond news sources to other multiple-source domains such
as office email and voice-mail, or classroom materials such as lec-
tures, notes and texts.
4. REFERENCES
[1] P. F. Brown, S. A. DellaPietra, V. J. D. Pietra, and R. L.
Mercer. The mathematics of statistical translation.
Computational Linguistics, 19(2), 1993.
[2] K. K. et al Statistical machine translation, WS?99 Final
Report, Johns Hopkins University, 1999.
http://www.clsp.jhu.edu/ws99/projects/mt.
[3] F. J. Och and H. Ney. Improved statistical alignment models.
In ACL?00, pages 440?447, 2000.
[4] F. J. Och, C. Tillmann, and H. Ney. Improved alignment
models for statistical machine translation. In EMNLP/VLC?99,
pages 20?28, 1999.
[5] Proceedings of the Topic Detection and Tracking workshop.
University of Maryland, College Park, MD, October 1997.
[6] C. Tillmann and H. Ney. Selection criteria for word trigger
pairs in language modelling. In ICGI?96, pages 95?106, 1996.
[7] C. Tillmann and H. Ney. Statistical language modeling and
word triggers. In SPECOM?96, pages 22?27, 1996.
[8] D. Yarowsky. Exploiting nonlocal and syntactic word
relationships in language models for conversational speech
recognition, a NSF STIMULATE Project IRI9618874, 1997.
Johns Hopkins University.
The RWTH System for Statistical Translation of Spoken
Dialogues
H. Ney, F. J. Och, S. Vogel
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen, University of Technology
D-52056 Aachen, Germany
ABSTRACT
This paper gives an overview of our work on statistical ma-
chine translation of spoken dialogues, in particular in the
framework of the Verbmobil project. The goal of the
Verbmobil project is the translation of spoken dialogues
in the domains of appointment scheduling and travel plan-
ning. Starting with the Bayes decision rule as in speech
recognition, we show how the required probability distri-
butions can be structured into three parts: the language
model, the alignment model and the lexicon model. We
describe the components of the system and report results
on the Verbmobil task. The experience obtained in the
Verbmobil project, in particular a large-scale end-to-end
evaluation, showed that the statistical approach resulted in
significantly lower error rates than three competing transla-
tion approaches: the sentence error rate was 29% in compar-
ison with 52% to 62% for the other translation approaches.
1. INTRODUCTION
In comparison with written language, speech and espe-
cially spontaneous speech poses additional difficulties for
the task of automatic translation. Typically, these difficul-
ties are caused by errors of the recognition process, which is
carried out before the translation process. As a result, the
sentence to be translated is not necessarily well-formed from
a syntactic point-of-view. Even without recognition errors,
speech translation has to cope with a lack of conventional
syntactic structures because the structures of spontaneous
speech differ from that of written language.
The statistical approach shows the potential to tackle
these problems for the following reasons. First, the statisti-
cal approach is able to avoid hard decisions at any level of
the translation process. Second, for any source sentence, a
translated sentence in the target language is guaranteed to
be generated. In most cases, this will be hopefully a syn-
tactically perfect sentence in the target language; but even
if this is not the case, in most cases, the translated sentence
will convey the meaning of the spoken sentence.
.
Whereas statistical modelling is widely used in speech
recognition, there are so far only a few research groups that
apply statistical modelling to language translation. The pre-
sentation here is based on work carried out in the framework
of the EuTrans project [8] and the Verbmobil project [25].
2. STATISTICAL DECISION THEORY
AND LINGUISTICS
2.1 The Statistical Approach
The use of statistics in computational linguistics has been
extremely controversial for more than three decades. The
controversy is very well summarized by the statement of
Chomsky in 1969 [6]:
?It must be recognized that the notion of a ?probability
of a sentence? is an entirely useless one, under any
interpretation of this term?.
This statement was considered to be true by the major-
ity of experts from artificial intelligence and computational
linguistics, and the concept of statistics was banned from
computational linguistics for many years.
What is overlooked in this statement is the fact that, in an
automatic system for speech recognition or text translation,
we are faced with the problem of taking decisions. It is
exactly here where statistical decision theory comes in. In
speech recognition, the success of the statistical approach is
based on the equation:
Speech Recognition = Acoustic?Linguistic Modelling
+ Statistical Decision Theory
Similarly, for machine translation, the statistical approach
is expressed by the equation:
Machine Translation = Linguistic Modelling
+ Statistical Decision Theory
For the ?low-level? description of speech and image signals,
it is widely accepted that the statistical framework allows
an efficient coupling between the observations and the mod-
els, which is often described by the buzz word ?subsymbolic
processing?. But there is another advantage in using prob-
ability distributions in that they offer an explicit formalism
for expressing and combining hypothesis scores:
? The probabilities are directly used as scores: These
scores are normalized, which is a desirable property:
when increasing the score for a certain element in the
set of all hypotheses, there must be one or several other
elements whose scores are reduced at the same time.
? It is straightforward to combine scores: depending
on the task, the probabilities are either multiplied or
added.
? Weak and vague dependencies can be modelled eas-
ily. Especially in spoken and written natural language,
there are nuances and shades that require ?grey levels?
between 0 and 1.
2.2 Bayes Decision Rule and
System Architecture
In machine translation, the goal is the translation of a
text given in a source language into a target language. We
are given a source string fJ1 = f1...fj ...fJ , which is to be
translated into a target string eI1 = e1...ei...eI . In this arti-
cle, the term word always refers to a full-form word. Among
all possible target strings, we will choose the string with the
highest probability which is given by Bayes decision rule [5]:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)} .
Here, Pr(eI1) is the language model of the target language,
and Pr(fJ1 |eI1) is the string translation model which will be
decomposed into lexicon and alignment models. The argmax
operation denotes the search problem, i.e. the generation
of the output sentence in the target language. The overall
architecture of the statistical translation approach is sum-
marized in Figure 1.
In general, as shown in this figure, there may be additional
transformations to make the translation task simpler for the
algorithm. The transformations may range from the cate-
gorization of single words and word groups to more complex
preprocessing steps that require some parsing of the source
string. We have to keep in mind that in the search procedure
both the language and the translation model are applied af-
ter the text transformation steps. However, to keep the
notation simple, we will not make this explicit distinction in
the subsequent exposition.
3. ALIGNMENT MODELLING
3.1 Concept
A key issue in modelling the string translation probabil-
ity Pr(fJ1 |eI1) is the question of how we define the corre-
spondence between the words of the target sentence and the
words of the source sentence. In typical cases, we can as-
sume a sort of pairwise dependence by considering all word
pairs (fj , ei) for a given sentence pair (fJ1 ; eI1). Here, we will
further constrain this model by assigning each source word
to exactly one target word. Later, this requirement will be
relaxed. Models describing these types of dependencies are
referred to as alignment models [5, 24].
When aligning the words in parallel texts, we typically
observe a strong localization effect. Figure 2 illustrates this
effect for the language pair German?English. In many cases,
although not always, there is an additional property: over
large portions of the source string, the alignment is mono-
tone.
Source Language Text
Transformation
 Lexicon Model
Language Model
Global Search:
 
 
Target Language Text
 
over
 
 Pr(f1  J  |e1I )
 
 
 Pr(   e1I )
 
 
 Pr(f1  J  |e1I )   Pr(   e1I )
  
e1I
f1 J
maximize  Alignment Model
Transformation
Figure 1: Architecture of the translation approach
based on Bayes decision rule.
3.2 Basic Models
To arrive at a quantitative specification, we define the
alignment mapping: j ? i = aj , which assigns a word fj
in position j to a word ei in position i = aj . We rewrite
the probability for the translation model by introducing the
?hidden? alignments aJ1 := a1...aj ...aJ for each sentence pair
(fJ1 ; eI1). To structure this probability distribution, we fac-
torize it over the positions in the source sentence and limit
the alignment dependencies to a first-order dependence:
Pr(fJ1 |eI1) = p(J |I) ?
X
aJ1
J
Y
j=1
[p(aj |aj?1, I, J) ? p(fj |eaj )] .
Here, we have the following probability distributions:
? the sentence length probability: p(J |I), which is in-
cluded here for completeness, but can be omitted with-
out loss of performance;
? the lexicon probability: p(f |e);
? the alignment probability: p(aj |aj?1, I, J).
By making the alignment probability p(aj |aj?1, I, J) depen-
dent on the jump width aj ? aj?1 instead of the absolute
positions aj , we obtain the so-called homogeneous hidden
Markov model, for short HMM [24].
We can also use a zero-order model p(aj |j, I, J), where
there is only a dependence on the absolute position index j
of the source string. This is the so-called model IBM-2 [5].
Assuming a uniform alignment probability p(aj |j, I, J) =
1/I, we arrive at the so-called model IBM-1.
These models can be extended to allow for source words
having no counterpart in the translation. Formally, this
is incorporated into the alignment models by adding a so-
called ?empty word? at position i = 0 to the target sentence
and aligning all source words without a direct translation to
this empty word.
well
I
think
if
we
can
make
it
at
eight
on
both
days
ja ich den
ke
we
nn wir das
hin
kri
ege
n an
bei
den
Ta
ge
n
ac
ht Uh
r
Figure 2: Word-to-word alignment.
In [5], more refined alignment models are introduced by
using the concept of fertility. The idea is that often a word
in the target language may be aligned to several words in
the source language. This is the so-called model IBM-3. Us-
ing, in addition, first-order alignment probabilities along the
positions of the source string leads us to model IBM-4. Al-
though these models take one-to-many alignments explicitly
into account, the lexicon probabilities p(f |e) are still based
on single words in each of the two languages.
In systematic experiments, it was found that the qual-
ity of the alignments determined from the bilingual training
corpus has a direct effect on the translation quality [14].
3.3 Alignment Template Approach
A general shortcoming of the baseline alignment models
is that they are mainly designed to model the lexicon de-
pendences between single words. Therefore, we extend the
approach to handle word groups or phrases rather than sin-
gle words as the basis for the alignment models [15]. In
other words, a whole group of adjacent words in the source
sentence may be aligned with a whole group of adjacent
words in the target language. As a result, the context of
words tends to be explicitly taken into account, and the
differences in local word orders between source and target
languages can be learned explicitly. Figure 3 shows some of
the extracted alignment templates for a sentence pair from
the Verbmobil training corpus. The training algorithm for
the alignment templates extracts all phrase pairs which are
aligned in the training corpus up to a maximum length of 7
words. To improve the generalization capability of the align-
ment templates, the templates are determined for bilingual
word classes rather than words directly. These word classes
are determined by an automatic clustering procedure [13].
4. SEARCH
The task of the search algorithm is to generate the most
likely target sentence eI1 of unknown length I for an observed
source sentence fJ1 . The search must make use of all three
knowledge sources as illustrated by Figure 4: the alignment
model, the lexicon model and the language model. All three
okay
,
how
about
the
nineteenth
at
maybe
,
two
o?clock
in
the
afternoon
?
oka
y ,
wie
sie
ht es am
ne
un
ze
hnt
en aus ,
vie
lle
ich
t um
zw
ei Uhr
na
chm
itt
ags ?
Figure 3: Example of a word alignment and of ex-
tracted alignment templates.
of them must contribute in the final decision about the words
in the target language.
To illustrate the specific details of the search problem, we
slightly change the definitions of the alignments:
? we use inverted alignments as in the model IBM-4 [5]
which define a mapping from target to source positions
rather the other way round.
? we allow several positions in the source language to be
covered, i.e. we consider mappings B of the form:
B : i ? Bi ? {1, ...j, ...J}
We replace the sum over all alignments by the best
alignment, which is referred to as maximum approxima-
tion in speech recognition. Using a trigram language model
p(ei|, ei?2, ei?1), we obtain the following search criterion:
max
BI1 ,eI1
I
Y
i=1
2
4[p(ei|ei?1i?2) ? p(Bi|Bi?1, I, J) ?
Y
j?Bi
p(fj |ei)]
3
5
Considering this criterion, we can see that we can build
up hypotheses of partial target sentences in a bottom-to-
top strategy over the positions i of the target sentence ei1
as illustrated in Figure 5. An important constraint for the
alignment is that all positions of the source sentence should
be covered exactly once. This constraint is similar to that
of the travelling salesman problem where each city has to
be visited exactly once. Details on various search strategies
can be found in [4, 9, 12, 21].
In order to take long context dependences into account,
we use a class-based five-gram language model with backing-
off. Beam-search is used to handle the huge search space. To
normalize the costs of partial hypotheses covering different
parts of the input sentence, an (optimistic) estimation of the
remaining cost is added to the current accumulated cost as
follows. For each word in the source sentence, a lower bound
on its translation cost is determined beforehand. Using this
SENTENCE INSOURCE LANGUAGE
TRANSFORMATION
SENTENCE GENERATEDIN TARGET LANGUAGE
SENTENCE 
 
KNOWLEDGE SOURCESSEARCH: INTERACTION OF 
                            KNOWLEDGE SOURCES
WORD + POSITION
ALIGNMENT 
     
LANGUAGE MODEL
BILINGUAL LEXICON 
ALIGNMENTMODELWORD RE-ORDERING
SYNTACTIC ANDSEMANTIC ANALYSIS
LEXICAL CHOICE
HYPOTHESES
HYPOTHESES
HYPOTHESES
TRANSFORMATION
Figure 4: Illustration of search in statistical trans-
lation.
lower bound, it is possible to achieve an efficient estimation
of the remaining cost.
5. EXPERIMENTAL RESULTS
5.1 The Task and the Corpus
Within the Verbmobil project, spoken dialogues were
recorded. These dialogues were manually transcribed and
later manually translated by Verbmobil partners (Hildes-
heim for Phase I and Tu?bingen for Phase II). Since different
human translators were involved, there is great variability
in the translations.
Each of these so-called dialogues turns may consist of sev-
eral sentences spoken by the same speaker and is sometimes
SOURCE POSITION
TA
RG
ET
 P
O
SI
TI
O
N
i
i-1
j
Figure 5: Illustration of bottom-to-top search.
rather long. As a result, there is no one-to-one correspon-
dence between source and target sentences. To achieve a
one-to-one correspondence, the dialogue turns are split into
shorter segments using punctuation marks as potential split
points. Since the punctuation marks in source and target
sentences are not necessarily identical, a dynamic program-
ming approach is used to find the optimal segmentation
points. The number of segments in the source sentence and
in the test sentence can be different. The segmentation is
scored using a word-based alignment model, and the seg-
mentation with the best score is selected. This segmented
corpus is the starting point for the training of translation
and language models. Alignment models of increasing com-
plexity are trained on this bilingual corpus [14].
A standard vocabulary had been defined for the various
speech recognizers used in Verbmobil. However, not all
words of this vocabulary were observed in the training cor-
pus. Therefore, the translation vocabulary was extended
semi-automatically by adding about 13 000 German?English
word pairs from an online bilingual lexicon available on the
web. The resulting lexicon contained not only word-word
entries, but also multi-word translations, especially for the
large number of German compound words. To counteract
the sparseness of the training data, a couple of straightfor-
ward rule-based preprocessing steps were applied before any
other type of processing:
? categorization of proper names for persons and cities,
? normalization of:
? numbers,
? time and date phrases,
? spelling: don?t ? do not,...
? splitting of
German compound words.
Table 1 gives the characteristics of the training corpus
and the lexicon. The 58 000 sentence pairs comprise about
half a million running words for each language of the bilin-
gual training corpus. The vocabulary size is the number of
distinct full-form words seen in the training corpus. Punctu-
ation marks are treated as regular words in the translation
approach. Notice the large number of word singletons, i. e.
words seen only once. The extended vocabulary is the vo-
cabulary after adding the manual bilingual lexicon.
5.2 Offline Results
During the progress of the Verbmobil project, different
variants of statistical translation were implemented, and ex-
Table 1: Bilingual training corpus, recognition lex-
icon and translation lexicon (PM = punctuation
mark).
German English
Training Text Sentences 58 332
Words (+PMs) 519 523 549 921
Vocabulary 7 940 4 673
Singletons 44.8% 37.6%
Recognition Vocabulary 10 157 6 871
Translation Manual Pairs 12 779
Ext. Vocab. 11 501 6 867
perimental tests were performed for both text and speech
input. To summarize these experimental tests, we briefly
report experimental offline results for the following transla-
tion approaches:
? single-word based approach [20];
? alignment template approach [15];
? cascaded transducer approach [23]:
unlike the other two-approaches, this approach re-
quires a semi-automatic training procedure, in which
the structure of the finite state transducers is designed
manually. For more details, see [23].
The offline tests were performed on text input for the trans-
lation direction from German to English. The test set con-
sisted of 251 sentences, which comprised 2197 words and 430
punctuation marks. The results are shown in Table 2. To
judge and compare the quality of different translation ap-
proaches in offline tests, we typically use the following error
measures [11]:
? mWER (multi-reference word error rate):
For each test sentence sk in the source language, there
are several reference translationsRk = {rk1, . . . , rknk}
in the target language. For each translation of the test
sentence sk, the edit distances (number of substitu-
tions, deletions and insertions as in speech recognition)
to all sentences in Rk are calculated, and the smallest
distance is selected and used as error measure.
? SSER (subjective sentence error rate):
Each translated sentence is judged by a human exam-
iner according to an error scale from 0.0 (semantically
and syntactically correct) to 1.0 (completely wrong).
Both error measures are reported in Table 2. Although
the experiments with the cascaded transducers [23] were not
fully optimized yet, the preliminary results indicated that
this semi-automatic approach does not generalize as well
as the other two fully automatic approaches. Among these
two, the alignment template approach was found to work
consistently better across different test sets (and also tasks
different from Verbmobil). Therefore, the alignment tem-
plate approach was used in the final Verbmobil prototype
system.
5.3 Disambiguation Examples
In the statistical translation approach as we have pre-
sented it, no explicit word sense disambiguation is per-
formed. However, a kind of implicit disambiguation is pos-
sible due to the context information of the alignment tem-
plates and the language model as shown by the examples
in Table 3. The first two groups of sentences contain the
Table 2: Comparison of three statistical translation
approaches (test on text input: 251 sentences =
2197 words + 430 punctuation marks).
Translation mWER SSER
Approach [%] [%]
Single-Word Based 38.2 35.7
Alignment Template 36.0 29.0
Cascaded Transducers >40.0 >40.0
verbs ?gehen? and ?annehmen? which have different transla-
tions, some of which are rather collocational. The correct
translation is only possible by taking the whole sentence
into account. Some improvement can be achieved by ap-
plying morpho-syntactic analysis, e.g handling of the sepa-
rated verb prefixes in German [10]. The last two sentences
show the implicit disambiguation of the temporal and spa-
tial sense for the German preposition ?vor?. Although the
system has not been tailored to handle such types of disam-
biguation, the translated sentences are all acceptable, apart
from the sentence: The meeting is to five.
5.4 Integration into the Verbmobil Prototype
System
The statistical approach to machine translation is em-
bodied in the stattrans module which is integrated into the
Verbmobil prototype system. We briefly review those as-
pects of it that are relevant for the statistical translation ap-
proach. The implementation supports the translation direc-
tions from German to English and from English to German.
In regular processing mode, the stattrans module receives
its input from the repair module [18]. At that time, the
word lattices and best hypotheses from the speech recogni-
tion systems have already been prosodically annotated, i.e.
information about prosodic segment boundaries, sentence
mode and accentuated syllables are added to each edge in
the word lattice [2]. The translation is performed on the
single best sentence hypothesis of the recognizer.
The prosodic boundaries and the sentence mode informa-
tion are utilized by the stattrans module as follows. If there
is a major phrase boundary, a full stop or question mark is
inserted into the word sequence, depending on the sentence
mode as indicated by the prosody module. Additional com-
mas are inserted for other types of segment boundaries. The
prosody module calculates probabilities for segment bound-
aries, and thresholds are used to decide if the sentence marks
are to be inserted. These thresholds have been selected in
such a way that, on the average, for each dialogue turn, a
good segmentation is obtained. The segment boundaries re-
strict possible word reordering between source and target
language. This not only improves translation quality, but
also restricts the search space and thereby speeds up the
translation process.
5.5 Large-Scale End-to-End Evaluation
Whereas the offline tests reported above were important
for the optimization and tuning of the system, the most
important evaluation was the final evaluation of the Verb-
mobil prototype in spring 2000. This end-to-end evaluation
of the Verbmobil system was performed at the University
of Hamburg [19]. In each session of this evaluation, two
native speakers conducted a dialogue. They did not have
any direct contact and could only interact by speaking and
listening to the Verbmobil system.
Three other translation approaches had been integrated
into the Verbmobil prototype system:
? a classical transfer approach [3, 7, 22],
which is based on a manually designed analysis gram-
mar, a set of transfer rules, and a generation grammar,
? a dialogue act based approach [16],
which amounts to a sort of slot filling by classifying
Table 3: Disambiguation examples (?: using morpho-syntactic analysis).
Ambiguous Word Text Input Translation
gehen Wir gehen ins Theater. We will go to the theater.
Mir geht es gut. I am fine.
Es geht um Geld. It is about money.
Geht es bei Ihnen am Montag? Is it possible for you on Monday?
Das Treffen geht bis 5 Uhr. The meeting is to five.
annehmen Wir sollten das Angebot annehmen. We should accept that offer.
Ich nehme das Schlimmste an. I will assume the worst.?
vor Wir treffen uns vor dem Fru?hstu?ck. We meet before the breakfast.
Wir treffen uns vor dem Hotel. We will meet in front of the hotel.
each sentence into one out of a small number of possi-
ble sentence patterns and filling in the slot values,
? an example-based approach [1],
where a sort of nearest neighbour concept is applied to
the set of bilingual training sentence pairs after suit-
able preprocessing.
In the final end-to-end evaluation, human evaluators
judged the translation quality for each of the four trans-
lation results using the following criterion:
Is the sentence approximatively correct: yes/no?
The evaluators were asked to pay particular attention to
the semantic information (e.g. date and place of meeting,
participants etc) contained in the translation. A missing
translation as it may happen for the transfer approach or
other approaches was counted as wrong translation. The
evaluation was based on 5069 dialogue turns for the trans-
lation from German to English and on 4136 dialogue turns
for the translation from English to German. The speech
recognizers used had a word error rate of about 25%. The
overall sentence error rates, i.e. resulting from recognition
and translation, are summarized in Table 4. As we can see,
the error rates for the statistical approach are smaller by a
factor of about 2 in comparison with the other approaches.
In agreement with other evaluation experiments, these ex-
periments show that the statistical modelling approach may
be comparable to or better than the conventional rule-based
approach. In particular, the statistical approach seems to
have the advantage if robustness is important, e.g. when
the input string is not grammatically correct or when it is
corrupted by recognition errors.
Although both text and speech input are translated with
good quality on the average by the statistical approach,
Table 4: Sentence error rates of end-to-end evalua-
tion (speech recognizer with WER=25%; corpus of
5069 and 4136 dialogue turns for translation Ger-
man to English and English to German, respec-
tively).
Translation Method Error [%]
Semantic Transfer 62
Dialogue Act Based 60
Example Based 52
Statistical 29
there are examples where the syntactic structure of the pro-
duced sentence is not correct. Some of these syntactic errors
are related to long range dependencies and syntactic struc-
tures that are not captured by the m-gram language model
used. To cope with these problems, morpho-syntactic anal-
ysis [10] and grammar-based language models [17] are cur-
rently being studied.
6. SUMMARY
In this paper, we have given an overview of the statistical
approach to machine translation and especially its imple-
mentation in the Verbmobil prototype system. The sta-
tistical system has been trained on about 500 000 running
words from a bilingual German?English corpus. Transla-
tions are performed for both directions, i.e. from German
to English and from English to German. Comparative eval-
uations with other translation approaches of the Verbmo-
bil prototype system show that the statistical translation
is superior, especially in the presence of speech input and
ungrammatical input.
Acknowledgment
The work reported here was supported partly by the Verb-
mobil project (contract number 01 IV 701 T4) by the Ger-
man Federal Ministry of Education, Science, Research and
Technology and as part of the EuTrans project (ESPRIT
project number 30268) by the European Community.
Training Toolkit
In a follow-up project of the statistical machine translation
project during the 1999 Johns Hopkins University workshop,
we have developped a publically available toolkit for the
training of different alignment models, including the models
IBM-1 to IBM-5 [5] and an HMM alignment model [14, 24].
The software can be downloaded at
http://www-i6.Informatik.RWTH-Aachen.DE/
~och/software/GIZA++.html.
7. REFERENCES
[1] M. Auerswald: Example-based machine translation
with templates. In [25], pp. 418?427.
[2] A. Batliner, J. Buckow, H. Niemann, E. No?th,
V. Warnke: The prosody module. In [25], pp. 106?
121.
[3] T. Becker, A. Kilger, P. Lopez, P. Poller: The
Verbmobil generation component VM-GECO. In [25],
pp. 481?496.
[4] A. L. Berger, P. F. Brown, J. Cocke, S. A. Della Pietra,
V. J. Della Pietra, J. R. Gillett, J. D. Lafferty,
R. L. Mercer, H. Printz,L. Ures: The Candide Sys-
tem for Machine Translation. ARPA Human Lan-
guage Technology Workshop, Plainsboro, NJ, Morgan
Kaufmann Publishers, pp. 152-157, San Mateo, CA,
March 1994.
[5] P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
R. L. Mercer: The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, Vol. 19, No. 2, pp. 263?311, 1993.
[6] N. Chomsky: ?Quine?s Empirical Assumptions?, in
D. Davidson, J. Hintikka (eds.): Words and objections.
Essays on the work of W. V. Quine, Reidel, Dordrecht,
The Netherlands, 1969.
[7] M. C. Emele, M. Dorna, A. Lu?deling, H. Zinsmeister,
C. Rohrer: Semantic-based transfer. In [25], pp. 359?
376.
[8] EuTrans Project; Instituto Tecnolo?gico de Informa?tica
(ITI, Spain), Fondazione Ugo Bordoni (FUB, Italy),
RWTH Aachen, Lehrstuhl f. Informatik VI (Ger-
many), Zeres GmbH Bochum (Germany): Example-
Based Language Translation Systems. Final report of
the EuTrans project (EU project number 30268), July
2000.
[9] H. Ney, S. Nie?en, F. J. Och, H. Sawaf, C. Tillmann,
S. Vogel: Algorithms for statistical translation of spo-
ken language. IEEE Trans. on Speech and Audio Pro-
cessing Vol. 8, No. 1, pp. 24?36, Jan. 2000.
[10] S. Nie?en, H. Ney: Improving SMT quality with
morpho-syntactic analysis. 18th Int. Conf. on Compu-
tational Linguistics, pp. 1081-1085, Saarbru?cken, Ger-
many, July 2000.
[11] S. Nie?en, F.-J. Och, G. Leusch, H. Ney: An evalua-
tion tool for machine translation: Fast evaluation for
MT research. 2nd Int. Conf. on Language Resources
and Evaluation, pp.39?45, Athens, Greece, May 2000.
[12] S. Nie?en, S. Vogel, H. Ney, C. Tillmann: A DP
based search algorithm for statistical machine transla-
tion. COLING?ACL ?98: 36th Annual Meeting of the
Association for Computational Linguistics and 17th
Int. Conf. on Computational Linguistics, pp. 960?967,
Montreal, Canada, Aug. 1998.
[13] F. J. Och: An efficient method to determine bilingual
word classes. 9th Conf. of the European Chapter of the
Association for Computational Linguistics, pp. 71?76,
Bergen, Norway, June 1999.
[14] F. J. Och, H. Ney: A comparison of alignment
models for statistical machine translation. 18th Int.
Conf. on Computational Linguistics, pp. 1086-1090,
Saarbru?cken, Germany, July 2000.
[15] F. J. Och, C. Tillmann, H. Ney: Improved alignment
models for statistical machine translation. Joint SIG-
DAT Conf. on Empirical Methods in Natural Language
Processing and Very Large Corpora, 20?28, University
of Maryland, College Park, MD, June 1999.
[16] N. Reithinger, R. Engel: Robust content extraction for
translation and dialog processing. In [25], pp. 428?437.
[17] H. Sawaf, K. Schu?tz, H. Ney: On the use of grammar
based language models for statistical machine trans-
lation. 6th Int. Workshop on Parsing Technologies,
pp. 231?241, Trento, Italy, Feb. 2000.
[18] J. Spilker, M. Klarner, G. Go?rz: Processing self-
corrections in a speech-to-speech system. In [25],
pp. 131?140.
[19] L. Tessiore, W. v. Hahn: Functional validation of
a machine translation system: Verbmobil. In [25],
pp. 611?631.
[20] C. Tillmann, H. Ney: Word re-ordering in a DP-based
approach to statistical MT. 18th Int. Conf. on Com-
putational Linguistics 2000, Saarbru?cken, Germany,
pp. 850-856, Aug. 2000.
[21] C. Tillmann, S. Vogel, H. Ney, A. Zubiaga: A DP-
based search using monotone alignments in statisti-
cal translation. 35th Annual Conf. of the Association
for Computational Linguistics, pp. 289?296, Madrid,
Spain, July 1997.
[22] H. Uszkoreit, D. Flickinger, W. Kasper, I. A. Sag:
Deep linguistic analysis with HPSG. In [25], pp. 216?
263.
[23] S. Vogel, H. Ney: Translation with Cascaded Finite-
State Transducers. ACL Conf. (Assoc. for Comput.
Linguistics), Hongkong, pp. 23-30, Oct. 2000.
[24] S. Vogel, H. Ney, C. Tillmann: HMM-based word
alignment in statistical translation. 16th Int. Conf. on
Computational Linguistics, pp. 836?841, Copenhagen,
Denmark, August 1996.
[25] W. Wahlster (Ed.): Verbmobil: Foundations of speech-
to-speech translations. Springer-Verlag, Berlin, Ger-
many, 2000.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 763?770, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Word-Level Confidence Estimation for Machine Translation
using Phrase-Based Translation Models
Nicola Ueffing and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{ueffing,ney}@informatik.rwth-aachen.de
Abstract
Confidence measures for machine transla-
tion is a method for labeling each word in
an automatically generated translation as
correct or incorrect. In this paper, we will
present a new approach to confidence es-
timation which has the advantage that it
does not rely on system output such as N -
best lists or word graphs as many other
confidence measures do. It is, thus, appli-
cable to any kind of machine translation
system.
Experimental evaluation has been per-
formed on translation of technical manu-
als in three different language pairs. Re-
sults will be presented for different ma-
chine translation systems to show that the
new approach is independent of the under-
lying machine translation system which
generated the translations. To the best
of our knowledge, the performance of the
new confidence measure is better than that
of any existing confidence measure.
1 Introduction
The work presented in this paper deals with con-
fidence estimation for machine translation (MT).
Since sentences produced by a machine translation
system are often incorrect but may contain correct
parts, a method for identifying those correct parts
and finding possible errors is desirable. For this pur-
pose, each word in the generated target sentence is
assigned a value expressing the confidence that it is
correct.
Confidence measures have been extensively stud-
ied for speech recognition, but are not well known
in other areas. Only recently have researchers
started to investigate confidence measures for ma-
chine translation (Blatz et al, 2004; Gandrabur and
Foster, 2003; Quirk, 2004; Ueffing et al, 2003).
We apply word confidence measures in MT as fol-
lows: For a given translation generated by a machine
translation system, we determine a confidence value
for each word and compare it to a threshold. All
words whose confidence is above this threshold are
tagged as correct and all others are tagged as incor-
rect translations. The threshold is optimized on a
distinct development set beforehand.
Possible applications for confidence measures in-
clude
? post-editing, where words with low confidence
could be marked as potential errors,
? improving translation prediction accuracy in
trans-type-style interactive machine transla-
tion (Gandrabur and Foster, 2003; Ueffing and
Ney, 2005),
? combining output from different machine
translation systems: hypotheses with low confi-
dence can be discarded before selecting one of
the system translations (Akiba et al, 2004), or
the word confidence scores can be used for gen-
erating new hypotheses from the output of dif-
ferent systems (Jayaraman and Lavie, 2005), or
the sentence confidence value can be employed
for re-ranking (Blatz et al, 2003).
In this paper, we will present several approaches
to word-level confidence estimation and develop a
new phrase-based confidence measure which is in-
dependent of the machine translation system which
763
generated the translation. The paper is organized as
follows: In section 2, we will briefly review the sta-
tistical approach to machine translation. The phrase-
based translation system, which serves as basis for
the new confidence measure, will be presented in
section 2.2. Section 3 will give an overview of re-
lated work on confidence estimation for statistical
machine translation (SMT). In section 4, we will
describe methods for confidence estimation which
make use of SMT system output such as word
graphs and N -best lists. In section 5, we will present
the new phrase-based confidence measure. Section 6
contains a short description of an IBM-1 based con-
fidence measure to which we will compare the other
measures. Experimental evaluation and comparison
of the different confidence measures will be shown
in section 7, and section 8 will conclude the paper.
2 Statistical machine translation
2.1 General
In statistical machine translation, the translation is
modeled as a decision process: Given a source string
fJ1 = f1 . . . fj . . . fJ , we seek the target string eI1 =
e1 . . . ei . . . eI with maximal posterior probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1 | fJ1 )
}
(1)
= argmax
I,eI1
{
Pr(fJ1 | eI1) ? Pr(eI1)
}
Through this decomposition of the probability, we
obtain two knowledge sources: the translation
model Pr(fJ1 | eI1) and the language model Pr(eI1).
Both of them can be modeled independently of each
other. The translation model is responsible for link-
ing the source string fJ1 and the target string eI1,
i.e. it captures the semantics of the sentence. The
target language model captures the well-formedness
or the syntax in the target language. Nowadays,
most of the state-of-the-art SMT systems are based
on bilingual phrases (Bertoldi et al, 2004; Koehn
et al, 2003; Och and Ney, 2004; Tillmann, 2003;
Vogel et al, 2004; Zens and Ney, 2004). Note that
those phrases are sequences of words in the two lan-
guages and not necessarily phrases in the linguistic
sense. A more detailed description of a phrase-based
approach to statistical machine translation will be
given in section 2.2.
2.2 Review of phrase-based translation system
For the confidence measures which will be intro-
duced in section 5, we use a state-of-the-art phrase-
based approach as described in (Zens and Ney,
2004). The key elements of this translation approach
are bilingual phrases, i.e. pairs of source and target
language phrases where a phrase is simply a con-
tiguous sequence of words. These bilingual phrases
are extracted from a word-aligned bilingual training
corpus.
We will present the equations for a mono-
tone search here in order to keep the equa-
tions simple. Let (jK0 , iK0 ) be a segmentation of
the source sentence into phrases, with the cor-
responding (bilingual) phrase pairs (f?k, e?k) =
(f jkjk?1+1, e
ik
ik?1+1), k = 1, . . . ,K. The phrase-
based approach to SMT is then expressed by the fol-
lowing equation:
e?I?1 = argmax
jK0 ,iK0 ,I,eI1
{ I?
i=1
[
c1 ? p(ei | ei?1i?2)?1
]
(2)
?
K?
k=1
[
c2 ? p(f?k | e?k)?2 ? p(e?k | f?k)?3
?
jk?
j=jk?1+1
p(fj | e?k)?4 ?
ik?
i=ik?1+1
p(ei | f?k)?5
]}
,
where p(f?k | e?k) and p(e?k | f?k) are the phrase lexicon
models in both translation directions. The phrase
translation probabilities are computed as a log-linear
interpolation of the relative frequencies and the
IBM-1 probability. The single word based lexicon
models are denoted as p(fj | e?k) and p(ei | f?k), re-
spectively. p(fj | e?k) is defined as the IBM-1 model
probability of fj over the whole phrase e?k, and
p(ei | f?k) is the inverse model, respectively.
c1 is the so-called word penalty, and c2 is the
phrase penalty, assigning constant costs to each tar-
get language word/phrase. The language model
is a trigram model with modified Kneser-Ney dis-
counting and interpolation (Stolcke, 2002). The
search determines the target sentence and segmen-
tation which maximize the objective function.
As equation 2 shows, the sub-models are com-
bined via weighted log-linear interpolation. The
model scaling factors ?1, . . . , ?5 and the word and
phrase penalties are optimized with respect to some
evaluation criterion (Och, 2003), e.g. BLEU score.
764
3 Confidence measures for SMT
3.1 Related work
In this paper, we will present a new approach to
word-level confidence estimation which makes ex-
plicit use of a phrase-based translation model. Most
of the word-level confidence measures which have
been presented in the literature so far are either
based on relatively simple translation models such
as IBM-1 (Blatz et al, 2003) or make use of infor-
mation provided by the SMT system such as N -best
lists or word graphs (Blatz et al, 2003; Gandrabur
and Foster, 2003; Ueffing et al, 2003). In contrast
to this, our method is based on a state-of-the-art
statistical machine translation model, but neverthe-
less is independent of the machine translation sys-
tem which generates the translation hypotheses.
The word-level confidence measures which
showed the best performance in comparative experi-
ments (Blatz et al, 2003) are word posterior prob-
abilities and the IBM-1 based measure. Our new
confidence measure will be compared to those ap-
proaches in section 7.3.
3.2 Word posterior probabilities
The confidence of a target word can be expressed by
its posterior probability, i.e. the probability of the
word to occur in the target sentence, given the source
sentence. Consider a target word e occurring in the
sentence in position i1. The posterior probability of
this event can be determined by summing over all
possible target sentences eI1 containing the word e in
position i:
pi(e, fJ1 ) =
?
I,eI1: ei=e
p(eI1, fJ1 ) (3)
This value has to be normalized in order to ob-
tain a probability distribution over all possible target
words:
pi(e | fJ1 ) =
pi(e, fJ1 )?
e?
pi(e?, fJ1 )
(4)
1This is a rather strict assumption, because the position of a
word in the target sentence can differ largely due to reorderings
in the translation process. We present this variant here to keep
the notation simple. Improved methods will be shown in the
following sections.
4 System based confidence measures
In this section, we will present confidence measures
which are based on N -best lists or word graphs gen-
erated by the SMT system. Those are representa-
tions of the space of the most likely translations of
the source sentence.
The summation given in equation 3 is performed
over all sentences which are contained in the N -best
list or word graph. For a more detailed description,
see (Ueffing et al, 2003).
4.1 Word graph based approach
The word posterior probability pi(e | fJ1 ) can be
calculated over a word graph using the forward-
backward algorithm.
Let n?, n be nodes in a word graph, and (n?, n) the
directed edge connecting them. The edge is anno-
tated with a target word which we denote by e(n?, n)
and the probability which this word contributes to
the overall sentence probability, denoted by p(n?, n).
The forward probability ?i(n?, n) of an edge is
the probability of reaching this edge from the source
of the graph, where the word e(n?, n) is the i-th word
on the path. It can be obtained by summing the prob-
abilities of all incoming paths of length i? 1, which
allows for recursive calculation. This leads to the
following formula:
?i(n?, n) = p(n?, n) ?
?
n??
?i?1(n??, n?) .
The backward probability expresses the probabil-
ity of completing a sentence from the current edge,
i.e. of reaching the sink of the graph. It can be de-
termined recursively in descending order of i as fol-
lows:
?i(n?, n) = p(n?, n) ?
?
n?
?i+1(n, n?) .
Using the forward-backward algorithm, the word
posterior probability of word e in position i is de-
termined by combining the forward and backward
probabilities of all edges which are annotated with
e. This yields
pi(e, fJ1 ) =
?
(n?,n) : e(n?,n)=e
?i(n?, n) ??i(n?, n)
p(n?, n) . (5)
Note that (for computational reasons) the term
p(n?, n) is included both in the forward and in the
765
backward probability so that we have to divide the
product by this term.
To obtain a posterior probability, a normalization,
as shown in equation 4, has to be performed. The
normalization term ? := ?
e?
pi(e?, fJ1 ) corresponds
to the probability mass contained in the word graph
and can be calculated by summing the backward
probabilities of all outgoing edges leaving the source
s of the graph:
? =
?
(s,n)
?1(s, n) .
As stated above, the position of word e in the tar-
get sentence can vary due to reorderings in the trans-
lation process. Therefore, we would like to relax
the condition that e has to occur exactly in position
i. This can be achieved by introducing a window
of size t over the neighboring target positions and
computing the sum of the word posterior probabili-
ties over all positions i ? t, . . . , i, . . . , i + t. In our
experiments we found that a window over ?3 posi-
tions yields the best performance.
4.2 N -best list based approach
N -best lists are an alternative representation of the
space of translation hypotheses. They have the ad-
vantage that the Levenshtein alignment between a
hypothesis and all sentences contained in the list can
be performed easily. This makes it possible to con-
sider not only target sentences, which contain the
word e exactly in a position i (as given in equa-
tion 3), but to allow for some variation.
Let L(eI1, e?I?1) be the Levenshtein alignment be-
tween sentences eI1 and e?I?1. Then, Li(eI1, e?I?1) denotes
the Levenshtein alignment of word ei, i.e. the word
in sentence e?I?1 which ei is Levenshtein-aligned to.
The word posterior probability is then calculated
by summing over all target sentences containing
word e in a position which is Levenshtein-aligned
to i:
pi(e|fJ1 , I, eI1) =
pi(e, fJ1 , I, eI1)?
e?
pi(e?, fJ1 , I, eI1)
,
where
pi(e, fJ1 , I, eI1) =
?
I?,e?I?1: Li(eI1,e?I?1)=e
p(e?I?1, fJ1 ) . (6)
The confidence of word e then depends on the source
sentence fJ1 as well as the target sentence eI1, be-
cause the whole target sentence is relevant for the
Levenshtein alignment.
5 Phrase-based confidence measures
In contrast to the approaches presented in section 4,
the phrase-based confidence measures do not not use
the context information at the sentence level, but
only at the phrase level. We want to determine a
sort of marginal probability Q(e, fJ1 ). Therefore,
we extract all source phrases f j+sj which occur in
the given source sentence. For such source phrases,
we find the possible translations ei+ti in the bilin-
gual phrase lexicon. The confidence of target word
e is then calculated by summing over all phrase pairs
(f j+sj , ei+ti ) where the target part ei+ti contains the
word e.
Let p(ei+ti ) be the language model score of the
target phrase together with the word penalty c1, i.e.
p(ei+ti ) =
i+t?
i?=i
c1 ? p(ei? | ei??1i??2)?1 .
Analogously, define p(f j+sj , ei+ti ) as the score of the
phrase pair which consists of the phrase penalty and
the phrase and word lexicon model scores (cf. sec-
tion 2.2). Following equation 2, the (unnormalized)
confidence is then determined as:
Q(e, fJ1 ) =
J?
j=1
min{smax,J?j}?
s=0
(7)
?
ei+ti : e ? ei+ti
p(ei+ti ) ? p(f j+sj , ei+ti ) ,
where s ? smax and t are source and target phrase
lengths, smax being the maximal source phrase
length.
In equation 7, the language model only deter-
mines the probability of the words within the tar-
get part of the phrase, and not across the phrase
boundaries, because we consider only the single tar-
get phrases without context. Therefore, we assumed
that the language model would not have much influ-
ence on the confidence estimation and also investi-
gated a model without a language model. The same
holds for word and phrase penalty: In the translation
process they are useful for adjusting the length of the
766
generated target hypothesis and for assigning more
weight to longer phrases. Since this does not make
much sense in our setting, we also investigated con-
fidence estimation without word and phrase penalty.
Note that the value calculated in equation 7 is
not normalized. In order to obtain a word posterior
probability, we divide this value by the sum over the
(unnormalized) confidence of all target words:
pphr(e | fJ1 ) =
Q(e, fJ1 )?
e?
Q(e?, fJ1 )
. (8)
Unlike the word posterior probabilities presented
in the previous section, this value is completely in-
dependent of the target sentence position in which
the word e occurs.
As stated in section 2.2, the scaling factors of the
different sub-models and the penalties in the trans-
lation system are optimized with respect to some
evaluation criterion. But since the values which are
optimal for translation are not necessarily optimal
for confidence estimation, we perform optimization
here as well: We train the probability models on the
training corpus, estimate the word confidences on
the development corpus, and optimize the scaling
factors with respect to the classification error rate
described in section 7.2. The optimization is per-
formed with the Downhill Simplex algorithm (Press
et al, 2002).
6 IBM-1 based approach
Another type of confidence measure which does not
rely on system output and is thus applicable to any
kind of machine translation system is the IBM-1
model based confidence measure which was intro-
duced in (Blatz et al, 2003). We modified this con-
fidence measure because we found that the average
lexicon probability used there is dominated by the
maximum. Therefore, we determine the maximal
translation probability of the target word e over the
source sentence words:
pIBM?1(e|fJ1 ) = maxj=0,...,J p(e|fj) , (9)
where f0 is the ?empty? source word (Brown et al,
1993). The probabilities p(e|fj) are word-based lex-
icon probabilities.
Investigations on the use of the IBM-1 model
for word confidence measures showed promising re-
sults (Blatz et al, 2003; Blatz et al, 2004). Thus,
we apply this method here in order to compare it to
the other types of confidence measures.
7 Experiments
7.1 Experimental setting
The experiments were performed on three different
language pairs. All corpora were compiled in the EU
project TransType2; they consist of technical manu-
als. The corpus statistics are given in table 1. The
SMT systems that the confidence estimation was
performed for were trained on these corpora. The
same holds for the probability models that were used
to estimate the word confidences.
We used several (S)MT systems for testing the
confidence measures. A detailed analysis will be
given for two of them; the so-called alignment tem-
plate system (Och and Ney, 2004), (denoted as AT
in the tables) and the phrase-based translation sys-
tem described in section 2.2 (denoted as PBT in the
tables). They are both state-of-the-art SMT systems.
We produced single best translations, word graphs
and N -best lists on all three language pairs using
these systems. The translation quality in terms of
WER, PER (position independent word error rate),
BLEU and NIST score is given in tables 2 and 3.
We see that the best results are obtained on Spanish
to English translation, followed by French to English
and German to English.
Two more translation systems were used for com-
parative experiments: One is a statistical MT system
which is based on a finite state architecture (FSA).
For a description of this system, see (Kanthak et al,
2005). Additionally, we used translations generated
by Systran2. Table 3 presents the translation error
rates and scores for all systems on the German ?
English test corpus. These hypotheses were used
to investigate whether the phrase-based confidence
measures perform well independently of the transla-
tion system.
All three SMT systems (AT, PBT and FSA) show
very similar performance on the German ? English
test corpus. The fact that Systran generates transla-
tions of much lower quality is due to the fact that the
technical manuals are very specific in terminology,
and the SMT systems have been trained on similar
corpora.
2http://babelfish.altavista.com/tr, June 2005
767
Table 1: Statistics of the training, development and test corpora.
French English Spanish English German English
TRAIN Sentences 53 046 55 761 49 376
Running Words 680 796 628 329 752 606 665 399 537 464 589 531
Vocabulary 15 632 13 816 11 050 7 956 23 845 13 223
DEV Sentences 994 1 012 964
Running Words 11 674 10 903 15 957 14 278 10 462 10 642
OOVs 184 141 54 27 147 29
TEST Sentences 984 1 125 996
Running Words 11 709 11 177 10 106 8 370 11 704 12 298
OOVs 204 201 69 49 485 141
Table 2: Translation quality of systems AT and PBT
on the test corpora described in table 1.
AT PBT
S?E F?E S?E F?E
WER[%] 29.6 54.8 26.1 54.9
PER[%] 20.1 43.7 17.5 43.4
BLEU[%] 63.4 31.5 66.9 31.3
NIST 8.80 6.64 8.98 6.62
Table 3: Translation quality of all MT systems on
the German ? English test corpus.
AT PBT FSA Systran
WER[%] 62.7 61.6 63.2 79.2
PER[%] 49.8 49.6 50.4 66.4
BLEU[%] 26.6 25.7 26.5 12.0
NIST 5.92 5.72 5.79 4.09
To determine the true class of each word in a gen-
erated translation hypothesis, we use the word er-
ror rate (WER). That is, a target word is considered
correct if it is aligned to itself in the Levenshtein
alignment between hypothesis and reference trans-
lation(s). We also investigated PER based classifi-
cation, but since the tendencies of the results were
similar, we omit them here.
7.2 Evaluation metrics
After computing the confidence measure, each gen-
erated word is tagged as either correct or false, de-
pending on whether its confidence exceeds the tag-
ging threshold that has been optimized on the devel-
opment set beforehand. The performance of the con-
fidence measure is evaluated using the Classification
Error Rate (CER). This is defined as the number of
incorrect tags divided by the total number of gener-
ated words in the translated sentence. The baseline
CER is determined by assigning the most frequent
class to all translations. In the case that the most fre-
quent class is ?correct? (meaning at least half of the
words in the generated translation are correct w.r.t.
to WER), this is the number of substitutions and in-
sertions, divided by the number of generated words.
The CER strongly depends on the tagging threshold.
Therefore, the tagging threshold is adjusted before-
hand (to minimize CER) on a development corpus
distinct to the test set.
7.3 Experimental results
Table 4 shows the performance of all different con-
fidence measures on the hypotheses generated by
the alignment template system and the phrase-based
system. For the baseline CER, we determined the
90%- and 99%-confidence intervals using the boot-
strap estimation method described in (Bisani and
Ney, 2004)3. We see that, in all settings but one, the
word graph and the N -best list based method out-
perform the IBM-1 based confidence measure. On
French ? English, the improvement over the base-
line is significant at the 1%-level for these methods,
whereas on Spanish ? English this is only the case
at 10%. The performance of the N -best list based
approach is better than that of the word graph based
3The tool is freely available from http://www-i6.informatik.
rwth-aachen.de/web/Software/index.html
768
confidence measures for the alignment template sys-
tem. This is probably due to the fact that the former
can take the Levenshtein alignment into account and
thus estimate the word confidence more reliably.
The phrase-based confidence measures show a
performance which is clearly better than that of the
other methods. We obtain a relative improvement of
up to 7.8% over the best existing method on these
language pairs. The improvement over the baseline
is significant even at the 1%-level in all cases.
When analyzing the impact of the different sub-
models in the phrase-based approach, we found that
the language model does not have much impact on
the confidence estimation. There are only slight
variations in the CER if the model is omitted. The
word and phrase penalty on the other hand seem to
be important (with one exception in the first setting).
The evaluation of the system-independent confi-
dence measures (i.e. those based on IBM-1 and
the new phrase-based method we presented) for four
different translation systems is shown in table 5. We
see that, for all of them, the phrase-based approach
outperforms the IBM-1 based method significantly.
The largest gain in terms of CER is achieved for the
Systran translations: 23.8% relative over the IBM-1
based measure.
8 Conclusion and outlook
We presented a new approach to word-level con-
fidence estimation for machine translation which
makes use of bilingual phrases. By using models
from a state-of-the-art phrase-based statistical ma-
chine translation system, the word confidences are
estimated only on the basis of single best system
output. Unlike other confidence measures, this does
not rely on information from the machine translation
system which generated the translation.
Experimental evaluation on three different lan-
guage pairs and on output from structurally differ-
ent translation systems showed that the new confi-
dence measures perform better than existing confi-
dence measures in all cases. The application on out-
put from different MT systems yielded a significant
reduction of the error rate over the existing mea-
sures. This proves that the method is well-suited for
word confidence estimation on statistical as well as
non-statistical MT systems.
The task investigated in this work was a text trans-
lation task in the domain of technical manuals. We
are currently investigating the use of word-level con-
fidence measures on data from the European parlia-
ment. It will be interesting to see whether a similar
performance can be achieved on this large vocabu-
lary speech translation task.
Acknowledgement
This work was partly funded by the European Union
under the RTD project TransType2 (IST?2001?
32091), and under the integrated project TC-STAR
? Technology and Corpora for Speech to Speech
Translation (IST-2002-FP6-506738).
References
Y. Akiba, E. Sumita, H. Nakaiwa, S. Yamamoto, and
H. G. Okuno. 2004. Using a mixture of n-best lists
from multiple MT systems in rank-sum-based con-
fidence measure for MT outputs. In Proc. CoLing,
pages 322?328, August.
N. Bertoldi, R. Cattoni, M. Cettolo, and M. Federico.
2004. The ITC-irst statistical machine translation sys-
tem for IWSLT-2004. In Proc. IWSLT, pages 51?58,
Kyoto, Japan, September.
M. Bisani and H. Ney. 2004. Bootstrap estimates
for confidence intervals in asr performance evalua-
tionx. In IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 409?412, Mon-
treal, Canada, May.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for machine transla-
tion. Final report, JHU/CLSP Summer Workshop.
http://www.clsp.jhu.edu/ws2003/groups/
estimate/.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004.
Confidence estimation for machine translation. In
Proc. CoLing, pages 315?321, Geneva, Switzerland,
August.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
S. Gandrabur and G. Foster. 2003. Confidence estima-
tion for text prediction. In Proc. CoNLL, pages 95?
102, Edmonton, Canada, May.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
769
Table 4: CER for different confidence measures, reference based on WER. Hypotheses from the alignment
template system and the phrase-based system. The best value is printed in bold.
alignment template system phrase-based system
Model S ? E F ? E S ? E F ? E
Baseline 20.8 42.5 19.2 42.7
99%-confidence interval [18.8,22.7] [40.1,44.7] [17.2,21.2] [40.4,45.0]
90%-confidence interval [19.6,22.1] [40.9,43.9] [17.9,20.5] [41.2,44.2]
Word graphs from the system (eq. 5) 20.1 32.9 17.9 30.5
N -best lists from the system (eq. 6) 19.8 31.9 17.9 30.9
phrase-based (eq. 8) 17.5 30.2 16.5 30.0
without language model 17.4 30.3 16.5 30.3
without word and phrase penalty 17.5 30.6 16.9 30.5
IBM-1 (eq. 9) 20.0 34.1 18.3 35.1
Table 5: CER for different confidence measures on the German? English test set, reference based on WER.
Hypotheses from different MT systems.
Model AT PBT FSA Systran
Baseline 49.2 48.4 46.6 37.4
99%-confidence interval [48.6,53.1] [45.9,50.7] [44.2,49.0] [36.0,38.9]
phrase-based (eq. 8) 27.6 26.4 30.2 24.3
IBM-1 (eq. 9) 32.8 32.8 37.0 31.9
In Proc. EAMT, pages 143?152, Budapest, Hungary,
May.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In ACL 2005 ?
Proc. Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
167?174, Ann Arbor, Michigan, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May/June.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449, December.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, pages 160?167,
Sapporo, Japan, July.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
C. Quirk. 2004. Training a sentence-level machine trans-
lation confidence metric. In Proc. LREC, pages 825?
828, Lisbon, Portugal, May.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. ICSLP, volume 2, pages 901?
904, Denver.
C. Tillmann. 2003. A projection extension algorithm
for statistical machine translation. In Proc. EMNLP,
pages 1?8, Sapporo, Japan, July.
N. Ueffing and H. Ney. 2005. Application of word-level
confidence measures in interactive statistical machine
translation. In Proc. EAMT, pages 262?270, Budapest,
Hungary, May.
N. Ueffing, K. Macherey, and H. Ney. 2003. Confi-
dence Measures for Statistical Machine Translation.
In Proc. MT Summit IX, pages 394?401, New Orleans,
LA, September.
S. Vogel, S. Hewavitharana, M. Kolss, and A. Waibel.
2004. The ISL statistical translation system for spoken
language translation. In Proc. IWSLT, pages 65?72,
Kyoto, Japan, September.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. HLT-
NAACL, pages 257?264, Boston, MA, May.
770
c? 2003 Association for Computational Linguistics
A Systematic Comparison of Various
Statistical Alignment Models
Franz Josef Och? Hermann Ney?
University of Southern California RWTH Aachen
We present and compare various methods for computing word alignments using statistical or
heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della
Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and
refinements. These statistical models are compared with two heuristic models based on the Dice
coefficient. We present different methods for combining word alignments to perform a symmetriza-
tion of directed statistical alignment models. As evaluation criterion, we use the quality of the
resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate
the models on the German-English Verbmobil task and the French-English Hansards task. We
perform a detailed analysis of various design decisions of our statistical alignment system and
evaluate these on training corpora of various sizes. An important result is that refined align-
ment models with a first-order dependence and a fertility model yield significantly better results
than simple heuristic models. In the Appendix, we present an efficient training algorithm for the
alignment models presented.
1. Introduction
We address in this article the problem of finding the word alignment of a bilingual
sentence-aligned corpus by using language-independent statistical methods. There is
a vast literature on this topic, and many different systems have been suggested to
solve this problem. Our work follows and extends the methods introduced by Brown,
Della Pietra, Della Pietra, and Mercer (1993) by using refined statistical models for
the translation process. The basic idea of this approach is to develop a model of the
translation process with the word alignment as a hidden variable of this process, to
apply statistical estimation theory to compute the ?optimal? model parameters, and
to perform alignment search to compute the best word alignment.
So far, refined statistical alignment models have in general been rarely used. One
reason for this is the high complexity of these models, which makes them difficult
to understand, implement, and tune. Instead, heuristic models are usually used. In
heuristic models, the word alignments are computed by analyzing some association
score metric of a link between a source language word and a target language word.
These models are relatively easy to implement.
In this article, we focus on consistent statistical alignment models suggested in the
literature, but we also describe a heuristic association metric. By providing a detailed
description and a systematic evaluation of these alignment models, we give the reader
various criteria for deciding which model to use for a given task.
? Information Science Institute (USC/ISI), 4029 Via Marina, Suite 1001, Marina del Rey, CA 90292.
? Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,
D-52056 Aachen, Germany.
20
Computational Linguistics Volume 29, Number 1
Figure 1
Example of a word alignment (VERBMOBIL task).
We propose to measure the quality of an alignment model by comparing the qual-
ity of the most probable alignment, the Viterbi alignment, with a manually produced
reference alignment. This has the advantage of enabling an automatic evaluation to be
performed. In addition, we shall show that this quality measure is a precise and reli-
able evaluation criterion that is well suited to guide designing and training statistical
alignment models.
The software used to train the statistical alignment models described in this article
is publicly available (Och 2000).
1.1 Problem Definition
We follow Brown, Della Pietra, Della Pietra, and Mercer (1993) to define alignment
as an object for indicating the corresponding words in a parallel text. Figure 1 shows
an example. Very often, it is difficult for a human to judge which words in a given
target string correspond to which words in its source string. Especially problematic
is the alignment of words within idiomatic expressions, free translations, and missing
function words. The problem is that the notion of ?correspondence? between words
is subjective. It is important to keep this in mind in the evaluation of word alignment
quality. We shall deal with this problem in Section 5.
The alignment between two word strings can be quite complicated. Often, an
alignment includes effects such as reorderings, omissions, insertions, and word-to-
phrase alignments. Therefore, we need a very general representation of alignment.
Formally, we use the following definition for alignment in this article. We are given
a source (French) string f J1 = f1, . . . , fj, . . . , fJ and a target language (English) string
eI1 = e1, . . . , ei, . . . , eI that have to be aligned. We define an alignment between the two
word strings as a subset of the Cartesian product of the word positions; that is, an
21
Och and Ney Comparison of Statistical Alignment Models
alignment A is defined as
A ? {(j, i): j = 1, . . . , J; i = 1, . . . , I} (1)
Modeling the alignment as an arbitrary relation between source and target language
positions is quite general. The development of alignment models that are able to deal
with this general representation, however, is hard. Typically, the alignment models pre-
sented in the literature impose additional constraints on the alignment representation.
Typically, the alignment representation is restricted in a way such that each source
word is assigned to exactly one target word. Alignment models restricted in this way
are similar to the concept of hidden Markov models (HMMs) in speech recognition.
The alignment mapping in such models consists of associations j ? i = aj from source
position j to target position i = aj. The alignment a
J
1 = a1, . . . , aj, . . . , aJ may contain
alignments aj = 0 with the ?empty? word e0 to account for source words that are
not aligned with any target word. Constructed in such a way, the alignment is not
a relation between source and target language positions, but only a mapping from
source to target language positions.
In Melamed (2000), a further simplification is performed that enforces a one-to-one
alignment for nonempty words. This means that the alignment mapping aJ1 must be
injective for all word positions aj > 0. Note that many translation phenomena cannot
be handled using restricted alignment representations such as this one. Especially,
methods such as Melamed?s are in principle not able to achieve a 100% recall. The
problem can be reduced through corpus preprocessing steps that perform grouping
and splitting of words.
Some papers report improvements in the alignment quality of statistical methods
when linguistic knowledge is used (Ker and Chang 1997; Huang and Choi 2000). In
these methods, the linguistic knowledge is used mainly to filter out incorrect align-
ments. In this work, we shall avoid making explicit assumptions concerning the lan-
guage used. By avoiding these assumptions, we expect our approach to be applicable
to almost every language pair. The only assumptions we make are that the parallel
text is segmented into aligned sentences and that the sentences are segmented into
words. Obviously, there are additional implicit assumptions in the models that are
needed to obtain a good alignment quality. For example, in languages with a very
rich morphology, such as Finnish, a trivial segmentation produces a high number of
words that occur only once, and every learning method suffers from a significant data
sparseness problem.
1.2 Applications
There are numerous applications for word alignments in natural language processing.
These applications crucially depend on the quality of the word alignment (Och and
Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word align-
ment methods is the automatic extraction of bilingual lexica and terminology from
corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000).
Statistical alignment models are often the basis of single-word-based statistical
machine translation systems (Berger et al 1994; Wu 1996; Wang and Waibel 1998;
Nie?en et al 1998; Garc??a-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney
2001; Germann et al 2001). In addition, these models are the starting point for re-
fined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999)
or example-based translation systems (Brown 1997). In such systems, the quality of
the machine translation output directly depends on the quality of the initial word
alignment (Och and Ney 2000).
22
Computational Linguistics Volume 29, Number 1
Another application of word alignments is in the field of word sense disambigua-
tion (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used
to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers
from a language, such as English, for which many tools already exist to languages for
which such resources are scarce.
1.3 Overview
In Section 2, we review various statistical alignment models and heuristic models.
We present a new statistical alignment model, a log-linear combination of the best
models of Vogel, Ney, and Tillmann (1996) and Brown, Della Pietra, Della Pietra, and
Mercer (1993). In Section 3, we describe the training of the alignment models and
present a new training schedule that yields significantly better results. In addition,
we describe how to deal with overfitting, deficient models, and very small or very
large training corpora. In Section 4, we present some heuristic methods for improving
alignment quality by performing a symmetrization of word alignments. In Section 5,
we describe an evaluation methodology for word alignment methods dealing with
the ambiguities associated with the word alignment annotation based on generalized
precision and recall measures. In Section 6, we present a systematic comparison of the
various statistical alignment models with regard to alignment quality and translation
quality. We assess the effect of training corpora of various sizes and the use of a
conventional bilingual dictionary. In the literature, it is often claimed that the refined
alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993) are not
suitable for small corpora because of data sparseness problems. We show that this is
not the case if these models are parametrized suitably. In the Appendix, we describe
some methods for efficient training of fertility-based alignment models.
2. Review of Alignment Models
2.1 General Approaches
We distinguish between two general approaches to computing word alignments: sta-
tistical alignment models and heuristic models. In the following, we describe both
types of models and compare them from a theoretical viewpoint.
The notational convention we employ is as follows. We use the symbol Pr(?)
to denote general probability distributions with (almost) no specific assumptions. In
contrast, for model-based probability distributions, we use the generic symbol p(?).
2.1.1 Statistical Alignment Models. In statistical machine translation, we try to model
the translation probability Pr(f J1 | eI1), which describes the relationship between a
source language string f J1 and a target language string e
I
1. In (statistical) alignment
models Pr(f J1, a
J
1 | eI1), a ?hidden? alignment a
J
1 is introduced that describes a mapping
from a source position j to a target position aj. The relationship between the translation
model and the alignment model is given by
Pr(f J1 | eI1) =
?
aJ1
Pr(f J1, a
J
1 | eI1) (2)
The alignment aJ1 may contain alignments aj = 0 with the empty word e0 to account
for source words that are not aligned with any target word.
In general, the statistical model depends on a set of unknown parameters ? that is
learned from training data. To express the dependence of the model on the parameter
23
Och and Ney Comparison of Statistical Alignment Models
set, we use the following notation:
Pr(f J1, a
J
1 | eI1) = p?(f
J
1, a
J
1 | eI1) (3)
The art of statistical modeling is to develop specific statistical models that capture
the relevant properties of the considered problem domain. In our case, the statistical
alignment model has to describe the relationship between a source language string
and a target language string adequately.
To train the unknown parameters ?, we are given a parallel training corpus con-
sisting of S sentence pairs {(fs, es) : s = 1, . . . , S}. For each sentence pair (fs, es), the
alignment variable is denoted by a = aJ1. The unknown parameters ? are determined
by maximizing the likelihood on the parallel training corpus:
?? = argmax
?
S
?
s=1
?
a
p?(fs, a | es) (4)
Typically, for the kinds of models we describe here, the expectation maximization (EM)
algorithm (Dempster, Laird, and Rubin 1977) or some approximate EM algorithm is
used to perform this maximization. To avoid a common misunderstanding, however,
note that the use of the EM algorithm is not essential for the statistical approach, but
only a useful tool for solving this parameter estimation problem.
Although for a given sentence pair there is a large number of alignments, we can
always find a best alignment:
a?J1 = argmax
aJ1
p??(f
J
1, a
J
1 | eI1) (5)
The alignment a?J1 is also called the Viterbi alignment of the sentence pair (f
J
1, e
I
1). (For
the sake of simplicity, we shall drop the index ? if it is not explicitly needed.)
Later in the article, we evaluate the quality of this Viterbi alignment by comparing
it to a manually produced reference alignment. The parameters of the statistical align-
ment models are optimized with respect to a maximum-likelihood criterion, which
is not necessarily directly related to alignment quality. Such an approach, however,
requires training with manually defined alignments, which is not done in the research
presented in this article. Experimental evidence shows (Section 6) that the statistical
alignment models using this parameter estimation technique do indeed obtain a good
alignment quality.
In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della
Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel,
Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which
we call Model 6. All these models use a different decomposition of the probability
Pr(f J1, a
J
1 | eI1).
2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments
use a function of the similarity between the types of the two languages (Smadja, Mc-
Keown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently,
variations of the Dice coefficient (Dice 1945) are used as this similarity function. For
each sentence pair, a matrix including the association scores between every word at
every position is then obtained:
dice(i, j) =
2 ? C(ei, fj)
C(ei) ? C(fj)
(6)
24
Computational Linguistics Volume 29, Number 1
C(e, f ) denotes the co-occurrence count of e and f in the parallel training corpus. C(e)
and C(f ) denote the count of e in the target sentences and the count of f in the source
sentences, respectively. From this association score matrix, the word alignment is then
obtained by applying suitable heuristics. One method is to choose as alignment aj = i
for position j the word with the largest association score:
aj = argmax
i
{dice(i, j)} (7)
A refinement of this method is the competitive linking algorithm (Melamed 2000).
In a first step, the highest-ranking word position (i, j) is aligned. Then, the correspond-
ing row and column are removed from the association score matrix. This procedure is
iteratively repeated until every source or target language word is aligned. The advan-
tage of this approach is that indirect associations (i.e., words that co-occur often but
are not translations of each other) occur less often. The resulting alignment contains
only one-to-one alignments and typically has a higher precision than the heuristic
model defined in equation (7).
2.1.3 A Comparison of Statistical Models and Heuristic Models. The main advan-
tage of the heuristic models is their simplicity. They are very easy to implement and
understand. Therefore, variants of the heuristic models described above are widely
used in the word alignment literature.
One problem with heuristic models is that the use of a specific similarity function
seems to be completely arbitrary. The literature contains a large variety of different
scoring functions, some including empirically adjusted parameters. As we show in
Section 6, the Dice coefficient results in a worse alignment quality than the statistical
models.
In our view, the approach of using statistical alignment models is more coherent.
The general principle for coming up with an association score between words results
from statistical estimation theory, and the parameters of the models are adjusted such
that the likelihood of the models on the training corpus is maximized.
2.2 Statistical Alignment Models
2.2.1 Hidden Markov Alignment Model. The alignment model Pr(f J1, a
J
1 | eI1) can be
structured without loss of generality as follows:
Pr(f J1, a
J
1 | eI1) = Pr(J | eI1) ?
J
?
j=1
Pr(fj, aj | f j?11 , a
j?1
1 , e
I
1) (8)
= Pr(J | eI1) ?
J
?
j=1
Pr(aj | f j?11 , a
j?1
1 , e
I
1) ? Pr(fj | f
j?1
1 , a
j
1, e
I
1) (9)
Using this decomposition, we obtain three different probabilities: a length probability
Pr(J | eI1), an alignment probability Pr(aj | f
j?1
1 , a
j?1
1 , e
I
1) and a lexicon probability
Pr(fj | f j?11 , a
j
1, e
I
1). In the hidden Markov alignment model, we assume a first-order
dependence for the alignments aj and that the lexicon probability depends only on the
word at position aj:
Pr(aj | f j?11 , a
j?1
1 , e
I
1) = p(aj | aj?1, I) (10)
Pr(fj | f j?11 , a
j
1, e
I
1) = p(fj | eaj) (11)
25
Och and Ney Comparison of Statistical Alignment Models
Later in the article, we describe a refinement with a dependence on eaj?1 in the
alignment model. Putting everything together and assuming a simple length model
Pr(J | eI1) = p(J | I), we obtain the following basic HMM-based decomposition of
p(f J1 | eI1):
p(f J1 | eI1) = p(J | I) ?
?
aJ1
J
?
j=1
[p(aj | aj?1, I) ? p(fj | eaj)] (12)
with the alignment probability p(i | i?, I) and the translation probability p(f | e).
To make the alignment parameters independent of absolute word positions, we
assume that the alignment probabilities p(i | i?, I) depend only on the jump width
(i? i?). Using a set of non-negative parameters {c(i? i?)}, we can write the alignment
probabilities in the form
p(i | i?, I) = c(i ? i
?)
?I
i??=1 c(i
?? ? i?)
(13)
This form ensures that the alignment probabilities satisfy the normalization constraint
for each conditioning word position i?, i? = 1, . . . , I. This model is also referred to as a
homogeneous HMM (Vogel, Ney, and Tillmann 1996). A similar idea was suggested
by Dagan, Church, and Gale (1993).
In the original formulation of the hidden Markov alignment model, there is no
empty word that generates source words having no directly aligned target word. We
introduce the empty word by extending the HMM network by I empty words e2II+1.
The target word ei has a corresponding empty word ei+I (i.e., the position of the empty
word encodes the previously visited target word). We enforce the following constraints
on the transitions in the HMM network (i ? I, i? ? I) involving the empty word e0:1
p(i + I | i?, I) = p0 ? ?(i, i?) (14)
p(i + I | i? + I, I) = p0 ? ?(i, i?) (15)
p(i | i? + I, I) = p(i | i?, I) (16)
The parameter p0 is the probability of a transition to the empty word, which has to be
optimized on held-out data. In our experiments, we set p0 = 0.2.
Whereas the HMM is based on first-order dependencies p(i = aj | aj?1, I) for the
alignment distribution, Models 1 and 2 use zero-order dependencies p(i = aj | j, I, J):
? Model 1 uses a uniform distribution p(i | j, I, J) = 1/(I + 1):
Pr(f J1, a
J
1 | eI1) =
p(J | I)
(I + 1)J
?
J
?
j=1
p(fj | eaj) (17)
Hence, the word order does not affect the alignment probability.
? In Model 2, we obtain
Pr(f J1, a
J
1 | eI1) = p(J | I) ?
J
?
j=1
[p(aj | j, I, J) ? p(fj | eaj)] (18)
1 ?(i, i?) is the Kronecker function, which is one if i = i? and zero otherwise.
26
Computational Linguistics Volume 29, Number 1
To reduce the number of alignment parameters, we ignore the
dependence on J in the alignment model and use a distribution p(aj | j, I)
instead of p(aj | j, I, J).
2.3 Fertility-Based Alignment Models
In the following, we give a short description of the fertility-based alignment models
of Brown, Della Pietra, Della Pietra, and Mercer (1993). A gentle introduction can be
found in Knight (1999b).
The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra,
Della Pietra, and Mercer 1993) have a significantly more complicated structure than
the simple Models 1 and 2. The fertility ?i of a word ei in position i is defined as the
number of aligned source words:
?i =
?
j
?(aj, i) (19)
The fertility-based alignment models contain a probability p(? | e) that the target word
e is aligned to ? words. By including this probability, it is possible to explicitly describe
the fact that for instance the German word u?bermorgen produces four English words
(the day after tomorrow). In particular, the fertility ? = 0 is used for prepositions
or articles that have no direct counterpart in the other language.
To describe the fertility-based alignment models in more detail, we introduce,
as an alternative alignment representation, the inverted alignments, which define a
mapping from target to source positions rather than the other way around. We allow
several positions in the source language to be covered; that is, we consider alignments
B of the form
B: i ? Bi ? {1, . . . , j, . . . , J}. (20)
An important constraint for the inverted alignment is that all positions of the source
sentence must be covered exactly once; that is, the Bi have to form a partition of the
set {1, . . . , j, . . . , J}. The number of words ?i = |Bi| is the fertility of the word ei. In the
following, Bik refers to the kth element of Bi in ascending order.
The inverted alignments BI0 are a different way to represent normal alignments
aJ1. The set B0 contains the positions of all source words that are aligned with the
empty word. Fertility-based alignment models use the following decomposition and
assumptions:2
Pr(f J1, a
J
1 | eI1) = Pr(f
J
1, B
I
0 | eI1) (21)
= Pr(B0 | BI1) ?
I
?
i=1
Pr(Bi | Bi?11 , eI1) ? Pr(f
J
1 | BI0, eI1) (22)
= p(B0 | BI1) ?
I
?
i=1
p(Bi | Bi?1, ei) ?
I
?
i=0
?
j?Bi
p(fj | ei) (23)
As might be seen from this equation, we have tacitly assumed that the set B0 of words
aligned with the empty word is generated only after the nonempty positions have
2 The original description of the fertility-based alignment models in Brown, Della Pietra, Della Pietra,
and Mercer (1993) includes a more refined derivation of the fertility-based alignment models.
27
Och and Ney Comparison of Statistical Alignment Models
been covered. The distribution p(Bi | Bi?1, ei) is different for Models 3, 4, and 5:
? In Model 3, the dependence of Bi on its predecessor Bi?1 is ignored:
p(Bi | Bi?1, ei) = p(?i | ei) ?i!
?
j?Bi
p(j | i, J) (24)
We obtain an (inverted) zero-order alignment model p(j | i, J).
? In Model 4, every word is dependent on the previous aligned word and
on the word classes of the surrounding words. First, we describe the
dependence of alignment positions. (The dependence on word classes is
for now ignored and will be introduced later.) We have two (inverted)
first-order alignment models: p=1(?j | ? ? ?) and p>1(?j | ? ? ?). The
difference between this model and the first-order alignment model in the
HMM lies in the fact that here we now have a dependence along the
j-axis instead of a dependence along the i-axis. The model p=1(?j | ? ? ?) is
used to position the first word of a set Bi, and the model p>1(?j | ? ? ?) is
used to position the remaining words from left to right:
p(Bi | Bi?1, ei) = p(?i | ei) ?p=1(Bi1 ?B?(i) | ? ? ?)
?i
?
k=2
p>1(Bik ?Bi,k?1 | ? ? ?) (25)
The function i ? i? = ?(i) gives the largest value i? < i for which |Bi? | > 0.
The symbol B?(i) denotes the average of all elements in B?(i).
? Both Model 3 and Model 4 ignore whether or not a source position has
been chosen. In addition, probability mass is reserved for source
positions outside the sentence boundaries. For both of these reasons, the
probabilities of all valid alignments do not sum to unity in these two
models. Such models are called deficient (Brown, Della Pietra, Della
Pietra, and Mercer 1993). Model 5 is a reformulation of Model 4 with a
suitably refined alignment model to avoid deficiency. (We omit the
specific formula. We note only that the number of alignment parameters
for Model 5 is significantly larger than for Model 4.)
Models 3, 4, and 5 define the probability p(B0 | BI1) as uniformly distributed for the
?0! possibilities given the number of words aligned with the empty word ?0 = |B0|.
Assuming a binomial distribution for the number of words aligned with the empty
word, we obtain the following distribution for B0:
p(B0 | BI1) = p
(
?0 |
I
?
i=1
?i
)
? 1
?0!
(26)
=
(
J ? ?0
?0
)
(1 ? p1)J?2?0 p?01 ?
1
?0!
(27)
The free parameter p1 is associated with the number of words that are aligned with
the empty word. There are ?0! ways to order the ?0 words produced by the empty
word, and hence, the alignment model of the empty word is nondeficient. As we will
28
Computational Linguistics Volume 29, Number 1
see in Section 3.2, this creates problems for Models 3 and 4. Therefore, we modify
Models 3 and 4 slightly by replacing ?0! in equation (27) with J?0 :
p(B0 | BI1) =
(
J ? ?0
?0
)
(1 ? p1)J?2?0 p?01 ?
1
J?0
(28)
As a result of this modification, the alignment models for both nonempty words and
the empty word are deficient.
2.3.1 Model 6. As we shall see, the alignment models with a first-order dependence
(HMM, Models 4 and 5) produce significantly better results than the other alignment
models. The HMM predicts the distance between subsequent source language po-
sitions, whereas Model 4 predicts the distance between subsequent target language
positions. This implies that the HMM makes use of locality in the source language,
whereas Model 4 makes use of locality in the target language. We expect to achieve
better alignment quality by using a model that takes into account both types of de-
pendencies. Therefore, we combine HMM and Model 4 in a log-linear way and call
the resulting model Model 6:
p6(f, a | e) =
p4(f, a | e)? ? pHMM(f, a | e)
?
a?,f? p4(f
?, a? | e)? ? pHMM(f?, a? | e)
(29)
Here, the interpolation parameter ? is employed to weigh Model 4 relative to the
hidden Markov alignment model. In our experiments, we use Model 4 instead of
Model 5, as it is significantly more efficient in training and obtains better results.
In general, we can perform a log-linear combination of several models pk(f, a | e),
k = 1, . . . , K by
p6(f, a | e) =
?K
k=1 pk(f, a | e)?k
?
a?,f?
?K
k=1 pk(f
?, a? | e)?k
(30)
The interpolation parameters ?k are determined in such a way that the alignment
quality on held-out data is optimized.
We use a log-linear combination instead of the simpler linear combination be-
cause the values of Pr(f, a | e) typically differ by orders of magnitude for HMM and
Model 4. In such a case, we expect the log-linear combination to be better than a linear
combination.
2.3.2 Alignment Models Depending on Word Classes. For HMM and Models 4 and
5, it is straightforward to extend the alignment parameters to include a dependence
on the word classes of the surrounding words (Och and Ney 2000). In the hidden
Markov alignment model, we allow for a dependence of the position aj on the class
of the preceding target word C(eaj?1): p(aj | aj?1, I, C(eaj?1)). Similarly, we can include
dependencies on source and target word classes in Models 4 and 5 (Brown, Della
Pietra, Della Pietra, and Mercer 1993). The categorization of the words into classes
(here: 50 classes) is performed automatically by using the statistical learning procedure
described in Kneser and Ney (1993).
2.3.3 Overview of Models. The main differences among the statistical alignment mod-
els lie in the alignment model they employ (zero-order or first-order), the fertility
model they employ, and the presence or absence of deficiency. In addition, the models
differ with regard to the efficiency of the E-step in the EM algorithm (Section 3.1).
Table 1 offers an overview of the properties of the various alignment models.
29
Och and Ney Comparison of Statistical Alignment Models
Table 1
Overview of the alignment models.
Model Alignment model Fertility model E-step Deficient
Model 1 uniform no exact no
Model 2 zero-order no exact no
HMM first-order no exact no
Model 3 zero-order yes approximative yes
Model 4 first-order yes approximative yes
Model 5 first-order yes approximative no
Model 6 first-order yes approximative yes
2.4 Computation of the Viterbi Alignment
We now develop an algorithm to compute the Viterbi alignment for each alignment
model. Although there exist simple polynomial algorithms for the baseline Models 1
and 2, we are unaware of any efficient algorithm for computing the Viterbi alignment
for the fertility-based alignment models.
For Model 2 (also for Model 1 as a special case), we obtain
a?J1 = argmax
aJ1
Pr(f J1, a
J
1 | eI1) (31)
= argmax
aJ1
?
?
?
p(J | I) ?
J
?
j=1
[p(aj | j, I) ? p(fj | eaj)]
?
?
?
(32)
=
[
argmax
aj
{p(aj | j, I) ? p(fj | eaj)}
]J
j=1
(33)
Hence, the maximization over the (I+1)J different alignments decomposes into J max-
imizations of (I + 1) lexicon probabilities. Similarly, the Viterbi alignment for Model 2
can be computed with a complexity of O(I ? J).
Finding the optimal alignment for the HMM is more complicated than for Model 1
or Model 2. Using a dynamic programming approach, it is possible to obtain the Viterbi
alignment for the HMM with a complexity of O(I2 ? J) (Vogel, Ney, and Tillmann 1996).
For the refined alignment models, however, namely, Models 3, 4, 5, and 6, max-
imization over all alignments cannot be efficiently carried out. The corresponding
search problem is NP-complete (Knight 1990a). For short sentences, a possible so-
lution could be an A* search algorithm (Och, Ueffing, and Ney 2001). In the work
presented here, we use a more efficient greedy search algorithm for the best align-
ment, as suggested in Brown, Della Pietra, Della Pietra, and Mercer (1993). The basic
idea is to compute the Viterbi alignment of a simple model (such as Model 2 or HMM).
This alignment is then iteratively improved with respect to the alignment probability
of the refined alignment model. (For further details on the greedy search algorithm,
see Brown, Della Pietra, Della Pietra, and Mercer [1993].) In the Appendix, we present
methods for performing an efficient computation of this pseudo-Viterbi alignment.
3. Training
3.1 EM Algorithm
In this section, we describe our approach to determining the model parameters ?.
Every model has a specific set of free parameters. For example, the parameters ? for
30
Computational Linguistics Volume 29, Number 1
Model 4 consist of lexicon, alignment, and fertility parameters:
? = {{p(f | e)}, {p=1(?j | ? ? ?)}, {p>1(?j | ? ? ?)}, {p(? | e)}, p1} (34)
To train the model parameters ?, we use a maximum-likelihood approach, as described
in equation (4), by applying the EM algorithm (Baum 1972). The different models are
trained in succession on the same data; the final parameter values of a simpler model
serve as the starting point for a more complex model.
In the E-step of Model 1, the lexicon parameter counts for one sentence pair (e, f)
are calculated:
c(f | e; e, f) =
?
e,f
N(e, f)
?
a
Pr(a | e, f)
?
j
?(f , fj)?(e, eaj) (35)
Here, N(e, f) is the training corpus count of the sentence pair (f, e). In the M-step, the
lexicon parameters are computed:
p(f | e) =
?
s c(f | e; fs, es)
?
s,f c(f | e; fs, es)
(36)
Similarly, the alignment and fertility probabilities can be estimated for all other align-
ment models (Brown, Della Pietra, Della Pietra, and Mercer 1993). When bootstrapping
from a simpler model to a more complex model, the simpler model is used to weigh the
alignments, and the counts are accumulated for the parameters of the more complex
model.
In principle, the sum over all (I+1)J alignments has to be calculated in the E-step.
Evaluating this sum by explicitly enumerating all alignments would be infeasible.
Fortunately, Models 1 and 2 and HMM have a particularly simple mathematical form
such that the EM algorithm can be implemented efficiently (i.e., in the E-step, it is
possible to efficiently evaluate all alignments). For the HMM, this is referred to as the
Baum-Welch algorithm (Baum 1972).
Since we know of no efficient way to avoid the explicit summation over all align-
ments in the EM algorithm in the fertility-based alignment models, the counts are
collected only over a subset of promising alignments. For Models 3 to 6, we perform
the count collection only over a small number of good alignments. To keep the training
fast, we consider only a small fraction of all alignments. We compare three different
methods for using subsets of varying sizes:
? The simplest method is to perform Viterbi training using only the best
alignment found. As the Viterbi alignment computation itself is very
time consuming for Models 3 to 6, the Viterbi alignment is computed
only approximately, using the method described in Brown, Della Pietra,
Della Pietra, and Mercer (1993).
? Al-Onaizan et al (1999) suggest using as well the neighboring
alignments of the best alignment found. (For an exact definition of the
neighborhood of an alignment, the reader is referred to the Appendix.)
? Brown, Della Pietra, Della Pietra, and Mercer (1993) use an even larger
set of alignments, including also the pegged alignments, a large set of
alignments with a high probability Pr(f J1, a
J
1 | eI1). The method for
constructing these alignments (Brown, Della Pietra, Della Pietra, and
Mercer 1993) guarantees that for each lexical relationship in every
sentence pair, at least one alignment is considered.
31
Och and Ney Comparison of Statistical Alignment Models
In Section 6, we show that by using the HMM instead of Model 2 in bootstrap-
ping the fertility-based alignment models, the alignment quality can be significantly
improved. In the Appendix, we present an efficient training algorithm of the fertility-
based alignment models.
3.2 Is Deficiency a Problem?
When using the EM algorithm on the standard versions of Models 3 and 4, we observe
that during the EM iterations more and more words are aligned with the empty word.
This results in a poor alignment quality, because too many words are aligned to the
empty word. This progressive increase in the number of words aligned with the empty
word does not occur when the other alignment models are used. We believe that this
is due to the deficiency of Model 3 and Model 4.
The use of the EM algorithm guarantees that the likelihood increases for each
iteration. This holds for both deficient and nondeficient models. For deficient models,
however, as the amount of deficiency in the model is reduced, the likelihood increases.
In Models 3 and 4 as defined in Brown, Della Pietra, Della Pietra, and Mercer (1993),
the alignment model for nonempty words is deficient, but the alignment model for
the empty word is nondeficient. Hence, the EM algorithm can increase likelihood by
simply aligning more and more words with the empty word.3
Therefore, we modify Models 3 and 4 slightly, such that the empty word also has
a deficient alignment model. The alignment probability is set to p(j | i, J) = 1/J for each
source word aligned with the empty word. Another remedy, adopted in Och and Ney
(2000), is to choose a value for the parameter p1 of the empty-word fertility and keep
it fixed.
3.3 Smoothing
To overcome the problem of overfitting on the training data and to enable the models
to cope better with rare words, we smooth the alignment and fertility probabilities. For
the alignment probabilities of the HMM (and similarly for Models 4 and 5), we perform
an interpolation with a uniform distribution p(i | j, I) = 1/I using an interpolation
parameter ?:
p?(aj | aj?1, I) = (1 ? ?) ? p(aj | aj?1, I) + ? ?
1
I
(37)
For the fertility probabilities, we assume that there is a dependence on the number
of letters g(e) of e and estimate a fertility distribution p(? | g) using the EM algorithm.
Typically, longer words have a higher fertility. By making this assumption, the model
can learn that the longer words usually have a higher fertility than shorter words.
Using an interpolation parameter ?, the fertility distribution is then computed as
p?(? | e) =
(
1 ? ?
? + n(e)
)
? p(? | e) + ?
? + n(e)
? p(? | g(e)) (38)
Here, n(e) denotes the frequency of e in the training corpus. This linear interpolation
ensures that for frequent words (i.e., n(e)  ?), the specific distribution p(? | e) dom-
inates, and that for rare words (i.e., n(e)  ?), the general distribution p(? | g(e))
dominates.
The interpolation parameters ? and ? are determined in such a way that the
alignment quality on held-out data is optimized.
3 This effect did not occur in Brown, Della Pietra, Della Pietra, and Mercer (1993), as Models 3 and 4
were not trained directly.
32
Computational Linguistics Volume 29, Number 1
3.4 Bilingual Dictionary
A conventional bilingual dictionary can be considered an additional knowledge source
that can be used in training. We assume that the dictionary is a list of word strings
(e, f). The entries for each language can be a single word or an entire phrase.
To integrate a dictionary into the EM algorithm, we compare two different
methods:
? Brown, Della Pietra, Della Pietra, Goldsmith, et al (1993) developed a
multinomial model for the process of constructing a dictionary (by a
human lexicographer). By applying suitable simplifications, the method
boils down to adding every dictionary entry (e, f) to the training corpus
with an entry-specific count called effective multiplicity, expressed as
?(e, f):
?(e, f) =
?(e) ? p(f | e)
1 ? e?(e)?p(f|e) (39)
In this section, ?(e) is an additional parameter describing the size of the
sample that is used to estimate the model p(f | e). This count is then
used instead of N(e, f) in the EM algorithm as shown in equation (35).
? Och and Ney (2000) suggest that the effective multiplicity of a dictionary
entry be set to a large value ?+  1 if the lexicon entry actually occurs
in one of the sentence pairs of the bilingual corpus and to a low value
otherwise:
?(e, f) =
{
?+ if e and f co-occur
?? otherwise
(40)
As a result, only dictionary entries that indeed occur in the training
corpus have a large effect in training. The motivation behind this is to
avoid a deterioration of the alignment as a result of out-of-domain
dictionary entries. Every entry in the dictionary that does co-occur in the
training corpus can be assumed correct and should therefore obtain a
high count. We set ?? = 0.
4. Symmetrization
In this section, we describe various methods for performing a symmetrization of our
directed statistical alignment models by applying a heuristic postprocessing step that
combines the alignments in both translation directions (source to target, target to
source).
The baseline alignment model does not allow a source word to be aligned with
more than one target word. Therefore, lexical correspondences like that of the German
compound word Zahnarzttermin with the English dentist?s appointment cause problems,
because a single source word must be mapped to two or more target words. Therefore,
the resulting Viterbi alignment of the standard alignment models has a systematic loss
in recall.
To solve this problem, we perform training in both translation directions (source to
target, target to source). As a result, we obtain two alignments aJ1 and b
I
1 for each pair
of sentences in the training corpus. Let A1 = {(aj, j) | aj > 0} and A2 = {(i, bi) | bi > 0}
denote the sets of alignments in the two Viterbi alignments. To increase the quality
of the alignments, we combine A1 and A2 into one alignment matrix A using the
33
Och and Ney Comparison of Statistical Alignment Models
following combination methods:
? Intersection: A = A1 ? A2.
? Union: A = A1 ? A2.
? Refined method: In a first step, the intersection A = A1 ? A2 is
determined. The elements of this intersection result from both Viterbi
alignments and are therefore very reliable. Then, we extend the
alignment A iteratively by adding alignments (i, j) occurring only in the
alignment A1 or in the alignment A2 if neither fj nor ei has an alignment
in A, or if both of the following conditions hold:
? The alignment (i, j) has a horizontal neighbor (i ? 1, j), (i + 1, j)
or a vertical neighbor (i, j ? 1), (i, j + 1) that is already in A.
? The set A ? {(i, j)} does not contain alignments with both
horizontal and vertical neighbors.
Obviously, the intersection of the two alignments yields an alignment consisting of
only one-to-one alignments with a higher precision and a lower recall than either
one separately. The union of the two alignments yields a higher recall and a lower
precision of the combined alignment than either one separately. Whether a higher
precision or a higher recall is preferred depends on the final application for which
the word alignment is intended. In applications such as statistical machine translation
(Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000),
so an alignment union would probably be chosen. In lexicography applications, we
might be interested in alignments with a very high precision obtained by performing
an alignment intersection.
5. Evaluation Methodology
In the following, we present an annotation scheme for single-word-based alignments
and a corresponding evaluation criterion.
It is well known that manually performing a word alignment is a complicated
and ambiguous task (Melamed 1998). Therefore, in performing the alignments for
the research presented here, we use an annotation scheme that explicitly allows for
ambiguous alignments. The persons conducting the annotation are asked to specify
alignments of two different kinds: an S (sure) alignment, for alignments that are un-
ambiguous, and a P (possible) alignment, for ambiguous alignments. The P label is
used especially to align words within idiomatic expressions and free translations and
missing function words (S ? P).
The reference alignment thus obtained may contain many-to-one and one-to-many
relationships. Figure 2 shows an example of a manually aligned sentence with S and
P labels.
The quality of an alignment A = {(j, aj) | aj > 0} is then computed by appropriately
redefined precision and recall measures:
recall =
|A ? S|
|S| , precision =
|A ? P|
|A| (41)
and the following alignment error rate (AER), which is derived from the well-known
F-measure:
AER(S, P; A) = 1 ? |A ? S|+ |A ? P||A|+ |S| (42)
34
Computational Linguistics Volume 29, Number 1
Figure 2
A manual alignment with S (filled squares) and P (unfilled squares) connections.
These definitions of precision, recall and the AER are based on the assumption
that a recall error can occur only if an S alignment is not found and a precision error
can occur only if the found alignment is not even P.
The set of sentence pairs for which the manual alignment is produced is randomly
selected from the training corpus. It should be emphasized that all the training of the
models is performed in a completely unsupervised way (i.e., no manual alignments
are used). From this point of view, there is no need to have a test corpus separate from
the training corpus.
Typically, the annotation is performed by two human annotators, producing sets
S1, P1, S2, P2. To increase the quality of the resulting reference alignment, the anno-
tators are presented with the mutual errors and asked to improve their alignments
where possible. (Mutual errors of the two annotators A and B are the errors in the
alignment of annotator A if we assume the alignment of annotator B as reference and
the errors in the alignment of annotator B if we assume the alignment of annotator A
as reference.) From these alignments, we finally generate a reference alignment that
contains only those S connections on which both annotators agree and all P connec-
tions from both annotators. This can be accomplished by forming the intersection of
the sure alignments (S = S1?S2) and the union of the possible alignments (P = P1?P2),
respectively. By generating the reference alignment in this way, we obtain an alignment
error rate of 0 percent when we compare the S alignments of every single annotator
with the combined reference alignment.
6. Experiments
We present in this section results of experiments involving the Verbmobil and Hansards
tasks. The Verbmobil task (Wahlster 2000) is a (German-English) speech translation task
35
Och and Ney Comparison of Statistical Alignment Models
Table 2
Corpus characteristics of the Verbmobil task.
German English
Training corpus Sentences 34,446 ? 34K
Words 329,625 343,076
Vocabulary 5,936 3,505
Singletons 2,600 1,305
Bilingual dictionary Entries 4,404
Words 4,758 5,543
Test corpus Sentences 354
Words 3,233 3,109
Table 3
Corpus characteristics of the Hansards task.
French English
Training corpus Sentences 1470K
Words 24.33M 22.16M
Vocabulary 100,269 78,332
Singletons 40,199 31,319
Bilingual dictionary Entries 28,701
Words 28,702 30,186
Test corpus Sentences 500
Words 8,749 7,946
in the domain of appointment scheduling, travel planning, and hotel reservation. The
bilingual sentences used in training are correct transcriptions of spoken dialogues.
However, they include spontaneous speech effects such as hesitations, false starts, and
ungrammatical phrases. The French-English Hansards task consists of the debates in
the Canadian parliament. This task has a very large vocabulary of about 100,000 French
words and 80,000 English words.4
Statistics for the two corpora are shown in Tables 2 and 3. The number of running
words and the vocabularies are based on full-form words and the punctuation marks.
We produced smaller training corpora by randomly choosing 500, 2,000 and 8,000
sentences from the Verbmobil task and 500, 8,000, and 128,000 sentences from the
Hansards task.
For both tasks, we manually aligned a randomly chosen subset of the training
corpus. From this subset of the corpus, the first 100 sentences are used as the de-
velopment corpus to optimize the model parameters that are not trained via the EM
4 We do not use the Blinker annotated corpus described in Melamed (1998), since the domain is very
special (the Bible) and a different annotation methodology is used.
36
Computational Linguistics Volume 29, Number 1
Table 4
Comparison of alignment error rate percentages for various training schemes (Verbmobil task;
Dice+C: Dice coefficient with competitive linking).
Size of training corpus
Model Training scheme 0.5K 2K 8K 34K
Dice 28.4 29.2 29.1 29.0
Dice+C 21.5 21.8 20.1 20.4
Model 1 15 19.3 19.0 17.8 17.0
Model 2 1525 27.7 21.0 15.8 13.5
HMM 15H5 19.1 15.4 11.4 9.2
Model 3 152533 25.8 18.4 13.4 10.3
15H533 18.1 14.3 10.5 8.1
Model 4 15253343 23.4 14.6 10.0 7.7
15H543 17.3 11.7 9.1 6.5
15H53343 16.8 11.7 8.4 6.3
Model 5 15H54353 17.3 11.4 8.7 6.2
15H5334353 16.9 11.8 8.5 5.8
Model 6 15H54363 17.2 11.3 8.8 6.1
15H5334363 16.4 11.7 8.0 5.7
algorithm (e.g., the smoothing parameters). The remaining sentences are used as the
test corpus.
The sequence of models used and the number of training iterations used for each
model is referred to in the following as the training scheme. Our standard train-
ing scheme on Verbmobil is 15H5334363. This notation indicates that five iterations
of Model 1, five iterations of HMM, three iterations of Model 3, three iterations
of Model 4, and three iterations of Model 6 are performed. On Hansards, we use
15H10334363. This training scheme typically gives very good results and does not lead
to overfitting. We use the slightly modified versions of Model 3 and Model 4 described
in Section 3.2 and smooth the fertility and the alignment parameters. In the E-step of
the EM algorithm for the fertility-based alignment models, we use the Viterbi align-
ment and its neighborhood. Unless stated otherwise, no bilingual dictionary is used
in training.
6.1 Models and Training Schemes
Tables 4 and 5 compare the alignment quality achieved using various models and
training schemes. In general, we observe that the refined models (Models 4, 5, and 6)
yield significantly better results than the simple Model 1 or Dice coefficient. Typically,
the best results are obtained with Model 6. This holds across a wide range of sizes
for the training corpus, from an extremely small training corpus of only 500 sentences
up to a training corpus of 1.5 million sentences. The improvement that results from
using a larger training corpus is more significant, however, if more refined models are
used. Interestingly, even on a tiny corpus of only 500 sentences, alignment error rates
under 30% are achieved for all models, and the best models have error rates somewhat
under 20%.
We observe that the alignment quality obtained with a specific model heavily
depends on the training scheme that is used to bootstrap the model.
37
Och and Ney Comparison of Statistical Alignment Models
Table 5
Comparison of alignment error rate percentages for various training schemes (Hansards task;
Dice+C: Dice coefficient with competitive linking).
Size of training corpus
Model Training scheme 0.5K 8K 128K 1.47M
Dice 50.9 43.4 39.6 38.9
Dice+C 46.3 37.6 35.0 34.0
Model 1 15 40.6 33.6 28.6 25.9
Model 2 1525 46.7 29.3 22.0 19.5
HMM 15H5 26.3 23.3 15.0 10.8
Model 3 152533 43.6 27.5 20.5 18.0
15H533 27.5 22.5 16.6 13.2
Model 4 15253343 41.7 25.1 17.3 14.1
15H53343 26.1 20.2 13.1 9.4
15H543 26.3 21.8 13.3 9.3
Model 5 15H54353 26.5 21.5 13.7 9.6
15H5334353 26.5 20.4 13.4 9.4
Model 6 15H54363 26.0 21.6 12.8 8.8
15H5334363 25.9 20.3 12.5 8.7
Figure 3
Comparison of alignment error rate (in percent) for Model 1 and Dice coefficient (left: 34K
Verbmobil task, right: 128K Hansards task).
6.2 Heuristic Models versus Model 1
We pointed out in Section 2 that from a theoretical viewpoint, the main advantage
of statistical alignment models in comparison to heuristic models is the well-founded
mathematical theory that underlies their parameter estimation. Tables 4 and 5 show
that the statistical alignment models significantly outperform the heuristic Dice coef-
ficient and the heuristic Dice coefficient with competitive linking (Dice+C). Even the
simple Model 1 achieves better results than the two Dice coefficient models.
It is instructive to analyze the alignment quality obtained in the EM training of
Model 1. Figure 3 shows the alignment quality over the iteration numbers of Model 1.
We see that the first iteration of Model 1 achieves significantly worse results than the
Dice coefficient, but by only the second iteration, Model 1 gives better results than the
Dice coefficient.
38
Computational Linguistics Volume 29, Number 1
Table 6
Effect of using more alignments in training fertility models on alignment error rate (Verbmobil
task). Body of table presents error rate percentages.
Size of training corpus
Training scheme Alignment set 0.5K 2K 8K 34K
Viterbi 17.8 12.6 8.6 6.6
15H5334363 +neighbors 16.4 11.7 8.0 5.7
+pegging 16.4 11.2 8.2 5.7
Viterbi 24.1 16.0 11.6 8.6
1525334353 +neighbors 22.9 14.2 9.8 7.6
+pegging 22.0 13.3 9.7 6.9
Table 7
Effect of using more alignments in training fertility models on alignment error rate (Hansards
task). Body of table presents error rate percentages.
Size of training corpus
Training scheme Alignment set 0.5K 8K 128K
Viterbi 25.8 20.3 12.6
15H10334363 +neighbors 25.9 20.3 12.5
+pegging 25.8 19.9 12.6
Viterbi 41.9 25.1 17.6
1525334353 +neighbors 41.7 24.8 16.1
+pegging 41.2 23.7 15.8
6.3 Model 2 versus HMM
An important result of these experiments is that the hidden Markov alignment model
achieves significantly better results than Model 2. We attribute this to the fact that the
HMM is a homogeneous first-order alignment model, and such models are able to
better represent the locality and monotonicity properties of natural languages. Both
models have the important property of allowing an efficient implementation of the
EM algorithm (Section 3). On the largest Verbmobil task, the HMM achieves an im-
provement of 3.8% over Model 2. On the largest Hansards task, the improvement is
8.7%. Interestingly, this advantage continues to hold after bootstrapping more refined
models. On Model 4, the improvement is 1.4% and 4.8%, respectively.
We conclude that it is important to bootstrap the refined alignment models with
good initial parameters. Obviously, if we use Model 2 for bootstrapping, we eventually
obtain a poor local optimum.
6.4 The Number of Alignments in Training
In Tables 6 and 7, we compare the results obtained by using different numbers of
alignments in the training of the fertility-based alignment models. We compare the
three different approaches described in Section 3: using only the Viterbi alignment,
using in addition the neighborhood of the Viterbi alignment, and using the pegged
alignments. To reduce the training time, we restrict the number of pegged alignments
by using only those in which Pr(f, a | e) is not much smaller than the probability of the
Viterbi alignment. This reduces the training time drastically. For the large Hansards
39
Och and Ney Comparison of Statistical Alignment Models
Table 8
Computing time on the 34K Verbmobil task (on 600 MHz Pentium III machine).
Seconds per iteration
Alignment set Model 3 Model 4 Model 5
Viterbi 48.0 251.0 248.0
+neighbors 101.0 283.0 276.0
+pegging 129.0 3,348.0 3,356.0
Table 9
Effect of smoothing on alignment error rate (Verbmobil task, Model 6). Body of table presents
error rate percentages.
Size of training corpus
Smoothing method 0.5K 2K 8K 34K
None 19.7 14.9 10.9 8.3
Fertility 18.4 14.3 10.3 8.0
Alignment 16.8 13.2 9.1 6.4
Alignment and fertility 16.4 11.7 8.0 5.7
corpus, however, there still is an unacceptably large training time. Therefore, we report
the results for only up to 128,000 training sentences.
The effect of pegging strongly depends on the quality of the starting point used
for training the fertility-based alignment models. If we use Model 2 as the starting
point, we observe a significant improvement when we use the neighborhood align-
ments and the pegged alignments. If we use only the Viterbi alignment, the results are
significantly worse than using additionally the neighborhood of the Viterbi alignment.
If we use HMM as the starting point, we observe a much smaller effect. We conclude
that using more alignments in training is a way to avoid a poor local optimum.
Table 8 shows the computing time for performing one iteration of the EM algo-
rithm. Using a larger set of alignments increases the training time for Model 4 and
Model 5 significantly. Since using the pegging alignments yields only a moderate
improvement in performance, all following results are obtained by using the neigh-
borhood of the Viterbi alignment without pegging.
6.5 Effect of Smoothing
Tables 9 and 10 show the effect on the alignment error rate of smoothing the alignment
and fertility probabilities. We observe a significant improvement when we smooth
the alignment probabilities and a minor improvement when we smooth the fertility
probabilities. An analysis of the alignments shows that smoothing the fertility proba-
bilities significantly reduces the frequently occurring problem of rare words forming
?garbage collectors? in that they tend to align with too many words in the other
language (Brown, Della Pietra, Della Pietra, Goldsmith, et al 1993).
Without smoothing, we observe early overfitting: The alignment error rate in-
creases after the second iteration of HMM, as shown in Figure 4. On the Verbmobil
task, the best alignment error rate is obtained in the second iteration. On the Hansards
task, the best alignment error rate is obtained in the sixth iteration. In iterations sub-
sequent to the second on the Verbmobil task and the sixth on the Hansards task, the
alignment error rate increases significantly. With smoothing of the alignment param-
40
Computational Linguistics Volume 29, Number 1
Figure 4
Overfitting on the training data with the hidden Markov alignment model using various
smoothing parameters (top: 34K Verbmobil task, bottom: 128K Hansards task).
41
Och and Ney Comparison of Statistical Alignment Models
Table 10
Effect of smoothing on alignment error rate (Hansards task, Model 6). Body of table presents
error rate percentages.
Size of training corpus
Smoothing method 0.5K 8K 128K 1470K
None 28.6 23.3 13.3 9.5
Fertility 28.3 22.5 12.7 9.3
Alignment 26.5 21.2 13.0 8.9
Alignment and fertility 25.9 20.3 12.5 8.7
Table 11
Effect of word classes on alignment error rate (Verbmobil task). Body of table presents error
rate percentages.
Size of training corpus
Word classes 0.5K 2K 8K 34K
No 16.5 11.7 8.0 6.3
Yes 16.4 11.7 8.0 5.7
Table 12
Effect of word classes on alignment error rate (Hansards task). Body of table presents error
rate percentages.
Size of training corpus
Word classes 0.5K 8K 128K 1470K
No 25.5 20.7 12.8 8.9
Yes 25.9 20.3 12.5 8.7
eters, we obtain a lower alignment error rate, overfitting occurs later in the process,
and its effect is smaller.
6.6 Alignment Models Depending on Word Classes
Tables 11 and 12 show the effects of including a dependence on word classes in the
alignment model, as described in Section 2.3. The word classes are always trained
on the same subset of the training corpus as is used for the training of the align-
ment models. We observe no significant improvement in performance as a result
of including dependence on word classes when a small training corpus is used. A
possible reason for this lack of improvement is that either the word classes them-
selves or the resulting large number of alignment parameters cannot be estimated
reliably using a small training corpus. When a large training corpus is used, however,
there is a clear improvement in performance on both the Verbmobil and the Hansards
tasks.
6.7 Using a Conventional Bilingual Dictionary
Tables 13 and 14 show the effect of using a conventional bilingual dictionary in training
on the Verbmobil and Hansards tasks, respectively. We compare the two methods for
using the dictionary described in Section 3.4. We observe that the method with a fixed
42
Computational Linguistics Volume 29, Number 1
Table 13
Effect of using a conventional dictionary on alignment error rate (Verbmobil task). Body of
table presents error rate percentages.
Size of training corpus
Bilingual dictionary 0.5K 2K 8K 34K
No 16.4 11.7 8.0 5.7
Yes/? var. 10.9 9.0 6.9 5.1
Yes/?+ = 8 9.7 7.6 6.0 5.1
Yes/?+ = 16 10.0 7.8 6.0 4.6
Yes/?+ = 32 10.4 8.5 6.4 4.7
Table 14
Effect of using a conventional dictionary on alignment error rate (Hansards task). Body of
table presents error rate percentages.
Size of training corpus
Bilingual dictionary 0.5K 8K 128K 1470K
No 25.9 20.3 12.5 8.7
Yes/? var. 23.3 18.3 12.3 8.6
Yes/?+ = 8 22.7 18.5 12.2 8.6
Yes/?+ = 16 23.1 18.7 12.1 8.6
Yes/?+ = 32 24.9 20.2 11.7 8.3
threshold of ?+ = 16 gives the best results. The method with a varying ? gives worse
results, but this method has one fewer parameter to be optimized on held-out data.
On small corpora, there is an improvement of up to 6.7% on the Verbmobil task
and 3.2% on the Hansards task, but when a larger training corpus is used, the im-
provements are reduced to 1.1% and 0.4%, respectively. Interestingly, the amount
of the overall improvement contributed by the use of a conventional dictionary is
small compared to the improvement achieved through the use of better alignment
models.
6.8 Generalized Alignments
In this section, we compare the results obtained using different translation directions
and using the symmetrization methods described in Section 4. Tables 15 and 16 show
precision, recall, and alignment error rate for the last iteration of Model 6 for both
translation directions. In this experiment, we use the conventional dictionary as well.
Particularly for the Verbmobil task, with the language pair German-English, we ob-
serve that for German as the source language the alignment error rate is much higher
than for English as source language. A possible reason for this difference in the align-
ment error rates is that the baseline alignment representation as a vector aJ1 does not
allow German word compounds (which occur frequently) to be aligned with more
than one English word.
The effect of merging alignments by forming the intersection, the union, or the
refined combination of the Viterbi alignments in both translation directions is shown in
Tables 17 and 18. Figure 5 shows the corresponding precision/recall graphs. By using
the refined combination, we can increase precision and recall on the Hansards task. The
lowest alignment error rate on the Hansards task is obtained by using the intersection
43
Och and Ney Comparison of Statistical Alignment Models
Table 15
Effect of training corpus size and translation direction on precision, recall, and alignment error
rate (Verbmobil task + dictionary). All figures are percentages.
English ? German German ? English
Corpus size Precision Recall AER Precision Recall AER
0.5K 87.6 93.1 10.0 77.9 80.3 21.1
2K 90.5 94.4 7.8 88.1 88.1 11.9
8K 92.7 95.7 6.0 90.2 89.1 10.3
34K 94.6 96.3 4.6 92.5 89.5 8.8
Table 16
Effect of training corpus size and translation direction on precision, recall, and alignment error
rate (Hansards task + dictionary). All figures are percentages.
English ? French French ? English
Corpus size Precision Recall AER Precision Recall AER
0.5K 73.0 83.8 23.1 68.5 79.1 27.8
8K 77.0 88.9 18.7 76.0 88.5 19.5
128K 84.5 93.5 12.1 84.6 93.3 12.2
1470K 89.4 94.7 8.6 89.1 95.2 8.6
Table 17
Effect of alignment combination on precision, recall, and alignment error rate (Verbmobil task
+ dictionary). All figures are percentages.
Intersection Union Refined method
Corpus size Precision Recall AER Precision Recall AER Precision Recall AER
0.5K 97.5 76.8 13.6 74.8 96.1 16.9 87.8 92.9 9.9
2K 97.2 85.6 8.6 84.1 96.9 10.6 91.3 94.2 7.4
8K 97.5 86.6 8.0 87.0 97.7 8.5 92.8 96.0 5.8
34K 98.1 87.6 7.2 90.6 98.4 6.0 94.0 96.9 4.7
Table 18
Effect of alignment combination on precision, recall, and alignment error rate (Hansards task +
dictionary). All figures are percentages.
Intersection Union Refined method
Corpus size Precision Recall AER Precision Recall AER Precision Recall AER
0.5K 91.5 71.3 18.7 63.4 91.6 29.0 75.5 84.9 21.1
8K 95.6 82.8 10.6 68.2 94.4 24.2 83.3 90.0 14.2
128K 96.7 90.0 6.3 77.8 96.9 16.1 89.4 94.4 8.7
1470K 96.8 92.3 5.2 84.2 97.6 11.3 91.5 95.5 7.0
44
Computational Linguistics Volume 29, Number 1
Figure 5
Effect of various symmetrization methods on precision and recall for different training corpus
sizes (top: Verbmobil task, bottom: Hansards task).
45
Och and Ney Comparison of Statistical Alignment Models
method. By forming a union or intersection of the alignments, we can obtain very high
recall or precision values on both the Hansards task and the Verbmobil task.
6.9 Effect of Alignment Quality on Translation Quality
Alignment models similar to those studied in this article have been used as a start-
ing point for refined phrase-based statistical machine translation systems (Alshawi,
Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al 2000). In Och
and Ney (2000), the overall result of the experimental evaluation has been that an
improved alignment quality yields an improved subjective quality of the statistical
machine translation system as well.
7. Conclusion
In this article, we have discussed in detail various statistical and heuristic word align-
ment models and described various modifications and extensions to models known in
the literature. We have developed a new statistical alignment model (Model 6) that has
yielded the best results among all the models we considered in the experiments we
have conducted. We have presented two methods for including a conventional bilin-
gual dictionary in training and described heuristic symmetrization algorithms that
combine alignments in both translation directions possible between two languages,
producing an alignment with a higher precision, a higher recall, or an improved align-
ment error rate.
We have suggested measuring the quality of an alignment model using the quality
of the Viterbi alignment compared to that achieved in a manually produced reference
alignment. This quality measure has the advantage of automatic evaluation. To pro-
duce the reference alignment, we have used a refined annotation scheme that reduces
the problems and ambiguities associated with the manual construction of a word
alignment.
We have performed various experiments to assess the effect of different alignment
models, training schemes, and knowledge sources. The key results of these experi-
ments are as follows:
? Statistical alignment models outperform the simple Dice coefficient.
? The best results are obtained with our Model 6. In general, very
important ingredients of a good model seem to be a first-order
dependence between word positions and a fertility model.
? Smoothing and symmetrization have a significant effect on the alignment
quality achieved by a particular model.
? The following methods have only a minor effect on the quality of
alignment achieved by a particular model:
? adding entries of a conventional bilingual dictionary to the
training data.
? making the alignment models dependent on word classes (as in
Models 4 and 5).
? increasing the number of alignments used in the approximation
of the EM algorithm for the fertility-based alignment models.
Further improvements in alignments are expected to be produced through the
adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment
46
Computational Linguistics Volume 29, Number 1
models based on word groups rather than single words (Och, Tillmann, and Ney
1999). The use of models that explicitly deal with the hierarchical structures of natural
language is very promising (Wu 1996; Yamada and Knight 2001).
We plan to develop structured models for the lexicon, alignment, and fertility prob-
abilities using maximum-entropy models. This is expected to allow an easy integration
of more dependencies, such as in a second-order alignment model, without running
into the problem of the number of alignment parameters getting unmanageably large.
Furthermore, it will be important to verify the applicability of the statistical align-
ment models examined in this article to less similar language pairs such as Chinese-
English and Japanese-English.
Appendix: Efficient Training of Fertility-Based Alignment Models
In this Appendix, we describe some methods for efficient training of fertility-based
alignment models. The core idea is to enumerate only a small subset of good align-
ments in the E-step of the EM algorithm instead of enumerating all (I + 1)J align-
ments. This small subset of alignments is the set of neighboring alignments of the
best alignment that can be found by a greedy search algorithm. We use two operators
to transform alignments: The move operator m[i,j](a) changes aj := i, and the swap
operator s[j1,j2](a) exchanges aj1 and aj2 . The neighborhood N (a) of an alignment a is
then defined as the set of all alignments that differ by one move or one swap from
alignment a:
N (a) = {a? : ?i,j : a? = m[i,j](a) ? ?j1,j2 : a? = s[j1,j2](a)} (43)
For one step of the greedy search algorithm, we define the following hill-climbing
operator (for Model 3), which yields for an alignment a the most probable alignment
b(a) in the neighborhood N (a):
b(a) = argmax
a??N (a)
p3(a? | e, f) (44)
Similarly, we define a hill-climbing operator for the other alignment models.
Straightforward Implementation
A straightforward count collection procedure for a sentence pair (f,e) following the
description in Brown, Della Pietra, Della Pietra, and Mercer (1993) is as follows:5
1. Calculate the Viterbi alignment of Model 2: a0 := argmaxa p2(f, a | e),
n := 0.
2. While in the neighborhood N (an) an alignment a? exists with
p3(a? | e, f) > p3(an | e, f):
(a) Set an+1 to the best alignment in the neighborhood.
(b) n := n + 1.
3. Calculate
s :=
?
a?N (an)
Pr(f, a | e) (45)
5 To simplify the description, we ignore the process known as pegging, which generates a bigger number
of alignments considered in training.
47
Och and Ney Comparison of Statistical Alignment Models
4. For each alignment a in the neighborhood N (an)
(a) Calculate
p := Pr(a | e, f) (46)
=
Pr(f, a | e)
s
(47)
(b) For each j := 1 to J: Increase alignment counts
c(j | aj, m, l; e, f) := c(j | aj, m, l; e, f) + p (48)
(c) For each i := 1 to I: Increase the fertility counts with p:
c(?i | ei; e, f) := c(?i | ei; e, f) + p (49)
(d) Increase the counts for p1:
c(1; e, f) := c(1; e, f) + p ? ?0 (50)
A major part of the time in this procedure is spent on calculating the probability
Pr(a? | e, f) of an alignment a?. In general, this takes about (I + J) operations. Brown,
Della Pietra, Della Pietra, and Mercer (1993) describe a method for obtaining Pr(a? |
e, f) incrementally from Pr(a | e, f) if alignment a differs only by moves or swaps from
alignment a?. This method results in a constant number of operations that is sufficient
to calculate the score of a move or the score of a swap.
Refined Implementation: Fast Hill Climbing
Analyzing the training program reveals that most of the time is spent on the compu-
tation of the costs of moves and swaps. To reduce the number of operations required
in such computation, these values are cached in two matrices. We use one matrix for
the scores of a move aj := i:
Mij =
Pr(m[i,j](a) | e, f)
Pr(a | e, f) ? (1 ? ?(aj, i)) (51)
and an additional matrix for the scores of a swap of aj and aj? :
Sjj ? =
?
?
?
Pr(s[j,j?](a) | e, f)
Pr(a | e, f) ? (1 ? ?(aj, aj
?)) if j < j?
0 otherwise
(52)
During the hill climbing, it is sufficient, after making a move or a swap, to update
only those rows or columns in the matrix that are affected by the move or swap. For
example, when performing a move aj := i, it is necessary to
? update in matrix M the columns j? with aj? = aj or aj? = i.
? update in matrix M the rows aj and i.
? update in matrix S the rows and the columns j? with aj? = aj or aj? = i.
Similar updates have to be performed after a swap. In the count collection (step 3), it
is possible to use the same matrices as obtained in the last hill-climbing step.
By restricting in this way the number of matrix entries that need to be updated,
it is possible to reduce the number of operations in hill climbing by about one order
of magnitude.
48
Computational Linguistics Volume 29, Number 1
Refined Implementation: Fast Count Collection
The straightforward algorithm given for performing the count collection has the dis-
advantage of requiring that all alignments in the neighborhood of alignment a be
enumerated explicitly. In addition, it is necessary to perform a loop over all targets
and a loop over all source positions to update the lexicon/alignment and the fertil-
ity counts. To perform the count collection in an efficient way, we use the fact that
the alignments in the neighborhood N (a) are very similar. This allows the sharing of
many operations in the count collection process.
To efficiently obtain the alignment and lexicon probability counts, we introduce the
following auxiliary quantities that use the move and swap matrices that are available
after performing the hill climbing described above:
? probability of all alignments in the neighborhood N (a):
Pr(N (a) | e, f) =
?
a??N (a)
Pr(a? | e, f) (53)
= Pr(a | e, f) ?
?
?1 +
?
i,j
Mij +
?
j,j?
Sjj ?
?
? (54)
? probability of all alignments in the neighborhood N (a) that differ in
position j from alignment a:
Pr(Nj(a) | e, f) =
?
a??N (a)
Pr(a? | e, f)(1 ? ?(aj, a?j)) (55)
= Pr(a | e, f)
?
?
?
i
Mij +
?
j?
(Sjj ? + Sj?j)
?
? (56)
For the alignment counts c(j | i; e, f) and the lexicon counts c(f | e; e, f), we have
c(j | i; e, f) =
{
Pr(N (a) | e, f)?Pr(Nj(a) | e, f) if i=aj
Pr(a | e, f)
(
Mij +
?
j? ?(aj? , i)?(Sjj ?+Sj?j)
)
if i =aj
(57)
c(f | e; e, f) =
?
i
?
j
c(j | i; e, f) ? ?(f , fj) ? ?(e, ei) (58)
To obtain the fertility probability counts and the count for p1 efficiently, we intro-
duce the following auxiliary quantities:
? probability of all alignments that have an increased fertility for position i:
Pr(N+1i (a) | e, f) = Pr(a | f, e)
?
?
?
j
(1 ? ?(aj, i)) ? Mij
?
? (59)
? probability of all alignments that have a decreased fertility for position i:
Pr(N?1i (a) | e, f) = Pr(a | e, f)
?
?
?
j
?(aj, i)
?
i?
Mi?j
?
? (60)
49
Och and Ney Comparison of Statistical Alignment Models
? probability of all alignments that have an unchanged fertility for posi-
tion i:
Pr(N+0i (a) | e, f) = Pr(N (a) | e, f)
? Pr(N+1i (a) | e, f)? Pr(N
?1
i (a) | e, f) (61)
These quantities do not depend on swaps, since a swap does not change the fertilities
of an alignment. For the fertility counts, we have:
c(? | e; e, f) =
?
i
?(e, ei)
?
k
Pr(N+ki (a) | e, f)?(?i + k,?) (62)
For p1, we have:
c(1; e, f) =
?
k
Pr(N+k0 (a) | e, f)(?0 + k) (63)
Using the auxiliary quantities, a count collection algorithm can be formulated that
requires about O(max(I, J)2) operations. This is one order of magnitude faster than the
straightforward algorithm described above. In practice, we observe that the resulting
training is 10?20 times faster.
Acknowledgments
This work has been partially supported as
part of the Verbmobil project (contract
number 01 IV 701 T4) by the German
Federal Ministry of Education, Science,
Research and Technology and as part of the
EuTrans project (project number 30268) by
the European Union. In addition, this work
has been partially supported by the
National Science Foundation under grant
no. IIS-9820687 through the 1999 Workshop
on Language Engineering, Center for
Language and Speech Processing, Johns
Hopkins University. All work for this paper
was done at RWTH Aachen.
References
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John D. Lafferty, I. Dan
Melamed, David Purdy, Franz J. Och,
Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation. Final
Report, JHU Workshop. Available at
http://www.clsp.jhu.edu/ws99/projects
/mt/final report/mt-final-report.ps.
Alshawi, Hiyan, Srinivas Bangalore, and
Shona Douglas. 1998. Automatic
acquisition of hierarchical transduction
models for machine translation. In
COLING?ACL ?98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, volume 1,
pages 41?47, Montreal, Canada, August.
Baum, L. E. 1972. An inequality and
associated maximization technique in
statistical estimation for probabilistic
functions of Markov processes.
Inequalities, 3:1?8.
Berger, Adam L., Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Harry
Printz, and Lubos Ures. 1994. The
Candide system for machine translation.
In Proceedings of the ARPA Workshop on
Human Language Technology,
pages 157?162, Plainsboro, New Jersey,
March.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, M. J. Goldsmith,
J. Hajic, R. L. Mercer, and S. Mohanty.
1993. But dictionaries are data too. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 202?205,
Plainsboro, New Jersey, March.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Brown, Ralf D. 1997. Automated dictionary
extraction for ?knowledge-free?
example-based translation. In Seventh
International Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-97), pages 111?118, Santa Fe, New
Mexico, July.
Dagan, Ido, Kenneth W. Church, and
50
Computational Linguistics Volume 29, Number 1
William A. Gale. 1993. Robust bilingual
word alignment for machine aided
translation. In Proceedings of the Workshop
on Very Large Corpora, pages 1?8,
Columbus, Ohio, June.
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society,
Series B, 39(1):1?22.
Diab, Mona. 2000. An unsupervised method
for multilingual word sense tagging using
parallel corpora: A preliminary
investigation. In ACL-2000 Workshop on
Word Senses and Multilinguality, pages 1?9,
Hong Kong, October.
Dice, Lee R. 1945. Measures of the amount
of ecologic association between species.
Journal of Ecology, 26:297?302.
Garc??a-Varea, Ismael, Francisco Casacuberta,
and Hermann Ney. 1998. An iterative,
DP-based search algorithm for statistical
machine translation. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP?98), pages 1235?1238,
Sydney, Australia, November.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 228?235, Toulouse, France, July.
Huang, Jin-Xia and Key-Sun Choi. 2000.
Chinese-Korean word alignment based on
linguistic comparison. In Proceedings of the
38th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 392?399, Hong Kong, October.
Ker, Sue J. and Jason S. Chang. 1997. A
class-based approach to word alignment.
Computational Linguistics, 23(2):313?343.
Kneser, Reinhard and Hermann Ney. 1993.
Improved clustering techniques for
class-based statistical language modelling.
In European Conference on Speech
Communication and Technology,
pages 973?976, Berlin, Germany,
September.
Knight, Kevin. 1999a. Decoding complexity
in word-replacement translation models.
Computational Linguistics, 25(4):607?615.
Knight, Kevin. 1999b. A Statistical MT
Tutorial Workbook. Available at
http://www.isi.edu/natural-language/
mt/wkbk.rtf.
Melamed, I. Dan. 1998. Manual annotation
of translational equivalence: The blinker
project. Technical Report 98-07, Institute
for Research in Cognitive Science,
Philadelphia.
Melamed, I. Dan. 2000. Models of
translational equivalence among words.
Computational Linguistics, 26(2):221?249.
Ney, Hermann, Sonja Nie?en, Franz J. Och,
Hassan Sawaf, Christoph Tillmann, and
Stephan Vogel. 2000. Algorithms for
statistical translation of spoken language.
IEEE Transactions on Speech and Audio
Processing, 8(1):24?36.
Nie?en, Sonja, Stephan Vogel, Hermann
Ney, and Christoph Tillmann. 1998. A
DP-based search algorithm for statistical
machine translation. In COLING-ACL ?98:
36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 960?967, Montreal,
Canada, August.
Och, Franz J. 2000. Giza++: Training of
statistical translation models. Available at
http://www-i6.informatik.rwth-
aachen.de/?och/software/GIZA++.html.
Och, Franz J. and Hermann Ney. 2000. A
comparison of alignment models for
statistical machine translation. In COLING
?00: The 18th International Conference on
Computational Linguistics, pages 1086?1090,
Saarbru?cken, Germany, August.
Och, Franz J., Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pages 20?28, University of Maryland,
College Park, June.
Och, Franz J., Nicola Ueffing, and Hermann
Ney. 2001. An efficient A* search
algorithm for statistical machine
translation. In Data-Driven Machine
Translation Workshop, pages 55?62,
Toulouse, France, July.
Och, Franz J. and Hans Weber. 1998.
Improving statistical natural language
translation with categories and rules. In
COLING-ACL ?98: 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 985?989,
Montreal, Canada, August.
Simard, M., G. Foster, and P. Isabelle. 1992.
Using cognates to align sentences in
bilingual corpora. In Fourth International
Conference on Theoretical and Methodological
Issues in Machine Translation (TMI-92),
pages 67?81, Montreal, Canada.
Smadja, Frank, Kathleen R. McKeown, and
Vasileios Hatzivassiloglou. 1996.
Translating collocations for bilingual
lexicons: A statistical approach.
Computational Linguistics, 22(1):1?38.
51
Och and Ney Comparison of Statistical Alignment Models
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In COLING ?96: The 16th International
Conference on Computational Linguistics,
pages 836?841, Copenhagen, Denmark,
August.
Wahlster, Wolfgang, editor. 2000. Verbmobil:
Foundations of speech-to-speech translations.
Springer Verlag, Berlin.
Wang, Ye-Yi and Alex Waibel. 1998. Fast
decoding for statistical machine
translation. In Proceedings of the
International Conference on Speech and
Language Processing, pages 1357?1363,
Sydney, Australia, November.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Conference of the Association for
Computational Linguistics (ACL ?96),
pages 152?158, Santa Cruz, California,
June.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 523?530, Toulouse, France,
July.
Yarowsky, David, Grace Ngai, and Richard
Wicentowski. 2001. Inducing multilingual
text analysis tools via robust projection
across aligned corpora. In Human
Language Technology Conference, pages
109?116, San Diego, California, March.
Yarowsky, David and Richard Wicentowski.
2000. Minimally supervised
morphological analysis by multimodal
alignment. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 207?216, Hong
Kong, October.
c? 2003 Association for Computational Linguistics
Word Reordering and a Dynamic
Programming Beam Search Algorithm for
Statistical Machine Translation
Christoph Tillmann? Hermann Ney?
IBM T. J. Watson Research Center RWTH Aachen
In this article, we describe an efficient beam search algorithm for statistical machine translation
based on dynamic programming (DP). The search algorithm uses the translation model presented
in Brown et al (1993). Starting from a DP-based solution to the traveling-salesman problem,
we present a novel technique to restrict the possible word reorderings between source and target
language in order to achieve an efficient search algorithm. Word reordering restrictions especially
useful for the translation direction German to English are presented. The restrictions are gener-
alized, and a set of four parameters to control the word reordering is introduced, which then can
easily be adopted to new translation directions. The beam search procedure has been successfully
tested on the Verbmobil task (German to English, 8,000-word vocabulary) and on the Canadian
Hansards task (French to English, 100,000-word vocabulary). For the medium-sized Verbmobil
task, a sentence can be translated in a few seconds, only a small number of search errors occur,
and there is no performance degradation as measured by the word error criterion used in this
article.
1. Introduction
This article is about a search procedure for statistical machine translation (MT). The
task of the search procedure is to find the most likely translation given a source sen-
tence and a set of model parameters. Here, we will use a trigram language model and
the translation model presented in Brown et al (1993). Since the number of possible
translations of a given source sentence is enormous, we must find the best output
without actually generating the set of all possible translations; instead we would like
to focus on the most likely translation hypotheses during the search process. For this
purpose, we present a data-driven beam search algorithm similar to the one used in
speech recognition search algorithms (Ney et al 1992). The major difference between
the search problem in speech recognition and statistical MT is that MT must take into
account the different word order for the source and the target language, which does
not enter into speech recognition. Tillmann, Vogel, Ney, and Zubiaga (1997) proposes
a dynamic programming (DP)?based search algorithm for statistical MT that mono-
tonically translates the input sentence from left to right. The word order difference is
dealt with using a suitable preprocessing step. Although the resulting search proce-
dure is very fast, the preprocessing is language specific and requires a lot of manual
? IBM T. J. Watson Research Center, Yorktown Heights, NY 10598. E-mail: ctill@us.ibm.com. The research
reported here was carried out while the author was with Lehrstuhl fu?r Informatik VI, Computer
Science Department, RWTH Aachen.
? Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen, D-52056 Aachen,
Germany. E-mail: ney@informatik.rwth-aachen.de.
98
Computational Linguistics Volume 29, Number 1
work. Currently, most search algorithms for statistical MT proposed in the literature
are based on the A? concept (Nilsson 1971). Here, the word reordering can be easily
included in the search procedure, since the input sentence positions can be processed
in any order. The work presented in Berger et al (1996) that is based on the A? concept,
however, introduces word reordering restrictions in order to reduce the overall search
space.
The search procedure presented in this article is based on a DP algorithm to solve
the traveling-salesman problem (TSP). A data-driven beam search approach is pre-
sented on the basis of this DP-based algorithm. The cities in the TSP correspond to
source positions of the input sentence. By imposing constraints on the possible word
reorderings similar to that described in Berger et al (1996), the DP-based approach
becomes more effective: when the constraints are applied, the number of word re-
orderings is greatly reduced. The original reordering constraint in Berger et al (1996)
is shown to be a special case of a more general restriction scheme in which the word
reordering constraints are expressed in terms of simple combinatorical restrictions on
the processed sets of source sentence positions.1 A set of four parameters is given to
control the word reordering. Additionally, a set of four states is introduced to deal
with grammatical reordering restrictions (e.g., for the translation direction German to
English, the word order difference between the two languages is mainly due to the
German verb group. In combination with the reordering restrictions, a data-driven
beam search organization for the search procedure is proposed. A beam search prun-
ing technique is conceived that jointly processes partial hypotheses according to two
criteria: (1) The partial hypotheses cover the same set of source sentence positions,
and (2) the partial hypotheses cover sets C of source sentence positions of equal car-
dinality. A partial hypothesis is said to cover a set of source sentence positions when
exactly the positions in the set have already been processed in the search process. To
verify the effectiveness of the proposed techniques, we report and analyze results for
two translation tasks: the German to English Verbmobil task and French to English
Canadian Hansards task.
The article is structured as follows. Section 2 gives a short introduction to the trans-
lation model used and reports on other approaches to the search problem in statistical
MT. In Section 3, a DP-based search approach is presented, along with appropriate
pruning techniques that yield an efficient beam search algorithm. Section 4 reports
and analyzes translation results for the different translation directions. In Section 5,
we conclude with a discussion of the achieved results.
2. Previous Work
2.1 IBM Translation Approach
In this article, we use the translation model presented in Brown et al (1993), and the
mathematical notation we use here is taken from that paper as well: a source string
f J1 = f1 ? ? ? fj ? ? ? fJ is to be translated into a target string eI1 = e1 ? ? ? ei ? ? ? eI. Here, I is the
length of the target string, and J is the length of the source string. Among all possible
target strings, we will choose the string with the highest probability as given by Bayes?
1 The word reordering restriction used in the search procedure described in Berger et al (1996) is not
mentioned in Brown et al (1993), although exactly the translation model described there is used.
Equivalently, we use exactly the translation model described in Brown et al (1993) but try different
reordering restrictions for the DP-based search procedure.
99
Tillmann and Ney DP Beam Search for Statistical MT
Figure 1
Architecture of the statistical translation approach based on Bayes? decision rule.
decision rule:
e?I1 = arg max
eI1
{Pr(eI1 | f
J
1)}
= arg max
eI1
{Pr(eI1) ? Pr(f
J
1 | eI1)} (1)
Pr(eI1) is the language model of the target language, whereas Pr(f
J
1 | eI1) is the string
translation model. The language model probability is computed using a trigram lan-
guage model. The string translation probability Pr(f J1 | eI1) is modeled using a series of
five models of increasing complexity in training. Here, the model used for the trans-
lation experiments is the IBM-4 model. This model uses the same parameter set as
the IBM-5 model, which in preliminary experiments did not yield better translation
results. The actual implementation used during the experiments is described in Al-
Onaizan et al (1999) and in Och and Ney (2000). The argmax operation denotes the
search problem (i.e., the generation of the output sentence in the target language). The
overall architecture of the statistical translation approach is summarized in Figure 1.
In general, as shown in this figure, there may be additional transformations to make
the translation task simpler for the algorithm. The transformations may range from
simple word categorization to more complex preprocessing steps that require some
parsing of the source string. In this article, however, we will use only word catego-
100
Computational Linguistics Volume 29, Number 1
rization as an explicit transformation step. In the search procedure both the language
and the translation model are applied after the text transformation steps. The following
?types? of parameters are used for the IBM-4 translation model:
Lexicon probabilities: We use the lexicon probability p(f | e) for translating the
single target word e as the single source word f . A source word f may
be translated by the ?null? word e0 (i.e., it does not produce any target
word e). A translation probability p(f | e0) is trained along with the regular
translation probabilities.
Fertilities: A single target word e may be aligned to n = 0, 1 or more source words.
This is explicitly modeled by the fertility parameter ?(n | e): the probability
that the target word e is translated by n source words is ?(n | e). The
fertility for the ?null? word is treated specially (for details see Brown et al
[1993]). Berger et al (1996) describes the extension of a partial hypothesis
by a pair of target words (e?, e), where e? is not connected to any source
word f . In this case, the so-called spontaneous target word e? is accounted
for with the fertility. Here, the translation probability ?(0 | e?) and no-
translation probability p(f | e?).
Class-based distortion probabilities: When covering a source sentence position
j, we use distortion probabilities that depend on the previously covered
source sentence positions (we say that a source sentence position j is cov-
ered for a partial hypothesis when it is taken account of in the translation
process by generating a target word or the ?null? word e0 ). In Brown et
al. (1993), two types of distortion probabilities are distinguished: (1) the
leftmost word of a set of source words f aligned to the same target word
e (which is called the ?head?) is placed, and (2) the remaining source
words are placed. Two separate distributions are used for these two cases.
For placing the ?head? the center function center(i) (Brown et al [1993]
uses the notation i) is used: the average position of the source words
with which the target word ei?1 is aligned. The distortion probabilities
are class-based: They depend on the word class F(f ) of a covered source
word f as well as on the word class E(e) of the previously generated target
word e. The classes are automatically trained (Brown et al 1992).
When the IBM-4 model parameters are used during search, an input sentence can be
processed one source position at a time in a certain order primarily determined by the
distortion probabilities. We will use the following simplified set of translation model
parameters: lexicon probabilities p(f | e) and distortion probabilities p(j | j?, J). Here, j
is the currently covered input sentence position and j? is the previously covered input
sentence position. The input sentence length J is included, since we would like to think
of the distortion probability as normalized according to J. No fertility probabilities or
?null? word probabilities are used; thus each source word f is translated as exactly one
target word e and each target word e is translated as exactly one source word f . The
simplified notation will help us to focus on the most relevant details of the DP-based
search procedure. The simplified set of parameters leads to an unrealistic assumption
about the length of the source and target sentence, namely, I = J. During the translation
experiments we will, of course, not make this assumption. The implementation details
for using the full set of IBM-4 model parameters are given in Section 3.9.2.
101
Tillmann and Ney DP Beam Search for Statistical MT
2.2 Search Algorithms for Statistical Machine Translation
In this section, we give a short overview of search procedures used in statistical MT:
Brown et al (1990) and Brown et al (1993) describe a statistical MT system that is based
on the same statistical principles as those used in most speech recognition systems
(Jelinek 1976). Berger et al (1994) describes the French-to-English Candide translation
system, which uses the translation model proposed in Brown et al (1993). A detailed
description of the decoder used in that system is given in Berger et al (1996) but has
never been published in a paper: Throughout the search process, partial hypotheses
are maintained in a set of priority queues. There is a single priority queue for each
subset of covered positions in the source string. In practice, the priority queues are
initialized only on demand; far fewer than the full number of queues possible are actu-
ally used. The priority queues are limited in size, and only the 1,000 hypotheses with
the highest probability are maintained. Each priority queue is assigned a threshold
to select the hypotheses that are going to be extended, and the process of assigning
these thresholds is rather complicated. A restriction on the possible word reorderings,
which is described in Section 3.6, is applied.
Wang and Waibel (1997) presents a search algorithm for the IBM-2 translation
model based on the A? concept and multiple stacks. An extension of this algorithm
is demonstrated in Wang and Waibel (1998). Here, a reshuffling step on top of the
original decoder is used to handle more complex translation models (e.g., the IBM-3
model is added). Translation approaches that use the IBM-2 model parameters but are
based on DP are presented in Garc??a-Varea, Casacuberta, and Ney (1998) and Niessen
et al (1998). An approach based on the hidden Markov model alignments as used
in speech recognition is presented in Tillmann, Vogel, Ney, and Zubiaga (1997) and
Tillmann, Vogel, Ney, Zubiaga, and Sawaf (1997). This approach assumes that source
and target language have the same word order, and word order differences are dealt
with in a preprocessing stage. The work by Wu (1996) also uses the original IBM model
parameters and obtains an efficient search algorithm by restricting the possible word
reorderings using the so-called stochastic bracketing transduction grammar.
Three different decoders for the IBM-4 translation model are compared in Germann
et al (2001). The first is a reimplementation of the stack-based decoder described in
Berger et al (1996). The second is a greedy decoder that starts with an approximate
solution and then iteratively improves this first rough solution. The third converts
the decoding problem into an integer program (IP), and a standard software package
for solving IP is used. Although the last approach is guaranteed to find the optimal
solution, it is tested only for input sentences of length eight or shorter.
This article will present a DP-based beam search decoder for the IBM-4 translation
model. The decoder is designed to carry out an almost full search with a small number
of search errors and with little performance degradation as measured by the word error
criterion. A preliminary version of the work presented here was published in Tillmann
and Ney (2000).
3. Beam Search in Statistical Machine Translation
3.1 Inverted Alignment Concept
To explicitly describe the word order difference between source and target language,
Brown et al (1993) introduced an alignment concept, in which a source position j is
mapped to exactly one target position i:
regular alignment: j ? i = aj
102
Computational Linguistics Volume 29, Number 1
e
.
In
this
case
my
colleague
can
k
not
visit
on
the
fourth
of
May
m K S
you
a v M n
c
h
t
bF
e
s
u
c
h
e
n
.
n
d
e
a
n
n
o
e
g
e
e e
n
i a
i
I
i
ii
m
e
s
i
r
t
e
l
ll
l
a m
n
Figure 2
Regular alignment example for the translation direction German to English. For each German
source word there is exactly one English target word on the alignment path.
An example for this kind of alignment is given in Figure 2, in which each German
source position j is mapped to an English target position i. In Brown et al (1993), this
alignment concept is used for model IBM-1 through model IBM-5. For search purposes,
we use the inverted alignment concept as introduced in Niessen et al (1998) and Ney
et al (2000). An inverted alignment is defined as follows:
inverted alignment: i ? j = bi
Here, a target position i is mapped to a source position j. The coverage constraint for
an inverted alignment is not expressed by the notation: Each source position j should
be ?hit? exactly once by the path of the inverted alignment bI1 = b1 ? ? ? bi ? ? ? bI. The
advantage of the inverted alignment concept is that we can construct target sentence
hypotheses from bottom to top along the positions of the target sentence. Using the
inverted alignments in the maximum approximation, we rewrite equation (1) to obtain
the following search criterion, in which we are looking for the most likely target
103
Tillmann and Ney DP Beam Search for Statistical MT
Figure 3
Illustration of the transitions in the regular and in the inverted alignment model. The regular
alignment model (left figure) is used to generate the sentence from left to right; the inverted
alignment model (right figure) is used to generate the sentence from bottom to top.
sentence eI1 of length I = J for an observed source sentence f
J
1 of length J:
max
I
{
p(J | I) ? max
eI1
{p(eI1) ? p(f
J
1 | eI1)}
}
(2)
?= max
I
{
p(J | I) ? max
eI1
{
I
?
i=1
p(ei | ei?1, ei?2) ? max
bI1
I
?
i=1
[p(bi | bi?1, J) ? p(fbi | ei)]
}}
= max
I
{
p(J | I) ? max
eI1,b
I
1
{
I
?
i=1
[p(ei | ei?1, ei?2) ? p(bi | bi?1, J) ? p(fbi | ei)]
}}
The following notation is used: ei?1, ei?2 are the immediate predecessor target words,
ei is the word to be hypothesized, p(ei | ei?1, ei?2) denotes the trigram language model
probability, p(fbi | ei) denotes the lexicon probability for translating the target word ei
as source word fbi , and p(bi | bi?1, J) is the distortion probability for covering source
position bi after source position bi?1. Note that in equation (2) two products over i are
merged into a single product over i. The translation probability p(f J1 | eI1) is computed in
the maximum approximation using the distortion and the lexicon probabilities. Finally,
p(J | I) is the sentence length model, which will be dropped in the following (it is not
used in the IBM-4 translation model). For each source sentence f J1 to be translated, we
are searching for the unknown mapping that optimizes equation (2):
i ? (bi, ei)
In Section 3.3, we will introduce an auxiliary quantity that can be evaluated recursively
using DP to find this unknown mapping. We will explicitly take care of the coverage
constraint by introducing a coverage set C of source sentence positions that have
already been processed. Figure 3 illustrates the concept of the search algorithm using
inverted alignments: Partial hypotheses are constructed from bottom to top along the
positions of the target sentence. Partial hypotheses of length i?1 are extended to obtain
partial hypotheses of the length i. Extending a partial hypothesis means covering a
source sentence position j that has not yet been covered. For a given grid point in the
104
Computational Linguistics Volume 29, Number 1
Table 1
DP-based algorithm for solving traveling-salesman problems due to Held and Karp. The
outermost loop is over the cardinality of subsets of already visited cities.
input: cities j = 1, . . . , J with distance matrix djj ?
initialization: D({k}, k) := d1k
for each path length c = 2, . . . , J do
for each pair (C, j), where C ? {2, . . . , J} and j ? C and |C| = c do
D(C, j) = min
j??C\{j}
{djj ? + D(C\{j}, j?)}
traceback:
? find shortest tour: D? = min
k?{2,...,J}
[D({2, . . . , J}, k) + dk1]
? recover optimal sequence of cities
translation lattice, the unknown target word sequence can be obtained by tracing back
the translation decisions to the partial hypothesis at stage i = 1. The grid points are
defined in Section 3.3. In the left part of the figure the regular alignment concept is
shown for comparison purposes.
3.2 Held and Karp Algorithm for Traveling-Salesman Problem
Held and Karp (1962) presents a DP approach to solve the TSP, an optimization prob-
lem that is defined as follows: Given are a set of cities {1, . . . , J} and for each pair
of cities j, j? the cost djj ? > 0 for traveling from city j to city j?. We are looking for
the shortest tour, starting and ending in city 1, that visits all cities in the set of cities
exactly once. We are using the notation C for the set of cities, since it corresponds to
a coverage set of processed source positions in MT. A straightforward way to find
the shortest tour is by trying all possible permutations of the J cities. The resulting
algorithm has a complexity of O(J!). DP can be used, however, to find the shortest tour
in O(J2 ? 2J), which is a much smaller complexity for larger values of J. The approach
recursively evaluates the quantity D(C, j):
D(C, j) := costs of the partial tour starting in city 1, ending
in city j, and visiting all cities in C
Subsets of cities C of increasing cardinality c are processed. The algorithm, shown in
Table 1, works because not all permutations of cities have to be considered explicitly.
During the computation, for a pair (C, j), the order in which the cities in C have been
visited can be ignored (except j); only the costs for the best path reaching j has to be
stored. For the initialization the costs for starting from city 1 are set: D({k}, k) = d1k for
each k ? {2, . . . , |C|}. Then, subsets C of increasing cardinality are processed. Finally,
the cost for the optimal tour is obtained in the second-to-last line of the algorithm.
The optimal tour itself can be found using a back-pointer array in which the optimal
decision for each grid point (C, j) is stored.
Figure 4 illustrates the use of the algorithm by showing the ?supergraph? that is
searched in the Held and Karp algorithm for a TSP with J = 5 cities. When traversing
the lattice from left to right following the different possibilities, a partial path to a node
j corresponds to the subset C of all cities on that path together with the last visited
105
Tillmann and Ney DP Beam Search for Statistical MT
Figure 4
Illustration of the algorithm by Held and Karp for a traveling salesman problem with J = 5
cities. Not all permutations of cities have to be evaluated explicitly. For a given subset of cities
the order in which the cities have been visited can be ignored.
city j. Of all the different paths merging into the node j, only the partial path with the
smallest cost has to be retained for further computation.
3.3 DP-Based Algorithm for Statistical Machine Translation
In this section, the Held and Karp algorithm is applied to statistical MT. Using the
concept of inverted alignments as introduced in Section 3.1, we explicitly take care of
the coverage constraint by introducing a coverage set C of source sentence positions
that have already been processed. Here, the correspondence is according to the fact that
each source sentence position has to be covered exactly once, fulfilling the coverage
constraint. The cities of the more complex translation TSP correspond roughly to triples
(e?, e, j), the notation for which is given below. The final path output by the translation
algorithm will contain exactly one triple (e?, e, j) for each source position j.
The algorithm processes subsets of partial hypotheses with coverage sets C of
increasing cardinality c. For a trigram language model, the partial hypotheses are of
the form (e?, e, C, j), where e?, e are the last two target words, C is a coverage set for
the already covered source positions, and j is the last covered position. The target
word sequence that ends in e?, e is stored as a back pointer to the predecessor partial
hypothesis (and recursively to its predecessor hypotheses) and is not shown in the
notation. Each distance in the TSP now corresponds to the negative logarithm of the
product of the translation, distortion, and language model probabilities. The following
106
Computational Linguistics Volume 29, Number 1
Table 2
DP-based algorithm for statistical MT that consecutively processes subsets C of source
sentence positions of increasing cardinality.
input: source language string f1 ? ? ? fj ? ? ? fJ
initialization
for each cardinality c = 1, 2, . . . , J do
for each pair (C, j), where C ? {1, . . . , J} and j ? C and |C| = c do
for each pair of target words e?, e ? E
Qe?(e, C, j) = p(fj | e) max
e??
j??C\{j}
{p(j | j?, J) ? p(e | e?, e??) ? Qe??(e?, C\{j}, j?)}
traceback:
? find best end hypothesis: max
e,e? ,j
{p($ | e, e?) ? Qe?(e, {1, . . . , J}, j)}
? recover optimal word sequence
auxiliary quantity is defined:
Qe?(e, C, j) := probability of the best partial hypothesis (ei1, bi1), where
C = {bk | k = 1, . . . , i}, bi = j, ei = e, and ei?1 = e?
The above auxiliary quantity satisfies the following recursive DP equation:
Qe?(e, C, j) = p(fj | e) ? max
e??
j??C\{j}
{
p(j | j?, J) ? p(e | e?, e??) ? Qe??(e?, C\{j}, j
?
)
}
Here, j? is the previously covered source sentence position and e?, e?? are the predecessor
words. The DP equation is evaluated recursively for each hypothesis (e?, e, C, j). The
resulting algorithm is depicted in Table 2. Some details concerning the initialization
and the finding of the best target language string are presented in Section 3.4. p($ | e, e?)
is the trigram language probability for predicting the sentence boundary symbol $. The
complexity of the algorithm is O(E3 ? J2 ? 2J), where E is the size of the target language
vocabulary.
3.4 Verb Group Reordering: German to English
The above search space is still too large to translate even a medium-length input
sentence. On the other hand, only very restricted reorderings are necessary; for ex-
ample, for the translation direction German to English, the word order difference is
mostly restricted to the German verb group. The approach presented here assumes a
mostly monotonic traversal of the source sentence positions from left to right.2 A small
number of positions may be processed sooner than they would be in that monotonic
traversal. Each source position then generates a certain number of target words. The
restrictions are fully formalized in Section 3.5.
A typical situation is shown in Figure 5. When translating the sentence monotoni-
cally from left to right, the translation of the German finite verb kann, which is the left
verbal brace in this case, is skipped until the German noun phrase mein Kollege, which
is the subject of the sentence, is translated. Then, the right verbal brace is translated:
2 Also, this assumption is necessary for the beam search pruning techniques to work efficiently.
107
Tillmann and Ney DP Beam Search for Statistical MT
i
.
fourth
the
of
May
this
case
colleague
can
not
visit
In
e
e
F
a
k
n
n
m
i
dI
n
s
l
l
a
m K
o
S .
you
on
my
n
ie
i
n
i
e
m
l
l
e
g
e
b
e
s
u
c
h
e
n
c
h
t
i
e
r
t
e
n
a v
a
M
Figure 5
Word reordering for the translation direction German to English: The reordering is restricted
to the German verb group.
The infinitive besuchen and the negation particle nicht. The following restrictions are
used: One position in the source sentence may be skipped for a distance of up to L = 4
source positions, and up to two source positions may be moved for a distance of at
most R = 10 source positions (the notation L and R shows the relation to the handling
of the left and right verbal brace). To formalize the approach, we introduce four verb
group states S:
? Initial : A contiguous initial block of source positions is covered.
? Skip: One word may be skipped, leaving a ?hole? in the monotonic
traversal.
? Move: Up to two words may be ?moved? from later in the sentence.
? Cover : The sentence is traversed monotonically until the state Initial is
reached.
108
Computational Linguistics Volume 29, Number 1
Initial
Skip
Move Cover
11. vierten
5. Kollege
4. mein
1. In
3. Fall
2. diesem 12. Mai
6. kann
13. .
8. besuchen
7. nicht
10. am
9. Sie
Figure 6
Order in which the German source positions are covered for the German-to-English reordering
example given in Figure 5.
The states Move and Skip both allow a set of upcoming words to be processed sooner
than would be the case in the monotonic traversal. The state Initial is entered whenever
there are no uncovered positions to the left of the rightmost covered position. The
sequence of states needed to carry out the word reordering example in Figure 5 is
given in Figure 6. The 13 source sentence words are processed in the order shown.
A formal specification of the state transitions is given in Section 3.5. Any number of
consecutive German verb phrases in a sentence can be processed by the algorithm. The
finite-state control presented here is obtained from a simple analysis of the German-
to-English word reordering problem and is not estimated from the training data. It
can be viewed as an extension of the IBM-4 model distortion probabilities.
Using the above states, we define partial hypothesis extensions of the following
type:
(S ?, C\{j}, j?) ? (S, C, j)
Not only the coverage set C and the positions j, j?, but also the verb group states S,S ?,
are taken into account. For the sake of brevity, we have omitted the target language
words e, e? in the notation of the partial hypothesis extension. For each extension an
uncovered position is added to the coverage set C of the partial hypothesis, and the
verb group state S may change. A more detailed description of the partial hypoth-
esis extension for a certain state S is given in the next section in a more general
context. Covering the first uncovered position in the source sentence, we use the lan-
109
Tillmann and Ney DP Beam Search for Statistical MT
guage model probability p(e | $, $). Here, $ is the sentence boundary symbol, which
is thought to be at position 0 in the target sentence. The search starts in the hypoth-
esis (Initial , {?}, 0). {?} denotes the empty set, where no source sentence position is
covered. The following recursive equation is evaluated:
Qe?(e,S, C, j) (3)
= p(fj | e) max
e?? ,S? ,j?
(S? ,C\{j},j?)?(S,C,j)
j??C\{j}
{p(j | j?, J) ? p(e | e?, e??) ? Qe??(e?,S ?, C\{j}, j?)}
The search ends in the hypotheses (Initial , {1, . . . , J}, j); the last covered position may
be in the range j ? {J?L, . . . , J}, because some source positions may have been skipped
at the end of the input sentence. {1, . . . , J} denotes a coverage set including all positions
from position 1 to position J. The final translation probability QF is
QF = max
e,e?
j?{J?L,...,J}
p($ | e, e?) ? Qe?(e, Initial , {1, . . . , J}, j) (4)
where p($ | e, e?) denotes the trigram language model, which predicts the sentence
boundary $ at the end of the target sentence. QF can be obtained using an algorithm
very similar to the one given in Table 2. The complexity of the verb group reordering
for the translation direction German to English is O(E3 ? J ? (R2 ? L ? R)), as shown in
Tillmann (2001).
3.5 Word Reordering: Generalization
For the translation direction English to German, the word reordering can be restricted
in a similar way as for the translation direction German to English. Again, the word
order difference between the two languages is mainly due to the German verb group.
During the translation process, the English verb group is decomposed as shown in
Figure 7. When the sentence is translated monotonically from left to right, the trans-
lation of the English finite verb can is moved, and it is translated as the German left
verbal brace before the English noun phrase my colleague, which is the subject of the
sentence. The translations of the infinitive visit and of the negation particle not are
skipped until later in the translation process. For this translation direction, the trans-
lation of one source sentence position may be moved for a distance of up to L = 4
source positions, and the translation of up to two source positions may be skipped
for a distance of up to R = 10 source positions (we take over the L and R notation
from the previous section). Thus, the role of the skipping and the moving are simply
reversed with respect to their roles in German-to-English translation. For the example
translation in Figure 7, the order in which the source sentence positions are covered
is given in Figure 8.
We generalize the two approaches for the different translation directions as fol-
lows: In both approaches, we assume that the source sentence is mainly processed
monotonically. A small number of upcoming source sentence positions may be pro-
cessed earlier than they would be in the monotonic traversal: The states Skip and Move
are used as explained in the preceding section. The positions to be processed outside
the monotonic traversal are restricted as follows:
? The number of positions dealt with in the states Move and Skip is
restricted.
? There are distance restrictions on the source positions processed in those
states.
110
Computational Linguistics Volume 29, Number 1
.
 
In
diesem
Fall
kann
mein
Kollege
Sie
am
vierten
Mai
nicht
besuchen
.
n h
c
o
n
o
v y
o
u 
o
n o
u
u
o MI t
i
s
c
a
s
e
m
y
l
l
e
a
g
e
c
a
n t
i
s
i
t
t
h
e
f
t
h
r
f a
y
Figure 7
Word reordering for the translation direction English to German: The reordering is restricted
to the English verb group.
These restrictions will be fully formalized later in this section. In the state Move, some
source sentence positions are ?moved? from later in the sentence to earlier. After source
sentence positions are moved, they are marked, and the translation of the sentence is
continued monotonically, keeping track of the positions already covered. To formalize
the approach, we introduce four reordering states S:
? Initial : A contiguous initial block of source positions is covered.
? Skip: A restricted number of source positions may be skipped, leaving
?holes? in the monotonic traversal.
? Move: A restricted number of words may be ?moved? from later in the
sentence.
? Cover : The sentence is traversed monotonically until the state Initial is
reached.
To formalize the approach, the following notation is introduced:
rmax(C) = max
c?C
c
111
Tillmann and Ney DP Beam Search for Statistical MT
InitialSkip
Cover
Move
1. In
13. not
2. this
3. case
6. colleague
14. visit
15. . 4. can
5. my
7. you
8. on
9. the
10. fourth
11. of
12. May
Figure 8
Order in which the English source positions are covered for the English-to-German reordering
example given in Figure 7.
lmin(C) = min
c/?C
c
u(C) = card({c | c /? C and c < rmax(C)})
m(C) = card({c | c ? C and c > lmin(C)})
w(C) = rmax(C)? lmin(C)
rmax(C) is the rightmost covered and lmin(C) is the leftmost uncovered source position.
u(C) is the number of ?skipped? positions, and m(C) is the number of ?moved? po-
sitions. The function card(?) returns the cardinality of a set of source positions. The
function w(C) describes the ?window? size in which the word reordering takes place.
A procedural description for the computation of the set of successor hypotheses for
a given partial hypothesis (S, C, j) is given in Table 3. There are restrictions on the
possible successor states: A partial hypothesis in state Skip cannot be expanded into
a partial hypothesis in state Move and vice versa. If the coverage set for the newly
generated hypothesis covers a contiguous initial block of source positions, the state
Initial is entered. No other state S is considered as a successor state in this case (hence
the use of the continue statement in the procedural description). The set of successor
hypotheses Succ by which to extend the partial hypothesis (S, C, j) is computed using
the constraints defined by the values for numskip, widthskip, nummove, and widthmove ,
as explained in the Appendix. In particular, a source position k is discarded for ex-
tension if the ?window? restrictions are violated. Within the restrictions all possible
successors are computed. It can be observed that the set of successors, as computed
in Table 3, is never empty.
112
Computational Linguistics Volume 29, Number 1
Table 3
Procedural description to compute the set Succ of successor hypotheses by which to extend a
partial hypothesis (S, C, j).
input: partial hypothesis (S, C, j)
Succ := {?}
for each k /? C do
Set C? = C ? {k}
if u(C?) = 0
Succ := Succ ? (Initial, C?, k)
continue
if (S = Initial) or (S = Skip)
if w(C?) ? widthskip and u(C?) ? numskip
Succ := Succ ? (Skip, C?, k)
if (S = Initial) or (S = Move)
if k = lmin(C?) and w(C?) ? widthmove and m(C?) ? nummove
Succ := Succ ? (Move, C?, k)
if (S = Move) or (S = Cover)
if (lmin(C?) = k)
Succ := Succ ? (Cover, C?, k)
output: set Succ of successor hypotheses
There is an asymmetry between the two reordering states Move and Skip: While in
state Move, the algorithm is not allowed to cover the position lmin(C). It must first enter
the state Cover to do so. In contrast, for the state Skip, the newly generated hypothesis
always remains in the state Skip (until the state Initial is entered.) This is motivated
by the word reordering for the German verb group. After the right verbal brace has
been processed, no source words may be moved into the verbal brace from later in
the sentence. There is a redundancy in the reorderings: The same reordering might be
carried out using either the state Skip or Move, especially if widthskip and widthmove
are about the same. The additional computational burden is alleviated somewhat by
the fact that the pruning, as introduced in Section 3.8, does not distinguish hypotheses
according to the states. A complexity analysis for different reordering constraints is
given in Tillmann (2001).
3.6 Word Reordering: IBM-Style Restrictions
We now compare the new word reordering approach with the approach used in Berger
et al (1996). In the approach presented in this article, source sentence words are aligned
with hypothesized target sentence words.3 When a source sentence word is aligned, we
say its position is covered. During the search process, a partial hypothesis is extended
by choosing an uncovered source sentence position, and this choice is restricted. Only
one of the first n uncovered positions in a coverage set may be chosen, where n is
set to 4. This choice is illustrated in Figure 9. In the figure, covered positions are
marked by a filled circle, and uncovered positions are marked by an unfilled circle.
Positions that may be covered next are marked by an unfilled square. The restrictions
for a coverage set C can be expressed in terms of the expression u(C) defined in the
previous section: The number of uncovered source sentence positions to the left of
the rightmost covered position. Demanding u(C) ? 3, we obtain the S3 restriction
3 In Berger et al (1996), a morphological analysis is carried out and word morphemes are processed
during the search. Here, we process only full-form words.
113
Tillmann and Ney DP Beam Search for Statistical MT
uncovered position for extension
covered position
uncovered position
J1 j
Figure 9
Illustration of the IBM-style reordering constraint.
introduced in the Appendix. An upper bound of O(E3 ? J4) for the word reordering
complexity is given in Tillmann (2001).
3.7 Empirical Complexity Calculations
In order to demonstrate the complexity of the proposed reordering constraints, we
have modified our translation algorithm to show, for the different reordering con-
straints, the overall number of successor states generated by the algorithm given in
Table 3. The number of successors shown in Figure 10 is counted for a pseudotransla-
tion task in which a pseudo?source word x is translated into the identically pseudo?
target word x. No actual optimization is carried out; the total number of successors
is simply counted as the algorithm proceeds through subsets of increasing cardinality.
The complexity differences for the different reordering constraints result from the dif-
ferent number of coverage subsets C and corresponding reordering states S allowed.
For the different reordering constraints we obtain the following results (the abbrevia-
tions MON, GE, EG, and S3 are taken from the Appendix):
? MON: For this reordering restriction, a partial hypothesis is always
extended by the position lmin(C), hence the number of processed arcs is J.
? GE, EG: These two reordering constraints are very similar in terms of
complexity: The number of word reorderings is heavily restricted in
each. Actually, since the distance restrictions (expressed by the variables
widthskip and widthmove) apply, the complexity is linear in the length of
the input sentence J.
? S3: The S3 reordering constraint has a complexity close to J4. Since no
distance restrictions for the skipped positions apply, the overall search
space is significantly larger than for the GE or EG restriction.
114
Computational Linguistics Volume 29, Number 1
1
10
100
1000
10000
100000
1e+06
1e+07
0 5 10 15 20 25 30 35 40 45 50
"J4"
"S3"
"EG"
"GE"
"MON"
Figure 10
Number of processed arcs for the pseudotranslation task as a function of the input sentence
length J (y-axis is given in log scale). The complexity for the four different reordering
constraints MON, GE, EG, and S3 is given. The complexity of the S3 constraint is close to J4.
3.8 Beam Search Pruning Techniques
To speed up the search, a beam search strategy is used. There is a direct analogy to
the data-driven search organization used in continuous-speech recognition (Ney et al
1992). The full DP search algorithm proceeds cardinality-synchronously over subsets
of source sentence positions of increasing cardinality. Using the beam search concept,
the search can be focused on the most likely hypotheses. The hypotheses Qe?(e, C, j)
are distinguished according to the coverage set C, with two kinds of pruning based
on this coverage set:
1. The coverage pruning is carried out separately for each coverage set C.
2. The cardinality pruning is carried out jointly for all coverage sets C with
the same cardinality c = c(C).
After the pruning is carried out, we retain for further consideration only hypothe-
ses with a probability close to the maximum probability. The number of surviving
hypotheses is controlled by four kinds of thresholds:
? the coverage pruning threshold tC
? the coverage histogram threshold nC
? the cardinality pruning threshold tc
? the cardinality histogram threshold nc
For the coverage and the cardinality pruning, the probability Qe?(e, C, j) is adjusted to
take into account the uncovered source sentence positions C? = {1, . . . , J}\C. To make
115
Tillmann and Ney DP Beam Search for Statistical MT
this adjustment, for a source word f at an uncovered source position, we precompute
an upper bound p?(f ) for the product of language model and lexicon probability:
p?(f ) = max
e??,e?,e
{p(e | e?, e??) ? p(f | e)}
The above optimization is carried out only over the word trigrams (e, e?, e??) that have
actually been seen in the training data. Additionally, the observation pruning described
below is applied to the possible translations e of a source word f . The upper bound
is used in the beam search concept to increase the comparability between hypotheses
covering different coverage sets. Even more benefit from the upper bound p?(f ) can be
expected if the distortion and the fertility probabilities are taken into account (Tillmann
2001). Using the definition of p?(f ), the following modified probability Q?e?(e, C, j) is used
to replace the original probability Qe?(e, C, j), and all pruning is applied to the new
probability:
Q?e?(e, C, j) = Qe?(e, C, j) ?
?
j?C?
p?(fj)
For the translation experiments, equation (3) is recursively evaluated over subsets of
source positions of equal cardinality. For reasons of brevity, we omit the state descrip-
tion S in equation (3), since no separate pruning according to the states S is carried out.
The set of surviving hypotheses for each cardinality c is referred to as the beam. The
size of the beam for cardinality c depends on the ambiguity of the translation task for
that cardinality. To fully exploit the speedup of the DP beam search, the search space
is dynamically constructed as described in Tillmann, Vogel, Ney, Zubiaga, and Sawaf
(1997), rather than using a static search space. To carry out the pruning, the maximum
probabilities with respect to each coverage set C and cardinality c are computed:
? Coverage pruning: Hypotheses are distinguished according to the subset
of covered positions C. The probability Q?(C) is defined:
Q?(C) = max
e,e?,j
Q?e?(e, C, j)
? Cardinality pruning: Hypotheses are distinguished according to the
cardinality c(C) of subsets C of covered positions. The probability Q?(c) is
defined for all hypotheses with c(C) = c:
Q?(c) = max
C
c(C)=c
Q?(C)
The coverage pruning threshold tC and the cardinality pruning threshold tc are used
to prune active hypotheses. We call this pruning translation pruning. Hypotheses are
pruned according to their translation probability:
Q?e?(e, C, j) < tC ? Q?(C)
Q?e?(e, C, j) < tc ? Q?(c)
For the translation experiments presented in Section 4, the negative logarithms of the
actual pruning thresholds tc and tC are reported. A hypothesis (e?, e, C, j) is discarded if
its probability is below the corresponding threshold. For the current experiments, the
116
Computational Linguistics Volume 29, Number 1
coverage and the cardinality threshold are constant for different coverage sets C and
cardinalities c. Together with the translation pruning, histogram pruning is carried
out: The overall number N(C) of active hypotheses for the coverage set C and the
overall number N(c) of active hypotheses for all subsets of a given cardinality may
not exceed a given number; again, different numbers are used for coverage and cardi-
nality pruning. The coverage histogram pruning is denoted by nC , and the cardinality
histogram pruning is denoted by nc:
N(C) > nC
N(c) > nc
If the numbers of active hypotheses for each coverage set C and cardinality c, N(C)
and N(c), exceed the above thresholds, only the partial hypotheses with the highest
translation probabilities are retained (e.g., we may use nC = 1,000 for the coverage
histogram pruning).
The third type of pruning conducted observation pruning: The number of words
that may be produced by a source word f is limited. For each source language word
f the list of its possible translations e is sorted according to
p(f | e) ? puni(e)
where puni(e) is the unigram probability of the target language word e. Only the best no
target words e are hypothesized during the search process (e.g., during the experiments
to hypothesize, the best no = 50 words was sufficient.
3.9 Beam Search Implementation
In this section, we describe the implementation of the beam search algorithm presented
in the previous sections and show how it is applied to the full set of IBM-4 model
parameters.
3.9.1 Baseline DP Implementation. The implementation described here is similar to
that used in beam search speech recognition systems, as presented in Ney et al (1992).
The similarities are given mainly in the following:
? The implementation is data driven. Both its time and memory
requirements are strictly linear in the number of path hypotheses
(disregarding the sorting steps explained in this section).
? The search procedure is developed to work most efficiently when the
input sentences are processed mainly monotonically from left to right.
The algorithm works cardinality-synchronously, meaning that all the
hypotheses that are processed cover subsets of source sentence positions
of equal cardinality c.
? Since full search is prohibitive, we use a beam search concept, as in
speech recognition. We use appropriate pruning techniques in connection
with our cardinality-synchronous search procedure.
Table 4 shows a two-list implementation of the search algorithm given in Table 2 in
which the beam pruning is included. The two lists are referred to as S and Snew: S
is the list of hypotheses that are currently expanded, and Snew is the list of newly
117
Tillmann and Ney DP Beam Search for Statistical MT
Table 4
Two-list implementation of a DP-based search algorithm for statistical MT.
input: source string f1 ? ? ? fj ? ? ? fJ
initial hypothesis lists: S = {($, $, {?}, 0)}
for each cardinality c = 1, 2, . . . , J do
Snew = {?}
for each hypothesis (e?, e, C, j?) ? S, where j? ? C and |C| = c do
Expand (e?, e, C, j?) using probabilities p(fj | e) ? p(j | j?, J) ? p(e | e?, e??)
Look up and add or update expanded hypothesis in Snew
Sort hypotheses in Snew according to translation score
Carry out cardinality pruning
Sort hypotheses in Snew according to coverage set C and translation score
Carry out coverage pruning
Bookkeeping of surviving hypotheses in Snew
S := Snew
output: get best target word sequence eI1 from bookkeeping array
generated hypotheses. The search procedure processes subsets of covered source sen-
tence positions of increasing cardinality. The search starts with S = {($, $, {?}, 0)},
where $ denotes the sentence start symbol for the immediate two predecessor words
and {?} denotes the empty coverage set, in which no source position is covered yet.
For the initial search state, the position last covered is set to 0. A set S of active
hypotheses is expanded for each cardinality c using lexicon model, language model,
and distortion model probabilities. The newly generated hypotheses are added to the
hypothesis set Snew; for hypotheses that are not distinguished according to our DP
approach, only the best partial hypothesis is retained for further consideration. This
so-called recombination is implemented as a set of simple lookup and update opera-
tions on the set Snew of partial hypotheses. During the partial hypothesis extensions,
an anticipated pruning is carried out: Hypotheses are discarded before they are con-
sidered for recombination and are never added to Snew. (The anticipated pruning is not
shown in Table 4. It is based on the pruning thresholds described in Section 3.8.) After
the extension of all partial hypotheses in S, a pruning step is carried out for the hy-
potheses in the newly generated set Snew. The pruning is based on two simple sorting
steps on the list of partial hypotheses Snew. (Instead of sorting the partial hypothe-
ses, we might have used hashing.) First, the partial hypotheses are sorted according
to their translation scores (within the implementation, all probabilities are converted
into translation scores by taking the negative logarithm ? log()). Cardinality prun-
ing can then be carried out simply by running down the list of hypotheses, starting
with the maximum-probability hypothesis, and applying the cardinality thresholds.
Then, the partial hypotheses are sorted a second time according to their coverage set
C and their translation score. After this sorting step, all partial hypotheses that cover
the same subset of source sentence positions are located in consecutive fragments in
the overall list of partial hypotheses. Coverage pruning is carried out in a single run
over the list of partial hypotheses: For each fragment corresponding to the same cov-
erage set C, the coverage pruning threshold is applied. The partial hypotheses that
survive the two pruning stages are then written into the so-called bookkeeping array
(Ney et al 1992). For the next expansion step, the set S is set to the newly generated
list of hypotheses. Finally, the target translation is constructed from the bookkeeping
array.
118
Computational Linguistics Volume 29, Number 1
3.9.2 Details for IBM-4 Model. In this section, we outline how the DP-based beam
search approach can be carried out using the full set of IBM-4 parameters. (More
details can be found in Tillmann [2001] or in the cited papers.) First, the full set of
IBM-4 parameters does not make the simplifying assumption given in Section 3.1,
namely, that source and target sentences are of equal length: Either a target word e
may be aligned with several source words (its fertility is greater than one) or a single
source word may produce zero, one, or two target words, as described in Berger et
al. (1996), or both. Zero target words are generated if f is aligned to the ?null? word
e0. Generating a single target word e is the regular case. Two target words (e?, e??)
may be generated. The costs for generating the target word e? are given by its fertility
?(0 | e?) and the language model probability; no lexicon probability is used. During the
experiments, we restrict ourselves to triples of target words (e, e?, e??) actually seen in the
training data. This approach is used for the French-to-English translation experiments
presented in this article.
Another approach for mapping a single source language word to several target
language words involves preprocessing by the word-joining algorithm given in Till-
mann (2001), which is similar to the approach presented in Och, Tillmann, and Ney
(1999). Target words are joined during a training phase, and several joined target lan-
guage words are dealt with as a new lexicon entry. This approach is used for the
German-to-English translation experiments presented in this article.
In order to deal with the IBM-4 fertility parameters within the DP-based concept,
we adopt the distinction between open and closed hypotheses given in Berger et al
(1996). A hypothesis is said to be open if it is to be aligned with more source positions
than it currently is (i.e., at least two). Otherwise it is called closed. The difference
between open and closed is used to process the input sentence one position a time
(for details see Tillmann 2001). The word reordering restrictions and the beam search
pruning techniques are directly carried over to the full set of IBM-4 parameters, since
they are based on restrictions on the coverage vectors C only.
To ensure its correctness, the implementation was tested by carrying out forced
alignments on 500 German-to-English training sentence pairs. In a forced alignment,
the source sentence f J1 and the target sentence e
I
1 are kept fixed, and a full search with-
out re-ordering restrictions is carried out only over the unknown alignment aJ1. The
language model probability is divided out, and the resulting probability is compared to
the Viterbi probability as obtained by the training procedure. For 499 training sentences
the Viterbi alignment probability as obtained by the forced-alignment search was ex-
actly the same as the one produced by the training procedure. In one case the forced-
alignment search did obtain a better Viterbi probability than the training procedure.
4. Experimental Results
Translation experiments are carried out for the translation directions German to En-
glish and English to German (Verbmobil task) and for the translation directions French
to English and English to French (Canadian Hansards task). Section 4.1 reports on the
performance measures used. Section 4.2 shows translation results for the Verbmobil
task. Sections 4.2.1 and 4.2.2 describe that task and the preprocessing steps applied.
In Sections 4.2.3 through 4.2.5, the efficiency of the beam search pruning techniques is
shown for German-to-English translation, as the most detailed experiments are con-
ducted for that direction. Section 4.2.6 gives translation results for the translation direc-
tion English to German. In Section 4.3, translation results for the Canadian Hansards
task are reported.
119
Tillmann and Ney DP Beam Search for Statistical MT
4.1 Performance Measures for Translation Experiments
To measure the performance of the translation methods, we use three types of au-
tomatic and easy-to-use measures of the translation errors. Additionally, a subjective
evaluation involving human judges is carried out (Niessen et al 2000). The following
evaluation criteria are employed:
? WER (word error rate): The WER is computed as the minimum number of
substitution, insertion, and deletion operations that have to be
performed to convert the generated string into the reference target string.
This performance criterion is widely used in speech recognition. The
minimum is computed using a DP algorithm and is typically referred to
as edit or Levenshtein distance.
? mWER (multireference WER): We use the Levenshtein distance between
the automatic translation and several reference translations as a measure
of the translation errors. For example, on the Verbmobil TEST-331 test
set, an average of six reference translations per automatic translation are
available. The Levenshtein distance between the automatic translation
and each of the reference translations is computed, and the minimum
Levenshtein distance is taken. The resulting measure, the mWER, is
more robust than the WER, which takes into account only a single
reference translation.
? PER (position-independent word error rate): In the case in which only a
single reference translation per sentence is available, we introduce as an
additional measure the position-independent word error rate (PER). This
measure compares the words in the two sentences without taking the
word order into account. Words in the reference translation that have no
counterpart in the translated sentence are counted as substitution errors.
Depending on whether the translated sentence is longer or shorter than
the reference translation, the remaining words result in either insertion
(if the translated sentence is longer) or deletion (if the translated
sentence is shorter) errors. The PER is guaranteed to be less than or
equal to the WER. The PER is more robust than the WER since it ignores
translation errors due to different word order in the translated and
reference sentences.
? SSER (subjective sentence error rate): For a more fine-grained evaluation of
the translation results and to check the validity of the automatic
evaluation measures subjective judgments by test persons are carried out
(Niessen et al 2000). The following scale for the error count per sentence
is used in these subjective evaluations:
0.0 : semantically correct and syntactically correct
? ? ? : ? ? ?
0.5 : semantically correct and syntactically wrong
? ? ? : ? ? ?
1.0 : semantically wrong (independent of syntax)
Each translated sentence is judged by a human examiner according to
the above error scale; several human judges may be involved in judging
the same translated sentence. Subjective evaluation is carried out only
for the Verbmobil TEST-147 test set.
120
Computational Linguistics Volume 29, Number 1
Table 5
Training and test conditions for the German-to-English Verbmobil corpus (*number of words
without punctuation).
German English
Training: Sentences 58,073
Words 519,523 549,921
Words* 418,979 453,632
Vocabulary: Size 7,911 4,648
Singletons 3,453 1,699
TEST-331: Sentences 331
Words 5,591 6,279
Bigram/Trigram Perplexity 84.0/68.2 49.3/38.3
TEST-147: Sentences 147
Words 1,968 2,173
Bigram/Trigram Perplexity ? 34.6/28.1
4.2 Verbmobil Translation Experiments
4.2.1 The Task and the Corpus. The translation system is tested on the Verbmobil task
(Wahlster 2000). In that task, the goal is the translation of spontaneous speech in face-
to-face situations for an appointment scheduling domain. We carry out experiments for
both translation directions: German to English and English to German. Although the
Verbmobil task is still a limited-domain task, it is rather difficult in terms of vocabulary
size, namely, about 5,000 words or more for each of the two languages; second, the
syntactic structures of the sentences are rather unrestricted. Although the ultimate goal
of the Verbmobil project is the translation of spoken language, the input used for the
translation experiments reported on in this article is mainly the (more or less) correct
orthographic transcription of the spoken sentences. Thus, the effects of spontaneous
speech are present in the corpus; the effect of speech recognition errors, however, is
not covered. The corpus consists of 58,073 training pairs; its characteristics are given in
Table 5. For the translation experiments, a trigram language model with a perplexity of
28.1 is used. The following two test corpora are used for the translation experiments:
TEST-331: This test set consists of 331 test sentences. Only automatic evaluation is
carried out on this test corpus: The WER and the mWER are computed. For
each test sentence in the source language there is a range of acceptable
reference translations (six on average) provided by a human translator,
who is asked to produce word-to-word translations wherever it is possi-
ble. Part of the reference sentences are obtained by correcting automatic
translations of the test sentences that are produced using the approach pre-
sented in this article with different reordering constraints. The other part
is produced from the source sentences without looking at any of their
translations. The TEST-331 test set is used as held-out data for parameter
optimization (for the language mode scaling factor and for the distortion
model scaling factor). Furthermore, the beam search experiments in which
the effect of the different pruning thresholds is demonstrated are carried
out on the TEST-331 test set.
TEST-147: The second, separate test set consists of 147 test sentences. Translation
results are given in terms of mWER and SSER. No parameter optimization
121
Tillmann and Ney DP Beam Search for Statistical MT
is carried out on the TEST-147 test set; the parameter values as obtained
from the experiments on the TEST-331 test set are used.
4.2.2 Preprocessing Steps. To improve the translation performance the following
preprocessing steps are carried out:
Categorization: We use some categorization, which consists of replacing a single
word by a category. The only words that are replaced by a category label
are proper nouns denoting German cities. Using the new labeled corpus,
all probability models are trained anew. To produce translations in the
?normal? language, the categories are translated by rule and are inserted
into the target sentence.
Word joining: Target language words are joined using a method similar to the one
described in Och, Tillmann, and Ney (1999). Words are joined to handle
cases like the German compound noun ?Zahnarzttermin? for the English
?dentist?s appointment,? because a single word has to be mapped to two
or more target words. The word joining is applied only to the target lan-
guage words; the source language sentences remain unchanged. During
the search process several joined target language words may be generated
by a single source language word.
Manual lexicon: To account for unseen words in the test sentences and to obtain a
greater number of focused translation probabilities p(f | e), we use a bilin-
gual German-English dictionary. For each word e in the target vocabulary,
we create a list of source translations f according to this dictionary. The
translation probability pdic(f | e) for the dictionary entry (f , e) is defined as
pdic(f | e) =
?
?
?
1
Ne
if (f , e) is in dictionary
0 otherwise
where Ne is the number of source words listed as translations of the tar-
get word e. The dictionary probability pdic(f | e) is linearly combined
with the automatically trained translation probabilities paut(f | e) to ob-
tain smoothed probabilities p(f | e):
p(f | e) = (1 ? ?) ? pdic(f | e) + ? ? paut(f | e)
For the translation experiments, the value of the interpolation parameter
is fixed at ? = 0.5.
4.2.3 Effect of the Scaling Factors. In speech recognition, in which Bayes? decision rule
is applied, a language model scaling factor ?LM is used; a typical value is ?LM ? 15.
This scaling factor is employed because the language model probabilities are more
reliably estimated than the acoustic probabilities. Following this use of a language
model scaling factor in speech recognition, such a factor is introduced into statistical
MT, too. The optimization criterion in equation (1) is modified as follows:
e?I1 = arg max
eI1
{p(eI1)?LM ? p(f
J
1 | eI1)}
where p(eI1) is the language model probability of the target language sentence. In the
experiments presented here, a trigram language model is used to compute p(eI1). The
122
Computational Linguistics Volume 29, Number 1
Table 6
Computing time, mWER, and SSER for three different reordering constraints on the TEST-147
test set. During the translation experiments, reordered words are not allowed to cross
punctuation marks.
Reordering CPU time mWER SSER
constraint [sec] [%] [%]
MON 0.2 40.6 28.6
GE 5.2 33.3 21.0
S3 13.7 34.4 19.9
effect of the language model scaling factor ?LM is studied on the TEST-331 test set. A
minimum mWER is obtained for ?LM = 0.8, as reported in Tillmann (2001). Unlike in
speech recognition, the translation model probabilities seem to be estimated as reliably
as the language model probabilities in statistical MT.
A second scaling factor ?D is introduced for the distortion model probabilities
p(j | j?, J). A minimum mWER is obtained for ?D = 0.4, as reported in Tillmann
(2001). The WER and mWER on the TEST-331 test set increase significantly, if no
distortion probability is used, for the case ?D = 0.0. The benefit of a distortion prob-
ability scaling factor of ?D = 0.4 comes from the fact that otherwise, a low distor-
tion probability might suppress long-distant word reordering that is important for
German-to-English verb group reordering. The setting ?LM = 0.8 and ?D = 0.4 is used
for all subsequent translation results (including the translation direction English to
German).
4.2.4 Effect of the Word Reordering Constraints. Table 6 shows the computing time,
mWER, and SSER on the TEST-147 test set as a function of three reordering constraints:
MON, GE, and S3 (as discussed in the Appendix). The computing time is given in
terms of central processing unit (CPU) time per sentence (on a 450 MHz Pentium
III personal computer). For the SSER, it turns out that restricting the word reorder-
ing such that it may not cross punctuation marks improves translation performance
significantly. The average length of the sentence fragments that are separated by punc-
tuation marks is rather small: 4.5 words per fragment. A coverage pruning threshold
of tC = 5.0 and an observation pruning of no = 50 are applied during the experiments.4
No other type of pruning is used.5
The MON constraint performs worst in terms of both mWER and SSER. The
computing time is small, since no reordering is carried out. Constraints GE and S3
perform nearly identically in terms of both mWER and SSER. The GE constraint,
however, works about three times as fast as the S3 constraint.
Table 7 shows example translations obtained under the three different reordering
constraints. Again, the MON reordering constraint performs worst. In the second and
third translation examples, the S3 word reordering constraint performs worse than the
GE reordering constraint, since it cannot take the word reordering due to the German
verb group properly into account. The German finite verbs bin (second example) and
ko?nnten (third example) are too far away from the personal pronouns ich and Sie (six
4 For the translation experiments, the negative logarithm of the actual pruning thresholds tc and tC is
reported; for simplicity reasons we do not change the notation.
5 In a speech-to-speech demo system, we would use the GE reordering restriction and a slightly sharper
pruning in order to achieve translation times of about one second per sentence.
123
Tillmann and Ney DP Beam Search for Statistical MT
Table 7
Example translations for the translation direction German to English using three different
reordering constraints: MON, GE, and S3.
Input: Ja, wunderbar. Ko?nnen wir machen.
MON: Yes, wonderful. Can we do.
GE: Yes, wonderful. We can do that.
S3: Yes, wonderful. We can do that.
Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin.
Genaugenommen nur am dritten.
Wie wa?re es denn am a?hm Samstag, dem zehnten Februar?
MON: That is too tight , because I from the third in Kaiserslautern.
In fact only on the third. How about a?hm Saturday , the tenth
of February?
GE: That is too tight, because I am from the third in Kaiserslautern.
In fact only on the third. A?hm how about Saturday, February
the tenth?
S3: That is too tight, from the third because I will be in
Kaiserslautern. In fact only on the third. A?hm how
about Saturday, February the tenth?
Input: Wenn Sie dann noch den siebzehnten ko?nnten, wa?re das toll, ja.
MON: If you then also the seventeenth could, would be the great, yes.
GE: If you could then also the seventeenth, that would be great, yes.
S3: Then if you could even take seventeenth, that would be great, yes.
Input: Ja, das kommt mir sehr gelegen. Machen wir es dann
am besten so.
MON: Yes, that suits me perfectly. Do we should best like that.
GE: Yes, that suits me fine. We do it like that then best.
S3: Yes, that suits me fine. We should best do it like that.
and five source sentence positions, respectively) to be reordered properly. In the last
example, the less restrictive S3 reordering constraint leads to a better translation; the
GE translation is still acceptable, though.
4.2.5 Effect of the Beam Search Pruning Thresholds. In this section, the effect of the
beam search pruning is demonstrated. Translation results on the TEST-331 test set are
presented to evaluate the effectiveness of the pruning techniques.6 The quality of the
search algorithm with respect to the GE and S3 reordering constraints is evaluated
using two criteria:
1. The number of search errors for a certain combination of pruning
thresholds is counted. A search error occurs for a test sentence if the
final translation probability QF for a candidate translation eI1 as given in
equation (4) is smaller than a reference probability for that test sentence.
We will compute reference probabilities two ways, as explained below.
2. The mWER performance measure is computed as a function of the
pruning thresholds used. Generally, decreasing the pruning threshold
6 The CPU times on the TEST-331 set are higher, since the average fragment length is greater than for the
TEST-147 set.
124
Computational Linguistics Volume 29, Number 1
Table 8
Effect of the coverage pruning threshold tC on the number of search errors and mWER on the
TEST-331 test set (no cardinality pruning carried out: tc = ?). A cardinality histogram pruning
of 200,000 is applied to restrict the maximum overall size of the search space. The negative
logarithm of tC is reported.
Reordering tC CPU time Search errors mWER
constraint [sec] Qref > QF QF? > QF [%]
GE 0.01 0.21 318 323 73.5
0.1 0.43 231 301 53.1
1.0 1.43 10 226 30.3
2.5 4.75 5 142 25.8
5.0 29.6 ? 35 24.6
7.5 156 ? 2 24.9
10.0 630 ? ? 24.9
12.5 1300 ? ? 24.9
S3 0.01 5.48 314 324 70.0
0.1 9.21 225 303 50.9
1.0 46.2 4 223 31.6
2.5 190 ? 129 28.4
5.0 830 ? ? 28.3
leads to a higher word error rate, since the optimal path through the
translation lattice is missed, resulting in translation errors.
Two automatically generated reference probabilities are used. These probabilities are
computed separately for the reordering constraints GE and S3 (the difference is not
shown in the notation, but will be clear from the context):
Qref: A forced alignment is carried out between each of the test sentences and
its corresponding reference translation; only a single reference translation
for each test sentence is used. The probability obtained for the reference
translation is denoted by Qref.
QF? : A translation is carried out with conservatively large pruning thresholds,
yielding a translation close to the one with the maximum translation prob-
ability. The translation probability for that translation is denoted by QF? .
First, in a series of experiments we study the effect of the coverage and cardinality
pruning for the reordering constraints GE and S3. (When we report on the different
pruning thresholds, we will show the negative logarithm of those pruning thresholds.)
The experiments are carried out on two different pruning ?dimensions?:
1. In Table 8, only coverage pruning using threshold tC is carried out; no
cardinality pruning is applied: tc = ?.
2. In Table 9, only cardinality pruning using threshold tc is carried out; no
coverage pruning is applied: tC = ?.
Both tables use an observation pruning of no = 50. The effect of the coverage prun-
ing threshold tC is demonstrated in Table 8. For the translation experiments reported
in this table, the cardinality pruning threshold is set to tc = ?; thus, no compari-
son between partial hypotheses that do not cover the same set C of source sentence
125
Tillmann and Ney DP Beam Search for Statistical MT
Table 9
Effect of the cardinality pruning threshold tc on the number of search errors and mWER on
the TEST-331 test set (no coverage pruning is carried out: tC = ?). A coverage histogram
pruning of 1,000 is applied to restrict the overall size of the search space. The negative
logarithm of tc is shown.
Reordering tc CPU time Search errors mWER
constraint [sec] Qref > QF QF? > QF [%]
GE 1.0 0.03 45 287 48.5
2.0 0.06 20 277 41.9
3.0 0.13 16 266 37.7
4.0 0.30 6 239 34.1
5.0 0.55 2 212 30.5
7.5 3.2 ? 106 26.6
10.0 14.2 ? 32 25.1
12.5 42.2 ? 5 24.9
15.0 93.9 ? ? 24.9
17.5 176.7 ? ? 24.9
S3 1.0 0.02 10 331 51.4
2.0 0.05 1 283 46.2
3.0 0.10 1 274 43.3
4.0 0.22 ? 251 40.2
5.0 0.50 ? 227 37.5
7.5 4.3 ? 171 32.9
10.0 26.8 ? 99 30.8
12.5 123.3 ? 49 28.9
15.0 430 ? ? 28.2
positions is carried out. To restrict the overall size of the search space in terms of
CPU time and memory requirements, a cardinality pruning of nc = 200,000 is ap-
plied. As can be seen from Table 8, mWER and the number of search errors decrease
significantly as the coverage pruning threshold tC increases. For the GE reordering
constraint, mWER decreases from 73.5% to 24.9%. For a coverage pruning threshold
tC ? 5.0, mWER remains nearly constant at 25.0%, although search errors still occur.
For the S3 reordering constraint, mWER decreases from 70.0% to 28.3%. The largest
coverage threshold tested for the S3 constraint is tC = 5.0, since for larger threshold
values tC , the search procedure cannot be carried out because of memory and time
restrictions. The number of search errors is reduced as the coverage pruning thresh-
old is increased. It turns out to be difficult to verify search errors by looking at the
reference translation probabilities Qref alone. The translation with the maximum trans-
lation probability seems to be quite narrowly defined. The coverage pruning is more
effective for the GE constraint than for the S3 constraint, since the overall search space
for the GE reordering is smaller.
Table 9 shows the effect of the cardinality pruning threshold tc on mWER when
no coverage pruning is carried out (a histogram coverage pruning of 1,000 is applied
to restrict the overall size of the search space). The cardinality threshold tc has a
strong effect on mWER, which decreases significantly as the cardinality threshold tc
increases. For the GE reordering constraint, mWER decreases from 48.5% to 24.9%; for
the S3 reordering constraint, mWER decreases from 51.4% to 28.2%. For the coverage
threshold t = 15.0, the GE constraint works about four times as fast as the S3 constraint,
since the overall search space for the S3 constraint is much larger. Although the overall
search space is much larger for the S3 constraint, for smaller values of the coverage
126
Computational Linguistics Volume 29, Number 1
Table 10
Effect of observation pruning on the number of search errors and mWER on the TEST-331 test
set (parameter setting: tc = ?, tC = 10.0 ). No histogram pruning is applied. The results are
reported for the GE constraint.
Observation CPU time Search errors mWER
pruning no [sec] Qref > QF QF? > QF [%]
1 2.0 13 284 29.3
2 5.9 6 239 26.9
3 10.8 2 196 25.7
5 23.6 2 140 25.3
10 62.9 ? 99 24.8
25 238 ? 44 24.5
50 630 ? ? 24.9
threshold tC ? 5.0, the S3 constraint works as fast as the GE constraint or even faster,
because only a very small portion of the overall search space is searched for small
values of the cardinality pruning threshold tc. There is some computational overhead
in expanding a partial hypothesis for the GE constraint because the finite-state control
has to be handled. No results are obtained for the S3 constraint and the coverage
threshold tc = 17.5 because of memory restrictions. The number of search errors is
reduced as the cardinality pruning threshold is increased. Again, it is difficult to verify
search errors by looking at the reference translation probabilities alone.
Both coverage and cardinality pruning are more efficient for the GE reordering
constraint than for the S3 reordering constraint. For the S3 constraint, no translation
results are obtained for a coverage threshold tc > 5.0 without cardinality pruning
applied because of memory and computing time restrictions. For the GE constraint
virtually a full search can be carried out where only observation pruning is applied:
Identical target translations and translation probabilities are produced for the hypoth-
esis files for the two cases (1) tC = 10.0, tc = ?, and (2) tC = ?, tc = 15.0. (Actually,
for one test sentence in the TEST-331 test set, the translations are different, although
the translation probabilities are exactly the same.) Since the pruning is carried out
independently on two different pruning dimensions, no search errors will occur if the
thresholds are further increased.
Table 10 shows the effect of the observation pruning parameter no on mWER for
the reordering constraint GE. mWER is significantly reduced by hypothesizing up to
the best 50 target words e for a source language word f . mWER increases from 24.9%
to 29.3% when the number of hypothesized words is decreased to only a single word.
Table 11 demonstrates the effect of the combination of the coverage pruning thresh-
old tC = 5.0 and the cardinality pruning threshold tc = 12.5, where the actual values
are found in informal experiments: In a typical setting of the two parameters tc should
be at least twice as big as tC . For the GE reordering constraint, the average computing
time is about seven seconds per sentence without any loss in translation performance
as measured in terms of mWER. For the S3 reordering constraint, the average comput-
ing time per sentence is 27 seconds. Again, the combination of coverage and cardinality
pruning works more efficiently for the GE constraint. The memory requirement for
the algorithm is about 100 MB.
4.2.6 English-to-German Translation Experiments. A series of translation experiments
for the translation direction English to German are also carried out. The results, given
127
Tillmann and Ney DP Beam Search for Statistical MT
Table 11
Demonstration of the combination of the two pruning thresholds tC = 5.0 and tc = 12.5 to
speed up the search process for the two reordering constraints GE and S3 (no = 50). The
translation performance is shown in terms of mWER on the TEST-331 test set.
Reordering tC tc CPU time Search errors mWER
constraint [sec] Qref > QF QF? > QF [%]
GE 5.0 12.5 6.9 0 38 24.7
S3 5.0 12.5 26.9 0 65 29.2
Table 12
Translation results for the translation direction English to German on the TEST-331 test set.
The results are given in terms of computing time, WER, and PER for three different reordering
constraints: MON, EG, and S3.
Reordering CPU time WER PER
constraint [sec] [%] [%]
MON 0.5 70.6 57.0
EG 10.1 70.1 55.9
S3 53.2 70.1 55.8
in terms of WER and PER, are shown in Table 12. For the English-to-German translation
direction, a single reference translation for each test sentence is used to carry out
the automatic evaluation. The translation task for the translation direction English
to German is more difficult than for the translation direction German to English; the
trigram language model perplexity increases from 38.3 to 68.2 on the TEST-331 test set,
as can be seen in Table 5. No parameter optimization is carried out for this translation
direction; the parameter settings are carried over from the results obtained in Table 11.
The word error rates for the translation direction English to German are signif-
icantly higher than those for the translation direction German to English. There are
several reasons for this: German vocabulary and perplexity are significantly larger
than those for English, and only a single reference translation per test sentence is
available for English-to-German translation. There is only a very small difference in
terms of word error rates for the reordering constraints EG and S3; in particular, WER
is 70.1% for both. The reordering constraint MON performs slightly worse: WER in-
creases to 70.6%, and PER increases to 57.0%. Table 13 shows translation examples
for the translation direction English to German. The MON constraint performs worst;
there is no significant difference in quality of translations produced under the EG and
the S3 constraints.
4.3 Canadian Hansards Translation Experiments
4.3.1 The Task and the Corpus. The second corpus on which we perform translation
experiments is the Hansard corpus. By law, the proceedings of the Canadian parlia-
ment are recorded in both French and English. (For historical reasons, these proceed-
ings are called ?Hansards.?) The remarks of the parliament members are written down
in whichever of the two languages they use. They are then translated into the other
language to produce complete sets of the proceedings, one in French and the other
in English. The resulting bilingual data have been sentence-aligned using statistical
methods (Brown et al 1990). Originally, about three million sentences were selected.
Here, we use a subset of the original training data; the details regarding this subset
128
Computational Linguistics Volume 29, Number 1
Table 13
Example translations for the translation direction English to German using three different
reordering constraints: MON, EG, and S3.
Input: Yeah , that wouldn?t be bad. Do you have any ideas where
I could stay?
MON: Ja, das wa?re schade. Haben Sie irgendwelche Ideen wo ich
ko?nnten u?bernachten?
EG: Ja, das wa?re nicht schlecht. Haben Sie irgendwelche Ideen wo
wir wohnen ko?nnten?
S3: Ja, das wa?re nicht schlecht. Haben Sie irgendwelche Ideen
wo wir wohnen ko?nnten?
Input: Oh, that sounds great . Could you arrange a suite for me?
MON: Oh, das klingt gut. Ko?nnten Sie unbedingt ein Suite bei mir?
EG: Oh, das klingt gut. Ko?nnten Sie einen Suite ausmachen fu?r mich?
S3: Oh, das klingt gut. Ko?nnten Sie mir einen Suite ausmachen?
Input: Well, I still need your signature here and then I will check
with your company.
MON: Also, ich konnte Arbeitskraft Unterschrift hier und ich werde
nachsehen mit Ihrer Firma.
EG: Also, ich bra?uchte noch Ihre Unterschrift und dann gucke ich hier
mit Ihrer Firma.
S3: Also, ich brauche hier noch Ihre Unterschrift und dann werde ich
veranlassen mit Ihrer Firma.
Table 14
Training and test conditions for the Hansards task (*number of words without punctuation).
French English
Train: Sentences 1,470,473
Words 24,338,195 22,163,092
Words* 22,175,069 20,063,378
Vocabulary: Size 100,269 78,332
Singletons 40,199 31,319
Test: Sentences 5,432
Words 97,646 80,559
Bigr./Tri. Perplexity 196.9/121.8 269.9/179.8
are given in Table 14. The Hansards corpus presents by far a more difficult task than
the Verbmobil corpus in terms of vocabulary size and number of training sentences.
The training and test sentences are less restrictive than for the Verbmobil task. For the
translation experiments on the Hansards corpus, no word joining is carried out. Two
target words can be produced by a single source word, as described in Section 3.9.2.
4.3.2 Translation Results. As can be seen in Table 15 for the translation direction
French to English and in Table 16 for the translation direction English to French, the
word error rates are rather high compared to those for the Verbmobil task. The reason
for the higher error rates is that, as noted in the previous section, the Hansards task
is by far less restrictive than the Verbmobil task, and the vocabulary size is much
129
Tillmann and Ney DP Beam Search for Statistical MT
Table 15
Computing time, WER, and PER for the translation direction French to English using the two
reordering constraints MON and S3. An almost ?full? search is carried out.
Reordering CPU time WER PER
constraint [sec] [%] [%]
MON 2.5 65.5 53.0
S3 580.0 64.9 51.4
Table 16
Computing time, WER, and PER for the translation direction English to French using the two
reordering constraints MON and S3. An almost ?full? search is carried out.
Reordering CPU time WER PER
constraint [sec] [%] [%]
MON 2.2 66.6 56.3
S3 189.1 66.0 54.4
larger. There is only a slight difference in performance between the MON and the
S3 reordering constraints on the Hansards task. The computation time is also rather
high compared to the Verbmobil task: For the S3 constraint, the average translation
time is about 3 minutes per sentence for the translation direction English to French
and about 10 minutes per sentence for the translation direction French to English.
The following parameter setting is used for the experiment conducted here: tC = 5.0,
tc = 10.0, nC = 250, and to = 12. (The actual parameters are chosen in informal
experiments to obtain reasonable CPU times while permitting only a small number of
search errors.) No cardinality histogram pruning is carried out. As for the German-
to-English translation experiments, word reordering is restricted so that it may not
cross punctuation boundaries. The resulting fragment lengths are much larger for
the translation direction English to French, and still larger for the translation direction
French to English, when compared to the fragment lengths for the translation direction
German to English, hence the high CPU times. In an additional experiment for the
translation direction French to English and the reordering constraint S3, we find we can
speed up the translation time to about 18 seconds per sentence by using the following
parameter setting: tC = 3.0, tc = 7.5, nC = 20, nc = 400, and no = 5. For the resulting
hypotheses file, PER increases only slightly, from 51.4% to 51.6%.
Translation examples for the translation direction French to English under the S3
reordering constraint are given in Table 17. The French input sentences show some
preprocessing that is carried out beforehand to simplify the translation task (e.g., des
is transformed into de les and l?est is transformed into le est). The translations pro-
duced are rather approximative in some cases, although the general meaning is often
preserved.
5. Conclusions
We have presented a DP-based beam search algorithm for the IBM-4 translation model.
The approach is based on a DP solution to the TSP, and it gains efficiency by imposing
constraints on the allowed word reorderings between source and target language. A
data-driven search organization in conjunction with appropriate pruning techniques
130
Computational Linguistics Volume 29, Number 1
Table 17
Example translations for the translation direction French to English using the S3 reordering
constraint.
Input Je crois que cela donne une bonne ide?e de les principes a`
retenir et de ce que devraient e?tre nos responsabilite?s.
S3 I think it is a good idea of the principles and to what
should be our responsibility.
Input Je pense que, inde?pendamment de notre parti, nous trouvons
tous cela inacceptable.
S3 I think, regardless of our party, we find that unacceptable.
Input Je ai le intention de parler surtout aujourd? hui de les nombreuses
ame?liorations apporte?es a` les programmes de pensions de tous les
Canadiens.
S3 I have the intention of speaking today about the many improvements
in pensions for all Canadians especially those programs.
Input Chacun en lui - me?me est tre`s complexe et le lien entre les deux le
est encore davantage de sorte que pour beaucoup la situation
pre?sente est confuse.
S3 Each in itself is very complex and the relationship between the two is more
so much for the present situation is confused.
is proposed. For the medium-sized Verbmobil task, a sentence can be translated in a
few seconds on average, with a small number of search errors and no performance
degradation as measured by the word error criterion used.
Word reordering is parameterized using a set of four parameters, in such a way
that it can easily be adopted to new translation directions. A finite-state control is
added, and its usefulness is demonstrated for the translation direction German to
English, in which the word order difference between the two languages is mainly due
to the German verb group. Future work might aim at a tighter integration of the IBM-4
model distortion probabilities and the finite-state control; the finite-state control itself
may be learned from training data.
The applicability of the algorithm applied in the experiments in this article is
not restricted to the IBM translation models or to the simplified translation model
used in the description of the algorithm in Section 3. Since the efficiency of the beam
search approach is based on restrictions on the allowed coverage vectors C alone,
the approach may be used for different types of translation models as well (e.g., for
the multiword-based translation model proposed in Och, Tillmann, and Ney [1999]).
On the other hand, since the decoding problem for the IBM-4 translation model is
provably NP-complete, as shown in Knight (1999) and Germann et al (2001), word
reordering restrictions as introduced in this article are essential for obtaining an effi-
cient search algorithm that guarantees that a solution close to the optimal one will be
found.
Appendix: Quantification of Reordering Restrictions
To quantify the reordering restrictions in Section 3.5, the four non-negative num-
bers numskip, widthskip, nummove, and widthmove are used (widthskip corresponds
to L, widthmove corresponds to R in Section 3.4; here, we use a more intuitive nota-
tion). Within the implementation of the DP search, the restrictions are provided to the
131
Tillmann and Ney DP Beam Search for Statistical MT
algorithm as an input parameter of the following type:
S numskip widthskip M nummove widthmove
The meaning of the reordering string is as follows: The two numbers following S that
are separated by an underscore describe the way words may be skipped; the two
numbers following M that are separated by an underscore describe the way words
may be moved during word reordering. The first number after S and M denotes
the number of positions that may be skipped or moved, respectively (e.g., for the
translation direction German to English [GE in the chart below], one position may
be skipped and two positions may be moved). The second number after S and M
restricts the distance a word may be skipped or moved, respectively. These ?width?
parameters restrict the word reordering to take place within a ?window? of a certain
size, established by the distance between the positions lmin(C) and rmax(C) as defined
in Section 3.5. In the notation, either the substring headed by S or that headed by M
(or both) may be omitted altogether to indicate that the corresponding reordering is
not allowed. Any numerical value in the string may be set to INF, denoting that an
arbitrary number of positions may be skipped/moved or that the moving or skipping
distance may be arbitrarily large. The following reordering strings are used in this
article:
Word reordering Description
string
 The empty string denotes the reordering restriction in which
(short: MON) no reordering is allowed.
S 01 04 M 02 10 This string describes the German-to-English word reordering.
(short: GE) Up to one word may be skipped for at most 4 positions,
and up to two words may be moved up to 10 positions.
S 02 10 M 01 04 This string describes the English-to-German word reordering.
(short: EG) Up to two words may be skipped for at most 10 positions
and up to one word may be moved for up to 4 positions.
S 03 INF This string describes the IBM-style word reordering
(short: S3) given in Section 3.6. Up to three words may be skipped for
an unrestricted number of positions. No words may be moved.
S INF INF or These strings denote the word re-ordering without
M INF INF restrictions.
(short: NO)
The word reordering strings can be directly used as input parameters to the DP-based
search procedure to test different reordering restrictions within a single implementa-
tion.
Acknowledgments
This work has been supported as part of the
Verbmobil project (contract number
01 IV 601 A) by the German Federal
Ministry of Education, Science, Research
and Technology and as part of the Eutrans
132
Computational Linguistics Volume 29, Number 1
project (ESPRIT project number 30268) by
the European Community. Some of the
experiments on the Canadian Hansards task
have been carried out by Nicola Ueffing
using the existing implementation of the
search algorithm (Och, Ueffing, and Ney
[2001]). We would like to thank the
anonymous reviewers for their detailed
comments on an earlier version of this
article. Also, we would like to thank Niyu
Ge, Scott McCarley, Salim Roukos, Nicola
Ueffing, and Todd Ward for their valuable
remarks.
References
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy,
Noah Smith, and David Yarowsky. 1999.
Statistical machine translation. Final
Report, Johns Hopkins University
Summer Workshop (WS 99) on Language
Engineering, Center for Language and
Speech Processing, Baltimore.
Berger, Adam L., Peter F. Brown, Stephen
A. Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Robert L.
Mercer, Harry Printz, and Lubos Ures.
1994. The Candide system for machine
translation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 152?157, San Mateo, California,
March.
Berger, Adam L., Peter F. Brown, Stephen
A. Della Pietra, Vincent J. Della Pietra,
Andrew S. Kehler, and Robert L. Mercer.
1996. Language translation apparatus and
method of using context-based translation
models. U.S. Patent 5510981.
Brown, Peter F., John Cocke, Vincent J. Della
Pietra, Stephen A. Della Pietra, Fred
Jelinek, John Lafferty, Robert L. Mercer,
and Paul S. Roosin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter F., Vincent J. Della Pietra,
Stephen A. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Brown, Peter F., Peter V. deSouza, Vincent
J. Della Pietra, and Robert L. Mercer.
1992. Class-based n-gram models of
natural language. Computational
Linguistics, 18(4):467?479.
Garc??a-Varea, Ismael, Francisco Casacuberta,
and Hermann Ney. 1998. An iterative
DP-based search algorithm for statistical
machine translation. In Proceedings of the
Fifth International Conference on Spoken
Language Processing (ICSLP 98),
pages 1135?1139, Sydney, Australia,
November.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 38th Annual Conference of the Association
for Computational Linguistics (ACL 2001),
pages 228?235, Toulouse, France, July.
Held, Michael and Richard M. Karp. 1962.
A dynamic programming approach to
sequencing problems. SIAM,
10(1):196?210.
Jelinek, Fred. 1976. Speech recognition by
statistical methods. Proceedings of the IEEE,
64:532?556.
Knight, Kevin. 1999. Decoding complexity
in word-replacement translation models.
Computational Linguistics, 25(4):607?615.
Ney, Hermann, Dieter Mergel, Andreas
Noll, and Annedore Paeseler. 1992. Data
driven search organization for continuous
speech recognition in the SPICOS system.
IEEE Transactions on Signal Processing,
40(2):272?281.
Ney, Hermann, Sonja Niessen, Franz-Josef
Och, Hassan Sawaf, Christoph Tillmann,
and Stefan Vogel. 2000. Algorithms for
statistical translation of spoken language.
IEEE Transactions on Speech and Audio
Processing, 8(1):24?36.
Niessen, Sonja, Franz-Josef Och, Gregor
Leusch, and Hermann Ney. 2000. An
evaluation tool for machine translation:
Fast evaluation for MT research. In
Proceedings of the Second International
Conference on Language Resources and
Evaluation, pages 39?45, Athens, Greece,
May.
Niessen, Sonja, Stefan Vogel, Hermann Ney,
and Christoph Tillmann. 1998. A
DP-based search algorithm for statistical
machine translation. In Proceedings of the
36th Annual Conference of the Association for
Computational Linguistics and the 17th
International Conference on Computational
Linguistics (ACL/COLING 98),
pages 960?967, Montreal, Canada, August.
Nilsson, Nils J. 1971. Problem Solving Methods
in Artificial Intelligence. McGraw Hill, New
York.
Och, Franz-Josef and Hermann Ney. 2000. A
comparison of alignment models for
statistical machine translation. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 1086?1090,
Saarbru?cken, Germany, July?August.
133
Tillmann and Ney DP Beam Search for Statistical MT
Och, Franz-Josef, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Very Large Corpora
(EMNLP/VLC 99), pages 20?28, College
Park, Maryland, June.
Och, Franz-Josef, Nicola Ueffing, and
Hermann Ney. 2001. An efficient (A)*
search algorithm for statistical machine
translation. In Proceedings of the
Data-Driven Machine Translation Workshop,
39th Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 55?62, Toulouse, France, July.
Tillmann, Christoph. 2001. Word Re-ordering
and Dynamic Programming Based Search
Algorithm for Statistical Machine Translation.
Ph.D. thesis, University of Technology,
Aachen, Germany.
Tillmann, Christoph and Hermann Ney.
2000. Word re-ordering and DP-based
search in statistical machine translation.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 850?856,
Saarbru?cken, Germany, July?August.
Tillmann, Christoph, Stefan Vogel, Hermann
Ney, and Alex Zubiaga. 1997. A DP-based
search using monotone alignments in
statistical translation. In Proceedings of the
35th Annual Conference of the Association for
Computational Linguistics (ACL 97),
pages 289?296, Madrid, July.
Tillmann, Christoph, Stefan Vogel, Hermann
Ney, Alex Zubiaga, and Hassan Sawaf.
1997. Accelerated DP-based search for
statistical translation. In Proceedings of the
Fifth European Conference on Speech
Communication and Technology (Eurospeech
97), pages 2667?2670, Rhodos, Greece,
September.
Wahlster, Wolfgang. 2000. Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer Verlag, Berlin.
Wang, Ye-Yi and Alex Waibel. 1997.
Decoding algorithm in statistical
translation. In Proceedings of the 35th
Annual Conference of the Association for
Computational Linguistics (ACL 97),
pages 366?372, Madrid, July.
Wang, Ye-Yi and Alex Waibel. 1998. Fast
decoding for statistical machine
translation. In Proceedings of the Fifth
International Conference on Spoken Language
Processing (ICSLP 98), pages 2775?2778,
Sydney, Australia, December.
Wu, Dekai. 1996. A polynomial-time
algorithm for statistical machine
translation. In Proceedings of the 34th
Annual Conference of the Association for
Computational Linguistics (ACL 96),
pages 152?158, Santa Cruz, California,
June.
c? 2004 Association for Computational Linguistics
Statistical Machine Translation with
Scarce Resources Using Morpho-syntactic
Information
Sonja Nie?en? Hermann Ney?
RWTH Aachen RWTH Aachen
In statistical machine translation, correspondences between the words in the source and the
target language are learned from parallel corpora, and often little or no linguistic knowledge is
used to structure the underlying models. In particular, existing statistical systems for machine
translation often treat different inflected forms of the same lemma as if they were independent of one
another. The bilingual training data can be better exploited by explicitly taking into account the
interdependencies of related inflected forms. We propose the construction of hierarchical lexicon
models on the basis of equivalence classes of words. In addition, we introduce sentence-level
restructuring transformations which aim at the assimilation of word order in related sentences.
We have systematically investigated the amount of bilingual training data required to maintain
an acceptable quality of machine translation. The combination of the suggested methods for
improving translation quality in frameworks with scarce resources has been successfully tested:
We were able to reduce the amount of bilingual training data to less than 10% of the original
corpus, while losing only 1.6% in translation quality. The improvement of the translation results
is demonstrated on two German-English corpora taken from the Verbmobil task and the Nespole!
task.
1. Introduction
The statistical approach to machine translation has proved successful in various com-
parative evaluations since its revival by the work of the IBM research group more
than a decade ago. The IBM group dispensed with linguistic analysis, at least in its
earliest publications. Although the IBM group finally made use of morphological and
syntactic information to enhance translation quality (Brown et al 1992; Berger et al
1996), most of today?s statistical machine translation systems still consider only surface
forms and use no linguistic knowledge about the structure of the languages involved.
In many applications only small amounts of bilingual training data are available
for the desired domain and language pair, and it is highly desirable to avoid at least
parts of the costly data collection process. The main objective of the work reported in
this article is to introduce morphological knowledge in order to reduce the amount
of bilingual data necessary to sufficiently cover the vocabulary expected in testing.
This is achieved by explicitly taking into account the interdependencies of related
inflected forms. In this work, a hierarchy of equivalence classes at different levels of
abstraction is proposed. Features from those hierarchy levels are combined to form
hierarchical lexicon models, which can replace the standard probabilistic lexicon used
? Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,
D-52056 Aachen, Germany. E-mail: sonja.niessen@gmx.de; ney@cs.rwth-aachen.de.
182
Computational Linguistics Volume 30, Number 2
in most statistical machine translation systems. Apart from the improved coverage,
the proposed lexicon models enable the disambiguation of ambiguous word forms by
means of annotation with morpho-syntactic tags.
1.1 Overview
The article is organized as follows. After briefly reviewing the basic concepts of the
statistical approach to machine translation, we discuss the state of the art and related
work as regards the incorporation of morphological and syntactic information into
systems for natural language processing. Section 2 describes the information provided
by morpho-syntactic analysis and introduces a suitable representation of the analyzed
corpus. Section 3 suggests solutions for two specific aspects of structural difference,
namely, question inversion and separated verb prefixes. Section 4 is dedicated to hi-
erarchical lexicon models. These models are able to infer translations of word forms
from the translations of other word forms of the same lemma. Furthermore, they use
morpho-syntactic information to resolve categorial ambiguity. In Section 5, we describe
how disambiguation between different readings and their corresponding translations
can be performed when no context is available, as is typically the case for conven-
tional electronic dictionaries. Section 6 provides an overview of our procedure for
training model parameters for statistical machine translation with scarce resources.
Experimental results are reported in Section 7. Section 8 concludes the presentation
with a discussion of the achievements of this work.
1.2 Statistical Machine Translation
In statistical machine translation, every target language string eI1 = e1 ? ? ? eI is assigned
a probability Pr(eI1) of being a valid word sequence in the target language and a
probability Pr(eI1|f
J
1) of being a translation for the given source language string f
J
1 =
f1 ? ? ? fJ. According to Bayes? decision rule, the optimal translation for f J1 is the target
string that maximizes the product of the target language model Pr(eI1) and the string
translation model Pr(f J1 |eI1). Many existing systems for statistical machine translation
(Garc??a-Varea and Casacuberta 2001; Germann et al 2001; Nie?en et al 1998; Och,
Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della
Pietra, and Mercer (1993): The correspondence between the words in the source and
the target strings is described by alignments that assign target word positions to each
source word position. The probability that a certain target language word will occur
in the target string is assumed to depend basically only on the source words aligned
with it.
1.3 Related Work
1.3.1 Morphology. Some publications have already dealt with the treatment of mor-
phology in the framework of language modeling and speech recognition: Kanevsky,
Roukos, and Sedivy (1997) propose a statistical language model for inflected languages.
They decompose word forms into stems and affixes. Maltese and Mancini (1992) re-
port that a linear interpolation of word n-grams, part of speech n-grams, and lemma
n-grams yields lower perplexity than pure word-based models. Larson et al (2000)
apply a data-driven algorithm for decomposing compound words in compounding
languages as well as for recombining phrases to enhance the pronunciation lexicon
and the language model for large-vocabulary speech recognition systems.
As regards machine translation, the treatment of morphology is part of the analysis
and generation step in virtually every symbolic machine translation system. For this
purpose, the lexicon should contain base forms of words and the grammatical category,
183
Nie?en and Ney SMT with Scarce Resources
subcategorization features, and semantic information in order to enable the size of the
lexicon to be reduced and in order to account for unknown word forms, that is, word
forms not present explicitly in the dictionary.
Today?s statistical machine translation systems build upon the work of P. F. Brown
and his colleagues at IBM. The translation models they presented in various papers
between 1988 and 1993 (Brown et al 1988; Brown et al 1990; Brown, Della Pietra, Della
Pietra, and Mercer 1993) are commonly referred to as IBM models 1?5, based on the
numbering in Brown, Della Pietra, Della Pietra, and Mercer (1993). The underlying
(probabilistic) lexicon contains only pairs of full forms. On the other hand, Brown
et al (1992) had already suggested word forms be annotated with morpho-syntactic
information, but they did not perform any investigation on the effects.
1.3.2 Translation with Scarce Resources. Some recent publications, like Al-Onaizan
et al (2000), have dealt with the problem of translation with scarce resources. Al-
Onaizan et al report on an experiment involving Tetun-to-English translation by dif-
ferent groups, including one using statistical machine translation. Al-Onaizan et al
assume the absence of linguistic knowledge sources such as morphological analyzers
and dictionaries. Nevertheless, they found that the human mind is very well capable
of deriving dependencies such as morphology, cognates, proper names, and spelling
variations and that this capability was finally at the basis of the better results produced
by humans compared to corpus-based machine translation. The additional information
results from complex reasoning, and it is not directly accessible from the full-word-
form representation in the data.
This article takes a different point of view: Even if full bilingual training data
are scarce, monolingual knowledge sources like morphological analyzers and data for
training the target language model as well as conventional dictionaries (one word
and its translation[s] per entry) may be available and of substantial usefulness for
improving the performance of statistical translation systems. This is especially the case
for more-inflecting major languages like German. The use of dictionaries to augment
or replace parallel corpora has already been examined by Brown, Della Pietra, Della
Pietra, and Goldsmith (1993) and Koehn and Knight (2001), for instance.
2. Morpho-syntactic Information
A prerequisite for the methods for improving the quality of statistical machine trans-
lation described in this article is the availability of various kinds of morphological
and syntactic information. This section describes the output resulting from morpho-
syntactic analysis and explains which parts of the analysis are used and how the
output is represented for further processing.
2.1 Description of the Analysis Results
For obtaining the required morpho-syntactic information, the following analyzers for
German and English were applied: gertwol and engtwol for lexical analysis and gercg
and engcg for morphological and syntactic disambiguation. For a description of the
underlying approach, the reader is referred to Karlsson (1990). Tables 1 and 2 give
examples of the information provided by these tools.
2.2 Treatment of Ambiguity
The examples in Tables 1 and 2 demonstrate the capability of the tools to disambiguate
among different readings: For instance, they infer that the word wollen is a verb in the
indicative present first-person plural form. Without any context taken into account,
184
Computational Linguistics Volume 30, Number 2
Table 1
Sample analysis of a German sentence. Input: Wir wollen nach dem
Abendessen nach Essen aufbrechen. (In English: We want to start for Essen
after dinner.)
Original Base form Tags
Wir wir personal-pronoun plural first nominative
wollen wollen verb indicative present plural first
nach nach preposition dative
dem das definite-article singular dative neuter
Abendessen Abend#essen noun neuter singular dative
nach nach preposition dative
Essen Essen noun name neuter singular dative
Esse noun feminine plural dative
Essen noun neuter plural dative
Essen noun neuter singular dative
aufbrechen auf|brechen verb separable infinitive
Table 2
Sample analysis of an English sentence. Input: Do we have to reserve
rooms?.
Original Base form Tags
Do do verb present not-singular-third finite auxiliary
we we personal-pronoun nominative plural first subject
have have verb infinitive not-finite main
to to infinitive-marker
reserve reserve verb infinitive not-finite main
rooms room noun nominative plural object
wollen has other readings. It can even be interpreted as derived from an adjective
with the meaning ?made of wool.? The inflected word forms on the German part of
the Verbmobil (cf. Section 7.1.1) corpus have on average 2.85 readings (1.86 for the
English corpus), 58% of which can be eliminated by the syntactic analyzers on the
basis of sentence context.
Common bilingual corpora normally contain full sentences, which provide enough
context information for ruling out all but one reading for an inflected word form. To
reduce the remaining uncertainty, preference rules have been implemented. For in-
stance, it is assumed that the corpus is correctly true-case-converted beforehand, and
as a consequence, non-noun readings of uppercase words are dropped. Furthermore,
indicative verb readings are preferred to subjunctive or imperative. In addition, some
simple domain-specific heuristics are applied. The reading ?plural of Esse? for the
German word form Essen, for instance, is much less likely in the domain of appoint-
ment scheduling and travel arrangements than the readings ?proper name of the town
Essen? or the German equivalent of the English word meal. As can be seen in Table 3,
the reduction in the number of readings resulting from these preference rules is fairly
small in the case of the Verbmobil corpus.
The remaining ambiguity often lies in those parts of the information which are
not used or which are not relevant to the translation task. For example, the analyzers
cannot tell accusative from dative case in German, but the case information is not
essential for the translation task (see also Table 4). Section 2.4 describes a method
185
Nie?en and Ney SMT with Scarce Resources
Table 3
Resolution of ambiguity on the Verbmobil corpus.
Number of readings per word form
Disambiguation German English
None 2.85 1.86
By context 1.20 1.02
By preference 1.19 1.02
By selecting relevant tags 1.06 1.01
By resorting to unambiguous part 1.00 1.00
for selecting morpho-syntactic tags considered relevant for the translation task, which
results in a further reduction in the number of readings per word form to 1.06 for
German and 1.01 for English. In these rare cases of ambiguity it is admissible to resort
to the unambiguous parts of the readings, that is, to drop all tags causing mixed
interpretations. Table 3 summarizes the gradual resolution of ambiguity.
The analysis of conventional dictionaries poses some special problems, because
they do not provide enough context to enable effective disambiguation. For handling
this special situation, dedicated methods have been implemented; these are presented
in Section 5.1.
2.3 The Lemma-Tag Representation
A full word form is represented by the information provided by the morpho-syntactic
analysis: from the interpretation gehen verb indicative present first singular, that
is, the base form plus part of speech plus the other tags, the word form gehe can be
restored. It has already been mentioned that the analyzers can disambiguate among
different readings on the basis of context information. In this sense, the information
inherent in the original word forms is augmented by the disambiguating analyzer.
This can be useful for choosing the correct translation of ambiguous words. Of course,
these disambiguation clues result in an enlarged vocabulary. The vocabulary of the new
representation of the German part of the Verbmobil corpus, for example, in which full
word forms are replaced by base form plus morphological and syntactic tags (lemma-
tag representation), is one and a half times as large as the vocabulary of the original
corpus. On the other hand, the information in the lemma-tag representation can be
accessed gradually and ultimately reduced: For example, certain instances of words
can be considered equivalent. This fact is used to better exploit the bilingual training
data along two directions: detecting and omitting unimportant information (see Section
2.4) and constructing hierarchical translation models (see Section 4). To summarize,
the lemma-tag representation of a corpus has the following main advantages: It makes
context information locally available, and it allows information to be explicitly accessed
at different levels of abstraction.
2.4 Equivalence Classes of Words with Similar Translation
Inflected word forms in the input language often contain information that is not rel-
evant for translation. This is especially true for the task of translating from a more
inflecting language like German into English, for instance: In parallel German/English
corpora, the German part contains many more distinct word forms than the English
part (see, for example, Table 5). It is useful for the process of statistical machine trans-
lation to define equivalence classes of word forms which tend to be translated by
the same target language word: The resulting statistical translation lexicon becomes
186
Computational Linguistics Volume 30, Number 2
Table 4
Candidates for equivalence classes.
Part of speech Candidates
Noun Gender (masculine, feminine, neuter)
and case (nominative, dative, accusative)
Verb Number (singular, plural) and person (first, second, third)
Adjective Gender, case, and number
Number Case
smoother, and the coverage is considerably improved. Such equivalence classes are
constructed by omitting those items of information from morpho-syntactic analysis
which are not relevant for translation.
The lemma-tag representation of the corpus helps to identify the unimportant
information. The definition of relevant and unimportant information, respectively, de-
pends on many factors like the languages involved, the translation direction, and the
choice of the models. We detect candidates for equivalence classes of words automat-
ically from the probabilistic lexicon trained for translation from German to English.
For this purpose, those inflected forms of the same base form which result in the same
translation are inspected. For each set of tags T, the algorithm counts how often an
additional tag t1 can be replaced with a certain other tag t2 without effect on the trans-
lation. As an example, let T = ?blau-adjective?, t1 =?masculine? and t2 =?feminine?.
The two entries (?blau-adjective-masculine?|?blue?) and (?blau-adjective-feminine?|?blue?)
are hints for detecting gender as nonrelevant when translating adjectives into English.
Table 4 lists some of the most frequently identified candidates to be ignored while
translating: The gender of nouns is irrelevant for their translation (which is straight-
forward, as the gender of a noun is unambiguous), as are the cases nominative, dative,
accusative. (For the genitive forms, the translation in English differs.) For verbs the
candidates number and person were found: The translation of the first-person singular
form of a verb, for example, is often the same as the translation of the third-person
plural form. Ignoring (dropping) those tags most often identified as irrelevant for
translation results in the building of equivalence classes of words. Doing so results in
a smaller vocabulary, one about 65.5% the size of the vocabulary of the full lemma-
tag representation of the Verbmobil corpus, for example?it is even smaller than the
vocabulary of the original full-form corpus.
The information described in this section is used to improve the quality of statis-
tical machine translation and to better exploit the available bilingual resources.
3. Treatment of Structural Differences
Difference in sentence structure is one of the main sources of errors in machine trans-
lation. It is thus promising to ?harmonize? the word order in corresponding sentences.
The presentation in this section focuses on the following aspects: question inversion
and separated verb prefixes. For a more detailed discussion of restructuring for statis-
tical machine translation the reader is referred to Nie?en and Ney (2000, 2001).
3.1 Question Inversion
In many languages, the sentence structure of questions differs from the structure in
declarative sentences in that the order of the subject and the corresponding finite verb
is inverted. From the perspective of statistical translation, this behavior has some dis-
187
Nie?en and Ney SMT with Scarce Resources
advantages: The algorithm for training the parameters of the target language model
Pr(eI1), which is typically a standard n-gram model, cannot deduce the probability
of a word sequence in an interrogative sentence from the corresponding declarative
form. The same reasoning is valid for the lexical translation probabilities of multiword-
phrase pairs. To harmonize the word order of questions with the word order in declar-
ative sentences, the order of the subject (including the appendant articles, adjectives
etc.) and the corresponding finite verb is inverted. In English questions supporting
dos are removed. The application of the described preprocessing step in the bilingual
training corpus implies the necessity of restoring the correct forms of the translations
produced by the machine translation algorithm. This procedure was suggested by
Brown et al (1992) for the language pair English and French, but they did not re-
port on experimental results revealing the effect of the restructuring on the translation
quality.
3.2 Separated Verb Prefixes
German prefix verbs consist of a main part and a detachable prefix, which can be
shifted to the end of the clause. For the automatic alignment process, it is often dif-
ficult to associate one English word with more than one word in the corresponding
German sentence, namely, the main part of the verb and the separated prefix. To solve
the problem of separated prefixes, all separable word forms of verbs are extracted
from the training corpus. The resulting list contains entries of the form prefix|main.
In all clauses containing a word matching a main part and a word matching the cor-
responding prefix part occurring at the end of the clause, the prefix is prepended to
the beginning of the main part.
4. Hierarchical Lexicon Models
In general, the probabilistic lexicon resulting from training the translation model con-
tains all word forms occurring in the training corpus as separate entries, not taking
into account whether or not they are inflected forms of the same lemma. Bearing in
mind that typically more than 40% of the word forms are seen only once in training
(see, for example, Table 5), it is obvious that for many words, learning the correct
translations is difficult. Furthermore, new input sentences are expected to contain un-
known word forms, for which no translation can be retrieved from the lexicon. This
problem is especially relevant for more-inflecting languages like German: Texts in Ger-
man contain many more distinct word forms than their English translations. Table 5
also reveals that these words are often generated via inflection from a smaller set of
base forms.
4.1 A Hierarchy of Equivalence Classes of Inflected Word Forms
As mentioned in Section 2.3, the lemma-tag representation of the information from
morpho-syntactic analysis makes it possible to gradually access information with dif-
ferent grades of abstraction. Consider, for example, the German verb form ankomme,
which is the indicative present first-person singular form of the lemma ankommen and
can be translated into English by arrive. The lemma-tag representation provides an
?observation tuple? consisting of
? the original full word form (e.g., ankomme),
? morphological and syntactic tags (part of speech, tense, person, case, . . . )
(e.g., verb, indicative, present tense, 1st person singular), and
188
Computational Linguistics Volume 30, Number 2
? the base form (e.g., ankommen).
In the following, ti0 = t0, . . . , ti denotes the representation of a word where the base
form t0 and i additional tags are taken into account. For the example above, t0 =
ankommen, t1 = verb, and so on. The hierarchy of equivalence classes F0, . . . ,Fn is as
follows:
Fn = F(tn0) = ankommen verb indicative present singular 1
Fn?1 = F(tn?10 ) = ankommen verb indicative present singular
Fn?2 = F(tn?20 ) = ankommen verb indicative present
...
F0 = F(t0) = ankommen
where n is the maximum number of morpho-syntactic tags. The mapping from the
full lemma-tag representation back to inflected word forms is generally unambigu-
ous; thus Fn contains only one element, namely, ankomme. Fn?1 contains the forms
ankomme, ankommst, and ankommt; in Fn?2 the number (singular or plural) is ig-
nored, and so on. The largest equivalence class contains all inflected forms of the
base form ankommen.1 Section 4.2 introduces the concept of combining information at
different levels of abstraction.
4.2 Log-Linear Combination
In modeling for statistical machine translation, a hidden variable aJ1, denoting the
hidden alignment between the words in the source and target languages, is usually
introduced into the string translation probability:
Pr(f J1 |eI1) =
?
aJ1
Pr(f J1, a
J
1|eI1) =
?
aJ1
Pr(aJ1|eI1) ? Pr(f
J
1 |a
J
1, e
I
1) (1)
In the following, Tj =
(
tn0
)
j denotes the lemma-tag representation of the jth word in
the input sentence. The sequence TJ1 stands for the sequence of readings for the word
sequence f J1 and can be introduced as a new hidden variable:
Pr(f J1 |a
J
1, e
I
1) =
?
TJ1
Pr(f J1, T
J
1|a
J
1, e
I
1) (2)
which can be decomposed into
Pr(f J1 |a
J
1, e
I
1) =
?
TJ1
J
?
j=1
Pr(fj, Tj|f j?11 , T
j?1
1 , a
J
1, e
I
1) (3)
1 The order of omitting tags can be defined in a natural way depending on the part of speech. In
principle this decision can also be left to the maximum-entropy training, when features for all possible
sets of tags are defined, but this would cause the number of parameters to explode. As the experiments
in this work have been carried out only with up to three levels of abstraction as defined in Section 4.2,
the set of tags of the intermediate level is fixed, and thus the priority of the tags needs not be specified.
The relation between this equivalence class hierarchy and the suggestions in Section 2.4 is clear:
Choosing candidates for morpho-syntactic tags not relevant for translation amounts to fixing a level in
the hierarchy. This is exactly what has been done to define the intermediate level in Section 4.2.
189
Nie?en and Ney SMT with Scarce Resources
Let T (fj) be the set of interpretations which are regarded valid readings of fj by the
morpho-syntactic analyzers on the basis of the whole-sentence context f J1. We assume
that the probability functions defined above yield zero for all other readings, that is,
when Tj ? T (fj). Under the usual independence assumption, which states that the
probability of the translation of words depends only on the identity of the words
associated with each other by the word alignment, we get
Pr(f J1 |a
J
1, e
I
1) =
?
TJ1
Tj ? T (fj)
J
?
j=1
p(fj, Tj|eaj) (4)
As has been argued in Section 2.2, the number of readings |T (fj)| per word form can
be reduced to one for the tasks for which experimental results are reported here.
The elements in equation (4) are the joint probabilities p(f , T|e) of f and the read-
ings T of f given the target language word e. The maximum-entropy principle rec-
ommends choosing for p the distribution which preserves as much uncertainty as
possible in terms of maximizing the entropy, while requiring p to satisfy constraints
which represent facts known from the data. These constraints are encoded on the basis
of feature functions hm(x), and the expectation of each feature hm over the model p is
required to be equal to the observed expectation. The maximum-entropy model can
be shown to be unique and to have an exponential form involving a weighted sum
over the feature functions hm (Ratnaparkhi 1997). In equation (5), the notation tn0 is
used again for the lemma-tag representation of an input word (this was denoted by T
in equations (2)?(4) for notational simplicity):
p(f , T|e) = p?(f , tn0 |e) =
exp
[
?
m
?mhm(e, f , tn0)
]
?
f? ,?tn0
exp
[
?
m
?mhm(e, f? , t?n0)
] (5)
where ? = {?m} is the set of model parameters with one weight ?m for each feature
function hm. These model parameters can be trained using converging iterative training
procedures like the ones described by Darroch and Ratcliff (1972) or Della Pietra, Della
Pietra, and Lafferty (1995).
In the experiments presented in this article, the sum over the word forms f? and
the readings t?n0 in the denominator of equation (5) is restricted to the readings of word
forms having the same base form and partial reading as a word form f ?? aligned at
least once to e.
The new lexicon model p?(f , tn0 |e) can now replace the usual lexicon model p(f |e),
over which it has the following main advantages:
? The decomposition of the modeled events into feature functions allows
meaningful probabilities to be provided for word forms that have not
occurred during training as long as the feature functions involved are
well-defined. (See also the argument later in the article and the definition
of first-level and second-level feature functions presented in
Section 4.2.1.)
? Introducing the hidden variable T = tn0 and constraining the lexicon
probability to be zero for interpretations considered nonvalid readings of
190
Computational Linguistics Volume 30, Number 2
f (that is, for tn0 ? T (f )) amounts to making context information from the
complete sentence f J1 locally available: The sentence context was taken
into account by the morpho-syntactic analyzer, which chose the valid
readings T (f ).
4.2.1 Definition of Feature Functions. There are numerous possibilities for defining
feature functions. We do not need to require that they all have the same parametric
form or that the components be disjoint and statistically independent. Still, it is nec-
essary to restrict the number of parameters so that optimizing them is practical. We
used the following types of feature functions, which have been defined on the basis
of the lemma-tag representation (see Section 2.3):
First level: m = {L, e?}, where L is the base form:
h1L,?e(e, f , t
n
0) =
{
1 if e = e? and t0 = L and f ? F(tn0) (?)
0 otherwise
Second level: m = {T, L, e?}, with subsets T of cardinality ? n of morpho-syntactic
tags considered relevant (see Section 2.4 for a description of the detection
of relevant tags):
h2T,L,?e(e, f , t
n
0) =
{
1 if (?) and T ? tn1 (??)
0 otherwise
Third level: m = {F, T, L, e?}, with the fully inflected original word form F:
h3F,T,L,?e(e, f , t
n
0) =
{
1 if (??) and F = f
0 otherwise
In terms of the hierarchy introduced in Section 4.1, this means that information at three
different levels in the hierarchy is combined. The subsets T of relevant tags mentioned
previously fix the intermediate level.2 This choice of the types of features as well as
the choice of the subsets T is reasonable but somewhat arbitrary. Alternatively one
can think of defining a much more general set of features and applying some method
of feature selection, as has been done, for example, by Foster (2000), who compared
different methods for feature selection within the task of translation modeling for
statistical machine translation. Note that the log-linear model introduced here uses one
parameter per feature. For the Verbmobil task, for example, there are approximately
162, 000 parameters: 47,800 for the first-order features, 55,700 for the second-order
features, and 58,500 for the third-order features. No feature selection or threshold was
applied: All features seen in training were used.
4.2.2 Training Procedure. The overall process of training and testing with hierarchical
lexicon models is depicted in Figure 1. This figure includes the possibility of using
restructuring operations as suggested in Section 3 in order to deal with structural dif-
ferences between the languages involved. This can be especially advantageous in the
case of multiword phrases which jointly fulfill a syntactic function: Not merging them
2 Of course, there is not only one set of relevant tags, but at least one per part of speech. In order to
keep the notation as simple as possible, this fact is not accounted for in the formulas and the textual
descriptions.
191
Nie?en and Ney SMT with Scarce Resources
training of
alignment
restructuring
training of
language model
language
model
alignment
model
alignment
extract 
event counts
lexicon
model
ME training
source
sentence
search for 
optimal translation
output
sentence
analyze
annotation
all readings
of vocabulary
restructuring
restructuring inverse
restructuring
annotation
vocabulary 
supported in test
annotation
source
part
target
part
training corpus
Figure 1
Training and test with hierarchical lexicon. ?(Inverse) restructuring,? ?analyze,? and
?annotation? all require morpho-syntactic analysis of the transformed sentences.
would raise the question of how to distribute the syntactic tags which have been asso-
ciated with the whole phrase. In Section 5.2 we describe a method of learning multi-
word phrases using conventional dictionaries. The alignment on the training corpus
is trained using the original source language corpus containing inflected word forms.
This alignment is then used to count the co-occurrences of the annotated ?words? in
the lemma-tag representation of the source language corpus with the words in the tar-
get language corpus. These event counts are used for the maximum-entropy training
of the model parameters ?.
The probability mass is distributed over (all readings of) the source language
word forms to be supported for test (not necessarily restricted to those occurring dur-
ing training). The only precondition is that the firing features for these unseen events
are known. This ?vocabulary supported in test,? as it is called in Figure 1, can be a
predefined closed vocabulary, as is the case in Verbmobil, in which the output of a
speech recognizer with limited output vocabulary is to be translated. In the easiest
case it is identical to the vocabulary found in the source language part of the training
corpus. The other extreme would be an extended vocabulary containing all automati-
cally generated inflected forms of all base forms occurring in the training corpus. This
vocabulary is annotated with morpho-syntactic tags, ideally under consideration of all
possible readings of all word forms.
192
Computational Linguistics Volume 30, Number 2
To enable the application of the hierarchical lexicon model, the source language
input sentences in test have to be analyzed and annotated with their lemma-tag rep-
resentation before the actual translation process. So far, the sum over the readings in
equation (4) has been ignored, because when the techniques for reducing the amount
of ambiguity described in Section 2.2 and the disambiguated conventional dictionaries
resulting from the approach presented in Section 5.1 are applied, there remains almost
always only one reading per word form.
5. Conventional Dictionaries
Conventional dictionaries are often used as additional evidence to better train the
model parameters in statistical machine translation. The expression conventional dictio-
nary here denotes bilingual collections of word or phrase pairs predominantly collected
?by hand,? usually by lexicographers, as opposed to the probabilistic lexica, which are
learned automatically. Apart from the theoretical problem of how to incorporate ex-
ternal dictionaries in a mathematically sound way into a statistical framework for
machine translation (Brown, Della Pietra, Della Pietra, and Goldsmith 1993) there are
also some pragmatic difficulties: As discussed in Section 2.2, one of the disadvantages
of these conventional dictionaries as compared to full bilingual corpora is that their
entries typically contain single words or short phrases on each language side. Conse-
quently, it is not possible to distinguish among the translations for different readings
of a word. In normal bilingual corpora, the words can often be disambiguated by
taking into account the sentence context in which they occur. For example, from the
context in the sentence Ich werde die Zimmer buchen, it is possible to infer that Zimmer
in this sentence is plural and has to be translated by rooms in English, whereas the
correct translation of Zimmer in the sentence Ich ha?tte gerne ein Zimmer is the singular
form room. The dictionary used by our research group for augmenting the bilingual
data contains two entries for Zimmer: (?Zimmer?|?room?) and (?Zimmer?|?rooms?).
5.1 Disambiguation without Context
The approach described in this section is based on the observation that in many of the
cases of ambiguous entries in dictionaries, the second part of the entry?that is, the
other-language side?contains the information necessary to decide upon the interpre-
tation. In some other cases, the same kind of ambiguity is present in both languages,
and it would be possible and desirable to associate the (semantically) corresponding
readings with one another. The method proposed here takes advantage of these facts
in order to disambiguate dictionary entries.
Figure 2 sketches the procedure for the disambiguation of a conventional dictio-
nary D. In addition to D, a bilingual corpus C1 of the same language pair is required
to train the probability model for tag sequence translations. The word forms in C1
need not match those in D. C1 is not necessarily the training corpus for the translation
task in which the disambiguated version of D will be used. It does not even have to
be taken from the same domain.
A word alignment between the sentences in C1 is trained with some automatic
alignment algorithm. Then the words in the bilingual corpus are replaced by a reduced
form of their lemma-tag representation, in which only a subset of their morpho-syntactic
tags is retained?even the base form is dropped. The remaining subset of tags, in
the following denoted by Tf for the source language and Te for the target language,
consists of tags considered relevant for the task of aligning corresponding readings.
This is not necessarily the same set of tags considered relevant for the task of translation
which was used, for example, to fix the intermediate level for the log-linear lexicon
193
Nie?en and Ney SMT with Scarce Resources
merge phrases
learn phrases
conventional
dictionary
analyze
annotation
all readings
of entries
align corresponding
readings
tag translation
probabilities
disambiguated
dictionary
training of
alignmentalignment
count tag
co-occurences
annotation
bilingual corpus
Figure 2
Disambiguation of conventional dictionaries. ?Learn phrases,? ?analyze,? and ?annotation?
require morpho-syntactic analysis of the transformed sentences.
combination in Section 4.2.1. In the case of the Verbmobil corpus, the maximum length
of a tag sequence is five.
The alignment is used to count the frequency of a certain tag sequence tf in the
source language to be associated with another tag sequence te in the target language
and to compute the tag sequence translation probabilities p(tf |te) as relative frequen-
cies. For the time being, these tag sequence translation probabilities associate readings
of words in one language with readings of words in the other language: Multiword
sequences are not accounted for.
To alleviate this shortcoming it is possible and advisable to automatically detect
and merge multiword phrases. As will be described in Section 5.2, the conventional
bilingual dictionary itself can be used to learn and validate these phrases. The resulting
multiword phrases Pe for the target language and Pf for the source language are
afterwards concatenated within D to form entries consisting of pairs of ?units.?
The next step is to analyze the word forms in D and generate all possible readings
of all entries. It is also possible to ignore those readings that are considered unlikely
for the task under consideration by applying the domain-specific preference rules
proposed in Section 2.2. The process of generating all readings includes replacing word
forms with their lemma-tag representation, which is thereafter reduced by dropping
all morpho-syntactic tags not contained in the tag sets Tf and Te.
Using the tag sequence translation probabilities p(tf |te), the readings in one lan-
guage are aligned with readings in the other language. These alignments are applied to
the full lemma-tag representation (not only tags in Tf and Te) of the expanded dictio-
nary containing one entry per reading of the original word forms. The highest-ranking
aligned readings according to p(tf |te) for each lemma are preserved.
194
Computational Linguistics Volume 30, Number 2
The resulting disambiguated dictionary contains two entries for the German word
Zimmer: (?Zimmer-noun-sg.?|?room-noun-sg.?) and (?Zimmer-noun-pl.?|?room-noun-
pl.?). The target language part is then reduced to the surface forms: (?Zimmer-noun-sg.?|
?room?) and (?Zimmer-noun-pl.?|?rooms?). Note that this augmented dictionary, in the
following denoted by D?, has more entries than D as a result of the step of generating
all readings. The two entries (?beabsichtigt?|?intends?) and (?beabsichtigt?|?intended?),
for example, produce three new entries: (?beabsichtigt-verb-ind.-pres.-sg.-
3rd?|?intends?), (?beabsichtigt-verb-past-part.?|?intended?), and (?beabsichtigt-
adjective-pos.?|?intended?).
5.2 Multiword Phrases
Some recent publications deal with the automatic detection of multiword phrases (Och
and Weber 1998; Tillmann and Ney 2000). These methods are very useful, but they have
one drawback: They rely on sufficiently large training corpora, because they detect
the phrases from automatically learned word alignments. In this section a method for
detecting multiword phrases is suggested which merely requires monolingual syntactic
analyzers and a conventional dictionary.
Some multiword phrases which jointly fulfill a syntactic function are provided
by the analyzers. The phrase irgend etwas (?anything?), for example, may form either
an indefinite determiner or an indefinite pronoun. irgend=etwas is merged by the
analyzer in order to form one single vocabulary entry. In the German part of the Verb-
mobil training corpus 26 different, nonidiomatic multiword phrases are merged, while
there are 318 phrases suggested for the English part. In addition, syntactic informa-
tion like the identification of infinitive markers, determiners, modifying adjectives (for
example, single room), premodifying adverbials (more comfortable), and premodifying
nouns (account number) are used for detecting multiword phrases. When applied to
the English part of the Verbmobil training corpus, these hints suggest 7,225 different
phrases.
Altogether, 26 phrases for German and about 7,500 phrases for English are detected
in this way. It is quite natural that there are more multiword phrases found for English,
as German, unlike English, uses compounding. But the experiments show that it is not
advantageous to use all these phrases for English. Electronic dictionaries can be useful
for detecting those phrases which are important in a statistical machine translation
context: A multiword phrase is considered useful if it is translated into a single word
or a distinct multiword phrase (suggested in a similar way by syntactic analysis) in
another language. There are 290 phrases chosen in this way for the English language.
6. Overall Procedure for Training with Scarce Resources
Taking into account the interdependencies of inflected forms of the same base form
is especially relevant when inflected languages like German are involved and when
training data are sparse. In this situation many of the inflected word forms to account
for in test do not occur during training. Sparse bilingual training data also make ad-
ditional conventional dictionaries especially important. Enriching the dictionaries by
aligning corresponding readings is particularly useful when the dictionaries are used
in conjunction with a hierarchical lexicon, which can access the information neces-
sary to distinguish readings via morpho-syntactic tags. The restructuring operations
described in Section 3 also help in coping with the data sparseness problem, because
they make corresponding sentences more similar. This section proposes a procedure
for combining all these methods in order to improve the translation quality despite
sparseness of data. Figure 3 sketches the proposed procedure.
195
Nie?en and Ney SMT with Scarce Resources
learn phrases
disambiguated
dictionary D?
conventional
dictionary D
bil. corpus
C1
disambiguate
dictionary
restructuring restructuring
monolingual
corpus
LM
training
language
model
restructuring
annotation
train 
alignment
combined
corpus
train hierarch.
lexicon alignment
on combined
corpus
alignment
model
lexicon
model
bil. corpus
C2
Figure 3
Training with scarce resources. ?Restructuring,? ?learn phrases,? and ?annotation? all require
morpho-syntactic analysis of the transformed sentences.
Two different bilingual corpora C1 and C2, one monolingual target language cor-
pus, and a conventional bilingual dictionary D can contribute in various ways to the
overall result. It is important to note here that C1 and C2 can, but need not, be dis-
tinct, and that the monolingual corpus can be identical to the target language part of
C2. Furthermore these corpora can be taken from different domains, and C1 can be
(very) small. Only C2 has to represent the domain and the vocabulary for which the
translation system is built, and only the size of C2 and the monolingual corpus have
a substantial effect on the translation quality. It is interesting to note, though, that
a basic statistical machine translation system with an accuracy near 50% can be built
without any domain-specific bilingual corpus C2, solely on the basis of a disambiguated
dictionary and the hierarchical lexicon models, as Table 9 shows.
? In the first step, multiword phrases are learned and validated on the
dictionary D in the way described in Section 5.2. These multiword
phrases are concatenated in D. Then an alignment is trained on the first
bilingual corpus C1. On the basis of this alignment, the tag sequence
translation probabilities which are needed to align corresponding
readings in the dictionary are extracted, as proposed in Section 5.1. The
result of this step is an expanded and disambiguated dictionary D?. For
this purpose, C1 does not have to cover the vocabulary of D. Besides C1
196
Computational Linguistics Volume 30, Number 2
can be comparatively small, given the limited number of tag sequence
pairs (tf |te) for which translation probabilities must be provided: In the
Verbmobil training corpus, for example, there are only 261 different
German and 110 different English tag sequences.
? In the next step, the second bilingual corpus C2 and D? are combined,
and a word alignment A for both is trained. C2, D?, and A are presented
as input to the maximum-entropy training of a hierarchical lexicon
model as described in Section 4.2.
? The language model can be trained on a separate monolingual corpus.
As monolingual data are much easier and cheaper to compile, this
corpus might be (substantially) larger than the target language part of C2.
7. Experimental Results
7.1 The Tasks and the Corpora
Tests were carried out on Verbmobil data and on Nespole! data. As usual, the sentences
from the test sets were not used for training. The training corpora were used for
training the parameters of IBM model 4.
7.1.1 Verbmobil. Verbmobil was a project for automatic translation of spontaneously
spoken dialogues. A detailed description of the statistical translation system within
Verbmobil is given by Ney et al (2000) and by Och (2002). Table 5 summarizes the
characteristics of the English and German parallel corpus used for training the param-
eters of IBM model 4. A conventional dictionary complements the training corpus (see
Table 6 for the statistics). The vocabulary in Verbmobil was considered closed: There
are official lists of word forms which can be produced by the speech recognizers. Such
lists exist for German and English (see Table 7). Table 8 lists the characteristics of the
two test sets Test and Develop taken from the end-to-end evaluation in Verbmobil, the
development part being meant to tune system parameters on a held-out corpus dif-
ferent from the training as well as the test corpus. As no parameters are optimized on
the development set for the methods described in this article, most of the experiments
were carried out on a joint set containing both test sets.
Table 5
Statistics of corpora for training: Verbmobil and Nespole! Singletons are types occurring only
once in training.
Verbmobil Nespole!
English German English German
Number of sentences 58,073 58,073 3,182 3,182
Number of distinct sentences 57,731 57,771 1,758 1,767
Number of running word forms 549,921 519,523 15,568 14,992
Number of running word forms without punctuation 453,612 418,974 12,461 11,672
Number of word forms 4,673 7,940 1,034 1,363
Number of singleton word forms 1,698 3,453 403 641
Number of base forms 3,639 6,063 1,072 870
Number of singleton base forms 1,236 2,546 461 326
197
Nie?en and Ney SMT with Scarce Resources
Table 6
Conventional dictionary used to complement the
training corpus.
English German
Number of entries 10,498 10,498
Number of running word forms 15,305 12,784
Number of word forms 5,161 7,021
Number of base forms 3,666 5,479
Table 7
The official vocabularies in Verbmobil.
English German
Number of word forms 6,871 10,157
Number of base forms 3,268 6,667
Table 8
Statistics for the test sets for German to English translation: Verbmobil
Eval-2000 (Test and Develop) and Nespole!
Verbmobil Nespole!
Test Develop
Number of sentences 251 276 70
Number of running word forms in German part 2,628 3,159 456
Number of word forms in German part 429 434 180
Trigram LM perplexity of reference translation 30.5 28.1 76.9
7.1.2 Nespole!. Nespole! is a research project that ran from January 2000 to June 2002.
It aimed to provide multimodel support for negotiation (Nespole! 2000; Lavie et al
2001). Table 5 summarizes the corpus statistics of the Nespole! training set. Table 8
provides the corresponding figures for the test set used in this work.
7.2 The Translation System
For testing we used the alignment template translation system, described in Och,
Tillmann, and Ney (1999). Training the parameters for this system entails training of
IBM model 4 parameters in both translation directions and combining the resulting
alignments into one symmetrized alignment. From this symmetrized alignment, the
lexicon probabilities as well as the so-called alignment templates are extracted. The
latter are translation patterns which capture phrase-level translation pairs.
7.3 Performance Measures
The following evaluation criteria were used in the experiments:
BLEU (Bilingual Evaluation Understudy): This score, proposed by Papineni et
al. (2001), is based on the notion of modified n-gram precision, with
n ? {1, . . . , 4}: All candidate unigram, bigram, trigram, and four-gram
counts are collected and clipped against their corresponding maximum
reference counts. The reference n-gram counts are calculated on a corpus
198
Computational Linguistics Volume 30, Number 2
of reference translations for each input sentence. The clipped candidate
counts are summed and normalized by the total number of candidate n-
grams. The geometric mean of the modified precision scores for a test
corpus is calculated and multiplied by an exponential brevity penalty fac-
tor to penalize too-short translations. BLEU is an accuracy measure, while
the others are error measures.
m-WER (multireference word error rate): For each test sentence there is a set of
reference translations. For each translation hypothesis, the edit distance
(number of substitutions, deletions, and insertions) to the most similar
reference is calculated.
SSER (subjective sentence error rate): Each translated sentence is judged by a
human examiner according to an error scale from 0.0 (semantically and
syntactically correct) to 1.0 (completely wrong).
ISER (information item semantic error rate): The test sentences are segmented
into information items; for each of these items, the translation candidates
are assigned either ?OK? or an error class. If the intended information
is conveyed, the translation of an information item is considered correct,
even if there are slight syntactic errors which do not seriously deteriorate
the intelligibility.
For evaluating the SSER and the ISER, we have used the evaluation tool EvalTrans
(Nie?en and Leusch 2000), which is designed to facilitate the work of manually judging
evaluation quality and to ensure consistency over time and across evaluators.
7.4 Impact of the Corpus Size
It is a costly and time-consuming task to compile large texts and have them translated
to form bilingual corpora suitable for training the model parameters for statistical
machine translation. As a consequence, it is important to investigate the amount of
data necessary to sufficiently cover the vocabulary expected in testing. Furthermore,
we want to examine to what extent the incorporation of morphological knowledge
sources can reduce this amount of necessary data. Figure 4 shows the relation between
the size of a typical German corpus and the corresponding number of different full
forms. At the size of 520,000 words, the size of the Verbmobil corpus used for training,
this curve still has a high growth rate.
To investigate the impact of the size of the bilingual corpus available for train-
ing, on translation quality three different setups for training the statistical lexicon on
Verbmobil data have been defined:
? using the full training corpus as described in Table 5, comprising 58,000
sentences
? restricting the corpus to 5,000 sentences (approximately every 11th
sentence)
? using no bilingual training corpus at all (only a bilingual dictionary; see
subsequent discussion)
The language model is always trained on the full English corpus. The argument for
this is that monolingual corpora are always easier and less expensive to obtain than
bilingual corpora. A conventional dictionary is used in all three setups to complement
199
Nie?en and Ney SMT with Scarce Resources
0
1
2
3
4
5
6
7
8
0 100 200 300 400 500
corpus size [1000 words]
vo
ca
bu
la
ry
 s
ize
 [1
00
0 w
ord
 fo
rm
s]
Figure 4
Impact of corpus size (measured in number of running words in the corpus) on vocabulary
size (measured in number of different full-form words found in the corpus) for the German
part of the Verbmobil corpus.
the bilingual corpus. In the last setup, the lexicon probabilities are trained exclusively
on this dictionary
As Table 9 shows, the quality of translation drops significantly when the amount
of bilingual data available during training is reduced: When the training corpus is
restricted to 5,000 sentences, the SSER increases by about 7% and the ISER by about
3%. As could be expected, the translations produced by the system trained exclusively
on a conventional dictionary are very poor: The SSER jumps over 60%.
7.5 Results for Log-Linear Lexicon Combination
7.5.1 Results on the Verbmobil Task. As was pointed out in Section 4, the hierarchi-
cal lexicon is expected to be especially useful in cases in which many of the inflected
word forms to be accounted for in test do not occur during training. To systematically
investigate the model?s generalization capability, it has been applied on the three dif-
ferent setups described in Section 7.4. The training procedure was the one proposed
in Section 6, which includes restructuring transformations in training and test. Table 9
summarizes the improvement achieved for all three setups.
Training on 58,000 sentences plus conventional dictionary: Compared to the ef-
fect of restructuring, the additional improvement achieved with the hier-
archical lexicon is relatively small in this setup. The combination of all
methods results in a relative improvement in terms of SSER of almost
13% and in terms of information ISER of more than 16% as compared to
the baseline.
Training on 5,000 sentences plus conventional dictionary: Restructuring alone
can improve the translation quality from 37.3% to 33.6%. The benefit from
the hierarchical lexicon is larger in this setup, and the resulting in SSER is
31.8%. This is a relative improvement of almost 15%. The relative improve-
ment in terms of ISER is almost 22%. Note that by applying the methods
200
Computational Linguistics Volume 30, Number 2
Table 9
Results for hierarchical lexicon models and translation with scarce resources.
?Restructuring? entails treatment of question inversion and separated verb prefixes as
well as merging of phrases in both languages. A conventional dictionary is available in
all three setups. The language model is always trained on the full monolingual English
corpus. Task: Verbmobil. Testing on 527 sentences (Test and Develop).
Number of sentences
for training BLEU m-WER SSER ISER
58,000 Baseline 53.7% 34.1% 30.2% 14.1%
Restructuring 56.3 32.5 26.6 12.8
+ dictionary disambiguated
+ hierarchical lexicon 57.1 31.8 26.3 11.8
5,000 Baseline 47.4 38.0 37.3 17.4
Restructuring 52.1 34.7 33.6 15.2
+ dictionary disambiguated
+ hierarchical lexicon 52.9 33.9 31.8 13.7
0 Baseline 23.3 53.6 60.4 29.8
Restructuring 29.1 50.2 57.8 30.0
+ dictionary disambiguated
+ hierarchical lexicon 32.6 48.0 52.8 24.1
proposed here, the corpus for training can be reduced to less than 10%
of the original size while increasing the SSER only from 30.2% to 31.8%
compared to the baseline when using the full corpus.
Training only on conventional dictionary: In this setup the impact of the hierar-
chical lexicon is clearly larger than the effect of the restructuring methods,
because here the data sparseness problem is much more important than
the word order problem. The overall relative reduction in terms of SSER
is 13.7% and in terms of ISER 19.1%. An error rate of about 52% is still
very poor, but it is close to what might be acceptable when only the gist
of the translated document is needed, as is the case in the framework of
document classification or multilingual information retrieval.
Examples taken from the Verbmobil Eval-2000 test set are given in Table 10.
Smoothing the lexicon probabilities over the inflected forms of the same lemma en-
ables the translation of sind as would instead of are. The smoothed lexicon contains the
translation convenient for any inflected form of bequem. The comparative more conve-
nient would be the completely correct translation. The last two examples in the table
demonstrate the effect of the disambiguating analyzer, which on the basis of the sen-
tence context identifies Zimmer as plural (it has been translated into the singular form
room by the baseline system) and das as an article to be translated by the instead of a
pronoun which would be translated as that. The last example demonstrates that over-
fitting on domain-specific training can be problematic in some cases: Generally, because
is a good translation for the co-ordinating conjunction denn, but in the appointment-
scheduling domain, denn is often an adverb, and it often occurs in the same sentence
as dann, as in Wie wa?re es denn dann?. The translation for this sentence is something
like How about then?. Because of the frequency of this domain-specific language use,
the word form denn is often aligned to then in the training corpus. The hierarchical
201
Nie?en and Ney SMT with Scarce Resources
Table 10
Examples of the effect of the hierarchical lexicon.
Input sind Sie mit einem Doppelzimmer einverstanden?
Baseline are you agree with a double room?
Hierarchical lexicon would you agree with a double room?
Input mit dem Zug ist es bequemer.
Baseline by train it is UNKNOWN-bequemer.
Hierarchical lexicon by train it is convenient.
Input wir haben zwei Zimmer.
Baseline we have two room.
Hierarchical lexicon we have two rooms.
Input ich wu?rde das Hilton vorschlagen denn es ist das beste.
Baseline I would suggest that Hilton then it is the best.
Hierarchical lexicon I would suggest the Hilton because it is the best.
lexicon distinguishes the adverb reading and the conjunction reading, and the correct
translation because is the highest-ranking one for the conjunction.
7.5.2 Results on the Nespole! Task. We were provided with a small German-English
corpus from the Nespole! project (see Section 7.1 for a description). From Table 5 it
is obvious that this task is an example of very scarce training data, and it is thus
interesting to test the performance of the methods proposed in this article on this
task. The same conventional dictionary as was used for the experiments on Verbmobil
data (cf. Table 6) complemented the small bilingual training corpus. Furthermore, the
(monolingual) English part of the Verbmobil corpus was used in addition to the English
part of the Nespole! corpus for training the language model. Table 11 summarizes the
results. Information items have not been defined for this test set. An overall relative
improvement of 16.5% in the SSER can be achieved.
8. Conclusion
In this article we have proposed methods of incorporating morphological and syntactic
information into systems for statistical machine translation. The overall goal was to
improve translation quality and to reduce the amount of parallel text necessary to
Table 11
Results for hierarchical lexicon model Nespole!
?Restructuring? entails treatment of question
inversion and separated verb prefixes as well as
merging of phrases in both languages. The same
conventional dictionary was used as in the
experiments the Verbmobil. The language model
was trained on a combination of the English parts
of the Nespole! corpus and the Verbmobil corpus.
BLEU m-WER SSER
Baseline 31.6% 50.2% 41.1%
Restructuring 33.7 45.9 38.1
+ hierarchical lexicon 36.5 44.1 34.3
202
Computational Linguistics Volume 30, Number 2
train the model parameters. Substantial improvements on the Verbmobil task and the
Nespole! task were achieved.
Some sentence-level restructuring transformations have been introduced which
are motivated by knowledge about the sentence structure in the languages involved.
These transformations aim at the assimilation of word orders in related sentences.
A hierarchy of equivalence classes has been defined on the basis of morpholog-
ical and syntactic information beyond the surface forms. The study of the effect of
using information from either degree of abstraction led to the construction of hier-
archical lexicon models, which combine different items of information in a log-linear
way. The benefit from these combined models is twofold: First, the lexical coverage is
improved, because the translation of unseen word forms can be derived by consider-
ing information from lower levels in the hierarchy. Second, category ambiguity can be
resolved, because syntactical context information is made locally accessible by means
of annotation with morpho-syntactic tags. As a side effect of the preparative work for
setting up the underlying hierarchy of morpho-syntactic information, those pieces of
information inherent in fully inflected word forms that are not relevant for translation
are detected.
A method for aligning corresponding readings in conventional dictionaries con-
taining pairs of fully inflected word forms has been proposed. The approach uses
information deduced from one language side to resolve category ambiguity in the
corresponding entry in the other language. The resulting disambiguated dictionar-
ies have proven to be better suited for improving the quality of machine translation,
especially if they are used in combination with the hierarchical lexicon models.
The amount of bilingual training data required to achieve an acceptable quality of
machine translation has been systematically investigated. All the methods mentioned
previously contribute to a better exploitation of the available bilingual data and thus
to improving translation quality in frameworks with scarce resources. Three setups for
training the parameters of the statistical lexicon on Verbmobil data have been exam-
ined: (1) Using the full 58,000 sentences comprising the bilingual training corpus, (2)
restricting the corpus to 5,000 sentences, and (3) using only a conventional dictionary.
For each of these setups, a relative improvement in terms of subjective sentence error
rate between 13% and 15% as compared to the baseline could be obtained using combi-
nations of the methods described in this article. The amount of bilingual training data
could be reduced to less than 10% of the original corpus, while losing only 1.6% in
accuracy as measured by the subjective sentence error rate. A relative improvement of
16.5% in terms of subjective sentence error rate could also be achieved on the Nespole!
task.
Acknowledgments
This work has been partially supported as
part of the Verbmobil project (contract
number 01 IV 701 T4) by the German
Federal Ministry of Education, Science,
Research and Technology and as part of the
EuTrans project (project number 30268) by
the European Union. For the provision of
the Nespole! data we thank the Nespole!
consortium, listed on the project?s home
page (Nespole! 2000). Special thanks to
Alon Lavie, Lori Levin, Stephan Vogel, and
Alex Waibel (in alphabetical order).
References
Al-Onaizan, Yaser, Ulrich Germann, Ulf
Hermjakob, Kevin Knight, Philipp Koehn,
Daniel Marcu, and Kenji Yamada. 2000.
Translating with scarce resources. In
Proceedings of the 17th National Conference on
Artificial Intelligence (AAAI), pages
672?678, Austin, TX, August.
Berger, Adam L., Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra, J. R.
Gillett, and A. S. Kehler. 1996. Language
translation apparatus and method of
using context-based translation models.
United States Patent, Patent Number
5510981, April.
203
Nie?en and Ney SMT with Scarce Resources
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and M. J.
Goldsmith. 1993. But dictionaries are data
too. In Proceedings of the ARPA Human
Language Technology Workshop ?93, pages
202?205, Princeton, NJ, March.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, Robert L. Mercer, and
Paul S. Roossin. 1988. A statistical
approach to language translation. In
Proceedings of COLING 1988: The 12th
International Conference on Computational
Linguistics, pages 71?76, Budapest,
August.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, John D. Lafferty,
and Robert L. Mercer. 1992. Analysis,
statistical transfer, and synthesis in
machine translation. In Proceedings of TMI
1992: Fourth International Conference on
Theoretical and Methodological Issues in MT,
pages 83?100, Montreal, Quebec, Canada,
June.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. Mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Darroch, J. N. and D. Ratcliff. 1972.
Generalized iterative scaling for log-linear
models. Annals of Mathematical Statistics,
43:1470?1480.
Della Pietra, Stephen A., Vincent J.
Della Pietra, and John D. Lafferty. 1995.
Inducing features of random fields. Tech-
nical Report CMU-CS-95-144, Carnegie
Mellon University, Pittsburgh, PA.
Foster, George. 2000. A maximum
entropy/minimum divergence translation
model. In Proceedings of ACL 2000: The 38th
Annual Meeting of the Association for
Computational Linguistics, pages 37?44,
Hong Kong, October.
Garc??a-Varea, Ismael and Francisco
Casacuberta. 2001. Search algorithms for
statistical machine translation based on
dynamic programming and pruning
techniques. In Proceedings of the MT
Summit VIII, pages 115?120, Santiago de
Compostela, Spain, September.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
ACL-EACL 2001: The 39th Annual Meeting
of the Association for Computational
Linguistics (joint with EACL 2001), pages
228?235, Toulouse, France, July.
Kanevsky, Dimitri, Salim Roukos, and Jan
Sedivy. 1997. Statistical language model
for inflected languages. United States
Patent, Patent Number 5835888.
Karlsson, Fred. 1990. Constraint grammar as
a framework for parsing running text. In
Proceedings of COLING 1990: The 13th
International Conference on Computational
Linguistics, volume 3, pages 168?173,
Helsinki, August.
Koehn, Philipp and Kevin Knight. 2001.
Knowledge sources for word-level
translation models. In Lillian Lee and
Donna Harman, editors, Proceedings of
EMNLP 2001: Conference on Empirical
Methods in Natural Language Processing,
pages 27?35, Pittsburgh, PA, June.
Larson, Martha, Daniel Willett, Joachim
Ko?hler, and Gerhard Rigoll. 2000.
Compound splitting and lexical unit
recombination for improved performance
of a speech recognition system for
German parliamentary speeches. In
Proceedings ICSLP 2000: Sixth International
Conference on Spoken Language Processing,
volume 3, pages 945?948, Beijing,
February.
Lavie, Alon, Chad Langley, Alex Waibel,
Fabio Pianesi, Gianni Lazzari, Paolo
Coletti, Loredana Taddei, and Franco
Balducci. 2001. Architecture and design
considerations in NESPOLE! A speech
translation system for e-commerce
applications. In James Allan, editor,
Proceedings of HLT 2001: First International
Conference on Human Language Technology
Research, pages 31?39, San Diego, March.
Maltese, G., and F. Mancini. 1992. An
automatic technique to include
grammatical and morphological
information in a trigram-based statistical
language model. In Proceedings of ICASSP
1992: International Conference on Acoustics,
Speech and Signal Processing, pages
157?160, San Francisco, March.
NESPOLE! (NEgotiating through SPOken
Language in e-commerce). 2000 Project
homepage. Available at http://
nespole.itc.it/.
Ney, Hermann, Sonja Nie?en, Franz Josef
Och, Hassan Sawaf, Christoph Tillmann,
and Stephan Vogel. 2000. Algorithms for
statistical translation of spoken language.
IEEE Transactions on Speech and Audio
Processing, 8(1):24?36.
204
Computational Linguistics Volume 30, Number 2
Nie?en, Sonja and Gregor Leusch. 2000.
EvalTrans, a tool for semi-automatic
evaluation of machine translation. In
Proceedings of LREC 2000, Athens. Tool is
available at http://www-i6.Informatik.
RWTH-Aachen.DE/?niessen/Evaluation/.
Nie?en, Sonja and Hermann Ney. 2000.
Improving SMT quality with
morpho-syntactic analysis. In Proceedings
of COLING 2000: The 18th International
Conference on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany,
July.
Nie?en, Sonja and Hermann Ney. 2001.
Morpho-syntactic analysis for reordering
in statistical machine translation. In
Proceedings of MT Summit VIII, pages
247?252, Santiago de Compostela, Spain,
September.
Nie?en, Sonja, Stephan Vogel, Hermann
Ney, and Christoph Tillmann. 1998. A DP
based search algorithm for statistical
machine translation. In Proceedings of
COLING-ACL 1998: The 36th Annual
Meeting of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics,
pages 960?967, Montreal, Quebec,
Canada, August.
Och, Franz Josef. 2002. Machine Translation:
From Single-Word Models to Alignment
Templates. Ph.D. thesis, Computer Science
Department, RWTH?University of
Technology, Aachen, Germany.
Och, Franz Josef, Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of EMNLP 1999: Conference
on Empirical Methods in Natural Language
Processing, pages 20?28, University of
Maryland, College Park, June.
Och, Franz Josef and Hans Weber. 1998.
Improving statistical natural language
translation with categories and rules. In
Proceedings of COLING-ACL 1998: The 36th
Annual Meeting of the Association for
Computational Linguistics and the 17th
International Conference on Computational
Linguistics, pages 985?989, Montreal,
Quebec, Canada, August.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. Bleu: A
method for automatic evaluation of
machine translation. Technical Report
RC22176 (W0109-022), IBM Research
Division, Yorktown Heights, NY,
September.
Ratnaparkhi, Adwait. 1997. A simple
introduction to maximum entropy models
for natural language processing. Technical
Report 97?08, Institute for Research in
Cognitive Science, University of
Pennsylvania, Philadelphia, May.
Tillmann, Christoph and Hermann Ney.
2000. Word re-ordering and DP-based
search in statistical machine translation.
In Proceedings of COLING 2000: The 18th
International Conference on Computational
Linguistics, pages 850?856, Saarbru?cken,
Germany, August.
c? 2004 Association for Computational Linguistics
The Alignment Template Approach to
Statistical Machine Translation
Franz Josef Och? Hermann Ney?
Google RWTH Aachen
A phrase-based statistical machine translation approach ? the alignment template approach ? is
described. This translation approach allows for general many-to-many relations between words.
Thereby, the context of words is taken into account in the translation model, and local changes
in word order from source to target language can be learned explicitly. The model is described
using a log-linear modeling approach, which is a generalization of the often used source?channel
approach. Thereby, the model is easier to extend than classical statistical machine translation
systems. We describe in detail the process for learning phrasal translations, the feature functions
used, and the search algorithm. The evaluation of this approach is performed on three different
tasks. For the German?English speech Verbmobil task, we analyze the effect of various sys-
tem components. On the French?English Canadian Hansards task, the alignment template
system obtains significantly better results than a single-word-based translation model. In the
Chinese?English 2002 National Institute of Standards and Technology (NIST) machine transla-
tion evaluation it yields statistically significantly better NIST scores than all competing research
and commercial translation systems.
1. Introduction
Machine translation (MT) is a hard problem, because natural languages are highly
complex, many words have various meanings and different possible translations, sen-
tences might have various readings, and the relationships between linguistic entities
are often vague. In addition, it is sometimes necessary to take world knowledge into
account. The number of relevant dependencies is much too large and those depen-
dencies are too complex to take them all into account in a machine translation system.
Given these boundary conditions, a machine translation system has to make decisions
(produce translations) given incomplete knowledge. In such a case, a principled ap-
proach to solving that problem is to use the concepts of statistical decision theory to try
to make optimal decisions given incomplete knowledge. This is the goal of statistical
machine translation.
The use of statistical techniques in machine translation has led to dramatic im-
provements in the quality of research systems in recent years. For example, the statis-
tical approaches of the Verbmobil evaluations (Wahlster 2000) or the U.S. National
? 1600 Amphitheatre Parkway, Mountain View, CA 94043. E-mail: och@google.com.
? Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,
Ahornstr. 55, 52056 Aachen, Germany. E-mail: ney@cs.rwth-aachen.de.
Submission received: 19 November 2002; Revised submission received: 7 October 2003; Accepted for
publication: 1 June 2004
418
Computational Linguistics Volume 30, Number 4
Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through
20031 obtain the best results. In addition, the field of statistical machine translation is
rapidly progressing, and the quality of systems is getting better and better. An im-
portant factor in these improvements is definitely the availability of large amounts of
data for training statistical models. Yet the modeling, training, and search methods
have also improved since the field of statistical machine translation was pioneered by
IBM in the late 1980s and early 1990s (Brown et al 1990; Brown et al 1993; Berger et
al. 1994). This article focuses on an important improvement, namely, the use of (gen-
eralized) phrases instead of just single words as the core elements of the statistical
translation model.
We describe in Section 2 the basics of our statistical translation model. We suggest
the use of a log-linear model to incorporate the various knowledge sources into an
overall translation system and to perform discriminative training of the free model
parameters. This approach can be seen as a generalization of the originally suggested
source?channel modeling framework for statistical machine translation.
In Section 3, we describe the statistical alignment models used to obtain a word
alignment and techniques for learning phrase translations from word alignments. Here,
the term phrase just refers to a consecutive sequence of words occurring in text and
has to be distinguished from the use of the term in a linguistic sense. The learned
bilingual phrases are not constrained by linguistic phrase boundaries. Compared to
the word-based statistical translation models in Brown et al (1993), this model is based
on a (statistical) phrase lexicon instead of a single-word-based lexicon. Looking at the
results of the recent machine translation evaluations, this approach seems currently to
give the best results, and an increasing number of researchers are working on different
methods for learning phrase translation lexica for machine translation purposes (Marcu
and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and
Marcu 2003). Our approach to learning a phrase translation lexicon works in two
stages: In the first stage, we compute an alignment between words, and in the second
stage, we extract the aligned phrase pairs. In our machine translation system, we then
use generalized versions of these phrases, called alignment templates, that also include
the word alignment and use word classes instead of the words themselves.
In Section 4, we describe the various components of the statistical translation
model. The backbone of the translation model is the alignment template feature func-
tion, which requires that a translation of a new sentence be composed of a set of align-
ment templates that covers the source sentence and the produced translation. Other
feature functions score the well-formedness of the produced target language sentence
(i.e., language model feature functions), the number of produced words, or the or-
der of the alignment templates. Note that all components of our statistical machine
translation model are purely data-driven and that there is no need for linguistically
annotated corpora. This is an important advantage compared to syntax-based trans-
lation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada
2003) that require a parser for source or target language.
In Section 5, we describe in detail our search algorithm and discuss an efficient
implementation. We use a dynamic-programming-based beam search algorithm that
allows a trade-off between efficiency and quality. We also discuss the use of heuristic
functions to reduce the number of search errors for a fixed beam size.
In Section 6, we describe various results obtained on different tasks. For the
German?English Verbmobil task, we analyze the effect of various system compo-
1 http://www.nist.gov/speech/tests/mt/.
419
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Figure 1
Architecture of the translation approach based on a log-linear modeling approach.
nents. On the French?English Canadian Hansards task, the alignment template sys-
tem obtains significantly better results than a single-word-based translation model. In
the Chinese?English 2002 NIST machine translation evaluation it yields results that are
significantly better statistically than all competing research and commercial translation
systems.
2. Log-Linear Models for Statistical Machine Translation
We are given a source (French) sentence f = f J1 = f1, . . . , fj, . . . , fJ, which is to be trans-
lated into a target (English) sentence e = eI1 = e1, . . . , ei, . . . , eI. Among all possible
target sentences, we will choose the sentence with the highest probability:2
e?I1 = argmax
eI1
{Pr(eI1 | f
J
1)} (1)
The argmax operation denotes the search problem, that is, the generation of the output
sentence in the target language.
As an alternative to the often used source?channel approach (Brown et al 1993),
we directly model the posterior probability Pr(eI1 | f
J
1) (Och and Ney 2002). An es-
pecially well-founded framework for doing this is the maximum-entropy framework
(Berger, Della Pietra, and Della Pietra 1996). In this framework, we have a set of M fea-
ture functions hm(eI1, f
J
1), m = 1, . . . , M. For each feature function, there exists a model
2 The notational convention employed in this article is as follows. We use the symbol Pr(?) to denote
general probability distributions with (nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
420
Computational Linguistics Volume 30, Number 4
parameter ?m, m = 1, . . . , M. The direct translation probability is given by
Pr(eI1 | f
J
1) = p?M1 (e
I
1 | f
J
1) (2)
=
exp[
?M
m=1 ?mhm(e
I
1, f
J
1)]
?
e? I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1)]
(3)
This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a
natural language understanding task.
We obtain the following decision rule:
e?I1 = argmax
eI1
{
Pr(eI1 | f
J
1)
}
= argmax
eI1
{
M
?
m=1
?mhm(eI1, f
J
1)
}
Hence, the time-consuming renormalization in equation (3) is not needed in search.
The overall architecture of the log-linear modeling approach is summarized in Figure 1.
A standard criterion on a parallel training corpus consisting of S sentence pairs
{(fs, es): s = 1, . . . , S} for log-linear models is the maximum class posterior probability
criterion, which can be derived from the maximum-entropy principle:
??M1 = argmax
?M1
{
S
?
s=1
log p?M1 (es | fs)
}
(4)
This corresponds to maximizing the equivocation or maximizing the likelihood of the
direct-translation model. This direct optimization of the posterior probability in Bayes?
decision rule is referred to as discriminative training (Ney 1995) because we directly
take into account the overlap in the probability distributions. The optimization prob-
lem under this criterion has very nice properties: There is one unique global optimum,
and there are algorithms (e.g. gradient descent) that are guaranteed to converge to the
global optimum. Yet the ultimate goal is to obtain good translation quality on un-
seen test data. An alternative training criterion therefore directly optimizes translation
quality as measured by an automatic evaluation criterion (Och 2003).
Typically, the translation probability Pr(eI1 | f
J
1) is decomposed via additional hid-
den variables. To include these dependencies in our log-linear model, we extend the
feature functions to include the dependence on the additional hidden variable. Using
for example the alignment aJ1 as hidden variable, we obtain M feature functions of the
form hm(eI1, f
J
1, a
J
1), m = 1, . . . , M and the following model:
Pr(eI1, a
J
1 | f
J
1) =
exp
(
?M
m=1 ?mhm(e
I
1, f
J
1, a
J
1)
)
?
e? I1,a
?J
1
exp
(
?M
m=1 ?mhm(e
?I
1, f
J
1, a
?J
1)
)
Obviously, we can perform the same step for translation models with an even richer
set of hidden variables than only the alignment aJ1.
3. Learning Translation Lexica
In this section, we describe methods for learning the single-word and phrase-based
translation lexica that are the basis of the machine translation system described in
421
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Section 4. First, we introduce the basic concepts of statistical alignment models, which
are used to learn word alignment. Then, we describe how these alignments can be
used to learn bilingual phrasal translations.
3.1 Statistical Alignment Models
In (statistical) alignment models Pr(f J1, a
J
1 | eI1), a ?hidden? alignment a = a
J
1 is intro-
duced that describes a mapping from a source position j to a target position aj. The
relationship between the translation model and the alignment model is given by
Pr(f J1 | eI1) =
?
aJ1
Pr(f J1, a
J
1 | eI1) (5)
The alignment aJ1 may contain alignments aj = 0 with the ?empty? word e0 to account
for source words that are not aligned with any target word.
In general, the statistical model depends on a set of unknown parameters ? that is
learned from training data. To express the dependence of the model on the parameter
set, we use the following notation:
Pr(f J1, a
J
1 | eI1) = p?(f
J
1, a
J
1 | eI1) (6)
A detailed description of different specific statistical alignment models can be found in
Brown et al (1993) and Och and Ney (2003). Here, we use the hidden Markov model
(HMM) alignment model (Vogel, Ney, and Tillmann 1996) and Model 4 of Brown et
al. (1993) to compute the word alignment for the parallel training corpus.
To train the unknown parameters ?, we are given a parallel training corpus con-
sisting of S sentence pairs {(fs, es): s = 1, . . . , S}. For each sentence pair (fs, es), the
alignment variable is denoted by a = aJ1. The unknown parameters ? are determined
by maximizing the likelihood on the parallel training corpus:
?? = argmax
?
{
S
?
s=1
[
?
a
p?(fs, a | es)
]}
(7)
This optimization can be performed using the expectation maximization (EM) algo-
rithm (Dempster, Laird, and Rubin 1977). For a given sentence pair there are a large
number of alignments. The alignment a?J1 that has the highest probability (under a
certain model) is also called the Viterbi alignment (of that model):
a?J1 = argmax
aJ1
p??(f
J
1, a
J
1 | eI1) (8)
A detailed comparison of the quality of these Viterbi alignments for various statistical
alignment models compared to human-made word alignments can be found in Och
and Ney (2003).
3.2 Symmetrization
The baseline alignment model does not allow a source word to be aligned with two
or more target words. Therefore, lexical correspondences like the German compound
word Zahnarzttermin for dentist?s appointment cause problems because a single source
word must be mapped onto two or more target words. Therefore, the resulting Viterbi
alignment of the standard alignment models has a systematic loss in recall. Here, we
422
Computational Linguistics Volume 30, Number 4
Figure 2
Example of a (symmetrized) word alignment (Verbmobil task).
describe various methods for performing a symmetrization of our directed statistical
alignment models by applying a heuristic postprocessing step that combines the align-
ments in both translation directions (source to target, target to source). Figure 2 shows
an example of a symmetrized alignment.
To solve this problem, we train in both translation directions. For each sentence
pair, we compute two Viterbi alignments aJ1 and b
I
1. Let A1 = {(aj, j) | aj > 0} and
A2 = {(i, bi) | bi > 0} denote the sets of alignments in the two Viterbi alignments. To
increase the quality of the alignments, we can combine (symmetrize) A1 and A2 into
one alignment matrix A using one of the following combination methods:
? Intersection: A = A1 ? A2.
? Union: A = A1 ? A2.
? Refined method: In a first step, the intersection A = A1 ? A2 is
determined. The elements of this intersection result from both Viterbi
alignments and are therefore very reliable. Then, we extend the
alignment A iteratively by adding alignments (i, j) occurring only in the
423
Och and Ney The Alignment Template Approach to Statistical Machine Translation
alignment A1 or in the alignment A2 if neither fj nor ei have an alignment
in A, or if the following conditions both hold:
? The alignment (i, j) has a horizontal neighbor (i ? 1, j), (i + 1, j)
or a vertical neighbor (i, j ? 1), (i, j + 1) that is already in A.
? The set A ? {(i, j)} does not contain alignments with both
horizontal and vertical neighbors.
Obviously, the intersection yields an alignment consisting of only one-to-one align-
ments with a higher precision and a lower recall. The union yields a higher recall
and a lower precision of the combined alignment. The refined alignment method is
often able to improve precision and recall compared to the nonsymmetrized align-
ments. Whether a higher precision or a higher recall is preferred depends on the final
application of the word alignment. For the purpose of statistical MT, it seems that a
higher recall is more important. Therefore, we use the union or the refined combination
method to obtain a symmetrized alignment matrix.
The resulting symmetrized alignments are then used to train single-word-based
translation lexica p(e | f ) by computing relative frequencies using the count N(e, f ) of
how many times e and f are aligned divided by the count N(f ) of how many times
the word f occurs:
p(e | f ) = N(e, f )
N(f )
3.3 Bilingual Contiguous Phrases
In this section, we present a method for learning relationships between whole phrases
of m source language words and n target language words. This algorithm, which
will be called phrase-extract, takes as input a general word alignment matrix (Sec-
tion 3.2). The output is a set of bilingual phrases.
In the following, we describe the criterion that defines the set of phrases that is
consistent with the word alignment matrix:
BP(f J1, eI1, A) =
{(
f j+mj , e
i+n
i
)
: ?(i?, j?) ? A : j ? j? ? j + m ? i ? i? ? i + n (9)
??(i?, j?) ? A : j ? j? ? j + m ? i ? i? ? i + n
}
Hence, the set of all bilingual phrases that are consistent with the alignment is con-
stituted by all bilingual phrase pairs in which all words within the source language
phrase are aligned only with the words of the target language phrase and the words
of the target language phrase are aligned only with the words of the source language
phrase. Note that we require that at least one word in the source language phrase be
aligned with at least one word of the target language phrase. As a result there are no
empty source or target language phrases that would correspond to the ?empty word?
of the word-based statistical alignment models.
These phrases can be computed straightforwardly by enumerating all possible
phrases in one language and checking whether the aligned words in the other lan-
guage are consecutive, with the possible exception of words that are not aligned at
all. Figure 3 gives the algorithm phrase-extract that computes the phrases. The algo-
rithm takes into account possibly unaligned words at the boundaries of the source or
target language phrases. Table 1 shows the bilingual phrases containing between two
and seven words that result from the application of this algorithm to the alignment
of Figure 2.
424
Computational Linguistics Volume 30, Number 4
Table 1
Examples of two- to seven-word bilingual phrases obtained by applying the algorithm
phrase-extract to the alignment of Figure 2.
ja , yes ,
ja , ich yes , I
ja , ich denke mal yes , I think
ja , ich denke mal , yes , I think ,
ja , ich denke mal , also yes , I think , well
, ich , I
, ich denke mal , I think
, ich denke mal , , I think ,
, ich denke mal , also , I think , well
, ich denke mal , also wir , I think , well we
ich denke mal I think
ich denke mal , I think ,
ich denke mal , also I think , well
ich denke mal , also wir I think , well we
ich denke mal , also wir wollten I think , well we plan to
denke mal , think ,
denke mal , also think , well
denke mal , also wir think , well we
denke mal , also wir wollten think , well we plan to
, also , well
, also wir , well we
, also wir wollten , well we plan to
also wir well we
also wir wollten well we plan to
wir wollten we plan to
in unserer in our
in unserer Abteilung in our department
in unserer Abteilung ein neues Netzwerk a new network in our department
in unserer Abteilung ein neues Netzwerk set up a new network in our department
aufbauen
unserer Abteilung our department
ein neues a new
ein neues Netzwerk a new network
ein neues Netzwerk aufbauen set up a new network
neues Netzwerk new network
It should be emphasized that this constraint to consecutive phrases limits the ex-
pressive power. If a consecutive phrase in one language is translated into two or three
nonconsecutive phrases in the other language, there is no corresponding bilingual
phrase pair learned by this approach. In principle, this approach to learning phrases
from a word-aligned corpus could be extended straightforwardly to handle noncon-
secutive phrases in source and target language as well. Informal experiments have
shown that allowing for nonconsecutive phrases significantly increases the number of
extracted phrases and especially increases the percentage of wrong phrases. Therefore,
we consider only consecutive phrases.
3.4 Alignment Templates
In the following, we add generalization capability to the bilingual phrase lexicon by
replacing words with word classes and also by storing the alignment information
for each phrase pair. These generalized and alignment-annotated phrase pairs are
called alignment templates. Formally, an alignment template z is a triple (FJ
?
1 , E
I?
1 , A?)
425
Och and Ney The Alignment Template Approach to Statistical Machine Translation
INPUT: eI1, f
J
1, A
i1 := 1
WHILE i1 ? I
i2 := i1
WHILE i2 ? I
TP := {j|?i : i1 ? i ? i2 ? A(i, j)}
IF quasi-consecutive(TP)
THEN j1 := min(TP)
j2 := max(TP)
SP := {i|?j : j1 ? j ? j2 ? A(i, j)}
IF SP ? {i1, i1 + 1, . . . , i2}
THEN BP := BP ? {(ei2i1 , f
j2
j1
)}
WHILE j1 > 0 ? ?i : A(i, j1) = 0
j?? := j2
WHILE j?? ? J ? ?i : A(i, j??) = 0
BP := BP ? {(ei2i1 , f
j??
j1
)}
j?? := j?? + 1
j1 := j1 ? 1
OUTPUT: BP
Figure 3
Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair. Here
quasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive,
with the possible exception of words that are not aligned.
that describes the alignment A? between a source class sequence FJ
?
1 and a target class
sequence EI
?
1 . If each word corresponds to one class, an alignment template corresponds
to a bilingual phrase together with an alignment within this phrase. Figure 4 shows
examples of alignment templates.
The alignment A? is represented as a matrix with J? ? (I? + 1) binary elements. A
matrix element with value 1 means that the words at the corresponding positions are
aligned, and the value 0 means that the words are not aligned. If a source word is not
aligned with a target word, then it is aligned with the empty word e0, which is at the
imaginary position i = 0.
The classes used in FJ
?
1 and E
I?
1 are automatically trained bilingual classes using
the method described in Och (1999) and constitute a partition of the vocabulary of
source and target language. In general, we are not limited to disjoint classes as long
as each specific instance of a word is disambiguated, that is, uniquely belongs to a
specific class. In the following, we use the class function C to map words to their
classes. Hence, it would be possible to employ parts-of-speech or semantic categories
instead of the automatically trained word classes used here.
The use of classes instead of the words themselves has the advantage of better
generalization. For example, if there exist classes in source and target language that
contain town names, it is possible that an alignment template learned using a specific
town name can be generalized to other town names.
In the following, e? and f? denote target and source phrases, respectively. To train
the probability of applying an alignment template p(z = (FJ
?
1 , E
I?
1 , A?) | f? ), we use an
extended version of the algorithm phrase-extract from Section 3.3. All bilingual
phrases that are consistent with the alignment are extracted together with the align-
426
Computational Linguistics Volume 30, Number 4
Figure 4
Examples of alignment templates obtained in training.
ment within this bilingual phrase. Thus, we obtain a count N(z) of how often an
alignment template occurred in the aligned training corpus. The probability of using
an alignment template to translate a specific source language phrase f? is estimated by
means of relative frequency:
p(z = (FJ
?
1 , E
I?
1 , A?) | f? ) =
N(z) ? ?(FJ
?
1 , C(f? ))
N(C(f? ))
(10)
To reduce the memory requirement of the alignment templates, we compute these
probabilities only for phrases up to a certain maximal length in the source language.
427
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Depending on the size of the corpus, the maximal length in the experiments is be-
tween four and seven words. In addition, we remove alignment templates that have
a probability lower than a certain threshold. In the experiments, we use a threshold
of 0.01.
It should be emphasized that this algorithm for computing aligned phrase pairs
and their associated probabilities is very easy to implement. The joint translation model
suggested by Marcu and Wong (2002) tries to learn phrases as part of a full EM
algorithm, which leads to very large memory requirements and a rather complicated
training algorithm. A comparison of the two approaches can be found in Koehn, Och,
and Marcu (2003).
4. Translation Model
To describe our translation model based on the alignment templates described in the
previous section in a formal way, we first decompose both the source sentence f J1 and
the target sentence eI1 into a sequence of phrases (k = 1, . . . , K):
f J1 = f?
K
1 , f?k = fjk?1+1, . . . , fjk (11)
eI1 = e?
K
1 , e?k = eik?1+1, . . . , eik (12)
Note that there are a large number of possible segmentations of a sentence pair into K
phrase pairs. In the following, we will describe the model for a specific segmentation.
Eventually, however, a model can be described in which the specific segmentation is
not known when new text is translated. Hence, as part of the overall search process
(Section 5), we also search for the optimal segmentation.
To allow possible reordering of phrases, we introduce an alignment on the phrase
level ?K1 between the source phrases f?
K
1 and the target phrases e?
K
1 . Hence, ?
K
1 is a
permutation of the phrase positions 1, . . . , K and indicates that the phrases e?k and
f??k are translations of one another. We assume that for the translation between these
phrases a specific alignment template zk is used:
e?k
zk?? f??k
Hence, our model has the following hidden variables:
?K1 , z
K
1
Figure 5 gives an example of the word alignment and phrase alignment of a
German?English sentence pair.
We describe our model using a log-linear modeling approach. Hence, all knowl-
edge sources are described as feature functions that include the given source language
string f J1, the target language string e
I
1, and the above-stated hidden variables. Hence,
we have the following functional form of all feature functions:
h(eI1, f
J
1, ?
K
1 , z
K
1 )
Figure 6 gives an overview of the decisions made in the alignment template model.
First, the source sentence words f J1 are grouped into phrases f?
K
1 . For each phrase f? an
alignment template z is chosen and the sequence of chosen alignment templates is
reordered (according to ?K1 ). Then, every phrase f? produces its translation e? (using the
corresponding alignment template z). Finally, the sequence of phrases e?K1 constitutes
the sequence of words eI1.
428
Computational Linguistics Volume 30, Number 4
Figure 5
Example of segmentation of German sentence and its English translation into alignment
templates.
Figure 6
Dependencies in the alignment template model.
429
Och and Ney The Alignment Template Approach to Statistical Machine Translation
4.1 Feature Functions
4.1.1 Alignment Template Selection. To score the use of an alignment template, we
use the probability p(z | f? ) defined in Section 3. We establish a corresponding feature
function by multiplying the probability of all used alignment templates and taking the
logarithm:
hAT(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
K
?
k=1
p(zk | f
j?k
j?k?1+1
) (13)
Here, j?k?1 + 1 is the position of the first word of alignment template zk in the source
language sentence and j?k is the position of the last word of that alignment template.
Note that this feature function requires that a translation of a new sentence be
composed of a set of alignment templates that covers both the source sentence and
the produced translation. There is no notion of ?empty phrase? that corresponds to
the ?empty word? in word-based statistical alignment models. The alignment on the
phrase level is actually a permutation, and no insertions or deletions are allowed.
4.1.2 Word Selection. For scoring the use of target language words, we use a lexicon
probability p(e | f ), which is estimated using relative frequencies as described in Sec-
tion 3.2. The target word e depends on the aligned source words. If we denote the
resulting word alignment matrix by A := A?K1 ,zK1 and the predicted word class for word
ei by Ei, then the feature function hWRD is defined as follows:
hWRD(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
I
?
i=1
p(ei | {fj | (i, j) ? A}, Ei) (14)
For p(ei | {fj | (i, j) ? A}) we use a uniform mixture of a single-word model p(e | f ),
which is constrained to predict only words that are in the predicted word class Ei:
p(ei | {fj | (i, j) ? A}, Ei) =
?
{j|(i,j)?A} p(ei | fj)
|{j | (i, j) ? A}| ? ?(C(ei), Ei)
A disadvantage of this model is that the word order is ignored in the translation
model. The translations the day after tomorrow or after the day tomorrow for the German
word u?bermorgen receive an identical probability. Yet the first one should obtain a
significantly higher probability. Hence, we also include a dependence on the word
positions in the lexicon model p(e | f , i, j):
p(ei | fj,
i?1
?
i?=1
[(i?, j) ? A],
j?1
?
j?=1
[(i, j?) ? A]) (15)
Here, [(i?, j) ? A] is 1 if (i?, j) ? A and 0 otherwise. As a result, the word ei depends
not only on the aligned French word fj, but also on the number of preceding French
words aligned with ei and on the number of the preceding English words aligned with
fj. This model distinguishes the positions within a phrasal translation. The number of
parameters of p(e | f , i, j) is significantly higher than that of p(e | f ) alone. Hence,
there is a data estimation problem especially for words that rarely occur. Therefore,
we linearly interpolate the models p(e | f ) and p(e | f , i, j).
4.1.3 Phrase Alignment. The phrase alignment feature simply takes into account that
very often a monotone alignment is a correct alignment. Hence, the feature function
hAL measures the ?amount of nonmonotonicity? by summing over the distance (in the
430
Computational Linguistics Volume 30, Number 4
source language) of alignment templates that are consecutive in the target language:
hAL(e
I
1, f
J
1, ?
K
1 , z
K
1 ) =
K+1
?
k=1
|j?k?1 ? j?k?1 | (16)
Here, j?0 is defined to equal 0 and j?K+1?1 is defined to equal J. The above-stated sum
includes k = K + 1 to include the distance from the end position of the last phrase to
the end of sentence.
The sequence of K = 6 alignment templates in Figure 5 corresponds to the follow-
ing sum of seven jump distances: 0 + 0 + 1 + 3 + 2 + 0 + 0 = 6.
4.1.4 Language Model Features. As a default language model feature, we use a stan-
dard backing-off word-based trigram language model (Ney, Generet, and Wessel 1995):
hLM(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
I+1
?
i=1
p(ei | ei?2, ei?1) (17)
In addition, we use a 5-gram class-based language model:
hCLM(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = log
I+1
?
i=1
p(C(ei) | C(ei?4), . . . , C(ei?1)) (18)
The use of the language model feature in equation (18) helps take long-range depen-
dencies better into account.
4.1.5 Word Penalty. To improve the scoring for different target sentence lengths, we
also use as a feature the number of produced target language words (i.e., the length
of the produced target language sentence):
hWP(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = I (19)
Without this feature, we typically observe that the produced sentences tend to be too
short.
4.1.6 Conventional Lexicon. We also use a feature that counts how many entries of a
conventional lexicon co-occur in the given sentence pair. Therefore, the weight for the
provided conventional dictionary can be learned:
hLEX(e
I
1, f
J
1, ?
K
1 , z
K
1 ) = #CO-OCCURRENCES(LEX, e
I
1, f
J
1) (20)
The intuition is that the conventional dictionary LEX is more reliable than the auto-
matically trained lexicon and therefore should get a larger weight.
4.1.7 Additional Features. A major advantage of the log-linear modeling approach
used is that we can add numerous features that deal with specific problems of the
baseline statistical MT system. Here, we will restrict ourselves to the described set
of features. Yet we could use grammatical features that relate certain grammatical
dependencies of source and target language. For example, using a function k(?) that
counts how many arguments the main verb of a sentence has in the source or target
sentence, we can define the following feature, which has a nonzero value if the verb
in each of the two sentences has the same number of arguments:
h(f J1, e
I
1, ?
K
1 , z
K
1 ) = ?(k(f
J
1), k(e
I
1)) (21)
In the same way, we can introduce semantic features or pragmatic features such as
the dialogue act classification.
431
Och and Ney The Alignment Template Approach to Statistical Machine Translation
4.2 Training
For the three different tasks on which we report results, we use two different training
approaches. For the Verbmobil task, we train the model parameters ?M1 according
to the maximum class posterior probability criterion (equation (4)). For the French?
English Hansards task and the Chinese?English NIST task, we simply tune the model
parameters by coordinate descent on held-out data with respect to the automatic eval-
uation metric employed, using as a starting point the model parameters obtained on
the Verbmobil task. Note that this tuning depends on the starting point of the model
parameters and is not guaranteed to converge to the global optimum on the training
data. As a result, this approach is limited to a very small number of model parame-
ters. An efficient algorithm for performing this tuning for a larger number of model
parameters can be found in Och (2003).
A standard approach to training the log-linear model parameters of the maximum
class posterior probability criterion is the GIS (Generalized Iterative Scaling) algorithm
(Darroch and Ratcliff 1972). To apply this algorithm, we have to solve various practi-
cal problems. The renormalization needed in equation (3) requires a sum over many
possible sentences, for which we do not know of an efficient algorithm. Hence, we ap-
proximate this sum by extracting a large set of highly probable sentences as a sample
from the space of all possible sentences (n-best approximation). The set of considered
sentences is computed by means of an appropriately extended version of the search
algorithm described in Section 5.
Using an n-best approximation, we might face the problem that the parameters
trained with the GIS algorithm yield worse translation results even on the training
corpus. This can happen because with the modified model scaling factors, the n-best
list can change significantly and can include sentences that have not been taken into
account in training. Using these sentences, the new model parameters might perform
worse than the old model parameters. To avoid this problem, we proceed as follows.
In a first step, we perform a search, compute an n-best list, and use this n-best list to
train the model parameters. Second, we use the new model parameters in a new search
and compute a new n-best list, which is combined with the existing n-best list. Third,
using this extended n-best list, new model parameters are computed. This process is
iterated until the resulting n-best list does not change. In this algorithm, convergence
is guaranteed, as in the limit the n-best list will contain all possible translations. In
practice, the algorithm converges after five to seven iterations. In our experiments this
final n-best list contains about 500?1000 alternative translations.
We might have the problem that none of the given reference translations is part
of the n-best list because the n-best list is too small or because the search algorithm
performs pruning which in principle limits the possible translations that can be pro-
duced given a certain input sentence. To solve this problem, we define as reference
translation for maximum-entropy training each sentence that has the minimal number
of word errors with respect to any of the reference translations in the n-best list. More
details of the training procedure can be found in Och and Ney (2002).
5. Search
In this section, we describe an efficient search architecture for the alignment template
model.
5.1 General Concept
In general, the search problem for statistical MT even using only Model 1 of Brown
et al (1993) is NP-complete (Knight 1999). Therefore, we cannot expect to develop
432
Computational Linguistics Volume 30, Number 4
efficient search algorithms that are guaranteed to solve the problem without search
errors. Yet for practical applications it is acceptable to commit some search errors
(Section 6.1.2). Hence, the art of developing a search algorithm lies in finding suitable
approximations and heuristics that allow an efficient search without committing too
many search errors.
In the development of the search algorithm described in this section, our main
aim is that the search algorithm should be efficient. It should be possible to translate
a sentence of reasonable length within a few seconds of computing time. We accept
that the search algorithm sometimes results in search errors, as long as the impact
on translation quality is minor. Yet it should be possible to reduce the number of
search errors by increasing computing time. In the limit, it should be possible to
search without search errors. The search algorithm should not impose any principal
limitations. We also expect that the search algorithm be able to scale up to very long
sentences with an acceptable computing time.
To meet these aims, it is necessary to have a mechanism that restricts the search
effort. We accomplish such a restriction by searching in a breadth-first manner with
pruning: beam search. In pruning, we constrain the set of considered translation can-
didates (the ?beam?) only to the promising ones. We compare in beam search those
hypotheses that cover different parts of the input sentence. This makes the comparison
of the probabilities problematic. Therefore, we integrate an admissible estimation of
the remaining probabilities to arrive at a complete translation (Section 5.6)
Many of the other search approaches suggested in the literature do not meet the
described aims:
? Neither optimal A* search (Och, Ueffing, and Ney 2001) nor optimal
integer programming (Germann et al 2001) for statistical MT allows
efficient search for long sentences.
? Greedy search algorithms (Wang 1998; Germann et al 2001) typically
commit severe search errors (Germann et al 2001).
? Other approaches to solving the search problem obtain polynomial time
algorithms by assuming monotone alignments (Tillmann et al 1997) or
imposing a simplified recombination structure (Nie?en et al 1998).
Others make simplifying assumptions about the search space
(Garc??a-Varea, Casacuberta, and Ney 1998; Garc??a-Varea et al 2001), as
does the original IBM stack search decoder (Berger et al 1994). All these
simplifications ultimately make the search problem simpler but
introduce fundamental search errors.
In the following, we describe our search algorithm based on the concept of beam
search, which allows a trade-off between efficiency and quality by adjusting the size of
the beam. The search algorithm can be easily adapted to other phrase-based translation
models. For single-word-based search in MT, a similar algorithm has been described
in Tillmann and Ney (2003).
5.2 Search Problem
Putting everything together and performing search in maximum approximation, we
obtain the following decision rule:
e?I1 = argmax
eI1,?
K
1 ,z
K
1
{
M
?
m=1
?m ? hm(eI1, f
J
1, ?
K
1 , z
K
1 )
}
(22)
433
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Using the four feature functions AT, AL, WRD, and LM, we obtain the following
decision rule:3
e?I1 = argmax
eI1,?
K
1 ,z
K
1
{
(23)
I
?
i=1
(
?LM log p(ei | ei?2, ei?1) + ?WRD log p(ei | {fj | (i, j) ? A}, Ei)
)
(24)
+
K
?
k=1
(
?AT log p(zk | f
j?k
j?k?1+1
) + ?AL ? |j?k ? j?k?1+1|
)
(25)
+?AL ? (J ? j?K) + ?LM log p(EOS | eI?1, eI)
}
(26)
Here, we have grouped the contributions of the various feature functions into those for
each word (from LM and WRD, expression (24)), those for every alignment template
(from AT and AL, expression (25)), and those for the end of sentence (expression (26)),
which includes a term log p(EOS | eI?1, eI) for the end-of-sentence language model
probability.
To extend this decision rule for the word penalty (WP) feature function, we sim-
ply obtain an additional term ?WP for each word. The class-based 5-gram language
model (CLM) can be included like the trigram language model. Note that all these fea-
ture functions decompose nicely into contributions for each produced target language
word or for each covered source language word. This makes it possible to develop
an efficient dynamic programming search algorithm. Not all feature functions have
this nice property: For the conventional lexicon feature function (LEX), we obtain an
additional term in our decision rule which depends on the full sentence. Therefore,
this feature function will not be integrated in the dynamic programming search but
instead will be used to rerank the set of candidate translations produced by the search.
5.3 Structure of Search Space
We have to structure the search space in a suitable way to search efficiently. In our
search algorithm, we generate search hypotheses that correspond to prefixes of target
language sentences. Each hypothesis is the translation of a part of the source language
sentence. A hypothesis is extended by appending one target word. The set of all hy-
potheses can be structured as a graph with a source node representing the sentence
start, goal nodes representing complete translations, and intermediate nodes repre-
senting partial translations. There is a directed edge between hypotheses n1 and n2 if
the hypothesis n2 is obtained by appending one word to hypothesis n1. Each edge has
associated costs resulting from the contributions of all feature functions. Finally, our
search problem can be reformulated as finding the optimal path through this graph.
In the first step, we determine the set of all source phrases in f? for which an appli-
cable alignment template exists. Every possible application of an alignment template
z = (FJ
?
1 , E
I?
1 , A?) to a subsequence f
j+J??1
j of the source sentence is called an alignment
template instantiation Z = (z, j). Hence, the set of all alignment template instantiations
for the source sentence f J1 is
{
Z = (z, j) | z = (FJ
?
1 , E
I?
1 , A?) ? ?j : p(z | f
j+J??1
j ) > 0
}
(27)
3 Note that here some of the simplifying notation of Section 4 has been used.
434
Computational Linguistics Volume 30, Number 4
If the source sentence contains words that have not been seen in the training data, we
introduce a new alignment template that performs a one-to-one translation of each of
these words by itself.
In the second step, we determine a set of probable target language words for
each target word position in the alignment template instantiation. Only these words
are then hypothesized in the search. We call this selection of highly probable words
observation pruning (Tillmann and Ney 2000). As a criterion for a word e at position
i in the alignment template instantiation, we use
?(Ei, C(e)) ?
J?
?
j=0
A?(i, j)
?
i? A?(i
?, j)
? p(e | fj) (28)
In our experiments, we hypothesize only the five best-scoring words.
A decision is a triple d = (Z, e, l) consisting of an alignment template instantiation
Z, the generated word e, and the index l of the generated word in Z. A hypothesis n
corresponds to a valid sequence of decisions di1. The possible decisions are as follows:
1. Start a new alignment template: di = (Zi, ei, 1). In this case, the index
l = 1. This decision can be made only if the previous decision di?1
finished an alignment template and if the newly chosen alignment
template instantiation does not overlap with any previously chosen
alignment template instantiation. The resulting decision score
corresponds to the contribution of the LM and the WRD features
(expression (24)) for the produced word and the contribution of AL and
AT features (expression (25)) for the started alignment template.
2. Extend an alignment template: di = (Zi, ei, l). This decision can be made
only if the previous decision uses the same alignment template
instantiation and has as index l ? 1: di?1 = (Zi, ei?1, l ? 1). The resulting
decision score corresponds to the contribution of the LM and the WRD
features (expression (24)).
3. Finish the translation of a sentence: di = (EOS, EOS, 0). In this case, the
hypothesis is marked as a goal hypothesis. This decision is possible only
if the previous decision di?1 finished an alignment template and if the
alignment template instantiations completely cover the input sentence.
The resulting decision score corresponds to the contribution of
expression (26).
Any valid and complete sequence of decisions dI+11 uniquely corresponds to a certain
translation eI1, a segmentation into K phrases, a phrase alignment ?
K
1 , and a sequence
of alignment template instantiations zK1 . The sum of the decision scores is equal to the
corresponding score described in expressions (24)?(26).
A straightforward representation of all hypotheses would be the prefix tree of all
possible sequences of decisions. Obviously, there would be a large redundancy in this
search space representation, because there are many search nodes that are indistin-
guishable in the sense that the subtrees following these search nodes are identical. We
can recombine these identical search nodes; that is, we have to maintain only the most
probable hypothesis (Bellman 1957).
In general, the criterion for recombining a set of nodes is that the hypotheses can be
distinguished by neither language nor translation model. In performing recombination,
435
Och and Ney The Alignment Template Approach to Statistical Machine Translation
INPUT: implicitly defined search space (functions Recombine, Extend)
H = {initial-hypothesis}
WHILE H = ?
Hext := ?
FOR n ? H
IF hypothesis n is final
THEN Hfin := Hfin ? {n}
ELSE Hext := Hext ? Extend(n)
H := Recombine(Hext)
Q? = maxn?H Q(n)
H := {n ? H : Q(n) > log(tp) + Q?}
H := HistogramPruning(H, Np)
n? = argmax
n?Hfin
Q(n)
OUTPUT: n?
Figure 7
Algorithm for breadth-first search with pruning.
we obtain a search graph instead of a search tree. The exact criterion for performing
recombination for the alignment templates is described in Section 5.5.
5.4 Search Algorithm
Theoretically, we could use any graph search algorithm to search the optimal path in
the search space. We use a breadth-first search algorithm with pruning. This approach
offers very good possibilities for adjusting the trade-off between quality and efficiency.
In pruning, we always compare hypotheses that have produced the same number of
target words.
Figure 7 shows a structogram of the algorithm. As the search space increases expo-
nentially, it is not possible to explicitly represent it. Therefore, we represent the search
space implicitly, using the functions Extend and Recombine. The function Extend pro-
duces new hypotheses extending the current hypothesis by one word. Some hypothe-
ses might be identical or indistinguishable by the language and translation models.
These are recombined by the function Recombine. We expand the search space such
that only hypotheses with the same number of target language words are recombined.
In the pruning step, we use two different types of pruning. First, we perform prun-
ing relative to the score Q? of the current best hypothesis. We ignore all hypotheses that
have a probability lower than log(tp)+Q?, where tp is an adjustable pruning parameter.
This type of pruning can be performed when the hypothesis extensions are computed.
Second, in histogram pruning (Steinbiss, Tran, and Ney 1994), we maintain only the
best Np hypotheses. The two pruning parameters tp and Np have to be optimized with
respect to the trade-off between efficiency and quality.
5.5 Implementation
In this section, we describe various issues involved in performing an efficient imple-
mentation of a search algorithm for the alignment template approach.
A very important design decision in the implementation is the representation of
a hypothesis. Theoretically, it would be possible to represent search hypotheses only
by the associated decision and a back-pointer to the previous hypothesis. Yet this
would be a very inefficient representation for the implementation of the operations
436
Computational Linguistics Volume 30, Number 4
that have to be performed in the search. The hypothesis representation should contain
all information required to perform efficiently the computations needed in the search
but should contain no more information than that, to keep the memory consumption
small.
In search, we produce hypotheses n, each of which contains the following infor-
mation:
1. e: the final target word produced
2. h: the state of the language model (to predict the following word)
3. c = cJ1: the coverage vector representing the already covered positions of
the source sentence (cj = 1 means the position j is covered, cj = 0 means
the position j is not covered)
4. Z: a reference to the alignment template instantiation that produced the
final target word
5. l: the position of the final target word in the alignment template
instantiation
6. Q(n): the accumulated score of all previous decisions
7. n?: a reference to the previous hypothesis
Using this representation, we can perform the following operations very efficiently:
? Determining whether a specific alignment template instantiation can be
used to extend a hypothesis. To do this, we check whether the positions
of the alignment template instantiation are still free in the hypothesis
coverage vector.
? Checking whether a hypothesis is final. To do this, we determine
whether the coverage vector contains no uncovered position. Using a bit
vector as representation, the operation to check whether a hypothesis is
final can be implemented very efficiently.
? Checking whether two hypotheses can be recombined. The criterion for
recombining two hypotheses n1 = (e1, h1, c1, Z1, l1) and
n2 = (e2, h2, c2, Z2, l2) is
h1 = h2? identical language model state
c1 = c2? identical coverage vector
( (Z1 = Z2 ? l1 = l2)? alignment template instantiation is identical
(J(Z1) = l1 ? J(Z2) = l2) ) alignment template instantiation finished
We compare in beam search those hypotheses that cover different parts of the input
sentence. This makes the comparison of the probabilities problematic. Therefore, we
integrate an admissible estimation of the remaining probabilities to arrive at a complete
translation. Details of the heuristic function for the alignment templates are provided
in the next section.
5.6 Heuristic Function
To improve the comparability of search hypotheses, we introduce heuristic functions.
A heuristic function estimates the probabilities of reaching the goal node from a certain
437
Och and Ney The Alignment Template Approach to Statistical Machine Translation
search node. An admissible heuristic function is always an optimistic estimate; that
is, for each search node, the product of edge probabilities of reaching a goal node
is always equal to or smaller than the estimated probability. For an A*-based search
algorithm, a good heuristic function is crucial to being able to translate long sentences.
For a beam search algorithm, the heuristic function has a different motivation. It is used
to improve the scoring of search hypotheses. The goal is to make the probabilities of
all hypotheses more comparable, in order to minimize the chance that the hypothesis
leading to the optimal translation is pruned away.
Heuristic functions for search in statistical MT have been used in Wang and Waibel
(1997) and Och, Ueffing, and Ney (2001). Wang and Waibel (1997) have described a
simple heuristic function for Model 2 of Brown et al (1993) that was not admissible.
Och, Ueffing, and Ney (2001) have described an admissible heuristic function for
Model 4 of Brown et al (1993) and an almost-admissible heuristic function that is
empirically obtained.
We have to keep in mind that a heuristic function is helpful only if the overhead
introduced in computing the heuristic function is more than compensated for by the
gain obtained through a better pruning of search hypotheses. The heuristic functions
described in the following are designed such that their computation can be performed
efficiently.
The basic idea for developing a heuristic function for an alignment model is that all
source sentence positions that have not been covered so far still have to be translated
to complete the sentence. If we have an estimation rX(j) of the optimal score for
translating position j, then the value of the heuristic function RX(n) for a node n
can be inferred by summing over the contribution for every position j that is not in
the coverage vector c(n) (here X denotes different possibilities to choose the heuristic
function):
RX(n) =
?
j?c(n)
rX(j) (29)
The situation in the case of the alignment template approach is more complicated, as
not every word is translated alone, but typically the words are translated in context.
Therefore, the basic quantity for the heuristic function in the case of the alignment
template approach is a function r(Z) that assigns to every alignment template in-
stantiation Z a maximal probability. Using r(Z), we can induce a position-dependent
heuristic function r(j):
r(j) := max
Z:j(Z)?j?j(Z)+J(Z)?1
r(Z)/J(Z) (30)
Here, J(Z) denotes the number of source language words produced by the alignment
template instantiation Z and j(Z) denotes the position of the first source language
word. It can be easily shown that if r(Z) is admissible, then r(j) is also admissible. We
have to show that for all nonoverlapping sequences ZK1 the following holds:
K
?
k=1
r(Zk) ?
?
j?c(ZK1 )
r(j) (31)
Here, c(ZK1 ) denotes the set of all positions covered by the sequence of alignment
templates ZK1 . This can be shown easily:
K
?
k=1
r(Zk) =
K
?
k=1
J(Zk)
?
j=1
r(Zk)/J(Zk) (32)
438
Computational Linguistics Volume 30, Number 4
INPUT: coverage vector cJ1, previously covered position j
ff = min({j? | cj? = 0})
mj = |j ? ff|
WHILE ff = (J + 1)
fo := min({j? | j? > ff ? cj? = 1})
ff := min({j? | j? > fo ? cj? = 0 ? j? = J + 1})
mj := mj + |ff ? fo|
OUTPUT: mj
Figure 8
Algorithm min-jumps to compute the minimum number of needed jumps D(cJ1, j) to complete
the translation.
=
?
j?c(ZK1 )
r(Zk(j))/J(Zk(j)) (33)
?
?
j?c(ZK1 )
max
Z:j(Z)?j?j(Z)+J(Z)?1
r(Z)/J(Z) (34)
Here, k(j) denotes the phrase index k that includes the target language word position j.
In the following, we develop various heuristic functions r(Z) of increasing complexity.
The simplest realization of a heuristic function r(Z) takes into account only the prior
probability of an alignment template instantiation:
RAT(Z = (z, j)) = ?AT ? log p(z | fj,j+J(z)?1) (35)
The lexicon model can be integrated as follows:
RWRD(Z) = ?WRD ?
j(Z)+J(Z)?1
?
j?=j(Z)
max
e
log p(e | fj?) (36)
The language model can be incorporated by considering that for each target word
there exists an optimal language model probability:
pL(e) = max
e?,e??
p(e | e?, e??) (37)
Here, we assume a trigram language model. In general, it is necessary to maximize
over all possible different language model histories. We can also combine the language
model and the lexicon model into one heuristic function:
RWRD+LM(Z) =
j(Z)+J(Z)?1
?
j?=j(Z)
max
e
?WRD log(p(e | fj?)) + ?LM log(p
L(e)) (38)
To include the phrase alignment probability in the heuristic function, we compute
the minimum sum of all jump widths that is needed to complete the translation. This
sum can be computed efficiently using the algorithm shown in Figure 8. Then, an
admissible heuristic function for the jump width is obtained by
RAL(c, j) = ?AL ? D(c, j) (39)
439
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 2
Statistics for Verbmobil task: training corpus (Train), conventional dictionary (Lex),
development corpus (Dev), test corpus (Test) (Words*: words without punctuation marks).
No Preprocessing With Preprocessing
German English German English
Train Sentences 58,073
Words 519,523 549,921 522,933 548,874
Words* 418,974 453,612 420,919 450,297
Singletons 3,453 1,698 3,570 1,763
Vocabulary 7,940 4,673 8,102 4,780
Lex Entries 12,779
Extended vocabulary 11,501 6,867 11,904 7,089
Dev Sentences 276
Words 3,159 3,438 3,172 3,445
Trigram perplexity ? 28.1 ? 26.3
Test Sentences 251
Words 2,628 2,871 2,640 2,862
Trigram perplexity ? 30.5 ? 29.9
Combining all the heuristic functions for the various models, we obtain as final heuris-
tic function for a search hypothesis n
R(n) = RAL(c(n), j(n)) +
?
j?c(n)
(
RAT(j) + RWRD+LM(j)
)
(40)
6. Results
6.1 Results on the Verbmobil Task
We present results on the Verbmobil task, which is a speech translation task in the
domain of appointment scheduling, travel planning, and hotel reservation (Wahlster
2000). Table 2 shows the corpus statistics for this task. We use a training corpus,
which is used to train the alignment template model and the language models, a
development corpus, which is used to estimate the model scaling factors, and a test
corpus. On average, 3.32 reference translations for the development corpus and 5.14
reference translations for the test corpus are used.
A standard vocabulary had been defined for the various speech recognizers used
in Verbmobil. However, not all words of this vocabulary were observed in the train-
ing corpus. Therefore, the translation vocabulary was extended semiautomatically by
adding about 13,000 German?English entries from an online bilingual lexicon avail-
able on the Web. The resulting lexicon contained not only word-word entries, but
also multi-word translations, especially for the large number of German compound
words. To counteract the sparseness of the training data, a couple of straightforward
rule-based preprocessing steps were applied before any other type of processing:
? normalization of
? numbers
? time and date phrases
? spelling (e.g., don?t ? do not)
? splitting of German compound words.
440
Computational Linguistics Volume 30, Number 4
So far, in machine translation research there is no generally accepted criterion
for the evaluation of experimental results. Therefore, we use various criteria. In the
following experiments, we use:
? WER (word error rate)/mWER (multireference word error rate): The
WER is computed as the minimum number of substitution, insertion,
and deletion operations that have to be performed to convert the
generated sentence into the target sentence. In the case of the
multireference word error rate for each test sentence, not just a single
reference translation is used, as for the WER, but a whole set of reference
translations. For each translation hypothesis, the edit distance to the
most similar sentence is calculated (Nie?en et al 2000).
? PER (position-independent WER): A shortcoming of the WER is the fact
that it requires a perfect word order. An acceptable sentence can have a
word order that is different from that of the target sentence, so the WER
measure alone could be misleading. To overcome this problem, we
introduce as an additional measure the position-independent word error
rate. This measure compares the words in the two sentences, ignoring
the word order.
? BLEU (bilingual evalutation understudy) score: This score measures the
precision of unigrams, bigrams, trigrams, and 4-grams with respect to a
whole set of reference translations, with a penalty for too-short sentences
(Papineni et al 2001). Unlike all other evaluation criteria used here,
BLEU measures accuracy, that is, the opposite of error rate. Hence, the
larger BLEU scores, the better.
In the following, we analyze the effect of various system components: alignment tem-
plate length, search pruning, and language model n-gram size. A systematic evaluation
of the alignment template system comparing it with other translation approaches (e.g.,
rule-based) has been performed in the Verbmobil project and is described in Tessiore
and von Hahn (2000). There, the alignment-template-based system achieved a sig-
nificantly larger number of ?approximately correct? translations than the competing
translation systems (Ney, Och, and Vogel 2001).
6.1.1 Effect of Alignment Template Length. Table 3 shows the effect of constraining
the maximum length of the alignment templates in the source language. Typically, it
is necessary to restrict the alignment template length to keep memory requirements
low. We see that using alignment templates with only one or two words in the source
languages results in very bad translation quality. Yet using alignment templates with
lengths as small as three words yields optimal results.
6.1.2 Effect of Pruning and Heuristic Function. In the following, we analyze the effect
of beam search pruning and of the heuristic function. We use the following criteria:
? Number of search errors: A search error occurs when the search
algorithm misses the most probable translation and produces a
translation which is less probable. As we typically cannot efficiently
compute the probability of the optimal translation, we cannot efficiently
compute the number of search errors. Yet we can compute a lower
bound on the number of search errors by comparing the translation
441
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 3
Effect of alignment template length on translation quality.
AT length PER [%] mWER [%] BLEU [%]
1 29.8 39.9 44.6
2 27.0 33.0 53.6
3 26.5 30.7 56.1
4 26.9 31.4 55.7
5 26.8 31.4 55.7
6 26.5 30.9 56.0
7 26.5 30.9 56.1
Table 4
Effect of pruning parameter tp and heuristic function on search efficiency for direct-translation
model (Np = 50,000).
no heuristic function AT+WRD +LM +AL
time search time search time search time search
tp [s] errors [s] errors [s] errors [s] errors
10?2 0.0 194 0.0 174 0.0 150 0.0 91
10?4 0.2 97 0.2 57 0.3 40 0.2 13
10?6 2.0 61 2.8 21 4.1 11 1.8 3
10?8 11.9 41 15.0 7 19.9 5 9.5 1
10?10 45.6 38 50.9 6 65.2 3 32.0 1
10?12 114.6 34 119.2 5 146.2 2 75.2 0
found under specific pruning thresholds with the best translation that
we have found using very conservative pruning thresholds.
? Average translation time per sentence: Pruning is used to adjust the
trade-off between efficiency and quality. Hence, we present the average
time needed to translate one sentence of the test corpus.
? Translation quality (mWER, BLEU): Typically, a sentence can have many
different correct translations. Therefore, a search error does not
necessarily result in poorer translation quality. It is even possible that a
search error can improve translation quality. Hence, we analyze the effect
of search on translation quality, using the automatic evaluation criteria
mWER and BLEU.
Tables 4 and 5 show the effect of the pruning parameter tp with the histogram
pruning parameter Np = 50,000. Tables 6 and 7 show the effect of the pruning pa-
rameter Np with the pruning parameter tp = 10?12. In all four tables, we provide the
results for using no heuristic functions and three variants of an increasingly infor-
mative heuristic function. The first is an estimate of the alignment template and the
lexicon probability (AT+WRD), the second adds an estimate of the language model
(+LM) probability, and the third also adds the alignment probability (+AL). These
heuristic functions are described in Section 5.6.
Without a heuristic function, even more than a hundred seconds per sentence
cannot guarantee search-error-free translation. We draw the conclusion that a good
heuristic function is very important to obtaining an efficient search algorithm.
442
Computational Linguistics Volume 30, Number 4
Table 5
Effect of pruning parameter tp and heuristic function on error rate for direct-translation model
(Np = 50,000).
error rates [%]
no heuristic function AT+WRD +LM +AL
tp mWER BLEU mWER BLEU mWER BLEU mWER BLEU
10?2 48.9 46.8 44.3 49.4 40.9 51.3 33.6 53.4
10?4 39.8 50.9 35.0 53.8 32.3 55.0 30.7 55.9
10?6 37.1 51.3 31.8 55.0 30.9 55.6 30.8 56.0
10?8 35.7 53.0 31.4 55.7 31.2 55.7 30.9 56.0
10?10 36.1 52.9 31.3 55.8 31.0 55.9 30.8 56.0
10?12 35.7 52.9 31.2 55.9 31.0 55.9 30.8 56.0
Table 6
Effect of pruning parameter Np and heuristic function on search efficiency for
direct-translation model (tp = 10?12).
no heuristic function AT+WRD +LM +AL
time search time search time search time search
Np [s] errors [s] errors [s] errors [s] errors
1 0.0 237 0.0 238 0.0 238 0.0 232
10 0.0 169 0.0 154 0.0 148 0.0 98
100 0.3 101 0.3 69 0.3 60 0.2 21
1,000 2.2 65 2.3 33 2.4 27 2.0 5
10,000 18.3 40 18.3 10 21.1 5 14.3 1
50,000 114.6 34 119.2 5 146.2 2 75.2 0
Table 7
Effect of pruning parameter Np and heuristic function on error rate for direct-translation
model (tp = 10?12).
error rates [%]
no heuristic function AT+WRD +LM +AL
Np mWER BLEU mWER BLEU mWER BLEU mWER BLEU
1 64.4 29.7 61.9 31.7 59.8 32.4 49.4 38.2
10 46.6 46.9 43.0 49.2 42.0 49.1 34.6 52.3
100 41.0 49.8 36.7 52.6 34.8 53.8 31.3 55.6
1,000 37.8 51.5 33.0 54.5 32.3 55.3 30.6 56.0
10,000 35.5 53.1 31.4 55.6 30.9 55.6 30.8 56.0
50,000 35.7 52.9 31.2 55.9 31.0 55.9 30.8 56.0
443
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 8
Effect of the length of the language model history (Unigram/Bigram/Trigram: word-based;
CLM: class-based 5-gram).
Language model type PP PER [%] mWER [%] BLEU [%]
Zerogram 4781.0 38.1 45.9 29.0
Unigram 203.1 30.2 40.9 37.7
Bigram 38.3 26.9 32.9 53.0
Trigram 29.9 26.8 31.8 55.2
Trigram + CLM ? 26.5 30.9 56.1
In addition, the search errors have a more severe effect on the error rates if we
do not use a heuristic function. If we compare the error rates in Table 7, which corre-
spond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search
errors) using no heuristic function and an mWER of 32.6% (57 search errors) using
the combined heuristic function. The reason is that without a heuristic function, often
the ?easy? part of the input sentence is translated first. This yields severe reordering
errors.
6.1.3 Effect of the Length of the Language Model History. In this work, we use only
n-gram-based language models. Ideally, we would like to take into account long-range
dependencies. Yet long n-grams are seen rarely and are therefore rarely used on unseen
data. Therefore, we expect that extending the history length will at some point not
improve further translation quality.
Table 8 shows the effect of the length of the language model history on translation
quality. We see that the language model perplexity improves from 4,781 for a unigram
model to 29.9 for a trigram model. The corresponding translation quality improves
from an mWER of 45.9% to an mWER of 31.8%. The largest effect seems to come from
taking into account the bigram dependence, which achieves an mWER of 32.9%. If we
perform log-linear interpolation of a trigram model with a class-based 5-gram model,
we observe an additional small improvement in translation quality to an mWER of
30.9%.
6.2 Results on the Hansards task
The Hansards task involves the proceedings of the Canadian parliament, which are
kept by law in both French and English. About three million parallel sentences of this
bilingual data have been made available by the Linguistic Data Consortium (LDC).
Here, we use a subset of the data containing only sentences of up to 30 words. Table 9
shows the training and test corpus statistics.
The results for French to English and for English to French are shown in Table 10.
Because of memory limitations, the maximum alignment template length has been
restricted to four words. We compare here against the single-word-based search for
Model 4 described in Tillmann (2001). We see that the alignment template approach
obtains significantly better results than the single-word-based search.
6.3 Results on Chinese?English
Various statistical, example-based, and rule-based MT systems for a Chinese?English
news domain were evaluated in the NIST 2002 MT evaluation.4 Using the alignment
4 Evaluation home page: http://www.nist.gov/speech/tests/mt/mt2001/index.htm.
444
Computational Linguistics Volume 30, Number 4
Table 9
Corpus statistics for Hansards task (Words*: words without punctuation marks).
French English
Training Sentences 1,470,473
Words 24,338,195 22,163,092
Words* 22,175,069 20,063,378
Vocabulary 100,269 78,332
Singletons 40,199 31,319
Test Sentences 5,432
Words 97,646 88,773
Trigram perplexity ? 179.8
Table 10
Translation results on the Hansards task.
French?English English?French
Translation approach WER [%] PER [%] WER [%] PER [%]
Alignment templates 61.5 49.2 60.9 47.9
Single-word-based: monotone search 65.5 53.0 66.6 56.3
Single-word-based: reordering search 64.9 51.4 66.0 54.4
Table 11
Corpus statistics for Chinese?English corpora?large data track (Words*: words without
punctuation marks).
No preprocessing With preprocessing
Chinese English Chinese English
Train Sentences 1,645,631
Unique sentences 1,289,890
Words 31,175,023 33,044,374 30,849,149 32,511,418
Words* 27,091,283 29,212,384 26,828,721 28,806,735
Singletons 15,324 24,933 5,336 26,344
Vocabulary 67,103 92,488 45,111 85,116
Lex Entries 80,977
Extended vocabulary 76,182 100,704 54,190 93,350
Dev Sentences 993
Words 26,361 32,267 25,852 31,607
Trigram perplexity ? 237,154 ? 171,922
Test Sentences 878
Words 24,540 ? 24,144 ?
template approach described in this article, we participated in these evaluations. The
problem domain is the translation of Chinese news text into English. Table 11 gives
an overview on the training and test data. The English vocabulary consists of full-
form words that have been converted to lowercase letters. The number of sentences
has been artificially increased by adding certain parts of the original training material
more than once to the training corpus, in order to give larger weight to those parts
of the training corpus that consist of high-quality aligned Chinese news text and are
therefore expected to be especially helpful for the translation of the test data.
445
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Table 12
Results of Chinese?English NIST MT evaluation, June 2002, large data track (NIST-09 score:
larger values are better).
System NIST-09 score
Alignment template approach 7.65
Competing research systems 5.03?7.34
Best of six commercial off-the-shelf systems 6.08
The Chinese language poses special problems because the boundaries of Chinese
words are not marked. Chinese text is provided as a sequence of characters, and it is
unclear which characters have to be grouped together to obtain entities that can be
interpreted as words. For statistical MT, it would be possible to ignore this fact and
treat the Chinese characters as elementary units and translate them into English. Yet
preliminary experiments showed that the existing alignment models produce better
results if the Chinese characters are segmented in a preprocessing step into single
words. We use the LDC segmentation tool.5
For the English corpus, the following preprocessing steps are applied. First, the
corpus is tokenized; it is then segmented into sentences, and all uppercase characters
are converted to lowercase. As the final evaluation criterion does not distinguish case,
it is not necessary to deal with the case information.
Then, the preprocessed Chinese and English corpora are sentence aligned in which
the lengths of the source and target sentences are significantly different. From the
resulting corpus, we automatically replace translations. In addition, only sentences
with less than 60 words in English and Chinese are used.
To improve the translation of Chinese numbers, we use a categorization of Chi-
nese number and date expressions. For the statistical learning, all number and date
expressions are replaced with one of two generic symbols, $number or $date. The num-
ber and date expressions are subjected to a rule-based translation by simple lexicon
lookup. The translation of the number and date expressions is inserted into the out-
put using the alignment information. For Chinese and English, this categorization is
implemented independently of the other language.
To evaluate MT quality on this task, NIST made available the NIST-09 evaluation
tool. This tool provides a modified BLEU score by computing a weighted precision of
n-grams modified by a length penalty for very short translations. Table 12 shows the
results of the official evaluation performed by NIST in June 2002. With a score of 7.65,
the results obtained were statistically significantly better than any other competing
approach. Differences in the NIST score larger than 0.12 are statistically significant at
the 95% level. We conclude that the developed alignment template approach is also
applicable to unrelated language pairs such as Chinese?English and that the developed
statistical models indeed seem to be largely language-independent. Table 13 shows
various example translations.
7. Conclusions
We have presented a framework for statistical MT for natural languages which is
more general than the widely used source?channel approach. It allows a baseline MT
5 The LDC segmentation tool is available at
http://morph.ldc.upenn.edu/Projects/Chinese/LDC ch.htm#cseg.
446
Computational Linguistics Volume 30, Number 4
Table 13
Example translations for Chinese?English MT.
Reference Significant Accomplishment Achieved in the Economic Construction of
the Fourteen Open Border Cities in China
Translation The opening up of the economy of China?s fourteen City made
significant achievements in construction
Reference Xinhua News Agency, Beijing, Feb. 12?Exciting accomplishment has
been achieved in 1995 in the economic construction of China?s fourteen
border cities open to foreigners.
Translation Xinhua News Agency, Beijing, February 12?China?s opening up to the
outside world of the 1995 in the fourteen border pleased
to obtain the construction of the economy.
Reference Foreign Investment in Jiangsu?s Agriculture on the Increase
Translation To increase the operation of foreign investment in Jiangsu agriculture
Reference According to the data provided today by the Ministry of Foreign Trade
and Economic Cooperation, as of November this year, China has
actually utilized 46.959 billion US dollars of foreign capital, including
40.007 billion US dollars of direct investment from foreign businessmen.
Translation The external economic and trade cooperation Department today
provided that this year, the foreign capital actually utilized by China on
November to US $46.959 billion, including of foreign company direct
investment was US $40.007 billion.
Reference According to officials from the Provincial Department of Agriculture
and Forestry of Jiangsu, the ?Three-Capital? ventures approved by
agencies within the agricultural system of Jiangsu Province since 1994
have numbered more than 500 and have utilized over 700 million US
dollars worth of foreign capital, respectively three times and seven
times more than in 1993.
Translation Jiangsu Province for the Secretaries said that, from the 1994 years,
Jiangsu Province system the approval of the ?three-funded?
enterprises, there are more than 500, foreign investment utilization
rate of more than US $700 million, 1993 years before three and seven.
Reference The actual amount of foreign capital has also increased more than 30%
as compared with the same period last year.
Translation The actual amount of foreign investment has increased by more
than 30% compared with the same period last year.
Reference Import and Export in Pudong New District Exceeding 9 billion US
dollars This Year
Translation Foreign trade imports and exports of this year to the Pudong
new Region exceeds US $9 billion
system to be extended easily by adding new feature functions. We have described
the alignment template approach for statistical machine translation, which uses two
different alignment levels: a phrase-level alignment between phrases and a word-
level alignment between single words. As a result the context of words has a greater
influence, and the changes in word order from source to target language can be learned
explicitly. An advantage of this method is that machine translation is learned fully
automatically through the use of a bilingual training corpus. We have shown that the
presented approach is capable of achieving better translation results on various tasks
compared to other statistical, example-based, or rule-based translation systems. This
is especially interesting, as our system is structured simpler than many competing
systems.
447
Och and Ney The Alignment Template Approach to Statistical Machine Translation
We expect that better translation can be achieved by using models that go beyond
the flat phrase segmentation that we perform in our model. A promising avenue is to
gradually extend the model to take into account to some extent the recursive structure
of natural languages using ideas from Wu and Wong (1998) or Alshawi, Bangalore, and
Douglas (2000). We expect other improvements as well from learning nonconsecutive
phrases in source or target language and from better generalization methods for the
learned-phrase pairs.
Acknowledgments
The work reported here was carried out
while the first author was with the
Lehrstuhl fu?r Informatik VI, Computer
Science Department, RWTH
Aachen?University of Technology.
References
Alshawi, Hiyan, Srinivas Bangalore, and
Shona Douglas. 2000. Learning
dependency translation models as
collections of finite state head transducers.
Computational Linguistics, 26(1):45?60.
Bellman, Richard. 1957. Dynamic
Programming. Princeton University Press,
Princeton.
Berger, Adam L., Peter F. Brown, Stephen A.
Della Pietra, Vincent J. Della Pietra,
John R. Gillett, John D. Lafferty, Harry
Printz, and Lubos Ures?. 1994. The
Candide system for machine translation.
In Proceedings of the ARPA Workshop on
Human Language Technology, pages
157?162, Plainsboro, NJ, March.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?72.
Brown, Peter F., J. Cocke, Stephen A. Della
Pietra, Vincent J. Della Pietra, Frederick
Jelinek, John D. Lafferty, Robert L. Mercer,
and Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and R. L. Mercer.
1993. The mathematics of statistical
machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Charniak, Eugene, Kevin Knight, and Kenji
Yamada. 2003. Syntax-based language
models for machine translation. In MT
Summit IX, pages 40?46, New Orleans,
September.
Darroch, J. N. and D. Ratcliff. 1972.
Generalized iterative scaling for log-linear
models. Annals of Mathematical Statistics,
43:1470?1480.
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society,
Series B, 39(1):1?22.
Garc??a-Varea, Ismael, Francisco Casacuberta,
and Hermann Ney. 1998. An iterative,
DP-based search algorithm for statistical
machine translation. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP?98), pages 1235?1238,
Sydney, November.
Garc??a-Varea, Ismael, Franz Josef Och,
Hermann Ney, and Francisco
Casacuberta. 2001. Refined lexicon models
for statistical machine translation using a
maximum entropy approach. In
Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 204?211, Toulouse, France,
July.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics (ACL), pages
228?235, Toulouse, France, July.
Gildea, Daniel. 2003. Loosely tree-based
alignment for machine translation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 80?87, Sapporo, Japan, July.
Knight, Kevin. 1999. Decoding complexity
in word-replacement translation models.
Computational Linguistics, 25(4):607?615.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the Human
Language Technology and North American
Association for Computational Linguistics
Conference (HLT/NAACL), pages 127?133,
Edmonton, Alberta.
Marcu, Daniel and William Wong. 2002. A
phrase-based, joint probability model for
statistical machine translation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP-2002), pages 133?139,
Philadelphia, July.
448
Computational Linguistics Volume 30, Number 4
Ney, Hermann. 1995. On the
probabilistic-interpretation of
neural-network classifiers and
discriminative training criteria. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 17(2):107?119.
Ney, Hermann, Margit Generet, and Frank
Wessel. 1995. Extensions of absolute
discounting for language modeling. In
Proceedings of the Fourth European Conference
on Speech Communication and Technology,
pages 1245?1248, Madrid, September.
Ney, Hermann, Franz Josef Och, and
Stephan Vogel. 2001. The RWTH system
for statistical translation of spoken
dialogues. In Proceedings of the ARPA
Workshop on Human Language Technology,
San Diego, March.
Nie?en, Sonja, Franz Josef Och, Gregor
Leusch, and Hermann Ney. 2000. An
evaluation tool for machine translation:
Fast evaluation for machine translation
research. In Proceedings of the Second
International Conference on Language
Resources and Evaluation (LREC), pages
39?45, Athens, May.
Nie?en, Sonja, Stephan Vogel, Hermann
Ney, and Christoph Tillmann. 1998. A
DP-based search algorithm for statistical
machine translation. In COLING-ACL ?98:
36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 960?967, Montreal,
August.
Och, Franz Josef. 1999. An efficient method
for determining bilingual word classes. In
EACL ?99: Ninth Conference of the European
Chapter of the Association for Computational
Linguistics, pages 71?76, Bergen, Norway,
June.
Och, Franz Josef. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan,
July.
Och, Franz Josef and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL), pages
295?302, Philadelphia, July.
Och, Franz Josef and Hermann Ney. 2003. A
systematic comparison of various
statistical alignment models.
Computational Linguistics, 29(1):19?51.
Och, Franz Josef, Nicola Ueffing, and
Hermann Ney. 2001. An efficient A*
search algorithm for statistical machine
translation. In Data-Driven Machine
Translation Workshop, pages 55?62,
Toulouse, France, July.
Papineni, Kishore A., Salim Roukos, and
R. Todd Ward. 1997. Feature-based
language understanding. In European
Conference on Speech Communication and
Technology, pages 1435?1438, Rhodes,
Greece, September.
Papineni, Kishore A., Salim Roukos, and
R. Todd Ward. 1998. Maximum likelihood
and discriminative training of direct
translation models. In Proceedings of the
International Conference on Acoustics, Speech,
and Signal Processing, pages 189?192,
Seattle, May.
Papineni, Kishore A., Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. Bleu:
A method for automatic evaluation of
machine translation. Technical Report
RC22176 (W0109-022), IBM Research
Division, Thomas J. Watson Research
Center, Yorktown Heights, NY.
Steinbiss, Volker, Bach-Hiep Tran, and
Hermann Ney. 1994. Improvements in
beam search. In Proceedings of the
International Conference on Spoken Language
Processing (ICSLP?94), pages 2143?2146,
Yokohama, Japan, September.
Tessiore, Lorenzo and Walther von Hahn.
2000. Functional validation of a machine
interpretation system: Verbmobil. In
Wolfgang Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translations,
pages 611?631. Springer, Berlin.
Tillmann, Christoph. 2001. Word Re-ordering
and Dynamic Programming Based Search
Algorithms for Statistical Machine
Translation. Ph.D. thesis, Computer
Science Department, RWTH Aachen,
Germany.
Tillmann, Christoph. 2003. A projection
extension algorithm for statistical
machine translation. In Michael Collins
and Mark Steedman, editors, Proceedings
of the 2003 Conference on Empirical Methods
in Natural Language Processing, pages 1?8,
Sapporo, Japan.
Tillmann, Christoph and Hermann Ney.
2000. Word re-ordering and DP-based
search in statistical machine translation.
In COLING ?00: The 18th International
Conference on Computational Linguistics,
pages 850?856, Saarbru?cken, Germany,
July.
Tillmann, Christoph and Hermann Ney.
2003. Word reordering and a dynamic
programming beam search algorithm for
statistical machine translation.
Computational Linguistics, 29(1):97?133.
449
Och and Ney The Alignment Template Approach to Statistical Machine Translation
Tillmann, Christoph, Stephan Vogel,
Hermann Ney, and Alex Zubiaga. 1997. A
DP-based search using monotone
alignments in statistical translation. In
Proceedings of the 35th Annual Conference of
the Association for Computational Linguistics,
pages 289?296, Madrid, July.
Venugopal, Ashish, Stephan Vogel, and
Alex Waibel. 2003. Effective phrase
translation extraction from alignment
models. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 319?326, Sapporo,
Japan, July.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. HMM-based
word alignment in statistical translation.
In COLING ?96: The 16th International
Conference on Computational Linguistics,
pages 836?841, Copenhagen, August.
Wahlster, Wolfgang, editor. 2000. Verbmobil:
Foundations of Speech-to-Speech Translations.
Springer, Berlin.
Wang, Ye-Yi. 1998. Grammar Inference and
Statistical Machine Translation. Ph.D. thesis,
School of Computer Science, Language
Technologies Institute, Carnegie Mellon
University, Pittsburgh.
Wang, Ye-Yi and Alex Waibel. 1997.
Decoding algorithm in statistical
translation. In Proceedings of the 35th
Annual Conference of the Association for
Computational Linguistics, pages 366?372,
Madrid, July.
Wu, Dekai and William Wong. 1998.
Machine translation with a stochastic
grammatical channel. In COLING-ACL
?98: 36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics, pages 1408?1414, Montreal,
August.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 523?530, Toulouse, France,
July.
Statistical Approaches to
Computer-Assisted Translation
Sergio Barrachina?
Universitat Jaume I
Oliver Bender??
RWTH Aachen
Francisco Casacuberta?
Universitat Polite`cnica de Vale`ncia
Jorge Civera?
Universitat Polite`cnica de Vale`ncia
Elsa Cubel?
Universitat Polite`cnica de Vale`ncia
Shahram Khadivi??
RWTH Aachen
Antonio Lagarda?
Universitat Polite`cnica de Vale`ncia
Hermann Ney??
RWTH Aachen
Jesu?s Toma?s?
Universitat Polite`cnica de Vale`ncia
Enrique Vidal?
Universitat Polite`cnica de Vale`ncia
Juan-Miguel Vilar?
Universitat Jaume I
Current machine translation (MT) systems are still not perfect. In practice, the output
from these systems needs to be edited to correct errors. A way of increasing the productivity of
the whole translation process (MT plus human work) is to incorporate the human correction
activities within the translation process itself, thereby shifting the MT paradigm to that of
computer-assisted translation. This model entails an iterative process in which the human
translator activity is included in the loop: In each iteration, a prefix of the translation is validated
(accepted or amended) by the human and the system computes its best (or n-best) translation
suffix hypothesis to complete this prefix. A successful framework for MT is the so-called statis-
tical (or pattern recognition) framework. Interestingly, within this framework, the adaptation
of MT systems to the interactive scenario affects mainly the search process, allowing a great
reuse of successful techniques and models. In this article, alignment templates, phrase-based
models, and stochastic finite-state transducers are used to develop computer-assisted translation
systems. These systems were assessed in a European project (TransType2) in two real tasks: The
translation of printer manuals; manuals and the translation of the Bulletin of the European
Union. In each task, the following three pairs of languages were involved (in both translation
directions): English?Spanish, English?German, and English?French.
? Departament d?Enginyeria i Cie`ncies dels Computadors, Universitat Jaume I, 12071 Castello? de la Plana,
Spain.
?? Lehrstuhl fu?r Informatik VI, RWTH Aachen University of Technology, D-52056 Aachen, Germany.
? Institut Tecnolo`gic d?Informa`tica, Departament de Sistemes Informa`tics i Computacio?, Universitat
Polite`cnica de Vale`ncia, 46071 Vale`ncia, Spain.
? Institut Tecnolo`gic d?Informa`tica, Departament de Comunicacions, Universitat Polite`cnica de Vale`ncia,
46071 Vale`ncia, Spain.
? Departament de Llenguatges i Sistemes Informa`tics, Universitat Jaume I, 12071 Castello? de la Plana,
Spain.
Submission received: 1 June 2006; revised submission received: 20 September 2007; accepted for publication:
19 December 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
1. Introduction to Computer-Assisted Translation
Research in the field of machine translation (MT) aims to develop computer systems
which are able to translate text or speech without human intervention. However,
present translation technology has not been able to deliver fully automated high-quality
translations. Typical solutions to improving the quality of the translations supplied by
an MT system require manual post-editing. This serial process prevents the MT system
from taking advantage of the knowledge of the human translator, and the human
translator cannot take advantage of the adaptive ability of the MT system.
An alternative way to take advantage of the existing MT technologies is to use
them in collaboration with human translators within a computer-assisted translation
(CAT) or interactive framework (Isabelle and Church 1997). Historically, CAT and MT
have been considered different but close technologies (Kay 1997) and more so for one
of the most popular CAT technologies, namely, translation memories (Bowker 2002;
Somers 2003). Interactivity in CAT has been explored for a long time. Systems have
been designed to interact with human translators in order to solve different types
of (lexical, syntactic, or semantic) ambiguities (Slocum 1985; Whitelock et al 1986).
Other interaction strategies have been considered for updating user dictionaries or for
searching through dictionaries (Slocum 1985; Whitelock et al 1986). Specific proposals
can be found in Tomita (1985), Zajac (1988), Yamron et al (1993), and Sen, Zhaoxiong,
and Heyan (1997), among others.
An important contribution to CAT technology, carried out within the TransType
project, is worth mentioning (Foster, Isabelle, and Plamondon 1997; Langlais, Foster,
and Lapalme 2000; Foster 2002; Langlais, Lapalme, and Loranger 2002). It entailed an
interesting focus shift in which interaction is directly aimed at the production of the
target text, rather than at the disambiguation of the source text, as in earlier interactive
systems. The idea proposed in that work was to embed data-driven MT techniques
within the interactive translation environment. The hope was to combine the best of
both paradigms: CAT, in which the human translator ensures high-quality output, and
MT, in which the machine ensures a significant gain in productivity.
Following these TransType ideas, the innovative embedding proposed here con-
sists in using a complete MT system to produce full target sentence hypotheses, or
portions thereof, which can be accepted or amended by a human translator. Each cor-
rect text segment is then used by the MT system as additional information to achieve
further, hopefully improved, suggestions. More specifically, in each iteration, a prefix
of the target sentence is somehow fixed by the human translator and, in the next itera-
tion, the system predicts a best (or n-best) translation suffix(es)1 to complete this prefix.
We will refer to this process as interactive-predictive machine translation (IPMT).
This approach introduces two important requirements: First, the models have to
provide adequate completions and, second, this has to happen efficiently. Taking these
requirements into account, stochastic finite-state transducers (SFSTs), alignment tem-
plates (ATs), and phrase-based models (PBMs) are compared in this work. In previous
works these models have proven adequate for conventional MT (Vidal 1997; Amengual
et al 2000; Ney et al 2000; Toma?s and Casacuberta 2001; Och and Ney 2003; Casacuberta
and Vidal 2004; Och and Ney 2004; Vidal and Casacuberta 2004). This article shows that
1 The terms prefix and suffix are used here to denote any substring at the beginning and end (respectively)
of a string of characters (including spaces and punctuation), with no implication of morphological
significance as is usually implied by these terms in linguistics.
4
Barrachina et al Statistical Computer-Assisted Translation
existing efficient searching algorithms can be adapted in order to provide completions
(rather than full translations) also in a very efficient way.
The work presented here has been carried out in the TransType2 (TT2) project
(SchlumbergerSema S.A. et al 2001), which is considered as a follow-up to the inter-
active MT concepts introduced in the precursory TransType project cited previously.
We should emphasize the novel contributions of the present work with respect
to TransType. First, we show how fully fledged statistical MT (SMT) systems can be
extended to handle IPMT. In particular, the TT2 systems always produce complete
sentence hypotheses on which the human translator can work. This is an important
difference to previous work, in which the use of basic MT techniques only allowed the
prediction of single tokens (c.f., Section 2.2). Second, using fully fledged SMT systems,
we have performed systematic offline experiments to simulate the specific conditions of
interactive translation and we report and study the results of these experiments. Thirdly,
the IPMT systems presented in this article were successfully used in several field trials
with professional translators (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).
We should finally mention that the work developed in TT2 has gone beyond con-
ventional keyboard-and-mouse interaction, leading to the development of advanced
multi-modal interfaces. Speech is the most natural form of human communication and
its use as feedback in the IPMT framework has been explored by Vidal et al (2006).
On the other hand, human translators can be faster dictating the translation text rather
than typing it, thus it has also been investigated how to improve system performance
and usability when the user dictates the translation first and then edits the recognized
text (Khadivi, Zolnay, and Ney 2005; Khadivi, Zens, and Ney 2006).
The rest of the article is structured as follows. The next section introduces the
general setting for SMT and IPMT. In Section 3, AT, PBM, and SFST are briefly surveyed
along with the corresponding learning procedures. In Section 4, general search proce-
dures for the previous models are outlined and a detailed description of the extension
of these procedures to IPMT scenarios is presented. Section 5 is devoted to introducing
the tasks used for the assessment of the proposal presented in the previous sections:
the pairs of languages, corpora, and assessment procedures. The results are reported in
Section 6. A discussion of these results and the conclusions which can be drawn from
this work are presented in the final section.
2. Statistical Framework
The statistical or pattern recognition framework constitutes a very successful frame-
work for MT. As we will see here, this framework also proves adequate for IPMT.
2.1 Statistical Machine Translation
Assuming that we are given a sentence s in a source language, the text-to-text translation
problem can be stated as finding its translation t in a target language. Using statistical
decision theory, the best translation is given by the equation2
t? = argmax
t
Pr(t|s) (1)
2 We follow the common notation of Pr(x) for Pr(X = x) and Pr(x|y) for Pr(X = x|Y = y), for any random
variables X and Y. Similarly, Pr() will be used to denote ?true? probability functions, and p() or q() will
denote model approximations.
5
Computational Linguistics Volume 35, Number 1
Using Bayes?s Theorem, we arrive at
t? = argmax
t
Pr(t) ? Pr(s|t) (2)
This equation is generally interpreted as follows. The best translation must be a correct
sentence in the target language that conveys the meaning of the source sentence. The
probability Pr(t) represents the well-formedness of t and it is generally called the
language model probability (n-gram models are usually adopted [Jelinek 1998]). On
the other hand, Pr(s|t) represents the relationship between the two sentences (the source
and its translation). It should be of a high value if the source is a good translation of
the target and of a low value otherwise. Note that the translation direction is inverted
from what would be normally expected; correspondingly the models built around this
equation are often called inverted translation models (Brown et al 1990, 1993). As we
will see in Section 3, these models are based on the notion of alignment. It is interesting to
note that if we had perfect models, the use of Equation (1) would suffice. Given that we
have only approximations, the use of Equation (2) allows the language model to correct
deficiencies in the translation model.
In practice all of these models (and possibly others) are often combined into a log-
linear model for Pr(t | s) (Och and Ney 2004):
t? = argmax
t
{
N
?
i=1
?i ? log fi(t, s)
}
(3)
where fi(t, s) can be a model for Pr(s|t), a model for Pr(t|s), a target language model
for Pr(t), or any model that represents an important feature for the translation. N is the
number of models (or features) and ?i are the weights of the log-linear combination.
When using SFSTs, a different transformation can be used. These transducers
have an implicit target language model (which can be obtained from the finite-state
transducer by dropping the source symbols of each transition (Vidal et al 2005)). There-
fore, this separation is no longer needed. SFSTs model joint probability distributions;
therefore, Equation (1) has to be rewritten as
t? = argmax
t
Pr(s, t) (4)
This is the approach followed in GIATI (Casacuberta et al 2004a; Casacuberta and Vidal
2004), but other models for the joint probability can be adopted.
If the input is a spoken sentence, instead of a written one, the problem becomes
more complex; we will not deal with this here. The interested reader may consult
Amengual et al (2000), Ney et al (2000), or Casacuberta et al (2004a, 2004b), for
instance.
2.2 Statistical Interactive-Predictive Machine Translation
Unfortunately, current models and therefore the systems which can be built from them
are still far from perfect. This implies that, in order to achieve good, or even acceptable,
translations, manual post-editing is needed. An alternative to this serial approach (first
MT, then manual correction) is given by the IPMT paradigm. Under this paradigm,
translation is considered as an iterative process where human and computer activity
6
Barrachina et al Statistical Computer-Assisted Translation
Figure 1
Typical example of IPMT with keyboard interaction. The aim is to translate the English sentence
Click OK to close the print dialog into Spanish. Each step starts with a previously fixed target
language prefix tp, from which the system suggests a suffix t?s. Then the user accepts a part of this
suffix (a) and types some keystrokes (k), possibly in order to amend the remaining part of ts.
This produces a new prefix, composed by the prefix from the previous iteration and the accepted
and typed text, (a) (k), to be used as tp in the next step. The process ends when the user enters
the special keystroke ?#?. System suggestions are printed in italics and user input in boldface
typewriter font. In the final translation t, text that has been typed by the user is underlined.
are interwoven. This way, the models take into account both the input sentence and the
corrections of the user.
As previously mentioned, this idea was originally proposed in the TransType
project (Foster, Isabelle, and Plamondon 1997; Langlais, Foster, and Lapalme 2000;
Langlais, Lapalme, and Loranger 2002). In that project, the parts proposed by the sys-
tems were produced using a linear combination of a target language model (trigrams)
and a lexicon model (so-called IBM-1 or -2) (Langlais, Lapalme, and Loranger 2002). As
a result, TransType allowed only single-token completions, where a token could be either
a word or a short sequence of words from a predefined set of sequences. This proposal
was extended to complete full target sentences in the TT2 project, as discussed hereafter.
The approach taken in TT2 is exemplified in Figure 1. Initially, the system provides
a possible translation. From this translation, the user marks a prefix as correct and
provides, as a hint, the beginning of the rest of the translation. Depending on the system
or the user preferences, the hint can be the next word or some letters from it (in the
figure, hints are assumed to be words and are referred to as k). Let us use tp for the prefix
validated by the user together with the hint. The system now has to produce (predict)
a suffix ts to complete the translation. The cycle continues with a new validation and
hint from the user until the translation is completed. This justifies our choice of the term
?interactive-predictive machine translation? for this approach.
The crucial step of the process is the production of the suffix. Again, decision theory
tells us to maximize the probability of the suffix given the available information. That
is, the best suffix will be
t?s = argmax
ts
Pr(ts|s, tp) (5)
which can be straightforwardly rewritten as
t?s = argmax
ts
Pr(tp, ts|s) (6)
7
Computational Linguistics Volume 35, Number 1
Note that, because tpts = t, this equation is very similar to Equation (1). The main
difference is that the argmax search now is performed over the set of suffixes ts that
complete tp instead of complete sentences (t in Equation (1)). This implies that we can
use the same models if the search procedures are adequately modified (Och, Zens, and
Ney 2003).
The situation with respect to finite-state models is similar. Now, Equation (5) is
rewritten as
t?s = argmax
ts
Pr(tp, ts, s) (7)
which allows the use of the same models as in Equation (4) as long as the search
procedure is changed appropriately (Cubel et al 2003, 2004; Civera et al 2004a,
2004b).
3. Statistical and Finite-State Models
The models used are presented in the following subsections: Section 3.1 for the condi-
tional distribution Pr(s|t) in Equation (2) and Section 3.2 for the joint distribution Pr(s, t)
in Equation (4).
3.1 Statistical Alignment Models
The translation models which Brown et al (1993) introduced to deal with Pr(s|t) in
Equation (2) are based on the concept of alignment between the components of a pair
(s, t) (thus they are called statistical alignment models). Formally, if the number of
the source words in s is J and the number of target words in t is I, an alignment is a
function a : {1, ..., J} ? {0, ..., I}. The image of j by a will be denoted as aj, in which the
particular case aj = 0 means that the position j in s is not aligned with any position of t.
By introducing the alignment as a hidden variable in Pr(s|t),
Pr(s|t) =
?
a
Pr(s, a|t) (8)
The alignment that maximizes Pr(s, a|t) is shown to be very useful in practice for
training and for searching.
Different approaches have been proposed for modeling Pr(s, a|t) in Equation (8):
Zero-order models such as model 1, model 2, and model 3 (Brown et al 1993) and the first-
order models such as model 4, model 5 (Brown et al 1993), hidden Markov model (Ney
et al 2000), and model 6 (Och and Ney 2003).
In all these models, single words are taken into account. Moreover, in practice the
summation operator is replaced with the maximization operator, which in turn reduces
the contribution of each individual source word in generating a target word. On the
other hand, modeling word sequences rather than single words in both the alignment
and lexicon models cause significant improvement in translation quality (Och and Ney
8
Barrachina et al Statistical Computer-Assisted Translation
2004). In this work, we use two closely related models: ATs (Och and Ney 2004) and
PBMs (Toma?s and Casacuberta 2001; Koehn, Och, and Marcu 2003; Zens and Ney 2004).
Both models are based on bilingual phrases3 (pairs of segments or word sequences)
in which all words within the source-language phrase are aligned only to words of
the target-language phrase and vice versa. Note that at least one word in the source-
language phrase must be aligned to one word of the target-language phrase, that is,
there are no empty phrases similar to the empty word of the word-based models. In
addition, no gaps and no overlaps between phrases are allowed.
We introduce some notation to deal with phrases. As before, s denotes a source-
language sentence; ?s denotes a generic phrase in s, and ?sk the kth phrase in s. sj denotes
the jth source word in s; s
j?
j denotes the contiguous sequence of words in s beginning
at position j and ending at position j? (inclusive); obviously, if s has J words, s
J
1 denotes
the whole sentence s. An analogous notation is used for target words, phrases, and
sequences in target sentence t.
3.1.1 Alignment Templates. The ATs are based on the bilingual phrases but they are
generalized by replacing words with word classes and by storing the alignment in-
formation for each phrase pair. Formally, an AT Z is a triple (S,T,?a), where S and
T are a source class sequence and a target class sequence, respectively, and ?a is an
alignment from the set of positions in S to the set of positions in T.4 Mapping of source
and target words to bilingual word classes is automatically trained using the method
described by Och (1999). The method is actually an unsupervised clustering method
which partitions the source and target vocabularies, so that assigning words to classes
is a deterministic operation. It is also possible to employ parts-of-speech or semantic
categories instead of the unsupervised clustering method used here. More details can
be found in Och (1999) and Och and Ney (2004). However, it should be mentioned
that the whole AT approach (and similar PBM approaches as they are now called) is
independent of the word clustering concept. In particular, for large training corpora,
omitting the word clustering in the AT system does not much affect the translation
accuracy.
To arrive at our translation model, we first perform a segmentation of the source
and target sentences into K ?blocks? dk ? (ik; bk, jk) (ik ? {1, . . . , I} and jk, bk ? {1, . . . , J}
for 1 ? k ? K). For a given sentence pair (sJ1, t
I
1), the kth bilingual segment (?sk,
?tk)
is (s
jk
bk?1+1
, t
ik
ik?1+1
) (Och and Ney 2003). The AT Zk = (Sk,Tk,?ak) associated with the kth
bilingual segment is: Sk the sequence of word classes in ?sk; Tk the sequence of word
classes in ?tk, and ?ak the alignment between positions in a source class sequence S and
positions in a target class sequence T.
For translating a given source sentence s we use the following decision rule as an
approximation to Equation (1):
(I?, t?I?1) = argmax
I,tI1
{
max
K,dK1 ,?a
K
1
log PAT(s
J
1, t
I
1; d
K
1 ,?a
K
1 )
}
(9)
3 Although the term ?phrase? has a more restricted meaning, in this article it refers to a word sequence.
4 Note that the phrases in an AT are sequences of word classes rather than words, which motivates the use
of a different notation.
9
Computational Linguistics Volume 35, Number 1
We use a log-linear model combination:
log PAT(s
J
1, t
I
1; d
K
1 ,?a
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[ ?4 + ?5 ? log q(bk|jk?1)+ ?6 ? log p(Tk,?ak|Sk)+
ik
?
i=ik?1+1
?7 ? log p(ti|?sk,?ak) ] (10)
with weights ?i, i = 1, ? ? ? , 7. The weights ?1 and ?4 play a special role and are used
to control the number I of words and number K of segments for the target sentence
to be generated, respectively. The log-linear combination uses the following set of
models:
 p(ti|t
i?1
i?2): Word-based trigram language model
 p(Ti|T
i?1
i?4 ): Class-based five-gram language model
 p(Tk,?ak|Sk): AT at class level, model parameters are estimated directly
from frequency counts in a training corpus
 p(ti|?sk,?ak): Single word model based on a statistical dictionary and ?ak. As
in the preceding model, the model parameters are estimated by using
frequency counts
 q(bk|jk?1) = e|bk?jk?1+1|: Re-ordering model using absolute j distance of
the phrases.
As can be observed, all models are implemented as feature functions which depend on
the source and the target language sentences, as well as on the two hidden variables
(?aK1 , b
K
1 ). Other feature functions can be added to this sort of model as needed. For a
more detailed description the reader is referred to Och and Ney (2004).
Learning alignment templates. To learn the probability of applying an AT, p(Z =
(S,T,?a)|?s ), all bilingual phrases that are consistent with the segmentation are extracted
from the training corpus together with the alignment within these phrases. Thus, we
obtain a count N(Z) of how often an AT occurred in the aligned training corpus. Using
the relative frequency
p(Z) = (S,T,?a)|?s) =
N(Z) ? ?(S,C(?s))
N(C(?s))
(11)
we estimate the probability of applying an AT Z to translate the source language phrase
?s, in which ? is Kronecker?s delta function. The class function C maps words onto their
10
Barrachina et al Statistical Computer-Assisted Translation
classes. To reduce the memory requirements, only probabilities for phrases up to a
maximal length are estimated, and phrases with a probability estimate below a certain
threshold are discarded.
The weights ?i in Equation (10) are usually estimated using held-out data with
respect to the automatic evaluation metric employed using the downhill simplex al-
gorithm from Press et al (2002).
3.1.2 Phrase-Based Models. A simple alternative to AT has been introduced in recent
works: The PBM approach (Toma?s and Casacuberta 2001; Marcu and Wong 2002; Zens,
Och, and Ney 2002; Toma?s and Casacuberta 2003; Zens and Ney 2004). These methods
learn the probability that a sequence of contiguous words?the source phrase?(as a
whole unit) in a source sentence is a translation of another sequence of contiguous
words?the target phrase?(as a whole unit) in the target sentence. In this case, the
statistical dictionaries of single word pairs are substituted by statistical dictionaries of
bilingual phrases or bilingual segments. These models are simpler than ATs, because no
alignments are assumed between word positions inside a bilingual segment and word
classes are not used in the definition of a bilingual phrase.
The simplest formulation is for monotone PBMs (Toma?s and Casacuberta 2007),
assuming a uniform distribution of the possible segmentations of the source and of the
target sentences. In this case, the approximation to Equation (1) is:
(I?, t?I?1) = argmax
I,tI1
{
max
K,dK1
log PPBM(s
J
1, t
I
1; d
K
1 )
}
(12)
In our implementation of this approach, we have also adopted a log-linear model
log PPBM(s
J
1, t
I
1; d
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[
?4 + ?5 ? log p(?tk|?sk)
]
(13)
with weights ?i, i = 1, ? ? ? , 5. The weights ?1 and ?4 play a special role and are used
to control the number I of words and number K of segments for the target sentence
to be generated, respectively. The log-linear combination uses the following set of
models:
 p(ti|t
i?1
i?2): Word-based trigram language model
 p(Ti|T
i?1
i?4 ): Class-based five-gram language model
 p(?tk|?sk): Statistical dictionary of bilingual phrases.
11
Computational Linguistics Volume 35, Number 1
If segment re-ordering is desired (non-monotone models), the probability of phrase-
alignment q can be introduced (a first-order distortion model is assumed):
log PPBM(s
J
1, t
I
1; d
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[
?4 + ?5 ? log p(?tk|?sk)+ ?6 ? log q(bk|jk?1)
]
(14)
with the additional model q, similar to the one used for AT.
Learning phrase-based alignment models. The parameters of each model and the weights
?i in Equations (13) and (14) have to be estimated. There are different approaches to
estimating the parameters of each model (Toma?s and Casacuberta 2007). Some of these
techniques correspond to a direct learning of the parameters from a sentence-aligned
corpus using a maximum likelihood approach (Toma?s and Casacuberta 2001; Marcu
and Wong 2002). Other techniques are heuristics based on the previous computation
of word alignments in the training corpus (Zens, Och, and Ney 2002; Koehn, Och, and
Marcu 2003). On the other hand, as for AT, the weights ?i in Equation (13) are usually
optimized using held-out data.
3.2 Stochastic Finite-State Transducers
SFSTs constitute an important framework in syntactic pattern recognition and nat-
ural language processing. The simplicity of finite-state models has given rise to some
concerns about their applicability to real tasks. Specifically in the field of language
translation, it is often argued that natural languages are so complex that these simple
models are never able to cope with the required source-target mappings. However, one
should take into account that the complexity of the mapping between the source and
target domains of a transducer is not always directly related to the complexity of the
domains themselves. Instead, a key factor is the degree of monotonicity or sequentiality
between source and target subsequences of these domains (Casacuberta, Vidal, and
Pico? 2005). Finite-state transducers have been shown to be adequate to handle complex
mappings efficiently (Berstel 1979) and SFSTs are closely related to monotone PBMs.
In Equation (4), Pr(s, t) can be modeled by an SFST T, which is defined as a tuple
??,?,Q, q0, p, f ?, where ? is a finite set of source symbols,? is a finite set of target symbols
(? ?? = ?), Q is a finite set of states, q0 is the initial state, p and f are two functions
p : Q ? ??? ? Q ? [0, 1] (for the probabilities of transitions) and f : Q ? [0, 1] (for the
probabilities of final states) that satisfy ?q ? Q:
f (q) +
?
(s,?t,q? )?????Q
p(q, s,?t, q?) = 1 (15)
Given T, a path with J transitions associated with the translation pair (s, t) ?
?? ??? is a sequence of transitions ? = (q0, s1 , t?1, q1) (q1, s2 , t?2, q2) (q2, s3 , t?3, q3) . . .
(qJ?1, sJ , t?J, qJ ), such that s1 s2 . . . sJ = s and t?1 t?2 . . . t?J = t. The probability of a path is
12
Barrachina et al Statistical Computer-Assisted Translation
the product of its transition probabilities, times the final-state probability of the last
state in the path:
PT(?) =
J
?
j=1
p(qj?1, sj , t?j, qj) ? f (qJ ) (16)
The probability of a translation pair (s, t) according to T is then defined as the sum of
the probabilities of all the paths associated with (s, t):
PT(s, t) =
?
?
PT(?) (17)
Learning finite-state transducers. There are different families of techniques to train an
SFST from a parallel corpus of source?target sentences (Casacuberta and Vidal 2007).
One of the techniques that has been adopted in this work is the grammatical inference
and alignments for transducer inference (GIATI) technique. This technique is in the
category of hybrid methods which use statistical techniques to guide the SFST structure
learning and simultaneously train the associated probabilities.
Given a finite sample of string pairs, the inference of SFSTs using the GIATI tech-
nique is performed as follows (Casacuberta and Vidal 2004; Casacuberta, Vidal, and
Pico? 2005): i) Building training strings: Each training pair is transformed into a single
string from an extended alphabet to obtain a new sample of strings. ii) Inferring a
(stochastic) regular grammar. Typically, a smoothed n-gram is inferred from the sample
of strings obtained in the previous step. iii) Transforming the inferred regular grammar
into a transducer: The symbols associated with the grammar rules are converted back
into input/output symbols, thereby transforming the grammar inferred in the previous
step into a transducer. The transformation of a parallel corpus into a string corpus
is performed using statistical alignments. These alignments are obtained using the
GIZA++ software (Och and Ney 2003).
4. Searching
Searching is an important computational problem in SMT. Algorithmic solutions de-
veloped for SMT can be adapted to the IPMT framework. The main general search
procedures for each model in Section 3 are presented in the following subsections,
each followed by a detailed description of the necessary adaptations to the interactive
framework.
4.1 Searching with Alignment Templates
In offline MT, the generation of the best translation for a given source sentence s is
carried out by producing the target sentence in left-to-right order using the model of
Equation (10). At each step of the generation algorithm we maintain a set of active
hypotheses and choose one of them for extension. A word of the target language is
then added to the chosen hypothesis and its costs get updated. This kind of generation
fits nicely into a dynamic programming (DP) framework, as hypotheses which are
indistinguishable by both language and translation models (and that have covered
the same source positions) can be recombined. Because the DP search space grows
13
Computational Linguistics Volume 35, Number 1
Figure 2
Example of a word graph for the source German sentence was hast du gesagt? (English reference
translation: ?what did you say??).
exponentially with the size of the input, standard DP search is prohibitive, and we resort
to a beam-search heuristic.
4.1.1 Adaptation to the Interactive-Predictive Scenario. The most important modification
is to rely on a word graph that represents possible translations of the given source
sentence. This word graph is generated once for each source sentence. During the
process of human?machine interaction the system makes use of this word graph in
order to complete the prefixes accepted by the human translator. In other words, after
the human translator has accepted a prefix string, the system finds the best path in the
word graph associated with this prefix string so that it is able to complete the target
sentence. Using the word graph in such a way, the system is able to interact with the
human translator in a time efficient way. In Och, Zens, and Ney (2003), an efficient
algorithm for interactive generation using word graphs was presented. A word graph
is a weighted directed acyclic graph, in which each node represents a partial translation
hypothesis and each edge is labeled with a word of the target sentence and is weighted
according to the language and translation model scores. In Ueffing, Och, and Ney (2002),
the authors give a more detailed description of word graphs and show how they can be
easily produced as a by-product of the search process. An example of a word graph is
shown in Figure 2.
The computational cost of this approach is much lower, as the whole search for the
translation must be carried out only once, and the generated word graph can be reused
for further completion requests.
For a fixed source sentence, if no pruning is applied in the production of the word
graph, it represents all possible sequences of target words for which the posterior
probability is greater than zero, according to the models used. However, because of
the pruning generally needed to render the problem computationally feasible, the
resulting word graph only represents a subset of the possible translations. Therefore,
it may happen that the user sets prefixes which cannot be found in the word graph. To
circumvent this problem some heuristics need to be implemented.
First, we look for the node with minimum edit distance to the prefix except for
its last (partial) word.5 Then we select the completion path which starts with the last
5 The edit distance concept for finding the prefix string in a word graph could be refined by casting the edit
distance operations into a suitable probabilistic model.
14
Barrachina et al Statistical Computer-Assisted Translation
(partial) word of the prefix and has the best backward score?this is the score associated
with a path going from the node to the final node. Now, because the original word graph
may not be compatible with the new information provided by the prefix, it might be
impossible to find a completion in this word graph due to incompatibility with the
last (partial) word in the prefix. This problem can be solved to a certain degree by
searching for a completion of the last word with the highest probability using only the
language model. This supplementary heuristic to the usual search increases the perfor-
mance of the system, because some of the rejected words in the pruning process can
be recovered.
A desirable feature of an IPMT system is the possibility of producing a list of
alternative target suffixes, instead of only one. This feature can be easily added by
computing the n-best hypotheses. Of course, these n-best hypotheses do not refer to
the whole target sentence, but only to the suffixes. However, the problem is that in
many cases the sentence hypotheses in the n-best list differ in only one or two words.
Therefore, we introduce the additional requirement that the first four words of the n-
best hypotheses must be different.
4.2 Searching with Phrase-Based Models
The generation of the best translation with PBMs is similar to the one described in the
previous section. Each hypothesis is composed of a prefix of the target sentence, a subset
of source positions that are aligned with the positions of the prefix of the target sentence,
and a score. In this case, we adopted an extension of the best-first strategy where the
hypotheses are stored in several sorted lists, depending on which words in the source
sentence have been translated. This strategy is related to the well-known multi-stack-
decoding algorithm (Berger et al 1996; Toma?s and Casacuberta 2004). In each iteration,
the algorithm extends the best hypothesis from each available list.
While target words are always generated from left to right, there are two alter-
natives in the source word extraction: Monotone search, which takes the source words
from left to right, and non-monotone search, which can take source words in any
order.
4.2.1 Adaptation to the Interactive-Predictive Scenario. Only a simple modification of this
search algorithm is necessary: If the new extended hypothesis is not compatible with
the fixed target prefix, tp, then this hypothesis is not considered. This compatibility is
verified at the character level; therefore the user does not need to type the whole target
word at the end of the target prefix.
In the interactive scenario, speed is a critical aspect. In the PBM approach, monotone
search is much faster than non-monotone search in the tasks which are considered in this
work (Toma?s and Casacuberta 2006). However, monotone search presents a problem for
interactive operation: If a user introduces a prefix that cannot be obtained in a monotone
way from the source, the search algorithm is not able to complete this prefix. In order
to solve this problem without losing computational efficiency, we use the following ap-
proach: Non-monotone search is used for target prefixes, whereas completions (suffixes)
are generated using monotone search.
As for AT models, a list of target suffixes can also be produced. This list can be
obtained easily by keeping the n-best hypotheses in each sorted list. To avoid generating
very similar hypotheses in the n-best list, we apply the following procedure: Starting
from the n-best list resulting from the normal search, we first add hypotheses obtained
15
Computational Linguistics Volume 35, Number 1
by translating a single untranslated word from the source, along with hypotheses
consisting of a single high-probability word according to the target language model; we
then re-order the hypotheses, maximizing the diversity at the beginning of the suffixes,
and keep only the n first hypotheses in the re-ordered list.
4.3 Searching with Stochastic Finite-State Transducers
As discussed by Pico? and Casacuberta (2001), the computation of Equation (4) for SFSTs
under a maximum approximation (i.e., using maximization in Equation (17) instead
of the sum) amounts to a conventional Viterbi search. The algorithm finds the most
probable path among those paths in the SFST which are compatible with the source
sentence s. The corresponding translation, t?, is simply obtained by concatenating the
target strings of the edges of this path.
4.3.1 Adaptation to the Interactive-Predictive Scenario. Here, Equation (7) is used wherein
the optimization is performed over the set of target suffixes (completions) rather than
the set of complete target sentences. To solve this maximization problem, an approach
similar to that proposed for AT in Section 4.1 has been adopted.
First, given the source sentence, a word graph is extracted from the SFST. In this
case, the word graph is just (a pruned version of) the Viterbi search trellis obtained when
translating the whole source sentence. The main difference between the word graphs
generated with ATs and SFSTs is how the nodes and edges are defined in each case. On
the one hand, the nodes are defined as partial hypotheses of the search procedure in
the AT approach, whereas the nodes in the case of SFSTs can be directly mapped into
states in the SFST representing a joint (source word/target string) language model. On
the other hand, the scores associated with the edges in the AT approach are computed
from a combination of the language and translation models, whereas in the case of
SFSTs these scores simply come from the joint language model estimated by the GIATI
technique.
Once the word graph has been generated, the search for the most probable com-
pletion as stated in Equation (6) is carried out in two steps, in a similar way to that
explained for the AT approach. In this case, the computation entailed by both the edit-
distance (prefix error-correcting) and the remaining search is significantly accelerated
by visiting the nodes in topological order and by the incorporation of the beam-search
technique (Amengual and Vidal 1998). Moreover, the error-correcting algorithm takes
advantage of the incremental way in which the user prefix is generated, parsing only
the new suffix appended by the user in the last interaction.
It may be the case that a user prefix ends in an incomplete word during the inter-
active translation process. Therefore, it is necessary to start the translation completion
with a word whose prefix matches this unfinished word. The proposed algorithm thus
searches for such a word. First, it considers the target words of the edges leaving
the nodes returned by the error-correcting algorithm. If this initial search fails, then
a matching word is looked up in the word-graph vocabulary. Finally, as a last resort,
the whole transducer vocabulary is taken into consideration to find a matching word;
otherwise this incomplete word is treated as an entire word.
This error-correcting algorithm returns a set of nodes from which the best comple-
tion would be selected according to the best backward score. Moreover, n-best com-
pletions can also be produced. Among many weighted-graph n-best path algorithms
which are available, the recursive enumeration algorithm presented in Jime?nez and
16
Barrachina et al Statistical Computer-Assisted Translation
Marzal (1999) was adopted for its simplicity in calculating best paths on demand and its
smooth integration with the error-correcting algorithm.
5. Experimental Framework
The models and search procedures introduced in the previous sections were assessed
through a series of IPMT experiments with different corpora. These corpora, along with
the corresponding pre- and post-processing and assessment procedures, are presented
in this section.
5.1 Pre- and Post-Processing
Usually, MT models are trained on a pre-processed version of an original corpus. Pre-
processing provides a simpler representation of the training corpus which makes token
or word forms more homogeneous. In this way automatic training of the MT models is
boosted, and the amount of computation decreases.
The pre-processing steps are: tokenization, removing unnecessary case information,
and tagging some special tokens like numerical sequences, e-mail addresses, and URLs
(?categorization?). In translation from a source language to a target language, there are
some words which are translated identically (because they have the same spelling in
both languages). Therefore, we identify them in the corpus and replace them with some
generic tags to help the translation system.
Post-processing takes place after the translation in order to hide the internal repre-
sentation of the text from the user. Thus, the user will only work with an output which
is very similar to human-generated texts. In detail, the post-processing steps are: de-
tokenization, true-casing, and replacing the tags with their corresponding words.
In an IPMT scenario, the pre-/post-processing must run in real-time and should be
reversible as much as possible. In each human?machine interaction, the current prefix
has to be pre-processed for the interactive-predictive engine and then the generated
completion has to be post-processed for the user. It is crucial that the pre-processing of
prefixes is fully compatible with the training corpus.
5.2 Xerox and EU Corpora
Six bilingual corpora were used for two different tasks and three different language
pairs in the framework of the TT2 project (SchlumbergerSema S.A. et al 2001).
The language pairs involved were English?Spanish, English?French, and English?
German (Khadivi and Goutte 2003), and the tasks were Xerox (Xerox printer manuals)
and EU (Bulletin of the European Union).
The three Xerox corpora were obtained from different user manuals for Xerox print-
ers (SchlumbergerSema S.A. et al 2001). The main features of these corpora are shown
in Table 1. Dividing the corpora into training and test sets was performed by randomly
selecting (without replacement) a specified amount of test sentences and leaving the
remaining ones for training. It is worth noting that the manuals were not the same in
each pair of languages. Even though all training and test sets have similar size, this
probably explains why the perplexity varies considerably over the different language
pairs. The vocabulary size was computed using the tokenized and true-case corpus.
The three bilingual EU corpora were extracted from the Bulletin of the European
Union, which exists in all official languages of the European Union (Khadivi and Goutte
17
Computational Linguistics Volume 35, Number 1
Table 1
The Xerox corpora. For all the languages, the training/test full-sentence overlap and the rate of
out-of-vocabulary test-set words were less than 10% and 1%, respectively. Trigram models were
used to compute the test word perplexity. (K and M denote thousands and millions,
respectively.)
English/Spanish English/German English/French
T
ra
in Sent. pairs (K) 56 49 53
Running words (M) 0.7/0.7 0.6/0.5 0.6/0.7
Vocabulary (K) 15/17 14/25 14/16
T
e
st
Sentences (K) 1.1 1.0 1.0
Running words (K) 8/10 12/12 11/12
Running chars. (K) 46/59 63/73 56/65
Perplexity 99/58 57/93 109/70
2003) and is publicly available on the Internet. The corpora used in the experiments
which are described subsequently were again acquired and processed in the framework
of the TT2 project. The main features of these corpora are shown in Table 2. The
vocabulary size and the training and test set partitions were obtained in a similar way
as with the Xerox corpora.
5.3 Assessment
In all the experiments reported in this article, system performance is assessed by
comparing test sentence translations produced by the translation systems with the
corresponding target language references of the test set. Some of the computed assess-
ment figures measure the quality of the translation engines without any system?user
interactivity:
 Word error rate (WER): The minimum number of substitution, insertion,
and deletion operations needed to convert the word strings produced by
the translation system into the corresponding single-reference word
strings. WER is normalized by the overall number of words in the
reference sentences (Och and Ney 2003).
Table 2
The EU corpora. For all the languages, the training/test full-sentence overlap and the rate of
out-of-vocabulary test-set words were less than 3% and 0.2%, respectively. Trigram models were
used to compute the test word perplexity. (K and M denote thousands and millions,
respectively.)
English/Spanish English/German English/French
T
ra
in Sent. pairs (K) 214 223 215
Running words (M) 5.2/5.9 5.7/5.4 5.3/6.0
Vocabulary (K) 84/97 86/153 84/91
T
e
st
Sentences (K) 0.8 0.8 0.8
Running words (K) 20/23 20/19 20/23
Running chars. (K) 119/135 120/134 119/134
Perplexity 58/46 57/87 58/45
18
Barrachina et al Statistical Computer-Assisted Translation
 Bilingual evaluation understudy (BLEU): This is based on the coverage of
n-grams of the hypothesized translation which occur in the reference
translations (Papineni et al 2001).
Other assessment figures are aimed at estimating the effort needed by a human
translator to produce correct translations using the interactive system. To this end, the
target translations which a real user would have in mind are simulated by the given
references. The first translation hypothesis for each given source sentence is compared
with a single reference translation and the longest common character prefix (LCP) is
obtained. The first non-matching character is replaced by the corresponding reference
character and then a new system hypothesis is produced. This process is iterated until
a full match with the reference is obtained.
Each computation of the LCP would correspond to the user looking for the next
error and moving the pointer to the corresponding position of the translation hypothesis.
Each character replacement, on the other hand, would correspond to a keystroke of
the user. If the first non-matching character is the first character of the new system
hypothesis in a given iteration, no LCP computation is needed; that is, no pointer
movement would be made by the user. Bearing this in mind, we define the following
interactive-predictive performance measures:
 Keystroke ratio (KSR): Number of keystrokes divided by the total number
of reference characters.
 Mouse-action ratio (MAR): Number of pointer movements plus one more
count per sentence (aimed at simulating the user action
needed to accept the final translation), divided by the total number of
reference characters.
 Keystroke and mouse-action ratio (KSMR): KSR plus MAR.
Note that KSR estimates only the user?s actions on the keyboard whereas MAR
estimates actions for which the user would typically use the mouse. From a user
point of view the two types of actions are different and require different types of
effort (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006). In any case, as an
approximation, KSMR accounts for both KSR and MAR, assuming that both actions
require a similar effort.
In the case of SMT systems, it is well known that an automatically computed
quality measure like BLEU correlates quite well with human judgment (Callison-Burch,
Osborne, and Koehn 2006). In the case of IPMT, we should keep in mind that the
main goal of (automatic) assessment is to estimate the effort of the human translator.
Moreover, translation quality is not an issue here, because the (simulated) human
intervention ensures ?perfect? translation results. The important question is whether
the (estimated) productivity of the human translator can really be increased or not by
the IPMT approach. In order to answer this question, the KSR and KSMR measures will
be used in the IPMT experiments to be reported in the next section.
In order to show the statistical significance of the results, all the assessment figures
reported in the next section are accompanied by the corresponding 95% confidence
intervals. These intervals have been computed using bootstrap sampling techniques, as
proposed by Bisani and Ney (2004), Koehn (2004), and Zhang and Vogel (2004).
19
Computational Linguistics Volume 35, Number 1
6. Results
Two types of results are reported for each corpus and for each translation approach.
The first are conventional MT results, obtained as a reference to give an idea of the
?classical? MT difficulty of the selected tasks. The second aim is to assess the interactive
MT (IPMT) approach proposed in this article.
The results are presented in different subsections. The first two subsections present
the MT and IPMT results for the 1-best translation obtained by the different techniques
in the Xerox and EU tasks, respectively. The third subsection presents further IPMT
results for the 5-best translations on a single pair of languages.
Some of these results may differ from results presented in previous works (Cubel
et al 2003; Och, Zens, and Ney 2003; Civera et al 2004a; Cubel et al 2004; Bender
et al 2005). The differences are due to variations in the pre-/post-processing procedures
and/or recent improvements of the search techniques used by the different systems.
6.1 Experiments with the Xerox Corpora
In this section, the translation results obtained using ATs, PBMs, and SFSTs for all six
language pairs of the Xerox corpus are reported. Word-based trigram and class-based
five-gram target-language models were used for the AT models (the parameters of the
log-linear model are tuned so as to minimize WER on a development corpus); word-
based trigram target-language models were used for PBMs and trigrams were used to
infer GIATI SFSTs.
Off-line MT Results. MT results with ATs, PBMs, and SFSTs are presented in Figure 3.
Results obtained using the PBMs are slightly but consistently better that those achieved
using the other models. In general, the different techniques perform similarly for the
various translation directions. However, the English?Spanish language pair is the one
for which the best translations can be produced.
IPMT Results. Performance has been measured in terms of KSRs and MARs (KSR and
MAR are represented as the lower and upper portions of each bar, respectively, and
KSMR is the whole bar length). The results are shown in Figure 4.
Figure 3
Off-line MT results (BLEU and WER) for the Xerox corpus. Segments above the bars show the
95% confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
20
Barrachina et al Statistical Computer-Assisted Translation
Figure 4
IPMT results for the Xerox corpus. In each bar, KSR is represented by the lower portion, MAR by
the upper portion, and KSMR is the whole bar. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
According to these results, a human translator assisted by an AT-based or a SFST-
based interactive system would only need an effort equivalent to typing about 20% of
the characters in order to produce the correct translations for the Spanish to English
task; or even less than 20% if a PBM-based system is used.
For the Xerox task, off-line MT performance and IPMT results show similar tenden-
cies. The PBMs show better performance for both the off-line MT and for the IPMT
assessment figures. The AT and SFST models perform more or less equivalently. In
both scenarios, the best results were achieved for the Spanish?English language pair
followed by French?English and German?English.
The computing times needed by all the systems involved in these experiments were
well within the range of the on-line operational requirements. The average initial time
for each source test sentence was very low (less than 50 msec) for PBMs and SFSTs
and adequate for ATs (772 msec). In the case of ATs and SFSTs, this included the time
required for the generation of the initial word-graph of each sentence. Moreover, the
most critical times incurred in the successive IPMT iterations were very low in all
the cases: 18 msec for ATs, 99 msec for PBMs, and 9 msec for SFSTs. Note, however,
that these average times are not exactly comparable because of the differences in the
computer hardware used by each system (2 Ghz AMD, 1.5 Ghz Pentium, and 2.4 Ghz
Pentium for ATs, PBMs, and SFSTs, respectively).
6.2 Experiments with the EU Corpora
The translation results using the AT, PBM, and SFST approaches for all six language
pairs of the EU corpus are reported in this section. As for the Xerox corpora, in the AT
experiments, word-based trigram and class-based five-gram target-language models
were used; in the PBM experiments, word-based trigram and class-based five-gram
target-language models were also used and five-grams were used to infer GIATI SFSTs.
Off-line MT Results. Figure 5 presents the results obtained using ATs, PBMs, and SFSTs.
Generally speaking, the results are comparable to those obtained on the Xerox corpus
with the exception of the English?Spanish language pair, which were better. With these
corpora, the best results were obtained with the ATs and PBMs for all the pairs and the
best translation direction was French-to-English with all the models used.
21
Computational Linguistics Volume 35, Number 1
Figure 5
Off-line MT results (BLEU and WER) for the EU corpus. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
IPMT Results. Figure 6 shows the performance of the AT, PBM, and SFST systems in
terms of KSRs and MARs in a similar way as for the Xerox corpora.
As in the MT experiments, the results are comparable to those obtained on the Xerox
corpus, with the exception of the English?Spanish pair. Similarly, as in MT, the best
results were obtained for the French-to-English translation direction.
Although EU is a more open-domain task, the results demonstrate again the poten-
tial benefit of computer-assisted translation systems. Using PBMs, a human translator
would only need an effort equivalent to typing about 20% of the characters in order
to produce the correct translations for French-to-English translation direction, whereas
for ATs and SFSTs the effort would be about 30%. For the other language pairs, the
efforts would be about 20?30% and 35% of the characters for PBMs and ATs/SFSTs,
respectively.
The systemwise correlation between MT and IPMT results on this corpus is not
as clear as in the Xerox case. One possible cause is the much larger size of the EU
corpus compared to the Xerox corpus. In order to run the EU experiments within rea-
sonable time limits, all the systems have required the use of beam search and/or other
Figure 6
IPMT results for the EU corpus. In each bar, KSR is represented by the lower portion, MAR by
the upper portion and KSMR is the whole bar. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
22
Barrachina et al Statistical Computer-Assisted Translation
Table 3
IPMT results (%) for the Xerox corpus (English?Spanish) using ATs, PBMs, and SFSTs for the
1-best hypothesis and 5-best hypotheses. 95% confidence intervals are shown.
1-best 5-best
Technique KSR KSMR KSR KSMR
AT 12.9?0.9 23.2?1.3 11.1?0.8 20.3?1.2
PBM 8.9?0.8 16.7?1.2 7.3?0.6 15.4?1.1
SFST 13.0?1.0 21.8?1.4 11.2?1.0 19.2?1.3
suboptimal pruning techniques, although this was largely unnecessary for the Xerox
corpus. Clearly, the pruning effects are different in the off-line (MT) and the on-line
(IPMT) search processes and the differences may lead to wide performance variations
for the AT, PBM, and SFST approaches.
Nevertheless, as can be seen in Bender et al (2005), the degradation in system
performance due to pruning is generally not too substantial and sufficiently accurate
real-time interactive operation could also be achieved in the EU task with the three
systems tested.
6.3 Results with n-Best Hypotheses
Further experiments were carried out to study the usefulness of n-best hypotheses in
the interactive framework. In this scenario, the user can choose one out of n proposed
translation suffixes and then proceed as in the usual IPMT paradigm. As with the
previous experiments, the automated evaluation is based on a selected target sentence
that best matches a prefix of the reference translation in each IPMT iteration (therefore
KSR is minimized).
Here, only IPMT results for the English-to-Spanish translation direction are re-
ported for both Xerox and EU tasks, using a list of the five best translations. These results
are shown in Tables 3 and 4.
In all the cases there is a clear and significant accuracy improvement when moving
from single-best to 5-best translations. This gain in translation quality diminishes in a
log-wise fashion as we increase the number of best translations. From a practical point
of view, the improvements provided by using n-best completions would come at the
cost of the user having to ponder which of these completions is more suitable. In a
real operational environment, this additional user effort may or may not outweigh the
Table 4
IPMT results (%) for the EU corpus (English?Spanish) using ATs, PBMs, and SFSTs for the 1-best
hypothesis and 5-best hypotheses. 95% confidence intervals are shown.
1-best 5-best
Technique KSR KSMR KSR KSMR
AT 20.2?0.9 32.6?1.3 18.5?0.8 29.9?1.2
PBM 16.3?0.7 27.8?1.1 13.2?0.6 25.0?1.1
SFST 21.3?0.9 33.0?1.3 19.3?0.9 29.9?1.3
23
Computational Linguistics Volume 35, Number 1
benefits of the n-best increased accuracy. Consequently, this feature should be offered to
the users as an option.
7. Practical Issues
IPMT results reported in the previous section provide reasonable estimations of potential
savings of human translator effort, assuming that the goal is to obtain high quality
translations. In real work, however, several practical issues not discussed in this article
may significantly affect the actual system usability and overall user productivity.
One of the most obvious issues is that a carefully designed graphical user interface
(GUI) is needed to let the users actually be in command of the translation process, so
that they really feel the system is assisting them rather than the other way around. In
addition, an adequate GUI has to provide adequate means for the users to easily and
intuitively change at will IPMT engine parameters that may have an impact on their
way of working with the system. To name just a few: The maximum length of system
hypotheses, the value of n for n-best suggestions, or the ?interaction step granularity?;
that is, whether the system should react at each user keystroke, or at the end of each
complete typed word, or after a sufficiently long typing pause, and so on.
Clearly, all these important issues are beyond the scope of the present article. But
we can comment that, in the TT2 project, complete prototypes of some of the systems
presented in this article, including the necessary GUI, were actually implemented and
thoroughly evaluated by professional human translators in their working environ-
ment (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).
The results of these field tests showed that the actual productivity depended not
only on the individual translators, but also on the given test texts. In cases where these
texts were quite unrelated to the training data, the system did not significantly help
the human translators to increase their productivity. However, when the test texts were
reasonably well related to the training data, high productivity gains were registered?
close to what could be expected according to the KSR/MAR empirical results.
8. Concluding Remarks
The IPMT paradigm proposed in this article allows for a close collaboration between a
human translator and a machine translation system. This paradigm entails an iterative
process where, in each iteration, a data-driven machine translation engine suggests a
completion for the current prefix of a target sentence which a human translator can
accept, modify, or ignore.
This idea was originally proposed in the TransType project (Langlais, Foster, and
Lapalme 2000), where a simple engine was used which only supported single-token
suggestions. Furthering these ideas, in the TransType2 project (SchlumbergerSema S.A.
et al 2001), state-of-the-art statistical machine translation systems have been developed
and integrated in the IPMT framework.
In a laboratory environment, results on two different tasks suggest that the pro-
posed techniques can reduce the typing effort needed to produce a high-quality transla-
tion of a given source text by as much as 80% with respect to the effort needed to simply
type the whole translation. In real conditions, a high productivity gain was achieved in
many cases.
We have studied here IPMT from the point of view of a standalone CAT tool.
Nevertheless, IPMT can of course be easily and conveniently combined with other
popular translator workbench tools. More specifically, IPMT lends itself particularly
24
Barrachina et al Statistical Computer-Assisted Translation
well to addressing the typical lack of generalization capabilities of translation memories.
When used as a CAT tool, translation memories allow the human translator to keep
producing increasingly long segments of correct target text. Clearly, these segments can
be used by an IPMT engine to suggest to the translator possible translations for source
text segments that are not found in the translation memories as exact matches.
Acknowledgments
This work has been partially supported by
the ST Programme of European Union under
grant IST-2001-32091, by the Spanish project
TIC?2003-08681-C02-02, and the Spanish
research programme Consolider
Ingenio-2010 CSD2007-00018. The authors
wish to thank the anonymous reviewers for
their criticisms and suggestions.
References
Amengual, J. C., J. M. Bened??, A. Castan?o,
A. Castellanos, V. M. Jime?nez, D. Llorens,
A. Marzal, M. Pastor, F. Prat, E. Vidal, and
J. M. Vilar. 2000. The EuTrans-I speech
translation system. Machine Translation,
15:75?103.
Amengual, J. C. and E. Vidal. 1998. Efficient
error-correcting Viterbi parsing. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 20(10):1109?1116.
Bender, O., S. Hasan, D. Vilar, R. Zens, and
H. Ney. 2005. Comparison of generation
strategies for interactive machine
translation. In Proceedings of the 10th
Annual Conference of the European
Association for Machine Translation (EAMT
05), pages 33?40, Budapest.
Berger, A. L., P. F. Brown, S. A. Della Pietra,
V. J. Della Pietra, J. R. Gillett, A. S. Kehler,
and R. L. Mercer. 1996. Language
translation apparatus and method of using
context-based translation models. United
States Patent No. 5510981, April.
Berstel, J. 1979. Transductions and Context-Free
Languages. B. G. Teubner, Stuttgart.
Bisani, M. and H. Ney. 2004. Bootstrap
estimates for confidence intervals in ASR
performance evaluation. In Proceedings of
the International Conference on Acoustic,
Speech and Signal Processing (ICASSP 04),
volume 1, pages 409?412, Montreal.
Bowker, L. 2002. Computer-Aided Translation
Technology: A Practical Introduction,
chapter 5: Translation-memory systems.
Didactics of Translation. University of
Ottawa Press, pages 92?127.
Brown, P. F., J. Cocke, S. A. Della Pietra,
V. J. Della Pietra, F. Jelinek, J. D. Lafferty,
R. L. Mercer, and P. S. Roosin. 1990.
A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Brown, P. F., S. A. Della Pietra, V. J.
Della Pietra, and R. L. Mercer. 1993. The
mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?310.
Callison-Burch, C., M. Osborne, and
P. Koehn. 2006. Re-evaluating the role of
BLEU in machine translation research. In
Proceedings of the 10th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL 06),
pages 249?256, Trento.
Casacuberta, F., H. Ney, F. J. Och, E. Vidal,
J. M. Vilar, S. Barrachina, I. Garc??a-Varea,
D. Llorens, C. Mart??nez, S. Molau,
F. Nevado, M. Pastor, D. Pico?, A. Sanchis,
and C. Tillmann. 2004a. Some approaches
to statistical and finite-state
speech-to-speech translation. Computer
Speech and Language, 18:25?47.
Casacuberta, F. and E. Vidal. 2004. Machine
translation with inferred stochastic
finite-state transducers. Computational
Linguistics, 30(2):205?225.
Casacuberta, F. and E. Vidal. 2007. Learning
finite-state models for machine translation.
Machine Learning, 66(1):69?91.
Casacuberta, F., E. Vidal, and D. Pico?. 2005.
Inference of finite-state transducers from
regular languages. Pattern Recognition,
38:1431?1443.
Casacuberta, F., E. Vidal, A. Sanchis, and
J. M. Vilar. 2004b. Pattern recognition
approaches for speech-to-speech
translation. Cybernetic and Systems: an
International Journal, 35(1):3?17.
Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,
S. Barrachina, E. Vidal, F. Casacuberta,
D. Pico?, and J. Gonza?lez. 2004a. From
machine translation to computer assisted
translation using finite-state models. In
Proceedings of the Conference on Empirical
Methods for Natural Language Processing
(EMNLP 04), pages 349?356, Barcelona.
Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,
S. Barrachina, F. Casacuberta, E. Vidal,
D. Pico?, and J. Gonza?lez. 2004b. A syntactic
pattern recognition approach to computer
assisted translation. In Advances in
25
Computational Linguistics Volume 35, Number 1
Statistical, Structural and Syntactical Pattern
Recognition, Proceedings of the Joint IAPR
International Workshops on Syntactical and
Structural Pattern Recognition (SSPR 04)
and Statistical Pattern Recognition
(SPR 04)), Lisbon, Portugal, August 18?20,
volume 3138 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 207?215.
Cubel, E., J. Civera, J. M. Vilar, A. L. Lagarda,
S. Barrachina, E. Vidal, F. Casacuberta,
D. Pico?, J. Gonza?lez, and L. Rodr??guez.
2004. Finite-state models for computer
assisted translation. In Proceedings of the
16th European Conference on Artificial
Intelligence (ECAI 04), pages 586?590,
Valencia.
Cubel, E., J. Gonza?lez, A. Lagarda,
F. Casacuberta, A. Juan, and E. Vidal. 2003.
Adapting finite-state translation to the
TransType2 project. In Proceedings of the
Joint Conference Combining the 8th
International Workshop of the European
Association for Machine Translation and the
4th Controlled Language Applications Workshop
(EAMT-CLAW 03), pages 54?60, Dublin.
Foster, G. 2002. Text Prediction for Translators.
Ph.D. thesis, Universite? de Montre?al,
Canada.
Foster, G., P. Isabelle, and P. Plamondon.
1997. Target-text mediated interactive
machine translation. Machine Translation,
12(1?2):175?194.
Isabelle, P. and K. Church. 1997. Special issue
on new tools for human translators.
Machine Translation, 12(1?2).
Jelinek, F. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge,
MA.
Jime?nez, V. M. and A. Marzal. 1999.
Computing the k shortest paths: a new
algorithm and an experimental
comparison. In Algorithm Engineering:
Proceedings of the 3rd International
Workshop (WAE 99), London, UK, July 19?21,
volume 1668 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 15?29.
Kay, M. 1997. The proper place of men and
machines in language translation. Machine
Translation, 12:3?23. [This article first
appeared as a Xerox PARC Working Paper
in 1980].
Khadivi, S. and C. Goutte. 2003. Tools for
corpus alignment and evaluation of the
alignments (deliverable d4.9). Technical
report, TransType2 (IST-2001-32091).
Khadivi, S., R. Zens, and H. Ney. 2006.
Integration of speech to computer-assisted
translation using finite-state automata.
In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics
and 21th International Conference on
Computational Linguistics (COLING/ACL
06), pages 467?474, Sydney.
Khadivi, S., A. Zolnay, and H. Ney. 2005.
Automatic text dictation in
computer-assisted translation. In
Proceedings of the European Conference on
Speech Communication and Technology,
(INTERSPEECH 05-EUROSPEECH),
pages 2265?2268, Lisbon.
Koehn, P. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the Conference on
Empirical Methods for Natural Language
Processing (EMNLP 04), pages 388?395,
Barcelona.
Koehn, P., F. J. Och, and D. Marcu. 2003.
Statistical phrase-based translation. In
Proceedings of the 2003 Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 03),
pages 127?133, Edmonton.
Langlais, P., G. Foster, and G. Lapalme. 2000.
TransType: a computer-aided translation
typing system. In Proceedings of the
NAACL/ANLP Workshop on Embedded
Machine Translation Systems, pages 46?52,
Seattle, WA.
Langlais, P., G. Lapalme, and M. Loranger.
2002. Transtype: Development-evaluation
cycles to boost translator?s productivity.
Machine Translation, 15(4):77?98.
Macklovitch, E. 2006. TransType2: The last
word. In Proceedings of the 5th International
Conference on Languages Resources and
Evaluation (LREC 06), pages 167?172,
Genoa.
Macklovitch, E., N. T. Nguyen, and R. Silva.
2005. User evaluation report. Technical
report, TransType2 (IST-2001-32091).
Marcu, D. and W. Wong. 2002. A
phrase-based, joint probability model
for statistical machine translation.
In Proceedings of the Conference on
Empirical Methods for Natural Language
Processing (EMNLP 02), pages 133?139,
Philadelphia, PA.
Ney, H., S. Nie?en, F. Och, H. Sawaf,
C. Tillmann, and S. Vogel. 2000.
Algorithms for statistical translation of
spoken language. IEEE Transactions on
Speech and Audio Processing, 8(1):24?36.
Och, F. J. 1999. An efficient method for
determining bilingual word classes. In
Proceedings of the 9th Conference of the
European Chapter of the Association for
26
Barrachina et al Statistical Computer-Assisted Translation
Computational Linguistics (EACL 99),
pages 71?76, Bergen.
Och, F. J. and H. Ney. 2003. A systematic
comparison of various statistical
alignment models. Computational
Linguistics, 29(1):19?51.
Och, F. J. and H. Ney. 2004. The alignment
template approach to statistical machine
translation. Computational Linguistics,
30(4):417?450.
Och, F. J., R. Zens, and H. Ney. 2003.
Efficient search for interactive statistical
machine translation. In Proceedings of
the 10th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL 03), pages 387?393,
Budapest.
Papineni, K., S. Roukos, T. Ward, and
W. Zhu. 2001. BLEU: a method for
automatic evaluation of machine
translation. Technical Report RC22176,
Thomas J. Watson Research Center.
Pico?, D. and F. Casacuberta. 2001. Some
statistical-estimation methods for
stochastic finite-state transducers. Machine
Learning, 44:121?142.
Press, W. H., S. A. Teukolsky, W. T.
Vetterling, and B. P. Flannery. 2002.
Numerical Recipes in C++: The Art of
Scientific Computing. Cambridge University
Press, Cambridge, UK.
SchlumbergerSema S.A., Intituto Tecnolo?gico
de Informa?tica, Rheinisch Westfa?lische
Technische Hochschule Aachen Lehrstul
fu?r Informatik VI, Recherche Applique?e
en Linguistique Informatique Laboratory
University of Montreal, Celer Soluciones,
Socie?te? Gamma, and Xerox
Research Centre Europe. 2001. TT2.
TransType2?computer-assisted
translation. Project technical annex.
Information Society Technologies (IST)
Programme, IST-2001-32091.
Sen, Z., Ch. Zhaoxiong, and H. Heyan. 1997.
Interactive approach in machine translation
systems. In Proceedings of IEEE International
Conference on Intelligent Processing Systems
(ICIPS 97), pages 1814?1819, Beijing.
Slocum, J. 1985. A survey of machine
translation: Its history, current status and
future prospects. Computational Linguistics,
11(1):1?17.
Somers, H., 2003. Computers and Translation: a
Translator?s Guide, chapter 3: Translation
memory systems. John Benjamins,
Amsterdam, pages 31?48.
Toma?s, J. and F. Casacuberta. 2001.
Monotone statistical translation using
word groups. In Proceedings of the Machine
Translation Summit VIII (MT SUMMIT
VIII), pages 357?361, Santiago de
Compostela.
Toma?s, J. and F. Casacuberta. 2003.
Combining phrase-based and
template-based alignment models in
statistical translation. In Pattern Recognition
and Image Analysis, Proceedings of the First
Iberian Conference (IbPRIA 03), Puerto
de Andratx, Mallorca, Spain, June 4-6,
volume 2652 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 1020?1031.
Toma?s, J. and F. Casacuberta. 2004. Statistical
machine translation decoding using
target word reordering. In Advances in
Statistical, Structural and Syntactical Pattern
Recognition, Proceedings of the Joint IAPR
International Workshops on Syntactical
and Structural Pattern Recognition
(SSPR 04) and Statistical Pattern Recognition
(SPR 04), Lisbon, Portugal, August 18?20,
volume 3138 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 734?743.
Toma?s, J. and F. Casacuberta. 2006. Statistical
phrase-based models for interactive
computer-assisted translation. In
Proceedings of the 44th Annual Meeting
of the Association for Computational
Linguistics and 21th International
Conference on Computational Linguistics
(COLING/ACL 06), pages 835?841, Sydney.
Toma?s, J. and F. Casacuberta. 2007. A
pattern recognition approach to
machine translation: Monotone and
non-monotone phrase-based statistical
models. Technical Report DSIC-II/18/07,
Departamento de Sistemas Informa?ticos y
Computacio?n, Universidad Polite?cnica
de Valencia.
Tomita, M. 1985. Feasibility study of
personal/interactive machine translation
systems. In Proceedings of the First
International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI 85), pages 289?297,
New York, NY.
Ueffing, N., F. J. Och, and H. Ney. 2002.
Generation of word graphs in statistical
machine translation. In Proceedings of
the Conference on Empirical Methods for
Natural Language Processing (EMNLP 02),
pages 156?163, Philadelphia, PA.
Vidal, E. 1997. Finite-state speech-to-speech
translation. In Proceedings of the
International Conference on Acoustic,
Speech and Signal Processing (ICASSP 97),
volume 1, pages 111?114, Munich.
27
Computational Linguistics Volume 35, Number 1
Vidal, E. and F. Casacuberta. 2004. Learning
finite-state models for machine translation.
In Grammatical Inference: Algorithms and
Applications, Proceedings of the 7th
International Coloquium on Grammatical
Inference (ICGI 04), Athens, Greece,
October 11?13, volume 3264 of Lecture
Notes in Artificial Intelligence. Springer,
Heidelberg, pages 16?27.
Vidal, E., F. Casacuberta, L. Rodr??guez,
J. Civera, and C. Mart??nez. 2006.
Computer-assisted translation using
speech recognition. IEEE Transactions
on Speech and Audio Processing,
14(3):941?951.
Vidal, E., F. Thollard, F. Casacuberta
C. de la Higuera, and R. Carrasco. 2005.
Probabilistic finite-state machines?
part II. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
27(7):1025?1039.
Whitelock, P. J., M. McGee Wood, B. J.
Chandler, N. Holden, and H. J. Horsfall.
1986. Strategies for interactive machine
translation: The experience and
implications of the UMIST Japanese
project. In Proceedings of the 11th
International Conference on Computational
Linguistics (COLING 86), pages 329?334,
Bonn.
Yamron, J., J. Baker, P. Bamberg,
H. Chevalier, T. Dietzel, J. Elder,
F. Kampmann, M. Mandel, L. Manganaro,
T. Margolis, and E. Steele. 1993.
LINGSTAT: an interactive, machine-aided
translation system. In Proceedings of the
Workshop on Human Language Technology,
pages 191?195, Princeton, NJ.
Zajac, R. 1988. Interactive translation: A new
approach. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING 88), pages 785?790,
Budapest.
Zens, R. and H. Ney. 2004. Improvements
in phrase-based statistical machine
translation. In Proceedings of the Human
Language Technology Conference / North
American Chapter of the Association for
Computational Linguistics Annual Meeting
(HLT-NAACL 04), pages 257?264,
Boston, MA.
Zens, R., F. J. Och, and H. Ney. 2002.
Phrase-based statistical machine
translation. In Advances in Artificial
Intelligence. 25th Annual German Conference
on Artificial Intelligence (KI 02), Aachen,
Germany, September 16?22, Proceedings,
volume 2479 of Lecture Notes on Artificial
Intelligence. Springer Verlag, Heidelberg,
pages 18?32.
Zhang, Y. and S. Vogel. 2004. Measuring
confidence intervals for the machine
translation evaluation metrics. In
Proceedings of the Tenth International
Conference on Theoretical and
Methodological Issues in Machine
Translation (TMI 04), pages 294?301,
Baltimore, MD.
28
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Improvements in Phrase-Based Statistical Machine Translation
Richard Zens and Hermann Ney
Chair of Computer Science VI
RWTH Aachen University
{zens,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, the currently
best performing systems are based in some way
on phrases or word groups. We describe the
baseline phrase-based translation system and
various refinements. We describe a highly ef-
ficient monotone search algorithm with a com-
plexity linear in the input sentence length. We
present translation results for three tasks: Verb-
mobil, Xerox and the Canadian Hansards. For
the Xerox task, it takes less than 7 seconds to
translate the whole test set consisting of more
than 10K words. The translation results for
the Xerox and Canadian Hansards task are very
promising. The system even outperforms the
alignment template system.
1 Introduction
In statistical machine translation, we are given a source
language (?French?) sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language (?English?)
sentence eI1 = e1 . . . ei . . . eI . Among all possible target
language sentences, we will choose the sentence with the
highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
} (1)
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
The decomposition into two knowledge sources in Equa-
tion 2 is known as the source-channel approach to statisti-
cal machine translation (Brown et al, 1990). It allows an
independent modeling of target language model Pr(eI1)
and translation model Pr(fJ1 |eI1)1. The target language
1The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
model describes the well-formedness of the target lan-
guage sentence. The translation model links the source
language sentence to the target language sentence. It can
be further decomposed into alignment and lexicon model.
The argmax operation denotes the search problem, i.e.
the generation of the output sentence in the target lan-
guage. We have to maximize over all possible target lan-
guage sentences.
An alternative to the classical source-channel ap-
proach is the direct modeling of the posterior probabil-
ity Pr(eI1|fJ1 ). Using a log-linear model (Och and Ney,
2002), we obtain:
Pr(eI1|fJ1 ) = exp
( M?
m=1
?mhm(eI1, fJ1 )
)
? Z(fJ1 )
Here, Z(fJ1 ) denotes the appropriate normalization con-
stant. As a decision rule, we obtain:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
This approach is a generalization of the source-channel
approach. It has the advantage that additional models or
feature functions can be easily integrated into the overall
system. The model scaling factors ?M1 are trained accord-
ing to the maximum entropy principle, e.g. using the GIS
algorithm. Alternatively, one can train them with respect
to the final translation quality measured by some error
criterion (Och, 2003).
The remaining part of this work is structured as fol-
lows: in the next section, we will describe the base-
line phrase-based translation model and the extraction of
bilingual phrases. Then, we will describe refinements
of the baseline model. In Section 4, we will describe a
monotone search algorithm. Its complexity is linear in
the sentence length. The next section contains the statis-
tics of the corpora that were used. Then, we will inves-
tigate the degree of monotonicity and present the transla-
tion results for three tasks: Verbmobil, Xerox and Cana-
dian Hansards.
2 Phrase-Based Translation
2.1 Motivation
One major disadvantage of single-word based approaches
is that contextual information is not taken into account.
The lexicon probabilities are based only on single words.
For many words, the translation depends heavily on the
surrounding words. In the single-word based translation
approach, this disambiguation is addressed by the lan-
guage model only, which is often not capable of doing
this.
One way to incorporate the context into the translation
model is to learn translations for whole phrases instead
of single words. Here, a phrase is simply a sequence of
words. So, the basic idea of phrase-based translation is
to segment the given source sentence into phrases, then
translate each phrase and finally compose the target sen-
tence from these phrase translations.
2.2 Phrase Extraction
The system somehow has to learn which phrases are
translations of each other. Therefore, we use the follow-
ing approach: first, we train statistical alignment models
using GIZA++ and compute the Viterbi word alignment of
the training corpus. This is done for both translation di-
rections. We take the union of both alignments to obtain a
symmetrized word alignment matrix. This alignment ma-
trix is the starting point for the phrase extraction. The fol-
lowing criterion defines the set of bilingual phrases BP
of the sentence pair (fJ1 ; eI1) and the alignment matrix
A ? J ? I that is used in the translation system.
BP(fJ1 , eI1, A) =
{(
f j2j1 , ei2i1
)
:
?(j, i) ? A : j1 ? j ? j2 ? i1 ? i ? i2
??(j, i) ? A : j1 ? j ? j2 ? i1 ? i ? i2
}
This criterion is identical to the alignment template cri-
terion described in (Och et al, 1999). It means that two
phrases are considered to be translations of each other, if
the words are aligned only within the phrase pair and not
to words outside. The phrases have to be contiguous.
2.3 Translation Model
To use phrases in the translation model, we introduce the
hidden variable S. This is a segmentation of the sentence
pair (fJ1 ; eI1) into K phrases (f?K1 ; e?K1 ). We use a one-to-
one phrase alignment, i.e. one source phrase is translated
by exactly one target phrase. Thus, we obtain:
Pr(fJ1 |eI1) =
?
S
Pr(fJ1 , S|eI1) (3)
=
?
S
Pr(S|eI1) ? Pr(fJ1 |S, eI1) (4)
? max
S
{
Pr(S|eI1) ? Pr(f?K1 |e?K1 )
}
(5)
In the preceding step, we used the maximum approxima-
tion for the sum over all segmentations. Next, we allow
only translations that are monotone at the phrase level.
So, the phrase f?1 is produced by e?1, the phrase f?2 is
produced by e?2, and so on. Within the phrases, the re-
ordering is learned during training. Therefore, there is no
constraint on the reordering within the phrases.
Pr(f?K1 |e?K1 ) =
K?
k=1
Pr(f?k|f?k?11 , e?K1 ) (6)
=
K?
k=1
p(f?k|e?k) (7)
Here, we have assumed a zero-order model at the phrase
level. Finally, we have to estimate the phrase translation
probabilities p(f? |e?). This is done via relative frequencies:
p(f? |e?) = N(f? , e?)?
f? ? N(f? ?, e?)
(8)
Here, N(f? , e?) denotes the count of the event that f? has
been seen as a translation of e?. If one occurrence of e? has
N > 1 possible translations, each of them contributes to
N(f? , e?) with 1/N . These counts are calculated from the
training corpus.
Using a bigram language model and assuming Bayes
decision rule, Equation (2), we obtain the following
search criterion:
e?I1 = argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (9)
= argmax
eI1
{ I?
i=1
p(ei|ei?1) (10)
?max
S
p(S|eI1) ?
K?
k=1
p(f?k|e?k)
}
? argmax
eI1,S
{ I?
i=1
p(ei|ei?1)
K?
k=1
p(f?k|e?k)
}
(11)
For the preceding equation, we assumed the segmentation
probability p(S|eI1) to be constant. The result is a simple
translation model. If we interpret this model as a feature
function in the direct approach, we obtain:
hphr(fJ1 , eI1, S,K) = log
K?
k=1
p(f?k|e?k)
We use the maximum approximation for the hidden vari-
able S. Therefore, the feature functions are dependent on
S. Although the number of phrases K is implicitly given
by the segmentation S, we used both S and K to make
this dependency more obvious.
3 Refinements
In this section, we will describe refinements of the
phrase-based translation model. First, we will describe
two heuristics: word penalty and phrase penalty. Sec-
ond, we will describe a single-word based lexicon model.
This will be used to smooth the phrase translation proba-
bilities.
3.1 Simple Heuristics
In addition to the baseline model, we use two simple
heuristics, namely word penalty and phrase penalty:
hwp(fJ1 , eI1, S,K) = I (12)
hpp(fJ1 , eI1, S,K) = K (13)
The word penalty feature is simply the target sentence
length. In combination with the scaling factor this re-
sults in a constant cost per produced target language
word. With this feature, we are able to adjust the sentence
length. If we set a negative scaling factor, longer sen-
tences are more penalized than shorter ones, and the sys-
tem will favor shorter translations. Alternatively, by us-
ing a positive scaling factors, the system will favor longer
translations.
Similar to the word penalty, the phrase penalty feature
results in a constant cost per produced phrase. The phrase
penalty is used to adjust the average length of the phrases.
A negative weight, meaning real costs per phrase, results
in a preference for longer phrases. A positive weight,
meaning a bonus per phrase, results in a preference for
shorter phrases.
3.2 Word-based Lexicon
We are using relative frequencies to estimate the phrase
translation probabilities. Most of the longer phrases are
seen only once in the training corpus. Therefore, pure
relative frequencies overestimate the probability of those
phrases. To overcome this problem, we use a word-based
lexicon model to smooth the phrase translation probabili-
ties. For a source word f and a target phrase e? = ei2i1 , we
use the following approximation:
p(f |ei2i1) ? 1?
i2?
i=i1
(1? p(f |ei))
This models a disjunctive interaction, also called noisy-
OR gate (Pearl, 1988). The idea is that there are multiple
independent causes ei2i1 that can generate an event f . It
can be easily integrated into the search algorithm. The
corresponding feature function is:
hlex(fJ1 , eI1, S,K) = log
K?
k=1
jk?
j=jk?1+1
p(fj |e?k)
Here, jk and ik denote the final position of phrase number
k in the source and the target sentence, respectively, and
we define j0 := 0 and i0 := 0.
To estimate the single-word based translation probabil-
ities p(f |e), we use smoothed relative frequencies. The
smoothing method we apply is absolute discounting with
interpolation:
p(f |e) = max {N(f, e)? d, 0}N(e) + ?(e) ? ?(f)
This method is well known from language modeling (Ney
et al, 1997). Here, d is the nonnegative discounting pa-
rameter, ?(e) is a normalization constant and ? is the nor-
malized backing-off distribution. To compute the counts,
we use the same word alignment matrix as for the ex-
traction of the bilingual phrases. The symbol N(e) de-
notes the unigram count of a word e and N(f, e) denotes
the count of the event that the target language word e is
aligned to the source language word f . If one occurrence
of e has N > 1 aligned source words, each of them con-
tributes with a count of 1/N . The formula for ?(e) is:
?(e) = 1N(e)
?
? ?
f :N(f,e)>d
d+
?
f :N(f,e)?d
N(f, e)
?
?
= 1N(e)
?
f
min{d,N(f, e)}
This formula is a generalization of the one typically used
in publications on language modeling. This generaliza-
tion is necessary, because the lexicon counts may be frac-
tional whereas in language modeling typically integer
counts are used. Additionally, we want to allow discount-
ing values d greater than one. One effect of the discount-
ing parameter d is that all lexicon entries with a count
less than d are discarded and the freed probability mass
is redistributed among the other entries.
As backing-off distribution ?(f), we consider two al-
ternatives. The first one is a uniform distribution and the
second one is the unigram distribution:
?1(f) = 1Vf (14)
?2(f) = N(f)?
f ? N(f ?)
(15)
Here, Vf denotes the vocabulary size of the source lan-
guage and N(f) denotes the unigram count of a source
word f .
4 Monotone Search
The monotone search can be efficiently computed with
dynamic programming. The resulting complexity is lin-
ear in the sentence length. We present the formulae for a
bigram language model. This is only for notational con-
venience. The generalization to a higher order language
model is straightforward. For the maximization problem
in (11), we define the quantity Q(j, e) as the maximum
probability of a phrase sequence that ends with the lan-
guage word e and covers positions 1 to j of the source
sentence. Q(J + 1, $) is the probability of the opti-
mum translation. The $ symbol is the sentence boundary
marker. We obtain the following dynamic programming
recursion.
Q(0, $) = 1
Q(j, e) = max
e?,e?,
j?M?j?<j
{
p(f jj?+1|e?) ? p(e?|e?) ?Q(j?, e?)
}
Q(J + 1, $) = max
e?
{Q(J, e?) ? p($|e?)}
Here, M denotes the maximum phrase length in the
source language. During the search, we store back-
pointers to the maximizing arguments. After perform-
ing the search, we can generate the optimum translation.
The resulting algorithm has a worst-case complexity of
O(J ?M ? Ve ? E). Here, Ve denotes the vocabulary size
of the target language and E denotes the maximum num-
ber of phrase translation candidates for a source language
phrase. Using efficient data structures and taking into ac-
count that not all possible target language phrases can oc-
cur in translating a specific source language sentence, we
can perform a very efficient search.
This monotone algorithm is especially useful for lan-
guage pairs that have a similar word order, e.g. Spanish-
English or French-English.
5 Corpus Statistics
In the following sections, we will present results on three
tasks: Verbmobil, Xerox and Canadian Hansards. There-
fore, we will show the corpus statistics for each of these
tasks in this section. The training corpus (Train) of each
task is used to train a word alignment and then extract the
bilingual phrases and the word-based lexicon. The re-
maining free parameters, e.g. the model scaling factors,
are optimized on the development corpus (Dev). The re-
sulting system is then evaluated on the test corpus (Test).
Verbmobil Task. The first task we will present re-
sults on is the German?English Verbmobil task (Wahlster,
2000). The domain of this corpus is appointment schedul-
ing, travel planning, and hotel reservation. It consists of
transcriptions of spontaneous speech. Table 1 shows the
corpus statistics of this task.
Xerox task. Additionally, we carried out experiments
on the Spanish?English Xerox task. The corpus consists
of technical manuals. This is a rather limited domain task.
Table 2 shows the training, development and test corpus
statistics.
Canadian Hansards task. Further experiments were
carried out on the French?English Canadian Hansards
Table 1: Statistics of training and test corpus for the Verb-
mobil task (PP=perplexity).
German English
Train Sentences 58 073
Words 519 523 549 921
Vocabulary 7 939 4 672
Dev Sentences 276
Words 3 159 3 438
Trigram PP - 28.1
Test Sentences 251
Words 2 628 2 871
Trigram PP - 30.5
Table 2: Statistics of training and test corpus for the Xe-
rox task (PP=perplexity).
Spanish English
Train Sentences 55 761
Words 752 606 665 399
Vocabulary 11 050 7 956
Dev Sentences 1012
Words 15 957 14 278
Trigram PP ? 28.1
Test Sentences 1125
Words 10 106 8 370
Trigram PP ? 48.3
task. This task contains the proceedings of the Cana-
dian parliament. About 3 million parallel sentences of
this bilingual data have been made available by the Lin-
guistic Data Consortium (LDC). Here, we use a subset
of the data containing only sentences with a maximum
length of 30 words. This task covers a large variety of
topics, so this is an open-domain corpus. This is also re-
flected by the large vocabulary size. Table 3 shows the
training and test corpus statistics.
6 Degree of Monotonicity
In this section, we will investigate the effect of the mono-
tonicity constraint. Therefore, we compute how many of
the training corpus sentence pairs can be produced with
the monotone phrase-based search. We compare this to
the number of sentence pairs that can be produced with a
nonmonotone phrase-based search. To make these num-
bers more realistic, we use leaving-one-out. Thus phrases
that are extracted from a specific sentence pair are not
used to check its monotonicity. With leaving-one-out it is
possible that even the nonmonotone search cannot gen-
erate a sentence pair. This happens if a sentence pair
contains a word that occurs only once in the training cor-
pus. All phrases that might produce this singleton are
excluded because of the leaving-one-out principle. Note
Table 3: Statistics of training and test corpus for the
Canadian Hansards task (PP=perplexity).
French English
Train Sentences 1.5M
Words 24M 22M
Vocabulary 100 269 78 332
Dev Sentences 500
Words 9 043 8 195
Trigram PP ? 57.7
Test Sentences 5432
Words 97 646 88 773
Trigram PP ? 56.7
that all these monotonicity consideration are done at the
phrase level. Within the phrases arbitrary reorderings are
allowed. The only restriction is that they occur in the
training corpus.
Table 4 shows the percentage of the training corpus
that can be generated with monotone and nonmonotone
phrase-based search. The number of sentence pairs that
can be produced with the nonmonotone search gives an
estimate of the upper bound for the sentence error rate of
the phrase-based system that is trained on the given data.
The same considerations hold for the monotone search.
The maximum source phrase length for the Verbmobil
task and the Xerox task is 12, whereas for the Canadian
Hansards task we use a maximum of 4, because of the
large corpus size. This explains the rather low coverage
on the Canadian Hansards task for both the nonmonotone
and the monotone search.
For the Xerox task, the nonmonotone search can pro-
duce 75.1% of the sentence pairs whereas the mono-
tone can produce 65.3%. The ratio of the two numbers
measures how much the system deteriorates by using the
monotone search and will be called the degree of mono-
tonicity. For the Xerox task, the degree of monotonicity
is 87.0%. This means the monotone search can produce
87.0% of the sentence pairs that can be produced with
the nonmonotone search. We see that for the Spanish-
English Xerox task and for the French-English Canadian
Hansards task, the degree of monotonicity is rather high.
For the German-English Verbmobil task it is significantly
lower. This may be caused by the rather free word order
in German and the long range reorderings that are neces-
sary to translate the verb group.
It should be pointed out that in practice the monotone
search will perform better than what the preceding esti-
mates indicate. The reason is that we assumed a perfect
nonmonotone search, which is difficult to achieve in prac-
tice. This is not only a hard search problem, but also a
complicated modeling problem. We will see in the next
section that the monotone search will perform very well
on both the Xerox task and the Canadian Hansards task.
Table 4: Degree of monotonicity in the training corpora
for all three tasks (numbers in percent).
Verbmobil Xerox Hansards
nonmonotone 76.3 75.1 59.7
monotone 55.4 65.3 51.5
deg. of mon. 72.6 87.0 86.3
7 Translation Results
7.1 Evaluation Criteria
So far, in machine translation research a single generally
accepted criterion for the evaluation of the experimental
results does not exist. Therefore, we use a variety of dif-
ferent criteria.
? WER (word error rate):
The WER is computed as the minimum number of
substitution, insertion and deletion operations that
have to be performed to convert the generated sen-
tence into the reference sentence.
? PER (position-independent word error rate):
A shortcoming of the WER is that it requires a per-
fect word order. The word order of an acceptable
sentence can be different from that of the target sen-
tence, so that the WER measure alone could be mis-
leading. The PER compares the words in the two
sentences ignoring the word order.
? BLEU score:
This score measures the precision of unigrams, bi-
grams, trigrams and fourgrams with respect to a ref-
erence translation with a penalty for too short sen-
tences (Papineni et al, 2001). BLEU measures ac-
curacy, i.e. large BLEU scores are better.
? NIST score:
This score is similar to BLEU. It is a weighted n-
gram precision in combination with a penalty for
too short sentences (Doddington, 2002). NIST mea-
sures accuracy, i.e. large NIST scores are better.
For the Verbmobil task, we have multiple references
available. Therefore on this task, we compute all the pre-
ceding criteria with respect to multiple references. To
indicate this, we will precede the acronyms with an m
(multiple) if multiple references are used. For the other
two tasks, only single references are used.
7.2 Translation Systems
In this section, we will describe the systems that were
used. On the one hand, we have three different variants
of the single-word based model IBM4. On the other hand,
we have two phrase-based systems, namely the alignment
templates and the one described in this work.
Single-Word Based Systems (SWB). First, there is a
monotone search variant (Mon) that translates each word
of the source sentence from left to right. The second vari-
ant allows reordering according to the so-called IBM con-
straints (Berger et al, 1996). Thus up to three words
may be skipped and translated later. This system will
be denoted by IBM. The third variant implements spe-
cial German-English reordering constraints. These con-
straints are represented by a finite state automaton and
optimized to handle the reorderings of the German verb
group. The abbreviation for this variant is GE. It is only
used for the German-English Verbmobil task. This is just
an extremely brief description of these systems. For de-
tails, see (Tillmann and Ney, 2003).
Phrase-Based System (PB). For the phrase-based sys-
tem, we use the following feature functions: a trigram
language model, the phrase translation model and the
word-based lexicon model. The latter two feature func-
tions are used for both directions: p(f |e) and p(e|f).
Additionally, we use the word and phrase penalty fea-
ture functions. The model scaling factors are optimized
on the development corpus with respect to mWER sim-
ilar to (Och, 2003). We use the Downhill Simplex al-
gorithm from (Press et al, 2002). We do not perform
the optimization on N -best lists but we retranslate the
whole development corpus for each iteration of the op-
timization algorithm. This is feasible because this system
is extremely fast. It takes only a few seconds to translate
the whole development corpus for the Verbmobil task and
the Xerox task; for details see Section 8. In the experi-
ments, the Downhill Simplex algorithm converged after
about 200 iterations. This method has the advantage that
it is not limited to the model scaling factors as the method
described in (Och, 2003). It is also possible to optimize
any other parameter, e.g. the discounting parameter for
the lexicon smoothing.
Alignment Template System (AT). The alignment
template system (Och et al, 1999) is similar to the sys-
tem described in this work. One difference is that the
alignment templates are not defined at the word level but
at a word class level. In addition to the word-based tri-
gram model, the alignment template system uses a class-
based fivegram language model. The search algorithm of
the alignment templates allows arbitrary reorderings of
the templates. It penalizes reorderings with costs that are
linear in the jump width. To make the results as compa-
rable as possible, the alignment template system and the
phrase-based system start from the same word alignment.
The alignment template system uses discriminative train-
ing of the model scaling factors as described in (Och and
Ney, 2002).
7.3 Verbmobil Task
We start with the Verbmobil results. We studied smooth-
ing the lexicon probabilities as described in Section 3.2.
The results are summarized in Table 5. We see that the
Table 5: Effect of lexicon smoothing on the translation
performance [%] for the German-English Verbmobil task.
system mWER mPER BLEU NIST
unsmoothed 37.3 21.1 46.6 7.96
uniform 37.0 20.7 47.0 7.99
unigram 38.2 22.3 45.5 7.79
uniform smoothing method improves translation quality.
There is only a minor improvement, but it is consistent
among all evaluation criteria. It is statistically signifi-
cant at the 94% level. The unigram method hurts perfor-
mance. There is a degradation of the mWER of 0.9%. In
the following, all phrase-based systems use the uniform
smoothing method.
The translation results of the different systems are
shown in Table 6. Obviously, the monotone phrase-based
system outperforms the monotone single-word based sys-
tem. The result of the phrase-based system is comparable
to the nonmonotone single-word based search with the
IBM constraints. With respect to the mPER, the PB sys-
tem clearly outperforms all single-word based systems.
If we compare the monotone phrase-based system with
the nonmonotone alignment template system, we see that
the mPERs are similar. Thus the lexical choice of words
is of the same quality. Regarding the other evaluation
criteria, which take the word order into account, the non-
monotone search of the alignment templates has a clear
advantage. This was already indicated by the low degree
of monotonicity on this task. The rather free word order
in German and the long range dependencies of the verb
group make reorderings necessary.
Table 6: Translation performance [%] for the German-
English Verbmobil task (251 sentences).
system variant mWER mPER BLEU NIST
SWB Mon 42.8 29.3 38.0 7.07
IBM 37.1 25.0 47.8 7.84
GE 35.4 25.3 48.5 7.83
PB 37.0 20.7 47.0 7.99
AT 30.3 20.6 56.8 8.57
7.4 Xerox task
The translation results for the Xerox task are shown in
Table 7. Here, we see that both phrase-based systems
clearly outperform the single-word based systems. The
PB system performs best on this task. Compared to the
AT system, the BLEU score improves by 4.1% absolute.
The improvement of the PB system with respect to the
AT system is statistically significant at the 99% level.
Table 7: Translation performance [%] for the Spanish-
English Xerox task (1125 sentences).
System WER PER BLEU NIST
SWB IBM 38.8 27.6 55.3 8.00
PB 26.5 18.1 67.9 9.07
AT 28.9 20.1 63.8 8.76
7.5 Canadian Hansards task
The translation results for the Canadian Hansards task are
shown in Table 8. As on the Xerox task, the phrase-based
systems perform better than the single-word based sys-
tems. The monotone phrase-based system yields even
better results than the alignment template system. This
improvement is consistent among all evaluation criteria
and it is statistically significant at the 99% level.
Table 8: Translation performance [%] for the French-
English Canadian Hansards task (5432 sentences).
System Variant WER PER BLEU NIST
SWB Mon 65.2 53.0 19.8 5.96
IBM 64.5 51.3 20.7 6.21
PB 57.8 46.6 27.8 7.15
AT 61.1 49.1 26.0 6.71
8 Efficiency
In this section, we analyze the translation speed of the
phrase-based translation system. All experiments were
carried out on an AMD Athlon with 2.2GHz. Note that
the systems were not optimized for speed. We used the
best performing systems to measure the translation times.
The translation speed of the monotone phrase-based
system for all three tasks is shown in Table 9. For the
Xerox task, the translation process takes less than 7 sec-
onds for the whole 10K words test set. For the Verbmobil
task, the system is even slightly faster. It takes about 1.6
seconds to translate the whole test set. For the Canadian
Hansards task, the translation process is much slower, but
the average time per sentence is still less than 1 second.
We think that this slowdown can be attributed to the large
training corpus. The system loads only phrase pairs into
memory if the source phrase occurs in the test corpus.
Therefore, the large test corpus size for this task also af-
fects the translation speed.
In Fig. 1, we see the average translation time per sen-
tence as a function of the sentence length. The translation
times were measured for the translation of the 5432 test
sentences of the Canadian Hansards task. We see a clear
linear dependency. Even for sentences of thirty words,
the translation takes only about 1.5 seconds.
Table 9: Translation Speed for all tasks on a AMD Athlon
2.2GHz.
Verbmobil Xerox Hansards
avg. sentence length 10.5 13.5 18.0
seconds / sentence 0.006 0.007 0.794
words / second 1642 1448 22.8
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0  5  10  15  20  25  30
t
i
m
e
sentence lengthFigure 1: Average translation time per sentence as a func-
tion of the sentence length for the Canadian Hansards task
(5432 test sentences).
9 Related Work
Recently, phrase-based translation approaches became
more and more popular. Some examples are the align-
ment template system in (Och et al, 1999; Och and Ney,
2002) that we used for comparison. In (Zens et al, 2002),
a simple phrase-based approach is described that served
as starting point for the system in this work. (Marcu
and Wong, 2002) presents a joint probability model for
phrase-based translation. It does not use the word align-
ment for extracting the phrases, but directly generates a
phrase alignment. In (Koehn et al, 2003), various aspects
of phrase-based systems are compared, e.g. the phrase
extraction method, the underlying word alignment model,
or the maximum phrase length. (Tomas and Casacuberta,
2003) describes a linear interpolation of a phrase-based
and an alignment template-based approach.
10 Conclusions
We described a phrase-based translation approach. The
basic idea of this approach is to remember all bilingual
phrases that have been seen in the word-aligned train-
ing corpus. As refinements of the baseline model, we
described two simple heuristics: the word penalty fea-
ture and the phrase penalty feature. Additionally, we pre-
sented a single-word based lexicon with two smoothing
methods. The model scaling factors were optimized with
respect to the mWER on the development corpus.
We described a highly efficient monotone search al-
gorithm. The worst-case complexity of this algorithm is
linear in the sentence length. This leads to an impressive
translation speed of more than 1000 words per second for
the Verbmobil task and for the Xerox task. Even for the
Canadian Hansards task the translation of sentences of
length 30 takes only about 1.5 seconds.
The described search is monotone at the phrase level.
Within the phrases, there are no constraints on the re-
orderings. Therefore, this method is best suited for lan-
guage pairs that have a similar order at the level of the
phrases learned by the system. Thus, the translation pro-
cess should require only local reorderings. As the exper-
iments have shown, Spanish-English and French-English
are examples of such language pairs. For these pairs,
the monotone search was found to be sufficient. The
phrase-based approach clearly outperformed the single-
word based systems. It showed even better performance
than the alignment template system.
The experiments on the German-English Verbmobil
task outlined the limitations of the monotone search.
As the low degree of monotonicity indicated, reordering
plays an important role on this task. The rather free word
order in German as well as the verb group seems to be dif-
ficult to translate. Nevertheless, when ignoring the word
order and looking at the mPER only, the monotone search
is competitive with the best performing system.
For further improvements, we will investigate the use-
fulness of additional models, e.g. modeling the segmen-
tation probability. Also, slightly relaxing the monotonic-
ity constraint in a way that still allows an efficient search
is of high interest. In spirit of the IBM reordering con-
straints of the single-word based models, we could allow
a phrase to be skipped and to be translated later.
Acknowledgment
This work has been partially funded by the EU project
TransType 2, IST-2001-32091.
References
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra,
J. R. Gillett, A. S. Kehler, and R. L. Mercer. 1996.
Language translation apparatus and method of using
context-based translation models, United States patent,
patent number 5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the Human Lan-
guage Technology Conf. (HLT-NAACL), pages 127?
133, Edmonton, Canada, May/June.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. Conf. on Empirical Methods for Natural Lan-
guage Processing, pages 133?139, Philadelphia, PA,
July.
H. Ney, S. Martin, and F. Wessel. 1997. Statistical lan-
guage modeling using leaving-one-out. In S. Young
and G. Bloothooft, editors, Corpus-Based Methods
in Language and Speech Processing, pages 174?207.
Kluwer.
F. J. Och and H. Ney. 2002. Discriminative training and
maximum entropy models for statistical machine trans-
lation. In Proc. of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 20?28, University of Maryland, Col-
lege Park, MD, June.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers, Inc., San Mateo, CA. Revised
second printing.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
C. Tillmann and H. Ney. 2003. Word reordering and a
dynamic programming beam search algorithm for sta-
tistical machine translation. Computational Linguis-
tics, 29(1):97?133, March.
J. Tomas and F. Casacuberta. 2003. Combining phrase-
based and template-based aligned models in statisti-
cal translation. In Proc. of the First Iberian Conf. on
Pattern Recognition and Image Analysis, pages 1020?
1031, Mallorca, Spain, June.
W. Wahlster, editor. 2000. Verbmobil: Foundations
of speech-to-speech translations. Springer Verlag,
Berlin, Germany, July.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In 25th German Confer-
ence on Artificial Intelligence (KI2002), pages 18?32,
Aachen, Germany, September. Springer Verlag.
Proceedings of NAACL HLT 2007, pages 492?499,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Efficient Phrase-table Representation for Machine Translation with
Applications to Online MT and Speech Translation
Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,ney}@cs.rwth-aachen.de
Abstract
In phrase-based statistical machine transla-
tion, the phrase-table requires a large amount
of memory. We will present an efficient repre-
sentation with two key properties: on-demand
loading and a prefix tree structure for the
source phrases.
We will show that this representation scales
well to large data tasks and that we are able
to store hundreds of millions of phrase pairs
in the phrase-table. For the large Chinese?
English NIST task, the memory requirements
of the phrase-table are reduced to less than
20MB using the new representation with no
loss in translation quality and speed. Addi-
tionally, the new representation is not limited
to a specific test set, which is important for
online or real-time machine translation.
One problem in speech translation is the
matching of phrases in the input word graph
and the phrase-table. We will describe a novel
algorithm that effectively solves this com-
binatorial problem exploiting the prefix tree
data structure of the phrase-table. This algo-
rithm enables the use of significantly larger
input word graphs in a more efficient way re-
sulting in improved translation quality.
1 Introduction
In phrase-based statistical machine translation, a
huge number of source and target phrase pairs
is memorized in the so-called phrase-table. For
medium sized tasks and phrase lengths, these
phrase-tables already require several GBs of mem-
ory or even do not fit at all. If the source text, which
is to be translated, is known in advance, a common
trick is to filter the phrase-table and keep a phrase
pair only if the source phrase occurs in the text. This
filtering is a time-consuming task, as we have to
go over the whole phrase-table. Furthermore, we
have to repeat this filtering step whenever we want
to translate a new source text.
To address these problems, we will use an ef-
ficient representation of the phrase-table with two
key properties: on-demand loading and a prefix tree
structure for the source phrases. The prefix tree
structure exploits the redundancy among the source
phrases. Using on-demand loading, we will load
only a small fraction of the overall phrase-table into
memory. The majority will remain on disk.
The on-demand loading is employed on a per sen-
tence basis, i.e. we load only the phrase pairs that
are required for one sentence into memory. There-
fore, the memory requirements are low, e.g. less than
20MB for the Chin.-Eng. NIST task. Another ad-
vantage of the on-demand loading is that we are able
to translate new source sentences without filtering.
A potential problem is that this on-demand load-
ing might be too slow. To overcome this, we use a
binary format which is a memory map of the internal
representation used during decoding. Additionally,
we load coherent chunks of the tree structure instead
of individual phrases, i.e. we have only few disk ac-
cess operations. In our experiments, the on-demand
loading is not slower than the traditional approach.
As pointed out in (Mathias and Byrne, 2006),
one problem in speech translation is that we have
to match the phrases of our phrase-table against a
word graph representing the alternative ASR tran-
492
scriptions. We will present a phrase matching algo-
rithm that effectively solves this combinatorial prob-
lem exploiting the prefix tree data structure of the
phrase-table. This algorithm enables the use of sig-
nificantly larger input word graphs in a more effi-
cient way resulting in improved translation quality.
The remaining part is structured as follows: we
will first discuss related work in Sec. 2. Then, in
Sec. 3, we will describe the phrase-table represen-
tation. Afterwards, we will present applications in
speech translation and online MT in Sec. 4 and 5,
respectively. Experimental results will be presented
in Sec. 6 followed by the conclusions in Sec. 7.
2 Related Work
(Callison-Burch et al, 2005) and (Zhang and Vogel,
2005) presented data structures for a compact rep-
resentation of the word-aligned bilingual data, such
that on-the-fly extraction of long phrases is possi-
ble. The motivation in (Callison-Burch et al, 2005)
is that there are some long source phrases in the
test data that also occur in the training data. How-
ever, the more interesting question is if these long
phrases really help to improve the translation qual-
ity. We have investigated this and our results are in
line with (Koehn et al, 2003) showing that the trans-
lation quality does not improve if we utilize phrases
beyond a certain length. Furthermore, the suffix ar-
ray data structure of (Callison-Burch et al, 2005) re-
quires a fair amount of memory, about 2GB in their
example, whereas our implementation will use only
a tiny amount of memory, e.g. less than 20MB for
the large Chinese-English NIST task.
3 Efficient Phrase-table Representation
In this section, we will describe the proposed rep-
resentation of the phrase-table. A prefix tree, also
called trie, is an ordered tree data structure used to
store an associative array where the keys are symbol
sequences. In the case of phrase-based MT, the keys
are source phrases, i.e. sequences of source words
and the associated values are the possible transla-
tions of these source phrases. In a prefix tree, all
descendants of any node have a common prefix,
namely the source phrase associated with that node.
The root node is associated with the empty phrase.
The prefix tree data structure is quite common in
automatic speech translation. There, the lexicon, i.e.
the mapping of phoneme sequences to words, is usu-
ally organized as a prefix tree (Ney et al, 1992).
We convert the list of source phrases into a pre-
fix tree and, thus, exploit that many of them share
the same prefix. This is illustrated in Fig. 1 (left).
Within each node of the tree, we store a sorted ar-
ray of possible successor words along with pointers
to the corresponding successor nodes. Additionally,
we store a pointer to the possible translations.
One property of the tree structure is that we can
efficiently access the successor words of a given pre-
fix. This will be a key point to achieve an efficient
phrase matching algorithm in Sec. 4. When looking
for a specific successor word, we perform a binary
search in the sorted array. Alternatively, we could
use hashing to speed up this lookup. We have chosen
an array representation as this can be read very fast
from disk. Additionally, with the exception of the
root node, the branching factor of the tree is small,
i.e. the potential benefit from hashing is limited. At
the root node, however, the branching factor is close
to the vocabulary size of the source language, which
can be large. As we store the words internally as in-
tegers and virtually all words occur as the first word
of some phrase, we can use the integers directly as
the position in the array of the root node. Hence, the
search for the successors at the root node is a simple
table lookup with direct access, i.e. in O(1).
If not filtered for a specific test set, the phrase-
table becomes huge even for medium-sized tasks.
Therefore, we store the tree structure on disk
and load only the required parts into memory on-
demand. This is illustrated in Fig. 1 (right). Here,
we show the matching phrases for the source sen-
tence ?c a a c?, where the matching phrases are set in
bold and the phrases that are loaded into memory are
set in italics. The dashed part of the tree structure is
not loaded into memory. Note that some nodes of the
tree are loaded even if there is no matching phrase in
that node. These are required to actually verify that
there is no matching phrase. An example is the ?bc?
node in the lower right part of the figure. This node
is loaded to check if the phrase ?c a a? occurs in the
phrase-table. The translations, however, are loaded
only for matching source phrases.
In the following sections, we will describe appli-
cations of this phrase-table representation for speech
translation and online MT.
4 Speech Translation
In speech translation, the input to the MT system is
not a sentence, but a word graph representing alter-
493
a b a c
a a b b
a b b c
a b c c
b c a
b a c a b
b a a c a c
b a b
a
b
c
a
b
a
b
c
a
c
a
b
c
b
c
a b a c
a a b b
a b b c
a b c c
b c a
b a c a b
b a a c a c
b a b
a
b
c
a
b
a
b
c
a
c
a
b
c
b
c
Figure 1: Illustration of the prefix tree. Left: list of source phrases and the corresponding prefix tree. Right:
list of matching source phrases for sentence ?c a a c? (bold phrases match, phrases in italics are loaded in
memory) and the corresponding partially loaded prefix tree (the dashed part is not in memory).
jffifl
fi
sGj,nffifl
fi
sGj,1ffifl
fi
sGj,Njffifl
fi
-  
  
  
 
@@@@@@@R
fGj,1
fGj,n
fGj,Nj
.
.
.
.
.
.
.
.
.
.
.
.
Figure 2: Illustration for graph G: node j with suc-
cessor nodes sGj,1, ..., s
G
j,n..., s
G
j,Nj
and corresponding
edge labels fGj,1, ..., f
G
j,n, ..., f
G
j,Nj
.
native ASR transcriptions. As pointed out in (Math-
ias and Byrne, 2006), one problem in speech trans-
lation is that we have to match the phrases of our
phrase-table against the input word graph. This re-
sults in a combinatorial problem as the number of
phrases in a word graph increases exponentially with
the phrase length.
4.1 Problem Definition
In this section, we will introduce the notation and
state the problem of matching source phrases of
an input graph G and the phrase-table, represented
as prefix tree T . The input graph G has nodes
1, ..., j, ..., J . The outgoing edges of a graph node
j are numbered with 1, ..., n, ..., Nj , i.e. an edge in
the input graph is identified by a pair (j, n). The
source word labeling the nth outgoing edge of graph
node j is denoted as fGj,n and the successor node of
this edge is denoted as sGj,n ? {1, ..., J}. This nota-
tion is illustrated in Fig. 2.
We use a similar notation for the prefix tree T with
nodes 1, ..., k, ...,K. The outgoing edges of a tree
node k are numbered with 1, ...,m, ...,Mk, i.e. an
edge in the prefix tree is identified by a pair (k,m).
The source word labeling the mth outgoing edge of
tree node k is denoted as fTk,m and the successor
node of this edge is denoted as sTk,m ? {1, ...,K}.
Due to the tree structure, the successor nodes of a
tree node k are all distinct:
sTk,m = s
T
k,m? ? m = m
? (1)
Let k0 denote the root node of the prefix tree and
let f?k denote the prefix that leads to tree node k.
Furthermore, we define E(k) as the set of possible
translations of the source phrase f?k. These are the
entries of the phrase-table, i.e.
E(k) =
{
e?
?
?
? p(e?|f?k) > 0
}
(2)
We will need similar symbols for the input graph.
Therefore, we define F (j?, j) as the set of source
phrases of all paths from graph node j? to node j, or
formally:
F (j?, j) =
{
f?
?
?
? ?(ji, ni)Ii=1 : f? = f
G
j1,n1 , ..., f
G
jI ,nI
? j1 = j? ?
?I?1
i=1 s
G
ji,ni = ji+1 ? sjI ,nI = j
}
Here, the conditions ensure that the edge sequence
(ji, ni)Ii=1 is a proper path from node j
? to node j
in the input graph and that the corresponding source
phrase is f? = fGj1,n1 , ..., f
G
jI ,nI . This definition can
be expressed in a recursive way; the idea is to extend
the phrases of the predecessor nodes by one word:
F (j?, j) =
?
(j??,n):sG
j??,n
=j
{
f?fGj??,n
?
?
?f? ? F (j?, j??)
}
(3)
494
Here, the set is expressed as a union over all in-
bound edges (j??, n) of node j. We concatenate each
source phrase f? that ends at the start node of such
an edge, i.e. f? ? F (j?, j??), with the corresponding
edge label fGj??,n. Additionally, we define E(j
?, j)
as the set of possible translations of all paths from
graph node j? to graph node j, or formally:
E(j?, j) =
{
e?
?
?
? ?f? ? F (j?, j) : p(e?|f?) > 0
}
(4)
=
?
k:f?k?F (j?,j)
E(k) (5)
=
?
(j??,n):sG
j??,n
=j
?
k:f?k?F (j
?,j??)
m:fG
j??,n
=fTk,m
E(sTk,m) (6)
Here, the definition was first rewritten using Eq. 2
and then using Eq. 3. Again, the set is expressed
recursively as a union over the inbound edges. For
each inbound edge (j??, n), the inner union verifies
that there exists a corresponding edge (k,m) in the
prefix tree with the same label, i.e. fGj??,n = f
T
k,m.
Our goal is to find all non-empty sets of trans-
lation options E(j?, j). The naive approach would
be to enumerate all paths in the input graph from
node j? to node j, then lookup the corresponding
source phrase in the phrase-table and add the trans-
lations, if there are any, to the set of translation
options E(j?, j). This solution has some obvious
weaknesses: the number of paths between two nodes
is typically huge and the majority of the correspond-
ing source phrases do not occur in the phrase-table.
We omitted the probabilities for notational conve-
nience. The extensions are straightforward. Note
that we store only the target phrases e? in the set
of possible translations E(j?, j) and not the source
phrases f? . This is based on the assumption that the
models which are conditioned on the source phrase
f? are independent of the context outside the phrase
pair (f? , e?). This assumption holds for the standard
phrase and word translation models. Thus, we have
to keep only the target phrase with the highest prob-
ability. It might be violated by lexicalized distor-
tion models (dependent on the configuration); in that
case we have to store the source phrase along with
the target phrase and the probability, which is again
straightforward.
4.2 Algorithm
The algorithm for matching the source phrases of the
input graph G and the prefix tree T is presented in
Figure 3: Algorithm phrase-match for match-
ing source phrases of input graph G and prefix tree
T . Input: graph G, prefix tree T , translation options
E(k) for all tree nodes k; output: translation options
E(j?, j) for all graph nodes j? and j.
0 FOR j? = 1 TO J DO
1 stack.push(j?, k0)
2 WHILE not stack.empty() DO
3 (j, k) = stack.pop()
4 E(j?, j) = E(j?, j) ? E(k)
5 FOR n = 1 TO Nj DO
6 IF (fGj,n = )
7 THEN stack.push(sGj,n, k)
8 ELSE IF (?m : fGj,n = f
T
k,m)
9 THEN stack.push(sGj,n, s
T
k,m)
Fig. 3. Starting from a graph node j?, we explore the
part of the graph which corresponds to known source
phrase prefixes and generate the sets E(j?, j) incre-
mentally based on Eq. 6. The intermediate states
are represented as pairs (j, k) meaning that there ex-
ists a path in the input graph from node j? to node j
which is labeled with the source phrase f?k, i.e. the
source phrase that leads to node k in the prefix tree.
These intermediate states are stored on a stack. After
the initialization in line 1, the main loop starts. We
take one item from the stack and update the transla-
tion options E(j?, j) in line 4. Then, we loop over
all outgoing edges of the current graph node j. For
each edge, we first check if the edge is labeled with
an  in line 6. In this special case, we go to the suc-
cessor node in the input graph sGj,n, but remain in the
current node k of the prefix tree. In the regular case,
i.e. the graph edge label is a regular word, we check
in line 8 if the current prefix tree node k has an out-
going edge labeled with that word. If such an edge
is found, we put a new state on the stack with the
two successor nodes in the input graph sGj,n and the
prefix tree sTk,m, respectively.
4.3 Computational Complexity
In this section, we will analyze the computational
complexity of the algorithm. The computational
complexity of lines 5-9 is in O(Nj logMk), i.e. it
depends on the branching factors of the input graph
and the prefix tree. Both are typically small. An ex-
ception is the branching factor of the root node k0 of
the prefix tree, which can be rather large, typically it
is the vocabulary size of the source language. But,
as described in Sec. 3, we can access the successor
495
nodes of the root node of the prefix tree in O(1), i.e.
in constant time. So, if we are at the root node of the
prefix tree, the computational complexity of lines 5-
9 is inO(Nj). Using hashing at the interior nodes of
the prefix tree would result in a constant time lookup
at these nodes as well. Nevertheless, the sorted ar-
ray implementation that we chose has the advantage
of faster loading from disk which seems to be more
important in practice.
An alternative interpretation of lines 5-9 is that we
have to compute the intersection of the two sets fGj
and fTk , with
fGj =
{
fGj,n
?
? n = 1, ..., Nj
}
(7)
fTk =
{
fTk,m
?
?m = 1, ...,Mk
}
. (8)
Assuming both sets are sorted, this could be done in
linear time, i.e. in O(Nj + Mk). In our case, only
the edges in the prefix tree are sorted. Obviously, we
could sort the edges in the input graph and then ap-
ply the linear algorithm, resulting in an overall com-
plexity of O(Nj logNj + Mk). As the algorithm
visits nodes multiple times, we could do even better
by sorting all edges of the graph during the initial-
ization. Then, we could always apply the linear time
method. On the other hand, it is unclear if this pays
off in practice and an experimental comparison has
to be done which we will leave for future work.
The overall complexity of the algorithm depends
on how many phrases of the input graph occur in the
phrase-table. In the worst case, i.e. if all phrases oc-
cur in the phrase-table, the described algorithm is
not more efficient than the naive algorithm which
simply enumerates all phrases. Nevertheless, this
does not happen in practice and we observe an ex-
ponential speed up compared to the naive algorithm,
as will be shown in Sec. 6.3.
5 Online Machine Translation
Beside speech translation, the presented phrase-
table data structure has other interesting applica-
tions. One of them is online MT, i.e. an MT sys-
tem that is able to translate unseen sentences with-
out significant delay. These online MT systems are
typically required if there is some interaction with
human users, e.g. if the MT system acts as an in-
terpreter in a conversation, or in real-time systems.
This situation is different from the usual research
environment where typically a fair amount of time
is spent to prepare the MT system to translate a cer-
tain set of source sentences. In the research scenario,
Table 1: NIST task: corpus statistics.
Chinese English
Train Sentence pairs 7M
Running words 199M 213M
Vocabulary size 222K 351K
Test 2002 Sentences 878 3 512
Running words 25K 105K
2005 Sentences 1 082 4 328
Running words 33K 148K
this preparation usually pays off as the same set of
sentences is translated multiple times. In contrast,
an online MT system translates each sentence just
once. One of the more time-consuming parts of this
preparation is the filtering of the phrase-table. Us-
ing the on-demand loading technique we described
in Sec. 3, we can avoid the filtering step and di-
rectly translate the source sentence. An additional
advantage is that we load only small parts of the full
phrase-table into memory. This reduces the mem-
ory requirements significantly, e.g. for the Chinese?
English NIST task, the memory requirement of the
phrase-table is reduced to less than 20MB using on-
demand loading. This makes the MT system usable
on devices with limited hardware resources.
6 Experimental Results
6.1 Translation System
For the experiments, we use a state-of-the-art
phrase-based statistical machine translation system
as described in (Zens and Ney, 2004). We use a
log-linear combination of several models: a four-
gram language model, phrase-based and word-based
translation models, word, phrase and distortion
penalty and a lexicalized distortion model. The
model scaling factors are optimized using minimum
error rate training (Och, 2003).
6.2 Empirical Analysis for a Large Data Task
In this section, we present an empirical analysis of
the described data structure for the large data track
of the Chinese-English NIST task. The corpus statis-
tics are shown in Tab. 1.
The translation quality is measured using two ac-
curacy measures: the BLEU and the NIST score.
Additionally, we use the two error rates: the word
error rate (WER) and the position-independent word
error rate (PER). These evaluation criteria are com-
puted with respect to four reference translations.
In Tab. 2, we present the translation quality as a
496
Table 2: NIST task: translation quality as a function of the maximum source phrase length.
src NIST 2002 set (dev) NIST 2005 set (test)
len WER[%] PER[%] BLEU[%] NIST WER[%] PER[%] BLEU[%] NIST
1 71.9 46.8 27.07 8.37 78.0 49.0 23.11 7.62
2 62.4 41.2 34.36 9.39 68.5 42.2 30.32 8.74
3 62.0 41.1 34.89 9.33 67.7 42.1 30.90 8.74
4 61.7 41.1 35.05 9.27 67.6 41.9 30.99 8.75
5 61.8 41.2 34.95 9.25 67.6 41.9 30.93 8.72
? 61.8 41.2 34.99 9.25 67.5 41.8 30.90 8.73
Table 3: NIST task: phrase-table statistics.
src number of distinct avg. tgt
len src phrases src-tgt pairs candidates
1 221 505 17 456 415 78.8
2 5 000 041 39 436 617 7.9
3 20 649 699 58 503 904 2.8
4 31 383 549 58 436 271 1.9
5 32 679 145 51 255 866 1.6
total 89 933 939 225 089 073 2.5
function of the maximum source phrase length. We
observe a large improvement when going beyond
length 1, but this flattens out very fast. Using phrases
of lengths larger than 4 or 5 does not result in fur-
ther improvement. Note that the minor differences
in the evaluation results for length 4 and beyond are
merely statistical noise. Even a length limit of 3, as
proposed by (Koehn et al, 2003), would result in
almost optimal translation quality. In the following
experiments on this task, we will use a limit of 5 for
the source phrase length.
In Tab. 3, we present statistics about the extracted
phrase pairs for the Chinese?English NIST task as
a function of the source phrase length, in this case
for length 1-5. The phrases are not limited to a spe-
cific test set. We show the number of distinct source
phrases, the number of distinct source-target phrase
pairs and the average number of target phrases (or
translation candidates) per source phrase. In the ex-
periments, we limit the number of translation can-
didates per source phrase to 200. We store a to-
tal of almost 90 million distinct source phrases and
more than 225 million distinct source-target phrase
pairs in the described data structure. Obviously, it
would be infeasible to load this huge phrase-table
completely into memory. Nevertheless, using on-
demand loading, we are able to utilize all these
phrase pairs with minimal memory usage.
In Fig. 4, we show the memory usage of the de-
scribed phrase-table data structure per sentence for
0 20 40 60 80 100percentage of test set6
8
10
12
14
16
18
20
mem
ory u
sage
 [Me
gaBy
te]
Figure 4: NIST task: phrase-table memory usage
per sentence (sorted).
the NIST 2002 test set. The sentences were sorted
according to the memory usage. The maximum
amount of memory for the phrase-table is 19MB;
for more than 95% of the sentences no more than
15MB are required. Storing all phrase pairs for this
test set in memory requires about 1.7GB of mem-
ory, i.e. using the described data structures, we not
only avoid the limitation to a specific test set, but we
also reduce the memory requirements by about two
orders of a magnitude.
Another important aspect that should be consid-
ered is translation speed. In our experiments, the
described data structure is not slower than the tradi-
tional approach. We attribute this to the fact that we
use a binary format that is a memory map of the data
structure used internally and that we load the data in
rather large, coherent chunks. Additionally, there is
virtually no initialization time for the phrase-table
which decreases the overhead of parallelization and
therefore speeds up the development cycle.
6.3 Speech Translation
The experiments for speech translation were con-
ducted on the European Parliament Plenary Sessions
(EPPS) task. This is a Spanish-English speech-to-
speech translation task collected within the TC-Star
497
Table 4: EPPS task: corpus statistics.
Train Spanish English
Sentence pairs 1.2 M
Running words 31 M 30 M
Vocabulary size 140 K 94 K
Test confusion networks Full Pruned
Sentences 1 071
Avg. length 23.6
Avg. / max. depth 2.7 / 136 1.3 / 11
Avg. number of paths 1075 264K
project. The training corpus statistics are presented
in Tab. 4. The phrase-tables for this task were kindly
provided by ITC-IRST.
We evaluate the phrase-match algorithm in
the context of confusion network (CN) decoding
(Bertoldi and Federico, 2005), which is one ap-
proach to speech translation. CNs (Mangu et al,
2000) are interesting for MT because the reordering
can be done similar to single best input. For more
details on CN decoding, please refer to (Bertoldi et
al., 2007). Note that the phrase-match algo-
rithm is not limited to CNs, but can work on arbi-
trary word graphs.
Statistics of the CNs are also presented in Tab. 4.
We distinguish between the full CNs and pruned
CNs. The pruning parameters were chosen such that
the resulting CNs are similar in size to the largest
ones in (Bertoldi and Federico, 2005). The average
depth of the full CNs, i.e. the average number of al-
ternatives per position, is about 2.7 words whereas
the maximum is as high as 136 alternatives.
In Fig. 5, we present the average number of
phrase-table look-ups for the full EPPS CNs as a
function of the source phrase length. The curve ?CN
total? represents the total number of source phrases
in the CNs for a given length. This is the number
of phrase-table look-ups using the naive algorithm.
Note the exponential growth with increasing phrase
length. Therefore, the naive algorithm is only appli-
cable for very short phrases and heavily pruned CNs,
as e.g. in (Bertoldi and Federico, 2005).
The curve ?CN explored? is the number of phrase-
table look-ups using the phrase-match algo-
rithm described in Fig. 3. We do not observe the
exponential explosion as for the naive algorithm.
Thus, the presented algorithm effectively solves the
combinatorial problem of matching phrases of the
input CNs and the phrase-table. For comparison,
we plotted also the number of look-ups using the
phrase-match algorithm in the case of single-
0 2 4 6 8 10 12 14source phrase length
0.1
1
10
100
1000
10000
100000
1000000
10000000
100000000
phra
se ta
ble l
ook-
ups CN totalCN exploredsingle-best explored
Figure 5: EPPS task: avg. number of phrase-table
look-ups per sentence as a function of the source
phrase length.
Table 5: EPPS task: translation quality and time for
different input conditions (CN=confusion network,
time in seconds per sentence).
Input type BLEU[%] Time [sec]
Single best 37.6 2.7
CN pruned 38.5 4.8
full 38.9 9.2
best input, labeled ?single-best explored?. The maxi-
mum phrase length for these experiments is seven.
For CN input, this length can be exceeded as the
CNs may contain -transitions.
In Tab. 5, we present the translation results and
the translation times for different input conditions.
We observe a significant improvement in translation
quality as more ASR alternatives are taken into ac-
count. The best results are achieved for the full
CNs. On the other hand, the decoding time in-
creases only moderately. Using the new algorithm,
the ratio of the time for decoding the CNs and the
time for decoding the single best input is 3.4 for the
full CNs and 1.8 for the pruned CNs. In previous
work (Bertoldi and Federico, 2005), the ratio for the
pruned CNs was about 25 and the full CNs could not
be handled.
To summarize, the presented algorithm has two
main advantages for speech translation: first, it
enables us to utilize large CNs, which was pro-
hibitively expensive beforehand and second, the ef-
ficiency is improved significantly.
Whereas the previous approaches required care-
ful pruning of the CNs, we are able to utilize the un-
pruned CNs. Experiments on other tasks have shown
that even larger CNs are unproblematic.
498
7 Conclusions
We proposed an efficient phrase-table data structure
which has two key properties:
1. On-demand loading.
We are able to store hundreds of millions of
phrase pairs and require only a very small
amount of memory during decoding, e.g. less
than 20MB for the Chinese-English NIST task.
This enables us to run the MT system on devices
with limited hardware resources or alternatively
to utilize the freed memory for other models. Ad-
ditionally, the usual phrase-table filtering is obso-
lete, which is important for online MT systems.
2. Prefix tree data structure.
Utilizing the prefix tree structure enables us to ef-
ficiently match source phrases against the phrase-
table. This is especially important for speech
translation where the input is a graph represent-
ing a huge number of alternative sentences. Us-
ing the novel algorithm, we are able to handle
large CNs, which was prohibitively expensive
beforehand. This results in more efficient decod-
ing and improved translation quality.
We have shown that this data structure scales very
well to large data tasks like the Chinese-English
NIST task. The implementation of the described
data structure as well as the phrase-match al-
gorithm for confusion networks is available as open
source software in the MOSES toolkit1.
Not only standard phrase-based systems can ben-
efit from this data structure. It should be rather
straightforward to apply this data structure as well as
the phrase-match algorithm to the hierarchical
approach of (Chiang, 2005). As the number of rules
in this approach is typically larger than the number
of phrases in a standard phrase-based system, the
gains should be even larger.
The language model is another model with high
memory requirements. It would be interesting to in-
vestigate if the described techniques and data struc-
tures are applicable for reducing the memory re-
quirements of language models.
Some aspects of the phrase-match algorithm
are similar to the composition of finite-state au-
tomata. An efficient implementation of on-demand
loading (not only on-demand computation) for a
1http://www.statmt.org/moses
finite-state toolkit would make the whole range of
finite-state operations applicable to large data tasks.
Acknowledgments
This material is partly based upon work supported by the
DARPA under Contract No. HR0011-06-C-0023, and was
partly funded by the European Union under the integrated
project TC-STAR (IST-2002-FP6-506738, http://www.tc-
star.org). Additionally, we would like to thank all group
members of the JHU 2006 summer research workshop Open
Source Toolkit for Statistical Machine Translation.
References
N. Bertoldi and M. Federico. 2005. A new decoder for spo-
ken language translation based on confusion networks. In
Proc. IEEE Automatic Speech Recognition and Understand-
ing Workshop, pages 86?91, Mexico, November/December.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech trans-
lation by confusion networks decoding. In Proc. IEEE
Int. Conf. on Acoustics, Speech, and Signal Processing
(ICASSP), Honolulu, Hawaii, April.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005. Scal-
ing phrase-based statistical machine translation to larger cor-
pora and longer phrases. In Proc. 43rd Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages 255?262,
Ann Arbor, MI, June.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In Proc. 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages 263?
270, Ann Arbor, MI, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. Human Language Technology
Conf. / North American Chapter of the Assoc. for Compu-
tational Linguistics Annual Meeting (HLT-NAACL), pages
127?133, Edmonton, Canada, May/June.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373?400, October.
L. Mathias and W. Byrne. 2006. Statistical phrase-based
speech translation. In Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1, pages
561?564, Toulouse, France, May.
H. Ney, R. Haeb-Umbach, B. H. Tran, and M. Oerder. 1992.
Improvements in beam search for 10000-word continuous
speech recognition. In Proc. IEEE Int. Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1, pages
9?12, San Francisco, CA, March.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. 41st Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan, July.
R. Zens and H. Ney. 2004. Improvements in phrase-based sta-
tistical machine translation. In Proc.Human Language Tech-
nology Conf. / North American Chapter of the Assoc. for
Computational Linguistics Annual Meeting (HLT-NAACL),
pages 257?264, Boston, MA, May.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-phrase
alignment model for arbitrarily long phrases and large cor-
pora. In Proc. 10th Annual Conf. of the European Assoc. for
Machine Translation (EAMT), pages 294?301, Budapest,
Hungary, May.
499
Proceedings of NAACL HLT 2007, Companion Volume, pages 57?60,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Are Very Large N-best Lists Useful for SMT?
Sas?a Hasan, Richard Zens, Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{hasan,zens,ney}@cs.rwth-aachen.de
Abstract
This paper describes an efficient method
to extract large n-best lists from a word
graph produced by a statistical machine
translation system. The extraction is based
on the k shortest paths algorithm which
is efficient even for very large k. We
show that, although we can generate large
amounts of distinct translation hypothe-
ses, these numerous candidates are not
able to significantly improve overall sys-
tem performance. We conclude that large
n-best lists would benefit from better dis-
criminating models.
1 Introduction
This paper investigates the properties of large n-
best lists in the context of statistical machine trans-
lation (SMT). We present a method that allows for
fast extraction of very large n-best lists based on
the k shortest paths algorithm by (Eppstein, 1998).
We will argue that, despite being able to generate a
much larger amount of hypotheses than previously
reported in the literature, there is no significant gain
of such a method in terms of translation quality.
In recent years, phrase-based approaches evolved
as the dominating method for feasible machine
translation systems. Many research groups use a de-
coder based on a log-linear approach incorporating
phrases as main paradigm (Koehn et al, 2003). As a
by-product of the decoding process, one can extract
n-best translations from a word graph and use these
fully generated hypotheses for additional reranking.
In the past, several groups report on using n-best
lists with n ranging from 1 000 to 10 000. The ad-
vantage of n-best reranking is clear: we can apply
complex reranking techniques, based e.g. on syntac-
tic analyses of the candidates or using huge addi-
tional language models, since the whole sentence is
already generated. During the generation process,
these models would either need hard-to-implement
algorithms or large memory requirements.
1.1 Related work
The idea of n-best list extraction from a word graph
for SMT was presented in (Ueffing et al, 2002). In
(Zens and Ney, 2005), an improved method is re-
ported that overcomes some shortcomings, such as
duplicate removal by determinization of the word
graph (represented as a weighted finite state automa-
ton) and efficient rest-cost estimation with linear
time complexity.
There are several research groups that use a two-
pass approach in their MT systems. First, they gen-
erate n-best translation hypotheses with the decoder.
Second, they apply additional models to the out-
put and rerank the candidates (see e.g. (Chen et al,
2006)).
Syntactic features were investigated in (Och et al,
2004) with moderate success. Although complex
models, such as features based on shallow parsing or
treebank-based syntactic analyses, were applied to
the n-best candidates, the ?simpler? ones were more
promising (e.g. IBM model 1 on sentence-level).
In the following section 2, we describe our SMT
system and explain how an improved n-best extrac-
tion method is capable of generating a very large
number of distinct candidates from the word graph.
In section 3, we show our experiments related to
n-best list reranking with various sizes and the cor-
responding performance in terms of MT evaluation
measures. Finally, we discuss the results in section 4
and give some conclusive remarks.
57
2 Generating N-best lists
We use a phrase-based SMT system (Mauser et al,
2006) and enhance the n-best list extraction with
Eppstein?s k shortest path algorithm which allows
for generating a very large number of translation
candidates in an efficient way.
2.1 Baseline SMT system
The baseline system uses phrases automatically ex-
tracted from a word-aligned corpus (trained with
GIZA++) and generates the best translations using
weighted log-linear model combination with several
features, such as word lexicon, phrase translation
and language models. This direct approach is cur-
rently used by most state-of-the-art decoders. The
model scaling factors are trained discriminatively on
some evaluation measure, e.g. BLEU or WER, using
the simplex method.
2.2 N-best list extraction
We incorporated an efficient extraction of n best
translations using the k shortest path algorithm
(Eppstein, 1998) into a state-of-the-art SMT system.
The implementation is partly based on code that is
publicly available.1
Starting point for the extraction is a word graph,
generated separately by the decoder for each sen-
tence. Since these word graphs are directed and
acyclic, it is possible to construct a shortest path tree
spanning from the sentence begin node to the end
node. The efficiency of finding the k shortest paths
in this tree lies in the book-keeping of edges through
a binary heap that allows for an implicit representa-
tion of paths. The overall performance of the algo-
rithm is efficient even for large k. Thus, it is feasi-
ble to use in situations where we want to generate a
large number of paths, i.e. translation hypotheses in
this context.
There is another issue that has to be addressed.
In phrase-based SMT, we have to deal with differ-
ent phrase segmentations for each sentence. Due to
the large number of phrases, it is possible that we
have paths through the word graph representing the
same sentence but internally having different phrase
boundaries. In n-best list generation, we want to get
rid of these duplicates. Due to the efficiency of the
k shortest paths algorithm, we allow for generating
a very large number of hypotheses (e.g. 100 ? n) and
1http://www.ics.uci.edu/?eppstein/pubs/
graehl.zip
then filter the output via a prefix tree (also called
trie) until we get n distinct translations.
With this method, it is feasible to generate
100 000-best lists without much hassle. In gen-
eral, the file input/output operations are more time-
consuming than the actual n-best list extraction.
The average generation time of n-best candidates
for each of the sentences of the development list
is approximately 30 seconds on a 2.2GHz Opteron
machine, whereas 7.4 million hypotheses are com-
puted per sentence on average. The overall extrac-
tion time including filtering and writing to hard-disk
takes around 100 seconds per sentence. Note that
this value could be optimized drastically if checking
for how many duplicates are generated on average
beforehand and adjusting the initial number of hy-
potheses before applying the filtering. We only use
the k = 100 ? n as a proof of concept.
2.3 Rescoring models
After having generated the 100 000-best lists, we
have to apply additional rescoring models to all hy-
potheses. We select the models that have shown
to improve overall translation performance as used
for recent NIST MT evaluations. In addition to the
main decoder score (which is already a combination
of several models and constitutes a strong baseline),
these include several large language models trained
on up to 2.5 billion running words, a sentence-level
IBM model 1 score, m-gram posterior probabilities
and an additional sentence length model.
3 Experiments
The experiments in this section are carried out on n-
best lists with n going up to 100 000. We will show
that, although we are capable of generating this large
amount of hypotheses, the overall performance does
not seem to improve significantly beyond a certain
threshold. Or to put it simple: although we generate
lots of hypotheses, most of them are not very useful.
As experimental background, we choose the large
data track of the Chinese-to-English NIST task,
since the length of the sentences and the large vo-
cabulary of the task allow for large n-best lists. For
smaller tasks, e.g. the IWSLT campaign, the domain
is rather limited such that it does not make sense
to generate lists reaching beyond several thousand
hypotheses. As development data, we use the 2002
eval set, whereas for test, the 2005 eval set is chosen.
The corpus statistics are shown in Table 1.
58
Chinese English
Train Sentence Pairs 7M
Running Words 199M 213M
Vocabulary Size 222K 351K
Dev Sentence Pairs 878 3 512
Running Words 25K 105K
Test Sentence Pairs 1 082 4 328
Running Words 33K 148K
Table 1: Corpus statistics for the Chinese-English
NIST MT task.
3.1 Oracle-best hypotheses
In the first experiment, we examined the oracle-best
hypotheses in the n-best lists for several list sizes.
For an efficient calculation of the true BLEU oracle
(the hypothesis which has a maximum BLEU score
when compared to the reference translations), we
use approximations based on WER/PER-oracles, i.e.
we extract the hypotheses that have the lowest edit
distance (WER, word error rate) to the references.
The same is applied by disregarding the word or-
der (leading to PER, position-independent word er-
ror rate).
As can be seen in Table 2, the improvements are
steadily decreasing, i.e. with increasing number of
generated hypotheses, there are less and less use-
ful candidates among them. For the first 10 000
candidates, we therefore have the possibility to find
hypotheses that could increase the BLEU score by
at least 8.3% absolute if our models discriminated
them properly. For the next 90 000 hypotheses, there
is only a small potential to improve the whole sys-
tem by around 1%. This means that most of the
generated hypotheses are not very useful in terms of
oracle-WER and likely distracting the ?search? for
the needle(s) in the haystack. It has been shown in
(Och et al, 2004) that true BLEU oracle scores on
lists with much smaller n ? 4096 are more or less
linear in log(n). Our results support this claim since
the oracle-WER/PER is a lower bound of the real
BLEU oracle. For the PER criterion, the behavior of
the oracle-best hypotheses is similar. Here we can
notice that after 10,000 hypotheses, the BLEU score
of the oracle-PER hypotheses stays the same.
These observations already impair the alleged
usefulness of a large amount of translation hypothe-
ses by showing that the overall possible gain with in-
creasing n gets disproportionately small if one puts
it in relation to the exponential growth of the n.
Oracle-WER [%] Oracle-PER [%]
N BLEU abs. imp. BLEU abs. imp.
1 36.1 36.1
10 38.8 +2.7 38.0 +1.9
100 41.3 +2.5 39.8 +1.8
1000 43.3 +2.0 41.0 +1.2
10000 44.4 +1.1 42.0 +1.0
100000 45.3 +0.9 42.0 +0.0
Table 2: Dev BLEU scores of oracle-best hypothe-
ses based on minimum WER/PER.
3.2 Rescoring performance
As a next step, we show the performance of tuning
the model scaling factors towards best translation
performance. In our experiments, we use the BLEU
score as objective function of the simplex method.
Figure 1 shows the graphs for the development
(on the left) and test set (on the right). The up-
per graphs depict the oracle-WER BLEU scores (cf.
also Table 2) for comparison. As was already stated,
these are a lower bound since the real oracle-BLEU
hypotheses might have even higher scores. Still, it is
an indicator of what could be achieved if the models
discriminated good from bad hypotheses properly.
The lower two graphs show the behavior when
(a) optimizing and extracting hypotheses on a sub-
set (the first n) of the 100k-best hypotheses and (b)
optimizing on a subset but extracting from the full
100k set. As can be seen, extracting from the full
set does not even help for the development data on
which the scaling factors were tuned. Experiments
on the test list show similar results. We can also
observe that the improvement declines rapidly with
higher n. Note that an optimization on the full 100k
list was not possible due to huge memory require-
ments. The highest n that fit into the 16GB machine
was 60 000. Thus, this setting was used for extrac-
tion on the full 100k set.
The results so far indicate that it is not very use-
ful to go beyond n = 10000. For the development
set, the baseline of 36.1% BLEU can be improved
by 1.6% absolute to 37.7% for the first 10k entries,
whereas for the 60k setting, the absolute improve-
ment is only increased by a marginal 0.1%. For the
chosen setting, whose focus was on various list sizes
for optimization and extraction, the improvements
on the development lists do not carry over to the test
list. From the baseline of 31.5%, we only get a mod-
erate improvement of approximately 0.5% BLEU.
59
 36
 37
 38
 39
 40
 41
 42
 43
 44
 45
 46
 1  10  100  1000  10000  100000
BL
EU
[%
]
N
Oracle-WER
(a) opt. on N, extr. on N
(b) opt. on N, extr. on 100k
 31
 32
 33
 34
 35
 36
 37
 38
 1  10  100  1000  10000  100000
BL
EU
[%
]
N
Oracle-WER
(a) extraction on N
(b) extraction on 100k
Figure 1: BLEU scores of the reranked system. Development set (left) vs. Test set (right).
One possible explanation for this lies in the poor
performance of the rescoring models. A short test
was carried out in which we added the reference
translations to the n-best list and determined the cor-
responding scores of the additional models, such as
the large LM and the IBM model 1. Interestingly,
only less than 1/4 of the references was ranked as
the best hypothesis. Thus, most reference transla-
tions would never have been selected as final candi-
dates. This strongly indicates that we have to come
up with better models in order to make significant
improvements from large n-best lists. Furthermore,
it seems that the exponential growth of n-best hy-
potheses for maintaining a quasilinear improvement
in oracle BLEU score has a strong impact on the
overall system performance. This is in contrast to a
word graph, where a linear increment of its density
yields disproportionately high improvements in ora-
cle BLEU for lower densities (Zens and Ney, 2005).
4 Conclusion
We described an efficient n-best list extraction
method that is based on the k shortest paths algo-
rithm. Experiments with large 100 000-best lists in-
dicate that the models do not have the discriminating
power to separate the good from the bad candidates.
The oracle-best BLEU scores stay linear in log(n),
whereas the reranked system performance seems to
saturate at around 10k best translations given the ac-
tual models. Using more hypotheses currently does
not help to significantly improve translation quality.
Given the current results, one should balance the
advantages of n-best lists, e.g. easily testing com-
plex rescoring models, and word graphs, e.g. repre-
sentation of a much larger hypotheses space. How-
ever, as long as the models are not able to correctly
fire on good candidates, both approaches will stay
beneath their capabilities.
Acknowledgments
This material is partly based upon work supported by the De-
fense Advanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-C-0023, and was partly funded by
the Deutsche Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (NE 572/5-3).
References
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo, and M. Federico.
2006. The ITC-irst SMT system for IWSLT 2006. In Proc.
of the International Workshop on Spoken Language Transla-
tion, pages 53?58, Kyoto, Japan, November.
D. Eppstein. 1998. Finding the k shortest paths. SIAM J. Com-
puting, 28(2):652?673.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the Human Language Tech-
nology Conf. (HLT-NAACL), pages 127?133, Edmonton,
Canada, May/June.
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. Ney. 2006.
The RWTH statistical machine translation system for the
IWSLT 2006 evaluation. In Proc. of the International Work-
shop on Spoken Language Translation, pages 103?110, Ky-
oto, Japan, November.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A.
Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin,
and D. Radev. 2004. A smorgasbord of features for statisti-
cal machine translation. In Proc. 2004 Meeting of the North
American chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 161?168, Boston, MA, May.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word
graphs in statistical machine translation. In Proc. of the
Conf. on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 156?163, Philadelphia, PA, July.
R. Zens and H. Ney. 2005. Word graphs for statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts, pages 191?198, Ann Arbor, MI, June.
60
Proceedings of NAACL HLT 2007, Companion Volume, pages 137?140,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis and System Combination of Phrase- and N -gram-based
Statistical Machine Translation Systems
Marta R. Costa-jussa`1, Josep M. Crego1, David Vilar2
Jose? A. R. Fonollosa1, Jose? B. Marin?o1 and Hermann Ney2
1TALP Research Center (UPC), Barcelona 08034, Spain
{mruiz,jmcrego,adrian,canton}@gps.tsc.upc.edu
2RWTH Aachen University, Aachen D-52056, Germany
{vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
In the framework of the Tc-Star project,
we analyze and propose a combination of
two Statistical Machine Translation sys-
tems: a phrase-based and an N -gram-based
one. The exhaustive analysis includes a
comparison of the translation models in
terms of efficiency (number of translation
units used in the search and computational
time) and an examination of the errors in
each system?s output. Additionally, we
combine both systems, showing accuracy
improvements.
1 Introduction
Statistical machine translation (SMT) has evolved
from the initial word-based translation models to
more advanced models that take the context sur-
rounding the words into account. The so-called
phrase-based and N -gram-based models are two ex-
amples of these approaches (Zens and Ney, 2004;
Marin?o et al, 2006).
In current state-of-the-art SMT systems, the
phrase-based or the N -gram-based models are usu-
ally the main features in a log-linear framework, rem-
iniscent of the maximum entropy modeling approach.
Two basic issues differentiate the N -gram-based
system from the phrase-based one: the training data
is sequentially segmented into bilingual units; and
the probability of these units is estimated as a bilin-
gual N -gram language model. In the phrase-based
model, no monotonicity restriction is imposed on the
segmentation and the probabilities are normally es-
timated simply by relative frequencies.
This paper extends the analysis of both systems
performed in (Crego et al, 2005a) by additionally
performing a manual error analysis of both systems,
which were the ones used by UPC and RWTH in the
last Tc-Star evaluation.
Furthermore, we will propose a way to combine
both systems in order to improve the quality of trans-
lations.
Experiments combining several kinds of MT sys-
tems have been presented in (Matusov et al, 2006),
based only on the single best output of each system.
Recently, a more straightforward approach of both
systems has been performed in (Costa-jussa` et al,
2006) which simply selects, for each sentence, one of
the provided hypotheses.
This paper is organized as follows. In section 2,
we briefly describe the phrase and the N -gram-based
baseline systems. In the next section we present the
evaluation framework. In Section 4 we report a struc-
tural comparison performed for both systems and, af-
terwards, in Section 5, we analyze the errors of both
systems. Finally, in the last two sections we rescore
and combine both systems, and the obtained results
are discussed.
2 Baseline Systems
2.1 Phrase-based System
The basic idea of phrase-based translation is to seg-
ment the given source sentence into units (here called
phrases), then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions.
In order to train these phrase-based models, an
alignment between the source and target training
sentences is found by using the standard IBM mod-
els in both directions (source-to-target and target-
to-source) and combining the two obtained align-
ments. Given this alignment an extraction of con-
tiguous phrases is carried out, specifically we extract
all phrases that fulfill the following restrictions: all
source (target) words within the phrase are aligned
only to target (source) words within the phrase.
The probability of these phrases is normally esti-
mated by relative frequencies, normally in both di-
rections, which are then combined in a log-linear way.
137
2.2 N-gram-based System
In contrast with standard phrase-based approaches,
the N -gram translation model uses tuples as bilin-
gual units whose probabilities are estimated as an
N -gram language model (Marin?o et al, 2006). This
model approximates the joint probability between
the source and target languages by using N -grams.
Given a word alignment, tuples define a unique
and monotonic segmentation of each bilingual sen-
tence, building up a much smaller set of units
than with phrases and allowing N -gram estimation
to account for the history of the translation pro-
cess (Marin?o et al, 2006).
2.3 Feature functions
Both baseline systems are combined in a log-linear
way with several additional feature functions: a tar-
get language model, a forward and a backward lex-
icon model and a word bonus are common features
for both systems. The phrase-based system also in-
troduces a phrase bonus model.
3 Evaluation framework
The translation models presented so far were the ones
used by UPC and RWTH in the second evaluation
campaign of the Tc-Star project. The goal of this
project is to build a speech-to-speech translation sys-
tem that can deal with real life data.
The corpus consists of the official version of the
speeches held in the European Parliament Plenary
Sessions (EPPS), as available on the web page of the
European Parliament. Table 1 shows some statistics.
The following tools have been used for building
both systems: Word alignments were computed us-
ing GIZA++ (Och, 2003), language models were es-
timated using the SRILM toolkit (Stolcke, 2002), de-
coding was carried out by the free available MARIE
decoder (Crego et al, 2005b) and the optimization
was performed through an in-house implementation
of the simplex method (Nelder and Mead, 1965).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Dev Sentences 1 122 699
Words 26K 21K
Test Sentences 1 117 894
Words 26K 26K
Table 1: Statistics of the EPPS Corpora.
4 Structural comparison
Both approaches aim at improving accuracy by in-
cluding word context in the model. However, the
implementation of the models are quite different and
may produce variations in several aspects.
Table 2 shows the effect on decoding time intro-
duced through different settings of the beam size.
Additionally, the number of available translation
units is shown, corresponding to number of avail-
able phrases for the phrase-based system and 1gram,
2gram and 3gram entries for the N -gram-based sys-
tem. Results are computed on the development set.
Task Beam Time(s) Units
50 2,677
es?en 10 852 537k
5 311
50 2,689
en?es 10 903 594k
5 329
50 1,264
es?en 10 281 104k 288k 145k
5 138
50 1,508
en?es 10 302 118k 355k 178k
5 155
Table 2: Impact on efficiency of the beam size in PB
(top) and NB system (bottom).
As it can be seen, the number of translation units
is similar in both tasks for both systems (537k ?
537k for Spanish to English and 594k ? 651k for
English to Spanish) while the time consumed in de-
coding is clearly higher for the phrase-based system.
This can be explained by the fact that in the phrase-
based approach, the same translation can be hypoth-
esized following several segmentations of the input
sentence, as phrases appear (and are collected) from
multiple segmentations of the training sentence pairs.
In other words, the search graph seems to be over-
populated under the phrase-based approach.
Table 3 shows the effect on translation accuracy
regarding the size of the beam in the search. Results
are computed on the test set for the phrase-based
and N -gram-based systems.
Results of the N -gram-based system show that de-
creasing the beam size produces a clear reduction
of the accuracy results. The phrase-based system
shows that accuracy results remain very similar un-
der the different settings. The reason is found on
how translation models are used in the search. In
the phrase-based approach, every partial hypothesis
138
Task Beam BLEU NIST mWER
50 51.90 10.53 37.54
es?en 10 51.93 10.54 37.49
5 51.87 10.55 37.47
50 47.75 9.94 41.20
en?es 10 47.77 9.96 41.09
5 47.86 10.00 40.74
50 51.63 10.46 37.88
es?en 10 51.50 10.45 37.83
5 51.39 10.45 37.85
50 47.73 10.08 40.50
en?es 10 46.82 9.97 41.04
5 45.59 9.83 41.04
Table 3: Impact on accuracy of the beam size in PB
(top) and NB system (bottom).
is scored uncontextualized, hence, a single score is
used for a given partial hypothesis (phrase). In the
N -gram-based approach, the model is intrinsically
contextualized, which means that each partial hy-
pothesis (tuple) depends on the preceding sequence
of tuples. Thus, if a bad sequence of tuples (bad
scored) is composed of a good initial sequence (well
scored), it is placed on top of the first stacks (beam)
and may cause the pruning of the rest of hypotheses.
5 Error analysis
In order to better asses the quality and the differ-
ences between the two systems, a human error anal-
ysis was carried out. The guidelines for this error
analysis can be found in (Vilar et al, 2006). We
randomly selected 100 sentences, which were evalu-
ated by bilingual judges.
This analysis reveals that both systems produce
the same kind of errors in general. However some dif-
ferences were identified. For the English to Spanish
direction the greatest problem is the correct genera-
tion of the right tense for verbs, with around 20% of
all translation errors being of this kind. Reordering
also poses an important problem for both phrase and
N-gram-based systems, with 18% or 15% (respec-
tively) of the errors falling into this category. Miss-
ing words is also an important problem. However,
most of them (approximately two thirds for both sys-
tems) are filler words (i.e. words which do not con-
vey meaning), that is, the meaning of the sentence
is preserved. The most remarkable difference when
comparing both systems is that the N -gram based
system produces a relatively large amount of extra
words (approximately 10%), while for the phrase-
based system, this is only a minor problem (2% of
the errors). In contrast the phrase-based system has
more problems with incorrect translations, that is
words for which a human can find a correspondence
in the source text, but the translation is incorrect.
Similar conclusions can be drawn for the inverse di-
rection. The verb generating problem is not so acute
in this translation direction due to the much simpli-
fied morphology of English. An important problem
is the generation of the right preposition.
The N -gram based system seems to be able to pro-
duce more accurate translations (reflected by a lower
percentage of translation errors). However, it gener-
ates too many additional (and incorrect words) in
the process. The phrase-based system, in contrast,
counteracts this effect by producing a more direct
correspondence with the words present in the source
sentence at the cost of sometimes not being able to
find the exact translation.
6 System Rescoring and
Combination
Integration of both output translations in the search
procedure is a complex task. Translation units of
both models are quite different and generation his-
tories pose severe implementation difficulties. We
propose a method for combining the two systems at
the level of N -best lists.
Some features that are useful for SMT are too com-
plex for including them directly in the search pro-
cess. A clear example are the features that require
the entire target sentence to be evaluated, as this is
not compatible with the pruning and recombination
procedures that are necessary for keeping the target
sentence generation process manageable. A possible
solution for this problem is to apply sentence level
re-ranking by using N -best lists.
6.1 Rescoring Criteria
The aim of the rescoring procedure is to choose the
best translation candidate out of a given set of N
possible translations. In our approach this transla-
tion candidates are produced independently by both
of the systems and then combined by a simple con-
catenation1. In order for the hypothesis to have a
comparable set of scores, we perform an additional
?cross-rescoring? of the lists.
Given an N -best list of the phrase-based (N -gram-
based) system, we compute the cost of each target
sentence of this N -best list for the N -gram-based
(phrase-based) system. However this computation
is not possible in all cases. Table 4 shows the per-
centage of target sentences that the N -gram-based
1With removal of duplicates.
139
(phrase-based) system is able to produce given an N -
best list of target sentences computed by the phrase-
based (N -gram-based) system. This percentage is
calculated on the development set.
The vocabulary of phrases is bigger than the vo-
cabulary of tuples, due to the fact that phrases are
extracted from multiple segmentations of the train-
ing sentence pairs. Hence, the number of sentences
reproduced by the N -gram-based system is smaller
than the number of sentences reproduced by the
phrase-based system. Whenever a sentence can not
be reproduced by a given system, the cost of the
worst sentence in the N -best list is assigned to it.
Task N -best % NB % PB
es?en 1000 37.5 57.5
en?es 1000 37.2 48.6
Table 4: Sentences (%) produced by each system.
6.2 Results
Table 5 shows results of the rescoring and system
combination experiments on the test set. The first
two rows include results of systems non-rescored and
PB (NB) rescored by NB (PB). The third row corre-
sponds to the system combination. Here, PB (NB)
rescored by NB (PB) are simply merged and ranked
by rescored score.
System N -best BLEU NIST mWER
Spanish-to-English
PB 1 51.90 10.54 37.50
PB 1000 52.55 10.61 37.12
NB 1 51.63 10.46 37.88
NB 1000 52.25 10.55 37.43
PB+NB 2 51.77 10.49 37.68
PB+NB 2000 52.31 10.56 37.32
English-to-Spanish
PB 1 47.75 9.94 41.2
PB 1000 48.46 10.13 39.98
NB 1 47.73 10.09 40.50
NB 1000 48.33 10.15 40.13
PB+NB 2 48.26 10.05 40.61
PB+NB 2000 48.54 10.16 40.00
Table 5: Rescoring and system combination results.
7 Discussion
The structural comparison has shown on the one
hand that the N -gram-based system outperforms
the phrase-based in terms of search time efficiency
by avoiding the overpopulation problem presented
in the phrase-based approach. On the other hand
the phrase-based system shows a better performance
when decoding under a highly constrained search.
A detailed error analysis has also been carried out
in order to better determine the differences in per-
formance of both systems. The N -gram based sys-
tem produced more accurate translations, but also a
larger amount of extra (incorrect) words when com-
pare to the phrase-based translation system.
In section 6 we have presented a system combina-
tion method using a rescoring feature for each SMT
system, i.e. the N -gram-based feature for the phrase-
based system and vice-versa. For both systems, con-
sidering the feature of the opposite system leads to
an improvement of BLEU score.
References
M.R. Costa-jussa`, J.M. Crego, A. de Gispert,
P. Lambert, M. Khalilov J.A.R. Fonollosa, J.B.
Marin?o, and R. Banchs. 2006. Talp phrase-based
statistical machine translation and talp system
combination the iwslt 2006. IWSLT06.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005a. N-gram-based versus phrase-
based statistical machine translation. IWSLT05,
October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005b.
An Ngram-based statistical machine translation
decoder. ICSLP05, April.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M.R.
Costa-jussa`. 2006. N-gram based machine trans-
lation. Computational Linguistics, 32(4):527?549.
E. Matusov, N. Ueffing, and H. Ney. 2006. Com-
puting consensus translation from multiple ma-
chine translation systems using enhanced hypothe-
ses alignment. EACL06, pages 33?40.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. Proc. of the 7th Int. Conf. on
Spoken Language Processing, ICSLP?02, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and
Hermann Ney. 2006. Error Analysis of Machine
Translation Output. In LREC06, pages 697?702,
Genoa, Italy, May.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In HLT04, pages 257?264, Boston, MA, May.
140
Proceedings of NAACL HLT 2009: Short Papers, pages 17?20,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Comparison of Extended Lexicon Models in Search and Rescoring for SMT
Sa
?
sa Hasan and Hermann Ney
Human Language Technology and Pattern Recognition Group
Chair of Computer Science 6, RWTH Aachen University, Germany
{hasan,ney}@cs.rwth-aachen.de
Abstract
We show how the integration of an extended
lexicon model into the decoder can improve
translation performance. The model is based
on lexical triggers that capture long-distance
dependencies on the sentence level. The re-
sults are compared to variants of the model
that are applied in reranking of n-best lists.
We present how a combined application of
these models in search and rescoring gives
promising results. Experiments are reported
on the GALE Chinese-English task with im-
provements of up to +0.9% BLEU and -1.5%
TER absolute on a competitive baseline.
1 Introduction
Phrase-based statistical machine translation has im-
proved significantly over the last decade. The avail-
ability of large amounts of parallel data and access to
open-source software allow for easy setup of trans-
lation systems with acceptable performance. Pub-
lic evaluations such as the NIST MT Eval or the
WMT Shared Task help to measure overall progress
within the community. Most of the groups use a
phrase-based decoder (e.g. Pharaoh or the more re-
cent Moses) based on a log-linear fusion of models
that enable the avid researcher to quickly incorpo-
rate additional features and investigate the effect of
additional knowledge sources to guide the search for
better translation hypotheses.
In this paper, we deal with an extended lexicon
model and its incorporation into a state-of-the-art
decoder. We compare the results of the integration
to a similar setup used within a rescoring frame-
work and show the benefits of integrating additional
models directly into the search process. As will
be shown, although a rescoring framework is suit-
able for obtaining quick trends of incorporating ad-
ditional models into a system, an alternative that in-
cludes the model in search should be preferred. The
integration does not only yield better performance,
we will also show the benefit of combining both ap-
proaches in order to boost translation quality even
more. The extended lexicon model which we apply
is motivated by a trigger-based approach (Hasan et
al., 2008). A standard lexicon modeling dependen-
cies of target and source words, i.e. p(e|f), is ex-
tended with a second trigger f
?
on the source side,
resulting in p(e|f, f
?
). This model allows for a more
fine-grained lexical choice of the target word de-
pending on the additional source word f
?
. Since the
second trigger can move over the whole sentence,
we capture global (sentence-level) context that is not
modeled in local n-grams of the language model or
in bilingual phrase pairs that cover only a limited
amount of consecutive words.
Related work A similar approach has been tried
in the word-sense disambiguation (WSD) domain
where local but also across-sentence unigram collo-
cations of words are used to refine phrase pair selec-
tion dynamically by incorporating scores from the
WSD classifier (Chan et al, 2007). A maximum-
entropy based approach with different features of
surrounding words that are locally bound to a con-
text of three positions to the left and right is re-
ported in (Garc??a-Varea et al, 2001). A logistic
regression-based word translation model is investi-
gated by Vickrey et al (2005) but has not been eval-
uated on a machine translation task. Another WSD
approach incorporating context-dependent phrasal
translation lexicons is presented by Carpuat and Wu
(2007) and has been evaluated on several translation
17
tasks. The triplet lexicon model presented in this
work can also be interpreted as an extension of the
standard IBM model 1 (Brown et al, 1993) with an
additional trigger.
2 Setup
The main focus of this work investigates an extended
lexicon model in search and rescoring. The model
that we consider here and its integration in the de-
coder and setup for rescoring are presented in the
following sections.
2.1 Extended lexicon model
The triplets of the extended lexicon model p(e|f, f
?
)
are composed of two words in the source language
triggering one target word. In order to limit the over-
all number of triplets, we apply a training constraint
that reuses the word alignment information obtained
in the GIZA
++
step. For source words f , we only
consider the ones that are aligned to a target word e
given the GIZA
++
word alignment. The second trig-
ger f
?
is allowed to move over the whole source sen-
tence, thus capturing long-distance effects that can
be observed in the training data:
p(e
I
1
|f
J
1
, {a
ij
}) =
I
?
i=1
p(e
i
|f
J
1
, {a
ij
}) =
I
?
i=1
1
Z
i
?
j?{a
i
}
J
?
j
?
=1
p(e
i
|f
j
, f
j
?
) (1)
where {a
ij
} denotes the alignment matrix of the sen-
tence pair f
J
1
and e
I
1
and the first sum goes over all
f
j
that are aligned to the current e
i
(expressed as
j ? {a
i
}). The factor Z
i
= J ? |{a
i
}| normalizes
the double summation accordingly. Eq. 1 is used in
the iterative EM training on all sentence pairs of the
training data. Empty words are allowed on the trig-
gering part and low probability triplets are trimmed.
2.2 Decoding
Regarding the search, we can apply this model di-
rectly when scoring bilingual phrase pairs. Given a
trained model for p(e|f, f
?
), we compute the feature
score h
t
of a phrase pair (e?,
?
f) as
h
t
(e?,
?
f, {a?
ij
}, f
J
1
) = (2)
?
?
i
log
?
j?{a?
i
}
?
j
?
p(e?
i
|
?
f
j
, f
j
?
) +
?
i
logZ
i
where i moves over all target words in the phrase e?,
the sum over j selects the aligned source words
?
f
j
given {a?
ij
}, the alignment matrix within the phrase
pair, and j
?
incorporates the whole source sentence
f
J
1
. Analogous to Eq. 1, Z
i
= J ? |{a?
i
}| denotes
the number of overall source words times the num-
ber of aligned source words to each e?
i
. In Eq. 2,
we take negative log-probabilities and normalize to
obtain the final score (representing costs) for the
given phrase pair. Note that in search, we can only
use this direction, p(e|f, f
?
), since the whole source
sentence is available for triggering effects whereas
not all target words have been generated so far,
as it would be necessary for the reverse direction,
p(f |e, e
?
). Due to data sparseness, we smooth the
model by using a floor value of 10
?7
for unseen
events during decoding. Furthermore, an implicit
backoff to IBM1 exists if the second trigger is the
empty word, i.e. for events of the form p(e|f, ?).
2.3 Rescoring
In rescoring, we constrain the scoring of our hy-
potheses to a limited set of n-best translations that
are extracted from the word graph, a pruned com-
pact representation of the search space. The advan-
tage of n-best list rescoring is the full availability of
both source text and target translation, thus allow-
ing for the application of additional (possibly more
complex) models that are hard to implement directly
in search, such as e.g. syntactic models based on
parsers or huge LMs that would not fit in memory
during decoding. Since we are limiting ourselves to
a small extract of translation hypotheses, rescoring
models cannot outperform the same models if ap-
plied directly in search. One advantage though is
that we can apply the introduced trigger model also
in the other direction, i.e. using p(f |e, e
?
), where two
target words trigger one source word. Generally, the
combination of two directions of a model yields fur-
ther improvements, so we investigated how this ad-
ditional direction helps in rescoring (cf. Section 3.1).
In our experiments, we use 10 000-best lists ex-
tracted from the word graphs. An initial setting uses
the baseline system, whereas a comparative setup in-
corporates the (e|f, f
?
) direction of the trigger lexi-
con model in search and adds the reversed direction
in rescoring. Additionally, we use n-gram posteri-
ors, a sentence length model and two large language
18
train (ch/en) test08 (NW/WT)
Sent. pairs 9.1M 480 490
Run. words 259M/300M 14.8K 12.3K
Vocabulary 357K/627K 3.6K 3.2K
Table 1: GALE Chinese-English corpus statistics.
models, a 5-gram count LM trained on 2.5G running
words and the Google Web 1T 5-grams. The feature
weights of the log-linear mix are tuned on a separate
development set using the Downhill Simplex algo-
rithm.
3 Experiments
The experiments are carried out with a GALE sys-
tem using the official development and test sets of
the GALE 2008 evaluation. The corpus statistics
are shown in Table 1. The triplet lexicon model was
trained on a subset of the overall data. We used 1.4M
sentence pairs with 32.3M running words on the En-
glish side. The vocabulary sizes were 76.5K for the
source and 241.7K for the target language. The final
lexicon contains roughly 62 million triplets.
The baseline system incorporates the standard
model setup used in phrase-based SMT which com-
bines phrase translation and word lexicon models
in both directions, a 5-gram language model, word
and phrase penalties, and two models for reorder-
ing (a standard distortion model and a discriminative
phrase orientation model). For a fair comparison, we
also added the related IBM model 1 p(e|f) to the
baseline since it can be computed on the sentence-
level for this direction, target given source. This step
achieves +0.5% BLEU on the development set for
newswire but has no effect on test. As will be pre-
sented in the next section, the extension to another
trigger results in improvements over this baseline,
indicating that the extended triplet model is superior
to the standard IBM model 1. The feature weights
were optimized on separate development sets for
both newswire and web text.
We perform the following pipeline of experi-
ments: A first run generates word graphs using the
baseline models. From this word graph, we ex-
tract 10k-best lists and compare the performance to
a reranked version including the additional models.
In a second step, we add one of the trigger lexi-
Chinese-English newswire web text
GALE test08 BLEU TER BLEU TER
baseline 32.5 59.4 25.8 64.0
rescore, no triplets 32.8 59.0 26.6 63.5
resc. triplets fe+ef 33.2 58.6 27.1 63.0
triplets in search ef 33.1 58.8 26.0 63.5
rescore, no triplets 33.2 58.6 26.7 63.5
rescore, triplets fe 33.7 58.1 27.2 62.0
Table 2: Results obtained for the two test sets. For the
triplet models, ?fe? means p(f |e, e
?
) and ?ef? denotes
p(e|f, f
?
). BLEU/TER scores are shown in percent.
con models to the search process, regenerate word
graphs, extract updated n-best lists and add the re-
maining models again in a reranking step.
3.1 Results
Table 2 presents results that were obtained on the
test sets. All results are based on lowercase eval-
uations since the system is trained on lowercased
data in order to keep computational resources fea-
sible. For the newswire setting, the baseline is
32.5% BLEU and 59.4% TER. Rescoring with addi-
tional models not including triplets gives only slight
improvements. By adding the path-aligned triplet
model in both directions, we observe an improve-
ment of +0.7% BLEU and -0.8% TER. Using the
triplet model in source to target direction (e, f, f
?
)
during the search process, we arrive at a similar
BLEU improvement of +0.6% without any rerank-
ing models. We add the other direction of the triplets
(f, e, e
?
) (the one that can not be used directly in
search) and obtain 33.7% BLEU on the newswire
set. The overall cumulative improvements of triplets
in search and reranking are +0.9% BLEU and -0.9%
TER when compared to the rescored baseline not in-
corporating triplet models and +1.2%/-1.3% on the
decoder baseline, respectively.
For the web text setting, the baseline is consid-
erably lower at 25.8% BLEU and 64.0% TER (cf.
right part of Table 2). We observe an improvement
for the baseline reranking models, a large part of
which is due to the Google Web LM. Adding triplets
to search does not help significantly (+0.2%/-0.5%
BLEU/TER). This might be due to training the
triplet lexicon mainly on newswire data. Rerank-
ing without triplets performs similar to the baseline
19
experiment. Mixing in the (f, e, e
?
) direction helps
again: The final score comes out at 27.2% BLEU
and 62.0% TER, the latter being significantly better
than the reranked baseline (-1.5% in TER).
3.2 Discussion
The results indicate that it is worth moving models
from rescoring to the search process. This is not
surprising (and probably well known in the com-
munity). Interestingly, the triplet model can im-
prove translation quality in addition to its related
IBM model 1 which was already part of the base-
line. It seems that the extension by a second trigger
helps to capture some language specific properties
for Chinese-English which go beyond local lexical
(word-to-word) dependencies. In Table 3, we show
an example of improved translation quality where a
triggering effect can be observed. Due to the topic of
the sentence, the phrase local employment was cho-
sen over own jobs. One of the top triplets in this con-
text is p(employment | ?? , ?? ), where ??
is ?employment? due to the path-aligned constraint
and ?? means ?talent?. Note that the distance be-
tween these two triggers is five tokens.
4 Conclusion
We presented the integration of an extended lexicon
model into the search process and compared it to a
variant which was used in reranking n-best lists. In
order to keep the overall number of triplets feasi-
ble, and thus memory footprints and training times
low, we chose a path-constrained triplet model that
restricts the first source trigger to the aligned target
word, whereas the second trigger can move along
the whole source sentence. The motivation was to
allow for a more fine-grained lexical choice of tar-
get words by looking at sentence-level context. The
overall improvements that can be accounted to the
triplets are up to +0.9% BLEU and -1.5% TER.
In the future, we plan to investigate more triplet
model variants and work on additional language
pairs such as French-English or German-English.
The reverse direction, p(f |e, e
?
), is hard to imple-
ment outside of a reranking framework where the
full target hypotheses are already fully generated. It
might be worth looking at cross-lingual trigger mod-
els such as p(f |e, f
?
) or constrained variants like
source ??????????? ,???
??????????? .
baseline germany, in order to protect their own
jobs, the introduction of foreign talent,
a relatively high threshold.
triplets in order to protect local employment,
germany has a relatively high threshold
for the introduction of foreign talent.
reference in order to protect native employment,
germany has set a relatively high thresh-
old for bringing in foreign talents.
Table 3: Translation example on the newswire test set.
p(f |e, e
?
) with e
?
< e, i.e. the second trigger com-
ing from the left context within a sentence which has
already been generated.
Acknowledgments
This material is partly based upon work supported by the
Defense Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023, and was partly
realized as part of the Quaero Programme, funded by
OSEO, French State agency for innovation.
The authors would like to thank Juri Ganitkevitch for
training the triplet model.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
M. Carpuat and D. Wu. 2007. Improving statistical ma-
chine translation using word sense disambiguation. In
Proc. EMNLP-CoNLL, Prague, Czech Republic, June.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proc. ACL, pages 33?40, Prague, Czech Re-
public, June.
I. Garc??a-Varea, F. J. Och, H. Ney, and F. Casacuberta.
2001. Refined lexicon models for statistical machine
translation using a maximum entropy approach. In
Proc. ACL Data-Driven Machine Translation Work-
shop, pages 204?211, Toulouse, France, July.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr?es-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Proc. EMNLP, pages 372?381, Hon-
olulu, Hawaii, October.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation.
In Proc. HLT-EMNLP, pages 771?778, Morristown,
NJ, USA.
20
 
	ff 
	Refined Lexicon Models for Statistical Machine Translation using a
Maximum Entropy Approach
Ismael Garc??a Varea
Dpto. de Informa?tica
Univ. de Castilla-La Mancha
Campus Universitario s/n
02071 Albacete, Spain
ivarea@info-ab.uclm.es
Franz J. Och and
Hermann Ney
Lehrstuhl fu?r Inf. VI
RWTH Aachen
Ahornstr., 55
D-52056 Aachen, Germany
 
och|ney  @cs.rwth-aachen.de
Francisco Casacuberta
Dpto. de Sist. Inf. y Comp.
Inst. Tecn. de Inf. (UPV)
Avda. de Los Naranjos, s/n
46071 Valencia, Spain
fcn@iti.upv.es
Abstract
Typically, the lexicon models used in
statistical machine translation systems
do not include any kind of linguistic
or contextual information, which often
leads to problems in performing a cor-
rect word sense disambiguation. One
way to deal with this problem within
the statistical framework is to use max-
imum entropy methods. In this paper,
we present how to use this type of in-
formation within a statistical machine
translation system. We show that it is
possible to significantly decrease train-
ing and test corpus perplexity of the
translation models. In addition, we per-
form a rescoring of  -Best lists us-
ing our maximum entropy model and
thereby yield an improvement in trans-
lation quality. Experimental results are
presented on the so-called ?Verbmobil
Task?.
1 Introduction
Typically, the lexicon models used in statistical
machine translation systems are only single-word
based, that is one word in the source language cor-
responds to only one word in the target language.
Those lexicon models lack from context infor-
mation that can be extracted from the same paral-
lel corpus. This additional information could be:
 Simple context information: information of
the words surrounding the word pair;
 Syntactic information: part-of-speech in-
formation, syntactic constituent, sentence
mood;
 Semantic information: disambiguation in-
formation (e.g. from WordNet), cur-
rent/previous speech or dialog act.
To include this additional information within the
statistical framework we use the maximum en-
tropy approach. This approach has been applied
in natural language processing to a variety of
tasks. (Berger et al, 1996) applies this approach
to the so-called IBM Candide system to build con-
text dependent models, compute automatic sen-
tence splitting and to improve word reordering in
translation. Similar techniques are used in (Pap-
ineni et al, 1996; Papineni et al, 1998) for so-
called direct translation models instead of those
proposed in (Brown et al, 1993). (Foster, 2000)
describes two methods for incorporating informa-
tion about the relative position of bilingual word
pairs into a maximum entropy translation model.
Other authors have applied this approach to lan-
guage modeling (Rosenfeld, 1996; Martin et al,
1999; Peters and Klakow, 1999). A short review
of the maximum entropy approach is outlined in
Section 3.
2 Statistical Machine Translation
The goal of the translation process in statisti-
cal machine translation can be formulated as fol-
lows: A source language string 	  


 

is to be translated into a target language string










. In the experiments reported in
this paper, the source language is German and the
target language is English. Every target string is
considered as a possible translation for the input.
If we assign a probability      to each pair
of strings       , then according to Bayes? de-
cision rule, we have to choose the target string
that maximizes the product of the target language
model     and the string translation model
Discriminative Training and Maximum Entropy Models for Statistical
Machine Translation
Franz Josef Och and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
{och,ney}@informatik.rwth-aachen.de
Abstract
We present a framework for statistical
machine translation of natural languages
based on direct maximum entropy mod-
els, which contains the widely used sour-
ce-channel approach as a special case. All
knowledge sources are treated as feature
functions, which depend on the source
language sentence, the target language
sentence and possible hidden variables.
This approach allows a baseline machine
translation system to be extended easily by
adding new feature functions. We show
that a baseline statistical machine transla-
tion system is significantly improved us-
ing this approach.
1 Introduction
We are given a source (?French?) sentence fJ1 =
f1, . . . , fj , . . . , fJ , which is to be translated into a
target (?English?) sentence eI1 = e1, . . . , ei, . . . , eI .
Among all possible target sentences, we will choose
the sentence with the highest probability:1
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )} (1)
The argmax operation denotes the search problem,
i.e. the generation of the output sentence in the target
language.
1The notational convention will be as follows. We use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
1.1 Source-Channel Model
According to Bayes? decision rule, we can equiva-
lently to Eq. 1 perform the following maximization:
e?I1 = argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)} (2)
This approach is referred to as source-channel ap-
proach to statistical MT. Sometimes, it is also re-
ferred to as the ?fundamental equation of statisti-
cal MT? (Brown et al, 1993). Here, Pr(eI1) is
the language model of the target language, whereas
Pr(fJ1 |eI1) is the translation model. Typically, Eq. 2
is favored over the direct translation model of Eq. 1
with the argument that it yields a modular approach.
Instead of modeling one probability distribution,
we obtain two different knowledge sources that are
trained independently.
The overall architecture of the source-channel ap-
proach is summarized in Figure 1. In general, as
shown in this figure, there may be additional trans-
formations to make the translation task simpler for
the algorithm. Typically, training is performed by
applying a maximum likelihood approach. If the
language model Pr(eI1) = p?(eI1) depends on pa-
rameters ? and the translation model Pr(fJ1 |eI1) =
p?(fJ1 |eI1) depends on parameters ?, then the opti-
mal parameter values are obtained by maximizing
the likelihood on a parallel training corpus fS1 , eS1
(Brown et al, 1993):
?? = argmax
?
S?
s=1
p?(fs|es) (3)
?? = argmax
?
S?
s=1
p?(es) (4)
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 295-302.
                         Proceedings of the 40th Annual Meeting of the Association for
Source
Language Text
??
Preprocessing
Pr(eI1): Language Modeloo
Global Search
e?I1 = argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)}
??
??
Pr(fJ1 |eI1): Translation Modeloo
Postprocessing
??
Target
Language Text
Figure 1: Architecture of the translation approach based on source-channel models.
We obtain the following decision rule:
e?I1 = argmax
eI1
{p??(eI1) ? p??(fJ1 |eI1)} (5)
State-of-the-art statistical MT systems are based on
this approach. Yet, the use of this decision rule has
various problems:
1. The combination of the language model p??(eI1)
and the translation model p??(fJ1 |eI1) as shown
in Eq. 5 can only be shown to be optimal if the
true probability distributions p??(eI1) = Pr(eI1)
and p??(fJ1 |eI1) = Pr(fJ1 |eI1) are used. Yet,
we know that the used models and training
methods provide only poor approximations of
the true probability distributions. Therefore, a
different combination of language model and
translation model might yield better results.
2. There is no straightforward way to extend a
baseline statistical MT model by including ad-
ditional dependencies.
3. Often, we observe that comparable results are
obtained by using the following decision rule
instead of Eq. 5 (Och et al, 1999):
e?I1 = argmax
eI1
{p??(eI1) ? p??(eI1|fJ1 )} (6)
Here, we replaced p??(fJ1 |eI1) by p??(eI1|fJ1 ).
From a theoretical framework of the source-
channel approach, this approach is hard to jus-
tify. Yet, if both decision rules yield the same
translation quality, we can use that decision
rule which is better suited for efficient search.
1.2 Direct Maximum Entropy Translation
Model
As alternative to the source-channel approach, we
directly model the posterior probability Pr(eI1|fJ1 ).
An especially well-founded framework for doing
this is maximum entropy (Berger et al, 1996). In
this framework, we have a set of M feature func-
tions hm(eI1, fJ1 ),m = 1, . . . ,M . For each feature
function, there exists a model parameter ?m,m =
1, . . . ,M . The direct translation probability is given
Source
Language Text
??
Preprocessing
?? ?1 ? h1(eI1, fJ1 )oo
Global Search
argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
??
?2 ? h2(eI1, fJ1 )oo
. . .oo
Postprocessing
??
Target
Language Text
Figure 2: Architecture of the translation approach based on direct maximum entropy models.
by:
Pr(eI1|fJ1 ) = p?M1 (e
I
1|fJ1 ) (7)
= exp[
?M
m=1 ?mhm(eI1, fJ1 )]?
e?I1 exp[
?M
m=1 ?mhm(e?I1, fJ1 )]
(8)
This approach has been suggested by (Papineni et
al., 1997; Papineni et al, 1998) for a natural lan-
guage understanding task.
We obtain the following decision rule:
e?I1 = argmax
eI1
{
Pr(eI1|fJ1 )
}
= argmax
eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
Hence, the time-consuming renormalization in Eq. 8
is not needed in search. The overall architecture of
the direct maximum entropy models is summarized
in Figure 2.
Interestingly, this framework contains as special
case the source channel approach (Eq. 5) if we use
the following two feature functions:
h1(eI1, fJ1 ) = log p??(eI1) (9)
h2(eI1, fJ1 ) = log p??(fJ1 |eI1) (10)
and set ?1 = ?2 = 1. Optimizing the corresponding
parameters ?1 and ?2 of the model in Eq. 8 is equiv-
alent to the optimization of model scaling factors,
which is a standard approach in other areas such as
speech recognition or pattern recognition.
The use of an ?inverted? translation model in the
unconventional decision rule of Eq. 6 results if we
use the feature function logPr(eI1|fJ1 ) instead of
logPr(fJ1 |eI1). In this framework, this feature can
be as good as logPr(fJ1 |eI1). It has to be empirically
verified, which of the two features yields better re-
sults. We even can use both features logPr(eI1|fJ1 )
and logPr(fJ1 |eI1), obtaining a more symmetric
translation model.
As training criterion, we use the maximum class
posterior probability criterion:
??M1 = argmax
?M1
{ S?
s=1
log p?M1 (es|fs)
}
(11)
This corresponds to maximizing the equivocation
or maximizing the likelihood of the direct transla-
tion model. This direct optimization of the poste-
rior probability in Bayes decision rule is referred to
as discriminative training (Ney, 1995) because we
directly take into account the overlap in the proba-
bility distributions. The optimization problem has
one global optimum and the optimization criterion
is convex.
1.3 Alignment Models and Maximum
Approximation
Typically, the probability Pr(fJ1 |eI1) is decomposed
via additional hidden variables. In statistical align-
ment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is in-
troduced as a hidden variable:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1)
The alignment mapping is j ? i = aj from source
position j to target position i = aj .
Search is performed using the so-called maximum
approximation:
e?I1 = argmax
eI1
?
?
?Pr(e
I
1) ?
?
aJ1
Pr(fJ1 , aJ1 |eI1)
?
?
?
? argmax
eI1
{
Pr(eI1) ?maxaJ1
Pr(fJ1 , aJ1 |eI1)
}
Hence, the search space consists of the set of all pos-
sible target language sentences eI1 and all possible
alignments aJ1 .
Generalizing this approach to direct translation
models, we extend the feature functions to in-
clude the dependence on the additional hidden vari-
able. Using M feature functions of the form
hm(eI1, fJ1 , aJ1 ),m = 1, . . . ,M , we obtain the fol-
lowing model:
Pr(eI1, aJ1 |fJ1 ) =
=
exp
(?M
m=1 ?mhm(eI1, fJ1 , aJ1 )
)
?
e?I1,a?J1 exp
(?M
m=1 ?mhm(e?I1, fJ1 , a?J1 )
)
Obviously, we can perform the same step for transla-
tion models with an even richer structure of hidden
variables than only the alignment aJ1 . To simplify
the notation, we shall omit in the following the de-
pendence on the hidden variables of the model.
2 Alignment Templates
As specific MT method, we use the alignment tem-
plate approach (Och et al, 1999). The key elements
of this approach are the alignment templates, which
are pairs of source and target language phrases to-
gether with an alignment between the words within
the phrases. The advantage of the alignment tem-
plate approach compared to single word-based sta-
tistical translation models is that word context and
local changes in word order are explicitly consid-
ered.
The alignment template model refines the transla-
tion probability Pr(fJ1 |eI1) by introducing two hid-
den variables zK1 and aK1 for the K alignment tem-
plates and the alignment of the alignment templates:
Pr(fJ1 |eI1) =
?
zK1 ,aK1
Pr(aK1 |eI1) ?
Pr(zK1 |aK1 , eI1) ? Pr(fJ1 |zK1 , aK1 , eI1)
Hence, we obtain three different probability
distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and
Pr(fJ1 |zK1 , aK1 , eI1). Here, we omit a detailed de-
scription of modeling, training and search, as this is
not relevant for the subsequent exposition. For fur-
ther details, see (Och et al, 1999).
To use these three component models in a direct
maximum entropy approach, we define three dif-
ferent feature functions for each component of the
translation model instead of one feature function for
the whole translation model p(fJ1 |eI1). The feature
functions have then not only a dependence on fJ1
and eI1 but also on zK1 , aK1 .
3 Feature functions
So far, we use the logarithm of the components of
a translation model as feature functions. This is a
very convenient approach to improve the quality of
a baseline system. Yet, we are not limited to train
only model scaling factors, but we have many possi-
bilities:
? We could add a sentence length feature:
h(fJ1 , eI1) = I
This corresponds to a word penalty for each
produced target word.
? We could use additional language models by
using features of the following form:
h(fJ1 , eI1) = h(eI1)
? We could use a feature that counts how many
entries of a conventional lexicon co-occur in
the given sentence pair. Therefore, the weight
for the provided conventional dictionary can be
learned. The intuition is that the conventional
dictionary is expected to be more reliable than
the automatically trained lexicon and therefore
should get a larger weight.
? We could use lexical features, which fire if a
certain lexical relationship (f, e) occurs:
h(fJ1 , eI1) =
?
?
J?
j=1
?(f, fj)
?
? ?
( I?
i=1
?(e, ei)
)
? We could use grammatical features that relate
certain grammatical dependencies of source
and target language. For example, using a func-
tion k(?) that counts how many verb groups ex-
ist in the source or the target sentence, we can
define the following feature, which is 1 if each
of the two sentences contains the same number
of verb groups:
h(fJ1 , eI1) = ?(k(fJ1 ), k(eI1)) (12)
In the same way, we can introduce semantic
features or pragmatic features such as the di-
alogue act classification.
We can use numerous additional features that deal
with specific problems of the baseline statistical MT
system. In this paper, we shall use the first three of
these features. As additional language model, we
use a class-based five-gram language model. This
feature and the word penalty feature allow a straight-
forward integration into the used dynamic program-
ming search algorithm (Och et al, 1999). As this is
not possible for the conventional dictionary feature,
we use n-best rescoring for this feature.
4 Training
To train the model parameters ?M1 of the direct trans-
lation model according to Eq. 11, we use the GIS
(Generalized Iterative Scaling) algorithm (Darroch
and Ratcliff, 1972). It should be noted that, as
was already shown by (Darroch and Ratcliff, 1972),
by applying suitable transformations, the GIS algo-
rithm is able to handle any type of real-valued fea-
tures. To apply this algorithm, we have to solve var-
ious practical problems.
The renormalization needed in Eq. 8 requires a
sum over a large number of possible sentences,
for which we do not know an efficient algorithm.
Hence, we approximate this sum by sampling the
space of all possible sentences by a large set of
highly probable sentences. The set of considered
sentences is computed by an appropriately extended
version of the used search algorithm (Och et al,
1999) computing an approximate n-best list of trans-
lations.
Unlike automatic speech recognition, we do not
have one reference sentence, but there exists a num-
ber of reference sentences. Yet, the criterion as it
is described in Eq. 11 allows for only one reference
translation. Hence, we change the criterion to al-
low Rs reference translations es,1, . . . , es,Rs for the
sentence es:
??M1 = argmax
?M1
{ S?
s=1
1
Rs
Rs?
r=1
log p?M1 (es,r|fs)
}
We use this optimization criterion instead of the op-
timization criterion shown in Eq. 11.
In addition, we might have the problem that no
single of the reference translations is part of the n-
best list because the search algorithm performs prun-
ing, which in principle limits the possible transla-
tions that can be produced given a certain input sen-
tence. To solve this problem, we define for max-
imum entropy training each sentence as reference
translation that has the minimal number of word er-
rors with respect to any of the reference translations.
5 Results
We present results on the VERBMOBIL task, which
is a speech translation task in the domain of appoint-
ment scheduling, travel planning, and hotel reser-
vation (Wahlster, 1993). Table 1 shows the cor-
pus statistics of this task. We use a training cor-
pus, which is used to train the alignment template
model and the language models, a development cor-
pus, which is used to estimate the model scaling fac-
tors, and a test corpus.
Table 1: Characteristics of training corpus (Train),
manual lexicon (Lex), development corpus (Dev),
test corpus (Test).
German English
Train Sentences 58 073
Words 519 523 549 921
Singletons 3 453 1 698
Vocabulary 7 939 4 672
Lex Entries 12 779
Ext. Vocab. 11 501 6 867
Dev Sentences 276
Words 3 159 3 438
PP (trigr. LM) - 28.1
Test Sentences 251
Words 2 628 2 871
PP (trigr. LM) - 30.5
So far, in machine translation research does not
exist one generally accepted criterion for the evalu-
ation of the experimental results. Therefore, we use
a large variety of different criteria and show that the
obtained results improve on most or all of these cri-
teria. In all experiments, we use the following six
error criteria:
? SER (sentence error rate): The SER is com-
puted as the number of times that the generated
sentence corresponds exactly to one of the ref-
erence translations used for the maximum en-
tropy training.
? WER (word error rate): The WER is computed
as the minimum number of substitution, inser-
tion and deletion operations that have to be per-
formed to convert the generated sentence into
the target sentence.
? PER (position-independent WER): A short-
coming of the WER is the fact that it requires
a perfect word order. The word order of an
acceptable sentence can be different from that
of the target sentence, so that the WER mea-
sure alone could be misleading. To overcome
this problem, we introduce as additional mea-
sure the position-independent word error rate
(PER). This measure compares the words in the
two sentences ignoring the word order.
? mWER (multi-reference word error rate): For
each test sentence, there is not only used a sin-
gle reference translation, as for the WER, but
a whole set of reference translations. For each
translation hypothesis, the edit distance to the
most similar sentence is calculated (Nie?en et
al., 2000).
? BLEU score: This score measures the precision
of unigrams, bigrams, trigrams and fourgrams
with respect to a whole set of reference trans-
lations with a penalty for too short sentences
(Papineni et al, 2001). Unlike all other eval-
uation criteria used here, BLEU measures ac-
curacy, i.e. the opposite of error rate. Hence,
large BLEU scores are better.
? SSER (subjective sentence error rate): For a
more detailed analysis, subjective judgments
by test persons are necessary. Each trans-
lated sentence was judged by a human exam-
iner according to an error scale from 0.0 to 1.0
(Nie?en et al, 2000).
? IER (information item error rate): The test sen-
tences are segmented into information items.
For each of them, if the intended information
is conveyed and there are no syntactic errors,
the sentence is counted as correct (Nie?en et
al., 2000).
In the following, we present the results of this ap-
proach. Table 2 shows the results if we use a direct
translation model (Eq. 6).
As baseline features, we use a normal word tri-
gram language model and the three component mod-
els of the alignment templates. The first row shows
the results using only the four baseline features with
?1 = ? ? ? = ?4 = 1. The second row shows the
result if we train the model scaling factors. We see a
systematic improvement on all error rates. The fol-
lowing three rows show the results if we add the
word penalty, an additional class-based five-gram
Table 2: Effect of maximum entropy training for alignment template approach (WP: word penalty feature,
CLM: class-based language model (five-gram), MX: conventional dictionary).
objective criteria [%] subjective criteria [%]
SER WER PER mWER BLEU SSER IER
Baseline(?m = 1) 86.9 42.8 33.0 37.7 43.9 35.9 39.0
ME 81.7 40.2 28.7 34.6 49.7 32.5 34.8
ME+WP 80.5 38.6 26.9 32.4 54.1 29.9 32.2
ME+WP+CLM 78.1 38.3 26.9 32.1 55.0 29.1 30.9
ME+WP+CLM+MX 77.8 38.4 26.8 31.9 55.2 28.8 30.9
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0.9
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
se
nte
nc
e e
rro
r ra
te 
(SE
R)
 
number of iterations
MEME+WPME+WP+CLMME+WP+CLM+MX
Figure 3: Test error rate over the iterations of the
GIS algorithm for maximum entropy training of
alignment templates.
language model and the conventional dictionary fea-
tures. We observe improved error rates for using the
word penalty and the class-based language model as
additional features.
Figure 3 show how the sentence error rate (SER)
on the test corpus improves during the iterations of
the GIS algorithm. We see that the sentence error
rates converges after about 4000 iterations. We do
not observe significant overfitting.
Table 3 shows the resulting normalized model
scaling factors. Multiplying each model scaling fac-
tor by a constant positive value does not affect the
decision rule. We see that adding new features also
has an effect on the other model scaling factors.
6 Related Work
The use of direct maximum entropy translation mod-
els for statistical machine translation has been sug-
Table 3: Resulting model scaling factors of maxi-
mum entropy training for alignment templates; ?1:
trigram language model; ?2: alignment template
model, ?3: lexicon model, ?4: alignment model
(normalized such that ?4m=1 ?m = 4).
ME +WP +CLM +MX
?1 0.86 0.98 0.75 0.77
?2 2.33 2.05 2.24 2.24
?3 0.58 0.72 0.79 0.75
?4 0.22 0.25 0.23 0.24
WP ? 2.6 3.03 2.78
CLM ? ? 0.33 0.34
MX ? ? ? 2.92
gested by (Papineni et al, 1997; Papineni et al,
1998). They train models for natural language un-
derstanding rather than natural language translation.
In contrast to their approach, we include a depen-
dence on the hidden variable of the translation model
in the direct translation model. Therefore, we are
able to use statistical alignment models, which have
been shown to be a very powerful component for
statistical machine translation systems.
In speech recognition, training the parameters of
the acoustic model by optimizing the (average) mu-
tual information and conditional entropy as they are
defined in information theory is a standard approach
(Bahl et al, 1986; Ney, 1995). Combining various
probabilistic models for speech and language mod-
eling has been suggested in (Beyerlein, 1997; Peters
and Klakow, 1999).
7 Conclusions
We have presented a framework for statistical MT
for natural languages, which is more general than the
widely used source-channel approach. It allows a
baseline MT system to be extended easily by adding
new feature functions. We have shown that a base-
line statistical MT system can be significantly im-
proved using this framework.
There are two possible interpretations for a statis-
tical MT system structured according to the source-
channel approach, hence including a model for
Pr(eI1) and a model for Pr(fJ1 |eI1). We can inter-
pret it as an approximation to the Bayes decision rule
in Eq. 2 or as an instance of a direct maximum en-
tropy model with feature functions logPr(eI1) and
logPr(fJ1 |eI1). As soon as we want to use model
scaling factors, we can only do this in a theoretically
justified way using the second interpretation. Yet,
the main advantage comes from the large number of
additional possibilities that we obtain by using the
second interpretation.
An important open problem of this approach is
the handling of complex features in search. An in-
teresting question is to come up with features that
allow an efficient handling using conventional dy-
namic programming search algorithms.
In addition, it might be promising to optimize the
parameters directly with respect to the error rate of
the MT system as is suggested in the field of pattern
and speech recognition (Juang et al, 1995; Schlu?ter
and Ney, 2001).
References
L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-
cer. 1986. Maximum mutual information estimation
of hidden markov model parameters. In Proc. Int.
Conf. on Acoustics, Speech, and Signal Processing,
pages 49?52, Tokyo, Japan, April.
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?72, March.
P. Beyerlein. 1997. Discriminative model combina-
tion. In Proc. of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 238?
245, Santa Barbara, CA, December.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470?1480.
B. H. Juang, W. Chou, and C. H. Lee. 1995. Statisti-
cal and discriminative methods for speech recognition.
In A. J. R. Ayuso and J. M. L. Soler, editors, Speech
Recognition and Coding - New Advances and Trends.
Springer Verlag, Berlin, Germany.
H. Ney. 1995. On the probabilistic-interpretation of
neural-network classifiers and discriminative training
criteria. IEEE Trans. on Pattern Analysis and Machine
Intelligence, 17(2):107?119, February.
S. Nie?en, F. J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In Proc. of the Second Int.
Conf. on Language Resources and Evaluation (LREC),
pages 39?45, Athens, Greece, May.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 20?28, University of Maryland, Col-
lege Park, MD, June.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In European
Conf. on Speech Communication and Technology,
pages 1435?1438, Rhodes, Greece, September.
K. A. Papineni, S. Roukos, and R. T. Ward. 1998. Max-
imum likelihood and discriminative training of direct
translation models. In Proc. Int. Conf. on Acoustics,
Speech, and Signal Processing, pages 189?192, Seat-
tle, WA, May.
K. A. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, Yorktown Heights, NY, September.
J. Peters and D. Klakow. 1999. Compact maximum en-
tropy language models. In Proc. of the IEEE Workshop
on Automatic Speech Recognition and Understanding,
Keystone, CO, December.
R. Schlu?ter and H. Ney. 2001. Model-based MCE bound
to the true Bayes? error. IEEE Signal Processing Let-
ters, 8(5):131?133, May.
W. Wahlster. 1993. Verbmobil: Translation of face-to-
face dialogs. In Proc. of MT Summit IV, pages 127?
135, Kobe, Japan, July.
A Comparative Study on Reordering Constraints in Statistical Machine
Translation
Richard Zens and Hermann Ney
Chair of Computer Science VI
RWTH Aachen - University of Technology
{zens,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, the gen-
eration of a translation hypothesis is com-
putationally expensive. If arbitrary word-
reorderings are permitted, the search prob-
lem is NP-hard. On the other hand, if
we restrict the possible word-reorderings
in an appropriate way, we obtain a
polynomial-time search algorithm.
In this paper, we compare two different re-
ordering constraints, namely the ITG con-
straints and the IBM constraints. This
comparison includes a theoretical dis-
cussion on the permitted number of re-
orderings for each of these constraints.
We show a connection between the ITG
constraints and the since 1870 known
Schro?der numbers.
We evaluate these constraints on two
tasks: the Verbmobil task and the Cana-
dian Hansards task. The evaluation con-
sists of two parts: First, we check how
many of the Viterbi alignments of the
training corpus satisfy each of these con-
straints. Second, we restrict the search to
each of these constraints and compare the
resulting translation hypotheses.
The experiments will show that the base-
line ITG constraints are not sufficient
on the Canadian Hansards task. There-
fore, we present an extension to the ITG
constraints. These extended ITG con-
straints increase the alignment coverage
from about 87% to 96%.
1 Introduction
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =
f1 . . . fj . . . fJ , which is to be translated into a target
language (?English?) sentence eI1 = e1 . . . ei . . . eI .
Among all possible target language sentences, we
will choose the sentence with the highest probabil-
ity:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )} (1)
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)} (2)
The decomposition into two knowledge sources
in Eq. 2 is the so-called source-channel approach
to statistical machine translation (Brown et al,
1990). It allows an independent modeling of tar-
get language model Pr(eI1) and translation model
Pr(fJ1 |eI1). The target language model describes
the well-formedness of the target language sentence.
The translation model links the source language sen-
tence to the target language sentence. It can be fur-
ther decomposed into alignment and lexicon model.
The argmax operation denotes the search problem,
i.e. the generation of the output sentence in the tar-
get language. We have to maximize over all possible
target language sentences.
In this paper, we will focus on the alignment
problem, i.e. the mapping between source sen-
tence positions and target sentence positions. As
the word order in source and target language may
differ, the search algorithm has to allow certain
word-reorderings. If arbitrary word-reorderings are
allowed, the search problem is NP-hard (Knight,
1999). Therefore, we have to restrict the possible
reorderings in some way to make the search prob-
lem feasible. Here, we will discuss two such con-
straints in detail. The first constraints are based on
inversion transduction grammars (ITG) (Wu, 1995;
Wu, 1997). In the following, we will call these the
ITG constraints. The second constraints are the IBM
constraints (Berger et al, 1996). In the next section,
we will describe these constraints from a theoretical
point of view. Then, we will describe the resulting
search algorithm and its extension for word graph
generation. Afterwards, we will analyze the Viterbi
alignments produced during the training of the align-
ment models. Then, we will compare the translation
results when restricting the search to either of these
constraints.
2 Theoretical Discussion
In this section, we will discuss the reordering con-
straints from a theoretical point of view. We will
answer the question of how many word-reorderings
are permitted for the ITG constraints as well as for
the IBM constraints. Since we are only interested
in the number of possible reorderings, the specific
word identities are of no importance here. Further-
more, we assume a one-to-one correspondence be-
tween source and target words. Thus, we are inter-
ested in the number of word-reorderings, i.e. permu-
tations, that satisfy the chosen constraints. First, we
will consider the ITG constraints. Afterwards, we
will describe the IBM constraints.
2.1 ITG Constraints
Let us now consider the ITG constraints. Here, we
interpret the input sentence as a sequence of blocks.
In the beginning, each position is a block of its own.
Then, the permutation process can be seen as fol-
lows: we select two consecutive blocks and merge
them to a single block by choosing between two op-
tions: either keep them in monotone order or invert
the order. This idea is illustrated in Fig. 1. The white
boxes represent the two blocks to be merged.
Now, we investigate, how many permutations are
obtainable with this method. A permutation derived
by the above method can be represented as a binary
tree where the inner nodes are colored either black or
white. At black nodes the resulting sequences of the
children are inverted. At white nodes they are kept in
monotone order. This representation is equivalent to
source positions
ta
rg
et
 p
os
it
io
ns
without inversion with inversion
Figure 1: Illustration of monotone and inverted con-
catenation of two consecutive blocks.
the parse trees of the simple grammar in (Wu, 1997).
We observe that a given permutation may be con-
structed in several ways by the above method. For
instance, let us consider the identity permutation of
1, 2, ..., n. Any binary tree with n nodes and all in-
ner nodes colored white (monotone order) is a pos-
sible representation of this permutation. To obtain
a unique representation, we pose an additional con-
straint on the binary trees: if the right son of a node
is an inner node, it has to be colored with the oppo-
site color. With this constraint, each of these binary
trees is unique and equivalent to a parse tree of the
?canonical-form? grammar in (Wu, 1997).
In (Shapiro and Stephens, 1991), it is shown that
the number of such binary trees with n nodes is
the (n ? 1)th large Schro?der number Sn?1. The
(small) Schro?der numbers have been first described
in (Schro?der, 1870) as the number of bracketings of
a given sequence (Schro?der?s second problem). The
large Schro?der numbers are just twice the Schro?der
numbers. Schro?der remarked that the ratio between
two consecutive Schro?der numbers approaches 3 +
2?2 = 5.8284... . A second-order recurrence for
the large Schro?der numbers is:
(n+ 1)Sn = 3(2n? 1)Sn?1 ? (n? 2)Sn?2
with n ? 2 and S0 = 1, S1 = 2.
The Schro?der numbers have many combinatori-
cal interpretations. Here, we will mention only two
of them. The first one is another way of view-
ing at the ITG constraints. The number of permu-
tations of the sequence 1, 2, ..., n, which avoid the
subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large
Schro?der number Sn?1. More details on forbidden
subsequences can be found in (West, 1995). The
interesting point is that a search with the ITG con-
straints cannot generate a word-reordering that con-
tains one of these two subsequences. In (Wu, 1997),
these forbidden subsequences are called ?inside-out?
transpositions.
Another interpretation of the Schro?der numbers is
given in (Knuth, 1973): The number of permutations
that can be sorted with an output-restricted double-
ended queue (deque) is exactly the large Schro?der
number. Additionally, Knuth presents an approxi-
mation for the large Schro?der numbers:
Sn ? c ? (3 +
?
8)n ? n? 32 (3)
where c is set to 12
?
(3?2? 4)/pi. This approxi-
mation function confirms the result of Schro?der, and
we obtain Sn ? ?((3 +
?8)n), i.e. the Schro?der
numbers grow like (3 +?8)n ? 5.83n.
2.2 IBM Constraints
In this section, we will describe the IBM constraints
(Berger et al, 1996). Here, we mark each position in
the source sentence either as covered or uncovered.
In the beginning, all source positions are uncovered.
Now, the target sentence is produced from bottom to
top. A target position must be aligned to one of the
first k uncovered source positions. The IBM con-
straints are illustrated in Fig. 2.
J
uncovered position
covered positionuncovered position for extension
1 j
Figure 2: Illustration of the IBM constraints.
For most of the target positions there are k per-
mitted source positions. Only towards the end of the
sentence this is reduced to the number of remaining
uncovered source positions. Let n denote the length
of the input sequence and let rn denote the permitted
number of permutations with the IBM constraints.
Then, we obtain:
rn =
{ kn?k ? k! n > k
n! n ? k (4)
Typically, k is set to 4. In this case, we obtain an
asymptotic upper and lower bound of 4n, i.e. rn ?
?(4n).
In Tab. 1, the ratio of the number of permitted re-
orderings for the discussed constraints is listed as
a function of the sentence length. We see that for
longer sentences the ITG constraints allow for more
reorderings than the IBM constraints. For sentences
of length 10 words, there are about twice as many
reorderings for the ITG constraints than for the IBM
constraints. This ratio steadily increases. For longer
sentences, the ITG constraints allow for much more
flexibility than the IBM constraints.
3 Search
Now, let us get back to more practical aspects. Re-
ordering constraints are more or less useless, if they
do not allow the maximization of Eq. 2 to be per-
formed in an efficient way. Therefore, in this sec-
tion, we will describe different aspects of the search
algorithm for the ITG constraints. First, we will
present the dynamic programming equations and the
resulting complexity. Then, we will describe prun-
ing techniques to accelerate the search. Finally, we
will extend the basic algorithm for the generation of
word graphs.
3.1 Algorithm
The ITG constraints allow for a polynomial-time
search algorithm. It is based on the following dy-
namic programming recursion equations. During
the search a table Qjl,jr,eb,et is constructed. Here,
Qjl,jr,eb,et denotes the probability of the best hy-
pothesis translating the source words from position
jl (left) to position jr (right) which begins with the
target language word eb (bottom) and ends with the
word et (top). This is illustrated in Fig. 3.
Here, we initialize this table with monotone trans-
lations of IBM Model 4. Therefore, Q0jl,jr,eb,et de-
notes the probability of the best monotone hypothe-
sis of IBM Model 4. Alternatively, we could use any
other single-word based lexicon as well as phrase-
based models for this initialization. Our choice is
the IBM Model4 to make the results as comparable
Table 1: Ratio of the number of permitted reorderings with the ITG constraints Sn?1 and the IBM constraints
rn for different sentence lengths n.
n 1 ... 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Sn?1/rn ? 1.0 1.2 1.4 1.7 2.1 2.6 3.4 4.3 5.6 7.4 9.8 13.0 17.4 23.3 31.4
jl jr
e b
et
Figure 3: Illustration of the Q-table.
as possible to the search with the IBM constraints.
We introduce a new parameter pm (m=? monotone),
which denotes the probability of a monotone combi-
nation of two partial hypotheses.
Qjl,jr,eb,et = (5)
max
jl?k<jr,
e?,e??
{
Q0jl,jr,eb,et ,
Qjl,k,eb,e? ?Qk+1,jr,e??,et ? p(e??|e?) ? pm,
Qk+1,jr,eb,e? ?Qjl,k,e??,et ? p(e??|e?) ? (1? pm)
}
We formulated this equation for a bigram lan-
guage model, but of course, the same method can
also be applied for a trigram language model. The
resulting algorithm is similar to the CYK-parsing al-
gorithm. It has a worst-case complexity of O(J3 ?
E4). Here, J is the length of the source sentence
and E is the vocabulary size of the target language.
3.2 Pruning
Although the described search algorithm has a
polynomial-time complexity, even with a bigram
language model the search space is very large. A full
search is possible but time consuming. The situation
gets even worse when a trigram language model is
used. Therefore, pruning techniques are obligatory
to reduce the translation time.
Pruning is applied to hypotheses that translate the
same subsequence f jrjl of the source sentence. We
use pruning in the following two ways. The first
pruning technique is histogram pruning: we restrict
the number of translation hypotheses per sequence
f jrjl . For each sequence f
jr
jl , we keep only a fixed
number of translation hypotheses. The second prun-
ing technique is threshold pruning: the idea is to re-
move all hypotheses that have a low probability rela-
tive to the best hypothesis. Therefore, we introduce
a threshold pruning parameter q, with 0 ? q ? 1.
Let Q?jl,jr denote the maximum probability of all
translation hypotheses for f jrjl . Then, we prune a
hypothesis iff:
Qjl,jr,eb,et < q ?Q?jl,jr
Applying these pruning techniques the computa-
tional costs can be reduced significantly with almost
no loss in translation quality.
3.3 Generation of Word Graphs
The generation of word graphs for a bottom-top
search with the IBM constraints is described in
(Ueffing et al, 2002). These methods cannot be
applied to the CYK-style search for the ITG con-
straints. Here, the idea for the generation of word
graphs is the following: assuming we already have
word graphs for the source sequences fkjl and f
jr
k+1,
then we can construct a word graph for the sequence
f jrjl by concatenating the partial word graphs either
in monotone or inverted order.
Now, we describe this idea in a more formal way.
A word graph is a directed acyclic graph (dag) with
one start and one end node. The edges are annotated
with target language words or phrases. We also al-
low ?-transitions. These are edges annotated with
the empty word. Additionally, edges may be anno-
tated with probabilities of the language or translation
model. Each path from start node to end node rep-
resents one translation hypothesis. The probability
of this hypothesis is calculated by multiplying the
probabilities along the path.
During the search, we have to combine two word
graphs in either monotone or inverted order. This
is done in the following way: we are given two
word graphs w1 and w2 with start and end nodes
(s1, g1) and (s2, g2), respectively. First, we add
an ?-transition (g1, s2) from the end node of the
first graph w1 to the start node of the second graph
w2 and annotate this edge with the probability of a
monotone concatenation pm. Second, we create a
copy of each of the original word graphs w1 and w2.
Then, we add an ?-transition (g2, s1) from the end
node of the copied second graph to the start node of
the copied first graph. This edge is annotated with
the probability of a inverted concatenation 1 ? pm.
Now, we have obtained two word graphs: one for a
monotone and one for a inverted concatenation. The
final word graphs is constructed by merging the two
start nodes and the two end nodes, respectively.
Let W (jl, jr) denote the word graph for the
source sequence f jrjl . This graph is constructed
from the word graphs of all subsequences of (jl, jr).
Therefore, we assume, these word graphs have al-
ready been produced. For all source positions k with
jl ? k < jr, we combine the word graphs W (jl, k)
and W (k + 1, jr) as described above. Finally, we
merge all start nodes of these graphs as well as all
end nodes. Now, we have obtained the word graph
W (jl, jr) for the source sequence f jrjl . As initializa-
tion, we use the word graphs of the monotone IBM4
search.
3.4 Extended ITG constraints
In this section, we will extend the ITG constraints
described in Sec. 2.1. This extension will go beyond
basic reordering constraints.
We already mentioned that the use of consecutive
phrases within the ITG approach is straightforward.
The only thing we have to change is the initializa-
tion of the Q-table. Now, we will extend this idea to
phrases that are non-consecutive in the source lan-
guage. For this purpose, we adopt the view of the
ITG constraints as a bilingual grammar as, e.g., in
(Wu, 1997). For the baseline ITG constraints, the
resulting grammar is:
A ? [AA] | ?AA? | f/e | f/? | ?/e
Here, [AA] denotes a monotone concatenation and
?AA? denotes an inverted concatenation.
Let us now consider the case of a source phrase
consisting of two parts f1 and f2. Let e denote the
corresponding target phrase. We add the productions
A ? [e/f1 A ?/f2] | ?e/f1 A ?/f2?
to the grammar. The probabilities of these pro-
ductions are, dependent on the translation direction,
p(e|f1, f2) or p(f1, f2|e), respectively. Obviously,
these productions are not in the normal form of an
ITG, but with the method described in (Wu, 1997),
they can be normalized.
4 Corpus Statistics
In the following sections we will present results on
two tasks. Therefore, in this section we will show
the corpus statistics for each of these tasks.
4.1 Verbmobil
The first task we will present results on is the Verb-
mobil task (Wahlster, 2000). The domain of this
corpus is appointment scheduling, travel planning,
and hotel reservation. It consists of transcriptions
of spontaneous speech. Table 2 shows the corpus
statistics of this corpus. The training corpus (Train)
was used to train the IBM model parameters. The
remaining free parameters, i.e. pm and the model
scaling factors (Och and Ney, 2002), were adjusted
on the development corpus (Dev). The resulting sys-
tem was evaluated on the test corpus (Test).
Table 2: Statistics of training and test corpus for
the Verbmobil task (PP=perplexity, SL=sentence
length).
German English
Train Sentences 58 073
Words 519 523 549 921
Vocabulary 7 939 4 672
Singletons 3 453 1 698
average SL 8.9 9.5
Dev Sentences 276
Words 3 159 3 438
Trigram PP - 28.1
average SL 11.5 12.5
Test Sentences 251
Words 2 628 2 871
Trigram PP - 30.5
average SL 10.5 11.4
Table 3: Statistics of training and test corpus
for the Canadian Hansards task (PP=perplexity,
SL=sentence length).
French English
Train Sentences 1.5M
Words 24M 22M
Vocabulary 100 269 78 332
Singletons 40 199 31 319
average SL 16.6 15.1
Test Sentences 5432
Words 97 646 88 773
Trigram PP ? 179.8
average SL 18.0 16.3
4.2 Canadian Hansards
Additionally, we carried out experiments on the
Canadian Hansards task. This task contains the pro-
ceedings of the Canadian parliament, which are kept
by law in both French and English. About 3 million
parallel sentences of this bilingual data have been
made available by the Linguistic Data Consortium
(LDC). Here, we use a subset of the data containing
only sentences with a maximum length of 30 words.
Table 3 shows the training and test corpus statistics.
5 Evaluation in Training
In this section, we will investigate for each of the
constraints the coverage of the training corpus align-
ment. For this purpose, we compute the Viterbi
alignment of IBM Model 5 with GIZA++ (Och and
Ney, 2000). This alignment is produced without any
restrictions on word-reorderings. Then, we check
for every sentence if the alignment satisfies each of
the constraints. The ratio of the number of satisfied
alignments and the total number of sentences is re-
ferred to as coverage. Tab. 4 shows the results for
the Verbmobil task and for the Canadian Hansards
task. It contains the results for both translation direc-
tions German-English (S?T) and English-German
(T?S) for the Verbmobil task and French-English
(S?T) and English-French (T?S) for the Canadian
Hansards task, respectively.
For the Verbmobil task, the baseline ITG con-
straints and the IBM constraints result in a similar
coverage. It is about 91% for the German-English
translation direction and about 88% for the English-
German translation direction. A significantly higher
Table 4: Coverage on the training corpus for align-
ment constraints for the Verbmobil task (VM) and
for the Canadian Hansards task (CH).
coverage [%]
task constraint S?T T?S
VM IBM 91.0 88.1
ITG baseline 91.6 87.0
extended 96.5 96.9
CH IBM 87.1 86.7
ITG baseline 81.3 73.6
extended 96.1 95.6
coverage of about 96% is obtained with the extended
ITG constraints. Thus with the extended ITG con-
straints, the coverage increases by about 8% abso-
lute.
For the Canadian Hansards task, the baseline ITG
constraints yield a worse coverage than the IBM
constraints. Especially for the English-French trans-
lation direction, the ITG coverage of 73.6% is very
low. Again, the extended ITG constraints obtained
the best results. Here, the coverage increases from
about 87% for the IBM constraints to about 96% for
the extended ITG constraints.
6 Translation Experiments
6.1 Evaluation Criteria
In our experiments, we use the following error crite-
ria:
? WER (word error rate):
The WER is computed as the minimum num-
ber of substitution, insertion and deletion oper-
ations that have to be performed to convert the
generated sentence into the target sentence.
? PER (position-independent word error rate):
A shortcoming of the WER is the fact that it
requires a perfect word order. The PER com-
pares the words in the two sentences ignoring
the word order.
? mWER (multi-reference word error rate):
For each test sentence, not only a single refer-
ence translation is used, as for the WER, but a
whole set of reference translations. For each
translation hypothesis, the WER to the most
similar sentence is calculated (Nie?en et al,
2000).
? BLEU score:
This score measures the precision of unigrams,
bigrams, trigrams and fourgrams with respect
to a whole set of reference translations with a
penalty for too short sentences (Papineni et al,
2001). BLEU measures accuracy, i.e. large
BLEU scores are better.
? SSER (subjective sentence error rate):
For a more detailed analysis, subjective judg-
ments by test persons are necessary. Each
translated sentence was judged by a human ex-
aminer according to an error scale from 0.0 to
1.0 (Nie?en et al, 2000).
6.2 Translation Results
In this section, we will present the translation results
for both the IBM constraints and the baseline ITG
constraints. We used a single-word based search
with IBM Model 4. The initialization for the ITG
constraints was done with monotone IBM Model 4
translations. So, the only difference between the two
systems are the reordering constraints.
In Tab. 5 the results for the Verbmobil task are
shown. We see that the results on this task are sim-
ilar. The search with the ITG constraints yields
slightly lower error rates.
Some translation examples of the Verbmobil task
are shown in Tab. 6. We have to keep in mind,
that the Verbmobil task consists of transcriptions
of spontaneous speech. Therefore, the source sen-
tences as well as the reference translations may have
an unorthodox grammatical structure. In the first
example, the German verb-group (?wu?rde vorschla-
gen?) is split into two parts. The search with the
ITG constraints is able to produce a correct transla-
tion. With the IBM constraints, it is not possible to
translate this verb-group correctly, because the dis-
tance between the two parts is too large (more than
four words). As we see in the second example, in
German the verb of a subordinate clause is placed at
the end (?u?bernachten?). The IBM search is not able
to perform the necessary long-range reordering, as it
is done with the ITG search.
7 Related Work
The ITG constraints were introduced in (Wu, 1995).
The applications were, for instance, the segmenta-
tion of Chinese character sequences into Chinese
?words? and the bracketing of the source sentence
into sub-sentential chunks. In (Wu, 1996) the base-
line ITG constraints were used for statistical ma-
chine translation. The resulting algorithm is simi-
lar to the one presented in Sect. 3.1, but here, we
use monotone translation hypotheses of the full IBM
Model 4 as initialization, whereas in (Wu, 1996) a
single-word based lexicon model is used. In (Vilar,
1998) a model similar to Wu?s method was consid-
ered.
8 Conclusions
We have described the ITG constraints in detail and
compared them to the IBM constraints. We draw the
following conclusions: especially for long sentences
the ITG constraints allow for higher flexibility in
word-reordering than the IBM constraints. Regard-
ing the Viterbi alignment in training, the baseline
ITG constraints yield a similar coverage as the IBM
constraints on the Verbmobil task. On the Canadian
Hansards task the baseline ITG constraints were not
sufficient. With the extended ITG constraints the
coverage improves significantly on both tasks. On
the Canadian Hansards task the coverage increases
from about 87% to about 96%.
We have presented a polynomial-time search al-
gorithm for statistical machine translation based on
the ITG constraints and its extension for the gen-
eration of word graphs. We have shown the trans-
lation results for the Verbmobil task. On this task,
the translation quality of the search with the base-
line ITG constraints is already competitive with the
results for the IBM constraints. Therefore, we ex-
pect the search with the extended ITG constraints to
outperform the search with the IBM constraints.
Future work will include the automatic extraction
of the bilingual grammar as well as the use of this
grammar for the translation process.
References
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra,
J. R. Gillett, A. S. Kehler, and R. L. Mercer. 1996.
Language translation apparatus and method of using
context-based translation models, United States patent,
patent number 5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
Table 5: Translation results on the Verbmobil task.
type automatic human
System WER [%] PER [%] mWER [%] BLEU [%] SSER [%]
IBM 46.2 33.3 40.0 42.5 40.8
ITG 45.6 33.9 40.0 37.1 42.0
Table 6: Verbmobil: translation examples.
source ja, ich wu?rde den Flug um viertel nach sieben vorschlagen.
reference yes, I would suggest the flight at a quarter past seven.
ITG yes, I would suggest the flight at seven fifteen.
IBM yes, I would be the flight at quarter to seven suggestion.
source ich schlage vor, dass wir in Hannover im Hotel Gru?nschnabel u?bernachten.
reference I suggest to stay at the hotel Gru?nschnabel in Hanover.
ITG I suggest that we stay in Hanover at hotel Gru?nschnabel.
IBM I suggest that we are in Hanover at hotel Gru?nschnabel stay.
translation. Computational Linguistics, 16(2):79?85,
June.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
D. E. Knuth. 1973. The Art of Computer Program-
ming, volume 1 - Fundamental Algorithms. Addison-
Wesley, Reading, MA, 2nd edition.
S. Nie?en, F. J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In Proc. of the Second Int.
Conf. on Language Resources and Evaluation (LREC),
pages 39?45, Athens, Greece, May.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of the 38th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 440?447, Hong Kong, October.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295?302, July.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
E. Schro?der. 1870. Vier combinatorische Probleme.
Zeitschrift fu?r Mathematik und Physik, 15:361?376.
L. Shapiro and A. B. Stephens. 1991. Boostrap percola-
tion, the Schro?der numbers, and the n-kings problem.
SIAM Journal on Discrete Mathematics, 4(2):275?
280, May.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation
of word graphs in statistical machine translation. In
Proc. Conf. on Empirical Methods for Natural Lan-
guage Processing, pages 156?163, Philadelphia, PA,
July.
J. M. Vilar. 1998. Aprendizaje de Transductores Subse-
cuenciales para su empleo en tareas de Dominio Re-
stringido. Ph.D. thesis, Universidad Politecnica de Va-
lencia.
W. Wahlster, editor. 2000. Verbmobil: Foundations
of speech-to-speech translations. Springer Verlag,
Berlin, Germany, July.
J. West. 1995. Generating trees and the Catalan and
Schro?der numbers. Discrete Mathematics, 146:247?
262, November.
D. Wu. 1995. Stochastic inversion transduction gram-
mars, with application to segmentation, bracketing,
and alignment of parallel corpora. In Proc. of the 14th
International Joint Conf. on Artificial Intelligence (IJ-
CAI), pages 1328?1334, Montreal, August.
D. Wu. 1996. A polynomial-time algorithm for statis-
tical machine translation. In Proc. of the 34th Annual
Conf. of the Association for Computational Linguistics
(ACL ?96), pages 152?158, Santa Cruz, CA, June.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
FSA: An Efficient and Flexible C++ Toolkit for Finite State Automata
Using On-Demand Computation
Stephan Kanthak and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen ? University of Technology
52056 Aachen, Germany
{kanthak,ney}@informatik.rwth-aachen.de
Abstract
In this paper we present the RWTH FSA toolkit ? an
efficient implementation of algorithms for creating
and manipulating weighted finite-state automata.
The toolkit has been designed using the principle
of on-demand computation and offers a large range
of widely used algorithms. To prove the superior
efficiency of the toolkit, we compare the implemen-
tation to that of other publically available toolkits.
We also show that on-demand computations help to
reduce memory requirements significantly without
any loss in speed. To increase its flexibility, the
RWTH FSA toolkit supports high-level interfaces
to the programming language Python as well as a
command-line tool for interactive manipulation of
FSAs. Furthermore, we show how to utilize the
toolkit to rapidly build a fast and accurate statisti-
cal machine translation system. Future extensibility
of the toolkit is ensured as it will be publically avail-
able as open source software.
1 Introduction
Finite-state automata (FSA) methods proved to el-
egantly solve many difficult problems in the field
of natural language processing. Among the most
recent ones are full and lazy compilation of the
search network for speech recognition (Mohri et
al., 2000a), integrated speech translation (Vidal,
1997; Bangalore and Riccardi, 2000), speech sum-
marization (Hori et al, 2003), language modelling
(Allauzen et al, 2003) and parameter estimation
through EM (Eisner, 2001) to mention only a few.
From this list of different applications it is clear that
there is a high demand for generic tools to create
and manipulate FSAs.
In the past, a number of toolkits have been pub-
lished, all with different design principles. Here, we
give a short overview of toolkits that offer an almost
complete set of algorithms:
? The FSM LibraryTM from AT&T (Mohri et
al., 2000b) is judged the most efficient im-
plementation, offers various semirings, on-
demand computation and many algorithms, but
is available only in binary form with a propri-
etary, non commercial license.
? FSA6.1 from (van Noord, 2000) is imple-
mented in Prolog. It is licensed under the terms
of the (GPL, 1991).
? The WFST toolkit from (Adant, 2000) is built
on top of the Automaton Standard Template
Library (LeMaout, 1998) and uses C++ tem-
plate mechanisms for efficiency and flexibil-
ity, but lacks on-demand computation. Also
licensed under the terms of the (GPL, 1991).
This paper describes a highly efficient new im-
plementation of a finite-state automata toolkit that
uses on-demand computation. Currently, it is
being used at the Lehrstuhl fu?r Informatik VI,
RWTH Aachen in different speech recognition
and translation research applications. The toolkit
will be available under an open source license
(GPL, 1991) and can be obtained from our website
http://www-i6.informatik.rwth-aachen.de.
The remaining part of the paper is organized
as follows: Section 2 will give a short introduc-
tion to the theory of finite-state automata to re-
call part of the terminology and notation. We will
also give a short explanation of composition which
we use as an exemplary object of study in the fol-
lowing sections. In Section 2.3 we will discuss
the locality of algorithms defined on finite-state au-
tomata. This forms the basis for implementations
using on-demand computations. Then the RWTH
FSA toolkit implementation is detailed in Section
3. In Section 4.1 we will compare the efficiency of
different toolkits. As a showcase for the flexibility
we show how to use the toolkit to build a statistical
machine translation system in Section 4.2. We con-
clude the paper with a short summary in Section 5
and discuss some possible future extensions in Sec-
tion 6.
2 Finite-State Automata
2.1 Weighted Finite-State Transducer
The basic theory of weighted finite-state automata
has been reviewed in numerous papers (Mohri,
1997; Allauzen et al, 2003). We will introduce the
notation briefly.
A semiring (K,?,?, 0, 1) is a structure with a
set K and two binary operations ? and ? such
that (K,?, 0) is a commutative monoid, (K,?, 1)
is a monoid and ? distributes over ? and 0 ?
x = x ? 0 = 0 for any x ? K. We will
also associate the term weights with the elements
of a semiring. Semirings that are frequently used
in speech recognition are the positive real semir-
ing (IR?{??,+?},?log,+,+?, 0) with a?log
b = ?log(e?a + e?b) and the tropical semiring
(IR?{??,+?},min,+,+?, 0) representing the
well-known sum and maximum weighted path cri-
teria.
A weighted finite-state transducer (Q,? ?
{?},? ? {?},K,E, i, F, ?, ?) is a structure with a
set Q of states1, an alphabet ? of input symbols,
an alphabet ? of output symbols, a weight semir-
ing K (we assume it k-closed here for some algo-
rithms as described in (Mohri and Riley, 2001)), a
set E ? Q ? (? ? {?}) ? (? ? {?}) ?K ? Q of
arcs, a single initial state i with weight ? and a set of
final states F weighted by the function ? : F ? K.
To simplify the notation we will also denote with
QT and ET the set of states and arcs of a trans-
ducer T. A weighted finite-state acceptor is simply
a weighted finite-state transducer without the output
alphabet.
2.2 Composition
As we will refer to this example throughout the pa-
per we shortly review the composition algorithm
here. Let T1 : ????? ? K and T2 : ????? ? K
be two transducers defined over the same semiring
K. Their composition T1 ? T2 realizes the function
T : ????? ? K and the theory has been described
in detail in (Pereira and Riley, 1996).
For simplification purposes, let us assume that the
input automata are ?-free and S = (Q1?Q2,?,?
, empty) is a stack of state tuples of T1 and T2 with
push, pop and empty test operations. A non lazy
version of composition is shown in Figure 1.
Composition of automata containing ? labels is
more complex and can be solved by using an in-
termediate filter transducer that also has been de-
scribed in (Pereira and Riley, 1996).
1we do not restrict this to be a finite set as most algorithms
of the lazy implementation presented in this paper also support
a virtually infinite set
T = T1 ? T2 :
i = (i1, i2)
S ? (i1, i2)
while not S empty
(s1, s2) ? S
QT = QT ? (s1, s2)
foreach (s1, i1, o1, w1, t1) ? ET1
foreach (s2, i2, o2, w2, t2) ? ET2 with o1 = i2
ET = ET ? ((s1, s2), i1, o2, w1 ? w2, (t1, t2))
if (t1, t2) 6? QT then S ? (t1, t2)
Figure 1: Simplified version of composition (as-
sumes ?-free input transducers).
What we can see from the pseudo-code above is
that composition uses tuples of states of the two in-
put transducers to describe states of the target trans-
ducer. Other operations defined on weighted finite-
state automata use different abstract states. For
example transducer determinization (Mohri, 1997)
uses a set of pairs of states and weights. However,
it is more convenient to use integers as state indices
for an implementation. Therefore algorithms usu-
ally maintain a mapping from abstract states to in-
teger state indices. This mapping has linear mem-
ory requirements of O(|QT |) which is quite attrac-
tive, but that depends on the structure of the abstract
states. Especially in case of determinization where
the size of an abstract state may vary, the complex-
ity is no longer linear in general.
2.3 Local Algorithms
Mohri and colleagues pointed out (Mohri et al,
2000b) that a special class of transducer algorithms
can be computed on demand. We will give a more
detailed analysis here. We focus on algorithms that
produce a single transducer and refer to them as al-
gorithmic transducers.
Definition: Let ? be the input configuration of
an algorithm A(?) that outputs a single finite-state
transducer T. Additionally, let M : S ? QT be
a one-to-one mapping from the set of abstract state
descriptions S that A generates onto the set of states
of T . We call A local iff for all states s ? QT A
can generate a state s of T and all outgoing arcs
(s, i, o, w, s?) ? ET , depending only on its abstract
state M?1(s) and the input configuration ?.
With the preceding definition it is quite easy to
prove the following lemma:
Lemma: An algorithm A that has the local prop-
erty can be built on demand starting with the ini-
tial state iTA of its associated algorithmic transducer
TA.
Proof: For the proof it is sufficient to show that
we can generate and therefore reach all states of TA.
Let S be a stack of states of TA that we still have
to process. Due to the one-to-one mapping M we
can map each state of TA back to an abstract state
of A. By definition the abstract state is sufficient to
generate the complete state and its outgoing arcs.
We then push those target states of all outgoing arcs
onto the stack S that have not yet been processed.
As TA is finite the traversal ends after all states of
TA as been processed exactly once. 2
Algorithmic transducers that can be computed
on-demand are also called lazy or virtual transduc-
ers. Note, that due to the local property the set of
states does not necessarily be finite anymore.
3 The Toolkit
The current implementation is the second version of
this toolkit. For the first version ? which was called
FSM ? we opted for using C++ templates to gain ef-
ficiency, but algorithms were not lazy. It turned out
that the implementation was fast, but many opera-
tions wasted a lot of memory as their resulting trans-
ducer had been fully expanded in memory. How-
ever, we plan to also make this initial version publi-
cally available.
The design principles of the second version of the
toolkit, which we will call FSA, are:
? decoupling of data structures and algorithms,
? on-demand computation for increased memory
efficiency,
? low computational costs,
? an abstract interface to alphabets to support
lazy mappings from strings to indices for arc
labels,
? an abstract interface to semirings (should be k-
closed for at least some algorithms),
? implementation in C++, as it is fast, ubiquitous
and well-known by many other researchers,
? easy to use interfaces.
3.1 The C++ Library Implementation
We use the lemma from Section 2.3 to specify an
interface for lazy algorithmic transducers directly.
The code written in pseudo-C++ is given in Figure
2. Note that all lazy algorithmic transducers are de-
rived from the class Automaton.
The lazy interface also has disadvantages. The
virtual access to the data structure might slow com-
putations down, and obtaining global information
about the automaton becomes more complicated.
For example the size of an automaton can only be
class Automaton {
public:
struct Arc {
StateId target();
Weight weight();
LabelId input();
LabelId output();
};
struct State {
StateId id();
Weight weight();
ConstArcIterator arcsBegin();
ConstArcIterator arcsEnd();
};
virtual R<Alphabet> inputAlphabet();
virtual R<Alphabet> outputAlphabet();
virtual StateId initialState();
virtual R<State> getState(StateId);
};
Figure 2: Pseudo-C++ code fragment for the ab-
stract datatype of transducers. Note that R<T>
refers to a smart pointer of T.
computed by traversing it. Therefore central al-
gorithms of the RWTH FSA toolkit are the depth-
first search (DFS) and the computation of strongly
connected components (SCC). Efficient versions of
these algorithms are described in (Mehlhorn, 1984)
and (Cormen et al, 1990).
It is very costly to store arbitrary types as arc la-
bels within the arcs itself. Therefore the RWTH
FSA toolkit offers alphabets that define mappings
between strings and label indices. Alphabets are
implemented using the abstract interface shown in
Figure 4. With alphabets arcs only need to store
the abstract label indices. The interface for alpha-
bets is defined using a single constant: for each la-
bel index an alphabet reports it must ensure to al-
ways deliver the same symbol on request through
getSymbol().
class Alphabet {
public:
virtual LabelId begin();
virtual LabelId end();
virtual LabelId next(LabelId);
virtual string getSymbol(LabelId);
};
Figure 4: Pseudo-C++ code fragment for the ab-
stract datatype of alphabets.
3.2 Algorithms
The current implementation of the toolkit offers a
wide range of well-known algorithms defined on
weighted finite-state transducers:
? basic operations
sort (by input labels, output labels or by to-
compose(T1, T2) = simple-compose( cache(sort-output(map-output(T1, AT2,I))),
cache(sort-input(T2)))
Figure 3: Optimized composition where AT2,I denotes the input alphabet of T2. Six algorithmic transducers
are used to gain maximum efficiency. Mapping of arc labels is necessary as symbol indices may differ
between alphabets.
tal arc), map-input and -output labels sym-
bolically (as the user expects that two alpha-
bets match symbolically, but their mapping
to label indices may differ), cache (helps to
reduce computations with lazy implementa-
tions), topologically-sort states
? rational operations
project-input, project-output, transpose (also
known as reversal: calculates an equivalent au-
tomaton with the adjacency matrix being trans-
posed), union, concat, invert
? classical graph operations
depth-first search (DFS), single-source short-
est path (SSSP), connect (only keep accessi-
ble and coaccessible state), strongly connected
components (SCCs)
? operations on relations of sets
compose (filtered), intersect, complement
? equivalence transformations
determinize, minimize, remove-epsilons
? search algorithms
best, n-best
? weight/probability-based algorithms
prune (based on forward/backward state po-
tentials), posterior, push (push weights toward
initial/final states), failure (given an accep-
tor/transducer defined over the tropical semir-
ing converts ?-transitions to failure transitions)
? diagnostic operations
count (counts states, final states, different arc
types, SCCs, alphabet sizes, . . .)
? input/output operations
supported input and/or output formats are:
AT&T (currently, ASCII only), binary (fast,
uses fixed byte-order), XML (slower, any en-
coding, fully portable), memory-mapped
(also on-demand), dot (AT&T graphviz)
We will discuss some details and refer to the pub-
lication of the algorithms briefly. Most of the basic
operations have a straigthforward implementation.
As arc labels are integers in the implementation
and their meaning is bound to an appropriate sym-
bolic alphabet, there is the need for symbolic map-
ping between different alphabets. Therefore the
toolkit provides the lazy map-input and map-output
transducers, which map the input and output arc in-
dices of an automaton to be compatible with the in-
dices of another given alphabet.
The implementations of all classical graph algo-
rithms are based on the descriptions of (Mehlhorn,
1984) and (Cormen et al, 1990) and (Mohri and Ri-
ley, 2001) for SSSP. The general graph algorithms
DFS and SCC are helpful in the realisation of many
other operations, examples are: transpose, connect
and count. However, counting the number of states
of an automaton or the number of symbols of an al-
phabet is not well-defined in case of an infinite set
of states or symbols.
SSSP and transpose are the only two algorithms
without a lazy implementation. The result of SSSP
is a list of state potentials (see also (Mohri and Ri-
ley, 2001)). And a lazy implementation for trans-
pose would be possible if the data structures provide
lists of both successor and predecessor arcs at each
state. This needs either more memory or more com-
putations and increases the size of the abstract inter-
face for the lazy algorithms, so as a compromise we
omitted this.
The implementations of compose (Pereira and
Riley, 1996), determinize (Mohri, 1997), minimize
(Mohri, 1997) and remove-epsilons (Mohri, 2001)
use more refined methods to gain efficiency. All
use at least the lazy cache transducer as they re-
fer to states of the input transducer(s) more than
once. With respect to the number of lazy trans-
ducers involved in computing the result, compose
has the most complicated implementation. Given
the implementations for the algorithmic transduc-
ers cache, map-output, sort-input, sort-output and
simple-compose that assumes arc labels to be com-
patible and sorted in order to perform matching as
fast as possible, the final implementation of com-
pose in the RWTH FSA toolkit is given in figure 3.
So, the current implementation of compose uses 6
algorithmic transducers in addition to the two input
automata. Determinize additionally uses lazy cache
and sort-input transducers.
The search algorithms best and n-best are based
on (Mohri and Riley, 2002), push is based on (Mohri
and Riley, 2001) and failure mainly uses ideas from
(Allauzen et al, 2003). The algorithms posterior
and prune compute arc posterior probabilities and
prune arcs with respect to them. We believe they
are standard algorithms defined on probabilistic net-
works and they were simply ported to the frame-
work of weighted finite-state automata.
Finally, the RWTH FSA toolkit can be loosely
interfaced to the AT&T FSM LibraryTM through
its ASCII-based input/output format. In addition,
a new XML-based file format primarly designed as
being human readable and a fast binary file format
are also supported. All file formats support optional
on-the-fly compression using gzip.
3.3 High-Level Interfaces
In addition to the C++ library level interface the
toolkit also offers two high-level interfaces: a
Python interface, and an interactive command-line
interface.
The Python interface has been built using the
SWIG interface generator (Beazley et al, 1996)
and enables rapid development of larger applica-
tions without lengthy compilation of C++ code. The
command-line interface comes handy for quickly
applying various combinations of algorithms to
transducers without writing any line of code at all.
As the Python interface is mainly identical to the
C++ interface we will only give a short impression
of how to use the command-line interface.
The command-line interface is a single exe-
cutable and uses a stack-based execution model
(postfix notation) for the application of operations.
This is different from the pipe model that AT&T
command-line tools use. The disadvantage of us-
ing pipes is that automata must be serialized and
get fully expanded by the next executable in chain.
However, an advantage of multiple executables is
that memory does not get fragmented through the
interaction of different algorithms.
With the command-line interface, operations are
applied to the topmost transducers of the stack and
the results are pushed back onto the stack again. For
example,
> fsa A B compose determinize draw -
reads A and B from files, calculates the determinized
composition and writes the resulting automaton to
the terminal in dot format (which may be piped to
dot directly). As you can see from the examples
some operations like write or draw take additional
arguments that must follow the name of the opera-
tion. Although this does not follow the strict postfix
design, we found it more convenient as these param-
eters are not automata.
4 Experimental Results
4.1 Comparison of Toolkits
A crucial aspect of an FSA toolkit is its computa-
tional and memory efficiency. In this section we will
compare the efficiency of four different implemen-
tations of weighted-finite state toolkits, namely:
? RWTH FSA,
? RWTH FSM (predecessor of RWTH FSA),
? AT&T FSM LibraryTM 4.0 (Mohri et al,
2000b),
? WFST (Adant, 2000).
We opted to not evaluate the FSA6.1 from (van
Noord, 2000) as we found that it is not easy to in-
stall and it seemed to be significantly slower than
any of the other implementations. RWTH FSA and
the AT&T FSM LibraryTM use on-demand com-
putations whereas FSM and WFST do not. As the
algorithmic code between RWTH FSA and its pre-
decessor RWTH FSM has not changed much ex-
cept for the interface of lazy transducers, we can
also compare lazy versus non lazy implementation.
Nevertheless, this direct comparison is also possible
with RWTH FSA as it provides a static storage class
transducer and a traversing deep copy operation.
Table 1 summarizes the tasks used for the eval-
uation of efficiency together with the sizes of the
resulting transducers. The exact meaning of the dif-
ferent transducers is out of scope of this compari-
son. We simply focus on measuring the efficiency of
the algorithms. Experiment 1 is the full expansion
of the static part of a speech recognition search net-
work. Experiment 2 deals with a translation prob-
lem and splits words of a ?bilanguage? into single
words. The meaning of the transducers used for
Experiment 2 will be described in detail in Section
4.2. Experiment 3 is similar to Experiment 1 except
for that the grammar transducer is exchanged with
a translation transducer and the result represents the
static network for a speech-to-text translation sys-
tem.
Table 1: Tasks used for measuring the efficiency of
the toolkits. Sizes are given for the resulting trans-
ducers (VM = Verbmobil).
Experiment states arcs
1 VM, HCL ?G 12,203,420 37,174,684
2 VM, C1 ?A ? C2 341,614 832,225
3 Eutrans, HCL ? T 1,201,718 3,572,601
All experiments were performed on a PC with a
1.2GHz AMD Athlon processor and 2 GB of mem-
ory using Linux as operating system. Table 2 sum-
marizes the peak memory usage of the different
toolkit implementations for the given tasks and Ta-
ble 3 shows the CPU usage accordingly.
As can be seen from Tables 2 and 3 for all given
tasks the RWTH FSA toolkit uses less memory and
computational power than any of the other toolk-
its. However, it is unclear to the authors why the
AT&T LibraryTM is a factor of 1800 slower for ex-
periment 2. The numbers also do not change much
after additionally connecting the composition result
(as in RWTH FSA compose does not connect the
result by default): memory usage rises to 62 MB
and execution time increases to 9.7 seconds. How-
ever, a detailed analysis for the RWTH FSA toolkit
has shown that the composition task of experiment
2 makes intense use of the lazy cache transducer
due to the loop character of the two transducers C1
and C2.
It can also be seen from the two tables that
the lazy implementation RWTH FSA uses signif-
icantly less memory than the non lazy implemen-
tation RWTH FSM and less than half of the CPU
time. One explanation for this is the poor mem-
ory management of RWTH FSM as all interme-
diate results need to be fully expanded in mem-
ory. In contrast, due to its lazy transducer inter-
face, RWTH FSA may allocate memory for a state
only once and reuse it for all subsequent calls to the
getState() method.
Table 2: Comparison of peak memory usage in MB
(? aborted due to exceeded memory limits).
Exp. FSA FSM AT&T WFST
1 360 1700 1500 > 1850?
2 59 310 69 > 1850?
3 48 230 176 550
Table 3: Comparison of CPU time in seconds in-
cluding I/O using a 1.2GHz AMD Athlon proces-
sor (? exceeded memory limits: given time indicates
point of abortion).
Exp. FSA FSM AT&T WFST
1 105 203 515 > 40?
2 6.5 182 11760 > 64?
3 6.6 21 28 3840
4.2 Statistical Machine Translation
Statistical machine translation may be viewed as
a weighted language transduction problem (Vidal,
1997). Therefore it is fairly easy to build a machine
translation system with the use of weighted finite-
state transducers.
Let fJ1 and eIi be two sentences from a source
and target language respectively. Also assume that
we have word level alignments A of all sentences
from a bilingual training corpus. We denote with
epJp1 the segmentation of a target sentence eI1 into
phrases such that fJ1 and epJp1 can be aligned mono-
toneously. This segmentation can be directly calcu-
lated from the alignments A. Then we can formu-
late the problem of finding the best translation e?I1 of
a source sentence as follows:
e?I1 = argmax
eI1
Pr(fJ1 , eI1)
? argmax
A,epJp1
Pr(fJ1 , epJp1 )
= argmax
A,epJp1
?
fj :j=1..J
Pr(fj , epj |f j?11 , epj?1p1 )
? argmax
A,epJp1
?
fj :j=1..J
Pr(fj , epj |f j?1j?n, e
pj?1pj?n)
The last line suggests to solve the translation
problem by estimating a language model on a bi-
language (see also (Bangalore and Riccardi, 2000;
Casacuberta et al, 2001)). An example of sentences
from this bilanguage is given in Figure 5 for the
translation task Vermobil (German ? English). For
technical reasons, ?-labels are represented by a $
symbol. Note, that due to the fixed segmentation
given by the alignments, phrases in the target lan-
guage are moved to the last source word of an align-
ment block.
So, given an appropriate alignment which can
be obtained by means of the pubically available
GIZA++ toolkit (Och and Ney, 2000), the approach
is very easy in practice:
1. Transform the training corpus with a given
alignment into the corresponding bilingual cor-
pus
2. Train a language model on the bilingual corpus
3. Build an acceptor A from the language model
The symbols of the resulting acceptor are still a mix-
ture of words from the source language and phrases
from the target language. So, we additionally use
two simple transducers to split these bilingual words
(C1 maps source words fj to bilingual words that
start with fj and C2 maps bilingual words with the
target sequence epj to the sequences of target words
the phrase was made of):
4. Split the bilingual phrases of A into single
words:
T = C1 ?A ? C2
Then the translation problem from above can be
rewritten using finite-state terminology:
dann|$ melde|$ ich|I_am_calling mich|$ noch|$ einmal|once_more .|.
11U|eleven Uhr|o?clock ist|is hervorragend|excellent .|.
ich|I bin|have da|$ relativ|quite_a_lot_of frei|free_days_then .|.
Figure 5: Example corpus for the bilanguage (Verbmobil, German ? English).
Table 4: Translation results for different tasks compared to similar systems using the alignment template
(AT) approach (Tests were performed on a 1.2GHz AMD Athlon).
Task System Translation WER PER 100-BLEU Memory Time/Sentence
[%] [%] [MB] [ms]
Eutrans FSA Spanish ? English 8.12 7.64 10.7 6-8 20
AT 8.25 - - - -
FUB FSA Italian ? English 27.0 21.5 37.7 3-5 22
AT 23.7 18.1 36.0 - -
Verbmobil FSA German ? English 48.3 41.6 69.8 65-90 460
AT 40.5 30.1 62.2 - -
PF-Star FSA Italian ? English 39.8 34.1 58.4 12-15 35
AT 36.8 29.1 54.3 - -
e? = project-output(best(f ? T ))
Translation results using this approach are summa-
rized in Table 4 and are being compared with results
obtained using the alignment template approach
(Och and Ney, 2000). Results for both approaches
were obtaining using the same training corpus align-
ments. Detailed task descriptions for Eutrans/FUB
and Verbmobil can be found in (Casacuberta et al,
2001) and (Zens et al, 2002) respectively. We use
the usual definitions for word error rate (WER), po-
sition independent word error rate (PER) and BLEU
statistics here.
For the simpler tasks Eutrans, FUB and PF-Star,
the WER, PER and the inverted BLEU statistics
are close for both approaches. On the German-to-
English Verbmobil task the FSA approach suffers
from long distance reorderings (captured through
the fixed training corpus segmentation), which is not
very surprising.
Although we do not have comparable numbers of
the memory usage and the translation times for the
alignment template approach, resource usage of the
finite-state approach is quite remarkable as we only
use generic methods from the RWTH FSA toolkit
and full search (i.e. we do not prune the search
space). However, informal tests have shown that
the finite-state approach uses much less memory
and computations than the current implementation
of the alignment template approach.
Two additional advantages of finite-state methods
for translation in general are: the input to the search
algorithm may also be a word lattice and it is easy
to combine speech recognition with translation in
order to do speech-to-speech translation.
5 Summary
In this paper we have given a characterization of al-
gorithms that produce a single finite-state automa-
ton and bear an on-demand implementation. For
this purpose we formally introduced the local prop-
erty of such an algorithm.
We have described the efficient implementation
of a finite-state toolkit that uses the principle of
lazy algorithmic transducers for almost all algo-
rithms. Among several publically available toolkits,
the RWTH FSA toolkit presented here turned out to
be the most efficient one, as several tests showed.
Additionally, with lazy algorithmic transducers we
have reduced the memory requirements and even in-
creased the speed significantly compared to a non
lazy implementation.
We have also shown that a finite-state automata
toolkit supports rapid solutions to problems from
the field of natural language processing such as sta-
tistical machine translation. Despite the genericity
of the methods, statistical machine translation can
be done very efficiently.
6 Shortcomings and Future Extensions
There is still room to improve the RWTH FSA
toolkit. For example, the current implementation
of determinization is not as general as described in
(Allauzen and Mohri, 2003). In case of ambiguous
input the algorithm still produces an infinite trans-
ducer. At the moment this can be solved in many
cases by adding disambiguation symbols to the in-
put transducer manually.
As the implementation model is based on virtual
C++ methods for all types of objects in use (semir-
ings, alphabets, transducers and algorithmic trans-
ducers) it should also be fairly easy to add support
for dynamically loadable objects to the toolkit.
Other semirings like the expectation semiring de-
scribed in (Eisner, 2001) are supported but not yet
implemented.
7 Acknowledgment
The authors would like to thank Andre Altmann for
his help with the translation experiments.
References
Alfred V. Aho and Jeffrey D. Ullman, 1972, The The-
ory of Parsing, Translation and Compiling, volume 1,
Prentice-Hall, Englewood Cliffs, NJ, 1972.
Arnaud Adant, 2000, WFST: A Finite-State Template Li-
brary in C++, http://membres.lycos.fr/adant/tfe/.
Cyril Allauzen, Mehryar Mohri, and Brian Roark, 2003,
Generalized Algorithms for Constructing Statistical
Language Models, In Proc. of the 41st Meeting of the
Association for Computational Linguistics, Sapporo,
Japan, July 2003.
Cyril Allauzen and Mehryar Mohri, 2003, General-
ized Optimization Algorithm for Speech Recognition
Transducers, In Proc. of the IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing, pp. , Hong
Kong, China, April 2003.
Srinivas Bangalore and Giuseppe Riccardi, 2000,
Stochastic Finite-State models for Spoken Language
Machine Translation, In Proc. of the Workshop on
Embedded Machine Translation Systems, pp. 52?59,
2000.
David Beazley, William Fulton, Matthias Ko?ppe, Lyle
Johnson, Richard Palmer, 1996, SWIG - Simplified
Wrapper and Interface Generator, Electronic Docu-
ment, http://www.swig.org, February 1996.
F. Casacuberta, D. Llorens, C. Martinez, S. Molau, F.
Nevado, H. Ney, M. Pasto, D. Pico, A. Sanchis, E. Vi-
dal and J.M. Vilar, 2001, Speech-to-Speech Transla-
tion based on Finite-State Transducer, In Proc. IEEE
Int. Conf. on Acoustics, Speech and Signal Process-
ing, pp. 613-616, Salt Lake City, Utah, May 2001.
Thomas H. Cormen, Charles E. Leiserson and Ronald L.
Rivest, 1990, Introductions to Algorithms, The MIT
Press, Cambridge, MA, 1990.
Jason Eisner, 2001, Expectation Semirings: Flexible
EM for Finite-State Transducers, In Proc. of the
ESSLLI Workshop on Finite-State Methods in NLP
(FSMNLP), Helsinki, August 2001.
Free Software Foundation, 1991, GNU General
Public License, Version 2, Electronic Document,
http://www.gnu.org/copyleft/gpl.html, June 1991.
Takaaki Hori, Chiori Hori and Yasuhiro Minami, 2003,
Speech Summarization using Weighted Finite-State
Transducers, In Proc. of the European Conf. on
Speech Communication and Technology, Geneva,
Switzerland, September 2003.
Vincent Le Maout, 1998, ASTL: Automaton Stan-
dard Template Library, http://www-igm.univ-
mlv.fr/?lemaout/.
Kurt Mehlhorn, 1984, Data Structures and Efficient Al-
gorithms, Chapter 4, Springer Verlag, EATCS Mono-
graphs, 1984, also available from http://www.mpi-
sb.mpg.de/m?ehlhorn/DatAlgbooks.html.
Mehryar Mohri, 1997, Finite-State Transducers in Lan-
guage and Speech Processing, Computational Lin-
guistics, 23:2, 1997.
Mehryar Mohri, Fernando C.N. Pereira, and Michael
Riley, 2000, Weighted Finite-State Transducers in
Speech Recognition, In Proc. of the ISCA Tutorial and
Research Workshop, Automatic Speech Recognition:
Challenges for the new Millenium (ASR2000), Paris,
France, September 2000.
Mehryar Mohri, Fernando C.N. Pereira, and Michael Ri-
ley, 2000, The Design Principles of a Weighted Finite-
State Transducer Library, Theoretical Computer Sci-
ence, 231:17-32, January 2000.
Mehryar Mohri and Michael Riley, 2000, A Weight
Pushing Algorithm for Large Vocabulary Speech
Recognition, In Proc. of the European Conf. on
Speech Communication and Technology, pp. 1603?
1606, A?alborg, Denmark, September 2001.
Mehryar Mohri, 2001, Generic Epsilon-Removal Algo-
rithm for Weighted Automata, In Sheng Yu and An-
drei Paun, editor, 5th Int. Conf., CIAA 2000, London
Ontario, Canada. volume 2088 of Lecture Notes in
Computer Science, pages 230-242. Springer-Verlag,
Berlin-NY, 2001.
Mehryar Mohri and Michael Riley, 2002, An Efficient
Algorithm for the N-Best-Strings Problem, In Proc.
of the Int. Conf. on Spoken Language Processing, pp.
1313?1316, Denver, Colorado, September 2002.
Franz J. Och and Hermann Ney, 2000, Improved Sta-
tistical Alignment Models, In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447, Hongkong, China, October
2000.
Fernando C.N. Pereira and Michael Riley, 1996, Speech
Recognition by Composition of Weighted Finite
Automata, Available from http://xxx.lanl.gov/cmp-
lg/9603001, Computation and Language, 1996.
Gertjan van Noord, 2000, FSA6 Reference Manual,
http://odur.let.rug.nl/v?annoord/Fsa/.
Enrique Vidal, 1997, Finite-State Speech-to-Speech
Translation, In Proc. of the IEEE Int. Conf. on Acous-
tics, Speech and Signal Processing, pp. 111?114, Mu-
nich, Germany, 1997.
Richard Zens, Franz J. Och and H. Ney, 2002, Phrase-
Based Statistical Machine Translation, In: M. Jarke,
J. Koehler, G. Lakemeyer (Eds.) : KI - 2002: Ad-
vances in artificial intelligence. 25. Annual German
Conference on AI, KI 2002, Vol. LNAI 2479, pp. 18-
32, Springer Verlag, September 2002.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 467?474,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integration of Speech to Computer-Assisted Translation Using
Finite-State Automata
Shahram Khadivi Richard Zens
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{khadivi,zens,ney}@cs.rwth-aachen.de
Hermann Ney
Abstract
State-of-the-art computer-assisted transla-
tion engines are based on a statistical pre-
diction engine, which interactively pro-
vides completions to what a human trans-
lator types. The integration of human
speech into a computer-assisted system is
also a challenging area and is the aim of
this paper. So far, only a few methods
for integrating statistical machine transla-
tion (MT) models with automatic speech
recognition (ASR) models have been stud-
ied. They were mainly based on N -
best rescoring approach. N -best rescor-
ing is not an appropriate search method
for building a real-time prediction engine.
In this paper, we study the incorporation
of MT models and ASR models using
finite-state automata. We also propose
some transducers based on MT models for
rescoring the ASR word graphs.
1 Introduction
A desired feature of computer-assisted transla-
tion (CAT) systems is the integration of the hu-
man speech into the system, as skilled human
translators are faster at dictating than typing the
translations (Brown et al, 1994). Additionally,
incorporation of a statistical prediction engine, i.e.
a statistical interactive machine translation system,
to the CAT system is another useful feature. A sta-
tistical prediction engine provides the completions
to what a human translator types (Foster et al,
1997; Och et al, 2003). Then, one possible proce-
dure for skilled human translators is to provide the
oral translation of a given source text and then to
post-edit the recognized text. In the post-editing
step, a prediction engine helps to decrease the
amount of human interaction (Och et al, 2003).
In a CAT system with integrated speech, two
sources of information are available to recognize
the speech input: the target language speech
and the given source language text. The target
language speech is a human-produced translation
of the source language text. Statistical machine
translation (MT) models are employed to take into
account the source text for increasing the accuracy
of automatic speech recognition (ASR) models.
Related Work
The idea of incorporating ASR and MT models
was independently initiated by two groups:
researchers at IBM (Brown et al, 1994),
and researchers involved in the TransTalk
project (Dymetman et al, 1994; Brousseau
et al, 1995). In (Brown et al, 1994), the
authors proposed a method to integrate the IBM
translation model 2 (Brown et al, 1993) with
an ASR system. The main idea was to design
a language model (LM) to combine the trigram
language model probability with the translation
probability for each target word. They reported a
perplexity reduction, but no recognition results.
In the TransTalk project, the authors improved
the ASR performance by rescoring the ASR
N -best lists with a translation model. They also
introduced the idea of a dynamic vocabulary for
a speech recognition system where translation
models were generated for each source language
sentence. The better performing of the two is the
N -best rescoring.
Recently, (Khadivi et al, 2005) and (Paulik et
al., 2005a; Paulik et al, 2005b) have studied the
integration of ASR and MT models. The first
work showed a detailed analysis of the effect of
different MT models on rescoring the ASR N -best
lists. The other two works considered two parallel
N -best lists, generated by MT and ASR systems,
467
respectively. They showed improvement in the
ASR N -best rescoring when some proposed fea-
tures are extracted from the MT N -best list. The
main concept among all features was to generate
different kinds of language models from the MT
N -best list.
All of the above methods are based on an N -
best rescoring approach. In this paper, we study
different methods for integrating MT models to
ASR word graphs instead of N -best list. We
consider ASR word graphs as finite-state automata
(FSA), then the integration of MT models to ASR
word graphs can benefit from FSA algorithms.
The ASR word graphs are a compact representa-
tion of possible recognition hypotheses. Thus, the
integration of MT models to ASR word graphs can
be considered as an N -best rescoring but with very
large value for N . Another advantage of working
with ASR word graphs is the capability to pass
on the word graphs for further processing. For
instance, the resulting word graph can be used in
the prediction engine of a CAT system (Och et al,
2003).
The remaining part is structured as follows: in
Section 2, a general model for an automatic text
dictation system in the computer-assisted transla-
tion framework will be described. In Section 3,
the details of the machine translation system and
the speech recognition system along with the lan-
guage model will be explained. In Section 4,
different methods for integrating MT models into
ASR models will be described, and also the exper-
imental results will be shown in the same section.
2 Speech-Enabled CAT Models
In a speech-enabled computer-assisted translation
system, we are given a source language sentence
fJ1 = f1 . . . fj . . . fJ , which is to be translated into
a target language sentence eI1 = e1 . . . ei . . . eI ,
and an acoustic signal xT1 = x1 . . . xt . . . xT ,
which is the spoken target language sentence.
Among all possible target language sentences, we
will choose the sentence with the highest probabil-
ity:
e?I?1= argmax
I,eI1
{Pr(eI1|fJ1 , xT1 )} (1)
?= argmax
I,eI1
{Pr(eI1)Pr(fJ1 |eI1)Pr(xT1 |eI1)}(2)
Eq. 1 is decomposed into Eq. 2 by assuming
conditional independency between xT1 and fJ1 .
The decomposition into three knowledge sources
allows for an independent modeling of the target
language model Pr(eI1), the translation model
Pr(fJ1 |eI1) and the acoustic model Pr(xT1 |eI1).
Another approach for modeling the posterior
probability Pr(eI1|fJ1 , xT1 ) is direct modeling us-
ing a log-linear model. The decision rule is given
by:
e?I?1 = argmax
I,eI1
{ M?
m=1
?mhm(eI1, fJ1 , xT1 )
}
(3)
Each of the terms hm(eI1, fJ1 , xT1 ) denotes one
of the various models which are involved in the
recognition procedure. Each individual model is
weighted by its scaling factor ?m. As there is
no direct dependence between fJ1 and xT1 , the
hm(eI1, fJ1 , xT1 ) is in one of these two forms:
hm(eI1, xT1 ) and hm(eI1, fJ1 ). Due to the argmax
operator which denotes the search, no renormal-
ization is considered in Eq. 3. This approach has
been suggested by (Papineni et al, 1997; Papineni
et al, 1998) for a natural language understanding
task, by (Beyerlein, 1998) for an ASR task, and
by (Och and Ney, 2002) for an MT task. This
approach is a generalization of Eq. 2. The di-
rect modeling has the advantage that additional
models can be easily integrated into the overall
system. The model scaling factors ?M1 are trained
on a development corpus according to the final
recognition quality measured by the word error
rate (WER)(Och, 2003).
Search
The search in the MT and the ASR systems is
already very complex, therefore a fully integrated
search to combine ASR and MT models will
considerably increase the complexity. To reduce
the complexity of the search, we perform two
independent searches with the MT and the ASR
systems, the search result of each system will be
represented as a large word graph. We consider
MT and ASR word graphs as FSA. Then, we are
able to use FSA algorithms to integrate MT and
ASR word graphs. The FSA implementation of
the search allows us to use standard optimized
algorithms, e.g. available from an open source
toolkit (Kanthak and Ney, 2004).
The recognition process is performed in two
steps. First, the baseline ASR system generates a
word graph in the FSA format for a given utterance
xT1 . Second, the translation models rescore each
word graph based on the corresponding source
language sentence. For each utterance, the deci-
sion about the best sentence is made according to
the recognition and the translation models.
468
3 Baseline Components
In this section, we briefly describe the basic sys-
tem components, namely the MT and the ASR
systems.
3.1 Machine Translation System
We make use of the RWTH phrase-based statis-
tical machine translation system for the English
to German automatic translation. The system in-
cludes the following models: an n-gram language
model, a phrase translation model and a word-
based lexicon model. The latter two models are
used for both directions: German to English and
English to German. Additionally, a word penalty
and a phrase penalty are included. The reordering
model of the baseline system is distance-based, i.e.
it assigns costs based on the distance from the end
position of a phrase to the start position of the next
phrase. More details about the baseline system
can be found in (Zens and Ney, 2004; Zens et al,
2005).
3.2 Automatic Speech Recognition System
The acoustic model of the ASR system is trained
on the VerbMobil II corpus (Sixtus et al, 2000).
The corpus consists of German large-vocabulary
conversational speech: 36k training sentences
(61.5h) from 857 speakers. The test corpus is
created from the German part of the bilingual
English-German XEROX corpus (Khadivi et al,
2005): 1562 sentences including 18k running
words (2.6h) from 10 speakers. The test cor-
pus contains 114 out-of-vocabulary (OOV) words.
The remaining part of the XEROX corpus is used
to train a back off trigram language model us-
ing the SRI language modeling toolkit (Stolcke,
2002). The LM perplexity of the speech recogni-
tion test corpus is about 83. The acoustic model of
the ASR system can be characterized as follows:
? recognition vocabulary of 16716 words;
? 3-state-HMM topology with skip;
? 2500 decision tree based generalized within-
word triphone states including noise plus one
state for silence;
? 237k gender independent Gaussian densities
with global pooled diagonal covariance;
? 16 MFCC features;
? 33 acoustic features after applying LDA;
? LDA is fed with 11 subsequent MFCC vec-
tors;
? maximum likelihood training using Viterbi
approximation.
Table 1: Statistics of the machine translation cor-
pus.
English German
Train: Sentences 47 619
Running Words 528 779 467 633
Vocabulary 9 816 16 716
Singletons 2 302 6 064
Dev: Sentences 700
Running Words 8 823 8 050
Unknown words 56 108
Eval: Sentences 862
Running Words 11 019 10 094
Unknown words 58 100
The test corpus recognition word error rate is
20.4%. Compared to the previous system (Khadivi
et al, 2005), which has a WER of 21.2%, we
obtain a 3.8% relative improvement in WER. This
improvement is due to a better and complete opti-
mization of the overall ASR system.
4 Integration Approaches
In this section, we will introduce several ap-
proaches to integrate the MT models with the ASR
models. To present the content of this section in a
more reader-friendly way, we will first explain the
task and corpus statistics, then we will present the
results of N -best rescoring. Afterwards, we will
describe the new methods for integrating the MT
models with the ASR models. In each sub-section,
we will also present the recognition results.
4.1 Task
The translation models are trained on the part of
the English-German XEROX corpus which was
not used in the speech recognition test corpus. We
divide the speech recognition test corpus into two
parts, the first 700 utterances as the development
corpus and the rest as the evaluation corpus. The
development corpus is used to optimize the scal-
ing factors of different models (explained in Sec-
tion 2). The statistics of the corpus are depicted in
Table 1. The German part of the training corpus is
also used to train the language model.
4.2 N -best Rescoring
To rescore the N -best lists, we use the method
of (Khadivi et al, 2005). But the results shown
here are different from that work due to a better
optimization of the overall ASR system, using a
469
Table 2: Recognition WER [%] using N -best
rescoring method.
Models Dev Eval
MT 47.1 50.5
ASR 19.3 21.3
ASR+MT IBM-1 17.8 19.0
HMM 18.2 19.2
IBM-3 17.1 18.4
IBM-4 17.1 18.3
IBM-5 16.6 18.2
Phrase
-based 18.8 20.3
better MT system, and generating a larger N -best
list from the ASR word graphs. We rescore the
ASR N -best lists with the standard HMM (Vogel
et al, 1996) and IBM (Brown et al, 1993) MT
models. The development and evaluation sets N -
best lists sizes are sufficiently large to achieve
almost the best possible results, on average 1738
hypotheses per each source sentence are extracted
from the ASR word graphs.
The recognition results are summarized in Ta-
ble 2. In this table, the translation results of the
MT system are shown first, which are obtained
using the phrase-based approach. Then the recog-
nition results of the ASR system are shown. After-
wards, the results of combined speech recognition
and translation models are presented.
For each translation model, the N -best lists
are rescored based on the translation probability
p(eI1|fJ1 ) of that model and the probabilities of
speech recognition and language models. In the
last row of Table 2, the N -best lists are rescored
based on the full machine translation system ex-
plained in Section 3.1.
The best possible hypothesis achievable from
the N -best list has the WER (oracle WER) of
11.2% and 12.4% for development and test sets,
respectively.
4.3 Direct Integration
At the first glance, an obvious method to combine
the ASR and MT systems is the integration at the
level of word graphs. This means the ASR system
generates a large word graph for the input target
language speech, and the MT system also gener-
ates a large word graph for the source language
text. Both MT and ASR word graphs are in the
target language. These two word graphs can be
considered as two FSA, then using FSA theory,
we can integrate two word graphs by applying the
composition algorithm.
We conducted a set of experiments to integrate
the ASR and MT systems using this method. We
obtain a WER of 19.0% and 20.9% for devel-
opment and evaluation sets, respectively. The
results are comparable to N -best rescoring results
for the phrase-based model which is presented in
Table 2. The achieved improvements over the
ASR baseline are statistically significant at the
99% level (Bisani and Ney, 2004). However, the
results are not promising compared to the results
of the rescoring method presented in Table 2 for
HMM and IBM translation models. A detailed
analysis revealed that only 31.8% and 26.7% of
sentences in the development and evaluation sets
have identical paths in both FSA, respectively. In
other words, the search algorithm was not able to
find any identical paths in two given FSA for the
remaining sentences. Thus, the two FSA are very
different from each other. One explanation for
the failure of this method is the large difference
between the WERs of two systems, as shown in
Table 2 the WER for the MT system is more than
twice as high as for the ASR system.
4.4 Integrated Search
In Section 4.3, two separate word graphs are
generated using the MT and the ASR systems.
Another explanation for the failure of the direct
integration method is the independent search to
generate the word graphs. The search in the MT
and the ASR systems is already very complex,
therefore a full integrated search to combine ASR
and MT models will considerably increase the
complexity.
However, it is possible to reduce this problem
by integrating the ASR word graphs into the gen-
eration process of the MT word graphs. This
means, the ASR word graph is used in addition to
the usual language model. This kind of integration
forces the MT system to generate identical paths to
those in the ASR word graph. Using this approach,
the number of identical paths in MT and ASR
word graphs are increased to 39.7% and 34.4%
of the sentences in development and evaluation
sets, respectively. The WER of the integrated
system are 19.0% and 20.7% for development and
evaluation sets.
4.5 Lexicon-Based Transducer
The idea of a dynamic vocabulary, restricting and
weighting the word lexicon of the ASR was first
470
introduced in (Brousseau et al, 1995). The idea
was also seen later in (Paulik et al, 2005b), they
extract the words of the MT N -best list to restrict
the vocabulary of the ASR system. But they both
reported a negative effect from this method on
the recognition accuracy. Here, we extend the
dynamic vocabulary idea by weighting the ASR
vocabulary based on the source language text and
the translation models. We use the lexicon model
of the HMM and the IBM MT models. Based on
these lexicon models, we assign to each possible
target word e the probability Pr(e|fJ1 ). One way
to compute this probability is inspired by IBM
Model 1:
Pr(e|fJ1 ) =
1
J + 1
J?
j=0
p(e|fj)
We can design a simple transducer (or more pre-
cisely an acceptor) using probability in Eq. 4 to
efficiently rescore all paths (hypotheses) in the
word graph with IBM Model 1:
PIBM-1(eI1|fJ1 ) =
1
(J + 1)I
I?
i=1
J?
j=0
p(ei|fj)
=
I?
i=1
1
(J + 1) ? p(ei|f
J
1 )
The transducer is formed by one node and a num-
ber of self loops for each target language word. In
each arc of this transducer, the input label is target
word e and the weight is ? log 1J+1 ? p(e|fJ1 ).
We conducted experiments using the proposed
transducer. We built different transducers with the
lexicons of HMM and IBM translation models. In
Table 3, the recognition results of the rescored
word graphs are shown. The results are very
promising compared to the N -best list rescoring,
especially as the designed transducer is very sim-
ple. Similar to the results for the N -best rescoring
approach, these experiments also show the benefit
of using HMM and IBM Models to rescore the
ASR word graphs.
Due to its simplicity, this model can be easily
integrated into the ASR search. It is a sentence
specific unigram LM.
4.6 Phrase-Based Transducer
The phrase-based translation model is the main
component of our translation system. The pairs
of source and corresponding target phrases are
extracted from the word-aligned bilingual training
Table 3: Recognition WER [%] using lexicon-
based transducer to rescore ASR word graphs.
Models Dev Eval
ASR 19.3 21.3
ASR+MT IBM-1 17.5 19.0
HMM 17.8 19.2
IBM-3 17.7 18.8
IBM-4 17.8 18.8
IBM-5 17.6 18.9
corpus (Zens and Ney, 2004). In this section, we
design a transducer to rescore the ASR word graph
using the phrase-based model of the MT system.
For each source language sentence, we extract all
possible phrases from the word-aligned training
corpus. Using the target part of these phrases
we build a transducer similar to the lexicon-based
transducer. But instead of a target word on each
arc, we have the target part of a phrase. The weight
of each arc is the negative logarithm of the phrase
translation probability.
This transducer is a good approximation of non-
monotone phrase-based-lexicon score. Using the
designed transducer it is possible that some parts
of the source texts are not covered or covered more
than once. Then, this model can be compared
to the IBM-3 and IBM-4 models, as they also
have the same characteristic in covering the source
words. The above assumption is not critical for
rescoring the ASR word graphs, as we are con-
fident that the word order is correct in the ASR
output. In addition, we assume low probability for
the existence of phrase pairs that have the same
target phrase but different source phrases within a
particular source language sentence.
Using the phrase-based transducer to rescore
the ASR word graph results in WER of 18.8%
and 20.2% for development and evaluation sets,
respectively. The improvements are statistically
significant at the 99% level compared to the ASR
system. The results are very similar to the results
obtained using N -best rescoring method. But
the transducer implementation is much simpler
because it does not consider the word-based lex-
icon, the word penalty, the phrase penalty, and
the reordering models, it just makes use of phrase
translation model. The designed transducer is
much faster in rescoring the word graph than the
MT system in rescoring the N -best list. The av-
erage speed to rescore the ASR word graphs with
this transducer is 49.4 words/sec (source language
471
text words), while the average speed to translate
the source language text using the MT system is
8.3 words/sec. The average speed for rescoring
the N -best list is even slower and it depends on
the size of N -best list.
A surprising result of the experiments as has
also been observed in (Khadivi et al, 2005), is that
the phrase-based model, which performs the best
in MT, has the least contribution in improving the
recognition results. The phrase-based model uses
more context in the source language to generate
better translations by means of better word selec-
tion and better word order. In a CAT system, the
ASR system has much better recognition quality
than MT system, and the word order of the ASR
output is correct. On the other hand, the ASR
recognition errors are usually single word errors
and they are independent from the context. There-
fore, the task of the MT models in a CAT system is
to enhance the confidence of the recognized words
based on the source language text, and it seems
that the single word based MT models are more
suitable than phrase-based model in this task.
4.7 Fertility-Based Transducer
In (Brown et al, 1993), three alignment models
are described that include fertility models, these
are IBM Models 3, 4, and 5. The fertility-based
alignment models have a more complicated struc-
ture than the simple IBM Model 1. The fertility
model estimates the probability distribution for
aligning multiple source words to a single target
word. The fertility model provides the probabili-
ties p(?|e) for aligning a target word e to ? source
words. In this section, we propose a method for
rescoring ASR word graphs based on the lexicon
and fertility models.
In (Knight and Al-Onaizan, 1998), some trans-
ducers are described to build a finite-state based
translation system. We use the same transduc-
ers for rescoring ASR word graphs. Here, we
have three transducers: lexicon, null-emitter, and
fertility. The lexicon transducer is formed by
one node and a number of self loops for each
target language word, similar to IBM Model 1
transducer in Section 4.5. On each arc of the
lexicon transducer, there is a lexicon entry: the
input label is a target word e, the output label is
a source word f , and the weight is ? log p(f |e).
The null-emitter transducer, as its name states,
emits the null word with a pre-defined probability
after each input word. The fertility transducer is
also a simple transducer to map zero or several
instances of a source word to one instance of the
source word.
The ASR word graphs are composed succes-
sively with the lexicon, null-emitter, fertility trans-
ducers and finally with the source language sen-
tence. In the resulting transducer, the input labels
of the best path represent the best hypothesis.
The mathematical description of the proposed
method is as follows. We can decompose Eq. 1
using Bayes? decision rule:
e?I?1= argmax
I,eI1
{Pr(eI1|fJ1 , xT1 )} (4)
?= argmax
I,eI1
{Pr(fJ1 )Pr(eI1|fJ1 )Pr(xT1 |eI1)}(5)
In Eq. 5, the term Pr(xT1 |eI1) is the acoustic model
and can be represented with the ASR word graph1,
the term Pr(eI1|fJ1 ) is the translation model of
the target language text to the source language
text. The translation model can be represented
by lexicon, fertility, and null-emitter transducers.
Finally, the term Pr(fJ1 ) is a very simple language
model, it is the source language sentence.
The source language model in Eq. 5 can be
formed into the acceptor form in two different
ways:
1. a linear acceptor, i.e. a sequence of nodes
with one incoming arc and one outgoing arc,
the words of source language text are placed
consecutively in the arcs of the acceptor,
2. an acceptor containing possible permuta-
tions. To limit the permutations, we used an
approach as in (Kanthak et al, 2005).
Each of these two acceptors results in different
constraints for the generation of the hypotheses.
The first acceptor restricts the system to generate
exactly the same source language sentence, while
the second acceptor forces the system to generate
the hypotheses that are a reordered variant of
the source language sentence. The experiments
conducted do not show any significant difference
in the recognition results among the two source
language acceptors, except that the second accep-
tor is much slower than the first acceptor. There-
fore, we use the first model in our experiments.
Table 4 shows the results of rescoring the ASR
word graphs using the fertility-based transducers.
1Actually, the ASR word graph is obtained by using
Pr(xT1 |eI1) and Pr(eI1) models. However, It does not cause
any problem in the modeling, especially when we make use
of the direct modeling, Eq. 3
472
Table 4: Recognition WER [%] using fertility-
based transducer to rescore ASR word graphs.
Models Dev Eval
ASR 19.3 21.3
ASR+MT IBM-3 17.4 18.6
IBM-4 17.4 18.5
IBM-5 17.6 18.7
As Table 4 shows, we get alost the same
or slightly better results when compared to the
lexicon-based transducers.
Another interesting point about Eq. 5 is its simi-
larity to speech translation (translation from target
spoken language to source language text). Then,
we can describe a speech-enabled CAT system
as similar to a speech translation system, except
that we aim to get the best ASR output (the best
path in the ASR word graph) rather than the best
translation. This is because the best translation,
which is the source language sentence, is already
given.
5 Conclusion
We have studied different approaches to integrate
MT with ASR models, mainly using finite-state
automata. We have proposed three types of trans-
ducers to rescore the ASR word graphs: lexicon-
based, phrase-based and fertility-based transduc-
ers. All improvements of the combined models
are statistically significant at the 99% level with
respect to the baseline system, i.e. ASR only.
In general, N -best rescoring is a simplification
of word graph rescoring. As the size of N -best
list is increased, the results obtained by N -best
list rescoring approach the results of the word
graph rescoring. But we should consider that the
statement is correct when we use exactly the same
model and the same implementation to rescore the
N -best list and word graph. Figure 1 shows the
effect of the N -best list size on the recognition
WER of the evaluation set. As we expected, the
recognition results of N -best rescoring improve
as N becomes larger, until the point that the
recognition result converges to its optimum value.
As shown in Figure 1, we should not expect that
word graph rescoring methods outperform the N -
best rescoring method, when the size of N -best
lists are large enough. In Table 2, the recognition
results are calculated using a large enough size for
N -best lists, a maximum of 5,000 per sentence,
which results in the average of 1738 hypotheses
 18
 18.5
 19
 19.5
 20
 20.5
 21
 21.5
 1  10  100  1000  10000
W
E
R
 
[
%
]
Size of N-best list (N), in log scale
IBM-1HMMIBM-3IBM-4IBM-5
Figure 1: The N -best rescoring results for differ-
ent N -best sizes on the evaluation set.
per sentence. An advantage of the word graph
rescoring is the confidence of achieving the best
possible results based on a given rescoring model.
The word graph rescoring methods presented in
this paper improve the baseline ASR system with
statistical significance. The results are competitive
with the best results of N -best rescoring. For the
simple models like IBM-1, the transducer-based
integration generates similar or better results than
N -best rescoring approach. For the more com-
plex translation models, IBM-3 to IBM-5, the
N -best rescoring produces better results than the
transducer-based approach, especially for IBM-
5. The main reason is due to exact estimation
of IBM-5 model scores on the N -best list, while
the transducer-based implementation of IBM-3 to
IBM-5 is not exact and simplified. However, we
observe that the fertility-based transducer which
can be considered as a simplified version of IBM-
3 to IBM-5 models can still obtain good results,
especially if we compare the results on the evalu-
ation set.
Acknowledgement
This work has been funded by the European
Union under the RTD project TransType2 (IST
2001 32091) and the integrated project TC-
STAR - Technology and Corpora for Speech
to Speech Translation -(IST-2002-FP6-506738,
http://www.tc-star.org).
References
P. Beyerlein. 1998. Discriminative model combina-
tion. In Proc. IEEE Int. Conf. on Acoustics, Speech,
and Signal Processing (ICASSP), volume 1, pages
481 ? 484, Seattle, WA, May.
473
M. Bisani and H. Ney. 2004. Bootstrap estimates
for confidence intervals in ASR performance evalu-
ationx. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 409?412,
Montreal, Canada, May.
J. Brousseau, C. Drouin, G. Foster, P. Isabelle,
R. Kuhn, Y. Normandin, and P. Plamondon. 1995.
French speech recognition in an automatic dictation
system for translators: the transtalk project. In Pro-
ceedings of Eurospeech, pages 193?196, Madrid,
Spain.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
P. F. Brown, S. F. Chen, S. A. Della Pietra, V. J. Della
Pietra, A. S. Kehler, and R. L. Mercer. 1994. Au-
tomatic speech recognition in machine-aided trans-
lation. Computer Speech and Language, 8(3):177?
187, July.
M. Dymetman, J. Brousseau, G. Foster, P. Isabelle,
Y. Normandin, and P. Plamondon. 1994. Towards
an automatic dictation system for translators: the
TransTalk project. In Proceedings of ICSLP-94,
pages 193?196, Yokohama, Japan.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Target-
text mediated interactive machine translation. Ma-
chine Translation, 12(1):175?194.
S. Kanthak and H. Ney. 2004. FSA: An efficient
and flexible C++ toolkit for finite state automata
using on-demand computation. In Proc. of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 510?517, Barcelona,
Spain, July.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and
H. Ney. 2005. Novel reordering approaches in
phrase-based statistical machine translation. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation
and Beyond, pages 167?174, Ann Arbor, Michigan,
June.
S. Khadivi, A. Zolnay, and H. Ney. 2005. Automatic
text dictation in computer-assisted translation. In
Interspeech?2005 - Eurospeech, 9th European Con-
ference on Speech Communication and Technology,
pages 2265?2268, Portugal, Lisbon.
K. Knight and Y. Al-Onaizan. 1998. Translation
with finite-state devices. In D. Farwell, L. Gerber,
and E. H. Hovy, editors, AMTA, volume 1529 of
Lecture Notes in Computer Science, pages 421?437.
Springer Verlag.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 295?302, Philadelphia, PA, July.
F. J. Och, R. Zens, and H. Ney. 2003. Efficient search
for interactive statistical machine translation. In
EACL03: 10th Conf. of the Europ. Chapter of the
Association for Computational Linguistics, pages
387?393, Budapest, Hungary, April.
F. J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In EU-
ROSPEECH, pages 1435?1438, Rhodes, Greece,
September.
K. A. Papineni, S. Roukos, and R. T. Ward. 1998.
Maximum likelihood and discriminative training
of direct translation models. In Proc. IEEE Int.
Conf. on Acoustics, Speech, and Signal Processing
(ICASSP), volume 1, pages 189?192, Seattle, WA,
May.
M. Paulik, S. Stu?ker, C. Fu?gen, , T. Schultz, T. Schaaf,
and A. Waibel. 2005a. Speech translation enhanced
automatic speech recognition. In Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 121?126, Puerto Rico, San Juan.
M. Paulik, C. Fu?gen, S. Stu?ker, T. Schultz, T. Schaaf,
and A. Waibel. 2005b. Document driven machine
translation enhanced ASR. In Interspeech?2005 -
Eurospeech, 9th European Conference on Speech
Communication and Technology, pages 2261?2264,
Portugal, Lisbon.
A. Sixtus, S. Molau, S.Kanthak, R. Schlu?ter, and
H. Ney. 2000. Recent improvements of the
RWTH large vocabulary speech recognition system
on spontaneous speech. In Proc. IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing (ICASSP),
pages 1671 ? 1674, Istanbul, Turkey, June.
A. Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL),
pages 257?264, Boston, MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system.
In Proceedings of the International Workshop on
Spoken Language Translation (IWSLT), pages 155?
162, Pittsburgh, PA, October.
474
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 101?104,
Prague, June 2007. c?2007 Association for Computational Linguistics
Minimum Bayes Risk Decoding for BLEU
Nicola Ehling and Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{ehling,zens,ney}@cs.rwth-aachen.de
Abstract
We present a Minimum Bayes Risk (MBR)
decoder for statistical machine translation.
The approach aims to minimize the expected
loss of translation errors with regard to the
BLEU score. We show that MBR decoding
on N -best lists leads to an improvement of
translation quality.
We report the performance of the MBR
decoder on four different tasks: the TC-
STAR EPPS Spanish-English task 2006, the
NIST Chinese-English task 2005 and the
GALE Arabic-English and Chinese-English
task 2006. The absolute improvement of the
BLEU score is between 0.2% for the TC-
STAR task and 1.1% for the GALE Chinese-
English task.
1 Introduction
In recent years, statistical machine translation
(SMT) systems have achieved substantial progress
regarding their perfomance in international transla-
tion tasks (TC-STAR, NIST, GALE).
Statistical approaches to machine translation were
proposed at the beginning of the nineties and found
widespread use in the last years. The ?standard? ver-
sion of the Bayes decision rule, which aims at a min-
imization of the sentence error rate is used in vir-
tually all approaches to statistical machine transla-
tion. However, most translation systems are judged
by their ability to minimize the error rate on the word
level or n-gram level. Common error measures are
the Word Error Rate (WER) and the Position Inde-
pendent Word Error Rate (PER) as well as evalua-
tion metric on the n-gram level like the BLEU and
NIST score that measure precision and fluency of a
given translation hypothesis.
The remaining part of this paper is structured as
follows: after a short overview of related work in
Sec. 2, we describe the MBR decoder in Sec. 3. We
present the experimental results in Sec. 4 and con-
clude in Sec. 5.
2 Related Work
MBR decoder for automatic speech recognition
(ASR) have been reported to yield improvement
over the widely used maximum a-posteriori prob-
ability (MAP) decoder (Goel and Byrne, 2003;
Mangu et al, 2000; Stolcke et al, 1997).
For MT, MBR decoding was introduced in (Ku-
mar and Byrne, 2004). It was shown that MBR is
preferable over MAP decoding for different evalu-
ation criteria. Here, we focus on the performance
of MBR decoding for the BLEU score on various
translation tasks.
3 Implementation of Minimum Bayes Risk
Decoding for the BLEU Score
3.1 Bayes Decision Rule
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Statistical decision the-
ory tells us that among all possible target language
sentences, we should choose the sentence which
minimizes the Bayes risk:
e?I?1 = argmin
I,eI1
{
?
I?,e?I
?
1
Pr(e?I
?
1 |f
J
1 ) ? L(e
I
1, e
?I?
1 )
}
Here, L(?, ?) denotes the loss function under con-
sideration. In the following, we will call this deci-
sion rule the MBR rule (Kumar and Byrne, 2004).
101
Although it is well known that this decision rule is
optimal, most SMT systems do not use it. The most
common approach is to use the MAP decision rule.
Thus, we select the hypothesis which maximizes the
posterior probability Pr(eI1|f
J
1 ):
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
}
This decision rule is equivalent to the MBR crite-
rion under a 0-1 loss function:
L0?1(e
I
1, e
?I?
1 ) =
{
1 if eI1 = e
?I?
1
0 else
Hence, the MAP decision rule is optimal for the
sentence or string error rate. It is not necessarily
optimal for other evaluation metrics as for example
the BLEU score. One reason for the popularity of
the MAP decision rule might be that, compared to
the MBR rule, its computation is simpler.
3.2 Baseline System
The posterior probability Pr(eI1|f
J
1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
I?,e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(1)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be easily
integrated into the overall system.
The denominator represents a normalization fac-
tor that depends only on the source sentence fJ1 .
Therefore, we can omit it in case of the MAP de-
cision rule during the search process. Note that the
denominator affects the results of the MBR decision
rule and, thus, cannot be omitted in that case.
We use a state-of-the-art phrase-based translation
system similar to (Matusov et al, 2006) including
the following models: an n-gram language model,
a phrase translation model and a word-based lex-
icon model. The latter two models are used for
both directions: p(f |e) and p(e|f). Additionally,
we use a word penalty, phrase penalty and a distor-
tion penalty. The model scaling factors ?M1 are opti-
mized with respect to the BLEU score as described
in (Och, 2003).
3.3 BLEU Score
The BLEU score (Papineni et al, 2002) measures
the agreement between a hypothesis eI1 generated by
the MT system and a reference translation e?I?1. It is
the geometric mean of n-gram precisions Precn(?, ?)
in combination with a brevity penalty BP(?, ?) for too
short translation hypotheses.
BLEU(eI1, e?
I?
1) = BP(I, I?) ?
4?
n=1
Precn(e
I
1, e?
I?
1)
1/4
BP(I, I?) =
{
1 if I? ? I
exp (1 ? I/I?) if I? < I
Precn(e
I
1, e?
I?
1) =
?
wn1
min{C(wn1 |e
I
1), C(w
n
1 |e?
I?
1)}
?
wn1
C(wn1 |e
I
1)
Here, C(wn1 |e
I
1) denotes the number of occur-
rences of an n-gram wn1 in a sentence e
I
1. The de-
nominator of the n-gram precisions evaluate to the
number of n-grams in the hypothesis, i.e. I ?n+1.
As loss function for the MBR decoder, we use:
L[eI1, e?
I?
1] = 1 ? BLEU(e
I
1, e?
I?
1) .
While the original BLEU score was intended to be
used only for aggregate counts over a whole test set,
we use the BLEU score at the sentence-level during
the selection of the MBR hypotheses. Note that we
will use this sentence-level BLEU score only during
decoding. The translation results that we will report
later are computed using the standard BLEU score.
3.4 Hypothesis Selection
We select the MBR hypothesis among the N best
translation candidates of the MAP system. For each
entry, we have to compute its expected BLEU score,
i.e. the weighted sum over all entries in the N -best
list. Therefore, finding the MBR hypothesis has a
quadratic complexity in the size of the N -best list.
To reduce this large work load, we stop the summa-
tion over the translation candidates as soon as the
risk of the regarded hypothesis exceeds the current
minimum risk, i.e. the risk of the current best hy-
pothesis. Additionally, the hypotheses are processed
according to the posterior probabilities. Thus, we
can hope to find a good candidate soon. This allows
for an early stopping of the computation for each of
the remaining candidates.
102
3.5 Global Model Scaling Factor
During the translation process, the different sub-
models hm(?) get different weights ?m. These scal-
ing factors are optimized with regard to a specific
evaluation criteria, here: BLEU. This optimization
describes the relation between the different models
but does not define the absolute values for the scal-
ing factors. Because search is performed using the
maximum approximation, these absolute values are
not needed during the translation process. In con-
trast to this, using the MBR decision rule, we per-
form a summation over all sentence probabilities
contained in the N -best list. Therefore, we use a
global scaling factor ?0 > 0 to modify the individ-
ual scaling factors ?m:
??m = ?0 ? ?m ,m = 1, ...,M.
For the MBR decision rule the modified scaling fac-
tors ??m are used instead of the original model scal-
ing factors ?m to compute the sentence probabilities
as in Eq. 1. The global scaling factor ?0 is tuned on
the development set. Note that under the MAP deci-
sion rule any global scaling factor ?0 > 0 yields the
same result. Similar tests were reported by (Mangu
et al, 2000; Goel and Byrne, 2003) for ASR.
4 Experimental Results
4.1 Corpus Statistics
We tested the MBR decoder on four translation
tasks: the TC-STAR EPPS Spanish-English task of
2006, the NIST Chinese-English evaluation test set
of 2005 and the GALE Arabic-English and Chinese-
English evaluation test set of 2006. The TC-STAR
EPPS corpus is a spoken language translation corpus
containing the verbatim transcriptions of speeches
of the European Parliament. The NIST Chinese-
English test sets consists of news stories. The GALE
project text track consists of two parts: newswire
(?news?) and newsgroups (?ng?). The newswire part
is similar to the NIST task. The newsgroups part
covers posts to electronic bulletin boards, Usenet
newsgroups, discussion groups and similar forums.
The corpus statistics of the training corpora are
shown in Tab. 1 to Tab. 3. To measure the trans-
lation quality, we use the BLEU score. With ex-
ception of the TC-STAR EPPS task, all scores are
computed case-insensitive. As BLEU measures ac-
curacy, higher scores are better.
Table 1: NIST Chinese-English: corpus statistics.
Chinese English
Train Sentences 9M
Words 232M 250M
Vocabulary 238K 412K
NIST 02 Sentences 878
Words 26 431 24 352
NIST 05 Sentences 1 082
Words 34 908 36 027
GALE 06 Sentences 460
news Words 9 979 11 493
GALE 06 Sentences 461
ng Words 9 606 11 689
Table 2: TC-Star Spanish-English: corpus statistics.
Spanish English
Train Sentences 1.2M
Words 35M 33M
Vocabulary 159K 110K
Dev Sentences 1 452
Words 51 982 54 857
Test Sentences 1 780
Words 56 515 58 295
4.2 Translation Results
The translation results for all tasks are presented
in Tab. 4. For each translation task, we tested the
decoder on N -best lists of size N=10 000, i.e. the
10 000 best translation candidates. Note that in some
cases the list is smaller because the translation sys-
tem did not produce more candidates. To analyze
the improvement that can be gained through rescor-
ing with MBR, we start from a system that has al-
ready been rescored with additional models like an
n-gram language model, HMM, IBM-1 and IBM-4.
It turned out that the use of 1 000 best candidates
for the MBR decoding is sufficient, and leads to ex-
actly the same results as the use of 10 000 best lists.
Similar experiences were reported by (Mangu et al,
2000; Stolcke et al, 1997) for ASR.
We observe that the improvement is larger for
Table 3: GALE Arabic-English: corpus statistics.
Arabic English
Train Sentences 4M
Words 125M 124M
Vocabulary 421K 337K
news Sentences 566
Words 14 160 15 320
ng Sentences 615
Words 11 195 14 493
103
Table 4: Translation results BLEU [%] for the NIST task, GALE task and TC-STAR task (S-E: Spanish-
English; C-E: Chinese-English; A-E: Arabic-English).
TC-STAR S-E NIST C-E GALE A-E GALE C-E
decision rule test 2002 (dev) 2005 news ng news ng
MAP 52.6 32.8 31.2 23.6 12.2 14.6 9.4
MBR 52.8 33.3 31.9 24.2 13.3 15.4 10.5
Table 5: Translation examples for the GALE Arabic-English newswire task.
Reference the saudi interior ministry announced in a report the implementation of the death penalty
today, tuesday, in the area of medina (west) of a saudi citizen convicted of murdering a
fellow citizen.
MAP-Hyp saudi interior ministry in a statement to carry out the death sentence today in the area of
medina (west) in saudi citizen found guilty of killing one of its citizens.
MBR-Hyp the saudi interior ministry announced in a statement to carry out the death sentence today
in the area of medina (west) in saudi citizen was killed one of its citizens.
Reference faruq al-shar?a takes the constitutional oath of office before the syrian president
MAP-Hyp farouk al-shara leads sworn in by the syrian president
MBR-Hyp farouk al-shara lead the constitutional oath before the syrian president
low-scoring translations, as can be seen in the GALE
task. For an ASR task, similar results were reported
by (Stolcke et al, 1997).
Some translation examples for the GALE Arabic-
English newswire task are shown in Tab. 5. The dif-
ferences between the MAP and the MBR hypotheses
are set in italics.
5 Conclusions
We have shown that Minimum Bayes Risk decod-
ing on N -best lists improves the BLEU score con-
siderably. The achieved results are promising. The
improvements were consistent among several eval-
uation sets. Even if the improvement is sometimes
small, e.g. TC-STAR, it is statistically significant:
the absolute improvement of the BLEU score is be-
tween 0.2% for the TC-STAR task and 1.1% for the
GALE Chinese-English task. Note, that MBR de-
coding is never worse than MAP decoding, and is
therefore promising for SMT. It is easy to integrate
and can improve even well-trained systems by tun-
ing them for a particular evaluation criterion.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A statistical approach to machine translation. Com-
putational Linguistics, 16(2):79?85, June.
V. Goel and W. Byrne. 2003. Minimum bayes-risk automatic
speech recognition. Pattern Recognition in Speech and Lan-
guage Processing.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk decod-
ing for statistical machine translation. In Proc. Human Lan-
guage Technology Conf. / North American Chapter of the
Assoc. for Computational Linguistics Annual Meeting (HLT-
NAACL), pages 169?176, Boston, MA, May.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373?400, October.
E. Matusov, R. Zens, D. Vilar, A. Mauser, M. Popovic?,
S. Hasan, and H. Ney. 2006. The RWTH machine trans-
lation system. In Proc. TC-Star Workshop on Speech-to-
Speech Translation, pages 31?36, Barcelona, Spain, June.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. 40th Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 295?302, Philadelphia, PA, July.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. 41st Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 160?167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proc. 40th Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA, July.
A. Stolcke, Y. Konig, and M. Weintraub. 1997. Explicit word
error minimization in N-best list rescoring. In Proc. Eu-
ropean Conf. on Speech Communication and Technology,
pages 163?166, Rhodes, Greece, September.
104
Stochastic Modelling: From Pattern Classification
to Language Translation
Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen ? University of Technology
D-52056 Aachen, Germany
ney@informatik.rwth-aachen.de
Abstract
This paper gives an overview of
the stochastic modelling approach to
machine translation. Starting with
the Bayes decision rule as in pattern
classification and speech recognition,
we show how the resulting system
architecture can be structured into three
parts: the language model probability,
the string translation model probability
and the search procedure that gener-
ates the word sequence in the target
language. We discuss the properties
of the system components and report
results on the translation of spoken
dialogues in the VERBMOBIL project.
The experience obtained in the VERB-
MOBIL project, in particular a large-
scale end-to-end evaluation, showed
that the stochastic modelling approach
resulted in significantly lower error
rates than three competing translation
approaches: the sentence error rate was
29% in comparison with 52% to 62%
for the other translation approaches.
1 Introduction
The use of statistics in computational linguistics
has been extremely controversial for more than
three decades. The controversy is very well
summarized by the statement of Chomsky in 1969
(Chomsky 1969):
?It must be recognized that the notion of
a ?probability of a sentence? is an entirely
useless one, under any interpretation of this
term?.
This statement was considered to be correct by
the majority of experts from artificial intelligence
and computational linguistics, and the concept
of statistics was banned from computational
linguistics for many years.
What is overlooked in this statement is the
fact that, in an automatic system for speech
recognition or language translation, we are faced
with the problem of taking decisions. It is
exactly here where statistical decision theory
comes in. In automatic speech recognition (ASR),
the success of the statistical approach is based on
the equation:
ASR = Acoustic?Linguistic Modelling
+ Statistical Decision Theory
Similarly, for machine translation (MT), the
statistical approach is expressed by the equation:
MT = Linguistic Modelling
+ Statistical Decision Theory
For the ?low-level? description of speech and
image signals, it is widely accepted that the
stochastic framework allows an efficient coupling
between the observations and the models, which
is often described by the buzz word ?subsymbolic
processing?. But there is another advantage in
using probability distributions in that they offer an
explicit formalism for expressing and combining
hypothesis scores:
? The probabilities are directly used as scores:
These scores are normalized, which is a
desirable property: when increasing the
score for a certain element in the set of all
hypotheses, there must be one or several
other elements whose scores are reduced at
the same time.
? It is evident how to combine scores:
depending on the task, the probabilities are
either multiplied or added.
? Weak and vague dependencies can be
modelled easily. Especially in spoken and
written natural language, there are nuances
and shades that require ?grey levels? between
0 and 1.
Even if we think we can manage without
statistics, we will need models which always
have some free parameters. Then the question is
how to train these free parameters. The obvious
approach is to adjust these parameters in such
a way that we get optimal results in terms of
error rates or similar criteria on a representative
sample. So we have made a complete cycle and
have reached the starting point of the stochastic
modelling approach again!
When building an automatic system for speech
or language, we should try to use as much
prior knowledge as possible about the task
under consideration. This knowledge is used
to guide the modelling process and to enable
improved generalization with respect to unseen
data. Therefore in a good stochastic modelling
approach, we try to identify the common patterns
underlying the observations, i.e. to capture
dependencies between the data in order to avoid
the pure ?black box? concept.
2 Language Translation as Pattern
Classification
2.1 Bayes Decision Rule
Knowing that language translation is a difficult
task, we want to keep the number of wrong
translations as small as possible. The corre-
sponding formalism is provided by the so-called
statistical decision theory. The resulting decison
rule is referred to as Bayes decision rule and is
the starting point for many techniques in pattern
classification (Duda et al 2001). To classify
an observation vector y into one out of several
classes c, the Bayes decision rule is:
c? = argmaxc {Pr(c|y)}
= argmaxc {Pr(c) ? Pr(y|c)} .
For language translation, the starting point is the
observed sequence of source symbols y = fJ1 =
f1...fJ , i.e. the sequence of source words, for
which the target word sequence c = eI1 = e1...eI
has to be determined. In order to minimize the
number of decision errors at the sentence level,
we have to choose the sequence of target words
e?I1 according to the equation (Brown et al 1993):
e?I1 = argmaxeI1
{
Pr(eI1|fJ1 )
}
= argmax
eI1
{
Pr(eI1) ? Pr(fJ1 |eI1)
}
.
Here, the posterior probability Pr(eI1|fJ1 ) is
decomposed into the language model probability
Pr(eJ1 ) and the string translation probability
Pr(fJ1 |eI1). Due to this factorization, we have
two separate probability distributions which can
be modelled and trained independently of each
other.
Fig.1 shows the architecture that results from
the Bayes decision theory. Here we have already
taken into account that, in order to implement
the string translation model, we will decompose
it into a so-called alignment model and a lexicon
model. As also shown in this figure, we explicitly
allow for optional transformations to make the
translation task simpler for the algorithm.
In total, we have the following crucial
constituents of the stochastic modelling approach
to language translation:
? There are two separate probability distribu-
tions or stochastic knowledge sources:
? the language model distribution
Pr(eI1), which is assigned to each
possible target word sequence eI1 and
which ultimately captures all syntactic,
semantic and pragmatic constraints
of the target language domain under
consideration;
? the string translation probability dis-
tribution Pr(fJ1 |eI1) which assigns a
score as to how well the source string
fJ1 matches the hypothesized target
sequence eI1.
? In addition to these two knowledge sources,
we need another system component which is
referred to as a search or decision process.
According to the Bayes decision rule, this
search has to carry out the maximization
of the product of the two probability
distributions and thus ensures an optimal
interaction of the two knowledge sources.
Source Language Text
Transformation
 Lexicon Model
Language Model
Global Search:
 
 
Target Language Text
 
over
 
 Pr(f1  J  |e1I )
 
 
 Pr(   e1I )
 
 
 Pr(f1  J  |e1I )   Pr(   e1I )
  
e1I
f1 J
maximize  Alignment Model
Transformation
Figure 1: Bayes architecture for language
translation.
Note that there is a guarantee of the minimization
of decision errors if we know the true probability
distributions Pr(eI1) and Pr(fJ1 |eI1) and if we
carry out a full search over all target word
sequences eI1. In addition, it should be noted
that both the sequence of source words fJ1 and
the sequence of unknown target words eI1 are
modelled as a whole. The advantage then is
that context dependencies can be fully taken into
account and the syntactic analysis of both source
and target sequences (at least in principle) can be
integrated into the translation process.
2.2 Implementation of Stochastic Modelling
To build a real operational system for language
translation, we are faced with the following three
problems:
? Search problem:
In principle, the innocent looking maximiza-
tion requires the evaluation of 20 00010 =
1043 possible target word sequences, when
we assume a vocabulary of 20 000 target
words and a sentence length of I =
10 words. This is the price we have
to pay for a full interaction between the
language model Pr(eI1) and and the string
translation model Pr(fJ1 |eI1). In such a
way, however, it is guaranteed that there is
no better way to take the decisions about
the words in the target language (for the
given probability distributions Pr(eI1) and
Pr(fJ1 |eI1)). In a practical system, we
of course use suboptimal search strategies
which require much less effort than a full
search, but nevertheless should find the
global optimum in virtually all cases.
? Modelling problem:
The two probability distributions Pr(eI1)
and Pr(fJ1 |eI1) are too general to be used
in a table look-up approach, because there
is a huge number of possible values fJ1
and eI1. Therefore we have to introduce
suitable structures into the distributions
such that the number of free parameters is
drastically reduced by taking suitable data
dependencies into account.
A key issue in modelling the string transla-
tion probability Pr(fJ1 |eI1) is the question
of how we define the correspondence
between the words of the target sentence
and the words of the source sentence.
In typical cases, we can assume a sort
of pairwise dependence by considering all
word pairs (fj , ei) for a given sentence
pair (fJ1 ; eI1). Typically, the dependence is
further constrained by assigning each source
word to exactly one target word. Models
describing these types of dependencies are
referred to as alignment mappings (Brown et
al. 1993):
alignment mapping: j ? i = aj ,
which assigns a source word fj in position j
to a target word ei in position i = aj . As a
result, the string translation probability can
be decomposed into a lexicon probability
and an alignment probability (Brown et al
1993).
? Training problem:
After choosing suitable models for the two
distributions Pr(eI1) and Pr(fJ1 |eI1), there
remain free parameters that have to be
learned from a set of training observa-
tions, which in the statistical terminology
is referred to as parameter estimation.
For several reasons, especially for the
interdependence of the parameters, this
learning task typically results in a com-
plex mathematical optimization problem the
details of which depend on the chosen
model and on the chosen training criterion
(such as maximum likelihood, squared error
criterion, discriminative criterion, minimum
number of recognition errors, ...).
In conclusion, stochastic modelling as such
does not solve the problems of automatic
language translation, but defines a basis on which
we can find the solutions to the problems. In
contradiction to a widely held belief, a stochastic
approach may very well require a specific model,
and statistics helps us to make the best of a
given model. Since undoubtedly we have to take
decisions in the context of automatic language
processing (and speech recognition), it can only
be a rhetoric question of whether we should use
statistical decision theory at all. To make a
comparison with another field: in constructing
a power plant, it would be foolish to ignore the
principles of thermodynamics!
As to the search problem, the most successful
strategies are based on either stack decoding
or A? search and dynamic programming beam
search. For comparison, in speech recognition,
over the last few years, there has been a lot
of progress in structuring the search process to
generate a compact word lattice or word graph.
To make this point crystal clear: The
characteristic property of the stochastic modelling
approach to language translation is not the use
of hidden Markov models or hidden alignments.
These methods are only the time-honoured
methods and successful methods of today. The
characteristic property lies in the systematic use
of a probabilistic framework for the construction
of models, in the statistical training of the free
parameters of these models and in the explicit
use of a global scoring criterion for the decision
making process.
3 Experimental Results
Whereas stochastic modelling is widely used in
speech recognition, there are so far only a few
research groups that apply stochastic modelling to
language translation (Berger et al 1994; Brown et
al. 1993; Knight 1999). The presentation here is
based on work carried out in the framework of the
EUTRANS project (Casacuberta et al 2001) and
the VERBMOBIL project (Wahlster 2000).
We will consider the experimental results
obtained in the VERBMOBIL project. The goal
of the VERBMOBIL project is the translation of
spoken dialogues in the domains of appointment
scheduling and travel planning. The languages
are German and English. Whereas during
the progress of the project many offline tests
were carried out for the optimization and tuning
of the statistical approach, the most important
evaluation was the final evaluation of the
VERBMOBIL prototype in spring 2000. This end-
to-end evaluation of the VERBMOBIL system was
performed at the University of Hamburg (Tessiore
et al 2000). In each session of this evaluation,
two native speakers conducted a dialogue. The
speakers did not have any direct contact and could
only interact by speaking and listening to the
VERBMOBIL system.
In addition to the statistical approach, three
other translation approaches had been integrated
into the VERBMOBIL prototype system (Wahlster
2000):
? a classical transfer approach,
which is based on a manually designed
analysis grammar, a set of transfer rules, and
a generation grammar,
? a dialogue act based approach,
which amounts to a sort of slot filling by
classifying each sentence into one out of a
small number of possible sentence patterns
and filling in the slot values,
? an example based approach,
where a sort of nearest neighbour concept
is applied to the set of bilingual training
sentence pairs after suitable preprocessing.
In the final end-to-end evaluation, human
evaluators judged the translation quality for each
of the four translation results using the following
criterion: Is the sentence approximatively correct:
yes/no? The evaluators were asked to pay
particular attention to the semantic information
(e.g. date and place of meeting, participants etc.)
contained in the translation. A missing translation
as it may happen for the transfer approach or other
approaches was counted as wrong translation.
The evaluation was based on 5069 dialogue turns
for the translation from German to English and
on 4136 dialogue turns for the translation from
Table 1: Error rates of spoken sentence translation
in the VERBMOBIL end-to-end evaluation.
Translation Method Error [%]
Semantic Transfer 62
Dialogue Act Based 60
Example Based 52
Statistical 29
English to German. The speech recognizers
used had a word error rate of about 25%. The
overall sentence error rates, i.e. resulting from
recognition and translation, are summarized in
Table 1. As we can see, the error rates for the
statistical approach are smaller by a factor of
about 2 in comparison with the other approaches.
In agreement with other evaluation experi-
ments, these experiments show that the statistical
modelling approach may be comparable to or
better than the conventional rule-based approach.
In particular, the statistical approach seems to
have the advantage if robustness is important, e.g.
when the input string is not grammatically correct
or when it is corrupted by recognition errors.
4 Conclusion
In summary, in the comparative evaluations, both
text and speech input were translated with good
quality on the average by the statistical approach.
Nevertheless, there are examples where the
syntactic structure of the produced target sentence
is not correct. Some of these syntactic errors
are related to long range dependencies and
syntactic structures that are not captured by the
m-gram language model used. To cope with
these problems, morpho-syntactic analysis and
grammar-based language models are currently
being studied.
Acknowledgment
This paper is based on work supported partly
by the VERBMOBIL project (contract number
01 IV 701 T4) by the German Federal Ministry
of Education, Science, Research and Technology
and as part of the EUTRANS project (ESPRIT
project number 30268) by the European Commu-
nity.
References
A. L. Berger, P. F. Brown, J. Cocke, S. A. Della
Pietra, V. J. Della Pietra, J. R. Gillett, J. D. Lafferty,
R. L. Mercer, H. Printz, L. Ures: ?The Candide
System for Machine Translation?, ARPA Human
Language Technology Workshop, Plainsboro, NJ,
Morgan Kaufmann Pub., San Mateo, CA, pp. 152-
157, March 1994.
P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, R. L. Mercer: ?Mathematics of Statistical
Machine Translation: Parameter Estimation?,
Computational Linguistics, Vol. 19.2, pp. 263-311,
June 1993.
N. Chomsky: ?Quine?s Empirical Assumptions?,
in D. Davidson, J. Hintikka (eds.): Words and
objections. Essays on the work of W. V. Quine,
Reidel, Dordrecht, The Netherlands, 1969.
F. Casacuberta, D. Llorenz, C. Martinez, S. Molau,
F. Nevado, H. Ney, M. Pastor, D. Pico, A. Sanchis,
E. Vidal, J. Vilar: ?Speech-To-Speech Translation
Based on Finite-State Transducers?, IEEE Int. Conf.
on Acoustics, Speech and Signal Processing, Salt
Lake City, UT, May 2001.
R. O. Duda, P. E. Hart, D. G. Stork: Pattern
Classification, 2nd ed., John Wiley & Sons, New
York, NY, 2001.
K. Knight: ?Decoding Complexity in Word-
Replacement Translation Models?, Computational
Linguistics, No. 4, Vol. 25, pp. 607-615, 1999.
H. Ney, F. J. Och, S. Vogel: ?The RWTH System
for Statistical Translation of Spoken Dialogues?,
Human Language Technology Conference, San
Diego, CA, Proceedings in press, March 2001.
F. J. Och, C. Tillmann, H. Ney: ?Improved Alignment
Models for Statistical Machine Translation?, Joint
SIGDAT Conf. on Empirical Methods in Natural
Language Processing and Very Large Corpora,
pp. 20?28, University of Maryland, College Park,
MD, June 1999.
L. Tessiore, W. v. Hahn: ?Functional Validation
of a Machine Interpretation System: Verbmobil?,
pp. 611?631, in (Wahlster 2000).
W. Wahlster (Ed.): Verbmobil: Foundations of Speech-
to-Speech Translation. Springer-Verlag, Berlin,
Germany, 2000.
Toward hierarchical models for statistical machine translation of
inflected languages
Sonja Nie?en and Hermann Ney
Lehrstuhl fu?r Informatik VI,
Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
 
niessen,ney  @informatik.rwth-aachen.de
Abstract
In statistical machine translation, cor-
respondences between the words in
the source and the target language are
learned from bilingual corpora on the
basis of so called alignment models.
Existing statistical systems for MT of-
ten treat different derivatives of the
same lemma as if they were indepen-
dent of each other. In this paper we
argue that a better exploitation of the
bilingual training data can be achieved
by explicitly taking into account the in-
terdependencies of the different deriva-
tives. We do this along two direc-
tions: Usage of hierarchical lexicon
models and the introduction of equiv-
alence classes in order to ignore in-
formation not relevant for the trans-
lation task. The improvement of the
translation results is demonstrated on a
German-English corpus.
1 Introduction
The statistical approach to machine translation
has become widely accepted in the last few years.
It has been successfully applied to realistic tasks
in various national and international research pro-
grams. However in many applications only small
amounts of bilingual training data are available
for the desired domain and language pair, and it
is highly desirable to avoid at least parts of the
costly data collection process.
Some recent publications have dealt with the
problem of translation with scarce resources.
(Brown et al, 1994) describe the use of dictio-
naries. (Al-Onaizan et al, 2000) report on an ex-
periment of Tetun-to-English translation by dif-
ferent groups, including one using statistical ma-
chine translation. They assume the absence of
linguistic knowledge sources such as morphologi-
cal analyzers and dictionaries. Nevertheless, they
found that human mind is very well capable of
deriving dependencies such as morphology, cog-
nates, proper names, spelling variations etc., and
that this capability was finally at the basis of the
better results produced by humans compared to
corpus based machine translation. The additional
information results from complex reasoning and it
is not directly accessible from the full word form
representation of the data.
In this paper, we take a different point of
view: Even if full bilingual training data is scarce,
monolingual knowledge sources like morpholog-
ical analyzers and data for training the target lan-
guage model as well as conventional dictionar-
ies (one word and its translation per entry) may
be available and of substantial usefulness for im-
proving the performance of statistical translation
systems. This is especially the case for highly in-
flected languages like German.
We address the question of how to achieve a
better exploitation of the resources for training the
parameters for statistical machine translation by
taking into account explicit knowledge about the
languages under consideration. In our approach
we introduce equivalence classes in order to ig-
nore information not relevant to the translation
process. We furthermore suggest the use of hi-
erarchical lexicon models.
The paper is organized as follows. After re-
viewing the statistical approach to machine trans-
lation, we first explain our motivation for exam-
ining the morphological characteristics of an in-
flected language like German. We then describe
the chosen output representation after the analysis
and present our approach for exploiting the infor-
mation from morpho-syntactic analysis. Experi-
mental results on the German-English Verbmobil
task are reported.
2 Statistical Machine Translation
The goal of the translation process in statisti-
cal machine translation can be formulated as fol-
lows: A source language string   		 

is to be translated into a target language string


 


 		



. In the experiments reported in this
paper, the source language is German and the tar-
get language is English. Every English string is
considered as a possible translation for the input.
If we assign a probability  
    to each pair
of strings  
     , then according to Bayes? de-
cision rule, we have to choose the English string
that maximizes the product of the English lan-
guage model  
    and the string translation
model  An Efficient A* Search Algorithm for Statistical Machine Translation
Franz Josef Och, Nicola Ueffing, Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
{och,ueffing,ney}@informatik.rwth-aachen.de
Abstract
In this paper, we describe an efficient
A* search algorithm for statistical ma-
chine translation. In contrary to beam-
search or greedy approaches it is possi-
ble to guarantee the avoidance of search
errors with A*. We develop various so-
phisticated admissible and almost ad-
missible heuristic functions. Especially
our newly developped method to per-
form a multi-pass A* search with an
iteratively improved heuristic function
allows us to translate even long sen-
tences. We compare the A* search al-
gorithm with a beam-search approach
on the Hansards task.
1 Introduction
The goal of machine translation is the transla-
tion of a text given in some source language into
a target language. We are given a source string
fJ1 = f1...fj ...fJ , which is to be translated into a
target string eI1 = e1...ei...eI . Among all possible
target strings, we will choose the string with the
highest probability:
e?I1 = argmaxeI1
{
Pr(eJ1 |f I1 )
}
= argmax
eI1
{
Pr(eI1) ? Pr(fJ1 |eI1)
}
The argmax operation denotes the search prob-
lem, i.e. the generation of the output sentence
in the target language. Pr(eI1) is the language
model of the target language, whereas Pr(fJ1 |eI1)
denotes the translation model.
Many statistical translation models (Brown et
al., 1993; Vogel et al, 1996; Och and Ney, 2000b)
try to model word-to-word correspondences be-
tween source and target words. These correspon-
dences are called an alignment. The model is
often further restricted in a way such that each
source word is assigned exactly one target word.
The alignment mapping is j ? i = aj from
source position j to target position i = aj . The
alignment aJ1 may contain alignments aj = 0
with the ?empty? word e0 to account for source
words that are not aligned to any target word. In
(statistical) alignment models Pr(fJ1 , aJ1 |eI1), the
alignment aJ1 is introduced as a hidden variable.
Typically, the search is performed using the so-
called maximum approximation:
e?I1 = argmaxeI1
?
??
??
Pr(eI1) ?
?
aJ1
Pr(fJ1 , aJ1 |eI1)
?
??
??
= argmax
eI1
{
Pr(eI1) ?maxaJ1
Pr(fJ1 , aJ1 |eI1)
}
The search space consists of the set of all possible
target language strings eI1 and all possible align-
ments aJ1 .
2 IBM Model 4
Various statistical alignment models of the form
Pr(fJ1 , aJ1 |eI1) have been introduced in (Brown
et al, 1993; Vogel et al, 1996; Och and Ney,
2000a). In this paper we use the so-called Model
4 from (Brown et al, 1993).
In Model 4 the statistical alignment model is
decomposed into five sub-models:
? the lexicon model p(f |e) for the probability
that the source word f is a translation of the
target word e,
? the distortion model p=1(j?j?|C(fj), E) for
the probability that the translations of two
consecutive target words have the position
difference j ? j? where C(fj) is the word
class of fj and E is the word class of the
first of the two consecutive target words,
? the distortion model p>1(j ? j?|C(fj)) for
the probability that the words aligned to one
target words have the position difference j?
j?,
? the fertility model p(?|e) for the probability
that a target language word e is aligned to ?
source language words,
? the empty word fertility model p(?0|e0) for
the probability that exactly ?0 words remain
unaligned to.
The final probability p(fJ1 , aJ1 |eI1) for Model 4 is
obtained by multiplying the probabilities of the
sub-models for all words. For a detailed descrip-
tion for Model 4 the reader is referred to (Brown
et al, 1993).
We use Model 4 in this paper for two reasons.
First, it has been shown that Model 4 produces
a very good alignment quality in comparison to
various other alignment models (Och and Ney,
2000b). Second, the dependences in the distortion
model along the target language words make it
quite easy to integrate standard n-gram language
models in the search process. This would be more
difficult in the HMM alignment model (Vogel et
al., 1996). Yet, many of the results presented in
the following are also applicable to other align-
ment models.
3 Search problem
The following tasks have to be performed both us-
ing A* and beam search (BS):
? The search space has to be structured into
a search graph. This search graph typically
includes an initial node, intermediary nodes
(partial hypotheses), and goal nodes (com-
pleted hypotheses). A node contains the fol-
lowing information:
? the predecessor words u, v in the target
language,
? the score of the hypothesis,
? a backpointer to the preceding partial
hypothesis,
? the model specific information de-
scribed at the end of this subsection.
? A scoring function Q(n) + h(n) has to be
defined which assigns a score to every node
n. For beam search, this is the score Q(n) of
a best path to this node. In the A* algorithm,
an estimation h(n) of the score of a best path
from node n to a goal node is added.
(Berger et al, 1996) presented a method to struc-
ture the search space. Our search algorithm for
Model 4 uses a similar structuring of the search
space. We will shortly review the basic concepts
of this search space structure: Every partial hy-
pothesis consists of a prefix of the target sentence
and a corresponding alignment. A partial hypoth-
esis is extended by accounting for exactly one ad-
ditional word of the source sentence. Every exten-
sion yields an extension score which is computed
by taking into account the lexicon, distortion, and
fertility probabilities involved with this extension.
A partial hypothesis is called open if more source
words are to be aligned to the current target word
in the following extensions. A hypothesis that is
not open is said to be closed. Every extension of
an open hypothesis will extend the fertility of the
previously produced target word and an extension
of a closed hypothesis will produce a new word.
Therefore, the language model score is added as
well if a closed hypothesis is extended.
It is prohibitive to consider all possible transla-
tions of all words. Instead, we restrict the search
to the most promising candidates by calculating
?inverse translations? (Al-Onaizan et al, 1999).
The inverse translation probability p(e | f) of a
source word f is calculated as
p(e | f) = p (f | e) p (e)?
e?
p (f | e?) p (e?) ,
where we use a unigram model p (e) to esti-
mate the prior probability of a target word be-
ing used. Like (Al-Onaizan et al, 1999), we use
only the top 12 translations of a given source lan-
guage word. In addition, we remove from this list
all words whose inverse translation probability is
lower than 0.01 times the best inverse translation
probability. This observation pruning is the only
pruning involved in our A* search algorithm. Ex-
periments showed this does not impair translation
quality, but the search becomes much more effi-
cient.
In order to keep the search space as small as
possible it is crucial to perform a recombina-
tion of search hypotheses. Every two hypothe-
ses which can be distinguished by neither the lan-
guage model state nor the translation model state
can be recombined, only the hypothesis with a
better score of the two needs to be considered in
the subsequent search process. We use a standard
trigram language model, so the relevant language
model state of node n consists of the current word
w(n) and the previous word v(n) (later on we will
describe an improvement to this). The translation
model state depends on the specific model depen-
dencies of Model 4:
? a coverage set C(n) containing the already
translated source language positions,
? the position j(n) of the previously translated
source word,
? a flag indicating whether the hypothesis is
open or closed,
? the number of source language words which
are aligned to the empty word,
? a flag showing whether the hypothesis is a
complete hypothesis or not.
Efficient language model recombination
The recombination procedure which is described
above can be improved by taking into account the
backing-off structure of the language model. The
trigram language model we use has the property
that if the count of the bigram N(u, v) = 0, then
the probability P (w|u, v) depends only on v. In
this case the recombination can be significantly
improved by recombining all nodes whose lan-
guage model state has the property N(u, v) = 0
only with respect to v. Obviously, this could be
generalized to other types of language models as
well.
Experiments have shown that by using this ef-
ficient recombination, the number of needed hy-
potheses can be reduced by about a factor of 4.
Search algorithms
We evaluate the following two search algorithms:
? beam search algorithm (BS): (Tillmann,
2001; Tillmann and Ney, 2000)
In this algorithm the search space is explored
in a breadth-first manner. The search algo-
rithm is based on a dynamic programming
approach and applies various pruning tech-
niques in order to restrict the number of con-
sidered hypotheses. For more details see
(Tillmann, 2001).
? A* search algorithm:
In A*, all search hypotheses are managed in
a priority queue. The basic A* search (Nils-
son, 1971) can be described as follows:
1. initialize priority queue with an empty
hypothesis
2. remove the hypothesis with the highest
score from the priority queue
3. if this hypothesis is a goal hypothesis:
output this hypothesis and terminate
4. produce all extensions of this hypothe-
sis and put the extensions to the queue
5. goto 2
The so-called heuristic function estimates
the probability of a completion of a partial
hypothesis. This function is called admissi-
ble if it never underestimates this probabil-
ity. Thus, admissible heuristic functions are
always optimistic. The A* search algorithm
corresponds to the Dijkstra algorithm if the
heuristic function is equal to zero.
4 Admissible heuristic function
In order to perform an efficient search with the
A* search algorithm it is crucial to use a good
heuristic function. We only know of the work by
(Wang and Waibel, 1997) dealing with heuristic
functions for search in statistical machine trans-
lation. They developed a simple heuristic func-
tion for Model 2 from (Brown et al, 1993) which
was non admissible. In the following we de-
velop a guaranteed admissible heuristic function
for Model 4 taking into account distortion proba-
bilities and the coupling of lexicon, fertility, and
language model probabilities.
The basic idea for developing a heuristic func-
tion for the alignment models is the fact that all
source sentence positions which have not been
covered so far still have to be translated in order
to complete the sentence. Therefore, the value of
the heuristic function HX(n) for a node n can
be deduced if we have an estimation hX(j) of the
optimal score of translating position j (here X de-
notes different possibilities to choose the heuristic
function):
HX(n) =
?
j 6?C(n)
hX(j) ,
where C(n) is the coverage set.
The simplest realization of a heuristic func-
tion, denoted as hT (j), takes into account only
the translation probability p(f |e):
hT (j) = maxe p(fj |e)
This heuristic function can be refined by intro-
ducing also the fertility probabilities (symbol F)
of a target word e:
hTF (j) =
= max
{
max
e 6=e0,?
p(fj |e) ?
?
p(?|e), p(f |e0)
}
Thereby, a coupling between the translation and
fertility probabilities is achieved. We have to
take the ?-th root in order to avoid that the fer-
tility probability of a target word whose fertility
is higher than one is taken into account for every
source word aligned to it. For words which are
translated by the empty word e0, no fertility prob-
ability is used.
The language model can be incorporated by
considering that for every target word there exists
an optimal language model probability:
pL(e) = maxu,v p(e|u, v)
Here, we assume a trigram language model.
Thus, a heuristic function including a coupling
between translation, fertility, and language model
probabilities (TFL) is given by:
hTFL(j) =
= max
{
max
e,?
p(fj |e) ?
?
p(?|e)pL(e), p(f |e0)
}
This value can be precomputed efficiently before
the search process itself starts.
The heuristic function for the distortion proba-
bilities depends on the used model. For Model 4,
we obtain:
hD(j) = max
j?,E
p(j ? j?|E,C(fj))
Here, E refers to the class of the previously
aligned target word.
The heuristic functions hD(j) involve maxi-
mizations over the source positions j?. The do-
main of this variable shrinks during search as
more and more words get translated. Therefore, it
is possible to improve this heuristic function dur-
ing search to perform a maximization only over
the free source language positions j?. For Model 4
we compute the following heuristic function with
two arguments:
hD(j?, j) = max
E
p(j ? j?|E,C(fj))
Thus, we obtain as an estimation of the distortion
probability
hD(j) = max
j? 6?C(n)
hD(j?, j) .
This yields the following heuristic functions tak-
ing into account translation, fertility, language,
and distortion model probabilities:
HTFLD(n) =
?
j 6?C(n)
hTFL(j) ? hD(j) (1)
Using these heuristic functions we have the over-
head of performing this rest cost estimation for
every coverage set in search. The experiments
will show that these additional costs are over-
compensated by the gain in reducing the search
space that has to be expanded during the A*
search.
To assess the predictive power of the vari-
ous components in the heuristic, we compare the
value of the heuristic function of the empty hy-
pothesis with the score of the optimal transla-
tion. A heuristic function is better if the dif-
ference between these two values is small. Ta-
ble 1 contains a comparison of various heuristic
functions. We compare the average costs (nega-
tive logarithm of the probabilities) of the optimal
translation and the average of the estimated costs
of the empty hypothesis. Typically, the estimated
costs of TFLD and the real costs differ by factor
3.
Table 1: Predictive power of admissible and almost admissible heuristic functions.
sentence HF for initial node empirical goal node
length T TF TFL TFLD score score
6 5.1 7.2 12.7 13.0 25.9 35.5
8 5.7 8.2 16.0 16.3 29.8 43.7
10 8.1 11.6 19.4 19.7 36.5 55.8
12 9.5 13.7 20.7 21.1 43.9 63.4
We will see later in Section 6 that the guar-
anteed admissible heuristic functions described
above result in dramatically more efficient search.
5 Empirical heuristic functions
In this section we describe a new method to ob-
tain an almost admissible heuristic function by a
multi pass search. This yields a significantly more
efficient search than using the admissible heuris-
tic functions. Thus, we lose the strict guarantee to
avoid search errors, but obtain a significant time
gain.
The idea of an empirical heuristic function
is to perform a multi-pass search. In the first
pass a good admissible heuristic function (here:
HTFLD) is used. If this search does not need too
much memory the search process is finished. If
the search failed, it is restarted using an improved
heuristic function which had been obtained during
the initial search process. This heuristic function
is computed such that it has the property that it
is admissible with respect to the explored search
space. That means, the heuristic function is op-
timistic with respect to every node in the search
space explored in the first pass.
Specifically, during the first pass, we maintain
a two-dimensional matrix hE(j, j?) with (J +2) ?
(J + 2) entries which are all initialized with ?.
The entry hE(j, j?) is the best score that was com-
puted for translating the source language word in
position j? if the previously covered source sen-
tence position is j. The matrix entry is updated
for every extension of a node n ? n?:
hE(j(n), j(n?)) :=
= max
{
hE(j(n), j(n?)), p(n, n?)
}
Here, p(n, n?) is the probability of the extension
n ? n?. hE(0, j) is the empirical score of start-
ing a sentence by covering the j-th source sen-
tence position first. Likewise, hE(j, J + 1) is the
empirical score of finishing a sentence with j as
the last source sentence position that was covered.
This yields
hE(j) = max
j? 6?C(n)?j?=J+1
hE(j, j?) .
In this calculation of hE(j), we maximize over
the columns of a matrix. The translation of the
source sentence can be viewed as a Traveling
Salesman Problem where the source sentence po-
sitions are the cities that have to be visited. Thus,
the maximization over the columns is equivalent
to assuring that the position j will be left after
the visit. We design an improved heuristic func-
tion using the following principle (Aigner, 1993):
Each city has to be both reached and left. There-
fore, in order to take an upper bound of reaching
a city into account, we divide each column of the
matrix by its maximum and maximize over the
rows of the matrix (Aigner, 1993):
hE+(j) = max
j? 6?C(n)?j?=j(n)
hE(j?, j)/hE(j?) .
We obtain the following empirical heuristic func-
tions:
HE(n) =
?
j 6?C(n)?j=j(n)
hE(j)
HE+(n) =
=
?
j 6?C(n)?j=j(n)
hE(j) ?
?
j? 6?C(n)?j?=J+1
hE+(j?)
If the search fails in the first pass due to the re-
striction of the number of hypotheses ? which was
1 million in all experiments ? the search can be
started again using HE+(n) as a heuristic. To
avoid an overestimation of the actual costs, we
multiply the empirical costs by a factor lower than
Table 3: Training corpus statistics (* without
punctuation marks).
French English
sentences 49000 49000
words 743903 816964
words* 664058 730880
average sentence length 16.9 14.6
vocabulary size 19831 24892
Table 4: Test corpora statistics.
Corpus # Sentences # Words
F E
T6 50 300 329
T8 50 400 403
T10 50 500 509
T12 50 600 601
T14 50 700 644
1. We found in our experiments that a factor of
0.7 is sufficient. The search was restarted up to 4
times if it failed. Using this method, it is possi-
ble to translate sentences that are longer than 10
words with a restriction to 1 million hypotheses.
Table 1 shows the value of the empirical heuris-
tic function of the empty node compared to the
score of the optimal goal node. The estimated
costs and the real costs now differ only by a fac-
tor of 1.5 instead of a factor of 3 for the TFLD
heuristic function before.
6 Results
We present results on the HANSARDS task which
consists of proceedings of the Canadian parlia-
ment that are kept both in French and in English.
Table 3 shows the details of our training corpus.
We used different the test corpora with sentences
of length 6-14 words (Table 4).
In all experiments, we use the following two
error criteria:
? WER (word error rate):
The WER is computed as the minimum
number of substitution, insertion and dele-
tion operations that have to be performed to
convert the generated string into the target
string.
? PER (position independent word error rate):
The word order of a French/English sentence
pair can be quite different. As a result, the
word order of the automatically generated
target sentence can be different from that of
the given target sentence, but nevertheless
acceptable so that the WER measure alone
could be misleading. In order to overcome
this problem, we introduce the position inde-
pendent word error rate (PER) as additional
measure. This measure compares the words
in the two sentences without taking the word
order into account.
In the following experiments we restricted the
maximum number of active search hypotheses in
A* search to 1 million. Every hypothesis has an
effective memory requirement of about 100 Byte.
Therefore, we obtain a dynamic memory require-
ment of about 100 MByte.
In order to speed up the search, we restricted
the reordering of words in IBM-style (Berger et
al., 1996; Tillmann, 2001). According to this re-
striction, up to 3 source sentence positions may be
skipped and translated later, i. e. during the search
process there may be up to 3 uncovered positions
left of the rightmost covered position in the source
sentence. The word error rate does not increase
compared to a non-restricted reordering, but the
search becomes much more efficient.
Table 5 shows how many sentences with differ-
ent sentence lengths can be translated using beam
search and A* with various heuristic functions.
Obviously, the BS approach is able to translate
any sentence length, therefore the search success
rate is 100%. Without any heuristic function A*
is only able to translate all 8-word sentences (with
the restriction of a maximum number of 1 million
hypotheses). Using more sophisticated heuristic
functions we are also able to translate all 10-word
sentences with A*.
Table 6 compares the search errors of A* and
BS. During the BS search, translation pruning
is carried out. The different hypotheses are dis-
tinguished according to the set of covered posi-
tions of the source sentence. For every set, the
best score of all hypotheses is computed. Only
those hypotheses are kept whose score is greater
than this best score multiplied with a threshold.
We chose the threshold to be 2.5, 5.0, 7.5 and 10.0
(see Table 6).
Table 2: Effect of observation pruning on the translation quality (average over all test sets).
# inverse 10 12 14 16 18 20
translations
WER 73.81 73.33 75.50 76.23 76.19 76.59
PER 68.02 66.93 70.07 71.16 71.24 71.16
Table 5: Search Success Rate (1 million hypothe-
ses) [%].
sentence length 6 8 10 12
BS 100 100 100 100
A*: no 100 100 86 12
T 100 100 88 20
TF 100 100 88 22
TFL 100 100 92 36
TFLD 100 100 92 36
E 100 100 100 74
E+ 100 100 100 84
Table 6: Search errors [%].
sentence length 6 8 10 12 14
BS 2.5 26 28 38 50 38
5.0 2 0 2 6 4
7.5 0 0 0 4 2
10.0 0 0 0 4 2
A* 0 0 0 0 0
For A* we never observe any search errors. In
the case of the admissible heuristic functions, this
is guaranteed by the approach. As can be seen
from Table 6, the BS algorithm with a large beam
rarely produces search errors.
Table 7 compares the translation efficiency of
the various search algorithms. We see that beam
search even with a very large beam producing
only very few search errors is much more efficient
than the used A* search algorithm.
Table 8 contains an assessment of translation
quality comparison of A* and BS using the T6,
T8, T10, T12-test corpus. For A*, we use the E+
rest cost estimation as this gives optimal results.
From the 200 sentences of these test corpora we
can translate 192 sentences using the 1 million hy-
potheses constraint. For the remaining sentences
we performed a search with 4 million hypotheses
Table 7: Average search time [s] per sentence.
sentence
length 6 8 10 12
BS: 2.5 0.06 0.18 0.60 1.16
5.0 0.24 0.84 2.90 6.48
7.5 0.50 2.14 7.06 16.26
10.0 0.78 3.30 11.86 26.42
A*: E+ 1.58 13.04 100 394
(cf. below) which lead to a success for all the 12-
word sentences.
The number of hypotheses in A* search
We restricted the maximal number of hypotheses
to 1 million. This was sufficient for translating
10-word sentences, as the search algorithm suc-
cess rate in Table 5 shows. For longer sentences
it is necessary to allow for a larger number of hy-
potheses. For the sentences of lengths 12 and 14,
we performed an A* search (E+) with 2, 4 and
8 million possible hypotheses. The search algo-
rithm success rate for those searches is contained
in Table 9. We see a significant effect on the num-
ber of successful searches.
7 Conclusion
We have developed sophisticated admissible and
almost admissible heuristic functions for statis-
tical machine translation. We have focussed on
Model 4, but most of the computations could
be easily extended to other statistical alignment
models (like HMM or Model 5). We especially
have observed the following effects:
? The heuristic function has a strong effect on
the efficiency of the A* search. Without any
heuristic function only 75 % of the test cor-
pus sentences can be translated (using the
1 million hypotheses constraint). Using the
Table 8: Translation quality.
BS (2.5) BS (5.0) BS (7.5) BS (10.0) A* (E+)
WER 69.65 68.78 68.68 68.68 68.68
PER 62.65 61.62 61.51 61.51 61.45
Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].
# hypotheses 1 million 2 million 4 million 8 million
12 42 80 100 100
14 2 20 70 100
best admissible heuristic function TFLD
we can translate 82 %.
? Using the empirical heuristic function we
can translate 96 % of the sentences with
A* search. This heuristic function does not
guarantee to avoid search errors, but this
case never occurred in our experiments.
From these results we conclude that it is often
possible to faster compute acceptable results us-
ing a beam search approach. Therefore, this is
the method of choice in practice. From a theo-
retical viewpoint it is interesting that using A* it
is possible to translate guaranteed without search
errors. In addition, without having a chance to
perform search without search errors it is almost
impossible to assess if errors in translation should
be assigned to the model/training or to the search
heuristics. Therefore, the A* algorithm is espe-
cially useful during the development of a statisti-
cal machine translation system.
Acknowledgment
This paper is based on work supported partly
by the VERBMOBIL project (contract number
01 IV 701 T4) by the German Federal Min-
istry of Education, Science, Research and Tech-
nology. In addition, this work was supported
by the National Science Foundation under Grant
No. IIS-9820687 through the 1999 Workshop on
Language Engineering, Center for Language and
Speech Processing, Johns Hopkins University.
References
M. Aigner. 1993. Diskrete Mathematik. Verlag Vieweg,
Braunschweig/Wiesbaden, Germany.
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical ma-
chine translation, final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/projects/
mt/final report/mt-final-report.ps.
A. L. Berger, S. A. Della Pietra P. F. Brown, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language translation apparatus and method of us-
ing context-based translation models. In United States
Patent, number 5510981. April.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
N. Nilsson. 1971. Problem-Solving Methods in Artificial
Intelligence. McGraw-Hill, McGraw-Hill, New York.
F. J. Och and H. Ney. 2000a. A comparison of alignment
models for statistical machine translation. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1086?1090, Saarbru?cken, Germany, August.
F. J. Och and H. Ney. 2000b. Improved statistical alignment
models. In Proc. of the 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 440?447,
Hongkong, China, October.
C. Tillmann and H. Ney. 2000. Word re-ordering and DP-
based search in statistical machine translation. In COL-
ING ?00: The 18th Int. Conf. on Computational Linguis-
tics, pages 850?856, Saarbru?cken, Germany, August.
C. Tillmann. 2001. Word Re-Ordering and Dynamic Pro-
gramming based Search Algorithms for Statistical Ma-
chine Translation. Ph.D. thesis, RWTH Aachen, Ger-
many, May.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING ?96: The
16th Int. Conf. on Computational Linguistics, pages 836?
841, Copenhagen, August.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in
statistical translation. In Proc. 35th Annual Conf. of the
Association for Computational Linguistics, pages 366?
372, Madrid, Spain, July.
 	
	 	 	ffMaximum Entropy Models for Named Entity Recognition
Oliver Bender
 
and Franz Josef Och

and Hermann Ney
 
 
Lehrstuhl fu?r Informatik VI

Information Sciences Institute
Computer Science Department University of Southern California
RWTH Aachen - University of Technology Marina del Rey, CA 90292
D-52056 Aachen, Germany och@isi.edu

bender,ney  @cs.rwth-aachen.de
Abstract
In this paper, we describe a system that applies
maximum entropy (ME) models to the task of
named entity recognition (NER). Starting with
an annotated corpus and a set of features which
are easily obtainable for almost any language,
we first build a baseline NE recognizer which
is then used to extract the named entities and
their context information from additional non-
annotated data. In turn, these lists are incor-
porated into the final recognizer to further im-
prove the recognition accuracy.
1 Introduction
In this paper, we present an approach for extracting the
named entities (NE) of natural language inputs which
uses the maximum entropy (ME) framework (Berger et
al., 1996). The objective can be described as follows.
Given a natural input sequence 	
    

we
choose the NE tag sequence 
 






with the
highest probability among all possible tag sequences:



  ffDo We Need Chinese Word Segmentation
for Statistical Machine Translation?
Jia Xu and Richard Zens and Hermann Ney
Chair of Computer Science VI
Computer Science Department
RWTH Aachen University, Germany
{xujia,zens,ney}@cs.rwth-aachen.de
Abstract
In Chinese texts, words are not separated by
white spaces. This is problematic for many nat-
ural language processing tasks. The standard
approach is to segment the Chinese character
sequence into words. Here, we investigate Chi-
nese word segmentation for statistical machine
translation. We pursue two goals: the first one
is the maximization of the final translation qual-
ity; the second is the minimization of the man-
ual effort for building a translation system.
The commonly used method for getting the
word boundaries is based on a word segmenta-
tion tool and a predefined monolingual dictio-
nary. To avoid the dependence of the trans-
lation system on an external dictionary, we
have developed a system that learns a domain-
specific dictionary from the parallel training
corpus. This method produces results that are
comparable with the predefined dictionary.
Further more, our translation system is able
to work without word segmentation with only a
minor loss in translation quality.
1 Introduction
In Chinese texts, words composed of single or
multiple characters, are not separated by white
spaces, which is different from most of the west-
ern languages. This is problematic for many
natural language processing tasks. Therefore,
the usual method is to segment a Chinese char-
acter sequence into Chinese ?words?.
Many investigations have been performed
concerning Chinese word segmentation. For
example, (Palmer, 1997) developed a Chinese
word segmenter using a manually segmented
corpus. The segmentation rules were learned
automatically from this corpus. (Sproat and
Shih, 1990) and (Sun et al, 1998) used a
method that does not rely on a dictionary or a
manually segmented corpus. The characters of
the unsegmented Chinese text are grouped into
pairs with the highest value of mutual informa-
tion. This mutual information can be learned
from an unsegmented Chinese corpus.
We will present a new method for segment-
ing the Chinese text without using a manually
segmented corpus or a predefined dictionary. In
statistical machine translation, we have a bilin-
gual corpus available, which is used to obtain
a segmentation of the Chinese text in the fol-
lowing way. First, we train the statistical trans-
lation models with the unsegmented bilingual
corpus. As a result, we obtain a mapping of
Chinese characters to the corresponding English
words for each sentence pair. By using this map-
ping, we can extract a dictionary automatically.
With this self-learned dictionary, we use a seg-
mentation tool to obtain a segmented Chinese
text. Finally, we retrain our translation system
with the segmented corpus.
Additionally, we have performed experiments
without explicit word segmentation. In this
case, each Chinese character is interpreted as
one ?word?. Based on word groups, our ma-
chine translation system is able to work without
a word segmentation, while having only a minor
translation quality relative loss of less than 5%.
2 Review of the Baseline System for
Statistical Machine Translation
2.1 Principle
In statistical machine translation, we are given
a source language (?French?) sentence fJ1 =
f1 . . . fj . . . fJ , which is to be translated into
a target language (?English?) sentence eI1 =
e1 . . . ei . . . eI . Among all possible target lan-
guage sentences, we will choose the sentence
with the highest probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
} (1)
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
The decomposition into two knowledge sources
in Equation 2 is known as the source-channel
approach to statistical machine translation
(Brown et al, 1990). It allows an independent
modeling of target language model Pr(eI1) and
translation model Pr(fJ1 |eI1)1. The target lan-
guage model describes the well-formedness of
the target language sentence. The translation
model links the source language sentence to the
target language sentence. The argmax opera-
tion denotes the search problem, i.e. the gener-
ation of the output sentence in the target lan-
guage. We have to maximize over all possible
target language sentences.
The resulting architecture for the statistical
machine translation approach is shown in Fig-
ure 1 with the translation model further decom-
posed into lexicon and alignment model.
Source Language Text
Transformation
 Lexicon Model
Language Model
Global Search:
 
 
Target Language Text
 
over
 
 Pr(f1  J  |e1I )
 
 
 Pr(   e1I )
 
 
 Pr(f1  J  |e1I )   Pr(   e1I )
  
e1I
f1 J
maximize  Alignment Model
Transformation
Figure 1: Architecture of the translation ap-
proach based on Bayes decision rule.
2.2 Alignment Models
The alignment model Pr(fJ1 , aJ1 |eI1) introduces
a ?hidden? alignment a = aJ1 , which describes
1The notational convention will be as follows: we use
the symbol Pr(?) to denote general probability distri-
butions with (nearly) no specific assumptions. In con-
trast, for model-based probability distributions, we use
the generic symbol p(?).
a mapping from a source position j to a target
position aj . The relationship between the trans-
lation model and the alignment model is given
by:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1) (3)
In this paper, we use the models IBM-1, IBM-
4 from (Brown et al, 1993) and the Hidden-
Markov alignment model (HMM) from (Vogel et
al., 1996). All these models provide different de-
compositions of the probability Pr(fJ1 , aJ1 |eI1).
A detailed description of these models can be
found in (Och and Ney, 2003).
A Viterbi alignment a?J1 of a specific model is
an alignment for which the following equation
holds:
a?J1 = argmax
aJ1
Pr(fJ1 , aJ1 |eI1). (4)
The alignment models are trained on a bilin-
gual corpus using GIZA++(Och et al, 1999;
Och and Ney, 2003). The training is done it-
eratively in succession on the same data, where
the final parameter estimates of a simpler model
serve as starting point for a more complex
model. The result of the training procedure is
the Viterbi alignment of the final training iter-
ation for the whole training corpus.
2.3 Alignment Template Approach
In the translation approach from Section 2.1,
one disadvantage is that the contextual informa-
tion is only taken into account by the language
model. The single-word based lexicon model
does not consider the surrounding words. One
way to incorporate the context into the trans-
lation model is to learn translations for whole
word groups instead of single words. The key
elements of this translation approach (Och et
al., 1999) are the alignment templates. These
are pairs of source and target language phrases
with an alignment within the phrases.
The alignment templates are extracted from
the bilingual training corpus. The extraction al-
gorithm (Och et al, 1999) uses the word align-
ment information obtained from the models in
Section 2.2. Figure 2 shows an example of a
word aligned sentence pair. The word align-
ment is represented with the black boxes. The
figure also includes some of the possible align-
ment templates, represented as the larger, un-
filled rectangles. Note that the extraction algo-
rithm would extract many more alignment tem-
plates from this sentence pair. In this example,
the system input was the sequence of Chinese
characters without any word segmentation. As
can be seen, a translation approach that is based
on phrases circumvents the problem of word seg-
mentation to a certain degree. This method will
be referred to as ?translation with no segmen-
tation? (see Section 5.2).
they
will
also
go
to
hangzhou
for
a
visit
Figure 2: Example of a word aligned sentence
pair and some possible alignment templates.
In the Chinese?English DARPA TIDES eval-
uations in June 2002 and May 2003, carried out
by NIST (NIST, 2003), the alignment template
approach performed very well and was ranked
among the best translation systems.
Further details on the alignment template ap-
proach are described in (Och et al, 1999; Och
and Ney, 2002).
3 Task and Corpus Statistics
In Section 5.3, we will present results for a
Chinese?English translation task. The domain
of this task is news articles. As bilingual train-
ing data, we use a corpus composed of the En-
glish translations of a Chinese Treebank. This
corpus is provided by the Linguistic Data Con-
sortium (LDC), catalog number LDC2002E17.
In addition, we use a bilingual dictionary with
10K Chinese word entries provided by Stephan
Vogel (LDC, 2003b).
Table 1 shows the corpus statistics of this
task. We have calculated both the number of
words and the number of characters in the cor-
pus. In average, a Chinese word is composed
of 1.49 characters. For each of the two lan-
guages, there is a set of 20 special characters,
such as digits, punctuation marks and symbols
like ?()%$...?
The training corpus will be used to train a
word alignment and then extract the alignment
templates and the word-based lexicon. The re-
sulting translation system will be evaluated on
the test corpus.
Table 1: Statistics of training and test corpus.
For each of the two languages, there is a set of 20
special characters, such as digits, punctuation
marks and symbols like ?()%$...?
Chinese English
Train Sentences 4 172
Characters 172 874 832 760
Words 116 090 145 422
Char. Vocab. 3 419 + 20 26 + 20
Word Vocab. 9 391 9 505
Test Sentences 993
Characters 42 100 167 101
Words 28 247 26 225
4 Segmentation Methods
4.1 Conventional Method
The commonly used segmentation method is
based on a segmentation tool and a monolingual
Chinese dictionary. Typically, this dictionary
has been produced beforehand and is indepen-
dent of the Chinese text to be segmented. The
dictionary contains Chinese words and their fre-
quencies. This information is used by the seg-
mentation tool to find the word boundaries. In
the LDC method (see Section 5.2) we have used
the dictionary and segmenter provided by the
LDC. More details can be found on the LDC
web pages (LDC, 2003a). This segmenter is
based on two ideas: it prefers long words over
short words and it prefers high frequency words
over low frequency words.
4.2 Dictionary Learning from Alignments
In this section, we will describe our method of
learning a dictionary from a bilingual corpus.
As mentioned before, the bilingual training
corpus listed in Section 3 is the only input to the
system. We firstly divide every Chinese charac-
ters in the corpus by white spaces, then train
the statistical translation models with this un-
segmented Chinese text and its English trans-
lation, details of the training method are de-
scribed in Section 2.2.
To extract Chinese words instead of phrases
as in Figure 2, we configure the training pa-
rameters in GIZA++, the alignment is then re-
stricted to a multi-source-single-target relation-
ship, i.e. one or more Chinese characters are
translated to one English word.
The result of this training procedure is an
alignment for each sentence pair. Such an align-
ment is represented as a binary matrix with J ?I
elements.
An example is shown in Figure 3. The un-
segmented Chinese training sentence is plotted
along the horizontal axes and the corresponding
English sentence along the vertical axes. The
black boxes show the Viterbi alignment for this
sentence pair. Here, for example the first two
Chinese characters are aligned to ?industry?,
the next four characters are aligned to ?restruc-
turing?.
industry
restructuring
made
vigorous
progress
Figure 3: Example of an alignment without
word segmentation.
The central idea of our dictionary learning
method is: a contiguous sequence of Chinese
characters constitute a Chinese word, if they
are aligned to the same English word. Using
this idea and the bilingual corpus, we can au-
tomatically generate a Chinese word dictionary.
Table 2 shows the Chinese words that are ex-
tracted from the alignment in Figure 3.
Table 2: Word entries in Chinese dictionary
learned from the alignment in Figure 3.
We extract Chinese words from all sentence
pairs in the training corpus. Therefore, it is
straightforward to collect word frequency statis-
tics that are needed for the segmentation tool.
Once, we have generated the dictionary, we can
produce a segmented Chinese corpus using the
method described in Section 4.1. Then, we
retrain the translation system using the seg-
mented Chinese text.
4.3 Word Length Statistics
In this section, we present statistics of the word
lengths in the LDC dictionary as well as in the
self-learned dictionary extracted from the align-
ment.
Table 3 shows the statistics of the word
lengths in the LDC dictionary as well as in
the learned dictionary. For example, there are
2 368 words consisting of a single character in
learned dictionary and 2 511 words in the LDC
dictionary. These single character words rep-
resent 16.9% of the total number of entries in
the learned dictionary and 18.6% in the LDC
dictionary.
We see that in the LDC dictionary more than
65% of the words consist of two characters and
about 30% of the words consist of a single char-
acter or three or four characters. Longer words
with more than four characters constitute less
than 1% of the dictionary. In the learned dic-
tionary, there are many more long words, about
15%. A subjective analysis showed that many
of these entries are either named entities or
idiomatic expressions. Often, these idiomatic
expressions should be segmented into shorter
words. Therefore, we will investigate methods
to overcome this problem in the future. Some
suggestions will be discussed in Section 6.
Table 3: Statistics of word lengths in the LDC
dictionary and in the learned dictionary.
word LDC dictionary learned dictionary
length frequency [%] frequency [%]
1 2 334 18.6 2 368 16.9
2 8 149 65.1 5 486 39.2
3 1 188 9.5 1 899 13.6
4 759 6.1 2 084 14.9
5 70 0.6 791 5.7
6 20 0.2 617 4.4
7 6 0.0 327 2.3
?8 11 0.0 424 3.0
total 12 527 100 13 996 100
5 Translation Experiments
5.1 Evaluation Criteria
So far, in machine translation research, a sin-
gle generally accepted criterion for the evalu-
ation of the experimental results does not ex-
ist. We have used three automatic criteria. For
the test corpus, we have four references avail-
able. Hence, we compute all the following cri-
teria with respect to multiple references.
? WER (word error rate):
The WER is computed as the minimum
number of substitution, insertion and dele-
tion operations that have to be performed
to convert the generated sentence into the
reference sentence.
? PER (position-independent word error
rate):
A shortcoming of the WER is that it re-
quires a perfect word order. The word or-
der of an acceptable sentence can be dif-
ferent from that of the target sentence, so
that the WER measure alone could be mis-
leading. The PER compares the words in
the two sentences ignoring the word order.
? BLEU score:
This score measures the precision of un-
igrams, bigrams, trigrams and fourgrams
with respect to a reference translation with
a penalty for too short sentences (Papineni
et al, 2001). The BLEU score measures
accuracy, i.e. large BLEU scores are bet-
ter.
5.2 Summary: Three Translation
Methods
In the experiments, we compare the following
three translation methods:
? Translation with no segmentation: Each
Chinese character is interpreted as a single
word.
? Translation with learned segmentation:
It uses the self-learned dictionary.
? Translation with LDC segmentation:
The predefined LDC dictionary is used.
The core contribution of this paper is the
method we called ?translation with learned seg-
mentation?, which consists of three steps:
? The input is a sequence of Chinese charac-
ters without segmentation. After the train-
ing using GIZA++, we extract a mono-
lingual Chinese dictionary from the align-
ment. This is discussed in Section 4.2, and
an example is given in Figure 3 and Table 2.
? Using this learned dictionary, we segment
the sequence of Chinese characters into
words. In other words, the LDC method
is used, but the LDC dictionary is replaced
by the learned dictionary (see Section 4.1).
? Based on this word segmentation, we
perform another training using GIZA++.
Then, after training the models IBM1,
HMM and IBM4, we extract bilingual word
groups, which are referred as alignment
templates.
5.3 Evaluation Results
The evaluation is performed on the LDC corpus
described in Section 3. The translation perfor-
mance of the three systems is summarized in
Table 4 for the three evaluation criteria WER,
PER and BLEU. We observe that the trans-
lation quality with the learned segmentation is
similar to that with the LDC segmentation. The
WER of the system with the learned segmenta-
tion is somewhat better, but PER and BLEU
are slightly worse. We conclude that it is possi-
ble to learn a domain-specific dictionary for Chi-
nese word segmentation from a bilingual corpus.
Therefore the translation system is independent
of a predefined dictionary, which may be unsuit-
able for a certain task.
The translation system using no segmenta-
tion performs slightly worse. For example, for
the WER there is a loss of about 2% relative
compared to the system with the LDC segmen-
tation.
Table 4: Translation performance of different
segmentation methods (all numbers in percent).
method error rates accuracy
WER PER BLEU
no segment. 73.3 56.5 27.6
learned segment. 70.4 54.6 29.1
LDC segment. 71.9 54.4 29.2
5.4 Effect of Segmentation on
Translation Results
In this section, we present three examples of the
effect that segmentation may have on transla-
tion quality. For each of the three examples in
Figure 4, we show the segmented Chinese source
sentence using either the LDC dictionary or the
self-learned dictionary, the corresponding trans-
lation and the human reference translation.
In the first example, the LDC dictionary
leads to a correct segmentation, whereas with
the learned dictionary the segmentation is erro-
neous. The second and third token should be
combined (?Hong Kong?), whereas the fifth to-
ken should be separated (?stabilize in the long
term?). In this case, the wrong segmentation of
the Chinese source sentence does not result in a
wrong translation. A possible reason is that the
translation system is based on word groups and
can recover from these segmentation errors.
In the second example, the segmentation with
the LDC dictionary produces at least one error.
The second and third token should be combined
(?this?). It is possible to combine the seventh
and eighth token to a single word because the
eighth token shows only the tense. The segmen-
tation with the learned dictionary is correct.
Here, the two segmentations result in different
translations.
In the third example, both segmentations are
incorrect and these segmentation errors affect
the translation results. In the segmentation
with the LDC dictionary, the first Chinese char-
acters should be segmented as a separate word.
The second and third character and maybe even
the fourth character should be combined to one
word.2 The fifth and sixth character should be
combined to a single word. In the segmentation
with the learned dictionary, the fifth and sixth
token (seventh and eighth character) should be
combined (?isolated?). We see that this term is
missing in the translation. Here, the segmenta-
tion errors result in translation errors.
6 Discussion and Future Work
We have presented a new method for Chinese
word segmentation. It avoids the use of a pre-
defined dictionary and instead learns a corpus-
specific dictionary from the bilingual training
corpus.
The idea is extracting a self-learned dictio-
nary from the trained alignment models. This
method has the advantage that the word entries
in the dictionary all occur in the training data,
and its content is much closer to the training
text as a predefined dictionary, which can never
cover all possible word occurrences. Here, if the
content of the test corpus is closer to that of the
2This is an example of an ambiguous segmentation.
Example 1
LDC dictionary:
                              
It will benefit Hong Kong's economy to prosper
and stabilize in the long term.
Learned dictionary:
                          
It will benefit Hong Kong's economy to prosper
and stabilize in the long term.
Reference:
It will be benificial for the stability and
prosperity of Hong Kong in the long run.
Example 2
LDC dictionary:
                               
         
but this meeting down or achieved certain
progress.
Learned dictionary:
                             
    
however, this meeting straight down still
achieved certain progress.
Reference:
Neverless, this meeting has achieved some
progress.
Example 3
LDC dictionary:
...                                 
 ...
... the unification of the world carried adjacent
isolate of proof, ...
Learned dictionary:
...                                
 ...
... in the world faced with a became another
proof, ...
Reference:
... another proof that ... is facing isolation in the
world ...
Figure 4: Translation examples using the
learned dictionary and the LDC dictionary.
training corpus, the quality of the dictionary is
higher and the translation performance would
be better.
The experiments showed that the transla-
tion quality with the learned segmentation is
competitive with the LDC segmentation. Ad-
ditionally, we have shown the feasibility of a
Chinese?English statistical machine translation
system that works without any word segmenta-
tion. There is only a minor loss in translation
performance. Further improvements could be
possible by tuning the system toward this spe-
cific task.
We expect that our method could be im-
proved by considering the word length as dis-
cussed in Section 4.3. As shown in the word
length statistics, long words with more than
four characters occur only occasionally. Most of
them are named entity words, which are writ-
ten in English in upper case. Therefore, we can
apply a simple rule: we accept a long Chinese
word only if the corresponding English word is
in upper case. This should result in an improved
dictionary. An alternative way is to use the
word length statistics in Table 3 as a prior dis-
tribution. In this case, long words would get a
penalty, because their prior probability is low.
Because the extraction of our dictionary is
based on bilingual information, it might be in-
teresting to combine it with methods that use
monolingual information only.
For Chinese?English, there is a large num-
ber of bilingual corpora available at the LDC.
Therefore using additional corpora, we can ex-
pect to get an improved dictionary.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J.
Della Pietra, F. Jelinek, J. D. Lafferty, R. L.
Mercer, and P. S. Roossin. 1990. A statisti-
cal approach to machine translation. Compu-
tational Linguistics, 16(2):79?85, June.
P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: Pa-
rameter estimation. Computational Linguis-
tics, 19(2):263?311, June.
LDC. 2003a. LDC Chinese resources home
page. http://www.ldc.upenn.edu/Projects/
Chinese/LDC ch.htm.
LDC. 2003b. LDC resources home page.
http://www.ldc.upenn.edu/Projects/TIDES/
mt2004cn.htm.
NIST. 2003. Machine translation home page.
http://www.nist.gov/speech/tests/mt/
index.htm.
F. J. Och and H. Ney. 2002. Discriminative
training and maximum entropy models for
statistical machine translation. In Proc. of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51,
March.
F. J. Och, C. Tillmann, and H. Ney. 1999. Im-
proved alignment models for statistical ma-
chine translation. In Proc. of the Joint SIG-
DAT Conf. on Empirical Methods in Natu-
ral Language Processing and Very Large Cor-
pora, pages 20?28, University of Maryland,
College Park, MD, June.
D. D. Palmer. 1997. A trainable rule-based
algorithm for word segmentation. In Proc.
of the 35th Annual Meeting of ACL and 8th
Conference of the European Chapter of ACL,
pages 321?328, Madrid, Spain, August.
K. A. Papineni, S. Roukos, T. Ward, and W. J.
Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Techni-
cal Report RC22176 (W0109-022), IBM Re-
search Division, Thomas J. Watson Research
Center, September.
R. W. Sproat and C. Shih. 1990. A statistical
method for finding word boundaries in Chi-
nese text. Computer Processing of Chinese
and Oriental Languages, 4:336?351.
M. Sun, D. Shen, and B. K. Tsou. 1998. Chi-
nese word segmentation without using lexi-
con and hand-crafted training data. In Proc.
of the 36th Annual Meeting of ACL and
17th Int. Conf. on Computational Linguistics
(COLING-ACL 98), pages 1265?1271, Mon-
treal, Quebec, Canada, August.
S. Vogel, H. Ney, and C. Tillmann. 1996.
HMM-based word alignment in statistical
translation. In COLING ?96: The 16th Int.
Conf. on Computational Linguistics, pages
836?841, Copenhagen, Denmark, August.
Error Measures and Bayes Decision Rules Revisited
with Applications to POS Tagging
Hermann Ney, Maja Popovic?, David Su?ndermann
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
Ahornstrasse 55
52056 Aachen, Germany
{popovic,ney}@informatik.rwth-aachen.de
Abstract
Starting from first principles, we re-visit the
statistical approach and study two forms of
the Bayes decision rule: the common rule for
minimizing the number of string errors and a
novel rule for minimizing the number of symbols
errors. The Bayes decision rule for minimizing
the number of string errors is widely used, e.g.
in speech recognition, POS tagging and machine
translation, but its justification is rarely questioned.
To minimize the number of symbol errors as is
more suitable for a task like POS tagging, we show
that another form of the Bayes decision rule can
be derived. The major purpose of this paper is to
show that the form of the Bayes decision rule should
not be taken for granted (as it is done in virtually
all statistical NLP work), but should be adapted
to the error measure being used. We present first
experimental results for POS tagging tasks.
1 Introduction
Meanwhile, the statistical approach to natural
language processing (NLP) tasks like speech
recognition, POS tagging and machine translation
has found widespread use. There are three
ingredients to any statistical approach to NLP,
namely the Bayes decision rule, the probability
models (like trigram model, HMM, ...) and the
training criterion (like maximum likelihood, mutual
information, ...).
The topic of this paper is to re-consider the form
of the Bayes decision rule. In virtually all NLP
tasks, the specific form of the Bayes decision rule
is never questioned, and the decision rule is adapted
from speech recognition. In speech recognition, the
typical decision rule is to maximize the sentence
probability over all possible sentences. However,
this decision rule is optimal for the sentence error
rate and not for the word error rate. This difference
is rarely studied in the literature.
As a specific NLP task, we will consider part-
of-speech (POS) tagging. However, the problem
addressed comes up in any NLP task which is
tackled by the statistical approach and which makes
use of a Bayes decision rule. Other prominent
examples are speech recognition and machine
translation. The advantage of the POS tagging
task is that it will be easier to handle from the
mathematical point of view and will result in closed-
form solutions for the decision rules. From this
point-of-view, the POS tagging task serves as a
good opportunity to illustrate the key concepts of
the statistical approach to NLP.
Related Work: For the task of POS tagging,
statistical approaches were proposed already in the
60?s and 70?s (Stolz et al, 1965; Bahl and Mercer,
1976), before they started to find widespread use
in the 80?s (Beale, 1985; DeRose, 1989; Church,
1989).
To the best of our knowledge, the ?standard?
version of the Bayes decision rule, which minimizes
the number of string errors, is used in virtually all
approaches to POS tagging and other NLP tasks.
There are only two research groups that do not take
this type of decision rule for granted:
(Merialdo, 1994): In the context of POS tagging,
the author introduces a method that he calls
maximum likelihood tagging. The spirit of this
method is similar to that of this work. However, this
method is mentioned as an aside and its implications
for the Bayes decision rule and the statistical
approach are not addressed. Part of this work
goes back to (Bahl et al, 1974) who considered
a problem in coding theory.
(Goel and Byrne, 2003): The error measure
considered by the authors is the word error rate in
speech recognition, i.e. the edit distance. Due to
the mathematical complexity of this error measure,
the authors resort to numeric approximations
to compute the Bayes risk (see next section).
Since this approach does not results in explicit
closed-form equations and involves many numeric
approximations, it is not easy to draw conclusions
from this work.
2 Bayes Decision Rule for Minimum Error
Rate
2.1 The Bayes Posterior Risk
Knowing that any task in NLP tasks is a difficult
one, we want to keep the number of wrong
decisions as small as possible. This point-of-view
has been used already for more than 40 years in
pattern classification as the starting point for many
techniques in pattern classification. To classify an
observation vector y into one out of several classes
c, we resort to the so-called statistical decision
theory and try to minimize the average risk or loss
in taking a decision. The result is known as Bayes
decision rule (Chapter 2 in (Duda and Hart, 1973)):
y ? c? = argmin
c
{
?
c?
Pr(c|y) ? L[c, c?]
}
where L[c, c?] is the so-called loss function or error
measure, i.e. the loss we incur in making decision c
when the true class is c?.
In the following, we will consider two specific
forms of the loss function or error measure L[c, c?].
The first will be the measure for string errors,
which is the typical loss function used in virtually
all statistical approaches. The second is the
measure for symbol errors, which is the more
appropriate measure for POS tagging and also
speech recognition with no insertion and deletion
errors (such as isolated word recognition).
2.2 String Error
For POS tagging, the starting point is the observed
sequence of words y = wN1 = w1...wN , i.e. the
sequence of words for which the POS tag sequence
has c = gN1 = g1...gN has to be determined.
The first error measure we consider is the string
error: the error is equal to zero only if the POS
symbols of the two strings are identical at each
position. In this case, the loss function is:
L[gN1 , g?N1 ] = 1 ?
N
?
n=1
?(gn, g?n)
with the Kronecker delta ?(c, c?). In other words,
the errors are counted at the string level and not
at the level of single symbols. Inserting this cost
function into the Bayes risk (see Section 2.1), we
immediately obtain the following form of Bayes
decision rule for minimum string error:
wN1 ? g?N1 = argmax
gN1
{
Pr(gN1 |wN1 )
}
= argmax
gN1
{
Pr(gN1 , wN1 )
}
This is the starting point for virtually all statistical
approaches in NLP like speech recognition and
machine translation. However, this decision rule is
only optimal when we consider string errors, e.g.
sentence error rate in POS tagging and in speech
recognition. In practice, however, the empirical
errors are counted at the symbol level. Apart
from (Goel and Byrne, 2003), this inconsistency of
decision rule and error measure is never addressed
in the literature.
2.3 Symbol Error
Instead of the string error rate, we can also consider
the error rate of single POS tag symbols (Bahl et
al., 1974; Merialdo, 1994).
This error measure is defined by the loss function:
L[gN1 , g?N1 ] =
N
?
n=1
[1 ? ?(gn, g?n)]
This loss function has to be inserted into the Bayes
decision rule in Section 2.1. The computation of the
expected loss, i.e. the averaging over all classes c? =
g?N1 , can be performed in a closed form. We omit
the details of the straightforward calculations and
state only the result. It turns out that we will need
the marginal (and posterior) probability distribution
Prm(g|wN1 ) at positions m = 1, ..., N :
Prm(g|wN1 ) :=
?
gN1 : gm=g
Pr(gN1 |wN1 )
where the sum is carried out over all POS tag strings
gN1 with gm = g, i.e. the tag gm at position m is
fixed at gm = g. The question of how to perform
this summation efficiently will be considered later
after we have introduced the model distributions.
Thus we have obtained the Bayes decision rule
for minimum symbol error at position m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
Prm(g|wN1 )
}
= argmax
g
{
Prm(g,wN1 )
}
By construction this decision rule has the special
property that it does not put direct emphasis on
local coherency of the POS tags produced. In other
words, this decision rule may produce a POS tag
string which is linguistically less likely.
3 The Modelling Approaches to POS
Tagging
The derivation of the Bayes decision rule assumes
that the probability distribution Pr(gN1 , wN1 ) (or
Pr(gN1 |wN1 )) is known. Unfortunately, this is not
the case in practice. Therefore, the usual approach
is to approximate the true but unknown distribution
by a model distribution p(gN1 , wN1 ) (or p(gN1 |wN1 )).
We will review two popular modelling approaches,
namely the generative model and the direct model,
and consider the associated Bayes decision rules for
both minimum string error and minimum symbol
error.
3.1 Generative Model: Trigram Model
We replace the true but unknown joint distribution
Pr(gN1 , wN1 ) by a model-based probability distribu-
tion p(gN1 , wN1 ):
Pr(gN1 , wN1 ) ? p(gN1 , wN1 ) = p(gN1 ) ? p(wN1 |gN1 )
We apply the so-called chain rule to factorize each
of the distributions p(gN1 ) and p(wN1 |gN1 ) into a
product of conditional probabilities using specific
dependence assumptions:
p(gN1 , wN1 ) =
N
?
n=1
[
p(gn|gn?1n?2) ? p(wn|gn)
]
with suitable definitions for the case n = 1.
Here, the specific dependence assumptions are that
the conditional probabilities can be represented
by a POS trigram model p(gn|gn?1n?2) and a word
membership model p(wn|gn). Thus we obtain
a probability model whose structure fits into
the mathematical framework of so-called Hidden
Markov Model (HMM). Therefore, this approach is
often also referred to as HMM-based POS tagging.
However, this terminology is misleading: The POS
tag sequence is observable whereas in the Hidden
Markov Model the state sequence is always hidden
and cannot be observed. In the experiments, we will
use a 7-gram POS model. It is clear how to extend
the equations from the trigram case to the 7-gram
case.
3.1.1 String Error
Using the above model distribution, we directly
obtain the decision rule for minimum string error:
wN1 ? g?N1 = argmax
gN1
{
p(gN1 , wN1 )
}
Since the model distribution is a basically a second-
order model (or trigram model), there is an efficient
algorithm for finding the most probable POS tag
string. This is achieved by a suitable dynamic
programming algorithm, which is often referred to
as Viterbi algorithm in the literature.
3.1.2 Symbol Error
To apply the Bayes decision rule for minimum
symbol error rate, we first compute the marginal
probability pm(g,wN1 ):
pm(g,wN1 ) =
?
gN1 : gm=g
p(gN1 , wN1 )
=
?
gN1 : gm=g
?
n
[
p(gn|gn?1n?2) ? p(wn|gn)
]
Again, since the model is a second-order model,
the sum over all possible POS tag strings gN1(with gm = g) can be computed efficiently
using a suitable extension of the forward-backward
algorithm (Bahl et al, 1974).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
pm(g,wN1 )
}
Here, after the the marginal probability pm(g,wN1 )
has been computed, the task of finding the most
probable POS tag at position m is computationally
easy. Instead, the lion?s share for the computational
effort is required to compute the marginal probabil-
ity pm(g,wN1 ).
3.2 Direct Model: Maximum Entropy
We replace the true but unknown posterior distri-
bution Pr(gN1 |wN1 ) by a model-based probability
distribution p(gN1 |wN1 ):
Pr(gN1 |wN1 ) ? p(gN1 |wN1 )
and apply the chain rule:
p(gN1 |wN1 ) =
N
?
n=1
p(gn|gn?11 , wN1 )
=
N
?
n=1
p(gn|gn?1n?2 , wn+2n?2)
As for the generative model, we have made specific
assumptions: There is a second-order dependence
for the tags gn1 , and the dependence on the words
wN1 is limited to a window wn+2n?2 around position
n. The resulting model is still rather complex
and requires further specifications. The typical
procedure is to resort to log-linear modelling, which
is also referred to as maximum entropy modelling
(Ratnaparkhi, 1996; Berger et al, 1996).
3.2.1 String Error
For the minimum string error, we obtain the
decision rule:
wN1 ? g?N1 = argmax
gN1
{
p(gN1 |wN1 )
}
Since this is still a second-order model, we can use
dynamic programming to compute the most likely
POS string.
3.2.2 Symbol Error
For the minimum symbol error, the marginal
(and posterior) probability pm(g|wN1 ) has to be
computed:
pm(g|wN1 ) =
?
gN1 : gm=g
Pr(gN1 |wN1 )
=
?
gN1 : gm=g
?
n
p(gn|gn?1n?2 , wn+2n?2)
which, due to the specific structure of the model
p(gn|gn?1n?2 , wn+2n?2), can be calculated efficiently
using only a forward algorithm (without a
?backward? part).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N :
(wN1 ,m) ? g?m = argmaxg
{
pm(g|wN1 )
}
As in the case of the generative model, the
computational effort is to compute the posterior
probability pm(g|wN1 ) rather than to find the most
probable tag at position m.
4 The Training Procedure
So far, we have said nothing about how we train
the free parameters of the model distributions. We
use fairly conventional training procedures that we
mention only for the sake of completeness.
4.1 Generative Model
We consider the trigram-based model. The free
parameters here are the entries of the POS trigram
distribution p(g|g??, g?) and of the word membership
distribution p(w|g). These unknown parameters are
computed from a labelled training corpus, i.e. a
collection of sentences where for each word the
associated POS tag is given.
In principle, the free parameters of the models
are estimated as relative frequencies. For the test
data, we have to allow for both POS trigrams (or n-
grams) and (single) words that were not seen in the
training data. This problem is tackled by applying
smoothing methods that were originally designed
for language modelling in speech recognition (Ney
et al, 1997).
4.2 Direct Model
For the maximum entropy model, the free param-
eters are the so-called ?i or feature parameters
(Berger et al, 1996; Ratnaparkhi, 1996). The
training criterion is to optimize the logarithm
of the model probabilities p(gn|gn?2n?1 , wn+2n?2) over
all positions n in the training corpus. The
corresponding algorithm is referred to as GIS
algorithm (Berger et al, 1996). As usual
with maximum entropy models, the problem of
smoothing does not seem to be critical and is not
addressed explicitly.
5 Experimental Results
Of course, there have already been many papers
about POS tagging using statistical methods. The
goal of the experiments is to compare the two
decision rules and to analyze the differences in
performance. As the results for the WSJ corpus will
show, both the trigram method and the maximum
entropy method have an tagging error rate of 3.0%
to 3.5% and are thus comparable to the best results
reported in the literature, e.g. (Ratnaparkhi, 1996).
5.1 Task and Corpus
The experiments are performed on the Wall Street
Journal (WSJ) English corpus and on the Mu?nster
Tagging Project (MTP) German corpus.
The POS tagging part of The WSJ corpus
(Table 1) was compiled by the University of
Pennsylvania and consists of about one million
English words with manually annotated POS tags.
Text POS
Train Sentences 43508
Words+PMs 1061772
Singletons 21522 0
Word Vocabulary 46806 45
PM Vocabulary 25 9
Test Sentences 4478
Words+PMs 111220
OOVs 2879 0
Table 1: WSJ corpus statistics.
The MTP corpus (Table 2) was compiled at the
University of Mu?nster and contains tagged German
words from articles of the newspapers Die Zeit
and Frankfurter Allgemeine Zeitung (Kinscher and
Steiner, 1995).
For the corpus statistics, it is helpful to
distinguish between the true words and the
punctuation marks (see Table 1 and Table 2). This
distinction is made for both the text and the POS
corpus. In addition, the tables show the vocabulary
size (number of different tokens) for the words and
for the punctuation marks.
Punctuation marks (PMs) are all tokens which
do not contain letters or digits. The total number
of running tokens is indicated as Words+PMs.
Singletons are the tokens which occur only once in
Text POS
Train Sentences 19845
Words+PMs 349699
Singletons 32678 11
Word Vocabulary 51491 68
PM Vocabulary 27 5
Test Sentences 2206
Words+PMs 39052
OOVs 3584 2
Table 2: MTP corpus statistics.
the training data. Out-of-Vocabulary words (OOVs)
are the words in the test data that did not not occur
in the training corpus.
5.2 POS Tagging Results
The tagging experiments were performed for both
types of models, each of them with both types of
the decision rules. The generative model is based on
the approach described in (Su?ndermann and Ney,
2003). Here the optimal value of the n-gram order
is determined from the corpus statistics and has a
maximum of n = 7. The experiments for the direct
model were performed using the maximum entropy
tagger described in (Ratnaparkhi, 1996).
The tagging error rates are showed in Table 3 and
Table 4. In addition to the overall tagging error rate
(Overall), the tables show the tagging error rates for
the Out-of-Vocabulary words (OOVs) and for the
punctuation marks (PMs).
For the generative model, both decision rules
yield similar results. For the direct model, the
overall tagging error rate increases on each of the
two tasks (from 3.0 % to 3.3 % on WSJ and from
5.4 % to 5.6 % on MTP) when we use the symbol
decision rule instead of the string decision rule. In
particular, for OOVs, the error rate goes up clearly.
Right now, we do not have a clear explanation
for this difference between the generative model
and the direct model. It might be related to the
?forward? structure of the direct model as opposed to
the ?forward-backward? structure of the generative
model. Anyway, the refined bootstrap method
(Bisani and Ney, 2004) has shown that differences
in the overall tagging error rate are statistically not
significant.
5.3 Examples
A detailed analysis of the tagging results showed
that for both models there are sentences where the
one decision rule is more efficient and sentences
where the other decision rule is better.
For the generative model, these differences seem
to occur at random, but for the direct model, some
distinct tendencies can be observed. For example,
WSJ Task Decision Overall OOVs PMs
Rule
Generative string 3.5 16.9 0
Model symbol 3.5 16.7 0
Direct string 3.0 15.4 0.08
Model symbol 3.3 16.6 0.1
Table 3: POS tagging error rates [%] for WSJ task.
MTP Task Decision Overall OOVs PMs
Rule
Generative string 5.4 13.4 3.6
Model symbol 5.4 13.4 3.6
Direct string 5.4 12.7 3.8
Model symbol 5.6 13.4 3.7
Table 4: POS tagging error rates [%] for MTP task.
for the WSJ corpus, the string decision rule is
significantly better for the present and past tense of
verbs (VBP, VBN), and the symbol decision rule
is better for adverb (RB) and verb past participle
(VBN). Typical errors generated by the symbol
decision rule are tagging present tense as infinitive
(VB) and past tense as past participle (VBN), and
for string decision rule, adverbs are often tagged as
preposition (IN) or adjective (JJ) and past participle
as past tense (VBD).
For the German corpus, the string decision
rule better handles demonstrative determiners
(Rr) and subordinate conjunctions (Cs) whereas
symbol decision rule is better for definite articles
(Db). The symbol decision rule typically tags
the demonstrative determiner as definite article
(Db) and subordinate conjunctions as interrogative
adverbs (Bi), and the string decision rule tends to
assign the demonstrative determiner tag to definite
articles.
These typical errors for the symbol decision rule
are shown in Table 5, and for the string decision rule
in Table 6.
6 Conclusion
So far, the experimental tests have shown no
improvement when we use the Bayes decision rule
for minimizing the number of symbol errors rather
than the number of string errors. However, the
important result is that the new approach results in
comparable performance. More work is needed to
contrast the two approaches.
The main purpose of this paper has been to show
that, in addition to the widely used decision rule for
minimizing the string errors, it is possible to derive a
decision rule for minimizing the number of symbol
errors and to build up the associated mathematical
framework.
There are a number of open questions for future
work:
1) The error rates for the two decision rules are
comparable. Is that an experimental coincidence?
Are there situations for which we must expect a
significance difference between the two decision
rules? We speculate that the two decision rules
could always have similar performance if the error
rates are small.
2) Ideally, the training criterion should be closely
related to the error measure used in the decision
rule. Right now, we have used the training criteria
that had been developed in the past and that had
been (more or less) designed for the string error rate
as error measure. Can we come up with a training
criterion tailored to the symbol error rate?
3) In speech recognition and machine translation,
more complicated error measures such as the edit
distance and the BLEU measure are used. Is it
possible to derive closed-form Bayes decision rules
(or suitable analytic approximations) for these error
measures? What are the implications?
References
L. Bahl, J. Cocke, F. Jelinek and J. Raviv.
1974. Optimal Decoding of Linear Codes for
Minimizing Symbol Error Rate. IEEE Trans. on
Information Theory, No. 20, pages 284?287
L. Bahl and L. R. Mercer. 1976. Part of Speech
Assignment by a Statistical Decision Algorithm.
In IEEE Symposium on Information Theory,
abstract, pages 88?89, Ronneby, Sweden.
A. D. Beale. 1985. A Probabilistic Approach
to Grammatical Analysis of Written English by
Computer. In 2nd Conf. of the European Chapter
of the ACL, pages 159?169, Geneva, Switzerland.
A. L. Berger, S. Della Pietra and V. Della Pietra.
1996. A Maximum Entropy Approach to
Natural Language Processing. Computational
Linguistics, No. 22, Vol. 1, pages 39?71.
M. Bisani and H. Ney. 2004. Bootstrap Estimates
for Confidence Intervals in ASR Performance
Evaluation. In IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, pages 409?412,
Montreal, Canada.
K. W. Church. 1989. A Stochastic Parts Program
Noun Phrase Parser for Unrestricted Text. In
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, pages 695?698, Glasgow, Scotland.
S. DeRose. 1989. Grammatical Category Disam-
biguation by Statistical Optimization. Computa-
tional Linguistics, No. 14, Vol. 1, pages 31?39
R. O. Duda and P. E. Hart. 1973. Pattern
Classification and Scene Analysis. John Wiley &
Sons, New York.
V. Goel and W. Byrne. 2003. Minimum Bayes-
risk Automatic Speech Recognition. In W. Chou
and B. H. Juang (editors): Pattern Recognition
in Speech and Language Processing. CRC Press,
Boca Rota, Florida.
J. Kinscher and P. Steiner. 1995. Mu?nster Tagging
Project (MTP). Handout for the 4th Northern
German Linguistic Colloquium, University of
Mu?nster, Internal report.
B. Merialdo. 1994. Tagging English Text with a
Probabilistic Model. Computational Linguistics,
No. 20, Vol. 2, pages 155?168.
H. Ney, S. Martin and F. Wessel. 1997.
Statistical Language Modelling by Leaving-One-
Out. In G. Bloothooft and S. Young (editors):
Corpus-Based Methods in Speech and Language,
pages 174?207. Kluwer Academic Publishers,
Dordrecht.
A. Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Conf.
on Empirical Methods in Natural Language
Processing and Very Large Corpora , pages 133?
142, Sommerset, NJ.
W. S. Stolz, P. H. Tannenbaum and F. V. Carstensen.
1965. Stochastic Approach to the Grammatical
Coding of English. Communications of the ACM,
No. 8, pages 399?405.
D. Su?ndermann and H. Ney. 2003. SYNTHER
- a New m-gram POS Tagger. In Proc. of
the Int. Conf. on Natural Language Processing
and Knowledge Engineering, pages 628?633,
Beijing, China.
VBP ? VB
reference ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
string ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
symbol ... investors/NNS already/RB have/VB sharply/RB scaled/VBN ...
reference We/PRP basically/RB think/VBP that/IN ...
string We/PRP basically/RB think/VBP that/IN ...
symbol We/PRP basically/RB think/VB that/IN ...
VBD ? VBN
reference ... plant-expansion/JJ program/NN started/VBD this/DT year/NN ...
string ... plant-expansion/NN program/NN started/VBD this/DT year/NN ...
symbol ... plant-expansion/NN program/NN started/VBN this/DT year/NN ...
reference ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
string ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
symbol ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBN agreements/NNS ...
Rr ? Db
reference Das/Db Sandma?nnchen/Ne ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
string Das/Db Sandma?nnchen/Ng ,/Fi das/Rr uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
symbol Das/Db Sandma?nnchen/Ng ,/Fi das/Db uns/Rp der/Db NDR/Ab pra?sentiert/Vf ...
reference ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
string ... fu?r/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
symbol ... fu?r/Po Leute/Ng ,/Fi die/Db glauben/Vf ...
Cs ? Bi
reference Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
string Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
symbol Denke/Vf ich/Rp nach/Qv ,/Fi warum/Bi mir/Rp die/Db Geschichte/Ng gefa?llt/Vf ...
Table 5: Examples of tagging errors for the symbol decision rule (direct model)
RB ? IN, JJ
reference The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
string The/DT negotiations/NNS allocate/VBP about/IN 15/CD %/NN ...
symbol The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
reference ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
string ... will/MD lead/VB to/TO a/DT much/JJ stronger/JJR performance/NN ...
symbol ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
VBN ? VBD
reference ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
string ... by/IN a/DT police/NN officer/NN named/VBD John/NNP Klute/NNP ...
symbol ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
Db ? Rr
reference er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
string er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Rr Emotionen/Ng zu/Qi kanalisieren/Vi ...
symbol er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
Table 6: Examples of tagging errors for the string decision rule (direct model)
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 41?48,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Augmenting a Small Parallel Text with Morpho-syntactic Language
Resources for Serbian-English Statistical Machine Translation
Maja Popovic?, David Vilar, Hermann Ney
Lehrstuhl fu?r Informatik VI
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{popovic,vilar,ney}@informatik.rwth-aachen.de
Slobodan Jovic?ic?, Zoran ?Saric?
Faculty of Electrical Engineering
University of Belgrade
Serbia and Montenegro
jovicic@etf.bg.ac.yu
Abstract
In this work, we examine the quality of
several statistical machine translation sys-
tems constructed on a small amount of
parallel Serbian-English text. The main
bilingual parallel corpus consists of about
3k sentences and 20k running words from
an unrestricted domain. The translation
systems are built on the full corpus as well
as on a reduced corpus containing only
200 parallel sentences. A small set of
about 350 short phrases from the web is
used as additional bilingual knowledge. In
addition, we investigate the use of mono-
lingual morpho-syntactic knowledge i.e.
base forms and POS tags.
1 Introduction and Related Work
The goal of statistical machine translation (SMT) is
to translate a source language sequence f1, . . . , fJ
into a target language sequence e1, . . . , eI by max-
imising the conditional probability Pr(eI1|fJ1 ). This
probability can be factorised into the translation
model probability P (fJ1 |eI1) which describes the
correspondence between the words in the source and
the target sequence, and the language model proba-
bility P (eJ1 ) which describes well-formedness of the
produced target sequence. These two probabilities
can be modelled independently of each other. For
detailed descriptions of SMT models see for exam-
ple (Brown et al, 1993; Och and Ney, 2003).
Translation probabilities are learnt from a bilin-
gual parallel text corpus and language model proba-
bilities are learnt from a monolingual text in the tar-
get language. Usually, the performance of a trans-
lation system strongly depends on the size of the
available training corpus. However, acquisition of
a large high-quality bilingual parallel text for the de-
sired domain and language pair requires lot of time
and effort, and, for many language pairs, is even not
possible. Besides, small corpora have certain advan-
tages - the acquisition does not require too much
effort and also manual creation and correction are
possible. Therefore there is an increasing number of
publications dealing with limited amounts of bilin-
gual data (Al-Onaizan et al, 2000; Nie?en and Ney,
2004).
For the Serbian language, as a rather minor and
not widely studied language, there are not many
language resources available, especially not parallel
texts. On the other side, investigations on this lan-
guage may be quite useful since the majority of prin-
ciples can be extended to the wider group of Slavic
languages (e.g. Czech, Polish, Russian, etc.).
In this work, we exploit small Serbian-English
parallel texts as a bilingual knowledge source for
statistical machine translation. In addition, we in-
vestigate the possibilities for improving the trans-
lation quality using morpho-syntactic information
in the source language. Some preliminary transla-
tion results on this language pair have been reported
in (Popovic? et al, 2004; Popovic? and Ney, 2004),
but no systematic investigation has been done so far.
This work presents several translation systems cre-
ated with different amounts and types of training
data and gives a detailed description of the language
resources used.
41
2 Language Resources
2.1 Language Characteristics
Serbian, as a Slavic language, has a very rich inflec-
tional morphology for all open word classes. There
are six distinct cases affecting not only common
nouns but also proper nouns as well as pronouns,
adjectives and some numbers. Some nouns and ad-
jectives have two distinct plural forms depending on
the number (if it is larger than four or not). There
are also three genders for the nouns, pronouns, ad-
jectives and some numbers leading to differences be-
tween the cases and also between the verb participles
for past tense and passive voice.
As for verbs, person and many tenses are ex-
pressed by the suffix, and the subject pronoun (e.g.
I, we, it) is often omitted (similarly as in Spanish and
Italian). In addition, negation of three quite impor-
tant verbs, ?biti? (to be, auxiliary verb for past tense,
conditional and passive voice), ?imati? (to have) and
?hteti? (to want, auxiliary verb for the future tense),
is done by adding the negative particle to the verb as
a prefix.
As for syntax, Serbian has a quite free word or-
der, and there are no articles, neither indefinite nor
definite.
All these characteristics indicate that morpho-
syntactic knowledge might be very useful for sta-
tistical machine translation involving Serbian lan-
guage, especially when only scarce amounts of par-
allel text are available.
2.2 Parallel Corpora
Finding high-quality bilingual or multilingual paral-
lel corpora involving Serbian language is a difficult
task. For example, there are several web-sites with
the news in both Serbian and English (some of them
in other languages as well), but these texts are only
comparable and not parallel at all. To our knowl-
edge, the only currently available Serbian-English
parallel text suitable for statistical machine trans-
lation is a manually created electronic version of
the Assimil language course which has been used
for some preliminary experiments in (Popovic? et al,
2004; Popovic? and Ney, 2004). We have used this
corpus for systematical investigations described in
this work.
2.2.1 Assimil Language Course
The electronic form of Assimil language course
contains about 3k sentences and 25k running words
of various types of conversations and descriptions as
well as a few short newspaper articles. Detailed cor-
pus statistics can be seen in Table 1. Since the do-
main of the corpus is basically not restricted, the vo-
cabulary size is relatively large. Due to the rich mor-
phology, the vocabulary for Serbian is almost two
times larger than for English. The average sentence
length for Serbian is about 8.5 words per sentence,
and for English about 9.5. This difference is mainly
caused by the lack of articles and omission of some
subject pronouns in Serbian .
The development and test set (500 sentences) are
randomly extracted from the original corpus and the
rest is used for training (referred to as 2.6k).
In order to investigate the scenario with extremely
scarce training material, a reduced training corpus
(referred to as 200) has been created by random ex-
traction of 200 sentences from the original training
corpus.
The morpho-syntactic annotation of the En-
glish part of the corpus has been done by the con-
straint grammar parser ENGCG for morphological
and syntactic analysis of English language. For each
word, this tool provides its base form and sequence
of morpho-syntactic tags.
For the Serbian corpus, to our knowlegde there
is no available tool for automatic annotation of this
language. Therefore, the base forms have been in-
troduced manually and the POS tags have been pro-
vided partly manually and partly automatically us-
ing a statistical maximum-entropy based POS tagger
similar to the one described in (Ratnaparkhi, 1996).
First, the 200 sentences of the reduced training cor-
pus have been annotated completely manually. Then
the first 500 sentences of the rest of the training cor-
pus have been tagged automatically and the errors
have been manually corrected. Afterwards, the POS
tagger has been trained on the extended corpus (700
sentences), the next 500 sentences of the rest are an-
notated, and the procedure has been repeated until
the annotation has been finished for the complete
corpus.
42
Table 1: Statistics of the Serbian-English Assimil corpus
Serbian English
Training: original base forms original no article
full corpus Sentences 2632 2632
(2.6k) Running Words + Punct. 22227 24808 23308
Average Sentence Length 8.4 9.5 8.8
Vocabulary Size 4546 2605 2645 2642
Singletons 2728 1253 1211
reduced corpus Sentences 200 200
(200) Running Words + Punct. 1666 1878 1761
Average Sentence Length 8.3 10.4 8.8
Vocabulary Size 778 596 603 600
Singletons 618 417 395
Dev+Test Sentences 500 500
Running Words + Punct. 4161 4657 4362
Average Sentence Length 8.3 9.3 8.7
Vocabulary Size 1457 1030 1055 1052
Running OOVs - 2.6k 12.1% 5.2% 4.8%
Running OOVs - 200 34.5% 27.6% 21.4%
OOVs - 2.6k 32.7% 19.5% 19.7%
OOVs - 200 76.2% 66.0% 66.8%
External Test Sentences 22 22
Running Words + Punct. 395 446 412
Average Sentence Length 18.0 20.3 18.7
Vocabulary Size 213 176 202 199
Running OOVs - 2.6k 44.3% 35.4% 32.1% 34.7%
Running OOVs - 200 53.7% 44.6% 43.7% 47.3 %
OOVs - 2.6k 61.5% 45.4% 44.0% 44.7%
OOVs - 200 74.6% 63.1% 63.9% 64.8%
Table 2: Statistics of the Serbian-English short phrases
Serbian English
Phrases original base forms original no article
Entries 351 351 351 351
Running Words + Punct. 617 617 730 700
Average Entry Length 1.8 1.8 2.1 2.0
Vocabulary Size 335 303 315 312
Singletons 239 209 209 208
New Running 2.6k 20.6% 14.4% 11.8% 11.8%
Words 200 50.6% 41.3% 36.7% 37.8%
New Vocabulary 2.6k 30.1% 22.1% 21.6% 21.2%
Words 200 70.7% 63.0% 63.2% 63.1%
43
2.2.2 Short Phrases
The short phrases used as an additional bilingual
knowledge source in our experiments have been col-
lected from the web and contain about 350 standard
words and short expressions with an average entry
length of 1.8 words for Serbian and 2 words for En-
glish. Table 2 shows that about 30% of words from
the phrase vocabulary are not present in the origi-
nal Serbian corpus and about 70% of those words
are not contained in the reduced corpus. For the
English language those numbers are smaller, about
20% for the original corpus and 60% for the reduced
one. These percentages are indicating that this par-
allel text, although very scarce, might be an useful
additional training material.
The phrases have also been morpho-syntactically
annotated in the same way as the main corpus.
2.2.3 External Test
In addition to the standard development and test
set described in Section 2.2.1, we also tested our
translation systems on a short external parallel text
collected from the BBC News web-site contain-
ing 22 sentences about relations between USA and
Ukraine after the revolution. As can be seen in Ta-
ble 1, this text contains very large portion of out-
of-vocabulary words (almost two thirds of Serbian
words and almost half of English words are not seen
in the training corpus), and has an average sentence
length about two times larger than the training cor-
pus.
3 Transformations in the Source Language
Standard SMT systems usually regard only full
forms of the words, so that translation of full forms
which have not been seen in the training corpus is
not possible even if the base form has been seen.
Since the inflectional morphology of the Serbian
language is very rich, as described in Section 2.1, we
investigate the use of the base forms instead of the
full forms to overcome this problem for the transla-
tion into English. We propose two types of trans-
formations of the Serbian corpus: conversion of the
full forms into the base forms and additional treat-
ment of the verbs.
For the other translation direction, we propose re-
moving the articles in the English part of the corpus
as the Serbian language does not have any.
3.1 Transformations of the Serbian Text
3.1.1 Base Forms
Serbian full forms of the words usually contain
information which is not relevant for translation into
English. Therefore, we propose conversion of all
Serbian words in their base forms. Although for
some other inflected languages like German and
Spanish this method did not yield any translation
improvement, we still considered it as promising be-
cause the number of Serbian inflections is consider-
ably higher than in the other two languages. Table 1
shows that this transformation significantly reduces
the Serbian vocabulary size so that it becomes com-
parable to the English one.
3.1.2 Treatment of Verbs
Inflections of Serbian verbs might contain rel-
evant information about the person, which is es-
pecially important when the pronoun is omitted.
Therefore, we apply an additional treatment of the
verbs. Whereas all other word classes are still re-
placed only by their base forms, for each verb a part
of the POS tag referring to the person is taken and
the verb is converted into a sequence of this tag and
its base form. For the three verbs described in Sec-
tion 2.1, the separation of the negative particle is also
applied: each negative full form is transformed into
the sequence of the POS tag, negative particle and
base form. The detailed statistics of this corpus is
not reported since there are no significant changes,
only the number of running words and average sen-
tence length increase thus becoming closer to the
values of the English corpus.
3.2 Transformations of the English Text
3.2.1 Removing Articles
Since the articles are one of the most frequent
word classes in English, but on the other side there
are no arcticles at all in Serbian, we propose remov-
ing the articles from the English corpus for trans-
lation into Serbian. Each English word which has
been detected as an article by means of its POS tag
has been removed from the corpus. In Table 1, it
can be seen that this method significantly reduces
the number of running words and the average sen-
tence length of the English corpus thus becoming
comparable to the values of the Serbian corpus.
44
4 Translation Experiments and Results
4.1 Experimental Settings
In order to systematically investigate the impact of
the bilingual training corpus size and the effects
of the morpho-syntactic information on the trans-
lation quality, the translation systems were trained
on the full training corpus (2.6k) and on the re-
duced training corpus (200), both with and with-
out short phrases. The translation is performed in
both directions, i.e. from Serbian to English and
other way round. For the Serbian to English trans-
lation systems, three versions of the Serbian corpus
have been used: original (baseline), base forms only
(sr base) and base forms with additional treatment
of the verbs (sr base+v-pos). For the translation into
Serbian, the systems were trained on two versions of
the English corpus: original (baseline) and without
articles (en no-article).
The baseline translation system is the Alignment
Templates system with scaling factors (Och and
Ney, 2002). Word alignments are produced using
GIZA++ toolkit without symmetrisation (Och and
Ney, 2003). Preprocessing of the source data has
been done before the training of the system, there-
fore modifications of the training and search pro-
cedure were not necessary for the translation of the
transformed source language corpora.
Although the development set has been used to
optimise the scaling factors, results obtained for this
set do not differ from those for the test set. There-
fore only the joint error rates (Development+Test)
are reported.
As for the external test set, results for this text are
reported only for the full corpus systems, since for
the reduced corpus the error rates are higher but the
effects of using phrases and morpho-syntactic infor-
mation are basically the same.
4.2 Translation Results
The evaluation metrics used in our experiments
are WER (Word Error Rate), PER (Position-
independent word Error Rate) and BLEU (BiLin-
gual Evaluation Understudy) (Papineni et al, 2002).
Since BLEU is an accuracy measure, we use 1-
BLEU as an error measure.
4.2.1 Translation from Serbian into English
Error rates for the translation from Serbian into
English are shown in Table 3 and some examples
are shown in Table 6. It can be seen that there is a
significant decrease in all error rates when the full
forms are replaced with their base forms. Since the
redundant information contained in the inflection is
removed, the system can better capture the relevant
information and is capable of producing correct or
approximatively correct translations even for unseen
full forms of the words (marked by ?UNKNOWN ?
in the baseline result example). The treatment of the
verbs yields some additional improvements.
From the first translation example in Table 6 it can
be seen how the problem of some out-of-vocabulary
words can be overcomed with the use of the base
forms. The second and third example are showing
the advantages of the verb treatment, the third one
illustrates the effect of separating the negative parti-
cle.
Reduction of the training corpus to only 200 sen-
tences (about 8% of the original corpus) leads to a
loss of error rates of about 45% relative. However,
the degradation is not higher than 35% if phrases and
morpho-syntactic information are available in addi-
tion to the reduced corpus.
The use of the phrases can improve the transla-
tion quality to some extent, especially for the sys-
tems with the reduced training corpus, but these im-
provements are less remarkable than those obtained
by replacing words with the base forms.
The best system with the complete corpus as well
as the best one with the reduced corpus use the
phrases and the transformed Serbian corpus where
the verb treatment has been applied.
4.2.2 Translation from English into Serbian
Table 4 shows results for the translation from En-
glish into Serbian. As expected, all error rates are
higher than for the other translation direction. Trans-
lation into the morphologically richer language al-
ways has poorer quality because it is difficult to find
the correct inflection.
The performance of the reduced corpus is de-
graded for about 40% relative for the baseline sys-
tem and for about 30% when the phrases are used
and the transformation of the English corpus has
been applied.
45
Table 3: Translation error rates [%] for Serbian?English
Serbian ? English Development+Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 45.6 39.6 70.0
2.6k sr base 43.5 38.2 68.9
2.6k sr base+v-pos 42.5 35.3 66.2
2.6k+phrases baseline 46.0 39.6 69.5
2.6k+phrases sr base 44.6 39.1 70.2
2.6k+phrases sr base+v-pos 42.1 35.3 66.0
200 baseline 66.5 61.1 91.6
200 sr base 63.2 58.2 90.3
200 sr base+v-pos 63.3 56.2 88.5
200+phrases baseline 65.2 59.5 90.2
200+phrases sr base 62.3 56.9 87.7
200+phrases sr base+v-pos 61.3 53.2 86.2
Table 4: Translation error rates [%] for English?Serbian
English ? Serbian Development+Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 53.1 46.9 78.6
2.6k en no-article 52.6 47.2 79.4
2.6k+phrases baseline 52.5 46.5 76.6
2.6k+phrases en no-article 52.3 47.0 79.6
200 baseline 73.6 68.0 93.0
200 en no-article 71.5 66.5 93.4
200+phrases baseline 71.7 66.7 92.3
200+phrases en no-article 67.9 62.9 92.1
Table 5: Translation error rates [%] for the external test
Serbian ? English External Test
Training Corpus Method WER PER 1-BLEU
2.6k baseline 72.2 64.8 92.2
2.6k sr base 66.8 61.4 86.9
2.6k sr base+v-pos 67.5 61.4 88.3
2.6k+phrases baseline 71.3 63.9 91.9
2.6k+phrases sr base 67.0 61.2 88.4
2.6k+phrases sr base+v-pos 69.7 61.2 89.8
English ? Serbian
2.6k baseline 85.3 77.0 96.4
2.6k en no-article 77.5 69.9 95.8
2.6k+phrases baseline 84.1 74.9 95.2
2.6k+phrases en no-article 77.7 70.1 94.8
46
The importance of the phrases seems to be larger
for this translation direction. Removing the English
articles does not have the significant role for the
translation systems with full corpus, but for the re-
duced corpus it has basically the same effect as the
use of phrases. The best system with the reduced
corpus has been built with the use of phrases and
removal of the articles.
Table 7 shows some examples of the translation
into Serbian with and without English articles. Al-
though these effects are not directly obvious, it can
be seen that removing of the redundant information
enables better learning of the relevant information
so that system is better capable of producing seman-
tically correct output. The first example illustrates
an syntactically incorrect output with the wrong in-
flection of the verb (?c?itam? means ?I read?). The
output of the system without articles is still not com-
pletely correct, but the semantic is completely pre-
served. The second example illustrates an output
produced by the baseline system which is neither
syntactically nor semantically correct (?you have I
drink?). The output of the new system still has an
error in the verb, informal form of ?you? instead of
the formal one, but nevertheless both the syntax and
semantics are correct.
4.2.3 Translation of the External Text
Translation results for the external test can be
seen in Table 5. As expected, the high number of
out-of-vocabulary words results in very high error
rates. Certain improvement is achieved with the
phrases, but the most significant improvements are
yielded by the use of Serbian base forms and re-
moval of English articles. Verb treatment in this case
does not outperform the base forms system, prob-
ably because there are not so many different verb
forms as in the other corpus, and only a small num-
ber of pronouns is missing.
5 Conclusions
In this work, we have examined the possibilities
for building a statistical machine translation system
with a small bilingual Serbian-English parallel text.
Our experiments showed that the translation results
for this language pair are comparable with results for
other language pairs, especially if the small size of
the corpus, unrestricted domain and rich inflectional
morphology of Serbian language are taken into ac-
count. With the baseline system, we obtained about
45% WER for translation into English and about
53% for translation into Serbian.
We have systematically investigated the impact of
the corpus size on translation quality, as well as the
importance of additional bilingual knowledge in the
form of short phrases. In addition, we have shown
that morpho-syntactic information is a valuable lan-
guage resource for translation of this language pair.
Depending on the availability of resources and
tools, we plan to examine parallel texts with other
languages, and also to do further investigations on
this language pair. We believe that more refined use
of the morpho-syntactic information can yield better
results (for example the hierarchical lexicon model
proposed in (Nie?en and Ney, 2001)). We also be-
lieve that the use of the conventional dictionaries
could improve the Serbian-English translation.
Acknowledgement
This work was partly funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistical Methods for Written Language Transla-
tion? (Ne572/5).
References
Y. Al-Onaizan, U. Germann, U. Hermjakob, K. Knight,
P. Koehn, D. Marcu, and K. Yamada. 2000. Translat-
ing with scarce resources. In National Conference on
Artificial Intelligence (AAAI).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
Sonja Nie?en and Hermann Ney. 2001. Toward hi-
erarchical models for statistical machine translation
of inflected languages. In 39th Annual Meeting of
the Assoc. for Computational Linguistics - joint with
EACL 2001: Proc. Workshop on Data-Driven Ma-
chine Translation, pages 47?54, Toulouse, France,
July.
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204, June.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
47
Table 6: Examples of Serbian?English translations with and without transformations
to je suvishe skupo . ? to biti suvishe skup . ? to SG3 biti suvishe skup .
base forms verb treatment
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
it is it is it is
too UNKNOWN skupo . too expensive . too expensive .
on ne igra . ? on ne igrati . ? on ne SG3 igrati .
base forms verb treatment
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
he he does not . he do not play . he does not play .
da , ali nemam ? da , ali nemati ? da , ali SG1 ne imati
mnogo vremena . base forms mnogo vreme . verb treatment mnogo vreme .
? Sr ? En (baseline) ? Sr? ? En ? Sr? ? En
yes , but I have yes , but not yes , but I have not got
much time . much time . much time .
Table 7: Examples of English?Serbian translations with and without transformations
you should not ? you should not
read in bed . remove articles read in bed .
? En ? Sr (baseline) ? En? ? Sr reference translation:
treba ne ne bi trebalo ne bi trebalo
c?itam u krevet . c?itate u krevet . da c?itate u krevetu .
have a drink . ? have drink .
remove articles
? En ? Sr (baseline) ? En? ? Sr reference translation:
imate pijem . uzmi nes?to za pic?e . uzmite nes?to za pic?e .
machine translation. In Proc. 40th Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
M. Popovic? and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In Proc. 4th Int. Conf. on Language Resources and
Evaluation (LREC), pages 1585?1588, Lisbon, Portu-
gal, May.
M. Popovic?, S. Jovic?ic?, and Z. ?Saric?. 2004. Statistical
machine translation of Serbian-English. In Proc. of
Int. Workshop on Speech and Computer (SPECOM),
pages 410?414, St. Petersburg, Russia, September.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
pages 133?142, Sommerset, NJ.
48
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167?174,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Novel Reordering Approaches in Phrase-Based Statistical Machine
Translation
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney
The authors are with the Lehrstuhl fu?r Informatik VI,
Computer Science Department, RWTH Aachen University,
D-52056 Aachen, Germany.
E-mail: {kanthak,vilar,matusov,zens,ney}@informatik.rwth-aachen.de.
Abstract
This paper presents novel approaches to
reordering in phrase-based statistical ma-
chine translation. We perform consistent
reordering of source sentences in train-
ing and estimate a statistical translation
model. Using this model, we follow a
phrase-based monotonic machine transla-
tion approach, for which we develop an ef-
ficient and flexible reordering framework
that allows to easily introduce different re-
ordering constraints. In translation, we
apply source sentence reordering on word
level and use a reordering automaton as in-
put. We show how to compute reordering
automata on-demand using IBM or ITG
constraints, and also introduce two new
types of reordering constraints. We further
add weights to the reordering automata.
We present detailed experimental results
and show that reordering significantly im-
proves translation quality.
1 Introduction
Reordering is of crucial importance for machine
translation. Already (Knight et al, 1998) use full un-
weighted permutations on the level of source words
in their early weighted finite-state transducer ap-
proach which implemented single-word based trans-
lation using conditional probabilities. In a refine-
ment with additional phrase-based models, (Kumar
et al, 2003) define a probability distribution over
all possible permutations of source sentence phrases
and prune the resulting automaton to reduce com-
plexity.
A second category of finite-state translation ap-
proaches uses joint instead of conditional probabili-
ties. Many joint probability approaches originate in
speech-to-speech translation as they are the natural
choice in combination with speech recognition mod-
els. The automated transducer inference techniques
OMEGA (Vilar, 2000) and GIATI (Casacuberta et
al., 2004) work on phrase level, but ignore the re-
ordering problem from the view of the model. With-
out reordering both in training and during search,
sentences can only be translated properly into a lan-
guage with similar word order. In (Bangalore et al,
2000) weighted reordering has been applied to tar-
get sentences since defining a permutation model on
the source side is impractical in combination with
speech recognition. In order to reduce the computa-
tional complexity, this approach considers only a set
of plausible reorderings seen on training data.
Most other phrase-based statistical approaches
like the Alignment Template system of Bender
et al (2004) rely on (local) reorderings which are
implicitly memorized with each pair of source and
target phrases in training. Additional reorderings on
phrase level are fully integrated into the decoding
process, which increases the complexity of the sys-
tem and makes it hard to modify. Zens et al (2003)
reviewed two types of reordering constraints for this
type of translation systems.
In our work we follow a phrase-based transla-
tion approach, applying source sentence reordering
on word level. We compute a reordering graph on-
demand and take it as input for monotonic trans-
lation. This approach is modular and allows easy
introduction of different reordering constraints and
probabilistic dependencies. We will show that it per-
forms at least as well as the best statistical machine
translation system at the IWSLT Evaluation.
167
In the next section we briefly review the basic
theory of our translation system based on weighted
finite-state transducers (WFST). In Sec. 3 we in-
troduce new methods for reordering and alignment
monotonization in training. To compare differ-
ent reordering constraints used in the translation
search process we develop an on-demand com-
putable framework for permutation models in Sec. 4.
In the same section we also define and analyze un-
restricted and restricted permutations with some of
them being first published in this paper. We con-
clude the paper by presenting and discussing a rich
set of experimental results.
2 Machine Translation using WFSTs
Let fJ1 and eIi be two sentences from a source and
target language. Assume that we have word level
alignments A of all sentence pairs from a bilingual
training corpus. We denote with e?J1 the segmenta-
tion of a target sentence eI1 into J phrases such that
fJ1 and e?J1 can be aligned to form bilingual tuples
(fj , e?j). If alignments are only functions of target
words A? : {1, . . . , I} ? {1, . . . , J}, the bilingual
tuples (fj , e?j) can be inferred with e. g. the GIATI
method of (Casacuberta et al, 2004), or with our
novel monotonization technique (see Sec. 3). Each
source word will be mapped to a target phrase of one
or more words or an ?empty? phrase ?. In particular,
the source words which will remain non-aligned due
to the alignment functionality restriction are paired
with the empty phrase.
We can then formulate the problem of finding the
best translation e?I1 of a source sentence fJ1 :
e?I1 = argmax
eI1
Pr(fJ1 , e
I
1)
= argmax
e?J1
?
A?A
Pr(fJ1 , e?
J
1 , A)
?= argmax
e?J1
max
A?A
Pr(A) ? Pr(fJ1 , e?
J
1 |A)
?= argmax
e?J1
max
A?A
?
fj :j=1...J
Pr(fj , e?j |f
j?1
1 , e?
j?1
1 , A)
= argmax
e?J1
max
A?A
?
fj :j=1...J
p(fj , e?j |f
j?1
j?m, e?
j?1
j?m, A)
In other words: if we assume a uniform distri-
bution for Pr(A), the translation problem can be
mapped to the problem of estimating an m-gram lan-
guage model over a learned set of bilingual tuples
(fj , e?j). Mapping the bilingual language model to a
WFST T is canonical and it has been shown in (Kan-
thak et al, 2004) that the search problem can then be
rewritten using finite-state terminology:
e?I1 = project-output(best(fJ1 ? T )) .
This implementation of the problem as WFSTs may
be used to efficiently solve the search problem in
machine translation.
3 Reordering in Training
When the alignment function A? is not monotonic,
target language phrases e? can become very long.
For example in a completely non-monotonic align-
ment all target words are paired with the last aligned
source word, whereas all other source words form
tuples with the empty phrase. Therefore, for lan-
guage pairs with big differences in word order, prob-
ability estimates may be poor.
This problem can be solved by reordering either
source or target training sentences such that align-
ments become monotonic for all sentences. We
suggest the following consistent source sentence re-
ordering and alignment monotonization approach in
which we compute optimal, minimum-cost align-
ments.
First, we estimate a cost matrix C for each sen-
tence pair (fJ1 , eI1). The elements of this matrix cij
are the local costs of aligning a source word fj to a
target word ei. Following (Matusov et al, 2004), we
compute these local costs by interpolating state oc-
cupation probabilities from the source-to-target and
target-to-source training of the HMM and IBM-4
models as trained by the GIZA++ toolkit (Och et al,
2003). For a given alignment A ? I ? J , we define
the costs of this alignment c(A) as the sum of the
local costs of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (1)
The goal is to find an alignment with the minimum
costs which fulfills certain constraints.
3.1 Source Sentence Reordering
To reorder a source sentence, we require the
alignment to be a function of source words A1:
{1, . . . , J} ? {1, . . . , I}, easily computed from the
cost matrix C as:
A1(j) = argmini cij (2)
168
We do not allow for non-aligned source words. A1
naturally defines a new order of the source words fJ1
which we denote by f?J1 . By computing this permu-
tation for each pair of sentences in training and ap-
plying it to each source sentence, we create a corpus
of reordered sentences.
3.2 Alignment Monotonization
In order to create a ?sentence? of bilingual tuples
(f?J1 , e?
J
1 ) we required alignments between reordered
source and target words to be a function of target
words A2 : {1, . . . , I} ? {1, . . . , J}. This align-
ment can be computed in analogy to Eq. 2 as:
A2(i) = argminj c?ij (3)
where c?ij are the elements of the new cost matrix
C? which corresponds to the reordered source sen-
tence. We can optionally re-estimate this matrix by
repeating EM training of state occupation probabili-
ties with GIZA++ using the reordered source corpus
and the original target corpus. Alternatively, we can
get the cost matrix C? by reordering the columns of
the cost matrix C according to the permutation given
by alignment A1.
In alignment A2 some target words that were pre-
viously unaligned in A1 (like ?the? in Fig. 1) may
now still violate the alignment monotonicity. The
monotonicity of this alignment can not be guaran-
teed for all words if re-estimation of the cost matri-
ces had been performed using GIZA++.
The general GIATI technique (Casacuberta et al,
2004) is applicable and can be used to monotonize
the alignment A2. However, in our experiments
the following method performs better. We make
use of the cost matrix representation and compute
a monotonic minimum-cost alignment with a dy-
namic programming algorithm similar to the Lev-
enshtein string edit distance algorithm. As costs of
each ?edit? operation we consider the local align-
ment costs. The resulting alignment A3 represents
a minimum-cost monotonic ?path? through the cost
matrix. To make A3 a function of target words we
do not consider the source words non-aligned in A2
and also forbid ?deletions? (?many-to-one? source
word alignments) in the DP search.
An example of such consistent reordering and
monotonization is given in Fig. 1. Here, we re-
order the German source sentence based on the ini-
tial alignment A1, then compute the function of tar-
get words A2, and monotonize this alignment to A3
the very beginning of May would suit me .
the very beginning of May would suit me .
sehr gut Anfang Mai w?rde passen mir .
sehr gut Anfang Mai w?rde passen mir .
the very beginning of May would suit me .
mir sehrw?rde gut Anfang Mai passen .
.Mai|of_May w?rde|would passen|suit mir|me |.sehr|the_very gut|$ Anfang|beginning
A
A
A1
2
3
Figure 1: Example of alignment, source sentence re-
ordering, monotonization, and construction of bilin-
gual tuples.
with the dynamic programming algorithm. Fig. 1
also shows the resulting bilingual tuples (f?j , e?j).
4 Reordering in Search
When searching the best translation e?J1 for a given
source sentence fJ1 , we permute the source sentence
as described in (Knight et al, 1998):
e?I1 = project-output(best(permute(fJ1 ) ? T ))
Permuting an input sequence of J symbols re-
sults in J ! possible permutations and representing
the permutations as a finite-state automaton requires
at least 2J states. Therefore, we opt for computing
the permutation automaton on-demand while apply-
ing beam pruning in the search.
4.1 Lazy Permutation Automata
For on-demand computation of an automaton in the
flavor described in (Kanthak et al, 2004) it is suffi-
cient to specify a state description and an algorithm
that calculates all outgoing arcs of a state from the
state description. In our case, each state represents
a permutation of a subset of the source words fJ1 ,
which are already translated.
This can be described by a bit vector bJ1 (Zens
et al, 2002). Each bit of the state bit vector corre-
sponds to an arc of the linear input automaton and is
set to one if the arc has been used on any path from
the initial to the current state. The bit vectors of two
states connected by an arc differ only in a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
169
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
Even with on-demand computation, complexity
using full permutations is unmanagable for long sen-
tences. We further reduce complexity by addition-
ally constraining permutations. Refer to Figure 2 for
visualizations of the permutation constraints which
we describe in the following.
4.2 IBM Constraints
The IBM reordering constraints are well-known in
the field of machine translation and were first de-
scribed in (Berger et al, 1996). The idea behind
these constraints is to deviate from monotonic trans-
lation by postponing translations of a limited num-
ber of words. More specifically, at each state we
can translate any of the first l yet uncovered word
positions. The implementation using a bit vector is
straightforward. For consistency, we associate win-
dow size with the parameter l for all constraints pre-
sented here.
4.3 Inverse IBM Constraints
The original IBM constraints are useful for a large
number of language pairs where the ability to skip
some words reflects the differences in word order
between the two languages. For some other pairs,
it is beneficial to translate some words at the end of
the sentence first and to translate the rest of the sen-
tence nearly monotonically. Following this idea we
can define the inverse IBM constraints. Let j be the
first uncovered position. We can choose any posi-
tion for translation, unless l ? 1 words on positions
j? > j have been translated. If this is the case we
must translate the word in position j. The inverse
IBM constraints can also be expressed by
invIBM(x) = transpose(IBM(transpose(x))) .
As the transpose operation can not be computed
on-demand, our specialized implementation uses bit
vectors bJ1 similar to the IBM constraints.
4.4 Local Constraints
For some language pairs, e.g. Italian ? English,
words are moved only a few words to the left or
right. The IBM constraints provide too many alter-
native permutations to chose from as each word can
be moved to the end of the sentence. A solution that
allows only for local permutations and therefore has
a)
0000 10001 11002 11103 11114
b)
0000
10001
01002 1100
2
10103
1
0110
3
11103
110141
01114
1111
4
13
2
10114 2
c)
0000
10001
01002
0010
3
0001
4 10014
1010
3 11002
1
1
1
1101
2
11113
11102 4
43
d)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2
Figure 2: Permutations of a) positions j = 1, 2, 3, 4
of a source sentence f1f2f3f4 using a window size
of 2 for b) IBM constraints, c) inverse IBM con-
straints and d) local constraints.
very low complexity is given by the following per-
mutation rule: the next word for translation comes
from the window of l positions1 counting from the
first yet uncovered position. Note, that the local con-
straints define a true subset of the permutations de-
fined by the IBM constraints.
4.5 ITG Constraints
Another type of reordering can be obtained using In-
version Transduction Grammars (ITG) (Wu, 1997).
These constraints are inspired by bilingual bracket-
ing. They proved to be quite useful for machine
translation, e.g. see (Bender et al, 2004). Here,
we interpret the input sentence as a sequence of seg-
ments. In the beginning, each word is a segment of
its own. Longer segments are constructed by recur-
sively combining two adjacent segments. At each
1both covered and uncovered
170
Chinese English Japanese English Italian English
train sentences 20 000 20 000 66107
words 182 904 160 523 209 012 160 427 410 275 427 402
singletons 3 525 2 948 4 108 2 956 6 386 3 974
vocabulary 7 643 6 982 9 277 6 932 15 983 10 971
dev sentences 506 506 500
words 3 515 3 595 4 374 3 595 3 155 3 253
sentence length (avg/max) 6.95 / 24 7.01 / 29 8.64 / 30 7.01 / 29 5.79 / 24 6.51 / 25
test sentences 500 500 506
words 3 794 ? 4 370 ? 2 931 3 595
sentence length (avg/max) 7.59 / 62 7.16 / 71 8.74 / 75 7.16 / 71 6.31 / 27 6.84 / 28
Table 1: Statistics of the Basic Travel Expression (BTEC) corpora.
combination step, we either keep the two segments
in monotonic order or invert the order. This pro-
cess continues until only one segment for the whole
sentence remains. The on-demand computation is
implemented in spirit of Earley parsing.
We can modify the original ITG constraints to
further limit the number of reorderings by forbid-
ding segment inversions which violate IBM con-
straints with a certain window size. Thus, the re-
sulting reordering graph contains the intersection of
the reorderings with IBM and the original ITG con-
straints.
4.6 Weighted Permutations
So far, we have discussed how to generate the per-
mutation graphs under different constraints, but per-
mutations were equally probable. Especially for the
case of nearly monotonic translation it is make sense
to restrict the degree of non-monotonicity that we
allow when translating a sentence. We propose a
simple approach which gives a higher probability
to the monotone transitions and penalizes the non-
monotonic ones.
A state description bJ1 , for which the following
condition holds:
Mon(j) : bj? = ?(j
? ? j) ? 1 ? j? ? J
represents the monotonic path up to the word fj . At
each state we assign the probability ? to that out-
going arc where the target state description fullfills
Mon(j+1) and distribute the remaining probability
mass 1? ? uniformly among the remaining arcs. In
case there is no such arc, all outgoing arcs get the
same uniform probability. This weighting scheme
clearly depends on the state description and the out-
going arcs only and can be computed on-demand.
5 Experimental Results
5.1 Corpus Statistics
The translation experiments were carried out on the
Basic Travel Expression Corpus (BTEC), a multilin-
gual speech corpus which contains tourism-related
sentences usually found in travel phrase books.
We tested our system on the so called Chinese-to-
English (CE) and Japanese-to-English (JE) Supplied
Tasks, the corpora which were provided during the
International Workshop on Spoken Language Trans-
lation (IWSLT 2004) (Akiba et al, 2004). In ad-
dition, we performed experiments on the Italian-to-
English (IE) task, for which a larger corpus was
kindly provided to us by ITC/IRST. The corpus
statistics for the three BTEC corpora are given in
Tab. 1. The development corpus for the Italian-to-
English translation had only one reference transla-
tion of each Italian sentence. A set of 506 source
sentences and 16 reference translations is used as
a development corpus for Chinese-to-English and
Japanese-to-English and as a test corpus for Italian-
to-English tasks. The 500 sentence Chinese and
Japanese test sets of the IWSLT 2004 evaluation
campaign were translated and automatically scored
against 16 reference translations after the end of the
campaign using the IWSLT evaluation server.
5.2 Evaluation Criteria
For the automatic evaluation, we used the crite-
ria from the IWSLT evaluation campaign (Akiba et
al., 2004), namely word error rate (WER), position-
independent word error rate (PER), and the BLEU
and NIST scores (Papineni et al, 2002; Doddington,
2002). The two scores measure accuracy, i. e. larger
scores are better. The error rates and scores were
computed with respect to multiple reference transla-
171
 40
 42
 44
 46
 48
 50
 52
 54
 56
 58
 60
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
Figure 3: Word error rate [%] as a function of the reordering window size for different reordering constraints:
Japanese-to-English (left) and Chinese-to-English (right) translation.
tions, when they were available. To indicate this, we
will label the error rate acronyms with an m. Both
training and evaluation were performed using cor-
pora and references in lowercase and without punc-
tuation marks.
5.3 Experiments
We used reordering and alignment monotonization
in training as described in Sec. 3. To estimate the
matrices of local alignment costs for the sentence
pairs in the training corpus we used the state occupa-
tion probabilities of GIZA++ IBM-4 model training
and interpolated the probabilities of source-to-target
and target-to-source training directions. After that
we estimated a smoothed 4-gram language model on
the level of bilingual tuples fj , e?j and represented it
as a finite-state transducer.
When translating, we applied moderate beam
pruning to the search automaton only when using re-
ordering constraints with window sizes larger than 3.
For very large window sizes we also varied the prun-
ing thresholds depending on the length of the input
sentence. Pruning allowed for fast translations and
reasonable memory consumption without a signifi-
cant negative impact on performance.
In our first experiments, we tested the four re-
ordering constraints with various window sizes. We
aimed at improving the translation results on the de-
velopment corpora and compared the results with
two baselines: reordering only the source training
sentences and translation of the unreordered test sen-
tences; and the GIATI technique for creating bilin-
gual tuples (fj , e?j) without reordering of the source
sentences, neither in training nor during translation.
5.3.1 Highly Non-Monotonic Translation (JE)
Fig. 3 (left) shows word error rate on the
Japanese-to-English task as a function of the win-
dow size for different reordering constraints. For
each of the constraints, good results are achieved
using a window size of 9 and larger. This can be
attributed to the Japanese word order which is very
different from English and often follows a subject-
object-verb structure. For small window sizes, ITG
or IBM constraints are better suited for this task, for
larger window sizes, inverse IBM constraints per-
form best. The local constraints perform worst and
require very large window sizes to capture the main
word order differences between Japanese and En-
glish. However, their computational complexity is
low; for instance, a system with local constraints
and window size of 9 is as fast (25 words per sec-
ond) as the same system with IBM constraints and
window size of 5. Using window sizes larger than
10 is computationally expensive and does not sig-
nificantly improve the translation quality under any
of the constraints.
Tab. 2 presents the overall improvements in trans-
lation quality when using the best setting: inverse
IBM constraints, window size 9. The baseline with-
out reordering in training and testing failed com-
pletely for this task, producing empty translations
for 37 % of the sentences2. Most of the original
alignments in training were non-monotonic which
resulted in mapping of almost all Japanese words to
? when using only the GIATI monotonization tech-
nique. Thus, the proposed reordering methods are of
crucial importance for this task.
2Hence a NIST score of 0 due to the brevity penalty.
172
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
BTEC Japanese-to-English (JE) dev
none 59.7 58.8 13.0 0.00
in training 57.8 39.4 14.7 3.27
+ 9-inv-ibm 40.3 32.1 45.1 8.59
+ rescoring* 39.1 30.9 53.2 9.93
BTEC Chinese-to-English (CE) dev
none 55.2 52.1 24.9 1.34
in training 54.0 42.3 23.0 4.18
+ 7-inv-ibm 47.1 39.4 34.5 6.53
+ rescoring* 48.3 40.7 39.1 8.11
Table 2: Translation results with optimal reorder-
ing constraints and window sizes for the BTEC
Japanese-to-English and Chinese-to-English devel-
opment corpora. *Optimized for the NIST score.
mWER mPER BLEU NIST
[%] [%] [%]
BTEC Japanese-to-English (JE) test
AT 41.9 33.8 45.3 9.49
WFST 42.1 35.6 47.3 9.50
BTEC Chinese-to-English (CE) test
AT 45.6 39.0 40.9 8.55
WFST 46.4 38.8 40.8 8.73
Table 3: Comparison of the IWSLT-2004 automatic
evaluation results for the described system (WFST)
with those of the best submitted system (AT).
Further improvements were obtained with a
rescoring procedure. For rescoring, we produced
a k-best list of translation hypotheses and used the
word penalty and deletion model features, the IBM
Model 1 lexicon score, and target language n-gram
models of the order up to 9. The scaling factors for
all features were optimized on the development cor-
pus for the NIST score, as described in (Bender et
al., 2004).
5.3.2 Moderately Non-Mon. Translation (CE)
Word order in Chinese and English is usually sim-
ilar. However, a few word reorderings over quite
large distances may be necessary. This is especially
true in case of questions, in which question words
like ?where? and ?when? are placed at the end of
a sentence in Chinese. The BTEC corpora contain
many sentences with questions.
The inverse IBM constraints are designed to per-
form this type of reordering (see Sec. 4.3). As shown
in Fig. 3, the system performs well under these con-
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
none 25.6 22.0 62.1 10.46
in training 28.0 22.3 58.1 10.32
+ 4-local 26.3 20.3 62.2 10.81
+ weights 25.3 20.3 62.6 10.79
+ 3-ibm 27.2 20.5 61.4 10.76
+ weights 25.2 20.3 62.9 10.80
+ rescoring* 22.2 19.0 69.2 10.47
Table 4: Translation results with optimal reordering
constraints and window sizes for the test corpus of
the BTEC IE task. *Optimized for WER.
straints already with relatively small window sizes.
Increasing the window size beyond 4 for these con-
straints only marginally improves the translation er-
ror measures for both short (under 8 words) and long
sentences. Thus, a suitable language-pair-specific
choice of reordering constraints can avoid the huge
computational complexity required for permutations
of long sentences.
Tab. 2 includes error measures for the best setup
with inverse IBM constraints with window size of 7,
as well as additional improvements obtained by a k-
best list rescoring.
The best settings for reordering constraints and
model scaling factors on the development corpora
were then used to produce translations of the IWSLT
Japanese and Chinese test corpora. These trans-
lations were evaluated against multiple references
which were unknown to the authors. Our system
(denoted with WFST, see Tab. 3) produced results
competitive with the results of the best system at the
evaluation campaign (denoted with AT (Bender et
al., 2004)) and, according to some of the error mea-
sures, even outperformed this system.
5.3.3 Almost Monotonic Translation (IE)
The word order in the Italian language does not
differ much from the English. Therefore, the abso-
lute translation error rates are quite low and translat-
ing without reordering in training and search already
results in a relatively good performance. This is re-
flected in Tab. 4. However, even for this language
pair it is possible to improve translation quality by
performing reordering both in training and during
translation. The best performance on the develop-
ment corpus is obtained when we constrain the re-
odering with relatively small window sizes of 3 to 4
and use either IBM or local reordering constraints.
173
On the test corpus, as shown in Tab. 4, all error mea-
sures can be improved with these settings.
Especially for languages with similar word order
it is important to use weighted reorderings (Sec. 4.6)
in order to prefer the original word order. Introduc-
tion of reordering weights for this task results in no-
table improvement of most error measures using ei-
ther the IBM or local constraints. The optimal prob-
ability ? for the unreordered path was determined
on the development corpus as 0.5 for both of these
constraints. The results on the test corpus using this
setting are also given in Tab. 4.
6 Conclusion
In this paper, we described a reordering framework
which performs source sentence reordering on word
level. We suggested to use optimal alignment func-
tions for monotonization and improvement of trans-
lation model training. This allowed us to translate
monotonically taking a reordering graph as input.
We then described known and novel reordering con-
straints and their efficient finite-state implementa-
tions in which the reordering graph is computed on-
demand. We also utilized weighted permutations.
We showed that our monotonic phrase-based trans-
lation approach effectively makes use of the reorder-
ing framework to produce quality translations even
from languages with significantly different word or-
der. On the Japanese-to-English and Chinese-to-
English IWSLT tasks, our system performed at least
as well as the best machine translation system.
Acknowledgement
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04 Evalu-
ation Campaign. Proc. Int. Workshop on Spoken Lan-
guage Translation, pp. 1?12, Kyoto, Japan.
S. Bangalore and G. Riccardi. 2000. Stochastic Finite-
State Models for Spoken Language Machine Transla-
tion. Proc. Workshop on Embedded Machine Transla-
tion Systems, pp. 52?59.
O. Bender, R. Zens, E. Matusov, and H. Ney. 2004.
Alignment Templates: the RWTH SMT System. Proc.
Int. Workshop on Spoken Language Translation, pp.
79?84, Kyoto, Japan.
A. L. Berger, P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language Translation Apparatus and Method
of Using Context-based Translation Models. United
States Patent 5510981.
F. Casacuberta and E. Vidal. 2004. Machine Transla-
tion with Inferred Stochastic Finite-State Transducers.
Computational Linguistics, vol. 30(2):205-225.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using n-gram Co-Occurrence
Statistics. Proc. Human Language Technology Conf.,
San Diego, CA.
S. Kanthak and H. Ney. 2004. FSA: an Efficient and
Flexible C++ Toolkit for Finite State Automata using
On-demand Computation. Proc. 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pp. 510?517, Barcelona, Spain.
K. Knight and Y. Al-Onaizan. 1998. Translation with
Finite-State Devices. Lecture Notes in Artificial Intel-
ligence, Springer-Verlag, vol. 1529, pp. 421?437.
S. Kumar and W. Byrne. 2003. A Weighted Finite State
Transducer Implementation of the Alignment Template
Model for Statistical Machine Translation. Proc. Hu-
man Language Technology Conf. NAACL, pp. 142?
149, Edmonton, Canada.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word
Alignments for Statistical Machine Translation. Proc.
20th Int. Conf. on Computational Linguistics, pp. 219?
225, Geneva, Switzerland.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, vol. 29, number 1, pp. 19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Machine
Translation. Proc. 40th Annual Meeting of the Associ-
ation for Computational Linguistics, Philadelphia, PA,
pp. 311?318.
J. M. Vilar, 2000. Improve the Learning of Sub-
sequential Transducers by Using Alignments and Dic-
tionaries. Lecture Notes in Artificial Intelligence,
Springer-Verlag, vol. 1891, pp. 298?312.
D. Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
R. Zens, F. J. Och and H. Ney. 2002. Phrase-Based Sta-
tistical Machine Translation. In: M. Jarke, J. Koehler,
G. Lakemeyer (Eds.): KI - Conference on AI, KI 2002,
Vol. LNAI 2479, pp. 18-32, Springer Verlag.
R. Zens and H. Ney. 2003. A Comparative Study on
Reordering Constraints in Statistical Machine Trans-
lation. Proc. Annual Meeting of the Association
for Computational Linguistics, pp. 144?151, Sapporo,
Japan.
174
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 191?198,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Word Graphs for Statistical Machine Translation
Richard Zens and Hermann Ney
Chair of Computer Science VI
RWTH Aachen University
{zens,ney}@cs.rwth-aachen.de
Abstract
Word graphs have various applications in
the field of machine translation. Therefore
it is important for machine translation sys-
tems to produce compact word graphs of
high quality. We will describe the gen-
eration of word graphs for state of the
art phrase-based statistical machine trans-
lation. We will use these word graph
to provide an analysis of the search pro-
cess. We will evaluate the quality of the
word graphs using the well-known graph
word error rate. Additionally, we intro-
duce the two novel graph-to-string crite-
ria: the position-independent graph word
error rate and the graph BLEU score.
Experimental results are presented for two
Chinese?English tasks: the small IWSLT
task and the NIST large data track task.
For both tasks, we achieve significant re-
ductions of the graph error rate already
with compact word graphs.
1 Introduction
A statistical machine translation system usually pro-
duces the single-best translation hypotheses for a
source sentence. For some applications, we are also
interested in alternative translations. The simplest
way to represent these alternatives is a list with the
N -best translation candidates. These N -best lists
have one major disadvantage: the high redundancy.
The translation alternatives may differ only by a sin-
gle word, but still both are listed completely. Usu-
ally, the size of the N -best list is in the range of a few
hundred up to a few thousand candidate translations
per source sentence. If we want to use larger N -best
lists the processing time gets very soon infeasible.
Word graphs are a much more compact represen-
tation that avoid these redundancies as much as pos-
sible. The number of alternatives in a word graph is
usually an order of magnitude larger than in an N -
best list. The graph representation avoids the com-
binatorial explosion that make large N -best lists in-
feasible.
Word graphs are an important data structure with
various applications:
? Word Filter.
The word graph is used as a compact repre-
sentation of a large number of sentences. The
score information is not contained.
? Rescoring.
We can use word graphs for rescoring with
more sophisticated models, e.g. higher-order
language models.
? Discriminative Training.
The training of the model scaling factors as de-
scribed in (Och and Ney, 2002) was done on
N -best lists. Using word graphs instead could
further improve the results. Also, the phrase
translation probabilities could be trained dis-
crimatively, rather than only the scaling factors.
? Confidence Measures.
Word graphs can be used to derive confidence
measures, such as the posterior probability
(Ueffing and Ney, 2004).
191
? Interactive Machine Translation.
Some interactive machine translation systems
make use of word graphs, e.g. (Och et al,
2003).
State Of The Art. Although there are these many
applications, there are only few publications directly
devoted to word graphs. The only publication, we
are aware of, is (Ueffing et al, 2002). The short-
comings of (Ueffing et al, 2002) are:
? They use single-word based models only. Cur-
rent state of the art statistical machine transla-
tion systems are phrase-based.
? Their graph pruning method is suboptimal as it
considers only partial scores and not full path
scores.
? The N -best list extraction does not eliminate
duplicates, i.e. different paths that represent the
same translation candidate.
? The rest cost estimation is not efficient. It has
an exponential worst-case time complexity. We
will describe an algorithm with linear worst-
case complexity.
Apart from (Ueffing et al, 2002), publications on
weighted finite state transducer approaches to ma-
chine translation, e.g. (Bangalore and Riccardi,
2001; Kumar and Byrne, 2003), deal with word
graphs. But to our knowledge, there are no publica-
tions that give a detailed analysis and evaluation of
the quality of word graphs for machine translation.
We will fill this gap and give a systematic descrip-
tion and an assessment of the quality of word graphs
for phrase-based machine translation. We will show
that even for hard tasks with very large vocabulary
and long sentences the graph error rate drops signif-
icantly.
The remaining part is structured as follows: first
we will give a brief description of the translation sys-
tem in Section 2. In Section 3, we will give a def-
inition of word graphs and describe the generation.
We will also present efficient pruning and N -best
list extraction techniques. In Section 4, we will de-
scribe evaluation criteria for word graphs. We will
use the graph word error rate, which is well known
from speech recognition. Additionally, we introduce
the novel position-independent word graph error rate
and the graph BLEU score. These are generaliza-
tions of the commonly used string-to-string evalua-
tion criteria in machine translation. We will present
experimental results in Section 5 for two Chinese?
English tasks: the first one, the IWSLT task, is in the
domain of basic travel expression found in phrase-
books. The vocabulary is limited and the sentences
are short. The second task is the NIST Chinese?
English large data track task. Here, the domain is
news and therefore the vocabulary is very large and
the sentences are with an average of 30 words quite
long.
2 Translation System
In this section, we give a brief description of the
translation system. We use a phrase-based transla-
tion approach as described in (Zens and Ney, 2004).
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a weighted log-linear combination of
a trigram language model and various translation
models: a phrase translation model and a word-
based lexicon model. These translation models are
used for both directions: p(f |e) and p(e|f). Addi-
tionally, we use a word penalty and a phrase penalty.
With the exception of the language model, all mod-
els can be considered as within-phrase models as
they depend only on a single phrase pair, but not on
the context outside of the phrase. The model scaling
factors are optimized with respect to some evalua-
tion criterion (Och, 2003).
We extended the monotone search algorithm from
(Zens and Ney, 2004) such that reorderings are pos-
sible. In our case, we assume that local reorder-
ings are sufficient. Within a certain window, all
possible permutations of the source positions are al-
lowed. These permutations are represented as a re-
ordering graph, similar to (Zens et al, 2002). Once
we have this reordering graph, we perform a mono-
tone phrase-based translation of this graph. More
details of this reordering approach are described in
(Kanthak et al, 2005).
3 Word Graphs
3.1 Definition
A word graph is a directed acyclic graph G = (V, E)
with one designated root node n0 ? V . The edges
are labeled with words and optionally with scores.
We will use (n, n?, w) to denote an edge from node
192
n to node n? with word label w. Each path through
the word graph represents a translation candidate. If
the word graph contains scores, we accumulate the
edge scores along a path to get the sentence or string
score.
The score information the word graph has to con-
tain depends on the application.
If we want to use the word graph as a word fil-
ter, we do not need any score information at all. If
we want to extract the single- or N -best hypotheses,
we have to retain the string or sentence score infor-
mation. The information about the hidden variables
of the search, e.g. the phrase segmentation, is not
needed for this purpose. For discriminative training
of the phrase translation probabilities, we need all
the information, even about the hidden variables.
3.2 Generation
In this section, we analyze the search process in de-
tail. Later, in Section 5, we will show the (experi-
mental) complexity of each step. We start with the
source language sentence that is represented as a lin-
ear graph. Then, we introduce reorderings into this
graph as described in (Kanthak et al, 2005). The
type of reordering should depend on the language
pair. In our case, we assume that only local reorder-
ings are required. Within a certain window, all pos-
sible reorderings of the source positions are allowed.
These permutations are represented as a reordering
graph, similar to (Knight and Al-Onaizan, 1998) and
(Zens et al, 2002).
Once we have this reordering graph, we perform
a monotone phrase-based translation of this graph.
This translation process consists of the following
steps that will be described afterward:
1. segment into phrase
2. translate the individual phrases
3. split the phrases into words
4. apply the language model
Now, we will describe each step. The first step is
the segmentation into phrases. This can be imag-
ined as introducing ?short-cuts? into the graph. The
phrase segmentation does not affect the number of
nodes, because only additional edges are added to
the graph.
In the segmented graph, each edge represents a
source phrase. Now, we replace each edge with one
edge for each possible phrase translation. The edge
scores are the combination of the different transla-
tion probabilities, namely the within-phrase models
mentioned in Section 2. Again, this step does not
increase the number of nodes, but only the number
of edges.
So far, the edge labels of our graph are phrases. In
the final word graph, we want to have words as edge
labels. Therefore, we replace each edge representing
a multi-word target phrase with a sequence of edges
that represent the target word sequence. Obviously,
edges representing a single-word phrase do not have
to be changed.
As we will show in the results section, the word
graphs up to this point are rather compact. The
score information in the word graph so far consists
of the reordering model scores and the phrase trans-
lation model scores. To obtain the sentence posterior
probability p(eI1|fJ1 ), we apply the target language
model. To do this, we have to separate paths accord-
ing to the language model history. This increases the
word graph size by an order of magnitude.
Finally, we have generated a word graph with full
sentence scores. Note that the word graph may con-
tain a word sequence multiple times with different
hidden variables. For instance, two different seg-
mentations into source phrases may result in the
same target sentence translation.
The described steps can be implemented using
weighted finite state transducer, similar to (Kumar
and Byrne, 2003).
3.3 Pruning
To adjust the size of the word graph to the desired
density, we can reduce the word graph size using
forward-backward pruning, which is well-known in
the speech recognition community, e.g. see (Mangu
et al, 2000). This pruning method guarantees that
the good strings (with respect to the model scores)
remain in the word graph, whereas the bad ones are
removed. The important point is that we compare
the full path scores and not only partial scores as, for
instance, in the beam pruning method in (Ueffing et
al., 2002).
The forward probabilities F (n) and backward
probabilities B(n) of a node n are defined by the
193
following recursive equations:
F (n) =
?
(n?,n,w)?E
F (n?) ? p(n?, n, w)
B(n) =
?
(n,n?,w)?E
B(n?) ? p(n, n?, w)
The forward probability of the root node and the
backward probabilities of the final nodes are initial-
ized with one. Using a topological sorting of the
nodes, the forward and backward probabilities can
be computed with linear time complexity. The pos-
terior probability q(n, n?, w) of an edge is defined
as:
q(n, n?, w) = F (n) ? p(n, n
?, w) ? B(n?)
B(n0)
The posterior probability of an edge is identical to
the sum over the probabilities of all full paths that
contain this edge. Note that the backward probabil-
ity of the root node B(n0) is identical to the sum
over all sentence probabilities in the word graph.
Let q? denoted the maximum posterior probability
of all edges and let ? be a pruning threshold, then
we prune an edge (n, n?, w) if:
q(n, n?, w) < q? ? ?
3.4 N -Best List Extraction
In this section, we describe the extraction of the N -
best translation candidates from a word graph.
(Ueffing et al, 2002) and (Mohri and Riley, 2002)
both present an algorithm based on the same idea:
use a modified A* algorithm with an optimal rest
cost estimation. As rest cost estimation, the negated
logarithm of the backward probabilities is used. The
algorithm in (Ueffing et al, 2002) has two disadvan-
tages: it does not care about duplicates and the rest
cost computation is suboptimal as the described al-
gorithm has an exponential worst-case complexity.
As mentioned in the previous section, the backward
probabilities can be computed in linear time.
In (Mohri and Riley, 2002) the word graph is rep-
resented as a weighted finite state automaton. The
word graph is first determinized, i.e. the nondeter-
ministic automaton is transformed in an equivalent
deterministic automaton. This process removes the
duplicates from the word graph. Out of this deter-
minized word graph, the N best candidates are ex-
tracted. In (Mohri and Riley, 2002), ?-transitions are
ignored, i.e. transitions that do not produce a word.
These ?-transitions usually occur in the backing-off
case of language models. The ?-transitions have to
be removed before using the algorithm of (Mohri
and Riley, 2002). In the presence of ?-transitions,
two path representing the same string are considered
equal only if the ?-transitions are identical as well.
4 Evaluation Criteria
4.1 String-To-String Criteria
To evaluate the single-best translation hypotheses,
we use the following string-to-string criteria: word
error rate (WER), position-independent word error
rate (PER) and the BLEU score. More details on
these standard criteria can be found for instance in
(Och, 2003).
4.2 Graph-To-String Criteria
To evaluate the quality of the word graphs, we
generalize the string-to-string criteria to work on
word graphs. We will use the well-known graph
word error rate (GWER), see also (Ueffing et al,
2002). Additionally, we introduce two novel graph-
to-string criteria, namely the position-independent
graph word error rate (GPER) and the graph BLEU
score (GBLEU). The idea of these graph-to-string
criteria is to choose a sequence from the word graph
and compute the corresponding string-to-string cri-
terion for this specific sequence. The choice of the
sequence is such that the criterion is the optimum
over all possible sequences in the word graph, i.e.
the minimum for GWER/GPER and the maximum
for GBLEU.
The GWER is a generalization of the word er-
ror rate. It is a lower bound for the WER. It can be
computed using a dynamic programming algorithm
which is quite similar to the usual edit distance com-
putation. Visiting the nodes of the word graph in
topological order helps to avoid repeated computa-
tions.
The GPER is a generalization of the position-
independent word error rate. It is a lower bound for
the PER. The computation is not as straightforward
as for the GWER.
In (Ueffing and Ney, 2004), a method for com-
puting the string-to-string PER is presented. This
method cannot be generalized for the graph-to-string
computation in a straightforward way. Therefore,
194
we will first describe an alternative computation for
the string-to-string PER and then use this idea for
the graph-to-string PER.
Now, we want to compute the number of position-
independent errors for two strings. As the word or-
der of the strings does not matter, we represent them
as multisets1 A and B. To do this, it is sufficient to
know how many words are in A but not in B, i.e.
a := |A?B|, and how many words are in B but not
in A, i.e. b := |B?A|. The number of substitutions,
insertions and deletions are then:
sub = min{a, b}
ins = a ? sub
del = b ? sub
error = sub + ins + del
= a + b ? min{a, b}
= max{a, b}
It is obvious that there are either no insertions or no
deletions. The PER is then computed as the num-
ber of errors divided by the length of the reference
string.
Now, back to the graph-to-string PER computa-
tion. The information we need at each node of the
word graph are the following: the remaining multi-
set of words of the reference string that are not yet
produced. We denote this multiset C. The cardinal-
ity of this multiset will become the value a in the
preceding notation. In addition to this multiset, we
also need to count the number of words that we have
produced on the way to this node but which are not
in the reference string. The identity of these words is
not important, we simply have to count them. This
count will become the value b in the preceding nota-
tion.
If we make a transition to a successor node along
an edge labeled w, we remove that word w from the
set of remaining reference words C or, if the word
w is not in this set, we increase the count of words
that are in the hypothesis but not in the reference.
To compute the number of errors on a graph, we
use the auxiliary quantity Q(n, C), which is the
count of the produced words that are not in the refer-
ence. We use the following dynamic programming
recursion equations:
1A multiset is a set that may contain elements multiple
times.
Q(n0, C0) = 0
Q(n, C) = min
n?,w:(n?,n,w)?E
{
Q(n?, C ? {w}),
Q(n?, C) + 1
}
Here, n0 denote the root node of the word graph,
C0 denotes the multiset representation of the refer-
ence string. As already mentioned in Section 3.1,
(n?, n, w) denotes an edge from node n? to node n
with word label w.
In the implementation, we use a bit vector to rep-
resent the set C for efficiency reasons. Note that in
the worst-case the size of the Q-table is exponen-
tial in the length of the reference string. However, in
practice we found that in most cases the computation
is quite fast.
The GBLEU score is a generalization of the
BLEU score. It is an upper bound for the BLEU
score. The computation is similar to the GPER com-
putation. We traverse the word graph in topologi-
cal order and store the following information: the
counts of the matching n-grams and the length of the
hypothesis, i.e. the depth in the word graph. Addi-
tionally, we need the multiset of reference n-grams
that are not yet produced.
To compute the BLEU score, the n-gram counts
are collected over the whole test set. This results in
a combinatorial problem for the computation of the
GBLEU score. We process the test set sentence-wise
and accumulate the n-gram counts. After each sen-
tence, we take a greedy decision and choose the n-
gram counts that, if combined with the accumulated
n-gram counts, result is the largest BLEU score.
This gives a conservative approximation of the true
GBLEU score.
4.3 Word Graph Size
To measure the word graph size we use the word
graph density, which we define as the number of
edges in the graph divided by the source sentence
length.
5 Experimental Results
5.1 Tasks
We will show experimental results for two Chinese?
English translation tasks.
195
Table 1: IWSLT Chinese?English Task: corpus
statistics of the bilingual training data.
Chinese English
Train Sentences 20 000
Running Words 182 904 160 523
Vocabulary 7 643 6 982
Test Sentences 506
Running Words 3 515 3 595
avg. SentLen 6.9 7.1
Table 2: NIST Chinese English task: corpus statis-
tics of the bilingual training data.
Chinese English
Train Sentences 3.2M
Running Words 51.4M 55.5M
Vocabulary 80 010 170 758
Lexicon Entries 81 968
Test Sentences 878
Running Words 26 431 23 694
avg. SentLen 30.1 27.0
IWSLT Chinese?English Task. The first task is
the Chinese?English supplied data track task of the
International Workshop on Spoken Language Trans-
lation (IWSLT 2004) (Akiba et al, 2004). The do-
main is travel expressions from phrase-books. This
is a small task with a clean training and test corpus.
The vocabulary is limited and the sentences are rel-
atively short. The corpus statistics are shown in Ta-
ble 1. The Chinese part of this corpus is already
segmented into words.
NIST Chinese?English Task. The second task
is the NIST Chinese?English large data track task.
For this task, there are many bilingual corpora avail-
able. The domain is news, the vocabulary is very
large and the sentences have an average length of 30
words. We train our statistical models on various
corpora provided by LDC. The Chinese part is seg-
mented using the LDC segmentation tool. After the
preprocessing, our training corpus consists of about
three million sentences with somewhat more than 50
million running words. The corpus statistics of the
preprocessed training corpus are shown in Table 2.
We use the NIST 2002 evaluation data as test set.
 15
 20
 25
 30
 35
 40
 45
 50
 0  200  400  600  800  1000  1200
gr
ap
h 
wo
rd
 e
rro
r r
at
e 
[%
]
word graph density
window-size-1
window-size-2
window-size-3
window-size-4
window-size-5
Figure 1: IWSLT Chinese?English: Graph error rate
as a function of the word graph density for different
window sizes.
5.2 Search Space Analysis
In Table 3, we show the search space statistics of the
IWSLT task for different reordering window sizes.
Each line shows the resulting graph densities after
the corresponding step in our search as described in
Section 3.2. Our search process starts with the re-
ordering graph. The segmentation into phrases in-
creases the graph densities by a factor of two. Doing
the phrase translation results in an increase of the
densities by a factor of twenty. Unsegmenting the
phrases, i.e. replacing the phrase edges with a se-
quence of word edges doubles the graph sizes. Ap-
plying the language model results in a significant in-
crease of the word graphs.
Another interesting aspect is that increasing the
window size by one roughly doubles the search
space.
5.3 Word Graph Error Rates
In Figure 1, we show the graph word error rate for
the IWSLT task as a function of the word graph den-
sity. This is done for different window sizes for
the reordering. We see that the curves start with a
single-best word error rate of about 50%. For the
monotone search, the graph word error rate goes
down to about 31%. Using local reordering during
the search, we can further decrease the graph word
error rate down to less than 17% for a window size
of 5. This is almost one third of the single-best word
error rate. If we aim at halving the single-best word
error rate, word graphs with a density of less than
196
Table 3: IWSLT Chinese?English: Word graph densities for different window sizes and different stages of
the search process.
language level graph type window size
1 2 3 4 5
source word reordering 1.0 2.7 6.2 12.8 24.4
phrase segmented 2.0 5.0 12.1 26.8 55.6
target translated 40.8 99.3 229.0 479.9 932.8
word TM scores 78.6 184.6 419.2 869.1 1 670.4
+ LM scores 958.2 2874.2 7649.7 18 029.7 39 030.1
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 0  200  400  600  800  1000  1200  1400
gr
ap
h 
wo
rd
 e
rro
r r
at
e 
[%
]
word graph density
window-size-1
window-size-2
window-size-3
window-size-4
window-size-5
Figure 2: NIST Chinese?English: Graph error rate
as a function of the word graph density for different
window sizes.
200 would already be sufficient.
In Figure 2, we show the same curves for the
NIST task. Here, the curves start from a single-best
word error rate of about 64%. Again, dependent on
the amount of reordering the graph word error rate
goes down to about 36% for the monotone search
and even down to 23% for the search with a window
of size 5. Again, the reduction of the graph word er-
ror rate compare to the single-best error rate is dra-
matic. For comparison we produced an N -best list
of size 10 000. The N -best list error rate (or oracle-
best WER) is still 50.8%. A word graph with a den-
sity of only 8 has about the same GWER.
In Figure 3, we show the graph position-
independent word error rate for the IWSLT task. As
this error criterion ignores the word order it is not
affected by reordering and we show only one curve.
We see that already for small word graph densities
the GPER drops significantly from about 42% down
to less than 14%.
 10
 15
 20
 25
 30
 35
 40
 45
 0  50  100  150  200  250  300  350
po
s.
-in
de
p.
gr
ap
h 
wo
rd
 e
rro
r r
at
e 
[%
]
word graph density
Figure 3: IWSLT Chinese?English: Graph position-
independent word error rate as a function of the
word graph density.
In Figure 4, we show the graph BLEU scores for
the IWSLT task. We observe that, similar to the
GPER, the GBLEU score increases significantly al-
ready for small word graph densities. We attribute
this to the fact that the BLEU score and especially
the PER are less affected by errors of the word or-
der than the WER. This also indicates that produc-
ing translations with correct word order, i.e. syntac-
tically well-formed sentences, is one of the major
problems of current statistical machine translation
systems.
6 Conclusion
We have described word graphs for statistical ma-
chine translation. The generation of word graphs
during the search process has been described in de-
tail. We have shown detailed statistics of the in-
dividual steps of the translation process and have
given insight in the experimental complexity of each
step. We have described an efficient and optimal
197
 30
 35
 40
 45
 50
 55
 60
 65
 70
 0  50  100  150  200  250
gr
ap
h 
BL
EU
 s
co
re
 [%
]
word graph density
window size = 1
window size = 2
window size = 3
window size = 4
window size = 5
Figure 4: IWSLT Chinese?English: Graph BLEU
score as a function of the word graph density.
pruning method for word graphs. Using these tech-
nique, we have generated compact word graphs for
two Chinese?English tasks. For the IWSLT task, the
graph error rate drops from about 50% for the single-
best hypotheses to 17% of the word graph. Even for
the NIST task, with its very large vocabulary and
long sentences, we were able to reduce the graph er-
ror rate significantly from about 64% down to 23%.
Acknowledgment
This work was partly funded by the European Union
under the integrated project TC-Star (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul, and
J. Tsujii. 2004. Overview of the IWSLT04 evaluation cam-
paign. In Proc. of the Int. Workshop on Spoken Language
Translation (IWSLT), pages 1?12, Kyoto, Japan, Septem-
ber/October.
S. Bangalore and G. Riccardi. 2001. A finite-state approach to
machine translation. In Proc. Conf. of the North American
Association of Computational Linguistics (NAACL), Pitts-
burgh, May.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts: Data-Driven Machine Translation and
Beyond, Ann Arbor, MI, June.
K. Knight and Y. Al-Onaizan. 1998. Translation with finite-
state devices. In D. Farwell, L. Gerber, and E. H. Hovy,
editors, AMTA, volume 1529 of Lecture Notes in Computer
Science, pages 421?437. Springer Verlag.
S. Kumar and W. Byrne. 2003. A weighted finite state trans-
ducer implementation of the alignment template model for
statistical machine translation. In Proc. of the Human Lan-
guage Technology Conf. (HLT-NAACL), pages 63?70, Ed-
monton, Canada, May/June.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: Word error minimization and other
applications of confusion networks. Computer, Speech and
Language, 14(4):373?400, October.
M. Mohri and M. Riley. 2002. An efficient algorithm for the n-
best-strings problem. In Proc. of the 7th Int. Conf. on Spoken
Language Processing (ICSLP?02), pages 1313?1316, Den-
ver, CO, September.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302, Philadelphia,
PA, July.
F. J. Och, R. Zens, and H. Ney. 2003. Efficient search for in-
teractive statistical machine translation. In EACL03: 10th
Conf. of the Europ. Chapter of the Association for Com-
putational Linguistics, pages 387?393, Budapest, Hungary,
April.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of the 41th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
160?167, Sapporo, Japan, July.
N. Ueffing and H. Ney. 2004. Bayes decision rule and
confidence measures for statistical machine translation. In
Proc. EsTAL - Espan?a for Natural Language Processing,
pages 70?81, Alicante, Spain, October.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word
graphs in statistical machine translation. In Proc. of the
Conf. on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 156?163, Philadelphia, PA, July.
R. Zens and H. Ney. 2004. Improvements in phrase-based
statistical machine translation. In Proc. of the Human
Language Technology Conf. (HLT-NAACL), pages 257?264,
Boston, MA, May.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical
machine translation. In M. Jarke, J. Koehler, and G. Lake-
meyer, editors, 25th German Conf. on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artificial Intel-
ligence (LNAI), pages 18?32, Aachen, Germany, September.
Springer Verlag.
198
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 17?24, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Preprocessing and Normalization
for Automatic Evaluation of Machine Translation
Gregor Leusch and Nicola Ueffing and David Vilar and Hermann Ney
Lehrstuhl fu?r Informatik VI
RWTH Aachen University
D-52056 Aachen, Germany,
{leusch,ueffing,vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
Evaluation measures for machine trans-
lation depend on several common meth-
ods, such as preprocessing, tokenization,
handling of sentence boundaries, and the
choice of a reference length. In this
paper, we describe and review some
new approaches to them and compare
these to state-of-the-art methods. We
experimentally look into their impact on
four established evaluation measures. For
this purpose, we study the correlation
between automatic and human evaluation
scores on three MT evaluation corpora.
These experiments confirm that the to-
kenization method, the reference length
selection scheme, and the use of sentence
boundaries we introduce will increase the
correlation between automatic and human
evaluation scores. We find that ignoring
case information and normalizing evalu-
ator scores has a positive effect on the
sentence level correlation as well.
1 Introduction
Machine translation (MT), as any other natural lan-
guage processing (NLP) research subject, depends
on the evaluation of its results. Unfortunately,
human evaluation of MT system output is a time
consuming and expensive task. This is why auto-
matic evaluation is preferred to human evaluation in
the research community.
Over the last years, a manifold of automatic evalu-
ation measures has been proposed and studied. This
underlines the importance, but also the complexity
of finding a suitable evaluation measure for MT.
We will give a short overview of some measures in
section 2 of this paper.
Although most of these measures share similar
ideas and foundation, we observe that researchers
tend to approach problems common to several
measures differently from each other. A noteworthy
example here is the determination of a translation
reference length.
In section 3, we will have a look onto structural
similarities and differences among several measures,
focussing on common steps. We will show that
decisions taken about them can be as important to
the outcome of an evaluation, as the choice of the
evaluation measure itself.
To this end, we will study the performance
of each error measure and setting by comparison
with human evaluation on three different evaluation
tasks in section 4. These experiments will show
that sophisticated tokenization as well as adding
sentence boundaries and a good choice for the
reference lengths will improve correlation between
automatic and human evaluation significantly. Case
normalization and evaluator normalization are help-
ful only when evaluating on sentence level; system
level evaluation is not affected by these methods.
After a discussion of these results in section 5, we
will conclude this paper in section 6.
2 Automatic evaluation measures
The majority of MT evaluation approaches are based
on the distance or similarity of MT candidate output
to a set of reference translations, i.e. to sentences
which are known to be correct. The lower this
distance is, or the higher the similarity, the better the
17
candidate translations are considered to be, and thus
the better the MT system.
2.1 Evaluation measures studied
Out of the vast amount of measures, we will focus
on the following measures that are widely used in
research and in evaluation campaigns: WER, PER,
BLEU, and NIST.
Let a test set consist of k = 1, . . . ,K candidate
sentences Ek generated by an MT system. For
each candidate sentence Ek, we have a set of r =
1, . . . , Rk reference sentences E?r,k. Let Ik denote
the length, and I?k the reference length for each
sentence Ek. We will explain in section 3.3 how the
reference length is calculated.
With this, we write the total candidate length over
the corpus as I? :=
?
k Ik, and the total reference
length as I?? :=
?
k I
?
k .
Let nem1 ,k denote the count of the m-gram e
m
1
within the candidate sentence Ek; similarly let
n?em1 ,r,k denote the same count within the reference
sentence E?r,k. The total m-gram count over the
corpus is then n?m :=
?
k
?
em1 ?Ek
nem1 ,k.
2.1.1 WER
The word error rate is defined as the Levenshtein
distance dL(Ek, E?r,k) between a candidate sentence
Ek and a reference sentence E?r,k, divided by the
reference length I?k for normalization.
For a whole candidate corpus with multiple
references, we define the WER to be:
WER :=
1
I??
?
k
min
r
dL
(
Ek, E?r,k
)
Note that the WER of a single sentence can be
calculated as the WER for a corpus of size K = 1.
2.1.2 PER
The position independent error rate (Tillmann et
al., 1997) ignores the ordering of the words within
a sentence. Independent of the word position, the
minimum number of deletions, insertions, and
substitutions to transform the candidate sentence
into the reference sentence is calculated. Using
the counts ne,r, n?e,r,k of a word e in the candidate
sentence Ek, and the reference sentence E?r,k, we
can calculate this distance as
dPER
(
Ek, E?r,k
)
:=
1
2
(?
?Ik?I?k
?
?+
?
e
?
?ne,k?n?e,r,k
?
?
)
This distance is then normalized into an error rate,
the PER, as described in section 2.1.1.
A promising approach is to compare bigram or
arbitrary m-gram count vectors instead of unigram
count vectors only. This will take into account the
ordering of the words within a sentence implicitly,
although not as strong as the WER does.
2.1.3 BLEU
BLEU (Papineni et al, 2001) is a precision
measure based on m-gram count vectors. The
precision is modified such that multiple references
are combined into a single m-gram count vector,
n?e,k := maxr n?e,r,k. Multiple occurrences of an
m-gram in the candidate sentence are counted as
correct only up to the maximum occurrence count
within the reference sentences. Typically, m =
1, . . . , 4.
To avoid a bias towards short candidate sentences
consisting of ?safe guesses? only, sentences shorter
than the reference length will be penalized with a
brevity penalty.
BLEU := lpBLEU ? gm
m
{
1
sm+n?m
?
(
sm+
?
k
?
em1 ?Ek
min
(
nem1 ,k , n?em1 ,k
))
}
with the geometric mean gm and a brevity penalty
lpBLEU := min
(
1 , exp
(
1 ?
I??
I?
))
In the original BLEU definition, the smoothing
term sm is zero. To allow for sentence-wise
evaluation, Lin and Och (2004) define the BLEU-S
measure with s1 := 1 and sm>1 := 0. We have
adopted this technique for this study.
2.1.4 NIST
The NIST score (Doddington, 2002) extends
the BLEU score by taking information weights of
the m-grams into account. The NIST information
weight is defined as
Info(em1 ) := ?
(
log2 ??nem1 ? log2
??nem?11
)
with ??nem1 :=
?
k,r
n?en1 ,k,r.
Note that the weight of a phrase occurring
in many references sentence for a candidate is
considered to be lower than the weight of a phrase
occurring only once!
18
The NIST score is the sum over all information
counts of the co-occurring m-grams, summed up
separately for each m = 1, . . . , 5 and normalized
by the total m-gram count.
NIST := lpNIST ?
?
m
(
1
n?m
?
?
k
?
em1 ?Ek
min
(
nem1 ,k , n?em1 ,k
)
? Info(em1 )
)
As in BLEU, there is a brevity penalty to avoid a
bias towards short candidates:
lpNIST := exp
(
? ? log22 min
(
1 ,
I?
I??
))
where ? := ? log2 2
log22 3
Due to the information weights, the value of the
NIST score depends highly on the selection of the
reference corpus. This must be taken into account
when comparing NIST scores of different evaluation
campaigns.
2.2 Other measures
Lin and Och (2004) introduce a family of three
measures named ROUGE. ROUGE-S is a skip-
bigram F-measure. ROUGE-L and ROUGE-W are
measures based on the length of the longest common
subsequence of the sentences. ROUGE-S has a
structure similar to the bigram PER presented here.
We expect ROUGE-L and ROUGE-W to have similar
properties to WER.
In (Leusch et al, 2003), we have described
INVWER, a word error rate enhanced by block
transposition edit operations. As structure and
scores of INVWER are similar to WER, we have
omitted INVWER experiments in this paper.
3 Preprocessing and normalization
Although the general idea is clear, there are still
several details to be specified when implementing
and using an automatic evaluation measure. We are
going to investigate the following problems:
The first detail we have to state more precisely is
the term ?word? in the above formulae. A common
approach for western languages is to consider spaces
as separators of words. The role of punctuation
marks in tokenization is arguable though. A
punctuation mark can separate words, it can be part
of a word, and it can be a word of its own. Equally
it can be irrelevant at all for evaluation.
On the same lines it is to be specified whether
we consider words to be equal if they differ only
with respect to upper and lower case. For the
IWSLT evaluation, (Paul et al, 2004) give an
introduction to how the handling of punctuation
and case information may affect automatic MT
evaluation.
Also, a method to calculate the ?reference
length? must specified if there are multiple reference
sentences of different length.
Since we want to compare automatic evaluation
with human evaluation, we have to clarify some
questions about assessing human evaluation as well:
Large evaluation tasks are usually distributed to
several human evaluators. To smooth evaluation
noise, it is common practice to have each candidate
sentence evaluated by at least two human judges in-
dependently. Therefore there are several evaluation
scores for each candidate sentence. We require a
single score for each system, though. Consequently,
we have to specify how to combine the evaluator
scores into sentence scores and then the sentence
scores into a system score.
Different definitions of this will have a significant
impact on automatic and human evaluation scores.
3.1 Tokenization and punctuation
The importance of punctuation as well as the
strictness of punctuation rules depends on the
language. In most western languages, correct
punctuation can vastly improve the legibility of
texts. Marks like full stop or comma separate words.
Other marks like apostrophes and hyphens can be
used to join words, forming new words by this. For
example, the spelling ?There?s? is a contraction of
?There is?.
Similar phenomena can be found in other lan-
guages, although the set of critical characters may
vary. Even when evaluating English translations, the
candidate sentences may contain source language
parts like proper names which should thus be treated
according to the source language.
From the viewpoint of an automatic evaluation
measure, we have to decide which units we would
consider to be words of their own.
We have studied four tokenization methods. The
simplest method is keeping the original sentences,
and considering only spaces as word separators.
Moreover, we can consider all punctuation marks to
separate words but remove them completely then.
The mteval tool (Papineni, 2002) improves this
19
Table 1: Tokenization methods studied
? Original candidate
Powell said: "We?d not be
alone; that?s for sure."
? Remove punctuation
Powell said We d not be alone
that s for sure
? Tokenization of punctuation (mteval)
Powell said : " We?d not be
alone ; that?s for sure . "
? Tokenization and treatment of abbreviations
and contractions
Powell said : " we would not be
alone ; that is for sure . "
scheme by keeping all punctuation marks as separate
words except for decimal points and hyphens
joining composita. We have extended this scheme
by implementing a treatment of common English
contractions. Table 1 illustrates these methods.
3.2 Case sensitivity
In western languages, maintaining correct upper
and lower case can improve the readability of a
text. Unfortunately, though the case of a word
depends on the word class, classification is not
always unambiguous. What is more, the first word
in a sentence is always written in upper case. This
lowers the significance of case information in MT
evaluation, as even a valid reordering of words
between candidate and reference sentence may lead
to conflicting cases. Consequently, we investigated
if and how case information can be exploited for
automatic evaluation.
3.3 Reference length
Each automatic evaluation measure we have taken
into account depends on the calculation of a refer-
ence length: WER, PER, and ROUGE are normalized
by it, whereas NIST or BLEU incorporate it for
the determination of the brevity penalty. In MT
evaluation practise, there are multiple reference
sentences for each candidate sentence, with different
lengths each. It is thus not intuitively clear what the
?reference length? is.
A simple choice here is the average length of the
reference sentences. Though this is modus operandi
for NIST, it is problematic with brevity penalty or F-
measure based scores, as even candidate sentences
that are identical to a shorter-than-average reference
sentence ? which we would intuitively consider to be
?optimal? ? will then receive a sub-optimal score.
BLEU incorporates a different method for the
determination of the reference length in its default
implementation: Reference length here is the
reference sentence length which is closest to the
candidate length. If there is more than one the
shortest of them is chosen.
For measures based on the comparison of single
sentences such as WER, PER, and ROUGE, at least
two more methods deserve consideration:
? The average length of the sentences with the
lowest absolute distance or highest similarity
to the candidate sentence. We call this method
?average nearest-sentence length?.
? The length of the sentence with the lowest
relative error rate or the highest relative
similarity. We call this method ?best length?.
Note that when using this method, not the
minimum absolute distance is used for the
error rate, but the distance that leads to
minimum relative error.
Other strategies studied by us, e.g. minimum
length of the reference sentences, did not show
any theoretical or experimental advantage over the
methods mentioned here. Thus we will not discuss
them in this paper.
3.4 Sentence boundaries
The position of a word within a sentence can be quite
significant for the correctness of the sentence.
WER, INVWER, and ROUGE-L take into account
the ordering explicitly. This is not the case with n-
PER, BLEU, or NIST, although the positions of inner
words are regarded implicitly by m-gram overlap.
To model the position of words at the initial or the
end of a sentence, one can enclose the sentence with
artificial sentence boundary words. Although this
is a common approach in language modelling, it
has to our knowledge not yet been applied to MT
evaluation.
3.5 Evaluator normalization
For human evaluation, it has to be specified how to
handle evaluator bias, and how to combine sentence
scores into system scores.
Regarding evaluator bias, even accurate evalua-
tion guidelines will not prevent a measurable dis-
crepancy between the scores assigned by different
human evaluators.
The 2003 TIDES/MT evaluation may serve as
an example here: Since the candidate sentences of
20
54321
0.0
0.2
0.4
0.6
0.8
1.0
Rel
ativ
e as
sess
men
t co
unt
Evaluator
Figure 1: Distribution of adequacy assessments for
each human evaluator. TIDES CE corpus.
the participating systems were randomly distributed
among ten human evaluators, one would expect the
assessed scores to be independent of the evaluator.
Figure 1 indicates that this is indeed not the case,
as the evaluators can clearly be distinguished by the
amount of good and bad marks they assessed.
(0, 1) evaluator normalization overcomes this
bias: For each human evaluator the average sentence
score given by him or her and its variance are
calculated. These assignments are then normalized
to (0, 1) expectation and standard deviation (Dod-
dington, 2003), separately for each evaluator.
Evaluator normalization should be unnecessary
for system evaluation, as the evaluator biases
tend to cancel out over the large amount of
candidate sentences if the alignment of evaluators
and systems is random enough. Moreover, with
(0, 1) normalization the calculated system scores are
relative, not absolute scores. As such they can only
be compared with scores out of the same evaluation.
Whereas the assessments by the human evaluators
are given on the sentence level, our interest may
lie on the evaluation of whole candidate systems.
Depending on the number of assessments per
candidate sentence, different combination methods
for the sentence scores can be considered for this,
e.g. mean or median. As our data consisted only
of two or three human assessments per sentence, we
have only applied the mean in our experiments.
It has to be defined how a system score is
calculated from the sentence scores. All of the
automatic evaluation measures implicitly weight the
candidate sentences by their length. Consequently,
we applied for the human evaluation scores a
weighting by length on sentence level as well.
Table 2: Corpus statistics
TIDES CE TIDES AE BTEC CE
Source language Chinese Arabic Chinese
Target language English English English
Sentences 919 663 500
Running words 25784 17763 3632
Punctuation marks 3760 2698 610
Ref. translations 4 4 16
Avg. ref. length 28.1 26.8 7.3
Candidate systems 7 6 11
4 Experimental results
To assess the impact of the mentioned preprocessing
steps, we calculated scores for several automatic
evaluation measures with varying preprocessing,
reference length calculation, etc. on three eval-
uation test sets from international MT evaluation
campaigns. We then compared these automatic eval-
uation results with human evaluation of adequacy
and fluency by determining a correlation coefficient
between human and automatic evaluation. We
chose Pearson?s r for this. Although all evaluation
measures were calculated using length weighting,
we did not do any weighting when calculating the
sentence level correlation.
Regarding the m-gram PER, we had studied m-
gram lengths of up to 8 both separately and in com-
bination with shorter m-gram lengths in previous
experiments. However, an m-gram length of greater
than 4 did not show noteworthy correlation. For this,
we will leave out these results in this paper.
For the sake of clarity, we will also leave
out measures that behave very similarly to akin
measures e.g. INVWER and WER, 2-PER and 1-
PER, or BLEU and BLEU-S.
Since WER and PER are error measures, whereas
BLEU and NIST are similarity measures, the
correlation coefficients with human evaluation will
have opposite signs. For convenience, we will look
at the absolute coefficients only.
4.1 Corpora
From the 2003 TIDES evaluation campaign we
included both the Chinese-English and the Arabic-
English test corpus in our experiments. Both were
provided with adequacy and fluency scores between
1 and 5 for seven and six candidate sets respectively.
As we wanted to perform experiments on a corpus
with a larger amount of MT systems, we also
included the IWSLT BTEC 2004 Chinese-English
21
evaluation (Akiba et al, 2004). We restricted our
experiments to the eleven MT systems that had been
trained on a common training corpus.
Corpus statistics can be found in table 2.
4.2 Experimental baseline
In our first experiment we studied the correlation
of the different evaluation measures with human
evaluation at ?baseline? conditions. These included
no sentence boundaries, but tokenization with
treatment of abbreviations, see table 1. For
sentence evaluation, conditions included evaluator
normalization. Case information was removed. We
used these settings in the other experiments, too, if
not stated otherwise.
Figure 2 shows the correlation between automatic
and human scores. On the TIDES corpora the
system level correlation is particularly high, at a
moderate sentence level correlation. We assume
the latter is due to the poor sentence inter-annotator
agreement on these corpora, which is then smoothed
out on system level. On the BTEC corpus
a high sentence level correlation accompanies a
significantly lower system level correlation. Note
that due to the much lower number of samples on
the system level (e.g. 5 vs. 5500), small changes
in the sentence level correlation are more likely to
be significant than such changes on system level.
We have verified these effects by inspecting the rank
correlation on both levels, as well as by experiments
on other corpora. Although these experiments
support our findings, we have omitted results here
WERPER BLEUSNIST l AdequacyFluency
TIDESCE TIDESAE BTECCE0.0
0.2
0.4
0.6
l l l l
l l l l
l
l
l
l
TIDESCE TIDESAE BTECCE0.0
0.20
.40.6
0.81
.0
l l l l
l l l l l
l
l
l
Figure 2: Pearson correlation coefficient between
automatic and human evaluation. Bars indicate
correlation with adequacy, circles with fluency
score.
Left: sentence, right: system level correlation.
W WERP PER B BLEUS ll no normalizationnormalization
0.0
0.2
0.4
0.6
l l l l l
l l l l l l l
TIDESCE TIDESAE
W P B W P B
0.00
.20.4
0.60
.81.0 l l l l l l l l l l l l
TIDESCE TIDESAE
W P B W P B
Figure 3: Effect of evaluator normalization.
Left: sentence, right: system level correlation.
W WERP PER B BLEUS ll use caseignore case
0.0
0.2
0.4
0.6
ll ll ll
ll ll l
l l
l
l
l l
l
TIDESCE TIDESAE BTECCE
W P B W P B W P B
0.00
.20.4
0.60
.81.0 l
l
ll
ll
ll ll ll ll
ll
ll
TIDESCE TIDESAE BTECCE
W P B W P B W P B
Figure 4: Effect of case normalization.
Left: sentence, right: system level correlation.
for the sake of clarity.
4.3 Evaluator normalization
We studied the effect of (0, 1)-normalization of
scores assigned by human evaluators. The NIST
measure showed a behavior very similar to that of
the other measures and is thus left out in the graph.
The correlation of all automatic measures both with
fluency and with adequacy increases significantly
at sentence level (figure 3). We do not notice a
positive effect on system level, which confirms the
assumption stated in section 3.5.
4.4 Tokenization and case normalization
The impact of case information was analyzed in our
next experiment. Figure 4 (again without the NIST
measure as it shows a similar behavior to the other
measures) indicates that it is advisable to disregard
case information when looking into adequacy on
sentence level. Surprisingly, this also holds for
22
W WERB BLEUS l AdequacyFluency ll llkeepremove tokenizetok+treat.
0.0
0.2
0.4
0.6
llll llll lll
l llll
TIDESCE TIDESAE
W B W B
0.00
.20.4
0.60
.81.0 llll llll llll llll
TIDESCE TIDESAE
W B W B
Figure 5: Effect of different tokenization steps.
Left: sentence, right: system level correlation.
fluency. We do no find a clear tendency on whether
or not to regard case information at system level.
Figure 5 indicates that the way of handling
punctuation we proposed does pay off when eval-
uating adequacy. For fluency our results were
contradictory: A slight decrease on the Arabic-
English corpus is accompanied by a slight decay on
the Chinese-English corpus. We did not investigate
the BTEC corpus here as most systems sticked to the
tokenization guidelines for this evaluation.
4.5 Reference length
The dependency of evaluation measures on the
selection of reference lengths is rarely covered in
the literature. However, as we can see in figure 6,
our experiments indicate a significant impact. The
selected three methods here are the default for
WER/PER, NIST, and BLEU, respectively. For the
distance based evaluation measures, represented by
W WERB BLEUS N NIST ll laveragenearest best
0.0
0.2
0.4
0.6
lllll
 
ll
 
lllll
 
ll
 
lllll
 
l
l
 TIDESCE TIDESAE BTECCE
W B N W B N W B N
0.00
.20.
40.6
0.81
.0
lllll
 
ll
 
lllll
 
ll
 
lllll
 
l
l
 TIDESCE TIDESAE BTECCE
W B N W B N W B N
Figure 6: Effect of different reference lengths.
Left: sentence, right: system level correlation.
P 2PERB BLEUS N NIST ll llnoneinitial endboth
0.0
0.2
0.4
0.6
llllllllllll lll
lllllllll
TIDESCE TIDESAE
P B N P B N
0.00
.20.4
0.60
.81.0 llllllllllll llllllllllll
TIDESCE TIDESAE
P B N P B N
Figure 7: Effect of sentence boundaries.
Left: sentence, right: system level correlation.
WER here, taking the length of the sentence leading
to the best score leads to the best correlation with
both fluency and adequacy. Taking the average
length instead seems to be the worst choice.
For brevity penalty based measures, the effect is
not as clear: On both TIDES corpora there is no
significant difference in correlation between using
the average length and the nearest length. On
the BTEC corpus, choosing the nearest sentence
length leads to a significantly higher correlation than
choosing the average length. We assume this is due
to the high number of reference sentences on this
corpus.
4.6 Sentence boundaries
As sentence boundaries will only influence m-gram
count vector based measures, we have restricted
our experiments to bigram PER, BLEU-S, and NIST
here. Including sentence boundaries (figure 7)
has a positive effect on correlation with fluency
and adequacy for both bigram PER and BLEU-S.
Sentence initials seem to be more important than
sentence ends here. For the NIST measure, we do
not find any significant effect.
5 Discussion
In a perfect MT world, any dependency of an
evaluation on case information or tokenization
should be inexistent, as MT systems already have
to deal with both in the translation process, and
could be designed to produce output according to
evaluation campaign guidelines. Once all translation
systems stick to the same specifications, no further
preprocessing steps should be necessary.
In practice there will be some systems that step
23
out of line. If we then choose strict rules regarding
case information and punctuation, automatic error
measures will penalize these systems rather hard,
whereas penalty is rather low if we choose lax ones.
In this situation case information will have a
large effect on the correlation between automatic
and human evaluation, depending on whether the
involved candidate systems will have a good or a bad
human evaluation. It is vital to keep this in mind
when drawing conclusions here regarding system
evaluation, despite the obvious importance of case
information in natural languages.
These considerations also hold for the treatment
of punctuation marks, as a special care should be
unnecessary if all systems sticked to tokenization
specifications. In practise, MT systems differ
in the way they generate and handle punctuation
marks. Therefore, appropriate preprocessing steps
are advisable.
Our experiments suggest that sentence boundaries
increase correlation between automatic scores and
adequacy both on sentence and on system level.
For fluency, the improvement is less significant, and
mainly depends on the sentence initials.
For length penalty based measures, we have found
that choosing the nearest sentence length yields the
highest correlation with human evaluation. For
distance based measures instead, it seems advisable
to choose the sentence that leads to the best relative
score as the one that determines the reference length.
6 Conclusion
We have described several MT evaluation measures.
We have pointed out common preprocessing steps
and auxiliary methods which have not been studied
in detail so far in spite of their importance for
the MT evaluation process. Particularly, we have
introduced a novel method for determining the
reference length of an evaluation candidate sentence,
and a simple method to incorporate sentence
boundary information to m-gram based evaluation
measures.
We then have performed several experiments
on these methods on three evaluation corpora.
The results indicate that both our new reference
length algorithm and the use of sentence boundaries
improve the correlation of the studied automatic
evaluation measures with human evaluation. Fur-
thermore, we have learned that case information
should be removed when performing automatic
sentence evaluation. On sentence level, evaluator
normalization can improve the correlation between
automatic and human evaluation.
Acknowledgements
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04
evaluation campaign. In Proc. IWSLT, pp. 1?12,
Kyoto, Japan, September.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
G. Doddington. 2003. NIST MT Evaluation Workshop.
Personal communication, July.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Proc. MT Summit
IX, pp. 240?247, New Orleans, LA, September.
C. Y. Lin and F. J. Och. 2004. Orange: a method for
evaluation automatic evaluation metrics for machine
translation. In Proc. COLING 2004, pp. 501?507,
Geneva, Switzerland, August.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
K. A. Papineni. 2002. The NIST mteval scor-
ing software. http://www.itl.nist.gov/iad/
894.01/tests/mt/resources/scoring.htm.
M. Paul, H. Nakaiwa, and M. Federico. 2004. Towards
innovative evaluation methodologies for speech trans-
lation. In Working Notes of the NTCIR-4 Meeting,
volume 2, pp. 17?21.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based search for
statistical translation. In European Conf. on Speech
Communication and Technology, pp. 2667?2670,
Rhodes, Greece, September.
24
Reranking Translation Hypotheses Using Structural Properties
Sas?a Hasan, Oliver Bender, Hermann Ney
Chair of Computer Science VI
RWTH Aachen University
D-52056 Aachen, Germany
{hasan,bender,ney}@cs.rwth-aachen.de
Abstract
We investigate methods that add syntac-
tically motivated features to a statistical
machine translation system in a reranking
framework. The goal is to analyze whether
shallow parsing techniques help in iden-
tifying ungrammatical hypotheses. We
show that improvements are possible by
utilizing supertagging, lightweight depen-
dency analysis, a link grammar parser and
a maximum-entropy based chunk parser.
Adding features to n-best lists and dis-
criminatively training the system on a de-
velopment set increases the BLEU score
up to 0.7% on the test set.
1 Introduction
Statistically driven machine translation systems
are currently the dominant type of system in the
MT community. Though much better than tradi-
tional rule-based approaches, these systems still
make a lot of errors that seem, at least from a hu-
man point of view, illogical.
The main purpose of this paper is to investigate
a means of identifying ungrammatical hypotheses
from the output of a machine translation system
by using grammatical knowledge that expresses
syntactic dependencies of words or word groups.
We introduce several methods that try to establish
this kind of linkage between the words of a hy-
pothesis and, thus, determine its well-formedness,
or ?fluency?. We perform rescoring experiments
that rerank n-best lists according to the presented
framework.
As methodologies deriving well-formedness of
a sentence we use supertagging (Bangalore and
Joshi, 1999) with lightweight dependency anal-
ysis (LDA)1 (Bangalore, 2000), link grammars
(Sleator and Temperley, 1993) and a maximum-
entropy (ME) based chunk parser (Bender et al,
2003). The former two approaches explicitly
model the syntactic dependencies between words.
Each hypothesis that contains irregularities, such
as broken linkages or non-satisfied dependencies,
should be penalized or rejected accordingly. For
the ME chunker, the idea is to train n-gram mod-
els on the chunk or POS sequences and directly
use the log-probability as feature score.
In general, these concepts and the underlying
programs should be robust and fast in order to be
able to cope with large amounts of data (as it is the
case for n-best lists). The experiments presented
show a small though consistent improvement in
terms of automatic evaluation measures chosen for
evaluation. BLEU score improvements, for in-
stance, lie in the range from 0.3 to 0.7% on the
test set.
In the following, Section 2 gives an overview
on related work in this domain. In Section 3
we review our general approach to statistical ma-
chine translation (SMT) and introduce the main
methodologies used for deriving syntactic depen-
dencies on words or word groups, namely su-
pertagging/LDA, link grammars and ME chunk-
ing. The corpora and the experiments are dis-
cussed in Section 4. The paper is concluded in
Section 5.
2 Related work
In (Och et al, 2004), the effects of integrating
syntactic structure into a state-of-the-art statistical
machine translation system are investigated. The
approach is similar to the approach presented here:
1In the context of this work, the term LDA is not to be
confused with linear discriminant analysis.
41
firstly, a word graph is generated using the base-
line SMT system and n-best lists are extracted ac-
cordingly, then additional feature functions repre-
senting syntactic knowledge are added and the cor-
responding scaling factors are trained discrimina-
tively on a development n-best list.
Och and colleagues investigated a large amount
of different feature functions. The field of appli-
cation varies from simple syntactic features, such
as IBM model 1 score, over shallow parsing tech-
niques to more complex methods using grammars
and intricate parsing procedures. The results were
rather disappointing. Only one of the simplest
models, i.e. the implicit syntactic feature derived
from IBM model 1 score, yielded consistent and
significant improvements. All other methods had
only a very small effect on the overall perfor-
mance.
3 Framework
In the following sections, the theoretical frame-
work of statistical machine translation using a di-
rect approach is reviewed. We introduce the su-
pertagging and lightweight dependency analysis
approach, link grammars and maximum-entropy
based chunking technique.
3.1 Direct approach to SMT
In statistical machine translation, the best trans-
lation e?I?1 = e?1 . . . e?i . . . e?I? of source words fJ1 =
f1 . . . fj . . . fJ is obtained by maximizing the con-
ditional probability
e?I?1 = argmax
I,eI1
{Pr(eI1|fJ1 )}
= argmax
I,eI1
{Pr(fJ1 |eI1) ? Pr(eI1)}
(1)
using Bayes decision rule. The first probability
on the right-hand side of the equation denotes the
translation model whereas the second is the target
language model.
An alternative to this classical source-channel
approach is the direct modeling of the posterior
probability Pr(eI1|fJ1 ) which is utilized here. Us-
ing a log-linear model (Och and Ney, 2002), we
obtain
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
) ,
(2)
where ?m are the scaling factors of the models de-
noted by feature functions hm(?). The denomina-
tor represents a normalization factor that depends
only on the source sentence fJ1 . Therefore, we can
omit it during the search process, leading to the
following decision rule:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(3)
This approach is a generalization of the source-
channel approach. It has the advantage that ad-
ditional models h(?) can be easily integrated into
the overall system. The model scaling factors
?M1 are trained according to the maximum en-
tropy principle, e.g., using the GIS algorithm. Al-
ternatively, one can train them with respect to
the final translation quality measured by an error
criterion (Och, 2003). For the results reported
in this paper, we optimized the scaling factors
with respect to a linear interpolation of word error
rate (WER), position-independent word error rate
(PER), BLEU and NIST score using the Downhill
Simplex algorithm (Press et al, 2002).
3.2 Supertagging/LDA
Supertagging (Bangalore and Joshi, 1999) uses the
Lexicalized Tree Adjoining Grammar formalism
(LTAG) (XTAG Research Group, 2001). Tree Ad-
joining Grammars incorporate a tree-rewriting for-
malism using elementary trees that can be com-
bined by two operations, namely substitution and
adjunction, to derive more complex tree structures
of the sentence considered. Lexicalization allows
us to associate each elementary tree with a lexical
item called the anchor. In LTAGs, every elemen-
tary tree has such a lexical anchor, also called head
word. It is possible that there is more than one el-
ementary structure associated with a lexical item,
as e.g. for the case of verbs with different subcat-
egorization frames.
The elementary structures, called initial and
auxiliary trees, hold all dependent elements within
the same structure, thus imposing constraints on
the lexical anchors in a local context. Basically,
supertagging is very similar to part-of-speech tag-
ging. Instead of POS tags, richer descriptions,
namely the elementary structures of LTAGs, are
annotated to the words of a sentence. For this pur-
pose, they are called supertags in order to distin-
guish them from ordinary POS tags. The result
is an ?almost parse? because of the dependencies
42
very[?2]
food[?1] delicious[?3]
the[?1]
was[?2]
Figure 1: LDA: example of a derivation tree, ?
nodes are the result of the adjunction operation on
auxiliary trees, ? nodes of substitution on initial
trees.
coded within the supertags. Usually, a lexical item
can have many supertags, depending on the vari-
ous contexts it appears in. Therefore, the local am-
biguity is larger than for the case of POS tags. An
LTAG parser for this scenario can be very slow, i.e.
its computational complexity is in O(n6), because
of the large number of supertags, i.e. elementary
trees, that have to be examined during a parse. In
order to speed up the parsing process, we can ap-
ply n-gram models on a supertag basis in order to
filter out incompatible descriptions and thus im-
prove the performance of the parser. In (Banga-
lore and Joshi, 1999), a trigram supertagger with
smoothing and back-off is reported that achieves
an accuracy of 92.2% when trained on one million
running words.
There is another aspect to the dependencies
coded in the elementary structures. We can use
them to actually derive a shallow parse of the sen-
tence in linear time. The procedure is presented
in (Bangalore, 2000) and is called lightweight de-
pendency analysis. The concept is comparable to
chunking. The lightweight dependency analyzer
(LDA) finds the arguments for the encoded depen-
dency requirements. There exist two types of slots
that can be filled. On the one hand, nodes marked
for substitution (in ?-trees) have to be filled by the
complements of the lexical anchor. On the other
hand, the foot nodes (i.e. nodes marked for adjunc-
tion in ?-trees) take words that are being modified
by the supertag. Figure 1 shows a tree derived by
LDA on the sentence the food was very delicious
from the C-Star?03 corpus (cf. Section 4.1).
The supertagging and LDA tools are available
from the XTAG research group website.2
As features considered for the reranking exper-
iments we choose:
2http://www.cis.upenn.edu/?xtag/
D D EA EA
P P
SS
the food very deliciouswas
Figure 2: Link grammar: example of a valid link-
age satisfying all constraints.
? Supertagger output: directly use the log-
likelihoods as feature score. This did not im-
prove performance significantly, so the model
was discarded from the final system.
? LDA output:
? dependency coverage: determine the
number of covered elements, i.e. where
the dependency slots are filled to the left
and right
? separate features for the number of mod-
ifiers and complements determined by
the LDA
3.3 Link grammar
Similar to the ideas presented in the previous sec-
tion, link grammars also explicitly code depen-
dencies between words (Sleator and Temperley,
1993). These dependencies are called links which
reflect the local requirements of each word. Sev-
eral constraints have to be satisfied within the link
grammar formalism to derive correct linkages, i.e.
sets of links, of a sequence of words:
1. Planarity: links are not allowed to cross each
other
2. Connectivity: links suffice to connect all
words of a sentence
3. Satisfaction: linking requirements of each
word are satisfied
An example of a valid linkage is shown in Fig-
ure 2. The link grammar parser that we use is
freely available from the authors? website.3 Sim-
ilar to LTAG, the link grammar formalism is lex-
icalized which allows for enhancing the methods
with probabilistic n-gram models (as is also the
case for supertagging). In (Lafferty et al, 1992),
the link grammar is used to derive a new class of
3http://www.link.cs.cmu.edu/link/
43
[NP the food ] [VP was] [ADJP very delicious]
the/DT food/NN was/VBD very/RB delicious/JJ
Figure 3: Chunking and POS tagging: a tag next
to the opening bracket denotes the type of chunk,
whereas the corresponding POS tag is given after
the word.
language models that, in comparison to traditional
n-gram LMs, incorporate capabilities for express-
ing long-range dependencies between words.
The link grammar dictionary that specifies the
words and their corresponding valid links cur-
rently holds approximately 60 000 entries and han-
dles a wide variety of phenomena in English. It is
derived from newspaper texts.
Within our reranking framework, we use link
grammar features that express a possible well-
formedness of the translation hypothesis. The sim-
plest feature is a binary one stating whether the
link grammar parser could derive a complete link-
age or not, which should be a strong indicator of
a syntactically correct sentence. Additionally, we
added a normalized cost of the matching process
which turned out not to be very helpful for rescor-
ing, so it was discarded.
3.4 ME chunking
Like the methods described in the two preced-
ing sections, text chunking consists of dividing a
text into syntactically correlated non-overlapping
groups of words. Figure 3 shows again our ex-
ample sentence illustrating this task. Chunks are
represented as groups of words between square
brackets. We employ the 11 chunk types as de-
fined for the CoNLL-2000 shared task (Tjong Kim
Sang and Buchholz, 2000).
For the experiments, we apply a maximum-
entropy based tagger which has been successfully
evaluated on natural language understanding and
named entity recognition (Bender et al, 2003).
Within this tool, we directly factorize the poste-
rior probability and determine the corresponding
chunk tag for each word of an input sequence. We
assume that the decisions depend only on a lim-
ited window ei+2i?2 = ei?2...ei+2 around the current
word ei and on the two predecessor chunk tags
ci?1i?2. In addition, part-of-speech (POS) tags gI1
are assigned and incorporated into the model (cf.
Figure 3). Thus, we obtain the following second-
order model:
Pr(cI1|eI1, gI1) =
=
I
?
i=1
Pr(ci|ci?11 , eI1, gI1) (4)
=
I
?
i=1
p(ci|ci?1i?2, ei+2i?2, gi+2i?2), (5)
where the step from Eq. 4 to 5 reflects our model
assumptions.
Furthermore, we have implemented a set of bi-
nary valued feature functions for our system, in-
cluding lexical, word and transition features, prior
features, and compound features, cf. (Bender et
al., 2003). We run simple count-based feature
reduction and train the model parameters using
the Generalized Iterative Scaling (GIS) algorithm
(Darroch and Ratcliff, 1972). In practice, the
training procedure tends to result in an overfitted
model. To avoid this, a smoothing method is ap-
plied where a Gaussian prior on the parameters is
assumed (Chen and Rosenfeld, 1999).
Within our reranking framework, we firstly use
the ME based tagger to produce the POS and
chunk sequences for the different n-best list hy-
potheses. Given several n-gram models trained on
the WSJ corpus for both POS and chunk models,
we then rescore the n-best hypotheses and simply
use the log-probabilities as additional features. In
order to adapt our system to the characteristics of
the data used, we build POS and chunk n-gram
models on the training corpus part. These domain-
specific models are also added to the n-best lists.
The ME chunking approach does not model ex-
plicit syntactic linkages of words. Instead, it in-
corporates a statistical framework to exploit valid
and syntactically coherent groups of words by ad-
ditionally looking at the word classes.
4 Experiments
For the experiments, we use the translation sys-
tem described in (Zens et al, 2005). Our phrase-
based decoder uses several models during search
that are interpolated in a log-linear way (as ex-
pressed in Eq. 3), such as phrase-based translation
models, word-based lexicon models, a language,
deletion and simple reordering model and word
and phrase penalties. A word graph containing
the most likely translation hypotheses is generated
during the search process. Out of this compact
44
Supplied Data Track
Arabic Chinese Japanese English
Train Sentences 20 000
Running Words 180 075 176 199 198 453 189 927
Vocabulary 15 371 8 687 9 277 6 870
Singletons 8 319 4 006 4 431 2 888
C-Star?03 Sentences 506
Running Words 3 552 3 630 4 130 3 823
OOVs (Running Words) 133 114 61 65
IWSLT?04 Sentences 500
Running Words 3 597 3 681 4 131 3 837
OOVs (Running Words) 142 83 71 58
Table 1: Corpus statistics after preprocessing.
representation, we extract n-best lists as described
in (Zens and Ney, 2005). These n-best lists serve
as a starting point for our experiments. The meth-
ods presented in Section 3 produce scores that are
used as additional features for the n-best lists.
4.1 Corpora
The experiments are carried out on a subset
of the Basic Travel Expression Corpus (BTEC)
(Takezawa et al, 2002), as it is used for the sup-
plied data track condition of the IWSLT evaluation
campaign. BTEC is a multilingual speech corpus
which contains tourism-related sentences similar
to those that are found in phrase books. For the
supplied data track, the training corpus contains
20 000 sentences. Two test sets, C-Star?03 and
IWSLT?04, are available for the language pairs
Arabic-English, Chinese-English and Japanese-
English.
The corpus statistics are shown in Table 1. The
average source sentence length is between seven
and eight words for all languages. So the task is
rather limited and very domain-specific. The ad-
vantage is that many different reranking experi-
ments with varying feature function settings can
be carried out easily and quickly in order to ana-
lyze the effects of the different models.
In the following, we use the C-Star?03 set for
development and tuning of the system?s parame-
ters. After that, the IWSLT?04 set is used as a
blind test set in order to measure the performance
of the models.
4.2 Rescoring experiments
The use of n-best lists in machine translation has
several advantages. It alleviates the effects of the
huge search space which is represented in word
graphs by using a compact excerpt of the n best
hypotheses generated by the system. Especially
for limited domain tasks, the size of the n-best list
can be rather small but still yield good oracle er-
ror rates. Empirically, n-best lists should have an
appropriate size such that the oracle error rate, i.e.
the error rate of the best hypothesis with respect to
an error measure (such asWER or PER) is approx-
imately half the baseline error rate of the system.
N -best lists are suitable for easily applying several
rescoring techniques since the hypotheses are al-
ready fully generated. In comparison, word graph
rescoring techniques need specialized tools which
can traverse the graph accordingly. Since a node
within a word graph allows for many histories, one
can only apply local rescoring techniques, whereas
for n-best lists, techniques can be used that con-
sider properties of the whole sentence.
For the Chinese-English and Arabic-English
task, we set the n-best list size to n = 1500. For
Japanese-English, n = 1000 produces oracle er-
ror rates that are deemed to be sufficiently low,
namely 17.7% and 14.8% for WER and PER, re-
spectively. The single-best output for Japanese-
English has a word error rate of 33.3% and
position-independent word error rate of 25.9%.
For the experiments, we add additional fea-
tures to the initial models of our decoder that have
shown to be particularly useful in the past, such as
IBM model 1 score, a clustered language model
score and a word penalty that prevents the hy-
potheses to become too short. A detailed defini-
tion of these additional features is given in (Zens
et al, 2005). Thus, the baseline we start with is
45
Chinese ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 8.17 46.2 48.6 41.4
with supertagging/LDA 8.29 46.5 48.4 41.0
with link grammar 8.43 45.6 47.9 41.1
with supertagging/LDA + link grammar 8.22 47.5 47.7 40.8
with ME chunker 8.65 47.3 47.4 40.4
with all models 8.42 47.0 47.4 40.5
Chinese ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 8.67 45.5 49.1 39.8
with supertagging/LDA 8.68 45.4 49.8 40.3
with link grammar 8.81 45.0 49.0 40.2
with supertagging/LDA+link grammar 8.56 46.0 49.1 40.6
with ME chunker 9.00 44.6 49.3 40.6
with all models 8.89 46.2 48.1 39.6
Table 2: Effect of successively adding syntactic features to the Chinese-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
BASE Any messages for me?
RESC Do you have any messages for me?
REFE Do you have any messages for me?
BASE She, not yet?
RESC She has not come yet?
REFE Lenny, she has not come in?
BASE How much is it to the?
RESC How much is it to the local call?
REFE How much is it to the city centre?
BASE This blot or.
RESC This is not clean.
REFE This still is not clean.
Table 3: Translation examples for the Chinese-
English test set (IWSLT?04): baseline system
(BASE) vs. rescored hypotheses (RESC) and refer-
ence translation (REFE).
already a very strong one. The log-linear inter-
polation weights ?m from Eq. 3 are directly opti-
mized using the Downhill Simplex algorithm on a
linear combination of WER (word error rate), PER
(position-independent word error rate), NIST and
BLEU score.
In Table 2, we show the effect of adding the
presented features successively to the baseline.
Separate entries for experiments using supertag-
ging/LDA and link grammars show that a combi-
nation of these syntactic approaches always yields
some gain in translation quality (regarding BLEU
score). The performance of the maximum-entropy
based chunking is comparable. A combination of
all three models still yields a small improvement.
Table 3 shows some examples for the Chinese-
English test set. The rescored translations are syn-
tactically coherent, though semantical correctness
cannot be guaranteed. On the test data, we achieve
an overall improvement of 0.7%, 0.5% and 0.3%
in BLEU score for Chinese-English, Japanese-
English and Arabic-English, respectively (cf. Ta-
bles 4 and 5).
4.3 Discussion
From the tables, it can be seen that the use of
syntactically motivated feature functions within
a reranking concept helps to slightly reduce the
number of translation errors of the overall trans-
lation system. Although the improvement on the
IWSLT?04 set is only moderate, the results are
nevertheless comparable or better to the ones from
(Och et al, 2004), where, starting from IBM
model 1 baseline, an additional improvement of
only 0.4% BLEU was achieved using more com-
plex methods.
For the maximum-entropy based chunking ap-
proach, n-grams with n = 4 work best for the
chunker that is trained on WSJ data. The domain-
specific rescoring model which results from the
chunker being trained on the BTEC corpora turns
out to prefer higher order n-grams, with n = 6 or
more. This might be an indicator of the domain-
specific rescoring model successfully capturing
more local context.
The training of the other models, i.e. supertag-
ging/LDA and link grammar, is also performed on
46
Japanese ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.09 57.8 31.3 25.0
with supertagging/LDA 9.13 57.8 31.3 24.8
with link grammar 9.46 57.6 31.9 25.3
with supertagging/LDA + link grammar 9.24 58.2 31.0 24.8
with ME chunker 9.31 58.7 30.9 24.4
with all models 9.21 58.9 30.5 24.3
Japanese ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.22 54.7 34.1 25.5
with supertagging/LDA 9.27 54.8 34.2 25.6
with link grammar 9.37 54.9 34.3 25.9
with supertagging/LDA + link grammar 9.30 55.0 34.0 25.6
with ME chunker 9.27 55.0 34.2 25.5
with all models 9.27 55.2 33.9 25.5
Table 4: Effect of successively adding syntactic features to the Japanese-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
Arabic ? English, C-Star?03 NIST BLEU[%] mWER[%] mPER[%]
Baseline 10.18 64.3 23.9 20.6
with supertagging/LDA 10.13 64.6 23.4 20.1
with link grammar 10.06 64.7 23.4 20.3
with supertagging/LDA + link grammar 10.20 65.0 23.2 20.2
with ME chunker 10.11 65.1 23.0 19.9
with all models 10.23 65.2 23.0 19.9
Arabic ? English, IWSLT?04 NIST BLEU[%] mWER[%] mPER[%]
Baseline 9.75 59.8 26.1 21.9
with supertagging/LDA 9.77 60.5 25.6 21.5
with link grammar 9.74 60.5 25.9 21.7
with supertagging/LDA + link grammar 9.86 60.8 26.0 21.6
with ME chunker 9.71 59.9 25.9 21.8
with all models 9.84 60.1 26.4 21.9
Table 5: Effect of successively adding syntactic features to the Arabic-English n-best list for C-Star?03
(development set) and IWSLT?04 (test set).
out-of-domain data. Thus, further improvements
should be possible if the models were adapted to
the BTEC domain. This would require the prepa-
ration of an annotated corpus for the supertagger
and a specialized link grammar, which are both
time-consuming tasks.
The syntactically motivated methods (supertag-
ging/LDA and link grammars) perform similarly
to the maximum-entropy based chunker. It seems
that both approaches successfully exploit struc-
tural properties of language. However, one outlier
is ME chunking on the Chinese-English test data,
where we observe a lower BLEU but a larger NIST
score. For Arabic-English, the combination of all
methods does not seem to generalize well on the
test set. In that case, supertagging/LDA and link
grammar outperforms the ME chunker: the over-
all improvement is 1% absolute in terms of BLEU
score.
5 Conclusion
We added syntactically motivated features to a sta-
tistical machine translation system in a rerank-
ing framework. The goal was to analyze whether
shallow parsing techniques help in identifying un-
grammatical hypotheses. We showed that some
improvements are possible by utilizing supertag-
ging, lightweight dependency analysis, a link
47
grammar parser and a maximum-entropy based
chunk parser. Adding features to n-best lists and
discriminatively training the system on a develop-
ment set helped to gain up to 0.7% in BLEU score
on the test set.
Future work could include developing an
adapted LTAG for the BTEC domain or incor-
porating n-gram models into the link grammar
concept in order to derive a long-range language
model (Lafferty et al, 1992). However, we feel
that the current improvements are not significant
enough to justify these efforts. Additionally, we
will apply these reranking methods to larger cor-
pora in order to study the effects on longer sen-
tences from more complex domains.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-Star (Technology and Corpora for Speech
to Speech Translation, IST-2002-FP6-506738,
http://www.tc-star.org), and by the R&D project
TRAMES managed by Bertin Technologies as
prime contractor and operated by the french DGA
(De?le?gation Ge?ne?rale pour l?Armement).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore. 2000. A lightweight dependency
analyzer for partial parsing. Computational Linguis-
tics, 6(2):113?138.
Oliver Bender, Klaus Macherey, Franz Josef Och, and
Hermann Ney. 2003. Comparison of alignment
templates and maximum entropy models for natural
language understanding. In EACL03: 10th Conf. of
the Europ. Chapter of the Association for Computa-
tional Linguistics, pages 11?18, Budapest, Hungary,
April.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMUCS-99-108, Carnegie Mellon
University, Pittsburgh, PA.
J. N. Darroch and D. Ratcliff. 1972. Generalized iter-
ative scaling for log-linear models. Annals of Math-
ematical Statistics, 43:1470?1480.
John Lafferty, Daniel Sleator, and Davy Temperley.
1992. Grammatical trigrams: A probabilistic model
of link grammar. In Proc. of the AAAI Fall Sympo-
sium on Probabilistic Approaches to Natural Lan-
guage, pages 89?97, Cambridge, MA.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 295?302, Philadelphia, PA,
July.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proc. 2004 Meeting of the North
American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), pages 161?168,
Boston, MA.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Daniel Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Third International
Workshop on Parsing Technologies, Tilburg/Durbuy,
The Netherlands/Belgium, August.
Toshiyuki Takezawa, Eiichiro Sumita, F. Sugaya,
H. Yamamoto, and S. Yamamoto. 2002. Toward
a broad-coverage bilingual corpus for speech trans-
lation of travel conversations in the real world. In
Proc. of the Third Int. Conf. on Language Resources
and Evaluation (LREC), pages 147?152, Las Pal-
mas, Spain, May.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared
task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, pages 127?132, Lisbon, Portugal,
September.
XTAG Research Group. 2001. A Lexicalized Tree
Adjoining Grammar for English. Technical Re-
port IRCS-01-03, IRCS, University of Pennsylvania,
Philadelphia, PA, USA.
Richard Zens and Hermann Ney. 2005. Word graphs
for statistical machine translation. In 43rd Annual
Meeting of the Assoc. for Computational Linguis-
tics: Proc. Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 191?198, Ann Arbor, MI, June.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based
statistical machine translation system. In Proceed-
ings of the International Workshop on Spoken Lan-
guage Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
48
Proceedings of the Workshop on Statistical Machine Translation, pages 1?6,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
Maja Popovic??
Hermann Ney?
Adria` de Gispert?
Jose? B. Marin?o?
Deepa Gupta?
Marcello Federico?
Patrik Lambert?
Rafael Banchs?
? Lehrstuhl fu?r Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
? TALP Research Center, Universitat Polite`cnica de Catalunya (UPC), Barcelona, Spain
? ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
Abstract
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
1 Introduction
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al, 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al, 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
2 Related Work
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al, 2002; Babych and Hartley, 2004; Ma-
tusov et al, 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nie?en et al, 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
1
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nie?en and Ney, 2000; Nie?en
and Ney, 2001a; Nie?en and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovic? et al, 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovic? and Ney, 2004; de
Gispert et al, 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
3 Morpho-syntactic Information and
Automatic Evaluation
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
? syntactic differences between the two lan-
guages considering nouns and adjectives
? inflections in the Spanish language considering
mainly verbs, adjectives and nouns
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
3.1 Syntactic differences
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
3.2 Spanish inflections
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
4 Experimental Settings
4.1 Task and Corpus
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al, 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
2
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
4.2 Translation System
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al, 2005; Zens et al, 2005).
4.3 Experiments
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovic? and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
Spanish?English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English?Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
Table 2: Translation Results [%]
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ?more important?) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ?economic and political?) are treated as
an adjective group. Standard translation results are
presented in Table 2.
5 Error Analysis
5.1 Syntactic errors
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
3
English output 1? PERWER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output 1? PERWER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
Table 3: Relative difference between PER and
WER [%] for different word classes
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
Table 4: PER [%] for different word classes
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
?export control = control de exportacio?n?.
5.2 Inflectional errors
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al, 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
4
Spanish output 1? PERbPERf
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
6 Conclusion
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al, 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
Acknowledgements
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
References
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, pages 65?72,
Ann Arbor, MI, June.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185?
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
5
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148?154, Pitts-
burgh, PA, October.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Sonja Nie?en and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247?252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nie?en and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47?54, Toulouse, France, July.
Sonja Nie?en, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39?45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2004. Towards the use
of word stems & suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585?1588, Lissabon,
Portugal, May.
Maja Popovic? and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran ?Saric?. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian?English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41?48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Sas?a Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155?162, Pittsburgh, PA, October.
6
Proceedings of the Workshop on Statistical Machine Translation, pages 15?22,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Arabic Preprocessing for Arabic-to-English Statistical
Machine Translation
Anas El Isbihani Shahram Khadivi Oliver Bender
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{isbihani,khadivi,bender,ney}@informatik.rwth-aachen.de
Hermann Ney
Abstract
The Arabic language has far richer sys-
tems of inflection and derivation than En-
glish which has very little morphology.
This morphology difference causes a large
gap between the vocabulary sizes in any
given parallel training corpus. Segmen-
tation of inflected Arabic words is a way
to smooth its highly morphological na-
ture. In this paper, we describe some
statistically and linguistically motivated
methods for Arabic word segmentation.
Then, we show the efficiency of proposed
methods on the Arabic-English BTEC and
NIST tasks.
1 Introduction
Arabic is a highly inflected language compared to
English which has very little morphology. This mor-
phological richness makes statistical machine trans-
lation from Arabic to English a challenging task. A
usual phenomenon in Arabic is the attachment of a
group of words which are semantically dependent on
each other. For instance, prepositions like ?and? and
?then? are usually attached to the next word. This
applies also to the definite article ?the?. In addi-
tion, personal pronouns are attached to the end of
verbs, whereas possessive pronouns are attached to
the end of the previous word, which constitutes the
possessed object. Hence, an Arabic word can be de-
composed into ?prefixes, stem and suffixes?. We re-
strict the set of prefixes and suffixes to those showed
in Table 1 and 2, where each of the prefixes and suf-
fixes has at least one meaning which can be repre-
sented by a single word in the target language. Some
prefixes can be combined. For example the word
wbAlqlm (????AK. ? which means ?and with the pen?)
has a prefix which is a combination of three pre-
fixes, namely w, b and Al. The suffixes we handle
in this paper can not be combined with each other.
Thus, the compound word pattern handled here is
?prefixes-stem-suffix?.
All possible prefix combinations that do not con-
tain Al allow the stem to have a suffix. Note that
there are other suffixes that are not handled here,
such as At ( H@), An ( 	?@) and wn ( 	??) which make
the plural form of a word. The reason why we omit
them is that they do not have their own meaning. The
impact of Arabic morphology is that the vocabulary
size and the number of singletons can be dramati-
cally high, i.e. the Arabic words are not seen often
enough to be learned by statistical machine transla-
tion models. This can lead to an inefficient align-
ment.
In order to deal with this problem and to improve
the performance of statistical machine translation,
each word must be decomposed into its parts. In
(Larkey et al, 2002) it was already shown that word
segmentation for Arabic improves information re-
trieval. In (Lee et al, 2003) a statistical approach
for Arabic word segmentation was presented. It de-
composes each word into a sequence of morphemes
(prefixes-stem-suffixes), where all possible prefixes
and suffixes (not only those we described in Table 1
and 2) are split from the original word. A compa-
rable work was done by (Diab et al, 2004), where
a POS tagging method for Arabic is also discussed.
As we have access to this tool, we test its impact
on the performance of our translation system. In
15
Table 1: Prefixes handled in this work and their meanings.
Prefix ? 	? ? ? H. ?@
Transliteration w f k l b Al
Meaning and and then as, like in order to with, in the
(Habash and Rambow, 2005) a morphology analyzer
was used for the segementation and POS tagging. In
contrast to the methods mentioned above, our seg-
mentation method is unsupervised and rule based.
In this paper we first explain our statistical ma-
chine translation (SMT) system used for testing the
impact of the different segmentation methods, then
we introduce some preprocessing and normalization
tools for Arabic and explain the linguistic motiva-
tion beyond them. Afterwards, we present three
word segmentation methods, a supervised learning
approach, a finite state automaton-based segmenta-
tion, and a frequency-based method. In Section 5,
the experimental results are presented. Finally, the
paper is summarized in Section 6 .
2 Baseline SMT System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
} (1)
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(2)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be eas-
ily integrated into the overall system. The model
scaling factors ?M1 are trained with respect to the fi-
nal translation quality measured by an error criterion
(Och, 2003).
We use a state-of-the-art phrase-based translation
system including the following models: an n-gram
language model, a phrase translation model and a
word-based lexicon model. The latter two mod-
els are used for both directions: p(f |e) and p(e|f).
Additionally, we use a word penalty and a phrase
penalty. More details about the baseline system can
be found in (Zens and Ney, 2004; Zens et al, 2005).
3 Preprocessing and Normalization Tools
3.1 Tokenizer
As for other languages, the corpora must be first to-
kenized. Here words and punctuations (except ab-
breviation) must be separated. Another criterion is
that Arabic has some characters that appear only at
the end of a word. We use this criterion to separate
words that are wrongly attached to each other.
3.2 Normalization and Simplification
The Arabic written language does not contain vow-
els, instead diacritics are used to define the pronun-
ciation of a word, where a diacritic is written under
or above each character in the word. Usually these
diacritics are omitted, which increases the ambigu-
ity of a word. In this case, resolving the ambiguity
of a word is only dependent on the context. Some-
times, the authors write a diacritic on a word to help
the reader and give him a hint which word is really
meant. As a result, a single word with the same
meaning can be written in different ways. For exam-
ple $Eb (I. ? ?) can be read1 as sha?ab (Eng. nation)
or sho?ab (Eng. options). If the author wants to give
the reader a hint that the second word is meant, he
1There are other possible pronunciations for the word $Eb
than the two mentioned.
16
Table 2: Suffixes handled in this work and their meanings.
Suffix ?


?


	
G ? 	?? , ?? , A??
Transliteration y ny k kmA, km, kn
Meaning my me you, your (sing.) you, your (pl.)
Suffix A 	K ? A? 	?? , ?? , A??
Transliteration nA h hA hmA, hm, hn
Meaning us, our his, him her them, their
can write $uEb (I. ?

?) or $uEab (I. ?

?). To avoid
this problem we normalize the text by removing all
diacritics.
After segmenting the text, the size of the sen-
tences increases rapidly, where the number of the
stripped article Al is very high. Not every article in
an Arabic sentence matches to an article in the target
language. One of the reasons is that the adjective in
Arabic gets an article if the word it describes is def-
inite. So, if a word has the prefix Al, then its adjec-
tive will also have Al as a prefix. In order to reduce
the sentence size we decide to remove all these arti-
cles that are supposed to be attached to an adjective.
Another way for determiner deletion is described in
(Lee, 2004).
4 Word Segmentation
One way to simplify inflected Arabic text for a SMT
system is to split the words in prefixes, stem and
suffixes. In (Lee et al, 2003), (Diab et al, 2004)
and (Habash and Rambow, 2005) three supervised
segmentation methods are introduced. However, in
these works the impact of the segmentation on the
translation quality is not studied. In the next subsec-
tions we will shortly describe the method of (Diab et
al., 2004). Then we present our unsupervised meth-
ods.
4.1 Supervised Learning Approach (SL)
(Diab et al, 2004) propose solutions to word seg-
mentation and POS Tagging of Arabic text. For the
purpose of training the Arabic TreeBank is used,
which is an Arabic corpus containing news articles
of the newswire agency AFP. In the first step the text
must be transliterated to the Buckwalter translitera-
tion, which is a one-to-one mapping to ASCII char-
acters. In the second step it will be segmented and
tokenized. In the third step a partial lemmatization is
done. Finally a POS tagging is performed. We will
test the impact of the step 3 (segmentation + lemma-
tization) on the translation quality using our phrase
based system described in Section 2.
4.2 Frequency-Based Approach (FB)
We provide a set of all prefixes and suffixes and
their possible combinations. Based on this set, we
may have different splitting points for a given com-
pound word. We decide whether and where to split
the composite word based on the frequency of dif-
ferent resulting stems and on the frequency of the
compound word, e.g. if the compound word has a
higher frequency than all possible stems, it will not
be split. This simple heuristic harmonizes the cor-
pus by reducing the size of vocabulary, singletons
and also unseen words from the test corpus. This
method is very similar to the method used for split-
ting German compound words (Koehn and Knight,
2003).
4.3 Finite State Automaton-Based Approach
(FSA)
To segment Arabic words into prefixes, stem and one
suffix, we implemented two finite state automata.
One for stripping the prefixes and the other for the
suffixes. Then, we append the suffix automaton to
the other one for stripping prefixes. Figure 1 shows
the finite state automaton for stripping all possible
prefix combinations. We add the prefix s (?), which
changes the verb tense to the future, to the set of
prefixes which must be stripped (see table 1). This
prefix can only be combined with w and f. Our mo-
tivation is that the future tense in English is built by
adding the separate word ?will?.
The automaton showed in Figure 1 consists of the
following states:
? S: the starting point of the automaton.
? E: tne end state, which can only be achieved if
17
S K
AL
B
L
WF
C
E
Figure 1: Finite state automaton for stripping pre-
fixes off Arabic words.
the resulting stem exists already in the text.
? WF: is achieved if the word begins with w or f.
? And the states , K, L, B and AL are achieved if
the word begins with s, k, l, b and Al, respec-
tively.
To minimize the number of wrong segmentations,
we restricted the transition from one state to the
other to the condition that the produced stem occurs
at least one time in the corpus. To ensure that most
compound words are recognized and segmented, we
run the segmenter itteratively, where after each it-
eration the newly generated words are added to the
vocabulary. This will enable recognizing new com-
pound words in the next iteration. Experiments
showed that running the segmenter twice is suffi-
cient and in higher iterations most of the added seg-
mentations are wrong.
4.4 Improved Finite State Automaton-Based
Approach (IFSA)
Although we restricted the finite state segmenter in
such a way that words will be segmented only if the
yielded stem already exists in the corpus, we still get
some wrongly segmented words. Thus, some new
stems, which do not make sense in Arabic, occur
in the segmented text. Another problem is that the
finite state segmenter does not care about ambigui-
ties and splits everything it recognizes. For example
let us examine the word frd (XQ 	?). In one case, the
character f is an original one and therefore can not
be segmented. In this case the word means ?per-
son?. In the other case, the word can be segmented
to ?f rd? (which means ?and then he answers? or
?and then an answer?). If the words Alfrd, frd and
rd(XQ 	? , XQ 	?? @ and XP) occur in the corpus, then the fi-
nite state segmenter will transform the Alfrd (which
means ?the person?) to Al f rd (which can be trans-
lated to ?the and then he answers?). Thus the mean-
ing of the original word is distorted. To solve all
these problems, we improved the last approach in a
way that prefixes and suffixes are recognized simul-
taneously. The segmentation of the ambiguous word
will be avoided. In doing that, we intend to postpone
resolving such ambiguities to our SMT system.
The question now is how can we avoid the seg-
mentation of ambiguous words. To do this, it is suf-
ficient to find a word that contains the prefix as an
original character. In the last example the word Al-
frd contains the prefix f as an original character and
therefore only Al can be stripped off the word. The
next question we can ask is, how can we decide if a
character belongs to the word or is a prefix. We can
extract this information using the invalid prefix com-
binations. For example Al is always the last prefix
that can occur. Therefore all characters that occur in
a word after Al are original characters. This method
can be applied for all invalid combinations to extract
new rules to decide whether a character in a word is
an original one or not.
On the other side, all suffixes we handle in this
work are pronouns. Therefore it is not possible to
combine them as a suffix. We use this fact to make
a decision whether the end characters in a word are
original or can be stripped. For example the word
trkhm (???QK) means ?he lets them?. If we suppose
that hm is a suffix and therefore must be stripped,
then we can conclude that k is an original character
and not a suffix. In this way we are able to extract
from the corpus itself decisions whether and how a
word can be segmented.
In order to implement these changes the original
automaton was modified. Instead of splitting a word
we mark it with some properties which corespond
to the states traversed untill the end state. On the
18
other side, we use the technique described above to
generate negative properties which avoid the corre-
sponding kind of splitting. If a property and its nega-
tion belong to the same word then the property is re-
moved and only the negation is considered. At the
end each word is split corresponding to the proper-
ties it is marked with.
5 Experimental Results
5.1 Corpus Statistics
The experiments were carried out on two tasks: the
corpora of the Arabic-English NIST task, which
contain news articles and UN reports, and the
Arabic-English corpus of the Basic Travel Expres-
sion Corpus (BTEC) task, which consists of typi-
cal travel domain phrases (Takezawa et al, 2002).
The corpus statistics of the NIST and BTEC corpora
are shown in Table 3 and 5. The statistics of the
news part of NIST corpus, consisting of the Ummah,
ATB, ANEWS1 and eTIRR corpora, is shown in Ta-
ble 4. In the NIST task, we make use of the NIST
2002 evaluation set as a development set and NIST
2004 evaluation set as a test set. Because the test
set contains four references for each senence we de-
cided to use only the first four references of the de-
velopment set for the optimization and evaluation.
In the BTEC task, C-Star?03 and IWSLT?04 copora
are considered as development and test sets, respec-
tively.
5.2 Evaluation Metrics
The commonly used criteria to evaluate the trans-
lation results in the machine translation commu-
nity are: WER (word error rate), PER (position-
independent word error rate), BLEU (Papineni et
al., 2002), and NIST (Doddington, 2002). The four
criteria are computed with respect to multiple ref-
erences. The number of reference translations per
source sentence varies from 4 to 16 references. The
evaluation is case-insensitive for BTEC and case-
sensitive for NIST task. As the BLEU and NIST
scores measure accuracy, higher scores are better.
5.3 Translation Results
To study the impact of different segmentation meth-
ods on the translation quality, we apply different
word segmentation methods to the Arabic part of the
BTEC and NIST corpora. Then, we make use of the
phrase-based machine translation system to translate
the development and test sets for each task.
First, we discuss the experimental results on the
BTEC task. In Table 6, the translation results on the
BTEC corpus are shown. The first row of the table is
the baseline system where none of the segmentation
methods is used. All segmentation methods improve
the baseline system, except the SL segmentation
method on the development corpus. The best per-
forming segmentation method is IFSA which gener-
ates the best translation results based on all evalua-
tion criteria, and it is consistent over both develop-
ment and evaluation sets. As we see, the segmen-
tation of Arabic words has a noticeable impact in
improving the translation quality on a small corpus.
To study the impact of word segmentation meth-
ods on a large task, we conduct two sets of experi-
ments on the NIST task using two different amounts
of the training corpus: only news corpora, and full
corpus. In Table 7, the translation results on the
NIST task are shown when just the news corpora
were used to train the machine translation models.
As the results show, except for the FB method, all
segmentation methods improve the baseline system.
For the NIST task, the SL method outperforms the
other segmentation methods, while it did not achieve
good results when comparing to the other methods
in the BTEC task.
We see that the SL, FSA and IFSA segmentation
methods consistently improve the translation results
in the BTEC and NIST tasks, but the FB method
failed on the NIST task, which has a larger training
corpus . The next step is to study the impact of the
segmentation methods on a very large task, the NIST
full corpus. Unfortunately, the SL method failed on
segmenting the large UN corpus, due to the large
processing time that it needs. Due to the negative
results of the FB method on the NIST news corpora,
and very similar results for FSA and IFSA, we were
interested to test the impact of IFSA on the NIST
full corpus. In Table 8, the translation results of the
baseline system and IFSA segmentation method for
the NIST full corpus are depicted. As it is shown in
table, the IFSA method slightly improves the trans-
lation results in the development and test sets.
The IFSA segmentation method generates the
best results among our proposed methods. It
acheives consistent improvements in all three tasks
over the baseline system. It also outperforms the SL
19
Table 3: BTEC corpus statistics, where the Arabic part is tokenized and segmented with the SL, FB, FSA
and the IFSA methods.
ARABIC ENGLISH
TOKENIZED SL FB FSA IFSA
Train: Sentences 20K
Running Words 159K 176.2K 185.5K 190.3K 189.1K 189K
Vocabulary 18,149 14,321 11,235 11,736 12,874 7,162
Dev: Sentences 506
Running Words 3,161 3,421 3,549 3,759 3,715 5,005
OOVs (Running Words) 163 129 149 98 118 NA
Test: Sentences 500
Running Words 3,240 3,578 3,675 3,813 3,778 4,986
OOVs (Running Words) 186 120 156 92 115 NA
Table 4: Corpus statistics for the news part of the NIST task, where the Arabic part is tokenized and seg-
mented with SL, FB, FSA and IFSA methods.
ARABIC ENGLISH
TOKENIZED SL FB FSA IFSA
Train: Sentences 284.9K
Running Words 8.9M 9.7M 12.2M 10.9M 10.9M 10.2M
Vocabulary 118.7K 90.5K 43.1K 68.4K 62.2K 56.1K
Dev: Sentences 1,043
Running Words 27.7K 29.1K 37.3K 34.4K 33.5K 33K
OOVs (Running Words) 714 558 396 515 486 NA
Test: Sentences 1,353
Running Words 37.9K 41.7K 52.6K 48.6K 48.3K 48.3K
OOVs (Running Words) 1,298 1,027 612 806 660 NA
segmentation on the BTEC task.
Although the SL method outperforms the IFSA
method on the NIST tasks, the IFSA segmentation
method has a few notable advantages over the SL
system. First, it is consistent in improving the base-
line system over the three tasks. But, the SL method
failed in improving the BTEC development corpus.
Second, it is fast and robust, and capable of being
applied to the large corpora. Finally, it employs an
unsupervised learning method, therefore can easily
cope with a new task or corpus.
We observe that the relative improvement over
the baseline system is decreased by increasing the
size of the training corpus. This is a natural effect
of increasing the size of the training corpus. As
the larger corpus provides higher probability to have
more samples per word, this means higher chance
to learn the translation of a word in different con-
texts. Therefore, larger training corpus makes a bet-
ter translation system, i.e. a better baseline, then it
would be harder to outperform this better system.
Using the same reasoning, we can realize why the
FB method achieves good results on the BTEC task,
but not on the NIST task. By increasing the size
of the training corpus, the FB method tends to seg-
ment words more than the IFSA method. This over-
segmentation can be compensated by using longer
phrases during the translation, in order to consider
the same context compared to the non-segmented
corpus. Then, it would be harder for a phrase-based
machine translation system to learn the translation
of a word (stem) in different contexts.
6 Conclusion
We presented three methods to segment Arabic
words: a supervised learning approach, a frequency-
20
Table 5: NIST task corpus statistics, where the Arabic part is tokenized and segmented with the IFSA
method.
ARABIC ENGLISH
TOKENIZED IFSA
Train: Sentences 8.5M
Running Words 260.5M 316.8M 279.2M
Vocabulary 510.3K 411.2K 301.2K
Dev: Sentences 1043
Running Words 30.2K 33.3K 33K
OOVs (Running Words) 809 399 NA
Test: Sentences 1353
Running Words 40K 47.9K 48.3K
OOVs (Running Words) 871 505 NA
Table 6: Case insensitive evaluation results for translating the development and test data of BTEC task after
performing divers preprocessing.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 21.4 24.6 63.9 10.0 23.5 27.2 58.1 9.6
SL Segmenter 21.2 24.4 62.5 9.7 23.4 27.4 59.2 9.7
FB Segmenter 20.9 24.4 65.3 10.1 22.1 25.8 59.8 9.7
FSA Segmenter 20.1 23.4 64.8 10.2 21.1 25.2 61.3 10.2
IFSA Segmenter 20.0 23.3 65.0 10.4 21.2 25.3 61.3 10.2
based approach and a finite state automaton-based
approach. We explained that the best of our pro-
posed methods, the improved finite state automaton,
has three advantages over the state-of-the-art Arabic
word segmentation method (Diab, 2000), supervised
learning. They are: consistency in improving the
baselines system over different tasks, its capability
to be efficiently applied on the large corpora, and its
ability to cope with different tasks.
7 Acknowledgment
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic
tagging of arabic text: From raw text to base phrase
chunks. In D. M. Susan Dumais and S. Roukos, edi-
tors, HLT-NAACL 2004: Short Papers, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
M. Diab. 2000. An unsupervised method for multi-
lingual word sense tagging using parallel corpora: A
preliminary investigation. In ACL-2000 Workshop on
Word Senses and Multilinguality, pages 1?9, Hong
Kong, October.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
21
Table 7: Case sensitive evaluation results for translating the development and test data of the news part of
the NIST task after performing divers preprocessing.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 43.7 56.4 43.6 9.9 46.1 58.0 37.4 9.1
SL Segmenter 42.0 54.7 45.1 10.2 44.3 56.3 39.9 9.6
FB Segmenter 43.4 56.1 43.2 9.8 45.6 57.8 37.2 9.2
FSA Segmenter 42.9 55.7 43.7 9.9 44.8 56.9 38.7 9.4
IFSA Segmenter 42.6 55.0 44.6 9.9 44.5 56.6 38.8 9.4
Table 8: Case-sensitive evaluation results for translating development and test data of NIST task.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 41.5 53.5 46.4 10.3 42.5 53.9 42.6 10.0
IFSA Segmenter 41.1 53.2 46.7 10.2 42.1 53.6 43.4 10.1
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological dis-
ambiguation in one fell swoop. In Proc. of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 573?580, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proc. 10th Conf. of the Europ.
Chapter of the Assoc. for Computational Linguistics
(EACL), pages 347?354, Budapest, Hungary, April.
L. S. Larkey, L. Ballesteros, and M. E. Connell. 2002.
Improving stemming for arabic information retrieval:
light stemming and co-occurrence analysis. In Proc.
of the 25th annual of the international Association
for Computing Machinery Special Interest Group on
Information Retrieval (ACM SIGIR), pages 275?282,
New York, NY, USA. ACM Press.
Y. S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In E. Hinrichs and D. Roth, editors, Proc.
of the 41st Annual Meeting of the Association for Com-
putational Linguistics.
Y. S. Lee. 2004. Morphological analysis for statisti-
cal machine translation. In D. M. Susan Dumais and
S. Roukos, editors, HLT-NAACL 2004: Short Papers,
pages 57?60, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295?302, Philadelphia, PA, July.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 311?318, Philadelphia, PA, July.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conver-
sations in the real world. In Proc. of the Third Int.
Conf. on Language Resources and Evaluation (LREC),
pages 147?152, Las Palmas, Spain, May.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL),
pages 257?264, Boston, MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
22
Proceedings of the Workshop on Statistical Machine Translation, pages 55?63,
New York City, June 2006. c?2006 Association for Computational Linguistics
Discriminative Reordering Models for Statistical Machine Translation
Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,ney}@cs.rwth-aachen.de
Abstract
We present discriminative reordering
models for phrase-based statistical ma-
chine translation. The models are trained
using the maximum entropy principle.
We use several types of features: based on
words, based on word classes, based on
the local context. We evaluate the overall
performance of the reordering models as
well as the contribution of the individual
feature types on a word-aligned corpus.
Additionally, we show improved transla-
tion performance using these reordering
models compared to a state-of-the-art
baseline system.
1 Introduction
In recent evaluations, phrase-based statistical ma-
chine translation systems have achieved good per-
formance. Still the fluency of the machine transla-
tion output leaves much to desire. One reason is
that most phrase-based systems use a very simple re-
ordering model. Usually, the costs for phrase move-
ments are linear in the distance, e.g. see (Och et al,
1999; Koehn, 2004; Zens et al, 2005).
Recently, in (Tillmann and Zhang, 2005) and in
(Koehn et al, 2005), a reordering model has been
described that tries to predict the orientation of a
phrase, i.e. it answers the question ?should the next
phrase be to the left or to the right of the current
phrase?? This phrase orientation probability is con-
ditioned on the current source and target phrase and
relative frequencies are used to estimate the proba-
bilities.
We adopt the idea of predicting the orientation,
but we propose to use a maximum-entropy based
model. The relative-frequency based approach may
suffer from the data sparseness problem, because
most of the phrases occur only once in the training
corpus. Our approach circumvents this problem by
using a combination of phrase-level and word-level
features and by using word-classes or part-of-speech
information. Maximum entropy is a suitable frame-
work for combining these different features with a
well-defined training criterion.
In (Koehn et al, 2005) several variants of the ori-
entation model have been tried. It turned out that for
different tasks, different models show the best per-
formance. Here, we let the maximum entropy train-
ing decide which features are important and which
features can be neglected. We will see that addi-
tional features do not hurt performance and can be
safely added to the model.
The remaining part is structured as follows: first
we will describe the related work in Section 2 and
give a brief description of the baseline system in
Section 3. Then, we will present the discriminative
reordering model in Section 4. Afterwards, we will
evaluate the performance of this new model in Sec-
tion 5. This evaluation consists of two parts: first we
will evaluate the prediction capabilities of the model
on a word-aligned corpus and second we will show
improved translation quality compared to the base-
line system. Finally, we will conclude in Section 6.
2 Related Work
As already mentioned in Section 1, many current
phrase-based statistical machine translation systems
use a very simple reordering model: the costs
55
for phrase movements are linear in the distance.
This approach is also used in the publicly available
Pharaoh decoder (Koehn, 2004). The idea of pre-
dicting the orientation is adopted from (Tillmann
and Zhang, 2005) and (Koehn et al, 2005). Here,
we use the maximum entropy principle to combine
a variety of different features.
A reordering model in the framework of weighted
finite state transducers is described in (Kumar and
Byrne, 2005). There, the movements are defined at
the phrase level, but the window for reordering is
very limited. The parameters are estimated using an
EM-style method.
None of these methods try to generalize from the
words or phrases by using word classes or part-of-
speech information.
The approach presented here has some resem-
blance to the bracketing transduction grammars
(BTG) of (Wu, 1997), which have been applied to
a phrase-based machine translation system in (Zens
et al, 2004). The difference is that, here, we do
not constrain the phrase reordering. Nevertheless
the inverted/monotone concatenation of phrases in
the BTG framework is similar to the left/right phrase
orientation used here.
3 Baseline System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|fJ1 )
} (1)
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
I?,e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
)
(2)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(3)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be eas-
ily integrated into the overall system. The model
scaling factors ?M1 are trained with respect to the fi-
nal translation quality measured by an error criterion
(Och, 2003).
We use a state-of-the-art phrase-based translation
system (Zens and Ney, 2004; Zens et al, 2005) in-
cluding the following models: an n-gram language
model, a phrase translation model and a word-based
lexicon model. The latter two models are used for
both directions: p(f |e) and p(e|f). Additionally,
we use a word penalty and a phrase penalty. The
reordering model of the baseline system is distance-
based, i.e. it assigns costs based on the distance from
the end position of a phrase to the start position of
the next phrase. This very simple reordering model
is widely used, for instance in (Och et al, 1999;
Koehn, 2004; Zens et al, 2005).
4 The Reordering Model
4.1 Idea
In this section, we will describe the proposed dis-
criminative reordering model.
To make use of word level information, we need
the word alignment within the phrase pairs. This can
be easily stored during the extraction of the phrase
pairs from the bilingual training corpus. If there are
multiple possible alignments for a phrase pair, we
use the most frequent one.
The notation is introduced using the illustration in
Figure 1. There is an example of a left and a right
phrase orientation. We assume that we have already
produced the three-word phrase in the lower part.
Now, the model has to predict if the start position
of the next phrase j? is to the left or to the right of
the current phrase. The reordering model is applied
only at the phrase boundaries. We assume that the
reordering within the phrases is correct.
In the remaining part of this section, we will de-
scribe the details of this reordering model. The
classes our model predicts will be defined in Sec-
tion 4.2. Then, the feature functions will be defined
56
ta
rg
et
 p
os
iti
on
s
source positions
j j?
i
ta
rg
et
 p
os
iti
on
s
source positions
i
right phrase orientationleft phrase orientation
jj?
Figure 1: Illustration of the phrase orientation.
in Section 4.3. The training criterion and the train-
ing events of the maximum entropy model will be
described in Section 4.4.
4.2 Class Definition
Ideally, this model predicts the start position of the
next phrase. But as predicting the exact position is
rather difficult, we group the possible start positions
into classes. In the simplest case, we use only two
classes. One class for the positions to the left and
one class for the positions to the right. As a refine-
ment, we can use four classes instead of two: 1) one
position to the left, 2) more than one positions to the
left, 3) one position to the right, 4) more than one
positions to the right.
In general, we use a parameter D to specify 2 ? D
classes of the types:
? exactly d positions to the left, d = 1, ...,D ? 1
? at least D positions to the left
? exactly d positions to the right, d = 1, ...,D?1
? at least D positions to the right
Let cj,j? denote the orientation class for a move-
ment from source position j to source position j? as
illustrated in Figure 1. In the case of two orientation
classes, cj,j? is defined as:
cj,j? =
{
left, if j? < j
right, if j? > j (4)
Then, the reordering model has the form
p(cj,j?|fJ1 , eI1, i, j)
A well-founded framework for directly modeling the
probability p(cj,j?|fJ1 , eI1, i, j) is maximum entropy
(Berger et al, 1996). In this framework, we have a
set of N feature functions hn(fJ1 , eI1, i, j, cj,j?), n =
1, . . . ,N . Each feature function hn is weighted with
a factor ?n. The resulting model is:
p?N1 (cj,j?|f
J
1 , eI1, i, j)
=
exp
( N
?
n=1
?nhn(fJ1 , eI1, i, j, cj,j?)
)
?
c?
exp
( N
?
n=1
?nhn(fJ1 , eI1, i, j, c?)
) (5)
The functional form is identical to Equation 2,
but here we will use a large number of binary
features, whereas in Equation 2 usually only a
very small number of real-valued features is used.
More precisely, the resulting reordering model
p?N1 (cj,j?|f
J
1 , eI1, i, j) is used as an additional com-
ponent in the log-linear combination of Equation 2.
4.3 Feature Definition
The feature functions of the reordering model de-
pend on the last alignment link (j, i) of a phrase.
Note that the source position j is not necessarily the
57
end position of the source phrase. We use the source
position j which is aligned to the last word of the
target phrase in target position i. The illustration in
Figure 1 contains such an example.
To introduce generalization capabilities, some of
the features will depend on word classes or part-
of-speech information. Let F J1 denote the word
class sequence that corresponds to the source lan-
guage sentence fJ1 and let EI1 denote the target word
class sequence that corresponds to the target lan-
guage sentence eI1. Then, the feature functions are
of the form hn(fJ1 , eI1, F J1 , EI1 , i, j, j?). We consider
the following binary features:
1. source words within a window around the cur-
rent source position j
hf,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (6)
= ?(fj+d, f) ? ?(c, cj,j?)
2. target words within a window around the cur-
rent target position i
he,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (7)
= ?(ei+d, e) ? ?(c, cj,j?)
3. word classes or part-of-speech within a window
around the current source position j
hF,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (8)
= ?(Fj+d, F ) ? ?(c, cj,j?)
4. word classes or part-of-speech within a window
around the current target position i
hE,d,c(fJ1 , eI1, F J1 , EI1 , i, j, j?) (9)
= ?(Ei+d, E) ? ?(c, cj,j?)
Here, ?(?, ?) denotes the Kronecker-function. In the
experiments, we will use d ? {?1, 0, 1}. Many
other feature functions are imaginable, e.g. combi-
nations of the described feature functions, n-gram
or multi-word features, joint source and target lan-
guage feature functions.
4.4 Training
As training criterion, we use the maximum class
posterior probability. This corresponds to maximiz-
ing the likelihood of the maximum entropy model.
Since the optimization criterion is convex, there is
only a single optimum and no convergence problems
occur. To train the model parameters ?N1 , we use the
Generalized Iterative Scaling (GIS) algorithm (Dar-
roch and Ratcliff, 1972).
In practice, the training procedure tends to result
in an overfitted model. To avoid overfitting, (Chen
and Rosenfeld, 1999) have suggested a smoothing
method where a Gaussian prior distribution of the
parameters is assumed.
This method tried to avoid very large lambda val-
ues and prevents features that occur only once for a
specific class from getting a value of infinity.
We train IBM Model 4 with GIZA++ (Och and
Ney, 2003) in both translation directions. Then the
alignments are symmetrized using a refined heuris-
tic as described in (Och and Ney, 2003). This word-
aligned bilingual corpus is used to train the reorder-
ing model parameters, i.e. the feature weights ?N1 .
Each alignment link defines an event for the max-
imum entropy training. An exception are the one-
to-many alignments, i.e. one source word is aligned
to multiple target words. In this case, only the top-
most alignment link is considered because the other
ones cannot occur at a phrase boundary. Many-to-
one and many-to-many alignments are handled in a
similar way.
5 Experimental Results
5.1 Statistics
The experiments were carried out on the Basic
Travel Expression Corpus (BTEC) task (Takezawa
et al, 2002). This is a multilingual speech cor-
pus which contains tourism-related sentences sim-
ilar to those that are found in phrase books. We
use the Arabic-English, the Chinese-English and the
Japanese-English data. The corpus statistics are
shown in Table 1.
As the BTEC is a rather clean corpus, the prepro-
cessing consisted mainly of tokenization, i.e., sep-
arating punctuation marks from words. Addition-
ally, we replaced contractions such as it?s or I?m in
the English corpus and we removed the case infor-
mation. For Arabic, we removed the diacritics and
we split common prefixes: Al, w, f, b, l. There
was no special preprocessing for the Chinese and the
Japanese training corpora.
To train and evaluate the reordering model, we
58
Table 1: Corpus statistics after preprocessing for the BTEC task.
Arabic Chinese Japanese English
Train Sentences 20 000
Running Words 180 075 176 199 198 453 189 927
Vocabulary 15 371 8 687 9 277 6 870
C-Star?03 Sentences 506
Running Words 3 552 3 630 4 130 3 823
Table 2: Statistics of the training and test word align-
ment links.
Ara-Eng Chi-Eng Jap-Eng
Training 144K 140K 119K
Test 16.2K 15.7K 13.2K
use the word aligned bilingual training corpus. For
evaluating the classification power of the reordering
model, we partition the corpus into a training part
and a test part. In our experiments, we use about
10% of the corpus for testing and the remaining
part for training the feature weights of the reorder-
ing model with the GIS algorithm using YASMET
(Och, 2001). The statistics of the training and test
alignment links is shown in Table 2. The number
of training events ranges from 119K for Japanese-
English to 144K for Arabic-English.
The word classes for the class-based features are
trained using the mkcls tool (Och, 1999). In the
experiments, we use 50 word classes. Alternatively,
one could use part-of-speech information for this
purpose.
Additional experiments were carried out on the
large data track of the Chinese-English NIST task.
The corpus statistics of the bilingual training cor-
pus are shown in Table 3. The language model was
trained on the English part of the bilingual train-
ing corpus and additional monolingual English data
from the GigaWord corpus. The total amount of lan-
guage model training data was about 600M running
words. We use a fourgram language model with
modified Kneser-Ney smoothing as implemented in
the SRILM toolkit (Stolcke, 2002). For the four En-
glish reference translations of the evaluation sets, the
accumulated statistics are presented.
Table 3: Chinese-English NIST task: corpus statis-
tics for the bilingual training data and the NIST eval-
uation sets of the years 2002 to 2005.
Chinese English
Train Sentence Pairs 7M
Running Words 199M 213M
Vocabulary Size 223K 351K
Dictionary Entry Pairs 82K
Eval 2002 Sentences 878 3 512
Running Words 25K 105K
2003 Sentences 919 3 676
Running Words 26K 122K
2004 Sentences 1788 7 152
Running Words 52K 245K
2005 Sentences 1082 4 328
Running Words 33K 148K
5.2 Classification Results
In this section, we present the classification results
for the three language pairs. In Table 4, we present
the classification results for two orientation classes.
As baseline we always choose the most frequent
orientation class. For Arabic-English, the baseline
is with 6.3% already very low. This means that the
word order in Arabic is very similar to the word or-
der in English. For Chinese-English, the baseline
is with 12.7% about twice as large. The most dif-
ferences in word order occur for Japanese-English.
This seems to be reasonable as Japanese has usu-
ally a different sentence structure, subject-object-
verb compared to subject-verb-object in English.
For each language pair, we present results for sev-
eral combination of features. The three columns per
language pair indicate if the features are based on the
words (column label ?Words?), on the word classes
(column label ?Classes?) or on both (column label
59
Table 4: Classification error rates [%] using two orientation classes.
Arabic-English Chinese-English Japanese-English
Baseline 6.3 12.7 26.2
Lang. Window Words Classes W+C Words Classes W+C Words Classes W+C
Tgt d = 0 4.7 5.3 4.4 9.3 10.4 8.9 13.6 15.1 13.4
d ? {0, 1} 4.5 5.0 4.3 8.9 9.9 8.6 13.7 14.9 13.4
d ? {?1, 0, 1} 4.5 4.9 4.3 8.6 9.5 8.3 13.5 14.6 13.3
Src d = 0 5.6 5.0 3.9 7.9 8.3 7.2 12.2 11.8 11.0
d ? {0, 1} 3.2 3.0 2.6 4.7 4.7 4.2 10.1 9.7 9.4
d ? {?1, 0, 1} 2.9 2.5 2.3 3.9 3.5 3.3 9.0 8.0 7.8
Src d = 0 4.3 3.9 3.7 7.1 7.8 6.5 10.8 10.9 9.8
+ d ? {0, 1} 2.9 2.6 2.5 4.6 4.5 4.1 9.3 9.1 8.6
Tgt d ? {?1, 0, 1} 2.8 2.1 2.1 3.9 3.4 3.3 8.7 7.7 7.7
?W+C?). We also distinguish if the features depend
on the target sentence (?Tgt?), on the source sentence
(?Src?) or on both (?Src+Tgt?).
For Arabic-English, using features based only on
words of the target sentence the classification er-
ror rate can be reduced to 4.5%. If the features are
based only on the source sentence words, a classifi-
cation error rate of 2.9% is reached. Combining the
features based on source and target sentence words,
a classification error rate of 2.8% can be achieved.
Adding the features based on word classes, the clas-
sification error rate can be further improved to 2.1%.
For the other language pairs, the results are similar
except that the absolute values of the classification
error rates are higher.
We observe the following:
? The features based on the source sentence per-
form better than features based on the target
sentence.
? Combining source and target sentence features
performs best.
? Increasing the window always helps, i.e. addi-
tional context information is useful.
? Often the word-class based features outperform
the word-based features.
? Combining word-based and word-class based
features performs best.
? In general, adding features does not hurt the
performance.
These are desirable properties of an appropriate
reordering model. The main point is that these are
fulfilled not only on the training data, but on unseen
test data. There seems to be no overfitting problem.
In Table 5, we present the results for four orien-
tation classes. The final error rates are a factor 2-4
larger than for two orientation classes. Despite that
we observe the same tendencies as for two orien-
tation classes. Again, using more features always
helps to improve the performance.
5.3 Translation Results
For the translation experiments on the BTEC task,
we report the two accuracy measures BLEU (Pap-
ineni et al, 2002) and NIST (Doddington, 2002) as
well as the two error rates: word error rate (WER)
and position-independent word error rate (PER).
These criteria are computed with respect to 16 refer-
ences.
In Table 6, we show the translation results for
the BTEC task. In these experiments, the reorder-
ing model uses two orientation classes, i.e. it pre-
dicts either a left or a right orientation. The fea-
tures for the maximum-entropy based reordering
model are based on the source and target language
words within a window of one. The word-class
based features are not used for the translation ex-
periments. The maximum-entropy based reordering
model achieves small but consistent improvement
for all the evaluation criteria. Note that the baseline
system, i.e. using the distance-based reordering, was
among the best systems in the IWSLT 2005 evalua-
60
Table 5: Classification error rates [%] using four orientation classes.
Arabic-English Chinese-English Japanese-English
Baseline 31.4 44.9 59.0
Lang. Window Words Classes W+C Words Classes W+C Words Classes W+C
Tgt d = 0 24.5 27.7 24.2 30.0 34.4 29.7 28.9 31.4 28.7
d ? {0, 1} 23.9 27.2 23.7 29.2 32.9 28.9 28.7 30.6 28.3
d ? {?1, 0, 1} 22.1 25.3 21.9 27.6 31.4 27.4 28.3 30.1 28.2
Src d = 0 22.1 23.2 20.4 25.9 27.7 20.4 24.1 24.9 22.3
d ? {0, 1} 11.9 12.0 10.8 14.0 14.9 13.2 18.6 19.5 17.7
d ? {?1, 0, 1} 10.1 8.7 8.0 11.4 11.1 10.5 15.6 15.6 14.5
Src d = 0 20.9 21.8 19.6 24.1 26.8 19.6 22.3 23.4 21.1
+ d ? {0, 1} 11.8 11.5 10.6 13.5 14.5 12.8 18.6 18.8 17.1
Tgt d ? {?1, 0, 1} 9.6 7.7 7.6 11.3 10.1 10.1 15.6 15.2 14.2
Table 6: Translation Results for the BTEC task.
Language Pair Reordering WER [%] PER [%] NIST BLEU [%]
Arabic-English Distance-based 24.1 20.9 10.0 63.8
Max-Ent based 23.6 20.7 10.1 64.8
Chinese-English Distance-based 50.4 43.0 7.67 44.4
Max-Ent based 49.3 42.4 7.36 45.8
Japanese-English Distance-based 32.1 25.2 8.96 56.2
Max-Ent based 31.2 25.2 9.00 56.8
tion campaign (Eck and Hori, 2005).
Some translation examples are presented in Ta-
ble 7. We observe that the system using the
maximum-entropy based reordering model produces
more fluent translations.
Additional translation experiments were carried
out on the large data track of the Chinese-English
NIST task. For this task, we use only the BLEU
and NIST scores. Both scores are computed case-
insensitive with respect to four reference translations
using the mteval-v11b tool1.
For the NIST task, we use the BLEU score as pri-
mary criterion which is optimized on the NIST 2002
evaluation set using the Downhill Simplex algorithm
(Press et al, 2002). Note that only the eight or nine
model scaling factors of Equation 2 are optimized
using the Downhill Simplex algorithm. The feature
weights of the reordering model are trained using
the GIS algorithm as described in Section 4.4. We
use a state-of-the-art baseline system which would
have obtained a good rank in the last NIST evalua-
1http://www.nist.gov/speech/tests/mt/resources/scoring.htm
tion (NIST, 2005).
The translation results for the NIST task are pre-
sented in Table 8. We observe consistent improve-
ments of the BLEU score on all evaluation sets. The
overall improvement due to reordering ranges from
1.2% to 2.0% absolute. The contribution of the
maximum-entropy based reordering model to this
improvement is in the range of 25% to 58%, e.g. for
the NIST 2003 evaluation set about 58% of the im-
provement using reordering can be attributed to the
maximum-entropy based reordering model.
We also measured the classification performance
for the NIST task. The general tendencies are iden-
tical to the BTEC task.
6 Conclusions
We have presented a novel discriminative reorder-
ing model for statistical machine translation. This
model is trained on the word aligned bilingual cor-
pus using the maximum entropy principle. Several
types of features have been used:
? based on the source and target sentence
61
Table 7: Translation examples for the BTEC task.
System Translation
Distance-based I would like to check out time one day before.
Max-Ent based I would like to check out one day before the time.
Reference I would like to check out one day earlier.
Distance-based I hate pepper green.
Max-Ent based I hate the green pepper.
Reference I hate green peppers.
Distance-based Is there a subway map where?
Max-Ent based Where is the subway route map?
Reference Where do they have a subway map?
Table 8: Translation results for several evaluation sets of the Chinese-English NIST task.
Evaluation set 2002 (dev) 2003 2004 2005
Reordering NIST BLEU[%] NIST BLEU[%] NIST BLEU[%] NIST BLEU[%]
None 8.96 33.5 8.67 32.7 8.76 32.0 8.62 30.8
Distance-based 9.19 34.6 8.85 33.2 9.05 33.2 8.79 31.6
Max-Ent based 9.24 35.5 8.87 33.9 9.04 33.6 8.78 32.1
? based on words and word classes
? using local context information
We have evaluated the performance of the re-
ordering model on a held-out word-aligned corpus.
We have shown that the model is able to predict the
orientation very well, e.g. for Arabic-English the
classification error rate is only 2.1%.
We presented improved translation results for
three language pairs on the BTEC task and for the
large data track of the Chinese-English NIST task.
In none of the cases additional features have hurt
the classification performance on the held-out test
corpus. This is a strong evidence that the maximum
entropy framework is suitable for this task.
Another advantage of our approach is the gener-
alization capability via the use of word classes or
part-of-speech information. Furthermore, additional
features can be easily integrated into the maximum
entropy framework.
So far, the word classes were not used for the
translation experiments. As the word classes help
for the classification task, we might expect further
improvements of the translation results. Using part-
of-speech information instead (or in addition) to the
automatically computed word classes might also be
beneficial. More fine-tuning of the reordering model
toward translation quality might also result in im-
provements. As already mentioned in Section 4.3, a
richer feature set could be helpful.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?72, March.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
S. F. Chen and R. Rosenfeld. 1999. A gaussian prior
for smoothing maximum entropy models. Technical
Report CMUCS-99-108, Carnegie Mellon University,
Pittsburgh, PA.
62
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470?1480.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
M. Eck and C. Hori. 2005. Overview of the IWSLT 2005
evaluation campaign. In Proc. International Workshop
on Spoken Language Translation (IWSLT), Pittsburgh,
PA, October.
P. Koehn, A. Axelrod, A. B. Mayne, C. Callison-Burch,
M. Osborne, and D. Talbot. 2005. Edinburgh sys-
tem description for the 2005 IWSLT speech translation
evaluation. In Proc. International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
October.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. 6th Conf. of the Assoc. for Machine Transla-
tion in the Americas (AMTA), pages 115?124, Wash-
ington DC, September/October.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proc. of the Human Language Technology Conf./Conf.
on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP), pages 161?168, Vancouver,
Canada, October.
NIST. 2005. NIST 2005 machine
translation evaluation official results.
http://www.nist.gov/speech/tests/mt/
mt05eval official results release
20050801 v3.html, August.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages
295?302, Philadelphia, PA, July.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. Joint SIGDAT Conf. on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 20?28, University of Maryland, College
Park, MD, June.
F. J. Och. 1999. An efficient method for determining
bilingual word classes. In Proc. 9th Conf. of the Europ.
Chapter of the Assoc. for Computational Linguistics
(EACL), pages 71?76, Bergen, Norway, June.
F. J. Och. 2001. YASMET: Toolkit for conditional maxi-
mum entropy models. http://www-i6.informatik.rwth-
aachen.de/web/Software/YASMET.html.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Assoc. for Computational Linguistics (ACL), pages
311?318, Philadelphia, PA, July.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing (ICSLP), volume 2, pages 901?904, Den-
ver, CO.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conver-
sations in the real world. In Proc. of the Third Int.
Conf. on Language Resources and Evaluation (LREC),
pages 147?152, Las Palmas, Spain, May.
C. Tillmann and T. Zhang. 2005. A localized prediction
model for statistical machine translation. In Proc. of
the 43rd Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 557?564, Ann Arbor,
MI, June.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), pages 257?264, Boston, MA,
May.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering constraints for phrase-based statistical ma-
chine translation. In Proc. 20th Int. Conf. on Computa-
tional Linguistics (COLING), pages 205?211, Geneva,
Switzerland, August.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proc. International Workshop on Spoken Language
Translation (IWSLT), pages 155?162, Pittsburgh, PA,
October.
63
Proceedings of the Workshop on Statistical Machine Translation, pages 72?77,
New York City, June 2006. c?2006 Association for Computational Linguistics
N -Gram Posterior Probabilities for Statistical Machine Translation
Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{zens,ney}@cs.rwth-aachen.de
Abstract
Word posterior probabilities are a com-
mon approach for confidence estimation
in automatic speech recognition and ma-
chine translation. We will generalize this
idea and introduce n-gram posterior prob-
abilities and show how these can be used
to improve translation quality. Addition-
ally, we will introduce a sentence length
model based on posterior probabilities.
We will show significant improvements on
the Chinese-English NIST task. The abso-
lute improvements of the BLEU score is
between 1.1% and 1.6%.
1 Introduction
The use of word posterior probabilities is a com-
mon approach for confidence estimation in auto-
matic speech recognition, e.g. see (Wessel, 2002).
This idea has been adopted to estimate confidences
for machine translation, e.g. see (Blatz et al, 2003;
Ueffing et al, 2003; Blatz et al, 2004). These confi-
dence measures were used in the computer assisted
translation (CAT) framework, e.g. (Gandrabur and
Foster, 2003). The (simplified) idea is that the con-
fidence measure is used to decide if the machine-
generated prediction should be suggested to the hu-
man translator or not.
There is only few work on how to improve
machine translation performance using confidence
measures. The only work, we are aware of, is
(Blatz et al, 2003). The outcome was that the con-
fidence measures did not result in improvements of
the translation quality measured with the BLEU and
NIST scores. Here, we focus on how the ideas and
methods commonly used for confidence estimation
can be adapted and/or extended to improve transla-
tion quality.
So far, always word-level posterior probabilities
were used. Here, we will generalize this idea to n-
grams.
In addition to the n-gram posterior probabili-
ties, we introduce a sentence-length model based
on posterior probabilities. The common phrase-
based translation systems, such as (Och et al, 1999;
Koehn, 2004), do not use an explicit sentence length
model. Only the simple word penalty goes into that
direction. It can be adjusted to prefer longer or
shorter translations. Here, we will explicitly model
the sentence length.
The novel contributions of this work are to in-
troduce n-gram posterior probabilities and sentence
length posterior probabilities. Using these methods,
we achieve significant improvements of translation
quality.
The remaining part of this paper is structured as
follows: first, we will briefly describe the baseline
system, which is a state-of-the-art phrase-based sta-
tistical machine translation system. Then, in Sec-
tion 3, we will introduce the n-gram posterior prob-
abilities. In Section 4, we will define the sentence
length model. Afterwards, in Section 5, we will
describe how these novel models can be used for
rescoring/reranking. The experimental results will
be presented in Section 6. Future applications will
be described in Section 7. Finally, we will conclude
in Section 8.
72
2 Baseline System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|fJ1 )
} (1)
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
I?,e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
)
(2)
The denominator is a normalization factor that de-
pends only on the source sentence fJ1 . Therefore,
we can omit it during the search process. As a deci-
sion rule, we obtain:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(3)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be eas-
ily integrated into the overall system. The model
scaling factors ?M1 are trained with respect to the fi-
nal translation quality measured by an error criterion
(Och, 2003).
We use a state-of-the-art phrase-based translation
system as described in (Zens and Ney, 2004; Zens
et al, 2005). The baseline system includes the fol-
lowing models: an n-gram language model, a phrase
translation model and a word-based lexicon model.
The latter two models are used for both directions:
p(f |e) and p(e|f). Additionally, we use a word
penalty and a phrase penalty.
3 N-Gram Posterior Probabilities
The idea is similar to the word posterior probabili-
ties: we sum the sentence posterior probabilities for
each occurrence of an n-gram.
Let ?(?, ?) denote the Kronecker function. Then,
we define the fractional count C(en1 , fJ1 ) of an n-
gram en1 for a source sentence fJ1 as:
C(en1 , fJ1 ) =
?
I,e?I1
I?n+1
?
i=1
p(e?I1|fJ1 ) ? ?(e?
i+n?1
i , en1 )
(4)
The sums over the target language sentences are lim-
ited to an N -best list, i.e. the N best translation
candidates according to the baseline model. In this
equation, the term ?(e?i+n?1i , en1 ) is one if and only
if the n-gram en1 occurs in the target sentence e?
I
1
starting at position i.
Then, the posterior probability of an n-gram is ob-
tained as:
p(en1 |fJ1 ) =
C(en1 , fJ1 )
?
e?n1
C(e?n1 , fJ1 )
(5)
Note that the widely used word posterior proba-
bility is obtained as a special case, namely if n is set
to one.
4 Sentence Length Posterior Probability
The common phrase-based translation systems, such
as (Och et al, 1999; Koehn, 2004), do not use an ex-
plicit sentence length model. Only the simple word
penalty goes into that direction. It can be adjusted to
prefer longer or shorter translations.
Here, we will use the posterior probability of a
specific target sentence length I as length model:
p(I|fJ1 ) =
?
eI1
p(eI1|fJ1 ) (6)
Note that the sum is carried out only over target sen-
tences eI1 with the a specific length I . Again, the
candidate target language sentences are limited to an
N -best list.
5 Rescoring/Reranking
A straightforward application of the posterior prob-
abilities is to use them as additional features in
a rescoring/reranking approach (Och et al, 2004).
The use of N -best lists in machine translation has
several advantages. It alleviates the effects of the
huge search space which is represented in word
73
graphs by using a compact excerpt of the N best hy-
potheses generated by the system. N -best lists are
suitable for easily applying several rescoring tech-
niques since the hypotheses are already fully gen-
erated. In comparison, word graph rescoring tech-
niques need specialized tools which can traverse the
graph accordingly.
The n-gram posterior probabilities can be used
similar to an n-gram language model:
hn(fJ1 , eI1) =
1
I log
( I
?
i=1
p(ei|ei?1i?n+1, fJ1 )
)
(7)
with:
p(ei|ei?1i?n+1, fJ1 ) =
C(eii?n+1, fJ1 )
C(ei?1i?n+1, fJ1 )
(8)
Note that the models do not require smoothing as
long as they are applied to the same N -best list they
are trained on.
If the models are used for unseen sentences,
smoothing is important to avoid zero probabilities.
We use a linear interpolation with weights ?n and
the smoothed (n ? 1)-gram model as generalized
distribution.
pn(ei|ei?1i?n+1, fJ1 ) = ?n ?
C(eii?n+1, fJ1 )
C(ei?1i?n+1, fJ1 )
(9)
+(1 ? ?n) ? pn?1(ei|ei?1i?n+2, fJ1 )
Note that absolute discounting techniques that are
often used in language modeling cannot be applied
in a straightforward way, because here we have frac-
tional counts.
The usage of the sentence length posterior prob-
ability for rescoring is even simpler. The resulting
feature is:
hL(fJ1 , eI1) = log p(I|fJ1 ) (10)
Again, the model does not require smoothing as long
as it is applied to the same N -best list it is trained
on. If it is applied to other sentences, smoothing
becomes important. We propose to smooth the sen-
tence length model with a Poisson distribution.
p?(I|fJ1 ) = ??p(I|fJ1 )+(1??)?
?I exp(??)
I! (11)
We use a linear interpolation with weight ?. The
mean ? of the Poisson distribution is chosen to
be identical to the mean of the unsmoothed length
model:
? =
?
I
I ? p(I|fJ1 ) (12)
6 Experimental Results
6.1 Corpus Statistics
The experiments were carried out on the large data
track of the Chinese-English NIST task. The cor-
pus statistics of the bilingual training corpus are
shown in Table 1. The language model was trained
on the English part of the bilingual training cor-
pus and additional monolingual English data from
the GigaWord corpus. The total amount of lan-
guage model training data was about 600M running
words. We use a fourgram language model with
modified Kneser-Ney smoothing as implemented in
the SRILM toolkit (Stolcke, 2002).
To measure the translation quality, we use the
BLEU score (Papineni et al, 2002) and the NIST
score (Doddington, 2002). The BLEU score is the
geometric mean of the n-gram precision in com-
bination with a brevity penalty for too short sen-
tences. The NIST score is the arithmetic mean of
a weighted n-gram precision in combination with a
brevity penalty for too short sentences. Both scores
are computed case-sensitive with respect to four ref-
erence translations using the mteval-v11b tool1. As
the BLEU and NIST scores measure accuracy higher
scores are better.
We use the BLEU score as primary criterion
which is optimized on the development set using the
Downhill Simplex algorithm (Press et al, 2002). As
development set, we use the NIST 2002 evaluation
set. Note that the baseline system is already well-
tuned and would have obtained a high rank in the
last NIST evaluation (NIST, 2005).
6.2 Translation Results
The translation results for the Chinese-English NIST
task are presented in Table 2. We carried out experi-
ments for evaluation sets of several years. For these
rescoring experiments, we use the 10 000 best trans-
lation candidates, i.e. N -best lists of size N=10 000.
1http://www.nist.gov/speech/tests/mt/resources/scoring.htm
74
Table 1: Chinese-English NIST task: corpus statis-
tics for the bilingual training data and the NIST eval-
uation sets of the years 2002 to 2005.
Chinese English
Train Sentence Pairs 7M
Running Words 199M 213M
Vocabulary Size 223K 351K
Dictionary Entry Pairs 82K
Eval 2002 Sentences 878 3 512
Running Words 25K 105K
2003 Sentences 919 3 676
Running Words 26K 122K
2004 Sentences 1788 7 152
Running Words 52K 245K
2005 Sentences 1082 4 328
Running Words 33K 148K
Using the 1-gram posterior probabilities, i.e. the
conventional word posterior probabilities, there is
only a very small improvement, or no improvement
at all. This is consistent with the findings of the
JHU workshop on confidence estimation for statis-
tical machine translation 2003 (Blatz et al, 2003),
where the word-level confidence measures also did
not help to improve the BLEU or NIST scores.
Successively adding higher order n-gram poste-
rior probabilities, the translation quality improves
consistently across all evaluation sets. We also
performed experiments with n-gram orders beyond
four, but these did not result in further improve-
ments.
Adding the sentence length posterior probability
feature is also helpful for all evaluation sets. For the
development set, the overall improvement is 1.5%
for the BLEU score. On the blind evaluation sets,
the overall improvement of the translation quality
ranges between 1.1% and 1.6% BLEU.
Some translation examples are shown in Table 3.
7 Future Applications
We have shown that the n-gram posterior probabil-
ities are very useful in a rescoring/reranking frame-
work. In addition, there are several other potential
applications. In this section, we will describe two of
them.
7.1 Iterative Search
The n-gram posterior probability can be used for
rescoring as described in Section 5. An alternative is
to use them directly during the search. In this second
search pass, we use the models from the first pass,
i.e. the baseline system, and additionally the n-gram
and sentence length posterior probabilities. As the
n-gram posterior probabilities are basically a kind
of sentence-specific language model, it is straight-
forward to integrate them. This process can also be
iterated. Thus, using the N -best list of the second
pass to recompute the n-gram and sentence length
posterior probabilities and do a third search pass,
etc..
7.2 Computer Assisted Translation
In the computer assisted translation (CAT) frame-
work, the goal is to improve the productivity of hu-
man translators. The machine translation system
takes not only the current source language sentence
but also the already typed partial translation into ac-
count. Based on this information, the system suggest
completions of the sentence. Word-level posterior
probabilities have been used to select the most ap-
propriate completion of the system, for more details
see e.g. (Gandrabur and Foster, 2003; Ueffing and
Ney, 2005). The n-gram based posterior probabili-
ties as described in this work, might be better suited
for this task as they explicitly model the dependency
on the previous words, i.e. the given prefix.
8 Conclusions
We introduced n-gram and sentence length poste-
rior probabilities and demonstrated their usefulness
for rescoring purposes. We performed systematic
experiments on the Chinese-English NIST task and
showed significant improvements of the translation
quality. The improvements were consistent among
several evaluation sets.
An interesting property of the introduced meth-
ods is that they do not require additional knowledge
sources. Thus the given knowledge sources are bet-
ter exploited. Our intuition is that the posterior mod-
els prefer hypotheses with n-grams that are common
in the N -best list.
The achieved results are promising. Despite that,
there are several ways to improve the approach.
75
Table 2: Case-sensitive translation results for several evaluation sets of the Chinese-English NIST task.
Evaluation set 2002 (dev) 2003 2004 2005
System NIST BLEU[%] NIST BLEU[%] NIST BLEU[%] NIST BLEU[%]
Baseline 8.49 30.5 8.04 29.5 8.14 29.0 8.01 28.2
+ 1-grams 8.51 30.5 8.08 29.5 8.17 29.0 8.03 28.2
+ 2-grams 8.47 30.8 8.03 29.7 8.12 29.2 7.98 28.1
+ 3-grams 8.73 31.6 8.25 30.1 8.45 30.0 8.20 28.6
+ 4-grams 8.74 31.7 8.26 30.1 8.47 30.1 8.20 28.6
+ length 8.87 32.0 8.42 30.9 8.60 30.6 8.34 29.3
Table 3: Translation examples for the Chinese-English NIST task.
Baseline At present, there is no organization claimed the attack.
Rescored At present, there is no organization claimed responsibility for the attack.
Reference So far, no organization whatsoever has claimed responsibility for the attack.
Baseline FIFA to severely punish football fraud
Rescored The International Football Federation (FIFA) will severely punish football?s deception
Reference FIFA will severely punish all cheating acts in the football field
Baseline In more than three months of unrest, a total of more than 60 dead and 2000 injured.
Rescored In more than three months of unrest, a total of more than 60 people were killed and more
than 2000 injured.
Reference During the unrest that lasted more than three months, a total of more than 60 people died
and over 2,000 were wounded.
For the decision rule in Equation 3, the model
scaling factors ?M1 can be multiplied with a constant
factor without changing the result. This global fac-
tor would affect the proposed posterior probabilities.
So far, we have not tuned this parameter, but a proper
adjustment might result in further improvements.
Currently, the posterior probabilities are com-
puted on an N -best list. Using word graphs instead
should result in more reliable estimates, as the num-
ber of hypotheses in a word graph is some orders of
a magnitude larger than in an N -best list.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly funded by the European Union un-
der the integrated project TC-STAR (Technology
and Corpora for Speech to Speech Translation, IST-
2002-FP6-506738, http://www.tc-star.org).
References
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for machine transla-
tion. Final report, JHU/CLSP Summer Workshop.
http://www.clsp.jhu.edu/ws2003/groups/
estimate/.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Con-
fidence estimation for machine translation. In Proc.
20th Int. Conf. on Computational Linguistics (COL-
ING), pages 315?321, Geneva, Switzerland, August.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
S. Gandrabur and G. Foster. 2003. Confidence estima-
tion for text prediction. In Proc. Conf. on Natural Lan-
76
guage Learning (CoNLL), pages 95?102, Edmonton,
Canada, May.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In 6th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA 04), pages 115?124,
Washington DC, September/October.
NIST. 2005. NIST 2005 machine
translation evaluation official results.
http://www.nist.gov/speech/tests/mt/
mt05eval official results release
20050801 v3.html, August.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295?302, Philadelphia, PA, July.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. Joint SIGDAT Conf. on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, pages 20?28, University of Maryland, College
Park, MD, June.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proc.
Human Language Technology Conf. / North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL), pages 161?168,
Boston,MA.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 311?318, Philadelphia, PA, July.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.
Flannery. 2002. Numerical Recipes in C++. Cam-
bridge University Press, Cambridge, UK.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Speech and Lan-
guage Processing (ICSLP), volume 2, pages 901?904,
Denver, CO, September.
N. Ueffing and H. Ney. 2005. Application of word-
level confidence measures in interactive statistical ma-
chine translation. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT), pages 262?270, Budapest, Hungary, May.
N. Ueffing, K. Macherey, and H. Ney. 2003. Confi-
dence Measures for Statistical Machine Translation.
In Proc. MT Summit IX, pages 394?401, New Orleans,
LA, September.
F. Wessel. 2002. Word Posterior Probabilities for Large
Vocabulary Continuous Speech Recognition. Ph.D.
thesis, RWTH Aachen University, Aachen, Germany,
January.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conf. / North American Chapter
of the Association for Computational Linguistics An-
nual Meeting (HLT-NAACL), pages 257?264, Boston,
MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
77
Proceedings of the Workshop on Statistical Machine Translation, pages 78?85,
New York City, June 2006. c?2006 Association for Computational Linguistics
Partitioning Parallel Documents Using Binary Segmentation
Jia Xu and Richard Zens and Hermann Ney
Chair of Computer Science 6
Computer Science Department
RWTH Aachen University
D-52056 Aachen Germany
{xujia,zens,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, large
numbers of parallel sentences are required
to train the model parameters. However,
plenty of the bilingual language resources
available on web are aligned only at the
document level. To exploit this data,
we have to extract the bilingual sentences
from these documents.
The common method is to break the doc-
uments into segments using predefined
anchor words, then these segments are
aligned. This approach is not error free,
incorrect alignments may decrease the
translation quality.
We present an alternative approach to ex-
tract the parallel sentences by partitioning
a bilingual document into two pairs. This
process is performed recursively until all
the sub-pairs are short enough.
In experiments on the Chinese-English
FBIS data, our method was capable of
producing translation results comparable
to those of a state-of-the-art sentence
aligner. Using a combination of the two
approaches leads to better translation per-
formance.
1 Introduction
Current statistical machine translation systems use
bilingual sentences to train the parameters of the
translation models. The exploitation of more bilin-
gual sentences automatically and accurately as well
as the use of these data with the limited computa-
tional requirements become crucial problems.
The conventional method for producing parallel
sentences is to break the documents into sentences
and to align these sentences using dynamic program-
ming. Previous investigations can be found in works
such as (Gale and Church, 1993) and (Ma, 2006).
A disadvantage is that only the monotone sentence
alignments are allowed.
Another approach is the binary segmentation
method described in (Simard and Langlais, 2003),
(Xu et al, 2005) and (Deng et al, 2006), which
separates a long sentence pair into two sub-pairs re-
cursively. The binary reordering in alignment is al-
lowed but the segmentation decision is only opti-
mum in each recursion step.
Hence, a combination of both methods is ex-
pected to produce a more satisfying result. (Deng
et al, 2006) performs a two-stage procedure. The
documents are first aligned at level using dynamic
programming, the initial alignments are then refined
to produce shorter segments using binary segmen-
tation. But on the Chinese-English FBIS training
corpus, the alignment accuracy and recall are lower
than with Champollion (Ma, 2006).
We refine the model in (Xu et al, 2005) using
a log-linar combination of different feature func-
tions and combine it with the approach of (Ma,
2006). Here the corpora produced using both ap-
proaches are concatenated, and each corpus is as-
signed a weight. During the training of the word
alignment models, the counts of the lexicon entries
78
are linear interpolated using the corpus weights. In
the experiments on the Chinese-English FBIS cor-
pus the translation performance is improved by 0.4%
of the BLEU score compared to the performance
only with Champollion.
The remainder of this paper is structured as fol-
lows: First we will briefly review the baseline statis-
tical machine translation system in Section 2. Then,
in Section 3, we will describe the refined binary seg-
mentation method. In Section 4.1, we will introduce
the methods to extract bilingual sentences from doc-
ument aligned texts. The experimental results will
be presented in Section 4.
2 Review of the Baseline Statistical
Machine Translation System
In this section, we briefly review our translation sys-
tem and introduce the word alignment models.
In statistical machine translation, we are given
a source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{Pr(eI1|fJ1 )
}
= argmax
I,eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
} (1)
The decomposition into two knowledge sources in
Equation 1 allows independent modeling of tar-
get language model Pr(eI1) and translation model
Pr(fJ1 |eI1)1. The translation model can be further
extended to a statistical alignment model with the
following equation:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1)
The alignment model Pr(fJ1 , aJ1 |eI1) introduces a
?hidden? word alignment a = aJ1 , which describes a
mapping from a source position j to a target position
aj .
1The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol p(?).
Monotone Non-
monotone
Target B A
Positions C D
Source Positions
Figure 1: Two Types of Alignment
The IBM model 1 (IBM-1) (Brown et al, 1993)
assumes that all alignments have the same probabil-
ity by using a uniform distribution:
p(fJ1 |eI1) =
1
IJ ?
J?
j=1
I?
i=1
p(fj |ei) (2)
We use the IBM-1 to train the lexicon parameters
p(f |e), the training software is GIZA++ (Och and
Ney, 2003).
To incorporate the context into the translation
model, the phrase-based translation approach (Zens
et al, 2005) is applied. Pairs of source and tar-
get language phrases are extracted from the bilin-
gual training corpus and a beam search algorithm is
implemented to generate the translation hypothesis
with maximum probability.
3 Binary Segmentation Method
3.1 Approach
Here a document or sentence pair (fJ1 , eI1) 2 is repre-
sented as a matrix. Every element in the matrix con-
tains a lexicon probability p(fj |ei), which is trained
on the original parallel corpora. Each position di-
vides a matrix into four parts as shown in Figure 1:
the bottom left (C), the upper left (A), the bottom
right (D) and the upper right (B). We use m to de-
note the alignment direction, m = 1 means that the
alignment is monotone, i.e. the bottom left part is
connected with the upper right part, and m = 0
means the alignment is non-monotone, i.e. the upper
left part is connected with the bottom right part, as
shown in Figure 1.
3.2 Log-Linear Model
We use a log-linear interpolation to combine differ-
ent models: the IBM-1, the inverse IBM-1, the an-
2Sentences are equivalent to segments in this paper.
79
chor words model as well as the IBM-4. K denotes
the total number of models.
We go through all positions in the bilingual sen-
tences and find the best position for segmenting the
sentence:
(?i, j?, m?) = argmax
i,j,m
{ K?
k=1
?khk(j, i,m|fJ1 , eI1)
}
,
where i ? [1, I ? 1] and j ? [1, J ? 1] are posi-
tions in the source and target sentences respectively.
The feature functions are described in the follow-
ing sections. In most cases, the sentence pairs are
quite long and even after one segmentation we may
still have long sub-segments. Therefore, we separate
the sub-segment pairs recursively until the length of
each new segment is less than a defined value.
3.3 Normalized IBM-1
The function in Equation 2 can be normalized by
the source sentence length with a weighting ? as de-
scribed in (Xu et al, 2005):
The monotone alignment is calculated as
h1(j, i, 1|fJ1 , eI1) = log(p(f j1 |ei1)??
1
j+(1??) (3)
?p(fJj+1|eIi+1)??
1
J?j+(1??)),
and the non-monotone alignment is formulated in
the same way.
We also use the inverse IBM-1 as a feature, by ex-
changing the place of ei1 and f j1 its monotone align-
ment is calculated as:
h2(j, i, 1|fJ1 , eI1) = log(p(ei1|f j1 )??
1
i+(1??) (4)
?p(eIi+1|fJj+1)??
1
I?i+(1??))
3.4 Anchor Words
In the task of extracting parallel sentences from
the paragraph-aligned corpus, selecting some anchor
words as preferred segmentation positions can ef-
fectively avoid the extraction of incomplete segment
pairs. Therefore we use an anchor words model to
prefer the segmentation at the punctuation marks,
where the source and target words are identical:
h3(j, i,m|fJ1 , eI1) =
{ 1 : fj = ei ? ei ? A
0 : otherwise
A is a user defined anchor word list, here we use
A={.,??;}. If the corresponding model scaling factor
?3 is assigned a high value, the segmentation posi-
tions are mostly after anchor words.
3.5 IBM-4 Word Alignment
If we already have the IBM-4 Viterbi word align-
ments for the parallel sentences and need to retrain
the system, for example to optimize the training pa-
rameters, we can include the Viterbi word align-
ments trained on the original corpora into the binary
segmentation. In the monotone case, the model is
represented as
h4(j, i, 1|fJ1 , eI1) =
log
(
N(f j1 , ei1) +N(fJj+1, eIi+1)
N(fJ1 , eI1)
)
,
where N(f j1 , ei1) denotes the number of the align-
ment links inside the matrix (1, 1) and (j, i). In the
non-monotone case the model is formulated in the
same way.
3.6 Word Alignment Concatenation
As described in Section 2, our translation is based on
phrases, that means for an input sentence we extract
all phrases matched in the training corpus and trans-
late with these phrase pairs. Although the aim of
segmentation is to split parallel text into translated
segment pairs, but the segmentation is still not per-
fect. During sentence segmentation we might sep-
arate a phrase into two segments, so that the whole
phrase pair can not be extracted.
To avoid this, we concatenate the word align-
ments trained with the segmentations of one sen-
tence pair. During the segmentation, the position of
each segmentation point in the sentence is memo-
rized. After training the word alignment model with
the segmented sentence pairs, the word alignments
are concatenated again according to the positions of
their segments in the sentences. The original sen-
tence pairs and the concatenated alignments are then
used for the phrase extraction.
80
Table 1: Corpus Statistics: NIST
Chinese English
Train Sentences 8.64 M
Running Words 210 M 226 M
Average Sentence Length 24.4 26.3
Vocabulary 224 268 359 623
Singletons 98 842 156 493
Segmentation Sentences 17.9 M
Running Words 210 M 226 M
Average Sentence Length 11.7 12.6
Vocabulary 221 517 353 148
Singletons 97 062 152 965
Segmentation with Additional Data Sentences 19.5 M
Running Words 230 M 248 M
Added Running Words 8.0% 8.2%
Evaluation Sentences 878 3 512
Running Words 24 111 105 516
Vocabulary 4 095 6 802
OOVs (Running Words) 8 658
4 Translation Experiments
4.1 Bilingual Sentences Extraction Methods
In this section, we describe the different methods to
extract the bilingual sentence pairs from the docu-
ment aligned corpus.
Given each document pair, we assume that the
paragraphs are aligned one to one monotone if both
the source and target language documents contain
the same number of paragraphs; otherwise the para-
graphs are aligned with the Champollion tool.
Starting from the parallel paragraphs we extract
the sentences using three methods:
1. Binary segmentation
The segmentation method described in Sec-
tion 3 is applied by treating the paragraph pairs
as long sentence pairs. We can use the anchor
words model described in Section 3.4 to prefer
splitting at punctuation marks.
The lexicon parameters p(f |e) in Equation 2
are estimated as follows: First the sentences are
aligned roughly using the dynamic program-
ming algorithm. Training on these aligned sen-
tences, we get the initial lexicon parameters.
Then the binary segmentation algorithm is ap-
plied to extract the sentences again.
2. Champollion
After a paragraph is divided into sentences at
punctuation marks, the Champollion tool (Ma,
2006) is used, which applies dynamic program-
ming for the sentence alignment.
3. Combination
The bilingual corpora produced by the binary
segmentation and Champollion methods are
concatenated and are used in the training of the
translation model. Each corpus is assigned a
weight. During the training of the word align-
ment models, the counts of the lexicon en-
tries are linearly interpolated using the corpus
weights.
4.2 Translation Tasks
We will present the translation results on two
Chinese-English tasks.
1. On the large data track NIST task (NIST,
2005), we will show improvements using the
refined binary segmentation method.
81
Table 2: Corpus Statistics: FBIS
Segmentation Champollion
Chinese English Chinese English
Train Sentences 739 899 177 798
Running Words 8 588 477 10 111 752 7 659 776 9 801 257
Average Sentence Length 11.6 13.7 43.1 55.1
Vocabulary 34 896 56 573 34 377 55 775
Singletons 4 775 19 283 4 588 19 004
Evaluation Sentences 878 3 513 878 3 513
Running Words 24 111 105 516 24 111 105 516
Vocabulary 4 095 6 802 4 095 6 802
OOVs (Running Words) 109 2 257 119 2 309
2. On the FBIS corpus, we will compare the dif-
ferent sentence extraction methods described in
Section 4.1 with respect to translation perfor-
mance. We do not apply the extraction meth-
ods on the whole NIST corpora, because some
corpora provided by the LDC (LDC, 2005) are
sentence aligned but not document aligned.
4.3 Corpus Statistics
The training corpora used in NIST task are a set of
individual corpora including the FBIS corpus. These
corpora are provided by the Linguistic Data Consor-
tium (LDC, 2005), the domains are news articles.
The translation experiments are carried out on the
NIST 2002 evaluation set.
As shown in Table 1, there are 8.6 million sen-
tence pairs in the original corpora of the NIST task.
The average sentence length is about 25. After seg-
mentation, there are twice as many sentence pairs,
i.e. 17.9 million, and the average sentence length
is around 12. Due to a limitation of GIZA++, sen-
tences consisting of more than one hundred words
are filtered out. Segmentation of long sentences cir-
cumvents this restriction and allows us include more
data. Here we were able to add 8% more Chinese
and 8.2% more English running words to the train-
ing data. The training time is also reduced.
Table 2 presents statistics of the FBIS data. Af-
ter the paragraph alignment described in Section 4.1
we have nearly 81 thousand paragraphs, 8.6 million
Chinese and 10.1 million English running words.
One of the advantages of the binary segmentation is
that we do not loose words during the bilingual sen-
tences extraction. However, we produce sentence
pairs with very different lengths. Using Champol-
lion we loose 10.8% of the Chinese and 3.1% of the
English words.
4.4 Segmentation Parameters
We did not optimize the log-linear model scaling
factors for the binary segmentation but used the fol-
lowing fixed values: ?1 = ?2 = 0.5 for the IBM-1
models in both directions; ?3 = 108, if the anchor
words model is is used; ?4 = 30, if the IBM-4 model
is used. The maximum sentence length is 25.
4.5 Evaluation Criteria
We use four different criteria to evaluate the transla-
tion results automatically:
? WER (word error rate):
The WER is computed as the minimum num-
ber of substitution, insertion and deletion oper-
ations that have to be performed to convert the
generated sentence into the reference sentence,
divided by the reference sentence length.
? PER (position-independent word error rate):
A shortcoming of the WER is that it requires a
perfect word order. The word order of an ac-
ceptable sentence can be differ from that of the
target sentence, so that the WER measure alone
could be misleading. The PER compares the
words in the two sentences ignoring the word
order.
? BLEU score:
This score measures the precision of unigrams,
82
0 0.2 0.4 0.6 0.8 131.8
31.9
32
32.1
32.2
Weight for the Binary Segmentation
BLE
U[%
]
Figure 2: Translation performance as a function of
the weight for the binary segmentation ? ( weight
for Champollion: 1? ? )
bigrams, trigrams and fourgrams with a penalty
for too short sentences. (Papineni et al, 2002).
? NIST score:
This score is similar to BLEU, but it uses
an arithmetic average of N-gram counts rather
than a geometric average, and it weights more
heavily those N-grams that are more informa-
tive. (Doddington, 2002).
The BLEU and NIST scores measure accuracy,
i.e. larger scores are better. In our evaluation the
scores are measured as case insensitive and with re-
spect to multiple references.
4.6 Translation Results
For the segmentation of long sentences into short
segments, we performed the experiments on the
NIST task. Both in the baseline and the segmenta-
tion systems we obtain 4.7 million bilingual phrases
during the translation. The method of alignment
concatenation increases the number of the extracted
bilingual phrase pairs from 4.7 million to 4.9 mil-
lion, the BLEU score is improved by 0.1%. By
including the IBM-4 Viterbi word alignment, the
NIST score is improved. The training of the base-
line system requires 5.9 days, after the sentence seg-
mentation it requires only 1.5 days. Moreover, the
segmentation allows the inclusion of long sentences
that are filtered out in the baseline system. Using
the added data, the translation performance is en-
hanced by 0.3% in the BLEU score. Because of
the long translation period, the translation parame-
ters are only optimized on the baseline system with
respect to the BLEU score, we could expect a further
improvement if the parameters were also optimized
on the segmentation system.
Our major objective here is to introduce another
approach to parallel sentence extraction: binary seg-
mentation of the bilingual texts recursively. We use
the paragraph-aligned corpus as a starting point. Ta-
ble 4 presents the translation results on the train-
ing corpora generated by the different methods de-
scribed in Section 4.1. The translation parameters
are optimized with the respect to the BLEU score.
We observe that the binary segmentation methods
are comparable to Champollion and the segmenta-
tion with anchors outperforms the one without an-
chors. By combining the methods of Champol-
lion and the binary segmentation with anchors, the
BLEU score is improved by 0.4% absolutely.
We optimized the weightings for the binary seg-
mentation method, the sum of the weightings for
both methods is one. As shown in Figure 2, using
one of the methods alone does not produce the best
result. The maximum BLEU score is attained when
both methods are combined with equal weightings.
5 Discussion and Future Work
We successfully applied the binary sentence seg-
mentation method to extract bilingual sentence pairs
from the document aligned texts. The experiments
on the FBIS data show an enhancement of 0.4% of
the BLEU score compared to the score obtained us-
ing a state-of-art sentence aligner. In addition to the
encouraging results obtained, further improvements
could be achieved in the following ways:
1. By extracting bilingual paragraphs from the
documents, we lost running words using Cham-
pollion. Applying the segmentation approach
to paragraph alignment might avoid the loss of
this data.
2. We combined a number of different models in
the binary segmentation, such as IBM-1, and
anchor words. The model weightings could be
optimized with respect to translation quality.
83
Table 3: Translation Results using Refined Segmentation Methods on NIST task
Error Rate[%] Accuracy
WER PER NIST BLEU[%]
Baseline 62.7 42.1 8.95 33.5
Segmentation 62.6 42.4 8.80 33.5
Segmentation + concatenation 62.4 42.3 8.84 33.6
Segmentation + concatenation + IBM-4 62.8 42.4 8.91 33.6
Segmentation + added data 62.9 42.5 9.00 33.9
Table 4: Translation Results on Sentence Alignment Task with FBIS Training Corpus
Error Rate[%] Accuracy
WER PER NIST BLEU[%]
Champollion 64.2 43.7 8.61 31.8
Segmentation without Anchors 64.3 44.4 8.57 31.8
Segmentation with Anchors 64.0 43.9 8.58 31.9
Champollion + Segmentation with Anchors 64.3 44.2 8.57 32.2
3. In the binary segmentation method, an incor-
rect segmentation results in further mistakes
in the segmentation decisions of all its sub-
segments. An alternative method (Wu, 1997)
makes decisions at the end but has a high com-
putational requirement. A restricted expansion
of the search space might better balance seg-
mentation accuracy and the efficiency.
6 Acknowledgments
This work was supported by the European Union
under the integrated project TC-Star (Technology
and Corpora for Speech to Speech Translation,
IST-2002-FP6-506738, http://www.tc-star.org) and
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Y. Deng, S. Kumar, and W. Byrne. 2006. Segmenta-
tion and alignment of parallel text for statistical ma-
chine translation. Natural Language Engineering, Ac-
cepted. To appear.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology,
pages 128?132, San Diego, California, March.
W. A. Gale and K. W. Church. 1993. A program for
aligning sentences in bilingual corpora. Computa-
tional Linguistics, 19(1):75?90.
LDC. 2005. Linguistic data consortium resource home
page. http://www.ldc.upenn.edu/Projects/TIDES.
X. Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proceedings of the fifth interna-
tional conference on Language Resources and Evalu-
ation (LREC), Genoa, Italy, Accepted. To appear.
NIST. 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, July.
M. Simard and P. Langlais. 2003. Statistical transla-
tion alignment with compositionality constraints. In
NAACL 2003 Workshop on Building and Using Paral-
lel Texts: Data Driven Machine Translation and Be-
yond, Edmonton, Canada, May.
84
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
J. Xu, R. Zens, and H. Ney. 2005. Sentence segmentation
using IBM word alignment model 1. In Proceedings of
EAMT 2005 (10th Annual Conference of the European
Association for Machine Translation), pages 280?287,
Budapest, Hungary, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
85
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 1?8,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Chunk-Level Reordering of Source Language Sentences with
Automatically Learned Rules for Statistical Machine Translation
Yuqi Zhang and Richard Zens and Hermann Ney
Human Language Technology and Pattern Recognition
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{yzhang,zens,ney}@cs.rwth-aachen.de
Abstract
In this paper, we describe a source-
side reordering method based on syntac-
tic chunks for phrase-based statistical ma-
chine translation. First, we shallow parse
the source language sentences. Then, re-
ordering rules are automatically learned
from source-side chunks and word align-
ments. During translation, the rules are
used to generate a reordering lattice for
each sentence. Experimental results are
reported for a Chinese-to-English task,
showing an improvement of 0.5%?1.8%
BLEU score absolute on various test sets
and better computational efficiency than
reordering during decoding. The exper-
iments also show that the reordering at
the chunk-level performs better than at the
POS-level.
1 Introduction
In machine translation, reordering is one of the ma-
jor problems, since different languages have differ-
ent word order requirements. Many reordering con-
straints have been used for word reorderings, such
as ITG constraints (Wu, 1996), IBM constraints
(Berger et al, 1996) and local constraints (Kanthak
et al, 2005). These approaches do not make use of
any linguistic knowledge.
Several methods have been proposed to use syn-
tactic information to handle the reordering problem,
e.g. (Wu, 1997; Yamada and Knight, 2001; Gildea,
2003; Melamed, 2004; Graehl and Knight, 2004;
Galley et al, 2006). One approach makes use of
bitext grammars to parse both the source and tar-
get languages. Another approach makes use of syn-
tactic information only in the target language. Note
that these models have radically different structures
and parameterizations than phrase-based models for
SMT.
Another kind of approaches is to use syntactic in-
formation in rescoring methods. (Koehn and Knight,
2003) apply a reranking approach to the sub-task
of noun-phrase translation. (Och et al, 2004) and
(Shen et al, 2004) describe the use of syntactic fea-
tures in reranking the output of a full translation sys-
tem, but the syntactic features give very small gains.
In this paper, we present a strategy to reorder
a source sentence using rules based on syntactic
chunks. It is possible to integrate reordering rules di-
rectly into the search process, but here, we consider
a more modular approach: easy to exchange reorder-
ing strategy. To avoid hard decisions before SMT,
we generate a source-reordering lattice instead of a
single reordered source sentence as input to the SMT
system. Then, the decoder uses the reordered source
language model as an additional feature function. A
language model trained on the reordered source-side
chunks gives a score for each path in the lattice. The
novel ideas in this paper are:
? reordering of the source sentence at the chunk
level,
? representing linguistic chunks-reorderings in a
lattice.
1
The rest of this paper is organized as follows. Sec-
tion 2 presents a review of related work. In Sec-
tions 3, we review the phrase-based translation sys-
tem used in this work and propose the framework
of the new reordering method. In Section 4, we in-
troduce the details of the reordering rules, how they
are defined and how to extract them. In Section 5,
we explain how to apply the rules and how to gen-
erate reordering lattice. In Section 6, we present
some results that show that the chunk-level source
reordering is helpful for phrase-based statistical ma-
chine translation. Finally, we conclude this paper
and discuss future work in Section 7.
2 Related Work
Beside the reordering methods during decoding, an
alternative approach is to reorder the input source
sentence to match the word order of the target sen-
tence.
Some reordering methods are carried out on syn-
tactic source trees. (Collins et al, 2005) describe
a method for reordering German for German-to-
English translation, where six transformations are
applied to the surface string of the parsed source
sentence. (Xia and McCord, 2004) propose an ap-
proach for translation from French-to-English. This
approach automatically extracts rewrite patterns by
parsing the source and target sides of the training
corpus. These rewrite patterns can be applied to any
input source sentence so that the rewritten source
and target sentences have similar word order. Both
methods need a parser to generate trees of source
sentences and are applied only as a preprocessing
step.
Another kind of source reordering methods be-
sides full parsing is based on Part-Of-Speech (POS)
tags or word classes. (Costa-jussa` and Fonollosa,
2006) view the source reordering as a translation
task that translate the source language into a re-
ordered source language. Then, the reordered source
sentence is taken as the single input to the standard
SMT system.
(Chen et al, 2006) automatically extract rules
from word alignments. These rules are defined at
the POS level and the scores of matching rules are
used as additional feature functions during rescor-
ing. (Crego and Marin?o, 2006) integrate source-side
reordering into SMT decoding. They automatically
learn rewrite patterns from word alignment and rep-
resent the patterns with POS tags. To our knowledge
no work is reported on the reordering with shallow
parsing.
Decoding lattices were already used in (Zens et
al., 2002; Kanthak et al, 2005). Those approaches
used linguistically uninformed word-level reorder-
ings.
3 System Overview
In this section, we will describe the phrase-based
SMT system which we use for the experiments.
Then, we will give an outline of the extentions with
the chunk-level source reordering model.
3.1 The Baseline Phrase-based SMT System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|fJ1 )
} (1)
= argmax
I,eI1
{
Pr(eI1) ? Pr(fJ1 |eI1)
} (2)
This decomposition into two knowledge sources
is known as the source-channel approach to sta-
tistical machine translation (Brown et al, 1990).
It allows an independent modeling of the target
language model Pr(eI1) and the translation model
Pr(fJ1 |eI1). The target language model describes
the well-formedness of the target language sentence.
The translation model links the source language sen-
tence to the target language sentence. The argmax
operation denotes the search problem, i.e., the gen-
eration of the output sentence in the target language.
A generalization of the classical source-channel
approach is the direct modeling of the posterior
probability Pr(eI1|fJ1 ). Using a log-linear model
2
(Och and Ney, 2002), we obtain:
Pr(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
I?,e?I?1
exp
(
?M
m=1 ?mhm(e?I
?
1 , fJ1 )
)
(3)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
(4)
The log-linear model has the advantage that addi-
tional models h(?) can be easily integrated into the
overall system. The model scaling factors ?M1 are
trained according to the maximum entropy principle,
e.g., using the GIS algorithm. Alternatively, one can
train them with respect to the final translation quality
measured by an error criterion (Och, 2003).
The log-linear model is a natural framework to in-
tegrate many models. The baseline system uses the
following models:
? phrase translation model
? phrase count features
? word-based translation model
? word and phrase penalty
? target language model (6-gram)
? distortion model (assigning costs based on the
jump width)
All the experiments in the paper are evaluated with-
out rescoring. More details about the baseline sys-
tem can be found in (Mauser et al, 2006)
3.2 Source Sentence Reordering Framework
Encouraged by the work of (Xia and McCord, 2004)
and (Crego and Marin?o, 2006), we also reorder the
source language side. Compared to reordering on
the target language side, one advantage is the effi-
ciency since the reordering lattice can be translated
monotonically as in (Zens et al, 2002). Another ad-
vantage is that there is correct sentence information
POS tagging
shallow chunking
Translation Process
Standard Translation Proces
with Source Reordering
source text sentences
reordering rules
SMT system
translation output translation output
source text sentences
SMT system
source reordering lattice
Figure 1: Illustration of the translation process with
and without source reordering.
for the reordering methods, because the source sen-
tences are always given. Syntactic reordering on tar-
get language is difficult, since the methods will de-
grade much because of the errors in hypothesis.
We apply reordering at the syntactic chunk level
which can been seen as an intermediate level be-
tween full parsing and POS tagging. Figure 1 shows
the differences between the new translation frame-
work and the standard translation process. A re-
ordering lattice replaces the original source sentence
as the input to the translation system. The use of a
lattice avoids hard decisions before translation. To
generate the reordering lattice, the source sentence is
first POS tagged and chunk parsed. Then, reorder-
ing rules are applied to the chunks to generate the
reordering lattice.
Reordering rules are the key information for
source reordering. They are automatically learned
from the training data. The details of these two mod-
ules will be introduced in Section 5.
4 Reordering Rules
There has been much work on learning and apply-
ing reordering rules on source language, such as
(Nie?en and Ney, 2001; Xia and McCord, 2004;
Collins et al, 2005; Chen et al, 2006; Crego and
Marin?o, 2006; Popovic? and Ney, 2006). The re-
ordering rules could be composed of words, POS
tags or syntactic tags of phrases. In our work, a rule
is composed of chunk tags and POS tags. There is
3
Table 1: Examples of reordering rules. (lhs: chunk
and POS tag sequence, rhs: permutation )
no. lhs rhs
1. NP0 PP1 u2 n3 0 1 2 3
2. NP0 PP1 u2 n3 3 0 1 2
3. DNP0 NP1 V P2 0 1 2
4. DNP0 NP1 V P2 1 0 2
5. DNP0 NP1 m2 0 1 2
6. DNP0 NP1 m2 ad3 3 0 1 2
7. DNP0 NP1 m2 ad3 v4 4 3 0 1 2
no hierarchical structure in a rule.
4.1 Definition of Reordering Rules
First, we show some rule examples in Table 1. A re-
ordering rule consists of a left-hand-side (lhs) and a
right-hand-side (rhs). The left-hand-side is a syn-
tactic rule (chunk or POS tags), while the right-
hand-side is the reordering positions of the rule. Dif-
ferent rules can share the same left-hand-side, such
as rules no. 1, 2 and no. 3, 4. The rules record
not only the real reordered chunk sequence, but also
the monotone chunk sequences, like no. 1, 3 and
5. Note that the same tag sequence can appear mul-
tiple times according to different contexts, such as
DNP0 NP1 m2 # 0 1 2 in rules no. 5, 6, 7.
4.2 Extraction of Reordering Rules
The extraction of reordering rules is based on the
word alignment and the source sentence chunks.
Here, we train word alignments in both directions
with GIZA++ (Och and Ney, 2003). To get algn-
ment with high accuracy, we use the intersection
alignment here.
For a given word-aligned sentence pair
(fJ1 , eI1, aJ1 ), the source word sequence fJ1 is
first parsed into a chunk sequence FK1 . Accord-
ingly, the word-to-word alignment aJ1 is changed
to a chunk-to-word alignment a?K1 which is the
combination of the target words aligned to the
source words in a chunk. It is defined as:
a?k = {i|i = aj ? j ? [jk, jk+1 ? 1]}
Figure 2: Illustration of three kinds of phrases:
(a)monotone phrase, (b)reordering phrase, (c)cross
phrase. The black box is a word-to-word alignment.
The gray box is a chunk-to-word alignment.
Here, jk denotes the position of the first source word
in kth chunk. The new alignment is 1 : m from
source chunks to target words. It also means a?k is a
set of positions of target words.
We apply the standard phrase extraction algorithm
(Zens et al, 2002) to (FK1 , eI1, a?K1 ). Discarding the
cross phrases, we keep the other phrases as rules. In
a cross phrase, at least two chunk-word alignments
overlap on the target language side. An example
of a cross phrase is illustrated in Figure 2(c). Fig-
ure 2(a) and (b) illustrate the phrases for reordering
rules, which could be monotone phrases or reorder-
ing phrases.
5 Reordering Lattice Generation
5.1 Parsing the Source Sentence
The first step of chunk parsing is word segmentation.
Then, a POS tagger is usually needed for further
syntactic analysis. In our experiments, we use the
tool of ?Inst. of Computing Tech., Chinese Lexical
Analysis System (ICTCLAS)? (Zhang et al, 2003),
which does the two tasks in one pass.
Referring to the description of the chunking task
in CoNLL-20001, instead of English, a Chinese
chunker is processed and evaluated. Each word is
assigned a chunk tag, which contains the name of the
chunk type and ?B? for the first word of the chunk
and ?I? for each other word in the chunk. The ?O?
chunk tag is used for tokens which are not part of
any chunk. We use the maximum entropy tool YAS-
1http://www.cnts.ua.ac.be/conll2000/chunking/
4
Figure 3: Example of applying rules. The left part is the used rules. The right part is the generated new
orders of source words.
MET2 to learn the chunking model. The model is
based on a combination of word and POS tags. Since
specific training and test data are not available for
Chinese chunking, we convert subtrees of the Chi-
nese treebank (LDC2005T01) into chunks. As there
are many ways to choose a subtree, we uses the min-
imum subtree with the following constraints:
? a subtree has more than one child,
? the children of a subtree are all leaves.
Compared to chunking of English as in CoNLL-
2000, there are more chunk types (24 instead of 6)
and no single-word chunks. These two aspects make
chunking for Chinese harder.
5.2 Applying Reordering Rules
First, we search the reordering rules, in which the
chunk sequence matches any tag sequence in the in-
put sentence. A source sentence has many paths
generated by the rules . For a word uncovered by any
rules, its POS tag is used. Each path corresponds to
one sentence permutation.
The left part of the Figure 3 shows seven possible
coverages, the right part is the reordering for each
coverage. Some of the reorderings are identical, like
the permutations in line 1, 3 and 5. That is because
one word sequence is memorized by several rules in
different contexts.
5.3 Lattice Weighting
All reorderings of an input sentence S are com-
pressed and stored in a lattice. Each path is a possi-
2http://www-i6.informatik.rwth-aachen.de/web/Software
/index.html
ble reordering S? and is given a weight W . In this
paper, the weight is computed using a source lan-
guage model p(S?). The weight is used directly in
the decoder, integrated into Equation (4). There is
also a scaling factor for this weight, which is op-
timized together with other scaling factors on the
development data. The probability of the reordered
source sentence is calculated as follows: for a re-
ordered source sentence w1w2...wn, the trigram lan-
guage model is:
p(S?) =
N
?
n=1
p(wn|wn?2, wn?1) (5)
Beside a word N-gram language model, a POS tag
N-gram model or a chunk tag N-gram model could
be used as well.
In this paper, we use a word trigram model. The
model is trained on reordered training source sen-
tences. A training source sentence is parsed into
chunks. In the same way as described in Section
4.2, word-to-word alignments is converted to chunk-
to-word alignments. We reorder the source chunks
to monotonize the chunk-to-word alignments. The
chunk boundaries are kept when this reordering is
done.
6 Experiments
6.1 Chunking Result
In this section, we report results for chunk parsing.
The annotation of the data is derived from the Chi-
nese treebank (LDC2005T01). The corpus is split
into two parts: 1000 sentences are randomly se-
5
Table 2: Statistics of training and test corpus for
chunk parsing.
train test
sentences 17 785 1 000
words 486 468 21 851
chunks 105 773 4 680
words out of chunks 244 416 10 282
Table 3: Chunk parsing result on 1000 sentences.
accuracy precision recall F-measure
74.51% 65.2% 61.5% 63.3
lected as test data. The remaining part is used for
training. The corpus is from the newswire domain.
Table 2 shows the corpus statistics. For the 4 680
chunks in the test set, the chunker has found 4 414
chunks, of which 2 879 are correct. Following the
criteria of CoNLL-2000, the chunker is evaluated
using the F-score, which is a combination of pre-
cision and recall. The result is shown in Table 3.
The accuracy is evaluated at the word level, the
other three metrics are evaluated at the chunk level.
The results at the chunk level are worse than at the
word level, because a chunk is counted as correct
only if the chunk tag and the chunk boundaries are
both correct.
6.2 Translation Results
For the translation experiments, we report the two
accuracy measures BLEU (Papineni et al, 2002)
and NIST (Doddington, 2002) as well as the two
error rates word error rate (WER) and position-
independent word error rate (PER).
We perform translation experiments on the Ba-
sic Traveling Expression Corpus (BTEC) for the
Chinese-English task. It is a speech translation task
in the domain of tourism-related information. We
report results on the IWSLT 2004, 2005 and 2006
evaluation test sets. There are 16 reference trans-
lations for the IWSLT 2004 and 2005 tasks and 7
reference translations for the IWSLT 2006 task.
Table 4 shows the corpus statistics of the task. A
training corpus is used to train the translation model,
the language model and to obtain the reordering
Table 4: Statistics of training and test corpora for the
IWSLT tasks.
Chinese English
Train Sentences 40k
Words 308k 377k
Dev Sentences 489
Words 5 478 6 008
Test Sentences 500
IWSLT04 Words 3 866 3 581
Test Sentences 506
IWSLT05 Words 3 652 3 579
Test Sentences 500
IWSLT06 Words 5 846 ?
rules. A development corpus is used to optimize the
scaling factors for the BLEU score. The English text
is processed using a tokenizer. The Chinese text pro-
cessing uses word segmentation with the ICTCLAS
segmenter (Zhang et al, 2003). The translation is
evaluated case-insensitive and without punctuation
marks.
The translation results are presented in Table 5.
The baseline system is a non-monotone translation
system, in which the decoder does reordering on
the target language side. Compared to the base-
line system, the source reordering method improves
the BLEU score by 0.5% ? 1.8% absolute. It also
achieves a better WER. Note that the used chun-
ker here is out-of-domain 3. An improvement is
achieved even with a low F-measure for chunking.
So, we could hope that larger improvement is possi-
ble using a high-accuracy chunker.
Though the input is a lattice, the source reordering
is still faster than the reordering during decoding,
e.g. for the IWSLT 2006 test set, the baseline system
took 17.5 minutes and the source reordering system
took 12.3 minutes. The result also indicates that the
non-monotone decoding hurts the performance in a
source reordering framework. A similar conclusion
is also presented in (Xia and McCord, 2004).
Additional experiments we carried out to compare
POS-level and chunk-level reorderings. We delete
the chunk information and keep the POS tags. Then,
3The chunker is trained on newswire data, but the test data
is from the tourism domain.
6
Table 5: Translation performance for the Chinese-English IWSLT task
WER[%] PER[%] NIST BLEU[%]
IWSLT04 baseline 47.3 38.2 7.78 39.1
source reordering 46.3 37.2 7.70 40.9
IWSLT05 baseline 45.0 37.3 7.40 41.8
source reordering 44.6 36.8 7.51 42.3
IWSLT06 baseline 67.4 50.0 6.65 22.4
source reordering 65.6 50.4 6.46 23.3
source reordering+non-monotone decoder 66.5 50.3 6.52 22.4
Table 6: Translation performance of reordering
methods on IWSLT 2004 test set
WER PER NIST BLEU
[%] [%] [%]
Baseline 47.3 38.2 7.78 39.1
POS 46.9 37.5 7.38 39.7
Chunk 46.3 37.2 7.70 40.9
Table 7: Lattice information for the Chinese-English
IWSLT 2004 test data
avg. density used translation
pro sent rules time [min/sec]
POS 15.7 6 868 7:08
Chunk 8.2 3 685 3:47
we rerun the source reordering system on the IWSLT
2004 test set. The translation results are shown in
Table 6. Though the accuracy of chunking is low,
the chunk-level method gets better results than POS-
level method. With POS tags, we get more reorder-
ing rules and more paths in the lattice, since the sen-
tence length is longer than with chunks. The statis-
tics are shown in Table 7.
7 Conclusions and Future Work
This paper presents a source-side reordering method
which is based on syntactic chunks. The reordering
rules are automatically learned from bilingual data.
To avoid hard decision before decoding, a reorder-
ing lattice representing all possible reorderings is
used instead of single source sentence for decoding.
The experiments demonstrate that even with a very
poor chunker, the chunk-level source reordering is
still helpful for a state-of-the-art statistical transla-
tion system and it has better performance than the
POS-level source reordering and target-side reorder-
ing.
There are some directions for future work. First,
we would like to try this method on larger data sets
and other language pairs. Second, we are going to
improve the chunking accuracy. Third, we would
reduce the number of rules and prune the lattice.
Acknowledgments
This material is partly based upon work sup-
ported by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-
C-0023, and was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5)
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?72, March.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A statistical approach to machine translation. Com-
putational Linguistics, 16(2):79?85, June.
B. Chen, M. Cettolo, and M. Federico. 2006. Reordering
rules for phrase-based statistical machine translation. In
Int. Workshop on Spoken Language Translation Evaluation
Campaign on Spoken Language Translation, pages 1?15,
Kyoto, Japan, November.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restructur-
ing for statistical machine translation. In Proc. of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 531?540, Ann Arbor, Michigan, June.
7
M. R. Costa-jussa` and J. A. R. Fonollosa. 2006. Statistical ma-
chine reordering. In Proc. of the Conf. on Empirical Meth-
ods in Natural Language Processing, pages 70?76, Sydney,
Australia, July.
J. M. Crego and J. B. Marin?o. 2006. Integration of postag-
based source reordering into SMT decoding by an extended
search graph. In Proc. of AMTA06, pages 29?36, Mas-
sachusetts, USA, August.
G. Doddington. 2002. Automatic evaluation of machine trans-
lation quality using n-gram co-occurrence statistics. In Proc.
ARPA Workshop on Human Language Technology.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and train-
ing of context-rich syntactic translation models. In Proc. of
the 21st Int. Conf. on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Lin-
guistics, pages 961?968, Sydney, Australia, July.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. In Proc. of the 41th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages 80?87,
Sapporo, Japan, July.
J. Graehl and K. Knight. 2004. Training tree transducers.
In HLT-NAACL 2004: Main Proc., pages 105?112, Boston,
Massachusetts, USA, May 2 - May 7.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In 43rd Annual Meeting of the Assoc. for
Computational Linguistics: Proc. Workshop on Building and
Using Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, Ann Arbor, Michigan, June.
P. Koehn and K. Knight. 2003. Empirical methods for com-
pound splitting. In Proc. 10th Conf. of the Europ. Chapter
of the Assoc. for Computational Linguistics (EACL), pages
347?354, Budapest, Hungary, April.
A. Mauser, R. Zens, E. Matusov, S. Hasan, and H. Ney. 2006.
The RWTH Statistical Machine Translation System for the
IWSLT 2006 Evaluation. In Proc. of the Int. Workshop
on Spoken Language Translation, pages 103?110, Kyoto,
Japan.
I. Melamed. 2004. Statistical machine translation by parsing.
In The Companion Volume to the Proc. of 42nd Annual Meet-
ing of the Association for Computational Linguistics, pages
653?660.
S. Nie?en and H. Ney. 2001. Morpho-syntactic analysis for
reordering in statistical machine translation. In Proc. of MT
Summit VIII, pages 247?252.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 295?302, Philadelphia,
PA, July.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51, March.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, and D. Radev. 2004. A smorgasbord of features for
statistical machine translation. In Proc. 2004 Human Lan-
guage Technology Conf. / North American Chapter of the
Association for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 161?168, Boston,MA.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of the 41th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proc. of the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311?318, Philadelphia,
PA, July.
M. Popovic? and H. Ney. 2006. POS-based word reorderings
for statistical machine translation. In Proc. of the Fifth Int.
Conf. on Language Resources and Evaluation (LREC).
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative rerank-
ing for machine translation. In HLT-NAACL 2004: Main
Proc., pages 177?184, Boston, Massachusetts, USA, May 2
- May 7.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997. A DP-
based search using monotone alignments in statistical trans-
lation. In Proc. 35th Annual Conf. of the Association for
Computational Linguistics, pages 289?296, Madrid, Spain,
July.
D. Wu. 1996. A polynomial-time algorithm for statistical ma-
chine translation. In Proc. 34th Annual Meeting of the As-
soc. for Computational Linguistics, pages 152?158, Santa
Cruz, CA, June.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?403, September.
F. Xia and M. McCord. 2004. Improving a statistical MT sys-
tem with automatically learned rewrite patterns. In Proc. of
COLING04, pages 508?514, Geneva, Switzerland, Aug 23?
Aug 27.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proc. of the 39th Annual Meeting of
the Association for Computational Linguistics (ACL), pages
523?530, Toulouse, France, July.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based statistical
machine translation. In M. Jarke, J. Koehler, and G. Lake-
meyer, editors, 25th German Conf. on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artificial Intel-
ligence (LNAI), pages 18?32, Aachen, Germany, September.
Springer Verlag.
H. P. Zhang, Q. Liu, X. Q. Cheng, H. Zhang, and H. K. Yu.
2003. Chinese lexical analysis using hierarchical hidden
markov model. In Proc. of the second SIGHAN workshop
on Chinese language processing, pages 63?70, Morristown,
NJ, USA.
8
Proceedings of the Second Workshop on Statistical Machine Translation, pages 33?39,
Prague, June 2007. c?2007 Association for Computational Linguistics
Can We Translate Letters?
David Vilar, Jan-T. Peter and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
D-52056 Aachen, Germany
{vilar,peter,ney}@cs.rwth-aachen.de
Abstract
Current statistical machine translation sys-
tems handle the translation process as the
transformation of a string of symbols into
another string of symbols. Normally the
symbols dealt with are the words in differ-
ent languages, sometimes with some addi-
tional information included, like morpho-
logical data. In this work we try to push
the approach to the limit, working not on the
level of words, but treating both the source
and target sentences as a string of letters.
We try to find out if a nearly unmodified
state-of-the-art translation system is able to
cope with the problem and whether it is ca-
pable to further generalize translation rules,
for example at the level of word suffixes and
translation of unseen words. Experiments
are carried out for the translation of Catalan
to Spanish.
1 Introduction
Most current statistical machine translation systems
handle the translation process as a ?blind? transfor-
mation of a sequence of symbols, which represent
the words in a source language, to another sequence
of symbols, which represent words in a target lan-
guage. This approach allows for a relative simplic-
ity of the models, but also has drawbacks, as re-
lated word forms, like different verb tenses or plural-
singular word pairs, are treated as completely differ-
ent entities.
Some efforts have been made e.g. to integrate
more information about the words in the form of Part
Of Speech tags (Popovic? and Ney, 2005), using addi-
tional information about stems and suffixes (Popovic?
and Ney, 2004) or to reduce the morphological vari-
ability of the words (de Gispert, 2006). State of the
art decoders provide the ability of handling different
word forms directly in what has been called factored
translation models (Shen et al, 2006).
In this work, we try to go a step further and treat
the words (and thus whole sentences) as sequences
of letters, which have to be translated into a new se-
quence of letters. We try to find out if the trans-
lation models can generalize and generate correct
words out of the stream of letters. For this approach
to work we need to translate between two related
languages, in which a correspondence between the
structure of the words can be found.
For this experiment we chose a Catalan-Spanish
corpus. Catalan is a romance language spoken in the
north-east of Spain and Andorra and is considered
by some authors as a transitional language between
the Iberian Romance languages (e.g. Spanish) and
Gallo-Romance languages (e.g. French). A common
origin and geographic proximity result in a similar-
ity between Spanish and Catalan, albeit with enough
differences to be considered different languages. In
particular, the sentence structure is quite similar in
both languages and many times a nearly monotoni-
cal word to word correspondence between sentences
can be found. An example of Catalan and Spanish
sentences is given in Figure 1.
The structure of the paper is as follows: In Sec-
tion 2 we review the statistical approach to machine
translation and consider how the usual techniques
can be adapted to the letter translation task. In Sec-
33
Catalan Perque` a mi m?agradaria estar-hi dues, una o dues setmanes, me?s o menys, depenent del
preu i cada hotel.
Spanish Porque a m?? me gustar??a quedarme dos, una o dos semanas, ma?s o menos, dependiendo del
precio y cada hotel.
English Because I would like to be there two, one or two weeks, more or less, depending on the
price of each hotel.
Catalan Si baixa aqu?? tenim una guia de la ciutat que li podem facilitar en la que surt informacio?
sobre els llocs me?s interessants de la ciutat.
Spanish Si baja aqu?? tenemos una gu??a de la ciudad que le podemos facilitar en la que sale infor-
macio?n sobre los sitios ma?s interesantes de la ciudad.
English If you come down here we have a guide book of the city that you can use, in there is
information about the most interesting places in the city.
Figure 1: Example Spanish and Catalan sentences (the English translation is provided for clarity).
tion 3 we present the results of the letter-based trans-
lation and show how to use it for improving transla-
tion quality. Although the interest of this work is
more academical, in Section 4 we discuss possible
practical applications for this approach. The paper
concludes in Section 5.
2 From Words To Letters
In the standard approach to statistical machine trans-
lation we are given a sentence (sequence of words)
fJ1 = f1 . . . fJ in a source language which is to be
translated into a sentence e?I1 = e?1 . . . e?I in a target
language. Bayes decision rule states that we should
choose the sentence which maximizes the posterior
probability
e?I1 = argmax
eI1
p(eI1|fJ1 ) , (1)
where the argmax operator denotes the search pro-
cess. In the original work (Brown et al, 1993) the
posterior probability p(eI1|fJ1 ) is decomposed fol-
lowing a noisy-channel approach, but current state-
of-the-art systems model the translation probabil-
ity directly using a log-linear model(Och and Ney,
2002):
p(eI1|fJ1 ) =
exp
(
?M
m=1 ?mhm(eI1, fJ1 )
)
?
e?I1
exp
(
?M
m=1 ?mhm(e?I1, fJ1 )
) ,
(2)
with hm different models, ?m scaling factors and
the denominator a normalization factor that can be
ignored in the maximization process. The ?m are
usually chosen by optimizing a performance mea-
sure over a development corpus using a numerical
optimization algorithm like the downhill simplex al-
gorithm (Press et al, 2002).
The most widely used models in the log lin-
ear combination are phrase-based models in source-
to-target and target-to-source directions, ibm1-like
scores computed at phrase level, also in source-to-
target and target-to-source directions, a target lan-
guage model and different penalties, like phrase
penalty and word penalty.
This same approach can be directly adapted to the
letter-based translation framework. In this case we
are given a sequence of letters FJ1 corresponding
to a source (word) string fJ1 , which is to be trans-
lated into a sequence of letters EI1 corresponding to
a string eI1 in a target language. Note that in this case
whitespaces are also part of the vocabulary and have
to be generated as any other letter. It is also impor-
tant to remark that, without any further restrictions,
the word sequences eI1 corresponding to a generated
letter sequence EI1 are not even composed of actual
words.
2.1 Details of the Letter-Based System
The vocabulary of the letter-based translation sys-
tem is some orders of magnitude smaller than the
vocabulary of a full word-based translation system,
at least for European languages. A typical vocabu-
lary size for a letter-based system would be around
70, considering upper- and lowercase letter, digits,
34
whitespace and punctuation marks, while the vocab-
ulary size of a word-based system like the ones used
in current evaluation campaigns is in the range of
tens or hundreds of thousands words. In a normal
situation there are no unknowns when carrying out
the actual translation of a given test corpus. The sit-
uation can be very different if we consider languages
like Chinese or Japanese.
This small vocabulary size allows us to deal with
a larger context in the models used. For the phrase-
based models we extract all phrases that can be used
when translating a given test corpus, without any
restriction on the length of the source or the tar-
get part1. For the language model we were able to
use a high-order n-gram model. In fact in our ex-
periments a 16-gram letter-based language model is
used, while state-of-the-art translation systems nor-
mally use 3 or 4-grams (word-based).
In order to better try to generate ?actual words?
in the letter-based system, a new model was added
in the log-linear combination, namely the count of
words generated that have been seen in the training
corpus, normalized with the length of the input sen-
tence. Note however that this models enters as an ad-
ditional feature function in the model and it does not
constitute a restriction of the generalization capabil-
ities the model can have in creating ?new words?.
Somehow surprisingly, an additional word language
model did not help.
While the vocabulary size is reduced, the average
sentence length increases, as we consider each let-
ter to be a unit by itself. This has a negative impact
in the running time of the actual implementation of
the algorithms, specially for the alignment process.
In order to alleviate this, the alignment process was
split into two passes. In the first part, a word align-
ment was computed (using the GIZA++ toolkit (Och
and Ney, 2003)). Then the training sentences were
split according to this alignment (in a similar way to
the standard phrase extraction algorithm), so that the
length of the source and target part is around thirty
letters. Then, a letter-based alignment is computed.
2.2 Efficiency Issues
Somewhat counter-intuitively, the reduced vocabu-
lary size does not necessarily imply a reduced mem-
1For the word-based system this is also the case.
ory footprint, at least not without a dedicated pro-
gram optimization. As in a sensible implementa-
tions of nearly all natural language processing tools,
the words are mapped to integers and handled as
such. A typical implementation of a phrase table is
then a prefix-tree, which is accessed through these
word indices. In the case of the letter-based transla-
tion, the phrases extracted are much larger than the
word-based ones, in terms of elements. Thus the to-
tal size of the phrase table increases.
The size of the search graph is also larger for
the letter-based system. In most current systems
the generation algorithm is a beam search algorithm
with a ?source synchronous? search organization.
As the length of the source sentence is dramatically
increased when considering letters instead of words,
the total size of the search graph is also increased, as
is the running time of the translation process.
The memory usage for the letter system can ac-
tually be optimized, in the sense that the letters can
act as ?indices? themselves for addressing the phrase
table and the auxiliary mapping structure is not nec-
essary any more. Furthermore the characters can be
stored in only one byte, which provides a signifi-
cant memory gain over the word based system where
normally four bytes are used for storing the indices.
These gains however are not expected to counteract
the other issues presented in this section.
3 Experimental Results
The corpus used for our experiment was built in the
framework of the LC-STAR project (Conejero et al,
2003). It consists of spontaneous dialogues in Span-
ish, Catalan and English2 in the tourism and travel-
ling domain. The test corpus (and an additional de-
velopment corpus for parameter optimization) was
randomly extracted, the rest of the sentences were
used as training data. Statistics for the corpus can
be seen in Table 1. Details of the translation system
used can be found in (Mauser et al, 2006).
The results of the word-based and letter-based
approaches can be seen in Table 2 (rows with la-
bel ?Full Corpus?). The high BLEU scores (up to
nearly 80%) denote that the quality of the trans-
lation is quite good for both systems. The word-
2The English part of the corpus was not used in our experi-
ments.
35
Spanish Catalan
Training Sentences 40 574
Running Words 482 290 485 514
Vocabulary 14 327 12 772
Singletons 6 743 5 930
Test Sentences 972
Running Words 12 771 12 973
OOVs [%] 1.4 1.3
Table 1: Corpus Statistics
based system outperforms the letter-based one, as
expected, but the letter-based system also achieves
quite a good translation quality. Example transla-
tions for both systems can be found in Figure 2. It
can be observed that most of the words generated
by the letter based system are correct words, and in
many cases the ?false? words that the system gen-
erates are very close to actual words (e.g. ?elos? in-
stead of ?los? in the second example of Figure 2).
We also investigated the generalization capabili-
ties of both systems under scarce training data con-
ditions. It was expected that the greater flexibility
of the letter-based system would provide an advan-
tage of the approach when compared to the word-
based approach. We randomly selected subsets of
the training corpus of different sizes ranging from
1 000 sentences to 40 000 (i.e. the full corpus) and
computed the translation quality on the same test
corpus as before. Contrary to our hopes, however,
the difference in BLEU score between the word-
based and the letter-based system remained fairly
constant, as can be seen in Figure 3, and Table 2
for representative training corpus sizes.
Nevertheless, the second example in Figure 2 pro-
vides an interesting insight into one of the possi-
ble practical applications of this approach. In the
example translation of the word-based system, the
word ?centreamericans? was not known to the sys-
tem (and has been explicitly marked as unknown in
Figure 2). The letter-based system, however, was
able to correctly learn the translation from ?centre-?
to ?centro-? and that the ending ?-ans? in Catalan
is often translated as ?-anos? in Spanish, and thus
a correct translation has been found. We thus chose
to combine both systems, the word-based system do-
ing most of the translation work, but using the letter-
based system for the translation of unknown words.
The results of this combined approach can be found
in Table 2 under the label ?Combined System?. The
combination of both approaches leads to a 0.5% in-
crease in BLEU using the full corpus as training ma-
terial. This increase is not very big, but is it over a
quite strong baseline and the percentage of out-of-
vocabulary words in this corpus is around 1% of the
total words (see Table 1). When the corpus size is
reduced, the gain in BLEU score becomes more im-
portant, and for the small corpus size of 1 000 sen-
tences the gain is 2.5% BLEU. Table 2 and Figure 3
show more details.
4 Practical Applications
The approach described in this paper is mainly of
academical interest. We have shown that letter-
based translation is in principle possible between
similar languages, in our case between Catalan and
Spanish, but can be applied to other closely related
language pairs like Spanish and Portuguese or Ger-
man and Dutch. The approach can be interesting for
languages where very few parallel training data is
available.
The idea of translating unknown words in a letter-
based fashion can also have applications to state-of-
the-art translation systems. Nowadays most auto-
matic translation projects and evaluations deal with
translation from Chinese or Arabic to English. For
these language pairs the translation of named en-
tities poses an additional problem, as many times
they were not previously seen in the training data
and they are actually one of the most informative
words in the texts. The ?translation? of these enti-
ties is in most cases actually a (more or less pho-
netic) transliteration, see for example (Al-Onaizan
and Knight, 2002). Using the proposed approach for
the translation of these words can provide a tighter
integration in the translation process and hopefully
increase the translation performance, in the same
way as it helps for the case of the Catalan-Spanish
translation for unseen words.
Somewhat related to this problem, we can find an
additional application in the field of speech recog-
nition. The task of grapheme-to-phoneme conver-
sion aims at increasing the vocabulary an ASR sys-
tem can recognize, without the need for additional
36
BLEU WER PER
Word-Based System Full Corpus 78.9 11.4 10.6
10k 74.0 13.9 13.2
1k 60.0 21.3 20.1
Letter-Based System Full Corpus 72.9 14.7 13.5
10k 69.8 16.5 15.1
1k 55.8 24.3 22.8
Combined System Full Corpus 79.4 11.2 10.4
10k 75.2 13.4 12.6
1k 62.5 20.2 19.0
Table 2: Translation results for selected corpus sizes. All measures are percentages.
Source (Cat) Be?, en principi seria per a les vacances de Setmana Santa que so?n les segu?ents que tenim
ara, entrant a juliol.
Word-Based Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando en julio.
Letter-Based Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando bamos en julio .
Reference Bueno, en principio ser??a para las vacaciones de Semana Santa que son las siguientes que
tenemos ahora, entrando julio.
Source (Cat) Jo li recomanaria per exemple que intente?s apropar-se a algun pa??s ve?? tambe? com poden ser
els pa??sos centreamericans, una mica me?s al nord Panama?.
Word-Based Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses UNKNOWN centreamericans, un poco ma?s al norte Panama?.
Letter-Based Yo le recomendar??a por ejemplo que intentaseo acercarse a algu?n pa??s ve?? tambie?n como
pueden ser elos pa??ses centroamericanos, un poco ma?s al norte Panama?.
Combined Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses centroamericanos, un poco ma?s al norte Panama?.
Reference Yo le recomendar??a por ejemplo que intentase acercarse a algu?n pa??s vecino tambie?n como
pueden ser los pa??ses centroamericanos, un poco ma?s al norte Panama?.
Figure 2: Example translations of the different approaches. For the word-based system an unknown word
has been explicitly marked.
37
 50
 55
 60
 65
 70
 75
 80
 0  5000  10000  15000  20000  25000  30000  35000  40000
Word-Based
Letter-Based
Combined
Figure 3: Translation quality depending of the corpus size.
acoustic data. The problem can be formulated as a
translation from graphemes (?letters?) to a sequence
of graphones (?pronunciations?), see for example
(Bisani and Ney, 2002). The proposed letter-based
approach can also be adapted to this task.
Lastly, a combination of both, word-based and
letter-based models, working in parallel and perhaps
taking into account additional information like base
forms, can be helpful when translating from or into
rich inflexional languages, like for example Spanish.
5 Conclusions
We have investigated the possibility of building a
letter-based system for translation between related
languages. The performance of the approach is quite
acceptable, although, as expected, the quality of the
word-based approach is superior. The combination
of both techniques, however, allows the system to
translate words not seen in the training corpus and
thus increase the translation quality. The gain is spe-
cially important when the training material is scarce.
While the experiments carried out in this work are
more interesting from an academical point of view,
several practical applications has been discussed and
will be the object of future work.
Acknowledgements
This work was partly funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (NE 572/5-3).
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 workshop on Computational ap-
proaches to semitic languages, pages 1?13, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Max Bisani and Hermann Ney. 2002. Investigations
on joint-multigram models for grapheme-to-phoneme
conversion. In Proceedings of the 7th International
Conference on Spoken Language Processing, vol-
ume 1, pages 105?108, Denver, CO, September.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
38
mation. Computational Linguistics, 19(2):263?311,
June.
D. Conejero, J. Gimnez, V. Arranz, A. Bonafonte, N. Pas-
cual, N. Castell, and A. Moreno. 2003. Lexica and
corpora for speech-to-speech translation: A trilingual
approach. In European Conf. on Speech Commu-
nication and Technology, pages 1593?1596, Geneva,
Switzerland, September.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge into Statistical Machine Translation. Ph.D. the-
sis, Universitat Polite`cnica de Catalunya, Barcelona,
October.
Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a
Hasan, and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of the International Workshop on
Spoken Language Translation, pages 103?110, Kyoto,
Japan.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 295?302, Philadelphia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Maja Popovic? and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference on
Language Resources and Evaluation (LREC), pages
1585?1588, Lisbon, Portugal, May.
Maja Popovic? and Hermann Ney. 2005. Exploiting
Phrasal Lexica and Additional Morpho-syntactic Lan-
guage Resources for Statistical Machine Translation
with Scarce Training Data. In 10th Annual Conference
of the European Association for Machine Translation
(EAMT), pages 212?218, Budapest, Hungary, May.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++. Cambridge University Press, Cambridge,
UK.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello
Federico. 2006. The JHU Workshop 2006 IWSLT
System. In Proc. of the International Workshop on
Spoken Language Translation, pages 59?63, Kyoto,
Japan.
39
Proceedings of the Second Workshop on Statistical Machine Translation, pages 48?55,
Prague, June 2007. c?2007 Association for Computational Linguistics
Word Error Rates: Decomposition over POS Classes and Applications for
Error Analysis
Maja Popovic?
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
Aachen, Germany
popovic@cs.rwth-aachen.de
Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
Aachen, Germany
ney@cs.rwth-aachen.de
Abstract
Evaluation and error analysis of machine
translation output are important but difficult
tasks. In this work, we propose a novel
method for obtaining more details about ac-
tual translation errors in the generated output
by introducing the decomposition of Word
Error Rate (WER) and Position independent
word Error Rate (PER) over different Part-
of-Speech (POS) classes. Furthermore, we
investigate two possible aspects of the use
of these decompositions for automatic er-
ror analysis: estimation of inflectional errors
and distribution of missing words over POS
classes. The obtained results are shown to
correspond to the results of a human error
analysis. The results obtained on the Euro-
pean Parliament Plenary Session corpus in
Spanish and English give a better overview
of the nature of translation errors as well as
ideas of where to put efforts for possible im-
provements of the translation system.
1 Introduction
Evaluation of machine translation output is a very
important but difficult task. Human evaluation is
expensive and time consuming. Therefore a variety
of automatic evaluation measures have been studied
over the last years. The most widely used are Word
Error Rate (WER), Position independent word Error
Rate (PER), the BLEU score (Papineni et al, 2002)
and the NIST score (Doddington, 2002). These mea-
sures have shown to be valuable tools for comparing
different systems as well as for evaluating improve-
ments within one system. However, these measures
do not give any details about the nature of translation
errors. Therefore some more detailed analysis of the
generated output is needed in order to identify the
main problems and to focus the research efforts. A
framework for human error analysis has been pro-
posed in (Vilar et al, 2006), but as every human
evaluation, this is also a time consuming task.
This article presents a framework for calculating
the decomposition of WER and PER over different
POS classes, i.e. for estimating the contribution of
each POS class to the overall word error rate. Al-
though this work focuses on POS classes, the method
can be easily extended to other types of linguis-
tic information. In addition, two methods for error
analysis using the WER and PER decompositons to-
gether with base forms are proposed: estimation of
inflectional errors and distribution of missing words
over POS classes. The translation corpus used for
our error analysis is built in the framework of the
TC-STAR project (tcs, 2005) and contains the tran-
scriptions of the European Parliament Plenary Ses-
sions (EPPS) in Spanish and English. The translation
system used is the phrase-based statistical machine
translation system described in (Vilar et al, 2005;
Matusov et al, 2006).
2 Related Work
Automatic evaluation measures for machine trans-
lation output are receiving more and more atten-
tion in the last years. The BLEU metric (Pap-
ineni et al, 2002) and the closely related NIST met-
ric (Doddington, 2002) along with WER and PER
48
have been widely used by many machine translation
researchers. An extended version of BLEU which
uses n-grams weighted according to their frequency
estimated from a monolingual corpus is proposed
in (Babych and Hartley, 2004). (Leusch et al, 2005)
investigate preprocessing and normalisation meth-
ods for improving the evaluation using the standard
measures WER, PER, BLEU and NIST. The same set
of measures is examined in (Matusov et al, 2005)
in combination with automatic sentence segmenta-
tion in order to enable evaluation of translation out-
put without sentence boundaries (e.g. translation of
speech recognition output). A new automatic met-
ric METEOR (Banerjee and Lavie, 2005) uses stems
and synonyms of the words. This measure counts
the number of exact word matches between the out-
put and the reference. In a second step, unmatched
words are converted into stems or synonyms and
then matched. The TER metric (Snover et al, 2006)
measures the amount of editing that a human would
have to perform to change the system output so that
it exactly matches the reference. The CDER mea-
sure (Leusch et al, 2006) is based on edit distance,
such as the well-known WER, but allows reordering
of blocks. Nevertheless, none of these measures or
extensions takes into account linguistic knowledge
about actual translation errors, for example what is
the contribution of verbs in the overall error rate,
how many full forms are wrong whereas their base
forms are correct, etc. A framework for human error
analysis has been proposed in (Vilar et al, 2006)
and a detailed analysis of the obtained results has
been carried out. However, human error analysis,
like any human evaluation, is a time consuming task.
Whereas the use of linguistic knowledge for im-
proving the performance of a statistical machine
translation system is investigated in many publi-
cations for various language pairs (like for exam-
ple (Nie?en and Ney, 2000), (Goldwater and Mc-
Closky, 2005)), its use for the analysis of translation
errors is still a rather unexplored area. Some auto-
matic methods for error analysis using base forms
and POS tags are proposed in (Popovic? et al, 2006;
Popovic? and Ney, 2006). These measures are based
on differences between WER and PER which are cal-
culated separately for each POS class using subsets
extracted from the original texts. Standard overall
WER and PER of the original texts are not at all
taken into account. In this work, the standard WER
and PER are decomposed and analysed.
3 Decomposition of WER and PER over
POS classes
The standard procedure for evaluating machine
translation output is done by comparing the hypoth-
esis document hyp with given reference translations
ref , each one consisting of K sentences (or seg-
ments). The reference document ref consists of
R reference translations for each sentence. Let the
length of the hypothesis sentence hypk be denoted
as Nhypk , and the reference lengths of each sentence
Nref k,r . Then, the total hypothesis length of the doc-
ument is Nhyp =
?
k Nhypk , and the total reference
length is Nref =
?
k N?ref k where N
?
ref k is defined
as the length of the reference sentence with the low-
est sentence-level error rate as shown to be optimal
in (Leusch et al, 2005).
3.1 Standard word error rates (overview)
The word error rate (WER) is based on the Lev-
enshtein distance (Levenshtein, 1966) - the mini-
mum number of substitutions, deletions and inser-
tions that have to be performed to convert the gen-
erated text hyp into the reference text ref . A short-
coming of the WER is the fact that it does not allow
reorderings of words, whereas the word order of the
hypothesis can be different from word order of the
reference even though it is correct translation. In
order to overcome this problem, the position inde-
pendent word error rate (PER) compares the words
in the two sentences without taking the word order
into account. The PER is always lower than or equal
to the WER. On the other hand, shortcoming of the
PER is the fact that the word order can be impor-
tant in some cases. Therefore the best solution is to
calculate both word error rates.
Calculation of WER: The WER of the hypothe-
sis hyp with respect to the reference ref is calculated
as:
WER = 1N?ref
K
?
k=1
min
r
dL(ref k,r, hypk)
where dL(ref k,r, hypk) is the Levenshtein dis-
tance between the reference sentence ref k,r and the
hypothesis sentence hypk. The calculation of WER
49
is performed using a dynamic programming algo-
rithm.
Calculation of PER: The PER can be calcu-
lated using the counts n(e, hypk) and n(e, ref k,r)
of a word e in the hypothesis sentence hypk and the
reference sentence ref k,r respectively:
PER = 1N?ref
K
?
k=1
min
r
dPER(ref k,r, hypk)
where
dPER(ref k,r, hypk) =
1
2
(
|Nref k,r ? Nhypk |+
?
e
|n(e, ref k,r) ? n(e, hypk)|
)
3.2 WER decomposition over POS classes
The dynamic programming algorithm for WER en-
ables a simple and straightforward identification of
each erroneous word which actually contributes to
WER. Let errk denote the set of erroneous words
in sentence k with respect to the best reference and
p be a POS class. Then n(p, errk) is the number of
errors in errk produced by words with POS class p.
It should be noted that for the substitution errors, the
POS class of the involved reference word is taken
into account. POS tags of the reference words are
also used for the deletion errors, and for the inser-
tion errors the POS class of the hypothesis word is
taken. The WER for the word class p can be calcu-
lated as:
WER(p) = 1N?ref
K
?
k=1
n(p, errk)
The sum over all classes is equal to the standard
overall WER.
An example of a reference sentence and hypothe-
sis sentence along with the corresponding POS tags
is shown in Table 1. The WER errors, i.e. actual
words participating in WER together with their POS
classes can be seen in Table 2. The reference words
involved in WER are denoted as reference errors,
and hypothesis errors refer to the hypothesis words
participating in WER.
Standard WER of the whole sentence is equal
to 4/12 = 33.3%. The contribution of nouns is
reference:
Mister#N Commissioner#N ,#PUN
twenty-four#NUM hours#N
sometimes#ADV can#V be#V too#ADV
much#PRON time#N .#PUN
hypothesis:
Mrs#N Commissioner#N ,#PUN
twenty-four#NUM hours#N is#V
sometimes#ADV too#ADV
much#PRON time#N .#PUN
Table 1: Example for illustration of actual errors: a
POS tagged reference sentence and a corresponding
hypothesis sentence
reference errors hypothesis errors error type
Mister#N Mrs#N substitution
sometimes#ADV is#V substitution
can#V deletion
be#V sometimes#ADV substitution
Table 2: WER errors: actual words which are partici-
pating in the word error rate and their corresponding
POS classes
WER(N) = 1/12 = 8.3%, of verbs WER(V) =
2/12 = 16.7% and of adverbs WER(ADV) =
1/12 = 8.3%
3.3 PER decomposition over POS classes
In contrast to WER, standard efficient algorithms for
the calculation of PER do not give precise informa-
tion about contributing words. However, it is pos-
sible to identify all words in the hypothesis which
do not have a counterpart in the reference, and vice
versa. These words will be referred to as PER errors.
reference errors hypothesis errors
Mister#N Mrs#N
be#V is#V
can#V
Table 3: PER errors: actual words which are partic-
ipating in the position independent word error rate
and their corresponding POS classes
An illustration of PER errors is given in Table 3.
50
The number of errors contributing to the standard
PER according to the algorithm described in 3.1 is 3
- there are two substitutions and one deletion. The
problem with standard PER is that it is not possible
to detect which words are the deletion errors, which
are the insertion errors, and which words are the sub-
stitution errors. Therefore we introduce an alterna-
tive PER based measure which corresponds to the
F-measure. Let herrk refer to the set of words in the
hypothesis sentence k which do not appear in the
reference sentence k (referred to as hypothesis er-
rors). Analogously, let rerrk denote the set of words
in the reference sentence k which do not appear in
the hypothesis sentence k (referred to as reference
errors). Then the following measures can be calcu-
lated:
? reference PER (RPER) (similar to recall):
RPER(p) = 1N?ref
K
?
k=1
n(p, rerrk)
? hypothesis PER (HPER) (similar to precision):
HPER(p) = 1Nhyp
K
?
k=1
n(p, herrk)
? F-based PER (FPER):
FPER(p) = 1N?ref + Nhyp
?
?
K
?
k=1
(n(p, rerrk) + n(p, herrk))
Since we are basically interested in all words with-
out a counterpart, both in the reference and in the
hypothesis, this work will be focused on FPER. The
sum of FPER over all POS classes is equal to the
overall FPER, and the latter is always less or equal
to the standard PER.
For the example sentence presented in Table 1, the
number of hypothesis errors n(e, herrk) is 2 and the
number of reference errors n(e, rerrk) is 3 where e
denotes the word. The number of errors contributing
to the standard PER is 3, since |Nref ? Nhyp | = 1
and
?
e |n(e, ref k) ? n(e, hypk)| = 5. The stan-
dard PER is normalised over the reference length
Nref = 12 thus being equal to 25%. The FPER is the
sum of hypothesis and reference errors divided by
the sum of hypothesis and reference length: FPER =
(2 + 3)/(11 + 12) = 5/23 = 21.7%. The contribu-
tion of nouns is FPER(N) = 2/23 = 8.7% and the
contribution of verbs is FPER(V) = 3/23 = 13%.
4 Applications for error analysis
The decomposed error rates described in Section 3.2
and Section 3.3 contain more details than the stan-
dard error rates. However, for more precise informa-
tion about certain phenomena some kind of further
analysis is required. In this work, we investigate two
possible aspects for error analysis:
? estimation of inflectional errors by the use of
FPER errors and base forms
? extracting the distribution of missing words
over POS classes using WER errors, FPER er-
rors and base forms.
4.1 Inflectional errors
Inflectional errors can be estimated using FPER
errors and base forms. From each reference-
hypothesis sentence pair, only erroneous words
which have the common base forms are taken
into account. The inflectional error rate of each POS
class is then calculated in the same way as FPER.
For example, from the PER errors presented in Ta-
ble 3, the words ?is? and ?be? are candidates for an
inflectional error because they are sharing the same
base form ?be?. Inflectional error rate in this exam-
ple is present only for the verbs, and is calculated in
the same way as FPER, i.e. IFPER(V) = 2/23 =
8.7%.
4.2 Missing words
Distribution of missing words over POS classes can
be extracted from the WER and FPER errors in the
following way: the words considered as missing are
those which occur as deletions in WER errors and
at the same time occur only as reference PER errors
without sharing the base form with any hypothesis
error. The use of both WER and PER errors is much
more reliable than using only the WER deletion er-
ros because not all deletion errors are produced by
missing words: a number of WER deletions appears
51
due to reordering errors. The information about the
base form is used in order to eliminate inflectional
errors. The number of missing words is extracted for
each word class and then normalised over the sum of
all classes. For the example sentence pair presented
in Table 1, from the WER errors in Table 2 and the
PER errors in Table 3 the word ?can? will be identi-
fied as missing.
5 Experimental settings
5.1 Translation System
The machine translation system used in this work
is based on the statistical aproach. It is built as
a log-linear combination of seven different statisti-
cal models: phrase based models in both directions,
IBM1 models at the phrase level in both directions,
as well as target language model, phrase penalty and
length penalty are used. A detailed description of the
system can be found in (Vilar et al, 2005; Matusov
et al, 2006).
5.2 Task and corpus
The corpus analysed in this work is built in the
framework of the TC-STAR project. The training
corpus contains more than one million sentences and
about 35 million running words of the European Par-
liament Plenary Sessions (EPPS) in Spanish and En-
glish. The test corpus contains about 1 000 sentences
and 28 000 running words. The OOV rates are low,
about 0.5% of the running words for Spanish and
0.2% for English. The corpus statistics can be seen
in Table 4. More details about the EPPS data can be
found in (Vilar et al, 2005).
TRAIN Spanish English
Sentences 1 167 627
Running words 35 320 646 33 945 468
Vocabulary 159 080 110 636
TEST
Sentences 894 1 117
Running words 28 591 28 492
OOVs 0.52% 0.25%
Table 4: Statistics of the training and test corpora
of the TC-STAR EPPS Spanish-English task. Test
corpus is provided with two references.
6 Error analysis
The translation is performed in both directions
(Spanish to English and English to Spanish) and the
error analysis is done on both the English and the
Spanish output. Morpho-syntactic annotation of the
English references and hypotheses is performed us-
ing the constraint grammar parser ENGCG (Vouti-
lainen, 1995), and the Spanish texts are annotated
using the FreeLing analyser (Carreras et al, 2004).
In this way, all references and hypotheses are pro-
vided with POS tags and base forms. The decom-
position of WER and FPER is done over the ten
main POS classes: nouns (N), verbs (V), adjectives
(A), adverbs (ADV), pronouns (PRON), determiners
(DET), prepositions (PREP), conjunctions (CON),
numerals (NUM) and punctuation marks (PUN). In-
flectional error rates are also estimated for each POS
class using FPER counts and base forms. Addition-
ally, details about the verb tense and person inflec-
tions for both languages as well as about the adjec-
tive gender and person inflections for the Spanish
output are extracted. Apart from that, the distribu-
tion of missing words over the ten POS classes is
estimated using the WER and FPER errors.
6.1 WER and PER (FPER) decompositions
Figure 1 presents the decompositions of WER and
FPER over the ten basic POS classes for both lan-
guages. The largest part of both word error rates
comes from the two most important word classes,
namely nouns and verbs, and that the least critical
classes are punctuations, conjunctions and numbers.
Adjectives, determiners and prepositions are sig-
nificantly worse in the Spanish output. This is partly
due to the richer morphology of the Spanish lan-
guage. Furthermore, the histograms indicate that the
number of erroneus nouns and pronouns is higher
in the English output. As for verbs, WER is higher
for English and FPER for Spanish. This indicates
that there are more problems with word order in the
English output, and more problems with the correct
verb or verb form in the Spanish output.
In addition, the decomposed error rates give an
idea of where to put efforts for possible improve-
ments of the system. For example, working on im-
provements of verb translations could reduce up to
about 10% WER and 7% FPER, working on nouns
52
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
PUNNUMPREP CONDETPRONADVAVN
WER over POS classes [%]
English
Spanish
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
PUNNUMPREP CONDETPRONADVAVN
FPER over POS classes [%]
English
Spanish
Figure 1: Decomposition of WER and FPER [%]
over the ten basic POS classes for English and Span-
ish output
up to 8% WER and 5% FPER, whereas there is no
reason to put too much efforts on e.g. adverbs since
this could lead only to about 2% of WER and FPER
reduction. 1
6.2 Inflectional errors
Inflectional error rates for the ten POS classes are
presented in Figure 2. For the English language,
these errors are significant only for two POS classes:
nouns and verbs. The verbs are the most problem-
atic category in both languages, for Spanish having
almost two times higher error rate than for English.
This is due to the very rich morphology of Spanish
verbs - one base form might have up to about fourty
different inflections.
1Reduction of FPER leads to a similar reduction of PER.
 0
 0.5
 1
 1.5
 2
 2.5
PUNNUMPREP CONDETPRONADVAVN
inflectional errors [%]
English
Spanish
Figure 2: Inflectional error rates [%] for English and
Spanish output
Nouns have a higher error rate for English than
for Spanish. The reason for this difference is not
clear, since the noun morphology of neither of the
languages is particularly rich - there is only distinc-
tion between singular and plural. One possible ex-
planation might be the numerous occurences of dif-
ferent variants of the same word, like for example
?Mr? and ?Mister?.
In the Spanish output, two additional POS classes
are showing significant error rate: determiners and
adjectives. This is due to the gender and number in-
flections of those classes which do not exist in the
English language - for each determiner or adjective,
there are four variants in Spanish and only one in En-
glish. Working on inflections of Spanish verbs might
reduce approximately 2% of FPER, on English verbs
about 1%. Improvements of Spanish determiners
could lead up to about 2% of improvements.
6.2.1 Comparison with human error analysis
The results obtained for inflectional errors are
comparable with the results of a human error anal-
ysis carried out in (Vilar et al, 2006). Although it
is difficult to compare all the numbers directly, the
overall tendencies are the same: the largest num-
ber of translation errors are caused by Spanish verbs,
and much less but still a large number of errors by
English verbs. A much smaller but still significant
number of errors is due to Spanish adjectives, and
only a few errors of English adjectives are present.
Human analysis was done also for the tense and
53
person of verbs, as well as for the number and gen-
der of adjectives. We use more detailed POS tags in
order to extract this additional information and cal-
culate inflectional error rates for such tags. It should
be noted that in contrast to all previous error rates,
these error rates are not disjunct but overlapping:
many words are contributing to both.
The results are shown in Figure 3, and the tenden-
cies are again the same as those reported in (Vilar
et al, 2006). As for verbs, tense errors are much
more frequent than person errors for both languages.
Adjective inflections cause certain amount of errors
only in the Spanish output. Contributions of gender
and of number are aproximately equal.
 0
 0.5
 1
 1.5
 2
A numberA genderV personV tense
inflectional errors of verbs and adjectives [%]
English
Spanish
Figure 3: More details about inflections: verb tense
and person error rates and adjective gender and num-
ber error rates [%]
6.3 Missing words
Figure 4 presents the distribution of missing words
over POS classes. This distribution has a same be-
haviour as the one obtained by human error analysis.
Most missing words for both languages are verbs.
For English, the percentage of missing verbs is sig-
nificantly higher than for Spanish. The same thing
happens for pronouns. The probable reason for this
is the nature of Spanish verbs. Since person and
tense are contained in the suffix, Spanish pronouns
are often omitted, and auxiliary verbs do not exist
for all tenses. This could be problematic for a trans-
lation system, because it processes only one Spanish
word which actually contains two (or more) English
words.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
PUNNUMPREP CONDETPRONADVAVN
missing words [%]
eng
esp
Figure 4: Distribution of missing words over POS
classes [%] for English and Spanish output
Prepositions are more often missing in Spanish
than in English, as well as determiners. A probable
reason is the disproportion of the number of occur-
rences for those classes between two languages.
7 Conclusions
This work presents a framework for extraction of lin-
guistic details from standard word error rates WER
and PER and their use for an automatic error analy-
sis. We presented a method for the decomposition of
standard word error rates WER and PER over ten ba-
sic POS classes. We also carried out a detailed anal-
ysis of inflectional errors which has shown that the
results obtained by our method correspond to those
obtained by a human error analysis. In addition, we
proposed a method for analysing missing word er-
rors.
We plan to extend the proposed methods in order
to carry out a more detailed error analysis, for ex-
ample examining different types of verb inflections.
We also plan to examine other types of translation
errors like for example errors caused by word order.
Acknowledgements
This work was partly funded by the European Union
under the integrated project TC-STAR? Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738).
54
References
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing BLEU MT Evaluation Method with Frequency
Weighting. In Proc. of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL),
Barcelona, Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In 43rd
Annual Meeting of the Assoc. for Computational Lin-
guistics: Proc. Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 65?72, Ann Arbor, MI, June.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proc. 4th Int. Conf. on Lan-
guage Resources and Evaluation (LREC), pages 239?
242, Lisbon, Portugal, May.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128?132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Gregor Leusch, Nicola Ueffing, David Vilar, and Her-
mann Ney. 2005. Preprocessing and Normalization
for Automatic Evaluation of Machine Translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 17?24, Ann Arbor, MI, June. Association
for Computational Linguistics.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In EACL06, pages 241?248, Trento, Italy,
April.
Vladimir Iosifovich Levenshtein. 1966. Binary Codes
Capable of Correcting Deletions, Insertions and Re-
versals. Soviet Physics Doklady, 10(8):707?710,
February.
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating Machine Transla-
tion Output with Automatic Sentence Segmentation.
In Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 148?154,
Pittsburgh, PA, October.
Evgeny Matusov, Richard Zens, David Vilar, Arne
Mauser, Maja Popovic?, and Hermann Ney. 2006.
The RWTH Machine Translation System. In TC-Star
Workshop on Speech-to-Speech Translation, pages 31?
36, Barcelona, Spain, June.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
?00: The 18th Int. Conf. on Computational Linguistics,
pages 1081?1085, Saarbru?cken, Germany, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
July.
Maja Popovic? and Hermann Ney. 2006. Error Analysis
of Verb Inflections in Spanish Translation Output. In
TC-Star Workshop on Speech-to-Speech Translation,
pages 99?103, Barcelona, Spain, June.
Maja Popovic?, Adria` de Gispert, Deepa Gupta, Patrik
Lambert, Hermann Ney, Jose? B. Marin?o, Marcello
Federico, and Rafael Banchs. 2006. Morpho-syntactic
Information for Automatic Error Analysis of Statisti-
cal Machine Translation Output. In Proc. of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 1?6, New York, NY, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Error Rate with Targeted Human An-
notation. In Proc. of the 7th Conf. of the Association
for Machine Translation in the Americas (AMTA 06),
pages 223?231, Boston, MA.
2005. TC-STAR - technology and corpora for speech to
speech translation. Integrated project TCSTAR (IST-
2002-FP6-506738) funded by the European Commis-
sion. http://www.tc-star.org/.
David Vilar, Evgeny Matusov, Sas?a Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical Machine Transla-
tion of European Parliamentary Speeches. In Proc. MT
Summit X, pages 259?266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proc. of the Fifth Int.
Conf. on Language Resources and Evaluation (LREC),
pages 697?702, Genoa, Italy, May.
Atro Voutilainen. 1995. ENGCG -
Constraint Grammar Parser of English.
http://www2.lingsoft.fi/doc/engcg/intro/.
55
Proceedings of the Second Workshop on Statistical Machine Translation, pages 96?103,
Prague, June 2007. c?2007 Association for Computational Linguistics
Human Evaluation of Machine Translation Through Binary System
Comparisons
David Vilar, Gregor Leusch
and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
D-52056 Aachen, Germany
{vilar,leusch,ney}@cs.rwth-aachen.de
Rafael E. Banchs
D. of Signal Theory and Communications
Universitat Polite`cnica de Catalunya
08034 Barcelona, Spain
rbanchs@gps.tsc.upc.edu
Abstract
We introduce a novel evaluation scheme for
the human evaluation of different machine
translation systems. Our method is based
on direct comparison of two sentences at a
time by human judges. These binary judg-
ments are then used to decide between all
possible rankings of the systems. The ad-
vantages of this new method are the lower
dependency on extensive evaluation guide-
lines, and a tighter focus on a typical eval-
uation task, namely the ranking of systems.
Furthermore we argue that machine transla-
tion evaluations should be regarded as sta-
tistical processes, both for human and au-
tomatic evaluation. We show how confi-
dence ranges for state-of-the-art evaluation
measures such as WER and TER can be
computed accurately and efficiently without
having to resort to Monte Carlo estimates.
We give an example of our new evaluation
scheme, as well as a comparison with classi-
cal automatic and human evaluation on data
from a recent international evaluation cam-
paign.
1 Introduction
Evaluation of machine translation (MT) output is a
difficult and still open problem. As in other natu-
ral language processing tasks, automatic measures
which try to asses the quality of the translation
can be computed. The most widely known are the
Word Error Rate (WER), the Position independent
word Error Rate (PER), the NIST score (Dodding-
ton, 2002) and, especially in recent years, the BLEU
score (Papineni et al, 2002) and the Translation Er-
ror Rate (TER) (Snover et al, 2005). All of the-
ses measures compare the system output with one
or more gold standard references and produce a nu-
merical value (score or error rate) which measures
the similarity between the machine translation and a
human produced one. Once such reference transla-
tions are available, the evaluation can be carried out
in a quick, efficient and reproducible manner.
However, automatic measures also have big dis-
advantages; (Callison-Burch et al, 2006) describes
some of them. A major problem is that a given sen-
tence in one language can have several correct trans-
lations in another language and thus, the measure of
similarity with one or even a small amount of ref-
erence translations will never be flexible enough to
truly reflect the wide range of correct possibilities of
a translation. 1 This holds in particular for long sen-
tences and wide- or open-domain tasks like the ones
dealt with in current MT projects and evaluations.
If the actual quality of a translation in terms of
usefulness for human users is to be evaluated, human
evaluation needs to be carried out. This is however
a costly and very time-consuming process. In this
work we present a novel approach to human evalu-
ation that simplifies the task for human judges. In-
stead of having to assign numerical scores to each
sentence to be evaluated, as is done in current evalu-
ation procedures, human judges choose the best one
out of two candidate translations. We show how this
method can be used to rank an arbitrary number of
systems and present a detailed analysis of the statis-
tical significance of the method.
1Compare this with speech recognition, where apart from
orthographic variance there is only one correct reference.
96
2 State-of-the-art
The standard procedure for carrying out a human
evaluation of machine translation output is based on
the manual scoring of each sentence with two nu-
merical values between 1 and 5. The first one mea-
sures the fluency of the sentence, that is its readabil-
ity and understandability. This is a monolingual fea-
ture which does not take the source sentence into
account. The second one reflects the adequacy, that
is whether the translated sentence is a correct trans-
lation of the original sentence in the sense that the
meaning is transferred. Since humans will be the
end users of the generated output,2 it can be ex-
pected that these human-produced measures will re-
flect the usability and appropriateness of MT output
better than any automatic measure.
This kind of human evaluation has however addi-
tional problems. It is much more time consuming
than the automatic evaluation, and because it is sub-
jective, results are not reproducible, even from the
same group of evaluators. Furthermore, there can
be biases among the human judges. Large amounts
of sentences must therefore be evaluated and proce-
dures like evaluation normalization must be carried
out before significant conclusions from the evalua-
tion can be drawn. Another important drawback,
which is also one of the causes of the aforemen-
tioned problems, is that it is very difficult to define
the meaning of the numerical scores precisely. Even
if human judges have explicit evaluation guidelines
at hand, they still find it difficult to assign a numeri-
cal value which represents the quality of the transla-
tion for many sentences (Koehn and Monz, 2006).
In this paper we present an alternative to this eval-
uation scheme. Our method starts from the obser-
vation that normally the final objective of a human
evaluation is to find a ?ranking? of different systems,
and the absolute score for each system is not relevant
(and it can even not be comparable between differ-
ent evaluations). We focus on a method that aims to
simplify the task of the judges and allows to rank the
systems according to their translation quality.
3 Binary System Comparisons
The main idea of our method relies in the fact
that a human evaluator, when presented two differ-
ent translations of the same sentence, can normally
choose the best one out of them in a more or less
2With the exception of cross-language information retrieval
and similar tasks.
definite way. In social sciences, a similar method
has been proposed by (Thurstone, 1927).
3.1 Comparison of Two Systems
For the comparison of two MT systems, a set of
translated sentence pairs is selected. Each of these
pairs consists of the translations of a particular
source sentence from the two systems. The human
judge is then asked to select the ?best? translation of
these two, or to mark the translations to be equally
good. We are aware that the definition of ?best? here
is fuzzy. In our experiments, we made a point of not
giving the evaluators explicit guidelines on how to
decide between both translations. As a consequence,
the judges were not to make a distinction between
fluency and adequacy of the translation. This has a
two-fold purpose: on the one hand it simplifies the
decision procedure for the judges, as in most of the
cases the decision is quite natural and they do not
need to think explicitly in terms of fluency and ade-
quacy. On the other hand, one should keep in mind
that the final goal of an MT system is its usefulness
for a human user, which is why we do not want to
impose artificial constraints on the evaluation proce-
dure. If only certain quality aspects of the systems
are relevant for the ranking, for example if we want
to focus on the fluency of the translations, explicit
guidelines can be given to the judges. If the evalua-
tors are bilingual they can use the original sentences
to judge whether the information was preserved in
the translation.
After our experiment, the human judges provided
feedback on the evaluation process. We learned
that the evaluators normally selected the translation
which preserved most of the information from the
original sentence. Thus, we expect to have a slight
preference for adequacy over fluency in this evalu-
ation process. Note however that adequacy and flu-
ency have shown a high correlation3 in previous ex-
periments. This can be explained by noting that a
low fluency renders the text incomprehensible and
thus the adequacy score will also be low.
The difference in the amount of selected sen-
tences of each system is an indicator of the differ-
ence in quality between the systems. Statistics can
be carried out in order to decide whether this differ-
ence is statistically significant; we will describe this
in more detail in Section 3.4.
3At least for ?sensible? translation systems. Academic
counter-examples could easily be constructed.
97
3.2 Evaluation of Multiple Systems
We can generalize our method to find a ranking of
several systems as follows: In this setting, we have
a set of n systems. Furthermore, we have defined an
order relationship ?is better than? between pairs of
these systems. Our goal now is to find an ordering
of the systems, such that each system is better than
its predecessor. In other words, this is just a sorting
problem ? as widely known in computer science.
Several efficient sorting algorithms can be found
in the literature. Generally, the efficiency of sort-
ing algorithms is measured in terms of the number
of comparisons carried out. State-of-the-art sort-
ing algorithms have a worst-case running time of
O(n log n), where n is the number of elements to
sort. In our case, because such binary comparisons
are very time consuming, we want to minimize the
absolute number of comparisons needed. This mini-
mization should be carried out in the strict sense, not
just in an asymptotic manner.
(Knuth, 1973) discusses this issue in detail. It is
relatively straightforward to show that, in the worst
case, the minimum number of comparisons to be
carried out to sort n elements is at least dlog n!e
(for which n log n is an approximation). It is not
always possible to reach this minimum, however, as
was proven e.g. for the case n = 12 in (Wells, 1971)
and for n = 13 in (Peczarski, 2002). (Ford Jr and
Johnson, 1959) propose an algorithm called merge
insertion which comes very close to the theoretical
limit. This algorithm is sketched in Figure 1. There
are also algorithms with a better asymptotic runtime
(Bui and Thanh, 1985), but they only take effect for
values of n too large for our purposes (e.g., more
than 100). Thus, using the algorithm from Figure 1
we can obtain the ordering of the systems with a
(nearly) optimal number of comparisons.
3.3 Further Considerations
In Section 3.1 we described how to carry out the
comparison between two systems when there is only
one human judge carrying out this comparison. The
comparison of systems is a very time consuming
task. Therefore it is hardly possible for one judge
to carry out the evaluation on a whole test corpus.
Usually, subsets of these test corpora are selected
for human evaluations instead. In order to obtain
a better coverage of the test corpus, but also to try
to alleviate the possible bias of a single evaluator, it
is advantageous to have several evaluators carrying
out the comparison between two systems. However,
there are two points that must be considered.
The first one is the selection of sentences each hu-
man judge should evaluate. Assume that we have al-
ready decided the amount of sentences m each eval-
uator has to work with (in our case m = 100). One
possibility is that all human judges evaluate the same
set of sentences, which presumably will cancel pos-
sible biases of the evaluators. A second possibility is
to give each judge a disjunct set of sentences. In this
way we benefit from a higher coverage of the corpus,
but do not have an explicit bias compensation.
In our experiments, we decided for a middle
course: Each evaluator receives a randomly selected
set of sentences. There are no restrictions on the se-
lection process. This implicitly produces some over-
lap while at the same time allowing for a larger set
of sentences to be evaluated. To maintain the same
conditions for each comparison, we also decided
that each human judge should evaluate the same set
of sentences for each system pair.
The other point to consider is how the evaluation
results of each of the human judges should be com-
bined into a decision for the whole system. One
possibility would be to take only a ?majority vote?
among the evaluators to decide which system is the
best. By doing this, however, possible quantitative
information on the quality difference of the systems
is not taken into account. Consequently, the output is
strongly influenced by statistical fluctuations of the
data and/or of the selected set of sentences to eval-
uate. Thus, in order to combine the evaluations we
just summed over all decisions to get a total count of
sentences for each system.
3.4 Statistical Significance
The evaluation of MT systems by evaluating trans-
lations of test sentences ? be it automatic evaluation
or human evaluation ? must always be regarded as
a statistical process: Whereas the outcome, or score
R, of an evaluation is considered to hold for ?all?
possible sentences from a given domain, a test cor-
pus naturally consists of only a sample from all these
sentences. Consequently, R depends on that sam-
ple of test sentences. Furthermore, both a human
evaluation score and an automatic evaluation score
for a hypothesis sentence are by itself noisy: Hu-
man evaluation is subjective, and as such is subject
to ?human noise?, as described in Section 2. Each
automatic score, on the other hand, depends heavily
on the ambiguous selection of reference translations.
Accordingly, evaluation scores underly a probability
98
1. Make pairwise comparisons of bn/2c disjoint pairs of elements. (If n is odd, leave one element out).
2. Sort the bn/2c larger elements found in step 1, recursively by merge insertion.
3. Name the bn/2c elements found in step 2 a1, a2, . . . , abn/2c and the rest b1, b2, . . . , bdn/2e, such that
a1 ? a2 ? ? ? ? ? abn/2c and bi ? ai for 1 ? i ? bn/2c. Call b1 and the a?s the ?main chain?.
4. Insert the remaining b?s into the main chain, using binary insertion, in the following order (ignore the
bj such that j > dn/2e): b3, b2; b5, b4; b11, . . . , b6; . . . ; btk , . . . , btk?1+1; . . . with tk =
2k+1+(?1)k
3 .
Figure 1: The merge insertion algorithm as presented in (Knuth, 1973).
distribution, and each evaluation result we achieve
must be considered as a sample from that distribu-
tion. Consequently, both human and automatic eval-
uation results must undergo statistical analysis be-
fore conclusions can be drawn from them.
A typical application of MT evaluation ? for ex-
ample in the method described in this paper ? is to
decide whether a given MT system X , represented
by a set of translated sentences, is significantly better
than another system Y with respect to a given eval-
uation measure. This outcome is traditionally called
the alternative hypothesis. The opposite outcome,
namely that the two systems are equal, is known
as the null hypothesis. We say that certain values
of RX , RY confirm the alternative hypothesis if the
null hypothesis can be rejected with a given level
of certainty, e.g. 95%. In the case of comparing
two MT systems, the null hypothesis would be ?both
systems are equal with regard to the evaluation mea-
sure; that is, both evaluation scoresR, R? come from
the same distribution R0?.
As R is randomly distributed, it has an expecta-
tion E[R] and a standard error se[R]. Assuming a
normal distribution for R, we can reject the null hy-
pothesis with a confidence of 95% if the sampled
score R is more than 1.96 times the standard error
away from the null hypothesis expectation:
R significant ? |E[R0] ? R| > 1.96 se[R0] (1)
The question we have to solve is: How can we es-
timate E[R0] and se[R0]? The first step is that we
consider R and R0 to share the same standard error
se[R0] = se[R]. This value can then be estimated
from the test data. In a second step, we give an es-
timate for E[R0], either inherent in the evaluation
measure (see below), or from the estimate for the
comparison system R?.
A universal estimation method is the bootstrap
estimate: The core idea is to create replications of
R by random sampling from the data set (Bisani
and Ney, 2004). Bootstrapping is generally possi-
ble for all evaluation measures. With a high number
of replicates, se[R] and E[R0] can be estimated with
satisfactory precision.
For a certain class of evaluation measures, these
parameters can be estimated more accurately and ef-
ficiently from the evaluation data without resorting
to Monte Carlo estimates. This is the class of er-
rors based on the arithmetic mean over a sentence-
wise score: In our binary comparison experiments,
each judge was given hypothesis translations ei,X ,
ei,Y . She could then judge ei,X to be better than,
equal to, or worse than ei,Y . All these judgments
were counted over the systems. We define a sentence
score ri,X,Y for this evaluation method as follows:
ri,X,Y :=
?
??
??
+1 ei,X is better than ei,Y
0 ei,X is equal to ei,Y
?1 ei,X is worse than ei,Y
. (2)
Then, the total evaluation score for a binary com-
parison of systems X and Y is
RX,Y :=
1
m
m?
i=1
ri,X,Y , (3)
with m the number of evaluated sentences.
For this case, namelyR being an arithmetic mean,
(Efron and Tibshirani, 1993) gives an explicit for-
mula for the estimated standard error of the score
RX,Y . To simplify the notation, we will use R in-
stead of RX,Y from now on, and ri instead of ri,X,Y .
se[R] =
1
m ? 1
?
?
?
?
m?
i=1
(ri ? R)2 . (4)
With x denoting the number of sentences where
ri = 1, and y denoting the number of sentences
99
where ri = ?1,
R =
x ? y
m
(5)
and with basic algebra
se[R] =
1
m ? 1
?
x + y ?
(x ? y)2
m
. (6)
Moreover, we can explicitly give an estimate for
E[R0]: The null hypothesis is that both systems are
?equally good?. Then, we should expect as many
sentences where X is better than Y as vice versa,
i.e. x = y. Thus, E[R0] = 0.
Using Equation 4, we calculate se[R] and thus a
significance range for adequacy and fluency judg-
ments. When comparing two systems X and Y ,
we assume for the null hypothesis that se[R0] =
se[RX ] and E[R0] = E[RY ] (or vice versa).
A very useful (and to our knowledge new) result
for MT evaluation is that se[R] can also be explic-
itly estimated for weighted means ? such as WER,
PER, and TER. These measures are defined as fol-
lows: Let di, i = 1, . . . ,m denote the number of ?er-
rors? (edit operations) of the translation candidate ei
with regard to a reference translation with length li.
Then, the total error rate will be computed as
R :=
1
L
m?
i=1
di (7)
where
L :=
m?
i=1
li (8)
As a result, each sentence ei affects the overall score
with weight li ? the effect of leaving out a sen-
tence with length 40 is four times higher than that
of leaving out one with length 10. Consequently,
these weights must be considered when estimating
the standard error of R:
se[R] =
?
?
?
? 1
(m ? 1)(L ? 1)
m?
i=1
(
di
li
? R
)2
? li
(9)
With this Equation, Monte-Carlo-estimates are no
longer necessary for examining the significance of
WER, PER, TER, etc. Unfortunately, we do not ex-
pect such a short explicit formula to exist for the
standard BLEU score. Still, a confidence range
for BLEU can be estimated by bootstrapping (Och,
2003; Zhang and Vogel, 2004).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Singletons 63K 46K
Test Sentences 1 117
Words 26K
OOV Words 72
Table 1: Statistics of the EPPS Corpus.
4 Evaluation Setup
The evaluation procedure was carried out on the data
generated in the second evaluation campaign of the
TC-STAR project4. The goal of this project is to
build a speech-to-speech translation system that can
deal with real life data. Three translation directions
are dealt with in the project: Spanish to English, En-
glish to Spanish and Chinese to English. For the sys-
tem comparison we concentrated only in the English
to Spanish direction.
The corpus for the Spanish?English language pair
consists of the official version of the speeches held in
the European Parliament Plenary Sessions (EPPS),
as available on the web page of the European Parlia-
ment. A more detailed description of the EPPS data
can be found in (Vilar et al, 2005). Table 1 shows
the statistics of the corpus.
A total of 9 different MT systems participated in
this condition in the evaluation campaign that took
place in February 2006. We selected five representa-
tive systems for our study. Henceforth we shall refer
to these systems as System A through System E. We
restricted the number of systems in order to keep the
evaluation effort manageable for a first experimental
setup to test the feasibility of our method. The rank-
ing of 5 systems can be carried out with as few as 7
comparisons, but the ranking of 9 systems requires
19 comparisons.
5 Evaluation Results
Seven human bilingual evaluators (6 native speakers
and one near-native speaker of Spanish) carried out
the evaluation. 100 sentences were randomly cho-
sen and assigned to each of the evaluators for every
system comparison, as discussed in Section 3.3. The
results can be seen in Table 2 and Figure 2. Counts
4http://www.tc-star.org/
100
0 10 20 30 40 50 60 70
0
10
20
30
40
50
60
70
l
l
l
l
ll
l
# "First system better"
# "S
eco
nd s
yste
m b
ette
r" l
B?AD?CA?CE?AE?BB?DD?A
(a) Each judge.
0 100 200 300 400
0
100
200
300
400
# "First system better"
# "S
eco
nd s
yste
m b
ette
r"
lB?AD?C
A?C E?A
E?B
B?D D?A
(b) All judges.
Figure 2: Results of the binary comparisons. Number of times the winning system was really judged ?better?
vs. number of times it was judged ?worse?. Results in hatched area can not reject null hypothesis, i.e. would
be considered insignificant.
missing to 100 and 700 respectively denote ?same
quality? decisions.
As can be seen from the results, in most of the
cases the judges clearly favor one of the systems.
The most notable exception is found when compar-
ing systems A and C, where a difference of only 3
sentences is clearly not enough to decide between
the two. Thus, the two bottom positions in the final
ranking could be swapped.
Figure 2(a) shows the outcome for the binary
comparisons separately for each judge, together with
an analysis of the statistical significance of the re-
sults. As can be seen, the number of samples (100)
would have been too low to show significant re-
sults in many experiments (data points in the hatched
area). In some cases, the evaluator even judged bet-
ter the system which was scored to be worse by the
majority of the other evaluators (data points above
the bisector). As Figure 2(b) shows, ?the only thing
better than data is more data?: When we summarize
R over all judges, we see a significant difference
(with a confidence of 95%) at all comparisons but
two (A vs. C, and E vs. B). It is interesting to note
that exactly these two pairs do not show a significant
difference when using a majority vote strategy.
Table 3 shows also the standard evaluation met-
rics. Three BLEU scores are given in this table, the
one computed on the whole corpus, the one com-
puted on the set used for standard adequacy and flu-
ency computations and the ones on the set we se-
lected for this task5. It can be seen that the BLEU
scores are consistent across all data subsets. In this
case the ranking according to this automatic measure
matches exactly the ranking found by our method.
When comparing with the adequacy and fluency
scores, however, the ranking of the systems changes
considerably: B D E C A. However, the difference
between the three top systems is quite small. This
can be seen in Figure 3, which shows some auto-
matic and human scores for the five systems in our
experiments, along with the estimated 95% confi-
dence range. The bigger difference is found when
comparing the bottom systems, namely System A
and System C. While our method produces nearly
no difference the adequacy and fluency scores indi-
cate System C as clearly superior to System A. It is
worth noting that the both groups use quite different
translation approaches (statistical vs. rule-based).
5Regretfully these two last sets were not the same. This is
due to the fact that the ?AF Test Set? was further used for eval-
uating Text-to-Speech systems, and thus a targeted subset of
sentences was selected.
101
Sys E1 E2 E3 E4 E5 E6 E7
?
A 29 19 38 17 32 29 41 205
B 40 59 48 53 63 64 45 372
C 32 22 29 23 32 34 42 214
D 39 61 59 50 64 58 46 377
A 32 31 31 31 47 38 40 250
C 37 29 32 22 39 45 43 247
A 36 28 17 28 34 37 31 211
E 41 47 44 43 53 45 58 331
B 26 29 18 24 43 36 33 209
E 34 33 28 27 32 29 43 226
B 34 28 30 31 40 41 48 252
D 23 17 23 17 24 28 38 170
A 36 14 27 9 31 30 34 181
D 34 50 40 50 57 61 57 349
Final ranking (best?worst): E B D A C
Table 2: Result of the binary system comparison.
Numbers of sentences for which each system was
judged better by each evaluator (E1-E7).
Subset: Whole A+F Binary
Sys BLEU BLEU A F BLEU
A 36.3 36.2 2.93 2.46 36.3
B 49.4 49.3 3.74 3.58 49.2
C 36.3 36.2 3.53 3.31 36.1
D 48.2 46.8 3.68 3.48 47.7
E 49.8 49.6 3.67 3.46 49.4
Table 3: BLEU scores and Adequacy and Fluency
scores for the different systems and subsets of the
whole test set. BLEU values in %, Adequacy (A)
and Fluency (F) from 1 (worst) to 5 (best).
6 Discussion
In this section we will review the main drawbacks of
the human evaluation listed in Section 2 and analyze
how our approach deals with them. The first one
was the use of explicit numerical scores, which are
difficult to define exactly. Our system was mainly
designed for the elimination of this issue.
Our evaluation continues to be time consuming.
Even more, the number of individual comparisons
needed is in the order of log(n!), in contrast with the
standard adequacy-fluency evaluation which needs
2n individual evaluations (two evaluations per sys-
tem, one for fluency, another one for adequacy). For
n in the range of 1 up to 20 (a realistic number of
systems for current evaluation campaigns) these two
quantities are comparable. And actually each of our
CA
DB
E
CA
DB
E
CA
DB
E
CA
DB
E
ll
ll
l
ll
ll
l
ll
ll
l
ll
l l
lFluency
Adequacy
1?WER
BLEU
0.3 0.4 0.5 0.6 0.7
          worse <?  normalized score  ?> better
Me
asu
re &
 Sys
tem
Figure 3: Normalized evaluation scores. Higher
scores are better. Solid lines show the 95% con-
fidence range. Automatic scores calculated on the
whole test set, human scores on the A+F subset.
evaluations should be simpler than the standard ad-
equacy and fluency ones. Therefore the time needed
for both evaluation procedures is probably similar.
Reproducibility of the evaluation is also an impor-
tant concern. We computed the number of ?errors?
in the evaluation process, i.e. the number of sen-
tences evaluated by two or more evaluators where
the evaluators? judgement was different. Only in
10% of the cases the evaluation was contradictory,
in the sense that one evaluator chose one sentence as
better than the other, while the other evaluator chose
the other one. In 30% of the cases, however, one
evaluator estimated both sentences to be of the same
quality while the other judged one sentence as supe-
rior to the other one. As comparison, for the fluency-
adequacy judgement nearly one third of the com-
mon evaluations have a difference in score greater or
equal than two (where the maximum would be four),
and another third a score difference of one point6.
With respect to biases, we feel that it is almost im-
possible to eliminate them if humans are involved. If
one of the judges prefers one kind of structure, there
will a bias for a system producing such output, in-
dependently of the evaluation procedure. However,
the suppression of explicit numerical scores elimi-
nates an additional bias of evaluators. It has been
observed that human judges often give scores within
6Note however that possible evaluator biases can have a
great influence in these statistics.
102
a certain range (e.g. in the mid-range or only ex-
treme values), which constitute an additional diffi-
culty when carrying out the evaluation (Leusch et
al., 2005). Our method suppresses this kind of bias.
Another advantage of our method is the possibil-
ity of assessing improvements within one system.
With one evaluation we can decide if some modi-
fications actually improve performance. This eval-
uation even gives us a confidence interval to weight
the significance of an improvement. Carrying out
a full adequacy-fluency analysis would require a lot
more effort, without giving more useful results.
7 Conclusion
We presented a novel human evaluation technique
that simplifies the task of the evaluators. Our method
relies on two basic observations. The first one is that
in most evaluations the final goal is to find a ranking
of different systems, the absolute scores are usually
not so relevant. Especially when considering human
evaluation, the scores are not even comparable be-
tween two evaluation campaigns. The second one
is the fact that a human judge can normally choose
the best one out of two translations, and this is a
much easier process than the assessment of numeri-
cal scores whose definition is not at all clear. Taking
this into consideration we suggested a method that
aims at finding a ranking of different MT systems
based on the comparison of pairs of translation can-
didates for a set of sentences to be evaluated.
A detailed analysis of the statistical significance
of the method is presented and also applied to some
wide-spread automatic measures. The evaluation
methodology was applied for the ranking of 5 sys-
tems that participated in the second evaluation cam-
paign of the TC-STAR project and comparison with
standard evaluation measures was performed.
8 Acknowledgements
We would like to thank the human judges who par-
ticipated in the evaluation. This work has been
funded by the integrated project TC-STAR? Tech-
nology and Corpora for Speech-to-Speech Transla-
tion ? (IST-2002-FP6-506738).
References
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluationx.
IEEE ICASSP, pages 409?412, Montreal, Canada,
May.
T. Bui and M. Thanh. 1985. Significant improvements to
the Ford-Johnson algorithm for sorting. BIT Numeri-
cal Mathematics, 25(1):70?75.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. Proceeding of the 11th Conference of the Eu-
ropean Chapter of the ACL: EACL 2006, pages 249?
256, Trento, Italy, Apr.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Proc. ARPA Workshop on Human Language
Technology.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman & Hall, New York and
London.
L. Ford Jr and S. Johnson. 1959. A Tournament Problem.
The American Mathematical Monthly, 66(5):387?389.
D. E. Knuth. 1973. The Art of Computer Programming,
volume 3. Addison-Wesley, 1st edition. Sorting and
Searching.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. Proceedings of the Workshop on Statisti-
cal Machine Translation, pages 102?121, New York
City, Jun.
G. Leusch, N. Ueffing, D. Vilar, and H. Ney. 2005.
Preprocessing and normalization for automatic evalu-
ation of machine translation. 43rd ACL: Proc. Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization, pages 17?24, Ann Ar-
bor, Michigan, Jun.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. Proc. of the 41st ACL, pages
160?167, Sapporo, Japan, Jul.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. Proc. of the 40th ACL, pages 311?318,
Philadelphia, PA, Jul.
M. Peczarski. 2002. Sorting 13 elements requires 34
comparisons. LNCS, 2461/2002:785?794, Sep.
M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micci-
ulla, and R. Weischedel. 2005. A study of translation
error rate with targeted human annotation. Technical
Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-
2005-58, University of Maryland, College Park, MD.
L. Thurstone. 1927. The method of paired comparisons
for social values. Journal of Abnormal and Social Psy-
chology, 21:384?400.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical Machine Translation of European
Parliamentary Speeches. Proceedings of MT Summit
X, pages 259?266, Phuket, Thailand, Sep.
M. Wells. 1971. Elements of combinatorial computing.
Pergamon Press.
Y. Zhang and S. Vogel. 2004. Measuring confidence
intervals for the machine translation evaluation met-
rics. Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation, pages 4?6, Baltimore, MD.
103
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 29?32,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Syntax-oriented evaluation measures for machine translation output
Maja Popovic? and Hermann Ney
RWTH Aachen University
Aachen, Germany
popovic,ney@informatik.rwth-aachen.de
Abstract
We explored novel automatic evaluation
measures for machine translation output
oriented to the syntactic structure of the
sentence: the BLEU score on the detailed
Part-of-Speech (POS) tags as well as the
precision, recall and F-measure obtained
on POS n-grams. We also introduced F-
measure based on both word and POS n-
grams. Correlations between the new met-
rics and human judgments were calcu-
lated on the data of the first, second and
third shared task of the Statistical Machine
Translation Workshop. Machine transla-
tion outputs in four different European
languages were taken into account: En-
glish, Spanish, French and German. The
results show that the new measures cor-
relate very well with the human judge-
ments and that they are competitive with
the widely used BLEU, METEOR and TER
metrics.
1 Introduction
We proposed several syntax-oriented automatic
evaluation measures based on sequences of POS
tags and investigated how they correlate with hu-
man judgments. The new measures are the POS-
BLEU score, i.e. the BLEU score calculated on
POS tags instead of words, as well as the POSP, the
POSR and the POSF score: precision, recall and F-
measure calculated on POS n-grams. In addition
to the metrics based only on POS tags, we investi-
gated a WPF score, i.e. an F-measure which takes
into account both word and POS n-grams.
The correlations on the document level were
computed on the English, French, Spanish and
German texts generated by various translation sys-
tems in the framework of the first (Koehn and
Monz, 2006), second (Callison-Burch et al, 2007)
and third shared translation task (Callison-Burch
et al, 2008). Preliminary experiments were car-
ried out on the data from the first (2006) and
the second task (2007) ? Spearman?s rank corre-
lation coefficients between the adequacy and flu-
ency scores and the POSBLEU, POSP, POSR and
POSF scores were calculated. The POSBLEU and
the POSF score were shown to be the most promis-
ing, so that these metrics were submitted to the
official shared evaluation task 2008. The results
of this evaluation showed that these metrics also
correlate well on the document level with another
human score, i.e. the sentence ranking. However,
on the sentence level the results were less promis-
ing. The possible reason for this is the main draw-
back of the metrics based on pure POS tags, i.e.
neglecting the lexical aspect. Therefore we also
introduced a WPF score which takes into account
both word n-grams and POS n-grams.
2 Syntactic-oriented evaluation metrics
We investigated the following metrics oriented on
the syntactic structure of a translation output:
? POSBLEU
The standard BLEU score (Papineni et al,
2002) calculated on POS tags instead of
words;
? POSP
POS n-gram precision: percentage of POS n-
grams in the hypothesis which have a coun-
terpart in the reference;
? POSR
Recall measure based on POS n-grams: per-
centage of POS n-grams in the reference
which are also present in the hypothesis;
? POSF
POS n-gram based F-measure: takes into ac-
count all POS n-grams which have a counter-
29
part, both in the reference and in the hypoth-
esis.
? WPF
F-measure based both on word and POS n-
grams: takes into account all word n-grams
and all POS n-grams which have a counter-
part both in the corresponding reference and
hypothesis.
The prerequisite for all metrics is availability of
an appropriate POS tagger for the target language.
It should be noted that the POS tags cannot be only
basic but must have all details (e.g. verb tenses,
cases, number, gender, etc.).
The n-gram scores as well as the POSBLEU
score are based on fourgrams (i.e. the value of
maximal n is 4). For the n-gram-based measures,
two types of n-gram averaging were investigated:
geometric mean and aritmetic mean. Geometric
mean is already widely used in the BLEU score, but
is also argued not to be optimal because the score
becomes equal to zero even if only one of the n-
gram counts is equal to zero. However, this prob-
lem is probably less critical for POS-based metrics
because the tag set sizes are much smaller than vo-
cabulary sizes.
3 Correlations between the new metrics
and human judgments
The syntax-oriented evaluation metrics were com-
pared with human judgments by means of Spear-
man correlation coefficients ?. Spearman?s rank
correlation coefficient is equivalent to Pearson cor-
relation on ranks, and its advantage is that it makes
fewer assumptions about the data. The possible
values of ? range between 1 (if all systems are
ranked in the same order) and -1 (if all systems are
ranked in the reverse order). Thus the higher value
of ? for an automatic metric, the more similar it
is to the human metric. Correlation coefficients
between human scores and three well-known au-
tomatic measures BLEU, METEOR and TER were
calculated as well, in order to see how the new
metrics perform in comparison with widely used
metrics. The scores were calculated for outputs
of translation from Spanish, French and German
into English and vice versa. English and Ger-
man POS tags were produced using the TnT tag-
ger (Brants, 2000), Spanish texts were annotated
using the FreeLing analyser (Carreras et al, 2004),
and French texts using the TreeTagger1. In this
way, all references and hypotheses were provided
with detailed POS tags.
Experiments on 2006 and 2007 test data
The preliminary experiments with the new eval-
uation metrics were performed on the data from
the first two shared tasks in order to investigate
Spearman correlation coefficients ? between POS-
based evaluation measures and the human scores
adequacy and fluency. The metrics described in
Section 2 (except the WPF score) were calculated
for all translation outputs. For each new metric,
the ? coefficient with the adequacy and with the
fluency score on the document level were calcu-
lated. Then the results were summarised by aver-
aging obtained coefficients over all translation out-
puts, and the average correlations are presented in
Table 1.
2006+2007 adequacy fluency
BLEU 0.590 0.544
METEOR 0.598 0.538
TER 0.496 0.479
POSBLEU 0.642 0.626
POSF gm 0.586 0.551
am 0.584 0.570
POSR gm 0.572 0.576
am 0.542 0.544
POSP gm 0.551 0.481
am 0.531 0.461
Table 1: Average system-level correlations be-
tween automatic evaluation measures and ade-
quacy/fluency scores for 2006 and 2007 test data
(gm = geometric mean for n-gram averaging, am
= arithmetic mean).
Table 1 shows that the new measures have
high ? coefficients both with respect to the ade-
quacy and to the fluency score. The POSBLEU
score has the highest correlations, followed by the
POSF score. Furthermore, the POSBLEU score has
higher correlations than each of the three widely
used metrics, and all the new metrics except the
POSP have higher correlations than the TER. The
POSF correlations with the fluency are higher than
those for the standard metrics, and with the ad-
equacy are comparable to those for the METEOR
and the BLEU score.
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
30
Table 2 presents the percentage of the docu-
ments for which the particular new metric has
higher correlation than BLEU, METEOR or TER. It
can be seen that on the majority of the documents
the POSBLEU metric outperforms all three stan-
dard measures, especially the correlation with the
fluency score. The geometric mean POSF shows
similar behaviour, having higher correlation than
the standard measures in majority of the cases but
slightly less often than the POSBLEU. The POSR
has higher correlation than the standard measures
in 50-70% of cases, and the POSP score has the
lowest percentage, 30-60%. It can be also seen
that the geometric mean averaging of the n-grams
correlates better with the human judgments more
often than the artimetic mean.
Experiments on 2008 test data
For the official shared evaluation task in 2008, the
human evaluation scores were different ? the ad-
equacy and fluency scores were abandoned being
rather time consuming and often inconsistent, and
the sentence ranking was proposed as one of the
human evaluation scores: the manual evaluators
were asked to rank translated sentences relative
to each other. RWTH participated in this shared
task with the two most promising metrics accord-
ing to the previous experiments, i.e. POSBLEU
and POSF, and the detailed results can be found
in (Callison-Burch et al, 2008). It was shown that
these metrics also correlate very well with the sen-
tence ranking on the document level. However,
on the sentence level the performance was much
weaker: a percentage of sentence pairs for which
the human comparison yields the same result as
the comparison using particular automatic metric
was not very high. We believe that the main rea-
son for this is the fact that the metrics based only
on the POS tags can assign high scores to transla-
tions without correct semantic meaning, because
they are taking into account only syntactic struc-
ture without taking into account the actual words.
For example, if the reference translation is ?This
sentence is correct?, a translation output ?This tree
is high? would have a POS-based matching score
of 100%. Therefore we introduced the WPF score
? an F-measure metrics which counts both match-
ing POS n-grams and matching word n-grams.
The ? coefficients for the POSBLEU, POSF and
WPF with the sentence ranking averaged over all
translation outputs are shown in Table 3. The cor-
relations for several known metrics are shown as
well, i.e. for the BLEU, METEOR and TER along
with their variants: METEOR-r denotes the vari-
ant optimised for ranking, whereas MBLEU and
MTER are BLEU and TER computed using the
flexible matching as used in METEOR. It can be
seen that the correlation coefficients for all three
syntactic metrics are high. The POSBLEU score
has the highest correlation with the sentence rank-
ing, followed by POSF and WPF. All three mea-
sures have higher average correlation than MTER,
MBLEU and BLEU. The purely syntactic metrics
outperform also the METEOR scores, whereas the
WPF correlations are comparable with those of the
METEOR scores.
2008 sentence ranking
BLEU 0.526
MBLEU 0.504
METEOR 0.638
METEOR-r 0.603
MTER 0.318
POSBLEU 0.712
POSF gm 0.663
am 0.661
WPF gm 0.600
am 0.628
Table 3: Average system-level correlations be-
tween automatic evaluation measures and human
ranking for 2008 test data.
Table 4 presents the percentage of the docu-
ments where the particular syntactic metric has
higher correlation with the sentence ranking than
the particular standard metric. All syntactic met-
rics have higher correlation than the MTER on al-
most all documents, and on a large number of doc-
uments than the MBLEU score. The correlations
for syntactic measures are better than those for the
BLEU score for more than 60% of documents. As
for the METEOR scores, the syntactic metrics are
comparable (about 50%).
4 Conclusions
The results presented in this article suggest that
the syntactic information has the potential to
strenghten automatic evaluation metrics, and there
are many possible directions for future work. We
proposed several syntax-oriented evaluation met-
rics based on the detailed POS tags: the POS-
BLEU score and POS-n-gram precision, recall and
31
adequacy fluency
2006+2007 BLEU METEOR TER BLEU METEOR TER
POSBLEU 77.3 58.3 75.0 81.8 83.3 83.3
POSF gm 72.7 58.3 75.0 63.6 75.0 83.3
am 68.2 58.3 75.0 63.6 66.7 68.1
POSR gm 63.6 75.0 58.3 68.1 66.7 58.3
am 54.5 75.0 58.3 63.6 58.3 50.0
POSP gm 63.6 50.0 75.0 45.4 50.0 58.3
am 54.5 41.7 66.7 36.4 50.0 58.3
Table 2: Percentage of documents from the 2006 and 2007 shared tasks where the particular new metric
has better correlation with adequacy/fluency than the particular standard metric.
2008 BLEU MBLEU MTER METEOR METEOR-r
POSBLEU 71.4 85.7 92.8 57.1 64.3
POSF am 64.3 78.6 92.8 50.0 50.0
gm 64.3 78.6 92.8 57.1 50.0
WPF am 57.1 64.3 100 42.8 50.0
gm 57.1 64.3 92.8 42.8 50.0
Table 4: Percentage of documents from the 2008 shared task where the new metric has better correlation
with the human sentence ranking than the standard metric.
F-measure, i.e. the POSP, POSR, and POSF score.
In addition, we introduced a measure which takes
into account both POS tags and words: the WPF
score. We carried out an extensive analysis of
the Spearman?s rank correlation coefficients be-
tween the syntactic evaluation metrics and the hu-
man judgments. The obtained results showed that
the new metrics correlate well with human judg-
ments, namely the adequacy and fluency scores,
as well as the sentence ranking. The results also
showed that the syntax-oriented metrics are com-
petitive with the widely used evaluation measures
BLEU, METEOR and TER. Especially promising
are the POSBLEU and the POSF score. The cor-
relations of the WPF score are slightly lower than
those of the purely POS based metrics ? however,
this metric has advantage of taking both syntactic
and lexical aspect into account.
Acknowledgments
This work was realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovati on.
References
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
pages 224?231, Seattle, WA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of Machine Translation. In Pro-
ceedings of the ACL Workshop on Statistical Ma-
chine Translation, pages 136?158, Prague, Czech
Republic, June.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, Columbus, Ohio, June.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 239?242, Lisbon, Portugal,
May.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proceedings on the Work-
shop on Statistical Machine Translation, New York
City, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
32
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 51?55,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH System Combination System for WMT 2009
Gregor Leusch, Evgeny Matusov, and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the System Combi-
nation task of the Fourth Workshop on Sta-
tistical Machine Translation (WMT 2009).
Hypotheses from 9 German?English MT
systems were combined into a consen-
sus translation. This consensus transla-
tion scored 2.1% better in BLEU and 2.3%
better in TER (abs.) than the best sin-
gle system. In addition, cross-lingual
output from 10 French, German, and
Spanish?English systems was combined
into a consensus translation, which gave
an improvement of 2.0% in BLEU/3.5% in
TER (abs.) over the best single system.
1 Introduction
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al, 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using the prior prob-
abilities for each system as well as other statistical
models such as a special n-gram language model.
2 System Combination Algorithm
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary hypothesis. From this, we create a confu-
sion network (CN), which we then rescore using
Figure 1: The system combination architecture.
system prior weights and a language model (LM).
The single best path in this CN then constitutes the
consensus translation.
2.1 Word Alignment
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each source sentence F in the test corpus,
we select one of its translations En, n=1, . . . ,M,
as the primary hypothesis. Then we align the sec-
ondary hypotheses Em(m = 1, . . . ,M ;n 6= m)
with En to match the word order in En. Since it is
not clear which hypothesis should be primary, i. e.
has the ?best? word order, we let every hypothesis
play the role of the primary translation, and align
all pairs of hypotheses (En, Em); n 6= m.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
to estimate the alignment model.
The alignment training corpus is created from a
test corpus1 of effectively M ? (M ? 1) ? N sen-
tences translated by the involved MT engines. The
single-word based lexicon probabilities p(e|e?) are
initialized from normalized lexicon counts col-
lected over the sentence pairs (Em, En) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring identical words,
i. e. whether em,j is the same word as en,i for some
i and j. In addition, we add a fraction of a count
for words with identical prefixes.
1A test corpus can be used directly because the align-
ment training is unsupervised and only automatically pro-
duced translations are considered.
51
The model parameters are trained iteratively us-
ing the GIZA++ toolkit (Och and Ney, 2003). The
training is performed in the directions Em ? En
and En ? Em. After each iteration, the updated
lexicon tables from the two directions are interpo-
lated. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model. Two differ-
ent alignments are computed using the cost matrix
C: the alignment a? used for reordering each sec-
ondary translation Em, and the alignment a? used
to build the confusion network.
In addition to the GIZA++ alignments, we have
also conducted preliminary experiments follow-
ing He et al (2008) to exploit character-based
similarity, as well as estimating p(e|e?) :=?
f p(e|f)p(f |e
?) directly from a bilingual lexi-
con. But we were not able to find improvements
over the GIZA++ alignments so far.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix according to a?, we determine M?1 mono-
tone one-to-one alignments between En as the pri-
mary translation and Em,m = 1, . . . ,M ;m 6= n.
We then construct the confusion network. In case
of many-to-one connections in a? of words in Em
to a single word from En, we only keep the con-
nection with the lowest alignment costs.
The use of the one-to-one alignment a? implies
that some words in the secondary translation will
not have a correspondence in the primary transla-
tion and vice versa. We consider these words to
have a null alignment with the empty word ?. In
the corresponding confusion network, the empty
word will be transformed to an ?-arc.
M ? 1 monotone one-to-one alignments can
then be transformed into a confusion network. We
follow the approach of Bangalore et al (2001)
with some extensions. Multiple insertions with re-
gard to the primary hypothesis are sub-aligned to
each other, as described by Matusov et al (2008).
Figure 2 gives an example for the alignment.
2.3 Voting in the confusion network
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for all hypotheses as pri-
mary, and unite them into a single lattice. In our
experience, this approach is advantageous in terms
of translation quality, e.g. by 0.7% in BLEU com-
pared to a minimum Bayes risk primary (Rosti et
al., 2007). Weighted majority voting on a single
confusion network is straightforward and analo-
gous to ROVER (Fiscus, 1997). We sum up the
probabilities of the arcs which are labeled with the
same word and have the same start state and the
same end state. To exploit the true casing abilities
of the input MT systems, we sum up the scores of
arcs bearing the same word but in different cases.
Here, we leave the decision about upper or lower
case to the language model.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we took the system hypotheses for the
same test corpus for which the consensus trans-
lations are to be produced. Using this ?adapted?
LM for lattice rescoring thus gives bonus to n-
grams from the original system hypotheses, in
most cases from the original phrases. Presum-
ably, many of these phrases have a correct word or-
der, since they are extracted from the training data.
Previous experimental results show that using this
LM in rescoring together with a word penalty (to
counteract any bias towards short sentences) no-
tably improves translation quality. This even re-
sults in better translations than using a ?classical?
LM trained on a monolingual training corpus. We
attribute this to the fact that most of the systems
we combine are phrase-based systems, which al-
ready include such general LMs. Since we are us-
ing a true-cased LM trained on the hypotheses, we
can exploit true casing information from the in-
put systems by using this LM to disambiguate be-
tween the separate arcs generated for the variants
(see Section 2.3).
After LM rescoring, we add the probabilities of
identical partial paths to improve the estimation
of the score for the best hypothesis. This is done
through determinization of the lattice.
2.5 Extracting Consensus Translations
To generate our consensus translation, we extract
the single-best path within the rescored confusion
network. With our approach, we could also extract
N -best hypotheses. In a subsequent step, these N -
best lists could be rescored with additional statis-
tical models (Matusov et al, 2008). But as we did
not have the resources in the WMT 2009 evalua-
tion, this step was dropped for our submission.
3 Tuning system weights
System weights, LM factor, and word penalty
need to be tuned to produce good consensus trans-
lations. We optimize these parameters using the
52
0.25 would your like coffee or tea
system 0.35 have you tea or Coffee
hypotheses 0.10 would like your coffee or
0.30 I have some coffee tea would you like
alignment have|would you|your $|like Coffee|coffee or|or tea|tea
and would|would your|your like|like coffee|coffee or|or $|tea
reordering I|$ would|would you|your like|like have|$ some|$ coffee|coffee $|or tea|tea
$ would your like $ $ coffee or tea
confusion $ have you $ $ $ Coffee or tea
network $ would your like $ $ coffee or $
I would you like have some coffee $ tea
$ would you $ $ $ coffee or tea
voting 0.7 0.65 0.65 0.35 0.7 0.7 0.5 0.7 0.9
(normalized) I have your like have some Coffee $ $
0.3 0.35 0.35 0.65 0.3 0.3 0.5 0.3 0.1
consensus translation would you like coffee or tea
Figure 2: Example of creating a confusion network from monotone one-to-one word alignments (denoted
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an ?-arc in the corresponding part of the confusion network.
Table 1: Systems combined for the WMT 2009
task. Systems written in oblique were also used in
the Cross Lingual task (rbmt3 for FR?EN).
DE?EN google, liu, rbmt3, rwth, stutt-
gart, systran, uedin, uka, umd
ES?EN google, nict, rbmt4, rwth,
talp-upc, uedin
FR?EN dcu, google, jhu, limsi, lium-
systran, rbmt4, rwth, uedin, uka
publicly available CONDOR optimization toolkit
(Berghen and Bersini, 2005). For the WMT
2009 Workshop, we selected a linear combina-
tion of BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) as optimization criterion,
?? := argmax? {(2 ? BLEU)? TER}, based on
previous experience (Mauser et al, 2008). We
used the whole dev set as a tuning set. For more
stable results, we used the case-insensitive variants
for both measures, despite the explicit use of case
information in our approach.
4 Experimental results
Due to the large number of submissions (71 in
total for the language pairs DE?EN, ES?EN,
FR?EN), we had to select a reasonable number
of systems to be able to tune the parameters in
a reliable way. Based on previous experience,
we manually selected the systems with the best
BLEU/TER score, and tried different variations of
this selection, e.g. by removing systems which
had low weights after optimization, or by adding
promising systems, like rule based systems.
Table 1 lists the systems which made it into
our final submission. In our experience, if a large
number of systems is available, using n-best trans-
lations does not give better results than using sin-
gle best translations, but raises optimization time
significantly. Consequently, we only used single
best translations from all systems.
The results also confirm another observation:
Even though rule-based systems by itself may
have significantly lower automatic evaluation
scores (e.g. by 2% or more in BLEU on DE?EN),
they are often very important in system combina-
tion, and can improve the consensus translation
e.g. by 0.5% in BLEU.
Having submitted our translations to the WMT
workshop, we calculated scores on the WMT 2009
test set, to verify the results on the tuning data.
Both the results on the tuning set and on the test
set can be found in the following tables.
4.1 The Google Problem
One particular thing we noticed is that in the lan-
guage pairs of FR?EN and ES?EN, the trans-
lations from one provided single system (Google)
were much better in terms of BLEU and TER than
those of all other systems ? in the former case
by more than 4% in BLEU. In our experience,
our system combination approach requires at least
three ?comparably good? systems to be able to
achieve significant improvements. This was con-
firmed in the WMT 2009 task as well: Neither in
FR?EN nor in ES?EN we were able to achieve
an improvement over the Google system. For this
reason, we did not submit consensus translations
for these two language pairs. On the other hand,
we would have achieved significant improvements
over all (remaining) systems leaving out Google.
4.2 German?English (DE?EN)
Table 2 lists the scores on the tuning and test set
for the DE?EN task. We can see that the best
systems are rather close to each other in terms
of BLEU. Also, the rule-based translation system
(RBMT), here SYSTRAN, scores rather well. As
a consequence, we find a large improvement using
system combination: 2.9%/2.7% abs. on the tun-
ing set, and still 2.1%/2.3% on test, which means
that system combination generalizes well here.
4.3 Spanish?English (ES?EN),
French?English (FR?EN)
In Table 3, we see that on the ES?EN and
FR?EN tasks, a single system ? Google ? scores
significantly better on the TUNE set than any other
53
Table 2: German?English task: case-insensitive
scores. Best single system was Google, second
best UKA, best RBMT Systran. SC stands for sys-
tem combination output.
TUNE TEST
German?English BLEU TER BLEU TER
Best single 23.2 59.5 21.3 61.3
Second best single 23.0 58.8 21.0 61.7
Best RBMT 21.3 61.3 18.9 63.7
SC (9 systems) 26.1 56.8 23.4 59.0
w/o RBMT 24.5 57.3 22.5 59.2
w/o Google 24.9 57.4 23.0 59.1
Table 3: Spanish?English and French?English
task: scores on the tuning set after system combi-
nation weight tuning (case-insensitive). Best sin-
gle system was Google, second best was Uedin
(Spanish) and UKA (French). No results on TEST
were generated.
ES?EN FR?EN
Spanish?English BLEU TER BLEU TER
Best single 29.5 53.6 32.2 50.1
Second best single 26.9 56.1 28.0 54.6
SC (6/9 systems) 28.7 53.6 30.7 52.5
w/o Google 27.5 55.6 30.0 52.8
system, namely by 2.6%/4.2% resp. in BLEU. As
a result, a combination of these systems scores
better than any other system, even when leaving
out the Google system. But it gives worse scores
than the single best system. This is explainable,
because system combination is trying to find a
consensus translation. For example, in one case,
the majority of the systems leave the French term
?wagon-lit? untranslated; spurious translations in-
clude ?baggage car?, ?sleeping car?, and ?alive?.
As a result, the consensus translation also contains
?wagon-lit?, not the correct translation ?sleeper?
which only the Google system provides. Even tun-
ing all other system weights to zero would not re-
sult in pure Google translations, as these weights
neither affect the LM nor the selection of the pri-
mary hypothesis in our approach.
4.4 Cross-Lingual?English (XX?EN)
Finally, we have conducted experiments on cross-
lingual system combination, namely combining
the output from DE?EN, ES?EN, and FR?EN
systems to a single English consensus transla-
tion. Some interesting results can be found in
Table 4. We see that this consensus translation
scores 2.0%/3.5% better than the best single sys-
tem, and 4.4%/5.6% better than the second best
single system. While this is only 0.8%/2.5% bet-
ter than the combination of only the three Google
systems, the combination of the non-Google sys-
Table 4: Cross-lingual task: combination
of German?English, Spanish?English, and
French?English. Case-insensitive scores. Best
single system was Google for all language pairs.
Cross-lingual TUNE TEST
? English BLEU TER BLEU TER
Best single German 23.2 59.5 21.3 61.3
Best single Spanish 29.5 53.6 28.7 53.8
Best single French 32.2 50.1 31.1 51.7
SC (10 systems) 35.5 46.4 33.1 48.2
w/o RBMT 35.1 46.5 32.7 48.3
w/o Google 32.3 48.8 29.9 50.5
3 Google systems 34.2 48.0 32.3 49.2
w/o German 34.0 49.3 31.5 50.9
w/o Spanish 33.4 49.8 31.0 51.9
w/o French 30.5 51.4 28.6 52.3
tems leads to translations that could compete with
the FR?EN Google system. Again, we see that
RBMT systems lead to a small improvement of
0.4% in BLEU, although their scores are signif-
icantly worse than those of the competing SMT
systems.
Regarding languages, we see that despite the
large differences in the quality of the systems (10
points between DE?EN and FR?EN), all lan-
guages seem to provide significant information to
the consensus translation: While FR?EN cer-
tainly has the largest influence (?4.5% in BLEU
when left out), even DE?EN ?contributes? 1.6
BLEU points to the final submission.
5 Conclusions
We have shown that our system combination sys-
tem can lead to significant improvements over
single best MT output where a significant num-
ber of comparably good translations is available
on a single language pair. For cross-lingual sys-
tem combination, we observe even larger improve-
ments, even if the quality in terms of BLEU or
TER between the systems of different language
pairs varies significantly. While the input of high-
quality SMT systems has the largest weight for the
consensus translation quality, we find that RBMT
systems can give important additional information
leading to better translations.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
54
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001.
Computing consensus translation from multiple ma-
chine translation systems. In IEEE Automatic
Speech Recognition and Understanding Workshop,
Madonna di Campiglio, Italy, December.
F. V. Berghen and H. Bersini. 2005. CONDOR,
a new parallel, constrained extension of Powell?s
UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore.
2008. Indirect-HMM-based hypothesis alignment
for combining outputs from machine translation sys-
tems. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 98?107, Honolulu, Hawaii, October.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312?319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
55
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 66?69,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH Machine Translation System for WMT 2009
Maja Popovic?, David Vilar, Daniel Stein, Evgeny Matusov and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the shared transla-
tion task of the Fourth Workshop of Sta-
tistical Machine Translation (WMT 2009)
with the German-English, French-English
and Spanish-English pair in each transla-
tion direction. The submissions were gen-
erated using a phrase-based and a hierar-
chical statistical machine translation sys-
tems with appropriate morpho-syntactic
enhancements. POS-based reorderings of
the source language for the phrase-based
systems and splitting of German com-
pounds for both systems were applied. For
some tasks, a system combination was
used to generate a final hypothesis. An ad-
ditional English hypothesis was produced
by combining all three final systems for
translation into English.
1 Introduction
For the WMT 2009 shared task, RWTH submit-
ted translations for the German-English, French-
English and Spanish-English language pair in both
directions. A phrase-based translation system en-
hanced with appropriate morpho-syntactic trans-
formations was used for all translation direc-
tions. Local POS-based word reorderings were ap-
plied for the Spanish-English and French-English
pair, and long range reorderings for the German-
English pair. For this language pair splitting
of German compounds was also applied. Spe-
cial efforts were made for the French-English and
German-English translation, where a hierarchi-
cal system was also used and the final submis-
sions are the result of a system combination. For
translation into English, an additional hypothesis
was produced as a result of combination of the
final German-to-English, French-to-English and
Spanish-to-English systems.
2 Translation models
2.1 Phrase-based model
We used a standard phrase-based system similar to
the one described in (Zens et al, 2002). The pairs
of source and corresponding target phrases are ex-
tracted from the word-aligned bilingual training
corpus. Phrases are defined as non-empty contigu-
ous sequences of words. The phrase translation
probabilities are estimated using relative frequen-
cies. In order to obtain a more symmetric model,
the phrase-based model is used in both directions.
2.2 Hierarchical model
The hierarchical phrase-based approach can be
considered as an extension of the standard phrase-
based model. In this model we allow the phrases
to have ?gaps?, i.e. we allow non-contiguous parts
of the source sentence to be translated into pos-
sibly non-contiguous parts of the target sentence.
The model can be formalized as a synchronous
context-free grammar (Chiang, 2007). The model
also included some additional heuristics which
have shown to be helpful for improving translation
quality, as proposed in (Vilar et al, 2008).
The first step in the hierarchical phrase extrac-
tion is the same as for the phrased-based model.
Having a set of initial phrases, we search for
phrases which contain other smaller sub-phrases
and produce a new phrase with gaps. In our sys-
tem, we restricted the number of non-terminals for
each hierarchical phrase to a maximum of two,
which were also not allowed to be adjacent. The
scores of the phrases are again computed as rela-
tive frequencies.
2.3 Common models
For both translation models, phrase-based and hi-
erarchical, additional common models were used:
word-based lexicon model, phrase penalty, word
penalty and target language model.
66
The target language model was a standard n-
gram language model trained by the SRI language
modeling toolkit (Stolcke, 2002). The smooth-
ing technique we apply was the modified Kneser-
Ney discounting with interpolation. In our case we
used a 4-gram language model.
3 Morpho-syntactic transformations
3.1 POS-based word reorderings
For the phrase-based systems, the local and
long range POS-based reordering rules described
in (Popovic? and Ney, 2006) were applied on the
training and test corpora as a preprocessing step.
Local reorderings were used for the Spanish-
English and French-English language pairs in or-
der to handle differences between the positions of
nouns and adjectives in the two languages. Adjec-
tives in Spanish and French, as in most Romanic
languages, are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, for these language pairs local
reorderings of nouns and adjective groups in the
source language were applied. The following se-
quences of words are considered to be an adjective
group: a single adjective, two or more consecutive
adjectives, a sequence of adjectives and coordinate
conjunctions, as well as an adjective along with its
corresponding adverb. If the source language is
Spanish or French, each noun is moved behind the
corresponding adjective group. If the source lan-
guage is English, each adjective group is moved
behind the corresponding noun.
Long range reorderings were applied on the
verb groups for the German-English language pair.
Verbs in the German language can often be placed
at the end of a clause. This is mostly the case
with infinitives and past participles, but there are
many cases when other verb forms also occur at
the clause end. For the translation from German
into English, following verb types were moved to-
wards the beginning of a clause: infinitives, infini-
tives+zu, finite verbs, past participles and negative
particles. For the translation from English to Ger-
man, infinitives and past participles were moved
to the end of a clause, where punctuation marks,
subordinate conjunctions and finite verbs are con-
sidered as the beginning of the next clause.
3.2 German compound words
For the translation from German into English, Ger-
man compounds were split using the frequency-
based method described in (Koehn and Knight,
2003). For the other translation direction, the En-
glish text was first translated into the modified
German language with split compounds. The gen-
erated output was then postprocessed, i.e. the
components were merged using the method de-
scribed in (Popovic? et al, 2006): a list of com-
pounds and a list of components are extracted from
the original German training corpus. If the word
in the generated output is in the component list,
check if this word merged with the next word is in
the compound list. If it is, merge the two words.
4 System combination
For system combination we used the approach de-
scribed in (Matusov et al, 2006). The method is
based on the generation of a consensus transla-
tion out of the output of different translation sys-
tems. The core of the method consists in building
a confusion network for each sentence by align-
ing and combining the (single-best) translation hy-
pothesis from one MT system with the translations
produced by the other MT systems (and the other
translations from the same system, if n-best lists
are used in combination). For each sentence, each
MT system is selected once as ?primary? system,
and the other hypotheses are aligned to this hy-
pothesis. The resulting confusion networks are
combined into a signle word graph, which is then
weighted with system-specific factors, similar to
the approach of (Rosti et al, 2007), and a trigram
LM trained on the MT hypotheses. The translation
with the best total score within this word graph is
selected as consensus translation. The scaling fac-
tors of these models are optimized using the Con-
dor toolkit (Berghen and Bersini, 2005) to achieve
optimal BLEU score on the dev set.
5 Experimental results
5.1 Experimental settings
For all translation directions, we used the provided
EuroParl and News parallel corpora to train the
translation models and the News monolingual cor-
pora to train the language models. All systems
were optimised for the BLEU score on the develop-
ment data (the ?dev-a? part of the 2008 evaluation
data). The other part of the 2008 evaluation set
(?dev-b?) is used as a blind test set. The results re-
ported in the next section will be referring to this
test set. For the tasks including a system combi-
nation, the parameters for the system combination
67
were also trained on the ?dev-b? set. The reported
evaluation metrics are the BLEU score and two
syntax-oriented metrics which have shown a high
correlation with human evaluations: the PBLEU
score (BLEU calculated on POS sequences) and
the POS-F-score PF (similar to the BLEU score but
based on the F-measure instead of precision and
on arithmetic mean instead of geometric mean).
The POS tags used for reorderings and for syn-
tactic evaluation metrics for the English and the
German corpora were generated using the statisti-
cal n-gram-based TnT-tagger (Brants, 2000). The
Spanish corpora are annotated using the FreeLing
analyser (Carreras et al, 2004), and the French
texts using the TreeTagger1.
5.2 Translation results
Table 1 presents the results for the German-
English language pair. For translation from Ger-
man into English, results for the phrase-based sys-
tem with and without verb reordering and com-
pound splitting are shown. The hierarchical sys-
tem was trained with split German compounds.
The final submission was produced by combining
those five systems. The improvement obtained by
system combination on the unseen test data 2009
is similar, i.e. from the systems with BLEU scores
of 17.0%, 17.2%, 17.5%, 17.6% and 17.7% to the
final system with 18.5%.
German?English BLEU PBLEU PF
phrase-based 17.8 31.6 39.7
+reorder verbs 18.2 32.6 40.3
+split compounds 18.0 31.9 40.0
+reord+split 18.4 33.1 40.7
hierarchical+split 18.5 33.5 40.1
system combination 19.2 33.8 40.9
English?German BLEU PBLEU PF
phrase-based 13.6 31.6 39.7
+reorder verbs 13.7 32.4 40.2
+split compounds 13.7 32.3 40.1
+reord+split 13.7 32.3 40.1
system combination 14.0 32.7 40.3
Table 1: Translation results [%] for the German-
English language pair, News2008 dev-b.
The other translation direction is more difficult
and improvements from morpho-syntactic trans-
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
formations are smaller. No hierarchical system
was trained for this translation direction. The com-
bination of the four phrase-based systems leads
to further improvements (on the unseen test set
as well: contrastive hypotheses have the BLEU
scores in the range from 12.7% to 13.0%, and the
final BLEU score is 13.2%).
The results for the French-English language
pair are shown in Table 2. For the French-to-
English system, we submitted the result of the
combination of three systems: a phrase-based with
and without local reorderings and a hierarchical
system. For the unseen test set, the BLEU score of
the system combination output is 24.4%, whereas
the contrastive hypotheses have 23.2%, 23.4% and
24.1%. For the other translation direction we did
not use the system combination, the submission is
produced by the phrase-based system with local
adjective reorderings.
French?English BLEU PBLEU PF
phrase-based 20.9 37.1 43.2
+reorder adjectives 21.3 38.2 43.6
hierarchical 20.3 36.7 42.6
system combination 21.7 38.5 43.8
English?French BLEU PBLEU PF
phrase-based 20.2 39.5 45.9
+reorder adjectives 20.7 40.6 46.4
Table 2: Translation results [%] for the French-
English language pair, News2008 dev-b.
Table 3 presents the results for the Spanish-
English language pair. As in the English-to-
French translation, the phrase-based system with
adjective reorderings is used to produce the sub-
mitted hypothesis for both translation directions.
Spanish?English BLEU PBLEU PF
phrase-based 22.1 38.5 44.1
+reorder adjectives 22.5 39.2 44.6
English?Spanish BLEU PBLEU PF
phrase-based 20.6 29.3 35.7
+reorder adjectives 21.1 29.7 35.9
Table 3: Translation results [%] for the Spanish-
English language pair, News2008 dev-b.
68
The result of the additional experiment, i.e. for
the multisource translation int English is presented
in Table 4. The English hypothesis is produced by
the combination of the three best systems for each
language pair, and it can be seen that the transla-
tion performance increases in all measures. This
suggests that each language pair poses different
difficulties for the translation task, and the com-
bination of all three can improve performance.
F+S+G?English BLEU PBLEU PF
system combination 25.1 41.0 46.4
Table 4: Multisource translation results [%]:
the English hypothesis is obtained as result of
a system combination of all language pairs,
News2008 dev-b.
6 Conclusions
The RWTH system submitted to the WMT 2009
shared translation task used a phrase-based sys-
tem and a hierarchical system with appropriate
morpho-syntactic extensions, i.e. POS based word
reorderings and splitting of German compounds
were used. System combination produced gains
in BLEU score over phrasal-system baselines in
the German-to-English, English-to-German and
French-to-English tasks.
Acknowledgments
This work was realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
Powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
pages 224?231, Seattle, WA.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 239?242, Lisbon, Portugal,
May.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, (33):201?228.
Philipp Koehn and Kevin Knight. 2003. Empiri-
cal methods for compound splitting. In Proceed-
ings 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 347?354, Budapest, Hungary, April.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Proceedings of EACL
2006 (11th Conference of the European Chapter
of the Association for Computational Linguistics),
pages 33?40, Trento, Italy, April.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Trans-
lation. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC), pages 1278?1283, Genoa, Italy, May.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of german compound
words. In Proceedings of the 5th International Con-
ference on Natural Language Processing (FinTAL),
pages 616?624, Turku, Finland, August. Lecture
Notes in Computer Science, Springer Verlag.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
25th German Conference on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artifi-
cial Intelligence (LNAI), pages 18?32, Aachen, Ger-
many, September. Springer Verlag.
69
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 233?241,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Deep Learning Approach to Machine Transliteration
Thomas Deselaers and Sas?a Hasan and Oliver Bender and Hermann Ney
Human Language Technology and Pattern Recognition Group ? RWTH Aachen University
<surname>@cs.rwth-aachen.de
Abstract
In this paper we present a novel translit-
eration technique which is based on deep
belief networks. Common approaches
use finite state machines or other meth-
ods similar to conventional machine trans-
lation. Instead of using conventional NLP
techniques, the approach presented here
builds on deep belief networks, a tech-
nique which was shown to work well for
other machine learning problems. We
show that deep belief networks have cer-
tain properties which are very interesting
for transliteration and possibly also for
translation and that a combination with
conventional techniques leads to an im-
provement over both components on an
Arabic-English transliteration task.
1 Introduction
Transliteration, i.e. the transcription of words such
as proper nouns from one language into another or,
more commonly from one alphabet into another, is
an important subtask of machine translation (MT)
in order to obtain high quality output.
We present a new technique for transliteration
which is based on deep belief networks (DBNs),
a well studied approach in machine learning.
Transliteration can in principle be considered to be
a small-scale translation problem and, thus, some
ideas presented here can be transferred to the ma-
chine translation domain as well.
Transliteration has been in use in machine trans-
lation systems, e.g. Russian-English, since the ex-
istence of the field of machine translation. How-
ever, to our knowledge it was first studied as a
machine learning problem by Knight and Graehl
(1998) using probabilistic finite-state transducers.
Subsequently, the performance of this system was
greatly improved by combining different spelling
and phonetic models (Al-Onaizan and Knight,
2002). Huang et al (2004) construct a proba-
bilistic Chinese-English edit model as part of a
larger alignment solution using a heuristic boot-
strapped procedure. Freitag and Khadivi (2007)
propose a technique which combines conventional
MT methods with a single layer perceptron.
In contrast to these methods which strongly
build on top of well-established natural language
processing (NLP) techniques, we propose an al-
ternative model. Our new model is based on deep
belief networks which have been shown to work
well in other machine learning and pattern recog-
nition areas (cf. Section 2). Since translation and
transliteration are closely related and translitera-
tion can be considered a translation problem on the
character level, we discuss various methods from
both domains which are related to the proposed
approach in the following.
Neural networks have been used in NLP in
the past, e.g. for machine translation (Asuncio?n
Castan?o et al, 1997) and constituent parsing
(Titov and Henderson, 2007). However, it might
not be straight-forward to obtain good results us-
ing neural networks in this domain. In general,
when training a neural network, one has to choose
the structure of the neural network which involves
certain trade-offs. If a small network with no hid-
den layer is chosen, it can be efficiently trained
but has very limited representational power, and
may be unable to learn the relationships between
the source and the target language. The DBN ap-
proach alleviates some of the problems that com-
monly occur when working with neural networks:
1. they allow for efficient training due to a good
initialisation of the individual layers. 2. Overfit-
ting problems are addressed by creating generative
models which are later refined discriminatively. 3.
The network structure is clearly defined and only a
few structure parameters have to be set. 4. DBNs
can be interpreted as Bayesian probabilistic gener-
ative models.
Recently, Collobert and Weston (2008) pro-
posed a technique which applies a convolutional
DBN to a multi-task learning NLP problem. Their
approach is able to address POS tagging, chunk-
ing, named entity tagging, semantic role and simi-
lar word identification in one model. Our model is
similar to this approach in that it uses the same ma-
chine learning techniques but the encoding and the
233
processing is done differently. First, we learn two
independent generative models, one for the source
input and one for the target output. Then, these
two models are combined into a source-to-target
encoding/decoding system (cf. Section 2).
Regarding that the target is generated and not
searched in a space of hypotheses (e.g. in a word
graph), our approach is similar to the approach
presented by Bangalore et al (2007) who present
an MT system where the set of words of the tar-
get sentence is generated based on the full source
sentence and then a finite-state approach is used to
reorder the words. Opposed to this approach we
do not only generate the letters/words in the target
sentence but we generate the full sentence with or-
dering.
We evaluate the proposed methods on an
Arabic-English transliteration task where Arabic
city names have to be transcribed into the equiva-
lent English spelling.
2 Deep Belief Networks for
Transliteration
Although DBNs are thoroughly described in the
literature, e.g. (Hinton et al, 2006), we give a short
overview on the ideas and techniques and intro-
duce our notation.
Deep architectures in machine learning and ar-
tificial intelligence are becoming more and more
popular after an efficient training algorithm has
been proposed (Hinton et al, 2006), although the
idea is known for some years (Ackley et al, 1985).
Deep belief networks consist of multiple layers of
restricted Boltzmann machines (RBMs). It was
shown that DBNs can be used for dimensionality
reduction of images and text documents (Hinton
and Salakhutdinow, 2006) and for language mod-
elling (Mnih and Hinton, 2007). Recently, DBNs
were also used successfully in image retrieval to
create very compact but meaningful representa-
tions of a huge set of images (nearly 13 million)
for retrieval (Torralba et al, 2008).
DBNs are built from RBMs by first training an
RBM on the input data. A second RBM is built
on the output of the first one and so on until a
sufficiently deep architecture is created. RBMs
are stochastic generative artificial neural networks
with restricted connectivity. From a theoretical
viewpoint, RBMs are interesting because they are
able to discover complex regularities and find no-
table features in data (Ackley et al, 1985).
Figure 1: A schematic representation of our DBN
for transliteration.
Hinton and Salakhutdinow (2006) present a
deep belief network to learn a tiny representation
of its inputs and to reconstruct the input with high
accuracy which is demonstrated for images and
textual documents. Here, we use DBNs similarly:
first, we learn encoders for the source and tar-
get words respectively and then connect these two
through a joint layer to map between the two lan-
guages. This joint layer is trained in the same way
as the top-level neurons in the deep belief classi-
fier from (Hinton et al, 2006).
In Figure 1, a schematic view of our DBN for
transliteration is shown. On the left and on the
right are encoders for the source and target words
respectively. To transliterate a source word, it is
passed through the layers of the network. First, it
traverses through the source encoder on the left,
then it passes into the joint layer, finally travers-
ing down through the target encoder. Each layer
consists of a set of neurons receiving the output
of the preceding layer as input. The first layers in
the source and target encoders consist of S1 and
T1 neurons, respectively; the second layers have
S2 and T2 nodes, and the third layers have S3 and
T3 nodes, respectively. A joint layer with J nodes
connects the source and the target encoders.
Here, the number of nodes in the individual lay-
ers are the most important parameters. The more
234
nodes a layer has, the more information can be
conveyed through it, but the harder the training:
the amount of data needed for training and thus
the computation time required is exponential in the
size of the network (Ackley et al, 1985).
To transliterate a source word, it is first encoded
as a DF -dimensional binary vector SF (cf. Sec-
tion 2.1) and then fed into the first layer of the
source encoder. The S1-dimensional output vec-
tor OS1 of the first layer is computed as
OS1 ? 1/ exp (1 + wS1SF + bS1) , (1)
where wS1 is a S1 ?DF -dimensional weight ma-
trix and bS1 is an S1-dimensional bias vector.
The output of each layer is used as input to the
next layer as follows:
OS2 ? 1/ exp (1 + wS2OS1 + bS2) , (2)
OS3 ? 1/ exp (1 + wS3OS2 + bS3) . (3)
After the source encoder has been traversed, the
joint layer is reached which processes the data
twice: once using the input from the source en-
coder to get a state of the hidden neurons OSJ and
then to infer an output state OJT as input to the
topmost level of the output encoder
OSJ ? 1/ exp (1 + wSJOS3 + bSJ) , (4)
OJT ? 1/ exp (1 + wJTOSJ + bJT ) . (5)
This output vector is decoded by traversing down-
wards through the output encoder:
OT3 ? 1/ exp (1 + wT3OJT + bT3) , (6)
OT2 ? 1/ exp (1 + wT2OT3 + bT2) , (7)
OT1 ? wT1OT2 + bT1, (8)
where OT1 is a vector encoding a word in the tar-
get language.
Note that this model is intrinsically bidirec-
tional since the individual RBMs are bidirectional
models and thus it is possible to transliterate from
source to target and vice versa.
2.1 Source and Target Encoding
A problem with DBNs and transliteration is the
data representation. The input and output data are
commonly sequences of varying length but a DBN
expects input data of constant length. To repre-
sent a source or target language word, it is con-
verted into a sparse binary vector of dimensional-
ity DF = |F | ? J or DE = |E| ? I , respectively,
where |F | and |E| are the sizes of the alphabets
and I and J are the lengths of the longest words.
If a word is shorter than this, a padding letter w0
is used to fill the spaces. This encoding is depicted
in the bottom part of Figure 1.
Since the output vector of the DBN is not bi-
nary, we infer the maximum a posterior hypothe-
sis by selecting the letter with the highest output
value for each position.
2.2 Training Method
For the training, we follow the method proposed
in (Hinton et al, 2006). To find a good starting
point for backpropagation on the whole network,
each of the RBMs is trained individually. First, we
learn the generative encoders for the source and
target words, i.e. the weights wS1 and wT1, respec-
tively. Therefore, each of the layers is trained as a
restricted Boltzmann machine, such that it learns
to generate the input vectors with high probability,
i.e. the weights are learned such that the data val-
ues have low values of the trained cost function.
After learning a layer, the activity vectors of the
hidden units, as obtained from the real training
data, are used as input data for the next layer. This
can be repeated to learn as many hidden layers as
desired. After learning multiple hidden layers in
this way, the whole network can be viewed as a
single, multi-layer generative model and each ad-
ditional hidden layer improves a lower bound on
the probability that the multi-layer model would
generate the training data (Hinton et al, 2006).
For each language, the output of the first layer is
used as input to learn the weights of the next lay-
ers wS2 and wT2. The same procedure is repeated
to learn wS3 and wT3. Note that so far no con-
nection between the individual letters in the two
alphabets is created but each encoder only learns
feature functions to represent the space of possi-
ble source and target words. Then, the weights
for the joined layer are learned using concatenated
outputs of the top layers of the source and target
encoders to find an initial set of weights wSJ and
wJT .
After each of the layers has been trained in-
dividually, backpropagation is performed on the
whole network to tune the weights and to learn the
connections between both languages. We use the
average squared error over the output vectors be-
tween reference and inferred words as the training
criterion. For the training, we split the training
235
data into batches of 100 randomly selected words
and allow for 10 training iterations of the individ-
ual layers and up to 200 training iterations for the
backpropagation. Currently, we only optimise the
parameters for the source to target direction and
thus do not retain the bidirectionality1.
Thus, the whole training procedure consists of
4 phases. First, an autoencoder for the source
words is learnt. Second, an autoencoder for
the target words is learnt. Third, these autoen-
coders are connected by a top connecting layer,
and finally backpropagation is performed over the
whole network for fine-tuning of the weights.
2.3 Creation of n-Best Lists
N -best lists are a common means for combination
of several systems in natural language processing
and for rescoring. In this section, we describe how
a set of hypotheses can be created for a given in-
put. Although these hypotheses are not n-best lists
because they have not been obtained from a search
process, they can be used similarly and can bet-
ter be compared to randomly sampled ?good? hy-
potheses from a full word-graph.
Since the values of the nodes in the individ-
ual layers are probabilities for this particular node
to be activated, it is possible to sample a set of
states from the distribution for the individual lay-
ers, which is called Gibbs sampling (Geman and
Geman, 1984). This sampling can be used to cre-
ate several hypotheses for a given input sentence,
and this set of hypotheses can be used similar to
an n-best list.
The layer in which the Gibbs sampling is done
can in principle be chosen arbitrarily. However,
we believe it is natural to sample in either the first
layer, the joint layer, or the last layer. Sampling in
the first layer leads to different features traversing
the full network. Sampling in the joint layer only
affects the generation of the target sentence, and
sampling in the last layer is equal to directly sam-
pling from the distribution of target hypotheses.
Conventional Gibbs sampling has a very strong
impact on the outcome of the network because the
smoothness of the distributions and the encoding
of similar matches is entirely lost. Therefore, we
use a weak variant of Gibbs sampling. Instead of
replacing the states? probabilities with fully dis-
cretely sampled states, we keep the probabilities
1Note that it is easily possible to extend the backpropaga-
tion to include both directions, but to keep the computational
demands lower we decided to start with only one direction.
and add a fraction of a sampled state, effectively
modifying the probabilities to give a slightly bet-
ter score to the last sampled state. Let p be the
D-dimensional vector of probabilities for D nodes
in an RBM to be on. Normal Gibbs sampling
would sample a D-dimensional vector S contain-
ing a state for each node from this distribution.
Instead of replacing the vector p with S, we use
p? ? p + ?S, leading to smoother changes than
conventional Gibbs sampling. This process can
easily be repeated to obtain multiple hypotheses.
3 Experimental Evaluation
In this section we present experimental results for
an Arabic-English transliteration task. For evalu-
ation we use the character error rate (CER) which
is the commonly used word error rate (WER) on
character level.
We use a corpus of 10,084 personal names in
Arabic and their transliterated English ASCII rep-
resentation (LDC corpus LDC2005G02). The
Arabic names are written in the usual way, i.e.
lacking vowels and diacritics. 1,000 names were
randomly sampled for development and evalua-
tion, respectively (Freitag and Khadivi, 2007).
The vocabulary of the source language is 33 and
the target language has 30 different characters
(including the padding character). The longest
word on both sides consists of 14 characters,
thus the feature vector on the source side is 462-
dimensional and the feature vector on the target
side is 420-dimensional.
3.1 Network Structure
First, we evaluate how the structure of the network
should be chosen. For these experiments, we fixed
the numbers of layers and the size of the bottom
layers in the target and source encoder and evalu-
ate different network structures and the size of the
joint layer.
The experiments we performed are described in
Table 1. The top part of the table gives the results
for different network structures. We compare net-
works with increasing layer sizes, identical layer
sizes, and decreasing layer sizes. It can be seen
that decreasing layer sizes leads to the best results.
In these experiments, we choose the number of
nodes in the joint layer to be three times as large
as the topmost encoder layers.
In the bottom part, we kept most of the network
structure fixed and only vary the number of nodes
236
Table 1: Transliteration experiments using differ-
ent network structures.
number of nodes CER [%]
S1,T1 S2,T2 S3,T3 J train dev eval
400 500 600 1800 0.3 27.2 28.1
400 400 400 1200 0.7 26.1 25.2
400 350 300 900 1.8 25.1 24.3
400 350 300 1000 1.7 24.8 24.0
400 350 300 1500 1.3 24.1 22.7
400 350 300 2000 0.2 24.2 23.5
in the joint layer. Here, a small number of nodes
leads to suboptimal performance and a very high
number of nodes leads to overfitting which can be
seen in nearly perfect performance on the training
data and an increasing CER on the development
and eval data.
3.2 Network Size
Next, we evaluate systems with different numbers
of nodes. Therefore, we start from the best param-
eters (400-350-300-1500) from the previous sec-
tion and scale the number of nodes in the individ-
ual layers by a certain factor, i.e. factor 1.5 leads
to (600-525-450-2250).
In Figure 2 and Table 2, the results from the
experimental evaluation on the transliteration task
are given. The network size denotes the number
of nodes in the bottom layers of the source and the
target encoder (i.e. S1 and T1) and the other layers
are chosen according to the results from the exper-
iments presented in the previous section.
The results show that small networks perform
badly, the optimal performance is reached with
medium sized networks of 400-600 nodes in the
bottom layers, and larger networks perform worse,
which is probably due to overfitting.
For comparison, we give results for a state-of-
the-art phrase-based MT system applied on the
character level with default system parameters (la-
belled as ?PBT untuned?), and the same system,
where all scaling factors were tuned on dev data
(labelled as ?PBT tuned?). The tuned phrase-based
MT system clearly outperforms our approach.
Additionally, we perform an experiment with
a standard multi-layer perceptron. Therefore, we
choose the network structure with 400-350-300-
1500 nodes, initialised these randomly and trained
the entire network with backpropagation training.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 50  200  300  400  500  600  1000
CE
R [%
]
network size
traindevtest
Figure 2: Results for the Arabic-English translit-
eration task depending on the network size.
The results (line ?MLP-400? in Table 2) of this ex-
periment are far worse than any of the other re-
sults, which shows that, apart from the convenient
theoretical interpretation, the creation of the DBN
as described is a suitable method to train the sys-
tem. The reason for the large difference is likely
the bad initialisation of the network and the fact
that the backpropagation algorithm gets stuck in a
local optimum at this point.
3.3 Reordering capabilities
Although reordering is not an issue in transliter-
ation, the proposed model has certain properties
which we investigated and where interesting prop-
erties can be observed.
To investigate the performance under adverse
reordering conditions, we also perform an exper-
iment with reversed ordering of the target letters
(i.e. a word w = c1, c2, . . . , cJ is now written
cJ , cJ?1, . . . , c1). Since the DBN is fully sym-
metric, i.e. each input node is connected with each
output node in the same way and vice versa, the
DBN result is not changed except for some minor
numerical differences due to random initialisation.
Indeed, the DBN obtained is nearly identical ex-
cept for a changed ordering of the weights in the
joint layer, and if desired it is possible to construct
a DBN for reverse-order target language from a
fully trained DBN by permuting the weights.
On the same setup an experiment with our
phrase-based decoder has been performed and
here the performance is strongly decreased (bot-
tom line of Table 2). The phrase-based MT sys-
tem for this experiment used a reordering with
IBM block-limit constraints with distortion lim-
its and all default parameters were reasonably
tuned. We observed that the position-independent
237
Table 2: Results for the Arabic-English translit-
eration task depending on the network size and a
comparison with state of the art results using con-
ventional phrase-based machine translation tech-
niques
network CER [%]
size train dev eval
50 35.8 43.7 43.6
100 26.4 36.3 35.8
200 5.8 25.2 24.3
300 3.9 24.3 24.4
400 1.3 24.1 22.7
500 1.2 22.9 22.8
600 1.0 24.1 22.6
1000 0.2 26.6 24.4
MLP-400 22.0 64.1 63.2
untuned PBT 4.9 23.3 23.6
tuned PBT 2.2 12.9 13.3
(Freitag and Khadivi, 2007) n/a 11.1 11.1
reversed task: PBT 13.0 35.2 35.7
error rate of the phrase-based MT system is hardly
changed which also underlines that, in principle,
the phrase-based MT system is currently better but
that under adverse reordering conditions the DBN
system has some advantages.
3.4 N-Best Lists
As described above, different possibilities to cre-
ate n-best lists exists. Starting from the system
with 400-350-300-1500 nodes, we evaluate the
creation of n-best lists in the first source layer, the
joint layer, and the last target layer. Therefore,
we create n best lists with up to 10 hypotheses
(sometimes, we have less due to duplicates after
sampling, on the average we have 8.3 hypotheses
per sequence), and evaluate the oracle error rate.
In Table 3 it can be observed that sampling in the
first layer leads to the best oracle error rates. The
baseline performance (first best) for this system is
24.1% CER on the development data, and 22.7%
CER on the eval data, which can be improved by
nearly 10% absolute using the oracle from a 10-
best list.
3.5 Rescoring
Using the n-best list sampled in the first source
layer, we also perform rescoring experiments.
Therefore, we rescore the transliteration hypothe-
Table 3: Oracle character error rates on 10-best
lists.
sampling layer oracle CER [%]
dev eval
S1 15.8 14.8
joint layer 17.5 16.4
T1 18.7 18.2
CER [%]
System dev eval
DBN w/o rescoring 24.1 22.7
w/ rescoring 21.3 20.1
Table 4: Results from the rescoring experiments
and fusion with the phrase-based MT system.
ses (after truncating the padding letters w0) with
additional models, which are commonly used in
MT, and which we have trained on the training
data:
? IBM model 1 lexical probabilities modelling
the probability for a target sequence given a
source sequence
hIBM1(f
J
1 , e
I
1)=? log
?
?
1
(I + 1)J
J?
j=1
I?
i=0
p(fj |ei)
?
?
? m-gram language model over the letter se-
quences
hLM(e
I
1) = ? log
I?
i=1
p(ei|e
i?1
i?m+1),
with m being the size of the m-gram, we
choose m = 9.
? sequence length model (commonly referred
to as word penalty).
Then, these models are fused in a log-linear model
(Och and Ney, 2002), and we tune the model scal-
ing factors discriminatively on the development n-
best list using the downhill simplex algorithm. Re-
sults from the rescoring experiments are given in
Table 4.
The performance of the DBN system is im-
proved on the dev data from 24.1% to 21.3% CER
and on the eval data from 22.7% to 20.1% CER.
238
3.6 Application Within a System
Combination Framework
Although being clearly outperformed by the
phrase-based MT system, we applied the translit-
eration candidates generated by the DBN ap-
proach within a system combination framework.
Motivated by the fact that the DBN approach
differs decisively from the other statistical ap-
proaches we applied to the machine transliteration
task, we wanted to investigate the potential ben-
efit of the diverse nature of the DBN transliter-
ations. Taking the transliteration candidates ob-
tained from another study which was intended to
perform a comparison of various statistical ap-
proaches to the transliteration task, we performed
the system combination as is customary in speech
recognition, i.e. following the Recognizer Output
Voting Error Reduction (ROVER) approach (Fis-
cus, 1997).
The following methods were investigated:
(Monotone) Phrase-based MT on character level:
A state-of-the-art phrase-based SMT system
(Zens and Ney, 2004) was used for name
transliteration, i.e. translation of characters
instead of words. No reordering model
was employed due to the monotonicity
of the transliteration task, and the model
scaling factors were tuned on maximum
transliteration accuracy.
Data-driven grapheme-to-phoneme conversion:
In Grapheme-to-Phoneme conversion (G2P),
or phonetic transcription, we seek the most
likely pronunciation (phoneme sequence)
for a given orthographic form (sequence of
letters). Then, a grapheme-phoneme joint
multi-gram, or graphone for short, is a pair
of a letter sequence and a phoneme sequence
of possibly different length (Bisani and Ney,
2008). The model training is done in two
steps: First, maximum likelihood is used to
infer the graphones. Second, the input is
segmented into a stream of graphones and
absolute discounting with leaving-one-out
is applied to estimate the actual M -gram
model. Interpreting the characters of the
English target names as phonemes, we used
the G2P toolkit of (Bisani and Ney, 2008) to
transliterate the Arabic names.
Position-wise maximum entropy models / CRFs:
The segmentation as provided by the G2P
model is used and ?null words? are inserted
such that the transliteration task can be
interpreted as a classical tagging task (e.g.
POS, conceptual tagging, etc.). This means
that we seek for a one-to-one mapping and
define feature functions to model the pos-
terior probability. Maximum entropy (ME)
models are defined position-wise, whereas
conditional random fields (CRFs) consider
full sequences. Both models were trained
according to the maximum class posterior
criterion. We used an ME tagger (Bender et
al., 2003) and the freely available CRF++
toolkit.2
Results for each of the individual systems and
different combinations are given in Table 5. As
expected, the DBN transliterations cannot keep up
with the other approaches. The additional models
(G2P, CRF and ME) perform slightly better than
the PBT method. If we look at combinations of
systems without the DBN approach, we observe
only marginal improvements of around 0.1-0.2%
CER. Interestingly, a combination of all 4 models
(PBT, G2P, ME, CRF) works as good as individual
3-way combinations (the same 11.9% on dev are
obtained). This can be interpreted as a potential
?similarity? of the approaches. Adding e.g. ME to
a combination of PBT, G2P and CRF does not im-
prove results because the transliteration hypothe-
ses are too similar. If we simply put together all
5 systems including DBN with equal weights, we
have a similar trend. Since all systems are equally
weighted and at least 3 of the systems are similar
in individual performance (G2P, ME, CRF have all
around 12% CER on the tested data sets), the DBN
approach does not get a large impact on overall
performance.
If we drop similar systems and tune for 3-way
combinations, we observe a large reduction in
CER if DBN comes into play. Compared to the
best individual system of 12% CER, we now ar-
rive at a CER of 10.9% for a combination of PBT,
CRF and DBN which is significantly better than
each of the individual methods. Our interpreta-
tion of this is that the DBN system has different
hypotheses compared to all other systems and that
the hypotheses from the other systems are too sim-
ilar to be apt for combination. So, although DBN
is much worse than the other approaches, it obvi-
ously helps in the system combination. Using the
rescored variant of the DBN transliterations from
2http://crfpp.sourceforge.net/
239
CER [%]
System dev eval
DBN 24.1 22.7
PBT 12.9 13.3
G2P 12.2 12.1
ME 12.3 12.4
CRF 12.0 12.0
ROVER
best setting w/o DBN 11.9 11.8
5-way equal weights 11.7 11.9
best setting w/ DBN 10.9 10.9
Table 5: Results from the individual methods in-
vestigated versus ROVER combination.
Section 3.5, performance is similar to the one ob-
tained for the DBN baseline.
4 Discussion and Conclusion
We have presented a novel method for machine
transliteration based on DBNs, which despite not
having competitive results can be an important ad-
ditional cue for system combination setups. The
DBN model has some immediate advantages: the
model is in principle fully bidirectional and is
based on sound and valid theories from machine
learning. Instead of common techniques which
are based on finite-state machines or phrase-based
machine translation, the proposed system does not
rely on word alignments and beam-search decod-
ing and has interesting properties regarding the re-
ordering of sequences. We have experimentally
evaluated the network structure and size, reorder-
ing capabilities, the creation of multiple hypothe-
ses, and rescoring and combination with other
transliteration approaches. It was shown that, al-
beit the approach cannot compete with the cur-
rent state of the art, deep belief networks might
be a learning framework with some potential for
transliteration. It was also shown that the pro-
posed method is suited for combination with dif-
ferent state-of-the-art systems and that improve-
ments over the single models can be obtained
in a ROVER-like setting. Furthermore, adding
DBN-based transliterations, although individually
far behind the other approaches, significantly im-
proves the overall results by 1% absolute.
Outlook
In the future we plan to investigate several details
of the proposed model: we will exploit the inher-
ent bidirectionality, further investigate the struc-
ture of the model, such as the number of layers
and the numbers of nodes in the individual lay-
ers. Also, it is important to improve the efficiency
of our implementation to allow for working on
larger datasets and obtain more competitive re-
sults. Furthermore, we are planning to investigate
convolutional input layers for transliteration and
use a translation approach analogous to the one
proposed by Collobert and Weston (2008) in or-
der to allow for the incorporation of reorderings,
language models, and to be able to work on larger
tasks.
Acknowledgement. We would like to thank Ge-
offrey Hinton for providing the Matlab Code ac-
companying (Hinton and Salakhutdinow, 2006).
References
D. Ackley, G. Hinton, and T. Sejnowski. 1985. A
learning algorithm for Boltzmann machines. Cog-
nitive Science, 9(1):147?169.
Y. Al-Onaizan and K. Knight. 2002. Machine
transliteration of names in Arabic text. In ACL
2002 Workshop on Computationaal Approaches to
Semitic Languages.
M. Asuncio?n Castan?o, F. Casacuberta, and E. Vidal.
1997. Machine translation using neural networks
and finite-state models. In Theoretical and Method-
ological Issues in Machine Translation (TMI), pages
160?167, Santa Fe, NM, USA, July.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Sta-
tistical machine translation through global lexical
selection and sentence reconstruction. In Annual
Meeting of the Association for Computational Lin-
gustic (ACL), pages 152?159, Prague, Czech Repub-
lic.
O. Bender, F. J. Och, and H. Ney. 2003. Maxi-
mum entropy models for named entity recognition.
In Proc. 7th Conf. on Computational Natural Lan-
guage Learning (CoNLL), pages 148?151, Edmon-
ton, Canada, May.
M. Bisani and H. Ney. 2008. Joint-sequence models
for grapheme-to-phoneme conversion. Speech Com-
munication, 50(5):434?451, May.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, Helsinki, Finn-
land, July.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 347?354, Santa Barbara, CA, USA, Decem-
ber.
240
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238?247, Prague, Czech
Republic, June.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transaction on Pattern Analysis and
Machine Intelligence, 6(6):721?741, November.
G. Hinton and R. R. Salakhutdinow. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313:504?507, July.
G. Hinton, S. Osindero, and Y.-W. Teh. 2006. A
fast learning algorithm for deep belief nets. Neural
Computation, 18:1527?1554.
F. Huang, S. Vogel, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In HLT-NAACL.
K. Knight and J. Graehl. 1998. Machine translitera-
tion. Computational Linguistics, 24(2).
A. Mnih and G. Hinton. 2007. Three new graphical
models for statistical language modelling. In ICML
?07: International Conference on Machine Learn-
ing, pages 641?648, New York, NY, USA. ACM.
F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Annual Meeting of the As-
soc. for Computational Linguistics, pages 295?302,
Philadelphia, PA, USA, July.
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics, pages 632?639,
Prague, Czech Republic, June.
A. Torralba, R. Fergus, and Y. Weiss. 2008. Small
codes and large image databases for recognition. In
IEEE Conference on Computer Vision and Pattern
Recognition, Anchorage, AK, USA, June.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: HLT-NAACL
2004, pages 257?264, Boston, MA, May.
241
Proceedings of NAACL HLT 2007, Companion Volume, pages 65?68,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
??ROVER: Improving System Combination with Classification
D. Hillard?, B. Hoffmeister?, M. Ostendorf?, R. Schlu?ter?, H. Ney?
?SSLI, Electrical Engineering Dept., University of Washington, Seattle, WA
{hillard,mo}@ee.washington.edu
?Informatik 6, Computer Science Dept., RWTH Aachen University, Aachen, Germany
{hoffmeister,schlueter,ney}@cs.rwth-aachen.de
Abstract
We present an improved system combination
technique, ??ROVER. Our approach obtains sig-
nificant improvements over ROVER, and is
consistently better across varying numbers of
component systems. A classifier is trained on
features from the system lattices, and selects
the final word hypothesis by learning cues to
choose the system that is most likely to be
correct at each word location. This approach
achieves the best result published to date on
the TC-STAR 2006 English speech recognition
evaluation set.
1 Introduction
State-of-the-art automatic speech recognition (ASR) sys-
tems today usually include multiple contrasting systems,
which are ultimately combined to produce the final hy-
pothesis. There is consensus that improvements from
combination are usually best when systems are suffi-
ciently different, but there is uncertainty about which sys-
tem combination method performs the best. In addition,
the success of commonly used combination techniques
varies depending on the number of systems that are com-
bined (Hoffmeister et al, 2007). In this work, we develop
a system combination method that outperforms all previ-
ously known techniques and is also robust to the number
of component systems. The relative improvements over
ROVER are particularly large for combination when only
using two systems.
The aim of system combination for ASR is to mini-
mize the expected word error rate (WER) given multiple
system outputs, which are ideally annotated with word
confidence information. The most widely used system
combination approach to date is ROVER (Fiscus, 1997).
It is a simple voting mechanism over just the top hy-
pothesis from each component system. Two alternatives
that incorporate information about multiple hypotheses
and leverage word posterior probabilities are confusion
network (CN) combination (Mangu et al, 2000; Ever-
mann and Woodland, 2000) and minimum Time Frame
Word Error (min-fWER) decoding (Hoffmeister et al,
2006), discussed further in the next section. Previous
work found that among ROVER, CN combination, and
min-fWER combination, no one method was consistently
superior across varying numbers and types of systems
(Hoffmeister et al, 2007).
The main contribution of this work is to develop an
approach that always outperforms other possible system
combination methods. We train a classifier to learn which
system should be selected for each output word, using
features that describe the characteristics of the compo-
nent systems. ROVER alignments on the 1-best hypothe-
ses are used for decoding, but many of the features are
derived from the system lattices. The classifier learns a
selection strategy (i.e. a decision function) from a devel-
opment set and then is able to make better selections on
the evaluation data then the current 1-best or lattice-based
system combination approaches.
Next, Section 2 describes previous work in system
combination techniques. Section 3 describes our ap-
proach, and Section 4 provides experiments and results.
Finally, we summarize the approach and findings in Sec-
tion 5.
2 Previous Work
Previous work in speech recognition system combination
has produced significant improvements over the results
possible with just a single system. The most popular, and
often best performing method is ROVER (Fiscus, 1997),
which selects the word that the most systems agree on
at a particular location (majority voting). An extended
version of ROVER also weights system votes by the word
confidence produced by the system (confidence voting).
Further improvements have been achieved by includ-
ing multiple system alternatives, with methods such as
Confusion Network Combination (CNC) (Evermann and
Woodland, 2000), or N-Best ROVER (Stolcke et al,
2000), which is a special case of CNC. Alternatively, the
combination can be performed at the frame level (min-
fWER) (Hoffmeister et al, 2006). Recent work found
that the best system combination method depended on the
number of systems being combined (Hoffmeister et al,
2007). When only two systems are available, approaches
considering multiple alternatives per system were bet-
65
ter, but as the number of systems increased the standard
ROVER with confidence scores was more robust and
sometimes even better than CNC or min-fWER combi-
nation.
Another approach (Zhang and Rudnicky, 2006) used
two stages of neural networks to select a system at each
word, with features that capture word frequency, posteri-
ors at the frame, word, and utterance level, LM back-off
mode, and system accuracy. They obtained consistent but
small improvements over ROVER: between 0.7 and 1.7%
relative gains for systems with about 30% WER.
3 Approach
We develop a system that uses the ROVER alignment but
learns to consistently make better decisions than those
of standard ROVER. We call the new system ??ROVER,
where the ?? stands for improved results, and/or intelligent
decisions. The following sections discuss the compo-
nents of our approach. First, we emulate the approach
of ROVER in our lattice preprocessing and system align-
ment. We then introduce new methods to extract hypoth-
esis features and train a classifier that selects the best
system at each slot in the alignment.
3.1 Lattice Preparation
Our experiments use lattice sets from four different sites.
Naturally, these lattice sets differ in their vocabulary,
segmentation, and density. A compatible vocabulary is
essential for good combination performance. The main
problems are related to contractions, e.g. ?you?ve? and
?you have?, and the alternatives in writing foreign names,
e.g. ?Schro?der? and ?Schroder?. In ASR this problem is
well-known and is addressed in scoring by using map-
pings that allow alternative forms of the same word.
Such a mapping is provided within the TC-STAR Eval-
uation Campaign and we used it to normalize the lat-
tices. In case of multiple alternative forms we used only
the most frequent one. Allowing multiple parallel alter-
natives would have distorted the posterior probabilities
derived from the lattice. Furthermore, we allowed only
one-to-one or one-to-many mappings. In the latter case
we distributed the time of the lattice arc according to the
character lengths of the target words.
In order to create comparable posterior probabilities
over the lattice sets we pruned them to equal average
density. The least dense lattice set defined the target
density: around 25 for the development and around 30
for the evaluation set.
Finally, we unified the segmentation by concatenat-
ing the lattices recording-wise. The concatenation was
complicated by segmentations with overlapping regions,
but our final concatenated lattices scored equally to the
original lattice sets. The unified segmentation is needed
for lattice-based system combination methods like frame-
based combination.
3.2 System Alignments
In this work we decided to use the ROVER alignment as
the basis for our system combination approach. At first
glance the search space used by ROVER is very limited
because only the first-best hypothesis from each compo-
nent system is used. But the oracle error rate is often very
low, normally less than half of the best system?s error rate.
The ROVER alignment can be interpreted as a confu-
sion network with an equal number of arcs in each slot.
The number of arcs per slot equals the number of compo-
nent systems and thus makes the training and application
of a classifier straightforward.
For the production of the alignments we use a stan-
dard, dynamic programming-based matching algorithm
that minimizes the global cost between two hypothesis.
The local cost function is based on the time overlap of
two words and is identical to the one used by the ROVER
tool. We also did experiments with alternative local cost
functions based on word equalities, but could not outper-
form the simple, time overlap-based distance function.
3.3 Hypothesis Features
We generate a cohort of features for each slot in the
alignment, which is then used as input to train the classi-
fier. The features incorporate knowledge about the scores
from the original systems, as well as comparisons among
each of the systems. The following paragraphs enumerate
the six classes of feature types used in our experiments
(with their names rendered in italics).
The primary, and most important feature class covers
the basic set of features which indicate string matches
among the top hypotheses from each system. In addition,
we include the systems? frame-based word confidence.
These features are all the information available to the
standard ROVER with confidences voting.
An additional class of features provides extended con-
fidence information about each system?s hypothesis. This
feature class includes the confusion network (CN) word
confidence, CN slot entropy, and the number of alter-
natives in the CN slot. The raw language model and
acoustic scores are also available. In addition, it in-
cludes a frame-based confidence that is computed from
only the acoustic model, and a frame-based confidence
that is computed from only the language model score.
Frame-based confidences are calculated from the lattices
according to (Wessel et al, 1998); the CN-algorithm is
an extension of (Xue and Zhao, 2005).
The next class of features describes durational aspects
of the top hypothesis for each system, including: charac-
ter length, frame duration, frames per character, and if the
word is the empty or null word. A feature that normalizes
the frames per character by the average over a window
of ten words is also generated. Here we use characters
as a proxy for phones, because phone information is not
available from all component systems.
We also identify the system dependent top error words
for the development set, as well as the words that occur
to the left and right of the system errors. We encode this
information by indicating if a system word is on the list
of top ten errors or the top one hundred list, and likewise
if the left or right system context word is found in their
corresponding lists.
In order to provide comparisons across systems, we
compute the character distance (the cost of aligning the
words at the character level) between the system words
66
and provide that as a feature. In addition, we include the
confidence of a system word as computed by the frame-
wise posteriors of each of the other systems. This allows
each of the other systems to ?score? the hypothesis of
a system in question. These cross-system confidences
could also act as an indicator for when one system?s hy-
pothesis is an OOV-word for another system. We also
compute the standard, confidence-based ROVER hypoth-
esis at each slot, and indicate whether or not a system
agrees with ROVER?s decision.
The last set of features is computed relative to the
combined min-fWER decoding. A confidence for each
system word is calculated from the combined frame-wise
posteriors of all component systems. The final feature
indicates whether each system word agrees with the com-
bined systems? min-fWER hypothesis.
3.4 Classifier
After producing a set of features to characterize the sys-
tems, we train a classifier with these features that will
decide which system will propose the final hypothesis at
each slot in the multiple alignment. The target classes
include one for each system and a null class (which is
selected when none of the system outputs are chosen, i.e.
a system insertion).
The training data begins with the multiple alignment
of the hypothesis systems, which is then aligned to the
reference words. The learning target for each slot is the
set of systems which match the reference word, or the
null class if no systems match the reference word. Only
slots where there is disagreement between the systems?
1-best hypotheses are included in training and testing.
The classifier for our work is Boostexter (Schapire and
Singer, 2000) using real Adaboost.MH with logistic loss
(which outperformed exponential loss in preliminary ex-
periments). Boostexter trains a series of weak classifiers
(tree stumps), while also updating the weights of each
training sample such that examples that are harder to
classify receive more weight. The weak classifiers are
then combined with the weights learned in training to
predict the most likely class in testing. The main dimen-
sions for model tuning are feature selection and number
of iterations, which are selected on the development set
as described in the next section.
4 Experiments
We first perform experiments using cross-validation on
the development set to determine the impact of different
feature classes, and to select the optimal number of iter-
ations for Boostexter training. We then apply the models
to the evaluation set.
4.1 Experimental setup
In our experiments we combine lattice sets for the English
task of the TC-STAR 2006 Evaluation Campaign from
four sites. The TC-STAR project partners kindly pro-
vided RWTH their development and evaluation lattices.
Systems and lattice sets are described in (Hoffmeister et
al., 2007).
Table 1 summarizes the best results achieved on the
single lattice sets. The latter columns show the results of
Viterbi min-fWER CN
dev eval dev eval dev eval
1 10.5 9.0 10.3 8.6 10.4 8.6
2 11.4 9.0 11.4 9.5 11.6 9.1
3 12.8 10.4 12.5 10.4 12.6 10.2
4 13.9 11.9 13.9 11.8 13.9 11.8
Table 1: WER[%] results for single systems.
CN and min-fWER based posterior decoding (Mangu et
al., 2000; Wessel et al, 2001).
4.2 Feature analysis on development data
We evaluate the various feature classes from Section 3.3
on the development set with a cross validation testing
strategy. The results in Tables 2 and 3 are generated
with ten-fold cross validation, which maintains a clean
separation of training and testing data. The total number
of training samples (alignment slots where there is system
disagreement) is about 3,700 for the 2 system case, 5,500
for the 3 system case, and 6,800 for the 4 system case.
The WER results for different feature conditions on the
development set are presented in Table 2. The typical
ROVER with word confidences is provided in the first
row for comparison, and the remainder of the rows con-
tain the results for various configurations of features that
are made available to the classifier.
The basic features are just those that encode the same
information as ROVER, but the classifier is still able to
learn better decisions than ROVER with only these fea-
tures. Each of the following rows provides the results for
adding a single feature class to the basic features, so that
the impact of each type can be evaluated.
The last two rows contain combinations of feature
classes. First, the best three classes are added, and then
all features. Using just the best three classes achieves
almost the best results, but a small improvement is gained
when all features are added. The number of iterations in
training is also optimized on the development set by se-
lecting the number with the lowest average classification
error across the ten splits of the training data.
Features 2 System 3 System 4 System
ROVER 10.2% 8.8% 9.0%
basic 9.4% 8.6% 8.5%
+confidences 9.3% 8.7% 8.4%
+durational 9.2% 8.6% 8.4%
+top error 9.0% 8.5% 8.4%
+comparisons 8.9% 8.6% 8.4%
+min-fWER 8.5% 8.5% 8.4%
+top+cmp+fWER 8.3% 8.3% 8.2%
all features 8.3% 8.2% 8.2%
Table 2: WER results for development data with different
feature classes.
67
 8
 8.5
 9
 9.5
 10
 10.5
 11
4321
[%
] W
ER
ROVER(maj.)
ROVER(conf.)
min-fWER
iROVER
2 System 3 System 4 System
ROVER (maj.) 10.8% 9.1% 9.1%
ROVER (conf.) 10.1% 8.8% 9.0%
min-fWER 9.6% 9.2 % 8.9 %
??ROVER 8.3% 8.2% 8.2%
oracle 6.5% 5.4% 4.7%
Table 3: WER[%] results for development data with
manual segmentation, and using cross-validation for
??ROVER.
4.3 Results on evaluation data
After analyzing the features and selecting the optimal
number of training iterations on the development data,
we train a final model on the full development set and
then apply it to the evaluation set. In all cases our clas-
sifier achieves a lower WER than ROVER (statistically
significant by NIST matched pairs test). Table 3 and Ta-
ble 4 present a comparison of the ROVER with majority
voting, confidence voting, frame-based combination, and
our improved ROVER (??ROVER).
5 Conclusions
In summary, we develop ??ROVER, a method for sys-
tem combination that outperforms ROVER consistently
across varying numbers of component systems. The rela-
tive improvement compared to ROVER is especially large
for the case of combining two systems (14.5% on the
evaluation set). The relative improvements are larger than
any we know of to date, and the four system case achieves
the best published result on the TC-STAR English evalu-
ation set. The classifier requires relatively little training
data and utilizes features easily available from system
lattices.
Future work will investigate additional classifiers, clas-
sifier combination, and expanded training data. We are
also interested in applying a language model to decode
an alignment network that has been scored with our clas-
sifier.
References
G. Evermann and P. Woodland. 2000. Posterior probability
decoding, confidence estimation and system combination. In
NIST Speech Transcription Workshop.
 6.5
 7
 7.5
 8
 8.5
 9
 9.5
4321
[%
] W
ER
ROVER(maj.)
ROVER(conf.)
min-fWER
iROVER
2 System 3 System 4 System
ROVER(maj.) 9.0% 7.2% 7.3%
ROVER(conf.) 8.2% 7.1% 7.0%
min-fWER 7.6 % 7.4 % 7.2 %
??ROVER 7.1% 6.9% 6.7%
oracle 5.2% 4.1% 3.6%
Table 4: WER[%] results for evaluation data.
J.G. Fiscus. 1997. A post-processing system to yield reduced
word error rates: Recognizer Output Voting Error Reduction
(ROVER). In Proc. ASRU.
B. Hoffmeister, T. Klein, R. Schlu?ter, and H. Ney. 2006. Frame
based system combination and a comparison with weighted
ROVER and CNC. In Proc. ICSLP.
B. Hoffmeister, D. Hillard, S. Hahn, R. Schu?lter, M. Ostendorf,
and H. Ney. 2007. Cross-site and intra-site ASR system
combination: Comparisons on lattice and 1-best methods. In
Proc. ICASSP.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding consensus
in speech recognition: word error minimization and other
applications of confusion networks. Computer Speech and
Language, 14:373?400.
R. E. Schapire and Y. Singer. 2000. Boostexter: A boosting-
based system for text categorization. Machine Learning,
39(2/3):135?168.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, J. Zheng,
and F. Weng. 2000. The SRI March 2000 Hub-5 conver-
sational speech transcription system. In NIST Speech Tran-
scription Workshop.
F. Wessel, K. Macherey, and R. Schlu?ter. 1998. Using word
probabilities as confidence measures. In Proc. ICASSP.
F. Wessel, R. Schlu?ter, and H. Ney. 2001. Explicit word error
minimization using word hypothesis posterior probabilities.
In Proc. ICASSP, volume 1.
Jian Xue and Yunxin Zhao. 2005. Improved confusion network
algorithm and shortest path search from word lattice. In
Proc. ICASSP.
R. Zhang and A. Rudnicky. 2006. Investigations of issues
for using multiple acoustic models to improve continuous
speech recognition. In Proc. ICSLP.
68
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1377?1381,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Statistical Machine Translation with Word Class Models
Joern Wuebker, Stephan Peitz, Felix Rietig and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
Automatically clustering words from a mono-
lingual or bilingual training corpus into
classes is a widely used technique in statisti-
cal natural language processing. We present
a very simple and easy to implement method
for using these word classes to improve trans-
lation quality. It can be applied across differ-
ent machine translation paradigms and with
arbitrary types of models. We show its ef-
ficacy on a small German?English and a
larger French?German translation task with
both standard phrase-based and hierarchical
phrase-based translation systems for a com-
mon set of models. Our results show that with
word class models, the baseline can be im-
proved by up to 1.4% BLEU and 1.0% TER
on the French?German task and 0.3% BLEU
and 1.1% TER on the German?English task.
1 Introduction
Data sparsity is one of the major problems for statis-
tical learning methods in natural language process-
ing (NLP) today. Even with the huge training data
sets available in some tasks, for many phenomena
that need to be modeled only few training instances
can be observed. This is partly due to the large vo-
cabularies of natural languages. One possiblity to
reduce the sparsity for model estimation is to re-
duce the vocabulary size. By clustering the vocab-
ulary into a fixed number of word classes, it is pos-
sible to train models that are less prone to sparsity
issues. This work investigates the performance of
standard models used in statistical machine transla-
tion when they are trained on automatically learned
word classes rather than the actual word identities.
In the popular tooklit GIZA++ (Och and Ney,
2003), word classes are an essential ingredient to
model alignment probabilities with the HMM or
IBM translation models. It contains the mkcls tool
(Och, 1999), which can automatically cluster the vo-
cabulary into classes.
Using this tool, we propose to re-parameterize the
standard models used in statistical machine transla-
tion (SMT), which are usually conditioned on word
identities rather than word classes. The idea is that
this should lead to a smoother distribution, which
is more reliable due to less sparsity. Here, we fo-
cus on the phrase-based and lexical channel models
in both directions, simple count models identifying
frequency thresholds, lexicalized reordering models
and an n-gram language model. Although our re-
sults show that it is not a good idea to replace the
original models, we argue that adding them to the
log-linear feature combination can improve transla-
tion quality. They can easily be computed for dif-
ferent translation paradigms and arbitrary models.
Training and decoding is possible without or with
only little change to the code base.
Our experiments are conducted on a medium-
sized French?German task and a small
German?English task and with both phrase-
based and hierarchical phrase-based translation
decoders. By using word class models, we can
improve our respective baselines by 1.4% BLEU and
1.0% TER on the French?German task and 0.3%
BLEU and 1.1% TER on the German?English task.
Training an additional language model for trans-
1377
lation based on word classes has been proposed in
(Wuebker et al, 2012; Mediani et al, 2012; Koehn
and Hoang, 2007). In addition to the reduced spar-
sity, an advantage of the smaller vocabulary is that
longer n-gram context can be modeled efficiently.
Mathematically, our idea is equivalent to a special
case of the Factored Translation Models proposed
by Koehn and Hoang (2007). We will go into more
detail in Section 4. Also related to our work, Cherry
(2013) proposes to parameterize a hierarchical re-
ordering model with sparse features that are condi-
tioned on word classes trained with mkcls. How-
ever, the features are trained with MIRA rather than
estimated by relative frequencies.
2 Word Class Models
2.1 Standard Models
The translation model of most phrase-based and hi-
erarchical phrase-based SMT systems is parameter-
ized by two phrasal and two lexical channel models
(Koehn et al, 2003) which are estimated as relative
frequencies. Their counts are extracted heuristically
from a word aligned bilingual training corpus.
In addition to the four channel models, our base-
line contains binary count features that fire, if the
extraction count of the corresponding phrase pair is
greater or equal to a given threshold ? . We use the
thresholds ? = {2, 3, 4}.
Our phrase-based baseline contains the hierarchi-
cal reordering model (HRM) described by Galley
and Manning (2008). Similar to (Cherry et al,
2012), we apply it in both translation directions
with separate scaling factors for the three orientation
classes, leading to a total of six feature weights.
An n-gram language model (LM) is another im-
portant feature of our translation systems. The
baselines apply 4-gram LMs trained by the SRILM
toolkit (Stolcke, 2002) with interpolated modified
Kneser-Ney smoothing (Chen and Goodman, 1998).
The smaller vocabulary size allows us to efficiently
model larger context, so in addition to the 4-gram
LM, we also train a 7-gram LM based on word
classes. In contrast to an LM of the same size trained
on word identities, the increase in computational re-
sources needed for translation is negligible for the
7-gram word class LM (wcLM).
2.2 Training
By replacing the words on both source and target
side of the training data with their respective word
classes and keeping the word alignment unchanged,
all of the above models can easily be trained con-
ditioned on word classes by using the same training
procedure as usual. We end up with two separate
model files, usually in the form of large tables, one
with word identities and one with classes. Next, we
sort both tables by their word classes. By walking
through both sorted tables simultaneously, we can
then efficiently augment the standard model file with
an additonal feature (or additional features) based on
word classes. The word class LM is directly passed
on to the decoder.
2.3 Decoding
The decoder searches for the best translation given
a set of models hm(eI1, s
K
1 , f
J
1 ) by maximizing the
log-linear feature score (Och and Ney, 2004):
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
, (1)
where fJ1 = f1 . . . fJ is the source sentence, e
I
1 =
e1 . . . eI the target sentence and sK1 = s1 . . . sK the
hidden alignment or derivation.
All the above mentioned models can easily be in-
tegrated into this framework as additional features
hm. The feature weights ?m are tuned with mini-
mum error rate training (MERT) (Och, 2003).
3 Experiments
3.1 Data
Our experiments are performed on a
French?German task. In addition to some
project-internal data, we train the system on the data
provided for the WMT 2012 shared task1. Both the
dev and the test set are composed of a mixture
of broadcast news and broadcast conversations
crawled from the web and have two references.
Table 1 shows the data statistics.
To confirm our results we also run experiments
on the German?English task of the IWSLT 2012
evaluation campaign2.
1http://www.statmt.org/wmt12/
2http://hltc.cs.ust.hk/iwslt/
1378
French German
train Sentences 1.9M
Running Words 57M 50M
dev Sentences 1900
Running Words 61K 55K
test Sentences 2037
Running Words 60K 54K
Table 1: Corpus statistics for the French?German task.
The running word counts for the German side of dev and
test are averaged over both references.
3.2 Setup
In the French?German task, our baseline is a stan-
dard phrase-based system augmented with the hier-
archical reordering model (HRM) described in Sec-
tion 2.1. The language model is a 4-gram LM
trained on all German monolingual sources provided
for WMT 2012. For the class-based models, we
run mkcls on the source and target side of the
bilingual training data to cluster the vocabulary into
100 classes each. This clustering is used to train
the models described above for word classes on the
same training data as their counterparts based on
word identity. This also holds for the wcLM, which
is a 4-gram LM trained on the same data as the base-
line LM. Further, the smaller vocabulary allows us
to build an additional wcLM with a 7-gram context
length. On this task we also run additional experi-
ments with 200 and 500 classes.
On the German?English task, we evaluate our
method for both a standard phrase-based and the hi-
erarchical phrase-based baseline. Again, the phrase-
based baseline contains the HRM model. As bilin-
gual training data we use the TED talks, which we
cluster into 100 classes on both source and target
side. The 4-gram LM is trained on the TED, Eu-
roparl and news-commentary corpora. On this data
set, we directly use a 7-gram wcLM.
In all setups, the feature weights are optimized
with MERT. Results are reported in BLEU (Pap-
ineni et al, 2002) and TER (Snover et al, 2006),
confidence level computation is based on (Koehn,
2004). Our experiments are conducted with the open
source toolkit Jane (Wuebker et al, 2012; Vilar et
al., 2010).
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
-TM +wcTM 21.2 64.2 24.7 59.5
-LM +wcLM 22.2 62.9 25.9 58.9
-HRM +wcHRM 24.6 61.9 27.5 58.1
phrase-based 24.6 61.8 27.8 57.6
+ wcTM 24.7 61.4 28.1 57.1
+ wcLM 24.9 61.2 28.4 57.1
+ wcHRM 25.4? 60.9? 28.9? 56.9?
+ wcLM7 25.5? 60.7? 29.2? 56.6?
+ wcModels200 25.5? 60.8? 29.3? 56.4?
+ wcModels500 25.2? 60.8? 29.0? 56.6?
Table 2: BLEU and TER results on the French?German
task. Results marked with ? are statistically significant
with 95% confidence, results marked with ? with 90%
confidence. -X +wcX denote the systems, where the
model X in the baseline is replaced by its word class
counterpart. The 7-gram word class LM is denoted
as wcLM7. wcModelsX denotes all word class models
trained on X classes.
3.3 Results
Results for the French?German task are given in
Table 2. In a first set of experiments we replaced one
of the standard TM, LM and HRM models by the
same model based on word classes. Unsurprisingly,
this degrades performance with different levels of
severity. The strongest degradation can be seen
when replacing the TM, while replacing the HRM
only leads to a small drop in performance. However,
when the word class models are added as additional
features to the baseline, we observe improvements.
The wcTM yields 0.3% BLEU and 0.5% TER on
test. By adding the 4-gram wcLM, we get another
0.3% BLEU and the wcHRM shows further improve-
ments of 0.5% BLEU and 0.2% TER. Extending the
context length of the wcLM to 7-grams gives an ad-
ditional boost, reaching a total gain over the baseline
of 1.4% BLEU and 1.0% TER. Using 200 classes
instead of 100 seems to perform slightly better on
test, but with 500 classes, translation quality de-
grades again.
On the German?English task, the results shown
in Table 3 are similar in TER, but less pronounced
in BLEU. Here we are able to improve over the
phrase-based baseline by 0.3% BLEU and 1.1% TER
1379
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
phrase-based 30.2 49.6 28.6 51.6
+ wcTM 30.2 49.2 28.9 51.3
+ wcLM7 30.5 48.3? 29.0 50.6?
+ wcHRM 30.8 48.3? 28.9 50.5?
hiero 29.6 50.3 27.9 52.5
+ wcTM 29.8 50.3 28.1 52.3
+ wcLM7 30.0 49.8 28.2 51.7
Table 3: BLEU and TER results on the German?English
task. Results marked with ? are statistically significant
with 95% confidence, results marked with ? with 90%
confidence.
by adding the wcTM, the 7-gram wcLM and the
wcHRM. With the hierarchical decoder we gain
0.3% BLEU and 0.8% TER by adding the wcTM and
the 7-gram wcLM.
4 Equivalence to Factored Translation
Koehn and Hoang (2007) propose to integrate differ-
ent levels of annotation (e.g. morphologial analysis)
as factors into the translation process. Here, the sur-
face form of the source word is analyzed to produce
the factors, which are then translated and finally the
surface form of the target word is generated from the
target factors. Although the translations of the fac-
tors operate on the same phrase segmentation, they
are assumed to be independent. In practice this is
done by phrase expansion, which generates a joint
phrase table as the cross product from the phrase ta-
bles of the individual factors.
In contrast, in this work each word is mapped to
a single class, which means that when we have se-
lected a translation option for the surface form, the
target side on the word class level is predetermined.
Thus, no phrase expansion or generation steps are
necessary to incorporate the word class information.
The phrase table can simply be extended with addi-
tional scores, keeping the set of phrases constant.
Although the implementation is simpler, our ap-
proach is mathematically equivalent to a special
case of the factored translation framework, which is
shown in Figure 1. The generation step from target
word e to its target class c(e) assigns all probability
Input Output
word f word e
class c(f) class c(e)
analysis
translation
translation
generation   
Figure 1: The factored translation model equivalent to
our approach. The generation step assigns all probability
mass to a single event: pgen(c(e)|e) = 1.
mass to a single event:
pgen(c|e) =
{
1, if c = c(e)
0, else
(2)
5 Conclusion
We have presented a simple and very easy to im-
plement method to make use of word clusters for
improving machine translation quality. It is appli-
cable across different paradigms and for arbitrary
types of models. Depending on the model type,
it requires little or no change to the training and
decoding software. We have shown the efficacy
of this method on two translation tasks and with
both the standard phrase-based and the hierarchi-
cal phrase-based translation paradigm. It was ap-
plied to relative frequency translation probabilities,
the n-gram language model and a hierarchical re-
ordering model. In our experiments, the baseline
is improved by 1.4% BLEU and 1.0% TER on the
French?German task and by 0.3% BLEU and 1.1%
TER on the German?English task.
In future work we plan to apply our method to a
wider range of languages. Intuitively, it should be
most effective for morphologically rich languages,
which naturally have stronger sparsity problems.
Acknowledgments
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French State
agency for innovation. The research leading to these
results has also received funding from the European
Union Seventh Framework Programme (FP7/2007-
2013) under grant agreement no 287658.
1380
References
Stanley F. Chen and Joshuo Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, MA,
August.
Colin Cherry, Robert C. Moore, and Chris Quirk. 2012.
On Hierarchical Re-ordering and Permutation Parsing
for Phrase-based Decoding. In Proceedings of the 7th
Workshop on Statistical Machine Translation, WMT
?12, pages 200?209, Montral, Canada.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In The 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2013), pages 22?
31, Atlanta, Georgia, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, USA, October.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 868?876, Prague, Czech Republic, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of the 2003
Meeting of the North American chapter of the Associa-
tion for Computational Linguistics (NAACL-03), pages
127?133, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of the Conf.
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 388?395, Barcelona, Spain, July.
Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, Jan
Niehues, Eunah Cho, Teresa Herrmann, and Alex
Waibel. 2012. The kit translation systems for iwslt
2012. In Proceedings of the International Work-
shop for Spoken Language Translation (IWSLT 2012),
Hong Kong.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
F. J. Och. 1999. An efficient method for determining
bilingual word classes. In Proc. of the Ninth Conf.
of the Europ. Chapter of the Association of Compu-
tational Linguistics, pages 71?76, Bergen, Norway,
June.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf. on
Speech and Language Processing (ICSLP), volume 2,
pages 901?904, Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open source hierarchical transla-
tion, extended with reordering and lexicon models. In
ACL 2010 Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 262?270, Upp-
sala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-
sour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mumbai,
India, December.
1381
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14?25,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Translation Modeling with Bidirectional Recurrent Neural Networks
Martin Sundermeyer
1
, Tamer Alkhouli
1
, Joern Wuebker
1
, and Hermann Ney
1,2
1
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2
Spoken Language Processing Group
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
{surname}@cs.rwth-aachen.de
Abstract
This work presents two different trans-
lation models using recurrent neural net-
works. The first one is a word-based ap-
proach using word alignments. Second,
we present phrase-based translation mod-
els that are more consistent with phrase-
based decoding. Moreover, we introduce
bidirectional recurrent neural models to
the problem of machine translation, allow-
ing us to use the full source sentence in our
models, which is also of theoretical inter-
est. We demonstrate that our translation
models are capable of improving strong
baselines already including recurrent neu-
ral language models on three tasks:
IWSLT 2013 German?English, BOLT
Arabic?English and Chinese?English.
We obtain gains up to 1.6% BLEU
and 1.7% TER by rescoring 1000-best
lists.
1 Introduction
Neural network models have recently experienced
unprecedented attention in research on statistical
machine translation (SMT). Several groups have
reported strong improvements over state-of-the-art
baselines using feedforward neural network-based
language models (Schwenk et al., 2006; Vaswani
et al., 2013), as well as translation models (Le et
al., 2012; Schwenk, 2012; Devlin et al., 2014).
Different from the feedforward design, recurrent
neural networks (RNNs) have the advantage of be-
ing able to take into account an unbounded his-
tory of previous observations. In theory, this en-
ables them to model long-distance dependencies
of arbitrary length. However, while previous work
on translation modeling with recurrent neural net-
works shows its effectiveness on standard base-
lines, so far no notable gains have been presented
on top of recurrent language models (Auli et al.,
2013; Kalchbrenner and Blunsom, 2013; Hu et al.,
2014).
In this work, we present two novel approaches
to recurrent neural translation modeling: word-
based and phrase-based. The word-based ap-
proach assumes one-to-one aligned source and
target sentences. We evaluate different ways of
resolving alignment ambiguities to obtain such
alignments. The phrase-based RNN approach is
more closely tied to the underlying translation
paradigm. It models actual phrasal translation
probabilities while avoiding sparsity issues by us-
ing single words as input and output units. Fur-
thermore, in addition to the unidirectional formu-
lation, we are the first to propose a bidirectional
architecture which can take the full source sen-
tence into account for all predictions. Our ex-
periments show that these models can improve
state-of-the-art baselines containing a recurrent
language model on three tasks. For our compet-
itive IWSLT 2013 German?English system, we
observe gains of up to 1.6% BLEU and 1.7% TER.
Improvements are also demonstrated on top of our
evaluation systems for BOLT Arabic?English
and Chinese?English, which also include recur-
rent neural language models.
The rest of this paper is structured as follows. In
Section 2 we review related work and in Section 3
an overview of long short-term memory (LSTM)
neural networks, a special type of recurrent neural
networks we make use of in this work, is given.
Section 4 describes our novel translation models.
Finally, experiments are presented in Section 5
and we conclude with Section 6.
14
2 Related Work
In this Section we contrast previous work to ours,
where we design RNNs to model bilingual depen-
dencies, which are applied to rerank n-best lists
after decoding.
To the best of our knowledge, the earliest at-
tempts to apply neural networks in machine trans-
lation (MT) are presented in (Casta?no et al.,
1997; Casta?no and Casacuberta, 1997; Casta?no
and Casacuberta, 1999), where they were used for
example-based MT.
Recently, Le et al. (2012) presented translation
models using an output layer with classes and
a shortlist for rescoring using feedforward net-
works. They compare between word-factored and
tuple-factored n-gram models, obtaining their best
results using the word-factored approach, which is
less amenable to data sparsity issues. Both of our
word-based and phrase-based models eventually
work on the word level. Kalchbrenner and Blun-
som (2013) use recurrent neural networks with
full source sentence representations. The continu-
ous representations are obtained by applying a se-
quence of convolutions, and the result is fed into
the hidden layer of a recurrent language model.
Rescoring results indicate no improvements over
the state of the art. Auli et al. (2013) also in-
clude source sentence representations built either
using Latent Semantic Analysis or by concatenat-
ing word embeddings. This approach produced
no notable gain over systems using a recurrent
language model. On the other hand, our pro-
posed bidirectional models include the full source
sentence relying on recurrency, yielding improve-
ments over competitive baselines already includ-
ing a recurrent language model.
RNNs were also used with minimum translation
units (Hu et al., 2014), which are phrase pairs un-
dergoing certain constraints. At the input layer,
each of the source and target phrases are mod-
eled as a bag of words, while the output phrase
is predicted word-by-word assuming conditional
independence. The approach seeks to alleviate
data sparsity problems that would arise if phrases
were to be uniquely distinguished. Our proposed
phrase-based models maintain word order within
phrases, but the phrases are processed in a word-
pair manner, while the phrase boundaries remain
implicitly encoded in the way the words are pre-
sented to the network. Schwenk (2012) proposed
a feedforward network that predicts phrases of a
fixed maximum length, such that all phrase words
are predicted at once. The prediction is condi-
tioned on the source phrase. Since our phrase-
based model predicts one word at a time, it does
not assume any phrase length. Moreover, our
model?s predictions go beyond phrase boundaries
and cover unbounded history and future contexts.
Using neural networks during decoding re-
quires tackling the costly output normalization
step. Vaswani et al. (2013) avoid this step by
training feedforward neural language models us-
ing noise contrastive estimation, while Devlin et
al. (2014) augment the training objective function
to produce approximately normalized scores di-
rectly. The latter work makes use of translation
and joint models, and pre-computes the first hid-
den layer beforehand, resulting in large speedups.
They report major improvements over strong base-
lines. The speedups achieved by both works al-
lowed to integrate feedforward neural networks
into the decoder.
3 LSTM Recurrent Neural Networks
Our work is based on recurrent neural networks.
In related fields like e. g. language modeling, this
type of neural network has been shown to perform
considerably better than standard feedforward ar-
chitectures (Mikolov et al., 2011; Arisoy et al.,
2012; Sundermeyer et al., 2013; Liu et al., 2014).
Most commonly, recurrent neural networks are
trained with stochastic gradient descent (SGD),
where the gradient of the training criterion is com-
puted with the backpropagation through time al-
gorithm (Rumelhart et al., 1986; Werbos, 1990;
Williams and Zipser, 1995). However, the combi-
nation of RNN networks with conventional back-
propagation training leads to conceptual difficul-
ties which are known as the vanishing (or explod-
ing) gradient problem, described e. g. in (Bengio
et al., 1994). To remedy this problem, in (Hochre-
iter and Schmidhuber, 1997) it was suggested to
modify the architecture of a standard RNN in such
a way that vanishing and exploding gradients are
avoided during backpropagation. In particular, no
modification of the training algorithm is necessary.
The resulting architecture is referred to as long
short-term memory (LSTM) neural network.
Bidirectional recurrent neural networks
(BRNNs) were first proposed in (Schuster and
Paliwal, 1997) and applied to speech recognition
tasks. They have been since applied to different
15
Surfers
,
for
example
,
know
this
incredibly
.
S
u
r
f
e
r
z
u
m
B
e
i
s
p
i
e
l
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
(a) Original
Surfers
,
for
example
,
know
this

unaligned
incredibly
.
S
u
r
f
e
r

a
l
i
g
n
e
d
z
u
m
B
e
i
s
p
i
e
l

u
n
a
l
i
g
n
e
d
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
(b) One-to-one alignment
Figure 1: Example sentence from the German?English IWSLT data. The one-to-one alignment is
created by introducing 
aligned
and 
unaligned
tokens.
tasks like parsing (Henderson, 2004) and spoken
language understanding (Mesnil et al., 2013).
Bidirectional long short-term memory (BLSTM)
networks are BRNNs using LSTM hidden layers
(Graves and Schmidhuber, 2005). This work
introduces BLSTMs to the problem of machine
translation, allowing powerful models that employ
unlimited history and future information to make
predictions.
While the proposed models do not make any as-
sumptions about the type of RNN used, all of our
experiments make use of recurrent LSTM neural
networks, where we include later LSTM exten-
sions proposed in (Gers et al., 2000; Gers et al.,
2003). The cross-entropy error criterion is used
for training. Further details on LSTM neural net-
works can be found in (Graves and Schmidhuber,
2005; Sundermeyer et al., 2012).
4 Translation Modeling with RNNs
In the following we describe our word- and
phrase-based translation models in detail. We also
show how bidirectional RNNs can enable such
models to include full source information.
4.1 Resolving Alignment Ambiguities
Our word-based recurrent models are only de-
fined for one-to-one-aligned source-target sen-
tence pairs. In this work, we always evaluate the
model in the order of the target sentence. How-
ever, we experiment with several different ways
to resolve ambiguities due to unaligned or mul-
tiply aligned words. To that end, we introduce
two additional tokens, 
aligned
and 
unaligned
. Un-
dev test
BLEU TER BLEU TER
baseline 33.5 45.8 30.9 48.4
w/o  34.2 45.3 31.8 47.7
w/o 
unaligned
34.4 44.8 31.7 47.4
source identity 34.5 45.0 31.9 47.5
target identity 34.5 44.6 31.9 47.0
all  34.6 44.5 32.0 47.1
Table 1: Comparison of including different sets
of  tokens into the one-to-one alignment on the
IWSLT 2013 German?English task using the uni-
directional RNN translation model.
aligned words are either removed or aligned to an
extra 
unaligned
token on the opposite side. If an

unaligned
is introduced on the target side, its posi-
tion is determined by the aligned source word that
is closest to the unaligned source word in question,
preferring left to right. To resolve one-to-many
alignments, we use an IBM-1 translation table to
decide for one of the alignment connections to be
kept. The remaining words are also either deleted
or aligned to additionally introduced 
aligned
to-
kens on the opposite side. Fig. 1 shows an ex-
ample sentence from the IWSLT data, where all 
tokens are introduced.
In a short experiment, we evaluated 5 differ-
ent setups with our unidirectional RNN translation
model (cf. next Section): without any  tokens,
without 
unaligned
, source identity, target identity
and using all  tokens. Source identity means we
16
introduce no  tokens on source side, but all on
target side. Target identity is defined analogously.
The results can be found in Tab. 1. We use the
setup with all  tokens in all following experi-
ments, which showed the best BLEU performance.
4.2 Word-based RNN Models
Given a pair of source sequence f
I
1
= f
1
. . . f
I
and target sequence e
I
1
= e
1
. . . e
I
, where we as-
sume a direct correspondence between f
i
and e
i
,
we define the posterior translation probability by
factorizing on the target words:
p(e
I
1
|f
I
1
) =
I
?
i=1
p(e
i
|e
i?1
1
, f
I
1
) (1)
?
I
?
i=1
p(e
i
|e
i?1
1
, f
i+d
1
) (2)
?
I
?
i=1
p(e
i
|f
i+d
1
). (3)
We denote the formulation (1) as the bidirectional
joint model (BJM). This model can be simplified
by several independence assumptions. First, we
drop the dependency on the future source infor-
mation, receiving what we denote as the unidirec-
tional joint model (JM) in (2). Here, d ? N
0
is
a delay parameter, which is set to d = 0 for all
experiments, except for the comparative results re-
ported in Fig. 7. Finally, assuming conditional in-
dependence from the previous target sequence, we
receive the unidirectional translation model (TM)
in (3). Analogously, we can define a bidirectional
translation model (BTM) by keeping the depen-
dency on the full source sentence f
I
1
, but dropping
the previous target sequence e
i?1
1
:
p(e
I
1
|f
I
1
) ?
I
?
i=1
p(e
i
|f
I
1
). (4)
Fig. 2 shows the dependencies of the word-
based neural translation and joint models. The
alignment points are traversed in target order and
at each time step one target word is predicted.
The pure translation model (TM) takes only source
words as input, while the joint model (JM) takes
the preceding target words as an additional input.
A delay of d > 0 is implemented by shifting the
target sequence by d time steps and filling the first
d target positions and the last d source positions
with a dedicated 
padding
symbol. The RNN archi-
tecture for the unidirectional word-based models
j
o
i
n
t
m
o
d
e
l
all models bidirectional
Surfers
,
for
example
,
know
this

unaligned
incredibly
.
S
u
r
f
e
r

a
l
i
g
n
e
d
z
u
m
B
e
i
s
p
i
e
l

u
n
a
l
i
g
n
e
d
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
Figure 2: Dependencies modeled within the word-
based RNN models when predicting the target
word ?know?. Directly processed information is
depicted with solid rectangles, and information
available through recurrent connections is marked
with dashed rectangles.
is illustrated in Fig. 3, which corresponds to the
following set of equations:
y
i
= A
1
?
f
i
+A
2
e?
i?1
z
i
= ?(y
i
;A
3
, y
i?1
1
)
p
(
c(e
i
)|e
i?1
1
, f
i
1
)
= ?
c(e
i
)
(A
4
z
i
)
p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
= ?
e
i
(A
c(e
i
)
z
i
)
p(e
i
|e
i?1
1
, f
i
1
) = p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
?
p
(
c(e
i
)|e
i?1
1
, f
i
1
)
Here, by
?
f
i
and e?
i?1
we denote the one-hot en-
coded vector representations of the source and
target words f
i
and e
i?1
. The outgoing activa-
tion values of the projection layer and the LSTM
layer are y
i
and z
i
, respectively. The matrices A
j
contain the weights of the neural network layers.
By ?(? ;A
3
, y
i?1
1
) we denote the LSTM formalism
that we plug in at the third layer. As the LSTM
layer is recurrent, we explicitly include the de-
pendence on the previous layer activations y
i?1
1
.
Finally, ? is the widely-used softmax function to
obtain normalized probabilities, and c denotes a
word class mapping from any target word to its
unique word class. For the bidirectional model,
the equations can be defined analogously.
Due to the use of word classes, the output
layer consists of two parts. The class probabil-
ity p
(
c(e
i
)|e
i?1
1
, f
i
1
)
is computed first, and then
17
p(
c(e
i
)|ei?11 , f i+d1
)
p
(
e
i
|c(e
i
), ei?11 , f i+d1
)
class layer output layer
LSTM layer
projection layer
input layer
e
i?1f
i+d)
Figure 3: Architecture of a recurrent unidirec-
tional translation model. By including the dashed
parts, a joint model is obtained.
the word probability p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
is ob-
tained given the word class. This trick helps avoid-
ing the otherwise computationally expensive nor-
malization sum, which would be carried out over
all words in the target vocabulary. In a class-
factorized output layer where each word belongs
to a single class, the normalization is carried out
over all classes, whose number is typically much
less than the vocabulary size. The other normal-
ization sum needed to produce the word probabil-
ity is limited to the words belonging to the same
class (Goodman, 2001; Morin and Bengio, 2005).
4.3 Phrase-based RNN Models
One of the conceptual disadvantages of word-
based modeling as introduced in the previous sec-
tion is that there is a mismatch between train-
ing and testing conditions: During neural network
training, the vocabulary has to be extended by ad-
ditional  tokens, and a one-to-one alignment is
used which does not reflect the situation in decod-
ing. In phrase-based machine translation, more
complex alignments in terms of multiple words
on both the source and the target sides are used,
which allow the decoder to make use of richer
short-distance dependencies and are crucial for the
performance of the resulting system.
From this perspective, it seems interesting to
standardize the alignments used in decoding, and
in training the neural network. However, it is dif-
ficult to use the phrases themselves as the vocab-
ulary of the RNN. Usually, the huge number of
potential phrases in comparison to the relatively
small amount of training data makes the learn-
ing of continuous phrase representations difficult
Surfer
Surfers
zum Beispiel
, for example ,
kennen
know
zur Gen?ge
incredibly
.
.
das
this
Figure 4: Example phrase alignment for a sen-
tence from the IWSLT training data.
due to data sparsity. This is confirmed by results
presented in (Le et al., 2012), which show that a
word-factored translation model outperforms the
phrase-factored version. Therefore, in this work
we continue relying on source and target word vo-
cabularies for building our phrase representations.
However, we no longer use a direct correspon-
dence between a source and a target word, as en-
forced in our word-based models.
Fig. 4 shows an example phrase alignment,
where a sequence of source words
?
f
i
is directly
mapped to a sequence of target words e?
i
for 1 ?
i ?
?
I . By
?
I , we denote the number of phrases in
the alignment. We decompose the target sentence
posterior probability in the following way:
p(e
I
1
|f
J
1
) =
?
I
?
i=1
p(e?
i
|e?
i?1
1
,
?
f
?
I
1
) (5)
?
?
I
?
i=1
p(e?
i
|e?
i?1
1
,
?
f
i
1
) (6)
where the joint model in Eq. 5 would correspond
to a bidirectional RNN, and Eq. 6 only requires a
unidirectional RNN. By leaving out the condition-
ing on the target side, we obtain a phrase-based
translation model.
As there is no one-to-one correspondence be-
tween the words within a phrase, the basic idea of
our phrase-based approach is to let the neural net-
work learn the dependencies itself, and present the
full source side of the phrase to the network be-
fore letting it predict target side words. Then the
probability for the target side of a phrase can be
computed, in case of Eq. 6, by:
p(e?
i
|e?
i?1
1
,
?
f
?
I
1
) =
|e?
i
|
?
j=1
p
(
(e?
i
)
j
|(e?
i
)
j?1
1
, e?
i?1
1
,
?
f
i
1
)
,
and analogously for the case of Eq. 5. Here, (e?
i
)
j
denotes the j-th word of the i-th aligned target
phrase.
We feed the source side of a phrase into the neu-
ral network one word at a time. Only when the
18
output layer
LSTM layer
projection layer
input layer
Surfers ,, for example know this incredibly .??
?s? Surfers
,, for example
know
this
incredibly
.Surfer
zum
B
eispiel
kennen
das zur Genu?ge
?? ???
Figure 5: A recurrent phrase-based joint translation model, unfolded over time. Source words are printed
in normal face, while target words are printed in bold face. Dashed lines indicate phrases from the
example sentence. For brevity, we omit the precise handling of sentence begin and end tokens.
presentation of the source side is finished we start
estimating probabilities for the target side. There-
fore, we do not let the neural network learn a target
distribution until the very last source word is con-
sidered. In this way, we break up the conventional
RNN training scheme where an input sample is di-
rectly followed by its corresponding teacher sig-
nal. Similarly, the presentation of the source side
of the next phrase only starts after the prediction
of the current target side is completed.
To this end, we introduce a no-operation token,
denoted by ?, which is not part of the vocabulary
(which means it cannot be input to or predicted by
the RNN). When the ? token occurs as input, it in-
dicates that no input needs to be processed by the
RNN. When the ? token occurs as a teacher signal
for the RNN, the output layer distribution is ig-
nored, and does not even have to be computed. In
both cases, all the other layers are still processed
during forward and backward passes such that the
RNN state can be advanced even without addi-
tional input or output.
Fig. 5 depicts the evaluation of a phrase-based
joint model for the example alignment from Fig. 4.
For a source phrase
?
f
i
, we include (|e?
i
|?1) many ?
symbols at the end of the phrase. Conversely, for
a target phrase e?
i
, we include (|
?
f
i
| ? 1) many ?
symbols at the beginning of the phrase.
E. g., in the figure, the second dashed rectan-
gle from the left depicts the training of the English
phrase ?, for example ,? and its German transla-
tion ?zum Beispiel?. At the input layer, we feed in
the source words one at a time, while we present
? tokens at the target side input layer and the out-
put layer (with the exception of the very first time
step, where we still have the last target word from
the previous phrase as input instead of ?). With
the last word of the source phrase ?Beispiel? being
presented to the network, the full source phrase is
stored in the hidden layer, and the neural network
is then trained to predict the target phrase words
at the output layer. Subsequently, the source input
is ?, and the target input is the most recent target
side history word.
To obtain a phrase-aligned training sequence for
the phrase-based RNN models, we force-align the
training data with the application of leave-one-out
as described in (Wuebker et al., 2010).
4.4 Bidirectional RNN Architecture
While the unidirectional RNNs include an un-
bounded sentence history, they are still limited in
the number of future source words they include.
Bidirectional models provide a flexible means to
also include an unbounded future context, which,
unlike the delayed unidirectional models, require
no tuning to determine the amount of delay.
Fig. 6 illustrates the bidirectional model archi-
tecture, which is an extension of the unidirectional
model of Fig. 3. First, an additional recurrent
hidden layer is added in parallel to the existing
one. This layer will be referred to as the back-
ward layer, since it processes information in back-
ward time direction. This hidden layer receives
source word input only, while target words in the
case of a joint model are fed to the forward layer
as in the unidirectional case. Due to the backward
recurrency, the backward layer will make the in-
formation f
I
i
available when predicting the target
word e
i
, while the forward layer takes care of the
source history f
i
1
. Jointly, the forward and back-
ward branches include the full source sentence f
I
1
,
as indicated in Fig. 2. Fig. 6 shows the ?deep?
variant of the bidirectional model, where the for-
19
p(
c(e
i
)|ei?11 , fI1
)
p
(
e
i
|c(e
i
), ei?11 , fI1
)
class layer output layer
2nd LSTM layer
1st LSTM layer
projection layer
input layer
e
i?1fi
(+)
(+)
(?)
Figure 6: Architecture of a recurrent bidirectional
translation model. By (+) and (?), we indicate
a processing in forward and backward time direc-
tions, respectively. The inclusion of the dashed
parts leads to a bidirectional joint model. One
source projection matrix is used for the forward
and backward branches.
ward and backward layers converge into a hidden
layer. A shallow variant can be obtained if the
parallel layers converge into the output layer di-
rectly
1
.
Due to the full dependence on the source se-
quence, evaluating bidirectional networks requires
computing the forward pass of the forward and
backward layers for the full sequence, before be-
ing able to evaluate the next layers. In the back-
ward pass of backpropagation, the forward and
backward recurrent layers are processed in de-
creasing and increasing time order, respectively.
5 Experiments
5.1 Setup
All translation experiments are performed with the
Jane toolkit (Vilar et al., 2010; Wuebker et al.,
2012). The largest part of our experiments is car-
ried out on the IWSLT 2013 German?English
shared translation task.
2
The baseline system is
trained on all available bilingual data, 4.3M sen-
tence pairs in total, and uses a 4-gram LM with
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998), trained with
the SRILM toolkit (Stolcke, 2002). As additional
1
In our implementation, the forward and backward layers
converge into an intermediate identity layer, and the aggre-
gate is weighted and fed to the next layer.
2
http://www.iwslt2013.org
data sources for the LM we selected parts of the
Shuffled News and LDC English Gigaword cor-
pora based on cross-entropy difference (Moore
and Lewis, 2010), resulting in a total of 1.7 bil-
lion running words for LM training. The state-of-
the-art baseline is a standard phrase-based SMT
system (Koehn et al., 2003) tuned with MERT
(Och, 2003). It contains a hierarchical reorder-
ing model (Galley and Manning, 2008) and a 7-
gram word cluster language model (Wuebker et
al., 2013). Here, we also compare against a feed-
forward joint model as described by Devlin et al.
(2014), with a source window of 11 words and a
target history of three words, which we denote as
BBN-JM. Instead of POS tags, we predict word
classes trained with mkcls. We use a shortlist
of size 16K and 1000 classes for the remaining
words. All neural networks are trained on the TED
portion of the data (138K segments) and are ap-
plied in a rescoring step on 1000-best lists.
To confirm our results, we run additional
experiments on the Arabic?English and
Chinese?English tasks of the DARPA BOLT
project. In both cases, the neural network models
are added on top of our most competitive eval-
uation system. On Chinese?English, we use a
hierarchical phrase-based system trained on 3.7M
segments with 22 dense features, including an ad-
vanced orientation model (Huck et al., 2013). For
the neural network training, we selected a subset
of 9M running words. The Arabic?English
system is a standard phrase-based decoder trained
on 6.6M segments, using 17 dense features. The
neural network training was performed using a
selection amounting to 15.5M running words.
For both tasks we apply the neural networks by
rescoring 1000-best lists and evaluate results on
two data sets from the ?discussion forum? domain,
test1 and test2. The sizes of the data sets
for the Arabic?English system are: 1219 (dev),
1510 (test1), and 1137 (test2) segments, and
for the Chinese?English system are: 5074 (dev),
1844 (test1), and 1124 (test2) segments. All
results are measured in case-insensitive BLEU [%]
(Papineni et al., 2002) and TER [%] (Snover et al.,
2006) on a single reference.
5.2 Results
Our results on the IWSLT German?English task
are summarized in Tab. 2. At this point, we
do not include a recurrent neural network lan-
20
dev test
BLEU TER BLEU TER
baseline 33.5 45.8 30.9 48.4
TM 34.6 44.5 32.0 47.1
JM 34.7 44.7 31.8 47.4
BTM 34.7 44.9 32.3 47.0
BTM (deep) 34.8 44.3 32.5 46.7
BJM 34.7 44.5 32.1 47.0
BJM (deep) 34.9 44.1 32.2 46.6
PTM 34.3 44.9 32.1 47.5
PJM 34.3 45.0 32.0 47.5
PJM (10-best) 34.4 44.8 32.0 47.3
PJM (deep) 34.6 44.7 32.0 47.6
PBJM (deep) 34.8 44.9 31.9 47.5
BBN-JM 34.4 44.9 31.9 47.6
Table 2: Results for the IWSLT 2013
German?English task with different RNN
models. T: translation, J: joint, B: bidirectional,
P: phrase-based.
guage model yet. Here, the delay parameter d
from Equations 2 and 3 is set to zero. We ob-
serve that for all recurrent translation models, we
achieve substantial improvements over the base-
line on the test data, ranging from 0.9 BLEU
up to 1.6 BLEU. These results are also consistent
with the improvements in terms of TER, where we
achieve reductions by 0.8 TER up to 1.8 TER.
These numbers can be directly compared to the
case of feedforward neural network-based transla-
tion modeling as proposed in (Devlin et al., 2014)
which we include in the very last row of the table.
Nearly all of our recurrent models outperform the
feedforward approach, where the RNN model per-
forming best on the dev data is better on test
by 0.3 BLEU and 1.0 TER.
Interestingly, for the recurrent word-based mod-
els, on the test data it can be seen that TMs per-
form better than JMs, even though TMs do not
take advantage of the target side history words.
However, exploiting this extra information does
not always need to result in a better model, as the
target side words are only derived from the given
source side, which is available to both TMs and
JMs. On the other hand, including future source
words in a bidirectional model clearly improves
the performance further. By adding another LSTM
 
31
 
31
.5
 
32
 
32
.5
 
33
0
1
2
3
4
BLEU[%]
De
layRN
N-
TM
RN
N-
BT
M
Figure 7: BLEU scores on the IWSLT test set
with different delays for the unidirectional RNN-
TM and the bidirectional RNN-BTM.
layer that combines forward and backward time
directions (indicated as ?deep? in the table), we ob-
tain our overall best model.
In Fig. 7 we compare the word-based bidirec-
tional TM with a unidirectional TM that uses dif-
ferent time delays d = 0, . . . , 4. For a delay d =
2, the same performance is obtained as with the
bidirectional model, but this comes at the price of
tuning the delay parameter.
In comparison to the unidirectional word-based
models, phrase-based models perform similarly.
In the tables, we include those phrase-based vari-
ants which perform best on the dev data, where
phrase-based JMs always are at least as good or
better than the corresponding TMs in terms of
BLEU. Therefore, we mainly report JM results
for the phrase-based networks. A phrase-based
model can also be trained on multiple variants for
the phrase alignment. For our experiments, we
tested 10-best alignments against the single best
alignment, which resulted in a small improvement
of 0.2 TER on both dev and test. We did not ob-
serve consistent gains by using an additional hid-
den layer or bidirectional models. To some ex-
tent, future information is already considered in
unidirectional phrase-based models by feeding the
complete source side before predicting the target
side.
Tab. 3 shows different model combination re-
sults for the IWSLT task, where a recurrent lan-
guage model is included in the baseline. Adding
a deep bidirectional TM or JM to the recur-
rent language model improves the RNN-LM base-
line by 1.2 BLEU or 1.1 BLEU, respectively. A
phrase-based model substantially improves over
21
dev eval11 test
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
baseline (w/ RNN-LM) 34.3 44.8 36.4 42.9 31.5 47.8
BTM (deep) 34.9 43.7 37.6 41.5 32.7 46.1
BJM (deep) 35.0 44.4 37.4 41.9 32.6 46.5
PBJM (deep) 34.8 44.6 36.9 42.6 32.3 47.2
4 RNN models 35.2 43.4 38.0 41.2 32.7 46.0
Table 3: Results for the IWSLT 2013 German?English task with different RNN models. All results
include a recurrent language model. T: translation, J: joint, B: bidirectional, P: phrase-based.
the RNN-LM baseline, but performs not as good
as its word-based counterparts. By adding four
different translation models, including models in
reverse word order and reverse translation direc-
tion, we are able to improve these numbers even
further. However, especially on the test data, the
gains from model combination saturate quickly.
Apart from the IWSLT track, we also ana-
lyze the performance of our translation models on
the BOLT Chinese?English and Arabic?English
translation tasks. Due to the large amount of train-
ing data, we concentrate on models of high perfor-
mance in the IWSLT experiments. The results can
be found in Tab. 4 and 5. In both cases, we see
consistent improvements over the recurrent neural
network language model baseline, improving the
Arabic?English system by 0.6 BLEU and 0.5 TER
on test1. This can be compared to the rescoring
results for the same task reported by (Devlin et al.,
2014), where they achieved 0.3 BLEU, despite the
fact that they used multiple references for scoring,
whereas in our experiments we rely on a single
reference only. The models are also able to im-
prove the Chinese?English system by 0.5 BLEU
and 0.5 TER on test2.
5.3 Analysis
To investigate whether bidirectional models ben-
efit from future source information, we compare
the single-best output of a system reranked with a
unidirectional model to the output reranked with
a bidirectional model. We choose the models
to be translation models in both cases, as they
predict target words independent of previous
predictions, given the source information (cf. Eqs.
(3, 4)). This makes it easier to detect the effect
of including future source information or the lack
thereof. The examples are taken from the IWSLT
test1 test2
BLEU TER BLEU TER
baseline 25.2 57.4 26.8 57.3
BTM (deep) 25.6 56.6 26.8 56.7
BJM (deep) 25.9 56.9 27.4 56.7
RNN-LM 25.6 57.1 27.5 56.7
+ BTM (deep) 25.9 56.7 27.3 56.8
+ BJM (deep) 26.2 56.6 27.9 56.5
Table 4: Results for the BOLT Arabic?English
task with different RNN models. The ?+? sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, J: joint, B: bidirectional.
task, where we include the one-to-one source
information, reordered according to the target
side.
source: nicht so wie ich
reference: not like me
Hypothesis 1:
1-to-1 source: so ich  nicht wie
1-to-1 target: so I do n?t like
Hypothesis 2:
1-to-1 source: nicht so wie ich
1-to-1 target: not  like me
In this example, the German phrase ?so wie?
translates to ?like? in English. The bidirectional
model prefers hypothesis 2, making use of the
future word ?wie? when translating the German
word ?so? to , because it has future insight that
this move will pay off later when translating
22
BLEU TER BLEU TER
baseline 18.3 63.6 16.7 63.0
BTM (deep) 18.7 63.3 17.1 62.6
BJM (deep) 18.5 63.1 17.2 62.3
RNN-LM 18.8 63.3 17.2 62.8
+ BTM (deep) 18.9 63.1 17.7 62.3
+ BJM (deep) 18.8 63.3 17.5 62.5
Table 5: Results for the BOLT Chinese?English
task with different RNN models. The ?+? sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, B: bidirectional.
the rest of the sentence. This information is
not available to the unidirectional model, which
prefers hypothesis 1 instead.
source: das taten wir dann auch und verschafften uns
so eine Zeit lang einen Wettbewerbs Vorteil .
reference: and we actually did that and it gave us a
competitive advantage for a while .
Hypothesis 1:
1-to-1 source: das    wir dann auch taten und
verschafften uns so eine Zeit lang einen Wettbewerbs
Vorteil .
1-to-1 target: that ?s just what we   did and gave us 
a time , a competitive advantage .
Hypothesis 2:
1-to-1 source: das    wir dann auch taten und
verschafften uns so einen Wettbewerbs Vorteil  eine
Zeit lang .
1-to-1 target: that ?s just what we   did and gave us 
a competitive advantage for a  while .
Here, the German phrase ?eine Zeit lang? trans-
lates to ?for a while? in English. Bidirectional
scoring favors hypothesis 2, while unidirectional
scoring favors hypothesis 1. It seems that the uni-
directional model translates ?Zeit? to ?time? as the
object of the verb ?give? in hypothesis 1, being
blind to the remaining part ?lang? of the phrase
which changes the meaning. The bidirectional
model, to its advantage, has the full source infor-
mation, allowing it to make the correct prediction.
6 Conclusion
We developed word- and phrase-based RNN trans-
lation models. The former is simple and performs
well in practice, while the latter is more consistent
with the phrase-based paradigm. The approach in-
herently evades data sparsity problems as it works
on words in its lowest level of processing. Our
experiments show the models are able to achieve
notable improvements over baselines containing a
recurrent LM.
In addition, and for the first time in statistical
machine translation, we proposed a bidirectional
neural architecture that allows modeling past and
future dependencies of any length. Besides its
good performance in practice, the bidirectional ar-
chitecture is of theoretical interest as it allows the
exact modeling of posterior probabilities.
Acknowledgments
This material is partially based upon work sup-
ported by the DARPA BOLT project under Con-
tract No. HR0011- 12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
The research leading to these results has also re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements n
o
287658 and n
o
287755.
Experiments were performed with computing re-
sources granted by JARA-HPC from RWTH
Aachen University under project ?jara0085?. We
would like to thank Jan-Thorsten Peter for provid-
ing the BBN-JM system.
References
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20?28. Association for
Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 1044?1054, Seattle, USA, Octo-
ber.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
23
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Maria Asunci?on Casta?no and Francisco Casacuberta.
1997. A connectionist approach to machine trans-
lation. In 5th International Conference on Speech
Communication and Technology (EUROSPEECH-
97), Rhodes, Greece.
Maria Asunci?on Casta?no and Francisco Casacuberta.
1999. Text-to-text machine translation using the
RECONTRA connectionist model. In Lecture Notes
in Computer Science (IWANN 99), volume 1607,
pages 683?692, Alicante, Spain.
Maria Asunci?on Casta?no, Francisco Casacuberta, and
Enrique Vidal. 1997. Machine translation using
neural networks and finite-state models. In 7th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation. TMI?97,
pages 160?167, Santa Fe, USA.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, page to appear, Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 848?856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Felix A. Gers, J?urgen Schmidhuber, and Fred Cum-
mins. 2000. Learning to forget: Contin-
ual prediction with LSTM. Neural computation,
12(10):2451?2471.
Felix A. Gers, Nicol N. Schraudolph, and J?urgen
Schmidhuber. 2003. Learning precise timing with
lstm recurrent networks. The Journal of Machine
Learning Research, 3:115?143.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Acoustics, Speech, and Signal
Processing, 2001. Proceedings.(ICASSP?01). 2001
IEEE International Conference on, volume 1, pages
561?564. IEEE.
Alex Graves and J?urgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5):602?610.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.
Sepp Hochreiter and J?urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735?1780.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for hi-
erarchical machine translation. In ACL 2013 Eighth
Workshop on Statistical Machine Translation, pages
452?463, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processingw, volume 1,
pages 181?184, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North Ameri-
can chapter of the Association for Computational
Linguistics (NAACL-03), pages 127?133, Edmon-
ton, Alberta.
Hai Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
39?48, Montreal, Canada, June.
Xunying Liu, Yongqiang Wang, Xie Chen, Mark J. F.
Gales, and Phil C. Woodland. 2014. Efficient lattice
rescoring using recurrent neural network language
models. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 4941?4945. IEEE.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech, pages
3771?3775.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
24
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In ACL
(Short Papers), pages 220?224, Uppsala, Sweden,
July.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In: J. L. McClelland, D. E.
Rumelhart, and The PDP Research Group: ?Paral-
lel Distributed Processing, Volume 1: Foundations?.
The MIT Press.
Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673?2681.
Holger Schwenk, Daniel D?echelotte, and Jean-Luc
Gauvain. 2006. Continuous Space Language Mod-
els for Statistical Machine Translation. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 723?730, Sydney, Australia,
July.
Holger Schwenk. 2012. Continuous Space Translation
Models for Phrase-Based Statistical Machine Trans-
lation. In 25th International Conference on Compu-
tational Linguistics (COLING), pages 1071?1080,
Mumbai, India, December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.
2012. LSTM neural networks for language model-
ing. In Interspeech, Portland, OR, USA, September.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of feedforward and recurrent
neural network language models. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, pages 8430?8434, Vancouver, Canada,
May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Paul J. Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550?1560.
Ronald J. Williams and David Zipser. 1995. Gradient-
Based Learning Algorithms for Recurrent Net-
works and Their Computational Complexity. In:
Yves Chauvain and David E. Rumelhart: ?Back-
Propagation: Theory, Architectures and Applica-
tions?. Lawrence Erlbaum Publishers.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, USA, October.
25
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1764?1768,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Improved Decipherment of Homophonic Ciphers
Malte Nuhn and Julian Schamper and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper, we present two improve-
ments to the beam search approach for
solving homophonic substitution ciphers
presented in Nuhn et al. (2013): An im-
proved rest cost estimation together with
an optimized strategy for obtaining the or-
der in which the symbols of the cipher are
deciphered reduces the beam size needed
to successfully decipher the Zodiac-408
cipher from several million down to less
than one hundred: The search effort is re-
duced from several hours of computation
time to just a few seconds on a single CPU.
These improvements allow us to success-
fully decipher the second part of the fa-
mous Beale cipher (see (Ward et al., 1885)
and e.g. (King, 1993)): Having 182 differ-
ent cipher symbols while having a length
of just 762 symbols, the decipherment is
way more challenging than the decipher-
ment of the previously deciphered Zodiac-
408 cipher (length 408, 54 different sym-
bols). To the best of our knowledge, this
cipher has not been deciphered automati-
cally before.
1 Introduction
State-of-the-art statistical machine translation sys-
tems use large amounts of parallel data to estimate
translation models. However, parallel corpora are
expensive and not available for every domain.
Decipherment uses only monolingual data to
train a translation model: Improving the core deci-
pherment algorithms is an important step for mak-
ing decipherment techniques useful for training
practical machine translation systems.
In this paper we present improvements to the
beam search algorithm for deciphering homo-
phonic substitution ciphers as presented in Nuhn
et al. (2013). We show significant improvements
in computation time on the Zodiac-408 cipher and
show the first decipherment of part two of the
Beale ciphers.
2 Related Work
Regarding the decipherment of 1:1 substitution ci-
phers, various works have been published: Most
older papers do not use a statistical approach and
instead define some heuristic measures for scoring
candidate decipherments. Approaches like Hart
(1994) and Olson (2007) use a dictionary to check
if a decipherment is useful. Clark (1998) defines
other suitability measures based on n-gram counts
and presents a variety of optimization techniques
like simulated annealing, genetic algorithms and
tabu search. On the other hand, statistical ap-
proaches for 1:1 substitution ciphers are published
in the natural language processing community:
Ravi and Knight (2008) solve 1:1 substitution ci-
phers optimally by formulating the decipherment
problem as an integer linear program (ILP) while
Corlett and Penn (2010) solve the problem using
A
?
search. Ravi and Knight (2011) report the
first automatic decipherment of the Zodiac-408 ci-
pher. They use a combination of a 3-gram lan-
guage model and a word dictionary. As stated in
the previous section, this work can be seen as an
extension of Nuhn et al. (2013). We will there-
fore make heavy use of their definitions and ap-
proaches, which we will summarize in Section 3.
3 General Framework
In this Section we recap the beam search frame-
work introduced in Nuhn et al. (2013).
3.1 Notation
We denote the ciphertext with f
N
1
=
f
1
. . . f
j
. . . f
N
which consists of cipher
1764
tokens f
j
? V
f
. We denote the plain-
text with e
N
1
= e
1
. . . e
i
. . . e
N
(and its
vocabulary V
e
respectively). We define
e
0
= f
0
= e
N+1
= f
N+1
= $ with ?$?
being a special sentence boundary token. Homo-
phonic substitutions are formalized with a general
function ? : V
f
? V
e
. Following (Corlett and
Penn, 2010), cipher functions ?, for which not all
?(f)?s are fixed, are called partial cipher func-
tions. Further, ?
?
is said to extend ?, if for all
f ? V
f
that are fixed in ?, it holds that f is also
fixed in ?
?
with ?
?
(f) = ?(f). The cardinality
of ? counts the number of fixed f ?s in ?. When
talking about partial cipher functions we use the
notation for relations, in which ? ? V
f
? V
e
.
3.2 Beam Search
The main idea of (Nuhn et al., 2013) is to struc-
ture all partial ??s into a search tree: If a cipher
containsN unique symbols, then the search tree is
of height N . At each level a decision about the n-
th symbol is made. The leaves of the tree form full
hypotheses. Instead of traversing the whole search
tree, beam search descents the tree top to bottom
and only keeps the most promising candidates at
each level. Practically, this is done by keeping
track of all partial hypotheses in two arraysH
s
and
H
t
. During search all allowed extensions of the
partial hypotheses in H
s
are generated, scored and
put into H
t
. Here, the function EXT ORDER (see
Section 5) chooses which cipher symbol is used
next for extension, EXT LIMITS decides which ex-
tensions are allowed, and SCORE (see Section 4)
scores the new partial hypotheses. PRUNE then
selects a subset of these hypotheses. Afterwards
the array H
t
is copied to H
s
and the search pro-
cess continues with the updated arrayH
s
. Figure 1
shows the general algorithm.
4 Score Estimation
The score estimation function is crucial to the
search procedure: It predicts how good or bad a
partial cipher function ?might become, and there-
fore, whether it?s worth to keep it or not.
To illustrate how we can calculate these scores,
we will use the following example with vocabular-
ies V
f
= {A,B,C,D}, V
e
= {a, b, c, d}, exten-
sion order (B,C,A,D), and cipher text
1
?(f
N
1
) = $ ABDD CABC DADC ABDC $
1
We include blanks only for clarity reasons.
1: function BEAM SEARCH(EXT ORDER)
2: init sets H
s
, H
t
3: CARDINALITY = 0
4: H
s
.ADD((?, 0))
5: while CARDINALITY < |V
f
| do
6: f = EXT ORDER[CARDINALITY]
7: for all ? ? H
s
do
8: for all e ? V
e
do
9: ?
?
:= ? ? {(e, f)}
10: if EXT LIMITS(?
?
) then
11: H
t
.ADD(?
?
,SCORE (?
?
))
12: end if
13: end for
14: end for
15: PRUNE(H
t
)
16: CARDINALITY = CARDINALITY + 1
17: H
s
= H
t
18: H
t
.CLEAR()
19: end while
20: return best scoring cipher function in H
s
21: end function
Figure 1: The general structure of the beam search
algorithm for decipherment of substitution ciphers
as presented in Nuhn et al. (2013). This paper im-
proves the functions SCORE and EXT ORDER.
and partial hypothesis ? = {(A, a), (B, b)}. This
yields the following partial decipherment
?(f
N
1
) = $ ab.. .ab. .a.. ab.. $
The score estimation function can only use this
partial decipherment to calculate the hypothesis?
score, since there are not yet any decisions made
about the other positions.
4.1 Baseline
Nuhn et al. (2013) present a very simple rest
cost estimator, which calculates the hypothesis?
score based only on fully deciphered n-grams, i.e.
those parts of the partial decipherment that form a
contiguous chunk of n deciphered symbols. For
all other n-grams containing not yet deciphered
symbols, a trivial estimate of probability 1 is as-
sumed, making it an admissible heuristic. For the
above example, this baseline yields the probability
p(a|$) ? p(b|a) ? 1
4
? p(b|a) ? 1
6
? p(b|a) ? 1
2
. The
more symbols are fixed, the more contiguous n-
grams become available. While being easy and ef-
ficient to compute, it can be seen that for example
the single ?a? is not involved in the computation of
1765
the score at all. In practical decipherment, like e.g.
the Zodiac-408 cipher, this forms a real problem:
While making the first decisions?i.e. traversing
the first levels of the search tree?only very few
terms actually contribute to the score estimation,
and thus only give a very coarse score. This makes
the beam search ?blind? when not many symbols
are deciphered yet. This is the reason, why Nuhn
et al. (2013) need a large beam size of several mil-
lion hypotheses in order to not lose the right hy-
pothesis during the first steps of the search.
4.2 Improved Rest Cost Estimation
The rest cost estimator we present in this paper
solves the problem mentioned in the previous sec-
tion by also including lower order n-grams: In the
example mentioned before, we would also include
unigram scores into the rest cost estimate, yielding
a score of p(a|$)?p(b|a)?1
3
?p(a)?p(b|a)?1
2
?p(a)1
2
?
p(a) ? p(b|a) ? 1
2
. Note that this is not a simple lin-
ear interpolation of different n-gram trivial scores:
Each symbol is scored only using the maximum
amount of context available. This heuristic is non-
admissible, since an increased amount of context
can always lower the probabilty of some symbols.
However, experiments show that this score estima-
tion function works great.
5 Extension Order
Besides having a generally good scoring function,
also the order in which decisions about the cipher
symbols are made is important for obtaining reli-
able cost estimates. Generally speaking we want
an extension order that produces partial decipher-
ments that contain useful information to decide
whether a hypothesis is worth being kept or not
as early as possible.
It is also clear that the choice of a good ex-
tension order is dependent on the score estima-
tion function SCORE. After presenting the previ-
ous state of the art, we introduce a new extension
order optimized to work together with our previ-
ously introduced rest cost estimator.
5.1 Baseline
In (Nuhn et al., 2013), two strategies are pre-
sented: One which at each step chooses the most
frequent remaining cipher symbol, and another,
which greedily chooses the next symbol to max-
imize the number of contiguously fixed n-grams
in the ciphertext.
LM order
Perplexity
Zodiac-408 Beale Pt. 2
1 19.49 18.35
2 14.09 13.96
3 12.62 11.81
4 11.38 10.76
5 11.19 9.33
6 10.13 8.49
7 10.15 8.27
8 9.98 8.27
Table 1: Perplexities of the correct decipherment
of Zodiac-408 and part two of the Beale ciphers
using the character based language model used in
beam search. The language model was trained on
the English Gigaword corpus.
5.2 Improved Extension Order
Each partial mapping ? defines a partial decipher-
ment. We want to choose an extension order such
that all possible partial decipherments following
this extension order are as informative as possible:
Due to that, we can only use information about
which symbols will be deciphered, not their actual
decipherment. Since our heuristic is based on n-
grams of different orders, it seems natural to evalu-
ate an extension order by counting how many con-
tiguously deciphered n-grams are available: Our
new strategy tries to find an extension order op-
timizing the weighted sum of contiguously deci-
phered n-gram counts
2
N
?
n=1
w
n
?#
n
.
Here n is the n-gram order, w
n
the weight for or-
der n, and #
n
the number of positions whose max-
imum context is of size n.
We perform a beam search over all possible
enumerations of the cipher vocabulary: We start
with fixing only the first symbol to decipher. We
then continue with the second symbol and evalu-
ate all resulting extension orders of length 2. In
our experiments, we prune these candidates to the
100 best ones and continue with length 3, and so
on.
Suitable values for the weights w
n
have to be
chosen. We try different weights for the different
2
If two partial extension orders have the same score after
fixing n symbols, we fall back to comparing the scores of
the partial extension orders after fixing only the first n ? 1
symbols.
1766
i02
h
08
a
03
v
01
e
05
d
09
e
07
p
03
o
07
s
10
i
11
t
03
e
14
d
03
i
03
n
05
t
06
h
01
e
13
c
04
o
10
u
01
n
01
t
04
y
01
o
12
f
04
b
04
e
15
d
09
f
03
o
04
r
06
d
04
a
07
b
07
o
09
u
03
t
13
f
01
o
01
u
08
r
05
m
03
i
08
l
09
e
14
s
06
f
01
r
05
o
07
m
04
b
06
u
02
f
04
o
10
r
07
d
01
s
11
i
03
n
02
a
06
n
03
e
05
x
01
c
03
a
01
v
01
a
03
t
10
i
13
o
03
n
05
o
08
r
06
v
01
a
08
u
03
l
01
t
11
s
12
i
04
x
01
f
01
e
01
e
03
t
02
b
06
e
07
l
02
o
11
w
06
t
08
h
08
e
15
s
06
u
04
r
06
f
04
a
10
.
.
.
p
04
a
14
p
01
e
07
r
05
n
02
u
02
m
02
b
01
e
14
r
05
o
03
n
05
e
15
d
10
e
01
s
01
c
01
r
01
i
03
b
05
e
06
s
08
t
01
h
08
c
04
e
10
x
01
a
14
c
07
t
02
l
09
o
12
c
02
a
04
l
09
i
13
t
02
y
01
o
02
f
03
t
07
h
02
e
11
v
01
a
10
r
07
l
07
t
11
s
09
o
04
t
01
h
03
a
06
t
04
n
03
o
06
d
05
i
13
f
02
f
03
i
03
c
04
u
07
l
09
t
02
y
01
w
04
i
12
l
01
l
02
b
03
e
01
h
02
a
09
d
10
i
07
n
06
f
01
i
13
n
01
d
10
i
03
n
05
g
04
i
03
t
05
Table 2: Beginning and end of part two of the Beale cipher. Here we show a relabeled version of the ci-
pher, which encodes knowledge of the gold decipherment to assign reasonable names to all homophones.
The original cipher just consists of numbers.
orders on the Zodiac-408 cipher with just a beam
size of 26. With such a small beam size, the exten-
sion order plays a crucial role for a successful de-
cipherment: Depending on the choice of the differ-
ent weights w
n
we can observe decipherment runs
with 3 out of 54 correct mappings, up to 52 out
of 54 mappings correct. Even though the choice
of weights is somewhat arbitrary, we can see that
generally giving higher weights to higher n-gram
orders yields better results.
We use the weights w
8
1
=
(0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 3.0) for the
following experiments. It is interesting to com-
pare these weights to the perplexities of the
correct decipherment measured using different
n-gram orders (Table 5). However, at this point
we do not see any obvious connection between
perplexities and weights w
n
, and leave this as a
further research direction.
6 Experimental Evaluation
6.1 Zodiac Cipher
Using our new algorithm we are able to decipher
the Zodiac-408 with just a beam size of 26 and a
language model order of size 8. By keeping track
of the gold hypothesis while performing the beam
search, we can see that the gold decipherment in-
deed always remains within the top 26 scoring hy-
potheses. Our new algorithm is able to decipher
the Zodiac-408 cipher in less than 10s on a sin-
gle CPU, as compared to 48h of CPU time using
the previously published heuristic, which required
a beam size of several million. Solving a cipher
with such a small beam size can be seen as ?read-
ing off the solution?.
6.2 Beale Cipher
We apply our algorithm to the second part of the
Beale ciphers with a 8-gram language model.
Compared to the Zodiac-408, which has length
408 while having 54 different symbols (7.55 ob-
servations per symbol), part two of the Beale ci-
phers has length 762 while having 182 different
symbols (4.18 observations per symbol). Com-
pared to the Zodiac-408, this is both, in terms of
redundancy, as well as in size of search space, a
way more difficult cipher to break.
Here we run our algorithm with a beam size of
10M and achieve a decipherment accuracy of 157
out of 185 symbols correct yielding a symbol error
rate of less than 5.4%. The gold decipherment is
pruned out of the beam after 35 symbols have been
fixed.
We also ran our algorithm on the other parts
of the Beale ciphers: The first part has a length
520 and contains 299 different cipher symbols
(1.74 observations per symbol), while part three
has length 618 and has 264 symbols which is
2.34 observations per mapping. However, our al-
gorithm does not yield any reasonable decipher-
ments. Since length and number of symbols indi-
cate that deciphering these ciphers is again more
difficult than for part two, it is not clear whether
the other parts are not a homophonic substitution
cipher at all, or whether our algorithm is still not
good enough to find the correct decipherment.
7 Conclusion
We presented two extensions to the beam search
method presented in (Nuhn et al., 2012), that re-
duce the search effort to decipher the Zodiac-408
enormously. These improvements allow us to au-
tomatically decipher part two of the Beale ciphers.
To the best of our knowledge, this has not been
1767
done before. This algorithm might prove useful
when applied to word substitution ciphers and to
learning translations from monolingual data.
Acknowledgements
The authors thank Mark Kozek from the Depart-
ment of Mathematics at Whittier College for chal-
lenging us with a homophonic cipher he created.
Working on his cipher led to developing the meth-
ods presented in this paper.
References
Andrew J. Clark. 1998. Optimisation heuristics for
cryptology. Ph.D. thesis, Faculty of Information
Technology, Queensland University of Technology.
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
George W. Hart. 1994. To decode short cryptograms.
Communications of the Association for Computing
Machinery (CACM), 37(9):102?108, September.
John C. King. 1993. A reconstruction of the key to
beale cipher number two. Cryptologia, 17(3):305?
317.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Annual Meeting of the Assoc. for Computational
Linguistics, pages 1569?1576, Sofia, Bulgaria, Au-
gust.
Edwin Olson. 2007. Robust dictionary attack of
short simple substitution ciphers. Cryptologia,
31(4):332?342, October.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239?247, Portland, Oregon, June. Association for
Computational Linguistics.
James B Ward, Thomas Jefferson Beale, and Robert
Morriss. 1885. The Beale Papers.
1768
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 29?32,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Jane: Open Source Machine Translation System Combination
Markus Freitag
1
and Matthias Huck
2
and Hermann Ney
1
1
Lehrstuhl f?ur Informatik 6
2
School of Informatics
Computer Science Department University of Edinburgh
RWTH Aachen University 10 Crichton Street
D-52056 Aachen, Germany Edinburgh EH8 9AB, UK
{freitag,ney}@cs.rwth-aachen.de mhuck@inf.ed.ac.uk
Abstract
Different machine translation engines can
be remarkably dissimilar not only with re-
spect to their technical paradigm, but also
with respect to the translation output they
yield. System combination is a method for
combining the output of multiple machine
translation engines in order to take benefit
of the strengths of each of the individual
engines.
In this work we introduce a novel system
combination implementation which is in-
tegrated into Jane, RWTH?s open source
statistical machine translation toolkit. On
the most recent Workshop on Statisti-
cal Machine Translation system combi-
nation shared task, we achieve improve-
ments of up to 0.7 points in BLEU over
the best system combination hypotheses
which were submitted for the official eval-
uation. Moreover, we enhance our sys-
tem combination pipeline with additional
n-gram language models and lexical trans-
lation models.
1 Introduction
We present a novel machine translation system
combination framework which has been imple-
mented and released as part of the most recent ver-
sion of the Jane toolkit.
1
Our system combina-
tion framework has already been applied success-
fully for joining the outputs of different individual
machine translation engines from several project
partners within large-scale projects like Quaero
(Peitz and others, 2013), EU-BRIDGE (Freitag
and others, 2013), and DARPA BOLT. The com-
bined translation is typically of better quality than
1
Jane is publicly available under an open source non-
commercial license and can be downloaded from http://
www.hltpr.rwth-aachen.de/jane/ .
any of the individual hypotheses. The source code
of our framework has now been released to the
public.
We focus on system combination via confusion
network decoding. This basically means that we
align all input hypotheses from individual machine
translation (MT) engines together and extract a
combination as a new output. For our baseline
algorithm we only need the first best translation
from each of the different MT engines, without
any additional information. Supplementary to the
baseline models integrated into our framework, we
optionally allow for utilization of n-gram language
models and IBM-1 lexicon models (Brown et al.,
1993), both trained on additional training corpora
that might be at hand.
We evaluate the Jane system combination
framework on the latest official Workshop on
Statistical Machine Translation (WMT) system
combination shared task (Callison-Burch et al.,
2011). Many state-of-the-art MT system combi-
nation toolkits have been evaluated on this task,
which allows us to directly compare the results ob-
tained with our novel Jane system combination
framework with the best known results obtained
with other toolkits.
The paper is structured as follows: We com-
mence with giving a brief outline of some related
work (Section 2). In Section 3 we describe the
techniques which are implemented in the Jane
MT system combination framework. The exper-
imental results are presented and analyzed in Sec-
tion 4. We conclude the paper in Section 5.
2 Related Work
The first application of system combination to MT
has been presented by Bangalore et al. (2001).
They used a multiple string alignment (MSA) ap-
proach to align the hypotheses together and built
a confusion network from which the system com-
bination output is determined using majority vot-
29
0
1
w
ill
:w
ill
/-0
.3
*E
PS
*:
*E
PS
*/
-0
.7
2
co
nt
ai
n:
co
nt
ai
n/
-0
.2
co
m
pr
ise
:c
om
pr
ise
/-0
.1
*E
PS
*:
*E
PS
*/
-0
.7
3
*E
PS
*:
*E
PS
*/
-0
.3
th
e:
th
e/
-0
.6
co
m
pr
isi
ng
:c
om
pr
isi
ng
/-0
.1
4
*E
PS
*:
*E
PS
*/
-0
.9
an
:a
n/
-0
.1
5
iso
la
te
d:
iso
la
te
d/
-0
.8
*E
PS
*:
*E
PS
*/
-0
.2
6
cd
na
:c
dn
a/
-1
7
*E
PS
*:
*E
PS
*/
-0
.4
lib
ra
ry
:li
br
ar
y/
-0
.6
Figure 1: Scored confusion network. *EPS* denotes the empty word, red arcs highlight the shortest path.
ing and an additional language model. Matusov
et al. (2006) proposed an alignment based on the
GIZA
++
toolkit which introduced word reordering
not present in MSA, and Sim et al. (2007) used
alignments produced by TER scoring (Snover et
al., 2006). Extensions of the last two are based on
hidden Markov models (He et al., 2008), inversion
transduction grammars (Karakos et al., 2008), or
METEOR (Heafield and Lavie, 2010).
3 The Jane MT System Combination
Framework
In this section we describe the techniques for MT
system combination which we implemented in the
Jane toolkit.
2
We first address the generation of a
confusion network from the input translations. For
that we need a pairwise alignment between all in-
put hypotheses. We then present word reordering
mechanisms, the baseline models, and additional
advanced models which can be applied for system
combination using Jane. The system combina-
tion decoding step basically involves determining
the shortest path through the confusion network
based on several model scores from this network.
3.1 Confusion Network
A confusion network represents all different com-
bined translations we can generate from the set of
provided input hypotheses. Figure 1 depicts an ex-
ample of a confusion network. A word alignment
between all pairs of input hypotheses is required
for generating a confusion network. For conve-
nience, we first select one of the input hypotheses
as the primary hypothesis. The primary hypothesis
then determines the word order and all remaining
hypotheses are word-to-word aligned to the given
word order.
To generate a meaningful confusion network,
we should adopt an alignment which only al-
lows to switch between words which are syn-
onyms, misspellings, morphological variants or
on a higher level paraphrases of the words from
the primary hypothesis. In this work we use
METEOR alignments. METEOR (Denkowski
2
Practical usage aspects are explained in the man-
ual: http://www.hltpr.rwth-aachen.de/jane/
manual.pdf
and Lavie, 2011) was originally designed to re-
order a translation for scoring and has a high pre-
cision. The recall is lower because synonyms
which are not in the METEOR database or punc-
tuation marks like ?!? and ??? are not aligned
to each other. For our purposes, we augment the
METEOR paraphrase table with entries like ?.|!?,
?.|??, or ?the|a?.
Figure 2 shows an example METEOR hypothe-
sis alignment. The primary hypothesis ?isolated
cdna lib? determines the word order. An entry
?a|b? means that word ?a? from a secondary hy-
pothesis has been aligned to word ?b? from the
primary one. ?*EPS*? is the empty word and
thus an entry ?*EPS*|b? means that no word could
be aligned to the primary hypothesis word ?b?.
?a|*EPS*? means that the word ?a? has not been
aligned to any word from the primary hypothesis.
After producing the alignment information, we
can build the confusion network. Now, we are able
to not only extract the original primary hypoth-
esis from the confusion network but also switch
words from the primary hypothesis to words from
any secondary hypothesis (also the empty word)
or insert words or sequences of words.
In the final confusion network, we do not stick
to one hypothesis as the primary system. For m in-
put hypotheses we build m different confusion net-
works, each having a different system as primary
system. The final confusion network is a union of
all m networks.
3
The most straightforward way to obtain a com-
bined hypothesis from a confusion network is to
extract it via majority voting. For example, in
the first column in Figure 3, ?the? has been seen
three times, but the translation options ?a? and
?an? have each been seen only once. By means
of a straight majority vote we would extract ?the?.
As the different single system translations are of
varying utility for system combination, we assign
a system weight to each input hypothesis. The sys-
tem weights are set by optimizing scaling factors
for binary system voting features (cf. Section 3.3).
We employ some more weighted baseline features
3
Jane?s implementation for building confusion networks
is based on the OpenFST library (Allauzen et al., 2007).
30
the|*EPS* isolated|isolated cdna|cdna *EPS*|lib
a|*EPS* isolated|isolated cdna|cdna lib|lib
an|*EPS* isolated|isolated cdna|cdna lib|lib
the|*EPS* *EPS*|isolated cdna|cdna *EPS*|lib
the|*EPS* *EPS*|isolated cdna|cdna lib|lib
Figure 2: Alignment result after running
METEOR. *EPS* denotes the empty word.
*EPS* isolated cdna lib
the isolated cdna *EPS*
a isolated cdna lib
an isolated cdna lib
the *EPS* cdna lib
the *EPS* cdna *EPS*
the isolated cdna lib
Figure 3: Majority vote on aligned words. The last
line is the system combination output.
and additional models (cf. Section 3.4) in the deci-
sion process. In Figure 1 we scored the confusion
network with some system weights and used the
shortest path algorithm to find the hypothesis with
the highest score (the hypothesis along the path
highlighted in red).
3.2 Word Reordering
Many words from secondary hypotheses can be
unaligned as they have no connection to any words
of the primary hypothesis. However, words from
different secondary systems could be related to
each other. In order to account for these relations
and to give the words from the secondary hypothe-
ses a higher chance to be present in the combined
output, we introduce some simple word reordering
mechanisms.
We rank the hypotheses according to a language
model trained on all input hypotheses. We initial-
ize the confusion network with the sentence from
the primary system. During the generation of the
confusion network we align the hypotheses con-
secutively into the confusion network via the fol-
lowing procedure:
? If a word w
i
from hypothesis A has a relation
to a word v
j
of the primary hypothesis, we
insert it as a new translation alternative to v
j
.
? If w
i
has no relation to the primary, but to
a word u
k
from a secondary hypothesis in
the confusion network, we insert w
i
as a new
translation alternative to u
k
.
? Otherwise we insert w
i
in front of the previ-
ous inserted word w
i?1
of hypothesis A. The
new position gets an epsilon arc for the pri-
mary and all unrelated secondary systems.
3.3 Baseline Models
Once we have the final confusion network, we
want to adopt models which are valuable features
to score the different translation options. In our
implementation we use the following set of stan-
dard models:
m binary system voting features For each word
the voting feature for system i (1? i?m) is 1
iff the word is from system i, otherwise 0.
Binary primary system feature A feature that
marks the primary hypothesis.
LM feature 3-gram language model trained on
the input hypotheses.
Word penalty Counts the number of words.
3.4 Additional Models
The Jane system combination toolkit also pro-
vides the possibility to utilize some additional
models for system combination. For the current
release we integrated the optional usage of the fol-
lowing additional models:
Big LM A big language model trained on larger
monolingual target-side corpora.
IBM-1 Source-to-target and target-to-source
IBM-1 lexical translation models obtained
from bilingual training data.
4 Experimental Results
All experiments are conducted on the latest offi-
cial WMT system combination shared task.
4
We
exclusively employ resources which were permit-
ted for the constrained track of the task in all our
setups. The big LM was trained on News Com-
mentary and Europarl data. As tuning set we
used newssyscombtune2011, as test set we used
newssyscombtest2011. Feature weights have been
optimized with MERT (Och, 2003). Table 1 con-
tains the empirical results (truecase). For all four
language pairs we achieve improvements over the
best 2011 evaluation system combination submis-
sion either in BLEU or TER. We get the highest
improvement of 0.7 points in BLEU for es?en
when adding both the big LM and IBM-1 features.
Adding the big LM over the baseline enhances
the translation quality for all four language pairs.
Adding IBM-1 lexicon models on top of the big
LM is of marginal or no benefit for most language
4
The most recent system combination shared task that
has been organized as part of the WMT evaluation cam-
paign took place in 2011. http://www.statmt.org/
wmt11/system-combination-task.html
31
Table 1: Experimental results on the WMT system combination tasks (newssyscombtest2011).
system cz?en de?en es?en fr?en
BLEU TER BLEU TER BLEU TER BLEU TER
best single system 28.7 53.4 23.0 59.5 28.9 51.2 29.4 52.0
best 2011 evaluation syscomb 28.8 55.2 25.1 57.4 32.4 49.9 31.3 50.1
Jane syscomb baseline 28.8 53.6 24.7 57.6 32.7 50.3 31.3 50.3
Jane syscomb + big LM 29.0 54.5 25.0 57.3 32.9 50.3 31.4 50.0
Jane syscomb + big LM + IBM-1 29.0 54.5 25.0 57.3 33.1 50.0 31.5 50.1
pairs, but at least provides slight improvements for
es?en.
5 Conclusion
RWTH?s open source machine translation toolkit
Jane now includes a state-of-the-art system com-
bination framework. We found that the Jane sys-
tem combination performs on a similar level or
better than the best evaluation system combina-
tion submissions on all WMT 2011 system com-
bination shared task language pairs (with English
as target language). We furthermore presented the
effects of integrating a big n-gram language model
and of lexical features from IBM-1 models.
Acknowledgements
This material is based upon work supported by
the DARPA BOLT project under Contract No.
HR0011- 12-C-0015. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n
o
287658.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
General and Efficient Weighted Finite-State Trans-
ducer Library. In Jan Holub and Jan Zd?arek, edi-
tors, Implementation and Application of Automata,
volume 4783 of Lecture Notes in Computer Science,
pages 11?23. Springer Berlin Heidelberg.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing Consensus Translation
from Multiple Machine Translation Systems. In
Proc. of ASRU, pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proc. of WMT, pages 22?64.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proc. of WMT, pages 85?91.
Markus Freitag et al. 2013. EU-BRIDGE MT: Text
Translation of Talks in the EU-BRIDGE Project. In
Proc. of IWSLT.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-
HMM-based Hypothesis Alignment for Combining
Outputs from Machine Translation Systems. In
Proc. of EMNLP, pages 98?107.
Kenneth Heafield and Alon Lavie. 2010. Combining
Machine Translation Output with Open Source: The
Carnegie Mellon Multi-Engine Machine Translation
Scheme. The Prague Bulletin of Mathematical Lin-
guistics, 93:27?36.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. of ACL: Short Papers, pages 81?84.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Proc. of EACL, pages
33?40.
Franz J. Och. 2003. Minimum Error Rate Training for
Statistical Machine Translation. In Proc. of ACL,
pages 160?167.
Stephan Peitz et al. 2013. Joint WMT 2013 Submis-
sion of the QUAERO Project. In Proc. of WMT,
pages 185?192.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007.
Consensus Network Decoding for Statistical Ma-
chine Translation System Combination. In
Proc. of ICASSP, volume 4, pages 105?108.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA, pages 223?231.
32
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174?179,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Simple and Effective Approach for Consistent Training of Hierarchical
Phrase-based Translation Models
Stephan Peitz
1
and David Vilar
2
and Hermann Ney
1
1
Lehrstuhl f?ur Informatik 6
Computer Science Department
2
Pixformance GmbH
RWTH Aachen University D-10587 Berlin, Germany
D-52056 Aachen, Germany david.vilar@gmail.com
{peitz,ney}@cs.rwth-aachen.de
Abstract
In this paper, we present a simple ap-
proach for consistent training of hierarchi-
cal phrase-based translation models. In
order to consistently train a translation
model, we perform hierarchical phrase-
based decoding on training data to find
derivations between the source and tar-
get sentences. This is done by syn-
chronous parsing the given sentence pairs.
After extracting k-best derivations, we
reestimate the translation model proba-
bilities based on collected rule counts.
We show the effectiveness of our proce-
dure on the IWSLT German?English and
English?French translation tasks. Our
results show improvements of up to 1.6
points BLEU.
1 Introduction
In state of the art statistical machine translation
systems, the translation model is estimated by fol-
lowing heuristic: Given bilingual training data,
a word alignment is trained with tools such as
GIZA
++
(Och and Ney, 2003) or fast align (Dyer
et al., 2013). Then, all valid translation pairs are
extracted and the translation probabilities are com-
puted as relative frequencies (Koehn et al., 2003).
However, this extraction method causes several
problems. First, this approach does not consider,
whether a translation pair is extracted from a likely
alignment or not. Further, during the extraction
process, models employed in decoding are not
considered.
For phrase-based translation, a successful ap-
proach addressing these issues is presented in
(Wuebker et al., 2010). By applying a phrase-
based decoder on the source sentences of the train-
ing data and constraining the translations to the
corresponding target sentences, k-best segmenta-
tions are produced. Then, the phrases used for
these segmentations are extracted and counted.
Based on the counts, the translation model prob-
abilities are recomputed. To avoid over-fitting,
leave-one-out is applied.
However, for hierarchical phrase-based transla-
tion an equivalent approach is still missing.
In this paper, we present a simple and effec-
tive approach for consistent reestimation of the
translation model probabilities in a hierarchical
phrase-based translation setup. Using a heuristi-
cally extracted translation model as starting point,
the training data are parsed bilingually. From the
resulting hypergraphs, we extract k-best deriva-
tions and the rules applied in each derivation. This
is done with a top-down k-best parsing algorithm.
Finally, the translation model probabilities are re-
computed based on the counts of the extracted
rules. In our procedure, we employ leave-one-out
to avoid over-fitting. Further, we consider all mod-
els which are used in translation to ensure a con-
sistent training.
Experimental results are presented on the
German?English and English?French IWSLT
shared machine translation task (Cettolo et al.,
2013). We are able to gain improvements of up to
1.6% BLEU absolute and 1.4% TER over a com-
petitive baseline. On all tasks and test sets, the
improvements are statistically significant with at
least 99% confidence.
The paper is structured as follow. First, we re-
vise the state of the art hierarchical phrase-based
extraction and translation process. In Section 3,
we propose our training procedure. Finally, ex-
perimental results are given in Section 4 and we
conclude with Section 5.
2 Hierarchical Phrase-based Translation
In hierarchical phrase-based translation (Chiang,
2005), discontinuous phrases with ?gaps? are
allowed. The translation model is formalized
as a synchronous context-free grammar (SCFG)
174
and consists of bilingual rules, which are based
on bilingual standard phrases and discontinuous
phrases. Each bilingual rule rewrites a generic
non-terminal X into a pair of strings
?
f and e?
with both terminals and non-terminals in both lan-
guages
X ? ?
?
f, e??. (1)
In a standard hierarchical phrase-based translation
setup, obtaining these rules is based on a heuristic
extraction from automatically word-aligned bilin-
gual training data. Just like in the phrase-based
approach, all bilingual rules of a sentence pair
are extracted given an alignment. The standard
phrases are stored as lexical rules in the rule set.
In addition, whenever a phrase contains a sub-
phrase, this sub-phrase is replaced by a generic
non-terminal X . With these hierarchical phrases
we can define the hierarchical rules in the SCFG.
The rule probabilities which are in general defined
as relative frequencies are computed based on the
joint counts C(X ? ?
?
f, e??) of a bilingual rule
X ? ?
?
f, e??
p
H
(
?
f |e?) =
C(X ? ?
?
f, e??)
?
?
f
?
C(X ? ?
?
f
?
, e??)
. (2)
The translation probabilities are computed in
source-to-target as well as in target-to-source di-
rection. In the translation processes, these proba-
bilities are integrated in the log-linear combination
among other models such as a language model,
word lexicon models, word and phrase penalty and
binary features marking hierarchical phrases, glue
rule and rules with non-terminals at the bound-
aries.
The translation process of hierarchical phrase-
based approach can be considered as parsing prob-
lem. Given an input sentence in the source lan-
guage, this sentence is parsed using the source lan-
guage part of the SCFG. In this work, we perform
this step with a modified version of the CYK+ al-
gorithm (Chappelier and Rajman, 1998). The out-
put of this algorithm is a hypergraph, which rep-
resents all possible derivations of the input sen-
tence. A derivation represents an application of
rules from the grammar to generate the given in-
put sentence. Using the the associated target part
of the applied rule, for each derivation a transla-
tion can be constructed. In a second step, the lan-
guage model score is incorporated. Given the hy-
pergraph, this is done with the cube pruning algo-
rithm presented in (Chiang, 2007).
3 Translation Model Training
We propose following pipeline for consistent hi-
erarchical phrase-based training: First we train a
word alignment, from which the baseline trans-
lation model is extracted as described in the pre-
vious section. The log-linear parameter weights
are tuned with MERT (Och, 2003) on a develop-
ment set to produce the baseline system. Next,
we perform decoding on the training data. As the
translations are constrained to the given target sen-
tences, we name this step forced decoding in the
following. Details are given in the next subsection.
Given the counts C
FD
(X ? ?
?
f, e??) of the rules,
which have been applied in the forced decoding
step, the translation probabilities p
FD
(
?
f |e?) for the
translation model are recomputed:
p
FD
(
?
f |e?) =
C
FD
(X ? ?
?
f, e??)
?
?
f
?
C
FD
(X ? ?
?
f
?
, e??)
. (3)
Finally, using the translation model with the
reestimated probabilities, we retune the log-linear
parameter weights and obtain our final system.
3.1 Forced Decoding
In this section, we describe the forced decoding
for hierarchical phrase-based translation in detail.
Given a sentence pair of the training data, we
constrain the translation of the source sentence to
produce the corresponding target sentence. For
this constrained decoding process, the language
model score is constant as the translation is fixed.
Hence, the incorporation of the a language model
is not needed. This results in a simplification of
the decoding process as we do not have to employ
the cube pruning algorithm as described in the pre-
vious section. Consequently, forced decoding for
hierarchical phrase-based translation is equivalent
to synchronous parsing of the training data. Dyer
(2010) has described an approach to reduce the
average-case run-time of synchronous parsing by
splitting one bilingual parse into two successive
monolingual parses. We adopt this method and
first parse the source sentence and then the target
sentence with CYK+.
If the given sentence pair has been parsed suc-
cessfully, we employ a top-down k-best parsing
algorithm (Chiang and Huang, 2005) on the re-
sulting hypergraph to find the k-best derivations
between the given source and target sentence. In
this step, all models of the translation process are
175
included (except for the language model). Further,
leave-one-out is applied to counteract overfitting.
Note, that the model weights of the baseline sys-
tem are used to perform forced decoding.
Finally, we extract and count the rules which
have been applied in the derivations. These counts
are used to recompute the translation probabilities.
3.2 Recombination
In standard hierarchical phrase-based decoding,
partial derivations that are indistinguishable from
each other are recombined. In (Huck et al., 2013)
two schemes are presented. Either derivations that
produce identical translations or derivations with
identical language model context are recombined.
As in forced decoding the translation is fixed and
a language model is missing, both schemes are not
suitable.
However, a recombination scheme is necessary
to avoid derivations with the same application
of rules. Further, recombining such derivations
increases simultaneously the amounts of consid-
ered derivations during k-best parsing. Given two
derivations with the same set of applied rules, the
order of application of the rules may be different.
Thus, we propose following scheme for recom-
bining derivations in forced decoding: Derivations
that produce identical sets of applied rules are re-
combined. Figure 1 shows an example for k = 3.
Employing the proposed scheme, derivations d
1
and d
2
are recombined since both share the same
set of applied rules ({r
1
, r
3
, r
2
}).
d
1
: {r
1
, r
3
, r
2
}
d
2
: {r
3
, r
2
, r
1
}
d
3
: {r
4
, r
5
, r
1
, r
2
}
(a)
d
1
: {r
1
, r
3
, r
2
}
d
3
: {r
4
, r
5
, r
1
, r
2
}
d
4
: {r
6
, r
5
, r
2
, r
3
}
(b)
Figure 1: Example search space before (a) and af-
ter (b) applying recombination.
4 Experiments
4.1 Setup
The experiments were carried out on the IWSLT
2013 German?English shared translation task.
1
1
http://www.iwslt2013.org
German English English French
Sentences 4.32M 5.23M
Run. Words 108M 109M 133M 147M
Vocabulary 836K 792K 845K 888K
Table 1: Statistics for the bilingual training
data of the IWSLT 2013 German?English and
English?French task.
It is focusing the translation of TED talks. Bilin-
gual data statistics are given in Table 1. The base-
line system was trained on all available bilingual
data and used a 4-gram LM with modified Kneser-
Ney smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998), trained with the SRILM toolkit
(Stolcke, 2002). As additional data sources for the
LM we selected parts of the Shuffled News and
LDC English Gigaword corpora based on cross-
entropy difference (Moore and Lewis, 2010). In
all experiments, the hierarchical search was per-
formed as described in Section 2.
To confirm the efficacy of our approach, addi-
tional experiments were run on the IWSLT 2013
English?French task. Statistics are given in Ta-
ble 1.
The training pipeline was set up as described
in the previous section. Tuning of the log-linear
parameter weights was done with MERT on a pro-
vided development set. As optimization criterion
we used BLEU (Papineni et al., 2001).
Forced decoding was performed on the TED
talks portion of the training data (?140K sen-
tences). In both tasks, around 5% of the sentences
could not be parsed. In this work, we just skipped
those sentences.
We report results in BLEU [%] and TER [%]
(Snover et al., 2006). All reported results are av-
erages over three independent MERT runs, and
we evaluated statistical significance with MultE-
val (Clark et al., 2011).
4.2 Results
Figure 2 shows the performance of setups us-
ing translation models with reestimated translation
probabilities. The setups vary in the k-best deriva-
tion size extracted in the forced decoding (fd) step.
Based on the performance on the development set,
we selected two setups with k = 500 using leave-
one-out (+l1o) and k = 750 without leave-one-
out (-l1o). Table 2 shows the final results for
the German?English task. Performing consistent
translation model training improves the translation
176
dev
*
eval11 test
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
baseline 33.1 46.8 35.7 44.1 30.5 49.7
forced decoding -l1o 33.2 46.3 36.3 43.4 31.2 48.8
forced decoding +l1o 33.6 46.2 36.6 43.0 31.8 48.3
Table 2: Results for the IWSLT 2013 German?English task. The development set used for MERT is
marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the
baseline are printed in boldface.
dev
*
eval11 test
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
baseline 28.1 55.7 37.5 42.7 31.7 49.5
forced decoding +l1o 28.8 55.0 39.1 41.6 32.4 49.0
Table 3: Results for the IWSLT 2013 English?French task. The development set used for MERT is
marked with an asterisk (*). Statistically significant improvements with at least 99% confidence over the
baseline are printed in boldface.
 31.5
 32
 32.5
 33
 33.5
 34
 1  10  100  1000  10000
B
L
E
U
[
%
]
k
dev fd +l1odev fd -l1odev baseline
Figure 2: BLEU scores on the IWSLT
German?English task of setups using trans-
lation models trained with different k-best
derivation sizes. Results are reported on dev with
(+l1o) and without leave-one-out (-l1o).
quality on all test sets significantly. We gain an
improvement of up to 0.7 points in BLEU and 0.9
points in TER. Applying leave-one-out results in
an additional improvement by up to 0.4 % BLEU
and 0.5 % TER. The results for English?French
are given in Table 3. We observe a similar im-
provement by up to 1.6 % BLEU and 1.1 % TER.
The improvements could be the effect of do-
main adaptation since we performed forced decod-
ing on the TED talks portion of the training data.
Thus, rules which were applied to decode the in-
domain data might get higher translation probabil-
ities.
Furthermore, employing leave-one-out seems to
avoid overfitting as the average source rule length
in training is reduced from 5.0 to 3.5 (k = 500).
5 Conclusion
We have presented a simple and effective approach
for consistent training of hierarchical phrase-based
translation models. By reducing hierarchical de-
coding on parallel training data to synchronous
parsing, we were able to reestimate the trans-
lation probabilities including all models applied
during the translation process. On the IWSLT
German?English and English?French tasks, the
final results show statistically significant improve-
ments of up to 1.6 points in BLEU and 1.4 points
in TER.
Our implementation was released as part of Jane
(Vilar et al., 2010; Vilar et al., 2012; Huck et al.,
2012; Freitag et al., 2014), the RWTH Aachen
University open source statistical machine trans-
lation toolkit.
2
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreements no 287658 and no 287755.
2
http://www.hltpr.rwth-aachen.de/jane/
177
References
Mauro Cettolo, Jan Nieheus, Sebastian St?uker, Luisa
Bentivogli, and Marcello Federico. 2013. Report on
the 10th iwslt evaluation campaign. In Proc. of the
International Workshop on Spoken Language Trans-
lation, Heidelberg, Germany, December.
J.-C. Chappelier and M. Rajman. 1998. A general-
ized CYK algorithm for parsing stochastic CFG. In
Proceedings of the First Workshop on Tabulation in
Parsing and Deduction, pages 133?137, April.
Stanley F. Chen and Joshuo Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
David Chiang and Liang Huang. 2005. Better k-best
Parsing. In Proceedings of the 9th Internation Work-
shop on Parsing Technologies, pages 53?64, Octo-
ber.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 263?270,
Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176?181, Portland, Oregon,
June.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A Simple, Fast, and Effective Reparameter-
ization of IBM Model 2. In Proceedings of NAACL-
HLT, pages 644?648, Atlanta, Georgia, June.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In In Proc. of HLT-
NAACL.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open source machine translation sys-
tem combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April. To appear.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and Her-
mann Ney. 2013. A performance study of cube
pruning for large-scale hierarchical machine transla-
tion. In Proceedings of the NAACL 7th Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 29?38, Atlanta, Georgia, USA, June.
Reinerd Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processingw, volume 1,
pages 181?184, May.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 127?133, Edmonton, Alberta.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220?224, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
178
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
179
Word-Level Confidence Estimation for
Machine Translation
Nicola Ueffing?
RWTH Aachen University
Hermann Ney?
RWTH Aachen University
This article introduces and evaluates several different word-level confidence measures for ma-
chine translation. These measures provide a method for labeling each word in an automatically
generated translation as correct or incorrect. All approaches to confidence estimation presented
here are based on word posterior probabilities. Different concepts of word posterior probabilities
as well as different ways of calculating them will be introduced and compared. They can be
divided into two categories: System-based methods that explore knowledge provided by the
translation system that generated the translations, and direct methods that are independent
of the translation system. The system-based techniques make use of system output, such as
word graphs or N-best lists. The word posterior probability is determined by summing the
probabilities of the sentences in the translation hypothesis space that contains the target word.
The direct confidence measures take other knowledge sources, such as word or phrase lexica,
into account. They can be applied to output from nonstatistical machine translation systems
as well.
Experimental assessment of the different confidence measures on various translation tasks
and in several language pairs will be presented. Moreover, the application of confidence measures
for rescoring of translation hypotheses will be investigated.
1. Introduction
The work presented in this article deals with confidence estimation for machine trans-
lation (MT). Because sentences generated by a machine translation system are often
incorrect but may contain correct substrings, a method for identifying these correct
substrings and finding possible errors is desirable. For this purpose, each word in
the generated target sentence is assigned a value expressing the confidence that it
is correct.
Confidence measures have been extensively studied for speech recognition. Only
recently have researchers started to investigate confidence measures for machine trans-
lation (Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003; Blatz et al
2004; Quirk 2004). In this article, we will develop a sound theoretical framework for
? Now at National Research Council Canada, Interactive Language Technologies Group, Gatineau, Que?bec
J8P 3G5, Canada. E-mail: nicola.ueffing@nrc.gc.ca.
? Lehrstuhl fu?r Informatik VI, Computer Science Department, D-52056 Aachen, Germany. E-mail:
ney@cs.rwth-aachen.de.
Submission received: 7 March 2006; revised submission received: 30 September 2006; accepted for publication:
3 October 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
calculating and evaluating word confidence measures. Possible applications of confi-
dence measures include:
 marking words with low confidence as potential errors for post-editing
 improving translation prediction accuracy in TransType-style interactive
machine translation (Gandrabur and Foster 2003; Ueffing and Ney 2005a)
 combining output from different machine translation systems: Hypotheses
with low confidence can be discarded before selecting one of the system
translations (Akiba et al 2004), or the word confidence scores can be used
in the generation of new hypotheses from the output of different systems
(Jayaraman and Lavie 2005), or the sentence confidence value can be
employed for reranking (Blatz et al 2003).
The article is organized as follows: In Section 2, we briefly review the statistical
approach to machine translation. The phrase-based translation system, which serves as
the basis for one of the direct confidence measures, will be presented. Section 3 gives an
overview of related work on confidence estimation for machine translation. Moreover,
word posterior probabilities will be introduced, and we will explain how they can be
used as word-level confidence measures. In Section 4, we describe so-called system-
based methods for confidence estimation, which make use of the output of a statistical
machine translation system, such as word graphs or N-best lists. In Section 5, we present
confidence measures based on direct models. The combination of several confidence
measures into one is described in Section 6. Experimental evaluation and comparison
of the different confidence measures is provided in Section 7. Section 8 deals with the
rescoring of translation hypotheses using confidence measures. The article concludes in
Section 9.
2. Statistical Machine Translation
2.1 General
In statistical machine translation (SMT), the translation is modeled as a decision process:
Given a source string f J1 = f1 . . . fj . . . fJ, we seek the target string e
I
1 = e1 . . . ei . . . eI with
maximal posterior probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1 | f
J
1 )
}
= argmax
I,eI1
{
Pr( f J1 | eI1) ? Pr(eI1)
}
(1)
Through this decomposition of the probability, we obtain two knowledge sources:
the translation model Pr( f J1 | eI1) and the language model Pr(eI1). Both can be mod-
eled independently of each other. The translation model is responsible for linking the
source string f J1 and the target string e
I
1. It captures the semantics of the sentence.
The target language model captures the well-formedness of the syntax in the target
language.
Nowadays, most state-of-the-art SMT systems are based on bilingual phrases (Och,
Tillmann, and Ney 1999; Koehn, Och, and Marcu 2003; Tillmann 2003; Bertoldi et al
2004; Vogel et al 2004; Zens and Ney 2004; Chiang 2005). A more detailed description of
10
Ueffing and Ney Word-Level Confidence Estimation for MT
a phrase-based approach to statistical machine translation will be given in the following
section.
2.2 Review of the Phrase-Based Translation System
For the confidence measures which will be introduced in Section 5.1, we use a state-
of-the-art phrase-based translation approach as described in Zens and Ney (2004).
The key elements of this translation approach are bilingual phrases. Note that these
phrases are sequences of words in the two languages and not necessarily phrases in
the linguistic sense. The bilingual phrases are extracted from a word-aligned bilingual
training corpus.
In this translation approach, the posterior probability Pr(eI1 | f
J
1 ) is modeled directly
using a weighted log-linear combination of a language model, a phrase translation
model, and a word-based lexicon model. The translation models are used for both
directions: p( f | e) and p(e | f ). Additionally, a word penalty and a phrase penalty are
applied. With the exception of the language model, all models can be considered as
within-phrase models as they depend only on a single phrase pair, but not on the context
outside the phrase.
In the following, we will present the generation criterion for the phrase-based trans-
lation approach. This will be done for a monotone search in order to keep the equations
simple. The extension to the non-monotone case is straightforward. Let ( jK0 , i
K
0 ) be a
segmentation of the source sentence into phrases, where jk?1 < jk and ik?1 < ik for
k = 1, . . . , K. The corresponding (bilingual) phrase pairs are denoted as
( f?k, e?k) = ( f
jk
jk?1+1
, eikik?1+1), k = 1, . . . , K
Assume a trigram language model. The phrase-based approach to SMT is then ex-
pressed by the following equation:
e?I?1 = argmax
jK0 ,i
K
0 ,I,e
I
1
{ I
?
i=1
[
c1 ? p(ei | ei?1i?2)?1
]
?
K
?
k=1
[
c2 ? p( f?k | e?k)?2 ? p(e?k | f?k)?3 ? (2)
?
jk
?
j=jk?1+1
p( fj | e?k)?4 ?
ik
?
i=ik?1+1
p(ei | f?k)?5
]
}
where p( f?k | e?k) and p(e?k | f?k) are the phrase lexicon models in both translation directions.
The phrase translation probabilities are computed as a log-linear interpolation of the
relative frequencies and the IBM model 1 probability. The single word?based lexicon
models are denoted as p( fj | e?k) and p(ei | f?k), respectively. p( fj | e?k) is defined as the
IBM model 1 probability of fj over the whole phrase e?k, and p(ei | f?k) is the inverse
model, respectively. c1 is the so-called word penalty, and c2 is the phrase penalty,
assigning constant costs to each target language word/phrase. The language model
is a trigram model with modified Kneser?Ney discounting and interpolation (Stolcke
2002). The search determines the target sentence and segmentation that maximize the
objective function.
11
Computational Linguistics Volume 33, Number 1
As Equation (2) shows, the sub-models are combined via weighted log-linear in-
terpolation. The model scaling factors ?1, . . . , ?5 and the word and phrase penalties
are optimized with respect to some evaluation criterion (Och 2003) such as BLEU
score.
The phrase-based translation model will be needed later to define the different
confidence measures. We therefore introduce the following notation: Let QPM( f?k, e?k) be
the score of the phrase pair, which consists of the phrase penalty c2, the phrase lexicon
scores, and the two word lexicon model scores (see Equation (2)):
QPM( f?k, e?k) := c2 ? p( f?k | e?k)?2 ? p(e?k | f?k)?3 ?
jk
?
j=jk?1+1
p( fj | e?k)?4 ?
ik
?
i=ik?1+1
p(ei | f?k)?5 (3)
3. Confidence Measures for MT
3.1 Related Work
In many areas of natural language processing, confidence measures have scarcely
been investigated. The exception is automatic speech recognition, where an exten-
sive amount of research on the topic exists. Confidence measures are widely used in
this area?for example, in dialogue systems and in unsupervised training. Recently,
researchers have started to investigate confidence measures for machine translation
(Blatz et al 2003, 2004; Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003;
Quirk 2004; Sanchis 2004). This section gives an overview of confidence estimation for
machine translation on the word level as well as the sentence level and discusses its
applications.
The first work that studied confidence estimation for statistical machine transla-
tion was Gandrabur and Foster (2003). Their confidence measures consist of a com-
bination of different features in a neural network. The confidence is estimated for a
sequence of up to four words in an interactive machine translation environment. The
probability of being a correct extension of a given sentence prefix is computed for this
word sequence. The authors report significant improvement in quality of the predicted
translations.
In 2003, a team at the yearly summer workshop at the Center for Language and
Speech Processing (CLSP) at Johns Hopkins University, Baltimore, MD, developed
confidence measures for machine translation. The combination of several confidence
features using neural networks and a naive Bayes classifier was investigated. The
workshop team studied confidence estimation on the word level as well as on the sen-
tence level, though the focus was on the sentence level. The features applied included
new features as well as those that had previously been developed by team members
(Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003). Among them were also
some of the word posterior probabilities, which will be presented here. Additionally,
heuristic and semantic features were studied. For a description of the features and
results, see Blatz et al (2003, 2004).
Following the work of the summer workshop team, Quirk (2004) presented an
investigation of different approaches to sentence-level confidence estimation. A set of
features is computed for each sentence generated by an MT system, and these features
are combined using several different methods: modified linear regression, neural nets,
support vector machines, and decision trees. Many of the sentence features are similar to
12
Ueffing and Ney Word-Level Confidence Estimation for MT
those presented in Blatz et al (2003); the others are specific to the underlying MT system
that generated the translations. Quirk (2004) also investigated the use of manually
tagged data for training the confidence measures. The author found that using a small
amount of manually labeled training data yields better performance than using large
quantities of automatically labeled data.
Akiba et al (2004) reported the application of confidence measures to the selection
of output on N-best lists produced by different MT systems. Word-level confidence mea-
sures, namely the rank-weighted sum as described in Section 4.1 (and first introduced
in Ueffing, Macherey and Ney [2003]), are used to discard low-quality system output
before selecting a translation from the various MT systems.
Zens and Ney (2006) presented an extension of the word posterior probabili-
ties presented in this article: Posterior probabilities are calculated not only on the
word level, but also for n-grams, and are successfully applied to the rescoring of MT
hypotheses.
3.2 Word Posterior Probabilities
The confidence of a target word can be expressed by its posterior probability, that is, the
probability of the word occurring in the target sentence, given the source sentence. Word
posterior probabilities are the basis of all approaches to confidence estimation presented
here. The following explains how they can be determined. The different methods can be
classified into two categories: system-based methods, which make use of system output
such as word graphs or N-best lists; and direct methods, which use external knowledge
sources such as statistical word or phrase lexica.
The system-based approaches derive the word posterior probability from the sen-
tence posterior. The posterior probability of a sentence eI1 can be approximated by the
joint probability p( f J1, e
I
1), which the statistical machine translation system assigns to
a generated translation. The sentence probabilities employed in the search (see Equa-
tion (1)) are not normalized, which does not affect the result of the search. But for use in
confidence estimation, they need to be normalized in order to obtain a probability distri-
bution over all target sentences (see Equation (6)). From the sentence posterior probabil-
ities, the word posterior probabilities can be calculated by summing up the probabilities
of all sentences containing the target word. For an exact quantification of word posterior
probabilities, we need to consider the following problem: How can we define a criterion
for the occurrence of a word in a sentence? The answer to this question is not at all triv-
ial. Due to ambiguities, the word position in the sentence is not fixed. Sentences can have
different numbers of words because of deletions and insertions. Additionally, the words
can be reordered in different ways during the translation process. The posterior proba-
bility of a target word e can depend on its occurrence in position i of the target sentence,
for example, or on the number of times the word is contained in the sentence. Thus, sev-
eral different definitions of posterior probabilities will be introduced and investigated in
the following discussion. The basic concept of calculating the posterior probability will
be explained for the target word e occurring in a fixed position i of the sentence. This is
a rather strict and simple criterion; it will be used here mainly to illustrate the idea. Sec-
tion 4 will describe several different concepts of word posterior probabilities that relax
this condition.
Let p( f J1, e
I
1) be the joint probability of source sentence f
J
1 and target sentence e
I
1.
Here, this is approximated by the probability that an SMT system assigns to the
sentence pair (see Section 2). The word posterior probability of e occurring in position
13
Computational Linguistics Volume 33, Number 1
i is calculated as the normalized sum of probabilities of all sentences containing e in
exactly this position:
pi(e | f J1 ) =
pi(e, f
J
1 )
?
e?
pi(e?, f
J
1 )
(4)
where pi(e, f
J
1 ) =
?
I,eI1
?(ei, e) ? p( f J1, eI1) (5)
Here ?(?, ?) is the Kronecker function. The normalization term in Equation (4) is
?
e?
pi(e?, f
J
1 ) =
?
I,eI1
p( f J1, e
I
1) = p( f
J
1 ) (6)
This definition of word posterior probabilities raises the question of how to calculate the
sums over the target sentences in Equations (5) and (6). This problem can be solved by
approximating the summation space via a word graph or an N-best list. The summation
is then performed explicitly over all sentences given in this restricted space. In the case
of an N-best list, this is straightforward because the sentences are already listed. On a
word graph, the forward?backward algorithm can be applied to carry out the summa-
tion efficiently. In these system-based approaches, the calculation depends on the output
of the SMT system that generated the translations. The sentence probabilities summed
in Equations (5) and (6) are the scores assigned by the underlying SMT system. The
summation space is restricted to those hypotheses that are assigned a high probability
by the SMT system, and the others are not considered.
The second approach to the calculation of word posterior probabilities is summa-
tion using direct models such as IBM model 1 or a phrase-based translation model.
These methods do not consider the whole target sentence. The summation of prob-
abilities is carried out over single words or phrases without context. These model-
based word posterior probabilities are independent of the system generating the
translations. They do not require the MT system to assign a probability to the translation
hypothesis. Thus, they can also be used for confidence estimation on hypotheses from
a non-statistical MT system or if only the single best translations without any scores
are given.
3.3 Word Confidence Measures
The idea behind word-level confidence estimation is to be able to detect possible errors
in the output of a machine translation system. Using confidence measures, individ-
ual words can be labeled as either correct or incorrect. This additional information
can be used in, for example, interactive TransType-style machine translation systems
(Gandrabur and Foster 2003; Ueffing and Ney 2005a).
Two problems have to be solved in order to compute confidence measures. First,
suitable confidence features have to be computed. Second, a binary classifier has to be
defined, which decides whether a word is correct or not. The word posterior proba-
bilities introduced in Section 3.2 can be interpreted as the probability of a word being
correct. That is, the probability can directly be used as a confidence measure. For this
14
Ueffing and Ney Word-Level Confidence Estimation for MT
purpose, it is compared to a threshold t. All words that have confidence above this
threshold are tagged as correct, and all others are tagged as incorrect, translations. Thus,
the binary classifier is defined as
class(e) =
?
?
?
correct if p(e | f J1 ) ? t
incorrect otherwise
(7)
The threshold t is optimized on a distinct development set beforehand.
The question of how the correctness of a word in MT output is determined is not at
all an easy one. We will address this issue in Section 7.2.
4. System-Based Confidence Measures
In this section, we will present confidence measures that are calculated over N-best
lists or word graphs generated by an SMT system. Several different models for the
occurrence of a target word in a sentence will be defined and experimentally evaluated.
These are the models that proved most promising from a theoretical viewpoint and in
the experimental evaluation:
 Target word e occurs in position i of the target sentence (see Section 4.1).
The calculation of word posterior probabilities over word graphs and
N-best lists is explained in detail for this concept.
 The word is considered if it occurs in a window around the position:
i ? t, t ? N, for some position i (see Section 4.2).
 The Levenshtein alignments between the hypothesis under consideration
and all other possible translations are determined. The target word e (in
some position i) is taken into account if it is Levenshtein-aligned to itself
(see Section 4.3).
 e is contained in the sentence at least n times, n ? N (see Section 4.4).
Section 4.5 will treat the issue of scaling the probabilities that the SMT system assigns
to the translation hypothesis.
4.1 Approach Based on the Fixed Target Position
In this approach, the word posterior probability is determined for word e occurring in
target position i as shown in Equation (4). This variant requires the word to occur exactly
in the given position i. Hence, a probability distribution over the pairs (e, i) of target
words e and positions i in the target sentence is obtained. This type of word posterior
probability was first introduced in Ueffing, Macherey, and Ney (2003).
The concept of word posterior probabilities based on the fixed target position allows
for easy calculation over word graphs and N-best lists. However, this concept is rather
restrictive. In practice, the target position of a word varies between different translation
alternatives. The method presented here is a starting point for more flexible approaches
that perform summation over a window of target positions.
In the following, we will show how the word posterior probabilities based on fixed
target positions are calculated over word graphs and over N-best lists.
15
Computational Linguistics Volume 33, Number 1
Calculation using word graphs. A word graph represents the most promising hypotheses
generated by the translation system (Ueffing, Och, and Ney 2002; Zens and Ney 2005).
It has the advantage of being a compact representation of the translation hypothesis
space, which allows for efficient calculation of word posterior probabilities. A word
graph is a directed acyclic graph G = (V, E) with vertex set V and edge set E. It has
one designated root node n0 ? V, representing the beginning of the sentence. Each
path through the word graph represents a translation candidate. The nodes of the
graph contain information such as the set of covered source positions and the language
model history. Two hypotheses can be recombined if their information is identical.
Recombination is carried out during decoding to accelerate the search process. If two
hypotheses represent the same information with respect to translation and language
models, they will be assigned the same probabilities by these models in the future.
Therefore, the outcome of the search is not altered, but the processing time can be
significantly decreased if only the more promising of the two hypotheses is considered
for further expansion. If no recombination were carried out, the word graph would have
the structure of a tree.
The edges in the word graph are annotated with target words. Additionally, they
contain weights representing the part of the probability that is assigned to each particu-
lar word as part of the target hypothesis. When multiplying the scores along a path, the
probability of the corresponding hypothesis is obtained. The sentence position of a word
refers to the path length in the word graph: Consider an edge (n, n?) that is annotated
with word e. If a path leading from source node n0 into n has i edges, then e will
be the (i + 1)-th word in the corresponding sentence. Note that due to recombination
this position is not unambiguous. If two hypotheses of different lengths i and i? are
recombined in node n, then e will be in position i + 1 in the one resulting sentence, and
in i? + 1 in the other sentence.
For an example of a word graph, see Figure 1. The source sentence is Wir ko?nnen
das machen!, and the reference translation is We can do that!. The leftmost node
is the root node n0. The other nodes represent different states with respect to the set
of covered source positions and language model history. In this example, a trigram
Figure 1
Example of forward?backward calculation on a word graph. The posterior probability of can
in the second position is obtained by multiplying the total probability of all incoming paths
(dashed lines) and outgoing paths (dotted), separately for the two edges, and summing the
products.
16
Ueffing and Ney Word-Level Confidence Estimation for MT
language model is applied, that is, all paths leading into a node share the last two words.
The translation alternatives contained in this word graph represent different reorderings
of the words in the sentence: The monotone translation that do as well as the correctly
reordered sequence do that occur. Note that in order to limit the size of the graph and
keep the presentation simple, an example was chosen where all target sentences have
the same length.
The posterior probabilities of word e in position i can be computed by summing up
the probabilities of all paths in the graph that contain an edge annotated with e in posi-
tion i of the target sentence. This summation is performed efficiently using the forward?
backward algorithm (Jurafsky and Martin 2000). This algorithm also determines the
total probability mass that is needed for normalization, as shown in Equation (6). In
the following, we will present the exact equations for a word graph generated by the
phrase-based translation system described in Section 2.2. In such a word graph, the
first word of a target phrase is assigned the score for the whole phrase. That is, when
translating a source phrase f?k by a target phrase e?k = eik?1+1 . . . eik , the full contribution
of all sub-models for this phrase is included for the first word eik?1+1. All following
words eik?1+2 . . . eik are assigned probability 1.
The forward?backward algorithm works as follows: Let QPM( f?k, e?k) be the phrase
model score of a phrase pair as defined in Equation (3) in Section 2.2. In order to keep
the notation simple, we assume a bigram language model. The extension to higher-order
language models is straightforward. The forward probability ?i(ei ; e?k, f?k) of word ei is
the probability of reaching word ei from the sentence start, where ei occurs in position
i of the sentence. It depends on the phrase pair ( f?k, e?k) for which ei ? e?k. Because the
full score of this phrase pair is included at the first word eik?1+1, two cases have to
be distinguished in the calculation: Either ei is the first word in the phrase, that is,
i = ik?1 + 1, or ik?1 + 1 < i ? ik. The forward probability can be determined by sum-
ming the probabilities of all partial hypotheses of length i ? 1. This allows for recursive
calculation in ascending order of i. We obtain the following formula:
?i(ei ; e?k, f?k) =
=
?
?
?
?
?
?
?
?
?
?
?
?
?
QPM( f?k, e?k) ?
ik
?
i?=i+1
c1 ? p(ei? | ei??1)?1 ?
?
e?k?1
p(ei | eik?1 )
?1
?
?
f?k?1
?i?1(eik?1 ; e?k?1, f?k?1) if i = ik?1 + 1
?i?1(ei?1 ; e?k, f?k) if ik?1 + 1 < i ? ik
The backward probability ?i(ei ; e?k, f?k) expresses the probability of completing a sen-
tence from the current word on. It can be determined recursively in descending order
of i. Again, we distinguish two cases:
?i(ei ; e?k, f?k) =
=
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e?k+1
ik+1
?
i?=i
c1 ? p(ei? | ei??1)?1 ?
?
f?k+1
QPM( f?k+1, e?k+1) ??i+1(ei+1 ; e?k+1, f?k+1)
if i = ik
?i+1(ei+1 ; e?k, f?k) if ik?1 < i < ik
17
Computational Linguistics Volume 33, Number 1
Using the forward?backward algorithm, the word posterior probability of word e in
position i is determined by combining the forward and backward probabilities of all
hypotheses containing e in this position. We carry out a summation over all correspond-
ing phrase pairs ( f?k, e?k). This yields
pi(e, f
J
1 ) =
?
e?k
?
f?k
?i(e ; e?k, f?k) ??i(e ; e?k, f?k) (8)
To obtain a posterior probability, a normalization (as shown in Equation (4)) has to
be performed. The normalization term p( f J1 ) :=
?
e?
pi(e?, f
J
1 ) corresponds to the probability
mass contained in the word graph and can be calculated by summing the backward
probabilities of all words that occur in the first sentence position:
p( f J1 ) =
?
e?1=e1...ei1
?
f?1
?1(e1; e?1, f?1)
Figure 1 illustrates the forward?backward algorithm. Assume the word posterior
probability of the word can appearing in the second position of the target sentence is
to be calculated. There are two edges in the graph that contain this word in the desired
target position. Thus, the probabilities of the paths leading through these edges have
to be summed. The forward probabilities are the probabilities of the incoming edges,
shown by dashed lines. The backward probabilities are those of the paths marked by
dotted lines. They are combined (separately for each edge) and then summed to obtain
the word posterior probability of can in position 2.
Calculation using N-best lists. An N-best list contains the n most promising translation
hypotheses generated by the statistical machine translation system. The N-best list
is extracted from a word graph. The hypotheses are sorted by their probability in
descending order. This representation allows for easy computation of the sum given in
Equation (5). Furthermore, the calculation of more complex variants of word posterior
probabilities, such as the approach based on Levenshtein alignment (see Section 4.3),
is feasible.
Let en,Inn,1 , n = 1, . . . , N, be the target hypotheses in the N-best list. The word pos-
terior probabilities presented in Equation (4) are calculated by summing the sentence
probabilities of all sentences containing target word e in target position i. The sentence
probability p( f J1, e
n,In
n,1 ) is given in the N-best list. The word posterior probability is then
determined as
pi(e | f J1 ) =
N
?
n=1
?(en,i, e) ? p( f J1, e
n,In
n,1 )
?
e?
N
?
n=1
?(en,i, e?) ? p( f J1, e
n,In
n,1 )
The normalization term in the denominator equals the probability mass contained in
the N-best list.
18
Ueffing and Ney Word-Level Confidence Estimation for MT
Instead of the sum of probabilities, one can also determine the relative frequency or
the rank-weighted frequency of a word as follows: The relative frequency of e occurring
in target position i in the N-best list is computed as
hi(e | f J1 ) := 1N
N
?
n=1
?(en,i, e) (9)
The rank-weighted frequency is determined as
ri(e | f J1 ) := 2N(N + 1)
N
?
n=1
?(en,i, e) ? (N + 1 ? n) (10)
Here, the inverted ranks N + 1 ? n are summed up because an occurrence of the word
in a hypothesis near the top of the list will score better than one in the lower ranks.
This value is normalized by the sum of all ranks in the list. Note that the values in
Equations (9) and (10) could also be calculated over N-best lists that do not contain the
sentence probability.
4.2 Approach Based on a Window over Target Positions
One way of accounting for slight variations in the target position i of word e is the intro-
duction of a window i ? t, t ? N, around position i. The word confidence is determined
as the sum of the word posterior probabilities calculated for the positions within this
window. This leads to
pi,t(e | f J1 ) =
i+t
?
k=i?t
pk(e | f J1 ) (11)
The window can easily be integrated both into the N-best list and the word graph?based
implementation: The target position-dependent word posterior probabilities are calcu-
lated as stated in Equation (4), and the summation over the positions in the window is
performed in an additional step.
4.3 Approach Based on the Levenshtein Alignment
Another way of accounting for variations in the target position of a word is to perform
the Levenshtein alignment (Levenshtein 1966) between sentence eI1 under consideration
and the other possible target sentences. The summation in Equation (5) is then per-
formed over all sentences containing e in a position Levenshtein-aligned to i (Ueffing,
Macherey, and Ney 2003).
The implementation of this summation over N-best lists is straightforward: The
Levenshtein alignment is performed between the hypothesis eI1 and every sentence
en,Inn,1 contained in the N-best list individually, and then the summation is carried out.
For word graphs, no efficient way of determining the Levenshtein alignments and
the resulting word posterior probabilities is known.
19
Computational Linguistics Volume 33, Number 1
Let L(eI1, e
n,In
n,1 ) be the Levenshtein alignment between sentences e
I
1 and e
n,In
n,1 , and
Li(eI1, e
n,In
n,1 ) that of word e in position i in e
I
1. Consider the following example: Cal-
culating the Levenshtein alignment between the sentences eI1 =?A B C D E? and
en,Inn,1 =?B C G E F? yields
L(eI1, e
n,In
n,1 ) = ? ? B C G E??
Using this representation, the word posterior probability of word e occurring in a
position Levenshtein-aligned to i is given by
plev(e | f J1, eI1,L) =
plev(e, f
J
1, e
I
1,L)
?
e?
plev(e?, f
J
1, e
I
1,L)
(12)
where plev(e, f
J
1, e
I
1,L) =
N
?
n=1
?(e,Li(eI1, e
n,In
n,1 )) ? p( f
J
1, e
n,In
n,1 )
The probability depends on all target words in the hypothesis eI1 under consideration,
because the Levenshtein alignment of the whole sentence, L(eI1, e
n,In
n,1 ), is determined.
This concept of word posterior probabilities is inspired by the error measure Word
Error Rate (WER). It can be shown that the word posterior probabilities form a part
of the Bayes risk for WER: Formulating the loss function and deriving the risk yields a
minimization criterion consisting of the word posterior probabilities defined previously,
one term representing the sentence length, and one for the deletion operations in the
Levenshtein alignment. For more details, see Ueffing and Ney (2004) and Ueffing (2006).
4.4 Count-Based Approach
Inspired by Bayes risk for Position-independent Word Error Rate (PER), the word
posterior probability can be defined by taking the counts of the words in the generated
sentence into account (Ueffing and Ney 2004). The probability of target word e occurring
in the sentence n times is determined as
pe(n| f J1 ) =
pe(n, f
J
1 )
nmax
?
n?=0
pe(n
? , f J1 )
(13)
where pe(n, f
J
1 ) =
?
I,eI1
?(ne, n) ? p( f J1, eI1)
Here, ne is the count of word e in sentence eI1, and nmax is the maximal count that is
observed. This term does not depend on the actual word sequence, but only on the
counts of the target words. Let nE1 be the counts of all target words 1, . . . , E in sentence
eI1. Analogously, n?
E
1 denotes the count sequence for sentence e?
I?
1. In practice, many of
these counts will be zero, of course. The posterior probabilities can then be expressed
by the distribution over the count sequences:
pe(n, f
J
1 ) =
?
nE1
?(ne, n) ? p(nE1 , f
J
1 )
20
Ueffing and Ney Word-Level Confidence Estimation for MT
where the distribution over the count sequences is determined by summing up the
probabilities of all sentences with these counts:
p(nE1 , f
J
1 ) =
?
I?,e?I?1
?(n?E1 , n
E
1 ) ? p( f
J
1, e
I
1)
Using this concept, the target position of the word is not taken into account, but the first
occurrence of a word in the sentence will obtain a word posterior probability different
from that of the second occurrence.
In Ueffing (2006), it is shown that the posterior risk for PER comprises one term
related to the count-based word posterior probabilities defined here and one term
related to the posterior probability of the sentence length. We can thus expect the count-
based word posterior probabilities to perform especially well if the word correctness
is defined on the basis of PER. The experimental results presented in Section 7.4 will
confirm this assumption.
The summation in Equation (13) can be performed over N-best lists (analogously
to the word posterior probability variants described so far), but it cannot efficiently
be determined over the word graph. The problem is that the number of occurrences
of a word on the whole path is needed. Because the word graph stores only local
information, this count cannot be determined efficiently. The normalization term in
Equation (13) corresponds to the total probability mass contained in the N-best list,
because the case n? = 0 is also included.
4.5 Scaling the Probabilities
During the translation process, the different sub-models (such as the language model
and the lexicon model) are weighted differently. These weights or scaling factors can
be optimized with respect to some evaluation criterion (Och 2003). Nevertheless, this
optimization determines only the relation between the different models, and not the
absolute values of the scaling factors. The absolute values are not needed for the
translation process, because the search is performed using the maximum approximation
(see Equations (1) and (2)). In contrast to this, the actual values of the weights make a
difference for confidence estimation, because the summation over the sentence proba-
bilities is performed. To account for this and to find the optimal values of the scaling
factors, a global weight ? is introduced, which scales the sentence probability. The
word posterior probability based on the fixed position i, for example, is then calculated
according to
pi(e | f J1 ) =
?
I,eI1
?(ei, e) ? p?( f J1, eI1)
?
e?
?
I,eI1
?(ei, e?) ? p?( f J1, eI1)
(14)
When determining the system-based word posterior probabilities, this scaling factor is
optimized with respect to some metric for confidence estimation on a development set
distinct from the test set.
21
Computational Linguistics Volume 33, Number 1
5. Confidence Measures Based on Direct Models
In the following, confidence measures based on direct models will be described. These
approaches model the word posterior probability directly instead of summing the prob-
abilities of sentences containing the target word. Confidence measures based on IBM
model 1 and phrase-based translation models were developed and will be presented
here. They make use of knowledge sources such as statistical word or phrase lexica for
estimating the word confidence. Unlike the system-based word posterior probabilities
presented so far, these confidence measures are completely independent of the target
sentence position in which the word e occurs. They determine the confidence of e being
contained anywhere in the sentence.
5.1 Direct Approach to Confidence Estimation Using Phrases
The statistical models presented in Section 2.2 can be used to estimate the confidence of
target words as first described in Ueffing and Ney (2005b). In contrast to the approaches
presented in Section 4, the direct phrase-based confidence measures do not use the
context information at the sentence level, but only at the phrase level.
For a given source sentence f J1 and a target word e, we want to determine a sort
of marginal probability Q(e, f J1 ). Therefore, we extract all source phrases f
j+s
j that occur
in the given source sentence f J1. For these source phrases, we find the possible transla-
tions ei+ti in the bilingual phrase lexicon. The confidence of target word e is then calcu-
lated by summing over all phrase pairs ( f j+sj , e
i+t
i ) where the target part e
i+t
i contains e.
Let QPM( f?k, e?k) be the phrase model score of a phrase pair as defined in Equation (3)
in Section 2.2. Analogously, we define QLM(e
i+t
i ) as the language model score of the
target phrase together with the word penalty c1 for each word in the phrase, that is,
QLM(e
i+t
i ) :=
i+t
?
i?=i
c1 ? p(ei? | ei
??1
i??2)
?1 (15)
Note that this is the within-phrase language model probability, which does not include
the context of the phrase. The language model probability at the phrase boundary is
approximated by a unigram and bigram.
The (unnormalized) confidence of target word e is then determined by summing
the product of the language model and the phrase model score of all phrase pairs
containing e:
Q(e, f J1 ) :=
J
?
j=1
min{smax,J?j}
?
s=0
?
ei+ti
?(e , ei+ti ) ? QLM(e
i+t
i ) ? QPM( f
j+s
j , e
i+t
i ) (16)
where s ? smax and t are source and target phrase lengths, smax being the maximal source
phrase length. ?(e, ei+ti ) denotes an extension of the Kronecker delta:
?(a, A) =
{
1 if a ? A
0 otherwise
22
Ueffing and Ney Word-Level Confidence Estimation for MT
The value calculated in Equation (16) is not normalized. In order to obtain a probability,
this value is divided by the sum over the (unnormalized) confidence values of all target
words:
pphr(e | f J1 ) =
Q(e, f J1 )
?
e?
Q(e?, f J1 )
(17)
As shown in Equations (3) and (15), the different sub-models of the phrase-based
translation approach are combined in a log-linear manner. The weights ?1, . . . , ?5 and
the penalties c1, c2 are optimized in the translation process with respect to some eval-
uation criterion such as WER or BLEU. This is done using the Downhill Simplex
algorithm (Press et al 2002). The resulting values of the weights express the relation
between the sub-models, but not their absolute values. They are usually normalized so
that they sum to 1. For use in confidence estimation, two different aspects thus have to
be considered:
 The relation of the sub-models that is optimal for translation quality
is not necessarily optimal for classification performance. Therefore, the
sub-model scaling factors are optimized with respect to some confidence
evaluation measure (see Section 7.3). The direct phrase-based confidence
measures provide a framework for optimizing the sub-model weights
efficiently. The optimization is performed analogously to the procedure
for machine translation: The confidence values are determined for all
words in the development corpus. Then, classification is carried out as
described in Section 3.3, and the result is evaluated. The weights are
then modified and the confidence estimation is repeated, until optimal
classification performance on the development set is achieved. Again,
the Downhill Simplex algorithm is used for optimization.
 For MT, only the relation between the different sub-models, but not the
actual values of the scaling factors, are important. Confidence measures,
however, also depend on these actual values. In MT, the sub-model scaling
factors are normalized such that they sum to 1. For the use in confidence
estimation, the value of this sum,
? :=
5
?
i=1
?i + c1 + c2
is also optimized. This ? is analogous to the global scaling factor for the
system-based confidence measures introduced in Section 4.5.
5.2 Confidence Measure Based on IBM Model 1
Another type of confidence measure that does not rely on system output and is thus
applicable to any kind of machine translation system is the IBM model 1?based confi-
dence measure that was introduced in Blatz et al (2003). We modified this confidence
measure because we found that the average lexicon probability used there is dominated
23
Computational Linguistics Volume 33, Number 1
by the maximum. Therefore, we determine the maximal translation probability of the
target word e over the source sentence words:
pibm1(e| f J1 ) = maxj=0,...,J p(e| fj) (18)
where f0 is the ?empty? source word (Brown et al 1993). The probabilities p(e| fj) are
word-based lexicon probabilities.
Investigations of the use of the IBM model 1 for word-level confidence estimation
showed promising results (Blatz et al 2003, 2004). Thus, we apply this method here
and compare it to the other types of confidence measures. Ueffing and Ney (2005a)
report on the use of this IBM model 1?based confidence measure in a TransType-style
interactive MT system. The work presented there shows that even this relatively simple
confidence measure yields a significant gain in the quality of the predictions proposed
by the interactive system.
6. Combination of Confidence Measures
In related work in MT as well as in speech recognition, the combination of numerous
confidence features has been suggested (Gandrabur and Foster 2003; Blatz et al 2004;
Quirk 2004; Sanchis 2004). Among the methods used for combination are multi-layer
artificial neural networks, naive Bayes classifiers, and modified linear regression.
Because the combination of several confidence measures proved successful, the
different word posterior probabilities proposed here were combined with each other.
The combination was performed in a log-linear manner. Let pm(e | f J1, . . .) , m = 1, . . . , M,
be the word posterior probabilities of e determined using different approaches. The
word confidence resulting from their combination is calculated as
c(e) = exp
{
?
M
?
m=1
?m ? log pm(e | f J1, . . .)
}
The interpolation weights ?m are optimized with respect to some confidence evaluation
metric on the development corpus using the Downhill Simplex algorithm (Press et al
2002). With this approach, the confidence error rates were reduced over the best single
confidence measure consistently on all corpora we examined. The experimental results
will be presented in Section 7.4. This section also contains details on which confidence
measures were combined.
However, the focus of this work is on word posterior probabilities as stand-alone
confidence measures. It was shown that they are the best single features for confidence
estimation (Blatz et al 2004). Moreover, they are closely related to Bayes risk, which
yields a sound theoretical foundation (Ueffing and Ney 2004).
7. Experiments
7.1 Experimental Setting
The experiments were performed on three translation tasks in different language pairs.
The corpora were compiled in the EU projects TransType2 (TransType2 2005) and
TC-STAR (TC-STAR 2005), and for the NIST MT evaluation campaign (NIST 2004). The
24
Ueffing and Ney Word-Level Confidence Estimation for MT
TransType2 corpora consist of technical manuals for Xerox devices such as printers.
They are available in three different language pairs. This domain is very specialized
with respect to terminology and style. The corpus statistics are given in Table 1. The
TC-STAR corpus consists of proceedings of the European Parliament. It is a spoken
language translation corpus containing the verbatim transcriptions of the speeches in
the European Parliament Plenary Sessions (EPPS). The domain is basically unrestricted
because a wide range of different topics is covered in the sessions. The translation
direction is from Spanish into English. For corpus statistics, see Table 2. The NIST corpus
was compiled for the yearly MT evaluation campaign carried out since 2001. Chinese
news articles are translated into English. Similarly to the EPPS data, the domain is
basically unrestricted, because a wide range of different topics is covered. However,
the vocabulary size and the training corpus are much larger than in the EPPS collection,
as the corpus statistics presented in Table 3 show. Additionally to the bilingual data, a
monolingual English corpus consisting of 636M running words was used for language
model training. The SMT systems that generated the translations for which confidence
estimation was performed were trained on these corpora. The same holds for the
probability models that were used to estimate the word confidences.
We translated the development and test corpora using several different MT systems
for testing the confidence measures:
 The phrase-based translation system described in Section 2.2 (denoted as
PBT in the tables); a large part of the results will be presented for output
of this system.
Table 1
Statistics of the training, development, and test corpora for the TransType2 task.
French English Spanish English German English
TRAIN Sentences 53,046 55,761 49,376
Running words 680,796 628,329 752,606 665,399 537,464 589,531
Vocabulary 15,632 13,816 11,050 7,956 23,845 13,223
DEV Sentences 994 1,012 964
Running words 11,674 10,903 15,957 14,278 10,462 10,642
TEST Sentences 984 1,125 996
Running words 11,709 11,177 10,106 8,370 11,704 12,298
Table 2
Statistics of the training, development, and test corpora for the TC-STAR EPPS Spanish?English
task. Both development and test corpus are provided with two English references.
Spanish English
TRAIN Sentences 1,652,174
Running words 32,554,077 31,147,901
Vocabulary 124,192 80,125
DEV Sentences 2,643
Running words 20,289 40,396
TEST Sentences 1,073
Running words 18,896 37,742
25
Computational Linguistics Volume 33, Number 1
Table 3
Statistics of the training, development, and test corpora for the NIST Chinese?English task. Both
development and test corpus are provided with four English references.
Chinese English
TRAIN Sentences 7M
Running words 199M 213M
Vocabulary 223K 351K
Dictionary entries 82K
DEV (2002 evaluation set) Sentences 878
Running words 25K 105K
TEST (2004 evaluation set) Sentences 1,788
Running words 52K 239K
 The alignment template system (Och and Ney 2004) (denoted as AT in the
tables), which is also a state-of-the art phrase-based translation system.
 The Systran version available at http://babelfish.altavista.com/tr in
June 2005. These hypotheses were used to investigate whether the direct
confidence measures perform well on translations generated by a
structurally different system.
The translation quality on the TransType2 task in terms of WER, PER, BLEU score
(Papineni et al 2002), and NIST score (NIST 2002) is given in Table 4. We see that
the best results are obtained on Spanish to English translation, followed by French to
English and German to English. The reason that Systran generates translations of much
lower quality than the SMT systems is due to the fact that the technical manuals are very
specific in terminology. The SMT systems were trained on similar corpora so that they
are familiar with the terminology. The table additionally shows the translation quality
achieved by the system PBT on the NIST test set.
Table 4
Translation quality of different MT systems on the TransType2 and the NIST test corpora.
Task Language pair System WER[%] PER[%] BLEU[%] NIST
TransType2 F ? E PBT 54.9 43.4 31.3 6.62
AT 54.8 43.7 31.5 6.64
Systran 81.5 71.7 12.5 4.23
S ? E PBT 26.1 17.5 66.9 8.98
AT 29.6 20.1 63.4 8.80
Systran 78.0 62.3 23.4 4.77
G ? E PBT 61.6 49.6 25.7 5.72
AT 62.7 49.8 26.6 5.92
Systran 79.2 66.4 12.0 4.09
NIST C ? E PBT 61.8 42.9 31.1 8.47
C = Chinese; E = English; F = French; G = German; S = Spanish.
26
Ueffing and Ney Word-Level Confidence Estimation for MT
On the EPPS task from TC-STAR, the confidence measures were tested on output
from the phrase-based translation system. The hypotheses are generated by the version
of the system that participated in the TC-STAR evaluation round in March 2005 and
that was ranked first there. The translation quality can be seen in Table 12 later in
this article.
7.2 Word Error Measures
In order to evaluate the classifier built from the confidence measures as described in
Section 3.3, reference tags are needed that define the true class of each word. In machine
translation, it is not intuitively clear how to determine the correctness of a word.
Therefore, a number of different measures for identifying the reference classes for single
words in a translation hypothesis were implemented (Ueffing 2006). They are inspired
by different translation evaluation measures like WER and PER. All of them compare
the translation hypothesis to one or?if available?several references to determine the
word errors. In this article, we will present results for the following error measures:
 WER: A word is counted as correct if it is Levenshtein-aligned to itself in
one of the references.
 PER: A word is tagged as correct if it occurs in one of the references. The
number of occurrences per word is taken into account, but the position of
the word in the sentence is completely disregarded.
Both word error measures exist in two variants: First, each translation hypothesis is
compared to the pool of all references (in case there exist different reference translations
for the development and test corpus). Second, the reference with minimum distance to
the hypothesis according to the translation evaluation measure under consideration is
determined. The true classes of the words are then defined with respect to this nearest
reference. For example, if the PER metric is applied, the pooled variant labels all those
words as correct that occur in any of the references (with this count). The second variant
considers as correct only those words that are contained in the nearest reference (with
this count). The latter corresponds to the procedure used for m-WER and m-PER in MT
evaluation (Nie?en et al 2000).
Table 5 shows the percentage of words that are labeled as correct according to the
different error measures on the development and test corpora of the EPPS task. It can
be seen that WER is the stricter error measure: It considers fewer words as correct
than PER does. A comparison of the pooled and the nearest reference shows that the
pooling yields a significant increase in the number of words labeled as correct. Note
that the figures in the table do not directly correspond to the translation error rates for
the system output. They are calculated only for the words contained in the generated
Table 5
Ratio of correct words (%) in the EPPS Spanish ? English development and test corpora,
according to different word error measures.
Error Measure WER PER
pooled nearest pooled nearest
DEV 78.6 72.9 81.5 77.4
TEST 76.5 69.8 81.5 76.5
27
Computational Linguistics Volume 33, Number 1
translation hypotheses and do not take deleted words into account. Moreover, they
are normalized by the hypothesis lengths. If WER and PER are applied as translation
evaluation measures (on the sentence level), deletions are counted as well, and the
number of errors is divided by the number of reference words.
7.3 Evaluation Metrics
After computing the confidence measure, each generated word is tagged as either
correct or incorrect, depending on whether its confidence exceeds the tagging thresh-
old that was optimized on the development set beforehand. The performance of the
confidence measures is evaluated using the following three measures:
 Classification or Confidence Error Rate (CER): This is defined as the
number of incorrect tags divided by the total number of generated words
in the translated sentence. The baseline CER is determined by assigning
the most frequent class (in the whole development or test corpus) to all
words. Assume that the correct classes of the words are defined on the
basis of WER. If the overall WER on the considered development or
test corpus is below 50%, the baseline CER is calculated by tagging all
words as correct. The baseline CER then corresponds to the number
of substitutions and insertions, divided by the number of generated
words. The CER strongly depends on the tagging threshold. Therefore,
the tagging threshold is adjusted beforehand (to minimize CER) on a
development corpus distinct from the test set. Moreover, we will present
significance bounds for the baseline CER. They were determined using
the bootstrap estimation method described in Bisani and Ney (2004).1
 Receiver Operating Characteristic (ROC) curve (Duda, Hart, and Stork
2001):2 The ROC curve plots the correct rejection rate versus the correct
acceptance rate for different values of the tagging threshold. The correct
rejection rate is the number of incorrectly translated words that were
tagged as false, divided by the total number of incorrectly translated
words. The correct acceptance rate is the ratio of correctly translated words
that were tagged as correct. These two rates depend on each other: If one
of them is restricted by a lower bound, the other one cannot be restricted.
The further the ROC curve lies away from the diagonal (and away from
the point of origin), the better the performance of the confidence measure.
Unlike the CER, the ROC curve is independent of the prior probability of
the two classes correct and incorrect. This means that ROC curves from
different data sets can be compared directly.
 Integral of the ROC curve (IROC): ROC curves provide for a qualitative
analysis of classifier performance; a related quantitative metric is IROC,
defined as the area under a ROC curve. The IROC takes on values in [0, 1],
with 0.5 corresponding to a random separation of correct and incorrect
words, 1.0 corresponding to a perfect separation, and 0.0 the opposite.
1 The tool described in this paper is freely available from http://www-i6.informatik.rwth-aachen.de/
web/Software/.
2 A variant of the ROC curve is the Detection Error Tradeoff (DET) curve which plots the false rejection rate
versus the false acceptance rate.
28
Ueffing and Ney Word-Level Confidence Estimation for MT
7.4 Experimental Results
TransType2 task. Table 6 compares the classification performance of several confidence
measures on the TransType2 French?English task. The CER and the IROC values are
given for WER- and PER-based classification. Note that lower CER and higher IROC
values express better performance. It is interesting to see that, in most of the cases, the
tendencies are consistent for the two evaluation metrics: Lower CER is accompanied by
higher IROC.
In general, one can see that the very simple approach that sums over sentences
in the N-best list or word graph considering the fixed target position of the word
clearly performs worst. This is to be expected, and the method was included only for
comparison. It can be considered as a simple baseline method. The other system-based
measures discriminate significantly better in both settings.
The system-based confidence measures show much better discriminative power
than the direct IBM model 1. The N-best list based measure with Levenshtein alignment
and the word posterior probabilities calculated over word graphs using a window
perform similarly well. For WER-based classification, they are outperformed only by
the direct phrase?based approach, which achieves the best CER and IROC values.
It is interesting to compare the two methods that were applied to both word
graphs and N-best lists: the approach based on the fixed target position and the one
summing over a window of positions. In both cases, the word graph?based calculation
is slightly superior to that based on 10,000-best lists. However, the difference in CER is
not significant.
The count-based method working on N-best lists is clearly the best confidence
measure for PER-based classification. This result was to be expected because the count-
based word posterior probability was derived from the Bayes risk for PER (Ueffing
and Ney 2004). Even if its CER does not differ much from that of the direct phrase-
based measure, there exists a clear predominance in terms of IROC. The IBM-1?based
confidence measure performs rather poorly compared to the other methods. This is not
surprising because the IBM model 1 is a very simple model.
Table 6
Classification performance in terms of CER (%) and IROC (%) for different confidence measures.
TransType2 French ? English test set. References based on WER and PER, confidence measures
optimized accordingly. Hypotheses from the phrase-based system.
Model WER PER
CER IROC CER IROC
baseline 42.2 ? 34.2 ?
99% confidence interval ?2.3 ? ?2.0 ?
10,000-best lists, fixed position 39.7 66.2 33.3 66.2
Levenshtein 31.3 72.6 28.1 74.8
window ?3 31.6 70.7 28.3 73.4
count-based 31.9 71.6 27.0 76.5
word graphs, fixed position 38.6 70.5 33.1 67.6
window ?3 31.1 72.4 27.3 75.4
IBM-1 (max.) 39.2 67.0 31.5 71.0
direct phrase-based 30.6 74.4 27.4 73.7
29
Computational Linguistics Volume 33, Number 1
The comparison of the IROC values for WER- and PER-based classification shows
that PER is easier to learn than WER: The IROC values for PER are higher for most
confidence measures. This is consistent with the results obtained in the CLSP workshop
(Blatz et al 2003). The classifiers investigated there also show better discriminative
power for reference classes based on PER than for WER.
To further illustrate the classification performance of the different confidence mea-
sures, the ROC curves for some of them are given in Figure 2. In each, the diagonal
line refers to random classification of words as correct and incorrect. The left curve
shows the results for WER-based classification, and the right one for PER, respectively.
The N-best list-based method considering the fixed target position is again given for
comparison. One can see that the IBM-1?based confidence measure is clearly better than
this baseline for PER, but not for WER. The curves for the direct phrase-based model and
the best N-best list-based method lie relatively close to each other. These two confidence
measures clearly dominate all others.
Because the direct phrase-based confidence measures perform so well on the output
of the phrase-based translation system, we were interested in finding out whether this is
due to the fact that the translation system and the confidence measure explore the same
statistical models. Therefore, the system-independent confidence measures (i.e., those
based on IBM model 1 and the direct phrase-based method) were tested on output from
different machine translation systems, including Systran as a non-statistical MT system.
The experimental results are shown in Table 7. They can be summarized as follows:
 In all settings, both measures distinctly decrease the CER compared to the
baseline. In one case (Spanish to English, Systran), the achieved CER is as
much as 60% lower than the baseline CER.
 On French to English and German to English, all improvements are
significant at the 1% level. On Spanish to English, which is the language
pair yielding by far the lowest baseline CER, only the phrase-based
measure achieves a reduction at this level of significance.
 In all but one case, the direct phrase-based approach outperforms the
IBM-1?based method significantly. This tendency is consistent for both
CER and IROC. The relative difference in CER is up to 20%.
Figure 2
ROC curves for different confidence measures. TransType2 French ? English test set. References
based on WER (left) and PER (right). Hypotheses from the phrase-based system.
30
Ueffing and Ney Word-Level Confidence Estimation for MT
Table 7
Classification performance in terms of CER (%) and IROC (%) for different system-independent
confidence measures. TransType2 test sets. Reference based on WER. Hypotheses from different
MT systems.
Task Model AT PBT Systran
CER IROC CER IROC CER IROC
F ? E baseline 42.5 ? 42.2 ? 32.8 ?
99% confidence interval ?2.3 ? ?2.3 ? ?1.7 ?
IBM-1 (max.) 34.1 68.3 35.6 66.9 26.0 81.3
direct phrase-based 30.2 73.0 30.6 74.4 22.7 83.2
S ? E baseline 20.8 ? 19.2 ? 43.7 ?
99% confidence interval ?1.9 ? ?2.0 ? ?1.5 ?
90% confidence interval ?1.2 ? ?1.3 ? ?1.0 ?
IBM-1 (max.) 20.0 66.8 18.3 73.2 21.7 85.5
direct phrase-based 17.5 76.0 16.4 77.0 17.3 87.5
G ? E baseline 49.2 ? 48.4 ? 37.4 ?
99% confidence interval ?2.2 ? ?2.4 ? ?1.4 ?
IBM-1 (max.) 32.7 73.3 32.8 72.2 23.6 80.7
direct phrase-based 27.6 79.1 26.4 80.3 24.3 81.4
 On the German to English Systran hypotheses, both confidence measures
discriminate similarly well. In terms of CER, the IBM model 1 is slightly
better, whereas the phrase-based method achieves the highest IROC value.
EPPS task. Further experiments comparing the classification performance of the differ-
ent confidence measures were carried out on the EPPS data task, which is structurally
different from the Xerox task. The EPPS collection consists of speeches given in the
plenary sessions of the European Parliament, translated from Spanish into English. The
EPPS task is more challenging than the Xerox manuals because the domain is almost
unrestricted and the translation has to cope with effects of spontaneous speech. The goal
of these experiments is to find out whether the confidence measures perform equally
well on this challenging task as on the Xerox task. The development and test set of the
EPPS data are provided with two references each. This makes it possible to compare the
two ways of handling multiple references: As explained in Section 7.2, the true class of a
word can be determined either with respect to the pooled references or to the reference
with minimal distance.
Table 8 presents the CER and IROC values for different confidence measures on the
EPPS task. The classification with respect to m-WER and m-PER (i.e., considering only
the nearest reference) as word error measures was investigated. The confidence mea-
sures based on the fixed position were not calculated because the previous experiments
showed that they perform significantly worse than the other measures. It can be seen in
the table that the word posterior probabilities derived from the Bayes risk for the word
error measures perform best: The Levenshtein-based confidence measure discriminates
best for m-WER and the count-based approach for m-PER. They are clearly superior to
all other confidence measures, especially in terms of IROC. For WER-based classifica-
tion, the word graph-based method performs similarly well to the Levenshtein-based
measure in terms of CER, but significantly worse if IROC is considered.
The results achieved by the direct phrase-based approach on this task are not as
good as on the Xerox data. The reason for this is that the domain of the EPPS collection
31
Computational Linguistics Volume 33, Number 1
Table 8
Classification performance in terms of CER (%) and IROC (%) for different confidence measures.
EPPS Spanish ? English test set. Reference based on m-WER and m-PER, confidence measures
optimized accordingly. Hypotheses from the phrase-based system.
Model m-WER m-PER
CER IROC CER IROC
baseline 30.2 ? 23.5 ?
99% confidence interval ?1.2 ? ?1.0 ?
15,000-best lists, Levenshtein 25.7 75.4 21.6 74.2
window ?3 26.7 69.9 21.4 71.8
count-based 27.6 71.4 21.2 78.3
word graphs, window ?3 25.6 72.1 21.9 73.2
IBM-1 (max.) 27.7 68.7 21.5 72.5
direct phrase-based 26.8 67.5 21.2 70.9
is almost unrestricted. We found in a data analysis that the phrase models do not
capture the data as well as they do in the Xerox domain (Ueffing 2006). Nevertheless, for
m-PER?based classification, the direct phrase-based measures achieve the same reduc-
tion in CER over the baseline as the system-based method using count information.
Because the direct phrase-based confidence measures completely disregard the target
position of the word, they are better suited for PER-based classification than for WER.
As is to be expected, the IBM model 1?based confidence measure performs better
for reference tags defined by m-PER than for m-WER. However, it is among the methods
with the worst discriminative power in both cases.
In general, the improvements over the CER baseline are not as high on these
EPPS data as on the TransType2 corpora. The relative gain in CER is 15% for the best
confidence measure. But because the test corpora are large?with 20,000 running words
they are about twice as big as the TransType2 test sets?all achieved improvements
are significant at the 1% level. The IROC values are comparable to those achieved on
TransType2 data. The fact that the IROC is independent of the baseline error allows
for the conclusion that the confidence measures are well-suited for this challenging
translation task as well.
Figure 3
ROC curves for different confidence measures. EPPS Spanish ? English test set. References
based on m-WER (left) and m-PER (right). Hypotheses from the phrase-based system.
32
Ueffing and Ney Word-Level Confidence Estimation for MT
The ROC curves shown in Figure 3 further illustrate the classification performance
of the different measures. The left curve shows the results for m-WER?based classifi-
cation, and the right one for m-PER. One can see that for m-WER, the IBM-1?based
and the direct phrase-based confidence measures perform very similarly. There is no
clear difference between these two approaches and the one calculated over a window
of target positions. The discriminative power of the direct model is higher for a lower
correct acceptance ratio, whereas the system-based measure performs better for a high
correct acceptance ratio. The Levenshtein-based word posterior probabilities are clearly
superior to all other approaches. The ROC curve lies beyond the others over the whole
range. For PER, the classifier based on word counts dominates all other confidence
measures. The three other methods show relatively similar performance.
For all results presented so far, the reference tags were determined by comparing
each hypothesis to the most similar reference. As mentioned in Section 7.2, it is also
possible to pool the references instead. Table 9 presents an assessment of the discrimi-
native power of different confidence measures for these reference tags. The conclusions
from these results are the same as for those in Table 8: The Levenshtein-based method
performs best for WER, and the count-based one for PER. All reported improvements in
CER are significant at the 1% level. The IROC values for the pooled error measures are
higher than for m-WER and m-PER for all confidence measures. Obviously, this method
of error counting is easier to assess using confidence measures. The differences in CER
are not as large here as in Table 8. However, the IROC values provide a clear indication
of the differences in quality between the classifiers.
NIST task. The third translation task that was used for the evaluation of the confidence
measures proposed in this article is part of the NIST MT evaluation campaign. The task
here is the translation of news articles from Chinese into English. As with the EPPS data,
the domain is basically unrestricted.
The experimental results are presented in Table 10. The confidence measures
that perform best on the two other tasks were evaluated on the NIST data. The
results support those achieved on the EPPS collection. All confidence measures reduce
CER over the baseline with significance at the 1% level. For reference tags defined
by m-WER, the confidence measure using Levenshtein alignment over N-best lists
Table 9
Classification performance in terms of CER (%) and IROC (%) for different confidence measures.
EPPS Spanish ? English test set. Reference based on pooled WER and PER, confidence
measures optimized accordingly. Hypotheses from the phrase-based system.
Model pooled WER pooled PER
CER IROC CER IROC
baseline 23.5 ? 18.5 ?
99% confidence interval ?1.1 ? ?1.0 ?
15,000-best lists, Levenshtein 21.3 77.5 17.0 76.4
window ?3 21.8 71.5 17.1 73.2
count-based 21.8 73.7 16.7 80.6
word graphs, window ?3 21.8 73.0 18.1 74.1
IBM-1 (max.) 21.7 70.2 16.9 74.3
direct phrase-based 21.3 69.5 16.8 69.3
33
Computational Linguistics Volume 33, Number 1
Table 10
Classification performance in terms of CER (%) and IROC (%) for different confidence measures.
NIST04 Chinese ? English test set. Reference based on m-WER and m-PER, confidence
measures optimized accordingly. Hypotheses from the phrase-based system.
model m-WER m-PER
CER IROC CER IROC
baseline 46.2 ? 32.7 ?
99% confidence interval ?1.0 ? ?0.6 ?
10,000-best lists, Levenshtein 37.2 67.4 30.5 67.5
window ?3 39.2 64.4 30.5 67.0
count-based 37.0 66.0 28.4 72.0
IBM-1 (max) 42.9 58.0 31.9 59.9
direct phrase-based 37.3 66.7 27.1 71.8
performs best. Especially in terms of IROC, this method is clearly superior to the
other confidence measures. The count-based method achieves a CER that is 0.2% lower,
which is not significant. For classification with respect to m-PER, there are two meth-
ods that outperform the others: the count-based confidence measure calculated over
N-best lists and the direct phrase-based approach. They achieve CER and IROC values
that differ significantly from those of the other measures. However, neither of the two
approaches is clearly superior to the other: The direct phrase-based confidence measure
achieves a lower CER of 27.1%, whereas the count-based confidence measure calculated
over N-best lists achieves a slightly higher IROC value. The confidence measure based
on IBM model 1 shows by far the worst discriminative power for both m-WER- and
m-PER-based classification. The CER obtained with this method is significantly higher
than those of all other measures.
Combination of features. Because feature combination yields good results in the exper-
iments reported in related work such as Blatz et al (2003), we performed similar
experiments. The confidence measures investigated here were combined log-linearly
as described in Section 6. The resulting confidence measures were evaluated on all
three translation tasks. The three single word posterior probabilities that perform best
in each setting were used in the combination. For the confidence estimation with respect
to reference tags defined by m-WER, these are:
 the system-based word posterior probabilities based on Levenshtein
alignment
 the system-based word posterior probabilities performing windowing
over target positions
 the direct phrase-based method
If the reference tags are determined by m-PER, the features used differ slightly, depend-
ing on the corpus. The measures that are combined are three of the following:
 the system-based word posterior probabilities based on the word count
 the system-based word posterior probabilities performing windowing
over target positions
34
Ueffing and Ney Word-Level Confidence Estimation for MT
 the confidence measure based on IBM model 1
 the direct phrase-based method
The experimental results for the combined confidence measures are presented in
Table 11. They show that the resulting confidence measure outperforms the best single
method. The improvement in CER is up to 1.8% in absolute terms. In terms of IROC,
the gain is up to 4.4 points. This is in the same range as the improvements achieved in
the CLSP summer workshop (Blatz et al 2003). However, there is one case in which the
IROC decreases, namely the m-PER?based classification on EPPS Spanish to English.
This can be explained by the fact that the combination was optimized with respect
to CER. In order to avoid this type of inconsistency, the optimization could be performed
considering a combination of CER and IROC as criterion.
8. Rescoring
8.1 Approach
This section reports on the use of word posterior probabilities for rescoring of N-best
lists. The rescoring is performed as follows: For every hypothesis in the N-best list, the
confidence of each word in the sentence is calculated. These word posterior probabilities
are multiplied to obtain a score for the whole sentence. This sentence score is then
used as an additional model for N-best list rescoring. It serves as an indicator of the
overall quality of the generated hypothesis. Additionally, the minimal word posterior
probability over the sentence is determined. This can be seen as an indicator of whether
the hypothesis contains words that are likely to be incorrect. These new models are
combined with the existing models (such as the score assigned by the underlying SMT
system and additional language model scores) in a log-linear manner. The scaling
factors of all models are optimized on the development corpus using the Downhill
Simplex algorithm. This combination using the optimized factors is then applied and
evaluated on the test set.
Table 11
Classification performance in terms of CER (%) and IROC (%) for a log-linear combination of
word posterior probabilities. Test sets from all three tasks. References based on m-WER and
m-PER. Hypotheses from the phrase-based system.
Reference tag m-WER m-PER
Task confidence measure CER IROC CER IROC
TransType2 F ? E baseline 42.2 ? 34.2 ?
best single 30.6 74.4 27.0 76.5
combination of 3 29.5 75.5 25.4 78.4
EPPS S ? E baseline 30.2 ? 23.5 ?
best single 25.7 75.4 21.2 78.3
combination of 3 25.7 76.1 20.1 76.9
NIST C ? E baseline 46.2 ? 32.7 ?
best single 37.3 67.2 27.4 71.3
combination of 3 35.5 68.0 25.8 75.7
35
Computational Linguistics Volume 33, Number 1
8.2 Experimental Results
Rescoring was carried out on EPPS data using the direct phrase-based confidence
measures. Within the project TC-STAR, an MT evaluation campaign was performed
in March 2005 to compare the research systems of the consortium members (Ney et al
2005). Different conditions concerning the input data were defined. In the following,
rescoring results on the verbatim transcriptions will be presented. The translations
that RWTH submitted to this evaluation were generated by the phrase-based translation
system described in Section 2.2. N-best lists were generated for development and test
corpus, with a maximum length of 20,000 and 15,000, respectively. These were then
rescored with an IBM model 1, a 4-gram language model, and a deletion model based
on IBM-1. The weights for all these models and for the sentence probability assigned
by the SMT system were optimized with respect to BLEU score on the development
corpus. For a detailed description of the system, see Vilar et al (2005). This system was
ranked first in the evaluation round according to all evaluation criteria (Ney et al 2005).
Two different sets of rescoring experiments were performed. They differ only in
their starting points: The first one starts from the baseline system without rescoring.
The sub-model weights of this system were optimized with respect to BLEU on the
development set, but no additional models were used for rescoring the N-best list.
This experiment was performed to analyze the maximum improvement that can be
achieved through rescoring with confidence measures. The second experiment starts
from the system that has already been rescored with the three different models men-
tioned above. This is the system that was used in the TC-STAR evaluation campaign,
and that was ranked first there. In this setting, it can be seen whether the rescoring
with confidence measures manages to improve upon the best available system as well.
Furthermore, it is possible to analyze whether the gains from all rescoring models are
additive.
The results are shown in Table 12. The upper block evaluates the translation quality
without considering case, and the second one contains the case-sensitive evaluation.
These different figures are presented here in order to separate the effect of the transla-
tion and the true-casing process. The translation system was trained on a lower-cased
corpus, and the true-casing is performed as an additional post-processing step.
Table 12
Translation quality for rescoring with confidence measures. EPPS Spanish ? English test set.
Optimized for BLEU.
case? System WER (%) PER (%) BLEU (%) NIST
no baseline 40.9 30.4 45.5 9.83
+ direct phrase-based confidence measure 40.8 29.9 46.5 9.93
+IBM-1+LM+deletion model 40.6 29.5 46.6 9.99
+direct phrase-based confidence measure 40.4 29.4 47.2 10.04
yes baseline 42.5 32.2 45.1 9.67
+ direct phrase-based confidence measure 42.7 32.0 45.6 9.68
+IBM-1+LM+deletion model 42.5 31.7 45.9 9.75
+direct phrase-based confidence measure 42.4 31.6 46.2 9.78
second best translation system 43.9 33.4 44.1 9.47
36
Ueffing and Ney Word-Level Confidence Estimation for MT
Let us first consider the case-insensitive results. The baseline is the single best
output of the translation system. This system can be improved through rescoring with
confidence measures by 1 BLEU point. This is only 0.1 BLEU points less than the
gain achieved from rescoring with the three other models. The system from the second
setup (rescored with IBM model 1, the language and the deletion model) improves the
BLEU score by 1.1 points over the baseline. Another 0.6 BLEU points can be gained
through additional rescoring with the direct phrase-based confidence measures. The
improvement is consistent across all four automatic evaluation criteria. Naturally, the
gain in BLEU score is higher than for the other measures, because the system was
optimized with respect to BLEU.
In the TC-STAR evaluation campaign, case information was taken into account.
The corresponding results are presented in the second block of the table. The overall
translation quality is lower if case is considered. For all models applied here, the gain
achieved through rescoring is not as big as in the case-insensitive evaluation. If only the
confidence measures are used for rescoring, the BLEU score is increased by 0.5 points.
The NIST score and the error measures change only slightly. However, when all four
rescoring models are applied, the system is significantly improved. The models used
in the TC-STAR evaluation yield an increase of 0.8 BLEU points. The word posterior
probabilities add another 0.3 points to this. This change is rather small, but comparable
to the contribution of each single rescoring model used in the evaluation campaign.
For comparison, the translation quality of the second best system in this campaign is
reported in the last row of the table. The difference in BLEU score between the RWTH
system and the second best can be significantly improved through rescoring.
9. Conclusion
In this work, we set up a probabilistic framework for the computation of word posterior
probabilities for machine translation. Within this framework, different concepts of word
posterior probabilities were defined and analyzed. Several approaches to the calculation
of word posterior probabilities were investigated and compared: system-based methods
that explore information provided by the SMT system that generated the translations,
and direct model-based methods that make use of statistical (translation) models.
The use of word posterior probabilities as confidence measures was studied, in-
cluding their application in a rescoring scenario. The proposed confidence measures
were systematically evaluated on different translation tasks and different language
pairs. On all corpora, the best methods developed here reduce the confidence error rate
significantly (at the 1% level). The direct confidence measures were also successfully
applied to output from a non-statistical MT system.
The results of the experiments can be summarized as follows:
 The performance of the confidence measures depends heavily on the
word error measure that defines the reference tags. Naturally, the
word posterior probabilities derived from Bayes risk for this word error
measure discriminate best. For WER, this is the approach based on the
Levenshtein alignment, and for PER this is the method that considers
the counts of the words.
 The direct phrase-based confidence measures perform very well on the
restricted domain of the TransType2 corpora consisting of technical
manuals. There, they outperform all other measures. However, this is
37
Computational Linguistics Volume 33, Number 1
not the case for data from domains that are basically unrestricted,
such as the EPPS and NIST corpora. There, the system-based measures
discriminate better for reference tags given by WER. For PER-based
confidence estimation, the direct phrase-based confidence measure and
the count-based confidence measure calculated over N-best lists show
the best performance.
 The confidence measures based on IBM model 1 normally perform worse
than the system-based or direct phrase-based methods. The reason for this
is that the IBM model 1 is a very simple model that does not consider the
context of a target word at all.
 The combination of several different word posterior probabilities into one
confidence measure yields better confidence estimation performance than
the best single feature. However, the word posterior probabilities
proposed here proved to be strong stand-alone features (see also
experiments reported in Blatz et al [2003]).
 Rescoring with confidence measures was shown to improve translation
quality. The SMT system investigated here was the one that was ranked
first in the TC-STAR evaluation campaign in March 2005. It was
consistently improved through rescoring with confidence measures.
Acknowledgments
This work was partly funded by the
European Union under the RTD project
TransType2 (IST?2001?32091), and under the
integrated project TC-STAR?Technology and
Corpora for Speech to Speech Translation
(IST-2002-FP6-506738). Nicola Ueffing would
like to thank her former and current
colleagues at RWTH Aachen University and
the National Research Council Canada and
everybody from the ?CE for SMT? workshop
team for their feedback and support, and the
anonymous reviewers for their helpful
comments on earlier versions of this article.
References
Akiba, Yasuhiro, Eiichiro Sumita, Hiromi
Nakaiwa, Seiichi Yamamoto, and
Hiroshi G. Okuno. 2004. Using a mixture
of N-best lists from multiple MT systems
in rank-sum-based confidence measure
for MT outputs. In Proceedings of COLING
?04: The 20th International Conference on
Computational Linguistics, pages 322?328,
Geneva, Switzerland.
Bertoldi, Nicola, Roldano Cattoni, Mauro
Cettolo, and Marcello Federico. 2004. The
ITC-irst statistical machine translation
system for IWSLT-2004. In Proceedings
of the International Workshop on Spoken
Language Translation (IWSLT), pages 51?58,
Kyoto, Japan.
Bisani, Maximilian and Hermann Ney.
2004. Bootstrap estimates for confidence
intervals in ASR performance evaluation.
In Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 409?412,
Montreal, Canada.
Blatz, John, Erin Fitzgerald, George
Foster, Simona Gandrabur, Cyril
Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003.
Confidence estimation for
machine translation. Final report,
JHU/CLSP Summer Workshop.
http://www.clsp.jhu.edu/ws2003/
groups/estimate/.
Blatz, John, Erin Fitzgerald, George Foster,
Simona Gandrabur, Cyril Goutte,
Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence
estimation for machine translation.
In Proceedings of COLING ?04: The 20th
International Conference on Computational
Linguistics, pages 315?321, Geneva,
Switzerland.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical
machine translation. In Proceedings of
38
Ueffing and Ney Word-Level Confidence Estimation for MT
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 263?270, Ann Arbor, MI.
Duda, Richard O., Peter E. Hart, and
David G. Stork. 2001. Pattern Classification
and Scene Analysis. John Wiley & Sons,
New York.
Gandrabur, Simona and George Foster. 2003.
Confidence estimation for text prediction.
In Proceedings of the Conference on Natural
Language Learning (CoNLL), pages 95?102,
Edmonton, Canada.
Jayaraman, Shyamsundar and Alon Lavie.
2005. Multi-engine machine translation
guided by explicit word matching. In
Proceedings of the 10th Annual Conference
of the European Association for Machine
Translation (EAMT), pages 143?152,
Budapest, Hungary.
Jurafsky, Daniel and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, NJ.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the
Human Language Technology Conference
(HLT/NAACL), pages 127?133, Edmonton,
Canada.
Levenshtein, Vladimir I. 1966. Binary codes
capable of correcting deletions, insertions
and reversals. Soviet Physics Doklady,
10(8):707?710.
Ney, Hermann, Volker Steinbiss, Richard
Zens, Evgeny Matusov, J. Gonzalez,
Young-Suk Lee, Salim Roukos, Marcello
Federico, Muntsin Kolss, and Rafael
Banchs. 2005. TC-STAR deliverable
no. D5: SLT progress report. Technical
report, Integrated project TC-STAR
(IST-2002-FP6-506738) funded by the
European Commission.
http://www.tc-star.org/.
Nie?en, Sonja, Franz J. Och, Gregor Leusch,
and Hermann Ney. 2000. An evaluation
tool for machine translation: Fast
evaluation for MT research. In Proceedings
of the Second International Conference on
Language Resources and Evaluation (LREC),
pages 39?45, Athens, Greece.
NIST. 2002. Automatic evaluation of
machine translation quality using
N-gram co-occurrence statistics.
http://nist.gov/speech/tests/mt/.
NIST. 2004. Machine translation evaluation
Chinese?English. http://nist.gov/
speech/tests/mt/.
Och, Franz J. 2003. Minimum error rate
training in statistical machine
translation. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages
160?167, Sapporo, Japan.
Och, Franz J. and Hermann Ney. 2004. The
alignment template approach to statistical
machine translation. Computational
Linguistics, 30(4):417?449.
Och, Franz J., Christoph Tillmann, and
Hermann Ney. 1999. Improved alignment
models for statistical machine translation.
In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora
(EMNLP/VLC-99), pages 20?28, University
of Maryland, College Park, MD.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of
machine translation. In Proceedings of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 311?318, Philadelphia, PA.
Press, William H., Saul A. Teukolsky,
William T. Vetterling, and Brian P.
Flannery. 2002. Numerical Recipes in
C++. Cambridge University Press,
Cambridge, UK.
Quirk, Chris. 2004. Training a sentence-level
machine translation confidence metric.
In Proceedings of the Fourth International
Conference on Language Resources and
Evaluation (LREC), pages 825?828,
Lisbon, Portugal.
Sanchis, Alberto. 2004. Estimacio?n y aplicacio?n
de medidas de confianza en reconocimiento
automa?tico del habla. Ph.D. thesis,
Departamento de Sistemas Informa?ticos y
Computacio?n, Universidad Polite?cnica de
Valencia, Valencia, Spain.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit.
In Proceedings of the International Conference
on Spoken Language Processing (ICSLP),
volume 2, pages 901?904, Denver, CO.
TC-STAR. 2005. TC-STAR?Technology
and corpora for speech to speech
translation. Integrated project TC-STAR
(IST-2002-FP6-506738) funded by the
European Commission.
http://www.tc-star.org/.
Tillmann, Christoph. 2003. A projection
extension algorithm for statistical
machine translation. In Proceedings of
the Conference on Empirical Methods for
Natural Language Processing (EMNLP),
pages 1?8, Sapporo, Japan.
TransType2. 2005. TransType2?Computer
assisted translation. RTD project
TransType2 (IST?2001?32091) funded
39
Computational Linguistics Volume 33, Number 1
by the European Commission.
http://tt2.atosorigin.es/.
Ueffing, Nicola. 2006. Word Confidence
Measures for Machine Translation. Ph.D.
thesis, Computer Science Department,
RWTH Aachen University, Aachen,
Germany.
Ueffing, Nicola, Klaus Macherey, and
Hermann Ney. 2003. Confidence
measures for statistical machine
translation. In Proceedings of the
MT Summit IX, pages 394?401,
New Orleans, LA.
Ueffing, Nicola and Hermann Ney.
2004. Bayes decision rule and
confidence measures for statistical
machine translation. In Proceedings of
EsTAL?Espan?a for Natural Language
Processing, pages 70?81, Alicante, Spain.
Lecture Notes in Computer Science,
Springer Verlag.
Ueffing, Nicola and Hermann Ney. 2005a.
Application of word-level confidence
measures in interactive statistical
machine translation. In Proceedings
of the 10th Annual Conference of the
European Association for Machine
Translation (EAMT), pages 262?270,
Budapest, Hungary.
Ueffing, Nicola and Hermann Ney. 2005b.
Word-level confidence estimation for
machine translation using phrase-based
translation models. In Proceedings
of the Human Language Technology
Conference (HLT/EMNLP), pages 763?770,
Vancouver, Canada.
Ueffing, Nicola, Franz J. Och, and
Hermann Ney. 2002. Generation of
word graphs in statistical machine
translation. In Proceedings of the Conference
on Empirical Methods for Natural Language
Processing (EMNLP), pages 156?163,
Philadelphia, PA.
Vilar, David, Evgeny Matusov, Sas?a Hasan,
Richard Zens, and Hermann Ney. 2005.
Statistical machine translation of European
parliamentary speeches. In Proceedings of
the MT Summit X, pages 259?266, Phuket,
Thailand.
Vogel, Stephan, Sanjika Hewavitharana,
Muntsin Kolss, and Alex Waibel. 2004.
The ISL statistical translation system for
spoken language translation. In Proceedings
of the International Workshop on Spoken
Language Translation (IWSLT), pages 65?72,
Kyoto, Japan.
Zens, Richard and Hermann Ney. 2004.
Improvements in phrase-based
statistical machine translation. In
Proceedings of the Human Language
Technology Conference (HLT/NAACL),
pages 257?264, Boston, MA.
Zens, Richard and Hermann Ney. 2005.
Word graphs for statistical machine
translation. In 43rd Annual Meeting of the
Association for Computational Linguistics:
Proceedings of the Workshop on Building and
Using Parallel Texts: Data-Driven Machine
Translation and Beyond, pages 191?198,
Ann Arbor, MI.
Zens, Richard and Hermann Ney. 2006.
N-gram posterior probabilities for
statistical machine translation. In
Human Language Technology
Conference of the North American Chapter
of the Association for Computational
Linguistics (HLT/NAACL): Proceedings
of the Workshop on Statistical Machine
Translation, pages 72?77,
New York, NY.
40
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 701?704,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Hybrid Morphologically Decomposed Factored Language Models for
Arabic LVCSR
Amr El-Desoky, Ralf Schlu?ter, Hermann Ney
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{desoky,schluter,ney}@cs.rwth-aachen.de
Abstract
In this work, we try a hybrid methodology for
language modeling where both morphological
decomposition and factored language model-
ing (FLM) are exploited to deal with the com-
plex morphology of Arabic language. At the
end, we are able to obtain from 3.5% to 7.0%
relative reduction in word error rate (WER)
with respect to a traditional full-words sys-
tem, and from 1.0% to 2.0% relative WER re-
duction with respect to a non-factored decom-
posed system.
1 Introduction
Arabic language is characterized by a complex mor-
phological structure where different kinds of pre-
fixes and suffixes are appended to the word stems
producing a very large number of inflectional forms.
This leads to poor language model (LM) probabil-
ity estimates, and thus high LM perplexities (PPLs)
causing problems in large vocabulary continuous
speech recognition (LVCSR). One successful ap-
proach to deal with this problem is to consider LMs
including morphologically decomposed words. An-
other approach is to use the factored language mod-
els (FLMs) which are powerful models that com-
bine multiple sources of information and efficiently
integrate them via a complex backoff mechanism
(Bilmes and Kirchhoff, 2003).
Morphological decomposition is successfully
used for Arabic LMs in several previous works.
Some are based on linguistic knowledge, and oth-
ers are based on unsupervised methods. Some of the
linguistic methods are based on the Buckwalter Ara-
bic Morphological Analyzer (BAMA) like in (Lamel
et al, 2008). Alternatively, in our previous work
(El-Desoky et al, 2009), we use the Morphological
Analyzer and Disambiguator for Arabic (MADA)
(Habash and Rambow, 2007). On the other side,
most of the unsupervised methods are based on the
minimum description length principle (MDL) like in
(Creutz et al, 2007).
Another type of models is the FLM, in which
words are viewed as vectors of K factors, so that
wt := {f1:Kt }. A factor could be any feature of the
word such as morphological class, stem, root or even
a semantic feature. An FLM is a model over factors,
i.e., p(f1:Kt |f
1:K
t?1 , f
1:K
t?2 , ..., f
1:K
t?n+1), which could be
reformed as a product of probabilities of the form
p(f |f1, f2, ..., fN ). The main idea of the model is to
backoff to other factors when some word n-gram is
not observed in the training data, thus improving the
probability estimates.
In this work we try to combine the strengths
of morphological decomposition and factored lan-
guage modeling. Therefore, language models with
factored morphemes are used. For this purpose, the
LM training data are processed such that full-words
are decomposed into prefix-stem-suffix format with
different added features. We compare our approach
with the standard full-word, decomposed word, and
factored full-word n-gram approaches.
2 Factorization and Decomposition
We use MADA 2.0 in order to perform morphologi-
cal analysis and attach a complete set of morpholog-
ical tags to Arabic words in context. From those tags
701
we derive three different features. Moreover, we de-
rive a fourth feature based on the root of the word
generated by ?Sebawai? (Darwish, 2002). The list
of features is:
? ?W? (Word): word surface form.
? ?L? (Lexeme): word lexeme.
? ?M? (Morph): morphological description.
? ?P? (Pattern): word after subtracting root.
The LM training corpora are processed so that
words are replaced by the factored representation as
required by SRILM-FLM extensions (Kirchhoff et
al., 2008). Then, word decomposition is performed
based on MADA as described in our previous publi-
cation (El-Desoky et al, 2009).
3 FLM topologies
In order to obtain a good performance via FLMs, we
need to optimize the FLM parameters: the combi-
nation of the conditioning factors, backoff path, and
smoothing options. For this purpose, we use a Ge-
netic Algorithm based FLM optimization tool (GA-
FLM) developed by Kirchhoff (2006) which seeks to
minimize the PPL over some held-out text. Further-
more, we apply some manual optimization to fine
tune the FLM parameters. For memory limitations,
we only use factors up to 2 previous time slots (tri-
gram like models). Finally, we come up with a set
of competing FLMs with rather close PPLs. In Ta-
ble 1, we record the PPLs measured for some held-
out text. The first column gives the combination of
the parent factors. So that, FLM1 corresponds to
the model P (Wt|Wt?1,Wt?2), which is the FLM
equivalent of the standard tri-gram LM (our base-
line model), while FLM2 & FLM3 correspond to
the model P (Wt|Wt?1,Mt?1, Lt?1, Pt?1,Wt?2),
however FLM4 & FLM5 correspond to the model
P (Wt|Wt?1,Mt?1, Lt?1,Wt?2,Mt?2, Lt?2). The
?gtmin? refers to the count threshold that is suffi-
cient to have a language model hit at some node of
the the backoff graph (for exact topologies, contact
the first author). From Table 1, comparing PPLs
(non-normalized) across factored and non-factored
LMs, we see that using more factors other than the
normal word could help decreasing the PPL. This is
true for all the used types of vocabulary units.
vocabulary
FLMx parent factors FW PD FD
1: W1 W2 (baseline) 302.6 284.1 82.7
W1 M1 L1 P1 W2
2: gtmin = 1 306.2 296.9 83.2
3: gtmin = 2-4 290.9 279.1 79.8
W1 M1 L1 W2 M2 L2
4: gtmin = 1 300.2 291.1 83.6
5: gtmin = 2-4 294.5 283.7 81.1
Table 1: perplexities of the FLMs using vocabularies:
(FW: 70k full-words; PD: partially decomposed with 20k
ful-words + 50k morphemes; FD: 70k fully decomposed).
FLMx parent factors WER [%]
1: W1 W2 (baseline) 20.4
W1 M1 L1 P1 W2
2: gtmin = 1 20.2
3: gtmin = 2-4 20.4
W1 M1 L1 W2 M2 L2
4: gtmin = 1 19.9
5: gtmin = 2-4 20.3
Table 2: WERs using FLMs based on 70k full-words.
In order to select the best FLM topology, we run
a simple one pass recognition for a small internal
dev corpus derived from GALE data sets, consists
of 40 minutes of audio data recorded during January
to March 2007. The acoustic models are within-
word tri-phone models trained using 1100h of au-
dio material. The basic acoustic models are trained
based on Maximum Likelihood (ML) method. Then,
a discriminative training based on Minimum Phone
Error (MPE) criterion is performed to enhance the
models. A 70k full-words lexicon is used. The
FLM training data consists of 206 Million running
full-words. A standard bi-gram LM based on full-
words is used to generate N-best lists, then N-best
list rescoring is performed using the different FLM
topologies shown in Table 1. We start by N = 1000-
best down to 3-best sentences. Using N = 10 always
gives the best results. The recognition WERs are
recorded in Table 2. The least WER is obtained with
FLM4. We note that the best FLM does not corre-
spond to the least PPL. This is because a higher ?gt-
min? value causes more backoff in cases of insuffi-
cient data leading to better estimates. Therefore, we
select FLM4 for the coming experiments.
702
4 Experimental Setup
Our recognition system is close to the one described
in section 3. However, we use within and across-
word models at different recognition passes. In ad-
dition, we use 70k or 256k lexicon of full-words or
partially decomposed words. Alternatively, we eval-
uate the results on the GALE 2007 development and
evaluation sets (dev07: 2.5h; eval07: 4h). Our rec-
ognizer works in 3 passes. In the first pass, within-
word acoustic models are used with no adaptation,
along with a standard bi-gram LM to generate lat-
tices, followed by a standard tri-gram or 4-gram LM
rescoring of lattices. The second pass does the same,
but it uses across-word models with Constrained
Maximum Likelihood Linear Regression (CMLLR)
adaptation. Then, a third pass with additional Max-
imum Likelihood Linear Regression (MLLR) adap-
tation is performed, using a standard bi-gram LM to
generate lattices or N-best lists. Then, one of the fol-
lowing is performed: 1) lattice rescoring using stan-
dard tri-gram or 4-gram LM, 2) N-best list rescoring
using FLMs based on full-words, partially or fully
decomposed words.
5 Experiments
In this section, we record our recognition results
for: 1) systems based on full-words, and 2) systems
based on decomposed words. Also, we introduce
additional results for larger lexicon sizes.
5.1 Systems Based on Full-words
In this section, we present the WERs of our recogni-
tion systems based on full-words. Where, during the
search, we use a lexicon of 70K full-words. In the
first 2 passes, we use a standard bi-gram LM to gen-
erate lattices, followed by a standard tri-gram LM
rescoring of lattices. However, in the third pass, we
generate both lattices and N-best lists based on the
same bi-gram LM. The final lattices and N-best lists
are rescored using different LMs as shown in Table
3. In case we perform N-best list rescoring with a
FLM, the N-best lists are processed to produce fac-
tored representation, followed by partial or full de-
composition as previously described in section 2.
It is clear from Table 3 that the least WER is
achieved when using N-best list rescoring using a
full-words based FLM. This gives an absolute im-
LM rescoring (3rd pass) Dev07 [%]
tri-gram lattice resc. (baseline) 16.5
4-gram lattice resc. 16.3
N-best FLM resc.:
+ FW (original N-best) 15.7
+ PD (decomposed N-best) 15.8
+ FD (decomposed N-best) 16.0
Table 3: WERs for 70k full-words systems.
LM rescoring (3rd pass) Dev07 [%]
tri-gram lattice resc. (baseline) 14.7
4-gram lattice resc. 14.5
N-best FLM resc.:
+ FW (re-joint N-best) 14.6
+ PD (original N-best) 14.3
+ FD (decomposed N-best) 14.4
Table 4: WERs for 70k partially decomposed systems
(20k full-words + 50k morphemes).
provement of 0.8% (about 4.8% relative) compared
to the standard tri-gram lattice rescoring. On the
other hand, we have 0.6% absolute improvement
(about 3.7% relative) compared to the standard 4-
gram lattice rescoring. Decomposition does not help
in this case. This is because the original N-best lists
are generated in full-words format, whose decom-
position might not lead to better LM scores. For this
reason, we expect that it is better to start with a de-
composed LM for lattice and N-best generation.
5.2 Systems Based on Decomposed Words
This section introduces the WERs of our systems
based on decomposed words. We use a similar setup
as in section 5.1. However, we use a lexicon and a
bi-gram LM based on a 70k partially decomposed
words (20k full-words + 50k morphemes). Table 4
presents the results. As expected, we get the best
WER when using N-best list rescoring with a FLM
based on partially decomposed words. An absolute
improvement of 0.4% (2.7% relative) is achieved
compared to the new baseline. Compared to the old
baseline of Table 3, we get an absolute improvement
of 2.2% (13.3% relative).
5.3 Larger Lexicon Sizes
Now, we increase the size of our lexicon to 256k
partially decomposed words (20k full-words + 236k
703
Dev07 Eval07
System [%] [%]
traditional full-words 14.9 16.5
partially decomposed
+ 4-gram lat. resc. (baseline) 14.2 16.1
+ N-best FLM resc.:
+ FW (re-joint N-best) 14.1 -
+ PD (original N-best) 13.9 15.9
+ FD (decomposed N-best) 14.0 -
Table 5: WERs for 256k full-words, and partially decom-
posed systems (20k full-words + 236k morphemes).
70k vocabularies 256k vocabularies
Corpus FW PD FD FW PD FD
Dev07 3.65 1.33 0.75 1.36 0.51 0.24
Eval07 4.82 1.94 1.13 1.85 0.64 0.41
Table 6: OOVs [%] of the used vocabularies.
morphemes). In addition, we use a standard 4-
gram LM for rescoring the bi-gram lattices in the
first 2 passes. To complete our comparisons, we
record the WERs using traditional 256k full-words
lexicon, standard bi-gram search, and standard 4-
gram LM for lattice rescoring, with no decomposi-
tion or factorization. In Table 5, we see that the im-
provement persists for the larger lexicon. Compared
to the new baseline, the 256k decomposed system
achieves WER reductions of [dev07: 0.3% absolute
(2.1% relative); eval07: 0.2% absolute (1.2% rela-
tive)] when using N-best list rescoring with a FLM
based on partially decomposed words. Moreover, it
improves over the traditional full-words by [dev07:
1.0% absolute (6.7% relative); eval07: 0.6% abso-
lute (3.6% relative)]. The out-of-vocabulary rates
(OOVs) are given in Table 6. It is worth noting that
using fully decomposed lexicons as well as higher
order LMs could not improve WERs, this we previ-
ously proved in (El-Desoky et al, 2009).
6 Conclusions
We have introduced a hybrid approach to Ara-
bic language modeling. Our approach combines
the strengths of both morphological decomposition
and factored language modeling. Thus, we have
used language models with factored morphemes.
We have compared our approach to traditional ap-
proaches like: standard full-word n-grams, standard
decomposed n-grams, and full-word based factored
language models. Finally, we could achieve some
improvements over all the traditional approaches.
Nevertheless, we have only considered the use of
factored language models in the rescoring phase.
Acknowledgments
This material is based upon work supported by the
DARPA under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions expressed in
this material are those of the authors and do not nec-
essarily reflect the views of DARPA.
References
J. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In Proc. Hu-
man Language Technology Conf. of the North Ameri-
can Chapter of the ACL, volume 2, pages 4 ? 6, Ed-
monton, Canada, May.
M. Creutz, T. Hirsimki, M. Kurimo, A. Puurula,
J. Pylkknen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Saraclar, and A. Stolcke. 2007. Morph-based
speech recognition and modeling of out-of-vocabulary
words across languages. ACM Transactions on Speech
and Language Processing, 5(1), December.
K. Darwish. 2002. Building a shallow Arabic morpho-
logical analyzer in one day. In ACL workshop on Com-
putational approaches to semitic languages, Philadel-
phia, PA, USA, July.
A. El-Desoky, C. Gollan, D. Rybach, R. Schlu?ter, and
H. Ney. 2009. Investigating the use of morphological
decomposition and diacritization for improving Arabic
LVCSR. In Interspeech, pages 2679 ? 2682, Brighton,
UK, September.
N. Habash and O. Rambow. 2007. Arabic diacritiza-
tion through full morphological tagging. In Proc. Hu-
man Language Technology Conf. of the North Ameri-
can Chapter of the ACL, volume Companion, pages 53
? 56, Rochester, NY, USA, April.
K. Kirchhoff, D. Vergyri, J. Bilmes, K. Duh, and A. Stol-
cke. 2006. Morphology-based language modeling for
conversational Arabic speech recognition. Computer
Speech and Language, 20(4):589 ? 608, October.
K. Kirchhoff, J. Bilmes, and K. Duh. 2008. Factored
language model tutorial. Technical report, Department
of Electrical Engineering, University of Washington,
Seattle, Washington, USA, February.
L. Lamel, A. Messaoudi, and J.L Gauvain. 2008. Investi-
gating morphological decomposition for transcription
of Arabic broadcast news and broadcast conversation
data. In Interspeech, volume 1, pages 1429 ? 1432,
Brisbane, Australia, September.
704
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 347?351,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Insertion and Deletion Models for Statistical Machine Translation
Matthias Huck and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{huck,ney}@cs.rwth-aachen.de
Abstract
We investigate insertion and deletion models
for hierarchical phrase-based statistical ma-
chine translation. Insertion and deletion mod-
els are designed as a means to avoid the omis-
sion of content words in the hypotheses. In
our case, they are implemented as phrase-level
feature functions which count the number of
inserted or deleted words. An English word is
considered inserted or deleted based on lex-
ical probabilities with the words on the for-
eign language side of the phrase. Related tech-
niques have been employed before by Och et
al. (2003) in an n-best reranking framework
and by Mauser et al (2006) and Zens (2008)
in a standard phrase-based translation system.
We propose novel thresholding methods in
this work and study insertion and deletion fea-
tures which are based on two different types of
lexicon models. We give an extensive exper-
imental evaluation of all these variants on the
NIST Chinese?English translation task.
1 Insertion and Deletion Models
In hierarchical phrase-based translation (Chiang,
2005), we deal with rules X ? ??, ?,? ? where
??, ?? is a bilingual phrase pair that may contain
symbols from a non-terminal set, i.e. ? ? (N ?
VF )+ and ? ? (N ?VE)+, where VF and VE are the
source and target vocabulary, respectively, and N is
a non-terminal set which is shared by source and tar-
get. The left-hand side of the rule is a non-terminal
symbol X ? N , and the ? relation denotes a one-
to-one correspondence between the non-terminals in
? and in ?. Let J? denote the number of terminal
symbols in ? and I? the number of terminal sym-
bols in ?. Indexing ? with j, i.e. the symbol ?j ,
1 ? j ? J?, denotes the j-th terminal symbol on
the source side of the phrase pair ??, ??, and analo-
gous with ?i, 1 ? i ? I? , on the target side.
With these notational conventions, we now de-
fine our insertion and deletion models, each in both
source-to-target and target-to-source direction. We
give phrase-level scoring functions for the four fea-
tures. In our implementation, the feature values are
precomputed and written to the phrase table. The
features are then incorporated directly into the log-
linear model combination of the decoder.
Our insertion model in source-to-target direction
ts2tIns(?) counts the number of inserted words on the
target side ? of a hierarchical rule with respect to the
source side ? of the rule:
ts2tIns(?, ?) =
I??
i=1
J??
j=1
[
p(?i|?j) < ??j
]
(1)
Here, [?] denotes a true or false statement: The result
is 1 if the condition is true and 0 if the condition is
false. The model considers an occurrence of a tar-
get word e an insertion iff no source word f exists
within the phrase where the lexical translation prob-
ability p(e|f) is greater than a corresponding thresh-
old ?f . We employ lexical translation probabilities
from two different types of lexicon models, a model
which is extracted from word-aligned training data
and?given the word alignment matrix?relies on
pure relative frequencies, and the IBM model 1 lex-
icon (cf. Section 2). For ?f , previous authors have
used a fixed heuristic value which was equal for all
347
f ? Vf . In Section 3, we describe how such a global
threshold can be computed and set in a reasonable
way based on the characteristics of the model. We
also propose several novel thresholding techniques
with distinct thresholds ?f for each source word f .
In an analogous manner to the source-to-target di-
rection, the insertion model in target-to-source di-
rection tt2sIns(?) counts the number of inserted words
on the source side ? of a hierarchical rule with re-
spect to the target side ? of the rule:
tt2sIns(?, ?) =
J??
j=1
I??
i=1
[p(?j |?i) < ??i ] (2)
Target-to-source lexical translation probabilities
p(f |e) are thresholded with values ?e which may be
distinct for each target word e. The model consid-
ers an occurrence of a source word f an insertion iff
no target word e exists within the phrase with p(f |e)
greater than or equal to ?e.
Our deletion model, compared to the insertion
model, interchanges the connection of the direction
of the lexical probabilities and the order of source
and target in the sum and product of the term. The
source-to-target deletion model thus differs from the
target-to-source insertion model in that it employs a
source-to-target word-based lexicon model.
The deletion model in source-to-target direction
ts2tDel(?) counts the number of deleted words on the
source side ? of a hierarchical rule with respect to
the target side ? of the rule:
ts2tDel(?, ?) =
J??
j=1
I??
i=1
[
p(?i|?j) < ??j
]
(3)
It considers an occurrence of a source word f a dele-
tion iff no target word e exists within the phrase with
p(e|f) greater than or equal to ?f .
The target-to-source deletion model tt2sDel(?) cor-
respondingly considers an occurrence of a target
word e a deletion iff no source word f exists within
the phrase with p(f |e) greater than or equal to ?e:
tt2sDel(?, ?) =
I??
i=1
J??
j=1
[p(?j |?i) < ??i ] (4)
2 Lexicon Models
We restrict ourselves to the description of the
source-to-target direction of the models.
2.1 Word Lexicon from Word-Aligned Data
Given a word-aligned parallel training corpus, we
are able to estimate single-word based translation
probabilities pRF(e|f) by relative frequency (Koehn
et al, 2003). With N(e, f) denoting counts of
aligned cooccurrences of target word e and source
word f , we can compute
pRF(e|f) =
N(e, f)
?
e? N(e
?, f)
. (5)
If an occurrence of e has multiple aligned source
words, each of the alignment links contributes with
a fractional count.
We denote this model as relative frequency (RF)
word lexicon.
2.2 IBM Model 1
The IBM model 1 lexicon (IBM-1) is the first and
most basic one in a sequence of probabilistic genera-
tive models (Brown et al, 1993). For IBM-1, several
simplifying assumptions are made, so that the proba-
bility of a target sentence eI1 given a source sentence
fJ0 (with f0 = NULL) can be modeled as
Pr(eI1|f
J
1 ) =
1
(J + 1)I
I?
i=1
J?
j=0
pibm1(ei|fj) . (6)
The parameters of IBM-1 are estimated iteratively
by means of the Expectation-Maximization algo-
rithm with maximum likelihood as training criterion.
3 Thresholding Methods
We introduce thresholding methods for insertion and
deletion models which set thresholds based on the
characteristics of the lexicon model that is applied.
For all the following thresholding methods, we dis-
regard entries in the lexicon model with probabilities
that are below a fixed floor value of 10?6. Again, we
restrict ourselves to the description of the source-to-
target direction.
individual ?f is a distinct value for each f , com-
puted as the arithmetic average of all entries
p(e|f) of any e with the given f in the lexicon
model.
348
MT06 (Dev) MT08 (Test)
NIST Chinese?English BLEU [%] TER [%] BLEU [%] TER [%]
Baseline (with s2t+t2s RF word lexicons) 32.6 61.2 25.2 66.6
+ s2t+t2s insertion model (RF, individual) 32.9 61.4 25.7 66.2
+ s2t+t2s insertion model (RF, global) 32.8 61.8 25.7 66.7
+ s2t+t2s insertion model (RF, histogram 10) 32.9 61.7 25.5 66.5
+ s2t+t2s insertion model (RF, all) 32.8 62.0 26.1 66.7
+ s2t+t2s insertion model (RF, median) 32.9 62.1 25.7 67.1
+ s2t+t2s deletion model (RF, individual) 32.7 61.4 25.6 66.5
+ s2t+t2s deletion model (RF, global) 33.0 61.3 25.8 66.1
+ s2t+t2s deletion model (RF, histogram 10) 32.9 61.4 26.0 66.1
+ s2t+t2s deletion model (RF, all) 33.0 61.4 25.9 66.4
+ s2t+t2s deletion model (RF, median) 32.9 61.5 25.8 66.7
+ s2t+t2s insertion model (IBM-1, individual) 33.0 61.4 26.1 66.4
+ s2t+t2s insertion model (IBM-1, global) 33.0 61.6 25.9 66.5
+ s2t+t2s insertion model (IBM-1, histogram 10) 33.7 61.3 26.2 66.5
+ s2t+t2s insertion model (IBM-1, median) 33.0 61.3 26.0 66.4
+ s2t+t2s deletion model (IBM-1, individual) 32.8 61.5 26.0 66.2
+ s2t+t2s deletion model (IBM-1, global) 32.9 61.3 25.9 66.1
+ s2t+t2s deletion model (IBM-1, histogram 10) 32.8 61.2 25.7 66.0
+ s2t+t2s deletion model (IBM-1, median) 32.8 61.6 25.6 66.7
+ s2t insertion + s2t deletion model (IBM-1, individual) 32.7 62.3 25.7 67.1
+ s2t insertion + t2s deletion model (IBM-1, individual) 32.7 62.2 25.9 66.8
+ t2s insertion + s2t deletion model (IBM-1, individual) 33.1 61.3 25.9 66.2
+ t2s insertion + t2s deletion model (IBM-1, individual) 33.0 61.3 26.1 66.0
+ source+target unaligned word count 32.3 61.8 25.6 66.7
+ phrase-level s2t+t2s IBM-1 word lexicons 33.8 60.5 26.9 65.4
+ source+target unaligned word count 34.0 60.4 26.7 65.8
+ s2t+t2s insertion model (IBM-1, histogram 10) 34.0 60.3 26.8 65.2
+ phrase-level s2t+t2s DWL + triplets + discrim. RO 34.8 59.8 27.7 64.7
+ s2t+t2s insertion model (RF, individual) 35.0 59.5 27.8 64.4
Table 1: Experimental results for the NIST Chinese?English translation task (truecase). s2t denotes source-to-target
scoring, t2s target-to-source scoring. Bold font indicates results that are significantly better than the baseline (p < .1).
global The same value ?f = ? is used for all f .
We compute this global threshold by averaging
over the individual thresholds.1
histogram n ?f is a distinct value for each f . ?f is
set to the value of the n+1-th largest probabil-
ity p(e|f) of any e with the given f .
1Concrete values from our experiments are: 0.395847 for
the source-to-target RF lexicon, 0.48127 for the target-to-source
RF lexicon. 0.0512856 for the source-to-target IBM-1, and
0.0453709 for the target-to-source IBM-1. Mauser et al (2006)
mention that they chose their heuristic thresholds for use with
IBM-1 between 10?1 and 10?4.
all All entries with probabilities larger than the floor
value are not thresholded. This variant may be
considered as histogram ?. We only apply it
with RF lexicons.
median ?f is a median-based distinct value for each
f , i.e. it is set to the value that separates the
higher half of the entries from the lower half of
the entries p(e|f) for the given f .
4 Experimental Evaluation
We present empirical results obtained with the dif-
ferent insertion and deletion model variants on the
349
Chinese?English 2008 NIST task.2
4.1 Experimental Setup
To set up our systems, we employ the open source
statistical machine translation toolkit Jane (Vilar et
al., 2010; Vilar et al, 2012), which is freely avail-
able for non-commercial use. Jane provides efficient
C++ implementations for hierarchical phrase extrac-
tion, optimization of log-linear feature weights, and
parsing-based decoding algorithms. In our experi-
ments, we use the cube pruning algorithm (Huang
and Chiang, 2007) to carry out the search.
We work with a parallel training corpus of 3.0M
Chinese-English sentence pairs (77.5M Chinese /
81.0M English running words). The counts for
the RF lexicon models are computed from a sym-
metrized word alignment (Och and Ney, 2003), the
IBM-1 models are produced with GIZA++. When
extracting phrases, we apply several restrictions, in
particular a maximum length of 10 on source and
target side for lexical phrases, a length limit of five
(including non-terminal symbols) for hierarchical
phrases, and no more than two gaps per phrase.
The models integrated into the baseline are: phrase
translation probabilities and RF lexical translation
probabilities on phrase level, each for both transla-
tion directions, length penalties on word and phrase
level, binary features marking hierarchical phrases,
glue rule, and rules with non-terminals at the bound-
aries, source-to-target and target-to-source phrase
length ratios, four binary features marking phrases
that have been seen more than one, two, three or
five times, respectively, and an n-gram language
model. The language model is a 4-gram with modi-
fied Kneser-Ney smoothing which was trained with
the SRILM toolkit (Stolcke, 2002) on a large collec-
tion of English data including the target side of the
parallel corpus and the LDC Gigaword v3.
Model weights are optimized against BLEU (Pa-
pineni et al, 2002) with standard Minimum Error
Rate Training (Och, 2003), performance is measured
with BLEU and TER (Snover et al, 2006). We em-
ploy MT06 as development set, MT08 is used as un-
seen test set. The empirical evaluation of all our se-
tups is presented in Table 1.
2http://www.itl.nist.gov/iad/mig/tests/
mt/2008/
4.2 Experimental Results
With the best model variant, we obtain a significant
improvement (90% confidence) of +1.0 points BLEU
over the baseline on MT08. A consistent trend to-
wards one of the variants cannot be observed. The
results on the test set with RF lexicons or IBM-1, in-
sertion or deletion models, and (in most of the cases)
with all of the thresholding methods are roughly at
the same level. For comparison we also give a result
with an unaligned word count model (+0.4 BLEU).
Huck et al (2011) recently reported substantial
improvements over typical hierarchical baseline se-
tups by just including phrase-level IBM-1 scores.
When we add the IBM-1 models directly, our base-
line is outperformed by +1.7 BLEU. We tried to
get improvements with insertion and deletion mod-
els over this setup again, but the positive effect was
largely diminished. In one of our strongest setups,
which includes discriminative word lexicon models
(DWL), triplet lexicon models and a discriminative
reordering model (discrim. RO) (Huck et al, 2012),
insertion models still yield a minimal gain, though.
5 Conclusion
Our results with insertion and deletion models for
Chinese?English hierarchical machine translation
are twofold. On the one hand, we achieved sig-
nificant improvements over a standard hierarchical
baseline. We were also able to report a slight gain
by adding the models to a very strong setup with
discriminative word lexicons, triplet lexicon mod-
els and a discriminative reordering model. On the
other hand, the positive impact of the models was
mainly noticeable when we exclusively applied lex-
ical smoothing with word lexicons which are simply
extracted from word-aligned training data, which
is however the standard technique in most state-of-
the-art systems. If we included phrase-level lexical
scores with IBM model 1 as well, the systems barely
benefited from our insertion and deletion models.
Compared to an unaligned word count model, inser-
tion and deletion models perform well.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
350
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter Es-
timation. Computational Linguistics, 19(2):263?311,
June.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc. of
the 43rd Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263?270, Ann Arbor,
MI, USA, June.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proc. of the Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL), pages 144?151, Prague,
Czech Republic, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and Her-
mann Ney. 2011. Lexicon Models for Hierarchi-
cal Phrase-Based Machine Translation. In Proc. of
the Int. Workshop on Spoken Language Translation
(IWSLT), pages 191?198, San Francisco, CA, USA,
December.
Matthias Huck, Stephan Peitz, Markus Freitag, and Her-
mann Ney. 2012. Discriminative Reordering Exten-
sions for Hierarchical Phrase-Based Machine Transla-
tion. In Proc. of the 16th Annual Conference of the Eu-
ropean Association for Machine Translation (EAMT),
Trento, Italy, May.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Human Language Technology Conf. / North
American Chapter of the Assoc. for Computational
Linguistics (HLT-NAACL), pages 127?133, Edmonton,
Canada, May/June.
Arne Mauser, Richard Zens, Evgeny Matusov, Sas?a
Hasan, and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of the Int. Workshop on Spoken
Language Translation (IWSLT), pages 103?110, Ky-
oto, Japan, November.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for Statistical Machine Translation. Technical re-
port, Johns Hopkins University 2003 Summer Work-
shop on Language Engineering, Center for Language
and Speech Processing, Baltimore, MD, USA, August.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Evalu-
ation of Machine Translation. In Proc. of the 40th An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 311?318, Philadelphia, PA, USA,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 223?231, Cambridge,
MA, USA, August.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, USA, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, pages 262?270,
Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2012. Jane: an advanced freely available hier-
archical machine translation toolkit. Machine Trans-
lation, pages 1?20. http://dx.doi.org/10.1007/s10590-
011-9120-y.
Richard Zens. 2008. Phrase-based Statistical Machine
Translation: Models, Search, Training. Ph.D. thesis,
RWTH Aachen University, Aachen, Germany, Febru-
ary.
351
Proceedings of NAACL-HLT 2013, pages 649?654,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Phrase Training Based Adaptation for Statistical Machine Translation
Saab Mansour and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department
RWTH Aachen University, Aachen, Germany
{mansour,ney}@cs.rwth-aachen.de
Abstract
We present a novel approach for translation
model (TM) adaptation using phrase train-
ing. The proposed adaptation procedure is ini-
tialized with a standard general-domain TM,
which is then used to perform phrase training
on a smaller in-domain set. This way, we bias
the probabilities of the general TM towards
the in-domain distribution. Experimental re-
sults on two different lectures translation tasks
show significant improvements of the adapted
systems over the general ones. Additionally,
we compare our results to mixture modeling,
where we report gains when using the sug-
gested phrase training adaptation method.
1 Introduction
The task of domain-adaptation attempts to exploit
data mainly drawn from one domain (e.g. news,
parliamentary discussion) to maximize the perfor-
mance on the test domain (e.g. lectures, web fo-
rums). In this work, we focus on translation model
(TM) adaptation. A prominent approach in recent
work is weighting at different levels of granularity.
Foster and Kuhn (2007) perform weighting at the
corpus level, where different corpora receive differ-
ent weights and are then combined using mixture
modeling. A finer grained weighting is that of Mat-
soukas et al (2009), who weight each sentence in the
bitexts using features of meta-information and opti-
mize a mapping from the feature vectors to weights
using a translation quality measure.
In this work, we propose to perform TM adapta-
tion using phrase training. We start from a general-
domain phrase table and adapt the probabilities by
training on an in-domain data. Thus, we achieve
direct phrase probabilities adaptation as opposed to
weighting. Foster et al (2010) perform weighting
at the phrase level, assigning each phrase pair a
weight according to its relevance to the test domain.
They compare phrase weighting to a ?flat? model,
where the weight directly approximates the phrase
probability. In their experiments, the weighting
method performs better than the flat model, there-
fore, they conclude that retaining the original rela-
tive frequency probabilities of the TM is important
for good performance. The ?flat? model of Foster
et al (2010) is similar to our work. We differ in
the following points: (i) we use the same procedure
to perform the phrase training based adaptation and
the search thus avoiding inconsistencies between the
two; (ii) we do not directly interpolate the original
statistics with the new ones, but use a training pro-
cedure to manipulate the original statistics. We per-
form experiments on the publicly available IWSLT
TED task, on both Arabic-to-English and German-
to-English lectures translation tracks. We compare
our suggested phrase training adaptation method to
a variety of baselines and show its effectiveness. Fi-
nally, we experiment with mixture modeling based
adaptation. We compare mixture modeling to our
adaptation method, and apply our method within a
mixture modeling framework.
In Section 2, we present the phrase training
method and explain how it is utilized for adaptation.
Experimental setup including corpora statistics and
the SMT system are described in Section 3. Sec-
tion 4 summarizes the phrase training adaptation re-
sults ending with a comparison to mixture modeling.
649
2 Phrase Training
The standard phrase extraction procedure in SMT
consists of two phases: (i) word-alignment training
(e.g., IBM alignment models), (ii) heuristic phrase
extraction and relative frequency based phrase trans-
lation probability estimation. In this work, we utilize
phrase training for the task of adaptation. We use
the forced alignment (FA) method (Wuebker et al,
2010) to perform the phrase alignment training and
probability estimation. We perform phrase training
by running a normal SMT decoder on the training
data and constrain the translation to the given target
instance. Using n-best possible phrase segmentation
for each training instance, the phrase probabilities
are re-estimated over the output. Leaving-one-out is
used during the forced alignment procedure phase to
avoid over-fitting (Wuebker et al, 2010).
In the standard phrase training procedure, we
are given a training set y, from which an initial
heuristics-based phrase table p0y is generated. FA
training is then done over the training set y using the
phrases and probabilities in p0y (possibly updated by
the leaving-one-out method). Finally, re-estimation
of the phrase probabilities is done over the decoder
output, generating the FA phrase table p1. We ex-
plain next how to utilize FA training for adaptation.
2.1 Adaptation
In this work, we utilize phrase training for the task
of adaptation. The main idea is to generate the initial
phrase table required for FA using a general-domain
training data y?, thus resulting in p0y? , and perform
the FA training over yIN , the in-domain training
data (instead of y? in the standard procedure). This
way, we bias the probabilities of p0y? towards the in-
domain distribution. We denote this new procedure
by Y?-FA-IN. This differs from the standard IN-FA-
IN by that we have more phrase pairs to use for FA.
Thus, we obtain phrase pairs relevant to IN in ad-
dition to ?general? phrase pairs which were not ex-
tracted from IN, perhaps due to faulty word align-
ments. The probabilities of the general phrase table
will be tailored towards IN. In practice, we usually
have in-domain IN and other-domain OD data. We
denote by ALL the concatenation of IN and OD. To
adapt the ALL phrase table, we perform the FA pro-
cedure ALL-FA-IN. We also utilize leaving-one-out
to avoid over-fitting.
Another procedure we experimented with is
adapting the OD phrase table using FA over IN,
without leaving-one-out. We denote it by OD-FA0-
IN. In this FA scenario, we do not use leaving-one-
out as IN is not contained in OD, therefore, over-
fitting will not occur. By this procedure, we train
phrases from OD that are relevant for both OD and
IN, while the probabilities will be tailored to IN. In
this case, we do not expect improvements over the
IN based phrase table, but, improvements over OD
and reduction in the phrase table size.
We compare our suggested FA based adaptation
to the standard FA procedure.
3 Experimental Setup
3.1 Training Corpora
To evaluate the introduced methods experimentally,
we use the IWSLT 2011 TED Arabic-to-English and
German-to-English translation tasks. The IWSLT
2011 evaluation campaign focuses on the transla-
tion of TED talks, a collection of lectures on a
variety of topics ranging from science to culture.
For Arabic-to-English, the bilingual data consists
of roughly 100K sentences of in-domain TED talks
data and 8M sentences of ?other?-domain United
Nations (UN) data. For the German-to-English task,
the data consists of 130K TED sentences and 2.1M
sentences of ?other?-domain data assembled from
the news-commentary and the europarl corpora. For
language model training purposes, we use an addi-
tional 1.4 billion words (supplied as part of the cam-
paign monolingual training data).
The bilingual training and test data for the Arabic-
to-English and German-to-English tasks are sum-
marized in Table 11. The English data was tok-
enized and lowercased while the Arabic data was
tokenized and segmented using MADA v3.1 (Roth
et al, 2008) with the ATB scheme. The German
source is decompounded (Koehn and Knight, 2003)
and part-of-speech-based long-range verb reorder-
ing rules (Popovic? and Ney, 2006) are applied.
From Table 1, we note that using the general
data considerably reduces the number of out-of-
1For a list of the IWSLT TED 2011 training cor-
pora, see http://www.iwslt2011.org/doku.php?
id=06_evaluation
650
Set Sen Tok OOV/IN OOV/ALL
German-to-English
IN 130K 2.5M
OD 2.1M 55M
dev 883 20K 398 (2.0%) 215 (1.1%)
test 1565 32K 483 (1.5%) 227 (0.7%)
eval 1436 27K 490 (1.8%) 271 (1.0%)
Arabic-to-English
IN 90K 1.6M
OD 7.9M 228M
dev 934 19K 408 (2.2%) 184 (1.0%)
test 1664 31K 495 (1.6%) 228 (0.8%)
eval 1450 27K 513 (1.9%) 163 (0.6%)
Table 1: IWSLT 2011 TED bilingual corpora statistics:
the number of tokens is given for the source side. OOV/X
denotes the number of OOV words in relation to corpus
X (the percentage is given in parentheses). IN is the TED
in-domain data, OD denotes other-domain data, ALL de-
notes the concatenation of IN and OD.
vocabulary (OOV) words. This comes with the price
of increasing the size of the training data by a factor
of more than 20. A simple concatenation of the cor-
pora might mask the phrase probabilities obtained
from the in-domain corpus, causing a deterioration
in performance. One way to avoid this contamina-
tion is by filtering the general corpus, but this dis-
cards phrase translations completely from the phrase
model. A more principled way is by adapting the
phrase probabilities of the full system to the domain
being tackled. We perform this by phrase training
the full phrase table over the in-domain training set.
3.2 Translation System
The baseline system is built using the open-source
SMT toolkit Jane 2.0, which provides a state-of-
the-art phrase-based SMT system (Wuebker et al,
2012a). In addition to the phrase based decoder,
Jane 2.0 implements the forced alignment procedure
used in this work for the purpose of adaptation. We
use the standard set of models with phrase transla-
tion probabilities for source-to-target and target-to-
source directions, smoothing with lexical weights,
a word and phrase penalty, distance-based reorder-
ing and an n-gram target language model. The SMT
systems are tuned on the dev (dev2010) development
set with minimum error rate training (Och, 2003) us-
ing BLEU (Papineni et al, 2002) accuracy measure
as the optimization criterion. We test the perfor-
mance of our system on the test (tst2010) and eval
(tst2011) sets using the BLEU and translation edit
rate (TER) (Snover et al, 2006) measures. We use
TER as an additional measure to verify the consis-
tency of our improvements and avoid over-tuning.
The Arabic-English results are case sensitive while
the German-English results are case insensitive.
4 Results
For TM training, we define three different sets: in-
domain (IN) which is the TED corpus, other-domain
(OD) which consists of the UN corpus for Arabic-
English and a concatenation of news-commentary
and europarl for German-English, and ALL which
consists of the concatenation of IN and OD. We ex-
periment with the following extraction methods:
? Heuristics: standard phrase extraction using
word-alignment training and heuristic phrase
extraction over the word alignment. The ex-
traction is performed for the three different
training data, IN, OD and ALL.
? FA standard: standard FA phrase training
where the same training set is used for initial
phrase table generation as well as the FA pro-
cedure. We perform the training on the three
different training sets and denote the resulting
systems by IN-FA, OD-FA and ALL-FA.
? FA adaptation: FA based adaptation phrase
training, where the initial table is generated
from some general data and the FA training is
performed on the IN data to achieve adapta-
tion. We perform two experiments, OD-FA0-
IN without leaving-one-out and ALL-FA-IN
with leaving-one-out.
The results of the various experiments over both
Arabic-English and German-English tasks are sum-
marized in Table 2. The usefulness of the OD
data differs between the Arabic-to-English and the
German-to-English translation tasks. For Arabic-to-
English, the OD system is 2.5%-4.3% BLEU worse
than the IN system, whereas for the German-to-
English task the differences between IN and OD are
smaller and range from 0.9% to 1.6% BLEU. The
651
Phrase training System Rules dev test eval
method number BLEU TER BLEU TER BLEU TER
Arabic-to-English
Heuristics
IN 1.1M 27.2 54.1 25.3 57.1 24.3 59.9
OD 36.3M 24.7 57.7 21.2 62.6 21.0 64.7
ALL 36.9M 27.1 54.8 24.4 58.6 23.8 61.1
FA standard
IN-FA 1.0M 27.0 54.4 25.0 57.5 23.8 60.3
OD-FA 1.8M 24.5 57.7 21.0 62.4 21.2 64.3
ALL-FA 2.0M 27.2 54.2 24.5 58.1 23.8 60.6
FA adaptation
OD-FA0-IN 0.3M 25.8 55.8 23.6 59.4 22.7 61.7
ALL-FA-IN 0.5M 27.7 53.7 25.3 56.9 24.7 59.3
German-to-English
Heuristics
IN 1.3M 31.0 48.9 29.3 51.0 32.7 46.8
OD 7.3M 29.8 49.2 27.7 51.5 31.8 47.5
ALL 7.8M 31.2 48.3 29.5 50.5 33.6 46.1
FA standard
IN-FA 0.5M 31.6 48.2 29.7 50.5 33.3 46.4
OD-FA 3.0M 29.1 51.0 27.6 53.0 30.7 49.6
ALL-FA 3.2M 31.4 48.3 29.4 50.8 33.6 46.2
FA adaptation
OD-FA0-IN 0.9M 31.2 48.7 29.1 50.9 32.7 46.9
ALL-FA-IN 0.9M 31.8 47.4 29.7 49.7 33.6 45.5
Table 2: TED 2011 translation results. BLEU and TER are given in percentages. IN denotes the TED lectures in-
domain corpus, OD denotes the other-domain corpus, ALL is the concatenation of IN and OD. FA0 denotes forced
alignment training without leaving-one-out (otherwise, leaving-one-out is used).
inferior performance of the OD system can be re-
lated to noisy data or bigger discrepancy between
the OD data domain distribution and the IN distri-
bution. The ALL system performs according to the
usefulness of the OD training set, where for Arabic-
to-English we observe deterioration in performance
for all test sets and up-to -0.9% BLEU on the test
set. On the other hand, for German-to-English, the
ALL system is improving over IN where the biggest
improvement is observed on the eval set with +0.9%
BLEU improvement.
The standard FA procedure achieves mixed re-
sults, where IN-FA deteriorates the results over the
IN counterpart for Arabic-English, while improving
for German-English. ALL-FA performs comparably
to the ALL system on both tasks, while reducing the
phrase table size considerably. The OD-FA system
deteriorates the results in comparison to the OD sys-
tem in most cases, which is expected as training over
the OD set fits the phrase model on the OD domain,
making it perform worse on IN. (Wuebker et al,
2012b) also report mixed results with FA training.
The FA adaptation results are summarized in the
last block of the experiments. The OD-FA0-IN im-
proves over the OD system, which means that the
training procedure was able to modify the OD prob-
abilities to perform well on the IN data. On the
German-to-English task, the OD-FA0-IN performs
comparably to the IN system, whereas for Arabic-
to-English OD-FA0-IN was able to close around half
of the gap between OD and IN.
The FA adapted ALL system (ALL-FA-IN) per-
forms best in our experiments, improving on both
BLEU and TER measures. In comparison to the
best heuristics system (IN for Arabic-English and
ALL for German-English), +0.4% BLEU and -0.6%
TER improvements are observed on the eval set for
Arabic-English. For German-English, the biggest
improvements are observed on TER with -0.8% on
test and -0.5% on eval. The results suggest that ALL-
FA-IN is able to learn more useful phrases than the
IN system and adjust the ALL phrase probabilities
towards the in-domain distribution.
652
System dev test
BLEU TER BLEU TER
Arabic-to-English
Heuristicsbest 27.2 54.1 25.3 57.1
IN,OD 28.2 53.1 25.5 56.8
IN,OD-FA0-IN 28.4 52.9 25.7 56.5
German-to-English
Heuristicsbest 31.2 48.3 29.5 50.5
IN,OD 31.6 48.2 29.9 50.5
IN,OD-FA0-IN 31.8 47.8 30.0 50.0
Table 3: TED 2011 mixture modeling results.
Heuristicsbest is the best heuristics based system, IN for
Arabic-English and ALL for German-English. X,Y de-
notes linear interpolation between X and Y phrase tables.
4.1 Mixture Modeling
In this section, we compare our method to mixture
modeling based adaptation, in addition to applying
mixture modeling on top of our method. We focus
on linear interpolation (Foster and Kuhn, 2007) of
the in-domain (IN) and other-domain phrase tables,
where we vary the latter between the heuristically
extracted phrase table (OD) and the FA adapted one
(OD-FA0-IN). The interpolation weight is uniform
for the interpolated phrase tables (0.5). The results
of mixture modeling are summarized in Table 3. In
this table, we include the best heuristics based sys-
tem (Heuristicsbest) from Table 2 as a reference sys-
tem. The results on the eval set are omitted as they
show similar tendencies to the test set results.
Linear interpolation of IN and OD (IN,OD) is per-
forming well in our experiments, with big improve-
ments over the dev set, +1.0% BLEU for Arabic-to-
English and +0.4% BLEU for German-to-English.
On the test set, we observe smaller improvements.
Interpolating IN with the phrase training adapted
system OD-FA0-IN (IN,OD-FA0-IN) achieves ad-
ditional gains over the IN,OD system, the biggest
are observed on TER for German-to-English, with
-0.4% and -0.5% improvements on the dev and test
sets correspondingly.
Comparing heuristics based interpolation
(IN,OD) to our best phrase training adapted system
(ALL-FA-IN) shows mixed results. For Arabic-to-
English, the systems are comparable, while for the
German-to-English test set, IN,OD is +0.2% BLEU
better and +0.8% TER worse than ALL-FA-IN. We
hypothesize that for Arabic-to-English interpolation
is important due to the larger size of the OD data,
where it could reduce the masking of the IN training
data by the much larger OD data. Nevertheless,
as mentioned previously, using phrase training
adapted phrase table in a mixture setup consistently
improves over using heuristically extracted tables.
5 Conclusions
In this work, we propose a phrase training procedure
for adaptation. The phrase training is implemented
using the FA method. First, we extract a standard
phrase table using the whole available training data.
Using this table, we initialize the FA procedure and
perform training on the in-domain set.
Experiments are done on the Arabic-to-English
and German-to-English TED lectures translation
tasks. We show that the suggested procedure is im-
proving over unadapted baselines. On the Arabic-
to-English task, the FA adapted system is +0.9%
BLEU better than the full unadapted counterpart on
both test sets. Unlike the Arabic-to-English setup,
the German-to-English OD data is helpful and pro-
duces a strong unadapted baseline in concatenation
with IN. In this case, the FA adapted system achieves
BLEU improvements mainly on the development set
with +0.6% BLEU, on the test and eval sets, im-
provements of -0.8% and -0.6% TER are observed
correspondingly. As a side effect of the FA training
process, the size of the adapted phrase table is less
than 10% of the size of the full table.
Finally, we experimented with mixture model-
ing where improvements are observed over the un-
adapted baselines. The results show that using our
phrase training adapted OD table yields better per-
formance than using the heuristically extracted OD
in a mixture framework.
Acknowledgments
This material is based upon work supported by the
DARPA BOLT project under Contract No. HR0011-
12-C-0015. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of DARPA.
653
References
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In Proc. 10th Conf. of the
Europ. Chapter of the Assoc. for Computational Lin-
guistics (EACL), pages 347?354, Budapest, Hungary,
April.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 708?717, Singapore, Au-
gust. Association for Computational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of the
41th Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic morphological tag-
ging, diacritization, and lemmatization using lexeme
models and feature ranking. In Proceedings of ACL-
08: HLT, Short Papers, pages 117?120, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of the
Assoc. for Computational Linguistics, pages 475?484,
Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Man-
sour, and Hermann Ney. 2012a. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, Mumbai, India, Decem-
ber.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012b. Leave-one-out phrase model training for large-
scale deployment. In NAACL 2012 Seventh Work-
shop on Statistical Machine Translation, pages 460?
467, Montreal, Canada, June. Association for Compu-
tational Linguistics.
654
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 475?484,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Training Phrase Translation Models with Leaving-One-Out
Joern Wuebker and Arne Mauser and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
Several attempts have been made to learn
phrase translation probabilities for phrase-
based statistical machine translation that
go beyond pure counting of phrases
in word-aligned training data. Most
approaches report problems with over-
fitting. We describe a novel leaving-
one-out approach to prevent over-fitting
that allows us to train phrase models that
show improved translation performance
on the WMT08 Europarl German-English
task. In contrast to most previous work
where phrase models were trained sepa-
rately from other models used in transla-
tion, we include all components such as
single word lexica and reordering mod-
els in training. Using this consistent
training of phrase models we are able to
achieve improvements of up to 1.4 points
in BLEU. As a side effect, the phrase table
size is reduced by more than 80%.
1 Introduction
A phrase-based SMT system takes a source sen-
tence and produces a translation by segmenting the
sentence into phrases and translating those phrases
separately (Koehn et al, 2003). The phrase trans-
lation table, which contains the bilingual phrase
pairs and the corresponding translation probabil-
ities, is one of the main components of an SMT
system. The most common method for obtain-
ing the phrase table is heuristic extraction from
automatically word-aligned bilingual training data
(Och et al, 1999). In this method, all phrases of
the sentence pair that match constraints given by
the alignment are extracted. This includes over-
lapping phrases. At extraction time it does not
matter, whether the phrases are extracted from a
highly probable phrase alignment or from an un-
likely one.
Phrase model probabilities are typically defined
as relative frequencies of phrases extracted from
word-aligned parallel training data. The joint
counts C(f? , e?) of the source phrase f? and the tar-
get phrase e? in the entire training data are normal-
ized by the marginal counts of source and target
phrase to obtain a conditional probability
pH(f? |e?) =
C(f? , e?)
C(e?)
. (1)
The translation process is implemented as a
weighted log-linear combination of several mod-
els hm(eI1, s
K
1 , f
J
1 ) including the logarithm of the
phrase probability in source-to-target as well as in
target-to-source direction. The phrase model is
combined with a language model, word lexicon
models, word and phrase penalty, and many oth-
ers. (Och and Ney, 2004) The best translation e?I?1
as defined by the models then can be written as
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(2)
In this work, we propose to directly train our
phrase models by applying a forced alignment pro-
cedure where we use the decoder to find a phrase
alignment between source and target sentences of
the training data and then updating phrase transla-
tion probabilities based on this alignment. In con-
trast to heuristic extraction, the proposed method
provides a way of consistently training and using
phrase models in translation. We use a modified
version of a phrase-based decoder to perform the
forced alignment. This way we ensure that all
models used in training are identical to the ones
used at decoding time. An illustration of the basic
475
Figure 1: Illustration of phrase training with
forced alignment.
idea can be seen in Figure 1. In the literature this
method by itself has been shown to be problem-
atic because it suffers from over-fitting (DeNero
et al, 2006), (Liang et al, 2006). Since our ini-
tial phrases are extracted from the same training
data, that we want to align, very long phrases can
be found for segmentation. As these long phrases
tend to occur in only a few training sentences, the
EM algorithm generally overestimates their prob-
ability and neglects shorter phrases, which better
generalize to unseen data and thus are more useful
for translation. In order to counteract these effects,
our training procedure applies leaving-one-out on
the sentence level. Our results show, that this leads
to a better translation quality.
Ideally, we would produce all possible segmen-
tations and alignments during training. However,
this has been shown to be infeasible for real-world
data (DeNero and Klein, 2008). As training uses
a modified version of the translation decoder, it is
straightforward to apply pruning as in regular de-
coding. Additionally, we consider three ways of
approximating the full search space:
1. the single-best Viterbi alignment,
2. the n-best alignments,
3. all alignments remaining in the search space
after pruning.
The performance of the different approaches is
measured and compared on the German-English
Europarl task from the ACL 2008 Workshop on
Statistical Machine Translation (WMT08). Our
results show that the proposed phrase model train-
ing improves translation quality on the test set by
0.9 BLEU points over our baseline. We find that
by interpolation with the heuristically extracted
phrases translation performance can reach up to
1.4 BLEU improvement over the baseline on the
test set.
After reviewing the related work in the fol-
lowing section, we give a detailed description
of phrasal alignment and leaving-one-out in Sec-
tion 3. Section 4 explains the estimation of phrase
models. The empirical evaluation of the different
approaches is done in Section 5.
2 Related Work
It has been pointed out in literature, that training
phrase models poses some difficulties. For a gen-
erative model, (DeNero et al, 2006) gave a de-
tailed analysis of the challenges and arising prob-
lems. They introduce a model similar to the one
we propose in Section 4.2 and train it with the EM
algorithm. Their results show that it can not reach
a performance competitive to extracting a phrase
table from word alignment by heuristics (Och et
al., 1999).
Several reasons are revealed in (DeNero et al,
2006). When given a bilingual sentence pair, we
can usually assume there are a number of equally
correct phrase segmentations and corresponding
alignments. For example, it may be possible to
transform one valid segmentation into another by
splitting some of its phrases into sub-phrases or by
shifting phrase boundaries. This is different from
word-based translation models, where a typical as-
sumption is that each target word corresponds to
only one source word. As a result of this am-
biguity, different segmentations are recruited for
different examples during training. That in turn
leads to over-fitting which shows in overly deter-
minized estimates of the phrase translation prob-
abilities. In addition, (DeNero et al, 2006) found
that the trained phrase table shows a highly peaked
distribution in opposition to the more flat distribu-
tion resulting from heuristic extraction, leaving the
decoder only few translation options at decoding
time.
Our work differs from (DeNero et al, 2006)
in a number of ways, addressing those problems.
476
To limit the effects of over-fitting, we apply the
leaving-one-out and cross-validation methods in
training. In addition, we do not restrict the train-
ing to phrases consistent with the word alignment,
as was done in (DeNero et al, 2006). This allows
us to recover from flawed word alignments.
In (Liang et al, 2006) a discriminative transla-
tion system is described. For training of the pa-
rameters for the discriminative features they pro-
pose a strategy they call bold updating. It is simi-
lar to our forced alignment training procedure de-
scribed in Section 3.
For the hierarchical phrase-based approach,
(Blunsom et al, 2008) present a discriminative
rule model and show the difference between using
only the viterbi alignment in training and using the
full sum over all possible derivations.
Forced alignment can also be utilized to train a
phrase segmentation model, as is shown in (Shen
et al, 2008). They report small but consistent
improvements by incorporating this segmentation
model, which works as an additional prior proba-
bility on the monolingual target phrase.
In (Ferrer and Juan, 2009), phrase models are
trained by a semi-hidden Markov model. They
train a conditional ?inverse? phrase model of the
target phrase given the source phrase. Addition-
ally to the phrases, they model the segmentation
sequence that is used to produce a phrase align-
ment between the source and the target sentence.
They used a phrase length limit of 4 words with
longer phrases not resulting in further improve-
ments. To counteract over-fitting, they interpolate
the phrase model with IBM Model 1 probabilities
that are computed on the phrase level. We also in-
clude these word lexica, as they are standard com-
ponents of the phrase-based system.
It is shown in (Ferrer and Juan, 2009), that
Viterbi training produces almost the same results
as full Baum-Welch training. They report im-
provements over a phrase-based model that uses
an inverse phrase model and a language model.
Experiments are carried out on a custom subset of
the English-Spanish Europarl corpus.
Our approach is similar to the one presented in
(Ferrer and Juan, 2009) in that we compare Viterbi
and a training method based on the Forward-
Backward algorithm. But instead of focusing on
the statistical model and relaxing the translation
task by using monotone translation only, we use a
full and competitive translation system as starting
point with reordering and all models included.
In (Marcu and Wong, 2002), a joint probability
phrase model is presented. The learned phrases
are restricted to the most frequent n-grams up to
length 6 and all unigrams. Monolingual phrases
have to occur at least 5 times to be considered
in training. Smoothing is applied to the learned
models so that probabilities for rare phrases are
non-zero. In training, they use a greedy algorithm
to produce the Viterbi phrase alignment and then
apply a hill-climbing technique that modifies the
Viterbi alignment by merge, move, split, and swap
operations to find an alignment with a better prob-
ability in each iteration. The model shows im-
provements in translation quality over the single-
word-based IBM Model 4 (Brown et al, 1993) on
a subset of the Canadian Hansards corpus.
The joint model by (Marcu and Wong, 2002)
is refined by (Birch et al, 2006) who use
high-confidence word alignments to constrain the
search space in training. They observe that due to
several constraints and pruning steps, the trained
phrase table is much smaller than the heuristically
extracted one, while preserving translation quality.
The work by (DeNero et al, 2008) describes
a method to train the joint model described in
(Marcu and Wong, 2002) with a Gibbs sampler.
They show that by applying a prior distribution
over the phrase translation probabilities they can
prevent over-fitting. The prior is composed of
IBM1 lexical probabilities and a geometric distri-
bution over phrase lengths which penalizes long
phrases. The two approaches differ in that we ap-
ply the leaving-one-out procedure to avoid over-
fitting, as opposed to explicitly defining a prior
distribution.
3 Alignment
The training process is divided into three parts.
First we obtain all models needed for a normal
translations system. We perform minimum error
rate training with the downhill simplex algorithm
(Nelder and Mead, 1965) on the development data
to obtain a set of scaling factors that achieve a
good BLEU score. We then use these models and
scaling factors to do a forced alignment, where
we compute a phrase alignment for the training
data. From this alignment we then estimate new
phrase models, while keeping all other models un-
477
changed. In this section we describe our forced
alignment procedure that is the basic training pro-
cedure for the models proposed here.
3.1 Forced Alignment
The idea of forced alignment is to perform a
phrase segmentation and alignment of each sen-
tence pair of the training data using the full transla-
tion system as in decoding. What we call segmen-
tation and alignment here corresponds to the ?con-
cepts? used by (Marcu and Wong, 2002). We ap-
ply our normal phrase-based decoder on the source
side of the training data and constrain the transla-
tions to the corresponding target sentences from
the training data.
Given a source sentence fJ1 and target sentence
eI1, we search for the best phrase segmentation and
alignment that covers both sentences. A segmen-
tation of a sentence into K phrase is defined by
k ? sk := (ik, bk, jk), for k = 1, . . . ,K
where for each segment ik is last position of kth
target phrase, and (bk, jk) are the start and end
positions of the source phrase aligned to the kth
target phrase. Consequently, we can modify Equa-
tion 2 to define the best segmentation of a sentence
pair as:
s?K?1 = argmax
K,sK1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(3)
The identical models as in search are used: condi-
tional phrase probabilities p(f?k|e?k) and p(e?k|f?k),
within-phrase lexical probabilities, distance-based
reordering model as well as word and phrase
penalty. A language model is not used in this case,
as the system is constrained to the given target sen-
tence and thus the language model score has no
effect on the alignment.
In addition to the phrase matching on the source
sentence, we also discard all phrase translation
candidates, that do not match any sequence in the
given target sentence.
Sentences for which the decoder can not find
an alignment are discarded for the phrase model
training. In our experiments, this is the case for
roughly 5% of the training sentences.
3.2 Leaving-one-out
As was mentioned in Section 2, previous ap-
proaches found over-fitting to be a problem in
phrase model training. In this section, we de-
scribe a leaving-one-out method that can improve
the phrase alignment in situations, where the prob-
ability of rare phrases and alignments might be
overestimated. The training data that consists ofN
parallel sentence pairs fn and en for n = 1, . . . , N
is used for both the initialization of the transla-
tion model p(f? |e?) and the phrase model training.
While this way we can make full use of the avail-
able data and avoid unknown words during train-
ing, it has the drawback that it can lead to over-
fitting. All phrases extracted from a specific sen-
tence pair fn, en can be used for the alignment of
this sentence pair. This includes longer phrases,
which only match in very few sentences in the
data. Therefore those long phrases are trained to
fit only a few sentence pairs, strongly overesti-
mating their translation probabilities and failing to
generalize. In the extreme case, whole sentences
will be learned as phrasal translations. The aver-
age length of the used phrases is an indicator of
this kind of over-fitting, as the number of match-
ing training sentences decreases with increasing
phrase length. We can see an example in Figure
2. Without leaving-one-out the sentence is seg-
mented into a few long phrases, which are unlikely
to occur in data to be translated. Phrase boundaries
seem to be unintuitive and based on some hidden
structures. With leaving-one-out the phrases are
shorter and therefore better suited for generaliza-
tion to unseen data.
Previous attempts have dealt with the over-
fitting problem by limiting the maximum phrase
length (DeNero et al, 2006; Marcu and Wong,
2002) and by smoothing the phrase probabilities
by lexical models on the phrase level (Ferrer and
Juan, 2009). However, (DeNero et al, 2006) expe-
rienced similar over-fitting with short phrases due
to the fact that the same word sequence can be seg-
mented in different ways, leading to specific seg-
mentations being learned for specific training sen-
tence pairs. Our results confirm these findings. To
deal with this problem, instead of simple phrase
length restriction, we propose to apply the leaving-
one-out method, which is also used for language
modeling techniques (Kneser and Ney, 1995).
When using leaving-one-out, we modify the
phrase translation probabilities for each sentence
pair. For a training example fn, en, we have to
remove all phrases Cn(f? , e?) that were extracted
from this sentence pair from the phrase counts that
478
Figure 2: Segmentation example from forced alignment. Top: without leaving-one-out. Bottom: with
leaving-one-out.
we used to construct our phrase translation table.
The same holds for the marginal counts Cn(e?) and
Cn(f?). Starting from Equation 1, the leaving-one-
out phrase probability for training sentence pair n
is
pl1o,n(f? |e?) =
C(f? , e?)? Cn(f? , e?)
C(e?)? Cn(e?)
(4)
To be able to perform the re-computation in an
efficient way, we store the source and target phrase
marginal counts for each phrase in the phrase ta-
ble. A phrase extraction is performed for each
training sentence pair separately using the same
word alignment as for the initialization. It is then
straightforward to compute the phrase counts after
leaving-one-out using the phrase probabilities and
marginal counts stored in the phrase table.
While this works well for more frequent obser-
vations, singleton phrases are assigned a probabil-
ity of zero. We refer to singleton phrases as phrase
pairs that occur only in one sentence. For these
sentences, the decoder needs the singleton phrase
pairs to produce an alignment. Therefore we retain
those phrases by assigning them a positive proba-
bility close to zero. We evaluated with two differ-
ent strategies for this, which we call standard and
length-based leaving-one-out. Standard leaving-
one-out assigns a fixed probability ? to singleton
phrase pairs. This way the decoder will prefer us-
ing more frequent phrases for the alignment, but is
able to resort to singletons if necessary. However,
we found that with this method longer singleton
phrases are preferred over shorter ones, because
fewer of them are needed to produce the target sen-
tence. In order to better generalize to unseen data,
we would like to give the preference to shorter
phrases. This is done by length-based leaving-
one-out, where singleton phrases are assigned the
probability ?(|f? |+|e?|) with the source and target
Table 1: Avg. source phrase lengths in forced
alignment without leaving-one-out and with stan-
dard and length-based leaving-one-out.
avg. phrase length
without l1o 2.5
standard l1o 1.9
length-based l1o 1.6
phrase lengths |f? | and |e?| and fixed ? < 1. In our
experiments we set ? = e?20 and ? = e?5. Ta-
ble 1 shows the decrease in average source phrase
length by application of leaving-one-out.
3.3 Cross-validation
For the first iteration of the phrase training,
leaving-one-out can be implemented efficiently as
described in Section 3.2. For higher iterations,
phrase counts obtained in the previous iterations
would have to be stored on disk separately for each
sentence and accessed during the forced alignment
process. To simplify this procedure, we propose
a cross-validation strategy on larger batches of
data. Instead of recomputing the phrase counts for
each sentence individually, this is done for a whole
batch of sentences at a time. In our experiments,
we set this batch-size to 10000 sentences.
3.4 Parallelization
To cope with the runtime and memory require-
ments of phrase model training that was pointed
out by previous work (Marcu and Wong, 2002;
Birch et al, 2006), we parallelized the forced
alignment by splitting the training corpus into
blocks of 10k sentence pairs. From the initial
phrase table, each of these blocks only loads the
phrases that are required for alignment. The align-
479
ment and the counting of phrases are done sep-
arately for each block and then accumulated to
build the updated phrase model.
4 Phrase Model Training
The produced phrase alignment can be given as a
single best alignment, as the n-best alignments or
as an alignment graph representing all alignments
considered by the decoder. We have developed
two different models for phrase translation proba-
bilities which make use of the force-aligned train-
ing data. Additionally we consider smoothing by
different kinds of interpolation of the generative
model with the state-of-the-art heuristics.
4.1 Viterbi
The simplest of our generative phrase models esti-
mates phrase translation probabilities by their rel-
ative frequencies in the Viterbi alignment of the
data, similar to the heuristic model but with counts
from the phrase-aligned data produced in training
rather than computed on the basis of a word align-
ment. The translation probability of a phrase pair
(f? , e?) is estimated as
pFA(f? |e?) =
CFA(f? , e?)
?
f? ?
CFA(f?
?, e?)
(5)
where CFA(f? , e?) is the count of the phrase pair
(f? , e?) in the phrase-aligned training data. This can
be applied to either the Viterbi phrase alignment
or an n-best list. For the simplest model, each
hypothesis in the n-best list is weighted equally.
We will refer to this model as the count model as
we simply count the number of occurrences of a
phrase pair. We also experimented with weight-
ing the counts with the estimated likelihood of the
corresponding entry in the the n-best list. The sum
of the likelihoods of all entries in an n-best list is
normalized to 1. We will refer to this model as the
weighted count model.
4.2 Forward-backward
Ideally, the training procedure would consider all
possible alignment and segmentation hypotheses.
When alternatives are weighted by their posterior
probability. As discussed earlier, the run-time re-
quirements for computing all possible alignments
is prohibitive for large data tasks. However, we
can approximate the space of all possible hypothe-
ses by the search space that was used for the align-
ment. While this might not cover all phrase trans-
lation probabilities, it allows the search space and
translation times to be feasible and still contains
the most probable alignments. This search space
can be represented as a graph of partial hypothe-
ses (Ueffing et al, 2002) on which we can com-
pute expectations using the Forward-Backward al-
gorithm. We will refer to this alignment as the full
alignment. In contrast to the method described in
Section 4.1, phrases are weighted by their poste-
rior probability in the word graph. As suggested in
work on minimum Bayes-risk decoding for SMT
(Tromble et al, 2008; Ehling et al, 2007), we use
a global factor to scale the posterior probabilities.
4.3 Phrase Table Interpolation
As (DeNero et al, 2006) have reported improve-
ments in translation quality by interpolation of
phrase tables produced by the generative and the
heuristic model, we adopt this method and also re-
port results using log-linear interpolation of the es-
timated model with the original model.
The log-linear interpolations pint(f? |e?) of the
phrase translation probabilities are estimated as
pint(f? |e?) =
(
pH(f? |e?)
)1??
?
(
pgen(f? |e?)
)(?)
(6)
where ? is the interpolation weight, pH the
heuristically estimated phrase model and pgen the
count model. The interpolation weight ? is ad-
justed on the development corpus. When inter-
polating phrase tables containing different sets of
phrase pairs, we retain the intersection of the two.
As a generalization of the fixed interpolation of
the two phrase tables we also experimented with
adding the two trained phrase probabilities as ad-
ditional features to the log-linear framework. This
way we allow different interpolation weights for
the two translation directions and can optimize
them automatically along with the other feature
weights. We will refer to this method as feature-
wise combination. Again, we retain the intersec-
tion of the two phrase tables. With good log-
linear feature weights, feature-wise combination
should perform at least as well as fixed interpo-
lation. However, the results presented in Table 5
480
Table 2: Statistics for the Europarl German-
English data
German English
TRAIN Sentences 1 311 815
Run. Words 34 398 651 36 090 085
Vocabulary 336 347 118 112
Singletons 168 686 47 507
DEV Sentences 2 000
Run. Words 55 118 58 761
Vocabulary 9 211 6 549
OOVs 284 77
TEST Sentences 2 000
Run. Words 56 635 60 188
Vocabulary 9 254 6 497
OOVs 266 89
show a slightly lower performance. This illustrates
that a higher number of features results in a less
reliable optimization of the log-linear parameters.
5 Experimental Evaluation
5.1 Experimental Setup
We conducted our experiments on the German-
English data published for the ACL 2008
Workshop on Statistical Machine Translation
(WMT08). Statistics for the Europarl data are
given in Table 2.
We are given the three data sets TRAIN ,DEV
and TEST . For the heuristic phrase model, we
first use GIZA++ (Och and Ney, 2003) to compute
the word alignment on TRAIN . Next we obtain
a phrase table by extraction of phrases from the
word alignment. The scaling factors of the trans-
lation models have been optimized for BLEU on
the DEV data.
The phrase table obtained by heuristic extraction
is also used to initialize the training. The forced
alignment is run on the training data TRAIN
from which we obtain the phrase alignments.
Those are used to build a phrase table according
to the proposed generative phrase models. After-
ward, the scaling factors are trained on DEV for
the new phrase table. By feeding back the new
phrase table into forced alignment we can reiterate
the training procedure. When training is finished
the resulting phrase model is evaluated on DEV
Table 3: Comparison of different training setups
for the count model on DEV .
leaving-one-out max phr.len. BLEU TER
baseline 6 25.7 61.1
none 2 25.2 61.3
3 25.7 61.3
4 25.5 61.4
5 25.5 61.4
6 25.4 61.7
standard 6 26.4 60.9
length-based 6 26.5 60.6
and TEST . Additionally, we can apply smooth-
ing by interpolation of the new phrase table with
the original one estimated heuristically, retrain the
scaling factors and evaluate afterwards.
The baseline system is a standard phrase-based
SMT system with eight features: phrase transla-
tion and word lexicon probabilities in both transla-
tion directions, phrase penalty, word penalty, lan-
guage model score and a simple distance-based re-
ordering model. The features are combined in a
log-linear way. To investigate the generative mod-
els, we replace the two phrase translation prob-
abilities and keep the other features identical to
the baseline. For the feature-wise combination
the two generative phrase probabilities are added
to the features, resulting in a total of 10 features.
We used a 4-gram language model with modified
Kneser-Ney discounting for all experiments. The
metrics used for evaluation are the case-sensitive
BLEU (Papineni et al, 2002) score and the trans-
lation edit rate (TER) (Snover et al, 2006) with
one reference translation.
5.2 Results
In this section, we investigate the different as-
pects of the models and methods presented be-
fore. We will focus on the proposed leaving-one-
out technique and show that it helps in finding
good phrasal alignments on the training data that
lead to improved translation models. Our final
results show an improvement of 1.4 BLEU over
the heuristically extracted phrase model on the test
data set.
In Section 3.2 we have discussed several meth-
ods which aim to overcome the over-fitting prob-
481
Figure 3: Performance on DEV in BLEU of the
count model plotted against size n of n-best list
on a logarithmic scale.
lems described in (DeNero et al, 2006). Table 3
shows translation scores of the count model on the
development data after the first training iteration
for both leaving-one-out strategies we have in-
troduced and for training without leaving-one-out
with different restrictions on phrase length. We
can see that by restricting the source phrase length
to a maximum of 3 words, the trained model is
close to the performance of the heuristic phrase
model. With the application of leaving-one-out,
the trained model is superior to the baseline, the
length-based strategy performing slightly better
than standard leaving-one-out. For these experi-
ments the count model was estimated with a 100-
best list.
The count model we describe in Section 4.1 esti-
mates phrase translation probabilities using counts
from the n-best phrase alignments. For smaller n
the resulting phrase table contains fewer phrases
and is more deterministic. For higher values of
n more competing alignments are taken into ac-
count, resulting in a bigger phrase table and a
smoother distribution. We can see in Figure 3
that translation performance improves by moving
from the Viterbi alignment to n-best alignments.
The variations in performance with sizes between
n = 10 and n = 10000 are less than 0.2 BLEU.
The maximum is reached for n = 100, which we
used in all subsequent experiments. An additional
benefit of the count model is the smaller phrase
table size compared to the heuristic phrase extrac-
tion. This is consistent with the findings of (Birch
et al, 2006). Table 4 shows the phrase table sizes
for different n. With n = 100 we retain only 17%
of the original phrases. Even for the full model, we
Table 4: Phrase table size of the count model for
different n-best list sizes, the full model and for
heuristic phrase extraction.
N # phrases % of full table
1 4.9M 5.3
10 8.4M 9.1
100 15.9M 17.2
1000 27.1M 29.2
10000 40.1M 43.2
full 59.6M 64.2
heuristic 92.7M 100.0
do not retain all phrase table entries. Due to prun-
ing in the forced alignment step, not all translation
options are considered. As a result experiments
can be done more rapidly and with less resources
than with the heuristically extracted phrase table.
Also, our experiments show that the increased per-
formance of the count model is partly derived from
the smaller phrase table size. In Table 5 we can see
that the performance of the heuristic phrase model
can be increased by 0.6 BLEU on TEST by fil-
tering the phrase table to contain the same phrases
as the count model and reoptimizing the log-linear
model weights. The experiments on the number of
different alignments taken into account were done
with standard leaving-one-out.
The final results are given in Table 5. We can
see that the count model outperforms the base-
line by 0.8 BLEU on DEV and 0.9 BLEU on
TEST after the first training iteration. The perfor-
mance of the filtered baseline phrase table shows
that part of that improvement derives from the
smaller phrase table size. Application of cross-
validation (cv) in the first iteration yields a perfor-
mance close to training with leaving-one-out (l1o),
which indicates that cross-validation can be safely
applied to higher training iterations as an alterna-
tive to leaving-one-out. The weighted count model
clearly under-performs the simpler count model.
A second iteration of the training algorithm shows
nearly no changes in BLEU score, but a small im-
provement in TER. Here, we used the phrase table
trained with leaving-one-out in the first iteration
and applied cross-validation in the second itera-
tion. Log-linear interpolation of the count model
with the heuristic yields a further increase, show-
ing an improvement of 1.3 BLEU onDEV and 1.4
BLEU on TEST over the baseline. The interpo-
482
Table 5: Final results for the heuristic phrase table
filtered to contain the same phrases as the count
model (baseline filt.), the count model trained with
leaving-one-out (l1o) and cross-validation (cv),
the weighted count model and the full model. Fur-
ther, scores for fixed log-linear interpolation of the
count model trained with leaving-one-out with the
heuristic as well as a feature-wise combination are
shown. The results of the second training iteration
are given in the bottom row.
DEV TEST
BLEU TER BLEU TER
baseline 25.7 61.1 26.3 60.9
baseline filt. 26.0 61.6 26.9 61.2
count (l1o) 26.5 60.6 27.2 60.5
count (cv) 26.4 60.7 27.0 60.7
weight. count 25.9 61.4 26.4 61.3
full 26.3 60.0 27.0 60.2
fixed interpol. 27.0 59.4 27.7 59.2
feat. comb. 26.8 60.1 27.6 59.9
count, iter. 2 26.4 60.3 27.2 60.0
lation weight is adjusted on the development set
and was set to ? = 0.6. Integrating both models
into the log-linear framework (feat. comb.) yields
a BLEU score slightly lower than with fixed inter-
polation on both DEV and TEST . This might
be attributed to deficiencies in the tuning proce-
dure. The full model, where we extract all phrases
from the search graph, weighted with their poste-
rior probability, performs comparable to the count
model with a slightly worse BLEU and a slightly
better TER.
6 Conclusion
We have shown that training phrase models can
improve translation performance on a state-of-
the-art phrase-based translation model. This is
achieved by training phrase translation probabil-
ities in a way that they are consistent with their
use in translation. A crucial aspect here is the use
of leaving-one-out to avoid over-fitting. We have
shown that the technique is superior to limiting
phrase lengths and smoothing with lexical prob-
abilities alone.
While models trained from Viterbi alignments
already lead to good results, we have demonstrated
that considering the 100-best alignments allows to
better model the ambiguities in phrase segmenta-
tion.
The proposed techniques are shown to be supe-
rior to previous approaches that only used lexical
probabilities to smooth phrase tables or imposed
limits on the phrase lengths. On the WMT08 Eu-
roparl task we show improvements of 0.9 BLEU
points with the trained phrase table and 1.4 BLEU
points when interpolating the newly trained model
with the original, heuristically extracted phrase ta-
ble. In TER, improvements are 0.4 and 1.7 points.
In addition to the improved performance, the
trained models are smaller leading to faster and
smaller translation systems.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR001-06-C-0023. Any opinions,
ndings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reect the views of the DARPA.
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constraining the
phrase-based, joint probability statistical translation
model. In smt2006, pages 154?157, Jun.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL-08:
HLT, pages 200?208, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?312, June.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 25?28, Morristown, NJ,
USA. Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proceedings of the
483
Workshop on Statistical Machine Translation, pages
31?38, New York City, June.
John DeNero, Alexandre Buchard-Co?te?, and Dan
Klein. 2008. Sampling Alignment Structure under
a Bayesian Translation Model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314?323, Honolulu,
October.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum bayes risk decoding for bleu. In ACL ?07:
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 101?104, Morristown, NJ, USA. Association
for Computational Linguistics.
Jesu?s-Andre?s Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine
translation. In Procedings of European Association
for Machine Translation (EAMT), Barcelona, Spain,
May. European Association for Machine Translation.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modelling. In
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing (ICASSP), pages 181?184, Detroit, MI,
May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, pages 48?54, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Percy Liang, Alexandre Buchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An End-to-End Discriminative
Approach to Machine Translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 761?
768, Sydney, Australia.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), July.
J.A. Nelder and R. Mead. 1965. A Simplex Method
for Function Minimization. The Computer Journal),
7:308?313.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449,
December.
F.J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proc. of the Joint SIGDAT Conf. on Empirical
Methods in Natural Language Processing and Very
Large Corpora (EMNLP99), pages 20?28, Univer-
sity of Maryland, College Park, MD, USA, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Wade Shen, Brian Delaney, Tim Anderson, and Ray
Slyh. 2008. The MIT-LL/AFRL IWSLT-2008 MT
System. In Proceedings of IWSLT 2008, pages 69?
76, Hawaii, U.S.A., October.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA, pages 223?231, Aug.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
620?629, Honolulu, Hawaii, October. Association
for Computational Linguistics.
N. Ueffing, F.J. Och, and H. Ney. 2002. Genera-
tion of word graphs in statistical machine translation.
In Proc. of the Conference on Empirical Methods
for Natural Language Processing, pages 156?163,
Philadelphia, PA, USA, July.
484
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156?164,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Deciphering Foreign Language by Combining Language Models and
Context Vectors
Malte Nuhn and Arne Mauser? and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper we show how to train statis-
tical machine translation systems on real-
life tasks using only non-parallel monolingual
data from two languages. We present a mod-
ification of the method shown in (Ravi and
Knight, 2011) that is scalable to vocabulary
sizes of several thousand words. On the task
shown in (Ravi and Knight, 2011) we obtain
better results with only 5% of the computa-
tional effort when running our method with
an n-gram language model. The efficiency
improvement of our method allows us to run
experiments with vocabulary sizes of around
5,000 words, such as a non-parallel version of
the VERBMOBIL corpus. We also report re-
sults using data from the monolingual French
and English GIGAWORD corpora.
1 Introduction
It has long been a vision of science fiction writers
and scientists to be able to universally communi-
cate in all languages. In these visions, even previ-
ously unknown languages can be learned automati-
cally from analyzing foreign language input.
In this work, we attempt to learn statistical trans-
lation models from only monolingual data in the
source and target language. The reasoning behind
this idea is that the elements of languages share sta-
tistical similarities that can be automatically identi-
fied and matched with other languages.
This work is a big step towards large-scale and
large-vocabulary unsupervised training of statistical
translation models. Previous approaches have faced
constraints in vocabulary or data size. We show how
?Author now at Google Inc., amauser@google.com.
to scale unsupervised training to real-life transla-
tion tasks and how large-scale experiments can be
done. Monolingual data is more readily available,
if not abundant compared to true parallel or even
just translated data. Learning from only monolin-
gual data in real-life translation tasks could improve
especially low resource language pairs where few or
no parallel texts are available.
In addition to that, this approach offers the op-
portunity to decipher new or unknown languages
and derive translations based solely on the available
monolingual data. While we do tackle the full unsu-
pervised learning task for MT, we make some very
basic assumptions about the languages we are deal-
ing with:
1. We have large amounts of data available in
source and target language. This is not a very
strong assumption as books and text on the in-
ternet are readily available for almost all lan-
guages.
2. We can divide the given text in tokens and
sentence-like units. This implies that we know
enough about the language to tokenize and
sentence-split a given text. Again, for the vast
majority of languages, this is not a strong re-
striction.
3. The writing system is one-dimensional left-to-
right. It has been shown (Lin and Knight, 2006)
that the writing direction can be determined
separately and therefore this assumption does
not pose a real restriction.
Previous approaches to unsupervised training for
SMT prove feasible only for vocabulary sizes up to
around 500 words (Ravi and Knight, 2011) and data
156
sets of roughly 15,000 sentences containing only
about 4 tokens per sentence on average. Real data
as it occurs in texts such as web pages or news texts
does not meet any of these characteristics.
In this work, we will develop, describe, and
evaluate methods for large vocabulary unsupervised
learning of machine translation models suitable for
real-world tasks. The remainder of this paper is
structured as follows: In Section 2, we will review
the related work and describe how our approach ex-
tends existing work. Section 3 describes the model
and training criterion used in this work. The im-
plementation and the training of this model is then
described in Section 5 and experimentally evaluated
in Section 6.
2 Related Work
Unsupervised training of statistical translations sys-
tems without parallel data and related problems have
been addressed before. In this section, we will re-
view previous approaches and highlight similarities
and differences to our work. Several steps have been
made in this area, such as (Knight and Yamada,
1999), (Ravi and Knight, 2008), or (Snyder et al,
2010), to name just a few. The main difference of
our work is, that it allows for much larger vocab-
ulary sizes and more data to be used than previous
work while at the same time not being dependent on
seed lexica and/or any other knowledge of the lan-
guages.
Close to the methods described in this work,
Ravi and Knight (2011) treat training and transla-
tion without parallel data as a deciphering prob-
lem. Their best performing approach uses an EM-
Algorithm to train a generative word based trans-
lation model. They perform experiments on a
Spanish/English task with vocabulary sizes of about
500 words and achieve a performance of around
20 BLEU compared to 70 BLEU obtained by a sys-
tem that was trained on parallel data. Our work uses
the same training criterion and is based on the same
generative story. However, we use a new training
procedure whose critical parts have constant time
and memory complexity with respect to the vocab-
ulary size so that our methods can scale to much
larger vocabulary sizes while also being faster.
In a different approach, Koehn and Knight (2002)
induce a bilingual lexicon from only non-parallel
data. To achieve this they use a seed lexicon which
they systematically extend by using orthographic as
well as distributional features such as context, and
frequency. They perform their experiments on non-
parallel German-English news texts, and test their
mappings against a bilingual lexicon. We use a
greedy method similar to (Koehn and Knight, 2002)
for extending a given lexicon, and we implicitly also
use the frequency as a feature. However, we perform
fully unsupervised training and do not start with a
seed lexicon or use linguistic features.
Similarly, Haghighi et al (2008) induce a one-
to-one translation lexicon only from non-parallel
monolingual data. Also starting with a seed lexi-
con, they use a generative model based on canoni-
cal correlation analysis to systematically extend the
lexicon using context as well as spelling features.
They evaluate their method on a variety of tasks,
ranging from inherently parallel data (EUROPARL)
to unrelated corpora (100k sentences of the GIGA-
WORD corpus). They report F-measure scores of the
induced entries between 30 to 70. As mentioned
above, our work neither uses a seed lexicon nor or-
thographic features.
3 Translation Model
In this section, we describe the statistical training
criterion and the translation model that is trained us-
ing monolingual data. In addition to the mathemat-
ical formulation of the model we describe approxi-
mations used.
Throughout this work, we denote the source lan-
guage words as f and target language words as e.
The source vocabulary is Vf and we write the size
of this vocabulary as |Vf |. The same notation holds
for the target vocabulary with Ve and |Ve|.
As training criterion for the translation model?s
parameters ?, Ravi and Knight (2011) suggest
arg max
?
?
?
?
?
f
?
e
P (e) ? p?(f |e)
?
?
?
(1)
We would like to obtain ? from Equation 1 using
the EM Algorithm (Dempster et al, 1977). This
becomes increasingly difficult with more complex
translation models. Therefore, we use a simplified
157
translation model that still contains all basic phe-
nomena of a generic translation process. We formu-
late the translation process with the same generative
story presented in (Ravi and Knight, 2011):
1. Stochastically generate the target sentence ac-
cording to an n-gram language model.
2. Insert NULL tokens between any two adjacent
positions of the target string with uniform prob-
ability.
3. For each target token ei (including NULL)
choose a foreign translation fi (including
NULL) with probability P?(fi|ei).
4. Locally reorder any two adjacent foreign words
fi?1, fi with probability P (SWAP) = 0.1.
5. Remove the remaining NULL tokens.
In practice, however, it is not feasible to deal with
the full parameter table P?(fi|ei) which models the
lexicon. Instead we only allow translation models
where for each source word f the number of words
e? with P (f |e?) 6= 0 is below some fixed value. We
will refer to this value as the maximum number of
candidates of the translation model and denote it
with NC . Note that for a given e this does not nec-
essarily restrict the number of entries P (f ?|e) 6= 0.
Also note that with a fixed value of NC , time and
memory complexity of the EM step isO(1) with re-
spect to |Ve| and |Vf |.
In the following we divide the problem of maxi-
mizing Equation 1 into two parts:
1. Determining a set of active lexicon entries.
2. Choosing the translation probabilities for the
given set of active lexicon entries.
The second task can be achieved by running the
EM algorithm on the restricted translation model.
We deal with the first task in the following section.
4 Monolingual Context Similarity
As described in Section 3 we need some mecha-
nism to iteratively choose an active set of translation
candidates. Based on the assumption that some of
the active candidates and their respective probabili-
ties are already correct, we induce new active candi-
dates. In the context of information retrieval, Salton
et al (1975) introduce a document space where each
document identified by one or more index terms is
represented by a high dimensional vector of term
weights. Given two vectors v1 and v2 of two doc-
uments it is then possible to calculate a similarity
coefficient between those given documents (which
is usually denoted as s(v1, v2)). Similar to this we
represent source and target words in a high dimen-
sional vector space of target word weights which we
call context vectors and use a similarity coefficient
to find possible translation pairs. We first initialize
these context vectors using the following procedure:
1. Using only the monolingual data for the target
language, prepare the context vectors vei with
entries vei,ej :
(a) Initialize all vei,ej = 0
(b) For each target sentence E:
For each word ei in E:
For each word ej 6= ei in E:
vei,ej = vei,ej + 1.
(c) Normalize each vector vei such that
?
ej
(vei,ej )
2 != 1 holds.
Using the notation ei =
(
ej : vei,ej , . . .
)
these
vectors might for example look like
work = (early : 0.2, late : 0.1, . . . )
time = (early : 0.2, late : 0.2, . . . ).
2. Prepare context vectors vfi,ej for the source
language using only the monolingual data for
the source language and the translation model?s
current parameter estimate ?:
(a) Initialize all vfi,ej = 0
(b) Let E??(F ) denote the most probable
translation of the foreign sentence F ob-
tained by using the current estimate ?.
(c) For each source sentence F :
For each word fi in F :
For each word ej 6= E?(fi)1 in
E?(F ):
vfi,ej = vfi,ej + 1
(d) Normalize each vector vfi such that
?
ej
(vfi,ej )
2 != 1 holds.
1denoting that ej is not the translation of fi in E?(F )
158
Adapting the notation described above, these
vectors might for example look like
Arbeit = (early : 0.25, late : 0.05, . . . )
Zeit = (early : 0.15, late : 0.25, . . . )
Once we have set up the context vectors ve and
vf , we can retrieve translation candidates for some
source word f by finding those words e? that maxi-
mize the similarity coefficient s(ve? , vf ), as well as
candidates for a given target word e by finding those
words f ? that maximize s(ve, vf ?). In our implemen-
tation we use the Euclidean distance
d(ve, vf ) = ||ve ? vf ||2. (2)
as distance measure.2 The normalization of context
vectors described above is motivated by the fact that
the context vectors should be invariant with respect
to the absolute number of occurrences of words.3
Instead of just finding the best candidates for a
given word, we are interested in an assignment that
involves all source and target words, minimizing the
sum of distances between the assigned words. In
case of a one-to-one mapping the problem of assign-
ing translation candidates such that the sum of dis-
tances is minimal can be solved optimally in poly-
nomial time using the hungarian algorithm (Kuhn,
1955). In our case we are dealing with a many-
to-many assignment that needs to satisfy the max-
imum number of candidates constraints. For this,
we solve the problem in a greedy fashion by simply
choosing the best pairs (e, f) first. As soon as a tar-
get word e or source word f has reached the limit
of maximum candidates, we skip all further candi-
dates for that word e (or f respectively). This step
involves calculating and sorting all |Ve| ? |Vf | dis-
tances which can be done in time O(V 2 ? log(V )),
with V = max(|Ve|, |Vf |). A simplified example of
this procedure is depicted in Figure 1. The example
already shows that the assignment obtained by this
algorithm is in general not optimal.
2We then obtain pairs (e, f) that minimize d.
3This gives the same similarity ordering as using un-
normalized vectors with the cosine similarity measure
ve?vf
||ve||2?||vf ||2
which can be interpreted as measuring the cosine
of the angle between the vectors, see (Manning et al, 2008).
Still it is noteworthy that this procedure is not equivalent to the
tf-IDF context vectors described in (Salton et al, 1975).
x
y
time (e)
Arbeit (f)
work (e)
Zeit (f)
Figure 1: Hypothetical example for a greedy one-to-one
assignment of translation candidates. The optimal assign-
ment would contain (time,Zeit) and (work,Arbeit).
5 Training Algorithm and Implementation
Given the model presented in Section 3 and the
methods illustrated in Section 4, we now describe
how to train this model.
As described in Section 4, the overall procedure
is divided into two alternating steps: After initializa-
tion we first perform EM training of the translation
model for 20-30 iterations using a 2-gram or 3-gram
language model in the target language. With the ob-
tained best translations we induce new translation
candidates using context similarity. This procedure
is depicted in Figure 2.
5.1 Initialization
Let NC be the maximum number of candidates per
source word we allow, Ve and Vf be the target/source
vocabulary and r(e) and r(f) the frequency rank of
a source/target word. Each word f ? Vf with fre-
quency rank r(f) is assigned to all words e ? Ve
with frequency rank
r(e) ? [ start(f) , end(f) ] (3)
where
start(f) = max(0 , min
(
|Ve| ?Nc ,
?
|Ve|
|Vf |
? r(f)?
Nc
2
?
)
)
(4)
end(f) = min (start(f) +Nc, |Ve|) . (5)
This defines a diagonal beam4 when visualizing
the lexicon entries in a matrix where both source
and target words are sorted by their frequency rank.
However, note that the result of sorting by frequency
4The diagonal has some artifacts for the highest and lowest
frequency ranks. See, for example, left side of Figure 2.
159
In
it
ia
li
za
ti
on
ta
rg
et
w
or
ds
source words
E
M
It
er
at
io
n
s
ta
rg
et
w
or
ds
source words C
on
te
xt
V
ec
to
rs
ta
rg
et
w
or
ds
source words
E
M
It
er
at
io
n
s
. . .
Figure 2: Visualization of the training procedure. The big rectangles represent word lexica in different stages of the
training procedure. The small rectangles represent word pairs (e, f) for which e is a translation candidate of f , while
dots represent word pairs (e, f) for which this is not the case. Source and target words are sorted by frequency so that
the most frequent source words appear on the very left, and the most frequent target words appear at the very bottom.
and thus the frequency ranks are not unique when
there are words with the same frequency. In this
case, we initially obtain some not further specified
frequency ordering, which is then kept throughout
the procedure.
This initialization proves useful as we show by
taking an IBM1 lexicon P (f |e) extracted on the
parallel VERBMOBIL corpus (Wahlster, 2000): For
each word e we calculate the weighted rank differ-
ence
?ravg(e) =
?
f
P (f |e) ? |(r(e)? r(f)| (6)
and count how many of those weighted rank dif-
ferences are smaller than a given value NC2 . Here
we see that for about 1% of the words the weighted
rank difference lies withinNC = 50, and even about
3% for NC = 150 respectively. This shows that the
initialization provides a first solid guess of possible
translations.
5.2 EM Algorithm
The generative story described in Section 3 is im-
plemented as a cascade of a permutation, insertion,
lexicon, deletion and language model finite state
transducers using OpenFST (Allauzen et al, 2007).
Our FST representation of the LM makes use of
failure transitions as described in (Allauzen et al,
2003). We use the forward-backward algorithm on
the composed transducers to efficiently train the lex-
icon model using the EM algorithm.
5.3 Context Vector Step
Given the trained parameters ? from the previous run
of the EM algorithm we set the context vectors ve
and vf up as described in Section 4. We then calcu-
late and sort all |Ve|?|Vf | distances which proves fea-
sible in a few CPU hours even for vocabulary sizes
of more than 50,000 words. This is achieved with
the GNU SORT tool, which uses external sorting for
sorting large amounts of data.
To set up the new lexicon we keep the bNC2 c
best translations for each source word with respect
to P (e|f), which we obtained in the previous EM
run. Experiments showed that it is helpful to also
limit the number of candidates per target words. We
therefore prune the resulting lexicon using P (f |e)
to a maximum of bN
?
C
2 c candidates per target word
afterwards. Then we fill the lexicon with new can-
didates using the previously sorted list of candidate
pairs such that the final lexicon has at most NC
candidates per source word and at most N ?C can-
didates per target word. We set N ?C to some value
N ?C > NC . All experiments in this work were run
with N ?C = 300. Values of N
?
C ? NC seem to pro-
duce poorer results. Not limiting the number of can-
didates per target word at all also typically results in
weaker performance. After the lexicon is filled with
candidates, we initialize the probabilities to be uni-
form. With this new lexicon the process is iterated
starting with the EM training.
6 Experimental Evaluation
We evaluate our method on three different corpora.
At first we apply our method to non-parallel Span-
ish/English data that is based on the OPUS corpus
(Tiedemann, 2009) and that was also used in (Ravi
and Knight, 2011). We show that our method per-
forms better by 1.6 BLEU than the best performing
method described in (Ravi and Knight, 2011) while
160
Name Lang. Sent. Words Voc.
OPUS
Spanish 13,181 39,185 562
English 19,770 61,835 411
VERBMOBIL
German 27,861 282,831 5,964
English 27,862 294,902 3,723
GIGAWORD
French 100,000 1,725,993 68,259
English 100,000 1,788,025 64,621
Table 1: Statistics of the corpora used in this paper.
being approximately 15 to 20 times faster than their
n-gram based approach.
After that we apply our method to a non-parallel
version of the German/English VERBMOBIL corpus,
which has a vocabulary size of 6,000 words on the
German side, and 3,500 words on the target side and
which thereby is approximately one order of magni-
tude larger than the previous OPUS experiment.
We finally run our system on a subset of the non-
parallel French/English GIGAWORD corpus, which
has a vocabulary size of 60,000 words for both
French and English. We show first interesting re-
sults on such a big task.
In case of the OPUS and VERBMOBIL corpus,
we evaluate the results using BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) to reference
translations. We report all scores in percent. For
BLEU higher values are better, for TER lower val-
ues are better. We also compare the results on these
corpora to a system trained on parallel data.
In case of the GIGAWORD corpus we show lexi-
con entries obtained during training.
6.1 OPUS Subtitle Corpus
6.1.1 Experimental Setup
We apply our method to the corpus described in
Table 6. This exact corpus was also used in (Ravi
and Knight, 2011). The best performing methods
in (Ravi and Knight, 2011) use the full 411 ? 579
lexicon model and apply standard EM training. Us-
ing a 2-gram LM they obtain 15.3 BLEU and with
a whole segment LM, they achieve 19.3 BLEU. In
comparison to this baseline we run our algorithm
with NC = 50 candidates per source word for both,
a 2-gram and a 3-gram LM. We use 30 EM iterations
between each context vector step. For both cases we
run 7 EM+Context cycles.
6.1.2 Results
Figure 3 and Figure 4 show the evolution of BLEU
and TER scores for applying our method using a 2-
gram and a 3-gram LM.
In case of the 2-gram LM (Figure 3) the transla-
tion quality increases until it reaches a plateau after
5 EM+Context cycles. In case of the 3-gram LM
(Figure 4) the statement only holds with respect to
TER. It is notable that during the first iterations TER
only improves very little until a large chunk of the
language unravels after the third iteration. This be-
havior may be caused by the fact that the corpus only
provides a relatively small amount of context infor-
mation for each word, since sentence lengths are 3-4
words on average.
0 1 2 3 4 5 6 7 88
10
12
14
16 Full EM best (BLEU)
Iteration
BL
EU
66
68
70
72
74
76
78
80
TE
R
BLEU
TER
Figure 3: Results on the OPUS corpus with a 2-gram LM,
NC = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
2-gram LM in (Ravi and Knight, 2011).
Table 2 summarizes these results and compares
them with (Ravi and Knight, 2011). Our 3-gram
based method performs by 1.6 BLEU better than
their best system which is a statistically significant
improvement at 95% confidence level. Furthermore,
Table 2 compares the CPU time needed for training.
Our 3-gram based method is 15-20 times faster than
running the EM based training procedure presented
in (Ravi and Knight, 2011) with a 3-gram LM5.
5(Ravi and Knight, 2011) only report results using a 2-gram
LM and a whole-segment LM.
161
0 1 2 3 4 5 6 7 88
10
12
14
16
18
20
22
24
Full EM best (BLEU)
Iteration
BL
EU
64
66
68
70
72
TE
R
BLEU
TER
Figure 4: Results on the OPUS corpus with a 3-gram LM,
NC = 50, and 30 EM iterations between each context
vector step. The dashed line shows the best result using a
whole-segment LM in (Ravi and Knight, 2011)
Method CPU BLEU TER
EM, 2-gram LM
411 cand. p. source word
(Ravi and Knight, 2011)
?850h6 15.3 ?
EM, Whole-segment LM
411 cand. p. source word
(Ravi and Knight, 2011)
?7 19.3 ?
EM+Context, 2-gram LM
50 cand. p. source word
(this work)
50h8 15.2 66.6
EM+Context, 3-gram LM
50 cand. p. source word
(this work)
200h8 20.9 64.5
Table 2: Results obtained on the OPUS corpus.
To summarize: Our method is significantly faster
than n-gram LM based approaches and obtains bet-
ter results than any previously published method.
6Estimated by running full EM using the 2-gram LM using
our implementation for 90 Iterations yielding 15.2 BLEU.
7?4,000h when running full EM using a 3-gram LM, using
our implementation. Estimated by running only the first itera-
tion and by assuming that the final result will be obtained after
90 iterations. However, (Ravi and Knight, 2011) report results
using a whole segment LM, assigning P (e) > 0 only to se-
quences seen in training. This seems to work for the given task
but we believe that it can not be a general replacement for higher
order n-gram LMs.
8Estimated by running our method for 5? 30 iterations.
6.2 VERBMOBIL Corpus
6.2.1 Experimental Setup
The VERBMOBIL corpus is a German/English
corpus dealing with short sentences for making ap-
pointments. We prepared a non-parallel subset of
the original VERBMOBIL (Wahlster, 2000) by split-
ting the corpus into two parts and then selecting only
the German side from the first half, and the English
side from the second half such that the target side
is not the translation of the source side. The source
and target vocabularies of the resulting non-parallel
corpus are both more than 9 times bigger compared
to the OPUS vocabularies. Also the total amount of
word tokens is more than 5 times larger compared
to the OPUS corpus. Table 6 shows the statistics of
this corpus. We run our method for 5 EM+Context
cycles (30 EM iterations each) using a 2-gram LM.
After that we run another five EM+Context cycles
using a 3-gram LM.
6.2.2 Results
Our results on the VERBMOBIL corpus are sum-
marized in Table 3. Even on this more complex
task our method achieves encouraging results: The
Method BLEU TER
5? 30 Iterations EM+Context
50 cand. p. source word, 2-gram LM
11.7 67.4
+ 5? 30 Iterations EM+Context
50 cand. p. source word, 3-gram LM
15.5 63.2
Table 3: Results obtained on the VERBMOBIL corpus.
translation quality increases from iteration to itera-
tion until the algorithm finally reaches 11.7 BLEU
using only the 2-gram LM. Running further five
cycles using a 3-gram LM achieves a final perfor-
mance of 15.5 BLEU. Och (2002) reports results of
48.2 BLEU for a single-word based translation sys-
tem and 56.1 BLEU using the alignment template
approach, both trained on parallel data. However, it
should be noted that our experiment only uses 50%
of the original VERBMOBIL training data to simulate
a truly non-parallel setup.
162
Iter. e p(f1|e) f1 p(f2|e) f2 p(f3|e) f3 p(f4|e) f4 p(f5|e) f5
1. the 0.43 la 0.31 l? 0.11 une 0.04 le 0.04 les
2. several 0.57 plusieurs 0.21 les 0.09 des 0.03 nombreuses 0.02 deux
3. where 0.63 ou` 0.17 mais 0.06 indique 0.04 pre?cise 0.02 appelle
4. see 0.49 e?viter 0.09 effet 0.09 voir 0.05 envisager 0.04 dire
5. January 0.25 octobre 0.22 mars 0.09 juillet 0.07 aou?t 0.07 janvier
? Germany 0.24 Italie 0.12 Espagne 0.06 Japon 0.05 retour 0.05 Suisse
Table 4: Lexicon entries obtained by running our method on the non-parallel GIGAWORD corpus. The first column
shows in which iteration the algorithm found the first correct translations f (compared to a parallely trained lexicon)
among the top 5 candidates
6.3 GIGAWORD
6.3.1 Experimental Setup
This setup is based on a subset of the monolingual
GIGAWORD corpus. We selected 100,000 French
sentences from the news agency AFP and 100,000
sentences from the news agency Xinhua. To have a
more reliable set of training instances, we selected
only sentences with more than 7 tokens. Note that
these corpora form true non-parallel data which, be-
sides the length filtering, were not specifically pre-
selected or pre-processed. More details on these
non-parallel corpora are summarized in Table 6. The
vocabularies have a size of approximately 60,000
words which is more than 100 times larger than the
vocabularies of the OPUS corpus. Also it incor-
porates more than 25 times as many tokens as the
OPUS corpus.
After initialization, we run our method with
NC = 150 candidates per source word for 20 EM
iterations using a 2-gram LM. After the first context
vector step with NC = 50 we run another 4 ? 20
iterations with NC = 50 with a 2-gram LM.
6.3.2 Results
Table 4 shows example lexicon entries we ob-
tained. Note that we obtained these results by us-
ing purely non-parallel data, and that we neither
used a seed lexicon, nor orthographic features to as-
sign e.g. numbers or proper names: All results are
obtained using 2-gram statistics and the context of
words only. We find the results encouraging and
think that they show the potential of large-scale un-
supervised techniques for MT in the future.
7 Conclusion
We presented a method for learning statistical ma-
chine translation models from non-parallel data. The
key to our method lies in limiting the translation
model to a limited set of translation candidates and
then using the EM algorithm to learn the probabil-
ities. Based on the translations obtained with this
model we obtain new translation candidates using
a context vector approach. This method increased
the training speed by a factor of 10-20 compared
to methods known in literature and also resulted
in a 1.6 BLEU point increase compared to previ-
ous approaches. Due to this efficiency improvement
we were able to tackle larger tasks, such as a non-
parallel version of the VERBMOBIL corpus having
a nearly 10 times larger vocabulary. We also had a
look at first results of our method on an even larger
Task, incorporating a vocabulary of 60,000 words.
We have shown that, using a limited set of trans-
lation candidates, we can significantly reduce the
computational complexity of the learning task. This
work serves as a big step towards large-scale unsu-
pervised training for statistical machine translation
systems.
Acknowledgements
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation. The authors would like to thank Su-
jith Ravi and Kevin Knight for providing us with the
OPUS subtitle corpus and David Rybach for kindly
sharing his knowledge about the OpenFST library.
163
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 40?47. Association for
Computational Linguistics.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In Jan Holub and Jan Zda?rek, editors, CIAA,
volume 4783 of Lecture Notes in Computer Science,
pages 11?23. Springer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, B, 39.
Aria Haghighi, Percy Liang, T Berg-Kirkpatrick, and
Dan Klein. 2008. Learning Bilingual Lexicons from
Monolingual Corpora. In Proceedings of ACL08 HLT,
pages 771?779. Association for Computational Lin-
guistics.
Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
ACL Workshop on Unsupervised Learning in Natural
Language Processing, number 1, pages 37?44. Cite-
seer.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL02 workshop on Unsupervised lex-
ical acquisition, number July, pages 9?16. Association
for Computational Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistic Quar-
terly, 2:83?97.
Shou-de Lin and Kevin Knight. 2006. Discovering
the linear writing order of a two-dimensional ancient
hieroglyphic script. Artificial Intelligence, 170:409?
421, April.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schuetze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, 1 edition, July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, RWTH Aachen University, Aachen, Ger-
many, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sujith Ravi and Kevin Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram mod-
els. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?08, pages 812?819, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 12?21,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Gerard M. Salton, Andrew K. C. Wong, and Chang S.
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613?620, November.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In 48th Annual Meeting of the Association for
Computational Linguistics, number July, pages 1048?
1057.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia, Borovets, Bul-
garia.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of speech-to-speech translations. Springer-
Verlag, Berlin.
164
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 28?32,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast and Scalable Decoding with Language Model Look-Ahead
for Phrase-based Statistical Machine Translation
Joern Wuebker, Hermann Ney
Human Language Technology
and Pattern Recognition Group
Computer Science Department
RWTH Aachen University, Germany
surname@cs.rwth-aachen.de
Richard Zens*
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
zens@google.com
Abstract
In this work we present two extensions to
the well-known dynamic programming beam
search in phrase-based statistical machine
translation (SMT), aiming at increased effi-
ciency of decoding by minimizing the number
of language model computations and hypothe-
sis expansions. Our results show that language
model based pre-sorting yields a small im-
provement in translation quality and a speedup
by a factor of 2. Two look-ahead methods are
shown to further increase translation speed by
a factor of 2 without changing the search space
and a factor of 4 with the side-effect of some
additional search errors. We compare our ap-
proach with Moses and observe the same per-
formance, but a substantially better trade-off
between translation quality and speed. At a
speed of roughly 70 words per second, Moses
reaches 17.2% BLEU, whereas our approach
yields 20.0% with identical models.
1 Introduction
Research efforts to increase search efficiency for
phrase-based MT (Koehn et al, 2003) have ex-
plored several directions, ranging from generalizing
the stack decoding algorithm (Ortiz et al, 2006) to
additional early pruning techniques (Delaney et al,
2006), (Moore and Quirk, 2007) and more efficient
language model (LM) querying (Heafield, 2011).
This work extends the approach by (Zens and
Ney, 2008) with two techniques to increase trans-
lation speed and scalability. We show that taking
a heuristic LM score estimate for pre-sorting the
phrase translation candidates has a positive effect on
both translation quality and speed. Further, we intro-
duce two novel LM look-ahead methods. The idea
of LM look-ahead is to incorporate the LM proba-
bilities into the pruning process of the beam search
as early as possible. In speech recognition it has
been used for many years (Steinbiss et al, 1994;
Ortmanns et al, 1998). First-word LM look-ahead
exploits the search structure to use the LM costs of
the first word of a new phrase as a lower bound for
the full LM costs of the phrase. Phrase-only LM
look-ahead makes use of a pre-computed estimate
of the full LM costs for each phrase. We detail the
implementation of these methods and analyze their
effect with respect to the number of LM computa-
tions and hypothesis expansions as well as on trans-
lation speed and quality. We also run comparisons
with the Moses decoder (Koehn et al, 2007), which
yields the same performance in BLEU, but is outper-
formed significantly in terms of scalability for faster
translation. Our implementation is available under
a non-commercial open source licence?.
2 Search Algorithm Extensions
We apply the decoding algorithm described in (Zens
and Ney, 2008). Hypotheses are scored by a
weighted log-linear combination of models. A beam
search strategy is used to find the best hypothesis.
During search we perform pruning controlled by the
parameters coverage histogram size? Nc and lexical
?Richard Zens?s contribution was during his time at RWTH.
?www-i6.informatik.rwth-aachen.de/jane
?number of hypothesized coverage vectors per cardinality
28
histogram size? Nl .
2.1 Phrase candidate pre-sorting
In addition to the source sentence f J1 , the beam
search algorithm takes a matrix E(?, ?) as input,
where for each contiguous phrase f? = f j . . . f j?
within the source sentence, E( j, j?) contains a list of
all candidate translations for f? . The candidate lists
are sorted according to their model score, which was
observed to speed up translation by Delaney et al
(2006). In addition to sorting according to the purely
phrase-internal scores, which is common practice,
we compute an estimate qLME(e?) for the LM score
of each target phrase e?. qLME(e?) is the weighted
LM score we receive by assuming e? to be a com-
plete sentence without using sentence start and end
markers. We limit the number of translation options
per source phrase to the No top scoring candidates
(observation histogram pruning).
The pre-sorting during phrase matching has two
effects on the search algorithm. Firstly, it defines
the order in which the hypothesis expansions take
place. As higher scoring phrases are considered first,
it is less likely that already created partial hypothe-
ses will have to be replaced, thus effectively reduc-
ing the expected number of hypothesis expansions.
Secondly, due to the observation pruning the sorting
affects the considered phrase candidates and conse-
quently the search space. A better pre-selection can
be expected to improve translation quality.
2.2 Language Model Look-Ahead
LM score computations are among the most expen-
sive in decoding. Delaney et al (2006) report signif-
icant improvements in runtime by removing unnec-
essary LM lookups via early pruning. Here we de-
scribe an LM look-ahead technique, which is aimed
at further reducing the number of LM computations.
The innermost loop of the search algorithm iter-
ates over all translation options for a single source
phrase to consider them for expanding the current
hypothesis. We introduce an LM look-ahead score
qLMLA(e?|e??), which is computed for each of the
translation options. This score is added to the over-
all hypothesis score, and if the pruning threshold is
?number of lexical hypotheses per coverage vector
exceeded, we discard the expansion without com-
puting the full LM score.
First-word LM look-ahead pruning defines the
LM look-ahead score qLMLA(e?|e??) = qLM(e?1|e??) to
be the LM score of the first word of target phrase e?
given history e??. As qLM(e?1|e??) is an upper bound for
the full LM score, the technique does not introduce
additional seach errors. The score can be reused, if
the LM score of the full phrase e? needs to be com-
puted afterwards.
We can exploit the structure of the search to speed
up the LM lookups for the first word. The LM prob-
abilities are stored in a trie, where each node cor-
responds to a specific LM history. Usually, each
LM lookup consists of first traversing the trie to find
the node corresponding to the current LM history
and then retrieving the probability for the next word.
If the n-gram is not present, we have to repeat this
procedure with the next lower-order history, until a
probability is found. However, the LM history for
the first words of all phrases within the innermost
loop of the search algorithm is identical. Just be-
fore the loop we can therefore traverse the trie once
for the current history and each of its lower order n-
grams and store the pointers to the resulting nodes.
To retrieve the LM look-ahead scores, we can then
directly access the nodes without the need to traverse
the trie again. This implementational detail was con-
firmed to increase translation speed by roughly 20%
in a short experiment.
Phrase-only LM look-ahead pruning defines the
look-ahead score qLMLA(e?|e??) = qLME(e?) to be the
LM score of phrase e?, assuming e? to be the full sen-
tence. It was already used for sorting the phrases,
is therefore pre-computed and does not require ad-
ditional LM lookups. As it is not a lower bound for
the real LM score, this pruning technique can intro-
duce additional search errors. Our results show that
it radically reduces the number of LM lookups.
3 Experimental Evaluation
3.1 Setup
The experiments are carried out on the
German?English task provided for WMT 2011?.
?http://www.statmt.org/wmt11
29
system BLEU[%] #HYP #LM w/s
No = ?
baseline 20.1 3.0K 322K 2.2
+pre-sort 20.1 2.5K 183K 3.6
No = 100
baseline 19.9 2.3K 119K 7.1
+pre-sort 20.1 1.9K 52K 15.8
+first-word 20.1 1.9K 40K 31.4
+phrase-only 19.8 1.6K 6K 69.2
Table 1: Comparison of the number of hypothesis expan-
sions per source word (#HYP) and LM computations per
source word (#LM) with respect to LM pre-sorting, first-
word LM look-ahead and phrase-only LM look-ahead on
newstest2009. Speed is given in words per second.
Results are given with (No = 100) and without (No = ?)
observation pruning.
The English language model is a 4-gram LM
created with the SRILM toolkit (Stolcke, 2002) on
all bilingual and parts of the provided monolingual
data. newstest2008 is used for parameter
optimization, newstest2009 as a blind test
set. To confirm our results, we run the final set of
experiments also on the English?French task of
IWSLT 2011?. We evaluate with BLEU (Papineni et
al., 2002) and TER (Snover et al, 2006).
We use identical phrase tables and scaling fac-
tors for Moses and our decoder. The phrase table
is pruned to a maximum of 400 target candidates per
source phrase before decoding. The phrase table and
LM are loaded into memory before translating and
loading time is eliminated for speed measurements.
3.2 Methodological analysis
To observe the effect of the proposed search al-
gorithm extensions, we ran experiments with fixed
pruning parameters, keeping track of the number of
hypothesis expansions and LM computations. The
LM score pre-sorting affects both the set of phrase
candidates due to observation histogram pruning and
the order in which they are considered. To sepa-
rate these effects, experiments were run both with
histogram pruning (No = 100) and without. From
Table 1 we can see that in terms of efficiency both
cases show similar improvements over the baseline,
?http://iwslt2011.org
 16
 17
 18
 19
 20
 1  4  16  64  256  1024  4096
B
L
E
U
[
%
]
words/sec
Mosesbaseline
+pre-sort
+first-word
+phrase-only
Figure 1: Translation performance in BLEU [%] on the
newstest2009 set vs. speed on a logarithmic scale.
We compare Moses with our approach without LM look-
ahead and LM score pre-sorting (baseline), with added
LM pre-sorting and with either first-word or phrase-only
LM look-ahead on top of +pre-sort. Observation his-
togram size is fixed to No = 100 for both decoders.
which performs pre-sorting with respect to the trans-
lation model scores only. The number of hypothesis
expansions is reduced by ?20% and the number of
LM lookups by ?50%. When observation pruning
is applied, we additionally observe a small increase
by 0.2% in BLEU.
Application of first-word LM look-ahead further
reduces the number of LM lookups by 23%, result-
ing in doubled translation speed, part of which de-
rives from fewer trie node searches. The heuristic
phrase-only LM look-ahead method introduces ad-
ditional search errors, resulting in a BLEU drop by
0.3%, but yields another 85% reduction in LM com-
putations and increases throughput by a factor of 2.2.
3.3 Performance evaluation
In this section we evaluate the proposed extensions
to the original beam search algorithm in terms of
scalability and their usefulness for different appli-
cation constraints. We compare Moses and four dif-
ferent setups of our decoder: LM score pre-sorting
switched on or off without LM look-ahead and both
LM look-ahead methods with LM score pre-sorting.
We translated the test set with the beam sizes set to
Nc = Nl = {1,2,4,8,16,24,32,48,64}. For Moses
we used the beam sizes 2i, i ? {1, . . . ,9}. Transla-
30
setup system WMT 2011 German?English IWSLT 2011 English?French
beam size speed BLEU TER beam size speed BLEU TER
(Nc,Nl) w/s [%] [%] (Nc,Nl) w/s [%] [%]
best Moses 256 0.7 20.2 63.2 16 10 29.5 52.8
this work: first-word (48,48) 1.1 20.2 63.3 (8,8) 23 29.5 52.9
phrase-only (64,64) 1.4 20.1 63.2 (16,16) 18 29.5 52.8
BLEU: Moses 16 12 19.6 63.7 4 40 29.1 53.2
? -1% this work: first-word (4,4) 67 20.0 63.2 (2,2) 165 29.1 53.1
phrase-only (8,8) 69 19.8 63.0 (4,4) 258 29.3 52.9
BLEU: Moses 8 25 19.1 64.2 2 66 28.1 54.3
? -2% this work: first-word (2,2) 233 19.5 63.4 (1,1) 525 28.4 53.9
phrase-only (4,4) 280 19.3 63.0 (2,2) 771 28.5 53.2
fastest Moses 1 126 15.6 68.3 1 116 26.7 55.9
this work: first-word (1,1) 444 18.4 64.6 (1,1) 525 28.4 53.9
phrase-only (1,1) 2.8K 16.8 64.4 (1,1) 2.2K 26.4 54.7
Table 2: Comparison of Moses with this work. Either first-word or phrase-only LM look-ahead is applied. We consider
both the best and the fastest possible translation, as well as the fastest settings resulting in no more than 1% and 2%
BLEU loss on the development set. Results are given on the test set (newstest2009).
tion performance in BLEU is plotted against speed
in Figure 1. Without the proposed extensions, Moses
slightly outperforms our decoder in terms of BLEU.
However, the latter already scales better for higher
speed. With LM score pre-sorting, the best BLEU
value is similar to Moses while further accelerat-
ing translation, yielding identical performance at 16
words/sec as Moses at 1.8 words/sec. Application
of first-word LM look-ahead shifts the graph to the
right, now reaching the same performance at 31
words/sec. At a fixed translation speed of roughly
70 words/sec, our approach yields 20.0% BLEU,
whereas Moses reaches 17.2%. For phrase-only LM
look-ahead the graph is somewhat flatter. It yields
nearly the same top performance with an even better
trade-off between translation quality and speed.
The final set of experiments is performed on both
the WMT and the IWSLT task. We directly com-
pare our decoder with the two LM look-ahead meth-
ods with Moses in four scenarios: the best possi-
ble translation, the fastest possible translation with-
out performance constraint and the fastest possible
translation with no more than 1% and 2% loss in
BLEU on the dev set compared to the best value.
Table 2 shows that on the WMT data, the top per-
formance is similar for both decoders. However, if
we allow for a small degradation in translation per-
formance, our approaches clearly outperform Moses
in terms of translation speed. With phrase-only LM
look-ahead, our decoder is faster by a factor of 6
for no more than 1% BLEU loss, a factor of 11 for
2% BLEU loss and a factor of 22 in the fastest set-
ting. The results on the IWSLT data are very similar.
Here, the speed difference reaches a factor of 19 in
the fastest setting.
4 Conclusions
This work introduces two extensions to the well-
known beam search algorithm for phrase-based ma-
chine translation. Both pre-sorting the phrase trans-
lation candidates with an LM score estimate and LM
look-ahead during search are shown to have a pos-
itive effect on translation speed. We compare our
decoder to Moses, reaching a similar highest BLEU
score, but clearly outperforming it in terms of scal-
ability with respect to the trade-off ratio between
translation quality and speed. In our experiments,
the fastest settings of our decoder and Moses differ
in translation speed by a factor of 22 on the WMT
data and a factor of 19 on the IWSLT data. Our soft-
ware is part of the open source toolkit Jane.
Acknowledgments
This work was partially realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for innovation.
31
References
[Delaney et al2006] Brian Delaney, Wade Shen, and
Timothy Anderson. 2006. An efficient graph search
decoder for phrase-based statistical machine transla-
tion. In International Workshop on Spoken Language
Translation, Kyoto, Japan, November.
[Heafield2011] Kenneth Heafield. 2011. KenLM: Faster
and Smaller Language Model Queries. In Proceedings
of the 6th Workshop on Statistical Machine Transla-
tion, pages 187?197, Edinburgh, Scotland, UK, July.
[Koehn et al2003] P. Koehn, F. J. Och, and D. Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-03), pages 127?133, Edmonton, Alberta.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondr?ej Bo-
jar, Alexandra Constantine, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association for
Computational Linguistics (ACL), demonstration ses-
sion, pages 177?180, Prague, Czech Republic, June.
[Moore and Quirk2007] Robert C. Moore and Chris
Quirk. 2007. Faster beam-search decoding for phrasal
statistical machine translation. In Proceedings of MT
Summit XI.
[Ortiz et al2006] Daniel Ortiz, Ismael Garcia-Varea, and
Francisco Casacuberta. 2006. Generalized stack de-
coding algorithms for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 64?71, New York City, June.
[Ortmanns et al1998] S. Ortmanns, H. Ney, and A. Ei-
den. 1998. Language-model look-ahead for large vo-
cabulary speech recognition. In International Confer-
ence on Spoken Language Processing, pages 2095?
2098, Sydney, Australia, October.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method
for Automatic Evaluation of Machine Translation. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
[Snover et al2006] Matthew Snover, Bonnie Dorr,
Richard Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of the 7th Conference of the Association for Ma-
chine Translation in the Americas, pages 223?231,
Cambridge, Massachusetts, USA, August.
[Steinbiss et al1994] V. Steinbiss, B. Tran, and Hermann
Ney. 1994. Improvements in Beam Search. In Proc.
of the Int. Conf. on Spoken Language Processing (IC-
SLP?94), pages 2143?2146, September.
[Stolcke2002] Andreas Stolcke. 2002. SRILM ? An
Extensible Language Modeling Toolkit. In Proceed-
ings of the Seventh International Conference on Spoken
Language Processing, pages 901?904. ISCA, Septem-
ber.
[Zens and Ney2008] Richard Zens and Hermann Ney.
2008. Improvements in Dynamic Programming Beam
Search for Phrase-based Statistical Machine Transla-
tion. In International Workshop on Spoken Language
Translation, pages 195?205, Honolulu, Hawaii, Octo-
ber.
32
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 322?332,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Advancements in Reordering Models for Statistical Machine Translation
Minwei Feng and Jan-Thorsten Peter and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper, we propose a novel re-
ordering model based on sequence label-
ing techniques. Our model converts the
reordering problem into a sequence label-
ing problem, i.e. a tagging task. Results
on five Chinese-English NIST tasks show
that our model improves the baseline sys-
tem by 1.32 BLEU and 1.53 TER on av-
erage. Results of comparative study with
other seven widely used reordering mod-
els will also be reported.
1 Introduction
The systematic word order difference between two
languages poses a challenge for current statistical
machine translation (SMT) systems. The system
has to decide in which order to translate the given
source words. This problem is known as the re-
ordering problem. As shown in (Knight, 1999), if
arbitrary reordering is allowed, the search problem
is NP-hard.
Many ideas have been proposed to address
the reordering problem. Within the phrase-based
SMT framework there are mainly three stages
where improved reordering could be integrated:
In the preprocessing: the source sentence is re-
ordered by heuristics, so that the word order of
source and target sentences is similar. (Wang et
al., 2007) use manually designed rules to reorder
parse trees of the source sentences. Based on shal-
low syntax, (Zhang et al, 2007) use rules to re-
order the source sentences on the chunk level and
provide a source-reordering lattice instead of a sin-
gle reordered source sentence as input to the SMT
system. Designing rules to reorder the source sen-
tence is conceptually clear and usually easy to im-
plement. In this way, syntax information can be in-
corporated into phrase-based SMT systems. How-
ever, one disadvantage is that the reliability of the
rules is often language pair dependent.
In the decoder: we can add constraints or mod-
els into the decoder to reward good reordering op-
tions or penalize bad ones. For reordering con-
straints, early work includes ITG constraints (Wu,
1995) and IBM constraints (Berger et al, 1996).
(Zens and Ney, 2003) did comparative study over
different reordering constraints. This paper fo-
cuses on reordering models. For reordering mod-
els, we can further roughly divide the existing
methods into three genres:
? The reordering is a classification problem.
The classifier will make decision on next
phrase?s relative position with current phrase.
The classifier can be trained with maximum
likelihood like Moses lexicalized reordering
(Koehn et al, 2007) and hierarchical lexical-
ized reordering model (Galley and Manning,
2008) or be trained under maximum entropy
framework (Zens and Ney, 2006).
? The reordering is a decoding order problem.
(Marin?o et al, 2006) present a translation
model that constitutes a language model of
a sort of bilanguage composed of bilingual
units. From the reordering point of view, the
idea is that the correct reordering is a suit-
able order of translation units. (Feng et al,
2010) present a simpler version of (Marin?o et
al., 2006)?s model which utilize only source
words to model the decoding order.
? The reordering can be solved by outside
heuristics. We can put human knowledge into
the decoder. For example, the simple jump
model using linear distance tells the decoder
that usually the long range reordering should
be avoided. (Cherry, 2008) uses information
from dependency trees to make the decod-
ing process keep syntactic cohesion. (Feng
et al, 2012) present a method that utilizes
predicate-argument structures from semantic
role labeling results as soft constraints.
In the reranking framework: in principle, all
322
the models in previous category can be used in
the reranking framework, because in the rerank-
ing we have all the information (source and tar-
get words/phrases, alignment) about the transla-
tion process. (Och et al, 2004) describe the use of
syntactic features in the rescoring step. However,
they report the syntactic features contribute very
small gains. One disadvantage of carrying out re-
ordering in reranking is the representativeness of
the N-best list is often a question mark.
In this paper, we propose a novel tagging style
reordering model which is under the category
?The reordering is a decoding order problem?.
Our model converts the decoding order problem
into a sequence labeling problem, i.e. a tagging
task. The remainder of this paper is organized
as follows: Section 2 introduces the basement
of this research: the principle of statistical ma-
chine translation. Section 3 describes the proposed
model. Section 4 briefly describes several reorder-
ing models with which we compare our method.
Section 5 provides the experimental configuration
and results. Conclusion will be given in Section 6.
2 Translation System Overview
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ .
The objective is to translate the source into a tar-
get language sentence eI1 = e1 . . . ei . . . eI . The
strategy is to choose the target sentence with the
highest probability among all others:
e?I?i = argmaxI,eI1
{Pr(eI1|fJ1 )} (1)
We model Pr(eI1|fJ1 ) directly using a log-linear
combination of several models (Och and Ney,
2002):
Pr(eI1|fJ1 ) =
exp
( M?
m=1
?mhm(eI1, fJ1 )
)
?
I? ,e? I
?
1
exp
( M?
m=1
?mhm(e?I
?
1 , fJ1 )
)
(2)
The denominator is to make the Pr(eI1|fJ1 ) to be a
probability distribution and it depends only on the
source sentence fJ1 . For search, the decision rule
is simply:
e?I?i = argmaxI,eI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
(3)
The model scaling factors ?M1 are trained with
Minimum Error Rate Training (MERT). In this pa-
per, the phrase-based machine translation system
is utilized (Och et al, 1999; Zens et al, 2002;
Koehn et al, 2003).
3 Tagging-style Reordering Model
In this section, we describe the proposed novel
model. First we will describe the training process.
Then we explain how to use the model in the de-
coder.
3.1 Modeling
Figure 1 shows the modeling steps. The first step
is word alignment training. Figure 1(a) is an ex-
ample after GIZA++ training. If we regard this
alignment as a translation result, i.e. given the
source sentence f71 , the system translates it into
the target sentence e71, then the alignment link set
{a1 = 3, a3 = 2, a4 = 4, a4 = 5, a5 = 7, a6 =
6, a7 = 6} reveals the decoding process, i.e. the
alignment implies the order in which the source
words should be translated, e.g. the first generated
target word e1 has no alignment, we can regard it
as a translation from a NULL source word; then
the second generated target word e2 is translated
from f3. We reorder the source side of the align-
ment to get Figure 1(b). Figure 1(b) implies the
source sentence decoding sequence information,
which is depicted in Figure 1(c). Using this ex-
ample we describe the strategies we used for spe-
cial cases in the transformation from Figure 1(b)
to Figure 1(c):
? ignore the unaligned target word, e.g. e1
? the unaligned source word should follow its
preceding word, the unaligned feature is kept
with a ? symbol, e.g. f?2 is after f1
? when one source word is aligned to multi-
ple target words, only keep the alignment that
links the source word to the first target word,
e.g. f4 is linked to e5 and e6, only f4 ? e5
is kept. In other words, we use this strategy
to guarantee that every source word appears
only once in the source decoding sequence.
? when multiple source words are aligned to
one target word, put together the source
words according to their original relative po-
sitions, e.g. e6 is linked to f6 and f7. So in
the decoding sequence, f6 is before f7.
Now Figure 1(c) shows the original source sen-
tence and its decoding sequence. By using the
strategies above, it is guaranteed that the source
sentence and its decoding sequence have the ex-
323
f1 f2 f3 f4 f5 f6 f7
e1 e2 e3 e4 e5 e6 e7
(a)
f3 f1 f2 f4 f6 f7 f5
e1 e2 e3 e4 e5 e6 e7
(b)
f1 f?2 f3 f4 f5 f6 f7
f3 f1 f2 f4 f6 f7 f5
(c)
f1 f?2 f3 f4 f5 f6 f7
+1 +1 ?2 0 +2 ?1 ?1
(d)
BEGIN-Rmono Unalign Lreorder-Rmono Lmono-Rmono Lmono-Rreorder Lreorder-Rmono END-Lmono
f1 f?2 f3 f4 f5 f6 f7
(e)
Figure 1: modeling process illustration.
actly same length. Hence the relation can be mod-
eled by a function F (f) which assigns a value for
each source word f . Figure 1(d) manifests this
function. The positive function values mean that
compared to the original position in the source
sentence, its position in the decoding sequence
should move rightwards. If the function value is
0, the word?s position in original source sentence
and its decoding sequence is same. For example,
f1 is the first word in the source sentence but it is
the second word in the decoding sequence. So its
function value is +1 (move rightwards one posi-
tion).
Now Figure 1(d) converts the reordering prob-
lem into a sequence labeling or tagging problem.
To make the computational cost to a reasonable
level, we do a final step simplification in Figure
1(e). Suppose the longest sentence length is 100,
then according to Figure 1(d), there are 200 tags
(from -99 to +99 plus the unalign tag). As we will
see later, this number is too large for our task. We
instead design nine tags. For a source word fj in
one source sentence fJ1 , the tag of fj will be one
of the following:
Unalign fj is an unaligned source word
BEGIN-Rmono j = 1 and fj+1 is translated af-
ter fj (Rmono for right monotonic)
BEGIN-Rreorder j = 1 and fj+1 is translated
before fj (Rreorder for right reordered)
END-Lmono j = J and fj?1 translated before
fj (Lmono for left monotonic)
END-Lreorder j = J and fj?1 translated after
fj (Lreorder for left reordered)
Lmono-Rmono 1 < j < J and fj?1 translated
before fj and fj translated before fj+1
Lreorder-Rmono 1 < j < J and fj?1 translated
after fj and fj translated before fj+1
Lmono-Rreorder 1 < j < J and fj?1 translated
before fj and fj translated after fj+1
Lreorder-Rreorder 1 < j < J and fj?1 trans-
lated after fj and fj translated after fj+1
Up to this point, we have converted the reorder-
ing problem into a tagging problem with nine tags.
The transformation in Figure 1 is conducted for
all the sentence pairs in the bilingual training cor-
pus. After that, we have built an ?annotated? cor-
pus for the training. For this supervised learning
task, we choose the approach conditional random
fields (CRFs) (Lafferty et al, 2001; Sutton and
Mccallum, 2006; Lavergne et al, 2010) and recur-
rent neural network (RNN) (Elman, 1990; Jordan,
1990; Lang et al, 1990).
For the first method, we adopt the linear-chain
CRFs. However, even for the simple linear-chain
CRFs, the complexity of learning and inference
grows quadratically with respect to the number of
output labels and the amount of structural features
which are with regard to adjacent pairs of labels.
Hence, to make the computational cost as low as
possible, two measures have been taken. Firstly,
as described above we reduce the number of tags
to nine. Secondly, we add source sentence part-of-
speech (POS) tags to the input. For features with
window size one to three, both source words and
its POS tags are used. For features with window
size four and five, only POS tags are used.
As the second method, we use recurrent neu-
ral network (RNN). RNN is closely related with
Multilayer Perceptrons (MLP) (Rumelhart et al,
1986), but the output of one ore more hidden lay-
ers is reused as additional inputs for the network in
the next time step. This structure allows the RNN
to learn whole sequences without restricting itself
to a fixed input window. A plain RNN has only ac-
cess to the previous events in the input sequence.
Hence we adopt the bidirectional RNN (BRNN)
(Schuster and Paliwal, 1997) which reads the input
sequence from both directions before making the
prediction. The long short-term memory (LSTM)
(Hochreiter and Schmidhuber, 1997) is applied to
324
counter the effects that long distance dependen-
cies are hard to learn with gradient descent. This
is often referred to as vanishing gradient problem
(Bengio et al, 1994).
3.2 Decoding
Once the model training is finished, we make in-
ference on develop and test corpora which means
that we get the labels of the source sentences that
need to be translated. In the decoder, we add
a new model which checks the labeling consis-
tency when scoring an extended state. During
the search, a sentence pair (fJ1 , eI1) will be for-
mally splitted into a segmentation SK1 which con-
sists of K phrase pairs. Each sk = (ik; bk, jk)
is a triple consisting of the last position ik of
the kth target phrase e?k. The start and end po-
sition of the kth source phrase f?k are bk and jk.
Suppose the search state is now extended with a
new phrase pair (f?k, e?k): f?k := fbk . . . fjk and
e?k := eik?1+1 . . . eik . We have access to the
old coverage vector, from which we know if the
new phrase?s left neighboring source word fbk?1
and right neighboring source word fjk+1 have
been translated. We also have the word alignment
within the new phrase pair, which is stored dur-
ing the phrase extraction process. Based on the
old coverage vector and alignment, we can repeat
the transformation in Figure 1 to calculate the la-
bels for the new phrase. The added model will
then check the consistence between the calculated
labels and the labels predicted by the reordering
model. The number of source words that have in-
consistent labels is the penalty and is then added
into the log-linear framework as a new feature.
4 Comparative Study
The second part of this paper is comparative study
on reordering models. Here we briefly describe
those models which will be compared to later.
4.1 Moses lexicalized reordering model
A B
Figure 2: lexicalized reordering model illustration.
Moses (Koehn et al, 2007) contains a word-
based orientation model, which has three types of
reordering: (m) monotone order, (s) switch with
previous phrase and (d) discontinuous. Figure 2
is an example. The definitions of reordering types
are as follows:
monotone for current phrase, if a word alignment
to the bottom left (point A) exists and there is no
word alignment point at the bottom right position
(point B) .
swap for current phrase, if a word alignment to
the bottom right (point B) exists and there is no
word alignment point at the bottom left position
(point A) .
discontinuous all other cases
Our implementation is same with the default
behavior of Moses lexicalized reordering model.
We count how often each extracted phrase pair is
found with each of the three reordering types. The
add-0.5 smoothing is then applied. Finally, the
probability is estimated with maximum likelihood
principle.
4.2 Maximum entropy reordering model
Figure 3 is an illustration of (Zens and Ney, 2006) .
j is the source word position which is aligned to
the last target word of the current phrase. j? is
the last source word position of the current phrase.
j?? is the source word position which is aligned to
the first target word position of the next phrase.
(Zens and Ney, 2006) proposed a maximum en-
tropy classifier to predict the orientation of the
next phrase given the current phrase. The orien-
tation class cj,j? ,j?? is defined as:
cj,j? ,j??=
?
?
?
left, if j??<j
right, if j??>j and j?? ? j?>1
monotone, if j??>j and j?? ? j?=1
(4)
The orientation probability is modeled in a log-
linear framework using a set of N feature func-
tions hn(fJ1 , eI1, i, j, cj,j? ,j?? ), n = 1, . . . , N . Thewhole model is:
p?N1 (cj,j? ,j?? |f
J
1 , eI1, i, j)
=
exp(
N?
n=1
?nhn(fJ1 ,eI1,i,j,cj,j? ,j?? ))
?
c?
exp(
N?
n=1
?nhn(fJ1 ,eI1,i,j,c
? ))
(5)
Different features can be used, we use the source
and target word features to train the model.
325
Figure 3: phrase orientation: left, right and monotone. j is the source word position aligned to the last target word of current
phrase. j? is the last source word position of current phrase. j?? is the source word position aligned to the first target word
position of the next phrase.
f1 f2 f3 f4 f5 f6 f7
e1 e2 e3 e4 e5 e6 e7
Figure 4: bilingual LM illustration. The bilingual sequence
is e1 , e2 f3 , e3 f1 , e4 f4 , e5 f4 , e6 f6 f7 , e7 f5 .
4.3 Bilingual LM
The previous two models belong to ?The reorder-
ing is a classification problem?. Now we turn
to ?The reordering is a decoding order problem?.
(Marin?o et al, 2006) implement a translation
model using n-grams. In this way, the translation
system can take full advantage of the smoothing
and consistency provided by standard back-off n-
gram models. Figure 4 is an example. The in-
terpretation is that given the sentence pair (f71 , e71)
and its alignment, the correct translation order is
e1 , e2 f3 , e3 f1 , e4 f4 , e5 f4 , e6 f6 f7 , e7 f5 .
Notice the bilingual units have been ordered ac-
cording to the target side, as the decoder writes
the translation in a left-to-right way. Using the ex-
ample we describe the strategies used for special
cases:
? keep the unaligned target word, e.g. e1
? remove the unaligned source word, e.g. f2
? when one source word aligned to multiple
target words, duplicate the source word for
each target word, e.g. e4 f4 , e5 f4
? when multiple source words aligned to one
target word, put together the source words for
that target word, e.g. e6 f6 f7
After the operation in Figure 4 was done for
all bilingual sentence pairs, we get a decoding
sequence corpus. We build a 9-gram LM us-
ing SRILM toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing.
The model is added as an additional feature in
Equation (2). To use the bilingual LM, the search
state must be augmented to keep the bilingual unit
decoding sequence. In search, the bilingual LM
is applied similar to the standard target side LM.
The bilingual sequence of phrase pairs will be ex-
tracted using the same strategy in Figure 4 . Sup-
pose the search state is now extended with a new
phrase pair (f? , e?). F? is the bilingual sequence for
the new phrase pair (f? , e?) and F? i is the ith unit
within F? . F? ? is the bilingual sequence history
for current state. We compute the feature score
hbilm(F? , F?
?) of the extended state as follows:
hbilm(F? , F?
?)=? ?
|F? |?
i=1
log p(F? i|F? ? , F? 1, ? ? ? , F? i?1)
(6)
? is the scaling factor for this model. |F? | is the
length of the bilingual decoding sequence.
4.4 Source decoding sequence LM
(Feng et al, 2010) present an simpler version of
the above bilingual LM where they use only the
source side to model the decoding order. The
source word decoding sequence in Figure 4 is then
f3 , f1 , f2 , f4 , f6 , f7 , f5 . We also build a 9-gram
LM based on the source word decoding sequences.
The usage of the model is same as bilingual LM.
4.5 Syntactic cohesion model
The previous two models belong to ?The reorder-
ing is a decoding order problem?. Now we turn to
?The reordering can be solved by outside heuris-
tics?. (Cherry, 2008) proposed a syntactic cohe-
sion model. The core idea is that the syntactic
structure of the source sentence should be pre-
served during translation. This structure is repre-
sented by a source sentence dependency tree. The
algorithm is as follows: given the source sentence
and its dependency tree, during the translation pro-
cess, once a hypothesis is extended, check if the
source dependency tree contains a subtree T such
that:
326
? Its translation is already started (at least one
node is covered)
? It is interrupted by the new added phrase (at
least one word in the new source phrase is not
in T )
? It is not finished (after the new phrase is
added, there is still at least one free node in
T )
If so, we say this hypothesis violates the subtree
T , and the model returns the number of subtrees
that this hypothesis violates.
4.6 Semantic cohesion model
(Feng et al, 2012) propose two structure features
from semantic role labeling (SRL) results. Simi-
lar to the previous model, the SRL information is
used as soft constraints. During decoding process,
the first feature will report how many event layers
that one search state violates and the second fea-
ture will report the amount of semantic roles that
one search state violates. In this paper, the two
features have been used together. So when the se-
mantic cohesion model is used, both features will
be triggered.
4.7 Tree-based jump model
(Wang et al, 2007) present a pre-reordering
method for Chinese-English translation task. In
Section 3.6 of (Zhang, 2013), instead of doing
hard reordering decision, the author uses the rules
as soft constraints in the decoder. In this paper,
we use the similar method as described in (Zhang,
2013). Our strategy is: firstly, we parse the source
sentences to get constituency trees. Then we ma-
nipulate the trees using heuristics described by
(Wang et al, 2007) . The leaf nodes in the revised
tree constitute the reordered source sentence. Fi-
nally, in the log-linear framework (Equation 2) a
new jump model is added which uses the reordered
source sentence to calculate the cost. For example,
the original sentence f1f2f3f4f5 is now converted
by rules into the new sentence f1f5f3f2f4 . For
decoding, we still use the original sentence. Sup-
pose previously translated source phrase is f1 and
the current phrase is f5 . Then the standard jump
model gives cost qDist = 4 and the new tree-based
jump model will return a cost qDist new = 1 .
5 Experiments
In this section, we describe the baseline setup, the
CRFs training results, the RNN training results
and translation experimental results.
5.1 Experimental Setup
Our baseline is a phrase-based decoder, which in-
cludes the following models: an n-gram target-
side language model (LM), a phrase translation
model and a word-based lexicon model. The latter
two models are used for both directions: p(f |e)
and p(e|f). Additionally we use phrase count
features, word and phrase penalty. The reorder-
ing model for the baseline system is the distance-
based jump model which uses linear distance.
This model does not have hard limit. We list the
important information regarding the experimental
setup below. All those conditions have been kept
same in this work.
? lowercased training data from the GALE task
(Table 1, UN corpus not included)
alignment trained with GIZA++
? tuning corpus: NIST06
test corpora: NIST02 03 04 05 and 08
? 5-gram LM (1 694 412 027 running words)
trained by SRILM toolkit (Stolcke, 2002)
with modified Kneser-Ney smoothing
training data: target side of bilingual data.
? BLEU (Papineni et al, 2001) and TER
(Snover et al, 2005) reported
all scores calculated in lowercase way.
? Wapiti toolkit (Lavergne et al, 2010) used for
CRFs; RNN is built by the RNNLIB toolkit.
Chinese English
Sentences 5 384 856
Running Words 115 172 748 129 820 318
Vocabulary 1 125 437 739 251
Table 1: translation model and LM training data statistics
Table 1 contains the data statistics used for
translation model and LM. For the reordering
model, we take two further filtering steps. Firstly,
we delete the sentence pairs if the source sentence
length is one. When the source sentence has only
one word, the translation will be always mono-
tonic and the reordering model does not need to
learn this. Secondly, we delete the sentence pairs if
the source sentence contains more than three con-
tiguous unaligned words. When this happens, the
sentence pair is usually low quality hence not suit-
able for learning. The main purpose of the two
filtering steps is to further lay down the computa-
tional burden. The label distribution is depicted in
Figure 5. We can see that most words are mono-
tonic. We then divide the corpus to three parts:
327
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5?107
BEGIN-RmonoBEGIN-Rreorder
END-LmonoEND-Lreorder
Lmono-RmonoLmono-Rreorder
Lreorder-RmonoLreorder-Rreorder
UNALIGN
Amount of Tags
Figure 5: Tags distribution illustration.
train, validation and test. The source side data
statistics for the reordering model training is given
in Table 2 (target side has only nine labels).
train validation test
Sentences 2 973 519 400 000 400 000
Running Words 62 263 295 8 370 361 8 382 086
Vocabulary 454 951 149686 150 007
Table 2: tagging-style model training data statistics
5.2 CRFs Training Results
The toolkit Wapiti (Lavergne et al, 2010) is used
in this paper. We choose the classical optimization
algorithm limited memory BFGS (L-BFGS) (Liu
and Nocedal, 1989). For regularization, Wapiti
uses both the `1 and `2 penalty terms, yielding the
elastic-net penalty of the form
?1? ? ? ?1 +
?2
2 ? ? ? ?
2
2 (7)
In this work, we use as many features as pos-
sible because `1 penalty ?1 ? ? ?1 is able to
yield sparse parameter vectors, i.e. using a `1
penalty term implicitly performs the feature selec-
tion. The computational costs are given here: on
a cluster with two AMD Opteron(tm) Processor
6176 (total 24 cores), the training time is about 16
hours, peak memory is around 120G. Several ex-
periments have been done to find the suitable hy-
perparameter ?1 and ?2, we choose the model with
lowest error rate on validation corpus for trans-
lation experiments. The error rate of the chosen
model on test corpus (the test corpus in Table 2)
is 25.75% for token error rate and 69.39% for se-
quence error rate. Table 3 is the feature template
we set initially which generates 722 999 637 fea-
tures. Some examples are given in Table 4. After
training 36 902 363 features are kept.
5.3 RNN Training Results
We also applied RNN to the task as an alternative
approach to CRFs. The here used RNN implemen-
tation is RNNLIB which has support for long short
term memory (LSTM) (Graves, 2008). We used
a one of k encoding for the input word and also
for the labels. After testing several configurations
over the validation corpus we used a network with
Feature Templates
1-gram source word features
x[-4,0], x[-3,0], x[-2,0], x[-1,0]
x[0,0], x[1,0], x[2,0], x[3,0], x[4,0]
1-gram source POS features
x[-4,1], x[-3,1], x[-2,1], x[-1,1]
x[0,1], x[1,1], x[2,1], x[3,1], x[4,1]
2-gram source word features
x[-1,0]/x[0,0], x[ 0,0]/x[1,0]
x[-1,1]/x[0,1], x[0,1]/x[1,1]
3-gram source word features
x[-1,0]/x[0,0]/x[1,0]
x[-2,0]/x[-1,0]/x[0,0]
x[0,0]/x[1,0]/x[2,0]
3-gram source POS features
x[0,1]/x[1,1]/x[2,1]
x[-2,1]/x[-1,1]/x[0,1]
x[-1,1]/x[0,1]/x[1,1]
4-gram source POS features
x[0,1]/x[1,1]/x[2,1]/x[3,1]
x[0,1]/x[-1,1]/x[-2,1]/x[-3,1]
x[-1,1]/x[0,1]/x[1,1]/x[2,1]
x[-2,1]/x[-1,1]/x[0,1]/x[1,1]
5-gram source POS features
x[0,1]/x[1,1]/x[2,1]/x[3,1]/x[4,1]
x[-4,1]/x[-3,1]/x[-2,1]/x[-1,1]/x[0,1]
x[-2,1]/x[-1,1]/x[0,1]/x[1,1]/x[2,1]
bigram output label feature
x[-1,2]/x[0,2]
Table 3: feature templates for CRFs training
Words POS Label
?? P BEGIN-Rmono
? DT Lmono-Rmono
? M Lmono-Rmono
?? NN Lmono-Rmono
, PU Lmono-Rmono
?? PN Lmono-Rmono
? VC UNALIGN  Current label
?? VV Lmono-Rmono
??? NN Lmono-Rmono
? DEC UNALIGN
? PU END-Lmono
Table 4: feature examples. x[row,col] specifies a token in the
input data. row specfies the relative position from the cur-
rent label and col specifies the absolute position of the col-
umn. So for the current lable in this table, x[?1, 2]/x[0, 2]
is Lmono-Rmono/UNALIGN and x[?1, 1]/x[0, 1]/x[1, 1] is
PN/VC/VV.
LSTM 200 nodes in the hidden layer. The RNN
has a token error rate of 27.31% and a sentence
error rate of 77.00% over the test corpus in Ta-
ble 2. The RNN is trained on a similar computer
as above. RNNLIB utilizes only one thread. The
training time is about three and a half days and
peak memory consumption is 1G .
5.4 Comparison of CRFs and RNN errors
CRFs performs better than RNN (token error rate
25.75% vs 27.31%). Both error rate values are
much higher than what we usually see in part-of-
speech tagging task. The main reason is that the
?annotated? corpus is converted from word align-
ment which contains lots of error. However, as we
328
hhhhhhhhhhReference
Prediction Unalign BEGIN-Rm BEGIN-Rr END-Lm END-Lr Lm-Rm Lr-Rm Lm-Rr Lr-Rr
Unalign 687724 15084 850 7347 716 493984 107364 43457 9194
BEGIN-Rmono 3537 338315 6209 0 0 0 0 0 0
BEGIN-Rreorder 419 12557 17054 0 0 0 0 0 0
END-Lmono 1799 0 0 365635 3196 0 0 0 0
END-Lreorder 510 0 0 5239 7913 0 0 0 0
Lmomo-Rmono 188627 0 0 0 0 4032738 176682 150952 13114
Lreorder-Rmono 88177 0 0 0 0 369232 433027 27162 15275
Lmomo-Rreorder 32342 0 0 0 0 268570 24558 296033 10645
Lreorder-Rreorder 9865 0 0 0 0 34746 20382 16514 45342
Recall 50.36% 97.20% 56.79% 98.65% 57.92% 88.40% 46.42% 46.83% 35.74%
Precision 67.89% 92.45% 70.73% 96.67% 66.92% 77.56% 56.83% 55.42% 48.46%
Table 5: CRF Confusion Matrix. Abbreviations: Lmono(Lm) Lreorder(Lr) Rmono(Rm) Rreorder(Rr)
hhhhhhhhhhReference
Prediction Unalign BEGIN-Rm BEGIN-Rr END-Lm END-Lr Lm-Rm Lr-Rm Lm-Rr Lr-Rr
Unalign 589100 17299 901 7870 1000 639555 82413 24277 3305
BEGIN-Rmono 1978 339686 6397 0 0 0 0 0 0
BEGIN-Rreorder 186 13812 16032 0 0 0 0 0 0
END-Lmono 2258 0 0 364121 4251 0 0 0 0
END-Lreorde 699 0 0 4693 8269 1 0 0 0
Lmomo-Rmono 142777 1 0 0 0 4232113 105266 78692 3264
Lreorder-Rmono 96278 0 1 0 0 491989 323272 14635 6698
Lmomo-Rreorder 31118 0 0 0 0 380483 18144 198068 4335
Lreorder-Rreorder 12366 0 1 0 0 50121 25196 17008 22157
Recall 43.13% 97.59% 53.39% 98.24% 60.53% 92.77% 34.65% 31.33% 17.47%
Precision 67.19% 91.61% 68.71% 96.66% 61.16% 73.04% 58.32% 59.54% 55.73%
Table 6: RNN Confusion Matrix. Abbreviations: Lmono(Lm) Lreorder(Lr) Rmono(Rm) Rreorder(Rr)
will show later, the model trained with both CRFs
and RNN help to improve the translation quality.
Table 5 and Table 6 demonstrate the confusion
matrix of the CRFs and RNN errors over the test
corpus. The rows represent the correct tag that the
classifier should have predicted and the columns
are the actually predicted tags. E.g. the number
687724 in first row and first column of Table 5
tells that there are 687724 correctly labeled Un-
align tags. The number 15084 in first row and
second column of Table 5 represents that there are
15084 Unalign tags labeled incorrectly to Begin-
Rmono. Therefore, numbers on the diagonal from
the upper left to the lower right corner represent
the amount of correctly classified tags and all other
numbers show the amount of false labels. The
many zeros show that both classifier rarely make
mistake for the label ?BEGIN-?? which only oc-
cur at the beginning of a sentence. The same is
true for the ?END-?? labels.
5.5 Translation Results
Results are summarized in Table 7. Please read
the caption for the meaning of abbreviations. An
Index column is added for score reference conve-
nience (B for BLEU; T for TER). For the proposed
model, significance testing results on both BLEU
and TER are reported (B2 and B3 compared to B1,
T2 and T3 compared to T1). We perform bootstrap
resampling with bounds estimation as described
in (Koehn, 2004). The 95% confidence threshold
(denoted by ? in the table) is used to draw signifi-
cance conclusions. We add a column avg. to show
the average improvements.
From Table 7 we see that the proposed reorder-
ing model using CRFs improves the baseline by
0.98 BLEU and 1.21 TER on average, while the
proposed reordering model using RNN improves
the baseline by 1.32 BLEU and 1.53 TER on av-
erage. For line B2 B3 and T2 T3, most scores
are better than their corresponding baseline values
with more than 95% confidence. The results show
that our proposed idea improves the baseline sys-
tem and RNN trained model performs better than
CRFs trained model, in terms of both automatic
measure and significance test. To investigate why
RNN has lower performance for the tagging task
but achieves better BLEU, we build a 3-gram LM
on the source side of the training corpus in Table
2 and perplexity values are listed in Table 8. The
perplexity of the test corpus for reordering model
comparison is much lower than those NIST cor-
pora for translation experiments. In other words,
there exists mismatch of the data for reordering
model training and actual MT data. This could
explain why CRFs is superior to RNN for labeling
problem while RNN is better for MT tasks.
For the comparative study, the best method is
the tree-based jump model (JUMPTREE). Our
proposed model ranks the second position. The
difference is tiny: on average only 0.08 BLEU (B3
and B10) and 0.15 TER (T3 and T10). Even with
329
Systems NIST02 NIST03 NIST04 NIST05 NIST08 avg. Index
BLEU scores
baseline 33.60 34.29 35.73 32.15 26.34 - B1
baseline+CRFs 34.53 35.19 36.56? 33.30? 27.41? 0.98 B2
baseline+RNN 35.30? 35.34? 37.03? 33.80? 27.23? 1.32 B3
baseline+LRM 34.87 34.90 36.40 33.43 27.45 0.99 B4
baseline+MERO 34.91 34.83 36.29 33.69 26.66 0.85 B5
baseline+BILM 35.21 35.00 36.83 33.64 27.39 1.19 B6
baseline+SRCLM 34.55 34.52 36.18 32.84 27.03 0.50 B7
baseline+SRL 35.05 34.93 36.71 33.22 26.89 0.93 B8
baseline+SC 34.96 34.52 36.37 33.35 26.90 0.79 B9
baseline+JUMPTREE 35.10 35.53 37.12 34.18 27.19 1.40 B10
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE 36.77 36.16 38.10 35.67 28.52 2.62 B11
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE+RNN 36.99 37.00 38.79 35.86 28.99 3.10 B12
TER scores
baseline 61.36 60.48 59.12 60.94 65.17 - T1
baseline+CRFs 60.14? 58.91? 57.91? 59.77? 64.30? 1.21 T2
baseline+RNN 59.38? 58.87? 57.60? 59.56? 63.99? 1.53 T3
baseline+LRM 60.07 59.08 58.42 59.74 64.50 1.05 T4
baseline+MERO 60.19 59.58 58.51 59.49 64.68 0.92 T5
baseline+BILM 60.23 59.93 58.59 60.09 64.72 0.70 T6
baseline+SRCLM 60.27 59.55 58.40 60.16 64.61 0.82 T7
baseline+SRL 60.05 59.55 58.14 59.69 64.74 0.98 T8
baseline+SC 59.90 59.37 58.27 59.69 64.44 1.08 T9
baseline+JUMPTREE 59.53 58.54 57.67 58.90 64.04 1.68 T10
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE 59.16 57.84 56.83 58.03 63.20 2.40 T11
baseline+LRM+MERO+BILM+SRCLM+SRL+SC+JUMPTREE+RNN 58.67 57.67 56.27 58.00 63.09 2.67 T12
Table 7: Experimental results. CRFs and RNN mean the tagging-style model trained with CRFs or RNN; LRM for lexicalized
reordering model (Koehn et al, 2007) ; MERO for maximum entropy reordering model (Zens and Ney, 2006) ; BILM for
bilingual language model (Marin?o et al, 2006) and SRCLM for its simpler version source decoding sequence model (Feng et
al., 2010) ; SC for syntactic cohesion model (Cherry, 2008) ; SRL for semantic cohesion model (Feng et al, 2012); JUMPTREE
for our tree-based jump model based on (Wang et al, 2007).
Running Words OOV Perplexity
Test in Table 2 8 382 086 33854 74.364
NIST02 22 749 195 176.806
NIST03 24 180 290 274.679
NIST04 49 612 320 170.507
NIST05 29 966 228 279.402
NIST08 32 502 511 408.067
Table 8: perplexity
a strong system (B11 and T11), our model is still
able to provide improvements (B12 and T12).
6 Conclusion
In this paper, a novel tagging style reordering
model has been proposed. By our method, the re-
ordering problem is converted into a sequence la-
beling problem so that the whole source sentence
is taken into consideration for reordering decision.
By adding an unaligned word tag, the unaligned
word phenomenon is automatically implanted in
the proposed model. The model is utilized as soft
constraints in the decoder. In practice, we do not
experience decoding memory increase nor speed
slow down.
We choose CRFs and RNN to accomplish the
sequence labeling task. The CRFs achieves lower
error rate on the tagging task but RNN trained
model is better for the translation task. Experi-
mental results show that our model is stable and
improves the baseline system by 0.98 BLEU and
1.21 TER (trained by CRFs) and 1.32 BLEU and
1.53 TER (trained by RNN). Most of the scores
are better than their corresponding baseline values
with more than 95% confidence. We also compare
our method with several other popular reorder-
ing models. Our model ranks the second position
which is slightly worse than the tree-based jump
model. However, the tree-based jump model re-
lies on manually designed reordering rules which
does not exist for many language pairs while our
model can be easily adapted to other translation
tasks. We also show that the proposed model is
able to improve a very strong baseline system.
The main contributions of the paper are: pro-
pose the tagging-style reordering model and im-
prove the translation quality; compare two se-
quence labeling techniques CRFs and RNN; com-
pare our method with seven other reordering mod-
els. To our best knowledge, it is the first time that
the above two comparisons have been reported .
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR0011-12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency (DARPA).
330
References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neu-
ral Networks, 5(2):157?166, March.
Adam Berger, Peter F. Brown, Stephen A. Pietra, Vin-
cent J. Pietra, Andrew S. Kehler, and Robert L.
Mercer. 1996. Language translation apparatus and
method of using Context-Based translation models.
United States Patent, No. 5,510,981.
Colin Cherry. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings
of Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL: HLT), pages 72?80, Columbus, Ohio, USA,
June. Association for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science, 14(2):179?211.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statisti-
cal machine translation. In Proceedings of the As-
sociation for Machine Translation in the Americas
(AMTA), Denver, CO, USA, October.
Minwei Feng, Weiwei Sun, and Hermann Ney. 2012.
Semantic cohesion model for phrase-based SMT.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 867?
878, Mumbai, India, December. The COLING 2012
Organizing Committee.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 848?856, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Alex Graves. 2008. Supervised Sequence Labelling
with Recurrent Neural Networks. Ph.D. thesis,
Technical University of Munich, July.
Sepp Hochreiter and Ju?rgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735?1780, November.
Michael I. Jordan. 1990. Attractor dynamics and
parallelism in a connectionist sequential machine.
IEEE Computer Society Neural Networks Technol-
ogy Series, pages 112?127.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (NAACL) - Volume 1,
pages 48?54, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), demon-
stration session, pages 177?180, Prague, Czech Re-
public, June.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388?395,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML),
pages 282?289, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Kevin J. Lang, Alex H. Waibel, and Geoffrey E. Hin-
ton. 1990. A time-delay neural network architec-
ture for isolated word recognition. Neural networks,
3(1):23?43, January.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Dong C. Liu and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45(3):503?528,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, Jose? A. R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549, December.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statisti-
cal machine translation. In Proceedings of Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 295?302, Philadelphia, Penn-
sylvania, USA, July.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical
machine translation. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora (EMNLP), pages 20?28, University of Mary-
land, College Park, MD, USA, June. Association for
Computational Linguistics.
331
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical machine
translation. In Proceedings of the Conference
on Statistical Machine Translation at the North
American Chapter of the Association for Compu-
tational Linguistics on Human Language Technol-
ogy (NAACL-HLT), pages 161?168, Boston, Mas-
sachusetts, USA, May. Association for Computa-
tional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Re-
port, RC22176 (W0109-022), September.
David. E. Rumelhart, Geoffrey E. Hinton, and
Ronald J. Williams. 1986. Learning internal repre-
sentations by error propagation. In David E. Rumel-
hart and James L. McClelland, editors, Parallel dis-
tributed processing: explorations in the microstruc-
ture of cognition, vol. 1, pages 318?362. MIT Press,
Cambridge, MA, USA.
Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673?2681, Novem-
ber.
Matthew Snover, Bonnie Dorr, Richard Schwartz, John
Makhoul, Linnea Micciulla, and Ralph Weischedel.
2005. A study of translation error rate with targeted
human annotation. Technical Report LAMP-TR-
126, CS-TR-4755, UMIACS-TR-2005-58, Univer-
sity of Maryland, College Park, MD.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing
(ICSLP), pages 901?904, Denver, Colorado, USA,
September. ISCA.
Charles Sutton and Andrew Mccallum, 2006. In-
troduction to Conditional Random Fields for Rela-
tional Learning, pages 93?128. MIT Press.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statis-
tical machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 737?745,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Dekai Wu. 1995. Stochastic inversion transduction
grammars with application to segmentation, brack-
eting, and alignment of parallel corpora. In Pro-
ceedings of the 14th international joint conference
on Artificial intelligence (IJCAI) - Volume 2, pages
1328?1335, San Francisco, CA, USA, August. Mor-
gan Kaufmann Publishers Inc.
Richard Zens and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguis-
tics (ACL) - Volume 1, pages 144?151, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Richard Zens and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings of the Workshop on Statistical
Machine Translation at the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (NAACL-HLT), pages
55?63, New York City, NY, June. Association for
Computational Linguistics.
Richard Zens, Franz J. Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In Ger-
man Conference on Artificial Intelligence, pages 18?
32. Springer Verlag, September.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007.
Chunk-level reordering of source language sen-
tences with automatically learned rules for statistical
machine translation. In Proceedings of the Work-
shop on Syntax and Structure in Statistical Transla-
tion at the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology (NAACL-HLT)/Association for
Machine Translation in the Americas (AMTA), pages
1?8, Morristown, NJ, USA, April. Association for
Computational Linguistics.
Yuqi Zhang. 2013. The Application of Source Lan-
guage Information in Chinese-English Statistical
Machine Translation. Ph.D. thesis, Computer Sci-
ence Department, RWTH Aachen University, May.
332
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 615?621,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Decipherment Complexity in 1:1 Substitution Ciphers
Malte Nuhn and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper we show that even for the
case of 1:1 substitution ciphers?which
encipher plaintext symbols by exchang-
ing them with a unique substitute?finding
the optimal decipherment with respect to a
bigram language model is NP-hard. We
show that in this case the decipherment
problem is equivalent to the quadratic as-
signment problem (QAP). To the best of
our knowledge, this connection between
the QAP and the decipherment problem
has not been known in the literature be-
fore.
1 Introduction
The decipherment approach for MT has recently
gained popularity for training and adapting trans-
lation models using only monolingual data. The
general idea is to find those translation model
parameters that maximize the probability of the
translations of a given source text in a given lan-
guage model of the target language.
In general, the process of translation has a wide
range of phenomena like substitution and reorder-
ing of words and phrases. In this paper we only
study models that substitute tokens?i.e. words
or letters?with a unique substitute. It therefore
serves as a very basic case for decipherment and
machine translation.
Multiple techniques like integer linear program-
ming (ILP), A? search, genetic algorithms, and
Bayesian inference have been used to tackle the
decipherment problem for 1:1 substitution ciphers.
The existence of such a variety of different ap-
proaches for solving the same problem already
shows that there is no obvious way to solve the
problem optimally.
In this paper we show that decipherment of 1:1
substitution ciphers is indeed NP-hard and thus ex-
plain why there is no single best approach to the
problem. The literature on decipherment provides
surprisingly little on the analysis of the complexity
of the decipherment problem. This might be re-
lated to the fact that a statistical formulation of the
decipherment problem has not been analyzed with
respect to n-gram language models: This paper
shows the close relationship of the decipherment
problem to the quadratic assignment problem. To
the best of our knowledge the connection between
the decipherment problem and the quadratic as-
signment problem was not known.
The remainder of this paper is structured as
follows: In Section 2 we review related work.
Section 3 introduces the decipherment problem
and describes the notation and definitions used
throughout this paper. In Section 4 we show that
decipherment using a unigram language model
corresponds to solving a linear sum assignment
problem (LSAP). Section 5 shows the connection
between the quadratic assignment problem and de-
cipherment using a bigram language model. Here
we also give a reduction of the traveling sales-
man problem (TSP) to the decipherment problem
to highlight the additional complexity in the deci-
pherment problem.
2 Related Work
In recent years a large number of publications
on the automatic decipherment of substitution ci-
phers has been published. These publications were
mostly dominated by rather heuristic methods and
did not provide a theoretical analysis of the com-
plexity of the decipherment problem: (Knight and
Yamada, 1999) and (Knight et al, 2006) use the
EM algorithm for various decipherment problems,
like e.g. word substitution ciphers. (Ravi and
Knight, 2008) and (Corlett and Penn, 2010) are
able to obtain optimal (i.e. without search errors)
decipherments of short cryptograms given an n-
615
gram language model. (Ravi and Knight, 2011),
(Nuhn et al, 2012), and (Dou and Knight, 2012)
treat natural language translation as a deciphering
problem including phenomena like reordering, in-
sertion, and deletion and are able to train transla-
tion models using only monolingual data.
In this paper we will show the connection be-
tween the decipherment problem and the linear
sum assignment problem as well as the quadratic
assignment problem: Regarding the linear sum as-
signment problem we will make use of definitions
presented in (Burkard and ela, 1999). Concern-
ing the quadratic assignment problem we will use
basic definitions from (Beckmann and Koopmans,
1957). Further (Burkard et al, 1998) gives a good
overview over the quadratic assignment problem,
including different formulations, solution meth-
ods, and an analysis of computational complexity.
The paper also references a vast amount of fur-
ther literature that might be interesting for future
research.
3 Definitions
In the following we will use the machine trans-
lation notation and denote the ciphertext with
fN1 = f1 . . . fj . . . fN which consists of cipher
tokens fj ? Vf . We denote the plaintext with
eN1 = e1 . . . ei . . . eN (and its vocabulary Ve re-
spectively). We define
e0 = f0 = eN+1 = fN+1 = $ (1)
with ?$? being a special sentence boundary token.
We use the abbreviations V e = Ve ? {$} and V f
respectively.
A general substitution cipher uses a table
s(e|f) which contains for each cipher token f a
probability that the token f is substituted with the
plaintext token e. Such a table for substituting
cipher tokens {A,B,C,D} with plaintext tokens
{a, b, c, d} could for example look like
a b c d
A 0.1 0.2 0.3 0.4
B 0.4 0.2 0.1 0.3
C 0.4 0.1 0.2 0.3
D 0.3 0.4 0.2 0.1
The 1:1 substitution cipher encrypts a given
plaintext into a ciphertext by replacing each plain-
text token with a unique substitute: This means
that the table s(e|f) contains all zeroes, except for
one ?1.0? per f ? Vf and one ?1.0? per e ? Ve.
For example the text
abadcab
would be enciphered to
BCBADBC
when using the substitution
a b c d
A 0 0 0 1
B 1 0 0 0
C 0 1 0 0
D 0 0 1 0
We formalize the 1:1 substitutions with a bijective
function ? : Vf ? Ve. The general decipher-
ment goal is to obtain a mapping ? such that the
probability of the deciphered text is maximal:
?? = argmax
?
p(?(f1)?(f2)?(f3)...?(fN )) (2)
Here p(. . . ) denotes the language model. De-
pending on the structure of the language model
Equation 2 can be further simplified.
Given a ciphertext fN1 , we define the unigram
count Nf of f ? V f as1
Nf =
N+1?
i=0
?(f, fi) (3)
This implies that Nf are integer counts > 0. We
similarly define the bigram count Nff ? of f, f ? ?
V f as
Nff ? =
N+1?
i=1
?(f, fi?1) ? ?(f ?, fi) (4)
This definition implies that
(a) Nff ? are integer counts > 0 of bigrams found
in the ciphertext fN1 .
(b) Given the first and last token of the cipher f1
and fN , the bigram counts involving the sen-
tence boundary token $ need to fulfill
N$f = ?(f, f1) (5)
Nf$ = ?(f, fN ) (6)
(c) For all f ? Vf
?
f ??Vf
Nff ? =
?
f ??Vf
Nf ?f (7)
must hold.
1Here ? denotes the Kronecker delta.
616
Similarly, we define language model matrices S
for the unigram and the bigram case. The uni-
gram language model Sf is defined as
Sf = log p(f) (8)
which implies that
(a) Sf are real numbers with
Sf ? [??, 0] (9)
(b) The following normalization constraint holds:
?
f?Vf
exp(Sf ) = 1 (10)
Similarly for the bigram language model matrix
Sff ? , we define
Sff ? = log p(f ?|f) (11)
This definition implies that
(a) Sff ? are real numbers with
Sff ? ? [??, 0] (12)
(b) For the sentence boundary symbol, it holds
that
S$$ = ?? (13)
(c) For all f ? Vf the following normalization
constraint holds:
?
f ??Vf
exp(Sff ?) = 1 (14)
4 Decipherment Using Unigram LMs
4.1 Problem Definition
When using a unigram language model, Equa-
tion 2 simplifies to finding
?? = argmax
?
N?
i=1
p(?(fi)) (15)
which can be rewritten as
?? = argmax
?
?
f?Vf
NfS?(f) (16)
When defining cff ? = Nf log p(f ?), for f, f ? ?
Vf , Equation 16 can be brought into the form of
?? = argmax
?
?
f?Vf
cf?(f) (17)
Figure 1 shows an illustration of this problem.
A
B
C
a
b
c
Ve Vf
cij A B Ca NA log p(a) NB log p(a) NC log p(a)b NA log p(b) NB log p(b) NC log p(b)c NA log p(c) NB log p(c) NC log p(c)
Figure 1: Linear sum assignment problem for a
cipher with Ve = {a, b, c}, Vf = {A,B,C}, uni-
gram counts Nf , and unigram probabilities p(e).
4.2 The Linear Sum Assignment Problem
The practical problem behind the linear sum
assignment problem can be described as fol-
lows: Given jobs {j1, . . . , jn} and workers
{w1, . . . , wn}, the task is to assign each job ji to a
worker wj . Each assignment incurs a cost cij and
the total cost for assigning all jobs and workers is
to be minimized.
This can be formalized as finding the assign-
ment
?? = argmin
?
n?
i=1
ci?(i) (18)
The general LSAP can be solved in polynomial
time using the Hungarian algorithm (Kuhn, 1955).
However, since the matrix cij occurring for the de-
cipherment using a unigram language model can
be represented as the product cij = ai ? bj the
decipherment problem can be solved more easily:
In the Section ?Optimal Matching?, (Bauer, 2010)
shows that in this case the optimal assignment is
found by sorting the jobs ji by ai and workers wj
by bj and then assigning the jobs ji to workers wj
that have the same rank in the respective sorted
lists. Sorting and then assigning the elements can
be done in O(n log n).
5 Decipherment Using Bigram LMs
5.1 Problem Definition
When using a 2-gram language model, Equation 2
simplifies to
?? = argmax
?
?
?
?
N+1?
j=1
p(?(fj)|?(fj?1))
?
?
? (19)
617
xy
l1l2
l3 l4
Assignments
l1 l2 l3 l4(a) f1 f2 f3 f4(b) f1 f4 f3 f2
Flows
f1 f2 f3 f4
f1 1
f2 1
f3 1
f4 1
Figure 2: Hypothetical quadratic assignment prob-
lem with locations l1 . . . l4 and facilities f1 . . . f4
with all flows being zero except f1 ? f2 and
f3 ? f4. The distance between locations l1 . . . l4
is implicitly given by the locations in the plane,
implying a euclidean metric. Two example assign-
ments (a) and (b) are shown, with (b) having the
lower overall costs.
Using the definitions from Section 3, Equation 19
can be rewritten as
?? = argmax
?
?
?
?
?
f?Vf
?
f ??Vf
Nff ?S?(f)?(f ?)
?
?
? (20)
(Bauer, 2010) arrives at a similar optimization
problem for the ?combined method of frequency
matching? using bigrams and mentions that it can
be seen as a combinatorial problem for which an
efficient way of solving is not known. However,
he does not mention the close connection to the
quadratic assignment problem.
5.2 The Quadratic Assignment Problem
The quadratic assignment problem was introduced
by (Beckmann and Koopmans, 1957) for the fol-
lowing real-life problem:
Given a set of facilities {f1, . . . , fn} and a set
of locations {l1, . . . , ln} with distances for each
pair of locations, and flows for each pair of facili-
ties (e.g. the amount of supplies to be transported
between a pair of facilities) the problem is to as-
sign the facilities to locations such that the sum
of the distances multiplied by the corresponding
flows (which can be interpreted as total transporta-
tion costs) is minimized. This is visualized in Fig-
ure 2.
Following (Beckmann and Koopmans, 1957)
we can express the quadratic assignment problem
as finding
?? = argmin
?
?
?
?
n?
i=1
n?
j=1
aijb?(i)?(j) +
n?
i=1
ci?(i)
?
?
?
(21)
where A = (aij), B = (bij), C = (cij) ? Nn?n
and ? a permutation
? : {1, . . . , n} ? {1, . . . , n}. (22)
This formulation is often referred to as Koopman-
Beckman QAP and often abbreviated as
QAP (A,B,C). The so-called pure or ho-
mogeneous QAP
?? = argmin
?
?
?
?
n?
i=1
n?
j=1
aijb?(i)?(j)
?
?
? (23)
is obtained by setting cij = 0, and is often denoted
as QAP (A,B).
In terms of the real-life problem presented in
(Beckmann and Koopmans, 1957) the matrix A
can be interpreted as distance matrix for loca-
tions {l1 . . . ln} and B as flow matrix for facilities
{f1 . . . fn}.
(Sahni and Gonzalez, 1976) show that the
quadratic assignment problem is strongly NP-
hard.
We will now show the relation between the
quadratic assignment problem and the decipher-
ment problem.
5.3 Decipherment Problem  Quadratic
Assignment Problem
Every decipherment problem is directly a
quadratic assignment problem, since the ma-
trices Nff ? and Sff ? are just special cases of
the general matrices A and B required for the
quadratic assignment problem. Thus a reduction
from the decipherment problem to the quadratic
assignment problem is trivial. This means that all
algorithms capable of solving QAPs can directly
be used to solve the decipherment problem.
5.4 Quadratic Assignment Problem 
Decipherment Problem
Given QAP (A,B) with integer matrices A =
(aij), B = (bij) i, j ? {1, . . . , n} we construct
the count matrix Nff ? and language model ma-
trix Sff ? in such a way that the solution for the
decipherment problem implies the solution to the
618
quadratic assignment problem, and vice versa. We
will use the vocabularies V e = V f = {1, . . . , n+
3}, with n + 3 being the special sentence bound-
ary token ?$?. The construction of Nff ? and Sff ?
is shown in Figure 3.
To show the validity of our construction, we will
1. Show that Nff ? is a valid count matrix.
2. Show that Sff ? is a valid bigram language
model matrix.
3. Show that the decipherment problem and
the newly constructed quadratic assignment
problem are equivalent.
We start by showing that Nff ? is a valid count
matrix:
(a) By construction, Nff ? has integer counts that
are greater or equal to 0.
(b) By construction, Nff ? at boundaries is:
? N$f = ?(f, 1)
? Nf$ = ?(f, n+ 2)
(c) Regarding the properties ?
f ?
Nff ? =
?
f ?
Nf ?f :
? For all f ? {1, . . . , n} the count proper-
ties are equivalent to
a?f? +
?
f ?
a?ff ? = a??f +
?
f ?
a?f ?f + ?(f, 1)
(24)
which holds by construction of a??f and
a?f?.
? For f = n+1 the count property is equiv-
alent to
1 +
?
f ?
a?f ?? = 2 +
?
f ?
a??f ? (25)
which follows from Equation (24) by
summing over all f ? {1, . . . , n}.
? For f = n+2 and f = n+3, the condi-
tion is fulfilled by construction.
We now show that Sff ? is a valid bigram lan-
guage model matrix:
(a) By construction, Sff ? ? [??, 0] holds.
(b) By construction, S$$ = ?? holds.
(c) By the construction of b?f?, the values Sff ? ful-
fill ?f ? exp(Sff ?) = 1 for all f . This works
since all entries b?ff ? are chosen to be smaller
than ?log(n+ 2).
We now show the equivalence of the quadratic
assignment problem and the newly constructed de-
cipherment problem. For this we will use the defi-
nitions
A? = {1, . . . , n} (26)
B? = {n+ 1, n+ 2, n+ 3} (27)
We first show that solutions of the constructed
decipherment problem with score > ?? fulfill
?(f) = f for f ? B?.
All mappings ?, with ?(f) = f ? for any f ?
A? and f ? ? B? will induce a score of ?? since
for f ? A? all Nff > 0 and Sf ?f ? = ?? for
f ? ? B?. Thus any ? with score > ?? will fulfill
?(f) ? B? for f ? B?. Further, by enumerating all
six possible permutations, it can be seen that only
the ? with ?(f) = f for f ? B? induces a score of
> ??. Thus we can rewrite
n+3?
f=1
n+3?
f ?=1
Nff ?S?(f)?(f ?) (28)
to
?
f?A?
?
f?A?
Nff ?S?(f)?(f ?)
? ?? ?
(AA)
+
?
f?A?
?
f ??B?
Nff ?S?(f)f ?
? ?? ?
(AB)
+
?
f?B?
?
f ??A?
Nff ?Sf?(f ?)
? ?? ?
(BA)
+
?
f?B?
?
f ??B?
Nff ?Sff ?
? ?? ?
(BB)
Here
? (AB) is independent of ? since
?f ? A?, f ? ? {n+ 1, n+ 3} : Sff ? = S1f ?
(29)
and
?f ? A? : Nf,n+2 = 0 (30)
? (BA) is independent of ? since
?f ? ? A?, f ? B? : Sff ? = Sf1 (31)
? (BB) is independent of ?.
619
Nff ? =
?
??????????
a?11 a?12 ? ? ? a?1n a?1? 0 0
a?21 a?22 ? ? ? a?2n a?2? 0 0... ... . . . ... ... ... ...
a?n1 a?n2 ? ? ? a?nn a?n? 0 0
a??1 a??2 ? ? ? a??n 0 2 0
0 0 ? ? ? 0 1 0 1
1 0 ? ? ? 0 0 0 0
?
??????????
Sff ? =
?
???????????
b?11 b?12 ? ? ? b?1n ?2 b?1? ?2
b?21 b?22 ? ? ? b?2n ?2 b?2? ?2... ... . . . ... ... ... ...
b?n1 b?n2 ? ? ? b?nn ?2 b?n? ?2
?1 ?1 ? ? ? ?1 ?? ?1 ??
?2 ?2 ? ? ? ?2 ?2 ?? ?2
?0 ?0 ? ? ? ?0 ?? ?? ??
?
???????????
a?ff ? = aff ? ?min
f? f? ?
{af? f? ?}+ 1 b?ff ? = bff ? ?maxf? f? ?
{bf? f? ?} ? log(n+ 2)
a?f? = max
?
?
?
n?
f ?=1
af ?f ? aff ? , 0
?
?
?+ ?(f, 1) b?f? = log
?
?1?
n?
f ?=1
exp(b?ff ?)?
2
n+ 2
?
?
a??f ? = max
?
?
?
n?
f=1
aff ? ? af ?f , 0
?
?
? ?i = ? log(n+ i)
Figure 3: Construction of matrices Nff ? and Sff ? of the decipherment problem from matrices A = (aij)
and B = (bij) of the quadratic assignment problem QAP (A,B).
Thus, with some constant c, we can finally rewrite
Equation 28 as
c+
n?
f=1
n?
f ?=1
Nff ?S?(f)?(f ?) (32)
Inserting the definition of Nff ? and Sff ? (simpli-
fied using constants c?, and c??) we obtain
c+
n?
f=1
n?
f ?=1
(aff ? + c?)(b?(f)?(f ?) + c??) (33)
which is equivalent to the original quadratic as-
signment problem
argmax
?
?
?
n?
f=1
n?
f ?=1
aff ?b?(f)?(f ?)
?
?
? (34)
Thus we have shown that a solution to the
quadratic assignment problem in Equation 34 is
a solution to the decipherment problem in Equa-
tion 20 and vice versa. Assuming that calculat-
ing elementary functions can be done inO(1), set-
ting up Nff ? and Sff ? can be done in polynomial
time.2 Thus we have given a polynomial time re-
duction from the quadratic assignment problem to
2This is the case if we only require a fixed number of dig-
its precision for the log and exp operations.
the decipherment problem: Since the quadratic as-
signment problem is NP-hard, it follows that the
decipherment problem is NP-hard, too.
5.5 Traveling Salesman Problem 
Decipherment Problem
Using the above construction we can immediately
construct a decipherment problem that is equiva-
lent to the traveling salesman problem by using
the quadratic assignment problem formulation of
the traveling salesman problem.
Without loss of generality3 we assume that the
TSP?s distance matrix fulfills the constraints of a
bigram language model matrix Sff ? . Then the
count matrix Nff ? needs to be chosen as
Nff ? =
?
??????????
0 1 0 ? ? ? 0 0 0
0 0 1 ? ? ? 0 0 0
0 0 0 ? ? ? 0 0 0
... ... ... . . . ... ... ...
0 0 0 ? ? ? 0 1 0
0 0 0 ? ? ? 0 0 1
1 0 0 ? ? ? 0 0 0
?
??????????
(35)
which fulfills the constraints of a bigram count
matrix.
3The general case can be covered using the reduction
shown in Section 5.
620
This matrix corresponds to a ciphertext of the
form
$abcd$ (36)
and represents the tour of the traveling salesman in
an intuitive way. The mapping ? then only decides
in which order the cities are visited, and only costs
between two successive cities are counted.
This shows that the TSP is only a special case
of the decipherment problem.
6 Conclusion
We have shown the correspondence between solv-
ing 1:1 substitution ciphers and the linear sum as-
signment problem and the quadratic assignment
problem: When using unigram language models,
the decipherment problem is equivalent to the lin-
ear sum assignment problem and solvable in poly-
nomial time. For a bigram language model, the de-
cipherment problem is equivalent to the quadratic
assignment problem and is NP-hard.
We also pointed out that all available algorithms
for the quadratic assignment problem can be di-
rectly used to solve the decipherment problem.
To the best of our knowledge, this correspon-
dence between the decipherment problem and the
quadratic assignment problem has not been known
previous to our work.
Acknowledgements
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation.
References
Friedrich L. Bauer. 2010. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer, 4th edition.
Martin J. Beckmann and Tjalling C. Koopmans. 1957.
Assignment problems and the location of economic
activities. Econometrica, 25(4):53?76.
Rainer E. Burkard and Eranda ela. 1999. Linear as-
signment problems and extensions. In Handbook
of Combinatorial Optimization - Supplement Volume
A, pages 75?149. Kluwer Academic Publishers.
Rainer E. Burkard, Eranda ela, Panos M. Pardalos, and
Leonidas S. Pitsoulis. 1998. The quadratic assign-
ment problem. In Handbook of Combinatorial Op-
timization, pages 241?338. Kluwer Academic Pub-
lishers.
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 266?275,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
Proceedings of the ACL Workshop on Unsupervised
Learning in Natural Language Processing, num-
ber 1, pages 37?44. Association for Computational
Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for deci-
pherment problems. In Proceedings of the Confer-
ence on Computational Linguistics and Association
of Computation Linguistics (COLING/ACL) Main
Conference Poster Sessions, pages 499?506, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistic
Quarterly, 2(1-2):83?97.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12?21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sartaj Sahni and Teofilo Gonzalez. 1976. P-complete
approximation problems. Journal of the Association
for Computing Machinery (JACM), 23(3):555?565,
July.
621
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1568?1576,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Beam Search for Solving Substitution Ciphers
Malte Nuhn and Julian Schamper and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
In this paper we address the problem of
solving substitution ciphers using a beam
search approach. We present a concep-
tually consistent and easy to implement
method that improves the current state of
the art for decipherment of substitution ci-
phers and is able to use high order n-gram
language models. We show experiments
with 1:1 substitution ciphers in which the
guaranteed optimal solution for 3-gram
language models has 38.6% decipherment
error, while our approach achieves 4.13%
decipherment error in a fraction of time
by using a 6-gram language model. We
also apply our approach to the famous
Zodiac-408 cipher and obtain slightly bet-
ter (and near to optimal) results than pre-
viously published. Unlike the previous
state-of-the-art approach that uses addi-
tional word lists to evaluate possible deci-
pherments, our approach only uses a letter-
based 6-gram language model. Further-
more we use our algorithm to solve large
vocabulary substitution ciphers and im-
prove the best published decipherment er-
ror rate based on the Gigaword corpus of
7.8% to 6.0% error rate.
1 Introduction
State-of-the-art statistical machine translation
(SMT) systems use large amounts of parallel data
to estimate translation models. However, parallel
corpora are expensive and not available for every
domain.
Recently different works have been published
that train translation models using only non-
parallel data. Although first practical applications
of these approaches have been shown, the overall
decipherment accuracy of the proposed algorithms
is still low. Improving the core decipherment algo-
rithms is an important step for making decipher-
ment techniques useful for practical applications.
In this paper we present an effective beam
search algorithm which provides high decipher-
ment accuracies while having low computational
requirements. The proposed approach allows us-
ing high order n-gram language models, is scal-
able to large vocabulary sizes and can be adjusted
to account for a given amount of computational
resources. We show significant improvements in
decipherment accuracy in a variety of experiments
while being computationally more effective than
previous published works.
2 Related Work
The experiments proposed in this paper touch
many of previously published works in the deci-
pherment field.
Regarding the decipherment of 1:1 substitution
ciphers various works have been published: Most
older papers do not use a statistical approach and
instead define some heuristic measures for scoring
candidate decipherments. Approaches like (Hart,
1994) and (Olson, 2007) use a dictionary to check
if a decipherment is useful. (Clark, 1998) defines
other suitability measures based on n-gram counts
and presents a variety of optimization techniques
like simulated annealing, genetic algorithms and
tabu search.
On the other hand, statistical approaches for
1:1 substitution ciphers were published in the nat-
ural language processing community: (Ravi and
Knight, 2008) solve 1:1 substitution ciphers opti-
mally by formulating the decipherment problem as
an integer linear program (ILP) while (Corlett and
Penn, 2010) solve the problem using A? search.
We use our own implementation of these methods
to report optimal solutions to 1:1 substitution ci-
1568
phers for language model orders n = 2 and n = 3.
(Ravi and Knight, 2011a) report the first au-
tomatic decipherment of the Zodiac-408 cipher.
They use a combination of a 3-gram language
model and a word dictionary. We run our beam
search approach on the same cipher and report
better results without using an additional word
dictionary?just by using a high order n-gram lan-
guage model.
(Ravi and Knight, 2011b) report experiments on
large vocabulary substitution ciphers based on the
Transtac corpus. (Dou and Knight, 2012) improve
upon these results and provide state-of-the-art re-
sults on a large vocabulary word substitution ci-
pher based on the Gigaword corpus. We run our
method on the same corpus and report improve-
ments over the state of the art.
(Ravi and Knight, 2011b) and (Nuhn et al,
2012) have shown that?even for larger vocabu-
lary sizes?it is possible to learn a full translation
model from non-parallel data. Even though this
work is currently only able to deal with substi-
tution ciphers, phenomena like reordering, inser-
tions and deletions can in principle be included in
our approach.
3 Definitions
In the following we will use the machine trans-
lation notation and denote the ciphertext with
fN1 = f1 . . . fj . . . fN which consists of cipher
tokens fj ? Vf . We denote the plaintext with
eN1 = e1 . . . ei . . . eN (and its vocabulary Ve re-
spectively). We define
e0 = f0 = eN+1 = fN+1 = $ (1)
with ?$? being a special sentence boundary token.
We use the abbreviations V e = Ve ? {$} and V f
respectively.
A general substitution cipher uses a table
s(e|f) which contains for each cipher token f a
probability that the token f is substituted with the
plaintext token e. Such a table for substituting
cipher tokens {A,B,C,D} with plaintext tokens
{a, b, c, d} could for example look like
a b c d
A 0.1 0.2 0.3 0.4
B 0.4 0.2 0.1 0.3
C 0.4 0.1 0.2 0.3
D 0.3 0.4 0.2 0.1
The 1:1 substitution cipher encrypts a given
plaintext into a ciphertext by replacing each plain-
text token with a unique substitute: This means
that the table s(e|f) contains all zeroes, except for
one ?1.0? per f ? Vf and one ?1.0? per e ? Ve.
For example the text
abadcab
would be enciphered to
BCBADBC
when using the substitution
a b c d
A 0 0 0 1
B 1 0 0 0
C 0 1 0 0
D 0 0 1 0
In contrast to the 1:1 substitution cipher, the ho-
mophonic substitution cipher allows multiple ci-
pher tokens per plaintext token, which means that
the table s(e|f) is all zero, except for one ?1.0? per
f ? Vf . For example the above plaintext could be
enciphered to
ABCDECF
when using the homophonic substitution
a b c d
A 1 0 0 0
B 0 1 0 0
C 1 0 0 0
D 0 0 0 1
E 0 0 1 0
F 0 1 0 0
We will use the definition
nmax = maxe
?
f
s(e|f) (2)
to characterize the maximum number of different
cipher symbols allowed per plaintext symbol.
We formalize the 1:1 substitutions with a bijec-
tive function ? : Vf ? Ve and homophonic sub-
stitutions with a general function ? : Vf ? Ve.
Following (Corlett and Penn, 2010), we call
cipher functions ?, for which not all ?(f)?s are
fixed, partial cipher functions . Further, ?? is
said to extend ?, if for all f that are fixed in ?, it
holds that f is also fixed in ?? with ??(f) = ?(f).
1569
The cardinality of ? counts the number of fixed
f ?s in ?.
When talking about partial cipher functions we
use the notation for relations, in which ? ? Vf ?
Ve. For example with
? = {(A, a)} ?? = {(A, a), (B, b)}
it follows that ? ?1?? and
|?| = 1 |??| = 2
?(A) = a ??(A) = a
?(B) = undefined ??(B) = b
The general decipherment goal is to obtain a
mapping ? such that the probability of the deci-
phered text is maximal:
?? = argmax
?
p(?(f1)?(f2)?(f3)...?(fN )) (3)
Here p(. . . ) denotes the language model. De-
pending on the structure of the language model
Equation 3 can be further simplified.
4 Beam Search
In this Section we present our beam search ap-
proach to solving Equation 3. We first present the
general algorithm, containing many higher level
functions. We then discuss possible instances of
these higher level functions.
4.1 General Algorithm
Figure 1 shows the general structure of the beam
search algorithm for the decipherment of substi-
tution ciphers. The general idea is to keep track
of all partial hypotheses in two arrays Hs and Ht.
During search all possible extensions of the partial
hypotheses in Hs are generated and scored. Here,
the function EXT ORDER chooses which cipher
symbol is used next for extension, EXT LIMITS
decides which extensions are allowed, and SCORE
scores the new partial hypotheses. PRUNE then se-
lects a subset of these hypotheses which are stored
to Ht. Afterwards the array Hs is copied to Ht
and the search process continues with the updated
array Hs.
Due to the structure of the algorithm the car-
dinality of all hypotheses in Hs increases in each
step. Thus only hypotheses of the same cardinality
1shorthand notation for ?? extends ?
1: function BEAM SEARCH(EXT ORDER,
EXT LIMITS, PRUNE)
2: init sets Hs, Ht
3: CARDINALITY = 0
4: Hs.ADD((?, 0))
5: while CARDINALITY < |Vf | do
6: f = EXT ORDER[CARDINALITY]
7: for all ? ? Hs do
8: for all e ? Ve do
9: ?? := ? ? {(e, f)}
10: if EXT LIMITS(??) then
11: Ht.ADD(??,SCORE (??))
12: end if
13: end for
14: end for
15: PRUNE(Ht)
16: CARDINALITY = CARDINALITY + 1
17: Hs = Ht
18: Ht.CLEAR()
19: end while
20: return best scoring cipher function in Hs
21: end function
Figure 1: The general structure of the beam
search algorithm for decipherment of substitu-
tion ciphers. The high level functions SCORE,
EXT ORDER, EXT LIMITS and PRUNE are de-
scribed in Section 4.
are compared in the pruning step. When Hs con-
tains full cipher relations, the cipher relation with
the maximal score is returned.2
Figure 2 illustrates how the algorithm explores
the search space for a homophonic substitution ci-
pher. In the following we show several instances
of EXT ORDER, EXT LIMITS, SCORE, and PRUNE.
4.2 Extension Limits (EXT LIMITS)
In addition to the implicit constraint of ? being
a function Vf ? Ve, one might be interested in
functions of a specific form:
For 1:1 substitution ciphers
(EXT LIMITS SIMPLE) ? must fulfill that the
number of cipher letters f ? Vf that map to any
e ? Ve is at most one. Since partial hypotheses
violating this condition can never ?recover? when
being extended, it becomes clear that these partial
hypotheses can be left out from search.
2n-best output can be implemented by returning the n best
scoring hypotheses in the final array Hs.
1570
?a
b
c
d
a
bc
d
a
bc
d
a
bc
d
a
bc
d
. . .
. . .
. . .
. . .
. . .
a
bc
d
a
bc
d
a
bc
d
a
bc
d
. . .
. . .
. . .
. . .
a
bc
d
a
bc
d
a
bc
d
a
bc
d
B C A D
Figure 2: Illustration of the search space explored by the beam search algorithm with cipher vocabulary
Vf = {A,B,C,D}, plaintext vocabulary Ve = {a, b, c, d}, EXT ORDER = (B,C,A,D), homophonic
extension limits (EXT LIMITS HOMOPHONIC) with nmax = 4, and histogram pruning with nkeep = 4.
Hypotheses are visualized as nodes in the tree. The x-axis represents the extension order. At each level
only those 4 hypotheses that survived the histogram pruning process are extended.
Homophonic substitution ciphers can be han-
dled by the beam search algorithm, too. Here
the condition that ? must fulfill is that the num-
ber of cipher letters f ? Vf that map to any
e ? Ve is at most nmax (which we will call
EXT LIMITS HOMOPHONIC). As soon as this con-
dition is violated, all further extensions will also
violate the condition. Thus, these partial hypothe-
ses can be left out.
4.3 Score Estimation (SCORE)
The score estimation function needs to predict
how good or bad a partial hypothesis (cipher func-
tion) might become. We propose simple heuristics
that use the n-gram counts rather than the original
ciphertext. The following formulas consider the
2-gram case. Equations for higher n-gram orders
can be obtained analogously.
With Equation 3 in mind, we want to estimate
the best possible score
N+1?
j=1
p(??(fj)|??(fj?1)) (4)
which can be obtained by extensions ?? ? ?. By
defining counts3
Nff ? =
N+1?
i=1
?(f, fi?1)?(f ?, fi) (5)
3? denotes the Kronecker delta.
we can equivalently use the scores
?
f,f ??V f
Nff ? log p(??(f ?)|??(f)) (6)
Using this formulation it is easy to propose
a whole class of heuristics: We only present
the simplest heuristic, which we call TRIV-
IAL HEURISTIC. Its name stems from the fact that
it only evaluates those parts of a given ?? that are
already fixed, and thus does not estimate any fu-
ture costs. Its score is calculated as
?
f,f ????
Nff ? log p(??(f ?)|??(f)). (7)
Here f, f ? ? ?? denotes that f and f ? need to
be covered in ??. This heuristic is optimistic since
we implicitly use ?0? as estimate for the non fixed
parts of the sum, for which Nff ? log p(?|?) ? 0
holds.
It should be noted that this heuristic can be im-
plemented very efficiently. Given a partial hypoth-
esis ? with given SCORE(?) the score of an exten-
sion ?? can be calculated as
SCORE(??) = SCORE(?) + NEWLY FIXED(?, ??)
(8)
where NEWLY FIXED only includes scores for
n-grams that have been newly fixed in ?? during
the extension step from ? to ??.
1571
4.4 Extension Order (EXT ORDER)
For the choice which ciphertext symbol should be
fixed next during search, several possibilities ex-
ist: The overall goal is to choose an extension or-
der that leads to an overall low error rate. Intu-
itively it seems a good idea to first try to decipher
higher frequent words rather than the lowest fre-
quent ones. It is also clear that the choice of a good
extension order is dependent on the score estima-
tion function SCORE: The extension order should
lead to informative scores early on so that mislead-
ing hypotheses can be pruned out early.
In most of our experiments we will
make use of a very simple extension order:
HIGHEST UNIGRAM FREQUENCY simply fixes
the most frequent symbols first.
In case of the Zodiac-408, we use another strat-
egy that we call HIGHEST NGRAM COUNT ex-
tension order. In each step it greedily chooses
the symbol that will maximize the number of
fixed ciphertext n-grams. This strategy is use-
ful because the SCORE function we use is TRIV-
IAL HEURISTIC, which is not able to provide in-
formative scores if only few full n-grams are fixed.
4.5 Pruning (PRUNE)
We propose two pruning methods:
HISTOGRAM PRUNING sorts all hypotheses
according to their score and then keeps only the
best nkeep hypotheses.
THRESHOLD PRUNING keeps only those hy-
potheses ?keep for which
SCORE(?keep) ? SCORE(?best)? ? (9)
holds for a given parameter ? ? 0. Even though
THRESHOLD PRUNING has the advantage of not
needing to sort all hypotheses, it has proven dif-
ficult to choose proper values for ?. Due to this,
all experiments presented in this paper only use
HISTOGRAM PRUNING.
5 Iterative Beam Search
(Ravi and Knight, 2011b) propose a so called ?it-
erative EM algorithm?. The basic idea is to run a
decipherment algorithm?in their case an EM al-
gorithm based approach?on a subset of the vo-
cabulary. After having obtained the results from
the restricted vocabulary run, these results are used
to initialize a decipherment run with a larger vo-
cabulary. The results from this run will then be
used for a further decipherment run with an even
larger vocabulary and so on. In our large vocabu-
lary word substitution cipher experiments we it-
eratively increase the vocabulary from the 1000
most frequent words, until we finally reach the
50000 most frequent words.
6 Experimental Evaluation
We conduct experiments on letter based 1:1 sub-
stitution ciphers, the homophonic substitution ci-
pher Zodiac-408, and word based 1:1 substitution
ciphers.
For a given reference mapping ?ref , we eval-
uate candidate mappings ? using two error mea-
sures: Mapping Error Rate MER(?, ?ref ) and
Symbol Error Rate SER(?, ?ref ). Roughly
speaking, SER reports the fraction of symbols
in the deciphered text that are not correct, while
MER reports the fraction of incorrect mappings
in ?.
Given a set of symbols Veval with unigram
countsN(v) for v ? Veval, and the total amount of
running symbols Neval = ?
v?Veval
N(v) we define
MER = 1?
?
v?Veval
1
|Veval|
? ?(?(v), ?ref (v))
(10)
SER = 1?
?
v?Veval
N(v)
Neval
? ?(?(v), ?ref (v))
(11)
Thus the SER can be seen as a weighted form of
the MER, emphasizing errors for frequent words.
In decipherment experiments, SER will often be
lower than MER, since it is often easier to deci-
pher frequent words.
6.1 Letter Substitution Ciphers
As ciphertext we use the text of the English
Wikipedia article about History4, remove all pic-
tures, tables, and captions, convert all letters to
lowercase, and then remove all non-letter and non-
space symbols. This corpus forms the basis for
shorter cryptograms of size 2, 4, 8, 16, 32, 64, 128,
and 256?of which we generate 50 each. We make
sure that these shorter cryptograms do not end or
start in the middle of a word. We create the ci-
phertext using a 1:1 substitution cipher in which
we fix the mapping of the space symbol ? ?. This
4http://en.wikipedia.org/wiki/History
1572
Order Beam MER [%] SER [%] RT [s]
3 10 33.15 25.27 0.01
3 100 12.00 6.95 0.06
3 1k 7.37 3.06 0.53
3 10k 5.10 1.42 5.33
3 100k 4.93 1.31 47.70
3 ?? 4.93 1.31 19 700.00
4 10 55.97 48.19 0.02
4 100 18.15 14.41 0.10
4 1k 5.13 3.42 0.89
4 10k 1.55 1.00 8.57
4 100k 0.39 0.06 81.34
5 10 69.19 60.13 0.02
5 100 35.57 29.02 0.14
5 1k 10.89 8.47 1.29
5 10k 0.38 0.06 11.91
5 100k 0.38 0.06 120.38
6 10 74.65 64.77 0.03
6 100 40.26 33.38 0.17
6 1k 13.53 10.08 1.58
6 10k 2.45 1.28 15.77
6 100k 0.09 0.05 151.85
Table 1: Symbol error rates (SER), Mapping er-
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for decipherment of letter
substitution ciphers of length 128. Runtimes are
reported on a single core machine. Results for
beam size ??? were obtained using A? search.
makes our experiments comparable to those con-
ducted in (Ravi and Knight, 2008). Note that fix-
ing the ? ? symbol makes the problem much eas-
ier: The exact methods show much higher com-
putational demands for lengths beyond 256 letters
when not fixing the space symbol.
The plaintext language model we use a letter
based (Ve = {a, . . . , z, }) language model trained
on a subset of the Gigaword corpus (Graff et al,
2007).
We use extension limits fitting the 1:1 substi-
tution cipher nmax = 1 and histogram pruning
with different beam sizes.
For comparison we reimplemented the ILP ap-
proach from (Ravi and Knight, 2008) as well as
the A? approach from (Corlett and Penn, 2010).
Figure 3 shows the results of our algorithm for
different cipher length. We use a beam size of
100k for the 4, 5 and 6-gram case. Most remark-
ably our 6-gram beam search results are signifi-
cantly better than all methods presented in the lit-
erature. For the cipher length of 32 we obtain a
symbol error rate of just 4.1% where the optimal
solution (i.e. without search errors) for a 3-gram
2 4 8 16 32 64 128 2560
10
20
30
40
50
60
70
80
90
100
Cipher Length
Sy
mb
ol
Er
ror
Ra
te
(%
)
Exact 2gram
Exact 3gram
Beam 3gram
Beam 4gram
Beam 5gram
Beam 6gram
Figure 3: Symbol error rates for decipherment of
letter substitution ciphers of different lengths. Er-
ror bars show the 95% confidence interval based
on decipherment on 50 different ciphers. Beam
search was performed with a beam size of ?100k?.
language model has a symbol error rate as high as
38.3%.
Table 1 shows error rates and runtimes of our
algorithm for different beam sizes and language
model orders given a fixed ciphertext length of 128
letters. It can be seen that achieving close to op-
timal results is possible in a fraction of the CPU
time needed for the optimal solution: In the 3-
gram case the optimal solution is found in 1400 thof the time needed using A? search. It can also
be seen that increasing the language model order
does not increase the runtime much while provid-
ing better results if the beam size is large enough:
If the beam size is not large enough, the decipher-
ment accuracy decreases when increasing the lan-
guage model order: This is because the higher or-
der heuristics do not give reliable scores if only
few n-grams are fixed.
To summarize: The beam search method is sig-
nificantly faster and obtains significantly better re-
sults than previously published methods. Further-
more it offers a good trade-off between CPU time
and decipherment accuracy.
1573
i l i k e k i l l i n g p e o p l
e b e c a u s e i t i s s o m u c
h f u n i t i n m o r e f u n t h
a n k i l l i n g w i l d g a m e
i n t h e f o r r e s t b e c a u
s e m a n i s t h e m o a t r a n
g e r o u e a n a m a l o f a l l
t o k i l l s o m e t h i n g g i
Figure 4: First 136 letters of the Zodiac-408 cipher
and its decipherment.
6.2 Zodiac-408 Cipher
As ciphertext we use a transcription of the
Zodiac-408 cipher. It consists of 54 different sym-
bols and has a length of 408 symbols.5 The ci-
pher has been deciphered by hand before. It con-
tains some mistakes and ambiguities: For exam-
ple, it contains misspelled words like forrest (vs.
forest), experence (vs. experience), or paradice
(vs. paradise). Furthermore, the last 17 letters
of the cipher do not form understandable English
when applying the same homophonic substitution
that deciphers the rest of the cipher. This makes
the Zodiac-408 a good candidate for testing the ro-
bustness of a decipherment algorithm.
We assume a homophonic substitution cipher,
even though the cipher is not strictly homophonic:
It contains three cipher symbols that correspond
to two or more plaintext symbols. We ignore this
fact for our experiments, and count?in case of the
MER only?the decipherment for these symbols
as correct when the obtained mapping is contained
in the set of reference symbols. We use extension
limits with nmax = 8 and histogram pruning
with beam sizes of 10k up to 10M .
The plaintext language model is based on the
same subset of Gigaword (Graff et al, 2007) data
as the experiments for the letter substitution ci-
phers. However, we first removed all space sym-
5hence its name
Order Beam MER [%] SER [%] RT [s]
4 10k 71.43 67.16 222
4 100k 66.07 61.52 1 460
4 1M 39.29 34.80 12 701
4 10M 19.64 16.18 125 056
5 10k 94.64 96.57 257
5 100k 10.71 5.39 1 706
5 1M 8.93 3.19 14 724
5 10M 8.93 3.19 152 764
6 10k 87.50 84.80 262
6 100k 94.64 94.61 1 992
6 1M 8.93 2.70 17 701
6 10M 7.14 1.96 167 181
Table 2: Symbol error rates (SER), Mapping er-
ror rates (MER) and runtimes (RT) in dependence
of language model order (ORDER) and histogram
pruning size (BEAM) for the decipherment of the
Zodiac-408 cipher. Runtimes are reported on a
128-core machine.
bols from the training corpus before training the
actual letter based 4-gram, 5-gram, and 6-gram
language model on it. Other than (Ravi and
Knight, 2011a) we do not use any word lists and
by that avoid any degrees of freedom in how to in-
tegrate it into the search process: Only an n-gram
language model is used.
Figure 4 shows the first parts of the cipher and
our best decipherment. Table 2 shows the results
of our algorithm on the Zodiac-408 cipher for dif-
ferent language model orders and pruning settings.
To summarize: Our final decipherment?for
which we only use a 6-gram language model?has
a symbol error rate of only 2.0%, which is slightly
better than the best decipherment reported in (Ravi
and Knight, 2011a). They used an n-gram lan-
guage model together with a word dictionary and
obtained a symbol error rate of 2.2%. We thus ob-
tain better results with less modeling.
6.3 Word Substitution Ciphers
As ciphertext, we use parts of the JRC corpus
(Steinberger et al, 2006) and the Gigaword cor-
pus (Graff et al, 2007). While the full JRC corpus
contains roughly 180k word types and consists of
approximately 70M running words, the full Giga-
word corpus contains around 2M word types and
roughly 1.5G running words.
We run experiments for three different setups:
The ?JRC? and ?Gigaword? setups use the first
half of the respective corpus as ciphertext, while
the plaintext language model of order n = 3 was
1574
Setup Top MER [%] SER [%] RT [hh:mm]
Gigaword 1k 81.91 27.38 03h 10m
Gigaword 10k 30.29 8.55 09h 21m
Gigaword 20k 21.78 6.51 16h 25m
Gigaword 50k 19.40 5.96 49h 02m
JRC 1k 73.28 15.42 00h 32m
JRC 10k 15.82 2.61 13h 03m
JRC-Shuf 1k 76.83 19.04 00h 31m
JRC-Shuf 10k 15.08 2.58 13h 03m
Table 3: Word error rates (WER), Mapping error
rates (MER) and runtimes (RT) for iterative deci-
pherment run on the (TOP) most frequent words.
Error rates are evaluated on the full vocabulary.
Runtimes are reported on a 128-core machine.
trained on the second half. The ?JRC-Shuf? setup
is created by randomly selecting half of the sen-
tences of the JRC corpus as ciphertext, while the
language model was trained on the complemen-
tary half of the corpus.
We encrypt the ciphertext using a 1:1 substi-
tution cipher on word level, imposing a much
larger vocabulary size. We use histogram prun-
ing with a beam size of 128 and use extension
limits of nmax = 1. Different to the previous
experiments, we use iterative beam search with
iterations as shown in Table 3.
The results for the Gigaword task are directly
comparable to the word substitution experiments
presented in (Dou and Knight, 2012). Their fi-
nal decipherment has a symbol error rate of 7.8%.
Our algorithm obtains 6.0% symbol error rate. It
should be noted that the improvements of 1.8%
symbol error rate correspond to a larger improve-
ment in terms of mapping error rate. This can also
be seen when looking at Table 3: An improvement
of the symbol error rate from 6.51% to 5.96% cor-
responds to an improvement of mapping error rate
from 21.78% to 19.40%.
To summarize: Using our beam search algo-
rithm in an iterative fashion, we are able to im-
prove the state-of-the-art decipherment accuracy
for word substitution ciphers.
7 Conclusion
We have presented a simple and effective beam
search approach to the decipherment problem. We
have shown in a variety of experiments?letter
substitution ciphers, the Zodiac-408, and word
substitution ciphers?that our approach outper-
forms the current state of the art while being con-
ceptually simpler and keeping computational de-
mands low.
We want to note that the presented algorithm is
not restricted to 1:1 and homophonic substitution
ciphers: It is possible to extend the algorithm to
solve n:m mappings. Along with more sophis-
ticated pruning strategies, score estimation func-
tions, and extension orders, this will be left for fu-
ture research.
Acknowledgements
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. Experiments were
performed with computing resources granted by
JARA-HPC from RWTH Aachen University un-
der project ?jara0040?.
References
Andrew J. Clark. 1998. Optimisation heuristics for
cryptology. Ph.D. thesis, Faculty of Information
Technology, Queensland University of Technology.
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 266?275,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Linguistic Data Consortium, Philadelphia.
George W. Hart. 1994. To decode short cryptograms.
Communications of the Association for Computing
Machinery (CACM), 37(9):102?108, September.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Edwin Olson. 2007. Robust dictionary attack of
short simple substitution ciphers. Cryptologia,
31(4):332?342, October.
1575
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011a. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
239?247, Portland, Oregon, June. Association for
Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011b. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12?21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz? Erjavec, and Dan Tufis?. 2006.
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC), pages 2142?2147.
European Language Resources Association.
1576
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759?764,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
EM Decipherment for Large Vocabularies
Malte Nuhn and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper addresses the problem of EM-
based decipherment for large vocabular-
ies. Here, decipherment is essentially
a tagging problem: Every cipher token
is tagged with some plaintext type. As
with other tagging problems, this one can
be treated as a Hidden Markov Model
(HMM), only here, the vocabularies are
large, so the usual O(NV
2
) exact EM ap-
proach is infeasible. When faced with
this situation, many people turn to sam-
pling. However, we propose to use a type
of approximate EM and show that it works
well. The basic idea is to collect fractional
counts only over a small subset of links
in the forward-backward lattice. The sub-
set is different for each iteration of EM.
One option is to use beam search to do the
subsetting. The second method restricts
the successor words that are looked at, for
each hypothesis. It does this by consulting
pre-computed tables of likely n-grams and
likely substitutions.
1 Introduction
The decipherment of probabilistic substitution ci-
phers (ciphers in which each plaintext token can
be substituted by any cipher token, following a
distribution p(f |e), cf. Table 2) can be seen as
an important step towards decipherment for MT.
This problem has not been studied explicitly be-
fore. Scaling to larger vocabularies for proba-
bilistic substitution ciphers decipherment is a dif-
ficult problem: The algorithms for 1:1 or homo-
phonic substitution ciphers are not applicable, and
standard algorithms like EM training become in-
tractable when vocabulary sizes go beyond a few
hundred words. In this paper we present an effi-
cient EM based training procedure for probabilis-
tic substitution ciphers which provides high deci-
pherment accuracies while having low computa-
tional requirements. The proposed approach al-
lows using high order n-gram language models,
and is scalable to large vocabulary sizes. We show
improvements in decipherment accuracy in a va-
riety of experiments (including MT) while being
computationally more efficient than previous pub-
lished work on EM-based decipherment.
2 Related Work
Several methods exist for deciphering 1:1 substi-
tution ciphers: Ravi and Knight (2008) solve 1:1
substitution ciphers by formulating the decipher-
ment problem as an integer linear program. Cor-
lett and Penn (2010) solve the same problem us-
ing A
?
search. Nuhn et al (2013) present a beam
search approach that scales to large vocabulary
and high order language models. Even though be-
ing successful, these algorithms are not applicable
to probabilistic substitution ciphers, or any of its
extensions as they occur in decipherment for ma-
chine translation.
EM training for probabilistic ciphers was first
covered in Ravi and Knight (2011). Nuhn et al
(2012) have given an approximation to exact EM
training using context vectors, allowing to train-
ing models even for larger vocabulary sizes. Ravi
(2013) report results on the OPUS subtitle corpus
using an elaborate hash sampling technique, based
on n-gram language models and context vectors,
that is computationally very efficient.
Conventional beam search is a well studied
topic: Huang et al (1992) present beam search for
automatic speech recognition, using fine-grained
pruning procedures. Similarly, Young and Young
(1994) present an HMM toolkit, including pruned
forward-backward EM training. Pal et al (2006)
use beam search for training of CRFs.
759
Method Publications Complexity
EM Full (Knight et al, 2006), (Ravi and Knight, 2011) O(NV
n
)
EM Fixed Candidates (Nuhn et al, 2012) O(N)
EM Beam This Work O(NV )
EM Lookahead This Work O(N)
Table 1: Different approximations to exact EM training for decipherment. N is the cipher sequence
length, V the size of the target vocabulary, and n the order of the language model.
The main contribution of this work is the pre-
selection beam search that?to the best of our
knowledge?was not known in literature before,
and serves as an important step to applying EM
training to the large vocabulary decipherment
problem. Table 1 gives an overview of the EM
based methods. More details are given in Sec-
tion 3.2.
3 Probabilistic Substitution Ciphers
We define probabilistic substitutions ciphers us-
ing the following generative story for ciphertext
sequences f
N
1
:
1. Stochastically generate a plaintext sequence
e
N
1
according to a bigram
1
language model.
2. For each plaintext token e
n
choose a substi-
tution f
n
with probability P (f
n
|e
n
, ?).
This generative story corresponds to the model
p(e
N
1
, f
N
1
, ?) = p(e
N
1
) ? p(f
N
1
|e
N
1
, ?) , (1)
with the zero-order membership model
p(f
N
1
|e
N
1
, ?) =
N
?
n=1
p
lex
(f
n
|e
n
, ?) (2)
with parameters p(f |e, ?) ? ?
f |e
and normaliza-
tion constraints ?e
?
f
?
f |e
= 1, and first-order
plaintext sequence model
P (e
N
1
) =
N
?
n=1
p
LM
(e
n
|e
n?1
) . (3)
Thus, the probabilistic substitution cipher can be
seen as a Hidden Markov Model. Table 2 gives an
overview over the model. We want to find those
parameters ? that maximize the marginal distribu-
tion p(f
N
1
|?):
? = argmax
?
?
?
?
?
?
[e
N
1
]
p(f
N
1
, e
N
1
|?
?
)
?
?
?
(4)
1
This can be generalized to n-gram language models.
After we obtained the parameters ? we
can obtain e
N
1
as the Viterbi decoding
argmax
e
N
1
{
p(e
N
1
|f
N
1
, ?)
}
.
3.1 Exact EM training
In the decipherment setting, we are given the ob-
served ciphertext f
N
1
and the model p(f
N
1
|e
N
1
, ?)
that explains how the observed ciphertext has been
generated given a latent plaintext e
N
1
. Marginaliz-
ing the unknown e
N
1
, we would like to obtain the
maximum likelihood estimate of ? as specified in
Equation 4. We iteratively compute the maximum
likelihood estimate by applying the EM algorithm
(Dempster et al, 1977):
?
?
f |e
=
?
n:f
n
=f
p
n
(e|f
N
1
, ?)
?
f
?
n:f
n
=f
p
n
(e|f
N
1
, ?)
(5)
with
p
n
(e|f
N
1
, ?) =
?
[e
N
1
:e
n
=e]
p(e
N
1
|f
N
1
, ?) (6)
being the posterior probability of observing the
plaintext symbol e at position n given the cipher-
text sequence f
N
1
and the current parameters ?.
p
n
(e|f
N
1
, ?) can be efficiently computed using the
forward-backward algorithm.
3.2 Approximations to EM-Training
The computational complexity of EM training
stems from the sum
?
[e
N
1
:e
n
=e]
contained in the
posterior p
n
(e|f
N
1
, ?). However, we can approx-
imate this sum (and hope that the EM training
procedure is still working) by only evaluating the
dominating terms, i.e. we only evaluate the sum
for sequences e
N
1
that have the largest contribu-
tions to
?
[e
N
1
:e
n
=e]
. Note that due to this approxi-
mation, the new parameter estimates in Equation 5
can become zero. This is a critical issue, since
pairs (e, f) with p(f |e) = 0 cannot recover from
760
Sequence of cipher tokens : f
N
1
= f
1
, . . . , f
N
Sequence of plaintext tokens : e
N
1
= e
1
, . . . , e
N
Joint probability : p(f
N
1
, e
N
1
|?) = p(e
N
1
) ? p(f
N
1
|e
N
1
, ?)
Language model : p(e
N
1
) =
N
?
n=1
p
LM
(e
n
|e
n?1
)
Membership probabilities : p(f
N
1
|e
N
1
, ?) =
N
?
n=1
p
lex
(f
n
|e
n
, ?)
Paramater Set : ? = {?
f |e
}, p(f |e, ?) = ?
f |e
Normalization : ?e :
?
f
?
f |e
= 1
Probability of cipher sequence : p(f
N
1
|?) =
?
[e
N
1
]
p(f
N
1
, e
N
1
|?)
Table 2: Definition of the probabilistic substitution cipher model. In contrast to simple or homophonic
substitution ciphers, each plaintext token can be substituted by multiple cipher text tokens. The parameter
?
f |e
represents the probability of substituting token e with token f .
acquiring zero probability in some early iteration.
In order to allow the lexicon to recover from these
zeros, we use a smoothed lexicon ?p
lex
(f |e) =
?p
lex
(f |e) + (1 ? ?)/|V
f
| with ? = 0.9 when
conducting the E-Step.
3.2.1 Beam Search
Instead of evaluating the sum for terms with the
exact largest contributions, we restrict ourselves to
terms that are likely to have a large contribution to
the sum, dropping any guarantees about the actual
contribution of these terms.
Beam search is a well known algorithm related
to this idea: We build up sequences e
c
1
with grow-
ing cardinality c. For each cardinality, only a set
of the B most promising hypotheses is kept. Then
for each active hypothesis of cardinality c, all pos-
sible extensions with substitutions f
c+1
? e
c+1
are explored. Then in turn only the best B out of
the resulting B ? V
e
many hypotheses are kept and
the algorithm continues with the next cardinality.
Reaching the full cardinality N , the algorithm ex-
plored B ?N ? V
e
many hypotheses, resulting in a
complexity of O(BNV
e
).
Even though EM training using beam search
works well, it still suffers from exploring all V
e
possible extensions for each active hypothesis, and
thus scaling linearly with the vocabulary size. Due
to that, standard beam search EM training is too
slow to be used in the decipherment setting.
3.2.2 Preselection Search
Instead of evaluating all substitutions f
c+1
?
e
c+1
? V
e
, this algorithm only expands a fixed
number of candidates: For a hypothesis ending in
a language model state ?, we only look at B
LM
many successor words e
c+1
with the highest LM
probability p
LM
(e
c+1
|?) and at B
lex
many suc-
cessor words e
c+1
with the highest lexical prob-
ability p
lex
(f
c+1
|e
c+1
). Altogether, for each hy-
pothesis we only look at (B
LM
+B
lex
) many suc-
cessor states. Then, just like in the standard beam
search approach, we prune all explored new hy-
potheses and continue with the pruned set of B
many hypotheses. Thus, for a cipher of length N
we only explore N ? B ? (B
LM
+ B
lex
) many hy-
potheses.
2
Intuitively speaking, our approach solves the
EM training problem for decipherment using large
vocabularies by focusing only on those substitu-
tions that either seem likely due to the language
model (?What word is likely to follow the cur-
rent partial decipherment??) or due to the lexicon
model (?Based on my knowledge about the cur-
rent cipher token, what is the most likely substitu-
tion??).
In order to efficiently find the maximizing e for
p
LM
(e|?) and p
lex
(f |e), we build a lookup ta-
ble that contains for each language model state ?
the B
LM
best successor words e, and a separate
lookup table that contains for each source word f
the B
lex
highest scoring tokens e. The language
model lookup table remains constant during all it-
erations, while the lexicon lookup table needs to
be updated between each iteration.
Note that the size of the LM lookup table scales
linearly with the number of language model states.
Thus the memory requirements for the lookup ta-
2
We always use B = 100, B
lex
= 5, and B
LM
= 50.
761
f1 f2 f3 f4 f5
e5e4e3e2e1
Beam Search Preselection SearchFull Search
f6
...start
Vocab
Sentence
Figure 1: Illustration of the search space explored by full search, beam search, and preselection search.
Full search keeps all possible hypotheses at cardinality c and explores all possible substitutions at (c+1).
Beam search only keeps the B most promising hypotheses and then selects the best new hypotheses for
cardinality (c+ 1) from all possible substitutions. Preselection search keeps only the B best hypotheses
for every cardinality c and only looks at the (B
lex
+ B
LM
) most promising substitutions for cardinality
(c+ 1) based on the current lexicon (B
lex
dashed lines) and language model (B
LM
solid lines).
Name Lang. Sent. Words Voc.
VERBMOBIL English 27,862 294,902 3,723
OPUS
Spanish 13,181 39,185 562
English 19,770 61,835 411
Table 3: Statistics of the copora used in this pa-
per: The VERBMOBIL corpus is used to conduct
experiments on simple substitution ciphers, while
the OPUS corpus is used in our Machine Transla-
tion experiments.
ble do not form a practical problem of our ap-
proach. Figure 1 illustrates full search, beam
search, and our proposed method.
4 Experimental Evaluation
We first show experiments for data in which the
underlying model is an actual 1:1 substitution ci-
pher. In this case, we report the word accuracy
of the final decipherment. We then show experi-
ments for a simple machine translation task. Here
we report translation quality in BLEU. The cor-
pora used in this paper are shown in Table 3.
4.1 Simple Substitution Ciphers
In this set of experiments, we compare the exact
EM training to the approximations presented in
this paper. We use the English side of the German-
English VERBMOBIL corpus (Wahlster, 2000) to
construct a word substitution cipher, by substitut-
ing every word type with a unique number. In or-
der to have a non-parallel setup, we train language
Vocab LM Method Acc.[%] Time[h]
200 2 exact 97.19 224.88
200 2 beam 98.87 9.04
200 2 presel. 98.50 4.14
500 2 beam 92.12 24.27
500 2 presel. 92.16 4.70
3 661 3 beam 91.16 302.81
3 661 3 presel. 90.92 19.68
3 661 4 presel. 92.14 23.72
Table 4: Results for simple substitution ciphers
based on the VERBMOBIL corpus using exact,
beam, and preselection EM. Exact EM is not
tractable for vocabulary sizes above 200.
models of order 2, 3 and 4 on the first half of the
corpus and use the second half as ciphertext. Ta-
ble 4 shows the results of our experiments.
Since exact EM is not tractable for vocabulary
sizes beyond 200 words, we train word classes on
the whole corpus and map the words to classes
(consistent along the first and second half of the
corpus). By doing this, we create new simple sub-
stitution ciphers with smaller vocabularies of size
200 and 500. For the smallest setup, we can di-
rectly compare all three EM variants. We also in-
clude experiments on the original corpus with vo-
cabulary size of 3661. When comparing exact EM
training with beam- and preselection EM training,
the first thing we notice is that it takes about 20
times longer to run the exact EM training than
training with beam EM, and about 50 times longer
than the preselection EM training. Interestingly,
762
Model Method BLEU [%] Runtime
2-gram Exact EM(Ravi and Knight, 2011) 15.3 850.0h
whole segment lm Exact EM(Ravi and Knight, 2011) 19.3 850.0h
2-gram Preselection EM (This work) 15.7 1.8h
3-gram Preselection EM (This work) 19.5 1.9h
Table 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on
the Spanish/English OPUS corpus using only non-parallel corpora for training.
the accuracy of the approximations to exact EM
training is better than that of the exact EM train-
ing. Even though this needs further investigation,
it is clear that the pruned versions of EM training
find sparser distributions p
lex
(f |e): This is desir-
able in this set of experiments, and could be the
reason for improved performance.
For larger vocabularies, exact EM training is not
tractable anymore. We thus constrain ourselves to
running experiments with beam and preselection
EM training only. Here we can see that the runtime
of the preselection search is roughly the same as
when running on a smaller vocabulary, while the
beam search runtime scales almost linearly with
the vocabulary size. For the full vocabulary of
3661 words, preselection EM using a 4-gram LM
needs less than 7% of the time of beam EM with a
3-gram LM and performs by 1% better in symbol
accuracy.
To summarize: Beam search EM is an or-
der of magnitude faster than exact EM training
while even increasing decipherment accuracy. Our
new preselection search method is in turn or-
ders of magnitudes faster than beam search EM
while even being able to outperform exact EM and
beam EM by using higher order language mod-
els. We were thus able to scale the EM deci-
pherment to larger vocabularies of several thou-
sand words. The runtime behavior is also consis-
tent with the computational complexity discussed
in Section 3.2.
4.2 Machine Translation
We show that our algorithm is directly applicable
to the decipherment problem for machine transla-
tion. We use the same simplified translation model
as presented by Ravi and Knight (2011). Because
this translation model allows insertions and dele-
tions, hypotheses of different cardinalities coex-
ist during search. We extend our search approach
such that pruning is done for each cardinality sep-
arately. Other than that, we use the same pres-
election search procedure as used for the simple
substitution cipher task.
We run experiments on the opus corpus as pre-
sented in (Tiedemann, 2009). Table 5 shows pre-
viously published results using EM together with
the results of our new method:
(Ravi and Knight, 2011) is the only publication
that reports results using exact EM training and
only n-gram language models on the target side:
It has an estimated runtime of 850h. All other
published results (using EM training and Bayesian
inference) use context vectors as an additional
source of information: This might be an explana-
tion why Nuhn et al (2012) and Ravi (2013) are
able to outperform exact EM training as reported
by Ravi and Knight (2011). (Ravi, 2013) reports
the most efficient method so far: It only consumes
about 3h of computation time. However, as men-
tioned before, those results are not directly compa-
rable to our work, since they use additional context
information on the target side.
Our algorithm clearly outperforms the exact
EM training in run time, and even slighlty im-
proves performance in BLEU. Similar to the sim-
ple substitution case, the improved performance
might be caused by inferring a sparser distribution
p
lex
(f |e). However, this requires further investi-
gation.
5 Conclusion
We have shown a conceptually consistent and easy
to implement EM based training method for deci-
pherment that outperforms exact and beam search
EM training for simple substitution ciphers and
decipherment for machine translation, while re-
ducing training time to a fraction of exact and
beam EM. We also point out that the preselection
method presented in this paper is not restricted to
word based translation models and can also be ap-
plied to phrase based translation models.
763
References
Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1040?1047, Uppsala, Sweden, July. The As-
sociation for Computer Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, B, 39.
Xuedong Huang, Fileno Alleva, Hsiao wuen Hon, Mei
yuh Hwang, and Ronald Rosenfeld. 1992. The
sphinx-ii speech recognition system: An overview.
Computer, Speech and Language, 7:137?148.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised Analysis for De-
cipherment Problems. In Proceedings of the
COLING/ACL on Main conference poster sessions,
COLING-ACL ?06, pages 499?506, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 156?164,
Jeju, Republic of Korea, July. Association for Com-
putational Linguistics.
Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Annual Meeting of the Assoc. for Computational
Linguistics, pages 1569?1576, Sofia, Bulgaria, Au-
gust.
Chris Pal, Charles Sutton, and Andrew McCallum.
2006. Sparse forward-backward using minimum di-
vergence beams for fast training of conditional ran-
dom fields. In International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP).
Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 812?819, Honolulu, Hawaii. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 12?21, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Sujith Ravi. 2013. Scalable decipherment for ma-
chine translation via hash sampling. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 362?371,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
J?org Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of speech-to-speech translations. Springer-
Verlag, Berlin.
S.J. Young and Sj Young. 1994. The htk hidden
markov model toolkit: Design and philosophy. En-
tropic Cambridge Research Laboratory, Ltd, 2:2?
44.
764
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93?97,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2010
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor Leusch,
Saab Mansour, Daniel Stein and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
In this paper we describe the statisti-
cal machine translation system of the
RWTH Aachen University developed for
the translation task of the Fifth Workshop
on Statistical Machine Translation. State-
of-the-art phrase-based and hierarchical
statistical MT systems are augmented
with appropriate morpho-syntactic en-
hancements, as well as alternative phrase
training methods and extended lexicon
models. For some tasks, a system combi-
nation of the best systems was used to gen-
erate a final hypothesis. We participated
in the constrained condition of German-
English and French-English in each trans-
lation direction.
1 Introduction
This paper describes the statistical MT system
used for our participation in the WMT 2010 shared
translation task. We used it as an opportunity to in-
corporate novel methods which have been investi-
gated at RWTH over the last year and which have
proven to be successful in other evaluations.
For all tasks we used standard alignment and
training tools as well as our in-house phrase-
based and hierarchical statistical MT decoders.
When German was involved, morpho-syntactic
preprocessing was applied. An alternative phrase-
training method and additional models were tested
and investigated with respect to their effect for the
different language pairs. For two of the language
pairs we could improve performance by system
combination.
An overview of the systems and models will fol-
low in Section 2 and 3, which describe the base-
line architecture, followed by descriptions of the
additional system components. Morpho-syntactic
analysis and other preprocessing issues are cov-
ered by Section 4. Finally, translation results for
the different languages and system variants are
presented in Section 5.
2 Translation Systems
For the WMT 2010 Evaluation we used stan-
dard phrase-based and hierarchical translation sys-
tems. Alignments were trained with a variant of
GIZA++. Target language models are 4-gram lan-
guage models trained with the SRI toolkit, using
Kneser-Ney discounting with interpolation.
2.1 Phrase-Based System
Our phrase-based translation system is similar to
the one described in (Zens and Ney, 2008). Phrase
pairs are extracted from a word-aligned bilingual
corpus and their translation probability in both di-
rections is estimated by relative frequencies. Ad-
ditional models include a standard n-gram lan-
guage model, phrase-level IBM1, word-, phrase-
and distortion-penalties and a discriminative re-
ordering model as described in (Zens and Ney,
2006).
2.2 Hierarchical System
Our hierarchical phrase-based system is similar to
the one described in (Chiang, 2007). It allows for
gaps in the phrases by employing a context-free
grammar and a CYK-like parsing during the de-
coding step. It has similar features as the phrase-
based system mentioned above. For some sys-
tems, we only allowed the non-terminals in hierar-
chical phrases to be substituted with initial phrases
as in (Iglesias et al, 2009), which gave better re-
sults on some language pairs. We will refer to this
as ?shallow rules?.
2.3 System Combination
The RWTH approach to MT system combination
of the French?English systems as well as the
German?English systems is a refined version of
the ROVER approach in ASR (Fiscus, 1997) with
93
German?English French?English English?French
BLEU # Phrases BLEU # Phrases BLEU # Phrases
Standard 19.7 128M 25.5 225M 23.7 261M
FA 20.0 12M 25.9 35M 24.0 33M
Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For
German?English and English?French phrase table interpolation was applied.
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al (2006). Several improvements have
been added later (Matusov et al, 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. Alignments between the sys-
tems are learned by GIZA++, a one-to-one align-
ment is generated from the learned state occupa-
tion probabilities.
From these alignments, a confusion network
(CN) is then built using one of the hypotheses as
?skeleton? or ?primary? hypothesis. We do not
make a hard decision on which of the hypothe-
ses to use for that, but instead combine all pos-
sible CNs into a single lattice. Majority voting on
the generated lattice is performed using the prior
probabilities for each system as well as other sta-
tistical models such as a special trigram language
model. This language model is also learned on
the input hypotheses. The intention is to favor
longer phrases contained in individual hypotheses.
The translation with the best total score within this
lattice is selected as consensus translation. Scal-
ing factors of these models are optimized similar
to MERT using the Downhill Simplex algorithm.
As the objective function for this optimization, we
selected a linear combination of BLEU and TER
with a weight of 2 on the former; a combination
that has proven to deliver stable results on sev-
eral MT evaluation measures in preceding experi-
ments.
In contrast to previous years, we now include a
separate consensus true casing step to exploit the
true casing capabilities of some of the input sys-
tems: After generating a (lower cased) consensus
translation from the CN, we sum up the counts of
different casing variants of each word in a sen-
tence over the input hypotheses, and use the ma-
jority casing over those. In previous experiments,
this showed to work significantly better than us-
ing a fixed non-consensus true caser, and main-
tains flexibility on the input systems.
3 New Additional Models
3.1 Forced Alignment
For the German?English, French?English and
English?French language tasks we applied a
forced alignment procedure to train the phrase
translation model with the EM algorithm, sim-
ilar to the one described in (DeNero et al,
2006). Here, the phrase translation probabil-
ities are estimated from their relative frequen-
cies in the phrase-aligned training data. The
phrase alignment is produced by a modified
version of the translation decoder. In addi-
tion to providing a statistically well-founded
phrase model, this has the benefit of produc-
ing smaller phrase tables and thus allowing
more rapid experiments. For the language pairs
German?English and English?French the best
results were achieved by log-linear interpolation
of the standard phrase table with the generative
model. For French?English we directly used the
model trained by forced alignment. A detailed
description of the training procedure is given in
(Wuebker et al, 2010). Table 1 shows the system
performances and phrase table sizes with the stan-
dard phrase table and the one trained with forced
alignment after the first EM iteration. We can see
that the generative model reduces the phrase table
size by 85-90% while increasing performance by
0.3% to 0.4% BLEU.
3.2 Extended Lexicon Models
In previous work, RWTH was able to show the
positive impact of extended lexicon models that
cope with lexical context beyond the limited hori-
zon of phrase pairs and n-gram language models.
Mauser et al (2009) report improvements of
up to +1% in BLEU on large-scale systems for
Chinese?English and Arabic?English by incor-
porating discriminative and trigger-based lexicon
models into a state-of-the-art phrase-based de-
coder. They discuss how the two types of lexicon
94
models help to select content words by capturing
long-distance effects.
The triplet model is a straightforward extension
of the IBM model 1 with a second trigger, and like
the former is trained iteratively using the EM al-
gorithm. In search, the triggers are usually on the
source side, i.e., p(e|f, f ?) is modeled. The path-
constrained triplet model restricts the first source
trigger to the aligned target word, whereas the sec-
ond trigger can move along the whole source sen-
tence. See (Hasan et al, 2008) for a detailed de-
scription and variants of the model and its training.
For the WMT 2010 evaluation, triplets mod-
eling p(e|f, f ?) were trained and applied di-
rectly in search for all relevant language pairs.
Path-constrained models were trained on the in-
domain news-commentary data only and on the
news-commentary plus the Europarl data. Al-
though experience from similar setups indicates
that triplet lexicon models can be beneficial for
machine translation between the languages En-
glish, French, and German, on this year?s WMT
translation tasks slight improvements on the devel-
opment sets did not or only partially carry over to
the held-out test sets. Nevertheless, systems with
triplets were used for system combination, as ex-
tended lexicon models often help to predict con-
tent words and to capture long-range dependen-
cies. Thus they can help to find a strong consensus
hypothesis.
3.3 Unsupervised Training
Due to the small size of the English?German re-
sources available for language modeling as well as
for lexicon extraction, we decided to apply the un-
supervised adaptation suggested in (Schwenk and
Senellart, 2009). We use a baseline SMT system to
translate in-domain monolingual source data, fil-
ter the translations according to a decoder score
normalized by sentence length, add this synthetic
bilingual data to the original one and rebuild the
SMT system from scratch.
The motivation behind the method is that the
phrase table will adapt to the genre, and thus
let phrases which are domain related have higher
probabilities. Two phenomena are observed from
phrase tables and the corresponding translations:
? Phrase translation probabilities are changed,
making the system choose better phrase
translation candidates.
Running Words
English German
Bilingual 44.3M 43.4M
Dict. 1.4M 1.2M
AFP 610.7M
AFP unsup. 152.0M 157.3M
Table 2: Overview on data for unsupervised train-
ing.
BLEU
Dev Test
baseline 15.0 14.7
+dict. 15.1 14.6
+unsup.+dict 15.4 14.9
Table 3: Results for unsupervised training method.
? Phrases which appear repeatedly in the do-
main get higher probabilities, so that the de-
coder can better segment the sentence.
To implement this idea, we translate the AFP part
of the English LDC Gigaword v4.0 and obtain the
synthetic data.
To decrease the number of OOV words, we use
dictionaries from the stardict directory as addi-
tional bilingual data to translate the AFP corpus.
We filter sentences with OOV words and sentences
longer than 100 tokens. A summary of the addi-
tional data used is shown in Table 2.
We tried to use the best 10%, 20% and 40% of
the synthetic data, where the 40% option worked
best. A summary of the results is given in Table 3.
Although this is our best result for the
English?German task, it was not submitted, be-
cause the use of the dictionary is not allowed in
the constrained track.
4 Preprocessing
4.1 Large Parallel Data
In addition to the provided parallel Europarl and
news-commentary corpora, also the large French-
English news corpus (about 22.5 Mio. sentence
pairs) and the French-English UN corpus (about
7.2 Mio. sentence pairs) were available. Since
model training and tuning with such large cor-
pora takes a very long time, we extracted about
2 Mio. sentence pairs of both of these corpora. We
filter sentences with the following properties:
95
? Only sentences of minimum length of 4 to-
kens were considered.
? At least 92% of the vocabulary of each sen-
tence occur in the development set.
? The ratio of the vocabulary size of a sen-
tence and the number of its tokens is mini-
mum 80%.
4.2 Morpho-Syntactic Analysis
German, as a flexible and morphologically rich
language, raises a couple of problems in machine
translation. We picked two major problems and
tackled them with morpho-syntactic pre- and post-
processing: compound splitting and long-range
verb reordering.
For the translation from German into English,
German compound words were split using the
frequency-based method described in (Koehn and
Knight, 2003). Thereby, we forbid certain words
and syllables to be split. For the other trans-
lation direction, the English text was first trans-
lated into the modified German language with
split compounds. The generated output was then
postprocessed by re-merging the previously gen-
erated components using the method described in
(Popovic? et al, 2006).
Additionally, for the German?English phrase-
based system, the long-range POS-based reorder-
ing rules described in (Popovic? and Ney, 2006)
were applied on the training and test corpora as a
preprocessing step. Thereby, German verbs which
occur at the end of a clause, like infinitives and
past participles, are moved towards the beginning
of that clause. With this, we improved our baseline
phrase-based system by 0.6% BLEU.
5 Experimental Results
For all translation directions, we used the provided
parallel corpora (Europarl, news) to train the trans-
lation models and the monolingual corpora to train
BLEU
Dev Test
phrase-based baseline 19.9 19.2
phrase-based (+POS+mero+giga) 21.0 20.3
hierarchical baseline 20.2 19.6
hierarchical (+giga) 20.5 20.1
system combination 21.4 20.4
Table 4: Results for the German?English task.
the language models. We improved the French-
English systems by enriching the data with parts of
the large addional data, extracted with the method
described in Section 4.1. Depending on the sys-
tem this gave an improvement of 0.2-0.7% BLEU.
We also made use of the large giga-news as well
as the LDC Gigaword corpora for the French and
English language models. All systems were opti-
mized for BLEU score on the development data,
newstest2008. The newstest2009 data is
used as a blind test set.
In the following, we will give the BLEU scores
for all language tasks of the baseline system and
the best setup for both, the phrase-based and the
hierarchical system. We will use the following
notations to indicate the several methods we used:
(+POS) POS-based verb reordering
(+mero) maximum entropy reordering
(+giga) including giga-news and
LDC Gigaword in LM
(fa) trained by forced alignment
(shallow) allow only shallow rules
We applied system combination of up to 6 sys-
tems with several setups. The submitted systems
are marked in tables 4-7.
6 Conclusion
For the participation in the WMT 2010 shared
translation task, RWTH used state-of-the-art
phrase-based and hierarchical translation systems.
To deal with the rich morphology and word or-
der differences in German, compound splitting
and long range verb reordering were applied in a
preprocessing step. For the French-English lan-
guage pairs, RWTH extracted parts of the large
news corpus and the UN corpus as additional
training data. Further, training the phrase trans-
lation model with forced alignment yielded im-
provements in BLEU. To obtain the final hypothe-
sis for the French?English and German?English
BLEU
Dev Test
phrase-based baseline 14.8 14.5
phrase-based (+mero) 15.0 14.7
hierarchical baseline 14.2 13.9
hierarchical (shallow) 14.5 14.3
Table 5: Results for the English?German task.
96
BLEU
Dev Test
phrase-based baseline 21.8 25.1
phrase-based (fa+giga) 23.0 26.1
hierarchical baseline 21.9 25.0
hierarchical (shallow+giga) 22.7 25.6
system combination 23.1 26.1
Table 6: Results for the French?English task.
BLEU
Dev Test
phrase-based baseline 20.9 23.2
phrase-based (fa+mero+giga) 23.0 24.6
hierarchical baseline 20.6 22.5
hierarchical (shallow,+giga) 22.4 24.3
Table 7: Results for the English?French task.
language pairs, RWTH applied system combina-
tion. Altogether, by application of these meth-
ods RWTH was able to increase performance in
BLEU by 0.8% for German?English, 0.2% for
English?German, 1.0% for French?English and
1.4% for English?French on the test set over the
respective baseline systems.
Acknowledgments
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Sur-
face Heuristics. In Proceedings of the Workshop on
Statistical Machine Translation, pages 31?38.
J.G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-
Ferrer. 2008. Triplet Lexicon Models for Statisti-
cal Machine Translation. In Proceedings of Emperi-
cal Methods of Natural Language Processing, pages
372?381.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extend-
ing Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 210?217.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278?1283.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
H. Schwenk and J. Senellart. 2009. Translation Model
Adaptation for an Arabic/French News Translation
System by Lightly-Supervised Training. In MT
Summit XII.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. To ap-
pear.
R. Zens and H. Ney. 2006. Discriminative Reorder-
ing Models for Statistical Machine Translation. In
Workshop on Statistical Machine Translation, pages
55?63.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statis-
tical Machine Translation. In International Work-
shop on Spoken Language Translation.
97
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 262?270,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Jane: Open Source Hierarchical Translation, Extended with Reordering
and Lexicon Models
David Vilar, Daniel Stein, Matthias Huck and Hermann Ney
Lehrstuhl fu?r Informatik 6
RWTH Aachen University
Aachen, Germany
{vilar,stein,huck,ney}@cs.rwth-aachen.de
Abstract
We present Jane, RWTH?s hierarchical
phrase-based translation system, which
has been open sourced for the scientific
community. This system has been in de-
velopment at RWTH for the last two years
and has been successfully applied in dif-
ferent machine translation evaluations. It
includes extensions to the hierarchical ap-
proach developed by RWTH as well as
other research institutions. In this paper
we give an overview of its main features.
We also introduce a novel reordering
model for the hierarchical phrase-based
approach which further enhances transla-
tion performance, and analyze the effect
some recent extended lexicon models have
on the performance of the system.
1 Introduction
We present a new open source toolkit for hi-
erarchical phrase-based translation, as described
in (Chiang, 2007). The hierarchical phrase model
is an extension of the standard phrase model,
where the phrases are allowed to have ?gaps?. In
this way, long-distance dependencies and reorder-
ings can be modelled in a consistent way. As in
nearly all current statistical approaches to machine
translation, this model is embedded in a log-linear
model combination.
RWTH has been developing this tool during
the last two years and it was used success-
fully in numerous machine translation evalua-
tions. It is developed in C++ with special at-
tention to clean code, extensibility and efficiency.
The toolkit is available under an open source
non-commercial license and downloadable from
http://www.hltpr.rwth-aachen.de/jane.
In this paper we give an overview of the main
features of the toolkit and introduce two new ex-
tensions to the hierarchical model. The first one
is an additional reordering model inspired by the
reordering widely used in phrase-based transla-
tion systems and the second one comprises two
extended lexicon models which further improve
translation performance.
2 Related Work
Jane implements many features presented in pre-
vious work developed both at RWTH and other
groups. As we go over the features of the system
we will provide the corresponding references.
Jane is not the first system of its kind, al-
though it provides some unique features. There
are other open source hierarchical decoders avail-
able. These include
? SAMT (Zollmann and Venugopal, 2006):
The original version is not maintained any
more and we had problems working on big
corpora. A new version which requires
Hadoop has just been released, however the
documentation is still missing.
? Joshua (Li et al, 2009): A decoder written
in Java by the John Hopkins University. This
project is the most similar to our own, how-
ever both were developed independently and
each one has some unique features. A brief
comparison between these two systems is in-
cluded in Section 5.1.
? Moses (Koehn et al, 2007): The de-facto
standard phrase-based translation decoder
has now been extended to support hierarchi-
cal translation. This is still in an experimental
branch, however.
3 Features
In this section we will only give a brief overview
of the features implemented in Jane. For de-
tailed explanation of previously published algo-
262
rithms and methods, we refer to the given litera-
ture.
3.1 Search Algorithms
The search for the best translation proceeds in two
steps. First, a monolingual parsing of the input
sentence is carried out using the CYK+ algorithm
(Chappelier and Rajman, 1998), a generalization
of the CYK algorithm which relaxes the require-
ment for the grammar to be in Chomsky normal
form. From the CYK+ chart we extract a hyper-
graph representing the parsing space.
In a second step the translations are generated,
computing the language model scores in an inte-
grated fashion. Both the cube pruning and cube
growing algorithms (Huang and Chiang, 2007) are
implemented. For the latter case, the extensions
concerning the language model heuristics similar
to (Vilar and Ney, 2009) have also been included.
3.2 Language Models
Jane supports four formats for n-gram language
models:
? The ARPA format for language models. We
use the SRI toolkit (Stolcke, 2002) to support
this format.
? The binary language model format supported
by the SRI toolkit. This format allows for a
more efficient language model storage, which
reduces loading times. In order to reduce
memory consumption, the language model
can be reloaded for every sentence, filtering
the n-grams that will be needed for scoring
the possible translations. This format is spe-
cially useful for this case.
? Randomized LMs as described in (Talbot and
Osborne, 2007), using the open source im-
plementation made available by the authors
of the paper. This approach uses a space ef-
ficient but approximate representation of the
set of n-grams in the language model. In
particular the probability for unseen n-grams
may be overestimated.
? An in-house, exact representation format
with on-demand loading of n-grams, using
the internal prefix-tree implementation which
is also used for phrase storage (see also Sec-
tion 3.9).
Several language models (also of mixed formats)
can be used during search. Their scores are com-
bined in the log-linear framework.
3.3 Syntactic Features
Soft syntactic features comparable to (Vilar et al,
2008) are implemented in the extraction step of
the toolkit. In search, they are considered as ad-
ditional feature functions of the translation rules.
The decoder is able to handle an arbitrary num-
ber of non-terminal symbols. The extraction has
been extended so that the extraction of SAMT-
rules is included (Zollmann and Venugopal, 2006)
but this approach is not fully supported (there
may be empty parses due to the extended num-
ber of non-terminals). We instead opted to sup-
port the generalization presented in (Venugopal et
al., 2009), where the information about the new
non-terminals is included as an additional feature
in the log-linear model.
In addition, dependency information in the
spirit of (Shen et al, 2008) is included. Jane fea-
tures models for string-to-dependency language
models and computes various scores based on the
well-formedness of the resulting dependency tree.
Jane supports the Stanford parsing format,1 but
can be easily extended to other parsers.
3.4 Additional Reordering Models
In the standard formulation of the hierarchical
phrase-based translation model two additional
rules are added:
S ? ?S?0X?1, S?0X?1?
S ? ?X?0, X?0?
(1)
This allows for a monotonic concatenation of
phrases, very much in the way monotonic phrase-
based translation is carried out.
It is a well-known fact that for phrase-based
translation, the use of additional reordering mod-
els is a key component, essential for achieving
good translation quality. In the hierarchical model,
the reordering is already integrated in the transla-
tion formalism, but there are still cases where the
required reorderings are not captured by the hier-
archical phrases alone.
The flexibility of the grammar formalism allows
us to add additional reordering models without the
need to explicitely modify the code for supporting
them. The most straightforward example would
1http://nlp.stanford.edu/software/lex-parser.shtml
263
be to include the ITG-Reorderings (Wu, 1997), by
adding following rule
S ? ?S?0S?1, S?1S?0? (2)
We can also model other reordering constraints.
As an example, phrase-level IBM reordering con-
straints with a window length of 1 can be included
substituting the rules in Equation (1) with follow-
ing rules
S ? ?M?0,M?0?
S ? ?M?0S?1,M?0S?1?
S ? ?B?0M?1,M?1B?0?
M ? ?X?0, X?0?
M ? ?M?0X?1,M?0X?1?
B ? ?X?0, X?0?
B ? ?B?0X?1, X?1B?0?
(3)
In these rules we have added two additional non-
terminals. The M non-terminal denotes a mono-
tonic block and the B non-terminal a back jump.
Actually both of them represent monotonic trans-
lations and the grammar could be simplified by
using only one of them. Separating them allows
for more flexibility, e.g. when restricting the jump
width, where we only have to restrict the maxi-
mum span width of the non-terminal B. These
rules can be generalized for other reordering con-
straints or window lengths.
Additionally distance-based costs can be com-
puted for these reorderings. To the best of our
knowledge, this is the first time such additional
reorderings have been applied to the hierarchical
phrase-based approach.
3.5 Extended Lexicon Models
We enriched Jane with the ability to score hy-
potheses with discriminative and trigger-based
lexicon models that use global source sentence
context and are capable of predicting context-
specific target words. This approach has recently
been shown to improve the translation results of
conventional phrase-based systems. In this sec-
tion, we briefly review the basic aspects of these
extended lexicon models. They are similar to
(Mauser et al, 2009), and we refer there for a more
detailed exposition on the training procedures and
results in conventional phrase-based decoding.
Note that the training for these models is not
distributed together with Jane.
3.5.1 Discriminative Word Lexicon
The first of the two lexicon models is denoted as
discriminative word lexicon (DWL) and acts as a
statistical classifier that decides whether a word
from the target vocabulary should be included in
a translation hypothesis. For that purpose, it con-
siders all the words from the source sentence, but
does not take any position information into ac-
count, i.e. it operates on sets, not on sequences or
even trees. The probability of a word being part
of the target sentence, given a set of source words,
are decomposed into binary features, one for each
source vocabulary entry. These binary features are
combined in a log-linear fashion with correspond-
ing feature weights. The discriminative word lex-
icon is trained independently for each target word
using the L-BFGS (Byrd et al, 1995) algorithm.
For regularization, Gaussian priors are utilized.
DWL model probabilities are computed as
p(e|f) =
?
e?VE
p(e?|f) ?
?
e?e
p(e+|f)
p(e?|f)
(4)
with VE being the target vocabulary, e the set of
target words in a sentence, and f the set of source
words, respectively. Here, the event e+ is used
when the target word e is included in the target
sentence and e? if not. As the left part of the prod-
uct in Equation (4) is constant given a source sen-
tence, it can be dropped, which enables us to score
partial hypotheses during search.
3.5.2 Triplet Lexicon
The second lexicon model we employ in Jane,
the triplet lexicon model, is in many aspects re-
lated to IBM model 1 (Brown et al, 1993), but
extends it with an additional word in the con-
ditioning part of the lexical probabilities. This
introduces a means for an improved representa-
tion of long-range dependencies in the data. Like
IBM model 1, the triplets are trained iteratively
with the Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977). Jane implements
the so-called inverse triplet model p(e|f, f ?).
The triplet lexicon model score t(?) of the ap-
plication of a rule X ? ??, ?? where (?, ?) is
a bilingual phrase pair that may contain symbols
from the non-terminal set is computed as
t(?, ?, fJ0 ) = (5)
?
?
e
log
?
?
2
J ? (J + 1)
?
j
?
j?>j
p(e|fj , fj?)
?
?
264
with e ranging over all terminal symbols in the tar-
get part ? of the rule. The second sum selects all
words from the source sentence fJ0 (including the
empty word that is denoted as f0 here). The third
sum incorporates the rest of the source sentence
right of the first triggering word. The order of
the triggers is not relevant because per definition
p(e|f, f ?) = p(e|f ?, f), i.e. the model is symmet-
ric. Non-terminals in ? have to be skipped when
the rule is scored.
In Jane, we also implemented scoring for a vari-
ant of the triplet lexicon model called the path-
constrained (or path-aligned) triplet model. The
characteristic of path-constrained triplets is that
the first trigger f is restricted to the aligned target
word e. The second trigger f ? is allowed to move
along the whole remaining source sentence. For
the training of the model, we use word alignment
information obtained by GIZA++ (Och and Ney,
2003). To be able to apply the model in search,
Jane has to be run with a phrase table that con-
tains word alignment for each phrase, too, with the
exception of phrases which are composed purely
of non-terminals. Jane?s phrase extraction can op-
tionally supply this information from the training
data.
(Hasan et al, 2008) and (Hasan and Ney, 2009)
employ similar techniques and provide some more
discussion on the path-aligned variant of the
model and other possible restrictions.
3.6 Forced Alignments
Jane has also preliminary support for forced align-
ments between a given source and target sentence.
Given a sentence in the source language and its
translation in the target language, we find the best
way the source sentence can be translated into
the given target sentence, using the available in-
ventory of phrases. This is needed for more ad-
vanced training approaches like the ones presented
in (Blunsom et al, 2008) or (Cmejrek et al, 2009).
As reported in these papers, due to the restrictions
in the phrase extraction process, not all sentences
in the training corpus can be aligned in this way.
3.7 Optimization Methods
Two method based on n-best for minimum error
rate training (MERT) of the parameters of the log-
linear model are included in Jane. The first one
is the procedure described in (Och, 2003), which
has become a standard in the machine translation
community. We use an in-house implementation
of the method.
The second one is the MIRA algorithm, first
applied for machine translation in (Chiang et al,
2009). This algorithm is more adequate when the
number of parameters to optimize is large.
If the Numerical Recipes library (Press et al,
2002) is available, an additional general purpose
optimization tool is also compiled. Using this
tool a single-best optimization procedure based on
the downhill simplex method (Nelder and Mead,
1965) is included. This method, however, can be
considered deprecated in favour of the above men-
tioned methods.
3.8 Parallelized operation
If the Sun Grid Engine2 is available, all operations
of Jane can be parallelized. For the extraction pro-
cess, the corpus is split into chunks (the granular-
ity being user-controlled) which are distributed in
the computer cluster. Count collection, marginal
computation and count normalization all happens
in an automatic and parallel manner.
For the translation process a batch job is started
on a number of computers. A server distributes the
sentences to translate to the computers that have
been made available to the translation job.
The optimization process also benefits from
the parallelized optimization. Additionally, for
the minimum error rate training methods, random
restarts may be performed on different computers
in a parallel fashion.
The same client-server infrastructure used for
parallel translation may also be reused for inter-
active systems. Although no code in this direction
is provided, one would only need to implement a
corresponding frontend which communicates with
the translation server (which may be located on an-
other machine).
3.9 Extensibility
One of the goals when implementing the toolkit
was to make it easy to extend it with new features.
For this, an abstract class was created which we
called secondary model. New models need only to
derive from this class and implement the abstract
methods for data reading and costs computation.
This allows for an encapsulation of the computa-
tions, which can be activated and deactivated on
demand. The models described in Sections 3.3
2http://www.sun.com/software/sge/
265
through 3.5 are implemented in this way. We thus
try to achieve loose coupling in the implementa-
tion.
In addition a flexible prefix tree implementation
with on-demand loading capabilities is included as
part of the code. This class has been used for im-
plementing on-demand loading of phrases in the
spirit of (Zens and Ney, 2007) and the on-demand
n-gram format described in Section 3.2, in addi-
tion to some intermediate steps in the phrase ex-
traction process. The code may also be reused in
other, independent projects.
3.10 Code
The main core of Jane has been implemented in
C++. Our guideline was to write code that was
correct, maintainable and efficient. We tried to
achieve correctness by means of unit tests inte-
grated in the source as well as regression tests. We
also defined a set of coding guidelines, which we
try to enforce in order to have readable and main-
tainable code. Examples include using descriptive
variable names, appending an underscore to pri-
vate members of classes or having each class name
start with an uppercase letter while variable names
start with lowercase letters.
The code is documented at great length using
the doxygen system,3 and the filling up of the
missing parts is an ongoing effort. Every tool
comes with an extensive help functionality, and
the main tools also have their own man pages.
As for efficiency we always try to speed up the
code and reduce memory consumption by imple-
menting better algorithms. We try to avoid ?dark
magic programming methods? and hard to follow
optimizations are only applied in critical parts of
the code. We try to document every such occur-
rence.
4 Experimental Results
In this section we will present some experimental
results obtained using Jane. We will pay special
attention to the performance of the new reordering
and lexicon models presented in this paper. We
will present results on three different large-scale
tasks and language pairs.
Additionally RWTH participated in this year?s
WMT evaluation, where Jane was one of the sub-
mitted systems. We refer to the system description
for supplementary experimental results.
3http://www.doxygen.org
dev test
System BLEU TER BLEU TER
Jane baseline 24.2 59.5 25.4 57.4
+ reordering 25.2 58.2 26.5 56.1
Table 1: Results for Europarl German-English
data. BLEU and TER results are in percentage.
4.1 Europarl Data
The first task is the Europarl as defined in the
Quaero project. The main part of the corpus in
this task consists of the Europarl corpus as used in
the WMT evaluation (Callison-Burch et al, 2009),
with some additional data collected in the scope of
the project.
We tried the reordering approach presented in
Section 3.4 on the German-English language pair.
The results are shown in Table 1. As can be seen
from these results, the additional reorderings ob-
tain nearly 1% improvement both in BLEU and
TER scores. Regrettably for this corpus the ex-
tended lexicon models did not bring any improve-
ments.
Table 2 shows the results for the French-English
language pair of the Europarl task. On this task
the extended lexicon models yield an improve-
ment over the baseline system of 0.9% in BLEU
and 0.9% in TER on the test set.
4.2 NIST Arabic-English
We also show results on the Arabic-English
NIST?08 task, using the NIST?06 set as develop-
ment set. It has been reported in other work that
the hierarchical system is not competitive with a
phrase-based system for this language pair (Birch
et al, 2009). We report the figures of our state-
of-the-art phrase-based system as comparison (de-
noted as PBT).
As can be seen from Table 3, the baseline
Jane system is in fact 0.6% worse in BLEU and
1.0% worse in TER than the baseline PBT sys-
tem. When we include the extended lexicon mod-
els we see that the difference in performance is re-
duced. For Jane the extended lexicon models give
an improvement of up to 1.9% in BLEU and 1.7%
in TER, respectively, bringing the system on par
with the PBT system extended with the same lex-
icon models, and obtaining an even slightly better
BLEU score.
266
dev test
BLEU TER BLEU TER
Baseline 30.0 52.6 31.1 50.0
DWL 30.4 52.2 31.4 49.6
Triplets 30.4 52.0 31.7 49.4
path-constrained Triplets 30.3 52.1 31.6 49.3
DWL + Triplets 30.7 52.0 32.0 49.1
DWL + path-constrained Triplets 30.8 51.7 31.6 49.3
Table 2: Results for the French-English task. BLEU and TER results are in percentage.
dev (MT?06) test (MT?08)
Jane PBT Jane PBT
BLEU TER BLEU TER BLEU TER BLEU TER
Baseline 43.2 50.8 44.1 49.4 44.1 50.1 44.7 49.1
DWL 45.3 48.7 45.1 48.4 45.6 48.4 45.6 48.4
Triplets 44.4 49.1 44.6 49.2 45.3 48.8 44.9 49.0
path-constrained Triplets 44.3 49.4 44.7 49.1 44.9 49.3 45.3 48.7
DWL + Triplets 45.0 48.9 45.1 48.5 45.3 48.6 45.5 48.5
DWL + path-constrained Triplets 45.2 48.8 45.1 48.6 46.0 48.5 45.8 48.3
Table 3: Results for the Arabic-English task. BLEU and TER results are in percentage.
5 Discussion
We feel that the hierarchical phrase-based transla-
tion approach still shares some shortcomings con-
cerning lexical selection with conventional phrase-
based translation. Bilingual lexical context be-
yond the phrase boundaries is barely taken into
account by the base model. In particular, if only
one generic non-terminal is used, the selection of
a sub-phrase that fills the gap of a hierarchical
phrase is not affected by the words composing the
phrase it is embedded in ? except for the language
model score. This shortcoming is one of the issues
syntactically motivated models try to address.
The extended lexicon models analyzed in this
work also try to address this issue. One can con-
sider that they complement the efforts that are be-
ing made on a deep structural level within the hi-
erarchical approach. Though they are trained on
surface forms only, without any syntactic informa-
tion, they still operate at a scope that exceeds the
capability of common feature sets of standard hi-
erarchical phrase-based SMT systems.
As the experiments in Section 4 show, the ef-
fect of these extended lexicon models is more im-
portant for the hierarchical phrase-based approach
than for the phrase-based approach. In our opinion
this is probably mainly due to the higher flexibil-
ity of the hierarchical system, both because of its
intrinsic nature and because of the higher number
of phrases extracted by the system. The scoring
of the phrases is still carried out by simple relative
frequencies, which seem to be insufficient. The
additional lexicon models seem to help in this re-
spect.
5.1 Short Comparison with Joshua
As mentioned in Section 2, Joshua is the most
similar decoder to our own. It was developed in
parallel at the Johns Hopkins University and it is
267
System words/sec
Joshua 11.6
Jane cube prune 15.9
Jane cube grow 60.3
Table 4: Speed comparison Jane vs. Joshua. We
measure the translated words per second.
currently used by a number of groups around the
world.
Jane was started separately and independently.
In their basic working mode, both systems imple-
ment parsing using a synchronous grammar and
include language model information. Each of the
projects then progressed independently, most of
the features described in Section 3 being only
available in Jane.
Efficiency is one of the points where we think
Jane outperforms Joshua. One of the reasons can
well be the fact that it is written in C++ while
Joshua is written in Java. In order to compare run-
ning times we converted a grammar extracted by
Jane to Joshua?s format and adapted the parame-
ters accordingly. To the best of our knowledge we
configured both decoders to perform the same task
(cube pruning, 300-best generation, same pruning
parameters). Except for some minor differences4
the results were equal.
We tried this setup on the IWSLT?08 Arabic to
English translation task. The speed results (mea-
sured in translated words per second) can be seen
in Table 4. Jane operating with cube prune is
nearly 50% faster than Joshua, at the same level
of translation performance. If we switch to cube
grow, the speed difference is even bigger, with
a speedup of nearly 4 times. However this usu-
ally comes with a penalty in BLEU score (nor-
mally under 0.5% BLEU in our experience). This
increased speed can be specially interesting for
applications like interactive machine translation
or online translation services, where the response
time is critical and sometimes even more impor-
tant than a small (and often hardly noticeable) loss
in translation quality.
Another important point concerning efficiency
is the startup time. Thanks to the binary format
described in Section 3.9, there is virtually no delay
4E.g. the OOVs seem to be handled in a slightly different
way, as the placement was sometimes different.
in the loading of the phrase table in Jane.5 In fact
Joshua?s long phrase table loading times were the
main reason the performance measures were done
on a small corpus like IWSLT instead of one of the
large tasks described in Section 4.
We want to make clear that we did not go into
great depth in the workings of Joshua, just stayed
at the basic level described in the manual. This
tool is used also for large-scale evaluations and
hence there certainly are settings for dealing with
these big tasks. Therefore this comparison has to
be taken with a grain of salt.
We also want to stress that we explicitly chose
to leave translation results out of this comparison.
Several different components have great impact
on translation quality, including phrase extraction,
minimum error training and additional parameter
settings of the decoder. As we pointed out we
do not have the expertise in Joshua to perform all
these tasks in an optimal way, and for that reason
we did not include such a comparison. However,
both JHU and RWTH participated in this year?s
WMT evaluation, where the systems, applied by
their respective authors, can be directly compared.
And in no way do we see Joshua and Jane as
?competing? systems. Having different systems
is always enriching, and particularly as system
combination shows great improvements in trans-
lation quality, having several alternative systems
can only be considered a positive situation.
6 Licensing
Jane is distributed under a custom open source
license. This includes free usage for non-
commercial purposes as long as any changes made
to the original software are published under the
terms of the same license. The exact formulation
is available at the download page for Jane.
7 Conclusion
With Jane, we release a state-of-the-art hi-
erarchical toolkit to the scientific community
and hope to provide a good starting point for
fellow researchers, allowing them to have a
solid system even if the research field is new
to them. It is available for download from
http://www.hltpr.rwth-aachen.de/jane. The
system in its current state is stable and efficient
enough to handle even large-scale tasks such as
5There is, however, still some delay when loading the lan-
guage model for some of the supported formats.
268
the WMT and NIST evaluations, while producing
highly competitive results.
Moreover, we presented additional reordering
and lexicon models that further enhance the per-
formance of the system.
And in case you are wondering, Jane is Just an
Acronym, Nothing Else. The name comes from
the character in the Ender?s Game series (Card,
1986).
Acknowledgments
Special thanks to the people who have contributed
code to Jane: Markus Freitag, Stephan Peitz, Car-
men Heger, Arne Mauser and Niklas Hoppe.
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation, and also partly based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. HR001-06-C-0023. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the DARPA.
References
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A Quantitative Analysis of Reordering Phe-
nomena. In Proc. of the Workshop on Statistical Ma-
chine Translation, pages 197?205, Athens, Greece,
March.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A Discriminative Latent Variable Model for Statis-
tical Machine Translation. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 200?208, Columbus, Ohio,
June.
Peter F. Brown, Stephan A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A Limited Memory Algorithm
for Bound Constrained Optimization. SIAM Journal
on Scientific Computing, 16(5):1190?1208.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation, pages 1?28, Athens, Greece, March.
Orson Scott Card. 1986. Speaker for the Dead. Tor
Books.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133?137,
Paris, France, April.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new Features for Statistical Machine Trans-
lation. In Proc. of the Human Language Technology
Conference / North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 218?226, Boulder, Colorado, June.
David Chiang. 2007. Hierarchical Phrase-based
Translation. Computational Linguistics, 33(2):201?
228, June.
Martin Cmejrek, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG Rules Directly From Efficient
Bilingual Chart Parsing. In Proc. of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 136?143, Tokyo, Japan.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the Royal
Statistical Society Series B, 39(1):1?22.
Sas?a Hasan and Hermann Ney. 2009. Comparison of
Extended Lexicon Models in Search and Rescoring
for SMT. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume short papers, pages 17?20, Boulder, CO, USA,
June.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet Lexicon Mod-
els for Statistical Machine Translation. In Proc. of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP), pages 372?381.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
144?151, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 177?180,
Prague, Czech Republic, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An Open Source Toolkit for Parsing-Based Machine
Translation. In Proc. of the Workshop on Statisti-
cal Machine Translation, pages 135?139, Athens,
Greece, March.
269
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conference on Empirical Methods
for Natural Language Processing (EMNLP), pages
210?218, Singapore, August.
John A. Nelder and Roger Mead. 1965. The Downhill
Simplex Method. Computer Journal, 7:308.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
Recipes in C++. Cambridge University Press, Cam-
bridge, UK.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proc. of the Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio, June.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 3, pages 901?904, Denver, Col-
orado, September.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom Filter Language Models: Tera-scale LMs on
the Cheap. In Proc. of the Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 468?476, Prague, Czech
Republic, June.
Ashish Venugopal, Andreas Zollmann, N.A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Human
Language Technology Conference / North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT-NAACL), pages 236?244, Boulder,
Colorado, June.
David Vilar and Hermann Ney. 2009. On LM Heuris-
tics for the Cube Growing Algorithm. In Proc. of
the Annual Conference of the European Association
for Machine Translation (EAMT), pages 242?249,
Barcelona, Spain, May.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing Soft Syntax Features and Heuristics for
Hierarchical Phrase Based Machine Translation.
In Proc. of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 190?197,
Waikiki, Hawaii, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2007. Efficient
Phrase-Table Representation for Machine Transla-
tion with Applications to Online MT and Speech
Translation. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 492?499, Rochester, New York, April.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax Augmented Machine Translation via Chart Pars-
ing. In Proc. of the Human Language Technology
Conference / North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pages 138?141, New York, June.
270
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 315?320,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The RWTH System Combination System for WMT 2010
Gregor Leusch and Hermann Ney
RWTH Aachen University
Aachen, Germany
{leusch,ney}@cs.rwth-aachen.de
Abstract
RWTH participated in the System Combi-
nation task of the Fifth Workshop on Sta-
tistical Machine Translation (WMT 2010).
For 7 of the 8 language pairs, we com-
bine 5 to 13 systems into a single con-
sensus translation, using additional n-best
reranking techniques in two of these lan-
guage pairs. Depending on the language
pair, improvements versus the best sin-
gle system are in the range of +0.5 and
+1.7 on BLEU, and between ?0.4 and
?2.3 on TER. Novel techniques compared
with RWTH?s submission to WMT 2009
include the utilization of n-best reranking
techniques, a consensus true casing ap-
proach, a different tuning algorithm, and
the separate selection of input systems
for CN construction, primary/skeleton hy-
potheses, HypLM, and true casing.
1 Introduction
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al, 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using prior weights
for each system as well as other statistical mod-
els such as a special n-gram language model. In
addition to lattice rescoring, n-best list reranking
techniques can be applied to n best paths of this
lattice. True casing is considered a separate step
in RWTH?s approach, which also takes the input
hypotheses into account.
The pipeline, and consequently the description
of the pipeline given in this paper, is based on our
pipeline for WMT 2009 (Leusch et al, 2009), with
several extensions as described.
2 System Combination Algorithm
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary or skeleton hypothesis. From this, we
create a lattice which we then rescore using sys-
tem prior weights and a language model (LM).
The single best path in this CN then constitutes
the consensus translation; alternatively the n best
paths are generated and reranked using additional
statistical models. The consensus translation is
then true cased and postprocessed.
2.1 Word Alignment
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each of the K source sentences in the
test corpus, we select one of its translations
En, n = 1, . . . ,M, as the primary hypothesis.
Then we align the secondary hypotheses Em(m=
1, . . . ,M ;n 6= m) with En to match the word or-
der in En. Since it is not clear which hypothesis
should be primary, i. e. has the ?best? word order,
we let several or all hypothesis play the role of the
primary translation, and align all pairs of hypothe-
ses (En, Em); n 6= m. In this paper, we denote
the number of possible primary hypotheses by N .
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
315
alignmentGIZA++- Network generation Weighting&RescoringReordering 200-bestlist
Hyp 1
Hyp k... ConsensusTranslation
nbestrescoring(Triplets,LM, ...)
Figure 1: The system combination architecture.
to estimate the alignment model.
The alignment training corpus is created from a
test corpus of effectively N ?(M?1)?K sentences
translated by the involved MT engines. Model pa-
rameters are trained iteratively using the GIZA++
toolkit (Och and Ney, 2003). The training is per-
formed in the directions Em ? En and En ?
Em. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix, we determine M?1 monotone one-to-one
alignments between En as the primary translation
and Em,m = 1, . . . ,M ;m 6= n. We then con-
struct the confusion network.
We consider words without a correspondence to
the primary translation (and vice versa) to have a
null alignment with the empty word ?, which will
be transformed to an ?-arc in the corresponding
confusion network.
The M?1 monotone one-to-one alignments can
then be transformed into a confusion network, as
described by Matusov et al (2008).
2.3 Voting in the Confusion Network
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for N possible hypothe-
ses as primary, and unite them into a single lattice.
In our experience, this approach is advantageous
in terms of translation quality compared to a min-
imum Bayes risk primary (Rosti et al, 2007).
Weighted majority voting on a single confu-
sion network is straightforward and analogous to
ROVER (Fiscus, 1997). We sum up the probabil-
ities of the arcs which are labeled with the same
word and have the same start state and the same
end state. This can also be regarded as having a
binary system feature in a log-linear model.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we take the system hypotheses for the
same test corpus for which the consensus transla-
tions are to be produced. Using this ?adapted? LM
for lattice rescoring thus gives bonus to n-grams
from the original system hypotheses, in most cases
from the original phrases. Presumably, many of
these phrases have a correct word order. Previous
experimental results show that using this LM in
rescoring together with a word penalty notably im-
proves translation quality. This even results in bet-
ter translations than using a ?classical? LM trained
on a monolingual training corpus. We attribute
this to the fact that most of the systems we com-
bine already include such general LMs.
2.5 Extracting Consensus Translations
To generate our consensus translation, we extract
the single-best path from the rescored lattice, us-
ing ?classical? decoding as in MT. Alternatively,
we can extract the n best paths for n-best list
rescoring.
2.6 n-best-List Reranking
If n-best lists were generated in the previous steps,
additional sentence-based features can be calcu-
lated on these sentences, and combined in a log-
linear way. These scores can then be used to re-
rank the sentences.
For the WMT 2010 FR?EN and the DE?EN
task, we generated 200-best lists, and calculated
the following features:
1. Total score from the lattice rescoring
2. NGram posterior weights on those (Zens and
Ney, 2006)
3. Word Penalty
4. HypLM trained on a different set of hypothe-
ses (FR?EN only)
5. Large fourgram model trained on Gigaword
(DE?EN) or Europarl (FR?EN)
6. IBM1 scores and deletion counts based on a
word lexicon trained on WMT training data
316
7. Discriminative word lexicon score (Mauser et
al., 2009)
8. Triplet lexicon score (Hasan et al, 2008)
Other features were also calculated, but did not
seem to give an improvement on the DEV set.
2.7 Consensus True Casing
Previous approaches to achieve true cased output
in system combination operated on true-cased lat-
tices, used a separate input-independent true caser,
or used a general true-cased LM to differenti-
ate between alternative arcs in the lattice, as in
(Leusch et al, 2009). For WMT 2010, we use
per-sentence information from the input systems
to determine the consensus case of each output
word. Lattice generation, rescoring, and rerank-
ing are performed on lower-cased input, with a
lower-cased consensus hypothesis as their result.
For each word in this hypothesis, we count how
often each casing variant occurs in the input hy-
potheses for this sentence. We then use the vari-
ant with the highest support for the final consen-
sus output. One advantage is that the set of sys-
tems used to determine the consensus case does
not have to be identical to those used for building
the lattice: Assuming that each word from the con-
sensus hypothesis also occurs in one or several of
the true casing input hypotheses, we can focus on
systems that show a good true casing performance.
3 Tuning
3.1 Tuning Weights for Lattice and n-best
Rescoring
For lattice rescoring, we need to tune system
weights, LM factor, and word penalty to produce
good consensus translations. The same holds for
the log-linear weights in n-best reranking.
For the WMT 2010 Workshop, we selected
a linear combination of BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) as optimiza-
tion criterion, ?? := argmax? {BLEU ? TER},
based on previous experience (Mauser et al,
2008). For more stable results, we use the case-
insensitive variants for both measures, despite the
explicit use of case information in the pipeline.
System weights were tuned to this criterion us-
ing the Downhill Simplex method. Because we
considered the number of segments in the tuning
set to be too small to allow for a further split into
an actual tuning and a control (dev) part, we went
for a method closely related to 5-fold cross valida-
tion: We randomly split the tuning set into 5 equal-
sized parts, and tune parameters on four fifth of
the set, measuring progress on the remaining fifth.
This was repeated for the other four choices for the
?dev? part. Only settings which reliably showed
progress on these five different versions were used
later on the test set. For the actual weights and
numerical parameters to be used on the test set,
we calculate the median of the five variants, which
lowered the risk of outliers and overfitting.
3.2 System Selection
With the large numbers of input systems ? e.g., 17
for DE?EN ? and their large spread in translation
quality ? e.g. 10% abs. in BLEU ? not all sys-
tems should participate in the system combination
process. For the generation of lattices, we con-
sidered several variants of systems, often starting
from the top, and either replacing some of the sys-
tems very similar to others with systems further
down the list, or not considering those as primary,
adding further systems as additional secondaries.
For true casing, and the additional HypLM for
FR?EN, we selected a set of 8 to 12 promising
systems, and ran an exhaustive search on all com-
binations of those to optimize the LM perplexity
on the dev set (LM) or the true case BLEU/TER
score on a consensus translation (TC). Further re-
search may include a weighted combination here,
followed by an optimization of the weights as de-
scribed in the previous paragraph.
4 Experimental Results
Each language pair and each direction in
WMT 2010 had its own set of systems, so we se-
lected and tuned for each direction separately. Af-
ter submission of our system combination output
to WMT 2010, we also calculated scores on the
test set (TEST), to validate our results, and as a
preparation for this report. Note that the scores re-
ported for DEV are calculated on the full DEV set,
but not on any combination of the one-fifth ?cross
validation? subcorpora.
4.1 FR?EN and EN?FR
For French?English, we selected a set of eight
systems for the primary submission, and eleven
systems for the contrastive system, of which six
served as skeleton. Six different systems were
used for an additional HypLM, five for consen-
sus true casing. Table 1 shows the distribution of
these systems. We see the results of system com-
bination on DEV and TEST (the latter calculated
after submission) in Table 2. System combination
itself turns out to have the largest improvement,
+0.5 in BLEU and -0.7 in TER on TEST over the
best single system. n-best reranking improves this
result even more, by +0.3/-0.3. The influence of
tuning and of TC selection is measurable on DEV,
but rather small on TEST.
For English?French, 13 systems were used to
construct the lattice, 5 serving as skeleton. Five
different systems were used for true casing. No
n-best list reranking was performed here, as pre-
liminary experiments did not show any significant
317
Table 1: Overview of systems used for FR/EN.
System FR?EN EN?FR
A B A B
cambridge P L C p P p
cu-zeman S
cmu-statxfer L s
dfki S
eu S
geneva S
huicong s
jhu P L p S p
koc S
lig s
limsi P C p S C p
lium P L C s P C p
nrc P C s S p
rali P L p P C p
rwth P p P C p
uedin P L C p P C p
?A? is the primary, ?B? the contrastive submission.
?P? denotes a system that served as skeleton.
?S? a system that was only aligned to others.
?L? denotes a system used for a larger HypLM-n-best-
rescoring.
?C? is a system used for consensus true casing.
Table 2: Results for FR?EN.
TUNE TEST
BLEU TER BLEU TER
Best single 27.9 55.4 28.5 54.0
Lattice SC 28.4 55.0 29.0 53.3
+ tuning 28.8 54.5 29.1 53.3
+ CV tuning 28.6 54.7 29.1 53.3
+ nbest rerank. 29.0 54.4 29.4 53.0
+ sel. for TC 29.1 54.3 29.3 53.0
Contrast. SC 28.9 54.3 28.8 53.4
?SC? stands for System Combination output.
?CV? denotes the split into five different tuning and valida-
tion parts.
?sel. TC? is the separate selection for consensus true casing.
Systems in bold were submitted for WMT 2010.
Table 3: Results for EN?FR.
TUNE TEST
BLEU TER BLEU TER
Best single 27.1 55.7 26.5 56.1
Primary SC 28.3 55.2 28.2 54.7
Contrast. SC 28.5 54.7 28.1 54.6
Table 4: Overview of systems used for DE/EN.
System DE?EN EN?DE
A B A B
cu-zeman S
cmu C P
dfki S p
fbk P C p P
jhu p
kit P C p P C p
koc S C p
limsi P p P C p
liu C S C p
rwth P p P C p
sfu S
uedin P C p P C p
umd P p
uppsala p S
For abbreviations see Table 1.
Table 5: Results for DE?EN.
TUNE TEST
BLEU TER BLEU TER
Best single 23.8 59.7 23.5 59.7
Lattice SC 24.7 58.5 25.0 57.9
+ tuning 25.1 57.6 25.0 57.6
+ CV tuning 24.8 58.0 24.9 57.8
+ nbest rerank. 25.3 57.6 24.9 57.6
+ sel. for TC 25.5 57.5 24.9 57.6
Contrast. SC 25.2 57.7 24.8 57.7
For abbreviations see Table 2.
gain in this direction. As a contrastive submission,
we submitted the consensus of 8 systems. These
are also listed in Table 1. The results can be found
in Table 3. Note that the contrastive system was
not tuned using the ?cross validation? approach;
as a result, we expected it to be sensitive to over-
fitting. We see improvements around +1.7/-1.4 on
TEST.
4.2 DE?EN and EN?DE
In the German?English language pair, 17 systems
were available, but incorporating only six of them
turned out to deliver optimal results on DEV. As
shown in Table 4, we used a combination of seven
systems in the contrastive submission. While a
Table 6: Results for EN?DE.
TUNE TEST
BLEU TER BLEU TER
Best single 16.1 66.3 16.4 65.7
Primary SC 16.4 64.9 17.0 63.7
Contrast. SC 16.4 64.9 17.3 63.4
318
Table 7: Overview of systems used for CZ/EN.
System CZ?EN EN?CZ
aalto P
cmu P C
cu-bojar P P
cu-tecto S
cu-zeman P S C
dcu P
eurotrans S
google P C P C
koc P C
pc-trans S
potsdam P C
sfu S
uedin P C P C
For abbreviations see Table 1.
No contrastive systems were built for this language pair.
Table 8: Results for CZ?EN and EN?CZ.
TUNE TEST
BLEU TER BLEU TER
CZ?EN
Best single 21.8 58.4 22.9 57.5
Primary SC 22.4 59.1 23.4 57.9
EN?CZ
Best single 17.0 67.1 16.6 66.4
Primary SC 16.7 65.4 17.4 63.6
different set of five systems was used for consen-
sus true casing, it turned out that using the same
six systems for the ?additional? HypLM as for
the lattice seemed to be optimal in our approach.
Table 5 shows the outcome of our experiments:
Again, we see that the largest effect on TEST re-
sults from system combination as such (+1.5/-1.8).
The other steps, in particular tuning and selection
for TC, seem to help on DEV, but make hardly
a difference on TEST. n-best reranking brings an
improvement of -0.2 in TER, but at a minor dete-
rioration (-0.1) in BLEU.
In the opposite direction, English?German, we
combined all twelve systems, five of them serv-
ing as skeleton. The contrastive submission con-
sists of a combination of eight systems. Six sys-
tems were used for true casing. Again, n-best
list rescoring did not result in any improvement
in preliminary experiments, and was skipped. Re-
sults are shown in Table 6: We see that even
though both versions perform equally well on
DEV (+0.4/-1.4), the contrastive system performs
better by +0.3/-0.3 on TEST (+0.9/-2.3).
4.3 CZ?EN and EN?CZ
In both directions involving Czech, the number of
systems was rather limited, so no additional se-
Table 9: Overview of systems used for ES/EN.
System EN?ES
A B
cambridge P C p
dcu P p
dfki P C p
jhu P C p
sfu P C p
uedin P C p
upv p
upv-nnlm P p
Table 10: Results for EN?ES.
TUNE TEST
BLEU TER BLEU TER
ES?EN
Best single 28.7 53.6 ? ?
SC 29.0 53.3 ? ?
EN?ES
Best single 27.8 55.2 28.7 54.0
Primary SC 29.5 52.9 30.0 51.4
Contrast. SC 29.6 52.8 30.1 51.7
lection turned out to be necessary, and we did not
build a contrastive system. For Czech?English, all
six systems were used; three of them for true cas-
ing. For English?Czech, all eleven systems were
used in building the lattice, six of them also as
skeleton. Five systems were used in the true cas-
ing step. Table 7 lists these systems. From the
results in Table 8, we see that for CZ?EN, system
combination gains around +0.5 in BLEU, but at
costs of +0.4 to +0.7 in TER. For EN?CZ, the re-
sults look more positive: While we see only -0.3/-
1.7 on DEV, there is a significant improvement of
+1.2/-2.8 on TEST.
4.4 ES?EN and EN?ES
In the Spanish?English language pair, we did not
see any improvement at all on the direction with
English as target in preliminary experiments. Con-
sequently, and given the time constraints, we did
not further investigate on this language pair. Post-
eval experiments revealed that improvements of
+0.3/-0.3 are possible, with far off-center weights
favoring the top three systems.
On English?Spanish, where these preliminary
experiments showed a gain, we used seven out of
the available ten systems in building the lattice
for the primary system, eight for the contrastive.
Five of those were uses for consensus true cas-
ing. Table 9 lists these systems. Table 10 shows
the results on this language pair: For both the pri-
mary and the contrastive systems we see improve-
319
ments of around +1.7/-2.3 on DEV, and +1.3/-2.6
on TEST. Except for the TER on TEST, these two
submissions differ only by ?0.1 from each other.
5 Conclusions
We have shown that our system combination sys-
tem can lead to significant improvements over sin-
gle best MT output where a significant number of
comparably good translations is available on a sin-
gle language pair. n-best reranking can further
improve the quality of the consensus translation;
results vary though. While consensus true casing
turned out to be very useful despite of its simplic-
ity, we were unable to find significant improve-
ments on TEST from the selection of a separate
set of true casing input systems.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andre?s-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Conference on Empirical Methods in
Natural Language Processing, pages 372?381, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
G. Leusch, E. Matusov, and H. Ney. 2009. The
RWTH system combination system for WMT 2009.
In Fourth Workshop on Statistical Machine Transla-
tion, pages 56?60, Athens, Greece, March. Associa-
tion for Computational Linguistics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending sta-
tistical machine translation with discriminative and
trigger-based lexicon models. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 210?217, Singapore, August.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312?319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
R. Zens and H. Ney. 2006. N-gram posterior prob-
abilities for statistical machine translation. In Hu-
man Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics
Annual Meeting (HLT-NAACL), Workshop on Statis-
tical Machine Translation, pages 72?77, New York
City, June.
320
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 152?158,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The RWTH System Combination System for WMT 2011
Gregor Leusch, Markus Freitag, and Hermann Ney
RWTH Aachen University
Aachen, Germany
{leusch,freitag,ney}@cs.rwth-aachen.de
Abstract
RWTH participated in the System Combi-
nation task of the Sixth Workshop on Sta-
tistical Machine Translation (WMT 2011).
For three language pairs, we combined
6 to 14 systems into a single consen-
sus translation. A three-level meta-
combination scheme combining six dif-
ferent system combination setups with
three different engines was applied on the
French?English language pair. Depend-
ing on the language pair, improvements
versus the best single system are in the
range of +1.9% and +2.5% abs. on
BLEU, and between ?1.8% and ?2.4%
abs. on TER. Novel techniques compared
with RWTH?s submission to WMT 2010
include two additional system combina-
tion engines, an additional word alignment
technique, meta combination, and addi-
tional optimization techniques.
1 Introduction
RWTH?s main approach to System Combination
(SC) for Machine Translation (MT) is a refined
version of the ROVER approach in Automatic
Speech Recognition (ASR) (Fiscus, 1997), with
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al (2006). Several improvements have
been added later (Matusov et al, 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. In contrast to existing ap-
proaches (Jayaraman and Lavie, 2005; Rosti et
al., 2007b), the context of the whole corpus rather
than a single sentence is considered in this itera-
tive, unsupervised procedure, yielding a more reli-
able alignment. Majority voting on the generated
lattice is performed using prior weights for each
system as well as other statistical models such
as a special n-gram language model. True cas-
ing is considered a separate step in RWTH?s ap-
proach, which also takes the input hypotheses into
account. The pipeline, and consequently the de-
scription of the main pipeline given in this paper, is
based on our pipeline for WMT 2010 (Leusch and
Ney, 2010), with extensions as described. When
necessary, we denote this pipeline as Align-to-
Lattice, or A2L .
For the French?English task, we used two ad-
ditional system combination engines for the first
time: The first one uses the same alignments as
A2L, but generates lattices in the OpenFST frame-
work (Allauzen et al, 2007). The OpenFST de-
coder (fstshortestpath) is then used to find
the best path (consensus translation) in this lattice.
Analogously, we call this engine A2FST . The sec-
ond additional engine, which we call SCUNC, uses
a TER-based alignment, similar to the approach by
Rosti et al (2007b). Instead of a lattice rescor-
ing, finding the consensus translation is consid-
ered a per-node classification problem: For each
slot, which one is the ?correct? one (i.e. will give
the ?best? output)? This approach is inspired by
iROVER (Hillard et al, 2007). Consensus trans-
lations from different settings of these approaches
could then be combined again by an additional ap-
plication of system combination ? which we refer
to as meta combination (Rosti et al, 2007a). These
three approaches are described in more detail in
Section 2. In Section 3 we describe how we tuned
the parameters and decisions of our system combi-
nation approaches for WMT 2011. Section 4 then
lists our experimental setup as well as the experi-
mental results we obtained on the WMT 2011 sys-
tem combination track. We conclude this paper in
Section 5.
2 System Combination Algorithm (A2L)
In this section we present the details of our main
system combination method, A2L. The upper part
of Figure 1 gives an overview of the system combi-
nation architecture described in this section. After
preprocessing the MT hypotheses, pairwise align-
152
ments between the hypotheses are calculated. The
hypotheses are then reordered to match the word
order of a selected primary (skeleton) hypothesis.
From this, we create a confusion network (CN)
which we then rescore using system prior weights
and a language model (LM). The single best path
in this CN then constitutes the consensus transla-
tion. The consensus translation is then true cased
and post processed.
2.1 Word Alignment
The main proposed alignment approach is a statis-
tical one. It takes advantage of multiple transla-
tions for a whole corpus to compute a consensus
translation for each sentence in this corpus. It also
takes advantage of the fact that the sentences to be
aligned are in the same language.
For each of the K source sentences in the test
corpus, we select one of its N translations from
different MT systems E,m=1, . . . , N, as the pri-
mary hypothesis. Then we align the secondary hy-
potheses En(n=1, . . . , ;n 6=m) with En to match
the word order in En. Since it is not clear which
hypothesis should be primary, i. e. has the ?best?
word order, we let several or all hypothesis play
the role of the primary translation, and align all
pairs of hypotheses (En, Em); n 6= m.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
to estimate the alignment model.
The alignment training corpus is created from a
test corpus of effectively N ?(N?1) ?K sentences
translated by the involved MT engines. Model pa-
rameters are trained iteratively using the GIZA++
toolkit (Och and Ney, 2003). The training is per-
formed in the directions Em ? En and En ?
Em. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model.
A different approach that has e.g. been pro-
posed by Rosti et al (2007b) is the utilization of a
TER alignment (Snover et al, 2006) for this pur-
pose. Because the original TER is insensitive to
small changes in spellings, synonyms etc., it has
been proposed to use more complex variants, e.g.
TERp. For our purposes, we utilized ?poor-man?s-
stemming?, i.e. shortening each word to its first
four characters when calculating the TER align-
ment. Since a TER alignment already implies a
reordering between the primary and the secondary
hypothesis, an explicit reordering step is not nec-
essary.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix, we determine N ? 1 monotone one-to-one
alignments between En as the primary translation
and Em,m = 1, . . . , N ;m 6= n. We then con-
struct the confusion network.
We consider words without a correspondence to
the primary translation (and vice versa) to have a
null alignment with the empty word ?, which will
be transformed to an ?-arc in the corresponding
confusion network.
The N?1 monotone one-to-one alignments can
then be transformed into a confusion network, as
described by Matusov et al (2008).
2.3 Voting in the Confusion Network (A2L,
A2FST)
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for N possible hypothe-
ses as primary, and unite them into a single lattice.
In our experience, this approach is advantageous
in terms of translation quality compared to a min-
imum Bayes risk primary (Rosti et al, 2007b).
Weighted majority voting on a single confu-
sion network is straightforward and analogous to
ROVER (Fiscus, 1997). We sum up the probabil-
ities of the arcs which are labeled with the same
word and have the same start state and the same
end state.
Compared to A2L, our new A2FST engine al-
lows for a higher number of features for each arc.
Consequently, we add a binary system feature for
each system in addition to the logarithm of the sum
of system weights, as before. The advantage of
these features is that the weights are linear within
a log-linear model, as opposed to be part of a loga-
rithmic sum. Consequently they can later be opti-
mized using techniques designed for linear feature
weights, such as MERT, or MIRA.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). When regarding
153
alignmentGIZA++-/TER- Network generation Weighting&Rescoring& Reordering
Hyp 1
Hyp k... ConsensusTranslation
CreatingClassificationProblem& Features
Classificationwithin eachslot ConsensusTranslation
A2L, A2FST
SCUNC
ShortestPath
Path of"recognized"arcs
Figure 1: The system combination architecture.
the lattice as a weighted Finite State Transducer
(FST), this can be regarded (and implemented) as
composition with a LM FST.
In our approach, we train a trigram LM on the
outputs of the systems involved in system combi-
nation. For LM training, we take the system hy-
potheses for the same test corpus for which the
consensus translations are to be produced. Using
this ?adapted? LM for lattice rescoring thus gives
bonus to n-grams from the original system hy-
potheses, in most cases from the original phrases.
Presumably, many of these phrases have a correct
word order. Previous experimental results show
that using this LM in rescoring together with a
word penalty notably improves translation quality.
This even results in better translations than using
a ?classical? LM trained on a monolingual train-
ing corpus. We attribute this to the fact that most
of the systems we combine already include such
general LMs. Nevertheless, one of the SC systems
we use for the French?English task (IV in Sec-
tion 4.1) uses a filtered fourgram LM trained on
GigaWord and other constrained training data sets
for this WMT tasks as an additional LM.
2.5 Extracting Consensus Translations
To generate our consensus translation, we ex-
tract the single-best path from the rescored lat-
tice, using ?classical? decoding as in MT. In A2L,
this is implemented as shortest-path decoder on a
pruned lattice. In A2FST, we use the OpenFST
fstshortestpath decoder, which does not re-
quire a pruning step for lattices of the size and den-
sity produced here.
2.6 Classification in the Confusion Network
(SCUNC)
Instead of considering the selection of the con-
sensus problem as a shortest-path problem in a
rescored confusion network, we can treat it instead
as a classification problem: For each slot (set of
outgoing arcs from one node in a CN), we consider
one or more arcs to be ?correct?, and train a clas-
sifier to identify these certain arcs. This is the idea
of the iROVER approach in ASR (Hillard et al,
2007). We call our implementation System Com-
bination Using N-gram Classifiers, or SCUNC.
For the WMT evaluation, we used the ICSI-
Boost framework (Favre et al, 2007) as classifier
(in binary mode, i.e. giving a yes/no-decision for
each single arc). We generated 109 features from
8 families: Pairwise equality of words from dif-
ferent systems, Number of votes for a word, word
that would win a simple majority voting, empty
word (also in previous two arcs), position at be-
ginning or end of sentence, cross-BLEU-S score
of hypothesis, equality of system with system of
last slot, and SRILM uni- to trigram scores. As
this approach requires strict CN instead of lattices,
a union of CNs for different primary hypotheses
was no longer possible. We decided to select
a fixed single primary system; other approaches
would have been to train an additional classifier
for this purpose, or to select a minimum-Bayes-
risk (MBR) skeleton.
2.7 Consensus True Casing
Previous approaches to achieve true cased output
in system combination operated on true-cased lat-
tices, used a separate input-independent true caser,
or used a general true-cased LM to differentiate
between alternative arcs in the lattice, as described
by Leusch et al (2009). For WMT 2011, we use
per-sentence information from the input systems
to determine the consensus case of each output
word. Lattice generation, rescoring, and rerank-
ing are performed on lower-cased input, with a
lower-cased consensus hypothesis as their result.
For each word in this hypothesis, we count how
often each casing variant occurs in the input hy-
potheses for this sentence. We then use the vari-
ant with the highest support for the final consensus
output.
154
Table 1: Corpus and Task statistics.
avg. # words #sys
TUNE DEV TEST
FR?EN 15670 11410 49832 25
DE?EN 15508 10878 49395 24
ES?EN 15989 11234 50612 15
# sent 609 394 2000
3 Tuning
3.1 Feature weights
For lattice rescoring, we selected a linear combi-
nation of BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) as optimization criterion,
?? := argmax? {BLEU ? TER} for the A2L
engine, based on previous experience (Mauser et
al., 2008). To achieve more stable results, we use
the case-insensitive variants for both measures, de-
spite the explicit use of case information in the
pipeline. System weights were tuned to this cri-
terion using the Downhill Simplex method.
In the A2FST setup, we were able to generate
full lattices, with separate costs for each individual
feature on all arcs (Power Semiring). This allowed
us to run Lattice MERT (Macherey et al, 2008)
on the full lattice, with no need for pruning (and
thus additional outer iterations for re-generating
lattices). We tried different strategies ? random
lines vs axis-parallel lines, regularization, random
restarts, etc, and selected the most stable results
on TUNE and DEV for this engine. Optimization
criterion here was BLEU.
3.2 Training a classifier for SCUNC
In MT system combination, even with given refer-
ence translations, there is no simple way to iden-
tify the ?correct? arc in a slot. This renders a
classifier-based approach even more difficult than
iROVER in ASR. The problem is even aggravated
because both the alignment of words, and their or-
der, can be incorrect already in the CN. We thus
consider an arc to be ?correct? within this task ex-
actly if it gives us the best possible total BLEU-S
score.1 These ?correct? arcs, which lie on such an
?oracle path? for BLEU-S, were therefore used as
reference classes when training the classifier.
3.3 System Selection
With the large numbers of input systems ? e.g.,
25 for FR?EN ? and their large spread in transla-
tion quality ? e.g. from 22.2 to 31.4% in BLEU
? not all systems should participate in the system
1We are looking at the sentence level, so we use BLEU-
S (Lin and Och, 2004) instead of BLEU
combination process. This is especially the case
since several of these e.g. 25 systems are often
only small variants of each other (contrastive vs.
primary submissions), which leads to a low vari-
ability of these translations. We considered several
variants of the set of input systems, often starting
from the top, and either replacing some of the sys-
tems very similar to others with systems further
down the list, or not considering those as primary,
adding further systems as additional secondaries.
Depending on the engine we were using, we se-
lected between 6 and 14 different systems as input.
4 Experimental Results
Each language pair in WMT 2011 had its own set
of systems, so we selected and tuned separately for
each language pair . Due to time constraints, we
only participated in tasks with English as the target
language. In preliminary experiments, it turned
out that System Combination was not able to get
a better result than the best single system on the
Czech?English task. Consequently, we focused
on the language pairs French?English, German?
English, and Spanish?English.
We split the available tuning data document-
wise into a 609-line TUNE set (for tuning), and a
394-line DEV set (to verify tuning results). More
statistics on these sets can be found in Table 1.
Unfortunately, late in the evaluation campaign
it turned out that the quality of several reference
sentences used in TUNE and DEV was rather low:
Many reference sentences contained spelling er-
rors, a few dozen lines even contained French
phrases or sentences within or after the English
text. We corrected many of these errors manually
in the references. In total 101 of 690 lines (16.6%)
in TUNE and 58 of 394 lines (14.7%) in DEV
were affected by this. While it was too late to re-
run all of the optimization runs, we re-optimized
at least a few final systems. All scores within this
section were calculated on the corrected reference
translations.
4.1 FR?EN
For French?English, we built in total seven differ-
ent system combination setups to generate a single
consensus translation and two contrastive transla-
tions. Figure 2 shows the structure and the data
flow of our setup for FR?EN. Table 2 lists more
details about the individual engines.
Our primary submission was focused on our ex-
perience that while rule-based MT systems (such
as RBMT-1..5 and systran) tend to have
lower BLEU scores than statistical (SMT) sys-
tems, they usually give considerable improve-
155
aalignm aaalietGIt aZlignAe+-i/T Zlignm ZalignAe+
cmu-denkowskicmu-hannemancu-zemanjhukitlia-liglimsiliumonline-Aonline-Brwth-hucksystranudeinrbmt-1rbmt-2rbmt-3rbmt-4rbmt-5
alignm
Zaalignm
primary contrastive 2contrastive 1
Bold arrows denote a system that is always considered as skeleton.
Note that there are two variants of setup II, see text.
Figure 2: System combination pipelines for FR?EN
Table 2: Engines and input systems for FR?EN.
Engine # Input submitted?
I A2L 6 RBMT
II A2L I + 6 primary
II? A2L fix I + 6 for VII
III SCUNC 6
IV A2FST GW, 8
V A2L 10 contrastive-2
VI A2FST 14
VII A2L II??VI contrastive-1
?GW? means a 4-gram LM trained on GigaWord.
II uses all skeletons, II? uses I as fixed skeleton.
Table 3: Results for FR?EN.
TUNE DEV
BLEU TER BLEU TER
kit 31.56 50.15 30.25 52.88
systran 28.18 53.32 26.50 56.07
I 27.37 54.73 26.72 57.73
II 33.69 48.47 32.45 51.09
II? 33.39 48.77 31.81 51.57
III 32.74 48.06 31.88 50.87
IV 34.16 48.31 31.95 51.64
V 33.17 48.95 32.60 51.14
VI 33.86 48.69 31.56 52.25
VII 34.41 48.20 32.15 51.49
kit is the best single system.
systran is the best single rule-based system.
All scores are case insensitive, and were calculated on the
corrected reference translations.
ments to the latter in a SC setup. Here, though,
the number of such systems was too high to sim-
ply add them to a reasonable set of SMT systems.
Consequently, we first built a SC system (I) com-
bining all RBMT/Systran systems, and then a sec-
ond SC system (II) which combines the output
of I, and 6 SMT systems. As further experi-
ments showed, allowing all hypotheses as primary
(or skeleton) gave significantly better scores than
forcing SC to use the output of I as primary only.
But vice versa, when looking at the meta combi-
nation scheme, VII, using I as primary only (a
setup which we will now denote as II?) gave
measurable improvements in the overall transla-
tion quality. We assume this is due to the similarity
of the output of II with that of the other setups.
Setup III is a SCUNC setup, that is, we built
a single CN for each sentence using poor-man?s-
stemming-TER, with rwth-huck as primary hy-
pothesis. We then generated a large number of fea-
tures for each arc, and trained an ICSIBoost clas-
sifier to recognize the arc (or system) that gave the
best BLEU-S score. This then gave us the consen-
sus translation.
For IV, we built an OpenFST lattice out of eight
systems, and rescored it with both the Hypothe-
sis LM (3-gram), and a 4-gram LM trained on Gi-
gaWord and other WMT constrained training data
for this task. The log-linear weights were trained
using lattice MERT for BLEU. Setup V is a clas-
sical A2L setup, using ten different input systems.
This setup was tuned on BLEU ? TER using the
Downhill-Simplex algorithm. In setup VI, again
the A2FST engine was used, this time using the
Hyp LM only, without an additional LM. Tuning
156
Table 4: Results for DE?EN.
TUNE DEV
BLEU TER BLEU TER
online-B 23.13 60.15 26.20 57.20
Primary 24.57 58.51 28.11 54.83
4 best sys 23.85 58.22 27.47 54.96
6 best sys 24.46 57.74 27.82 54.50
online-B is the best single system.
was also performed using lattice MERT towards
BLEU. And finally, setup VII combines the out-
put of II? to IV using the A2L engine again.
All the results of system combination on TUNE
and DEV are listed in Table 3. It turns out that
with the exception of I, all system combination
approaches were able to achieve a significant im-
provement of at least +1.8% abs. in BLEU com-
pared to the best input system. For I, we need
to keep in mind that all other systems were sev-
eral BLEU points worse than the best one ? a sce-
nario where we can expect system combination,
which is based on the consensus translation after
all, to underperform. We also see that both A2FST
and SCUNC, with their large number of features,
show a tendency to overfitting ? we see large im-
provements on TUNE, but significantly smaller
improvements on DEV. This tendency is, unfortu-
nately, also the case for meta combination: While
we see an additional +0.3% abs. in BLEU over
the best first-level system combination on TUNE,
this improvement does not reflect in the scores on
DEV: While we still see a +0.2% abs. improve-
ment in BLEU over the setup that performed best
on TUNE, there is even a small deterioration of
?0.4% in BLEU over the setup that performed
best on DEV. Because of this effect, we decided to
submit our meta combination output only as first
contrastive, and the output that performed well
both on TUNE and DEV as our primary submis-
sion for WMT. As second contrastive submission,
we selected the setup that performed best on DEV.
4.2 DE?EN
24 systems were available in the German?English
language pair, but incorporating only 7 of them
turned out to deliver optimal results on DEV. We
ran experiments on several settings of systems,
but only in our tried and tested A2L framework.
We settled for a combination of seven systems
(online-B,cmu-dyer,dfki-xu,limsi,
online-A,rwth-wuebker,kit) as primary
submission. Table 4 also lists two different set-
tings. One setting consists of the four best systems
Table 5: Results for ES?EN.
TUNE DEV
BLEU TER BLEU TER
online-A 30.58 51.69 30.77 51.95
Primary 34.29 48.47 33.41 49.71
Contrastive 34.23 48.27 33.30 49.51
online-A is the best single system.
(online-B,cmu-dyer,rwth-wuebker,
kit) and the other setting contains the six best
systems (online-B,cmu-dyer,dfki-xu,
rwth-wuebker,online-A,kit). When we
added more systems to system combination, we
lost performance in both TUNE and DEV.
4.3 ES?EN
For Spanish?English, we tried several settings
of systems. We sticked to our tried and tested
A2L framework. We settled for a combination
of six systems (alacant,koc,online-A,
online-B,rbmt-1,systran) as contrastive
submission, and a combination of ten systems
(+rbmt-2,rbmt-3,rbmt-4,udein) as pri-
mary submission. Table 5 lists the results for this
task. The difference between our primary setup
(10 systems) and our contrastive setup (6 systems)
is rather small, less than 0.1% abs. in BLEU. Nev-
ertheless, we see significant improvements over
the best single system of +2.4% abs. in BLEU,
and ?2.2% in TER.
5 Conclusions
We have shown that our system combination ap-
proach leads to significant improvements over sin-
gle best MT output where a significant number of
comparably good translations is available on a sin-
gle language pair. A meta combination can give
additional improvement, but can be sensitive to
overfitting; so in some cases, using one of its in-
put system combination hypothesis may be a bet-
ter choice. In any way, both of our new engines
have shown that they can compete with our present
approach, so we hope to make good use of the new
possibilities they may offer.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
157
References
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M. Mohri. 2007. OpenFst: A general and efficient
weighted finite-state transducer library. In Proc. of
the Twelfth International Conference on Implemen-
tation and Application of Automata (CIAA), volume
4783 of Lecture Notes in Computer Science, pages
11?23. Springer.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
B. Favre, D. Hakkani-Tu?r, and S. Cuendet. 2007.
Icsiboost. http://code.google.come/p/
icsiboost.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
D. Hillard, B. Hoffmeister, M. Ostendorf, R. Schlu?ter,
and H. Ney. 2007. iROVER: improving sys-
tem combination with classification. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association
for Computational Linguistics; Companion Volume,
Short Papers, NAACL-Short ?07, pages 65?68. As-
sociation for Computational Linguistics.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
G. Leusch and H. Ney. 2010. The rwth system com-
bination system for wmt 2010. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation
and Metrics MATR, pages 315?320, Uppsala, Swe-
den, July.
G. Leusch, E. Matusov, and H. Ney. 2009. The
RWTH system combination system for WMT 2009.
In Fourth Workshop on Statistical Machine Transla-
tion, pages 56?60, Athens, Greece, March. Associa-
tion for Computational Linguistics.
C. Y. Lin and F. J. Och. 2004. Orange: a method for
evaluation automatic evaluation metrics for machine
translation. In Proc. COLING 2004, pages 501?507,
Geneva, Switzerland, August.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based minimum error rate training for
statistical machine translation. In Proc. of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 725?734. Asso-
ciation for Computational Linguistics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas, R. M.
Schwartz, and B. J. Dorr. 2007a. Combining out-
puts from multiple machine translation systems. In
HLT-NAACL?07, pages 228?235.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007b.
Improved word-level system combination for ma-
chine translation. In Proc. of the 45th Annual Meet-
ing of the Association of Computational Linguis-
tics (ACL), pages 312?319, Prague, Czech Republic,
June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
158
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405?412,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2011
Matthias Huck, Joern Wuebker, Christoph Schmidt, Markus Freitag, Stephan Peitz,
Daniel Stein, Arnaud Dagnelies, Saab Mansour, Gregor Leusch and Hermann Ney
RWTH Aachen University
Aachen, Germany
surname@cs.rwth-aachen.de
Abstract
This paper describes the statistical machine
translation (SMT) systems developed by
RWTH Aachen University for the translation
task of the EMNLP 2011 Sixth Workshop on
Statistical Machine Translation. Both phrase-
based and hierarchical SMT systems were
trained for the constrained German-English
and French-English tasks in all directions. Ex-
periments were conducted to compare differ-
ent training data sets, training methods and op-
timization criteria, as well as additional mod-
els on dependency structure and phrase re-
ordering. Further, we applied a system com-
bination technique to create a consensus hy-
pothesis from several different systems.
1 Overview
We sketch the baseline architecture of RWTH?s se-
tups for the WMT 2011 shared translation task by
providing an overview of our translation systems in
Section 2. In addition to the baseline features, we
adopted several novel methods, which will be pre-
sented in Section 3. Details on the respective se-
tups and translation results for the French-English
and German-English language pairs (in both trans-
lation directions) are given in Sections 4 and 5. We
finally conclude the paper in Section 6.
2 Translation Systems
For the WMT 2011 evaluation we utilized RWTH?s
state-of-the-art phrase-based and hierarchical trans-
lation systems as well as our in-house system com-
bination framework. GIZA++ (Och and Ney, 2003)
was employed to train word alignments, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
2.1 Phrase-Based System
We applied a phrase-based translation (PBT) system
similar to the one described in (Zens and Ney, 2008).
Phrase pairs are extracted from a word-aligned bilin-
gual corpus and their translation probability in both
directions is estimated by relative frequencies. The
standard feature set moreover includes an n-gram
language model, phrase-level single-word lexicons
and word-, phrase- and distortion-penalties. To lexi-
calize reordering, a discriminative reordering model
(Zens and Ney, 2006a) is used. Parameters are opti-
mized with the Downhill-Simplex algorithm (Nelder
and Mead, 1965) on the word graph.
2.2 Hierarchical System
For the hierarchical setups described in this paper,
the open source Jane toolkit (Vilar et al, 2010) was
employed. Jane has been developed at RWTH and
implements the hierarchical approach as introduced
by Chiang (2007) with some state-of-the-art exten-
sions. In hierarchical phrase-based translation, a
weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The standard models integrated into
our Jane systems are: phrase translation probabil-
ities and lexical translation probabilities on phrase
level, each for both translation directions, length
405
penalties on word and phrase level, three binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, source-
to-target and target-to-source phrase length ratios,
four binary count features and an n-gram language
model. The model weights are optimized with stan-
dard MERT (Och, 2003) on 100-best lists.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (Matusov et al, 2006;
Matusov et al, 2008). This approach includes an
enhanced alignment and reordering framework. A
lattice is built from the input hypotheses. The trans-
lation with the best score within the lattice according
to a couple of statistical models is selected as con-
sensus translation.
3 Translation Modeling
We incorporated several novel methods into our sys-
tems for the WMT 2011 evaluation. This section
provides a short survey of three of the methods
which we suppose to be of particular interest.
3.1 Language Model Data Selection
For the English and German language models,
we applied the data selection method proposed in
(Moore and Lewis, 2010). Each sentence is scored
by the difference in cross-entropy between a lan-
guage model trained from in-domain data and a lan-
guage model trained from a similar-sized sample of
the out-of-domain data. As in-domain data we used
the news-commentary corpus. The out-of-domain
data from which the data was selected are the news
crawl corpus for both languages and for English the
109 corpus and the LDC Gigaword data. We used a
3-gram trained with the SRI toolkit to compute the
cross-entropy. For the news crawl corpus, only 1/8
of the sentences were discarded. Of the 109 corpus
we retained 1/2 and of the LDC Gigaword data we
retained 1/4 of the sentences to train the language
models.
3.2 Phrase Model Training
For the German?English and French?English
translation tasks we applied a forced alignment pro-
cedure to train the phrase translation model with the
EM algorithm, similar to the one described in (DeN-
ero et al, 2006). Here, the phrase translation prob-
abilities are estimated from their relative frequen-
cies in the phrase-aligned training data. The phrase
alignment is produced by a modified version of the
translation decoder. In addition to providing a statis-
tically well-founded phrase model, this has the ben-
efit of producing smaller phrase tables and thus al-
lowing more rapid experiments. A detailed descrip-
tion of the training procedure is given in (Wuebker
et al, 2010).
3.3 Soft String-to-Dependency
Given a dependency tree of the target language,
we are able to introduce language models that span
over longer distances than the usual n-grams, as in
(Shen et al, 2008). To obtain dependency structures,
we apply the Stanford parser (Klein and Manning,
2003) on the target side of the training material.
RWTH?s open source hierarchical translation toolkit
Jane has been extended to include dependency infor-
mation in the phrase table and to build dependency
trees on the output hypotheses at decoding time from
this information.
Shen et al (2008) use only phrases that meet cer-
tain restrictions. The first possibility is what the au-
thors call a fixed dependency structure. With the
exception of one word within this phrase, called
the head, no outside word may have a dependency
within this phrase. Also, all inner words may only
depend on each other or on the head. For a second
structure, called a floating dependency structure, the
head dependency word may also exist outside the
phrase. If the dependency structure of a phrase con-
forms to these restrictions, it is denoted as valid.
In our phrase table, we mark those phrases that
possess a valid dependency structure with a binary
feature, but all phrases are retained as translation op-
tions. In addition to storing the dependency informa-
tion, we also memorize for all hierarchical phrases
if the content of gaps has been dependent on the left
or on the right side. We utilize the dependency in-
formation during the search process by adding three
406
French English
Sentences 3 710 985
Running Words 98 352 916 87 689 253
Vocabulary 179 548 216 765
Table 1: Corpus statistics of the preprocessed high-
quality training data (Europarl, news-commentary, and
selected parts of the 109 and UN corpora) for the
RWTH systems for the WMT 2011 French?English and
English?French translation tasks. Numerical quantities
are replaced by a single category symbol.
features to the log-linear model: merging errors to
the left, merging errors to the right, and the ratio of
valid vs. non-valid dependency structures. The de-
coder computes the corresponding costs when it tries
to construct a dependency tree of a (partial) hypothe-
sis on-the-fly by merging the dependency structures
of the used phrase pairs.
In an n-best reranking step, we compute depen-
dency language model scores on the dependencies
which were assembled on the hypotheses by the
search procedure. We apply one language model
for left-side dependencies and one for right-side de-
pendencies. For head structures, we also compute
their scores by exploiting a simple unigram language
model. We furthermore include a language count
feature that is incremented each time we compute
a dependency language model score. As trees with
few dependencies have less individual costs to be
computed, they tend to obtain lower overall costs
than trees with more complex structures in other
sentences. The intention behind this feature is thus
comparable to the word penalty in combination with
a normal n-gram language model.
4 French-English Setups
We set up both hierarchical and standard phrase-
based systems for the constrained condition of the
WMT 2011 French?English and English?French
translation tasks. The English?French RWTH pri-
mary submission was produced with a single hierar-
chical system, while a system combination of three
systems was used to generate a final hypothesis for
the French?English primary submission.
Besides the Europarl and news-commentary cor-
pora, the provided parallel data also comprehends
French English
Sentences 29 996 228
Running Words 916 347 538 778 544 843
Vocabulary 1 568 089 1 585 093
Table 2: Corpus statistics of the preprocessed full training
data for the RWTH primary system for the WMT 2011
English?French translation task. Numerical quantities
are replaced by a single category symbol.
the large French-English 109 corpus and the French-
English UN corpus. Since model training with
such a huge amount of data requires a consider-
able computational effort, RWTH decided to select
a high-quality part of altogether about 2 Mio. sen-
tence pairs from the latter two corpora. The selec-
tion of parallel sentences was carried out according
to three criteria: (1) Only sentences of minimum
length of 4 tokens are considered, (2) at least 92%
of the vocabulary of each sentence occurs in new-
stest2008, and (3) the ratio of the vocabulary size
of a sentence and the number of its tokens is mini-
mum 80%. Word alignments in both directions were
trained with GIZA++ and symmetrized according to
the refined method that was proposed in (Och and
Ney, 2003). The phrase tables of the translation
systems are extracted from the Europarl and news-
commentary parallel training data as well as the se-
lected high-quality parts the 109 and UN corpora
only. The only exception is the hierarchical system
used for the English?French RWTH primary sub-
mission which comprehends a second phrase table
with lexical (i.e. non-hierarchical) phrases extracted
from the full parallel data (approximately 30 Mio.
sentence pairs).
Detailed statistics of the high-quality parallel
training data (Europarl, news-commentary, and the
selected parts of the 109 and UN corpora) are given
in Table 1, the corpus statistics of the full parallel
data from which the second phrase table with lexi-
cal phrases for the English?French RWTH primary
system was created are presented in Table 2.
The translation systems use large 4-gram lan-
guage models with modified Kneser-Ney smooth-
ing. The French language model was trained on
most of the provided French data including the
monolingual LDC Gigaword corpora, the English
407
newstest2009 newstest2010
French?English BLEU TER BLEU TER
System combination of ? systems (primary) 26.7 56.0 27.4 54.9
PBT with triplet lexicon, no forced alignment (contrastive) ? 26.2 56.7 27.2 55.3
Jane as below + improved LM (contrastive) 26.3 57.4 26.7 56.2
Jane with parse match + syntactic labels + dependency ? 26.2 57.5 26.5 56.4
PBT with forced alignment phrase training ? 26.0 57.1 26.3 56.0
Table 3: RWTH systems for the WMT 2011 French?English translation task (truecase). BLEU and TER results are
in percentage.
newstest2009 newstest2010
English?French BLEU TER BLEU TER
Jane shallow + in-domain TM + lexical phrases from full data 25.3 60.1 27.1 57.2
Jane shallow + in-domain TM + triplets + DWL + parse match 24.8 60.5 26.6 57.5
PBT with triplets, DWL, sentence-level word lexicon, discrim. reord. 24.8 60.1 26.5 57.3
Table 4: RWTH systems for the WMT 2011 English?French translation task (truecase). BLEU and TER results are
in percentage.
language model was trained on automatically se-
lected English data (cf. Section 3.1) from the pro-
vided resources including the 109 corpus and LDC
Gigaword.
The scaling factors of the log-linear model com-
bination are optimized towards BLEU on new-
stest2009, newstest2010 is used as an unseen test set.
4.1 Experimental Results French?English
The results for the French?English task are given in
Table 3. RWTH?s three submissions ? one primary
and two contrastive ? are labeled accordingly in the
table. The first contrastive submission is a phrase-
based system with a standard feature set plus an ad-
ditional triplet lexicon model (Mauser et al, 2009).
The triplet lexicon model was trained on in-domain
news commentary data only. The second contrastive
submission is a hierarchical Jane system with three
syntax-based extensions: A parse match model (Vi-
lar et al, 2008), soft syntactic labels (Stein et al,
2010), and the soft string-to-dependency extension
as described in Section 3.3. The primary submis-
sion combines the phrase-based contrastive system,
a hierarchical system that is very similar to the Jane
contrastive submission but with a slightly worse lan-
guage model, and an additional PBT system that has
been trained with forced alignment (Wuebker et al,
2010) on WMT 2010 data only.
4.2 Experimental Results English?French
The results for the English?French task are given
in Table 4. We likewise submitted two contrastive
systems for this translation direction. The first con-
trastive submission is a phrase-based system, en-
hanced with a triplet lexicon model and a discrim-
inative word lexicon model (Mauser et al, 2009) ?
both trained on in-domain news commentary data
only ? as well as a sentence-level single-word lex-
icon model and a discriminative reordering model
(Zens and Ney, 2006a). The second contrastive sub-
mission is a hierarchical Jane system with shallow
rules (Iglesias et al, 2009), a triplet lexicon model, a
discriminative word lexicon, the parse match model,
and a second phrase table extracted from in-domain
data only. Our primary submission is very similar
to the latter Jane setup. It does not comprise the ex-
tended lexicon models and the parse match exten-
sion, but instead includes lexical phrases from the
full 30 Mio. sentence corpus as described above.
5 German-English Setups
We trained phrase-based and hierarchical transla-
tion systems for both translation directions of the
German-English language pair. The corpus statis-
408
German English
Sentences 1 857 745
Running Words 48 449 977 50 559 217
Vocabulary 387 593 123 470
Table 5: Corpus statistics of the preprocessed train-
ing data for the WMT 2011 German?English and
English?German translation tasks. Numerical quantities
are replaced by a single category symbol.
tics can be found in Table 5. Word alignments were
generated with GIZA++ and symmetrized as for the
French-English setups.
The language models are 4-grams trained on the
bilingual data as well as the provided News crawl
corpus. For the English language model the 109
French-English and LDC Gigaword corpora were
used additionally. For the 109 French-English and
LDC Gigaword corpora RWTH applied the data se-
lection technique described in Section 3.1. We ex-
amined two different language models, one with
LDC data and one without.
Systems were optimized on the newstest2009 data
set, newstest2008 was used as test set. The scores
for newstest2010 are included for completeness.
5.1 Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the source side
was preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we performed the long-range
part-of-speech based reordering rules proposed by
(Popovic? et al, 2006). For additional experiments
we used the TreeTagger (Schmid, 1995) to produce
a lemmatized version of the German source.
5.2 Optimization Criterion
We studied the impact of different optimization cri-
teria on tranlsation performance. The usual prac-
tice is to optimize the scaling factors to maximize
BLEU. We also experimented with two different
combinations of BLEU and Translation Edit Rate
(TER): TER?BLEU and TER?4BLEU. The first
denotes the equally weighted combination, while for
the latter BLEU is weighted 4 times as strong as
TER.
5.3 Experimental Results German?English
For the German?English task we conducted ex-
periments comparing the standard phrase extraction
with the phrase training technique described in Sec-
tion 3.2. For the latter we applied log-linear phrase-
table interpolation as proposed in (Wuebker et al,
2010). Further experiments included the use of addi-
tional language model training data, reranking of n-
best lists generated by the phrase-based system, and
different optimization criteria. We also carried out
a system combination of several systems, including
phrase-based systems on lemmatized German and
on source data without compound splitting and two
hierarchical systems optimized for different criteria.
The results are given in Table 6.
A considerable increase in translation quality can
be achieved by application of German compound
splitting. The system that operates on German
surface forms without compound splitting (SUR)
clearly underperforms the baseline system with mor-
phological preprocessing. The system on lemma-
tized German (LEM) is at about the same level as
the system on surface forms.
In comparison to the standard heuristic phrase ex-
traction technique, performing phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006b),
sentence length model, a 6-gram LM and single-
word lexicon models in both normal and inverse di-
rection. These models are combined in a log-linear
fashion and the scaling factors are tuned in the same
manner as the baseline system (using TER?4BLEU
on newstest2009).
The table includes three identical Jane systems
which are optimized for different criteria. The one
optimized for TER?4BLEU offers the best balance
between BLEU and TER, but was not finished in
time for submission. As primary submission we
chose the reranked PBT system, as secondary the
system combination.
409
newstest2008 newstest2009 newstest2010
German?English opt criterion BLEU TER BLEU TER BLEU TER
Syscombi of ? (secondary) TER?BLEU 21.1 62.1 20.8 61.2 23.7 59.2
Jane +GW ? BLEU 21.5 63.9 21.0 63.3 22.9 61.7
Jane +GW TER?4BLEU 21.4 62.6 21.1 62.0 23.5 60.3
PBT (FA) rerank +GW (primary) ? TER?4BLEU 21.4 62.8 21.1 61.9 23.4 60.1
PBT (FA) +GW ? TER?4BLEU 21.1 63.0 21.1 62.2 23.3 60.3
Jane +GW ? TER?BLEU 20.9 61.1 20.4 60.5 23.4 58.3
PBT (FA) TER?4BLEU 21.1 63.2 20.6 62.4 23.2 60.4
PBT TER?4BLEU 20.6 62.7 20.3 61.9 23.3 59.7
PBT (SUR) ? TER?4BLEU 19.5 66.5 18.9 65.8 21.0 64.9
PBT (LEM) ? TER?4BLEU 19.2 66.1 18.9 65.4 21.0 63.5
Table 6: RWTH systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results
are in percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model.
SUR and LEM denote the systems without compound splitting and on the lemmatized source, respectively. The three
hierarchical Jane systems are identical, but used different parameter optimization criterea.
newstest2008 newstest2009 newstest2010
English?German opt criterion BLEU TER BLEU TER BLEU TER
PBT + discrim. reord. (primary) TER?4BLEU 15.3 70.2 15.1 69.8 16.2 65.6
PBT + discrim. reord. BLEU 15.2 70.6 15.2 70.1 16.2 66.0
PBT TER?4BLEU 15.2 70.7 15.2 70.2 16.2 66.1
Jane BLEU 15.1 72.1 15.4 71.2 16.4 67.4
Jane TER?4BLEU 15.1 68.4 14.6 69.5 14.6 65.9
Table 7: RWTH systems for the WMT 2011 English?German translation task (truecase). BLEU and TER results are
in percentage.
5.4 Experimental Results English?German
We likewise studied the effect of using BLEU only
versus using TER?4BLEU as optimization crite-
rion in the English?German translation direction.
Moreover, we tested the impact of the discriminative
reordering model (Zens and Ney, 2006a). The re-
sults can be found in Table 7. For the phrase-based
system, optimizing towards TER?4BLEU leads to
slightly better results both in BLEU and TER than
optimizing towards BLEU. Using the discriminative
reordering model yields some improvements both on
newstest2008 and newstest2010. In the case of the
hierarchical system, the effect of the optimization
criterion is more pronounced than for the phrase-
based system. However, in this case it clearly leads
to a tradeoff between BLEU and TER, as the choice
of TER?4BLEU harms the translation results of
test2010 with respect to BLEU.
6 Conclusion
For the participation in the WMT 2011 shared trans-
lation task, RWTH experimented with both phrase-
based and hierarchical translation systems. We used
all bilingual and monolingual data provided for the
constrained track. To limit the size of the lan-
guage model, a data selection technique was applied.
Several techniques yielded improvements over the
baseline, including three syntactic models, extended
lexicon models, a discriminative reordering model,
forced alignment training, reranking methods and
different optimization criteria.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
410
References
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the Workshop on Statis-
tical Machine Translation, pages 31?38.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380?388.
D. Klein and C.D. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics
- Volume 1, ACL ?03, pages 423?430.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending Sta-
tistical Machine Translation with Discriminative and
Trigger-Based Lexicon Models. In Conference on
Empirical Methods in Natural Language Processing,
pages 210?217.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin,
Ireland, March.
L. Shen, J. Xu, and R. Weischedel. 2008. A New String-
to-Dependency Machine Translation Algorithm with a
Target Dependency Language Model. In Proceedings
of ACL-08: HLT. Association for Computational Lin-
guistics, pages 577?585, June.
D. Stein, S. Peitz, D. Vilar, and H. Ney. 2010. A Cocktail
of Deep Syntactic Features for Hierarchical Machine
Translation. In Conference of the Association for Ma-
chine Translation in the Americas 2010, page 9, Den-
ver, USA, October.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901 ? 904, Denver, Col-
orado, USA, September.
D. Vilar, D. Stein, and H. Ney. 2008. Analysing Soft
Syntax Features and Heuristics for Hierarchical Phrase
Based Machine Translation. In Proc. of the Int. Work-
shop on Spoken Language Translation (IWSLT), pages
190?197, Waikiki, Hawaii, October.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006a. Discriminative Reordering
Models for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 55?63, New York City, June.
R. Zens and H. Ney. 2006b. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
411
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
412
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 91?96,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Lightly-Supervised Training for Hierarchical Phrase-Based Machine
Translation
Matthias Huck1 and David Vilar1,2 and Daniel Stein1 and Hermann Ney1
1 Human Language Technology and Pattern 2 DFKI GmbH
Recognition Group, RWTH Aachen University Berlin, Germany
<surname>@cs.rwth-aachen.de david.vilar@dfki.de
Abstract
In this paper we apply lightly-supervised
training to a hierarchical phrase-based statis-
tical machine translation system. We employ
bitexts that have been built by automatically
translating large amounts of monolingual data
as additional parallel training corpora. We ex-
plore different ways of using this additional
data to improve our system.
Our results show that integrating a second
translation model with only non-hierarchical
phrases extracted from the automatically gen-
erated bitexts is a reasonable approach. The
translation performance matches the result we
achieve with a joint extraction on all train-
ing bitexts while the system is kept smaller
due to a considerably lower overall number of
phrases.
1 Introduction
We investigate the impact of an employment of large
amounts of unsupervised parallel data as training
data for a statistical machine translation (SMT) sys-
tem. The unsupervised parallel data is created by au-
tomatically translating monolingual source language
corpora. This approach is called lightly-supervised
training in the literature and has been introduced by
Schwenk (2008). In contrast to Schwenk, we do not
apply lightly-supervised training to a conventional
phrase-based system (Och et al, 1999; Koehn et al,
2003) but to a hierarchical phrase-based translation
(HPBT) system.
In hierarchical phrase-based translation (Chiang,
2005) a weighted synchronous context-free gram-
mar is induced from parallel text, the search is
based on CYK+ parsing (Chappelier and Rajman,
1998) and typically carried out using the cube prun-
ing algorithm (Huang and Chiang, 2007). In addi-
tion to the contiguous lexical phrases as in standard
phrase-based translation, the hierarchical phrase-
based paradigm also allows for phrases with gaps
which are called hierarchical phrases. A generic
non-terminal symbol serves as a placeholder that
marks the gaps.
In this paper we study several different ways
of incorporating unsupervised training data into
a hierarchical system. The basic techniques we
employ are the use of multiple translation mod-
els and a distinction of the hierarchical and the
non-hierarchical (i.e. lexical) part of the transla-
tion model. We report experimental results on
the large-scale NIST Arabic-English translation task
and show that lightly-supervised training yields sig-
nificant gains over the baseline.
2 Related Work
Large-scale lightly-supervised training for SMT as
we define it in this paper has been first carried out
by Schwenk (2008). Schwenk translates a large
amount of monolingual French data with an initial
Moses (Koehn et al, 2007) baseline system into En-
glish. In Schwenk?s original work, an additional
bilingual dictionary is added to the baseline. With
lightly-supervised training, Schwenk achieves im-
provements of around one BLEU point over the
baseline. In a later work (Schwenk and Senellart,
2009) he applies the same method for translation
model adaptation on an Arabic-French task with
91
gains of up to 3.5 points BLEU. 1
Hierarchical phrase-based translation has been pi-
oneered by David Chiang (Chiang, 2005; Chiang,
2007) with his Hiero system. The hierarchical
paradigm has been implemented and extended by
several groups since, some have published their soft-
ware as open source (Li et al, 2009; Hoang et al,
2009; Vilar et al, 2010).
Combining multiple translation models has been
investigated for domain adaptation by Foster and
Kuhn (2007) and Koehn and Schroeder (2007) be-
fore. Heger et al (2010) exploit the distinction be-
tween hierarchical and lexical phrases in a similar
way as we do. They train phrase translation proba-
bilities with forced alignment using a conventional
phrase-based system (Wuebker et al, 2010) and em-
ploy them for the lexical phrases while the hierarchi-
cal phrases stay untouched.
3 Using the Unsupervised Data
The most straightforward way of trying to improve
the baseline with lightly-supervised training would
be to concatenate the human-generated parallel data
and the unsupervised data and to jointly extract
phrases from the unified parallel data (after having
trained word alignments for the unsupervised bitexts
as well). This method is simple and expected to
be effective usually. There may however be two
drawbacks: First, the reliability and the amount of
parallel sentences may differ between the human-
generated and the unsupervised part of the training
data. It might be desirable to run separate extrac-
tions on the two corpora in order to be able to dis-
tinguish and weight phrases (or rather their scores)
according to their origin during decoding. Second, if
we incorporate large amounts of additional unsuper-
vised data, the amount of phrases that are extracted
may become much larger. We would want to avoid
blowing up our phrase table sizes without an appro-
1Schwenk names the method lightly-supervised training be-
cause the topics that are covered in the monolingual source lan-
guage data that is being translated may potentially also be cov-
ered by parts of the language model training data of the system
which is used to translate them. This can be considered as a
form of light supervision. We loosely apply the term lightly-
supervised training if we mean the process of utilizing a ma-
chine translation system to produce additional bitexts that are
used as training data, but still refer to the automatically pro-
duced bilingual corpora as unsupervised data.
Arabic English
Sentences 2 514 413
Running words 54 324 372 55 348 390
Vocabulary 264 528 207 780
Singletons 115 171 91 390
Table 1: Data statistics for the preprocessed Arabic-
English parallel training corpus. In the corpus, numer-
ical quantities have been replaced by a special category
symbol.
dev (MT06) test (MT08)
Sentences 1 797 1 360
Running words 49 677 45 095
Vocabulary 9 274 9 387
OOV [%] 0.5 0.4
Table 2: Data statistics for the preprocessed Arabic part
of the dev and test corpora. In the corpus, numerical
quantities have been replaced by a special category sym-
bol.
priate effect on translation quality. This holds in par-
ticular in the case of hierarchical phrases. Phrase-
based machine translation systems are usually able
to correctly handle local context dependencies, but
often have problems in producing a fluent sentence
structure across long distances. It is thus an intuitive
supposition that using hierarchical phrases extracted
from unsupervised data in addition to the hierar-
chical phrases extracted from the presumably more
reliable human-generated bitexts does not increase
translation quality. We will compare a joint extrac-
tion to the usage of two separate translation mod-
els (either without separate weighting, with a binary
feature, or as a log-linear mixture). We will further
check if including hierarchical phrases from the un-
supervised data is beneficial or not.
4 Experiments
We use the open source Jane toolkit (Vilar et al,
2010) for our experiments, a hierarchical phrase-
based translation software written in C++.
4.1 Baseline System
The baseline system has been trained using a
human-generated parallel corpus of 2.5M Arabic-
English sentence pairs. Word alignments in both
92
directions were produced with GIZA++ and sym-
metrized according to the refined method that was
suggested by Och and Ney (2003).
The models integrated into our baseline system
are: phrase translation probabilities and lexical
translation probabilities for both translation direc-
tions, length penalties on word and phrase level,
three binary features marking hierarchical phrases,
glue rule, and rules with non-terminals at the bound-
aries, four simple additional count- and length-
based binary features, and a large 4-gram language
model with modified Kneser-Ney smoothing that
was trained with the SRILM toolkit (Stolcke, 2002).
We ran the cube pruning algorithm, the depth of
the hierarchical recursion was restricted to one by
using shallow rules as proposed by Iglesias et al
(2009).
The scaling factors of the log-linear model com-
bination have been optimized towards BLEU with
MERT (Och, 2003) on the MT06 NIST test corpus.
MT08 was employed as held-out test data. Detailed
statistics for the parallel training data are given in
Table 1, for the development and the test corpus in
Table 2.
4.2 Unsupervised Data
The unsupervised data that we integrate has been
created by automatic translations of parts of the
Arabic LDC Gigaword corpus (mostly from the
HYT collection) with a standard phrase-based sys-
tem (Koehn et al, 2003). We thus in fact conduct a
cross-system and cross-paradigm variant of lightly-
supervised training. Translating the monolingual
Arabic data has been performed by LIUM, Le Mans,
France. We thank Holger Schwenk for kindly pro-
viding the translations.
The score computed by the decoder for each
translation has been normalized with respect to the
sentence length and used to select the most reliable
sentence pairs. Word alignments for the unsuper-
vised data have been produced in the same way as
for the baseline bilingual training data. We report
the statistics of the unsupervised data in Table 3.
4.3 Translation Models
We extracted three different phrase tables, one from
the baseline human-generated parallel data only,
one from the unsupervised data only, and one joint
Arabic English
Sentences 4 743 763
Running words 121 478 207 134 227 697
Vocabulary 306 152 237 645
Singletons 130 981 102 251
Table 3: Data statistics for the Arabic-English unsuper-
vised training corpus after selection of the most reliable
sentence pairs. In the corpus, numerical quantities have
been replaced by a special category symbol.
phrase table from the concatenation of the baseline
data and the unsupervised data. We will denote the
different extractions as baseline, unsupervised, and
joint, respectively.
The conventional restrictions have been applied
for phrase extraction in all conditions, i.e. a maxi-
mum length of ten words on source and target side
for lexical phrases, a length limit of five (including
non-terminal symbols) on source side and ten on tar-
get side for hierarchical phrases, and at most two
non-terminals per rule which are not allowed to be
adjacent on the source side. To limit the number of
hierarchical phrases, a minimum count cutoff of one
and an extraction pruning threshold of 0.1 have been
applied to them. Note that we did not prune lexical
phrases.
Statistics on the phrase table sizes are presented
in Table 4.2 In total the joint extraction results in
almost three times as many phrases as the baseline
extraction. The extraction from the unsupervised
data exclusively results in more than two times as
many hierarchical phrases as from the baseline data.
The sum of the number of hierarchical phrases from
baseline and unsupervised extraction is very close
to the number of hierarchical phrases from the joint
extraction. If we discard the hierarchical phrases ex-
tracted from the unsupervised data and use the lex-
ical part of the unsupervised phrase table (27.3M
phrases) as a second translation model in addition to
the baseline phrase table (67.0M phrases), the over-
all number of phrases is increased by only 41% com-
pared to the baseline system.
2The phrase tables have been filtered towards the phrases
needed for the translation of a given collection of test corpora.
93
number of phrases
lexical hierarchical total
extraction from baseline data 19.8M 47.2M 67.0M
extraction from unsupervised data 27.3M 115.6M 142.9M
phrases present in both tables 15.0M 40.1M 55.1M
joint extraction baseline + unsupervised 32.1M 166.5M 198.6M
Table 4: Phrase table sizes. The phrase tables have been filtered towards a larger set of test corpora containing a total
of 2.3 million running words.
dev (MT06) test (MT08)
BLEU TER BLEU TER
[%] [%] [%] [%]
HPBT baseline 44.1 49.9 44.4?0.9 49.4?0.8
HPBT unsupervised only 45.3 48.8 45.2 49.1
joint extraction baseline + unsupervised 45.6 48.7 45.4?0.9 49.1?0.8
baseline hierarchical phrases + unsupervised lexical phrases 45.1 49.1 45.2 49.2
baseline hierarchical phrases + joint extraction lexical phrases 45.3 48.7 45.3 49.1
baseline + unsupervised lexical phrases 45.3 48.9 45.3 49.0
baseline + unsupervised lexical phrases (with binary feature) 45.3 48.8 45.4 49.0
baseline + unsupervised lexical phrases (separate scaling factors) 45.3 48.9 45.0 49.3
baseline + unsupervised full table 45.6 48.6 45.1 48.9
baseline + unsupervised full table (with binary feature) 45.5 48.6 45.2 48.8
baseline + unsupervised full table (separate scaling factors) 45.5 48.7 45.3 49.0
Table 5: Results for the NIST Arabic-English translation task (truecase). The 90% confidence interval is given for the
baseline system as well as for the system with joint phrase extraction. Results in bold are significantly better than the
baseline.
4.4 Experimental Results
The empirical evaluation of all our systems on the
two standard metrics BLEU (Papineni et al, 2002)
and TER (Snover et al, 2006) is presented in Ta-
ble 5. We have also checked the results for statistical
significance over the baseline. The confidence in-
tervals have been computed using bootstrapping for
BLEU and Cochran?s approximate ratio variance for
TER (Leusch and Ney, 2009).
When we combine the full baseline phrase ta-
ble with the unsupervised phrase table or the lexi-
cal part of it, we either use common scaling factors
for their source-to-target and target-to-source trans-
lation costs, or we use common scaling factors but
mark entries from the unsupervised table with a bi-
nary feature, or we optimize the four translation fea-
tures separately for each of the two tables as part of
the log-linear model combination.
Including the unsupervised data leads to a sub-
stantial gain on the unseen test set of up to +1.0%
BLEU absolute. The different ways of combining
the manually produced data with the unsupervised
have little impact on translation quality. This holds
specifically for the combination with only the lexical
phrases, which, when marked with a binary feature,
is able to obtain the same results as the full (joint
extraction) system but with much less phrases. We
compared the decoding speed of these two setups
and observed that the system with less phrases is
clearly faster (5.5 vs. 2.6 words per second, mea-
sured on MT08). The memory requirements of the
systems do not differ greatly as we are using a bi-
narized representation of the phrase table with on-
demand loading. All setups consume slightly less
than 16 gigabytes of RAM.
94
5 Conclusion
We presented several approaches of applying
lightly-supervised training to hierarchical phrase-
based machine translation. Using the additional au-
tomatically produced bitexts we have been able to
obtain considerable gains compared to the baseline
on the large-scale NIST Arabic-to-English transla-
tion task. We showed that a joint phrase extraction
from human-generated and automatically generated
parallel training data is not required to achieve sig-
nificant improvements. The same translation qual-
ity can be achieved by adding a second translation
model with only lexical phrases extracted from the
automatically created bitexts. The overall amount of
phrases can thus be kept much smaller.
Acknowledgments
The authors would like to thank Holger Schwenk
from LIUM, Le Mans, France, for making the au-
tomatic translations of the Arabic LDC Gigaword
corpus available. This work was partly supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-08-C-0110.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the DARPA.
References
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. In Proc. of the First Workshop on Tabulation
in Parsing and Deduction, pages 133?137, April.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc. of
the 43rd Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263?270, Ann Arbor,
MI, June.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228,
June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128?
135, Prague, Czech Republic, June.
Carmen Heger, Joern Wuebker, David Vilar, and Her-
mann Ney. 2010. A Combination of Hierarchical Sys-
tems with Forced Alignments from Phrase-Based Sys-
tems. In Proc. of the Int. Workshop on Spoken Lan-
guage Translation (IWSLT), Paris, France, December.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchical,
and Syntax-Based Statistical Machine Translation. In
Proc. of the Int. Workshop on Spoken Language Trans-
lation (IWSLT), pages 152?159, Tokyo, Japan.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proc. of the Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL), pages 144?151, Prague,
Czech Republic, June.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule Filtering by Pattern
for Efficient Hierarchical Translation. In Proc. of the
12th Conf. of the Europ. Chapter of the Assoc. for
Computational Linguistics (EACL), pages 380?388,
Athens, Greece, March.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 224?227, Prague, Czech
Republic, June.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc. of
the Human Language Technology Conf. / North Amer-
ican Chapter of the Assoc. for Computational Lin-
guistics (HLT-NAACL), pages 127?133, Edmonton,
Canada, May/June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 177?180, Prague, Czech Re-
public, June.
Gregor Leusch and Hermann Ney. 2009. Edit distances
with block movements and error rate confidence esti-
mates. Machine Translation, December.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An Open
Source Toolkit for Parsing-Based Machine Transla-
tion. In Proc. of the Workshop on Statistical Machine
Translation, pages 135?139, Athens, Greece, March.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora (EMNLP99), pages 20?
28, University of Maryland, College Park, MD, June.
95
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of the 40th
Annual Meeting of the Assoc. for Computational Lin-
guistics (ACL), pages 311?318, Philadelphia, PA, July.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/French News Trans-
lation System by Lightly-Supervised Training. In MT
Summit XII, Ottawa, Ontario, Canada, August.
Holger Schwenk. 2008. Investigations on Large-Scale
Lightly-Supervised Training for Statistical Machine
Translation. In Proc. of the Int. Workshop on Spo-
ken Language Translation (IWSLT), pages 182?189,
Waikiki, Hawaii, October.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), pages 223?231, Cambridge,
MA, August.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In ACL 2010 Joint Fifth Workshop on Statistical Ma-
chine Translation and Metrics MATR, pages 262?270,
Uppsala, Sweden, July.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training Phrase Translation Models with Leaving-
One-Out. In Proc. of the Annual Meeting of the As-
soc. for Computational Linguistics (ACL), pages 475?
484, Uppsala, Sweden, July.
96
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 191?199,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Review of Hypothesis Alignment Algorithms for MT System Combination
via Confusion Network Decoding
Antti-Veikko I. Rostia?, Xiaodong Heb, Damianos Karakosc, Gregor Leuschd?, Yuan Caoc,
Markus Freitage, Spyros Matsoukasf , Hermann Neye, Jason R. Smithc and Bing Zhangf
aApple Inc., Cupertino, CA 95014
arosti@apple.com
bMicrosoft Research, Redmond, WA 98052
xiaohe@microsoft.com
cJohns Hopkins University, Baltimore, MD 21218
{damianos,yuan.cao,jrsmith}@jhu.edu
dSAIC, Monheimsallee 22, D-52062 Aachen, Germany
gregor.leusch@saic.com
eRWTH Aachen University, D-52056 Aachen, Germany
{freitag,ney}@cs.rwth-aachen.de
fRaytheon BBN Technologies, 10 Moulton Street, Cambridge, MA 02138
{smatsouk,bzhang}@bbn.com
Abstract
Confusion network decoding has proven to
be one of the most successful approaches
to machine translation system combination.
The hypothesis alignment algorithm is a cru-
cial part of building the confusion networks
and many alternatives have been proposed in
the literature. This paper describes a sys-
tematic comparison of five well known hy-
pothesis alignment algorithms for MT sys-
tem combination via confusion network de-
coding. Controlled experiments using identi-
cal pre-processing, decoding, and weight tun-
ing methods on standard system combina-
tion evaluation sets are presented. Transla-
tion quality is assessed using case insensitive
BLEU scores and bootstrapping is used to es-
tablish statistical significance of the score dif-
ferences. All aligners yield significant BLEU
score gains over the best individual system in-
cluded in the combination. Incremental indi-
rect hidden Markov model and a novel incre-
mental inversion transduction grammar with
flexible matching consistently yield the best
translation quality, though keeping all things
equal, the differences between aligners are rel-
atively small.
?The work reported in this paper was carried out while the
authors were at Raytheon BBN Technologies and
?RWTH Aachen University.
1 Introduction
Current machine translation (MT) systems are based
on different paradigms, such as rule-based, phrase-
based, hierarchical, and syntax-based. Due to the
complexity of the problem, systems make various
assumptions at different levels of processing and
modeling. Many of these assumptions may be
suboptimal and complementary. The complemen-
tary information in the outputs from multiple MT
systems may be exploited by system combination.
Availability of multiple system outputs within the
DARPA GALE program as well as NIST Open MT
and Workshop on Statistical Machine Translation
evaluations has led to extensive research in combin-
ing the strengths of diverse MT systems, resulting in
significant gains in translation quality.
System combination methods proposed in the lit-
erature can be roughly divided into three categories:
(i) hypothesis selection (Rosti et al, 2007b; Hilde-
brand and Vogel, 2008), (ii) re-decoding (Frederking
and Nirenburg, 1994; Jayaraman and Lavie, 2005;
Rosti et al, 2007b; He and Toutanova, 2009; De-
vlin et al, 2011), and (iii) confusion network de-
coding. Confusion network decoding has proven to
be the most popular as it does not require deep N -
best lists1 and operates on the surface strings. It has
1N -best lists of around N = 10 have been used in confu-
sion network decoding yielding small gains over using 1-best
191
also been shown to be very successful in combining
speech recognition outputs (Fiscus, 1997; Mangu et
al., 2000). The first application of confusion net-
work decoding in MT system combination appeared
in (Bangalore et al, 2001) where a multiple string
alignment (MSA), made popular in biological se-
quence analysis, was applied to the MT system out-
puts. Matusov et al (2006) proposed an alignment
based on GIZA++ Toolkit which introduced word
reordering not present in MSA, and Sim et al (2007)
used the alignments produced by the translation edit
rate (TER) (Snover et al, 2006) scoring. Extensions
of the last two are included in this study together
with alignments based on hidden Markov model
(HMM) (Vogel et al, 1996) and inversion transduc-
tion grammars (ITG) (Wu, 1997).
System combinations produced via confusion net-
work decoding using different hypothesis alignment
algorithms have been entered into open evalua-
tions, most recently in 2011 Workshop on Statistical
Machine Translation (Callison-Burch et al, 2011).
However, there has not been a comparison of the
most popular hypothesis alignment algorithms us-
ing the same sets of MT system outputs and other-
wise identical combination pipelines. This paper at-
tempts to systematically compare the quality of five
hypothesis alignment algorithms. Alignments were
produced for the same system outputs from three
common test sets used in the 2009 NIST Open MT
Evaluation and the 2011 Workshop on Statistical
Machine Translation. Identical pre-processing, de-
coding, and weight tuning algorithms were used to
quantitatively evaluate the alignment quality. Case
insensitive BLEU score (Papineni et al, 2002) was
used as the translation quality metric.
2 Confusion Network Decoding
A confusion network is a linear graph where all
paths visit all nodes. Two consecutive nodes may be
connected by one or more arcs. Given the arcs repre-
sent words in hypotheses, multiple arcs connecting
two consecutive nodes can be viewed as alternative
words in that position of a set of hypotheses encoded
by the network. A special NULL token represents
a skipped word and will not appear in the system
combination output. For example, three hypotheses
outputs (Rosti et al, 2011).
?twelve big cars?, ?twelve cars?, and ?dozen cars?
may be aligned as follows:
twelve big blue cars
twelve NULL NULL cars
dozen NULL blue cars
This alignment may be represented compactly as the
confusion network in Figure 1 which encodes a total
of eight unique hypotheses.
40 1twelve(2)dozen(1) 2big(1)NULL(2) 3blue(2)NULL(1) cars(3)
Figure 1: Confusion network from three strings ?twelve
big blue cars?, ?twelve cars?, and ?dozen blue cars? us-
ing the first as the skeleton. The numbers in parentheses
represent counts of words aligned to the corresponding
arc.
Building confusion networks from multiple ma-
chine translation system outputs has two main prob-
lems. First, one output has to be chosen as the skele-
ton hypothesis which defines the final word order of
the system combination output. Second, MT system
outputs may have very different word orders which
complicates the alignment process. For skeleton se-
lection, Sim et al (2007) proposed choosing the out-
put closest to all other hypotheses when using each
as the reference string in TER. Alternatively, Ma-
tusov et al (2006) proposed leaving the decision to
decoding time by connecting networks built using
each output as a skeleton into a large lattice. The
subnetworks in the latter approach may be weighted
by prior probabilities estimated from the alignment
statistics (Rosti et al, 2007a). Since different align-
ment algorithm produce different statistics and the
gain from the weights is relatively small (Rosti et al,
2011), weights for the subnetworks were not used
in this work. The hypothesis alignment algorithms
used in this work are briefly described in the follow-
ing section.
The confusion networks in this work were repre-
sented in a text lattice format shown in Figure 2.
Each line corresponds to an arc, where J is the arc
index, S is the start node index, E is the end node in-
dex, SC is the score vector, and W is the word label.
The score vector has as many elements as there are
input systems. The elements correspond to each sys-
tem and indicate whether a word from a particular
192
J=0 S=0 E=1 SC=(1,1,0) W=twelve
J=1 S=0 E=1 SC=(0,0,1) W=dozen
J=2 S=1 E=2 SC=(1,0,0) W=big
J=3 S=1 E=2 SC=(0,1,1) W=NULL
J=4 S=2 E=3 SC=(1,0,1) W=blue
J=5 S=2 E=3 SC=(0,1,0) W=NULL
J=6 S=3 E=4 SC=(1,1,1) W=cars
Figure 2: A lattice in text format representing the con-
fusion network in Figure 1. J is the arc index, S and E
are the start and end node indexes, SC is a vector of arc
scores, and W is the word label.
system was aligned to a given link2. These may be
viewed as system specific word confidences, which
are binary when aligning 1-best system outputs. If
no word from a hypothesis is aligned to a given link,
a NULL word token is generated provided one does
not already exist, and the corresponding element in
the NULL word token is set to one. The system
specific word scores are kept separate in order to
exploit system weights in decoding. Given system
weights wn, which sum to one, and system specific
word scores snj for each arc j (the SC elements), the
weighted word scores are defined as:
sj =
Ns?
n=1
wnsnj (1)
where Ns is the number of input systems. The hy-
pothesis score is defined as the sum of the log-word-
scores along the path, which is linearly interpolated
with a logarithm of the language model (LM) score
and a non-NULL word count:
S(E|F ) =
?
j?J (E)
log sj + ?SLM (E) + ?Nw(E)
(2)
where J (E) is the sequence of arcs generating the
hypothesis E for the source sentence F , SLM (E)
is the LM score, and Nw(E) is the number of
non-NULL words. The set of weights ? =
{w1, . . . , wNs , ?, ?} can be tuned so as to optimize
an evaluation metric on a development set.
Decoding with an n-gram language model re-
quires expanding the lattice to distinguish paths with
2A link is used as a synonym to the set of arcs between two
consecutive nodes. The name refers to the confusion network
structure?s resemblance to a sausage.
unique n-gram contexts before LM scores can be as-
signed the arcs. Using long n-gram context may re-
quire pruning to reduce memory usage. Given uni-
form initial system weights, pruning may remove
desirable paths. In this work, the lattices were ex-
panded to bi-gram context and no pruning was per-
formed. A set of bi-gram decoding weights were
tuned directly on the expanded lattices using a dis-
tributed optimizer (Rosti et al, 2010). Since the
score in Equation 2 is not a simple log-linear inter-
polation, the standard minimum error rate training
(Och, 2003) with exact line search cannot be used.
Instead, downhill simplex (Press et al, 2007) was
used in the optimizer client. After bi-gram decod-
ing weight optimization, another set of 5-gram re-
scoring weights were tuned on 300-best lists gener-
ated from the bi-gram expanded lattices.
3 Hypothesis Alignment Algorithms
Two different methods have been proposed for
building confusion networks: pairwise and incre-
mental alignment. In pairwise alignment, each
hypothesis corresponding to a source sentence is
aligned independently with the skeleton hypothe-
sis. This set of alignments is consolidated using the
skeleton words as anchors to form the confusion net-
work (Matusov et al, 2006; Sim et al, 2007). The
same word in two hypotheses may be aligned with a
different word in the skeleton resulting in repetition
in the network. A two-pass alignment algorithm to
improve pairwise TER alignments was introduced in
(Ayan et al, 2008). In incremental alignment (Rosti
et al, 2008), the confusion network is initialized by
forming a simple graph with one word per link from
the skeleton hypothesis. Each remaining hypothesis
is aligned with the partial confusion network, which
allows words from all previous hypotheses be con-
sidered as matches. The order in which the hypothe-
ses are aligned may influence the alignment qual-
ity. Rosti et al (2009) proposed a sentence specific
alignment order by choosing the unaligned hypoth-
esis closest to the partial confusion network accord-
ing to TER. The following five alignment algorithms
were used in this study.
193
3.1 Pairwise GIZA++ Enhanced Hypothesis
Alignment
Matusov et al (2006) proposed using the GIZA++
Toolkit (Och and Ney, 2003) to align a set of tar-
get language translations. A parallel corpus where
each system output acting as a skeleton appears as
a translation of all system outputs corresponding to
the same source sentence. The IBM Model 1 (Brown
et al, 1993) and hidden Markov model (HMM) (Vo-
gel et al, 1996) are used to estimate the alignment.
Alignments from both ?translation? directions are
used to obtain symmetrized alignments by interpo-
lating the HMM occupation statistics (Matusov et
al., 2004). The algorithm may benefit from the fact
that it considers the entire test set when estimating
the alignment model parameters; i.e., word align-
ment links from all output sentences influence the
estimation, whereas other alignment algorithms only
consider words within a pair of sentences (pairwise
alignment) or all outputs corresponding to a single
source sentence (incremental alignment). However,
it does not naturally extend to incremental align-
ment. The monotone one-to-one alignments are then
transformed into a confusion network. This aligner
is referred to as GIZA later in this paper.
3.2 Incremental Indirect Hidden Markov
Model Alignment
He et al (2008) proposed using an indirect hidden
Markov model (IHMM) for pairwise alignment of
system outputs. The parameters of the IHMM are
estimated indirectly from a variety of sources in-
cluding semantic word similarity, surface word sim-
ilarity, and a distance-based distortion penalty. The
alignment between two target language outputs are
treated as the hidden states. A standard Viterbi al-
gorithm is used to infer the alignment. The pair-
wise IHMM was extended to operate incrementally
in (Li et al, 2009). Sentence specific alignment or-
der is not used by this aligner, which is referred to
as iIHMM later in this paper.
3.3 Incremental Inversion Transduction
Grammar Alignment with Flexible
Matching
Karakos et al (2008) proposed using inversion trans-
duction grammars (ITG) (Wu, 1997) for pairwise
alignment of system outputs. ITGs form an edit
distance, invWER (Leusch et al, 2003), that per-
mits properly nested block movements of substrings.
For well-formed sentences, this may be more nat-
ural than allowing arbitrary shifts. The ITG algo-
rithm is very expensive due to its O(n6) complexity.
The search algorithm for the best ITG alignment, a
best-first chart parsing (Charniak et al, 1998), was
augmented with an A? search heuristic of quadratic
complexity (Klein and Manning, 2003), resulting in
significant reduction in computational complexity.
The finite state-machine heuristic computes a lower
bound to the alignment cost of two strings by allow-
ing arbitrary word re-orderings. The ITG hypothesis
alignment algorithm was extended to operate incre-
mentally in (Karakos et al, 2010) and a novel ver-
sion where the cost function is computed based on
the stem/synonym similarity of (Snover et al, 2009)
was used in this work. Also, a sentence specific
alignment order was used. This aligner is referred
to as iITGp later in this paper.
3.4 Incremental Translation Edit Rate
Alignment with Flexible Matching
Sim et al (2007) proposed using translation edit rate
scorer3 to obtain pairwise alignment of system out-
puts. The TER scorer tries to find shifts of blocks
of words that minimize the edit distance between
the shifted reference and a hypothesis. Due to the
computational complexity, a set of heuristics is used
to reduce the run time (Snover et al, 2006). The
pairwise TER hypothesis alignment algorithm was
extended to operate incrementally in (Rosti et al,
2008) and also extended to consider synonym and
stem matches in (Rosti et al, 2009). The shift
heuristics were relaxed for flexible matching to al-
low shifts of blocks of words as long as the edit dis-
tance is decreased even if there is no exact match in
the new position. A sentence specific alignment or-
der was used by this aligner, which is referred to as
iTER later in this paper.
3.5 Incremental Translation Edit Rate Plus
Alignment
Snover et al (2009) extended TER scoring to con-
sider synonyms and paraphrase matches, called
3http://www.cs.umd.edu/?snover/tercom/
194
TER-plus (TERp). The shift heuristics in TERp
were also relaxed relative to TER. Shifts are allowed
if the words being shifted are: (i) exactly the same,
(ii) synonyms, stems or paraphrases of the corre-
sponding reference words, or (iii) any such combina-
tion. Xu et al (2011) proposed using an incremental
version of TERp for building consensus networks. A
sentence specific alignment order was used by this
aligner, which is referred to as iTERp later in this
paper.
4 Experimental Evaluation
Combination experiments were performed on (i)
Arabic-English, from the informal system combi-
nation track of the 2009 NIST Open MT Evalua-
tion4; (ii) German-English from the system com-
bination evaluation of the 2011 Workshop on Sta-
tistical Machine Translation (Callison-Burch et al,
2011) (WMT11) and (iii) Spanish-English, again
from WMT11. Eight top-performing systems (as
evaluated using case-insensitive BLEU) were used
in each language pair. Case insensitive BLEU scores
for the individual system outputs on the tuning and
test sets are shown in Table 1. About 300 and
800 sentences with four reference translations were
available for Arabic-English tune and test sets, re-
spectively, and about 500 and 2500 sentences with a
single reference translation were available for both
German-English and Spanish-English tune and test
sets. The system outputs were lower-cased and to-
kenized before building confusion networks using
the five hypothesis alignment algorithms described
above. Unpruned English bi-gram and 5-gram lan-
guage models were trained with about 6 billion
words available for these evaluations. Multiple com-
ponent language models were trained after dividing
the monolingual corpora by source. Separate sets
of interpolation weights were tuned for the NIST
and WMT experiments to minimize perplexity on
the English reference translations of the previous
evaluations, NIST MT08 and WMT10. The sys-
tem combination weights, both bi-gram lattice de-
coding and 5-gram 300-best list re-scoring weights,
were tuned separately for lattices build with each hy-
pothesis alignment algorithm. The final re-scoring
4http://www.itl.nist.gov/iad/mig/tests/
mt/2009/ResultsRelease/indexISC.html
outputs were detokenized before computing case in-
sensitive BLEU scores. Statistical significance was
computed for each pairwise comparison using boot-
strapping (Koehn, 2004).
Decode Oracle
Aligner tune test tune test
GIZA 60.06 57.95 75.06 74.47
iTER 59.74 58.63? 73.84 73.20
iTERp 60.18 59.05? 76.43 75.58
iIHMM 60.51 59.27?? 76.50 76.17
iITGp 60.65 59.37?? 76.53 76.05
Table 2: Case insensitive BLEU scores for NIST MT09
Arabic-English system combination outputs. Note, four
reference translations were available. Decode corre-
sponds to results after weight tuning and Oracle corre-
sponds to graph TER oracle. Dagger (?) denotes statisti-
cally significant difference compared to GIZA and double
dagger (?) compared to iTERp and the aligners above it.
The BLEU scores for Arabic-English system
combination outputs are shown in Table 2. The first
column (Decode) shows the scores on tune and test
sets for the decoding outputs. The second column
(Oracle) shows the scores for oracle hypotheses ob-
tained by aligning the reference translations with the
confusion networks and choosing the path with low-
est graph TER (Rosti et al, 2008). The rows rep-
resenting different aligners are sorted according to
the test set decoding scores. The order of the BLEU
scores for the oracle translations do not always fol-
low the order for the decoding outputs. This may be
due to differences in the compactness of the confu-
sion networks. A more compact network has fewer
paths and is therefore less likely to contain signif-
icant parts of the reference translation, whereas a
reference translation may be generated from a less
compact network. On Arabic-English, all incremen-
tal alignment algorithms are significantly better than
the pairwise GIZA, incremental IHMM and ITG
with flexible matching are significantly better than
all other algorithms, but not significantly different
from each other. The incremental TER and TERp
were statistically indistinguishable. Without flexi-
ble matching, iITG yields a BLEU score of 58.85 on
test. The absolute BLEU gain over the best individ-
ual system was between 6.2 and 7.6 points on the
test set.
195
Arabic German Spanish
System tune test tune test tune test
A 48.84 48.54 21.96 21.41 27.71 27.13
B 49.15 48.97 22.61 21.80 28.42 27.90
C 49.30 49.50 22.77 21.99 28.57 28.23
D 49.38 49.59 22.90 22.41 29.00 28.41
E 49.42 49.75 22.90 22.65 29.15 28.50
F 50.28 50.69 22.98 22.65 29.53 28.61
G 51.49 50.81 23.41 23.06 29.89 29.82
H 51.72 51.74 24.28 24.16 30.55 30.14
Table 1: Case insensitive BLEU scores for the individual system outputs on the tune and test sets for all three source
languages.
Decode Oracle
Aligner tune test tune test
GIZA 25.93 26.02 37.32 38.22
iTERp 26.46 26.10 38.16 38.76
iTER 26.27 26.39? 37.00 37.66
iIHMM 26.34 26.40? 37.87 38.48
iITGp 26.47 26.50? 37.99 38.60
Table 3: Case insensitive BLEU scores for WMT11
German-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
iTERp and GIZA.
The BLEU scores for German-English system
combination outputs are shown in Table 3. Again,
the graph TER oracle scores do not follow the same
order as the decoding scores. The scores for GIZA
and iTERp are statistically indistinguishable, and
iTER, iIHMM, and iITGp are significantly better
than the first two. However, they are not statistically
different from each other. Without flexible match-
ing, iITG yields a BLEU score of 26.47 on test. The
absolute BLEU gain over the best individual system
was between 1.9 and 2.3 points on the test set.
The BLEU scores for Spanish-English system
combination outputs are shown in Table 4. All align-
ers but iIHMM are statistically indistinguishable and
iIHMM is significantly better than all other align-
ers. Without flexible matching, iITG yields a BLEU
score of 33.62 on test. The absolute BLEU gain over
the best individual system was between 3.5 and 3.9
Decode Oracle
Aligner tune test tune test
iTERp 34.20 33.61 50.45 51.28
GIZA 34.02 33.62 50.23 51.20
iTER 34.44 33.79 50.39 50.39
iITGp 34.41 33.85 50.55 51.33
iIHMM 34.61 34.05? 50.48 51.27
Table 4: Case insensitive BLEU scores for WMT11
Spanish-English system combination outputs. Note, only
a single reference translation per segment was available.
Decode corresponds to results after weight tuning and
Oracle corresponds to graph TER oracle. Dagger (?)
denotes statistically significant difference compared to
aligners above iIHMM.
points on the test set.
5 Error Analysis
Error analysis was performed to better understand
the gains from system combination. Specifically, (i)
how the different types of translation errors are af-
fected by system combination was investigated; and
(ii) an attempt to quantify the correlation between
the word agreement that results from the different
aligners and the translation error, as measured by
TER (Snover et al, 2006), was made.
5.1 Influence on Error Types
For each one of the individual systems, and for each
one of the three language pairs, the per-sentence er-
rors that resulted from that system, as well as from
each one of the the different aligners studied in this
paper, were computed. The errors were broken
196
down into insertions/deletions/substitutions/shifts
based on the TER scorer.
The error counts at the document level were ag-
gregated. For each document in each collection, the
number of errors of each type that resulted from each
individual system as well as each system combina-
tion were measured, and their difference was com-
puted. If the differences are mostly positive, then
it can be said (with some confidence) that system
combination has a significant impact in reducing the
error of that type. A paired Wilcoxon test was per-
formed and the p-value that quantifies the probabil-
ity that the measured error reduction was achieved
under the null hypothesis that the system combina-
tion performs as well as the best system was com-
puted.
Table 5 shows all conditions under consideration.
All cases where the p-value is below 10?2 are con-
sidered statistically significant. Two observations
are in order: (i) all alignment schemes significantly
reduce the number of substitution/shift errors; (ii)
in the case of insertions/deletions, there is no clear
trend; there are cases where the system combination
increases the number of insertions/deletions, com-
pared to the individual systems.
5.2 Relationship between Word Agreement
and Translation Error
This set of experiments aimed to quantify the rela-
tionship between the translation error rate and the
amount of agreement that resulted from each align-
ment scheme. The amount of system agreement at
a level x is measured by the number of cases (con-
fusion network arcs) where x system outputs con-
tribute the same word in a confusion network bin.
For example, the agreement at level 2 is equal to 2
in Figure 1 because there are exactly 2 arcs (with
words ?twelve? and ?blue?) that resulted from the
agreement of 2 systems. Similarly, the agreement at
level 3 is 1, because there is only 1 arc (with word
?cars?) that resulted from the agreement of 3 sys-
tems. It is hypothesized that a sufficiently high level
of agreement should be indicative of the correctness
of a word (and thus indicative of lower TER). The
agreement statistics were grouped into two values:
the ?weak? agreement statistic, where at most half
of the combined systems contribute a word, and the
?strong? agreement statistic, where more than half
non-NULL words NULL words
weak strong weak strong
Arabic 0.087 -0.068 0.192 0.094
German 0.117 -0.067 0.206 0.147
Spanish 0.085 -0.134 0.323 0.102
Table 6: Regression coefficients of the ?strong? and
?weak? agreement features, as computed with a gener-
alized linear model, using TER as the target variable.
of the combined systems contribute a word. To sig-
nify the fact that real words and ?NULL? tokens
have different roles and should be treated separately,
two sets of agreement statistics were computed.
A regression with a generalized linear model
(glm) that computed the coefficients of the agree-
ment quantities (as explained above) for each align-
ment scheme, using TER as the target variable, was
performed. Table 6 shows the regression coeffi-
cients; they are all significant at p-value < 0.001.
As is clear from this table, the negative coefficient of
the ?strong? agreement quantity for the non-NULL
words points to the fact that good aligners tend to
result in reductions in translation error. Further-
more, increasing agreements on NULL tokens does
not seem to reduce TER.
6 Conclusions
This paper presented a systematic comparison of
five different hypothesis alignment algorithms for
MT system combination via confusion network de-
coding. Pre-processing, decoding, and weight tun-
ing were controlled and only the alignment algo-
rithm was varied. Translation quality was compared
qualitatively using case insensitive BLEU scores.
The results showed that confusion network decod-
ing yields a significant gain over the best individ-
ual system irrespective of the alignment algorithm.
Differences between the combination output using
different alignment algorithms were relatively small,
but incremental alignment consistently yielded bet-
ter translation quality compared to pairwise align-
ment based on these experiments and previously
published literature. Incremental IHMM and a novel
incremental ITG with flexible matching consistently
yield highest quality combination outputs. Further-
more, an error analysis shows that most of the per-
197
Language Aligner ins del sub shft
GIZA 2.2e-16 0.9999 2.2e-16 2.2e-16
iHMM 2.2e-16 0.433 2.2e-16 2.2e-16
Arabic iITGp 0.8279 2.2e-16 2.2e-16 2.2e-16
iTER 4.994e-07 3.424e-11 2.2e-16 2.2e-16
iTERp 2.2e-16 1 2.2e-16 2.2e-16
GIZA 7.017e-12 2.588e-06 2.2e-16 2.2e-16
iHMM 6.858e-07 0.4208 2.2e-16 2.2e-16
German iITGp 0.8551 0.2848 2.2e-16 2.2e-16
iTER 0.2491 1.233e-07 2.2e-16 2.2e-16
iTERp 0.9997 0.007489 2.2e-16 2.2e-16
GIZA 2.2e-16 0.8804 2.2e-16 2.2e-16
iHMM 2.2e-16 1 2.2e-16 2.2e-16
Spanish iITGp 2.2e-16 0.9999 2.2e-16 2.2e-16
iTER 2.2e-16 1 2.2e-16 2.2e-16
iTERp 3.335e-16 1 2.2e-16 2.2e-16
Table 5: p-values which show which error types are statistically significantly improved for each language and aligner.
formance gains from system combination can be at-
tributed to reductions in substitution errors and word
re-ordering errors. Finally, better alignments of sys-
tem outputs, which tend to cause higher agreement
rates on words, correlate with reductions in transla-
tion error.
References
Necip Fazil Ayan, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Proc.
Coling, pages 33?40.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351?354.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Proc.
WMT, pages 22?64.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-based best-first chart parsing. In Proc.
Sixth Workshop on Very Large Corpora, pages 127?
133. Morgan Kaufmann.
Jacob Devlin, Antti-Veikko I. Rosti, Shankar Ananthakr-
ishnan, and Spyros Matsoukas. 2011. System combi-
nation using discriminative cross-adaptation. In Proc.
IJCNLP, pages 667?675.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347?354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95?
100.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. EMNLP, pages 1202?1211.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. EMNLP, pages 98?
107.
Almut S. Hildebrand and Stephan Vogel. 2008. Combi-
nation of machine translation systems via hypothesis
selection from combined n-best lists. In AMTA, pages
254?261.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proc. ACL, pages 81?84.
Damianos Karakos, Jason R. Smith, and Sanjeev Khu-
danpur. 2010. Hypothesis ranking and two-pass ap-
proaches for machine translation system combination.
In Proc. ICASSP.
198
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Proc.
NAACL, pages 40?47.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP,
pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with appli-
cations to machine translation evaluation. In Proc. MT
Summit 2003, pages 240?247, September.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. ACL/IJCNLP, pages 949?957.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical ma-
chine translation. In Proc. COLING, pages 219?225.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33?40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311?318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical recipes:
the art of scientific computing. Cambridge University
Press, 3rd edition.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Rirchard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. ACL, pages
312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proc. NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 183?186.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2009. Incremental hypothesis
alignment with flexible matching for building confu-
sion networks: BBN system description for WMT09
system combination task. In Proc. WMT, pages 61?
65.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system descrip-
tion for WMT10 system combination task. In Proc.
WMT, pages 321?326.
Antti-Veikko I. Rosti, Evgeny Matusov, Jason Smith,
Necip Fazil Ayan, Jason Eisner, Damianos Karakos,
Sanjeev Khudanpur, Gregor Leusch, Zhifei Li, Spy-
ros Matsoukas, Hermann Ney, Richard Schwartz, Bing
Zhang, and Jing Zheng. 2011. Confusion network de-
coding for MT system combination. In Joseph Olive,
Caitlin Christianson, and John McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation: DARPA Global Autonomous Language
Exploitation, pages 333?361. Springer.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. ICASSP.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy or
HTER? exploring different human judgments with a
tunable MT metric. In Proc. WMT, pages 259?268.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. ICCL, pages 836?841.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the JHU system combination scheme
for WMT 2011. In Proc. WMT, pages 171?176.
199
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 304?311,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2012
Matthias Huck, Stephan Peitz, Markus Freitag, Malte Nuhn and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems developed at
RWTH Aachen University for the translation
task of the NAACL 2012 Seventh Workshop on
Statistical Machine Translation (WMT 2012).
We participated in the evaluation campaign
for the French-English and German-English
language pairs in both translation directions.
Both hierarchical and phrase-based SMT sys-
tems are applied. A number of different tech-
niques are evaluated, including an insertion
model, different lexical smoothing methods,
a discriminative reordering extension for the
hierarchical system, reverse translation, and
system combination. By application of these
methods we achieve considerable improve-
ments over the respective baseline systems.
1 Introduction
For the WMT 2012 shared translation task1 RWTH
utilized state-of-the-art phrase-based and hierarchi-
cal translation systems as well as an in-house sys-
tem combination framework. We give a survey of
these systems and the basic methods they implement
in Section 2. For both the French-English (Sec-
tion 3) and the German-English (Section 4) language
pair, we investigate several different advanced tech-
niques. We concentrate on specific research direc-
tions for each of the translation tasks and present the
respective techniques along with the empirical re-
sults they yield: For the French?English task (Sec-
tion 3.1), we apply a standard phrase-based system.
1http://www.statmt.org/wmt12/
translation-task.html
For the English?French task (Section 3.2), we aug-
ment a hierarchical phrase-based setup with a num-
ber of enhancements like an insertion model, dif-
ferent lexical smoothing methods, and a discrimina-
tive reordering extension. For the German?English
(Section 4.3) and English?German (Section 4.4)
tasks, we utilize morpho-syntactic analysis to pre-
process the data (Section 4.1) and employ sys-
tem combination to produce a consensus hypothesis
from normal and reverse translations (Section 4.2) of
phrase-based and hierarchical phrase-based setups.
2 Translation Systems
2.1 Phrase-Based System
The phrase-based translation (PBT) system used
in this work is an in-house implementation of the
state-of-the-art decoder described in (Zens and Ney,
2008). We use the standard set of models with
phrase translation probabilities and lexical smooth-
ing in both directions, word and phrase penalty,
distance-based distortion model, an n-gram target
language model and three binary count features. The
parameter weights are optimized with minimum er-
ror rate training (MERT) (Och, 2003).
2.2 Hierarchical Phrase-Based System
For our hierarchical phrase-based translation
(HPBT) setups, we employ the open source trans-
lation toolkit Jane (Vilar et al, 2010; Stein et
al., 2011; Vilar et al, 2012), which has been
developed at RWTH and is freely available for
non-commercial use. In hierarchical phrase-based
translation (Chiang, 2007), a weighted synchronous
context-free grammar is induced from parallel text.
304
In addition to contiguous lexical phrases, hierar-
chical phrases with up to two gaps are extracted.
The search is carried out with a parsing-based
procedure. The standard models integrated into our
Jane systems are: phrase translation probabilities
and lexical smoothing probabilities in both trans-
lation directions, word and phrase penalty, binary
features marking hierarchical phrases, glue rule,
and rules with non-terminals at the boundaries,
four binary count features, and an n-gram language
model. Optional additional models comprise IBM
model 1 (Brown et al, 1993), discriminative word
lexicon (DWL) models and triplet lexicon models
(Mauser et al, 2009), discriminative reordering ex-
tensions (Huck et al, 2011a), insertion and deletion
models (Huck and Ney, 2012), and several syntactic
enhancements like preference grammars (Stein
et al, 2010) and string-to-dependency features
(Peter et al, 2011). We utilize the cube pruning
algorithm (Huang and Chiang, 2007) for decoding
and optimize the model weights with MERT.
2.3 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses generated
with different translation engines. The basic concept
of RWTH?s approach to machine translation system
combination is described in (Matusov et al, 2006;
Matusov et al, 2008). This approach includes an
enhanced alignment and reordering framework. A
lattice is built from the input hypotheses. The trans-
lation with the best score within the lattice according
to a couple of statistical models is selected as con-
sensus translation.
2.4 Other Tools and Techniques
We employ GIZA++ (Och and Ney, 2003) to train
word alignments. The two trained alignments are
heuristically merged to obtain a symmetrized word
alignment for phrase extraction. All language mod-
els (LMs) are created with the SRILM toolkit (Stol-
cke, 2002) and are standard 4-gram LMs with in-
terpolated modified Kneser-Ney smoothing (Kneser
and Ney, 1995; Chen and Goodman, 1998). We
evaluate in truecase, using the BLEU (Papineni et al,
2002) and TER (Snover et al, 2006) measures.
French English
EP + NC Sentences 2.1M
Running Words 63.3M 57.6M
Vocabulary 147.8K 128.5K
Singletons 5.4K 5.1K
+ 109 Sentences 22.9M
Running Words 728.6M 624.0M
Vocabulary 1.7M 1.7M
Singletons 0.8M 0.8M
+ UN Sentences 35.4M
Running Words 1 113.5M 956.4M
Vocabulary 1.9M 2.0M
Singletons 0.9M 1.0M
Table 1: Corpus statistics of the preprocessed French-
English parallel training data. EP denotes Europarl, NC
denotes News Commentary. In the data, numerical quan-
tities have been replaced by a single category symbol.
3 French-English Setups
We trained phrase-based translation systems for
French?English and hierarchical phrase-based
translation systems for English?French. Corpus
statistics for the French-English parallel data are
given in Table 1. The LMs are 4-grams trained on
the provided resources for the respective language
(Europarl, News Commentary, UN, 109, and mono-
lingual News Crawl language model training data).2
For French?English we also investigate a smaller
English LM on Europarl and News Commentary
data only. For English?French we experiment with
additional target-side data from the LDC French Gi-
gaword Second Edition (LDC2009T28), which is an
archive of newswire text data that has been acquired
over several years by the LDC.3 The LDC French
Gigaword v2 is permitted for constrained submis-
sions in the WMT shared translation task. As a de-
velopment set for MERT, we use newstest2009 in all
setups.
3.1 Experimental Results French?English
For the French?English task, the phrase-based
SMT system (PBT) is set up using the standard mod-
els listed in Section 2.1. We vary the training data
we use to train the system and compare the results.
2The parallel 109 corpus is often also referred to as WMT
Giga French-English release 2.
3http://www.ldc.upenn.edu
305
newstest2008 newstest2009 newstest2010 newstest2011
French?English BLEU TER BLEU TER BLEU TER BLEU TER
PBT baseline 20.3 63.8 23.0 60.0 23.2 59.1 24.7 57.3
+ LM: +109+UN 22.5 61.4 26.2 57.3 26.6 56.1 27.7 54.5
+ TM: +109 23.3 60.8 27.6 56.2 27.6 55.4 29.1 53.4
Table 2: Results for the French?English task (truecase). newstest2009 is used as development set. BLEU and TER
are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011
English?French BLEU TER BLEU TER BLEU TER BLEU TER
HPBT 20.9 66.0 23.6 62.5 25.1 60.2 27.4 57.6
+ 109 and UN 22.5 63.2 25.4 59.8 27.0 57.1 29.9 53.9
+ LDC Gigaword v2 23.0 63.0 25.9 59.4 27.3 56.9 29.6 54.1
+ insertion model 23.0 62.9 26.1 59.2 27.2 56.8 30.0 53.7
+ noisy-or lexical scores 23.2 62.5 26.1 59.0 27.6 56.4 30.2 53.4
+ DWL 23.3 62.5 26.2 58.9 27.9 55.9 30.4 53.2
+ IBM-1 23.4 62.3 26.2 58.8 28.0 55.7 30.4 53.1
+ discrim. RO 23.5 62.2 26.7 58.5 28.1 55.9 30.8 52.8
Table 3: Results for the English?French task (truecase). newstest2009 is used as development set. BLEU and TER
are given in percentage.
It should be noted that these setups do not use any
English LDC Gigaword data for LM training at all.
Our baseline system uses the Europarl and News
Commentary data for training LM and phrase table.
Corpus statistics are shown in the ?EP+NC? section
of Table 1. This results in a performance of 24.7
points BLEU on newstest2011. Then we add the 109
as well as UN data and more monolingual English
data from the News Crawl corpus to the data used
for training the language model. This system ob-
tains a score of 27.7 points BLEU on newstest2011.
Our final system uses Europarl, News Commentary,
109 and UN data and News Crawl monolingual data
for LM training and the Europarl, News Commen-
tary and 109 data (Table 1) for phrase table training.
Using these data sets the system reaches 29.1 points
BLEU.
The experimental results are summarized in Ta-
ble 2.
3.2 Experimental Results English?French
For the English?French task, the baseline system is
a hierarchical phrase-based setup including the stan-
dard models as listed in Section 2.2, apart from the
binary count features. We limit the recursion depth
for hierarchical rules with a shallow-1 grammar (de
Gispert et al, 2010).
In a shallow-1 grammar, the generic non-terminal
X of the standard hierarchical approach is replaced
by two distinct non-terminals XH and XP . By
changing the left-hand sides of the rules, lexical
phrases are allowed to be derived from XP only, hi-
erarchical phrases from XH only. On all right-hand
sides of hierarchical rules, the X is replaced by XP .
Gaps within hierarchical phrases can thus solely be
filled with purely lexicalized phrases, but not a sec-
ond time with hierarchical phrases. The initial rule
is substituted with
S ? ?XP?0,XP?0?
S ? ?XH?0,XH?0? ,
(1)
and the glue rule is substituted with
S ? ?S?0XP?1, S?0XP?1?
S ? ?S?0XH?1, S?0XH?1? .
(2)
The main benefit of a restriction of the recursion
depth is a gain in decoding efficiency, thus allow-
ing us to set up systems more rapidly and to explore
more model combinations and more system config-
urations.
306
The experimental results for English?French are
given in Table 3. Starting from the shallow hi-
erarchical baseline setup on Europarl and News
Commentary parallel data only (but Europarl, News
Commentary, 109, UN, and News Crawl data for LM
training), we are able to improve translation qual-
ity considerably by first adopting more parallel (109
and UN) and monolingual (French LDC Gigaword
v2) training resources and then employing several
different models that are not included in the baseline
already. We proceed with individual descriptions of
the methods we use and report their respective effect
in BLEU on the test sets.
109 and UN (up to +2.5 points BLEU) While the
amount of provided parallel data from Europarl
and News Commentary sources is rather lim-
ited (around 2M sentence pairs in total), the
UN and the 109 corpus each provide a substan-
tial collection of further training material. By
appending both corpora, we end up at roughly
35M parallel sentences (cf. Table 1). We utilize
this full amount of data in our system, but ex-
tract a phrase table with only lexical (i.e. non-
hierarchical) phrases from the full parallel data.
We add it as a second phrase table to the base-
line system, with a binary feature that enables
the system to reward or penalize the application
of phrases from this table.
LDC Gigaword v2 (up to +0.5 points BLEU)
The LDC French Gigaword Second Edition
(LDC2009T28) provides some more monolin-
gual French resources. We include a total of
28.2M sentences from both the AFP and APW
collections in our LM training data.
insertion model (up to +0.4 points BLEU) We add
an insertion model to the log-linear model com-
bination. This model is designed as a means to
avoid the omission of content words in the hy-
potheses. It is implemented as a phrase-level
feature function which counts the number of in-
serted words. We apply the model in source-to-
target and target-to-source direction. A target-
side word is considered inserted based on lexi-
cal probabilities with the words on the foreign
language side of the phrase, and vice versa for
a source-side word. As thresholds, we compute
individual arithmetic averages for each word
from the vocabulary (Huck and Ney, 2012).
noisy-or lexical scores (up to +0.4 points BLEU) In
our baseline system, the tNorm(?) lexical scor-
ing variant as described in (Huck et al, 2011a)
is employed with a relative frequency (RF) lex-
icon model for phrase table smoothing. The
single-word based translation probabilities of
the RF lexicon model are extracted from word-
aligned parallel training data, in the fashion
of (Koehn et al, 2003). We exchange the base-
line lexical scoring with a noisy-or (Zens and
Ney, 2004) lexical scoring variant tNoisyOr(?).
DWL (up to +0.3 points BLEU) We augment
our system with phrase-level lexical scores
from discriminative word lexicon (DWL) mod-
els (Mauser et al, 2009; Huck et al, 2011a)
in both source-to-target and target-to-source di-
rection. The DWLs are trained on News Com-
mentary data only.
IBM-1 (up to +0.1 points BLEU) On News Com-
mentary and Europarl data, we train IBM
model-1 (Brown et al, 1993) lexicons in both
translation directions and also use them to com-
pute phrase-level scores.
discrim. RO (up to +0.4 points BLEU) The modi-
fication of the grammar to a shallow-1 version
restricts the search space of the decoder and is
convenient to prevent overgeneration. In order
not to be too restrictive, we reintroduce more
flexibility into the search process by extending
the grammar with specific reordering rules
XP ? ?XP?0XP?1,XP?1XP?0?
XP ? ?XP?0XP?1,XP?0XP?1? .
(3)
The upper rule in Equation (3) is a swap rule
that allows adjacent lexical phrases to be trans-
posed, the lower rule is added for symmetry
reasons, in particular because sequences as-
sembled with these rules are allowed to fill gaps
within hierarchical phrases. Note that we apply
a length constraint of 10 to the number of ter-
minals spanned by an XP . We introduce two
binary indicator features, one for each of the
two rules in Equation (3). In addition to adding
307
German English
Sentences 2.0M
Running Words 55.3M 55.7M
Vocabulary 191.6K 129.0K
Singletons 75.5K 51.8K
Table 4: Corpus statistics of the preprocessed German-
English parallel training data (Europarl and News Com-
mentary). In the data, numerical quantities have been re-
placed by a single category symbol.
these rules, a discriminatively trained lexical-
ized reordering model is applied (Huck et al,
2012).
4 German-English Setups
We trained phrase-based and hierarchical transla-
tion systems for both translation directions of the
German-English language pair. Corpus statistics for
German-English can be found in Table 4. The lan-
guage models are 4-grams trained on the respective
target side of the bilingual data as well as on the pro-
vided News Crawl corpus. For the English language
model the 109 French-English, UN and LDC Giga-
word Fourth Edition corpora are used additionally.
For the 109 French-English, UN and LDC Gigaword
corpora we apply the data selection technique de-
scribed in (Moore and Lewis, 2010). We examine
two different language models, one with LDC data
and one without. All German?English systems are
optimized on newstest2010. For English?German,
we use newstest2009 as development set. The news-
test2011 set is used as test set and the scores for new-
stest2008 are included for completeness.
4.1 Morpho-Syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the German text
is preprocessed by splitting German compound
words with the frequency-based method described in
(Koehn and Knight, 2003). To further reduce trans-
lation complexity of PBT, we employ the long-range
part-of-speech based reordering rules proposed by
Popovic? and Ney (2006).
4.2 Reverse Translation
For reverse translations we need to change the word
order of the bilingual corpus. For example, if we re-
verse both source and target language, the original
training example ?der Hund mag die Katze . ? the
dog likes the cat .? is converted into a new training
example ?. Katze die mag Hund der? . cat the likes
dog the?. We call this type of modification of source
or target language reversion. A system trained of
this data is called reverse. This modification changes
the corpora and hence the language model and align-
ment training produce different results.
4.3 Experimental Results German?English
Our results for the German?English task are shown
in Table 5. For this task, we apply the idea of reverse
translation for both the phrase-based and the hierar-
chical approach. It seems that the reversed systems
perform slightly worse. However, when we em-
ploy system combination using both reverse trans-
lation setups (PBT reverse and HPBT reverse) and
both baseline setups (PBT baseline and HPBT base-
line), the translation quality is improved by up to 0.4
points in BLEU and 1.0 points TER compared to the
best single system.
The addition of LDC Gigaword corpora (+GW)
to the language model training data of the baseline
setups shows improvements in both BLEU and TER.
Furthermore, with the system combination including
these setups, we are able to report an improvement
of up to 0.7 points BLEU and 1.0 points TER over the
best single setup. Compared to the system combina-
tion based on systems which are not using the LDC
Gigaword corpora, we gain 0.3 points in BLEU and
0.4 points in TER.
4.4 Experimental Results English?German
Our results for the English?German task are shown
in Table 6. For this task, we first compare sys-
tems using one, two or three language models of
different parts of the data. The language model
for systems with only one language model is cre-
ated with all monolingual and parallel data. A lan-
guage model with all monolingual data and a lan-
guage model with all parallel data is created for the
systems with two language models. For the systems
with three language models, we also split the parallel
data in two parts consisting of either only Europarl
data or only News Commentary data. For PBT the
system with two language models performs best for
all test sets. Further, we apply the idea of reverse
308
newstest2008 newstest2009 newstest2010 newstest2011
German?English BLEU TER BLEU TER BLEU TER BLEU TER
PBT baseline 21.1 62.3 20.8 61.4 23.7 59.3 21.3 61.3
PBT reverse 20.8 62.4 20.6 61.5 23.6 59.2 21.2 61.2
HPBT baseline 21.3 62.5 20.9 61.7 23.9 59.4 21.3 61.6
HPBT reverse 21.2 63.5 20.9 62.0 23.6 59.2 21.4 61.9
system combination (secondary) 21.5 61.6 21.2 60.6 24.3 58.3 21.7 60.3
PBT baseline +GW 21.5 61.9 21.2 61.1 24.0 59.0 21.3 61.4
PBT reverse 20.8 62.4 20.6 61.5 23.6 59.2 21.2 61.2
HPBT baseline +GW 21.6 62.3 21.3 61.3 24.0 59.4 21.6 61.5
HPBT reverse 21.2 63.5 20.9 62.0 23.6 59.2 21.4 61.9
system combination (primary) 21.9 61.2 21.4 60.5 24.7 58.0 21.9 60.2
Table 5: Results for the German?English task (truecase). +GW denotes the usage of LDC Gigaword data for the
language model, newstest2010 serves as development set. BLEU and TER are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011
English?German BLEU TER BLEU TER BLEU TER BLEU TER
PBT baseline 1 LM 14.6 71.7 14.8 70.8 15.8 66.9 15.3 70.0
PBT baseline 2 LM (*) 14.9 70.9 14.9 70.4 16.0 66.3 15.4 69.5
PBT baseline 3 LM 14.8 71.5 14.9 70.5 16.0 66.7 15.1 70.1
PBT reverse 2 LM (*) 14.9 71.4 15.1 70.2 15.9 66.5 15.0 69.7
HPBT baseline 2 LM (*) 15.1 71.8 15.3 71.1 16.2 67.4 15.4 70.3
HPBT baseline 2 LM opt on 4bleu-ter 15.2 68.4 15.0 67.7 15.9 64.6 15.1 67.1
HPBT reverse 2 LM (*) 15.4 71.3 15.3 70.7 16.7 66.9 15.5 70.1
syscombi of (*) 15.6 69.2 15.4 68.9 16.5 65.0 15.6 68.0
Table 6: Results for the English?German task (truecase). newstest2009 is used as development set. BLEU and TER
are given in percentage.
translation for both the phrase-based and the hier-
archical approach. The PBT reverse 2 LM systems
perform slightly worse compared to PBT baseline 2
LM. The HPBT reverse 2 LM performs better com-
pared to HPBT baseline 2 LM. When we employ
system combination using both reverse translation
setups (PBT reverse 2 LM and HPBT reverse 2 LM)
and both baseline setups (PBT baseline 2 LM and
HPBT baseline 2 LM), the translation quality is im-
proved by up to 0.2 points in BLEU and 2.1 points in
TER compared to the best single system.
5 Conclusion
For the participation in the WMT 2012 shared trans-
lation task, RWTH experimented with both phrase-
based and hierarchical translation systems. Several
different techniques were evaluated and yielded con-
siderable improvements over the respective base-
line systems as well as over our last year?s setups
(Huck et al, 2011b). Among these techniques are
an insertion model, the noisy-or lexical scoring vari-
ant, additional phrase-level lexical scores from IBM
model 1 and discriminative word lexicon models, a
discriminative reordering extension for hierarchical
translation, reverse translation, and system combi-
nation.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The Mathemat-
309
ics of Statistical Machine Translation: Parameter Es-
timation. Computational Linguistics, 19(2):263?311,
June.
Stanley F. Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, Mas-
sachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. Compu-
tational Linguistics, 36(3):505?533.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 144?151,
Prague, Czech Republic, June.
Matthias Huck and Hermann Ney. 2012. Insertion
and Deletion Models for Statistical Machine Trans-
lation. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics -
Human Language Technologies conference, Montreal,
Canada, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and Her-
mann Ney. 2011a. Lexicon Models for Hierarchical
Phrase-Based Machine Translation. In International
Workshop on Spoken Language Translation, pages
191?198, San Francisco, California, USA, December.
Matthias Huck, Joern Wuebker, Christoph Schmidt,
Markus Freitag, Stephan Peitz, Daniel Stein, Arnaud
Dagnelies, Saab Mansour, Gregor Leusch, and Her-
mann Ney. 2011b. The RWTH Aachen Machine
Translation System for WMT 2011. In EMNLP 2011
Sixth Workshop on Statistical Machine Translation,
pages 405?412, Edinburgh, UK, July.
Matthias Huck, Stephan Peitz, Markus Freitag, and Her-
mann Ney. 2012. Discriminative Reordering Exten-
sions for Hierarchical Phrase-Based Machine Transla-
tion. In 16th Annual Conference of the European As-
sociation for Machine Translation, Trento, Italy, May.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, volume 1, pages 181?
184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical Meth-
ods for Compound Splitting. In Proceedings of Euro-
pean Chapter of the ACL (EACL 2009), pages 187?
194.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of the Human Language Technology Conf.
(HLT-NAACL), pages 127?133, Edmonton, Canada,
May/June.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Multi-
ple Machine Translation Systems Using Enhanced Hy-
potheses Alignment. In Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
the Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 210?218, Singapore, Au-
gust.
Robert C. Moore and William Lewis. 2010. Intelli-
gent Selection of Language Model Training Data. In
ACL (Short Papers), pages 220?224, Uppsala, Swe-
den, July.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Jan-Thorsten Peter, Matthias Huck, Hermann Ney, and
Daniel Stein. 2011. Soft String-to-Dependency Hier-
archical Machine Translation. In International Work-
shop on Spoken Language Translation, pages 246?
253, San Francisco, California, USA, December.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Translation.
In International Conference on Language Resources
and Evaluation, pages 1278?1283, Genoa, Italy, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
310
Association for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA, Au-
gust.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Features
for Hierarchical Machine Translation. In Conf. of the
Association for Machine Translation in the Americas
(AMTA), Denver, Colorado, USA, October/November.
Daniel Stein, David Vilar, Stephan Peitz, Markus Fre-
itag, Matthias Huck, and Hermann Ney. 2011. A
Guide to Jane, an Open Source Hierarchical Trans-
lation Toolkit. The Prague Bulletin of Mathematical
Linguistics, (95):5?18, April.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf. on
Speech and Language Processing (ICSLP), volume 2,
pages 901?904, Denver, Colorado, USA, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open source hierarchical transla-
tion, extended with reordering and lexicon models. In
ACL 2010 Joint Fifth Workshop on Statistical Machine
Translation and Metrics MATR, pages 262?270, Upp-
sala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2012. Jane: an advanced freely available hier-
archical machine translation toolkit. Machine Trans-
lation, pages 1?20. http://dx.doi.org/10.1007/s10590-
011-9120-y.
Richard Zens and Hermann Ney. 2004. Improve-
ments in Phrase-Based Statistical Machine Transla-
tion. In Proc. Human Language Technology Conf. /
North American Chapter of the Association for Com-
putational Linguistics Annual Meeting (HLT-NAACL),
pages 257?264, Boston, Massachusetts, USA, May.
Richard Zens and Hermann Ney. 2008. Improvements
in Dynamic Programming Beam Search for Phrase-
based Statistical Machine Translation. In Interna-
tional Workshop on Spoken Language Translation,
pages 195?205, Honolulu, Hawaii, USA, October.
311
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 450?459,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Phrase Model Training for Statistical Machine Translation with
Word Lattices of Preprocessing Alternatives
Joern Wuebker and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
{wuebker,ney}@cs.rwth-aachen.de
Abstract
In statistical machine translation, word lattices
are used to represent the ambiguities in the
preprocessing of the source sentence, such as
word segmentation for Chinese or morpholog-
ical analysis for German. Several approaches
have been proposed to define the probability
of different paths through the lattice with ex-
ternal tools like word segmenters, or by apply-
ing indicator features. We introduce a novel
lattice design, which explicitly distinguishes
between different preprocessing alternatives
for the source sentence. It allows us to make
use of specific features for each preprocess-
ing type and to lexicalize the choice of lattice
path directly in the phrase translation model.
We argue that forced alignment training can
be used to learn lattice path and phrase trans-
lation model simultaneously. On the news-
commentary portion of the German?English
WMT 2011 task we can show moderate im-
provements of up to 0.6% BLEU over a state-
of-the-art baseline system.
1 Introduction
The application of statistical machine translation
(SMT) to word lattice input was first introduced for
the translation of speech recognition output. Rather
than translating the single-best transcription, the
speech recognition system encodes all possible tran-
scriptions and their probabilities within a word lat-
tice, which is then used as input for the machine
translation system (Ney, 1999; Matusov et al, 2005;
Bertoldi et al, 2007).
Since then, several groups have adapted this ap-
proach to model ambiguities in representing the
source language with lattices and were able to re-
port improvements over their respective baselines.
The probabilities for different paths through the lat-
tice are usually modeled by assigning probabilities
to arcs as a byproduct of the lattice generation or
by defining binary indicator features. Applying the
first method only makes sense if the lattice construc-
tion is based on a single, comprehensive probabilis-
tic method, like a Chinese word segmentation model
as is used by Xu et al (2005). In applications like
the one described by Dyer et al (2008), where sev-
eral different segmenters for Chinese are combined
to create the lattice, this is not possible. Also, our
intuition suggests that simply defining indicator fea-
tures for each of the segmenters may not be ideal, if
we assume that there is not a single best segmenter,
but rather that for different data instances a different
one works best.
In this paper, we propose to model the lattice
path implicitly within the phrase translation model.
We introduce a novel lattice design, which explic-
itly distinguishes between different ways of prepro-
cessing the source sentence. It enables us to define
specific binary features for each preprocessing type
and to learn lexicalized lattice path probabilities and
the phrase translation model simultaneously with a
forced alignment training procedure.
To train the phrase translation model, most state-
of-the-art SMT systems rely on heuristic phrase ex-
traction from a word-aligned training corpus. Us-
ing a modified version of the translation decoder to
450
force-align the training data provides a more consis-
tent way of training. Wuebker et al (2010) intro-
duce a leave-one-out method which can overcome
the over-fitting effects inherent to this training pro-
cedure (DeNero et al, 2006). The authors report this
to yield both a significantly smaller phrase table and
higher translation quality than the heuristic phrase
extraction.
We argue that applying forced alignment train-
ing helps to exploit the full potential of word lattice
translation. The effects of the training on lattice in-
put are analyzed on the news-commentary portion of
the German?English WMT 2011 task. Our results
show moderate improvements of up to 0.6% BLEU
over the baseline.
This paper is organized as follows: We will re-
view related work in Section 2, describe the decoder
in Section 3 and present our novel lattice design in
Section 4. The phrase training algorithm is intro-
duced in Section 5, and Section 6 gives a detailed
account of the experimental setup and discusses the
results. Finally, our findings are summarized in Sec-
tion 7.
2 Related work
Word lattices have been used for machine transla-
tion of text in a variety of ways. Dyer et al (2008)
use it to encode different Chinese word segmenta-
tions or Arabic morphological analyses. For the
phrase-based model, they report improvements of
up to 0.9% BLEU for Chinese?English and 1.6%
BLEU for Arabic?English over the respective sin-
gle best word segmented and morphologically ana-
lyzed source. These results are achieved without an
explicit way of modeling probabilities for different
paths within the lattice. The training of the phrase
model is done by generating one version of the train-
ing data for each segmentation method or morpho-
logical analysis. The word alignments are trained
separately, and are then concatenated for phrase ex-
traction. Our work differs from (Dyer et al, 2008) in
that we explicitly distinguish the various preprocess-
ing types in the lattice so that we can define specific
path features and lexicalize the lattice path probabil-
ities within the phrase model.
In (Xu et al, 2005) the probability of a segmen-
tation, as given by the Chinese word segmentation
model, and the translation model are combined into
a global decision rule. This is done by weighting
the lattice edges with a source language model. The
authors report an improvement of 1.5% BLEU over
translation of the single best segmentation with a
phrase-based SMT system.
Dyer (2009) introduces a maximum entropy
model for compound word splitting, which he
uses to create word lattices for translation in-
put. He shows improvements in German-English,
Hungarian-English and Turkish-English over state-
of-the-art baselines.
For the German?English WMT 2010 task, Hard-
meier et al (2010) encode the morphological re-
duction and decompounding of the German surface
form as alternative paths in a word lattice. They
show improvements of roughly 0.5% BLEU over the
baseline. A binary indicator feature is added to the
log-linear framework for the alternative edges. Ad-
ditionally, they integrate long-range reorderings of
the source sentence into the lattice, in order to match
the word order of the English language, which yields
another improvement of up to 0.5% BLEU.
Niehues and Kolss (2009) also use lattices to en-
code different alternative reorderings of the source
sentence which results in an improvement of
2.0% BLEU over the baseline on the WMT 2008
German?English task.
Onishi et al (2010) propose a method of modeling
paraphrases in a lattice. They perform experiments
on the English?Japanese and English?Chinese
IWSLT 2007 tasks, and report improvements of
1.1% and 0.9% BLEU over a paraphrase-augmented
baseline.
Schroeder et al (2009) generalize usage of lattices
to combine input from multiple source languages.
Factored translation models (Koehn and Hoang,
2007) approach the idea of integrating annotation
into translation from the opposite direction. Where
lattices allow the decoder to choose a single level of
annotation as translation source, factored models are
designed to jointly translate several annotation lev-
els (factors). Thus, they are more suited to integrate
low-level annotation that by itself does not provide
sufficient information for accurate translation, like
451
part-of-speech tags, gender, etc. On the other hand,
they require a one-to-one correspondence between
the factors, which makes them unsuitable to model
word segmentation or decompounding.
The problem of performing real training for the
phrase translation model has been approached in a
number of different ways in the past. The first one,
to the best of our knowledge, was the joint proba-
bility phrase model presented by Marcu and Wong
(2002). It is shown to perform slightly inferior to
the standard heuristic phrase extraction from word
alignments by Koehn et al (2003).
A detailed analysis of the inherent over-fitting
problems when training a generative phrase model
with the EM algorithm is given in (DeNero et al,
2006). These findings are in principle confirmed by
Moore and Quirk (2007) who, however, can show
that their model is less sensitive to reducing compu-
tational resources than the state-of-the-art heuristic.
Birch et al (2006) and DeNero et al (2008)
present alternative training procedures for the joint
model introduced by Marcu and Wong (2002),
which are shown to improve its performance.
In (Mylonakis and Sima?an, 2008) a phrase model
is described, whose training procedure is designed
to counteract the inherent over-fitting problem by in-
cluding prior probabilities based on Inversion Trans-
duction Grammar and smoothing as learning objec-
tive. It yields a small improvement over a standard
phrase-based baseline.
Ferrer and Juan (2009) present an approach,
where the phrase model is trained by a semi-hidden
Markov model.
In this work we apply the phrase training method
introduced by Wuebker et al (2010), where the
phrase translation model of a fully competitive SMT
system is trained in a generative way. The key to
avoiding the over-fitting effects described by DeN-
ero et al (2006) is their novel leave-one-out proce-
dure.
3 Decoding
3.1 Phrase-based translation
We use a standard phrase-based decoder which
searches for the best translation e?I?1 for a given input
sentence fJ1 by maximizing the posterior probability
e?I?1 = argmax
I,eI1
Pr(eI1|f
J
1 ). (1)
Generalizing the noisy channel approach (Brown
et al, 1990) and making use of the maximum ap-
proximation (Viterbi), the decoder directly mod-
els the posterior probability by a log-linear combi-
nation of several feature functions hm(eI1, s
K
1 , f
J
1 )
weighted with scaling factors ?m, which results in
the decision rule (Och and Ney, 2004)
e?I?1 = argmax
I,eI1,K,s
K
1
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
. (2)
Here, sK1 denotes the segmentation of e
I
1 and f
J
1
into K phrase-pairs and their alignment. The fea-
tures used are the language model, phrase translation
and lexical smoothing models in both directions,
word and phrase penalty and a simple distance-
based reordering penalty.
3.2 Lattice translation
For lattice input we generalize Equation 2 to also
maximize over the set of sentences F(L) encoded
by a given source word lattice L:
e?I?1 =
argmax
I,eI1,K,s
K
1 ,f
J
1 ?F(L)
{
M?
m=1
?mhm(e
I
1, s
K
1 , f
J
1 )
}
(3)
Note that in this formulation there are no prob-
abilities assigned to the arcs of L. We define ad-
ditional binary indicator features hm and lexical-
ize path probabilities by encoding the path into the
word identities. To translate lattice input, we adapt
the standard phrase-based decoding algorithm as de-
scribed in (Matusov et al, 2008). The decoder keeps
track of the covered slots, which represent the topo-
logical order of the nodes, rather than the covered
words. When expanding a hypothesis, it has to be
verified that there is no overlap between the covered
nodes and that a path exists from start to goal node,
452
Pakistans Streitkr?fte - wiederholt Ziel von Selbstmordattent?tern - sind demoralisiert .
Pakistan Streit Kraft
Streit Kr?fte
selbst Mord Attentat
Selbst Mord Attent?tern
sein demoralisierenLemma
Surface
Compound
0 1
2
2
3 4 5 6 7
8 9
8 9
10 11 12 13 14
Pakistans Streitkr?fte - wiederholt Ziel von Selbstmordattent?tern - sind demoralisiert .
Pakistan Streit Kraft
Streit Kr?fte
selbst Mord Attentat
Selbst Mord Attent?tern
sein demoralisierenLemma
Surface
Compound
0 1
2
2
3 4 5 6 7
8 9
8 9
10 11 12 13 14
Pakistans
-
wiederholt Ziel von
- 
sind demoralisiert
.
- - .
wiederholt Ziel von
Figure 1: Top: Slim lattice. Bottom: Full lattice. The sentence is taken from the training data. The three layers
Surface, Compound and Lemma are separated with dashed lines. Nodes are labeled with slot information. Slots are
ordered horizontally, layers vertically.
which passes through all covered nodes. In prac-
tice, when considering a possible expansion cover-
ing slots j?, ..., j?? with start and end states n? and
n??, we make sure that the following two conditions
hold:
? n? is reachable from the lattice node that cor-
responds to the nearest already covered slot to
the left of j?.
? The node that corresponds to the nearest al-
ready covered slot to the right of j?? is reachable
from n??.
It was noted by Dyer et al (2008) that the stan-
dard distance-based reordering model needs to be
redefined for lattice input. We define the distortion
penalty as the difference in slot number. Using the
shortest path within the lattice is reported to have
better performance in (Dyer et al, 2008), however
we did not implement it due to time constraints.
4 Lattice design
We construct lattices from three different prepro-
cessing variants of the German source side of the
data. The surface form is the standard tokenization
of the source sentence. The word compounds are
produced by the frequency-based compound split-
ting method described in (Koehn and Knight, 2003),
applied to the tokenized sentence. From the com-
pound split sentence we produce the lemma of the
German words by applying the TreeTagger toolkit
(Schmid, 1995). Each of the different preprocess-
ing variants is assigned a separate layer within the
lattice. For the phrase model, word identities are de-
fined by both the word and its layer. In this way, the
phrase model can assign different scores to phrases
in different layers, allowing it to guide the search to-
wards a specific layer for each word. In practice, this
is done by annotating words with a unique identifier
for each layer. For example, the word sein from the
lemmatized layer will be written as LEM.sein within
both the data and the phrase table. If sein appears in
the surface form layer, it will be written as SUR.sein
and is treated as a different word. SUR is the identi-
fier for the compound layer.
We experiment with two different lattice designs.
In the full lattice, all three layers are included for
each source word in surface form. The slim lattice
only includes arcs for the lemma layer if it differs
from the surface form, and only includes arcs for the
compound layer if it differs from both surface form
and lemma. Figure 1 shows a slim and a full lattice
for the same training data sentence.
For each layer, we add two indicator features to
the phrase table: One binary feature which is set
to 1 if the phrase is taken from this layer, and one
feature which is equal to the number of words from
this layer. This results in six additional feature func-
tions, whose weights are optimized jointly with the
standard features described in Section 3.1. We will
453
denote them as layer features.
5 Phrase translation model training
To train the phrase model, we use a modified version
of the translation decoder to force-align the training
data. We apply the method described in (Wuebker et
al., 2010), but with word lattices on the source side.
To avoid over-fitting, we use their cross-validation
technique, which is described as a low-cost alterna-
tive to leave-one-out. For cross-validation we seg-
ment the training data into batches containing 5000
sentences. For each batch, the phrase table is up-
dated by reducing the phrase counts by the local
counts produced by the current batch in the previ-
ous training iteration. For the first iteration, we per-
form the standard phrase extraction separately for
each batch to produce the local counts. Singleton
phrases are assigned the probability ?(|f? |+|e?|) with
the source and target phrase lengths |f? | and |e?| and
fixed ? = e?5 (length-based leave-one-out). Sen-
tences for which the decoder is not able to find an
alignment are discarded (about 4% for our experi-
ments). To estimate the probabilities of the phrase
model, we count all phrase pairs used in training
within an n-best list (equally weighted). The trans-
lation probability for a phrase pair (f? , e?) is estimated
as
pFA(e?|f?) =
CFA(f? , e?)
Cmon(f?)
, (4)
where CFA(f? , e?) is the count of the phrase pair
(f? , e?) in the force-aligned training data. In order to
learn the lattice path along with the phrase transla-
tion probabilities, we make the following modifica-
tion to the original formulation in (Wuebker et al,
2010). The denominator Cmon(f?) is the count of
f? in the target side of the training data, rather than
using the real marginal counts. This means that it
is independent of the training procedure, and can be
computed by ignoring one side of the training data
and performing a simple n-gram count on the other.
In this way the model learns to prefer lattice paths
which are taken more often in training. For exam-
ple, if the phrase (LEM.Streit LEM.Kraft) is used
to align the sentence from Figure 1, Cmon(f?) will
be increased for f? = (SUR.Streitkr?fte) and f? =
(SPL.Streit SPL.Kr?fte) without affecting their joint
counts. This leads to a lower probability for these
phrases, which is not the case if marginal counts
are used. Note that on the source side we have one
training corpus for each lattice layer, which are con-
catenated to compute Cmon(f?). The size of the n-
best lists used in this work is fixed to 20000. Using
smaller n-best lists was tested, but seems to have dis-
advantages for the application to lattices. After re-
estimation of the phrase model, the feature weights
are optimized again.
In order to achieve a good coverage of the train-
ing data, we allow the decoder to generate backoff
phrases. If a source phrase consisting of a single
word does not have any translation candidates left
after the bilingual phrase matching, one phrase pair
is added to the translation candidates for each word
in the target sentence. The backoff phrases are as-
signed a fixed probability ? = e?12. Note that this
is smaller than the probability the phrase would be
assigned according to the length-based leave-one-
out heuristic, leading to a preference of singleton
phrases over backoff phrases. The lexical smooth-
ing models are applied in the usual way to both sin-
gleton and backoff phrases. After each sentence, the
backoff phrases are discarded. However, in the ex-
periments for this work, introducing backoff phrases
only increases the coverage from 95.8% to 96.2% of
the sentences.
6 Experimental evaluation
6.1 Experimental setup
Our experiments are carried out on the news-
commentary portion of the German?English data
provided for the EMNLP 2011 Sixth Workshop
on Statistical Machine Translation (WMT 2011).?
We use newstest2008 as development set and
newstest2009 and newstest2010 as unseen
test sets. The word alignments are produced with
GIZA++ (Och and Ney, 2003). To optimize the log-
linear parameters, the Downhill-Simplex algorithm
(Nelder and Mead, 1965) is applied with BLEU (Pa-
pineni et al, 2002) as optimization criterion. The
?http://www.statmt.org/wmt11
454
German English
Surface Compound Lemma
Train Sentences 136K
Running Words 3.4M 3.5M 3.3M
Vocabulary Size 118K 81K 52K 57K
newstest2008 Sentences 2051
Running Words 48K 50K 50K
Vocabulary Size 10.3K 9.7K 7.3K 8.1K
OOVs (Running Words) 3041 2092 1742 2070
newstest2009 Sentences 2525
Running Words 63K 66K 66K
Vocabulary Size 12.2K 11.4K 8.4K 9.4K
OOVs (Running Words) 4058 2885 2400 2729
newstest2010 Sentences 2489
Running Words 62K 65K 62K
Vocabulary Size 12.3K 11.4K 8.5K 9.2K
OOVs (Running Words) 4357 2952 2565 2742
Table 1: Corpus Statistics for the WMT 2011 news-commentary data, the development set (newstest2008) and
the two test sets (newstest2009, newstest2010). For the source side, three different preprocessing alternatives
are included: Surface, Compound and Lemma.
language model is a standard 4-gram LM with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998) produced with the SRILM toolkit (Stolcke,
2002). It is trained on the full bilingual data and
parts of the monolingual News crawl corpus pro-
vided for WMT 2011. Numbers are replaced with
a single category symbol in a separate preprocess-
ing step and we apply the long-range part-of-speech
based reordering rules proposed by (Popovic? and
Ney, 2006).
Table 1 shows statistics for the bilingual training
data and the development and test corpora for the
three different German preprocessing alternatives.
It can be seen that both compound splitting and
lemmatization reduce the vocabulary size and num-
ber of out-of-vocabulary (OOV) words. Results are
measured in BLEU and TER (Snover et al, 2006),
which are computed case-insensitively with a single
reference.
6.2 Baseline experiments
To get an overview over the effects of the different
preprocessing alternatives for the German source,
we built three baseline systems, one for each prepro-
cessing type. The phrase tables are extracted heuris-
tically in the standard way from the word-aligned
training data. Additionally, we performed phrase
training for the compound split version of the data.
The results are shown in Table 2. When moving
from the Surface to the Compound layer, we observe
improvements of up to 1.0% in BLEU and 1.1% in
TER. Reducing the morphological richness further
(Lemma) leads to a clear performance drop. Appli-
cation of phrase training on the compound split data
yields a small degradation in TER on all data sets and
in BLEU on newstest2010. We assume that this
is due to the small size of the training data and its
heterogeneity, which makes it hard for the decoder
to find good phrase alignments.
6.3 Lattice experiments: Heuristic extraction
We generated both slim and full lattices for all data
sets. Similar to (Dyer et al, 2008), we concate-
nate the three training data sets and their word align-
ments to extract the phrases. Note that this only pro-
duces single-layer phrases. It can be seen in Table
2 that without the application of layer features the
slim lattice slightly outperforms the full lattice. In-
455
newstest2008 newstest2009 newstest2010
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
Baseline Surface 19.5 64.6 18.6 64.4 20.6 62.8
Compounds 20.5 63.5 19.1 63.5 21.1 61.9
FA Compounds 20.5 63.9 19.1 63.8 20.9 62.3
Lemma 19.2 65.4 18.2 65.2 19.9 63.9
Slim Lattice without layer feat. 19.9 64.4 18.9 64.1 20.8 62.6
(heuristic) with layer feat. 20.5 63.8 19.4 63.9 21.0 62.4
Full Lattice without layer feat. 19.8 64.6 18.7 64.2 20.6 62.8
(heuristic) with layer feat. 20.4 64.0 19.5 63.8 21.3 62.3
Full Lattice without layer feat. 20.0 64.3 19.3 64.1 20.8 62.6
(FA w/o layer feat.) with layer feat. 20.2 64.3 19.1 64.2 20.7 62.8
Full Lattice without layer feat. 20.5 63.7 19.5 63.6 21.3 62.1
(FA w/ layer feat.) with layer feat. 20.7 63.6 19.7 63.4 21.4 61.8
Table 2: Results on the German-English WMT 2011 data. Scores are computed case-insensitively for BLEU [%]
and TER [%]. We evaluate performance of the baseline systems, one for each of the three different encodings, with
both slim and full lattices using heuristic phrase extraction and with full lattices using forced alignment phrase model
training (FA). All lattice systems are evaluated with and without layer features. The best scores in each column are in
boldface, statistically significant improvement over the Compounds baseline is marked with blue color.
troducing layer features boosts the performance for
both lattice types. However, the performance in-
crease is considerably larger for the full lattice sys-
tems, which now outperform the slim lattice systems
on newstest2009 and newstest2010. Com-
pared to the Compounds baseline, the full lattice
system with layer features shows a small improve-
ment of up to 0.4% BLEU on newstest2009 and
newstest2010, but a degradation in TER.
6.4 Lattice experiments: Phrase training
The experiments on phrase training are setup as fol-
lows. The phrase table is initialized with the stan-
dard extraction and is identical to the one used for
the experiments in Section 6.3. The log-linear scal-
ing factors used in training are the optimized param-
eters on the corresponding lattice, also taken from
the experiments described in Section 6.3. The forced
alignment procedure was run for one iteration. Fur-
ther iterations were tested, but did not give any im-
provements.
The phrase training was performed on the full lat-
tice design. The reason for this is that we want the
system to learn all possible phrases. Even if there is
no difference in wording between the layers in train-
ing, the additional phrases could be useful for un-
seen test data. The training was performed both with
and without layer features. The resulting systems
were also optimized with and without layer features,
resulting in four different setups.
From the results in Table 2 it is clear that phrase
training without layer features does not have the
desired effect. Even if we apply layer features to
the system trained without them, we do not reach
the performance of the best standard lattice system.
We conclude that, without these indicator features,
the standard lattice system does not produce good
phrase alignments.
When the layer features are applied for both train-
ing and translation, we observe improvements of up
to 0.2% in BLEU and 0.5% in TER over the corre-
sponding standard lattice system. The gap between
the systems with and without layer features is much
smaller than for the heuristically trained lattices.
This indicates that our goal of encoding the best lat-
tice path directly in the phrase model was at least
partially achieved. However, in order to exceed the
performance of our state-of-the-art baseline on both
measures, the layer features are still needed within
the phrase training procedure and for translation. Al-
456
source Das Warten hat gedauert mehr als NUM Minuten, was im Fall einer Stra?e, wo
werden erwartet NUM Menschen, ist unverst?ndlich.
reference The wait lasted more than NUM minutes, something incomprehensible for a race
where you expect more than NUM people.
lattice (heuristic) The wait has taken more than NUM minutes, which in the case of a street, where
NUM people are expected to be, can?t understand it.
lattice (FA) The wait has taken more than NUM minutes, which in the case of a street, where
expected NUM people, is incomprehensible.
Figure 2: Example sentence from the newstest2009 data set. The faulty phrase in the heuristic lattice translation
is marked in boldface.
together, our phrase trained lattice approach outper-
forms the state-of-the-art baseline on all three data
sets by up to 0.6% BLEU. On newstest2009,
this result is statistically significant with 95% confi-
dence according to the bootstrap resampling method
described by Koehn (2004).
For a direct comparison between the heuristic and
phrase-trained full lattice systems, we manually in-
spected the optimized log-linear parameter values
for the layer features. We observe that for the stan-
dard lattices, paths through the lemmatized layer are
heavily penalized. In the phrase trained lattice setup,
the penalty is much smaller. As a result, the num-
ber of words from the Lemma layer used for transla-
tion of the newstest2009 data set is increased by
49% from 1828 to 2715 words. However, a manual
inspection of the translations reveals that the main
improvement seems to come from a better choice
of phrases from the Compound layer. More specif-
ically, the used phrases tend to be shorter ? the av-
erage phrase length of Compound layer phrases is
1.5 words for both the baseline and the heuristic lat-
tice system. In the phrase trained lattice system, it
is 1.3 words. An example is given in Figure 2. We
focus on the end of the sentence, where the heuris-
tic system uses the rather disfluent phrase (ist unver-
st?ndlich. # can?t understand it.), whereas the forced
alignment trained system applies the three phrases
(ist # is), (unverst?ndlich # incomprehensible) and
(. # .).
This effect can be explained by the leave-one-out
procedure. As lemmatized phrases usually map to
several phrases in the other layers, their count is gen-
erally higher. Application of leave-one-out, which
reduces the counts of all phrases extracted from the
current sentence by a fixed value, therefore has a
stronger penalizing effect on Surface and Compound
layer phrases. In the extreme case, phrases which are
singletons in the Compound layer are unlikely to be
used at all in training, if the corresponding phrase
in the Lemma layer has a higher count. While this
rarely leads to the competing lemmatized phrases
being used in free translation, it allows for shorter,
more general phrases from the more expressive lay-
ers to be applied. Indeed, the ?bad? phrase (ist unver-
st?ndlich. # can?t understand it.) from the example
in Figure 2 is a singleton.
7 Conclusion and future work
In this work we apply a forced alignment phrase
training technique to input word lattices in SMT for
the first time. The goal of encoding better lattice
path probabilities directly into the phrase model was
at least partially successful. The proposed method
outperforms our baseline by up to 0.6% BLEU. To
achieve this, we presented a novel lattice design,
which distinguishes between different layers, for
which we can define separate indicator features. Al-
though these layer features are still necessary for the
final system to improve over state-of-the-art perfor-
mance, they are less important than in the heuristi-
cally trained setup.
One advantage of our approach is its adaptability
to a variety of scenarios. In future work, we plan
to apply it to additional language pairs. Arabic and
Chinese on the source side, where the layers could
represent different word segmentations, seem a nat-
ural choice. We also hope to be able to leverage
larger training data sets. As a natural extension we
plan to allow learning of cross-layer phrases. Fur-
457
ther, applying this framework to lattices modeling
different reorderings could be an interesting direc-
tion.
Acknowledgments
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French State
agency for innovation, and also partially funded by
the European Union under the FP7 project T4ME
Net, Contract No. 249119.
References
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
translation by confusion network decoding. In Pro-
ceedings of ICASSP 2007, pages 1297?1300, Hon-
olulu, Hawaii, April.
Alexandra Birch, Chris Callison-Burch, Miles Osborne,
and Philipp Koehn. 2006. Constraining the phrase-
based, joint probability statistical translation model. In
Proceedings of the Workshop on Statistical Machine
Translation, pages 154?157, Jun.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16:79?85, June.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Computer Science
Group, Harvard University, Aug.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why Generative Phrase Models Underperform
Surface Heuristics. In Proceedings of the Workshop
on Statistical Machine Translation, pages 31?38, New
York City, June.
John DeNero, Alexandre Buchard-C?t?, and Dan Klein.
2008. Sampling Alignment Structure under a Bayesian
Translation Model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314?323, Honolulu, October.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generaliz-
ing word lattice translation. In Proceedings of Annual
Meeting of the Association for Computational Linguis-
tics, pages 1012?1020, Columbus, Ohio, June.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceedings
of the 2009 Annual Conference of the North Ameri-
can Chapter of the ACL, pages 406?414, Boulder, Col-
orado, June.
Jes?s-Andr?s Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-markov approach to machine trans-
lation. In Proceedings of European Association for
Machine Translation (EAMT), Barcelona, Spain, May.
European Association for Machine Translation.
C. Hardmeier, A. Bisazza, and M. Federico. 2010. FBK
at WMT 2010: Word Lattices for Morphological Re-
duction and Chunk-based Reordering. In Proceedings
of the Joint 5th Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 88?92, Uppsala, Swe-
den, July.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology - Volume 1, pages
48?54, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. pages 388?395,
Barcelona, Spain, July.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2002), July.
E. Matusov, H. Ney, and R. Schl?ter. 2005. Phrase-
based translation of speech recognizer word lattices us-
ing loglinear model combination. In Proceedings of
the IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU), pages 110?115, San Juan,
Puerto Rico.
Evgeny Matusov, Bj?rn Hoffmeister, and Hermann Ney.
2008. ASR Word Lattice Translation with Exhaustive
Reordering is Possible. In Interspeech, pages 2342?
2345, Brisbane, Australia, September.
Robert C. Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 112?119, Prague, June.
Markos Mylonakis and Khalil Sima?an. 2008. Phrase
Translation Probabilities with ITG Priors and Smooth-
ing as Learning Objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630?639, Honolulu, October.
458
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
H. Ney. 1999. Speech translation: Coupling of recog-
nition and translation. In Proceedings of IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP), pages 517?520, Phoenix, Ari-
zona, USA, March.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
T. Onishi, M. Utiyama, and E. Sumita. 2010. Paraphrase
lattice for statistical machine translation. In Proceed-
ings of the ACL 2010 Conference Short Papers, pages
1?5, Uppsala, Sweden, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting on Association for Computational Lin-
guistics, pages 311?318, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
M. Popovic? and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In Inter-
national Conference on Language Resources and Eval-
uation, pages 1278?1283.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50, Dublin,
Ireland, March.
Josh Schroeder, Trevor Cohn, and Philipp Koehn. 2009.
Word lattices for multi-source translation. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL, pages 719?727, Athens, Greece.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In
Proc. of AMTA, pages 223?231, Aug.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901 ? 904, Denver, Col-
orado, USA, September.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of the
Assoc. for Computational Linguistics, pages 475?484,
Uppsala, Sweden, July.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated chinese word segmentation in
statistical machine translation. In International Work-
shop on Spoken Language Translation, pages 141?147,
Pittsburgh, PA, USA, October.
459
Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 29?38,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
A Performance Study of Cube Pruning for Large-Scale Hierarchical
Machine Translation
Matthias Huck1 and David Vilar2 and Markus Freitag1 and Hermann Ney1
1 Human Language Technology and Pattern 2 DFKI GmbH
Recognition Group, RWTH Aachen University Alt-Moabit 91c
D-52056 Aachen, Germany D-10559 Berlin, Germany
<surname>@cs.rwth-aachen.de david.vilar@dfki.de
Abstract
In this paper, we empirically investigate the
impact of critical configuration parameters in
the popular cube pruning algorithm for decod-
ing in hierarchical statistical machine transla-
tion. Specifically, we study how the choice
of the k-best generation size affects trans-
lation quality and resource requirements in
hierarchical search. We furthermore exam-
ine the influence of two different granular-
ities of hypothesis recombination. Our ex-
periments are conducted on the large-scale
Chinese?English and Arabic?English NIST
translation tasks. Besides standard hierarchi-
cal grammars, we also explore search with re-
stricted recursion depth of hierarchical rules
based on shallow-1 grammars.
1 Introduction
Cube pruning (Chiang, 2007) is a widely used
search strategy in state-of-the-art hierarchical de-
coders. Some alternatives and extensions to the
classical algorithm as proposed by David Chiang
have been presented in the literature since, e.g. cube
growing (Huang and Chiang, 2007), lattice-based
hierarchical translation (Iglesias et al, 2009b; de
Gispert et al, 2010), and source cardinality syn-
chronous cube pruning (Vilar and Ney, 2012). Stan-
dard cube pruning remains the commonly adopted
decoding procedure in hierarchical machine transla-
tion research at the moment, though. The algorithm
has meanwhile been implemented in many publicly
available toolkits, as for example in Moses (Koehn
et al, 2007; Hoang et al, 2009), Joshua (Li et
al., 2009a), Jane (Vilar et al, 2010), cdec (Dyer et
al., 2010), Kriya (Sankaran et al, 2012), and Niu-
Trans (Xiao et al, 2012). While the plain hierar-
chical approach to machine translation (MT) is only
formally syntax-based, cube pruning can also be uti-
lized for decoding with syntactically or semantically
enhanced models, for instance those by Venugopal
et al (2009), Shen et al (2010), Xie et al (2011),
Almaghout et al (2012), Li et al (2012), Williams
and Koehn (2012), or Baker et al (2010).
Here, we look into the following key aspects of hi-
erarchical phrase-based translation with cube prun-
ing:
? Deep vs. shallow grammar.
? k-best generation size.
? Hypothesis recombination scheme.
We conduct a comparative study of all combinations
of these three factors in hierarchical decoding and
present detailed experimental analyses with respect
to translation quality and search efficiency. We fo-
cus on two tasks which are of particular interest to
the research community: the Chinese?English and
Arabic?English NIST OpenMT translation tasks.
The paper is structured as follows: We briefly out-
line some important related work in the following
section. We subsequently give a summary of the
grammars used in hierarchical phrase-based trans-
lation, including a presentation of the difference be-
tween a deep and a shallow-1 grammar (Section 3).
Essential aspects of hierarchical search with the
cube pruning algorithm are explained in Section 4.
We show how the k-best generation size is defined
(we apply the limit without counting recombined
29
candidates), and we present the two different hy-
pothesis recombination schemes (recombination T
and recombination LM). Our empirical investiga-
tions and findings constitute the major part of this
work: In Section 5, we first accurately describe our
setup, then conduct a number of comparative exper-
iments with varied parameters on the two translation
tasks, and finally analyze and discuss the results. We
conclude the paper in Section 6.
2 Related Work
Hierarchical phrase-based translation (HPBT) was
first proposed by Chiang (2005). Chiang also in-
troduced the cube pruning algorithm for hierarchical
search (Chiang, 2007). It is basically an adaptation
of one of the k-best parsing algorithms by Huang
and Chiang (2005). Good descriptions of the cube
pruning implementation in the Joshua decoder have
been provided by Li and Khudanpur (2008) and Li
et al (2009b). Xu and Koehn (2012) implemented
hierarchical search with the cube growing algorithm
in Moses and compared its performance to Moses?
cube pruning implementation. Heafield et al re-
cently developed techniques to speed up hierarchical
search by means of an improved language model in-
tegration (Heafield et al, 2011; Heafield et al, 2012;
Heafield et al, 2013).
3 Probabilistic SCFGs for HPBT
In hierarchical phrase-based translation, a proba-
bilistic synchronous context-free grammar (SCFG)
is induced from a bilingual text. In addition to con-
tinuous lexical phrases, hierarchical phrases with
usually up to two gaps are extracted from the word-
aligned parallel training data.
Deep grammar. The non-terminal set of a stan-
dard hierarchical grammar comprises two symbols
which are shared by source and target: the initial
symbol S and one generic non-terminal symbol X .
Extracted rules of a standard hierarchical grammar
are of the form X ? ??, ?,? ? where ??, ?? is a
bilingual phrase pair that may contain X , i.e. ? ?
({X } ? VF )+ and ? ? ({X } ? VE)+, where VF
and VE are the source and target vocabulary, respec-
tively. The ? relation denotes a one-to-one corre-
spondence between the non-terminals in ? and in ?.
A non-lexicalized initial rule and a special glue rule
complete the grammar. We denote standard hierar-
chical grammars as deep grammars here.
Shallow-1 grammar. Iglesias et al (2009a) pro-
pose a limitation of the recursion depth for hierar-
chical rules with shallow grammars. In a shallow-1
grammar, the generic non-terminal X of the stan-
dard hierarchical approach is replaced by two dis-
tinct non-terminals XH and XP . By changing the
left-hand sides of the rules, lexical phrases are al-
lowed to be derived from XP only, hierarchical
phrases from XH only. On all right-hand sides of
hierarchical rules, the X is replaced by XP . Gaps
within hierarchical phrases can thus be filled with
continuous lexical phrases only, not with hierarchi-
cal phrases. The initial and glue rules are adjusted
accordingly.
4 Hierarchical Search with Cube Pruning
Hierarchical search is typically carried out with a
parsing-based procedure. The parsing algorithm is
extended to handle translation candidates and to in-
corporate language model scores via cube pruning.
The cube pruning algorithm. Cube pruning op-
erates on a hypergraph which represents the whole
parsing space. This hypergraph is built employ-
ing a customized version of the CYK+ parsing al-
gorithm (Chappelier and Rajman, 1998). Given
the hypergraph, cube pruning expands at most k
derivations at each hypernode.1 The pseudocode
of the k-best generation step of the cube pruning
algorithm is shown in Figure 1. This function is
called in bottom-up topological order for all hy-
pernodes. A heap of active derivations A is main-
tained. A initially contains the first-best derivations
for each incoming hyperedge (line 1). Active deriva-
tions are processed in a loop (line 3) until a limit k
is reached or A is empty. If a candidate deriva-
tion d is recombinable, the RECOMBINE auxiliary
function recombines it and returns true; otherwise
(for non-recombinable candidates) RECOMBINE re-
turns false. Non-recombinable candidates are ap-
pended to the list D of k-best derivations (line 6).
This list will be sorted before the function terminates
1The hypergraph on which cube pruning operates can be
constructed based on other techniques, such as tree automata,
but CYK+ parsing is the dominant approach.
30
(line 8). The PUSHSUCC auxiliary function (line 7)
updates A with the next best derivations following d
along the hyperedge. PUSHSUCC determines the
cube order by processing adjacent derivations in a
specific sequence (of predecessor hypernodes along
the hyperedge and phrase translation options).2
k-best generation size. Candidate derivations are
generated by cube pruning best-first along the in-
coming hyperedges. A problem results from the lan-
guage model integration, though: As soon as lan-
guage model context is considered, monotonicity
properties of the derivation cost can no longer be
guaranteed. Thus, even for single-best translation,
k-best derivations are collected to a buffer in a beam
search manner and finally sorted according to their
cost. The k-best generation size is consequently a
crucial parameter to the cube pruning algorithm.
Hypothesis recombination. Partial hypotheses
with states that are indistinguishable from each other
are recombined during search. We define two no-
tions of when to consider two derivations as indis-
tinguishable, and thus when to recombine them:
Recombination T. The T recombination scheme
recombines derivations that produce identical
translations.
Recombination LM. The LM recombination
scheme recombines derivations with identical
language model context.
Recombination is conducted within the loop of
the k-best generation step of cube pruning. Re-
combined derivations do not increment the gener-
ation count; the k-best generation limit is thus ef-
fectively applied after recombination.3 In general,
more phrase translation candidates per hypernode
are being considered (and need to be rated with the
language model) in the recombination LM scheme
compared to the recombination T scheme. The more
partial hypotheses can be recombined, the more it-
erations of the inner code block of the k-best gen-
eration loop are possible. The same internal k-best
2See Vilar (2011) for the pseudocode of the PUSHSUCC
function and other details which are omitted here.
3Whether recombined derivations contribute to the genera-
tion count or not is a configuration decision (or implementa-
tion decision). Please note that some publicly available toolkits
count recombined derivations by default.
Input: a hypernode and the size k of the k-best list
Output: D, a list with the k-best derivations
1 let A? heap({(e,1|e|) | e ? incoming edges)})
2 let D ? [ ]
3 while |A| > 0 ? |D| < k do
4 d? pop(A)
5 if not RECOMBINE(D, d) then
6 D ? D ++ [d]
7 PUSHSUCC(d,A)
8 sort D
Figure 1: k-best generation with the cube pruning al-
gorithm.
generation size results in a larger search space for re-
combination LM. We will examine how the overall
number of loop iterations relates to the k-best gener-
ation limit. By measuring the number of derivations
as well as the number of recombination operations
on our test sets, we will be able to give an insight
into how large the fraction of recombinable candi-
dates is for different configurations.
5 Experiments
We conduct experiments which evaluate perfor-
mance in terms of both translation quality and
computational efficiency, i.e. translation speed and
memory consumption, for combinations of deep
or shallow-1 grammars with the two hypothesis
recombination schemes and an exhaustive range
of k-best generation size settings. Empirical re-
sults are presented on the Chinese?English and
Arabic?English 2008 NIST tasks (NIST, 2008).
5.1 Experimental Setup
We work with parallel training corpora of 3.0 M
Chinese?English sentence pairs (77.5 M Chinese /
81.0 M English running words after preprocessing)
and 2.5 M Arabic?English sentence pairs (54.3 M
Arabic / 55.3 M English running words after prepro-
cessing), respectively. Word alignments are created
by aligning the data in both directions with GIZA++
and symmetrizing the two trained alignments (Och
and Ney, 2003). When extracting phrases, we apply
several restrictions, in particular a maximum length
of ten on source and target side for lexical phrases,
a length limit of five on source and ten on target
side for hierarchical phrases (including non-terminal
symbols), and no more than two gaps per phrase.
31
Table 1: Data statistics for the test sets. Numbers have
been replaced by a special category symbol.
Chinese MT08 Arabic MT08
Sentences 1 357 1 360
Running words 34 463 45 095
Vocabulary 6 209 9 387
The decoder loads only the best translation options
per distinct source side with respect to the weighted
phrase-level model scores (100 for Chinese, 50 for
Arabic). The language models are 4-grams with
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998) which have been
trained with the SRILM toolkit (Stolcke, 2002).
During decoding, a maximum length constraint
of ten is applied to all non-terminals except the ini-
tial symbol S . Model weights are optimized with
MERT (Och, 2003) on 100-best lists. The op-
timized weights are obtained (separately for deep
and for shallow-1 grammars) with a k-best gen-
eration size of 1 000 for Chinese?English and of
500 for Arabic?English and kept for all setups.
We employ MT06 as development sets. Trans-
lation quality is measured in truecase with BLEU
(Papineni et al, 2002) on the MT08 test sets.
Data statistics for the preprocessed source sides of
both the Chinese?English MT08 test set and the
Arabic?English MT08 test set are given in Table 1.
Our translation experiments are conducted with
the open source translation toolkit Jane (Vilar et
al., 2010; Vilar et al, 2012). The core imple-
mentation of the toolkit is written in C++. We
compiled with GCC version 4.4.3 using its -O2
optimization flag. We employ the SRILM li-
braries to perform language model scoring in the
decoder. In binarized version, the language mod-
els have a size of 3.6G (Chinese?English) and 6.2G
(Arabic?English). Language models and phrase ta-
bles have been copied to the local hard disks of the
machines. In all experiments, the language model
is completely loaded beforehand. Loading time of
the language model and any other initialization steps
are not included in the measured translation time.
Phrase tables are in the Jane toolkit?s binarized for-
mat. The decoder initializes the prefix tree struc-
ture, required nodes get loaded from secondary stor-
age into main memory on demand, and the loaded
content is being cleared each time a new input sen-
tence is to be parsed. There is nearly no overhead
due to unused data in main memory. We do not
rely on memory mapping. Memory statistics are
with respect to virtual memory. The hardware was
equipped with RAM well beyond the requirements
of the tasks, and sufficient memory has been re-
served for the processes.
5.2 Experimental Results
Figures 2 and 3 depict how the Chinese?English
and Arabic?English setups behave in terms of
translation quality. The k-best generation size in
cube pruning is varied between 10 and 10 000.
The four graphs in each plot illustrate the results
with combinations of deep grammar and recombi-
nation scheme T, deep grammar and recombination
scheme LM, shallow grammar and recombination
scheme T, as well as shallow grammar and recom-
bination scheme LM. Figures 4 and 5 show the cor-
responding translation speed in words per second for
these settings. The maximum memory requirements
in gigabytes are given in Figures 6 and 7. In order
to visualize the trade-offs between translation qual-
ity and resource consumption somewhat better, we
plotted translation quality against time requirements
in Figures 8 and 9 and translation quality against
memory requirements in Figures 10 and 11. Transla-
tion quality and model score (averaged over all sen-
tences; higher is better) are nicely correlated for all
configurations, as can be concluded from Figures 12
through 15.
5.3 Discussion
Chinese?English. For Chinese?English trans-
lation, the system with deep grammar performs gen-
erally a bit better with respect to quality than the
shallow one, which accords with the findings of
other groups (de Gispert et al, 2010; Sankaran et
al., 2012). The LM recombination scheme yields
slightly better quality than the T scheme, and with
the shallow-1 grammar it outperforms the T scheme
at any given fixed amount of time or memory allo-
cation (Figures 8 and 10).
Shallow-1 translation is up to roughly 2.5 times
faster than translation with the deep grammar. How-
ever, the shallow-1 setups are considerably slowed
down at higher k-best sizes as well, while the ef-
fort pays off only very moderately. Overall, the
32
 23
 23.5
 24
 24.5
 25
 25.5
 10  100  1000  10000
BLE
U [%
]
k-best generation size
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 2: Chinese?English translation quality (truecase).
 42.5
 43
 43.5
 44
 44.5
 45
 10  100  1000  10000
BLE
U [%
]
k-best generation size
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 3: Arabic?English translation quality (truecase).
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10  100  1000  10000
wo
rds
 pe
r se
con
d
k-best generation size
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 4: Chinese?English translation speed.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 10  100  1000  10000
wo
rds
 pe
r se
con
d
k-best generation size
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 5: Arabic?English translation speed.
 0
 8
 16
 24
 32
 40
 10  100  1000  10000
giga
byte
s
k-best generation size
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 6: Chinese?English memory requirements.
 0
 8
 16
 24
 32
 40
 10  100  1000  10000
giga
byte
s
k-best generation size
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 7: Arabic?English memory requirements.
33
 23
 23.5
 24
 24.5
 25
 25.5
 0.125 0.25  0.5  1  2  4  8  16  32
BLE
U [%
]
seconds per word
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 8: Trade-off between translation quality and speed
for Chinese?English.
 42.5
 43
 43.5
 44
 44.5
 45
 0.125 0.25  0.5  1  2  4  8  16  32
BLE
U [%
]
seconds per word
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 9: Trade-off between translation quality and speed
for Arabic?English.
 23
 23.5
 24
 24.5
 25
 25.5
 8  16  32  64
BLE
U [%
]
gigabytes
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 10: Trade-off between translation quality and mem-
ory requirements for Chinese?English.
 42.5
 43
 43.5
 44
 44.5
 45
 16  32  64  128
BLE
U [%
]
gigabytes
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
shallow-1, recombination T
shallow-1, recombination LM
Figure 11: Trade-off between translation quality and mem-
ory requirements for Arabic?English.
shallow-1 grammar at a k-best size between 100 and
1 000 seems to offer a good compromise of quality
and efficiency. Deep translation with k = 2000 and
the LM recombination scheme promises high qual-
ity translation, but note the rapid memory consump-
tion increase beyond k = 1000 with the deep gram-
mar. At k ? 1 000, memory consumption is not an
issue in both deep and shallow systems, but transla-
tion speed starts to drop at k > 100 already.
Arabic?English. Shallow-1 translation produces
competitive quality for Arabic?English translation
(de Gispert et al, 2010; Huck et al, 2011). The
LM recombination scheme boosts the BLEU scores
slightly. The systems with deep grammar are slowed
down strongly with every increase of the k-best size.
Their memory consumption likewise inflates early.
We actually stopped running experiments with deep
grammars for Arabic?English at k = 7000 for the
T recombination scheme, and at k = 700 for the LM
recombination scheme because 124G of memory did
not suffice any more for higher k-best sizes. The
memory consumption of the shallow systems stays
nearly constant across a large range of the surveyed
k-best sizes, but Figure 11 reveals a plateau where
more resources do not improve translation quality.
Increasing k from 100 to 2 000 in the shallow setup
with LM recombination provides half a BLEU point,
but reduces speed by a factor of more than 10.
34
 23
 23.5
 24
 24.5
 25
 25.5
-8.7 -8.65 -8.6 -8.55 -8.5 -8.45 -8.4
BLE
U [%
]
average model score
NIST Chinese-to-English (MT08)
deep, recombination Tdeep, recombination LM
Figure 12: Relation of translation quality and average
model score for Chinese?English (deep grammar).
 42.5
 43
 43.5
 44
 44.5
 45
-6.6 -6.5 -6.4 -6.3 -6.2 -6.1
BLE
U [%
]
average model score
NIST Arabic-to-English (MT08)
deep, recombination Tdeep, recombination LM
Figure 13: Relation of translation quality and average
model score for Arabic?English (deep grammar).
 23
 23.5
 24
 24.5
 25
 25.5
-9.4 -9.35 -9.3 -9.25 -9.2 -9.15 -9.1
BLE
U [%
]
average model score
NIST Chinese-to-English (MT08)
shallow-1, recombination T
shallow-1, recombination LM
Figure 14: Relation of translation quality and average
model score for Chinese?English (shallow-1 grammar).
 42.5
 43
 43.5
 44
 44.5
 45
-12.1 -12 -11.9 -11.8 -11.7 -11.6
BLE
U [%
]
average model score
NIST Arabic-to-English (MT08)
shallow-1, recombination T
shallow-1, recombination LM
Figure 15: Relation of translation quality and average
model score for Arabic?English (shallow-1 grammar).
Actual amount of derivations. We measured the
amount of hypernodes (Table 2), the amount of actu-
ally generated derivations after recombination, and
the amount of generated candidate derivations in-
cluding recombined ones?or, equivalently, loop it-
erations in the algorithm from Figure 1?for se-
lected limits k (Tables 3 and 4). The ratio of the
average amount of derivations per hypernode after
and before recombination remains consistently at
low values for all recombination T setups. For the
setups with LM recombination scheme, this recom-
bination factor rises with larger k, i.e. the fraction
of recombinable candidates increases. The increase
is remarkably pronounced for Arabic?English with
deep grammar. The steep slope of the recombina-
tion factor may be interpreted as an indicator for un-
desired overgeneration of the deep grammar on the
Arabic?English task.
6 Conclusion
We systematically studied three key aspects of hier-
archical phrase-based translation with cube pruning:
Deep vs. shallow-1 grammars, the k-best generation
size, and the hypothesis recombination scheme. In
a series of empirical experiments, we revealed the
trade-offs between translation quality and resource
requirements to a more fine-grained degree than this
is typically done in the literature.
35
Table 2: Average amount of hypernodes per sentence and average length of the preprocessed input sentences on the
NIST Chinese?English (MT08) and Arabic?English (MT08) tasks.
Chinese?English Arabic?English
deep shallow-1 deep shallow-1
avg. #hypernodes per sentence 480.5 200.7 896.4 308.4
avg. source sentence length 25.4 33.2
Table 3: Detailed statistics about the actual amount of derivations on the NIST Chinese?English task (MT08).
deep
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 10.0 11.7 1.17 10.0 18.2 1.82
100 99.9 120.1 1.20 99.9 275.8 2.76
1000 950.1 1142.3 1.20 950.1 4246.9 4.47
10000 9429.8 11262.8 1.19 9418.1 72008.4 7.65
shallow-1
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 9.7 11.3 1.17 9.6 13.6 1.41
100 90.8 105.2 1.16 90.4 168.6 1.86
1000 707.3 811.3 1.15 697.4 2143.4 3.07
10000 6478.1 7170.4 1.11 6202.8 34165.6 5.51
Table 4: Detailed statistics about the actual amount of derivations on the NIST Arabic?English task (MT08).
deep
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 10.0 18.3 1.83 10.0 71.5 7.15
100 98.0 177.4 1.81 98.0 1726.0 17.62
500 482.1 849.0 1.76 482.1 14622.1 30.33
1000 961.8 1675.0 1.74 ? ? ?
shallow-1
recombination T recombination LM
avg. #derivations avg. #derivations avg. #derivations avg. #derivations
per hypernode per hypernode per hypernode per hypernode
k (after recombination) (incl. recombined) factor (after recombination) (incl. recombined) factor
10 9.6 12.1 1.26 9.6 16.6 1.73
100 80.9 105.2 1.30 80.2 193.8 2.42
1000 690.1 902.1 1.31 672.1 2413.0 3.59
10000 5638.6 7149.5 1.27 5275.1 31283.6 5.93
36
Acknowledgments
This work was partly achieved as part of the Quaero
Programme, funded by OSEO, French State agency
for innovation. This material is also partly based
upon work supported by the DARPA BOLT project
under Contract No. HR0011-12-C-0015. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the
DARPA. The research leading to these results has
received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no 287658.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2012. Ex-
tending CCG-based Syntactic Constraints in Hierar-
chical Phrase-Based SMT. In Proc. of the Annual
Conf. of the European Assoc. for Machine Translation
(EAMT), pages 193?200, Trento, Italy, May.
Kathryn Baker, Michael Bloodgood, Chris Callison-
Burch, Bonnie Dorr, Nathaniel Filardo, Lori
Levin, Scott Miller, and Christine Piatko. 2010.
Semantically-Informed Syntactic Machine Transla-
tion: A Tree-Grafting Approach. In Proc. of the Conf.
of the Assoc. for Machine Translation in the Americas
(AMTA), Denver, CO, USA, October/November.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. In Proc. of the First Workshop on Tabulation in
Parsing and Deduction, pages 133?137, Paris, France,
April.
Stanley F. Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University, Cambridge, MA,
USA, August.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 263?270, Ann Arbor, MI,
USA, June.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. Compu-
tational Linguistics, 36(3):505?533.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning framework for
finite-state and context-free translation models. In
Proc. of the ACL 2010 System Demonstrations, pages
7?12, Uppsala, Sweden, July.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left Language
Model State for Syntactic Machine Translation. In
Proc. of the Int. Workshop on Spoken Language Trans-
lation (IWSLT), pages 183?190, San Francisco, CA,
USA, December.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language Model Rest Costs and Space-Efficient Stor-
age. In Proc. of the 2012 Joint Conf. on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL ?12, pages 1169?1178, Jeju Island, Korea,
July.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013.
Grouping Language Model Boundary Words to Speed
K-Best Extraction from Hypergraphs. In Proc. of the
Human Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics
(HLT-NAACL), Atlanta, GA, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchical,
and Syntax-Based Statistical Machine Translation. In
Proc. of the Int. Workshop on Spoken Language Trans-
lation (IWSLT), pages 152?159, Tokyo, Japan, Decem-
ber.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proc. of the 9th Int. Workshop on Parsing
Technologies, pages 53?64, October.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proc. of the Annual Meeting of the Assoc. for Com-
putational Linguistics (ACL), pages 144?151, Prague,
Czech Republic, June.
Matthias Huck, David Vilar, Daniel Stein, and Hermann
Ney. 2011. Advancements in Arabic-to-English Hier-
archical Machine Translation. In 15th Annual Confer-
ence of the European Association for Machine Trans-
lation, pages 273?280, Leuven, Belgium, May.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Rule Filtering by Pat-
tern for Efficient Hierarchical Translation. In Proc. of
the 12th Conf. of the Europ. Chapter of the Assoc. for
Computational Linguistics (EACL), pages 380?388,
Athens, Greece, March.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Hierarchical Phrase-
Based Translation with Weighted Finite State Trans-
37
ducers. In Proc. of the Human Language Technology
Conf. / North American Chapter of the Assoc. for Com-
putational Linguistics (HLT-NAACL), pages 433?441,
Boulder, CO, USA, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In
Proc. of the International Conf. on Acoustics, Speech,
and Signal Processing, volume 1, pages 181?184, De-
troit, MI, USA, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 177?180, Prague, Czech Republic, June.
Zhifei Li and Sanjeev Khudanpur. 2008. A Scalable
Decoder for Parsing-Based Machine Translation with
Equivalent Language Model State Maintenance. In
Proceedings of the Second Workshop on Syntax and
Structure in Statistical Translation, SSST ?08, pages
10?18, Columbus, OH, USA, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009a. Joshua: An Open
Source Toolkit for Parsing-Based Machine Transla-
tion. In Proc. of the Workshop on Statistical Machine
Translation (WMT), pages 135?139, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur, and
Wren Thornton. 2009b. Decoding in Joshua: Open
Source, Parsing-Based Machine Translation. The
Prague Bulletin of Mathematical Linguistics, (91):47?
56, January.
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van
Genabith. 2012. Using Syntactic Head Information in
Hierarchical Phrase-Based Translation. In Proc. of the
Workshop on Statistical Machine Translation (WMT),
pages 232?242, Montre?al, Canada, June.
NIST. 2008. Open Machine Translation 2008 Evalua-
tion. http://www.itl.nist.gov/iad/mig/
tests/mt/2008/.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the An-
nual Meeting of the Assoc. for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, USA, July.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya - An end-to-end Hierarchical Phrase-
based MT System. The Prague Bulletin of Mathemat-
ical Linguistics, (97):83?98, April.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Translation.
Computational Linguistics, 36(4):649?671, Decem-
ber.
Andreas Stolcke. 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, USA, September.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statisti-
cal Machine Translation. In Proc. of the Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics (HLT-
NAACL), pages 236?244, Boulder, CO, USA, June.
David Vilar and Hermann Ney. 2012. Cardinality
pruning and language model heuristics for hierarchi-
cal phrase-based translation. Machine Translation,
26(3):217?254, September.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2010. Jane: Open Source Hierarchical Transla-
tion, Extended with Reordering and Lexicon Models.
In Proc. of the Workshop on Statistical Machine Trans-
lation (WMT), pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Hermann
Ney. 2012. Jane: an advanced freely available hierar-
chical machine translation toolkit. Machine Transla-
tion, 26(3):197?216, September.
David Vilar. 2011. Investigations on Hierarchi-
cal Phrase-Based Machine Translation. Ph.D. the-
sis, RWTH Aachen University, Aachen, Germany,
November.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proc. of the Workshop on Statistical Machine Transla-
tion (WMT), pages 388?394, Montre?al, Canada, June.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012.
NiuTrans: An Open Source Toolkit for Phrase-based
and Syntax-based Machine Translation. In Proc. of
the ACL 2012 System Demonstrations, pages 19?24,
Jeju, Republic of Korea, July.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A Novel
Dependency-to-String Model for Statistical Machine
Translation. In Proc. of the Conf. on Empirical Meth-
ods for Natural Language Processing (EMNLP), pages
216?226, Edinburgh, Scotland, UK, July.
Wenduan Xu and Philipp Koehn. 2012. Extending Hiero
Decoding in Moses with Cube Growing. The Prague
Bulletin of Mathematical Linguistics, (98):133?142,
October.
38
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joint WMT 2013 Submission of the QUAERO Project
?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,
?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Quoc Khanh Do,
?Bianka Buschbeck, ?Tonio Wandmacher
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint submis-
sion of the QUAERO project for the
German?English translation task of the
ACL 2013 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2013).
The submission was a system combina-
tion of the output of four different transla-
tion systems provided by RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy (KIT), LIMSI-CNRS and SYSTRAN
Software, Inc. The translations were
joined using the RWTH?s system com-
bination approach. Experimental results
show improvements of up to 1.2 points in
BLEU and 1.2 points in TER compared to
the best single translation.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in ma-
chine translation is mainly assigned to the four
groups participating in this joint submission. The
aim of this submission was to show the quality of
a joint translation by combining the knowledge of
the four project partners. Each group develop and
maintain their own different machine translation
system. These single systems differ not only in
their general approach, but also in the preprocess-
ing of training and test data. To take advantage
of these differences of each translation system, we
combined all hypotheses of the different systems,
using the RWTH system combination approach.
This paper is structured as follows. First, the
different engines of all four groups are introduced.
In Section 3, the RWTH Aachen system combina-
tion approach is presented. Experiments with dif-
ferent system selections for system combination
are described in Section 4. This paper is concluded
in Section 5.
2 Translation Systems
For WMT 2013, each QUAERO partner trained
their systems on the parallel Europarl (EPPS),
News Commentary (NC) corpora and the web-
crawled corpus. All single systems were tuned on
the newstest2009 and newstest2010 development
set. The newstest2011 development set was used
to tune the system combination parameters. Fi-
nally, on newstest2012 the results of the different
system combination settings are compared. In this
Section, all four different translation engines are
presented.
2.1 RWTH Aachen Single System
For the WMT 2013 evaluation, RWTH utilized a
phrase-based decoder based on (Wuebker et al,
2012) which is part of RWTH?s open-source SMT
toolkit Jane 2.1 1. GIZA++ (Och and Ney, 2003)
was employed to train a word alignment, language
models have been created with the SRILM toolkit
(Stolcke, 2002).
After phrase pair extraction from the word-
aligned parallel corpus, the translation probabil-
ities are estimated by relative frequencies. The
standard feature set alo includes an n-gram lan-
guage model, phrase-level IBM-1 and word-,
phrase- and distortion-penalties, which are com-
bined in log-linear fashion. Furthermore, we used
an additional reordering model as described in
(Galley and Manning, 2008). By this model six
1http://www-i6.informatik.rwth-aachen.
de/jane/
185
additional feature are added to the log-linear com-
bination. The model weights are optimized with
standard Mert (Och, 2003a) on 200-best lists. The
optimization criterion is BLEU.
2.1.1 Preprocessing
In order to reduce the source vocabulary size trans-
lation, the German text was preprocessed by split-
ting German compound words with the frequency-
based method described in (Koehn and Knight,
2003). To further reduce translation complexity
for the phrase-based approach, we performed the
long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.2 Translation Model
We applied filtering and weighting for domain-
adaptation similarly to (Mansour et al, 2011) and
(Mansour and Ney, 2012). For filtering the bilin-
gual data, a combination of LM and IBM Model
1 scores was used. In addition, we performed
weighted phrase extraction by using a combined
LM and IBM Model 1 weight.
2.1.3 Language Model
During decoding a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl,
the 109 French-English, UN and LDC Gigaword
Fourth Edition corpora.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
The training data was preprocessed prior to the
training. Symbols such as quotes, dashes and
apostrophes are normalized. Then the first words
of each sentence are smart-cased. For the Ger-
man part of the training corpus, the hunspell2 lex-
icon was used, in order to learn a mapping from
old German spelling to new German writing rules.
Compound-splitting was also performed as de-
scribed in Koehn and Knight (2003). We also re-
moved very long sentences, empty lines, and sen-
tences which show big mismatch on the length.
2.2.2 Filtering
The web-crawled corpus was filtered using an
SVM classifier as described in (Mediani et al,
2011). The lexica used in this filtering task were
obtained from Giza alignments trained on the
2http://hunspell.sourceforge.net/
cleaner corpora, EPPS and NC. Assuming that this
corpus is very noisy, we biased our classifier more
towards precision than recall. This was realized
by giving higher number of false examples (80%
of the training data).
This filtering technique ruled out more than
38% of the corpus (the unfiltered corpus contains
around 2.4M pairs, 0.9M of which were rejected
in the filtering task).
2.2.3 System Overview
The in-house phrase-based decoder (Vogel, 2003)
is used to perform decoding. Optimization with
regard to the BLEU score is done using Minimum
Error Rate Training (MERT) as described in Venu-
gopal et al (2005).
2.2.4 Reordering Model
We applied part-of-speech (POS) based reordering
using probabilistic continuous (Rottmann and Vo-
gel, 2007) and discontinuous (Niehues and Kolss,
2009) rules. This was learned using POS tags gen-
erated by the TreeTagger (Schmid, 1994) for short
and long range reorderings respectively.
In addition to this POS-based reordering, we
also used tree-based reordering rules. Syntactic
parse trees of the whole training corpus and the
word alignment between source and target lan-
guage are used to learn rules on how to reorder the
constituents in a German source sentence to make
it match the English target sentence word order
better (Herrmann et al, 2013). The training corpus
was parsed by the Stanford parser (Rafferty and
Manning, 2008). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are en-
coded in a word lattice which is used as input to
the decoder.
Moreover, our reordering model was extended
so that it could include the features of lexicalized
reordering model. The reordering probabilities for
each phrase pair are stored as well as the origi-
nal position of each word in the lattice. During
the decoding, the reordering origin of the words
is checked along with its probability added as an
additional score.
2.2.5 Translation Models
The translation model uses the parallel data of
EPPS, NC, and the filtered web-crawled data. As
word alignment, we used the Discriminative Word
Alignment (DWA) as shown in (Niehues and Vo-
186
gel, 2008). The phrase pairs were extracted using
different source word order suggested by the POS-
based reordering models presented previously as
described in (Niehues et al, 2009).
In order to extend the context of source lan-
guage words, we applied a bilingual language
model (Niehues et al, 2011). A Discriminative
Word Lexicon (DWL) introduced in (Mauser et
al., 2009) was extended so that it could take the
source context also into the account. For this,
we used a bag-of-ngrams instead of representing
the source sentence as a bag-of-words. Filtering
based on counts was then applied to the features
for higher order n-grams. In addition to this, the
training examples were created differently so that
we only used the words that occur in the n-best list
but not in the reference as negative example.
2.2.6 Language Models
We build separate language models and combined
them prior to decoding. As word-token based
language models, one language model is built on
EPPS, NC, and giga corpus, while another one is
built using crawled data. We combined the LMs
linearly by minimizing the perplexity on the de-
velopment data. As a bilingual language model we
used the EPPS, NC, and the web-crawled data and
combined them. Furthermore, we use a 5-gram
cluster-based language model with 1,000 word
clusters, which was trained on the EPPS and NC
corpus. The word clusters were created using the
MKCLS algorithm.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine transla-
tion system based on bilingual n-gram3. In this
approach, the translation model relies on a spe-
cific decomposition of the joint probability of a
sentence pair using the n-gram assumption: a sen-
tence pair is decomposed into a sequence of bilin-
gual units called tuples, defining a joint segmen-
tation of the source and target. In the approach of
(Marin?o et al, 2006), this segmentation is a by-
product of source reordering which ultimately de-
rives from initial word and phrase alignments.
2.3.2 An overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using
3http://ncode.limsi.fr/
a n-gram model of (source,target) pairs (Casacu-
berta and Vidal, 2004). Training this model re-
quires to reorder source sentences so as to match
the target word order. This is performed by
a stochastic finite-state reordering model, which
uses part-of-speech information4 to generalize re-
ordering patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized re-
ordering models (Tillmann, 2004) aiming at pre-
dicting the orientation of the next translation unit;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which compensate for the system prefer-
ence for short translations. The four lexicon mod-
els are similar to the ones use in a standard phrase
based system: two scores correspond to the rel-
ative frequencies of the tuples and two lexical
weights estimated from the automatically gener-
ated word alignments. The weights associated to
feature functions are optimally combined using a
discriminative training framework (Och, 2003b).
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the
tuple extraction process. The resulting reordering
hypotheses are passed to the decoder in the form
of word lattices (Crego and Mario, 2006).
2.3.3 Continuous space translation models
One critical issue with standard n-gram translation
models is that the elementary units are bilingual
pairs, which means that the underlying vocabu-
lary can be quite large, even for small translation
tasks. Unfortunately, the parallel data available to
train these models are typically order of magni-
tudes smaller than the corresponding monolingual
corpora used to train target language models. It is
very likely then, that such models should face se-
vere estimation problems. In such setting, using
neural network language model techniques seem
all the more appropriate. For this study, we fol-
low the recommendations of Le et al (2012), who
propose to factor the joint probability of a sen-
tence pair by decomposing tuples in two (source
and target) parts, and further each part in words.
This yields a word factored translation model that
4Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
187
can be estimated in a continuous space using the
SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the com-
putational cost of computing n-gram probabilities.
The solution used here was to resort to a two pass
approach: the first pass uses a conventional back-
off n-gram model to produce a k-best list; in the
second pass, the k-best list is reordered using the
probabilities of m-gram SOUL translation models.
In the following experiments, we used a fixed con-
text size for SOUL of m= 10, and used k = 300.
2.3.4 Corpora and data pre-processing
All the parallel data allowed in the constrained
task are pooled together to create a single par-
allel corpus. This corpus is word-aligned using
MGIZA++5 with default settings. For the English
monolingual training data, we used the same setup
as last year6 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we also took advantage of our in-
house text processing tools for the tokenization
and detokenization steps (Dchelotte et al, 2008)
and our system is built in ?true-case?. As Ger-
man is morphologically more complex than En-
glish, the default policy which consists in treat-
ing each word form independently is plagued with
data sparsity, which is detrimental both at training
and decoding time. Thus, the German side was
normalized using a specific pre-processing scheme
(described in (Allauzen et al, 2010; Durgar El-
Kahlout and Yvon, 2010)), which notably aims at
reducing the lexical redundancy by (i) normalizing
the orthography, (ii) neutralizing most inflections
and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
In the past few years, SYSTRAN has been focus-
ing on the introduction of statistical approaches
to its rule-based backbone, leading to Hybrid Ma-
chine Translation.
The technique of Statistical Post-Editing
(Dugast et al, 2007) is used to automatically edit
the output of the rule-based system. A Statistical
Post-Editing (SPE) module is generated from a
bilingual corpus. It is basically a translation mod-
ule by itself, however it is trained on rule-based
5http://geek.kyloo.net/software
6The fifth edition of the English Gigaword
(LDC2011T07) was not used.
translations and reference data. It applies correc-
tions and adaptations learned from a phrase-based
5-gram language model. Using this two-step
process will implicitly keep long distance re-
lations and other constraints determined by the
rule-based system while significantly improving
phrasal fluency. It has the advantage that quality
improvements can be achieved with very little
but targeted bilingual data, thus significantly
reducing training time and increasing translation
performance.
The basic setup of the SPE component is identi-
cal to the one described in (Dugast et al, 2007).
A statistical translation model is trained on the
rule-based translation of the source and the target
side of the parallel corpus. Language models are
trained on each target half of the parallel corpora
and also on additional in-domain corpora. More-
over, the following measures - limiting unwanted
statistical effects - were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is sig-
nificantly reduced. In addition, entity trans-
lation is handled more reliably by the rule-
based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the
reference translation) is used to produce an
additional parallel corpus (whose target is
identical to the source). This was added to the
parallel text in order to improve word align-
ment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side
are also discarded.
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained on 2M
phrases from the news/europarl and Common-
Crawl corpora, provided as training data for WMT
2013. Weights for these separate models were
tuned by the Mert algorithm provided in the Moses
toolkit (Koehn et al, 2007), using the provided
news development set.
188
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
3 RWTH Aachen System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses gener-
ated with different translation engines. First, a
word to word alignment for the given single sys-
tem hypotheses is produced. In a second step a
confusion network is constructed. Then, the hy-
pothesis with the highest probability is extracted
from this confusion network. For the alignment
procedure, each of the given single systems gen-
erates one confusion network with its own as pri-
mary system. To this primary system all other hy-
potheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1. The final network for one source sen-
tence is the union of all confusion networks gen-
erated from the different primary systems. That
allows the system combination to select the word
order from different system outputs.
Before performing system combination, each
translation output was normalized by tokenization
and lowercasing. The output of the combination
was then truecased based on the original truecased
output.
The model weights of the system combination
are optimized with standard Mert (Och, 2003a)
on 100-best lists. We add one voting feature for
each single system to the log-linear framework of
the system combination. The voting feature fires
for each word the single system agrees on. More-
over, a word penalty, a language model trained on
the input hypotheses, a binary feature which pe-
nalizes word deletions in the confusion network
and a primary feature which marks the system
which provides the word order are combined in
this log-linear model. The optimization criterion
is 4BLEU-TER.
4 Experimental Results
In this year?s experiments, we tried to improve the
result of the system combination further by com-
bining single systems tuned on different develop-
Table 1: Comparison of single systems tuned on
newstest2009 and newstest2010. The results are
reported on newstest2012.
single systems tuned on newstest2012
newstest BLEU TER
KIT 2009 24.6 58.4
2010 24.6 58.6
LIMSI 2009 22.5 61.5
2010 22.6 59.8
SYSTRAN 2009 20.9 63.3
2010 21.2 62.2
RWTH 2009 23.7 60.8
2010 24.4 58.8
ment sets. The idea is to achieve a more stable
performance in terms of translation quality, if the
single systems are not optimized on the same data
set. In Table 1, the results of each provided single
system tuned on newstest2009 and newstest2010
are shown. For RWTH, LIMSI and SYSTRAN,
it seems that the performance of the single system
depends on the chosen tuning set. However, the
translation quality of the single systems provided
by KIT is stable.
As initial approach and for the final submis-
sion, we grouped single systems with dissimilar
approaches. Thus, KIT (phrase-based SMT) and
SYSTRAN (rule-based MT) tuned their system on
newstest2010, while RWTH (phrase-based SMT)
and LIMSI (n-gram) optimized on newstest2009.
To compare the impact of this approach, all pos-
sible combinations were checked (Table 2). How-
ever, it seems that the translation quality can not be
improved by this approach. For the test set (new-
stest2012), BLEU is steady around 25.6 points.
Even if the single system with lowest BLEU are
combined (KIT 2010, LIMSI 2009, SYSTRAN
2010, RWTH 2009), the translation quality in
terms of BLEU is comparable with the combina-
tion of the best single systems (KIT 2009, LIMSI
2010, SYSTRAN 2010, RWTH 2010). However,
we could gain 1.0 point in TER.
Due to the fact, that for the final submission the
initial grouping was available only, we kept this
189
Table 2: Comparison of different system combination settings. For each possible combination of systems
tuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluated
on newstest2012. The setting used for further experiments is set in boldface.
single systems system combinations
KIT LIMSI SYSTRAN RWTH newstest2011 newstest2012
tuned on newstest BLEU TER BLEU TER
2009 2009 2009 2009 24.6 58.0 25.6 56.8
2010 2010 2010 2010 24.2 58.1 25.6 57.7
2010 2009 2009 2009 24.5 57.9 25.7 57.4
2009 2010 2009 2009 24.4 58.3 25.7 57.0
2009 2009 2010 2009 24.5 57.9 25.6 57.0
2009 2009 2009 2010 24.5 58.0 25.6 56.8
2009 2010 2010 2010 24.1 57.5 25.4 56.4
2010 2009 2010 2010 24.3 57.6 25.6 56.9
2010 2010 2009 2010 24.2 58.0 25.6 57.3
2010 2010 2010 2009 24.3 57.9 25.5 57.6
2010 2010 2009 2009 24.4 58.1 25.6 57.5
2009 2009 2010 2010 24.4 57.8 25.5 56.6
2009 2010 2010 2009 24.4 58.2 25.5 57.0
2009 2010 2009 2010 24.2 57.8 25.5 56.8
2010 2009 2009 2010 24.4 57.9 25.6 57.4
2010 2009 2010 2009 24.4 57.7 25.6 57.4
Table 3: Results of the final submission (bold-
face) compared with best single system on new-
stest2012.
newstest2011 newstest2012
BLEU TER BLEU TER
best single 23.2 60.9 24.6 58.4
system comb. 24.4 57.7 25.6 57.4
+ IBM-1 24.6 58.1 25.6 57.6
+ bigLM 24.6 57.9 25.8 57.2
combination. To improve this baseline further, two
additional models were added. We applied lexi-
cal smoothing (IBM-1) and an additional language
model (bigLM) trained on the English side of the
parallel data and the News shuffle corpus. The re-
sults are presented in Table 3.
The baseline was slightly improved by 0.2
points in BLEU and TER. Note, this system com-
bination was the final submission.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, the partners of the QUAERO
project (Karlsruhe Institute of Technology, RWTH
Aachen University, LIMSI-CNRS and SYSTRAN
Software, Inc.) provided a joint submission. By
joining the output of four different translation sys-
tems with RWTH?s system combination, we re-
ported an improvement of up to 1.2 points in
BLEU and TER.
Combining systems optimized on different tun-
ing sets does not seem to improve the translation
quality. However, by adding additional model, the
baseline was slightly improved.
All in all, we conclude that the variability in
terms of BLEU does not influence the final result.
It seems that using different approaches of MT in
a system combination is more important (Freitag
et al, 2012).
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
190
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Josep M. Crego and Jose? B. Mario. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on systran?s rule-based trans-
lation system. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
?07, pages 220?223, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Markus Freitag, Stephan Peitz, Matthias Huck, Her-
mann Ney, Teresa Herrmann, Jan Niehues, Alex
Waibel, Alexandre Allauzen, Gilles Adda, Bianka
Buschbeck, Josep Maria Crego, and Jean Senellart.
2012. Joint wmt 2012 submission of the quaero
project. In NAACL 2012 Seventh Workshop on Sta-
tistical Machine Translation, pages 322?329, Mon-
treal, Canada, June.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 847?855, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Altanta, Georgia, USA,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177?180, Prague, Czech Republic, June.
Alon Lavie and Abhaya Agarwal. 2007. ME-
TEOR: An Automatic Metric for MT Evaluation
with High Levels of Correlation with Human Judg-
ments. pages 228?231, Prague, Czech Republic,
June.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Sab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA,
December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-based Lexicon Models. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, Singapore.
191
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation Systems for IWSLT
2011. In Proceedings of the Eighth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proc. of Third ACL Workshop on Statistical Ma-
chine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Muntsin Kolss, and
Alex Waibel. 2009. The Universita?t Karlsruhe
Translation System for the EACL-WMT 2009. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003a. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Franz Josef Och. 2003b. Minimum error rate training
in statistical machine translation. In ACL ?03: Proc.
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Work-
shop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In TMI, Sko?vde,
Sweden.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Evelyne Tzoukermann and SusanEditors Arm-
strong, editors, Proceedings of the ACL SIGDAT-
Workshop, pages 47?50. Kluwer Academic Publish-
ers.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Int. Conf. on Spo-
ken Language Processing, volume 2, pages 901?
904, Denver, Colorado, USA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, MI.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language
Processing and Knowledge Engineering, Beijing,
China.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
192
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 193?199,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The RWTH Aachen Machine Translation System for WMT 2013
Stephan Peitz, Saab Mansour, Jan-Thorsten Peter, Christoph Schmidt,
Joern Wuebker, Matthias Huck, Markus Freitag and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems devel-
oped at RWTH Aachen University for
the translation task of the ACL 2013
Eighth Workshop on Statistical Machine
Translation (WMT 2013). We partici-
pated in the evaluation campaign for the
French-English and German-English lan-
guage pairs in both translation directions.
Both hierarchical and phrase-based SMT
systems are applied. A number of dif-
ferent techniques are evaluated, including
hierarchical phrase reordering, translation
model interpolation, domain adaptation
techniques, weighted phrase extraction,
word class language model, continuous
space language model and system combi-
nation. By application of these methods
we achieve considerable improvements
over the respective baseline systems.
1 Introduction
For the WMT 2013 shared translation task1
RWTH utilized state-of-the-art phrase-based and
hierarchical translation systems as well as an in-
house system combination framework. We give
a survey of these systems and the basic meth-
ods they implement in Section 2. For both
the French-English (Section 3) and the German-
English (Section 4) language pair, we investigate
several different advanced techniques. We con-
centrate on specific research directions for each
of the translation tasks and present the respec-
tive techniques along with the empirical results
they yield: For the French?English task (Sec-
tion 3.2), we apply a standard phrase-based sys-
tem with up to five language models including a
1http://www.statmt.org/wmt13/
translation-task.html
word class language model. In addition, we em-
ploy translation model interpolation and hierarchi-
cal phrase reordering. For the English?French
task (Section 3.1), we train translation mod-
els on different training data sets and augment
the phrase-based system with a hierarchical re-
ordering model, a word class language model,
a discriminative word lexicon and a insertion
and deletion model. For the German?English
(Section 4.3) and English?German (Section 4.4)
tasks, we utilize morpho-syntactic analysis to pre-
process the data (Section 4.1), domain-adaptation
(Section 4.2) and a hierarchical reordering model.
For the German?English task, an augmented hi-
erarchical phrase-based system is set up and we
rescore the phrase-based baseline with a continu-
ous space language model. Finally, we perform a
system combination.
2 Translation Systems
In this evaluation, we employ phrase-based trans-
lation and hierarchical phrase-based translation.
Both approaches are implemented in Jane (Vilar et
al., 2012; Wuebker et al, 2012), a statistical ma-
chine translation toolkit which has been developed
at RWTH Aachen University and is freely avail-
able for non-commercial use.2
2.1 Phrase-based System
In the phrase-based decoder (source cardinality
synchronous search, SCSS), we use the standard
set of models with phrase translation probabilities
and lexical smoothing in both directions, word and
phrase penalty, distance-based distortion model,
an n-gram target language model and three bi-
nary count features. Optional additional models
used in this evaluation are the hierarchical reorder-
ing model (HRM) (Galley and Manning, 2008), a
word class language model (WCLM) (Wuebker et
2http://www.hltpr.rwth-aachen.de/jane/
193
al., 2012), a discriminative word lexicon (DWL)
(Mauser et al, 2009), and insertion and deletion
models (IDM) (Huck and Ney, 2012). The param-
eter weights are optimized with minimum error
rate training (MERT) (Och, 2003). The optimiza-
tion criterion is BLEU.
2.2 Hierarchical Phrase-based System
In hierarchical phrase-based translation (Chiang,
2007), a weighted synchronous context-free gram-
mar is induced from parallel text. In addition to
continuous lexical phrases, hierarchical phrases
with up to two gaps are extracted. The search is
carried out with a parsing-based procedure. The
standard models integrated into our Jane hierar-
chical systems (Vilar et al, 2010; Huck et al,
2012c) are: phrase translation probabilities and
lexical smoothing probabilities in both translation
directions, word and phrase penalty, binary fea-
tures marking hierarchical phrases, glue rule, and
rules with non-terminals at the boundaries, four
binary count features, and an n-gram language
model. Optional additional models comprise IBM
model 1 (Brown et al, 1993), discriminative word
lexicon and triplet lexicon models (Mauser et al,
2009; Huck et al, 2011), discriminative reordering
extensions (Huck et al, 2012a), insertion and dele-
tion models (Huck and Ney, 2012), and several
syntactic enhancements like preference grammars
(Stein et al, 2010) and soft string-to-dependency
features (Peter et al, 2011). We utilize the cube
pruning algorithm for decoding (Huck et al, 2013)
and optimize the model weights with MERT. The
optimization criterion is BLEU.
2.3 System Combination
System combination is used to produce consensus
translations from multiple hypotheses generated
with different translation engines. First, a word
to word alignment for the given single system hy-
potheses is produced. In a second step a confusion
network is constructed. Then, the hypothesis with
the highest probability is extracted from this con-
fusion network. For the alignment procedure, one
of the given single system hypotheses is chosen as
primary system. To this primary system all other
hypotheses are aligned using the METEOR (Lavie
and Agarwal, 2007) alignment and thus the pri-
mary system defines the word order. Once the
alignment is given, the corresponding confusion
network is constructed. An example is given in
Figure 1.
The model weights of the system combination
are optimized with standard MERT on 100-best
lists. For each single system, a factor is added to
the log-linear framework of the system combina-
tion. Moreover, this log-linear model includes a
word penalty, a language model trained on the in-
put hypotheses, a binary feature which penalizes
word deletions in the confusion network and a pri-
mary feature which marks the system which pro-
vides the word order. The optimization criterion is
4BLEU-TER.
2.4 Other Tools and Techniques
We employ GIZA++ (Och and Ney, 2003) to train
word alignments. The two trained alignments are
heuristically merged to obtain a symmetrized word
alignment for phrase extraction. All language
models (LMs) are created with the SRILM toolkit
(Stolcke, 2002) and are standard 4-gram LMs
with interpolated modified Kneser-Ney smooth-
ing (Kneser and Ney, 1995; Chen and Goodman,
1998). The Stanford Parser (Klein and Manning,
2003) is used to obtain parses of the training data
for the syntactic extensions of the hierarchical sys-
tem. We evaluate in truecase with BLEU (Papineni
et al, 2002) and TER (Snover et al, 2006).
2.5 Filtering of the Common Crawl Corpus
The new Common Crawl corpora contain a large
number of sentences that are not in the labelled
language. To clean these corpora, we first ex-
tracted a vocabulary from the other provided cor-
pora. Then, only sentences containing at least
70% word from the known vocabulary were kept.
In addition, we discarded sentences that contain
more words from target vocabulary than source
vocabulary on the source side. These heuristics
reduced the French-English Common Crawl cor-
pus by 5,1%. This filtering technique was also ap-
plied on the German-English version of the Com-
mon Crawl corpus.
3 French?English Setups
We trained phrase-based translation systems for
French?English and for English?French. Cor-
pus statistics for the French-English parallel data
are given in Table 1. The LMs are 4-grams trained
on the provided resources for the respective lan-
guage (Europarl, News Commentary, UN, 109,
Common Crawl, and monolingual News Crawl
194
0
1
5:
th
at
/1
7:
th
is/
3
2
3:
is/
3
8:
w
as
/1
3
0:
*E
PS
*/
3
4:
it/
1
4
0:
*E
PS
*/
3
2:
in
/1
5
0:
*E
PS
*/
3
6:
th
e/
1
6
0:
*E
PS
*/
1
1:
fu
tu
re
/3
Figure 1: Confusion network of four different hypotheses.
Table 1: Corpus statistics of the preprocessed
French-English parallel training data. EPPS de-
notes Europarl, NC denotes News Commentary,
CC denotes Common Crawl. In the data, numeri-
cal quantities have been replaced by a single cate-
gory symbol.
French English
EPPS Sentences 2.2M
+ NC Running Words 64.7M 59.7M
Vocabulary 153.4K 132.2K
CC Sentences 3.2M
Running Words 88.1M 80.9.0M
Vocabulary 954.8K 908.0K
UN Sentences 12.9M
Running Words 413.3M 362.3M
Vocabulary 487.1K 508.3K
109 Sentences 22.5M
Running Words 771.7M 661.1M
Vocabulary 1 974.0K 1 947.2K
All Sentences 40.8M
Running Words 1 337.7M 1 163.9M
Vocabulary 2 749.8K 2 730.1K
language model training data).3
3.1 Experimental Results English?French
For the English?French task, separate translation
models (TMs) were trained for each of the five
data sets and fed to the decoder. Four additional
indicator features are introduced to distinguish the
different TMs. Further, we applied the hierar-
chical reordering model, the word class language
model, the discriminative word lexicon, and the
insertion and deletion model. Table 2 shows the
results of our experiments.
As a development set for MERT, we use new-
stest2010 in all setups.
3.2 Experimental Results French?English
For the French?English task, a translation model
(TM) was trained on all available parallel data.
For the baseline, we interpolated this TM with
3The parallel 109 corpus is often also referred to as WMT
Giga French-English release 2.
an in-domain TM trained on EPPS+NC and em-
ployed the hierarchical reordering model. More-
over, three language models were used: The first
language model was trained on the English side
of all available parallel data, the second one on
EPPS and NC and the third LM on the News Shuf-
fled data. The baseline was improved by adding a
fourth LM trained on the Gigaword corpus (Ver-
sion 5) and a 5-gram word class language model
trained on News Shuffled data. For the WCLM,
we used 50 word classes clustered with the tool
mkcls (Och, 2000). All results are presented in Ta-
ble 3.
4 German?English Setups
For both translation directions of the German-
English language pair, we trained phrase-based
translation systems. Corpus statistics for German-
English can be found in Table 4. The language
models are 4-grams trained on the respective tar-
get side of the bilingual data as well as on the
provided News Crawl corpus. For the English
language model the 109 French-English, UN and
LDC Gigaword Fifth Edition corpora are used ad-
ditionally.
4.1 Morpho-syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation, the German text
is preprocessed by splitting German compound
words with the frequency-based method described
in (Koehn and Knight, 2003). To further reduce
translation complexity, we employ the long-range
part-of-speech based reordering rules proposed by
Popovic? and Ney (2006).
4.2 Domain Adaptation
This year, we experimented with filtering and
weighting for domain-adaptation for the German-
English task. To perform adaptation, we define a
general-domain (GD) corpus composed from the
news-commentary, europarl and Common Crawl
corpora, and an in-domain (ID) corpus using
a concatenation of the test sets (newstest{2008,
2009, 2010, 2011, 2012}) with the correspond-
ing references. We use the test sets as in-domain
195
Table 2: Results for the English?French task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?French BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
TM:EPPS + HRM 22.9 63.0 25.0 60.0 27.8 56.7 28.9 54.4 27.2 57.1
TM:UN + HRM 22.7 63.4 25.0 60.0 28.3 56.4 29.5 54.2 27.3 57.1
TM:109 + HRM 23.5 62.3 26.0 59.2 29.6 55.2 30.3 53.3 28.0 56.4
TM:CC + HRM 23.5 62.3 26.2 58.8 29.2 55.3 30.3 53.3 28.2 56.0
TM:NC 21.0 64.8 22.3 61.6 25.6 58.7 26.9 56.6 25.7 58.5
+ HRM 21.5 64.3 22.6 61.2 26.1 58.4 27.3 56.1 26.0 58.2
+ TM:EPPS,CC,UN 23.9 61.8 26.4 58.6 29.9 54.7 31.0 52.7 28.6 55.6
+ TM:109 24.0 61.5 26.5 58.4 30.2 54.2 31.1 52.3 28.7 55.3
+ WCLM, DWL, IDM 24.0 61.6 26.5 58.3 30.4 54.0 31.4 52.1 28.8 55.2
Table 3: Results for the French?English task (truecase). newstest2010 is used as development set.
BLEU and TER are given in percentage.
newstest2010 newstest2011 newstest2012
French?English BLEU TER BLEU TER BLEU TER
SCSS baseline 28.1 54.6 29.1 53.3 - -
+ GigaWord.v5 LM 28.6 54.2 29.6 52.9 29.6 53.3
+ WCLM 29.1 53.8 30.1 52.5 29.8 53.1
(newswire) as the other corpora are coming from
differing domains (news commentary, parliamen-
tary discussions and various web sources), and on
initial experiments, the other corpora did not per-
form well when used as an in-domain representa-
tive for adaptation. To check whether over-fitting
occurs, we measure the results of the adapted
systems on the evaluation set of this year (new-
stest2013) which was not used as part of the in-
domain set.
The filtering experiments are done similarly to
(Mansour et al, 2011), where we compare filtering
using LM and a combined LM and IBM Model 1
(LM+M1) based scores. The scores for each sen-
tence pair in the general-domain corpus are based
on the bilingual cross-entropy difference of the
in-domain and general-domain models. Denoting
HLM (x) as the cross entropy of sentence x ac-
cording to LM , then the cross entropy difference
DHLM (x) can be written as:
DHLM (x) = HLMID(x)?HLMGD(x)
The bilingual cross entropy difference for a sen-
tence pair (s, t) in the GD corpus is then defined
by:
DHLM (s) + DHLM (t)
For IBM Model 1 (M1), the cross-entropy
HM1(s|t) is defined similarly to the LM cross-
entropy, and the resulting bilingual cross-entropy
difference will be of the form:
DHM1(s|t) + DHM1(t|s)
The combined LM+M1 score is obtained by
summing the LM and M1 bilingual cross-entropy
difference scores. To perform filtering, the GD
corpus sentence pairs are scored by the appropri-
ate method, sorted by the score, and the n-best sen-
tences are then used to build an adapted system.
In addition to adaptation using filtering, we ex-
periment with weighted phrase extraction similar
to (Mansour and Ney, 2012). We differ from their
work by using a combined LM+M1 weight to per-
form the phrase extraction instead of an LM based
weight. We use a combined LM+M1 weight as
this worked best in the filtering experiments, mak-
ing scoring with LM+M1 more reliable than LM
scores only.
4.3 Experimental Results German?English
For the German?English task, the baseline is
trained on all available parallel data and includes
the hierarchical reordering model. The results of
the various filtering and weighting experiments are
summarized in Table 5.
196
Table 5: German-English results (truecase). BLEU and TER are given in percentage. Corresponding
development set is marked with *. ? labels the single systems selected for the system combination.
newstest2009 newstest2010 newstest2011 newstest2012 newstest2013
German?English BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 21.7 61.1 24.8* 58.9* 22.0 61.1 23.4 60.0 26.1 56.4
LM 800K-best 21.6 60.5 24.7* 58.3* 22.0 60.5 23.6 59.7 - -
LM+M1 800K-best 21.4 60.5 24.7* 58.1* 22.0 60.4 23.7 59.2 - -
(LM+M1)*TM 22.1 60.2 25.4* 57.8* 22.5 60.1 24.0 59.1 - -
(LM+M1)*TM+GW 22.8 59.5 25.7* 57.2* 23.1 59.5 24.4 58.6 26.6 55.5
(LM+M1)*TM+GW? 22.9* 61.1* 25.2 59.3 22.8 61.5 23.7 60.8 26.4 57.1
SCSS baseline 22.6* 61.6* 24.1 60.1 22.1 62.0 23.1 61.2 - -
CSLM rescoring? 22.0 60.4 25.1* 58.3* 22.4 60.2 23.9 59.3 26.0 56.0
HPBT? 21.9 60.4 24.9* 58.2* 22.3 60.3 23.6 59.6 25.9 56.3
system combination - - - - 23.4* 59.3* 24.7 58.5 27.1 55.3
Table 6: English-German results (truecase). newstest2009 was used as development set. BLEU and TER
are given in percentage.
newstest2008 newstest2009 newstest2010 newstest2011 newstest2012
English?German BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER
SCSS baseline 14.9 70.9 14.9 70.4 16.0 66.3 15.4 69.5 15.7 67.5
LM 800K-best 15.1 70.9 15.1 70.3 16.2 66.3 15.6 69.4 15.9 67.4
(LM+M1) 800K-best 15.8 70.8 15.4 70.0 16.2 66.2 16.0 69.3 16.1 67.4
(LM+M1) ifelse 16.1 70.6 15.7 69.9 16.5 66.0 16.2 69.2 16.3 67.2
Table 4: Corpus statistics of the preprocessed
German-English parallel training data (Europarl,
News Commentary and Common Crawl). In the
data, numerical quantities have been replaced by a
single category symbol.
German English
Sentences 4.1M
Running Words 104M 104M
Vocabulary 717K 750K
For filtering, we use the 800K best sentences
from the whole training corpora, as this se-
lection performed best on the dev set among
100K,200K,400K,800K,1600K setups. Filtering
seems to mainly improve on the TER scores, BLEU
scores are virtually unchanged in comparison to
the baseline. LM+M1 filtering improves further
on TER in comparison to LM-based filtering.
The weighted phrase extraction performs best
in our experiments, where the weights from the
LM+M1 scoring method are used. Improvements
in both BLEU and TER are achieved, with BLEU
improvements ranging from +0.4% up-to +0.6%
and TER improvements from -0.9% and up-to -
1.1%.
As a final step, we added the English Gigaword
corpus to the LM (+GW). This resulted in further
improvements of the systems.
In addition, the system as described above was
tuned on newstest2009. Using this development
set results in worse translation quality.
Furthermore, we rescored the SCSS baseline
tuned on newstest2009 with a continuous space
language model (CSLM) as described in (Schwenk
et al, 2012). The CSLM was trained on the eu-
roparl and news-commentary corpora. For rescor-
ing, we used the newstest2011 set as tuning set and
re-optimized the parameters with MERT on 1000-
best lists. This results in an improvement of up to
0.8 points in BLEU compared to the baseline.
We compared the phrase-based setups with a
hierarchical translation system, which was aug-
mented with preference grammars, soft string-
to-dependency features, discriminative reordering
extensions, DWL, IDM, and discriminative re-
197
ordering extensions. The phrase table of the hier-
archical setup has been extracted from News Com-
mentary and Europarl parallel data only (not from
Common Crawl).
Finally, three setups were joined in a system
combination and we gained an improvement of up
to 0.5 points in BLEU compared to the best single
system.
4.4 Experimental Results English?German
The results for the English?German task are
shown in Table 6. While the LM-based filter-
ing led to almost no improvement over the base-
line, the LM+M1 filtering brought some improve-
ments in BLEU. In addition to the sentence fil-
tering, we tried to combine the translation model
trained on NC+EPPS with a TM trained on Com-
mon Crawl using the ifelse combination (Mansour
and Ney, 2012). This combination scheme con-
catenates both TMs and assigns the probabilities
of the in-domain TM if it contains the phrase,
else it uses the probabilities of the out-of-domain
TM. Appling this method, we achieved further im-
provements.
5 Conclusion
For the participation in the WMT 2013 shared
translation task, RWTH experimented with both
phrase-based and hierarchical translation systems.
Several different techniques were evaluated and
yielded considerable improvements over the re-
spective baseline systems as well as over our last
year?s setups (Huck et al, 2012b). Among these
techniques are a hierarchical phrase reordering
model, translation model interpolation, domain
adaptation techniques, weighted phrase extraction,
a word class language model, a continuous space
language model and system combination.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, Massachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Matthias Huck and Hermann Ney. 2012. Insertion and
Deletion Models for Statistical Machine Translation.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics - Human
Language Technologies conference, pages 347?351,
Montre?al, Canada, June.
Matthias Huck, Saab Mansour, Simon Wiesler, and
Hermann Ney. 2011. Lexicon Models for Hierar-
chical Phrase-Based Machine Translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 191?198, San
Francisco, California, USA, December.
Matthias Huck, Stephan Peitz, Markus Freitag, and
Hermann Ney. 2012a. Discriminative Reordering
Extensions for Hierarchical Phrase-Based Machine
Translation. In 16th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
313?320, Trento, Italy, May.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012b. The RWTH
Aachen Machine Translation System for WMT
2012. In NAACL 2012 Seventh Workshop on
Statistical Machine Translation, pages 304?311,
Montre?al, Canada, June.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012c. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and
Hermann Ney. 2013. A Performance Study of
Cube Pruning for Large-Scale Hierarchical Machine
Translation. In Proceedings of the NAACL 7thWork-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 29?38, Atlanta, Georgia,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proc. of the 41th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 423?430, Sapporo, Japan,
July.
198
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181?184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings of
European Chapter of the ACL (EACL 2009), pages
187?194.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
ACL 2007 Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic, June.
Saab Mansour and Hermann Ney. 2012. A Simple and
Effective Weighted Phrase Extraction for Machine
Translation Adaptation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 193?200, Hong Kong, December.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, California, USA, December.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conf. on Empirical Methods for Natu-
ral Language Processing (EMNLP), pages 210?218,
Singapore, August.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2000. mkcls: Training
of word classes for language modeling.
http://www.hltpr.rwth-aachen.de/
web/Software/mkcls.html.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
Jan-Thorsten Peter, Matthias Huck, Hermann Ney, and
Daniel Stein. 2011. Soft String-to-Dependency
Hierarchical Machine Translation. In International
Workshop on Spoken Language Translation, pages
246?253, San Francisco, California, USA, Decem-
ber.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation, pages 1278?1283, Genoa,
Italy, May.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT 2012 Workshop: Will
We Ever Really Replace the N-gram Model? On the
Future of Language Modeling for HLT, pages 11?
19, Montre?al, Canada, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Conf.
of the Association for Machine Translation in the
Americas (AMTA), Denver, Colorado, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, Colorado, USA,
September.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statis-
tical Machine Translation. In International Confer-
ence on Computational Linguistics, pages 483?491,
Mumbai, India, December.
199
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 309?319,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Length-incremental Phrase Training for SMT
Joern Wuebker and Hermann Ney
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
{wuebker,ney}@cs.rwth-aachen.de
Abstract
We present an iterative technique to gener-
ate phrase tables for SMT, which is based
on force-aligning the training data with
a modified translation decoder. Differ-
ent from previous work, we completely
avoid the use of a word alignment or
phrase extraction heuristics, moving to-
wards a more principled phrase generation
and probability estimation. During train-
ing, we allow the decoder to generate new
phrases on-the-fly and increment the max-
imum phrase length in each iteration. Ex-
periments are carried out on the IWSLT
2011 Arabic-English task, where we are
able to reach moderate improvements on a
state-of-the-art baseline with our training
method. The resulting phrase table shows
only a small overlap with the heuristically
extracted one, which demonstrates the re-
strictiveness of limiting phrase selection
by a word alignment or heuristics. By
interpolating the heuristic and the trained
phrase table, we can improve over the
baseline by 0.5% BLEU and 0.5% TER.
1 Introduction
Most state-of-the-art SMT systems get the statis-
tics from which the different component models
are estimated via heuristics using a Viterbi word
alignment. The word alignment is usually gener-
ated with tools like GIZA++ (Och and Ney, 2003),
that apply the EM algorithm to estimate the align-
ment with the HMM or IBM-4 translation mod-
els. This is also the case for the phrases or rules
which serve as translation units for the decoder.
All phrases that do not violate the word alignment
are extracted and their probabilities are estimated
as relative frequencies (Koehn et al, 2003).
A number of different approaches have tried to
do away with the heuristics and close this gap be-
tween the phrase table generation and translation
decoding. However, most of these approaches ei-
ther fail to achieve state-of-the-art performance or
still make use of the word alignment or the ex-
traction heuristics, e.g. as a prior in discriminative
training or to initialize a generative or generatively
inspired training procedure and are thus biased by
their weaknesses. Here, we aim at moving towards
the ideal situation, where a unified framework in-
duces the phrases based on the same models as in
decoding.
We train the phrase table without using a word
alignment or the extraction heuristics. Different
from previous work, we are able to generate all
possible phrase pairs on-the-fly during the train-
ing procedure. A further advantage of our pro-
posed algorithm is that we use basically the same
beam search as in translation. This makes it easy
to re-implement by modifying any translation de-
coder, and makes sure that training and translation
are consistent. In principle, we apply the forced
decoding approach described in (Wuebker et al,
2010) with cross-validation to prevent over-fitting,
but we initialize the phrase table with IBM-1 lex-
ical probabilities (Brown et al, 1993) instead of
heuristically extracted relative frequencies. The
algorithm is extended with the concept of back-
off phrases, so that new phrase pairs can be gener-
ated at training time. The size of the newly gener-
ated phrases is incremented over the training iter-
ations. By introducing fallback decoding runs, we
are able to successfully align the complete training
data. Local language models are used for better
phrase pair pre-selection.
309
The experiments are carried out on the IWSLT
2011 Arabic-English shared task. We are able to
show that it is possible and feasible to reach state-
of-the-art performance without the need to word-
align the bilingual training data. The small over-
lap of 18.5% between the trained and the heuristi-
cally extracted phrase table demonstrates the limi-
tations of previous work, where training is initial-
ized by the baseline phrase table or phrase selec-
tion is restricted by a word alignment. With a lin-
ear interpolation of phrase tables an improvement
of 0.5% BLEU and 0.5% TER over the baseline
can be achieved. The result in BLEU is statisti-
cally significant on the test set with 90% confi-
dence. Further, we can confirm the observation
of previous work, that phrases with near-zero en-
tropies seem to be a disadvantage for translation
quality. Although we use a phrase-based decoder
here, the principles of our work can be applied to
any statistical machine translation paradigm. The
software used for our experiments is available un-
der a non-commercial open source licence.
The paper is organized as follows. We review
related work in Section 2. The decoder and its
features are described in Section 3 and we give
an overview of the training procedure in Section
4. The complete algorithm is described in Section
5 and experiments are presented in Section 6. We
conclude with Section 7.
2 Related Work
Marcu and Wong (2002) present a joint probabil-
ity model, which is trained with a hill-climbing
technique based on break, merge, swap and move
operations. Due to the computational complexity
they are only able to consider phrases, which ap-
pear at least five times in the data. The model is
shown to slightly underperform heuristic extrac-
tion in (Koehn et al, 2003). For higher efficiency,
it is constrained by a word alignment in (Birch et
al., 2006). DeNero et al (2008) introduce a differ-
ent training procedure for this model based on a
Gibbs sampler. They make use of the word align-
ment for initialization.
A generative phrase model trained with the
Expectation-Maximization (EM) algorithm is
shown in (DeNero et al, 2006). It also does not
reach the same top performance as heuristic ex-
traction. The authors identify the hidden segmen-
tation variable, which results in over-fitting, as the
main problem.
Liang et al (2006) present a discriminative
translation system. One of the proposed strategies
for training, which the authors call bold updating,
is similar to our training scheme. They use heuris-
tically extracted phrase translation probabilities as
blanket features in all setups.
Another iteratively-trained phrase model is de-
scribed by Moore and Quirk (2007). Their model
is segmentation-free and, confirming the findings
in (DeNero et al, 2006), can close the gap to
phrase tables induced from surface heuristics. It
relies on word alignment for phrase selection.
Mylonakis and Sima?an (2008) present a phrase
model, whose training procedure uses prior prob-
abilities based on Inversion Transduction Gram-
mar and smoothing as learning objective to pre-
vent over-fitting. They also rely on the word align-
ment to select phrase pairs.
Blunsom et al (2009) perform inference over
latent synchronous derivation trees under a non-
parametric Bayesian model with a Gibbs sampler.
Training is also initialized by extracting rules from
a word alignment, but the authors let the sampler
diverge from the initial value for 1000 passes over
the data, before the samples are used. However,
as the model is to weak for actual translation, the
usual extraction heuristics are applied on the hier-
archical alignments to infer a distribution over rule
tables.
Wuebker et al (2010) use a forced decoding
training procedure, which applies a leave-one-out
technique to prevent over-fitting. They are able to
show improvements over a heuristically extracted
phrase table, which is used for initialization of the
training.
In (Saers and Wu, 2011), the EM algorithm is
applied for principled induction of bilexica based
on linear inversion transduction grammar. The
model itself underperforms the baseline, but the
authors show moderate improvements by combin-
ing it with the baseline phrase table, which is sim-
ilar to our results.
(Neubig et al, 2011) also propose a probabilis-
tic model based on inversion transduction gram-
mar, which allows for direct phrase table extrac-
tion from unaligned data. They show results simi-
lar to the heuristic baseline on several tasks.
A number of different models that can be
trained from forced derivation trees are shown in
(Duan et al, 2012), including a re-estimated trans-
lation model, two reordering models and a rule se-
310
quence model. For inference, they optimize their
parameters towards alignment F-score. The forced
derivations are initialized with the standard heuris-
tic extraction scheme.
He and Deng (2012) describe a discriminative
phrase training procedure, where n-best transla-
tions are produced by the decoder on the whole
training data. The heuristically extracted relative
frequencies serve as a prior, and the probabili-
ties are updated with a maximum BLEU criterion
based on the n-best lists.
3 Translation Model
We use the standard phrase-based translation de-
coder from the open source toolkit Jane 2 (Wue-
bker et al, 2012a) for both the training proce-
dure and the translation experiments. It makes use
of the usual features: Translation channel mod-
els in both directions, lexical smoothing models in
both directions, an n-gram language model (LM),
phrase and word penalty and a jump-distance-
based distortion model. Formally, the best trans-
lation e?I?1 as defined by the models hm(eI1, sK1 , fJ1 )
can be written as (Och and Ney, 2004)
e?I?1 = argmax
I,eI1
{ M?
m=1
?mhm(eI1, sK1 , fJ1 )
}
, (1)
where fJ1 = f1 . . . fJ is the source sentence,
eI1 = e1 . . . eI the target sentence and sK1 =
s1 . . . sK their phrase segmentation and align-
ment. We define sk := (ik, bk, jk), where ik is
the last position of kth target phrase, and (bk, jk)
are the start and end positions of the source phrase
aligned to the kth target phrase. Different from
many standard systems, the lexical smoothing
scores are not estimated by extracting counts from
a word alignment, but with IBM-1 model scores
trained on the bilingual data with GIZA++. They
are computed as (Zens, 2008)
hlex(eI1, sK1 , fJ1 ) =
K?
k=1
jk?
j=bk
log
?
?p(fj |e0) +
ik?
i=ik?1+1
p(fj |ei)
?
?
(2)
Here, e0 denotes the empty target word. The
lexical smoothing model for the inverse direc-
tion is computed analogously. The log-linear fea-
ture weights ?m are optimized on a development
data set with minimum error rate training (MERT)
(Och, 2003). As optimization criterion we use
BLEU (Papineni et al, 2001).
4 Training
4.1 Overview
In this work we employ a training procedure in-
spired by the Expectation-Maximization (EM) al-
gorithm.
The E-step corresponds to force-aligning the
training data with a modified translation decoder,
which yields a distribution over possible phrasal
segmentations and their alignment. Different from
original EM, we make use of not only the two
translation channel models that are being learned,
but the full log-linear combination of models as in
translation decoding. Formally, we are searching
for the best phrase segmentation and alignment for
the given sentence pair, which is defined by
s?K?1 = argmax
K,sK1
{ M?
m=1
?mhm(eI1, sK1 , fJ1 )
}
(3)
To force-align the training data, the translation
decoder is constrained to the given target sentence.
The translation candidates applicable for each sen-
tence pair are selected through a bilingual phrase
matching before the actual search.
In the M-step, we re-estimate the phrase table
from the phrase alignments. The translation prob-
ability of a phrase pair (f? , e?) is estimated as
pFA(f? |e?) =
CFA(f? , e?)?
f? ?
CFA(f? ?, e?)
(4)
where CFA(f? , e?) is the count of the phrase pair
(f? , e?) in the phrase-aligned training data.
In contrast to original EM, this is done by tak-
ing the phrase counts from a uniformly weighted
n-best list. The limitation to n phrase alignments
helps keeping the number of considered phrases
reasonably small. Because the log-linear feature
weights have been tuned in a discriminative fash-
ion to optimize the ranking of translation hypothe-
ses, rather than their probability distribution, pos-
terior probabilities received by exponentiation and
renormalization need to be scaled similar to (Wue-
bker et al, 2012b). Uniform weights can alle-
viate this mismatch between the discriminatively
311
trained log-linear feature weights and the actual
probability distribution, without having to resort
to an arbitrarily chosen global scaling factor. This
corresponds to the count model in (Wuebker et al,
2010) and was shown by the authors to perform
similar or better than using actual posterior proba-
bilities. In our experiments, we set the size of the
n-best list to n = 1000.
The first iteration of phrase training is initialized
with an empty phrase table. We use the notion of
backoff phrases to generate new phrase pairs on-
the-fly. To avoid over-fitting, we apply the cross-
validation technique presented in (Wuebker et al,
2010) with a batch-size of 2000 sentences. This
means that for each batch the phrase and marginal
counts from the full phrase table are reduced by
the statistics taken from the same batch in the pre-
vious iteration. The phrase translation probabili-
ties are then estimated from these updated counts.
Phrase pairs only appearing in a single batch are
assigned a fixed penalty.
4.2 Backoff Phrases
Backoff phrases are phrase pairs that are generated
on-the-fly by the decoder at training time. When
aligning a sentence pair, for a given maximum
phrase length m, the decoder inserts all combi-
nations of source ms-grams and target mt-grams
into the translation options, that are present in the
sentence pair and with ms,mt ? m. Formally,
for the sentence pair (fJ1 , eI1), fJ1 = f1 . . . fJ ,
eI1 = e1 . . . eI , and maximum length m, we gen-
erate all phrase pairs (f? , e?) where
?ms,mt, j, i :
1 ? ms,mt ? m ? 1 ? j ? J ?ms + 1
? 1 ? i ? I ?mt + 1
? f? = f (j+ms?1)j ? e? = e
(i+mt?1)
i . (5)
These generated phrase pairs are given a fixed
penalty penp per phrase, pens per source word and
pent per target word, which are summed up and
substituted for the two channel models. The lex-
ical smoothing scores are computed in the usual
way based on an IBM-1 table. Note that this table
is not extracted from a word alignment, but con-
tains the real probabilities trained with the IBM-1
model by GIZA++.
We use backoff phrases in two different con-
texts. In the first mmax = 6 iterations, they are
applied as a means to generate new phrase pairs on
the fly. We increase the maximum phrase length
m in each iteration and always generate all possi-
ble backoff phrases before aligning each sentence.
Later, when a sufficient number of phrases have
been generated in the previous iterations, they are
used as a last resort in order to avoid alignment
failures.
At the later stage of the length-incremental
training, we also make use of a modified version,
where we only allow new phrase pairs (f? , e?) to be
generated, if no translation candidates exist for f?
after the bilingual phrase matching. However, in
this case, backoff phrases are only used if a first
decoding run fails and we have to resort to fallback
runs, which are described in the next Section.
4.3 Fallback Decoding Runs
To maximize the number of successfully aligned
sentences, we allow for fallback decoding runs
with slightly altered parameterization, whenever
constrained decoding fails. In this work, we
only change the parameterization of the backoff
phrases. After mmax = 6 iterations, we no longer
generate any backoff phrases in the first decoding
run. If it fails, a second run is performed, where
we allow to generate backoff phrases for all source
phrases, which have no target candidates after the
bilingual phrase matching. Finally, if this one also
fails, all possible phrases are generated in the third
run. Here, the maximum backoff phrase length is
fixed to m = 1. We denote the number of fallback
runs with nfb = 2. In our experiments, the two
fallback runs enable us to align every sentence pair
of the training data after the sixth iteration.
4.4 Local Language Models
To make the training procedure feasible, it is par-
allelized by splitting the training data into batches
of 2000 sentences. The batches are aligned inde-
pendently. For each batch, we produce a local
language model, which is a unigram LM trained
on the target side of the current batch. We pre-
sort the phrases before search by their log-linear
model score, which uses the phrase-internal uni-
gram LM costs as one feature function. One ef-
fect of this is that the order in which phrase candi-
dates are considered is adjusted to the local part of
the data, which has a positive effect on decoding
speed. Secondly, we limit the number of transla-
tion candidates for each source phrase to the best
scoring 500 before the bilingual phrase matching.
312
 23
 24
 25
 26
 27
 28
 2  3  4  5  6
 80
 90
 100
B
L
E
U
[
%
]
w
o
r
d
 
c
o
v
e
r
a
g
e
 
[
%
]
Iteration
wp=-0.1
wp=-0.3
wp=-0.5
wp=-0.7
Figure 1: BLEU scores and word coverages on
dev over the first 6 training iterations with dif-
ferent word penalties (wp).
Using the local LM for this means that the pre-
selection better suits the current data batch. As a
result, the number of phrases remaining after the
phrase matching is increased as compared to the
same setup without a local language model.
4.5 Parameterization
The training procedure has a number of hyper pa-
rameters, most of which do not seem to have a
strong impact on the results. This section de-
scribes the parameters that have to be chosen care-
fully. To successfully align a sentence pair, our
decoder is required to fully cover the source sen-
tence. However, in order to achieve a good suc-
cess rate in terms of number of aligned sentence
pairs, we allow for incompletely aligned target
sentences. We denote the percentage of success-
fully aligned sentence pairs as sentence coverage.
Note that we count a sentence pair as successfully
aligned, even if the target sentence is not fully
covered. the word penalty (wp) feature weight
?wp needs to be adjusted carefully. A high value
leads to a high sentence coverage, but many of
their target sides may be incompletely aligned. A
 24
 25
 26
 27
 2  3  4  5  6
 0
 20
 40
 60
 80
 100
B
L
E
U
[
%
]
s
u
r
p
l
u
s
 
p
h
r
a
s
e
s
 
[
%
]
Iteration
pen0=4pen0=3pen0=2pen0=1pen0=0.5
Figure 2: BLEU scores and percentage of surplus
phrases on dev over the first 6 training iterations
with different backoff phrase penalties pen0.
low word penalty can decrease the sentence cover-
age, while aligning larger parts of the target sen-
tences. We denote the total percentage of suc-
cessfully aligned target words as word coverage.
Please note the distincton to the sentence cover-
age, which is defined above. Figure 1 shows the
word coverages and BLEU scores for training iter-
ations 2 through 6 with different word penalties. In
the first iteration, the results are identical, as only
one-to-one phrases are allowed and the number of
aligned target words is therefore predetermined.
For ?wp = ?0.1, the word coverages are continu-
ously decreasing with each iteration, although not
by much. For ?wp = ?0.3 to ?wp = ?0.7 the
word coverage slightly increases from iteration 2
to 3 and then decreases again. In terms of BLEU
score, ?wp = ?0.3 has a slight advantage over the
other values and we decided to continue using this
value in all subsequent experiments.
The backoff phrase penalties directly affect
the learning rate of the training procedure. With
low penalties, only few, very good phrases get
an advantage over the ones generated on-the-fly,
which corresponds to a slow learning rate. In-
313
1. Initialize with empty phrase table
2. Set backoff phrase penalties to
pen0 = 3 and m = 1
3. Until m = mmax, iterate:
? If iteration > 1: set
m = m+ 1
?s2t = ?s2t + ?
?t2s = ?t2s + ?
? Force-align training data and
re-estimate phrase table
4. Set m = 1 and nfb = 2
5. Iterate:
? Force-align training data and
re-estimate phrase table
Figure 3: The complete training algorithm.
creasing the penalties means that a larger per-
centage of the phrase pairs generated in the pre-
vious iterations will be favored over new back-
off phrases, which corresponds to a faster learn-
ing rate. We denote phrase pairs that are more
expensive than their backoff phrase counterparts
as surplus phrases. Figure 2 shows the behavior
over the training iterations 2 through 6 with differ-
ent penalties pen0 in terms of percentage of sur-
plus phrase pairs and BLEU score. Here we set
pens = pent = pen0 and penp = 5pen0. We can
see that pen0 = 4 yields less than 0.1% surplus
phrases through all iterations, whereas pen0 = 0.5
starts off with 98.2% surplus phrases and goes
down to 55.9% in iteration 6. In terms of BLEU, a
fast learning rate seems to be preferable. The best
results are achieved with pen0 = 3, where the rate
of surplus phrases starts at 6.8% and decreases to
1.7% until iteration 6. In all subsequent experi-
ments, we set pen0 = 3.
5 Length-incremental Training
In this section we describe the complete training
algorithm. The first training iteration is initial-
ized with an empty phrase table. The phrases used
in alignment are backoff phrases, which are gen-
erated on-the-fly. The maximum backoff phrase
length is set to m = 1. Then the forced alignment
is iterated, increasing m by 1 in each iteration, up
to a maximum of mmax = 6.
After mmax = 6 iterations, we have created a
sufficient number of phrase pairs and continue it-
erating the training procedure with new parame-
Arabic English
train Sentences 305K
Running Words 6.5M 6.5M
Vocabulary 104K 74K
dev Sentences 934
Run. Words 19K 20K
Vocabulary 4293 3182
OOVs (run. words) 445 182
test Sentences 1664
Run. Words 31K 32K
Vocabulary 5415 3650
OOVs (run. words) 658 159
Table 1: Statistics for the IWSLT 2011 Arabic-
English data. The out-of-vocabulary words are de-
noted as OOVs.
ters. Now, we do not allow usage of any back-
off phrases in the first decoding run. If the first
run fails, we allow a fallback decoding run, where
backoff phrases are generated only for source
phrases without translation candidates. If this
one also fails, in a final fallback run all possible
phrases are generated. Here we allow a maximum
backoff phrase length of m = 1.
The log-linear feature weights ?i used for train-
ing are mostly standard values. Only ?wp for
the word penalty is adjusted as described in Sec-
tion 4.5, and ?s2t,?t2s for the two phrasal channel
models are incremented with each iteration. We
start off with ?s2t = ?t2s = 0 and increment the
weights by ? = 0.02 in each iteration, until the
standard value ?s2t = ?t2s = 0.1 is reached in
iteration 6, after which the values are kept fixed.
MERT is not part of the training procedure, but
only used afterwards for evaluation. The full algo-
rithm is illustrated in Figure 3.
6 Experiments
6.1 Data
We carry out our experiments on the IWSLT 2011
Arabic-English shared task1. It focuses on the
translation of TED talks, a collection of lectures
on a variety of topics ranging from science to cul-
ture. Our bilingual training data is composed of all
available in-domain (TED) data and a selection of
the out-of-domain MultiUN data provided for the
evaluation campaign. The bilingual data selection
1www.iwslt2011.org
314
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 2  4  6  8  10  12  14  16  18  20
B
L
E
U
[
%
]
Iteration
Length-incremental Training (dev)Baseline (dev)Length-incremental Training (test)Baseline (test)
Figure 4: BLEU scores on dev and test over 20
training iterations.
is based on (Axelrod et al, 2011). Data statistics
are given in Table 1. The language model is a 4-
gram LM trained on all provided in-domain mono-
lingual data and a selection based on (Moore and
Lewis, 2010) of the out-of-domain corpora. To ac-
count for statistical variation, all reported results
are average scores over three independent MERT
runs.
6.2 Results
To build the baseline phrase table, we perform
the standard phrase extraction from a symmetrized
word alignment created with the IBM-4 model by
GIZA++. The length of the extracted phrases is
limited to a maximum of six words. The lexical
smoothing scores are computed from IBM-1 prob-
abilities. We run MERT on the development set
(dev) and evaluate on the test set (test). A sec-
ond baseline is the technique described in (Wue-
bker et al, 2010), which we denote as leave-one-
out. It is initialized with the heuristically extracted
table and run for one iteration, which the authors
have shown to be sufficient.
Length-incremental training is performed as de-
scribed in Section 5. After each iteration, we run
MERT on dev using the resulting phrase table and
evaluate. The set of models used here is identical
to the baseline.
The results in BLEU are plotted in Figure 4. We
can see that the performance increases up to it-
eration 5, after which only small changes can be
observed. The performance on dev is similar to
dev test
BLEU TER BLEU TER
[%] [%] [%] [%]
baseline 27.4 54.0 24.6 57.8
leave-one-out 27.3 54.2 24.6 57.7
length-increm. 27.5 53.8 24.9 57.4
lin. interp. 27.9 53.5 25.1? 57.3
Table 2: BLEU and TER scores of the baseline,
phrase training with leave-one-out and length-
incremental training after 12 iterations, as well as
a linear interpolation of the baseline with length-
incremental phrase table. Results marked with ?
are statistically significant with 90% confidence.
the baseline, on test the trained phrase tables
are consistently slightly above the baseline. The
optimum on dev is reached in iteration 12. Ex-
act BLEU and TER (Snover et al, 2006) scores of
the optimum on dev and the baseline are given
in Table 2. The phrase table trained with leave-
one-out (Wuebker et al, 2010) performs simlar to
the heuristic baseline. Length-incremental train-
ing is slightly superior to the baseline, yielding
an improvement of 0.3% BLEU and 0.4% TER
on test. Similar to results observed in (DeN-
ero et al, 2006) and (Wuebker et al, 2010), a lin-
ear interpolation with the baseline containing all
phrase pairs from either of the two tables yields a
moderate improvement of 0.5% BLEU and 0.5%
TER both data sets. The BLEU improvement on
test is statistically significant with 90% confi-
dence based on bootstrap resampling as described
by Koehn (2004).
6.3 Analysis
In Figure 5 we plot the number of phrase pairs
present in the phrase tables after each iteration.
In the first 6 iterations, we keep generating new
phrase pairs via backoff phrases. The maximum
of 14.4M phrase pairs is reached after three itera-
tions. For comparison, the size of the heuristically
extracted table is 19M phrase pairs. Afterwards,
backoff phrases are only used in fallback decoding
runs, which leads to drop in the number of phrase
pairs that are being used. It levels out at 10.4M
phrases.
When we take a look at the phrase length distri-
butions in both the baseline and the trained phrase
table shown in Figure 6, we can see that in the lat-
ter the phrases are generally shorter, which con-
315
5.0 M
10.0 M
15.0 M
 2  4  6  8  10  12  14  16  18  20
#
 
p
h
r
a
s
e
 
p
a
i
r
s
Iteration
Figure 5: Number of generated phrase pairs over
20 training iterations.
firms observations from previous work. In the
trained phrase table, phrases of length one and two
make up 47% of all phrases. In the heuristically
extracted table it is only 32%. This is even more
pronounced in the intersection of the two tables,
where 68% of the phrases are of length one and
two.
Interestingly, the total overlap between the two
phrase tables is rather small. Only about 18.5%
of the phrases from the trained table also appear in
the heuristically extracted one. This shows that, by
generating phrases on-the-fly without restrictions
based on a word alignment or a bias from intializa-
tion, our training procedure strongly diverges from
the baseline phrase table. We conclude that most
previous work in this area, which adhered to the
above mentioned restrictions, was only able to ex-
plore a fraction of the full potential of real phrase
training.
Following (DeNero et al, 2006), we compute
the entropy of the distributions within the phrase
tables to quantify the ?smoothness? of the distri-
bution. For a given source phrase f? , it is defined
as
H(f?) =
?
e?
p(e?|f?)log(p(e?|f?)). (6)
A flat distribution with a high level of uncer-
tainty yields a high entropy, whereas a peaked dis-
tribution with little uncertainty produces a low en-
tropy. We analyze the phrase tables filtered to-
wards the dev and test sets. The average en-
 0
 5
 10
 15
 20
 25
 30
 35
 40
1 2 3 4 5 6
#
 
p
h
r
a
s
e
 
p
a
i
r
s
 
[
%
]
source phrase length
BaselineLength-incremental trainingIntersection
Figure 6: Histogram of the phrase lengths present
in the phrase tables.
tropy, weighted by frequency, is 3.1 for the ta-
ble learned with length-incremental training, com-
pared to 2.7 for the heuristically extracted one.
However, the interpolated table, which has the best
performance, lies in between with an average en-
tropy of 2.9. When we consider the histogram of
entropies for the phrase tables in Figure 7, we can
see that in the baseline phrase table 3.8% of the
phrases haven an entropy below 0.5, compared to
0.90% for length-incremental training and 0.16%
for the linear interpolation. Therefore, we can
confirm the observation in (DeNero et al, 2006),
that phrases with a near-zero entropy are undesir-
able for decoding. The distribution of the higher
entropies, however, does not seem to matter for
translation quality. This also gives us a handle for
understanding, why phrase table interpolation of-
ten improves results: It largely seems to eliminate
near-zero entropies from either table.
6.4 Training time
The training was not run under controlled condi-
tions, so we can only give a rough estimate of
how the training times between the different meth-
ods compare. Also, some of the steps were par-
allelized while others are not. To account for
the computational resources needed, we report the
trainig times on a single machine by summing the
times for all parallel and sequential processes.
Heuristc phrase extraction from the word align-
ment took us about 1.7 hours. A single itera-
tion of standard phrase training (leave-one-out)
needs about 24 hours. The first iteration of length-
316
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
0-0.5 0.5-1 1-2 2-3 3-4 >4
#
 
p
h
r
a
s
e
 
p
a
i
r
s
 
[
%
]
entropy
BaselineLength-incrementalLinear interpolation
Figure 7: Histogram of entropies present in the
phrase tables.
incremental training as well as all iterations after
the sixth also took roughly 24 hours. The itera-
tions two through six of length-incremental train-
ing are considerably more expensive due to the
larger size of backoff phrases. Iteration six, with
a maximum backoff phrase size of six words on
source and target side, was the slowest with around
740 hours.
7 Conclusion
In this work we presented a training procedure for
phrase or rule tables in statistical machine trans-
lation. It is based on force-aligning the training
data with a modified version of the translation de-
coder. Different from previous work, we com-
pletely avoid the use of a word alignment on the
bilingual training corpus. Instead, we initialize the
procedure with an empty phrase table and gener-
ate all possible phrases on-the-fly through the con-
cept of backoff phrases. Starting with a maximum
phrase length of m = 1, we increment m in each
iteration, until we reach mmax. Then, we con-
tinue training in a more conventional fashion, al-
lowing creation of new phrases only in fallback
runs. As additional extensions to previous work
we introduce fallback decoding runs for higher
coverage of the data and local language models
for better pre-selection of phrases. The effects
of the most important hyper parameters of our
procedure are discussed and we show how they
were selected in our setup. The experiments are
carried out with a phrase-based decoder on the
IWSLT 2011 Arabic-English shared task. The
trained phrase table slightly outperforms our state-
of-the-art baseline and a linear interpolation yields
an improvement of 0.5% BLEU and 0.5% TER.
The BLEU improvement on test is statistically
significant with 90% confidence. The small over-
lap of 18.5% between the trained and the heuris-
tically extracted phrase table shows how initial-
ization or restrictions based on word alignments
would have biased the training procedure. We also
analyzed the distribution of entropies within the
phrase tables, confirming the previous observation
that fewer near-zero entropy phrases are advanta-
geous for decoding. We also showed that, in our
setup, near-zero entropies are largely eliminated
by phrase table interpolation.
In future work we plan to apply this technique
as a more principled way to train a wider range of
models similar to (Duan et al, 2012). But even
for the phrase models, we have only scratched the
surface of its potential. We hope that by finding
a meaningful way to set the hyper parameters of
our training procedure, better and smaller phrase
tables can be created.
Acknowledgments
This work was partially realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. The material is
also partially based upon work supported by
the DARPA BOLT project under Contract No.
HR0011- 12-C-0015. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constraining the
Phrase-Based, Joint Probability Statistical Transla-
tion Model. In Human Language Technology Conf.
(HLT-NAACL): Proc. Workshop on Statistical Ma-
chine Translation, pages 154?157, New York City,
NY, June.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of
317
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP:
Volume 2 - Volume 2, ACL ?09, pages 782?790,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263?311, June.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proceedings of
the Workshop on Statistical Machine Translation,
pages 31?38, New York City, June.
John DeNero, Alexandre Buchard-Co?te?, and Dan
Klein. 2008. Sampling Alignment Structure under
a Bayesian Translation Model. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 314?323, Honolulu,
October.
Nan Duan, Mu Li, and Ming Zhou. 2012. Forced
derivation tree based model training to statistical
machine translation. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 445?454, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 292?301, Jeju, Republic of Korea, Jul.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 127?133, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 388?395, Barcelona,
Spain, July.
Percy Liang, Alexandre Buchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An End-to-End Discrimina-
tive Approach to Machine Translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 761?768, Sydney, Australia.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proc. of the Conf. on Em-
pirical Methods for Natural Language Processing
(EMNLP), pages 133?139, Philadelphia, PA, July.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Robert C. Moore and Chris Quirk. 2007. An
Iteratively-Trained Segmentation-Free Phrase
Translation Model for Statistical Machine Transla-
tion. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 112?119,
Prague, June.
Markos Mylonakis and Khalil Sima?an. 2008.
Phrase Translation Probabilities with ITG Priors and
Smoothing as Learning Objective. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 630?639, Hon-
olulu, October.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 632?641, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449,
December.
Franz J. Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Markus Saers and Dekai Wu. 2011. Principled induc-
tion of phrasal bilexica. In Proceedings of the 15th
International Conference of the European Associa-
tion for Machine Translation, pages 313?320, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
318
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012a. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Joern Wuebker, Mei-Yuh Hwang, and Chris Quirk.
2012b. Leave-One-Out Phrase Model Training
for Large-Scale Deployment. In Proceedings of
the NAACL 2012 Seventh Workshop on Statisti-
cal Machine Translation, pages 460?467, Montreal,
Canada, June.
Richard Zens. 2008. Phrase-based Statistical Machine
Translation: Models, Search, Training. Ph.D. the-
sis, Computer Science Department, RWTH Aachen
? University of Technology, Germany, February.
319
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 452?463,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Phrase Orientation Model for Hierarchical Machine Translation
Matthias Huck and Joern Wuebker and Felix Rietig and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{huck,wuebker,rietig,ney}@i6.informatik.rwth-aachen.de
Abstract
We introduce a lexicalized reordering
model for hierarchical phrase-based ma-
chine translation. The model scores mono-
tone, swap, and discontinuous phrase ori-
entations in the manner of the one pre-
sented by Tillmann (2004). While this
type of lexicalized reordering model is a
valuable and widely-used component of
standard phrase-based statistical machine
translation systems (Koehn et al, 2007), it
is however commonly not employed in hi-
erarchical decoders.
We describe how phrase orientation prob-
abilities can be extracted from word-
aligned training data for use with hierar-
chical phrase inventories, and show how
orientations can be scored in hierarchi-
cal decoding. The model is empirically
evaluated on the NIST Chinese?English
translation task. We achieve a signifi-
cant improvement of +1.2 %BLEU over
a typical hierarchical baseline setup and
an improvement of +0.7 %BLEU over a
syntax-augmented hierarchical setup. On
a French?German translation task, we
obtain a gain of up to +0.4 %BLEU.
1 Introduction
In hierarchical phrase-based translation (Chiang,
2005), a probabilistic synchronous context-free
grammar (SCFG) is induced from bilingual train-
ing corpora. In addition to continuous lexical
phrases as in standard phrase-based translation,
hierarchical phrases with usually up to two non-
terminals are extracted from the word-aligned par-
allel training data.
Hierarchical decoding is typically carried out
with a parsing-based procedure. The parsing al-
gorithm is extended to handle translation candi-
dates and to incorporate language model scores
via cube pruning (Chiang, 2007). During decod-
ing, a hierarchical translation rule implicitly spec-
ifies the placement of the target part of a sub-
derivation which is substituting one of its non-
terminals in a partial hypothesis. The hierarchical
phrase-based model thus provides an integrated re-
ordering mechanism. The reorderings which are
being conducted by the hierarchical decoder are
a result of the application of SCFG rules, which
generally means that there must have been some
evidence in the training data for each reordering
operation. At first glance one might be tempted to
believe that any additional designated phrase ori-
entation modeling would be futile in hierarchical
translation as a consequence of this. We argue
that such a conclusion is false, and we will pro-
vide empirical evidence in this work that lexical-
ized phrase orientation scoring can be highly ben-
eficial not only in standard phrase-based systems,
but also in hierarchical ones.
The purpose of a phrase orientation model is
to assess the adequacy of phrase reordering dur-
ing search. In standard phrase-based translation
with continuous phrases only and left-to-right hy-
pothesis generation (Koehn et al, 2003; Zens and
Ney, 2008), phrase reordering is implemented by
jumps within the input sentence. The choice of the
best order for the target sequence is made based
on the language model score of this sequence and
a distortion cost that is computed from the source-
side jump distances. Though the space of admis-
sible reorderings is in most cases contrained by a
maximum jump width or coverage-based restric-
tions (Zens et al, 2004) for efficiency reasons,
the basic approach of arbitrarily jumping to un-
covered positions on source side is still very per-
missive. Lexicalized reordering models assist the
decoder in taking a good decision. Phrase-based
decoding allows for a straightforward integration
of lexicalized reordering models which assign
452
different scores depending on how a currently
translated phrase has been reordered with respect
to its context. Popular lexicalized reordering mod-
els for phrase-based translation distinguish three
orientation classes: monotone, swap, and discon-
tinuous (Tillmann, 2004; Koehn et al, 2007; Gal-
ley and Manning, 2008). To obtain such a model,
scores for the three classes are calculated from the
counts of the respective orientation occurrences in
the word-aligned training data for each extracted
phrase. The left-to-right orientation of phrases
during phrase-based search can be easily deter-
mined from the start and end positions of con-
tinuous phrases. Approximations may need to be
adopted for the right-to-left scoring direction.
The utility of phrase orientation models in stan-
dard phrase-based translation is plausible and has
been empirically established in practice. In hierar-
chical phrase-based translation, some other types
of lexicalized reordering models have been inves-
tigated recently (He et al, 2010a; He et al, 2010b;
Hayashi et al, 2010; Huck et al, 2012a), but
in none of them are the orientation scores condi-
tioned on the lexical identity of each phrase in-
dividually. These models are rather word-based
and applied on block boundaries. Experimental
results obtained with these other types of lexical-
ized reordering models have been very encourag-
ing, though.
There are certain reasons why assessing the ad-
equacy of phrase reordering should be useful in
hierarchical search:
? Albeit phrase reorderings are always a result
of the application of SCFG rules, the decoder
is still able to choose from many different
parses of the input sentence.
? The decoder can furthermore choose from
many translation options for each given
parse, which result in different reorderings
and different phrases being embedded in the
reordering non-terminals.
? All other models only weakly connect an em-
bedded phrase with the hierarchical phrase it
is placed into, in particular as the set of non-
terminals of the hierarchical grammar only
contains two generic non-terminal symbols.
We therefore investigate phrase orientation mod-
eling for hierarchical translation in this work.
2 Outline
The remainder of the paper is structured as fol-
lows: We briefly outline important related pub-
lications in the following section. We subse-
quently give a summary of some essential aspects
of the hierarchical phrase-based translation ap-
proach (Section 4). Phrase orientation modeling
and a way in which a phrase orientation model can
be trained for hierarchical phrase inventories are
explained in Section 5. In Section 6 we introduce
an extension of hierarchical search which enables
the decoder to score phrase orientations. Empiri-
cal results are presented in Section 7. We conclude
the paper in Section 8.
3 Related Work
Hierarchical phrase-based translation was pro-
posed by Chiang (2005). He et al (2010a) inte-
grated a maximum entropy based lexicalized re-
ordering model with word- and class-based fea-
tures. Different classifiers for different rule pat-
terns are trained for their model (He et al,
2010b). A comparable discriminatively trained
model which applies a single classifier for all types
of rules was developed by Huck et al (2012a).
Hayashi et al (2010) explored the word-based re-
ordering model by Tromble and Eisner (2009) in
hierarchical translation.
For standard phrase-based translation, Galley
and Manning (2008) introduced a hierarchical
phrase orientation model. Similar to previous ap-
proaches (Tillmann, 2004; Koehn et al, 2007), it
distinguishes the three orientation classes mono-
tone, swap, and discontinuous. However, it differs
in that it is not limited to model local reordering
phenomena, but allows for phrases to be hierarchi-
cally combined into blocks in order to determine
the orientation class. This has the advantage that
probability mass is shifted from the rather uninfor-
mative default category discontinuous to the other
two orientation classes, which model the location
of a phrase more specifically. In this work, we
transfer this concept to a hierarchical phrase-based
machine translation system.
4 Hierarchical Phrase-Based Translation
The non-terminal set of a standard hierarchical
grammar comprises two symbols which are shared
by source and target: the initial symbol S and one
generic non-terminal symbol X . The generic non-
terminal X is used as a placeholder for the gaps
453
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(a) Monotone phrase orientation.
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(b) Swap phrase orientation.
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(c) Discontinuous phrase orientation.
Figure 1: Extraction of the orientation classes monotone, swap, and discontinuous from word-aligned
training samples. The examples show the left-to-right orientation of the shaded phrases. The dashed
rectangles indicate how the predecessor words are merged into blocks with regard to their word align-
ment.
within the right-hand side of hierarchical transla-
tion rules as well as on all left-hand sides of the
translation rules that are extracted from the paral-
lel training corpus.
Extracted rules of a standard hierarchical gram-
mar are of the form X ? ??, ?,? ? where ??, ??
is a bilingual phrase pair that may contain X , i.e.
? ? ({X } ? VF )+ and ? ? ({X } ? VE)+, where
VF and VE are the source and target vocabulary,
respectively. The non-terminals on the source side
and on the target side of hierarchical rules are
linked in a one-to-one correspondence. The ? re-
lation defines this one-to-one correspondence. In
addition to the extracted rules, a non-lexicalized
initial rule
S ? ?X?0, X?0? (1)
is engrafted into the hierarchical grammar, as well
as a special glue rule
S ? ?S?0X?1, S?0X?1? (2)
that the system can use for serial concatenation
of phrases as in monotonic phrase-based transla-
tion. The initial symbol S is the start symbol of
the grammar.
Hierarchical search is conducted with a cus-
tomized version of the CYK+ parsing algo-
rithm (Chappelier and Rajman, 1998) and cube
pruning (Chiang, 2007). A hypergraph which rep-
resents the whole parsing space is built employing
CYK+. Cube pruning operates in bottom-up topo-
logical order on this hypergraph and expands at
most k derivations at each hypernode.
5 Modeling Phrase Orientation for
Hierarchical Machine Translation
The phrase orientation model we are using was
introduced by Galley and Manning (2008). To
model the sequential order of phrases within the
global translation context, the three orientation
classes monotone (M), swap (S) and discontinu-
ous (D) are distinguished, each in both left-to-
right and right-to-left direction. In order to cap-
ture the global rather than the local context, previ-
ous phrases can be merged into blocks if they are
consistent with respect to the word alignment. A
phrase is in monotone orientation if a consistent
monotone predecessor block exists, and in swap
orientation if a consistent swap predecessor block
exists. Otherwise it is in discontinuous orientation.
Given a sequence of source words fJ1 and a se-
quence of target words eI1, a block ?f j2j1 , ei2i1? (with
1 ? j1 ? j2 ? J and 1 ? i1 ? i2 ? I)
is consistent with respect to the word alignment
A ? {1, ..., I} ? {1, ..., J} iff
?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2
? ?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2.
(3)
Consistency is based upon two conditions in this
definition: (1.) At least one source and target po-
sition within the block must be aligned, and (2.)
words from inside the source interval may only
be aligned to words from inside the target inter-
val and vice versa. These are the same condi-
tions as those that are applied for the extraction of
454
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(a) A monotone orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 1
N(S|f2X?0f4, e2X?0e4) = 0
N(D|f2X?0f4, e2X?0e4) = 0
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(b) Another monotone orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 2
N(S|f2X?0f4, e2X?0e4) = 0
N(D|f2X?0f4, e2X?0e4) = 0
f1
f2
f3
f4
f5
e1 e2 e3 e4 e5
target
so
ur
ce
(c) A swap orientation.
Left-to-right orientation counts:
N(M |f2X?0f4, e2X?0e4) = 2
N(S|f2X?0f4, e2X?0e4) = 1
N(D|f2X?0f4, e2X?0e4) = 0
Figure 2: Accumulation of orientation counts for hierarchical phrases during extraction. The hierarchical
phrase ?f2X?0f4, e2X?0e4? (dark shaded) can be extracted from all the three training samples. Its
orientation is identical to the orientation of the continuous phrase (lightly shaded) which the sub-phrase
is cut out of, respectively. Note that the actual lexical content of the sub-phrase may differ. For instance,
the sub-phrase ?f3, e3? is being cut out in Fig. 2a, and the sub-phrase ?f6, e6? is being cut out in Fig. 2b.
standard continuous phrases. The only difference
is that length constraints are applied to phrases, but
not to blocks.
Figure 1 illustrates the extraction of monotone,
swap, and discontinuous orientation classes in
left-to-right direction from word-aligned bilingual
training samples. The right-to-left direction works
analogously.
We found that this concept can be neatly
plugged into the hierarchical phrase-based trans-
lation paradigm, without having to resort to ap-
proximations in decoding, which is necessary to
determine the right-to-left orientation in a standard
phrase-based system (Cherry et al, 2012). To train
the orientations, the extraction procedure from the
standard phrase-based version of the reordering
model can be used with a minor extension. The
model is trained on the same word-aligned data
from which the translation rules are extracted. For
each training sentence, we extract all phrases of
unlimited length that are consistent with the word
alignment, and store their corners in a matrix. The
corners are distinguished by their location: top-
left, top-right, bottom-left, and bottom-right. For
each bilingual phrase, we determine its left-to-
right and right-to-left orientation by checking for
adjacent corners.
The lexicalized orientation probability for the
orientation O ? {M,S,D} and the phrase pair
??, ?? is estimated as its smoothed relative fre-
quency:
p(O) = N(O)?
O??{M,S,D}N(O?)
(4)
p(O|?, ?) = ? ? p(O) +N(O|?, ?)
? +
?
O??{M,S,D}N(O?|f? , e?)
.
(5)
Here, N(O) denotes the global count and
N(O|?, ?) the lexicalized count for the orienta-
tion O. ? is a smoothing constant.
To determine the orientation frequency for a hi-
erarchical phrase with non-terminal symbols, the
orientation counts of all those phrases are accu-
mulated from which a sub-phrase is cut out and
replaced by a non-terminal symbol to obtain this
hierarchical phrase. Figure 2 gives an example.
Negative logarithms of the values are used as
costs in the log-linear model combination (Och
and Ney, 2002). Cost 0 for all orientations is as-
signed to the special rules which are not extracted
from the training data (initial and glue rule).
455
f1
f2
f3
45e
f
	1 	2 	3 45e	
target
so
ur
ce
(a) Monotone non-terminal orientation.
f1
234
f5
fe
f
	1 	5 	e 234	
target
so
ur
ce
(b) Swap non-terminal orientation.
f1
f2
345
fe
f
	1 	2 345 	
target
so
ur
ce
	e
(c) Discontinuous non-terminal orienta-
tion.
Figure 3: Scoring with the orientation classes monotone, swap, and discontinuous. Each picture shows
exactly one hierarchical phrase. The block which replaces the non-terminalX during decoding is embed-
ded with the orientation of this non-terminal X within the hierarchical phrase. The examples show the
left-to-right orientation of the non-terminal. The left-to-right orientation can be detected from the word
alignment of the hierarchical phrase, except for cases where the non-terminal is in boundary position on
target side.
6 Phrase Orientation Scoring in
Hierarchical Decoding
Our implementation of phrase orientation scoring
in hierarchical decoding is based on the observa-
tion that hierarchical rule applications, i.e. the us-
age of rules with non-terminals within their right-
hand sides, settle the target sequence order. Mono-
tone, swap, or discontinuous orientations of blocks
are each due to monotone, swap, or discontinuous
placement of non-terminals which are being sub-
stituted by these blocks.
The problem of phrase orientation scoring can
thus be mostly reduced to three steps which need
to be carried out whenever a hierarchical rule is
applied:
1. Determining the orientations of the non-
terminals in the rule.
2. Retrieving the proper orientation cost of the
topmost rule application in the sub-derivation
which corresponds to the embedded block for
the respective non-terminal.
3. Applying the orientation cost to the log-linear
model combination for the current derivation.
The orientation of a non-terminal in a hierarchi-
cal rule is dependent on the word alignments in
its context. Figure 3 depicts three examples.1 We
however need to deal with special cases where a
non-terminal orientation cannot be established at
the moment when the hierarchical rule is consid-
ered. We first describe the non-degenerate case
(Section 6.1). Afterwards we briefly discuss our
strategy in the special situation of boundary non-
terminals where the non-terminal orientation can-
not be determined from information which is in-
herent to the hierarchical rule under consideration
(Section 6.3).
We focus on left-to-right orientation scoring;
right-to-left scoring is symmetric.
6.1 Determining Orientations
In order to determine the orientation class of a
non-terminal, we rely on the word alignments
within the phrases. With each phrase, we store
the alignment matrix that has been seen most fre-
quently during phrase extraction. Non-terminal
symbols on target side are assumed to be aligned
to the respective non-terminal symbols on source
1Note that even maximal consecutive lexical intervals (ei-
ther on source or target side) are not necessarily aligned in
a way which makes them consistent bilingual blocks. In
Fig. 3a, e4 is for instance aligned both below and above
the non-terminal. In Fig. 3c, neither ?f1f2, e1e2? nor
?f1f2, e3e4? would be valid continuous phrases (the same
holds for ?f3f4, e1e2? and ?f3f4, e3e4?). We actually need
the generalization of the phrase orientation model to hierar-
chical phrases as described in Section 5 for this reason. Oth-
erwise we would be able to just score neighboring consistent
sub-blocks with a model that does not account for hierarchi-
cal phrases with non-terminals.
456
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(a) Last previous aligned target position.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(b) Initial box.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(c) Expansion of the initial box.
f1
f2
f3
45e
f
	1 	2 	3 45e	
f

		

target
so
ur
ce
(d) The final box is a consistent left-to-right mono-
tone predecessor block of the non-terminal.
Figure 4: Determining the orientation class during decoding. Starting from the last previous aligned
target position, a box is spanned across the relevant alignment links onto the corner of the non-terminal.
The box is then checked for consistency.
side according to the ? relation. In the alignment
matrix, the rows and columns of non-terminals can
obviously contain only exactly this one alignment
link.
Starting from the last previous aligned target po-
sition to the left of the non-terminal, the algorithm
expands a box that spans across the other rele-
vant alignment links onto the corner of the non-
terminal. Afterwards it checks whether the areas
on the opposite sides of the non-terminal position
are non-aligned in the source and target intervals
of this box. The non-terminal is in discontinu-
ous orientation if the box is not a consistent block.
If the box is a consistent block, the non-terminal
is in monotone orientation if its source-side posi-
tion is larger than the maximum of the source-side
interval of the box, and in swap orientation if its
source-side position is smaller than the minimum
of the source-side interval of the box.
Figure 4 illustrates how the procedure operates.
In left-to-right direction, an initial box is spanned
from the last previous aligned target position to
the lower (monotone) or upper (swap) left cor-
ner of the non-terminal. In the example, starting
from ?f3, e5? (Fig. 4a), this initial box is spanned
to the lower left corner by iterating from f3 to
f4 and expanding its target interval to the mini-
mum aligned target position within these two rows
of the alignment matrix. The initial box cov-
ers ?f3f4, e3e4e5? (Fig. 4b). The procedure then
repeatedly checks whether the box needs to be
expanded?alternating to the bottom (monotone)
or top (swap) and to the left?until no alignment
links below or to the left of the box break the
consistency. Two box expansion are conducted
in the example: the first one expands the ini-
tial box below, resulting in a larger box which
covers ?f1f2f3f4, e3e4e5? (Fig. 4c); the second
457
f1
f2
345
e1345
target
so
ur
ce
(a) Left boundary non-
terminal that can be placed
in left-to-right monotone or
discontinuous orientation
when the phrase is embedded
into another one.
f1
f2
345
e1345
target
so
ur
ce
(b) Left boundary non-
terminal that can be placed
in left-to-right discontinuous
or swap orientation when
the phrase is embedded into
another one.
f1
f2345
e1 345
target
so
ur
ce
(c) Left boundary non-
terminal that can be placed in
left-to-right monotone, swap,
or discontinuous orientation
when the phrase is embedded
into another one.
f1
f2345
e1345
target
so
ur
ce
(d) Left boundary non-
terminal that can only be
placed in left-to-right dis-
continuous orientation when
the phrase is embedded into
another one.
Figure 5: Left boundary non-terminal symbols. Orientations the non-terminal can eventually turn out to
get placed in differ depending on existing alignment links in the rest of the phrase. Delayed left-to-right
scoring is not required in cases as in Fig. 5d. Fractional costs for the possible orientations are temporarily
applied in the other cases and recursively corrected as soon as an orientation is constituted in an upper
hypernode.
one expands this new box to the left, resulting in
a final box which covers ?f1f2f3f4, e1e2e3e4e5?
(Fig. 4d) and does not need to be expanded to-
wards the lower left corner any more. Afterwards
the procedure examines whether the final box is
a consistent block by inspecting whether the ar-
eas on the opposite side of the non-terminal po-
sition are non-aligned in the intervals of the box
(areas with waved lines in the Fig. 4d). These ar-
eas do not contain alignment links in the example:
the orientation class of the non-terminal is mono-
tone as it has a consistent left-to-right monotone
predecessor block. (Suppose an alignment link
?f5, e2? would break the consistency: the orienta-
tion class would then be discontinuous as the final
box would not be a consistent block.)
Orientations of non-terminals could basically be
precomputed and stored in the translation table.
We however compute them on demand during de-
coding. The computational overhead did not seem
to be too severe in our experiments.
6.2 Scoring Orientations
Once the orientation is determined, the proper ori-
entation cost of the embedded block needs to be
retrieved. We access the topmost rule application
in the sub-derivation which corresponds to the em-
bedded block for the respective non-terminal and
read the orientation model costs for this rule. The
special case of delayed scoring for boundary non-
terminals as described in the subsequent section is
recursively processed if necessary. The retrieved
orientation costs of the embedded blocks of all
non-terminals are finally added to the log-linear
model combination for the current derivation.
6.3 Boundary Non-Terminals
Cases where a non-terminal orientation cannot be
established at the moment when the hierarchi-
cal rule is considered arise when a non-terminal
symbol is in a boundary position on target side.
We define a non-terminal to be in (left or right)
boundary position iff no symbols are aligned be-
tween the phrase-internal target-side index of the
non-terminal and the (left or right) phrase bound-
ary. Left boundary positions of non-terminals
are critical for left-to-right orientation scoring,
right boundary positions for right-to-left orienta-
tion scoring. We denote non-terminals in bound-
ary position as boundary non-terminals.
The procedure as described in Section 6.1 is not
applicable to boundary non-terminals because a
last previous aligned target position does not ex-
ist. If it is impossible to determine the final non-
terminal orientation in the hypothesis from infor-
mation which is inherent to the phrase, we are
forced to delay the orientation scoring of the em-
bedded block. Our solution in these cases is to
heuristically add fractional costs of all orientations
the non-terminal can still eventually turn out to get
placed in (cf. Figure 5). We do so because not
adding an orientation cost to the derivation would
give it an unjustified advantage over other ones.
As soon as an orientation is constituted in an up-
458
per hypernode, any heuristic and actual orientation
costs can be collected by means of a recursive call.
Note that monotone or swap orientations in upper
hypernodes can top-down transition into discon-
tinuous orientations for boundary non-terminals,
depending on existing phrase-internal alignment
links in the context of the respective boundary
non-terminal. In the derivation at the upper hyper-
node, the heuristic costs are subtracted and the cor-
rect actual costs added. Delayed scoring can lead
to search errors; in order to keep them confined,
the delayed scoring needs to be done separately
for all derivations, not just for the first-best sub-
derivations along the incoming hyperedges.
7 Experiments
We evaluate the effect of phrase orienta-
tion scoring in hierarchical translation on the
Chinese?English 2008 NIST task2 and on the
French?German language pair using the standard
WMT3 newstest sets for development and testing.
7.1 Experimental Setup
We work with a Chinese?English parallel train-
ing corpus of 3.0 M sentence pairs (77.5 M Chi-
nese / 81.0 M English running words). To train the
German?French baseline system, we use 2.0 M
sentence pairs (53.1 M French / 45.8 M German
running words) that are partly taken from the
Europarl corpus (Koehn, 2005) and have partly
been collected within the Quaero project.4
Word alignments are created by aligning the
data in both directions with GIZA++5 and sym-
metrizing the two trained alignments (Och and
Ney, 2003). When extracting phrases, we ap-
ply several restrictions, in particular a maximum
length of ten on source and target side for lexi-
cal phrases, a length limit of five on source and
ten on target side for hierarchical phrases (includ-
ing non-terminal symbols), and no more than two
non-terminals per phrase.
A standard set of models is used in the base-
lines, comprising phrase translation probabilities
and lexical translation probabilities in both direc-
tions, word and phrase penalty, binary features
marking hierarchical rules, glue rule, and rules
2http://www.itl.nist.gov/iad/mig/
tests/mt/2008/
3http://www.statmt.org/wmt13/
translation-task.html
4http://www.quaero.org
5http://code.google.com/p/giza-pp/
with non-terminals at the boundaries, three sim-
ple count-based binary features, phrase length ra-
tios, and a language model. The language models
are 4-grams with modified Kneser-Ney smooth-
ing (Kneser and Ney, 1995; Chen and Goodman,
1998) which have been trained with the SRILM
toolkit (Stolcke, 2002).
Model weights are optimized against BLEU (Pa-
pineni et al, 2002) with MERT (Och, 2003) on
100-best lists. For Chinese?English we employ
MT06 as development set, MT08 is used as unseen
test set. For German?French we employ news-
test2009 as development set, newstest2008, news-
test2010, and newstest2011 are used as unseen test
sets. During decoding, a maximum length con-
straint of ten is applied to all non-terminals except
the initial symbol S . Translation quality is mea-
sured in truecase with BLEU and TER (Snover et
al., 2006). The results on MT08 are checked for
statistical significance over the baseline. Confi-
dence intervals have been computed using boot-
strapping for BLEU and Cochran?s approximate
ratio variance for TER (Leusch and Ney, 2009).
7.2 Chinese?English Experimental Results
Table 1 comprises all results of our empirical eval-
uation on the Chinese?English task.
We first compare the performance of the phrase
orientation model in left-to-right direction only
with the performance of the phrase orientation
model in left-to-right and right-to-left direction
(bidirectional). In all experiments, monotone,
swap, and discontinuous orientation costs are
treated as being from different feature functions
in the log-linear model combination: we assign
a separate scaling factor to each of the orienta-
tions. We have three more scaling factors than in
the baseline for left-to-right direction only, and six
more scaling factors for bidirectional phrase ori-
entation scoring. As can be seen from the results
table, the left-to-right model already yields a gain
of 1.1 %BLEU over the baseline on the unseen test
set (MT08). The bidirectional model performs just
slightly better (+1.2 %BLEU over the baseline).
With both models, the TER is reduced significantly
as well (-1.1 / -1.3 compared to the baseline). We
adopted the discriminative lexicalized reordering
model (discrim. RO) that has been suggested by
Huck et al (2012a) for comparison purposes. The
phrase orientation model provides clearly better
translation quality in our experiments.
459
MT06 (Dev) MT08 (Test)
NIST Chinese?English BLEU [%] TER [%] BLEU [%] TER [%]
HPBT Baseline 32.6 61.2 25.2 66.6
+ discrim. RO 33.0 61.3 25.8 66.0
+ phrase orientation (left-to-right) 33.3 60.7 26.3 65.5
+ phrase orientation (bidirectional) 33.2 60.6 26.4 65.3
+ swap rule 32.8 61.7 25.8 66.6
+ discrim. RO 33.1 61.2 26.0 66.1
+ phrase orientation (bidirectional) 33.3 60.7 26.5 65.3
+ binary swap feature 33.2 61.0 25.9 66.2
+ discrim. RO 33.2 61.3 26.2 66.1
+ phrase orientation (bidirectional) 33.6 60.5 26.6 65.1
+ soft syntactic labels 33.4 60.8 26.1 66.4
+ phrase orientation (bidirectional) 33.7 60.1 26.8 65.1
+ phrase-level s2t+t2s DWL + triplets 34.3 60.1 27.7 65.0
+ discrim. RO 34.8 59.8 27.7 64.7
+ phrase orientation (bidirectional) 35.3 59.0 28.4 63.7
Table 1: Experimental results for the NIST Chinese?English translation task (truecase). On the test set,
bold font indicates results that are significantly better than the baseline (p < .05).
As a next experiment, we bring in more re-
ordering capabilities by augmenting the hierarchi-
cal grammar with a single swap rule
X ? ?X?0X?1,X?1X?0? (6)
supplementary to the initial rule and glue rule.
The swap rule allows adjacent phrases to be trans-
posed. The setup with swap rule and bidirectional
phrase orientation model is about as good as the
setup with just the bidirectional phrase orienta-
tion model and no swap rule. If we furthermore
mark the swap rule with a binary feature (binary
swap feature), we end up at an improvement of
+1.4 %BLEU over the baseline. The phrase ori-
entation model again provides higher translation
quality than the discriminative reordering model.
In a third experiment, we investigate whether
the phrase orientation model also has a positive in-
fluence when integrated into a syntax-augmented
hierarchical system. We configured a hierarchi-
cal setup with soft syntactic labels (Stein et al,
2010), a syntactic enhancement in the manner of
preference grammars (Venugopal et al, 2009). On
MT08, the syntax-augmented system performs 0.9
%BLEU above the baseline setup. We achieve an
additional improvement of +0.7 %BLEU and -1.3
TER by including the bidirectional phrase orien-
tation model. Interestingly, the translation quality
of the setup with soft syntactic labels (but with-
out phrase orientation model) is worse than of the
setup with phrase orientation model (but without
soft syntactic labels) on MT08. The combination
of both extensions provides the best result, though.
In a last experiment, we finally took a very
strong setup which improves over the baseline by
2.5 %BLEU through the integration of phrase-level
discriminative word lexicon (DWL) models and
triplet lexicon models in source-to-target (s2t) and
target-to-source (t2s) direction. The models have
been presented by Hasan et al (2008), Bangalore
et al (2007), and Mauser et al (2009). We apply
them in a similar manner as proposed by Huck et
al. (2011). In this strong setup, the discriminative
reordering model gives gains on the development
set which barely carry over to the test set. Adding
the bidirectional phrase orientation model, in con-
trast, results in a nice gain of +0.7 %BLEU and a
reduction of 1.3 points in TER on the test set, even
on top of the DWL and triplet lexicon models.
7.3 French?German Experimental Results
Table 2 comprises the results of our empirical eval-
uation on the French?German task.
The left-to-right phrase orientation model
boosts the translation quality by up to 0.3 %BLEU.
The reduction in TER is in a similar order of
magnitude. The bidirectional model performs a
bit better again, with an advancement of up to
0.4 %BLEU and a maximal reduction in TER of
0.6 points.
460
newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER
French?German [%] [%] [%] [%] [%] [%] [%] [%]
HPBT Baseline 15.2 71.7 15.0 71.7 15.7 69.5 14.2 72.2
+ phrase orientation (left-to-right) 15.1 71.4 15.3 71.4 15.9 69.2 14.5 71.8
+ phrase orientation (bidirectional) 15.4 71.1 15.4 71.3 15.9 69.1 14.6 71.6
Table 2: Experimental results for the French?German translation task (truecase). newstest2009 is used
as development set.
8 Conclusion
In this paper, we introduced a phrase orientation
model for hierarchical machine translation. The
training of a lexicalized reordering model which
assigns probabilities for monotone, swap, and dis-
continuous orientation of phrases was generalized
from standard continuous phrases to hierarchical
phrases. We explained how phrase orientation
scoring can be implemented in hierarchical decod-
ing and conducted a number of experiments on a
Chinese?English and a French?German transla-
tion task. The results indicate that phrase orienta-
tion modeling is a very suitable enhancement of
the hierarchical paradigm.
Our implementation will be released as part of
Jane (Vilar et al, 2010; Vilar et al, 2012; Huck
et al, 2012b), the RWTH Aachen University open
source statistical machine translation toolkit.6
Acknowledgments
This work was partly achieved as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This material is also
partly based upon work supported by the DARPA
BOLT project under Contract No. HR0011-12-
C-0015. Any opinions, findings and conclu-
sions or recommendations expressed in this ma-
terial are those of the authors and do not neces-
sarily reflect the views of the DARPA. The re-
search leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no 287658.
References
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical Machine Translation through
6http://www.hltpr.rwth-aachen.de/jane/
Global Lexical Selection and Sentence Reconstruc-
tion. In Proc. of the Annual Meeting of the Assoc. for
Computational Linguistics (ACL), pages 152?159,
Prague, Czech Republic, June.
Jean-Ce?dric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133?137,
Paris, France, April.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, USA, August.
Colin Cherry, Robert C. Moore, and Chris Quirk.
2012. On Hierarchical Re-ordering and Permuta-
tion Parsing for Phrase-based Decoding. In Proc. of
the Workshop on Statistical Machine Translation
(WMT), pages 200?209, Montre?al, Canada, June.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263?270, Ann Ar-
bor, MI, USA, June.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of the Conf. on Empirical Meth-
ods for Natural Language Processing (EMNLP),
pages 847?855, Honolulu, HI, USA, October.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet Lexicon Mod-
els for Statistical Machine Translation. In Proc. of
the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 372?381, Hon-
olulu, HI, USA, October.
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh,
Kevin Duh, and Seiichi Yamamoto. 2010. Hi-
erarchical Phrase-based Machine Translation with
Word-based Reordering Model. In Proc. of the
Int. Conf. on Computational Linguistics (COLING),
pages 439?446, Beijing, China, August.
461
Zhongjun He, Yao Meng, and Hao Yu. 2010a. Extend-
ing the Hierarchical Phrase Based Model with Max-
imum Entropy Based BTG. In Proc. of the Conf. of
the Assoc. for Machine Translation in the Americas
(AMTA), Denver, CO, USA, October/November.
Zhongjun He, Yao Meng, and Hao Yu. 2010b. Max-
imum Entropy Based Phrase Reordering for Hier-
archical Phrase-based Translation. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 555?563, Cambridge,
MA, USA, October.
Matthias Huck, Saab Mansour, Simon Wiesler, and
Hermann Ney. 2011. Lexicon Models for Hierar-
chical Phrase-Based Machine Translation. In Proc.
of the Int. Workshop on Spoken Language Transla-
tion (IWSLT), pages 191?198, San Francisco, CA,
USA, December.
Matthias Huck, Stephan Peitz, Markus Freitag, and
Hermann Ney. 2012a. Discriminative Reordering
Extensions for Hierarchical Phrase-Based Machine
Translation. In Proc. of the Annual Conf. of the
European Assoc. for Machine Translation (EAMT),
pages 313?320, Trento, Italy, May.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012b. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-Off for M-gram Language Modeling. In
Proc. of the Int. Conf. on Acoustics, Speech, and Sig-
nal Processing (ICASSP), volume 1, pages 181?184,
Detroit, MI, USA, May.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Human Language Technology Conf. / North
American Chapter of the Assoc. for Computational
Linguistics (HLT-NAACL), pages 127?133, Edmon-
ton, Canada, May/June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of the Annual Meeting of the Assoc. for
Computational Linguistics (ACL), Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proc. of the MT
Summit X, Phuket, Thailand, September.
Gregor Leusch and Hermann Ney. 2009. Edit dis-
tances with block movements and error rate confi-
dence estimates. Machine Translation, 23(2):129?
140, December.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Proc. of the Conf. on Empirical Methods for Natu-
ral Language Processing (EMNLP), pages 210?218,
Singapore, August.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 295?302, Philadelphia, PA, USA, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proc. of
the Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311?318, Philadelphia, PA,
USA, July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proc. of the Conf. of the Assoc. for
Machine Translation in the Americas (AMTA), pages
223?231, Cambridge, MA, USA, August.
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Proc.
of the Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), Denver, CO, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 2,
pages 901?904, Denver, CO, USA, September.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Boston, MA,
USA.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Proc.
of the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 1007?1016, Sin-
gapore, August.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
462
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Hu-
man Language Technology Conf. / North American
Chapter of the Assoc. for Computational Linguistics
(HLT-NAACL), pages 236?244, Boulder, CO, USA,
June.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchical
Translation, Extended with Reordering and Lexicon
Models. In Proc. of the Workshop on Statistical Ma-
chine Translation (WMT), pages 262?270, Uppsala,
Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Richard Zens and Hermann Ney. 2008. Improvements
in Dynamic Programming Beam Search for Phrase-
Based Statistical Machine Translation. In Proc. of
the Int. Workshop on Spoken Language Translation
(IWSLT), pages 195?205, Waikiki, HI, USA, Octo-
ber.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering Constraints for
Phrase-Based Statistical Machine Translation. In
Proc. of the Int. Conf. on Computational Linguis-
tics (COLING), pages 205?211, Geneva, Switzer-
land, August.
463
Proceedings of the Second Workshop on Hybrid Approaches to Translation, page 7,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Statistical MT Systems Revisited:
How much Hybridity do they have?
Hermann Ney
RWTH Aachen University, Aachen and DIGITEO Chair, LIMSI-CNRS, Paris
Lehrstuhl fu?r Informatik 6
RWTH Aachen
Ahornstr. 55
52056 Aachen
ney@informatik.rwth-aachen.de
Abstract
The statistical approach to MT started about
twenty-five years ago and has now been widely ac-
cepted as an alternative to the classical approach
with manually designed rules. Among the attrac-
tive properties of the statistical approach is its ca-
pability to learn the translation models automati-
cally from a (sufficiently) large amount of source-
target sentence pairs. Thus the need for the manual
design of suitable rules and for human interaction
can be reduced dramatically when developing an
MT system for a new application or language pair.
The idea of hybrid MT is to combine the ad-
vantages of both the rule-based and statistical ap-
proaches. In practice, most statistical MT sys-
tems make use of manually designed rules in or-
der to improve the MT accuracy. We revisit the
RWTH systems in order to study the effect of typ-
ical preprocessing steps based on manually de-
signed rules. The RWTH systems cover various
tasks (e.g. news, patents, lectures) and various lan-
guages (e.g. Arabic, Chinese, English, Japanese).
The preprocessing steps may include a categoriza-
tion of numbers, date and time expressions, a word
decomposition based on morphological analysis
and explicit word re-ordering based on a syntactic
analysis. In general, the preprocessing steps may
depend heavily on the language pair under consid-
eration.
We will also address concepts that aim at a
tighter integration of the conventional rule-based
and the statistical approaches. We will consider
the implications of such a tight integration for the
architecture of an MT system.
7
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 48?56,
Gothenburg, Sweden, 26-27 April 2014. c?2014 Association for Computational Linguistics
German Compounds and Statistical Machine Translation.
Can they get along?
Carla Parra Escart?n
University of Bergen
Bergen, Norway
carla.parra@uib.no
Stephan Peitz
RWTH Aachen University
Aachen, Germany
peitz@cs.rwth-aachen.de
Hermann Ney
RWTH Aachen University
Aachen, Germany
ney@cs.rwth-aachen.de
Abstract
This  paper  reports  different  experiments
created  to  study  the  impact  of  using
linguistics  to  preprocess  German  com-
pounds  prior  to  translation  in  Statistical
Machine  Translation  (SMT).  Compounds
are a known challenge both in Machine
Translation (MT) and Translation in gen-
eral as well as in other Natural Language
Processing (NLP) applications. In the case
of SMT, German compounds are split into
their constituents to decrease the number
of  unknown words  and  improve  the  re-
sults of evaluation measures like the Bleu
score. To assess to which extent it is neces-
sary to deal with German compounds as a
part of preprocessing in SMT systems, we
have  tested  different  compound splitters
and strategies, such as adding lists of com-
pounds and their translations to the train-
ing set. This  paper  summarizes  the re-
sults of our experiments and attempts to
yield better translations of German nom-
inal compounds into Spanish and shows
how our approach improves by up to 1.4
Bleu points with respect to the baseline.
1 Introduction
The pair of languages German?Spanish is not a
widely researched combination in Statistical Ma-
chine Translation (SMT) and yet it is a challenging
one as both languages belong to different language
families (Germanic and Romance) and their char-
acteristics and inner structure differ greatly. As it
may happen with other language pair combinations
involving a Germanic and a Romance language,
when it comes to the translation of German com-
pounds into Spanish, the challenge is greater than
when translating into other Germanic languages
such as English. The translation of the German
compound does not correspond to the translation
of its parts, but rather constitutes a phraseological
structure which must conform the Spanish gram-
matical rules. Examples 1 and 2 show the split-
tings of the German compoundsWarmwasserbere-
itung andW?rmer?ckgewinnungssysteme and their
translations into English and Spanish.
(1) Warm
caliente
warm
Wasser
agua
water
Bereitung
preparaci?n
production
[ES]: ?Preparaci?n de agua caliente?
[EN]: ?Warm water production?
(2) W?rme
calor
heat
R?ckgewinnung
recuperaci?n
recovery
s
?
?
Systeme
sistemas
Systems
[ES]: ?sistemas de recuperaci?n de calor?
[EN]: ?heat recovery systems?
As may be observed in Examples 1 and 2, in
Spanish not only there is word reordering, but also
there is usage of other word categories such as
prepositions. While the examples above are quite
simple, the work done by researchers such as An-
gele (1992), G?mez P?rez (2001) and Oster (2003)
for the pair of languages German?Spanish shows
that the translational equivalences in Spanish not
only are very varied, but also unpredictable to a
certain extent. Thus, while a mere compound split-
ting strategy may work for English, in the case of
Spanish further processing is required to yield the
correct translation.
According  to  Atkins  et  al.  (2001)
1
, complex
nominals  (i.e. nominal  compounds  and  some
nominal phrases) are to be considered a special
type of MWE because they do have some partic-
ular features and to some extent they behave as
a single unit because they refer to a single con-
cept. Despite focusing on another language pair
1
Appendix  F of  Deliverable  D2.2-D3.2  of  the  ISLE
project.
48
(English?Italian), in the case of our language pair
(German?Spanish) a similar claim could be done.
Besides, the issue of compounds being translated
into phrases in different languages is essentially a
MWE problem.
In this paper, we report on the results of our
research facing this  particular  challenge. More
concretely, Section 2 briefly discusses the prob-
lem of compounds in general and Section 3 de-
scribes our case of study. Subsection 3.1 briefly
discusses the large presence of German nominal
compounds in specialized corpora and presents the
results of a preliminary study and Subsection 3.2
summarizes the state-of-the-art strategies to deal
with compounds in SMT. Section 4 focuses on the
experiments carried out and reported here and the
results thereof are presented and discussed in Sec-
tion 5. Finally, Section 6 summarizes the findings
of our research and discusses future work.
2 German Compounds
German compounds  may be  lexicalized  or  not.
Lexicalized compounds are those which can be
found  in  general  dictionaries, such  as Stra?en-
lampe (?street lamp/light? in German). Non lex-
icalized compounds are formed in a similar man-
ner  to  that  of  phrases  and/or  sentences and are
coined on-the-fly (i.e. Warmwasserbereitungsan-
lagen, see  Example  3). Non  lexicalized  com-
pounds usually appear in technical and formal texts
and German shows a great tendency to produce
them. In SMT, the translational correspondences
are computed from a sentence aligned training cor-
pus and translation dictionaries  are  not  present.
Rather, word alignment algorithms are used to pro-
duce the phrase tables that will  in turn be used
to produce the translations. Thus, although non
lexicalized compounds pose a greater  challenge
(they are unpredictable), lexicalized compounds
are not distinguished either. As this formal distinc-
tion cannot be done when dealing with SMT, here
we will refer to compounds irrespectively whether
they are lexicalized or not, unless otherwise spec-
ified.
Moreover, German compounds may be nouns,
adjectives, adverbs and verbs, although the largest
group is the one corresponding to nominal com-
pounds. Finally, it is also important to highlight
that sometimes more than one compound-forming
phenomenon may take place subsequently to form
a new, longer, compound. Previous Example 1 is
the result of such a process, and as illustrated in Ex-
ample 3 it can, in turn, be the base for a yet newer
compound.
(3) warm (ADJ) + Wasser(N) =Warmwasser (N)
+ Bereitung(N) =Warmwasserbereitung
(N) + s + Anlagen(N) =
Warmwasserbereitungsanlagen (N) [EN:
warm water production systems]
As may also be observed in Example 3, the word
class of the compound is determined by the ele-
ment located in the rightmost position of the com-
pound (i.e. the combination of the adjective warm
and the nounWasser yields a nominal compound).
Finally, it is also important to highlight that be-
sides words, compounds may also include particles
to join those words together, as the ?s? between
Warmwasserbereitung and Anlagen in Example 3
or truncations (part of one of the component words
is deleted). Example 4 illustrates the case when
one of the component words has been truncated:
(4) abstellen(V) - en + Anlagen(N) =
Abstellanlagen (N) [EN: parking facilities]
The  morphology  of  German  compounds  has
been  widely  researched, both  within  linguistics
(Fleischer, 1975; Wellman, 1984; Eichinger, 2000,
among others), as in NLP (Langer, 1998; Girju et
al., 2005; Marek, 2006; Girju, 2008, among oth-
ers). Here, we will focus on the impact of prepro-
cessing nominal compounds in SMT.
Baroni et al. (2002) report that 47% of the vo-
cabulary (types)  in  the APA corpus
2
were com-
pounds. As will be observed in Section 4, the com-
pound splitters we used also detected a high per-
centage of compounds in the corpora used in our
experiments. This fact confirms that it is crucial to
find a successful way of processing compounds in
NLP applications and in our case in SMT.
3 Case Study
The experiments carried out here have used the
texts corresponding to the domain B00: Construc-
tion of  the TRIS corpus  (Parra Escart?n, 2012),
and an internally compiled version of the Europarl
Corpus (Koehn, 2005) for the pair of languages
German-Spanish
3
. The domain (B00: Construc-
tion) was selected because it is the biggest one of
2
Corpus of the Austria Presse Agentur (APA). Recently it
has been released as the AMC corpus (Austrian Media Cor-
pus) (Ransmayr et al., 2013).
3
See Table 2 for an overview of the corpus statistics.
49
the three domains currently available in the TRIS
corpus
4
. Only one domain was used because we
aimed at testing in-domain translation. Besides,
the TRIS corpus was selected because it is a spe-
cialised German-Spanish parallel corpus. As op-
posed to the Europarl, the TRIS corpus is divided in
domains and the source and target languages have
been verified (i.e. the texts were originally written
in German and translated into Spanish). Moreover,
the texts included in the Europarl are transcrip-
tions of the sessions of the European Parliament,
and thus the style is rather oral and less technical.
As compounds tend to be more frequent in domain
specific texts, the TRIS corpus has been used for
testing, while the Europarl Corpus has been used
in the training set to avoid data scarcity problems
and increase the vocabulary coverage of the SMT
system.
In the case of Machine Translation (MT), both
rule-based MT systems (RBMT systems) and Sta-
tistical MT systems (SMT systems) encounter prob-
lems when dealing with compounds. For the pur-
poses of this paper, the treatment of compounds
in German has been tested within the SMT toolkit
Jane (Wuebker et al., 2012; Vilar et al., 2010).
We have carried out several experiments translat-
ing German specialized texts into Spanish to test
to which extent incorporating a linguistic analy-
sis of the corpora and compiling compound lists
improves the overall SMT results. At this stage, in-
cluding further linguistic information such as Part-
of-Speech tagging (POS tagging) or phrase chunk-
ing has been disregarded. Forcing the translation
of compounds in the phrase tables produced by
Jane has also been disregarded. The overall aim
was to test how the SMT system performs using dif-
ferent pre-processing strategies of the training data
but without altering its mechanism. Since it is a
challenge to factor out what is really the translation
of the compounds, the overall quality of the trans-
lations at document level has been measured as an
indirect way of assessing the quality of the com-
pound translations
5
. To evaluate the compound
translations into Spanish, these need to be man-
ually validated because we currently do not have
access to fully automatic methods. A qualitative
analysis of the compound translations will be done
in future work.
4
The domain C00A: Agriculture, Fishing and Foodstuffs
has 137.354 words and the domain H00: Domestic Leisure
Equipment has 58328 words).
5
The results of this evaluation are reported in Section 5.
3.1 Preliminary study
With the purpose of assessing the presence of com-
pounds in the TRIS corpus and evaluating the split-
tings at a later stage as well as the impact of such
splittings in SMT, we analysed manually two short
texts of the TRIS corpus. The two files correspond
to the subcorpus B30: Construction - Environment
and account for 261 sentences and 2870 words.
For this  preliminary study, all  German nominal
compounds and their corresponding Spanish trans-
lations were manually extracted. Adjectival and
verbal compounds were not included at this stage.
Abbreviated nominal compounds (i.e. ?EKZ? in-
stead of ?Energiekennzahl?, [energy index]) were
not included either. Table 1 offers an overview of
the number of running words in each file without
punctuation, the number of nominal compounds
found (with an indication as to which percentage
of the total number of words they account for),
the number of unique compounds (i.e. compound
types), and the number of lexicalized and non lexi-
calized compounds in total (with the percentage of
the text they account for), and unique. For the pur-
poses of this study, all compounds found in a Ger-
man monolingual dictionary were considered lex-
icalized, whereas those not appearing where con-
sidered non-lexicalized.
As can be seen in Table 1, compound nominals
constitute a relatively high percentage of the total
number of words in a text. This is specially the
case of domain specific texts such as the ones taken
into consideration here. We can thus assume that
finding a way to translate compounds appropri-
ately into other languages would improve the over-
all quality of the translations produced by SMT.
3.2 Related work: compounds in SMT
RBMT systems  require  that  compounds  are  in-
cluded in their dictionaries to be able to retrieve
the appropriate translation in each case. Alterna-
tively, they should include a special rule for han-
dling compounds which are beyond their lexical
coverage. On the other hand, SMT systems en-
counter problems when dealing with compounds
because they rely on the words observed during the
training phase. Thus, if the compound did not ap-
pear in the training set of the system its translation
will subsequently fail. The state-of-the-art strat-
egy to deal with compounds in SMT systems con-
sists on splitting the compounds to reduce the num-
ber of unseen words. Previous research (Koehn
50
Text A Text B
Number of words 2431 439
Number of comp. 265 (10.9%) 62 (14.12%)
Number of unique comp. 143 25
Lexicalized comp. 99 (4.07%) 18 (4.1%)
Unique lexicalized comp. 63 4
Not lexicalized comp. 166 (6.8%) 44 (10.06%)
Unique not lexicalized comp. 80 21
Table 1: Compound nominals found in the two texts taken for the preliminary study.
and Knight, 2003; Popovi? et al., 2006; Stymne,
2008; Fritzinger and Fraser, 2010; Stymne et al.,
2013) has shown that splitting the compounds in
German results in better Bleu scores (Papineni et
al., 2001) and vocabulary coverage (fewer ?un-
known? words). However, the experiments car-
ried out so far have also claimed that significant
changes in error measures were not to be expected
because the percentage of running words affected
by compound splitting was rather low (Popovi? et
al., 2006; Stymne, 2008). As will be observed in
Section 4.1, in our case the percentage of running
words affected by compound splitting was higher.
This might be due to the kind of texts used in our
experiments.
4 Experiments
As  mentioned  in  Section 3, for  the  experi-
ments reported here two corpora have been used:
the TRIS corpus  and  the  Europarl  corpus  for
German?Spanish. In order to focus on in-domain
translation, only the largest subcorpus of TRIS has
been used.
Table 2 summarizes the number of sentences
and words in our experiment setup.
To reduce possible mistakes and mismatches ob-
served in the corpora used in the experiments, the
spelling of the German vowels named umlaut (???)
was simplified. Thus, ??, ?, ?, ?, ?, ?? were trans-
formed into ?Ae, Oe, Ue, ae, oe, ue? correspond-
ingly. Also the German ??? was substituted by a
double s: ?ss?. By doing this, words appearing in
the corpus and written differently were unified and
thus their frequencies were higher.
Additionally, a list of 185 German nominal com-
pounds present in the training set was manually ex-
tracted together with their translations into Span-
ish. If different translations had been found for
the same compound, these were included in our
list too. This list was used in some of our exper-
iments to determine whether extracting such lists
has an impact in the overall translation quality of
SMT systems. As the texts belong to the same
domain, there was partial overlap with the com-
pounds found in the test set. However, not all com-
pounds in the test set were present in the training
corpus and viceversa.
4.1 Training environments
Taking the normalised version of our corpus as
a baseline, different training environments have
been tested. We designed five possible training
environments in which German compounds were
preprocessed.
In our first experiment (hereinafter ?compList?),
the list of manually extracted compounds was ap-
pended to the end of the training set and no further
preprocessing was carried out.
In our second experiment (hereinafter ?RWTH?),
the state-of-the-art compound splitting approach
implemented by Popovi? et al. (2006) was used to
split all possible compounds. As also implemented
by Koehn and Knight (2003), this approach uses
the corpus itself to create a vocabulary that is then
subsequently used to calculate the possible split-
tings in the corpus. It has the advantage of being
a stand-alone approach which does not depend on
any external resources. A possible drawback of
this approach would be that it relies on a large cor-
pus to be able to compute the splittings. Thus, it
may not be as efficient with smaller corpora (i.e. if
we were to use only the TRIS corpus, for instance).
The  third  experiment  (hereinafter
?RWTH+compList?)  used  the  split  corpus  pre-
pared  in  our  second  experiment  (?RWTH?) but
merged with the list of compounds that was also
used in the first experiment. In total, 128 of all
compounds detected by the splitter were also in
our compound list. In order to avoid noise, the
compounds present in the list were deleted from
51
training dev test
Sentences 1.8M 2382 1192
Running words without punctuation (tokens) 40.8M 20K 11K
Vocabulary size (types) 338K 4050 2087
Table 2: Corpus statistics. The training corpus is a concatenation of the complete Europarl Corpus
German?Spanish and a greater part of the TRIS corpus, while in dev and test only texts from the
TRIS corpus were used.
the list of splittings to be carried out in the corpus.
Thus, after all possible splittings were calculated,
those splittings that were present in the manually
compiled compound list  were deleted to ensure
that they were not split in the corpus and remained
the same.
In the fourth experiment (hereinafter ?IMS?) we
used another compound splitter developed at the
Institut f?r Maschinelle Sprachverarbeitung of the
University of Stuttgart (Weller and Heid, 2012).
This splitter was also developed using a frequency-
based approach. However, in this case the train-
ing data consists  of  a  list  of  lemmatized word-
forms together with their POS tags. A set of rules
to model transitional elements is also used. While
this splitter might be used by processing our corpus
with available tools such as TreeTagger (Schmid,
1994)
6
and then computing frequencies, in our ex-
periments we used the CELEX
7
database for Ger-
man (Baayen et al., 1993). This was done so be-
cause CELEX is an extensive high quality lexical
database which already included all the informa-
tion we needed to process and did not require any
further preprocessing and clean up of our corpus.
In  the  fifth  experiment  (hereinafter
?IMS+compList?), we repeated the same procedure
of our third experiment (?RWTH+compList?): we
added the compound list  to  our training corpus
already split, but this time using the compound
splitter  developed  in  Stuttgart. In  total, 125
of  all  compounds  detected  by  the  splitter  were
also in our compound list. The splitting of such
compounds was avoided.
4.2 Compounds detected
Table 3 summarizes the number of compounds de-
tected by the two compound splitters and the per-
centage they account for with respect to the vocab-
ulary and the number of running words.
6http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
7http://wwwlands2.let.kun.nl/members/
As can be observed in Table 3, the percentage
of compounds in the test set is considerably higher
than in the training set. This is due to the fact that
in the test set only a subcorpus of the TRIS corpus
was used, whereas in the training corpus Europarl
was also used and as stated earlier (cf. Subsec-
tion 3.1 and table 1), domain specific corpora tend
to have more compounds. It is also noticeable that
the compound splitter developed in Stuttgart de-
tects and splits fewer compounds. A possible ex-
planation would be that Weller and Heid (2012)
only split words into content words and use POS
tags to filter out other highly frequent words that do
not create compounds. The presence of lexicalized
compounds in the CELEX database does not seem
to have affected the accuracy of the splitter (i.e.
they were not skipped by the splitter). Finally, it is
also noticeable that the percentage of compounds
detected in the training set is similar to the one re-
ported by Baroni et al. (2002) and referenced to in
Section 2. This seems to indicate that both splitting
algorithms perform correctly. A thorough analy-
sis of their outputs has been carried out confirm-
ing this hypothesis as the accuracies of both split-
ters were considerably high: 97.19% (RWTH) and
97.49% IMS (Parra Escart?n, forthcoming)
8
.
As SMT system, we  employ  the  state-of-the-
art  phrase-based translation approach (Zens and
Ney, 2008) implemented in Jane. The baseline is
trained on the concatination of the TRIS and Eu-
roparl corpus. Word alignments are trained with
fastAlign (Dyer et al., 2013). Further, we apply
a 4-gram language model trained with the SRILM
toolkit (Stolcke, 2002) on the target side of the
training corpus. The log-linear parameter weights
are tuned with MERT (Och, 2003) on the develop-
ment set (dev). As optimization criterion we use
Bleu. The parameter setting for all experiments
was the same to allow for comparisons.
software/celex_gug.pdf
8
The analysis was done following the method proposed by
Koehn and Knight (2003).
52
Popovic et al. (2006) Weller and Heid (2012)
Compounds in training 182334 141789
% Vocabulary 54% 42%
% Running words 0.4% 0.3%
Compounds in test 924 444
% Vocabulary 44.3% 21.3%
% Running words 8.5% 4%
Table 3: Number of compounds detected by each of the splitters used and the percentages they account
for with respect to the vocabulary (types) and the number of running words (tokens) in the corpora used
in the experiments.
5 Results
Table 4 reports the results of the five training en-
vironments described in Subsection 4.1 and the
baseline. We report results in Bleu [%] and Ter
[%] (Snover et al., 2006). All reported results are
averages over three independent MERT runs, and
we evaluate statistical significance with MultEval
(Clark et al., 2011).
As can be observed in Table 4, adding com-
pound  lists  to  the  training  set  significantly  im-
proves the Bleu and Ter scores with respect to the
baseline. This is also the case when compounds
were preprocessed and split. Moreover, while the
Bleu scores for both splitters are the same when
processing the entire corpus, adding the compound
list to the training corpus yields better scores. In
fact, the combination of the compound list  and
the compound splitter  developed by Weller  and
Heid (2012) improves by 3.8 points in Bleu, while
the approach by Popovi? et al. (2006) improves by
3.4 Bleu points against Baseline. When comparing
it with compList, the improvements are of 3% and
2.4% Bleu respectively. To ensure a fair compar-
ison, RWTH is defined as second baseline. Again,
we observe significant improvement over this sec-
ond baseline by adding the compound list to the
training corpus. In terms of Bleu we gain an im-
provement of up to 1.4 points.
These results seem promising as they show sig-
nificant improvements both in terms of Bleu and
Ter scores. As  previously  mentioned  in  Sec-
tion 3.2, one possible explanation to the higher
Bleu scores we obtained might be that the num-
ber of running words affected by compound split-
ting  was  higher  than  in  other  experiments  like
the  ones  carried  out  by  Popovi?  et  al. (2006)
and Stymne (2008). Fritzinger and Fraser (2010)
used a hybrid splitting algorithm which combined
the  corpus-based  approach  and  linguistic  infor-
mation and also reported better Bleu scores for
German?English translations than splitting algo-
rithms based only in corpus frequencies. They sug-
gested that fewer split compounds but better split
could yield better results. However, in our case the
two splitters score the same in terms of Bleu. Fur-
ther experiments with other language pairs should
be carried out to test whether this is only the case
with  German?Spanish translation tasks  or  not.
If this were to be confirmed, a language depen-
dent approach to dealing with compounds in SMT
might then be needed. The improvements in terms
of Bleu and Ter obtained when adding the man-
ually extracted compound list to our training cor-
pus (particularly in the IMS+compList experiment)
suggest that further preprocessing than just split-
ting the compounds in the corpora would result
in  overall  better  quality  translations. It  is  par-
ticularly noticeable that while the fewest number
of unknown words occurs when using a corpus-
based splitting algorithm (experiments RWTH and
RWTH+compList), this does not seem to directly
correlate with better Bleu and Ter scores. Exper-
iments IMS and IMS+compList had in fact a larger
number of unknown words and yet obtain better
scores.
Table 5 reports the number of compounds of the
compound list found in the test sets across the dif-
ferent experiments. As the compound list was not
preprocessed, the number of compounds found in
RWTH and IMS is smaller than those found in Base-
line and compList. In the case of RWTH+compList
and IMS+compList, however, the productivity of
German compounds mentioned earlier in Section 2
may have influenced the number of compounds
found. If a compound found in our compound list
was present in other compounds and those were
split in such a way that it resulted in one of the
53
test
Experiment Splitting Method Compound List Bleu
[%]
Ter
[%]
OOVs
Baseline - no 45.9 43.9 181
compList - yes 46.7 42.9 169
RWTH Popovi? et al. (2006) no 48.3 40.8 104
RWTH+compList yes 49.1 40.5 104
IMS Weller and Heid (2012) no 48.3 40.5 114
IMS+compList yes 49.7 39.2 114
Table 4: Results for the German?Spanish TRIS data. Statistically significant improvements with at least
99% confidence over the respective baselines (Baseline and RWTH) are printed in boldface.
formants being that compound, its frequency got
higher. As can be observed, the highest number of
correct translations of compounds corresponds to
RWTH+compList and IMS+compList.
Table 6 shows the results of a sample sentence
in our test set including several compounds. As
can be observed, in the IMS+compList experiment
all compounds are correctly translated. This seems
to indicate that the manually compiled list of com-
pounds added to the training corpus helped to in-
crease the probabilities of alignment of 1:n corre-
spondences (German compound ? Spanish MWE)
and thus the compound translations in the phrase
tables are better.
6 Conclusion and future work
In this paper, we have reported the results of our
experiments processing German compounds and
carrying out SMT tasks into Spanish. As has been
observed, adding manually handcrafted compound
lists to the training set significantly improves the
qualitative results of SMT and therefore a way of
automating their extraction would be desired. Fur-
thermore, a combination of splitting compounds
and adding them already aligned to their transla-
tions in the training corpus yields also significant
improvements with respect to the baseline. A qual-
itative analysis is currently being done to assess the
kind of improvements that come from the splitting
and/or the compound list added to training.
As a follow up of the experiments reported here,
the compound splitters used have being evaluated
to assess their precision and recall and determine
which splitting algorithms could be more promis-
ing for SMT tasks and whether or not their quality
has a correlation with better translations. From the
experiments carried out so far, it seems that it may
be the case, but this shall be further explored as our
results do not differ greatly between each other.
In future work we will research whether the ap-
proach suggested here also yields better results in
data used by the MT community. Obtaining bet-
ter overall results would confirm that our approach
is right, in which case we will research how we
can combine both strategies (compound splitting
and adding compound lists and their translations
to training corpora) in a successful and automatic
way. We also intend to explore how we can do
so minimizing the amount of external resources
needed.
Obtaining positive results in these further ex-
periments would suggest that a similar approach
may also yield positive results in dealing with other
types of MWEs within SMT.
Acknowledgments
The research reported in this paper has received
funding from the EU under FP7, Marie Curie Ac-
tions, SP3 People ITN, grant agreement n
o
238405
(project CLARA
9
). The authors would also like to
thank the anonymous reviewers for their valuable
comments.
References
Sybille  Angele. 1992. Nominalkomposita  des
Deutschen und ihre Entsprechungen im Spanischen.
Eine  kontrastive  Untersuchung  anhand  von  Tex-
ten aus Wirtschaft und Literatur. iudicium verlag
GmbH, M?nchen.
S. Atkins, N. Bel, P. Bouillon, T. Charoenporn, D. Gib-
bon, R. Grishman, C.-R. Huan, A. Kawtrakul, N. Ide,
H.-Y.  Lee, P. J. K.  Li, J. McNaught, J. Odijk,
M. Palmer, V. Quochi, R. Reeves, D. M. Sharma,
V. Sornlertlamvanich, T. Tokunaga, G. Thurmair,
M.Villegas, A. Zampolli, and E. Zeiton. 2001. Stan-
dards and Best Practice for Multiligual Computa-
tional Lexicons. MILE (the Multilingual ISLE Lex-
9http://clara.uib.no
54
Experiment Compounds (DE) Compound translations (ES)
Baseline 154 48
compList 154 54
RWTH 85 61
RWTH+compList 175 80
IMS 46 57
IMS+compList 173 76
Table 5: Number of compounds present in our compound list found in the test set for each of the experi-
ments both in German and in Spanish. The experiments with the highest number of translations present
in our compound list are printed in boldface.
Sentence type Example
Original (DE) Abstellanlagen fuer Kraftfahrzeuge in Tiefgaragen oder in Parkdecks
mit mindestens zwei Geschossen
Reference (ES) instalaciones de estacionamiento de autom?viles en garajes subterr?neos
o en estacionamientos cubiertos que tengan como m?nimo dos plantas
Baseline (DE) Abstellanlagen fuer Kraftfahrzeuge in Tiefgaragen oder in Parkdecks
mit mindestens zwei Geschossen
Baseline (ES) plazas para veh?culos en aparcamientos subterr?neos o en plantas
con al menos dos pisos
IMS (DE) abstellen Anlagen fuer Kraft fahren Zeuge in tief Garagen oder in Park Decks
mit mindestens zwei Geschossen
IMS (ES) plazas para veh?culos en aparcamientos subterr?neos o en plantas
con al menos dos pisos
IMS+compList (DE) Abstellanlagen fuer Kraftfahrzeuge in Tiefgaragen oder in Parkdecks
mit mindestens zwei Geschossen
IMS+compList (ES) instalaciones de estacionamiento para autom?viles estacionamientos cubiertos
en garajes subterr?neos o en plantas con al menos dos pisos
Table 6: Sample  translations  for  German?Spanish  for  the  baseline  and  the  experiments IMS and
IMS+compList. Each compound and its translation have the same format.
ical Entry) Deliverable D2.2-D3.2. ISLE project:
ISLE Computational Lexicon Working Group.
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX Lexical Database (CD-ROM). Linguis-
tic  Data  Consortium, University  of  Pennsylvania,
Philadelphia, PA.
Marco Baroni, Johannes Matiasek, and Harald Trost.
2002. Wordform- and Class-based Prediction of the
Components of German Nominal Compounds in an
AAC System. In 19th International Conference on
Computational Linguistics, COLING 2002, Taipei,
Taiwan, August 24 - September 1, 2002.
Jonathan H.  Clark, Chris  Dyer, Alon  Lavie, and
Noah A.  Smith. 2011. Better  hypothesis  test-
ing for statistical machine translation: Controlling
for  optimizer  instability. In 49th  Annual  Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages  176?181, Portland, Ore-
gon, June.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proc. of NAACL.
Ludwig M. Eichinger. 2000. Deutsche Wortbildung.
Eine Einf?hrung. Gunter Narr Verlag T?bingen.
Wolfgang Fleischer. 1975. Wortbildung der deutschen
Gegenwartssprache. Max Niemeyer Verlag T?bin-
gen, 4 edition.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to  Avoid  Burning  Ducks: Combining  Linguistic
Analysis  and Corpus Statistics  for  German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical  Machine Translation and
MetricsMATR, WMT ?10, pages 224?234, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On  the  semantics  of  noun
compounds. Computer  Speech  and  Language,
(4):479?496.
Roxana  Girju. 2008. The  Syntax  and  Semantics
of  Prepositions  in  the  Task  of  Automatic  Inter-
pretation of Nominal Phrases and Compounds: A
Cross-Linguistic Study. Computational Linguistics,
35(2):185?228.
Carmen G?mez P?rez. 2001. La composici?n nominal
alemana desde la perspectiva textual: El compuesto
nominal como dificultad de traducci?n del alem?n al
espa?ol. Ph.D. thesis, Departamento de Traducci?n
55
e Interpretaci?n, Universidad de Salamanca, Sala-
manca.
Philipp  Koehn and Kevin  Knight. 2003. Empiri-
cal Methods for Compound splitting. In Proceed-
ings of the Tenth Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 187?193, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the Tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand.
Stefan Langer. 1998. Zur morphologie und seman-
tik  von  nominalkomposita. In Tagungsband  der
4. Konferenz zur Verarbeitung nat?rlicher Sprache
(KOVENS).
Torsten Marek. 2006. Analysis of German Compounds
Using Weighted Finite State Transducers. Technical
report, Eberhard-Karls-Universit?t T?bingen.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. pages 160?167,
Sapporo, Japan, July.
Ulrike Oster. 2003. Los t?rminos de la cer?mica en
alem?n y en espa?ol. An?lisis sem?ntico orientado
a la traducci?n de los compuestos nominales ale-
manes. Ph.D. thesis, Departament de Traducci? i
Comunicaci?, Universitat Jaume I, Castell?n.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBMResearch Divi-
sion, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Carla  Parra Escart?n. 2012. Design and compila-
tion of a specialized Spanish-German parallel cor-
pus. In Proceedings  of  the  Eight  International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, May. European Lan-
guage Resources Association.
Carla Parra Escart?n. forthcoming. Chasing the perfect
splitter: A comparison of different compound split-
ting tools. In Proceedings of the Ninth Conference
on International Language Resources and Evalua-
tion (LREC?14), Reykjavik, Island, May. European
Language Resources Association.
Maja Popovi?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of german compound
words. In Proceedings of the 5th international con-
ference on Advances in Natural Language Process-
ing, FinTAL?06, pages 616?624, Berlin, Heidelberg.
Springer-Verlag.
Jutta Ransmayr, Karlheinz Moerth, and Matej Durco.
2013. Linguistic variation in the austrian media cor-
pus. dealing with the challenges of large amounts of
data. In Proceedings of International Conference on
Corpus Linguistics (CILC), Alicante. University Al-
icante, University Alicante.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging  Using  Decision  Trees. In International
Conference on New Methods in Language Process-
ing, pages 44?49, Manchester, UK.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
Sara Stymne, Nicola Cancedda, and Lars Ahrenberg.
2013. Generation of Compound Words in Statistical
Machine Translation into Compounding Languages.
Computational Linguistics, pages 1?42.
Sara  Stymne. 2008. German Compounds in  Fac-
tored Statistical Machine Translation. InGoTAL?08:
Proceedings of the 6th international conference on
Advances in Natural Language Processing, pages
464?475. Springer-Verlag.
David Vilar, Daniel  Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ?10, pages 262?270, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Marion Weller and Ulrich Heid. 2012. Analyzing
and Aligning German compound nouns. In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, May. European Language Resources
Association.
Hans Wellman, 1984. DUDEN. Die Grammatik. Un-
entbehrlich f?r richtiges Deutsch, volume 4, chapter
Die Wortbildung. Duden Verlag.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus  Freitag, Jan-Thorsten  Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Richard Zens and Hermann Ney. 2008. Improvements
in Dynamic Programming Beam Search for Phrase-
based Statistical Machine Translation. In Interna-
tional Workshop on Spoken Language Translation,
pages 195?205, Honolulu, Hawaii, October.
56
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 157?162,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The RWTH Aachen German-English Machine Translation System for
WMT 2014
Stephan Peitz, Joern Wuebker, Markus Freitag and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
<surname>@cs.rwth-aachen.de
Abstract
This paper describes the statistical ma-
chine translation (SMT) systems devel-
oped at RWTH Aachen University for the
German?English translation task of the
ACL 2014 Eighth Workshop on Statisti-
cal Machine Translation (WMT 2014).
Both hierarchical and phrase-based SMT
systems are applied employing hierarchi-
cal phrase reordering and word class lan-
guage models. For the phrase-based sys-
tem, we run discriminative phrase training.
In addition, we describe our preprocessing
pipeline for German?English.
1 Introduction
For the WMT 2014 shared translation task
1
RWTH utilized state-of-the-art phrase-based and
hierarchical translation systems. First, we describe
our preprocessing pipeline for the language pair
German?English in Section 2. Furthermore, we
utilize morpho-syntactic analysis to preprocess the
data (Section 2.3). In Section 3, we give a survey
of the employed systems and the basic methods
they implement. More details are given about the
discriminative phrase training (Section 3.4) and
the hierarchical reordering model for hierarchical
machine translation (Section 3.5). Experimental
results are discussed in Section 4.
2 Preprocessing
In this section we will describe the modification of
our preprocessing pipeline compared to our 2013
WMT German?English setup.
2.1 Categorization
We put some effort in building better categories for
digits and written numbers. All written numbers
1
http://www.statmt.org/wmt14/
translation-task.html
were categorized. In 2013 they were just handled
as normal words which leads to a higher number of
out-of-vocabulary words. For German?English,
in most cases for numbers like ?3,000? or ?2.34?
the decimal mark ?,? and the thousands separator
?.? has to be inverted. As the training data and also
the test sets contain several errors for numbers in
the source as well as in the target part, we put more
effort into producing correct English numbers.
2.2 Remove Foreign Languages
The WMT German?English corpus contains
some bilingual sentence pairs with non-German
source or/and non-English target sentences. For
this WMT translation task, we filtered all non-
matching language pairs (in terms of source lan-
guage German and target language English) from
our bilingual training set.
First, we filtered languages which contain non-
ascii characters. For example Chinese, Arabic or
Russian can be easily filtered when deleting sen-
tences which contain more than 70 percent non-
ascii words. The first examples of Table 1 was
filtered due to the fact, that the source sentence
contains too many non-ascii characters.
In a second step, we filtered European lan-
guages containing ascii characters. We used the
WMT monolingual corpora in Czech, French,
Spanish, English and German to filter these lan-
guages from our bilingual data. We could both
delete a sentence pair if it contains a wrong source
language or a wrong target language. That is the
reason why we even search for English sentences
in the source part and for German sentences in
the target part. For each language, we built a
word count of all words in the monolingual data
for each language separately. We removed punc-
tuation which are no indicator of a language. In
our experiments, we only considered words with
frequency higher than 20 (e.g. to ignore names).
Given the word frequency, we removed a bilingual
157
Table 1: Examples of sentences removed in preprocessing.
Example
remove non-ascii symbols ????????? .
zum Bericht A?noveros Tr??as de Bes
remove wrong languages from target Honni soit qui mal y pense !
as you yourself have said : travailler plus pour gagner plus
remove wrong languages from source je d?eclare interrompue la session du Parlement europ?een .
Quelle der Tabelle : ? what Does the European Union do ? ?
sentence pair from our training data if more than
70 percent of the words had a higher count in a
different language then the one we expected. In
Table 1 some example sentences, which were re-
moved, are illustrated.
In Table 2 the amount of sentences and the cor-
responding vocabulary sizes of partial and totally
cleaned data sets are given. Further we provide the
number of out-of-vocabulary words (OOVs) for
newstest2012. The vocabulary size could be re-
duced by ?130k words for both source and target
side of our bilingual training data while the OOV
rate kept the same. Our experiments showed, that
the translation quality is the same with or with-
out removing wrong sentences. Nevertheless, we
reduced the training data size and also the vocabu-
lary size without any degradation in terms of trans-
lation quality.
2.3 Morpho-syntactic Analysis
In order to reduce the source vocabulary size for
the German?English translation further, the Ger-
man text is preprocessed by splitting German com-
pound words with the frequency-based method de-
scribed in (Koehn and Knight, 2003). To reduce
translation complexity, we employ the long-range
part-of-speech based reordering rules proposed by
Popovi?c and Ney (2006).
3 Translation Systems
In this evaluation, we employ phrase-based trans-
lation and hierarchical phrase-based translation.
Both approaches are implemented in Jane (Vilar et
al., 2012; Wuebker et al., 2012), a statistical ma-
chine translation toolkit which has been developed
at RWTH Aachen University and is freely avail-
able for non-commercial use.
2
In the newest inter-
nal version, we use the KenLM Language Model
Interface provided by (Heafield, 2011) for both de-
coders.
2
http://www.hltpr.rwth-aachen.de/jane/
3.1 Phrase-based System
In the phrase-based decoder (source cardinality
synchronous search, SCSS, Wuebker et al. (2012)),
we use the standard set of models with phrase
translation probabilities and lexical smoothing in
both directions, word and phrase penalty, distance-
based distortion model, an n-gram target language
model and three binary count features. Additional
models used in this evaluation are the hierarchical
reordering model (HRM) (Galley and Manning,
2008) and a word class language model (wcLM)
(Wuebker et al., 2013). The parameter weights
are optimized with minimum error rate training
(MERT) (Och, 2003). The optimization criterion
is BLEU (Papineni et al., 2002).
3.2 Hierarchical Phrase-based System
In hierarchical phrase-based translation (Chiang,
2007), a weighted synchronous context-free gram-
mar is induced from parallel text. In addition to
contiguous lexical phrases, hierarchical phrases
with up to two gaps are extracted. The search is
carried out with a parsing-based procedure. The
standard models integrated into our Jane hierar-
chical systems (Vilar et al., 2010; Huck et al.,
2012) are: Phrase translation probabilities and lex-
ical smoothing probabilities in both translation di-
rections, word and phrase penalty, binary features
marking hierarchical phrases, glue rule, and rules
with non-terminals at the boundaries, three binary
count features, and an n-gram language model.
We utilize the cube pruning algorithm for decod-
ing (Huck et al., 2013a) and optimize the model
weights with MERT. The optimization criterion is
BLEU.
3.3 Other Tools and Techniques
We employ GIZA
++
(Och and Ney, 2003) to train
word alignments. The two trained alignments
are heuristically merged to obtain a symmetrized
word alignment for phrase extraction. All lan-
158
Table 2: Corpus statistics after each filtering step and compound splitting.
Vocabulary OOVs
Sentences German English newstest2012
Preprocessing 2013 4.19M 1.43M 784K 1019
Preprocessing 2014 4.19M 1.42M 773K 1018
+ remove non-ascii symbols 4.17M 1.36M 713K 1021
+ remove wrong languages from target 4.15M 1.34M 675K 1027
+ remove wrong languages from source 4.08M 1.30M 655K 1039
+ compound splitting 4.08M 652K 655K 441
guage models (LMs) are created with the SRILM
toolkit (Stolcke, 2002) or with the KenLM lan-
guage model toolkit (Heafield et al., 2013) and are
standard 4-gram LMs with interpolated modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998). We evaluate in true-
case with BLEU and TER (Snover et al., 2006).
3.4 Discriminative Phrase Training
In our baseline translation systems the phrase ta-
bles are created by a heuristic extraction from
word alignments and the probabilities are esti-
mated as relative frequencies, which is still the
state-of-the-art for many standard SMT systems.
Here, we applied a more sophisticated discrimi-
native phrase training method for the WMT 2014
German?English task. Similar to (He and Deng,
2012), a gradient-based method is used to opti-
mize a maximum expected BLEU objective, for
which we define BLEU on the sentence level with
smoothed 3-gram and 4-gram precisions. To that
end, the training data is decoded to generate 100-
best lists. We apply a leave-one-out heuristic
(Wuebker et al., 2010) to make better use of the
training data. Using these n-best lists, we itera-
tively perform updates on the phrasal translation
scores of the phrase table. After each iteration,
we run MERT, evaluate on the development set
and select the best performing iteration. In this
work, we perform two rounds of discriminative
training on two separate data sets. In the first
round, training is performed on the concatenation
of newstest2008 through newstest2010 and an au-
tomatic selection from the News-commentary, Eu-
roparl and Common Crawl corpora. The selec-
tion is based on cross-entropy difference of lan-
guage models and IBM-1 models as described by
Mansour et al. (2011) and contains 258K sentence
pairs. The training took 4.5 hours for 30 iterations.
On top of the final phrase-based systems, a second
round of discriminative training is run on the full
news-commentary corpus concatenated with new-
stest2008 through newstest2010.
3.5 A Phrase Orientation Model for
Hierarchical Machine Translation
In Huck et al. (2013b) a lexicalized reorder-
ing model for hierarchical phrase-based machine
translation was introduced. The model scores
monotone, swap, and discontinuous phrase ori-
entations in the manner of the one presented by
(Tillmann, 2004). Since improvements were re-
ported on a Chinese?English translation task, we
investigate the impact of this model on a European
language pair. As in German the word order is
more flexible compared with the target language
English, we expect that an additional reordering
model could improve the translation quality. In
our experiments we use the same settings which
worked best in (Huck et al., 2013b).
4 Setup
We trained the phrase-based and the hierarchical
translation system on all available bilingual train-
ing data. Corpus statistics can be found in the
last row of Table 2. The language model are
4-grams trained on the respective target side of
the bilingual data,
1
2
of the Shuffled News Crawl
corpus,
1
4
of the 10
9
French-English corpus and
1
2
of the LDC Gigaword Fifth Edition corpus.
The monolingual data selection is based on cross-
entropy difference as described in (Moore and
Lewis, 2010). For the baseline language model,
we trained separate models for each corpus, which
were then interpolated. For our final experiments,
we also trained a single unpruned language model
on the concatenation of all monolingual data with
KenLM.
159
Table 3: Results (truecase) for the German?English translation task. BLEU and TER are given in
percentage. All HPBT setups are tuned on the concatenation of newstest2012 and newstest2013. The
very first SCSS setups are optimized on newstest2012 only.
newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
SCSS +HRM 22.4 60.1 23.7 59.0 25.9 55.7
+wcLM 22.8 59.6 24.0 58.6 26.3 55.4
+1st round discr. 23.0 59.5 24.2 58.2 26.8 55.1
+tune11+12. 23.4 59.5 24.2 58.6 26.8 55.2
+unprunedLM 23.6 59.5 24.2 58.6 27.1 55.0
+2nd round discr. 23.7 59.5 24.4 58.5 27.2 55.0
HPBT baseline 23.3 59.9 24.2 58.9 26.7 55.6
+wcLM 23.4 59.8 24.1 58.9 26.8 55.6
+HRM 23.3 60.0 24.2 58.9 26.9 55.5
+HRM +wcLM 23.3 59.9 24.1 59.1 26.7 55.9
4.1 Experimental Results
The results of the phrase-based system (SCSS)
as well as the hierarchical phrase-based system
(HPBT) are summarized in Table 3.
The phrase-based baseline system, which in-
cludes the hierarchical reordering model by (Gal-
ley and Manning, 2008) and is tuned on new-
stest2012, reaches a performance of 25.9% BLEU
on newstest2013. Adding the word class language
model improves performance by 0.4% BLEU ab-
solute and the first round of discriminative phrase
training by 0.5% BLEU absolute. Next, we
switched to tuning on a concatenation of new-
stest2011 and newstest2012, which we expect to
be more reliable with respect to unseen data. Al-
though the BLEU score does not improve and TER
goes up slightly, we kept this tuning set in the sub-
sequent setups, as it yielded longer translations,
which in our experience will usually be preferred
by human evaluators. Switching from the inter-
polated language model to the unpruned language
model trained with KenLM on the full concate-
nated monolingual training data in a single pass
gained us another 0.3% BLEU. For the final sys-
tem, we ran a second round of discriminative train-
ing on different training data (cf. Section 3.4),
which increased performance by 0.1% BLEU to
the final score 27.2.
For the phrase-based system, we also exper-
imented with weighted phrase extraction (Man-
sour and Ney, 2012), but did not observe improve-
ments.
The hierarchical phrase-based baseline without
any additional model is on the same level as the
phrase-based system including the word class lan-
guage model, hierarchical reordering model and
discriminative phrase training in terms of BLEU.
However, extending the system with a word class
language model or the additional reordering mod-
els does not seem to help. Even the combination
of both models does not improve the translation
quality. Note, that the hierarchical system was
tuned on the concatenation newstest2011 and new-
stest2012. The final system employs both word
class language model and hierarchical reordering
model.
Both phrase-based and hierarchical phrase-
based final systems are used in the EU-Bridge sys-
tem combination (Freitag et al., 2014).
5 Conclusion
For the participation in the WMT 2014 shared
translation task, RWTH experimented with both
phrase-based and hierarchical translation systems.
For both approaches, we applied a hierarchical
phrase reordering model and a word class lan-
guage model. For the phrase-based system we em-
ployed discriminative phrase training. Addition-
ally, improvements of our preprocessing pipeline
compared to our WMT 2013 setup were described.
New introduced categories lead to a lower amount
of out-of-vocabulary words. Filtering the corpus
for wrong languages gives us lower vocabulary
sizes for source and target without loosing any per-
formance.
160
Acknowledgments
The research leading to these results has partially
received funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n
o
287658.
Furthermore, this material is partially based
upon work supported by the DARPA BOLT
project under Contract No. HR0011- 12-C-0015.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA.
References
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, Massachusetts, USA, August.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201?
228.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 292?301, Jeju, Republic of Korea, Jul.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Matthias Huck, Jan-Thorsten Peter, Markus Freitag,
Stephan Peitz, and Hermann Ney. 2012. Hierar-
chical Phrase-Based Translation with Jane 2. The
Prague Bulletin of Mathematical Linguistics, 98:37?
50, October.
Matthias Huck, David Vilar, Markus Freitag, and Her-
mann Ney. 2013a. A Performance Study of
Cube Pruning for Large-Scale Hierarchical Machine
Translation. In Proceedings of the NAACL 7thWork-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 29?38, Atlanta, Georgia,
USA, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013b. A phrase orientation model
for hierarchical machine translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181?184, May.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings of
European Chapter of the ACL (EACL 2009), pages
187?194.
Saab Mansour and Hermann Ney. 2012. A Simple and
Effective Weighted Phrase Extraction for Machine
Translation Adaptation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 193?200, Hong Kong, December.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, California, USA, December.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In ACL
(Short Papers), pages 220?224, Uppsala, Sweden,
July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
161
Maja Popovi?c and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Transla-
tion. In International Conference on Language Re-
sources and Evaluation, pages 1278?1283, Genoa,
Italy, May.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, Colorado, USA,
September.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Boston, MA,
USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2012. Jane: an advanced freely avail-
able hierarchical machine translation toolkit. Ma-
chine Translation, 26(3):197?216, September.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statis-
tical Machine Translation. In International Confer-
ence on Computational Linguistics, pages 483?491,
Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, USA, October.
162
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 457?465,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Unsupervised Adaptation for Statistical Machine Translation
Saab Mansour and Hermann Ney
Human Language Technology and Pattern Recognition
Computer Science Department
RWTH Aachen University
Aachen, Germany
{mansour,ney}@cs.rwth-aachen.de
Abstract
In this work, we tackle the problem of
language and translation models domain-
adaptation without explicit bilingual in-
domain training data. In such a scenario,
the only information about the domain
can be induced from the source-language
test corpus. We explore unsupervised
adaptation, where the source-language test
corpus is combined with the correspond-
ing hypotheses generated by the transla-
tion system to perform adaptation. We
compare unsupervised adaptation to su-
pervised and pseudo supervised adapta-
tion. Our results show that the choice of
the adaptation (target) set is crucial for
successful application of adaptation meth-
ods. Evaluation is conducted over the
German-to-English WMT newswire trans-
lation task. The experiments show that the
unsupervised adaptation method generates
the best translation quality as well as gen-
eralizes well to unseen test sets.
1 Introduction
Over the last few years, large amounts of statistical
machine translation (SMT) monolingual and bilin-
gual corpora were collected. Early years focused
on structured data translation such as newswire.
Nowadays, due to the relative success of SMT,
new domains of translation are being explored,
such as lecture and patent translation (Cettolo et
al., 2012; Goto et al., 2013).
The task of domain adaptation tackles the prob-
lem of utilizing existing resources mainly drawn
from one domain (e.g. parliamentary discussion)
to maximize the performance on the target (test)
domain (e.g. newswire).
To be able to perform adaptation, a target set
representing the test domain is used to manipu-
late the general-domain models. Previous work
on SMT adaptation focused on the scenario where
(small) bilingual in-domain or pseudo in-domain
training data are available. Furthermore, small at-
tention was given to the choice of the target set for
adaptation. In this work, we explore the problem
of adaptation where no explicit bilingual data from
the test domain is available for training, and the
only resource encapsulating information about the
domain is the source-language test corpus itself.
We explore how to utilize the source-language
test corpus for adapting the language model (LM)
and the translation model (TM). A combination
of source and automatically translated target of
the test set is compared to using the source side
only for TM adaptation. Furthermore, we com-
pare using the test set to using in-domain data and
a pseudo in-domain data (e.g. news-commentary
as opposed to newswire).
Experiments are done on the WMT 2013
German-to-English newswire translation task.
Our best adaptation method shows competitive re-
sults to the best submissions of the evaluation.
This paper is structured as follows. We review
related work in Section 2 and introduce the basic
adaptation methods in Section 3. The experimen-
tal setup is described in Section 4, results are dis-
cussed in Section 5 and we conclude in Section 6.
2 Related Work
A broad range of methods and techniques have
been suggested in the past for domain adaptation
for both SMT and automatic speech recognition
(ASR).
For ASR, (Bellegarda, 2004) gives an overview
of LM adaptation methods. He differentiates be-
tween two cases regarding the availability of in-
domain adaptation data: (i) the data is available
and can be directly used to manipulate a back-
ground (general domain) corpus, and (ii) the data
is not available or too small, and then it can be
gathered or automatically generated during the
457
recognition process. (Bacchiani and Roark, 2003)
compare supervised against unsupervised (using
automatic transcriptions) in-domain data for LM
training for the task of ASR. They show that aug-
menting the supervised in-domain to the train-
ing of the LM performs better than the unsuper-
vised in-domain. In addition, they perform ?self-
training?, where the test set is automatically tran-
scribed and added to the LM. When using a strong
baseline, no improvements in recognition quality
are achieved. We differ from their work by us-
ing the unsupervised test data to adapt a general-
domain bilingual corpus. We also performed ini-
tial experiments of ?self-training? for language
modeling, where (artificial) perplexity improve-
ment was achieved but without an impact on the
machine translation (MT) quality.
(Zhao et al., 2004) tackle LM adaptation for
SMT. Similarly to our work, they use automati-
cally generated hypotheses to perform adaptation.
We extend their work by using the hypotheses
also for TM adaptation. (Hildebrand et al., 2005)
perform LM and TM adaptation based on infor-
mation retrieval methods. They use the source-
language test corpus to filter the bilingual data,
and then use the target side of the filtered bilingual
data to perform LM adaptation. We differ from
their work by using both the in-domain source-
language corpus and its corresponding automatic
translation for adaptation, which is shown in our
experiments to achieve superior results than when
using the source-side information only. (Foster
and Kuhn, 2007) perform LM and TM adaptation
using mixture modeling. In their setting, the mix-
ture weights are modified to express adaptation.
They compare cross-domain (in-domain available)
against dynamic adaptation. In the dynamic adap-
tation scenario, they utilize the source side of the
development set to adapt the mixture weights (LM
adaptation is possible as they only use parallel
training data, which enables filtering based on the
source side and then keeping the corresponding
target side of the data). For an in-domain test set,
the cross-domain setup performs better than the
dynamic adaptation method. (Ueffing et al., 2007)
use the test set translations as additional data to
train the TM. One important aspect in their work
is confidence measurement to remove noisy trans-
lation. In our approach, we use the automatic test
set translations to adapt the SMT models rather
than augmenting it as additional TM data. We also
compare different adaptation sets. Furthermore,
we do not use confidence measures to filter the au-
tomatic translations as they are only used to adapt
the general-domain system and are not augmented
to the TM.
In this work, we apply cross-entropy scoring for
adaptation as done by (Moore and Lewis, 2010).
Moore and Lewis (2010) apply adaptation by us-
ing an LM-based cross-entropy filtering for LM
training. Axelrod et al. (2011) generalized the
method for TM adaptation by interpolating the
source and target LMs. These two works focused
on a scenario where in-domain training data are
available for adaptation. In this work, we focus on
a scenario where in-domain training data is not la-
beled, and the main resource for adaptation is the
source-language test data.
In recent WMT evaluations, the method of
(Moore and Lewis, 2010) was utilized by several
translation systems (Koehn and Haddow, 2012;
Rubino et al., 2013). These systems use pseudo
in-domain corpus, i.e., news-commentary, as the
target domain (while the test domain is newswire).
The contribution of this work is two fold: we
show that the choice of the target set is crucial for
adaptation, in addition, we show that an unsuper-
vised target set performs best in terms of transla-
tion quality as well as generalization performance
to unseen test sets (in comparison to using pseudo
in-domain data or the references as target sets).
3 Cross-Entropy Adaptation
In this work, we use sample scoring for the pur-
pose of adaptation. We start by introducing the
scoring framework and then show how we utilize it
to perform filtering based adaptation and weighted
phrase extraction based adaptation.
LM cross-entropy scores can be used for both
monolingual data weighting for LM training as
done by (Moore and Lewis, 2010), or bilingual
weighting for TM training as done by (Axelrod et
al., 2011).
We differentiate between two types of data sets:
the adaptation set (target) representative of the
test-domain which we refer to also as in-domain
(IN), and the general-domain (GD) set which we
want to adapt.
The scores for each sentence in the general-
domain corpus are based on the cross-entropy dif-
ference of the IN and GD models. Denoting
H
M
(x) as the cross entropy of sentence x accord-
458
ing to model M , then the cross entropy difference
DH
M
(x) can be written as:
DH
M
(x) = H
M
IN
(x)?H
M
GD
(x) (1)
The intuition behind eq. (1) is that we are inter-
ested in sentences as close as possible to the in-
domain, but also as far as possible from the gen-
eral corpus. Moore and Lewis (2010) show that
using eq. (1) for LM filtering performs better in
terms of perplexity than using in-domain cross-
entropy only (H
M
IN
(x)). For more details about
the reasoning behind eq. (1) we refer the reader to
(Moore and Lewis, 2010).
Axelrod et al. (2011) adapted eq. (1) for bilin-
gual data filtering for the purpose of TM training.
The bilingual LM cross entropy difference for a
sentence pair (f
r
, e
r
) in the GD corpus is then de-
fined by:
DH
LM
(f
r
, e
r
) = DH
LM
src
(f
r
) +DH
LM
trg
(e
r
)
(2)
For IBM Model 1 (M1), the cross-entropy
H
M1
(f
r
|e
r
) is defined similarly to the LM cross-
entropy, and the resulting bilingual cross-entropy
difference will be of the form:
DH
M1
(f
r
, e
r
) = DH
M1
(f
r
|e
r
) +DH
M1
(e
r
|f
r
)
The combined LM+M1 score is obtained by
summing the LM and M1 bilingual cross-entropy
difference scores:
d
r
= DH
LM
(f
r
, e
r
) +DH
M1
(f
r
, e
r
) (3)
3.1 Filtering
A common framework to perform sample filtering
is to score each sample according to a model, and
then assigning a threshold on the score which fil-
ters out unwanted samples. If the score we gener-
ate is related to the probability that the sample was
drawn from the same distribution as the in-domain
data, we are selecting the samples most relevant to
our domain. In this way we can achieve adaptation
of the general-domain data.
We use the LM cross-entropy difference from
eq. (1) for LM filtering and a combined LM+M1
score (eq. (3) for TM filtering. We sort the sen-
tences in the general-domain according to the
score and select the best 50%,25%,...,6.25% train-
ing instances. Our models are then trained on
the selected portions of the training data, and the
best performing portion (according to perplexity
for LM training and BLEU for TM training) on the
development set is chosen as the adapted corpus.
3.2 Weighted Phrase Extraction
The classical phrase model is trained using a ?sim-
ple? maximum likelihood estimation, resulting in
phrase translation probabilities being defined by
relative frequency:
p(
?
f |e?) =
?
r
c
r
(
?
f, e?)
?
?
f
?
?
r
c
r
(
?
f
?
, e?)
(4)
Here,
?
f, e? are contiguous phrases, c
r
(
?
f, e?) de-
notes the count of (
?
f, e?) being a translation of each
other (usually according to word alignment and
heuristics) in sentence pair (f
r
, e
r
). One method
to introduce weights to eq. (4) is by weighting
each sentence pair by a weight w
r
. Eq. (4) will
now have the extended form:
p(
?
f |e?) =
?
r
w
r
? c
r
(
?
f, e?)
?
?
f
?
?
r
w
r
? c
r
(
?
f
?
, e?)
(5)
It is easy to see that setting {w
r
= 1} will result
in eq. (4) (or any non-zero equal weights). Increas-
ing the weight w
r
of the corresponding sentence
pair will result in an increase of the probabilities
of the phrase pairs extracted. Thus, by increasing
the weight of in-domain sentence pairs, the prob-
ability of in-domain phrase translations could also
increase.
We utilize d
r
from eq. (3) using a combined
LM+M1 scores for our suggested weighted phrase
extraction. d
r
can be assigned negative values, and
lower d
r
indicates sentence pairs which are more
relevant to the in-domain. Therefore, we negate
the term d
r
to get the notion of higher is closer
to the in-domain, and use an exponent to ensure
positive values. The final weight is of the form:
w
r
= e
?d
r
(6)
This term is proportional to perplexities, as the
exponent of entropy is perplexity by definition.
One could also use filtering for TM adaptation,
but, as shown in (Mansour and Ney, 2012), filter-
ing for TM could only reduce the size and weight-
ing performs better than filtering.
4 Experimental Setup
4.1 Training Data
The experiments are done on the recent German-
to-English WMT 2013 translation task
1
. For
1
The translation task resources of WMT 2013 are avail-
able under: http://www.statmt.org/wmt13/
459
Corpus Sent De En
Training data
news-commentary 177K 4.8M 4.5M
europarl 1 888K 51.5M 51.9M
common-crawl 2 030K 47.8M 47.7M
total 4 095K 104.1M 104M
Test data
newstest08 2051 52446 49749
newstest09 2525 68512 65648
newstest10 2489 68232 62024
newstest11 3003 80181 74856
newstest12 3003 79912 73089
newstest13 3000 69066 64900
Table 1: German-English bilingual training and
test data statistics: the number of sentence pairs
(Sent), German (De) and English (En) words are
given.
German-English WMT 2013, the common-crawl
bilingual corpus was introduced, enabling more
impact for TM adaptation on the SMT system
quality. Monolingual English data exists with
more than 1 billion words, making LM adapta-
tion and size reduction a wanted feature. We use
newstest08 throughout newstest13 to evaluate the
SMT systems. The baseline systems are built
using all (unfiltered) available monolingual and
bilingual training data. The bilingual corpora and
the test data statistics are summarized in Table 1.
In Table 2, we summarize the size and LM per-
plexity of the different monolingual corpora for
the German-English task over the LM develop-
ment set newstest09 and test set newstest13. The
corpora are split into three parts, the English side
of the bilingual side (bi.en), the giga-fren joined
with undoc (giun) and the news-shuffle (ns) cor-
pus. To keep the perplexity results comparable,
we use the intersection vocabulary of the different
corpora as a reference vocabulary. From the table,
we notice that as expected, the in-domain corpus
news-shuffle generate the best perplexity values.
4.2 SMT System
The baseline system is built using the open-source
SMT toolkit Jane
2
, which provides state-of-the-art
phrase-based SMT system (Wuebker et al., 2012).
We use the standard set of models with phrase
translation probabilities for source-to-target and
2
www.hltpr.rwth-aachen.de/jane
Corpus Tokens ppl
[M] dev test
bi.en 88 216.5 192.7
giun 775 229.0 198.9
ns 1 479 144.1 122.7
Table 2: German-English monolingual corpora
statistics: the number of tokens is given in millions
[M], ppl is the perplexity of the corresponding cor-
pus.
target-to-source directions, smoothing with lexi-
cal weights, a word and phrase penalty, distance-
based reordering, hierarchical reordering model
(Galley and Manning, 2008) and a 4-gram target
language model. The baseline system is compet-
itive and using adaptation we will show compa-
rable results to the best systems of WMT 2013.
The SMT system was tuned on the development
set newstest10 with minimum error rate training
(MERT) (Och, 2003) using the BLEU (Papineni
et al., 2002) error rate measure as the optimiza-
tion criterion. We test the performance of our sys-
tem on the newstest08...newstest13 sets using the
BLEU and translation edit rate (TER) (Snover et
al., 2006) measures. We use TER as an additional
measure to verify the consistency of our improve-
ments and avoid over-tuning. All results are based
on true-case evaluation. We perform bootstrap re-
sampling with bounds estimation as described by
(Koehn, 2004). We use the 90% and 95% (denoted
by ? and ? correspondingly in the tables) confi-
dence thresholds to draw significance conclusions.
5 Results
To perform adaptation, an adaptation set repre-
senting the in-domain needs to be specified to be
plugged in eq. (1) as IN. The choice of the adap-
tation corpus is crucial for the successful appli-
cation of the cross-entropy based scoring, as the
closer the corpus is to our test domain, the bet-
ter adaptation we get. For the WMT task, the
choice of the adaptation corpus is not an easy
task. The genre of the test sets is newswire, while
the bilingual training data is composed of news-
commentary, parliamentary records (europarl) and
common-crawl noisy data. On the other hand, the
monolingual data includes large amounts of in-
domain newswire data (news-shuffle).
For LM training, the task of adaptation might
be unprofitable in terms of performance, as the
460
 110
 120
 130
 140
 150
 160
 170
6.25% 12.5% 25% 50% 100%
per
ple
xity
size
REF-devREF-testHYP-devHYP-test
Figure 1: Size (fraction of news-shuffle data)
against the resulting LM perplexity on dev and
test, using different filtering sets.
majority of the training is in-domain. Still, one
might hope that by using adaptation, a more com-
pact and comparable LM can be generated. An-
other point is that LM training is less demanding
than TM training, and a comparison of the results
of LM and TM adaptation might prove fruitful and
convey additional information.
Next, we start with LM adaptation experiments
where we mainly compare different adaptation
sets for filtering over the final translation quality.
A comparison to the full (unfiltered LM) is also
produced. For TM adaptation, we repeat the adap-
tation sets choice experiment and analyze the dif-
ference between the sets.
5.1 LM Adaptation
To evaluate our methods experimentally, we use
the German-English translation task to compare
different adaptation sets for filtering and then an-
alyze the full versus the filtered LM SMT system
results. We recall that newstest09 is used as a de-
velopment set and newstest13 as a test set in the
LM experiments.
The different adaptation sets for filtering that we
explore are: (i) unsupervised: an automatic trans-
lation of the test sets (newstest08...newstest13),
where the baseline system (without adaptation)
is used to generate the hypotheses which then
define the adaptation corpus for filtering (HYP),
(ii) supervised: the references of the test sets new-
stest08...newstest12 concatenated, newstest13 is
kept as a blind set, which will also help us deter-
mine if overfitting occurs (REF), and (iii) pseudo
supervised: a pseudo in-domain corpus, news-
Corpus Adapt Optimal ppl
set size dev test
ns
none 100% 144 123
NC 100% 144 123
REF 6.25% 111 161
HYP 50% 139 118
giun
none 100% 229 199
NC 50% 215 185
REF 6.25% 161 171
HYP 12.5% 187 159
Table 3: Optimal size portion and resulting per-
plexities, across adaptation sets (NC, REF and
HYP) and monolingual LM training corpora.
commentary, where the domain is similar to the
test set domain, but the style might differ (NC).
Next, we filter the news-shuffle (ns) and giga-
fren+undoc (giun) according to the three sug-
gested adaptations sets, where we plug each adap-
tation set in eq. (1) as IN and compare their per-
formance.
5.1.1 Perplexity Results
In Figure 1, we draw the size portion versus the
dev and test perplexities for the REF and HYP
adaptation sets over the news-shuffle corpus. REF
performs best for filtering the dev set, where an
optimum is achieved when using only 6.25% of
the news-shuffle data, with a perplexity of 111 in
comparison to 144 perplexity of the full LM. Mea-
suring perplexities over newstest08-12, REF based
filtering achieves 109 while the full LM achieves
140. The good performance on the seen sets
comes with the cost of severe overfitting, where
the test set perplexity using 6.25% of the data is
161, much higher than 123 generated by the full
LM. On the other hand, HYP achieves an optimum
for both sets when using 50% of the data. A sum-
mary of the best results across monolingual cor-
pora and adaptation sets is given in Table 3. Fil-
tering the giun monolingual corpus shows similar
results to ns filtering, where overfitting occurs on
the blind test set when using REF as the target do-
main. HYP-based adaptation achieves the best LM
perplexity on the blind test set. NC-based adapta-
tion retains the biggest amount of data, 50% for
the giun corpus and 100% (no filtering) for the ns
corpus. REF-based adaptation shows overfitting
on the seen dev set, and the worst results on the
blind test set when filtering the ns corpus.
461
LM data Adapt. ppl newstest10 newstest11 newstest12 newstest13
set BLEU TER BLEU TER BLEU TER BLEU TER
bi.en+giun
none 162 23.2 59.6 21.2 61.0 21.8 60.9 24.6 57.2
NC 160 23.2 59.3 21.5 61.0 21.9 60.7 24.6 57.0
REF 158 23.7 59.2 21.9 60.5 22.2 60.5 24.5 57.3
HYP 151 23.6 59.2 21.5 60.9 22.2 60.4 25.1 56.7
+ns
none 111 24.5 59.1 22.1 61.3 23.3 60.1 25.9 56.7
NC 111 24.4 58.7 22.1 60.5 23.4 59.7 25.5 56.6
REF 143 25.7 57.8 23.0 59.9 24.2 59.4 24.1 57.8
HYP 109 25.0 58.2 22.1 60.6 23.5 59.6 25.9 56.3
Table 4: German-English LM filtering results using different adaptation sets. The LM perplexity over
the blind test set nestest13, as well as BLEU and TER percentages are presented.
5.1.2 Translation Results
Next, we measure whether the improvements of
the single adapted corpora carry over to the mix-
ture LM both in perplexity and translation quality.
The mixture LM is created by linear interpolation
(of bi.en, giun and ns) with perplexity minimiza-
tion on the dev set using the SRILM toolkit
3
. We
carry out two experiments, in the first we interpo-
late the English side of the bilingual data with a
giun LM, then we add the ns LM. This way we
measure whether the effects of adaptation carry
over to a stronger baseline.
The SMT systems built using the full and fil-
tered LMs are compared in Table 4. The table
includes the data used for LM training, the adap-
tation set used to filter the data, the perplexity
of the resulting LM on the test set (newstest13)
and the resulting SMT system quality over new-
stest10...newstest13.
Starting with the first block of experiments us-
ing LM data composed from the English side
of the bilingual corpora and the giun corpus
(bi.en+giun), the unfiltered LM performs worse,
both in terms of perplexity and translation qual-
ity. The NC based adaptation improves the results
slightly, with gains upto +0.3% BLEU on new-
stest11 and -0.3% TER on newstest10. The over-
fitting behavior of REF adapted LMs carries over
to the mixture LM, mainly on the translation qual-
ity. The REF adapted LM system translation re-
sults are better on the test sets used to perform the
adaptation, but worse on the blind test set (new-
stest13). The HYP system performs best in terms
of perplexity. REF is better than HYP over the
non-blind test sets, but HYP outperforms REF on
3
http://www.speech.sri.com/projects/srilm/
newstest13 with an improvement of +0.6% BLEU
and -0.6% TER.
The second block of experiments where news-
shuffle (ns) is added to the mixture shows even
stronger overfitting for REF. The REF based adap-
tation is performing worse in terms of perplexity,
143 in comparison to 111 for the full LM. On the
blind set newstest13, REF is hindering the results
with a loss of -1.8% BLEU in comparison to the
full system, and a loss of -0.4% BLEU in compar-
ison to the corresponding system without ns. On
the non-blind sets, REF is performing best, show-
ing typical overfitting. Comparing the full LM
system to the HYP adapted LM, big improvements
are mainly observed on TER, with significance at
the 95% level for newstest10.
We conclude that using the references as adap-
tation set causes overfitting, using a pseudo in-
domain set as the news-commentary does not im-
prove the results, and the best choice is using the
automatic translations (HYP).
As already mentioned in Section 2, we experi-
mented with adding the automatic translations of
the test sets (HYP) to the LM. Doing so resulted
in 8 points perplexity reduction, but no impact on
the MT quality was observed. Therefore, we deem
these perplexity improvements by adding HYP as
artificial.
5.2 TM Adaptation
In the LM adaptation experiments, we found that
using the test sets automatic translation as the
adaptation set (HYP system) for filtering per-
formed best, in terms of LM quality (perplex-
ity) and translation quality, when compared to the
other suggested adaptation sets, especially on the
blind test set.
462
LM TM newstest10 newstest11 newstest12 newstest13
BLEU TER BLEU TER BLEU TER BLEU TER
full full 24.5 59.1 22.1 61.3 23.3 60.1 25.9 56.7
HYP
full 25.0 58.2? 22.1 60.6 23.5 59.6 25.9 56.3
TM Filtering
REF-25% 25.1 57.9? 22.4 60.2? 24.0? 59.1? 25.5 56.7
HYP-50% 25.2 58.0? 22.2 60.5? 23.8? 59.4? 26.0 56.4
TM Weighting
ppl.NC 25.0 58.1? 22.5 60.2? 23.6 59.5? 26.1 56.2
ppl.TST 24.8 58.8 22.3 60.7 23.6 59.7 26.0 56.3
ppl.REF 24.8 58.2? 22.2 60.3? 23.7 59.5? 25.5 56.4
ppl.HYP 25.4? 57.8? 22.5 60.1? 23.9? 59.3? 26.4? 55.9?
Table 5: German-English TM filtering and weighting results using different adaptation sets. The results
are given in BLEU and TER percentages. Significance is measured over the full system (first row).
For TM adaptation, we experiment with filter-
ing and weighting based adaptation. By using
weighting, we expect further improvements over
the baseline and better differentiation between the
competing adaptation sets.
To perform filtering, we concatenate all the
bilingual corpora in Table 1 and sort them accord-
ing to the combined LM+M1 cross-entropy score.
We then extract the top 50%,25%,... bilingual sen-
tence from the sorted corpus, generate the phrase
table for each setup and reoptimize the system us-
ing MERT on the development set.
Weighted phrase extraction is based on the same
LM+M1 combined cross entropy score as filter-
ing, but instead of discarding whole sentences we
weight them according to their relevance to the
adaptation set being used.
In this section, we compare the three adapta-
tion sets suggested for LM filtering for the TM
component. In addition, one might argue that for
the bilingual case, the source side of the test set
might be sufficient to perform adaptation, or even
it might perform better for TM adaptation as the
automatically generated translation might not be
as reliable. We perform an experiment using the
source side of the test sets as an adaptation set to
score the source side of the bilingual corpora (de-
noted TST in the experiments). To summarize, we
collect 4 corpora as adaptation sets to be used for
adapting the TM: (i) NC, HYP, and REF as defined
for LM but using both source and target (automat-
ically generated for HYP) sides, and (ii) TST using
only the source side of the test sets.
The results comparing the 4 suggested adapta-
tion sets for filtering and weighting are given in
Table 5. In this table, we use newstest10 as be-
fore for MERT optimization and display results for
newstest10...newstest13. Note that for TM filter-
ing and weighting we use the HYP adapted LM as
it achieves the best results in the previous section.
For filtering, the NC and TST adaptation sets
could not improve the dev results over the full sys-
tem therefore they are omitted. REF based adapta-
tion achieves the best dev results when using 25%
of the bilingual data while HYP based adaptation
uses 50% of the data. For TM filtering, only slight
overfitting is observed, where the REF system is
slightly better than HYP on the non blind sets and
is worse on the blind test set. We hypothesize that
no severe overfitting is observed for TM filtering
as we use a strong LM adapted with the HYP set,
therefore degradation is lessened.
Next, we focus on weighted phrase extraction
for adaptation using the various adaptation sets.
Comparing filtering to weighting, weighting im-
proves for the ppl.HYP based adaptation but a
slight loss is observed for the ppl.REF system ex-
cept on the blind test set. We conclude that due to
the usage of more data in the weighting scenario,
overfitting is lessened. Using the source side of the
test sets for weighting (ppl.TST) achieves good re-
sults, with improvements over the ppl.REF system
on newstest13.
The ppl.HYP system achieves the best results
among the weighted systems. Comparing the
full unadapted system with the LM+TM adapted
ppl.HYP system, we achieve significant BLEU im-
provements on most sets, TER improvements are
significant in all cases with 95% significance level.
The highest gains are on the development set with
463
+0.9% BLEU and -1.3% TER improvements, on
the test sets, newstest12 improves with +0.6%
BLEU and -0.8% TER and newstest13 improves
with +0.5% BLEU and -0.8% TER. The ppl.HYP
system is comparable to the best single system
of WMT 2013
4
(26.4% BLEU vs 26.8% BLEU
for Edinburgh submission, RWTH submission is a
system combination). Note that we are not using
the LDC GigaWord corpus.
We conclude that using in-domain automatic
translations (HYP) for TM weighting performs
best, better than using source side only in-domain
(TST) and better than using the references (REF)
especially on the blind test set. TM adaptation
shows further improvements on top of LM adap-
tation and achieves significant gains.
6 Conclusion
In this work, we tackle the problem of adaptation
without labeled bilingual in-domain training data.
The only information about the test domain is en-
capsulated in the test sets themselves. We experi-
ment with unsupervised adaptation for SMT, using
automatic translations of the test sets, focusing on
adaptation for the LM and the TM components.
We use cross-entropy based scoring for the task
of adaptation, as this method proved successful in
previous work. We utilize filtering for LM adapta-
tion, while we compare filtering and weighting for
TM adaptation.
For LM adaptation, the setup we devise al-
ready contains a majority of in-domain data, still
we could report improvements over the unadapted
baseline. We compose three different adaptation
sets for filtering using automatic translation of the
test data (HYP), a pseudo in-domain set (NC) and
the references (REF) of the test sets (keeping one
blind test set). The NC based filtering is not able to
perform good selection, for news-shuffle the whole
corpus is retained and for giun 50% of the data is
retained. The perplexity results and the translation
quality are virtually unchanged in comparison to
the full system. Using REF as the target set causes
overfitting, where the results are better on the seen
test sets but worse on the blind test set. The best
performing target set in our experiments is the un-
supervised HYP adaptation set, achieving the best
perplexity as well as the best translation quality on
the blind test set. Therefore, we conclude that for
4
http://matrix.statmt.org/matrix/
systems_list/1712
developing a successful SMT system that can gen-
eralize to new data the HYP based adaptation is
preferred.
Next, we perform TM adaptation, where we re-
peat the comparison between the different adapta-
tion sets for filtering as well as weighting. We also
compare to adaptation based only on the source
side of the test sets (TST). The LM adaptation
results hold for TM adaptation, where using the
automatic translations method shows the best re-
sults for the blind test set. Our experiments show
that using the source side only of the test set for
adaptation performs worse than the unsupervised
method, reminiscent to results reported in previous
work comparing supervised source side against
bilingual filtering (Axelrod et al., 2011). For filter-
ing, the REF system suffers from overfitting, while
when using weighting for adaptation, overfitting
is lessened. Comparing the unadapted baseline to
the adapted LM and TM system using the HYP
set, improvements of +1.0% BLEU and -1.3% TER
are reported on the development set while +0.5%
BLEU and -0.8% TER improvements are reported
on the blind test set.
Acknowledgments
This material is based upon work supported by
the DARPA BOLT project under Contract No.
HR0011-12-C-0015. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
M. Bacchiani and B. Roark. 2003. Unsupervised
language model adaptation. In Acoustics, Speech,
and Signal Processing, 2003. Proceedings. (ICASSP
?03). 2003 IEEE International Conference on, vol-
ume 1, pages I?224 ? I?227 vol.1, april.
Jerome R Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42(1):93 ? 108. Adaptation Meth-
ods for Speech Recognition.
M Federico M Cettolo, L Bentivogli, M Paul, and
S St?uker. 2012. Overview of the iwslt 2012 eval-
uation campaign. In International Workshop on
464
Spoken Language Translation, pages 12?33, Hong
Kong, December.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic, June.
Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, Hawaii, USA,
October.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Conference, vol-
ume 10, pages 260?286, Tokyo, Japan, June.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
EAMT conference on ?Practical applications of ma-
chine translation?, pages 133?1142, May.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing (EMNLP), pages 388?395, Barcelona,
Spain, July.
Saab Mansour and Hermann Ney. 2012. A sim-
ple and effective weighted phrase extraction for ma-
chine translation adaptation. In International Work-
shop on Spoken Language Translation, pages 193?
200, Hong Kong, December.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
Raphael Rubino, Antonio Toral, Santiago
Cort?es Va??llo, Jun Xie, Xiaofeng Wu, Stephen
Doherty, and Qun Liu. 2013. The CNGL-DCU-
Prompsit translation systems for WMT13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 213?218, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 25?32, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, Mumbai, India, Decem-
ber.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
465
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?10,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Vector Space Models for Phrase-based Machine Translation
Tamer Alkhouli
1
, Andreas Guta
1
, and Hermann Ney
1,2
1
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2
Spoken Language Processing Group
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
{surname}@cs.rwth-aachen.de
Abstract
This paper investigates the application
of vector space models (VSMs) to the
standard phrase-based machine translation
pipeline. VSMs are models based on
continuous word representations embed-
ded in a vector space. We exploit word
vectors to augment the phrase table with
new inferred phrase pairs. This helps
reduce out-of-vocabulary (OOV) words.
In addition, we present a simple way to
learn bilingually-constrained phrase vec-
tors. The phrase vectors are then used to
provide additional scoring of phrase pairs,
which fits into the standard log-linear
framework of phrase-based statistical ma-
chine translation. Both methods result
in significant improvements over a com-
petitive in-domain baseline applied to the
Arabic-to-English task of IWSLT 2013.
1 Introduction
Categorical word representation has been widely
used in many natural language processing (NLP)
applications including statistical machine transla-
tion (SMT), where words are treated as discrete
random variables. Continuous word representa-
tions, on the other hand, have been applied suc-
cessfully in many NLP areas (Manning et al.,
2008; Collobert and Weston, 2008). However,
their application to machine translation is still an
open research question. Several works tried to ad-
dress the question recently (Mikolov et al., 2013b;
Zhang et al., 2014; Zou et al., 2013), and this work
is but another step in that direction.
While categorical representations do not encode
any information about word identities, continuous
representations embed words in a vector space, re-
sulting in geometric arrangements that reflect in-
formation about the represented words. Such em-
beddings open the potential for applying informa-
tion retrieval approaches where it becomes possi-
ble to define and compute similarity between dif-
ferent words. We focus on continuous represen-
tations whose training is influenced by the sur-
rounding context of the token being represented.
One motivation for such representations is to cap-
ture word semantics (Turney et al., 2010). This
is based on the distributional hypothesis (Harris,
1954) which says that words that occur in similar
contexts tend to have similar meanings.
We make use of continuous vectors learned
using simple neural networks. Neural networks
have been gaining increasing attention recently,
where they have been able to enhance strong SMT
baselines (Devlin et al., 2014; Sundermeyer et
al., 2014). While neural language and transla-
tion modeling make intermediate use of continu-
ous representations, there have been also attempts
at explicit learning of continuous representations
to improve translation (Zhang et al., 2014; Gao et
al., 2013).
This work explores the potential of word se-
mantics based on continuous vector representa-
tions to enhance the performance of phrase-based
machine translation. We present a greedy algo-
rithm that employs the phrase table to identify
phrases in a training corpus. The phrase table
serves to bilingually restrict the phrases spotted
in the monolingual corpus. The algorithm is ap-
plied separately to the source and target sides of
the training data, resulting in source and target cor-
pora of phrases (instead of words). The phrase
corpus is used to learn phrase vectors using the
same methods that produce word vectors. The
vectors are then used to provide semantic scor-
ing of phrase pairs. We also learn word vectors
and employ them to augment the phrase table with
paraphrased entries. This leads to a reduction in
1
the OOV rate which translates to improved BLEU
and and TER scores. We apply the two methods on
the IWSLT 2013 Arabic-to-English task and show
significant improvements over a strong in-domain
baseline.
The rest of the paper is structured as follows.
Section 2 presents a background on word and
phrase vectors. The construction of the phrase
corpus is discussed in Section 3, while Section 4
demonstrates how to use word and phrase vectors
in the standard phrase-based SMT pipeline. Ex-
periments are presented in Section 5, followed by
an overview of the related word in Section 6, and
finally Section 7 concludes the work.
2 Vector Space Models
One way to obtain context-based word vectors is
through a neural network (Bengio et al., 2003;
Schwenk, 2007). With a vocabulary size V , one-
hot encoding of V -dimensional vectors is used to
represent input words, effectively associating each
word with a D-dimensional vector in the V ?D
input weight matrix, where D is the size of the
hidden layer. Similarly, one-hot encoding on the
output layer associates words with vectors in the
output weight matrix.
Alternatively, a count-based V-dimensional
word co-occurrence vector can serve as a word
representation (Lund and Burgess, 1996; Lan-
dauer and Dumais, 1997). Such representations
are sparse and high-dimensional, which might re-
quire an additional dimensionality reduction step
(e.g. using SVD). In contrast, learning word rep-
resentations via neural models results directly in
relatively low-dimensional, dense vectors. In this
work, we follow the neural network approach to
extract the feature vectors. Whether word vectors
are extracted by means of a neural network or co-
occurrence counts, the context surrounding a word
influences its final representation by design. Such
context-based representations can be used to de-
termine semantic similarities.
The construction of phrase representations, on
the other hand, can be done in different ways.
The compositional approach constructs the vector
representation of a phrase by resorting to its con-
stituent words (or sub-phrases) (Gao et al., 2013;
Chen et al., 2010). Kalchbrenner and Blunsom
(2013) obtain continuous sentence representations
by applying a sequence of convolutions, starting
with word representations.
Another approach for phrase representation
considers phrases as atomic units that can not be
divided further. The representations are learned
directly in this case (Mikolov et al., 2013b; Hu et
al., 2014).
In this work, we follow the second approach to
obtain phrase vectors. To this end, we apply the
same methods that yield word vectors, with the
difference that phrases are used instead of words.
In the case of neural word representations, a neural
network that is presented with words at the input
layer is presented with phrases instead. The result-
ing vocabulary size in this case would be the num-
ber of distinct phrases observed during training.
Although learning phrase embeddings directly is
amenable to data sparsity issues, it provides us
with a simple means to build phrase vectors mak-
ing use of tools already developed for word vec-
tors, focussing the effort on preprocessing the data
as will be discussed in the next section.
3 Phrase Corpus
When training word vectors using neural net-
works, the network is presented with a corpus.
To build phrase vectors, we first identify phrases
in the corpus and generate a phrase corpus. The
phrase corpus is similar to the original corpus ex-
cept that its words are joined to make up phrases.
The new corpus is then used to train the neural net-
work. The columns of the resulting input weight
matrix of the network are the phrase vectors corre-
sponding to the phrases encountered during train-
ing.
Mikolov et al. (2013b) identify phrases using a
monolingual point-wise mutual information crite-
rion with discounting. Since our end goal is to
generate phrase vectors that are helpful for trans-
lation, we follow a different approach: we con-
strain the phrases by the conventional phrase table
of phrase-based machine translation. This is done
by limiting the phrases identified in the corpus to
high quality phrases occurring in the phrase table.
The quality is determined using bilingual scores
of phrase pairs. While the phrase vectors of a lan-
guage are eventually obtained by training the neu-
ral network on the monolingual phrase corpus of
that language, the reliance on bilingual scores to
2
Algorithm 1 Phrase Corpus Construction
1: p? 1
2: for p? numPasses do
3: i? 2
4: for i? corpus.size?1 do
5: w?? join(t
i
, t
i+1
) . create a phrase using the current and next tokens
6: v?? join(t
i?1
, t
i
) . create a phrase using the previous and current tokens
7: joinForward? score(w?)
8: joinBackward? score(v?)
9: if joinForward ? joinBackward and joinForward ? ? then
10: t
i
? w?
11: remove t
i+1
12: i? i+2 . newly created phrase not available for further merge during current pass
13: else
14: if joinBackward > joinForward and joinBackward ? ? then
15: t
i?1
? v?
16: remove t
i
17: i? i+2 . newly created phrase not available for further merge during current pass
18: else
19: i? i+1
20: end if
21: end if
22: end for
23: p? p+1
24: end for
construct the monolingual phrase corpus encodes
bilingual information in the corpus, namely, the
corpus will include phrases that having a match-
ing phrase in the other language, which is in line
with the purpose for which the phrases are con-
structed, that is, their use in the phrase-based ma-
chine translation pipeline which is explained in the
next section. In addition, the aforementioned scor-
ing serves to exclude noisy phrase-pair entries dur-
ing the construction of the phrase corpus. Next, we
explain the details of the construction algorithm.
3.1 Phrase Spotting
We propose Algorithm 1 as a greedy approach for
phrase corpus construction. It is a multi-pass algo-
rithm where each pass can extend tokens obtained
during the previous pass by a single token at most.
Before the first pass, all tokens are words. During
the passes the tokens might remain as words or can
be extended to become phrases. Given a token t
i
at position i, a scoring function is used to score
the phrase (t
i
, t
i+1
) and the phrase (t
i?1
, t
i
). The
phrase having a higher score is adopted as long as
its score exceeds a predefined threshold ? . The
scoring function used in lines 7 and 8 is based on
the phrase table. If the phrase does not belong to
the phrase table it is given a score ?
?
< ? . If the
phrase exists, a bilingual score is computed using
the phrase table fields as follows:
score(
?
f ) = max
e?
{
L
?
i=1
w
i
g
i
(
?
f , e?)
}
(1)
where g
i
(
?
f , e?) is the ith feature of the bilingual
phrase pair (
?
f , e?). The maximization is carried out
over all phrases e? of the other language. The score
is the weighted sum of the phrase pair features.
Throughout our experiments, we use 2 phrasal and
2 lexical features for scoring, with manual tuning
of the weights w
i
.
The resulting corpus is then used to train phrase
vectors following the same procedure of training
word vectors.
4 End-to-end Translation
In this section we will show how to employ phrase
vectors in the phrase-based statistical machine
translation pipeline.
3
4.1 Phrase-based Machine Translation
The phrase-based decoder consists of a search us-
ing a log-linear framework (Och and Ney, 2002)
as follows:
e?
?
I
1
= argmax
I,e
I
1
{
max
K,s
K
1
M
?
m=1
?
m
h
m
(e
I
1
,s
K
1
, f
J
1
)
}
(2)
where e
I
1
= e
1
...e
I
is the target sentence, f
J
1
=
f
1
... f
J
is the source sentence, s
K
1
= s
1
...s
K
is
the hidden alignment or derivation. The mod-
els h
m
(e
I
1
,s
K
1
, f
J
1
) are weighted by the weights ?
m
which are tuned using minimum error rate train-
ing (MERT) (Och, 2003). The rest of the section
presents two ways to integrate vector representa-
tions into the system described above.
4.2 Semantic Phrase Feature
Words that occur in similar contexts tend to have
similar meanings. This idea is known as the dis-
tributional hypothesis (Harris, 1954), and it moti-
vates the use of word context to learn word repre-
sentations that capture word semantics (Turney et
al., 2010). Extending this notion to phrases, phrase
vectors that are learned based on the surrounding
context encode phrase semantics. Since we will
use phrase vectors to compute a feature of a phrase
pair in the following, we refer to the feature as a
semantic phrase feature.
Given a phrase pair (
?
f , e?), we can use the phrase
vectors of the source and target phrases to compute
a semantic phrase feature as follows:
h
M+1
(
?
f , e?) = sim(Wx
?
f
,z
e?
) (3)
where sim is a similarity function, x
?
f
and z
e?
are the
S-dimensional source and T -dimensional target
vectors respectively corresponding to the source
phrase
?
f and target phrase e?. W is an S?T linear
projection matrix that maps the source space to the
target space (Mikolov et al., 2013a). The matrix
is estimated by optimizing the following criterion
with stochastic gradient descent:
min
W
N
?
i=1
||Wx
i
? z
i
||
2
(4)
where the training data consists of the pairs
{(x
1
,z
1
), ...,(x
N
,z
N
)} corresponding to the source
and target vectors.
Since the source and target phrase vectors are
learned separately, we do not have an immedi-
ate mapping between them. As such mapping is
needed for the training of the projection matrix,
we resort to the phrase table to obtain it. A source
and a target phrase vectors are paired if there is a
corresponding phrase pair entry in the phrase table
whose score exceeds a certain threshold. Scoring
is computed using Eq. 1. Similarly, word vectors
are paired using IBM 1 p(e| f ) and p( f |e) lexica.
Noisy entries are assumed to have a probability
less than a certain threshold and are not used to
pair word vectors.
4.3 Paraphrasing
While the standard phrase table is extracted using
parallel training data, we propose to extend it and
infer new entries relying on continuous representa-
tions. With a similarity measure (e.g. cosine sim-
ilarity) that computes the similarity between two
phrases, a new phrase pair can be generated by re-
placing either or both of its constituent phrases by
similar phrases. The new phrase is referred to as a
paraphrase of the phrase it replaces. This enables
a richer use of the bilingual data, as a source para-
phrase can be borrowed from a sentence that is not
aligned to a sentence containing the target side of
the phrase pair. It also enables the use of monolin-
gual data, as the source and target paraphrases do
not have to occur in the parallel data. The cross-
interaction between sentences in the parallel data
and the inclusion of the monolingual data to ex-
tend the phrase table are potentially capable of re-
ducing the out-of-vocabulary (OOV) rate.
In order to generate a new phrase rule, we en-
sure that noisy rules do not contribute to the gener-
ation process, depending on the score of the phrase
pair (cf. Eq. 1). High scoring entries are para-
phrased as follows. To paraphrase the source side,
we perform a k-nearest neighbor search over the
source phrase vectors. The top-k similar entries
are considered paraphrases of the given phrase.
The same can be done for the target side. We as-
sign the newly generated phrase pair the same fea-
ture values of the pair used to induce it. However,
two extra phrase features are added: one measur-
ing the similarity between the source phrase and
its paraphrase, and another for the target phrase
and its paraphrase. The new feature values for
the original non-paraphrased entries are set to the
4
highest similarity value.
We focus on a certain setting that avoids in-
terference with original phrase rules, by extend-
ing the phrase table to cover OOVs only. That
is, source-side paraphrasing is performed only if
the source paraphrase does not already occur in
the phrase table. This ensures that original entries
are not interfered with and only OOVs are affected
during translation. Reducing OOVs by extending
the phrase table has the advantage of exploiting
the full decoding capabilities (e.g. LM scoring),
as opposed to post-decoding translation of OOVs,
which would not exhibit any decoding benefits.
The k-nearest neighbor (k-NN) approach is
computationally prohibitive for large phrase tables
and large number of vectors. This can be allevi-
ated by resorting to approximate k-NN search (e.g.
locality sensitive hashing). Note that this search
is performed during training time to generate ad-
ditional phrase table entries, and does not affect
decoding time, except through the increase of the
phrase table size. In our experiments, the train-
ing time using exact k-NN search was acceptable,
therefore no search approximations were made.
5 Experiments
In the following we first provide an analysis of the
word vectors that are later used for translation ex-
periments. We use word vectors (as opposed to
phrase vectors) for phrase table paraphrasing to
reduce the OOV rate. Next, we present end-to-
end translation results using the proposed seman-
tic feature and our OOV reduction method.
The experiments are based on vectors trained
using the word2vec
1
toolkit, setting vector dimen-
sionality to 800 for Arabic and 200 for English
vectors. We used the skip-gram model with a max-
imum skip length of 10. The phrase corpus was
constructed using 5 passes, with scores computed
according to Eq. 1 using 2 phrasal and 2 lexical
features. The phrasal and lexical weights were set
to 1 and 0.5 respectively, with all features being
negative log-probabilities, and the scoring thresh-
old ? was set to 10. All translation experiments
are performed with the Jane toolkit (Vilar et al.,
2010; Wuebker et al., 2012).
1
https://code.google.com/p/word2vec/
5.1 Baseline System
Our phrase-based baseline system consists of two
phrasal and two lexical translation models, trained
using a word-aligned bilingual training corpus.
Word alignment is automatically generated by
GIZA
++
(Och and Ney, 2003) given a sentence-
aligned bilingual corpus. We also include bi-
nary count features and bidirectional hierarchical
reordering models (Galley and Manning, 2008),
with three orientation classes per direction result-
ing in six reordering models. The baseline also in-
cludes word penalty, phrase penalty and a simple
distance-based distortion model.
The language model (LM) is a 4-gram mix-
ture LM trained on several data sets using mod-
ified Kneser-Ney discounting with interpolation,
and combined with weights tuned to achieve the
lowest perplexity on a development set using the
SRILM toolkit (Stolcke, 2002). Data selection
is performed using cross-entropy filtering (Moore
and Lewis, 2010).
5.2 Word Vectors
Here we analyze the quality of word vectors used
in the OOV reduction experiments. The vectors
are trained using an unaltered word corpus. We
build a lexicon using source and target word vec-
tors together with the projection matrix using the
similarity score sim(Wx
f
,z
e
)), where the projec-
tion matrix W is used to project the source word
vector x
f
, corresponding to the source word f , to
the target vector space. The similarity between the
projection result Wx
f
and the target word vector
z
e
is computed. In the following we will refer to
these scores computed using vector representation
as VSM-based scores.
The resulting lexicon is compared to the IBM
1 lexicon
2
. Given a source word, we select the
the best target word according to the VSM-based
score. This is compared to the best translation
based on the IBM 1 probability. If both transla-
tions coincide, we refer to this as a 1-best match.
We also check whether the best translation accord-
ing to IBM 1 matches any of the top-5 translations
based on the VSM model. A match in this case is
referred to as a 5-best match.
2
We assume for the purpose of this experiment that the
IBM 1 lexicon provides perfect translations, which is not nec-
essarily the case in practice.
5
corpus Lang. # tokens # segments
WIT Ar 3,185,357 147,256
UN Ar 228,302,244 7,884,752
arGiga3 Ar 782,638,101 27,190,387
WIT En 2,951,851 147,256
UN En 226,280,918 7,884,752
news En 1,129,871,814 45,240,651
Table 1: Arabic and English corpora statistics.
The vectors are trained on a mixture of in-
domain data (WIT) which correspond to TED
talks, and out-of-domain data (UN). These sets are
provided as part of the IWSLT 2013 evaluation
campaign. We include the LDC2007T40 Arabic
Gigaword v3 (arGiga3) and English news crawl ar-
ticles (2007 through 2012) to experiment with the
effect of increasing the size of the training corpus
on the quality of the word vectors. Table 1 shows
the corpora statistics obtained after preprocessing.
The fractions of the 1- and 5-best matches are
shown in table 2. The table is split into two halves.
The upper part investigates the effect of increasing
the amount of Arabic data while keeping the En-
glish data fixed (2nd row), the effect of increasing
the amount of the English data while keeping the
Arabic data fixed (3rd row), and the effect of using
more data on both sides (4th row). The projection
is done on the representation of the Arabic word f ,
and the similarity is computed between the projec-
tion and the representation of the English word e.
In the lower half of the table, the same effects are
explored, except that the projection is performed
on the English side instead. The results indicate
that the accuracy increases when increasing the
amount of data only on the side being projected.
More data on the corresponding side (i.e. the side
being projected to) decreases the accuracy. The
same behavior is observed whether the projected
side is Arabic (upper half) or English (lower half).
All in all, the accuracy values are low. The accu-
racy increases about three times when looking at
the 5-best instead of the 1-best accuracy. While the
accuracies 32.2% and 33.1% are low, they reflect
that the word representations are encoding some
information about the words, although this infor-
mation might not be good enough to build a word-
to-word lexicon. However, using this information
for OOV reduction might still yield improvements
as we will see in the translation results.
Arabic English
word corpus size 231M 229M
phrase corpus size 126M 115M
word corpus vocab. size 467K 421K
phrase corpus vocab. size 5.8M 5.3M
# phrase vectors 934K 913K
Table 3: Phrase vectors statistics.
5.3 Phrase Vectors
Translation experiments pertaining to the pro-
posed semantic feature are presented here. The
feature is based on phrase vectors which are built
with the word2vec toolkit in a similar way word
vectors are trained, except that the training cor-
pus is the phrase corpus containing phrases con-
structed as described in section 3. Once trained, a
new feature is added to the phrase table. The fea-
ture is computed for each phrase pair using phrase
vectors as described in Eq. 3.
Table 3 shows statistics about the phrase corpus
and the original word corpus it is based on. Al-
gorithm 1 is used to build the phrase corpus using
5 passes. The number of phrase vectors trained
using the phrase corpus are also shown. Note that
the tool used does not produce vectors for all 5.8M
Arabic and 5.3M English phrases in the vocab-
ulary. Rather, noisy phrases are excluded from
training, eventually leading to 934K Arabic and
913K English phrase embeddings.
We perform two experiments on the IWSLT
2013 Arabic-to-English evaluation data set. In the
first experiment, we examine how the semantic
feature affects a small phrase table (2.3M phrase
pairs) trained on the in-domain data (WIT). The
second experiment deals with a larger phrase table
(34M phrase pairs), constructed by a linear inter-
polation between in- and out-of-domain phrase ta-
bles including UN data, resulting in a competitive
baseline. The two baselines have hierarchical re-
ordering models (HRMs) and a tuned mixture LM,
in addition to the standard models, as described in
section 5.1. The results are shown in table 4.
In the small experiment, the semantic phrase
feature improves TER by 0.7%, and BLEU by
0.4% on the test set eval13. The translation seems
to benefit from the contextual information en-
coded in the phrase vectors during training. This
is in contrast to the training of the standard phrase
6
Arabic Data English Data 1-best
Match %
5-best
Matches %
WIT+UN WIT+UN 8.0 26.1
WIT+UN+arGiga3 WIT+UN 10.9 32.2
WIT+UN WIT+UN+news 4.9 17.9
WIT+UN+arGiga3 WIT+UN+news 7.5 25.7
WIT+UN WIT+UN 8.4 27.2
WIT+UN WIT+UN+news 10.9 33.1
WIT+UN+arGiga3 WIT+UN 5.7 18.9
WIT+UN+arGiga3 WIT+UN+news 8.3 25.2
Table 2: The effect of increasing the amount of data on the quality of word vectors. VSM-based scores are
compared to IBM model 1 p(e| f ) (upper half) and p( f |e) (lower half), effectively regarding the IBM 1
models as the true probability distributions. In the upper part, the projection is done on the representation
of the Arabic word f , and the similarity is computed between the projection and the representation of the
English word e. In the lower half of the table, the role of f and e is interchanged, where the English side
in this case will be projected.
system dev2010 eval2013
BLEU TER BLEU TER
WIT 29.1 50.5 28.9 52.5
+ feature 29.1 ?50.1 ?29.3 ?51.8
+ paraph. 29.2 ?50.2 ?29.5 ?51.8
+ both 29.2 50.2 ?29.4 ?51.8
WIT+UN 29.7 49.3 30.5 50.5
+ feature 29.8 49.2 30.2 50.7
Table 4: Semantic feature and paraphrasing re-
sults. The symbol ? indicates statistical signifi-
cance with p < 0.01.
features, which disregards context. As for the hi-
erarchical reordering models which are part of the
baseline, they do not capture lexical information
about the context. They are only limited to the or-
dering information. The skip-gram-based phrase
vectors used for the semantic feature, on the other
hand, discard ordering information, but uses con-
textual lexical information for phrase representa-
tion. In this sense, HRMs and the semantic feature
can be said to complement each other. Using the
semantic feature for the large phrase table did not
yield improvements. The difference compared to
the baseline in this case is not statistically signifi-
cant.
All reported results are averages of 3 MERT op-
timizer runs. Statistical significance is computed
using the Approximate Randomization (AR) test.
We used the multeval toolkit (Clark et al., 2011)
for evaluation.
5.4 Paraphrasing and OOV Reduction
The next set of experiments investigates the re-
duction of the OOV rate through paraphrasing,
and its impact on translation. Paraphrasing is per-
formed employing the cosine similarity, and the k-
NN search is done on the source side, with k = 3.
The nearest neighbors are required to satisfy a ra-
dius threshold r > 0.3, i.e., neighbors with a simi-
larity value less or equal to r are rejected. Training
the projection matrices is performed using a small
amount of training data amounting to less than 30k
translation pairs.
To examine the effect of OOV reduction, we
perform paraphrasing on a resource-limited sys-
tem, where a small amount of parallel data ex-
ists, but a larger amount of monolingual data is
available. Such a system is simulated by train-
ing word vectors on the WIT+UN data monolin-
gually , while extracting the phrase table using the
much smaller in-domain WIT data set only. Table
5 shows the change in the number of OOV words
after introducing the paraphrased rules to the WIT-
based phrase table. 19% and 30% of the original
OOVs are eliminated in the dev and eval13 sets,
respectively. This reduction translates to an im-
provement of 0.6% BLEU and 0.7% TER as indi-
cated in table 4.
Since BLEU or TER are based on word iden-
tities and do not detect semantic similarities, we
make a comparison between the reference transla-
tions and translations of the system that employed
7
# OOV
phrase table dev eval13
WIT 185 254
WIT+paraph. 150 183
Vocab. size 3,714 4,734
Table 5: OOV change due to paraphrasing. Vocab-
ulary refers to the number of unique tokens in the
Arabic dev and test sets.
OOV VSM-based
Translation
Reference

I
	
?

??

K found unfolded

??


Qk interested keen
?


	
?m
.

?
jail imprisoned
	
?CK
.
claim report

??
.

J?? confusing confounding

I

Jk encourage rallied for
AK


?Q

? villagers redneck
Table 6: Examples of OOV words that were trans-
lated due to paraphrasing. The examples are
extracted from the translation hypotheses of the
small experiment.
OOV reduction. Examples are shown in Table 6.
Although the reference words are not matched ex-
actly, the VSM translations are semantically close
to them, suggesting that OOV reduction in these
cases was somewhat successful, although not re-
warded by either of the scoring measures used.
6 Related Work
Bilingually-constrained phrase embeddings were
developed in (Zhang et al., 2014). Initial embed-
dings were trained in an unsupervised manner, fol-
lowed by fine-tuning using bilingual knowledge to
minimize the semantic distance between transla-
tion equivalents, and maximizing the distance be-
tween non-translation pairs. The embeddings are
learned using recursive neural networks by de-
composing phrases to their constituents. While
our work includes bilingual constraints to learn
phrase vectors, the constraints are implicit in the
phrase corpus. Our approach is simple, focusing
on the preprocessing step of preparing the phrase
corpus, and therefore it can be used with different
existing frameworks that were developed for word
vectors.
Zou et al. (2013) learn bilingual word embed-
dings by designing an objective function that com-
bines unsupervised training with bilingual con-
straints based on word alignments. Similar to
our work, they compute an additional feature for
phrase pairs using cosine similarity. Word vec-
tors are averaged to obtain phrase representations.
In contrast, our approach learns phrase representa-
tions directly.
Recurrent neural networks were used with min-
imum translation units (Hu et al., 2014), which are
phrase pairs undergoing certain constraints. At the
input layer, each of the source and target phrases
are modeled as a bag of words, while the output
phrase is predicted word-by-word assuming con-
ditional independence. The approach seeks to al-
leviate data sparsity problems that would arise if
phrases were to be uniquely distinguished. Our
approach does not break phrases down to words,
but learns phrase embeddings directly.
Chen et al. (2010) represent a rule in the hierar-
chical phrase table using a bag-of-words approach.
Instead, we learn phrase vectors directly without
resorting to their constituent words. Moreover,
they apply a count-based approach and employ
IBM model 1 probabilities to project the target
space to the source space. In contrast, our map-
ping is similar to that of Mikolov et al. (2013a)
and is learned directly from a small set of bilin-
gual data.
Mikolov et al. (2013a) proposed an efficient
method to learn word vectors through feed-
forward neural networks by eliminating the hid-
den layer. They do not report end-to-end sentence
translation results as we do in this work.
Mikolov et al. (2013b) learn direct representa-
tions of phrases after joining a training corpus us-
ing a simple monolingual point-wise mutual in-
formation criterion with discounting. Our work
exploits the rich bilingual knowledge provided by
the phrase table to join the corpus instead.
Gao et al. (2013) learn shared space mappings
using a feed-forward neural network and represent
a phrase vector as a bag-of-words vector. The vec-
tors are learned aiming to optimize an expected
BLEU criterion. Our work is different in that we
learn two separate source and target mappings.
8
We also do not follow their bag-of-words phrase
model approach.
Marton et al. (2009) proposed to eliminate
OOVs by looking for similar words using distri-
butional vectors, but they prune the search space
limiting it to candidates observed in the same con-
text as that of the OOV. We do not employ such a
heuristic. Instead, we perform a k-nearest neigh-
bor search spanning the full phrase table to para-
phrase its rules and generate new entries.
Estimating phrase table scores using monolin-
gual data was investigated in (Klementiev et al.,
2012), by building co-occurrence context vectors
and using a small dictionary to induce new scores
for existing phrase rules. Our work explores the
use of distributional vectors extracted from neu-
ral networks, moreover, we induce new phrase
rules to extend the phrase table. New phrase rules
were also generated in (Irvine and Callison-Burch,
2014), where new phrases were produced as a
composition of unigram translations.
7 Conclusion
In this work we adapted vector space models to
provide the state-of-the-art phrase-based statisti-
cal machine translation system with semantic in-
formation. We leveraged the bilingual knowledge
of the phrase table to construct source and target
phrase corpora to learn phrase vectors, which were
used to provide semantic scoring of phrase pairs.
Word vectors allowed to extend the phrase table
and eliminate OOVs. Both methods proved bene-
ficial for low-resource tasks.
Future work would investigate decoder inte-
gration of semantic scoring that extends beyond
phrase boundaries to provide semantically coher-
ent translations.
Acknowledgments
This material is partially based upon work sup-
ported by the DARPA BOLT project under Con-
tract No. HR0011- 12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
The research leading to these results has also re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n
o
287658.
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Boxing Chen, George Foster, and Roland Kuhn. 2010.
Bilingual sense similarity for statistical machine
translation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 834?843.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176?181, Portland, Oregon,
June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 848?856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Zellig S Harris. 1954. Distributional structure. Word.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Ann Irvine and Chris Callison-Burch. 2014. Hal-
lucinating phrase translations for low resource mt.
In Proceedings of the Conference on Computational
Natural Language Learning (CoNLL).
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
9
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 130?140. Association for Computa-
tional Linguistics.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers, 28(2):203?208.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 381?390. Association for Com-
putational Linguistics.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220?224, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native Training and Maximum Entropy Models for
Statistical Machine Translation. In Proc. of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 295?302, Philadel-
phia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech & Language, 21(3):492?
518.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation Modeling
with Bidirectional Recurrent Neural Networks. In
Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing, October.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52th Annual Meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP, pages 1393?1398.
10
