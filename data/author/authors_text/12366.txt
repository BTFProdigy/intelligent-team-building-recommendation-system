Proceedings of the Workshop on BioNLP: Shared Task, pages 50?58,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
High-precision biological event extraction with a concept recognizer
K. Bretonnel Cohen?, Karin Verspoor?, Helen L. Johnson, Chris Roeder,
Philip V. Ogren, William A. Baumgartner Jr., Elizabeth White, Hannah Tipney, and Lawrence Hunter
Center for Computational Pharmacology
University of Colorado Denver School of Medicine
PO Box 6511, MS 8303, Aurora, CO 80045 USA
kevin.cohen@gmail.com, karin.verspoor@ucdenver.edu, helen.linguist@gmail.com,
chris.roeder@ucdenver.edu, philip@ogren.info, william.baumgartner@ucdenver.edu,
elizabeth.white@colorado.edu, hannah.tipney@ucdenver.edu, larry.hunter@ucdenver.edu
Abstract
We approached the problems of event detec-
tion, argument identification, and negation and
speculation detection as one of concept recog-
nition and analysis. Our methodology in-
volved using the OpenDMAP semantic parser
with manually-written rules. We achieved
state-of-the-art precision for two of the three
tasks, scoring the highest of 24 teams at pre-
cision of 71.81 on Task 1 and the highest of 6
teams at precision of 70.97 on Task 2.
The OpenDMAP system and the rule set are
available at bionlp.sourceforge.net.
*These two authors contributed equally to the
paper.
1 Introduction
We approached the problem of biomedical event
recognition as one of concept recognition and anal-
ysis. Concept analysis is the process of taking a
textual input and building from it an abstract rep-
resentation of the concepts that are reflected in it.
Concept recognition can be equivalent to the named
entity recognition task when it is limited to locat-
ing mentions of particular semantic types in text, or
it can be more abstract when it is focused on recog-
nizing predicative relationships, e.g. events and their
participants.
2 BioNLP?09 Shared Task
Our system was entered into all three of the
BioNLP?09 (Kim et al, 2009) shared tasks:
? Event detection and characterization This
task requires recognition of 9 basic biological
events: gene expression, transcription, protein
catabolism, protein localization, binding, phos-
phorylation, regulation, positive regulation and
negative regulation. It requires identification
of the core THEME and/or CAUSE participants
in the event, i.e. the protein(s) being produced,
broken down, bound, regulated, etc.
? Event argument recognition This task builds
on the previous task, adding in additional argu-
ments of the events, such as the site (protein or
DNA region) of a binding event, or the location
of a protein in a localization event.
? Recognition of negations and speculations
This task requires identification of negations of
events (e.g. event X did not occur), and specu-
lation about events (e.g. We claim that event X
should occur).
3 Our approach
We used the OpenDMAP system developed at the
University of Colorado School of Medicine (Hunter
et al, 2008) for our submission to the BioNLP
?09 Shared Task on Event Extraction. OpenDMAP
is an ontology-driven, integrated concept analysis
system that supports information extraction from
text through the use of patterns represented in a
classic form of ?semantic grammar,? freely mixing
text literals, semantically typed basal syntactic con-
stituents, and semantically defined classes of enti-
ties. Our approach is to take advantage of the high
50
quality ontologies available in the biomedical do-
main to formally define entities, events, and con-
straints on slots within events and to develop pat-
terns for how concepts can be expressed in text that
take advantage of both semantic and linguistic char-
acteristics of the text. We manually built patterns for
each event type by examining the training data and
by using native speaker intuitions about likely ways
of expressing relationships, similar to the technique
described in (Cohen et al, 2004). The patterns char-
acterize the linguistic expression of that event and
identify the arguments (participants) of the events
according to (a) occurrence in a relevant linguistic
context and (b) satisfaction of appropriate semantic
constraints, as defined by our ontology. Our solution
results in very high precision information extraction,
although the current rule set has limited recall.
3.1 The reference ontology
The central organizing structure of an OpenDMAP
project is an ontology. We built the ontology
for this project by combining elements of several
community-consensus ontologies?the Gene Ontol-
ogy (GO), Cell Type Ontology (CTO), BRENDA
Tissue Ontology (BTO), Foundational Model of
Anatomy (FMA), Cell Cycle Ontology (CCO), and
Sequence Ontology (SO)?and a small number of
additional concepts to represent task-specific aspects
of the system, such as event trigger words. Combin-
ing the ontologies was done with the Prompt plug-in
for Prote?ge?.
The ontology included concepts representing each
event type. These were represented as frames, with
slots for the various things that needed to be re-
turned by the system?the trigger word and the var-
ious slot fillers. All slot fillers were constrained to
be concepts in some community-consensus ontol-
ogy. The core event arguments were constrained in
the ontology to be of type protein from the Sequence
Ontology (except in the case of regulation events,
where biological events themselves could satisfy the
THEME role), while the type of the other event argu-
ments varied. For instance, the ATLOC argument
of a gene expression event was constrained to be
one of tissue (from BTO), cell type (from CTO), or
cellular component (from GO-Cellular Component),
while the BINDING argument of a binding event was
constrained to be one of binding site, DNA, domain,
or chromosome (all from the SO and all tagged by
LingPipe). Table 1 lists the various types.
3.2 Named entity recognition
For proteins, we used the gold standard annota-
tions provided by the organizers. For other seman-
tic classes, we constructed a compound named en-
tity recognition system which consists of a LingPipe
GENIA tagging module (LingPipe, (Alias-i, 2008)),
and several dictionary look-up modules. The dictio-
nary lookup was done using a component from the
UIMA (IBM, 2009; Ferrucci and Lally, 2004) sand-
box called the ConceptMapper.
We loaded the ConceptMapper with dictionar-
ies derived from several ontologies, including the
Gene Ontology Cellular Component branch, Cell
Type Ontology, BRENDA Tissue Ontology, and
the Sequence Ontology. The dictionaries contained
the names and name variants for each concept in
each ontology, and matches in the input documents
were annotated with the relevant concept ID for the
match. The only modifications that we made to
these community-consensus ontologies were to re-
move the single concept cell from the Cell Type On-
tology and to add the synonym nuclear to the Gene
Ontology Cell Component concept nucleus.
The protein annotations were used to constrain the
text entities that could satisfy the THEME role in the
events of interest. The other named entities were
added for the identification of non-core event partic-
ipants for Task 2.
3.3 Pattern development strategies
3.3.1 Corpus analysis
Using a tool that we developed for visualizing the
training data (described below), a subset of the gold-
standard annotations were grouped by event type
and by trigger word type (nominalization, passive
verb, active verb, or multiword phrase). This orga-
nization helped to suggest the argument structures of
the event predicates and also highlighted the varia-
tion within argument structures. It also showed the
nature of more extensive intervening text that would
need to be handled for the patterns to achieve higher
recall.
Based on this corpus analysis, patterns were de-
veloped manually using an iterative process in which
individual patterns or groups of patterns were tested
51
Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model
of Anatomy, other ontologies identified in the text.
Event Type Site AtLoc ToLoc
binding protein domain (SO),
binding site (SO), DNA
(SO), chromosome (SO)
gene expression gene (SO), biological
entity (CCO)
tissue (BTO), cell type
(CTO), cellular compo-
nent (GO)
localization cellular component
(GO)
cellular component
(GO)
phosphorylation amino acid (FMA),
polypeptide region (SO)
protein catabolism cellular component
(GO)
transcription gene (SO), biological
entity (CCO)
on the training data to determine their impact on per-
formance. Pattern writers started with the most fre-
quent trigger words and argument structures.
3.3.2 Trigger words
In the training data, we were provided annotations
of all relevant event types occurring in the training
documents. These annotations included a trigger
word specifying the specific word in the input text
which indicated the occurrence of each event. We
utilized the trigger words in the training set as an-
chors for our linguistic patterns. We built patterns
around the generic concept of, e.g. an expression
trigger word and then varied the actual strings that
were allowed to satisfy that concept. We then ran ex-
periments with our patterns and these varying sets of
trigger words for each event type, discarding those
that degraded system performance when evaluated
with respect to the gold standard annotations.
Most often a trigger word was removed from an
event type trigger list because it was also a trig-
ger word for another event type and therefore re-
duced performance by increasing the false positive
rate. For example, the trigger words ?level? and
?levels? appear in the training data trigger word lists
of gene expression, transcription, and all three regu-
lation event types.
The selection of trigger words was guided by a
frequency analysis of the trigger words provided in
the task training data. In a post-hoc analysis, we find
that a different proportion of the set of trigger words
was finally chosen for each different event type. Be-
tween 10-20% of the top frequency-ranked trigger
words were used for simple event types, with the
exception that phosphorylation trigger words were
chosen from the top 30%. For instance, for gene ex-
pression all of the top 15 most frequent trigger words
were used (corresponding to the top 16%). For com-
plex event types (the regulations) better performance
was achieved by limiting the list to between 5-10%
of the most frequent trigger words.
In addition, variants of frequent trigger words
were included. For instance, the nominalization ?ex-
pression? is the most frequent gene expression trig-
ger word and the verbal inflections ?expressed? and
?express? are also in the top 20%. The verbal inflec-
tion ?expresses? is ranked lower than the top 30%,
but was nonetheless included as a trigger word in the
gene expression patterns.
3.3.3 Patterns
As in our previous publications on OpenDMAP,
we refer to our semantic rules as patterns. For
this task, each pattern has at a minimum an event
argument THEME and an event-specific trigger
word. For example, {phosphorylation} :=
52
[phosphorylation nominalization][Theme],
where [phosphorylization nominalization]
represents a trigger word. Both elements are defined
semantically. Event THEMEs are constrained by
restrictions placed on them in the ontology, as
described above.
The methodology for creating complex event pat-
terns such as regulation was the same as for sim-
ple events, with the exception that the THEMEs
were defined in the ontology to also include bio-
logical processes. Iterative pattern writing and test-
ing was a little more arduous because these pat-
terns relied on the success of the simple event pat-
terns, and hence more in-depth analysis was re-
quired to perform performance-increasing pattern
adjustments. For further details on the pattern lan-
guage, the reader is referred to (Hunter et al, 2008).
3.3.4 Nominalizations
Nominalizations were very frequent in the train-
ing data; for seven out of nine event types, the most
common trigger word was a nominalization. In writ-
ing our grammars, we focused on these nominaliza-
tions. To write grammars for nominalizations, we
capitalized on some of the insights from (Cohen et
al., 2008). Non-ellided (or otherwise absent) argu-
ments of nominalizations can occur in three basic
positions:
? Within the noun phrase, after the nominaliza-
tion, typically in a prepositional phrase
? Within the noun phrase, immediately preceding
the nominalization
? External to the noun phrase
The first of these is the most straightforward to
handle in a rule-based approach. This is particu-
larly true in the case of a task definition like that
of BioNLP ?09, which focused on themes, since an
examination of the training data showed that when
themes were post-nominal in a prepositional phrase,
then that phrase was most commonly headed by of.
The second of these is somewhat more challeng-
ing. This is because both agents and themes can
occur immediately before the nominalization, e.g.
phenobarbital induction (induction by phenobarbi-
tal) and trkA expression (expression of trkA). To de-
cide how to handle pre-nominal arguments, we made
use of the data on semantic roles and syntactic posi-
tion found in (Cohen et al, 2008). That study found
that themes outnumbered agents in the prenominal
position by a ratio of 2.5 to 1. Based on this obser-
vation, we assigned pre-nominal arguments to the
theme role.
Noun-phrase-external arguments are the most
challenging, both for automatic processing and for
human interpreters; one of the major problems is
to differentiate between situations where they are
present but outside of the noun phrase, and situations
where they are entirely absent. Since the current im-
plementation of OpenDMAP does not have robust
access to syntactic structure, our only recourse for
handling these arguments was through wildcards,
and since they mostly decreased precision without a
corresponding increase in recall, we did not attempt
to capture them.
3.3.5 Negation and speculation
Corpus analysis of the training set revealed two
broad categories each for negation and speculation
modifications, all of which can be described in terms
of the scope of modification.
Negation
Broadly speaking, an event itself can be negated
or some aspect of an event can be negated. In other
words, the scope of a negation modification can be
over the existence of an event (first example below),
or over an argument of an existing event (second ex-
ample).
? This failure to degrade IkappaBalpha ...
(PMID 10087185)
? AP-1 but not NF-IL-6 DNA binding activity ...
(PMID 10233875)
Patterns were written to handle both types of
negation. The negation phrases ?but not? and ?but
neither? were appended to event patterns to catch
those events that were negated as a result of a
negated argument. For event negation, a more ex-
tensive list of trigger words was used that included
verbal phrases such as ?failure to? and ?absence of.?
The search for negated events was conducted in
two passes. Events for which negation cues fall out-
side the span of text that stretches from argument to
53
event trigger word were handled concurrently with
the search for events. A second search was con-
ducted on extracted events for negation cues that fell
within the argument to event trigger word span, such
as
. . . IL-2 does not induce I kappa B alpha degrada-
tion (PMID 10092783)
This second pass allowed us to capture one addi-
tional negation (6 rather than 5) on the test data.
Speculation
The two types of speculation in the training data
can be described by the distinction between ?de re?
and ?de dicto? assertions. The ?de dicto? assertions
of speculation in the training data are modifications
that call into question the degree of known truth of
an event, as in
. . . CTLA-4 ligation did not appear to affect the
CD28 - mediated stabilization (PMID 10029815)
The ?de re? speculation address the potential ex-
istence of an event rather that its degree of truth. In
these cases, the event is often being introduced in
text by a statement of intention to study the event, as
in
. . . we investigated CTCF expression
. . . [10037138]
To address these distinct types of speculation, two
sets of trigger words were developed. One set con-
sisted largely of verbs denoting research activities,
e.g. research, study, examine investigate, etc. The
other set consisted of verbs and adverbs that denote
uncertainty, and included trigger words such as sug-
gests, unknown, and seems.
3.4 Handling of coordination
Coordination was handled using the OpenNLP con-
stituent parser along with the UIMA wrappers that
they provide via their code repository. We chose
OpenNLP because it is easy to train a model, it in-
tegrates easily into a UIMA pipeline, and because
of competitive parsing results as reported by Buyko
(Buyko et al, 2006). The parser was trained using
500 abstracts from the beta version of the GENIA
treebank and 10 full-text articles from the CRAFT
corpus (Verspoor et al, In press). From the con-
stituent parse we extracted coordination structures
into a simplified data structure that captures each
conjunction along with its conjuncts. These were
provided to downstream components. The coordi-
nation component achieves an F-score of 74.6% at
the token level and an F-score of 57.5% at the con-
junct level when evaluated against GENIA. For both
measures the recall was higher than the precision by
4% and 8%, respectively.
We utilized the coordination analysis to identify
events in which the THEME argument was expressed
as a conjoined noun phrase. These were assumed to
have a distributed reading and were post-processed
to create an individual event involving each con-
junct, and further filtered to only include given (A1)
protein references. So, for instance, analysis of the
sentence in the example below should result in the
detection of three separate gene expression events,
involving the proteins HLA-DR, CD86, and CD40,
respectively.
NAC was shown to down-regulate the
production of cytokines by DC as well
as their surface expression of HLA-
DR, CD86 (B7-2), and CD40 molecules
. . . (PMID 10072497)
3.5 Software infrastructure
We took advantage of our existing infrastructure
based on UIMA (The Unstructured Information
Management Architecture) (IBM, 2009; Ferrucci
and Lally, 2004) to support text processing and data
analysis.
3.5.1 Development tools
We developed a visualization tool to enable the
linguistic pattern writers to better analyze the train-
ing data. This tool shows the source text one sen-
tence at a time with the annotated words highlighted.
A list following each sentence shows details of the
annotations.
3.6 Errors in the training data
In some cases, there were discrepancies between the
training data and the official problem definitions.
This was a source of problems in the pattern devel-
opment phase. For example, phosphorylation events
are defined in the task definition as having only a
THEME and a SITE. However, there were instances
in the training data that included both a THEME and
a CAUSE argument. When those events were identi-
fied by our system and the CAUSE was labelled, they
54
were rejected during a syntactic error check by the
test server.
4 Results
4.1 Official Results
We are listed as Team 13. Table 2 shows our re-
sults on the official metrics. Our precision was the
highest achieved by any group for Task 1 and Task
2, at 71.81 for Task 1 and 70.97 for task 2. Our re-
calls were much lower and adversely impacted our
F-measure; ranked by F-measure, we ranked 19th
out of 24 groups.
We noted that our results for the exact match met-
ric and for the approximate match metric were very
close, suggesting that our techniques for named en-
tity recognition and for recognizing trigger words
are doing a good job of capturing the appropriate
spans.
4.2 Other analysis: Bug fixes and coordination
handling
In addition to our official results, we also report in
Table 3 (see last page) the results of a run in which
we fixed a number of bugs. This represents our cur-
rent best estimate of our performance. The precision
drops from 71.81 for Task 1 to 67.19, and from 70.97
for Task 2 to 65.74, but these precisions are still
well above the second-highest precisions of 62.21
for Task 1 and 56.87 for Task 2. As the table shows,
we had corresponding small increases in our recall
to 17.38 and in our F-measure to 27.62 for Task 1,
and in our recall to 17.07 and F-measure to 27.10 for
Task 2.
We evaluated the effects of coordination handling
by doing separate runs with and without this ele-
ment of the processing pipeline. Compared to our
unofficial results, which had an overall F-measure
for Task 1 of 27.62 and for Task 2 of 27.10, a ver-
sion of the system without handling of coordination
had an overall F-measure for Task 1 of 24.72 and for
Task 2 of 24.21.
4.3 Error Analysis
4.3.1 False negatives
To better understand the causes of our low recall,
we performed a detailed error analysis of false neg-
atives using the devtest data. (Note that this section
includes a very small number of examples from the
devtest data.) We found five major causes of false
negatives:
? Intervening material between trigger words and
arguments
? Coordination that was not handled by our coor-
dination component
? Low coverage of trigger words
? Anaphora and coreference
? Appositive gene names and symbols
Intervening material For reasons that we detail
in the Discussion section, we avoided the use of
wildcards. This, and the lack of syntactic analy-
sis in the version of the system that we used (note
that syntactic analyses can be incorporated into an
OpenDMAP workflow), meant that if there was text
intervening between a trigger word and an argument,
e.g. in to efficiently [express] in developing thymo-
cytes a mutant form of the [NF-kappa B inhibitor]
(PMID 10092801), where the bracketed text is the
trigger word and the argument, our pattern would
not match.
Unhandled coordination Our coordination system
only handled coordinated protein names. Thus, in
cases where other important elements of the utter-
ance, such as the trigger word transcription in tran-
scription and subsequent synthesis and secretion
of galectin-3 (PMID 8623933) were in coordinated
structures, we missed the relevant event arguments.
Low coverage of trigger words As we discuss in
the Methods section, we did not attempt to cover
all trigger words, in part because some less-frequent
trigger words were involved in multiple event types,
in part because some of them were extremely low-
frequency and we did not want to overfit to the train-
ing data, and in part due to the time constraints of the
shared task.
Anaphora and coreference Recognition of some
events in the data would require the ability to do
anaphora and coreference resolution. For example,
in Although 2 early lytic transcripts, [BZLF1] and
[BHRF1], were also detected in 13 and 10 cases,
respectively, the lack of ZEBRA staining in any case
indicates that these lytic transcripts are most likely
55
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (18) 18 (18) 10.34 100.00 18.75 9.77 94.44 17.71
Binding 347 (44) 110 (44) 12.68 40.00 19.26 12.32 39.09 18.74
Gene expression 722 (263) 306 (263) 36.43 85.95 51.17 36.43 85.95 51.17
Transcription 137 (18) 20 (18) 13.14 90.00 22.93 13.14 90.00 22.93
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (30) 30 (30) 22.22 100.00 36.36 20.14 93.33 33.14
EVENT TOTAL 1529 (377) 490 (377) 24.66 76.94 37.35 24.30 76.12 36.84
Regulation 291 (9) 19 (9) 3.09 47.37 5.81 3.08 47.37 5.79
Positive regulation 983 (32) 65 (32) 3.26 49.23 6.11 3.24 49.23 6.08
Negative regulation 379 (10) 22 (10) 2.64 45.45 4.99 2.37 40.91 4.49
REGULATION TOTAL 1653 (51) 106 (51) 3.09 48.11 5.80 3.02 47.17 5.67
Negation 227 (4) 76 (4) 1.76 5.26 2.64
Speculation 208 (14) 105 (14) 6.73 13.33 8.95
MODIFICATION TOTAL 435 (18) 181 (18) 4.14 9.94 5.84
ALL TOTAL 3182 (428) 596 (428) 13.45 71.81 22.66 13.25 70.97 22.33
Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate span
matching/approximate recursive matching table. GS = gold standard (true positives) (given for Tasks 1/3 only), answer
= all responses (true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure. All results are as
calculated by the official scoring application.
[expressed] by rare cells in the biopsies entering
lytic cycle (PMID 8903467), where the bracketed
text is the arguments and the trigger word, the syn-
tactic object of the verb is the anaphoric noun phrase
these lytic transcripts, so even with the addition of
a syntactic component to our system, we still would
not have recognized the appropriate arguments with-
out the ability to do anaphora resolution.
Appositives The annotation guidelines for proteins
apparently specified that when a gene name was
present in an appositive with its symbol, the symbol
was selected as the gold-standard argument. For this
reason, in examples like [expression] of Fas ligand
[FasL] (PMID 10092076), where the bracketed text
is the trigger word and the argument, the gene name
constituted intervening material from the perspec-
tive of our patterns, which therefore did not match.
We return to a discussion of recall and its implica-
tions for systems like ours in the Discussion section.
4.3.2 False positives
Although our overall rate of false positives was
low, we sampled 45 false positive events distributed
across the nine event types and reviewed them with
a biologist.
We noted two main causes of error. The most
common was that we misidentified a slot filler or
were missing a slot filler completely for an actual
event. The other main reason for false positives was
when we erroneously identified a (non)event. For
example, in coexpression of NF-kappa B/Rel and
Sp1 transcription factors (PMID 7479915), we mis-
takenly identified Sp1 transcription as an event.
5 Discussion
Our results demonstrate that it is possible to achieve
state-of-the art precision over a broad range of tasks
and event types using our approach of manually
constructed, ontologically typed rules?our preci-
sion of 71.81 on Task 1 was ten points higher than
the second-highest precision (62.21), and our preci-
sion of 70.97 on Task 2 was 14 points higher than
the second-highest precision (56.87). It remains the
case that our recall was low enough to drop our F-
measure considerably. Will it be the case that a sys-
tem like ours can scale to practical performance lev-
els nonetheless? Four factors suggest that it can.
The first is that there is considerable redundancy
in the data; although we have not quantified it for
this data set, we note that the same event is often
56
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (33) 41 (33) 18.97 80.49 30.70 16.67 69.05 26.85
Binding 347 (62) 152 (62) 17.87 40.79 24.85 17.48 40.13 24.35
Gene expression 722 (290) 344 (290) 40.17 84.30 54.41 40.17 84.30 54.41
Transcription 137 (28) 31 (28) 20.44 90.32 33.33 20.44 90.32 33.33
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (47) 48 (47) 34.81 97.92 51.37 32.37 84.91 46.88
EVENT TOTAL 1529 (464) 622 (464) 30.35 74.60 43.14 29.77 72.77 42.26
Regulation 291 (11) 31 (11) 3.78 35.48 6.83 3.77 35.48 6.81
Positive regulation 983 (60) 129 (60) 6.10 46.51 10.79 6.08 46.51 10.75
Negative regulation 379 (18) 41 (18) 4.75 43.90 8.57 4.49 41.46 8.10
REGULATION TOTAL 1653 (89) 201 (89) 5.38 44.28 9.60 5.31 43.78 9.47
Negation 227 (6) 129 (6) 2.64 4.65 3.37
Speculation 208 (25) 165 (25) 12.02 15.15 13.40
MODIFICATION TOTAL 435 (31) 294 (31) 7.13 10.54 8.50
ALL TOTAL 3182 (553) 823 (553) 17.38 67.19 27.62 17.07 65.74 27.10
Table 3: Updated results on test data for Tasks 1-3, with important bug fixes in the code base. See key above.
mentioned repeatedly, but for knowledge base build-
ing and other uses of the extracted information, it is
only strictly necessary to recognize an event once
(although multiple recognition of the same assertion
may increase our confidence in its correctness).
The second is that there is often redundancy
across the literature; the best-supported assertions
will be reported as initial findings and then repeated
as background information.
The third is that these recall results reflect an ap-
proach that made no use of syntactic analysis be-
yond handling coordination. There is often text
present in the input that cannot be disregarded with-
out either using wildcards, which generally de-
creased precision in our experiments and which
we generally eschewed, or making use of syntac-
tic information to isolate phrasal heads. Syntactic
analysis, particularly when combined with analysis
of predicate-argument structure, has recently been
shown to be an effective tool in biomedical infor-
mation extraction (Miyao et al, 2009). There is
broad need for this?for example, of the thirty lo-
calization events in the training data whose trigger
word was translocation, a full eighteen had inter-
vening textual material that made it impossible for
simple patterns like translocationof [Theme] or
[ToLoc]translocation to match.
Finally, our recall numbers reflect a very short de-
velopment cycle, with as few as four patterns writ-
ten for many event types. A less time-constrained
pattern-writing effort would almost certainly result
in increased recall.
Acknowledgments
We gratefully acknowledge Mike Bada?s help in
loading the Sequence Ontology into Prote?ge?.
This work was supported by NIH
grants R01LM009254, R01GM083649, and
R01LM008111 to Lawrence Hunter and
T15LM009451 to Philip Ogren.
References
Alias-i. 2008. LingPipe 3.1.2.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically mapping an NLP
core engine to the biology domain. In Proceedings
of the ISMB 2006 joint BioLINK/Bio-Ontologies meet-
ing.
K. B. Cohen, L. Tanabe, S. Kinoshita, and L. Hunter.
2004. A resource for constructing customized test
suites for molecular biology entity identification sys-
tems. BioLINK 2004, pages 1?8.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9).
57
D. Ferrucci and A. Lally. 2004. Building an example
application with the unstructured information manage-
ment architecture. IBM Systems Journal, 43(3):455?
475, July.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
IBM. 2009. UIMA Java framework. http://uima-
framework.sourceforge.net/.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein-protein
interaction extraction. Bioinformatics, 25(3):394?400.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. In press. The textual characteristics of tradi-
tional and Open Access scientific journals are similar.
BMC Bioinformatics.
58
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886?897,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
What Can We Get From 1000 Tokens?
A Case Study of Multilingual POS Tagging For Resource-Poor Languages
Long Duong,
12
Trevor Cohn,
1
Karin Verspoor,
1
Steven Bird,
1
and Paul Cook
1
1
Department of Computing and Information Systems,
The University of Melbourne
2
National ICT Australia, Victoria Research Laboratory
lduong@student.unimelb.edu.au
{t.cohn, karin.verspoor, sbird, paulcook}@unimelb.edu.au
Abstract
In this paper we address the problem
of multilingual part-of-speech tagging for
resource-poor languages. We use par-
allel data to transfer part-of-speech in-
formation from resource-rich to resource-
poor languages. Additionally, we use a
small amount of annotated data to learn to
?correct? errors from projected approach
such as tagset mismatch between lan-
guages, achieving state-of-the-art perfor-
mance (91.3%) across 8 languages. Our
approach is based on modest data require-
ments, and uses minimum divergence clas-
sification. For situations where no uni-
versal tagset mapping is available, we
propose an alternate method, resulting
in state-of-the-art 85.6% accuracy on the
resource-poor language Malagasy.
1 Introduction
Part-of-speech (POS) tagging is a crucial task for
natural language processing (NLP) tasks, provid-
ing basic information about syntax. Supervised
POS tagging has achieved great success, reach-
ing as high as 95% accuracy for many languages
(Petrov et al., 2012). However, supervised tech-
niques need manually annotated data, and this
is either lacking or limited in most resource-
poor languages. Fully unsupervised POS tagging
is not yet useful in practice due to low accu-
racy (Christodoulopoulos et al., 2010). In this pa-
per, we propose a semi-supervised method to nar-
row the gap between supervised and unsupervised
approaches. We demonstrate that even a small
amount of supervised data leads to substantial im-
provement.
Our method is motivated by the availability of
parallel data. Thanks to the development of mul-
tilingual documents from government projects,
book translations, multilingual websites, and so
forth, parallel data between resource-rich and
resource-poor languages is relatively easy to ac-
quire. This parallel data provides the bridge that
permits us to transfer POS information from a
resource-rich to a resource-poor language.
Systems that make use of cross-lingual tag
projection typically face several issues, includ-
ing mismatches between the tagsets used for the
languages, artifacts from noisy alignments and
cross-lingual syntactic divergence. Our approach
compensates for these issues by training on a
small amount of annotated data on the target side,
demonstrating that only 1k tokens of annotated
data is sufficient to improve performance.
We first tag the resource-rich language using a
supervised POS tagger. We then project POS tags
from the resource-rich language to the resource-
poor language using parallel word alignments.
The projected labels are noisy, and so we use
various heuristics to select only ?good? training
examples. We train the model in two stages.
First, we build a maximum entropy classifier T
on the (noisy) projected data. Next, we train
a supervised classifier P on a small amount of
annotated data (1,000 tokens) in the target lan-
guage, using a minimum divergence technique
to incorporate the first model, T . Compared
with the state of the art (T?ackstr?om et al., 2013),
we make more-realistic assumptions (e.g. relying
on a tiny amount of annotated data rather than
a huge crowd-sourced dictionary) and use less
parallel data, yet achieve a better overall result.
We achieved 91.3% average accuracy over 8 lan-
guages, exceeding T?ackstr?om et al. (2013)?s result
of 88.8%.
The test data we employ makes use of map-
pings from language-specific POS tag inventories
to a universal tagset (Petrov et al., 2012). How-
ever, such a mapping might not be available for
resource-poor languages. Therefore, we also pro-
886
pose a variant of our method which removes the
need for identical tagsets between the projection
model T and the correction model P , based on
a two-output maximum entropy model over tag
pairs. Evaluating on the resource-poor language
Malagasy, we achieved 85.6% accuracy, exceed-
ing the state-of-the-art of 81.2% (Garrette et al.,
2013).
2 Background and Related Work
There is a wealth of prior work on multilingual
POS tagging. The simplest approach takes advan-
tage of the typological similarities that exist be-
tween languages pairs such as Czech and Russian,
or Serbian and Croatian. They build the tagger
? or estimate part of the tagger ? on one lan-
guage and apply it to the other language (Reddy
and Sharoff, 2011, Hana et al., 2004).
Yarowsky and Ngai (2001) pioneered the use of
parallel data for projecting tag information from
a resource-rich language to a resource-poor lan-
guage. Duong et al. (2013b) used a similar method
on using sentence alignment scores to rank the
goodness of sentences. They trained a seed model
from a small part of the data, then applied this
model to the rest of the data using self-training
with revision.
Das and Petrov (2011) also used parallel data
but additionally exploited graph-based label prop-
agation to expand the coverage of labelled tokens.
Each node in the graph represents a trigram in the
target language. Each edge connects two nodes
which have similar context. Originally, only some
nodes received a label from direct label projection,
and then labels were propagated to the rest of the
graph. They only extracted the dictionary from
the graph because the labels of nodes are noisy.
They used the dictionary as the constraints for a
feature-based HMM tagger (Berg-Kirkpatrick et
al., 2010). Both Duong et al. (2013b) and Das and
Petrov (2011) achieved 83.4% accuracy on the test
set of 8 European languages.
Goldberg et al. (2008) pointed out that, with the
presence of a dictionary, even an incomplete one,
a modest POS tagger can be built using simple
methods such as expectation maximization. This
is because most of the time, words have a very
limited number of possible tags, thus a dictionary
that specifies the allowable tags for a word helps
to restrict the search space. With a gold-standard
dictionary, Das and Petrov (2011) achieved an ac-
curacy of approximately 94% on the same 8 lan-
guages. The effectiveness of a gold-standard dic-
tionary is undeniable, however it is costly to build
one, especially for resource-poor languages. Li et
al. (2012) used the dictionary from Wiktionary,
1
a
crowd-sourced dictionary. They scored 84.8% ac-
curacy on the same 8 languages. Currently, Wik-
tionary covers over 170 languages, but the cov-
erage varies substantially between languages and,
unsurprisingly, it is poor for resource-poor lan-
guages. Therefore, relying on Wiktionary is not
effective for building POS taggers for resource-
poor languages.
T?ackstr?om et al. (2013) combined both token
information (from direct projected data) and type
constraints (from Wiktionary?s dictionary) to form
the state-of-the-art multilingual tagger. They built
a tag lattice and used these token and type con-
straints to prune it. The remaining paths are the
training data for a CRF tagger. They achieved
88.8% accuracy on the same 8 languages.
Table 1 summarises the performance of the
above models across all 8 languages. Note that
these methods vary in their reliance on external
resources. Duong et al. (2013b) use the least, i.e.
only the Europarl Corpus (Koehn, 2005). Das and
Petrov (2011) additionally use the United Nation
Parallel Corpus. Li et al. (2012) didn?t use any par-
allel text but used Wiktionary instead. T?ackstr?om
et al. (2013) exploited more parallel data than Das
and Petrov (2011) and also used a dictionary
from Li et al. (2012).
Another approach for resource-poor languages
is based on the availability of a small amount
of annotated data. Garrette et al. (2013) built a
POS tagger for Kinyarwanda and Malagasy. They
didn?t use parallel data but instead exploited four
hours of manual annotation to build?4,000 tokens
or ?3,000 word-types of annotated data. These
tokens or word-types were used to build a tag dic-
tionary. They employed label propagation for ex-
panding the coverage of this dictionary in a sim-
ilar vein to Das and Petrov (2011), but they also
used an external dictionary. They built training
examples using the combined dictionary and then
trained the tagger on this data. They achieved
81.9% and 81.2% accuracy for Kinyarwanda and
Malagasy respectively. Note that their usage of an
external dictionary compromises their claim of us-
ing only 4 hours of annotation.
1
http://www.wiktionary.org/
887
da nl de el it pt es sv Average
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages
? Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish
(sv) ? evaluated on CoNLL data (Buchholz and Marsi, 2006).
The method we propose in this paper is similar
in only using a small amount of annotation. How-
ever, we directly use the annotated data to train
the model rather than using a dictionary. We argue
that with a proper ?guide?, we can take advantage
of very limited annotated data.
2.1 Annotated data
Our annotated data mainly comes from CoNLL
shared tasks on dependency parsing (Buchholz
and Marsi, 2006). The language specific tagsets
are mapped into the universal tagset. We will
use this annotated data mainly for evaluation. Ta-
ble 2 shows the size of annotated data for each
language. The 8 languages we are considering
in this experiment are not actually resource-poor
languages. However, running on these 8 lan-
guages makes our system comparable with pre-
viously proposed methods. Nevertheless, we try
to use as few resources as possible, in order to
simulate the situation for resource-poor languages.
Later in Section 6 we adapt the approach for Mala-
gasy, a truly resource-poor language.
2.2 Universal tagset
We employ the universal tagset from (Petrov et
al., 2012) for our experiment. It consists of 12
common tags: NOUN, VERB, ADJ (adjective),
ADV (adverb), PRON (pronoun), DET (deter-
miner and article), ADP (preposition and post-
position), CONJ (conjunctions), NUM (numeri-
cal), PRT (particle), PUNC (punctuation) and X
(all other categories including foreign words and
abbreviations). Petrov et al. (2012) provide the
mapping from each language-specific tagset to the
universal tagset.
The idea of using the universal tagset is of great
use in multilingual applications, enabling compar-
ison across languages. However, the mapping is
not always straightforward. Table 2 shows the size
of the annotated data for each language, the num-
ber of tags presented in the data, and the list of
tags that are not matched. We can see that only 8
tags are presented in the annotated data for Dan-
ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are
missing.
2
Thus, a classifier using all 12 tags will
be heavily penalized in the evaluation.
Li et al. (2012) considered this problem and
tried to manually modify the Danish mappings.
Moreover, PRT is not really a universal tag since
it only appears in 3 out of the 8 languages. Plank
et al. (2014) pointed out that PRT often gets con-
fused with ADP even in English. We will later
show that the mapping problem causes substantial
degradation in the performance of a POS tagger
exploiting parallel data. The method we present
here is more target-language oriented: our model
is trained on the target language, in this way, only
relevant information from the source language is
retained. Thus, we automatically correct the map-
ping, and other incompatibilities arising from in-
correct alignments and syntactic divergence be-
tween the source and target languages.
Lang Size(k) # Tags Not Matched
da 94 8 DET, PRT, PUNC, NUM
nl 203 11 PRT
de 712 12
el 70 12
it 76 11 PRT
pt 207 11 PRT
es 89 11 PRT
sv 191 11 DET
AVG 205
Table 2: The size of annotated data from
CoNLL (Buchholz and Marsi, 2006), and the
number of tags included and missing for 8 lan-
guages.
2
Many of these are mistakes in the mapping, however,
they are indicative of the kinds of issues expected in low-
resource languages.
888
3 Directly Projected Model (DPM)
In this section we describe a maximum entropy
tagger that only uses information from directly
projected data.
3.1 Parallel data
We first collect Europarl data having English as
the source language, an average of 1.85 million
parallel sentences for each of the 8 language pairs.
In terms of parallel data, we use far less data com-
pared with other recent work. Das and Petrov
(2011) used Europarl and the ODS United Na-
tion dataset, while T?ackstr?om et al. (2013) addi-
tionally used parallel data crawled from the web.
The amount of parallel data is crucial for align-
ment quality. Since DPM uses alignments to trans-
fer tags from source to target language, the per-
formance of DPM (and other models that exploit
projection) largely depends on the quantity of par-
allel data. The ?No LP? model of Das and Petrov
(2011), which only uses directly projected labels
(without label propagation), scored 81.3% for 8
languages. However, using the same model but
with more parallel data, T?ackstr?om et al. (2013)
scored 84.9% on the same test set.
3.2 Label projection
We use the standard alignment tool Giza++ (Och
and Ney, 2003) to word align the parallel data. We
employ the Stanford POS tagger (Toutanova et al.,
2003) to tag the English side of the parallel data
and then project the label to the target side. It has
been confirmed in many studies (T?ackstr?om et al.,
2013, Das and Petrov, 2011, Toutanova and John-
son, 2008) that directly projected labels are noisy.
Thus we need a method to reduce the noise. We
employ the strategy of Yarowsky and Ngai (2001)
of ranking sentences using a their alignment scores
from IBM model 3.
Firstly, we want to know how noisy the pro-
jected data is. Thus, we use the test data to build
a simple supervised POS tagger using the TnT
tagger (Brants, 2000) which employs a second-
order Hidden Markov Model (HMM). We tag the
projected data and compare the label from direct
projection and from the TnT tagger. The labels
from the TnT Tagger are considered as pseudo-
gold labels. Column ?Without Mapping? from Ta-
ble 3 shows the average accuracy for the first n-
sentences (n = 60k, 100k, 200k, 500k) for 8 lan-
guages according to the ranking. Column ?Cov-
erage? shows the percentages of projected label
(the other tokens are Null aligned). We can see
that when we select more data, both coverage and
accuracy fall. In other words, using the sentence
alignment score, we can rank sentences with high
coverage and accuracy first. However, even after
ranking, the accuracy of projected labels is less
than 80% demonstrating how noisy the projected
labels are.
Table 3 (column ?With Mapping?) additionally
shows the accuracy using simple tagset mapping,
i.e. mapping each tag to the tag it is assigned most
frequently in the test data. For example DET, PRT,
PUNC, NUM, missing from Danish gold data, will
be matched to PRON, X, X, ADJ respectively. This
simple matching yields a ? 4% (absolute) im-
provement in average accuracy. This illustrates the
importance of handling tagset mapping carefully.
3.3 The model
In this section, we introduce a maximum entropy
tagger exploiting the projected data. We select the
first 200k sentences from Table 3 for this experi-
ment. This number represents a trade-off between
size and accuracy. More sentences provide more
information but at the cost of noisier data. Duong
et al. (2013b) also used sentence alignment scores
to rank sentences. Their model stabilizes after us-
ing 200k sentences. We conclude that 200k sen-
tences is enough and capture most information
from the parallel data.
Features Descriptions
W@-1 Previous word
W@+1 Next word
W@0 Current word
CAP First character is capitalized
NUMBER Is number
PUNCT Is punctuation
SUFFIX@k Suffix up to length 3 (k <= 3)
WC Word class
Table 4: Feature template for a maximum entropy
tagger
We ignore tokens that don?t have labels, which
arise from null alignments and constitute approxi-
mately 14% of the data. The remaining data (?1.4
million tokens) are used to train a maximum en-
tropy (MaxEnt) model. MaxEnt is one of the
simplest forms of probabilistic classifier, and is
appropriate in this setting due to the incomplete
889
Data Size (k) Coverage (%) Without Mapping With Mapping
60 91.5 79.9 84.2
100 89.1 79.4 83.6
200 86.1 79.1 82.9
500 82.4 78.0 81.5
Table 3: The coverage, and POS tagging accuracy with and without tagset mapping of directly projected
labels, averaged over 8 languages for different data sizes
Model da nl de el it pt es sv Avg
All features 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
- Word Class 64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9
- Suffix 64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6
- Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6
- Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8
Table 5: The accuracy of Directed Project Model (DPM) with different feature sets, removing one feature
set at a time
sequence data. While sequence models such as
HMMs or CRFs can provide more accurate mod-
els of label sequences, they impose a more strin-
gent training requirement.
3
We also experimented
with a first-order linear chain CRF trained on con-
tiguous sub-sequences but observed ? 4% (abso-
lute) drop in performance.
The maximum entropy classifier estimates the
probability of tag t given a word w as
P (t|w) =
1
Z(w)
exp
D
?
j=1
?
j
f
j
(w, t) ,
where Z(w) =
?
t
exp
?
D
j=1
?
j
f
j
(w, t) is the
normalization factor to ensure the probabilities
P (t|w) sum to one. Here f
j
is a feature function
and ?
j
is the weight for this feature, learned as
part of training. We use Maximum A Posteriori
(MAP) estimation to maximize the log likelihood
of the training data, D = {w
i
, t
i
}
N
i=1
, subject to a
zero-mean Gaussian regularisation term,
L = logP (?)
N
?
i=1
P (t
(i)
|w
(i)
)
= ?
D
?
j=1
?
2
j
2?
2
+
N
?
i=1
D
?
j=1
?
j
f
j
(w
i
, t
i
)? logZ(w
i
)
where the regularisation term limits over-fitting,
an important concern when using large feature
3
T?ackstr?om et al. (2013) train a CRF on incomplete data,
using a tag dictionary heuristic to define a ?gold standard?
lattice over label sequences.
sets. For our experiments we set ?
2
= 1. We use
L-BFGS which performs gradient ascent to maxi-
mize L. Table 4 shows the features we considered
for building the DPM. We use mkcls, an unsu-
pervised method for word class induction which is
widely used in machine translation (Och, 1999).
We run mkcls to obtain 100 word classes, using
only the target language side of the parallel data.
Table 5 shows the accuracy of the DPM evalu-
ated on 8 languages (?All features model?). DPM
performs poorly on Danish, probably because of
the tagset mapping issue discussed above. The
DPM result of 80.2% accuracy is encouraging,
particularly because the model had no explicit su-
pervision.
To see what features are meaningful for our
model, we remove features in turn and report
the result. The result in Table 5 disagrees with
T?ackstr?om et al. (2013) on the word class features.
They reported a gain of approximately 3% (ab-
solute) using the word class. However, it seems
to us that these features are not especially mean-
ingful (at least in the present setting). Possible
reasons for the discrepancy are that they train the
word class model on a massive quantity of exter-
nal monolingual data, or their algorithms for word
clustering are better (Uszkoreit and Brants, 2008).
We can see that the most informative features are
Capitalization, Number and Punctuation. This
makes sense because in languages such as Ger-
man, capitalization is a strong indicator of NOUN.
Number and punctuation features ensure that we
classify NUM and PUNCT tags correctly.
890
4 Correction Model
In this section we incorporate the directly pro-
jected model into a second correction model
trained on a small supervised sample of 1,000 an-
notated tokens. Our DPM model is not very accu-
rate; as we have discussed it makes many errors,
due to invalid or inconsistent tag mappings, noisy
alignments, and cross-linguistic syntactic diver-
gence. However, our aim is to see how effectively
we can exploit the strengths of the DPM model
while correcting for its inadequacies using direct
supervision. We select only 1,000 annotated to-
kens to reflect a low resource scenario. A small
supervised training sample is a more realistic form
of supervision than a tag dictionary (noisy or oth-
erwise). Although used in most prior work, a tag
dictionary for a new language requires significant
manual effort to construct. Garrette and Baldridge
(2013) showed that a 1,000 token dataset could be
collected very cheaply, requiring less than 2 hours
of non-expert time.
Our correction model makes use of a mini-
mum divergence (MD) model (Berger et al., 1996),
a variant of the maximum entropy model which
biases the target distribution to be similar to a
static reference distribution. The method has been
used in several language applications including
machine translation (Foster, 2000) and parsing
(Plank and van Noord, 2008, Johnson and Riezler,
2000). These previous approaches have used var-
ious sources of reference distribution, e.g., incor-
porating information from a simpler model (John-
son and Riezler, 2000) or combining in- and out-
of-domain models (Plank and van Noord, 2008).
Plank and van Noord (2008) concluded that this
method for adding prior knowledge only works
with high quality reference distributions, other-
wise performance suffers.
In contrast to these previous approaches, we
consider the specific setting where both the
learned model and the reference model s
o
=
P (t|w) are both maximum entropy models. In this
case we show that the MD setup can be simplified
to a regularization term, namely a Gaussian prior
with a non-zero mean. We model the classification
probability, P
?
(t|w) as the product between a base
model and a maximum entropy classifier,
P
?
(t|w) ? P (t|w) exp
D
?
j=1
?
j
f
j
(w, t)
where here we use the DPM model as base model
P (t|w). Under this setup, where P
?
uses the same
features as P , and both are log-linear models, this
simplifies to
P
?
(t|w) ? exp
?
?
D
?
j=1
?
j
f
j
(w, t) +
D
?
j=1
?
j
f
j
(w, t)
?
?
? exp
D
?
j=1
(?
j
+ ?
j
) f
j
(w, t) (1)
where the constant of proportionality is Z
?
(w) =
?
t
exp
?
D
j=1
(?
j
+ ?
j
) f
j
(w, t). It is clear that
Equation (1) also defines a maximum entropy clas-
sifier, with parameters ?
j
= ?
j
+ ?
j
, and conse-
quently this might seem to be a pointless exercise.
The utility of this approach arises from the prior:
MAP training with a zero mean Gaussian prior
over ? is equivalent to a Gaussian prior over the
aggregate weights, ?
j
? N (?
j
, ?
2
). This prior
enforces parameter sharing between the two mod-
els by penalising parameter divergence from the
underlying DPM model ?. The resulting training
objective is
L
corr
= logP (t|w, ?)?
1
2?
2
D
?
j=1
(?
j
? ?
j
)
2
which can be easily optimised using standard
gradient-based methods, e.g., L-BFGS. The con-
tribution of the regulariser is scaled by the constant
1
2?
2
.
4.1 Regulariser sensitivity
Careful tuning of the regularisation term ?
2
is crit-
ical for the correction model, both to limit over-
fitting on the very small training sample of 1,000
tokens, and to control the extent of the influence
of the DPM model over the correction model.
A larger value of ?
2
lessens the reliance on the
DPM and allows for more flexible modelling of
the training set, while a small value of ?
2
forces
the parameters to be close to the DPM estimates at
the expense of data fit. We expect the best value
to be somewhere between these extremes, and use
line-search to find the optimal value for ?
2
. For
this purpose, we hold out 100 tokens from the
1,000 instance training set, for use as our devel-
opment set for hyper-parameter selection.
From Figure 1, we can see that the model per-
forms poorly on small values of ?
2
. This is under-
standable because the small ?
2
makes the model
891
ll
l
l
l l l l l
l l
0.0
1 0.1 1 10 70 100 100
0
100
00
1e+
05
1e+
06
1e+
07
Variance
80
84
88
Acc
ura
cy (%
) 
l Average Acc
Figure 1: Sensitivity of regularisation parameter
?
2
against the average accuracy measured on 8
languages on the development set
too similar to DPM, which is not very accurate
(80.2%). At the other extreme, if ?
2
is large, the
DPM model is ignored, and the correction model
is equivalent with the supervised model (? 88%
accuracy). We select the value of ?
2
= 70, which
maximizes the accuracy on the development set.
4.2 The model
Using the value of ?
2
= 70, we retrain the model
on the whole 1,000-token training set and evalu-
ate the model on the rest of the annotated data.
Table 6 shows the performance of DPM, Super-
vised model, Correction model and the state-of-
the-art model (T?ackstr?om et al., 2013). The super-
vised model trains a maximum entropy tagger us-
ing the same features as in Table 4 on this 1000 to-
kens. The only difference between the supervised
model and the correction model is that in the cor-
rection model we additionally incorporate DPM as
the prior.
The supervised model performs surprisingly
well confirming that our features are meaning-
ful in distinguishing between tags. This model
achieves high accuracy on Danish compared with
other languages probably because Danish is eas-
ier to learn since it contains only 8 tags. Despite
the fact that the DPM is not very accurate, the cor-
rection model consistently outperforms the super-
vised model on all considered languages, approx-
imately 4.3% (absolute) better on average. This
shows that our method of incorporating DPM to
the model is efficient and robust.
The correction model performs much bet-
ter than the state-of-the-art for 7 languages but
l
l l
l l l
l l
l l l
100 300 500 700 100
0
150
0
200
0
500
0
100
00
150
00
500
00
Data Size
65
75
85
95
Acc
urac
y (%
) 
l Correction ModelSupervised Model
Figure 2: Learning curve for correction model and
supervised model: the x-axis is the size of data
(number of tokens); the y-axis is the average ac-
curacy measured on 8 languages; the dashed line
shows the data condition reported in Table 6
slightly worse for 1 language. On average we
achieve 91.3% accuracy compared with 88.8%
for the state-of-the-art, an error rate reduction of
22.3%. This is despite using fewer resources and
only modest supervision.
5 Analysis
Tagset mismatch In the correction model, we
implicitly resolve the mismatched tagset issue.
DPM might contain tags that don?t appear in the
target language or generally are errors in the map-
ping. However, when incorporating DPM into the
correction model, only the feature weight of tags
that appear in the target language are retained. In
general, because we don?t explicitly do any map-
ping between languages, we might have trouble if
the tagset size of the target language is bigger than
the source language tagset. However, this is not
the case for our experiment because we choose En-
glish as the source-side and English has the full 12
tags.
Learning curve We investigate the impact of
the number of available annotated tokens on the
correction model. Figure 2 shows the learning
curve of the correction model and the supervised
model. We can clearly see the differences be-
tween 2 models when the size of training data is
small. For example, at 100 tokens, the difference
is very large, approximately 18% (absolute), it is
also 6% (absolute) better than DPM. This differ-
ence diminishes as we add more data. This make
sense because when we add more data, the super-
vised model become stronger, while the effective-
892
Model da nl de el it pt es sv Avg
DPM 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0
Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3
DPM (with dict) 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8
Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6
Table 6: The comparison of our Directly Projected Model, Supervised Model, Correction Model and the
state-of-the-art system (T?ackstr?om et al., 2013). The best performance for each language is shown in
bold. The models that are built with a dictionary are provided for reference.
ness of the DPM prior on the correction model is
wearing off. An interesting observation is that the
correction model is always better, even when we
add massive amounts of annotated data. At 50,000
tokens, when the supervised model reaches 96%
accuracy, the correction model is still 0.3% (abso-
lute) better, reaching 96.3%. It means that even
at that high level of confidence, some informa-
tion can still be added from DPM to the correc-
tion model. This improvement probably comes
from the observation that the ambiguity in one
language is explained through the alignment. It
also suggests that this method could improve the
performance of a supervised POS tagger even for
resource-rich languages.
Our methods are also relevant for annotation
projects for resource-poor languages. Assuming
that it is very costly to annotate even 100 tokens,
applying our methods can save annotation effort
but maintain high performance. For example, we
just need 100 tokens to match the accuracy of a su-
pervised method trained on 700 tokens, or we just
need 500 tokens to match the performance with
nearly 2,000 tokens of supervised learning.
Our method is simple, but particularly suitable
for resource-poor languages. We need a small
amount of annotated data for a high performance
POS tagger. For example, we need only around
300 annotated tokens to reach the same accuracy
as the state-of-the-art unsupervised POS tagger
(88.8%).
Tag dictionary Although, it is not our objec-
tive to rely on the dictionary, we are interested
in whether the gains from the correction model
still persist when the DPM performance is im-
proved. We attempt to improve DPM, following
the method of Li et al. (2012) by building a tag dic-
tionary using Wiktionary. This dictionary is then
used as a feature which fires for word-tag pairings
present in the dictionary. We expect that when we
add this additional supervision, the DPM model
should perform better. Table 6 shows the perfor-
mance of DPM and the correction model when in-
corporating the dictionary. The DPM model only
increases 0.6% absolute but the correction model
increases 1.3%. Additionally, it shows that our
model can improve further by incorporating exter-
nal information where available.
CRF Our approach of using simple classifiers
begs the question of whether better results could
be obtained using sequence models, such as con-
ditional random fields (CRFs). As mentioned pre-
viously, a CRF is not well suited for incomplete
data. However, as our second ?correction? model
is trained on complete sequences, we now con-
sider using a CRF in this stage. The training al-
gorithm is as follows: first we estimate the DPM
feature weights on the incomplete data as before,
and next we incorporate the feature weights into a
CRF trained on the 1,000 annotated tokens. This is
complicated by the different feature sets between
the MaxEnt classifier and the CRF, however the
classifier uses a strict subset of the CRF features.
Thus, we use the minimum divergence prior for
the token level features, and a standard zero-mean
prior for the sequence features. That is, the ob-
jective function of the CRF correction model be-
comes:
L
corr
crf
= logP (t|w, ?)
?
1
2?
2
1
?
j?F
1
(?
j
? ?
j
)
2
?
1
2?
2
2
?
j?F
2
?
2
j
(2)
where F
1
is the set of features referring to only
one label as in the DPM maxent model and F
2
is the set of features over label pairs. The union
of F = F
1
? F
2
is the set of all features for
the CRF. We perform grid search using held out
893
data as before for ?
2
1
and ?
2
2
. The CRF correc-
tion model scores 88.1% compared with 86.5% of
the supervised CRF model trained on the 1,000
tokens. Clearly, this is beneficial, however, the
CRF correction model still performs worse than
the MaxEnt correction model (91.3%). We are not
sure why but one reason might be overfitting of
the CRF, due to its large feature set and tiny train-
ing sample. Moreover, this CRF approach is or-
thogonal to T?ackstr?om et al. (2013): we could use
their CRF model as the DPM model and train the
CRF correction model using the same minimum
divergence method, presumably resulting in even
higher performance.
6 Two-output model
Garrette and Baldridge (2013) also use only a
small amount of annotated data, evaluating on
two resource-poor languages Kinyarwanda (KIN)
and Malagasy (MLG). As a simple baseline, we
trained a maxent supervised classifier on this data,
achieving competitive results of 76.4% and 80.0%
accuracy compared with their published results
of 81.9% and 81.2% for KIN and MLG, respec-
tively. Note that the Garrette and Baldridge (2013)
method is much more complicated than this base-
line, and additionally uses an external dictionary.
We want to further improve the accuracy of
MLG using parallel data. Applying the technique
from Section 4 will not work directly, due to the
tagset mismatch (the Malagasy tagset contains 24
tags) which results in highly different feature sets.
Moreover, we don?t have the language expertise
to manually map the tagset. Thus, in this section,
we propose a method capable of handling tagset
mismatch. For data, we use a parallel English-
Malagasy corpus of ?100k sentences,
4
and the
POS annotated dataset developed by Garrette and
Baldridge (2013), which comprises 4230 tokens
for training and 5300 tokens for testing.
6.1 The model
Traditionally, MaxEnt classifiers are trained us-
ing a single label.
5
The method we propose is
trained with pairs of output labels: one for the
4
http://www.ark.cs.cmu.edu/global-voices/
5
Or else a sequence of labels, in the case of a conditional
random field (Lafferty et al., 2001). However, even in this
case, each token is usually assigned a single label. An excep-
tion is the factorial CRF (Sutton et al., 2007), which models
several co-dependent sequences. Our approach is equivalent
to a factorial CRF without edges between tags for adjacent
tokens in the input.
Malagasy tag (t
M
) and one for the universal tag
(t
U
), which are both predicted conditioned on a
Malagasy word (w
M
) in context. Our two-output
model is defined as
P (t
M
, t
U
|w
M
) =
1
Z(w
M
)
exp
(
D
?
j=1
?
j
f
M
j
(w, t
M
)
+
E
?
j=1
?
j
f
U
j
(w, t
U
) +
F
?
j=1
?
j
f
B
j
(w, t
M
, t
U
)
)
(3)
where f
M
, f
U
, f
B
are the feature functions con-
sidering t
M
only, t
U
only, and over both outputs
t
M
and t
U
respectively, and Z(w
M
) is the parti-
tion function. We can think of Eq. (3) as the com-
bination of 3 models: the Malagasy maxent super-
vised model, the DPM model, and the tagset map-
ping model. The central idea behind this model is
to learn to predict not just the MLG tags, as in a
standard supervised model, but also to learn the
mapping between MLG and the noisy projected
universal tags. Framing this as a two output model
allows for information to flow both ways, such that
confident taggings in either space can inform the
other, and accordingly the mapping weights ? are
optimised to maximally exploit this effect.
One important question is how to obtain la-
belled data for training the two-output model, as
our small supervised sample of MLG text is only
annotated for MLG labels t
M
. We resolve this
by first learning the DPM model on the projected
labels, after which we automatically label our
correction training set with predicted tags from
the DPM model. That is, we augment the an-
notated training data from (t
M
, w
M
) to become
(t
M
, t
U
, w
M
). This is then used to train the two-
output maxent classifier, optimising a MAP ob-
jective using standard gradient descent. Note that
it would be possible to apply the same minimum
divergence technique for the two-output maxent
model. In this case the correction model would
include a regularization term over the ? to bias to-
wards the DPM parameters, while ? and ? would
use a zero-mean regularizer. However, we leave
this for future work.
Table 7 summarises the performance of the
state-of-the-art (Garrette et al., 2013), the super-
vised model and the two-output maxent model
evaluated on the Malagasy test set. The two-output
maxent model performs much better than the su-
pervised model, achieving ?5.3% (absolute) im-
894
Model Accuracy (%)
Garrette et al. (2013) 81.2
MaxEnt Supervised 80.0
2-output MaxEnt (Universal tagset) 85.3
2-output MaxEnt (Penn tagset) 85.6
Table 7: The performance of different models for
Malagasy.
provement. An interesting property of this ap-
proach is that we can use different tagsets for the
DPM. We also tried the original Penn treebank
tagset which is much larger than the universal
tagset (48 vs. 12 tags). We observed a small im-
provement reaching 85.6%, suggesting that some
pertinent information is lost in the universal tagset.
All in all, this is a substantial improvement over
the state-of-the-art result of 81.2% (Garrette et al.,
2013) and an error reduction of 23.4%.
7 Conclusion
In this paper, we thoroughly review the work on
multilingual POS tagging of the past decade. We
propose a simple method for building a POS tag-
ger for resource-poor languages by taking advan-
tage of parallel data and a small amount of anno-
tated data. Our method also efficiently resolves
the tagset mismatch issue identified for some lan-
guage pairs. We carefully choose and tune the
model. Comparing with the state-of-the-art, we
are using the more realistic assumption that a
small amount of labelled data can be made avail-
able rather than requiring a crowd-sourced dic-
tionary. We use less parallel data which as we
pointed out in section 3.1, could have been a huge
disadvantage for us. Moreover, we did not exploit
any external monolingual data. Importantly, our
method is simpler but performs better than previ-
ously proposed methods. With only 1,000 anno-
tated tokens, less than 1% of the test data, we can
achieve an average accuracy of 91.3% compared
with 88.8% of the state-of-the-art (error reduction
rate ?22%). Across the 8 languages we are sub-
stantially better at 7 and slightly worse at one. Our
method is reliable and could even be used to im-
prove the performance of a supervised POS tagger.
Currently, we are building the tagger and eval-
uating through several layers of mapping. Each
layer might introduce some noise which accumu-
lates and leads to a biased model. Moreover,
the tagset mappings are not available for many
resource-poor languages. We therefore also pro-
posed a method to automatically match between
tagsets based on a two-output maximum entropy
model. On the resource-poor language Mala-
gasy, we achieved the accuracy of 85.6% com-
pared with the state-of-the-art of 81.2% (Garrette
et al., 2013). Unlike their method, we didn?t use an
external dictionary but instead use a small amount
of parallel data.
In future work, we would like to improve the
performance of DPM by collecting more parallel
data. Duong et al. (2013a) pointed out that using
a different source language can greatly alter the
performance of the target language POS tagger.
We would like to experiment with different source
languages other than English. We assume that we
have 1,000 tokens for each language. Thus, for the
8 languages we considered we will have 8,000 an-
notated tokens. Currently, we treat each language
independently, however, it might also be interest-
ing to find some way to incorporate information
from multiple languages simultaneously to build
the tagger for a single target language.
Acknowledgments
We would like to thank Dan Garreette, Jason
Baldridge and Noah Smith for Malagasy and Kin-
yarwanda datasets. This work was supported by
the University of Melbourne and National ICT
Australia (NICTA). NICTA is funded by the Aus-
tralian Federal and Victoria State Governments,
and the Australian Research Council through the
ICT Centre of Excellence program. Dr Cohn is the
recipient of an Australian Research Council Fu-
ture Fellowship (project number FT130101105).
895
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceeding of
HLT-NAACL, pages 582?590.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39?71.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
(ANLP ?00), pages 224?231, Seattle, Washington,
USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: How far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 575?584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013a. Increasing the quality and quan-
tity of source language data for Unsupervised Cross-
Lingual POS tagging. Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1243?1249. Asian Federation of
Natural Language Processing.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013b. Simpler unsupervised POS tagging
with bilingual projections. Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
634?639. Association for Computational Linguis-
tics.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
pages 138?147, June.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. pages 583?592,
August.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start. In In Proc. ACL, pages
746?754.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?04),
pages 222?229, Barcelona, Spain, July.
Mark Johnson and Stefan Riezler. 2000. Exploit-
ing auxiliary distributions in stochastic unification-
based grammars. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
154?161.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79?86, Phuket, Thailand. AAMT.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference on European Chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach
to domain adaptation of a syntactic disambigua-
tion model. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 9?16.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
896
Computational Linguistics, pages 742?751, Gothen-
burg, Sweden, April.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS taggers (and other tools) for Indian lan-
guages: An experiment with Kannada using Telugu
resources. In Proceedings of IJCNLP workshop on
Cross Lingual Information Access: Computational
Linguistics and the Information Need of Multilin-
gual Societies. (CLIA 2011 at IJNCLP 2011), Chi-
ang Mai, Thailand, November.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn.
Res., 8:693?723, May.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Kristina Toutanova and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In J.C. Platt, D. Koller, and
Y. Singer a nd S.T. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. Curran Associates, Inc.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1 (NAACL ?03), pages 173?180, Edmonton,
Canada.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In In
ACL International Conference Proceedings.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies, NAACL ?01, pages 1?8.
897
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 38?45,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Fast and simple semantic class assignment for biomedical text
K. Bretonnel Cohen
Computational Bioscience Program
U. Colorado School of Medicine
and
Department of Linguistics
U. of Colorado at Boulder
kevin.cohen@gmail.com
Tom Christiansen
Comput. Bioscience Prog.
U. Colorado Sch. of Medicine
tchrist@perl.com
William A. Baumgartner Jr.
Computational Bioscience Program
U. Colorado School of Medicine
william.baumgartner@ucdenver.edu
Karin Verspoor
Computational Bioscience Program
U. Colorado School of Medicine
karin.verspoor@ucdenver.edu
Lawrence E. Hunter
Computational Bioscience Program
U. Colorado School of Medicine
larry.hunter@ucdenver.edu
Abstract
A simple and accurate method for assigning
broad semantic classes to text strings is pre-
sented. The method is to map text strings
to terms in ontologies based on a pipeline of
exact matches, normalized strings, headword
matching, and stemming headwords. The
results of three experiments evaluating the
technique are given. Five semantic classes
are evaluated against the CRAFT corpus of
full-text journal articles. Twenty semantic
classes are evaluated against the correspond-
ing full ontologies, i.e. by reflexive match-
ing. One semantic class is evaluated against
a structured test suite. Precision, recall,
and F-measure on the corpus when evaluat-
ing against only the ontologies in the cor-
pus is micro-averaged 67.06/78.49/72.32 and
macro-averaged 69.84/83.12/75.31. Accuracy
on the corpus when evaluating against all
twenty semantic classes ranges from 77.12%
to 95.73%. Reflexive matching is generally
successful, but reveals a small number of er-
rors in the implementation. Evaluation with
the structured test suite reveals a number of
characteristics of the performance of the ap-
proach.
1 Introduction
Broad semantic class assignment is useful for a
number of language processing tasks, including
coreference resolution (Hobbs, 1978), document
classification (Caporaso et al, 2005), and informa-
tion extraction (Baumgartner Jr. et al, 2008). A
limited number of semantic classes have been stud-
ied extensively, such as assigning text strings to the
category gene or protein (Yeh et al, 2005;
Smith et al, 2008), or the PERSON, ORGANI-
ZATION, and LOCATION categories introduced in
the Message Understanding Conferences (Chinchor,
1998). A larger number of semantic classes have re-
ceived smaller amounts of attention, e.g. the classes
in the GENIA ontology (Kim et al, 2004), vari-
ous event types derived from the Gene Ontology
(Kim et al, 2009), and diseases (Leaman and Gon-
zalez, 2008). However, many semantic types have
not been studied at all. In addition, where ontolo-
gies are concerned, although there has been work
on finding mentions or evidence of specific terms in
text (Blaschke et al, 2005; Stoica and Hearst, 2006;
Davis et al, 2006; Shah et al, 2009), there has been
no work specifically addressing assigning multiple
very broad semantic classes with potential overlap.
In particular, this paper examines the problem of tak-
ing a set of ontologies and a text string (typically,
but not necessarily, a noun phrase) as input and de-
termining which ontology defines the semantic class
that that text string refers to. We make an equiva-
lence here between the notion of belonging to the
domain of an ontology and belonging to a specific
semantic class. For example, if a string in text refers
to something in the domain of the Gene Ontology,
we take it as belonging to a Gene Ontology seman-
tic class (using the name of the ontology only for
convenience); if a string in text refers to something
belonging to the domain of the Sequence Ontology,
we take it as belonging to a Sequence Ontology se-
mantic class. We focus especially on rapid, simple
methods for making such a determination.
The problem is most closely related to multi-class
38
classification, where in the case of this study we are
including an unusually large number of categories,
with possible overlap between them. A text string
might refer to something that legitimately belongs
to the domain of more than one ontology. For exam-
ple, it might belong to the semantic classes of both
the Gene Ontology and the Gene Regulation Ontol-
ogy; regulation is an important and frequent concept
in the Gene Ontology. This fact has consequences
for defining the notion of a false positive class as-
signment; we return to this issue in the Results sec-
tion.
2 Methods
2.1 Target semantic classes
The following ontologies were used to define se-
mantic classes:
? Gene Ontology
? Sequence Ontology
? Foundational Model of Anatomy
? NCBI Taxonomy
? Chemical Entities of Biological Interest
? Phenotypic Quality
? BRENDA Tissue/Enzyme Source
? Cell Type Ontology
? Gene Regulation Ontology
? Homology Ontology
? Human Disease Ontology
? Human Phenotype Ontology
? Mammalian Phenotype Ontology
? Molecule Role Ontology
? Mouse Adult Gross Anatomy Ontology
? Mouse Pathology Ontology
? Protein Modification Ontology
? Protein-Protein Interaction Ontology
? Sample Processing and Separation Techniques
Ontology
? Suggested Ontology for Pharmacogenomics
2.2 Methodology for assigning semantic class
We applied four simple techniques for attempting to
match a text string to an ontology. They are arranged
in order of decreasing stringency. That is, each sub-
sequent method has looser requirements for a match.
This both allows us to evaluate the contribution of
each component more easily and, at run time, allows
the user to set a stringency level, if the default is not
desired.
2.2.1 Exact match
The first and most stringent technique is exact
match. (This is essentially the only technique used
by the NCBO (National Center for Biomedical On-
tology) Annotator (Jonquet et al, 2009), although
it can also do substring matching.) We normalize
terms in the ontology and text strings in the input
for case and look for a match.
2.2.2 Stripping
All non-alphanumeric characters, including
whitespace, are deleted from the terms in the
ontology and from text strings in the input (e.g.
cadmium-binding and cadmium binding both
become cadmiumbinding) and look for a match.
2.2.3 Head nouns
This method involves a lightweight linguistic
analysis. We traversed each ontology and deter-
mined the head noun (see method below) of each
term and synonym in the ontology. We then pre-
pared a dictionary mapping from head nouns to lists
of ontologies in which those head nouns were found.
Head nouns were determined by two simple
heuristics (cf. (Collins, 1999)). For terms fitting the
pattern X of... (where of represents any preposi-
tion) the term X was taken as the head noun. For
all other terms, the rightmost word was taken as the
head noun. These two heuristics were applied in se-
quence when applicable, so that for example positive
regulation of growth (GO:0045927) becomes posi-
tive regulation by application of the first heuristic
and regulation by application of the second heuris-
tic. In the case of some ontologies, very limited pre-
39
processing was necessary?for example, it was nec-
essary to delete double quotes that appeared around
synonyms, and in some ontologies we had to delete
strings like [EXACT SYNONYM] from some terms
before extracting the head noun.
2.2.4 Stemming head nouns
In this technique, the headwords obtained by the
previous step were stemmed with the Porter stem-
mer.
2.3 Corpus and other materials
We made use of three sources in our evaluation.
One is the CRAFT (Colorado Richly Annotated Full
Text) corpus (Verspoor et al, 2009; Cohen et al,
2010a). This is a collection of 97 full-text journal
articles, comprising about 597,000 words, each of
which has been used as evidence for at least one an-
notation by the Mouse Genome Informatics group.
It has been annotated with a number of ontologies
and database identifiers, including:
? Gene Ontology
? Sequence Ontology
? Cell Type Ontology
? NCBI Taxonomy
? Chemical Entities of Biological Interest
(ChEBI)
In total, there are over 119,783 annotations. (For
the breakdown across semantic categories, see Ta-
ble 1.) All of these annotations were done by biolog-
ical scientists and have been double-annotated with
inter-annotator agreement in the nineties for most
categories.
The second source is the full sets of terms from
the twenty ontologies listed in the Introduction. All
of the twenty ontologies that we used were obtained
from the OBO portal. Version numbers are omitted
here due to space limitations, but are available from
the authors on request.
The third source is a structured test suite based on
the Gene Ontology (Cohen et al, 2010b). Structured
test suites are developed to test the performance
of a system on specific categories of input types.
This test set was especially designed to test diffi-
cult cases that do not correspond to exact matches
of Gene Ontology terms, as well as the full range of
types of terms. The test suite includes 300 concepts
from GO, as well as a number of transformations of
their terms, such as cells migrated derived from the
term cell migration and migration of cells derived
from cell migration, classified according to a num-
ber of linguistic attributes, such as length, whether
or not punctuation is included in the term, whether
or not it includes function (stop) words, etc. This
test suite determines at least one semantic category
that should be returned for each term. Unlike using
the entire ontologies, this evaluation method made
detailed error analysis possible. This test suite has
been used by other groups for broad characteriza-
tions of successes and failures of concept recogniz-
ers, and to tune the parameters of concept recogni-
tion systems.
2.4 Evaluation
We did three separate evaluations. In one, we com-
pared the output of our system against manually-
generated gold-standard annotations in the CRAFT
corpus (op. cit.). This was possible only for the on-
tologies that have been annotated in CRAFT, which
are listed above.
In the second evaluation, we used the entire on-
tologies themselves as inputs. In this method, all
responses should be the same?for example, every
term from the Gene Ontology should be classified
as belonging to the GO semantic class.
In the third, we utilized the structured test suite
described above.
2.4.1 Baselines
Two baselines are possible, but neither is optimal.
The first would be to use MetaMap (Aronson, 2001),
the industry standard for semantic category assign-
ment. (Note that MetaMap assigns specific cate-
gories, not broad ones.) However, MetaMap out-
puts only semantic classes that are elements of the
UMLS, which of the ontologies that we looked at,
includes only the Gene Ontology. The other is the
NCBO Annotator. The NCBO Annotator detects
only exact matches (or substring matches) to ontol-
ogy terms, so it is not clear that it is a strong enough
baseline to allow for a stringent analysis of our ap-
40
proach.
3 Results
We present our results in three sections:
? For the CRAFT corpus
? For the ontologies themselves
? For the Gene Ontology test suite
3.1 Corpus results
Table 1 (see next page) shows the results on the
CRAFT corpus if only the five ontologies that were
actually annotated in CRAFT are used as inputs.
The results are given for stemmed heads. Perfor-
mance on the four techniques that make up the ap-
proach is cumulative, and results for stemmed heads
reflects the application of all four techniques. In this
case, where we evaluate against the corpus, it is pos-
sible to determine false positives, so we can give
precision, recall, and F-measures for each semantic
class, as well as for the corpus as a whole. Micro-
averaged results were 67.06 precision, 78.49 recall,
and 72.32 F-measure. Macro-averaged results were
69.84 precision, 83.12 recall, and 75.31 F-measure.
Table 2 (see next page) shows the results for
the CRAFT corpus when all twenty ontologies are
matched against the corpus data, including the many
ontologies that are not annotated in the data. We
give results for just the five annotated ontologies
below. Rather than calculating precision, recall,
and F-measure, we calculate only accuracy. This
is because when classes other than the gold stan-
dard class is returned, we have no way of know-
ing if they are incorrect without manually examin-
ing them?that is, we have no way to identify false
positives. If the set of classes returned included the
gold standard class, a correct answer was counted. If
the classifier returned zero or more classes and none
of them was the gold standard, an incorrect answer
was counted. Results are given separately for each
of the four techniques. This allows us to evaluate
the contribution of each technique to the overall re-
sults; the value in each column is cumulative, so the
value for Stemmed head includes the contribution of
all four of the techniques that make up the general
approach. Accuracies of 77.12% to 95.73% were
achieved, depending on the ontology. We see that
the linguistic technique of locating the head noun
makes a contribution to all categories, but makes an
especially strong contribution to the Gene Ontology
and Cell Type Ontology classes. Stemming of head-
words is also effective for all five categories. We see
that exact match is effective only for those semantic
classes for which terminology is relatively fixed, i.e.
the NCBI taxonomy and chemical names. In some
of the others, matching natural language text is very
difficult by any technique. For example, of the 8,665
Sequence Ontology false negatives in the data re-
flected in the P/R/F values in Table 1, a full 2,050
are due to the single character +, which does not
appear in any of the twenty ontologies that we ex-
amined and that was marked by the annotators as a
Sequence Ontology term, wild type (SO:0000817).
3.2 Ontology results
As the second form of evaluation, we used the
terms from the ontologies themselves as the inputs
to which we attempted to assign a semantic class. In
this case, no annotation is required, and it is straight-
forwardly the case that each term in a given ontology
should be assigned the semantic class of that ontol-
ogy. We used only the head noun technique. We did
not use the exact match or stripping heuristics, since
they are guaranteed to return the correct answer, nor
did we use stemming. Thus, this section of the eval-
uation gives us a good indication of the performance
of the head noun approach.
As might be expected, almost all twenty on-
tologies returned results in the 97-100% correct
rate. However, we noted much lower performance
in two ontologies, the Sequence Ontology and the
Molecule Role Ontology. This lower performance
reflects a number of preprocessing errors or omis-
sions. The fact that we were able to detect these low-
performing ontologies indicates that our evaluation
technique in this experiment?trying to match terms
from an ontology against that ontology itself?is a
robust evaluation technique and should be used in
similar studies.
3.2.1 Structured test suite results
The third approach to evaluation involved use of
the structured test suite. The structured test suite re-
vealed a number of trends in the performance of the
system.
41
Ontology Annotations Precision Recall F-measure
Gene Ontology 39,626 66.31 73.06 69.52
Sequence Ontology 40,692 63.00 72.21 67.29
Cell Type Ontology 8,383 53.58 87.27 66.40
NCBI Taxonomy 11,775 96.24 92.51 94.34
ChEBI 19,307 70.07 90.53 79.00
Total (micro-averaged) 119,783 67.06 78.49 72.32
Total (macro-averaged) 69.84 83.12 75.31
Table 1: Results on the CRAFT corpus when only the CRAFT ontologies are used as input. Results are for stemmed
heads. Precision, recall, and F-measure are given for each semantic category in the corpus. Totals are micro-averaged
(over all tokens) and macro-averaged (over all categories), respectively. P/R/F are cumulative, so that the results for
stemmed heads reflect the application of all four techniques.
Ontology Exact Stripped Head noun Stemmed head
Gene Ontology 24.26 24.68 59.18 77.12
Sequence Ontology 44.28 47.63 56.63 73.33
Cell Type Ontology 25.26 25.80 70.09 88.38
NCBI Taxonomy 84.67 84.71 90.97 95.73
ChEBI 86.93 87.44 92.43 95.49
Table 2: Results on the CRAFT corpus when all twenty ontologies are used as input. Accuracy is given for each
technique. Accuracy is cumulative, so that accuracy in the final column reflects the application of all four techniques.
? The headword technique works very well for
recognizing syntactic variants. For example, if
the GO term induction of apoptosis is written
as apoptosis induction, the headword technique
allows it to be picked up.
? The headword technique works in situations
where text has been inserted into a term. For
example, if the GO term ensheathment of neu-
rons appears as ensheathment of some neu-
rons, the headword technique will allow it to be
picked up. If the GO term regulation of growth
shows up as regulation of vascular growth, the
headword technique will allow it to be picked
up.
? The headword stemming technique allows us to
pick up many verb phrases, which is important
for event detection and event coreference. For
example, if the GO term cell migration appears
in text as cells migrate, the technique will de-
tect it. The test suite also showed that failures
to recognize verb phrases still occur when the
morphological relationship between the nomi-
nal term and the verb are irregular, as for exam-
ple between the GO term growth and the verb
grows.
? The technique?s ability to handle coordination
is very dependent on the type of coordination.
For example, simple coordination (e.g. cell mi-
gration and proliferation) is handled well, but
complex coordination (e.g. cell migration, pro-
liferation and adhesion) is handled poorly.
? Stemming is necessary for recognition of plu-
rals, regardless of the length of the term in
words.
? The approach currently fails on irregular plu-
rals, due to failure of the Porter stemmer to han-
dle plurals like nuclei and nucleoli well.
? The approach handles classification of terms
that others have characterized as ?ungram-
matical,? such as transposition, DNA-mediated
(GO:0006313). This is important, because ex-
act matches will always fail on these terms.
42
4 Discussion
4.1 Related work
We are not aware of similar work that tries to assign
a large set of broad semantic categories to individ-
ual text strings. There is a body of work on selecting
a single ontology for a domain or text. (Mart??nez-
Romero et al, 2010) proposes a method for selecting
an ontology given a list of terms, all of which must
appear in the ontology. (Jonquet et al, 2009) de-
scribes an ontology recommender that first annotates
terms in a text with the Open Biomedical Annotator
service, then uses the sum of the scores of the indi-
vidual annotations to recommend a single ontology
for the domain as a whole.
4.2 Possible alternate approaches
Three possible alternative approaches exist, all of
which would have as their goal the returning of a sin-
gle best semantic class for every input. However, for
the use cases that we have identified?coreference
resolution, document classification, information ex-
traction, and curator assistance?we are more inter-
ested in wide coverage of a broad range of semantic
classes, so these approaches are not evaluated here.
However, we describe them for completeness and
for the use of researchers who might be interested
in pursuing single-class assignment.
4.2.1 Frequent words
One alternative approach would be to use simple
word frequencies. For example, for each ontology,
one could determine the N most frequent words, fil-
tering out stop words. At run time, check the words
in each noun phrase in the text against the lists of fre-
quent words. For every word from the text that ap-
peared in the list of frequent words from some ontol-
ogy, assign a score to each ontology in which it was
found, weighting it according to its position in the
list of frequent words. In theory, this could accom-
modate for the non-uniqueness of word-to-ontology
mappings, i.e. the fact that a single word might ap-
pear in the lists for multiple ontologies. However,
we found the technique to perform very poorly for
differentiating between ontologies and do not rec-
ommend it.
4.2.2 Measuring informativeness
If the system is desired to return only one sin-
gle semantic class per text string, then one approach
would be to determine the informativeness of each
word in each ontology. That is, we want to find the
maximal probability of an ontology given a word
from that ontology. This approach is very difficult
to normalize for the wide variability in size of the
many ontologies that we wanted to be able to deal
with.
4.2.3 Combining scores
Finally, one could conceivably combine scores for
matches obtained by the different strategies, weight-
ing them according to their stringency, i.e. exact
match receiving a higher weight than head noun
match, which in turn would receive a higher weight
than stemmed head noun match. This weighting
might also include informativeness, as described
above.
4.3 Why the linguistic method works
As pointed out above, the lightweight linguistic
method makes a large contribution to the perfor-
mance of the approach for some ontologies, partic-
ularly those for which the exact match and stripping
techniques do not perform well. It works for two
reasons, one related to the approach itself and one
related to the nature of the OBO ontologies. From
a methodological perspective, the approach is effec-
tive because headwords are a good reflection of the
semantic content of the noun phrase and they are
relatively easy to access via simple heuristics. Of
course simple heuristics will fail, as we can observe
most obviously in the cases where we failed to iden-
tify members of the ontologies in the second eval-
uation step. However, overall the approach works
well enough to constitute a viable tool for coref-
erence systems and other applications that benefit
from the ability to assign broad semantic classes to
text strings.
The approach is also able to succeed because of
the nature of the OBO ontologies. OBO ontologies
are meant to be orthogonal (Smith et al, 2007). A
distributional analysis of the distribution of terms
and words between the ontologies (data not shown
here, although some of it is discussed below), as well
as the false positives found in the corpus study, sug-
43
gests that orthogonality between the OBO ontolo-
gies is by no means complete. However, it holds
often enough for the headword method to be effec-
tive.
4.4 Additional error analysis
In the section on the results for the structured test
suite, we give a number of observations on contribu-
tions to errors, primarily related either to the char-
acteristics of individual words or to particular syn-
tactic instantiations of terms. Here, we discuss some
aspects of the distribution of lexical items and of the
corpus that contributed to errors.
? The ten most common headwords appear in
from 6-16 of the twenty ontologies. However,
they typically appear in one ontology at a fre-
quency many orders of magnitude greater than
their frequency in the other ontologies. Taking
this frequency data into account for just these
ten headwords would likely decrease false pos-
itives quite significantly.
? More than 50% of Gene Ontology terms share
one of only ten headwords. Many of our Gene
Ontology false negatives on the corpus are be-
cause the annotated text string does not contain
a word such as process or complex that is the
head word of the canonical term.
4.5 Future work
The heuristics that we implemented for extracting
headwords from OBO terms were very simple, in
keeping with our initial goal of developing an easy,
fast method for semantic class assignment. How-
ever, it is clear that we could achieve substantial per-
formance improvements from improving the heuris-
tics. We may pursue this track, if it becomes clear
that coreference performance would benefit from
this when we incorporate the semantic classification
approach into a coreference system.
On acceptance of the paper, we will make Perl and
Java versions of the semantic class assigner publicly
available on SourceForge.
4.6 Conclusion
The goal of this paper was to develop a simple ap-
proach to assigning text strings to an unprecedent-
edly large range of semantic classes, where mem-
bership in a semantic class is equated with belonging
to the semantic domain of a specific ontology. The
approach described in this paper is able to do that
at a micro-averaged F-measure of 72.32 and macro-
averaged F-measure of 75.31 as evaluated on a man-
ually annotated corpus where false positives can be
determined, and with an accuracy of 77.12-95.73%
when only true positives and false negatives can be
determined.
References
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: The MetaMap program.
In Proc AMIA 2001, pages 17?21.
William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-
son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-
mann, Elizabeth K. White, Olga Medvedeva, K. Bre-
tonnel Cohen, and Lawrence Hunter. 2008. Concept
recognition for extracting protein interaction relations
from biomedical text. Genome Biology, 9.
Christian Blaschke, Eduardo A. Leon, Martin Krallinger,
and Alfonso Valencia. 2005. Evaluation of BioCre-
ative assessment of task 2. BMC Bioinformatics, 6
Suppl 1.
J. Gregory Caporaso, William A. Baumgartner Jr..,
K. Bretonnel Cohen, Helen L. Johnson, Jesse Paque-
tte, and Lawrence Hunter. 2005. Concept recognition
and the TREC Genomics tasks. In The Fourteenth Text
REtrieval Conference (TREC 2005) Proceedings.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2.
K. Bretonnel Cohen, Helen L. Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E. Hunter. 2010a.
The structural and content aspects of abstracts versus
bodies of full text journal articles are different. BMC
Bioinformatics, 11(492).
K. Bretonnel Cohen, Christophe Roeder, William
A. Baumgartner Jr., Lawrence Hunter, and Karin Ver-
spoor. 2010b. Test suite design for biomedical ontol-
ogy concept recognition systems. In Proceedings of
the Language Resources and Evaluation Conference.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
N. Davis, H. Harkema, R. Gaizauskas, Y. K. Guo,
M. Ghanem, T. Barnwell, Y. Guo, and J. Ratcliffe.
2006. Three approaches to GO-tagging biomedical
abstracts. In Proceedings of the Second International
Symposium on Semantic Mining in Biomedicine, pages
21?28, Jena, Germany.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
44
C. Jonquet, N.H. Shah, and M.A. Musen. 2009. Pro-
totyping a biomedical ontology recommender ser-
vice. In Bio-Ontologies: Knowledge in Biology,
ISMB/ECCB SIG.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the international joint workshop on natu-
ral language processing in biomedicine and its appli-
cations, pages 70?75.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
BioNLP 2009 Companion Volume: Shared Task on En-
tity Extraction, pages 1?9.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survey of advances in biomedical
named entity recognition. In Pac Symp Biocomput.
Marcos Mart??nez-Romero, Jose? Va?zquez-Naya, Cris-
tian R. Munteanu, Javier Pereira, and Alejandro Pazos.
2010. An approach for the automatic recommendation
of ontologies using collaborative knowledge. In KES
2010, Part II, LNAI 6277, pages 74?81.
Nigam H. Shah, Nipun Bhatia, Clement Jonquet, Daniel
Rubin, Annie P. Chiang, and Mark A. Musen. 2009.
Comparison of concept recognizers for building the
Open Biomedical Annotator. BMC Bioinformatics,
10.
Barry Smith, Michael Ashburner, Cornelius Rosse,
Jonathan Bard, William Bug, Werner Ceusters,
Louis J. Goldberg, Karen Eilbeck, Amelia Ireland,
Christopher J. Mungall, The OBI Consortium, Neo-
cles Leontis, Philippe Rocca-Serra, Alan Ruttenberg,
Susanna-Assunta Sansone, Richard H. Scheuermann,
Nigam Shah, Patricia L. Whetzel, and Suzanna Lewis.
2007. The OBO Foundry: coordinated evolution of
ontologies to support biomedical data integration. Na-
ture Biotechnology, 25:1251?1255.
Larry Smith, Lorraine Tanabe, Rie Johnson nee Ando,
Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-
Shi Lin, Roman Klinger, Christof Friedrich, Kuzman
Ganchev, Manabu Torii, Hongfang Liu, Barry Had-
dow, Craig Struble, Richard Povinelli, Andreas Vla-
chos, William Baumgartner, Jr., Lawrence Hunter,
Bob Carpenter, Richard Tzong-Han Tsai, Hong-
Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun,
Sophia Katrenko, Pieter Adriaans, Christian Blaschke,
Rafael Torres Perez, Mariana Neves, Preslav Nakov,
Anna Divoli, Manuel Mana, Jacinto Mata-Vazquez,
and W. John Wilbur. 2008. Overview of BioCreative
II gene mention recognition. Genome Biology.
E. Stoica and M. Hearst. 2006. Predicting gene functions
from text using a cross-species approach. In Proceed-
ings of the 11th Pacific Symposium on Biocomputing.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of traditional
and Open Access scientific journals are similar. BMC
Bioinformatics, 10.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. BioCreatve task 1A: gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl. 1).
45
Proceedings of BioNLP Shared Task 2011 Workshop, pages 164?172,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
               From Graphs to Events: A Subgraph Matching Approach
                for Information Eextraction from Biomedical Text
Haibin Liu, Ravikumar Komandur, Karin Verspoor
Center for Computational Pharmacology
University of Colorado School of Medicine
PO Box 6511, MS 8303, Aurora, CO, 80045 USA
Abstract
We participated in the BioNLP Shared Task 2011,
addressing the GENIA event extraction (GE) and
the Epigenetics and Post-translational Modifica-
tions (EPI) tasks. A graph-based approach is
employed to automatically learn rules for detect-
ing biological events in the life-science literature.
The event rules are learned by identifying the
key contextual dependencies from full syntactic
parsing of annotated text. Event recognition is
performed by searching for an isomorphism be-
tween event rules and the dependency graphs of
sentences in the input texts. While we explored
methods such as performance-based rule rank-
ing to improve precision, we merged rules across
multiple event types in order to increase recall.
We achieved a 41.13% F-score in detecting events
of nine types in the Task 1 of the GE task, and a
52.67% F-score in identifying events across fif-
teen types in the core task of the EPI task. Our
performance on both tasks is comparable to the
state-of-the-art systems. Our approach does not
require any external domain-specific resources.
The consistent performance on the two tasks sup-
ports the claim that the method generalizes well
to extract events from different domains where
training data is available.
1 Introduction
Recent research in information extraction in the biolog-
ical domain has focused on extracting semantic events
involving genes or proteins, such as binding events or
post-translational modifications. To date, most of the
biological knowledge about these events has only been
available in the form of unstructured text in scientific
articles (Abulaish and Dey, 2007; Ananiadou et al,
2010).
When a biological event is described in text, it can
be analyzed by recognizing its type, the trigger that sig-
nals the event, and one or more event arguments. The
BioNLP-ST 2009 (Kim et al, 2009) focused on the
recognition of semantically typed, complex events in
the biological literature. Although the best-performing
system achieved a 51.95% F-score in identifying events
across nine types, only 4 of the rest 23 participating
teams obtained an F-score in the 40% range. This sug-
gests that the problem of biological event extraction is
difficult and far from solved.
Graphs provide a powerful primitive for modeling
biological data such as pathways and protein interac-
tion networks (Tian et al, 2007; Yan et al, 2006). More
recently, the dependency representations obtained from
full syntactic parsing, with its ability to reveal long-
range dependencies, has shown an advantage in bi-
ological relation extraction over the traditional Penn
Treebank-style phrase structure trees (Miyao et al,
2009). Since the dependency representation maps
straightforwardly onto a directed graph, operations on
graphs can be naturally applied to the problem of bio-
logical event extraction.
We participated in the BioNLP-ST 2011 (Kim et al,
2011a), and applied a graph matching-based approach
(Liu et al, 2010) to tackling the Task 1 of the GE-
NIA event extraction (GE) task (Kim et al, 2011b), and
the core task of the Epigenetics and Post-translational
Modifications (EPI) task (Ohta et al, 2011), two main
tasks of the BioNLP-ST 2011. Event recognition is
performed by searching for an isomorphism between
dependency representations of automatically learned
event rules and complete sentences in the input texts.
This process is treated as a subgraph matching problem,
which corresponds to the search for a subgraph isomor-
phic to a rule graph within a sentence graph. While
we explored methods such as performance-based rule
ranking to improve the precision of the GE and EPI
tasks, we merged rules across multiple event types in
order to increase the recall of the EPI task.
The rest of the paper is organized as follows: In Sec-
tion 2, we introduce the BioNLP Shared Task 2011.
Section 3 describes the subgraph matching-based event
extraction method. Section 4 and Section 5 elabo-
164
rate the implementation details and our performance
respectively. Finally, Section 6 summarizes the paper
and introduces future work.
2 BioNLP Shared Task 2011
The BioNLP-ST 2011 is the extension of the BioNLP-
ST 2009 that focused on the recognition of events in the
biological literature. The BioNLP-ST 2011 extends the
previous task in three directions: the type of the inves-
tigated text, the domain of the subject, and the targeted
event types. As a result, the shared task was organized
into four independent tasks: GENIA Event Extraction
Task (GE), Epigenetics and Post-translational Modifi-
cations Task (EPI), Infectious Diseases Task (ID) and
Bacteria Track.
The definition of the GE task remained the same as
the BioNLP-ST 2009. However, additional annotated
texts that come from full papers were provided together
with the dataset of the 2009 task to generalize the task
from PubMed abstracts to full text articles. The pri-
mary task of the GE task was to detect biological events
of nine types such as protein binding and regulation,
given the annotation of protein names. It was required
to extract type, trigger, and primary arguments of each
event. This task is an example of extraction of seman-
tically typed, complex events for which the arguments
can also be other events. Such embedding results in a
nested structure that captures the underlying biological
statements more accurately.
Different from the subject domain of the GE task on
transcription factors in human blood cells, the EPI task
focused on events related to epigenetic change, includ-
ing DNA methylation and histone modification, as well
as other common post-translational protein modifica-
tions. The core task followed the definition for Phos-
phorylation event extraction in the 2009 task, and ex-
tended that basic event type to a total of fifteen types
including both positive and negative variants, for ex-
ample Acetylation and Deacetylation. The task dataset
was prepared from relevant PubMed abstracts, with
additional evidence sentences from databases such as
PubMeth (Ongenaert et al, 2007). Given the annota-
tion of protein names, the core task required to extract
type, trigger, and primary arguments of each event.
We focused on the primary task of GE and the core
task of EPI, and tackled the event extraction problem in
both cases using a graph matching-based method.
inhibit-2/VBP
Interferons-1/NNS
nsubj
activation-3/NN
dobj
inducing-12/VBG
prepc_by
BIO_Entity-5/NNP(STAT6)
prep_of
BIO_Entity-7/NNP(interleukin 4)
prep_by
monocytes-10/NNS
prep_in
human-9/JJ
amod
expression-15/NN
dobj
BIO_Entity-13/NNP(SOCS-1)
nn
gene-14/NN
nn
Figure 1: Dependency Graph Example
3 Subgraph Matching-based Event Extraction
3.1 Dependency Representation
The dependency representation of a sentence is formed
by tokens in the sentence and binary relations between
them. A single dependency relation is represented
as relation(governor, dependent), where governor and
dependent are tokens, and relation is a type of the
grammatical dependency relation. This representation
is essentially a labeled directed graph, which is named
dependency graph and defined as follows:
Definition 1. A dependency graph is a pair of sets
G = (V,E), where V is a set of nodes that correspond
to the tokens in a sentence, and E is a set of directed
edges, for which the edge labels are types of depen-
dency relations between the tokens, and the edge direc-
tion is from governor to dependent node.
Figure 1 illustrates the dependency graph for the sen-
tence: ?Interferons inhibit activation of STAT6 by in-
terleukin 4 in human monocytes by inducing SOCS-1
gene expression.? (MEDLINE: 10485906). The token
number in the sentence is appended to each token in
order to differentiate identical tokens that co-occur in a
sentence. All the protein names in the sentence have
been replaced with a unified tag ?BIO Entity?. The
POS tag of each token is noted. ?BIO Entity? tokens
are uniformly tagged as proper nouns.
3.2 Event Rule Induction
The premise of our work is that there is a set of fre-
quently occurring event rules that match a majority of
165
stated events about protein biology. We consider that
an event rule encodes the detailed description and char-
acterizes the typical contextual structure of a group of
biological events. The rules are learned from labeled
training sentences using a graph-based rule induction
method (Liu et al, 2010), and we briefly describe the
algorithm as follows.
Starting with the dependency graph of each training
sentence, edge directions are first removed so that the
directed graph is transformed into an undirected graph,
where a path must exist between any two nodes since
the graph is always connected. For each gold event, the
shortest dependency path in the undirected graph con-
necting the event trigger nodes to each event argument
node is selected. The union of all shortest dependency
paths is then computed, and the original directed de-
pendency representation of the path union is retrieved
and used as the graph representation of the event.
For multi-token event triggers, the shortest depen-
dency path connecting the node of every trigger token
to the node of each event argument is selected, and the
union of the paths is then computed for each trigger.
For regulation events, when a sub-event is used as an
argument, only the type and the trigger of the sub-event
are preserved as the argument of the main events. The
shortest dependency path is extracted so as to connect
the trigger nodes of the main event to the trigger nodes
of the sub-event. In case that there exists more than
one shortest path, all of the paths are considered. As a
result, each gold event is transformed into the form of
a biological event rule. The algorithm is elaborated in
more detail in (Liu et al, 2010). The obtained rules are
categorized in terms of the event types of the tasks.
3.3 Sentence Matching
We attempted to match event rules to each testing sen-
tence to extract events from the sentence using a sen-
tence matching approach. Since the event rules and the
sentences all possess a dependency graph, the matching
process is a subgraph matching problem, which cor-
responds to the search for a subgraph isomorphic to
an event rule graph within the graph of a testing sen-
tence. The subgraph matching problem is also called
subgraph isomorphism, defined in this work as follows:
Definition 2. An event rule graph Gr = (Vr, Er)
is isomorphic to a subgraph of a sentence graph Gs =
(Vs, Es), denoted by Gr ?= Ss ? Gs, if there is an
injective mapping f : Vr ? Vs such that, for every
directed pair of nodes vi, vj ? Vr, if (vi, vj) ? Er then
(f(vi), f(vj)) ? Es, and the edge label of (vi, vj) is
the same as the edge label of (f(vi), f(vj)).
The subgraph isomorphism problem is NP-complete
(Cormen et al, 2001). A number of algorithms have
been designed to tackle the problem of subgraph iso-
morphism in different applications (Ullmann, 1976;
Cordella et al, 2004; Pelillo et al, 1999). Considering
that the graphs of rules and sentences involved in the
matching process are small, a simple subgraph match-
ing algorithm using a backtracking approach (Liu et
al., 2010) was used in this work. It is named ?Injec-
tive Graph Embedding Algorithm? and designed based
on the Huet?s graph unification algorithm (Huet, 1975).
The formalized algorithm and the detailed description
are given in (Liu et al, 2010).
When matching between graphs, different combina-
tions of matching features can be applied, resulting in
different matching criteria. The features include edge
features (E) which are edge label and edge direction,
and node features which are POS tags (P), trigger to-
kens (T), and all tokens (A), ranging from the least spe-
cific matching criterion, E, to the much stricter crite-
rion, A. For each sentence, the algorithm returns all the
matched rules together with the corresponding injec-
tive mappings from rule nodes to sentence tokens. Bio-
logical events are then extracted by applying the event
descriptions of tokens in each matched rule consisting
of the type, the trigger and the arguments to the corre-
sponding tokens of the sentence.
4 Implementation
4.1 Preprocessing
The same preprocessing steps as in (Liu et al, 2010)
are completed on the datasets of the GE and the EPI
tasks before performing text mining strategies. These
include sentence segmentation and tokenization, Part-
of-Speech tagging, and sentence parsing.
The Stanford unlexicalized natural language parser
(version 1.6.5), which includes Genia Treebank 1.0
(Ohta et al, 2005) as training material, is used to ana-
lyze the syntactic structure of the sentences. The parser
returns a dependency graph for each sentence.
4.2 Rule Induction and Sentence Matching
For each gold event, the shortest path in the undirected
graph connecting the event trigger to each event argu-
ment is extracted using Dijkstra?s algorithm (Cormen
et al, 2001) with equal weight for edges.
Sentence matching is performed and the raw match-
ing results are then postprocessed based on the specifi-
cations of the shared task, such as event trigger cannot
166
be a protein name or another event.
5 Results and Evaluation
This section presents our results on the GE and the EPI
tasks (Kim et al, 2011b; Ohta et al, 2011) respectively.
Different experimental methods in processing the ob-
tained event rules are described for the purpose of im-
proving the precision of both tasks and increasing the
recall of the EPI task.
5.1 GE task
5.1.1 Preprocessing Results
For training data, only sentences that contain at least
one protein and one event are considered candidates
for further processing. For testing data, candidate sen-
tences contain at least one protein. Our event recog-
nition method focuses on extracting events from sen-
tences. Therefore, only sentence-based events are con-
sidered in this work. Table 1 presents some statistics of
the preprocessed datasets.
Attributes Counted Training Dev. Testing
Abstracts&Full articles 908 259 347
Total sentences 8,759 2,954 3,437
Candidate sentences 3,615 1,989 2,353
Total events 10,287 3,243 4,457
Sentence-based events 9,583 3,058 hidden
Table 1: Statistics of GE dataset
We were able to build event rules for 9,414 gold
events. Gold events in which the event trigger and
an event argument are not connected by a path in the
undirected dependency graph of the sentence could not
be transformed into a biological event rule. After re-
moving duplicate rules, we obtained 8,677 event rules,
which are distributed over nine event types. The rules
that are isomorphic to each other in terms of their graph
representation are not filtered at this stage as the dupli-
cate events they produce will be removed eventually to
prepare the annotations for the shared task.
5.1.2 Probability-based rule refining
We observed that some event rules of an event type
overlap with rules of other event types. For instance, a
Transcription rule is isomorphic to a Gene expression
rule in terms of the graph representation and they also
share a same event trigger token. In fact, tokens like
?gene expression? and ?induction? are used as event
trigger of both Transcription and Gene expression
in training data. Therefore, the detection of some
Gene expression events is always accompanied by cer-
tain Transcription events. This will have detrimen-
tal effects on the precision of both Transcription and
Gene expression event types.
As transcription is the first step leading to gene ex-
pression (Ananiadou and Mcnaught, 2005), there ex-
ist some correlations or associations between the two
event types. In tackling this problem, we processed
the overlapping rules based on a conditional probability
P (t|E), where t stands for an event trigger and E repre-
sents one of the event types. Eq.(1) is used to estimate
the value of P (ti|E).
P (ti|E) = f(ti, E)?
i f(ti, E)
, (1)
where f(ti, E) is the frequency of the event trigger ti
of the event type E in the training data, and?i f(ti, E)
calculates the total frequency of all event triggers of the
event type E in the training data.
P (ti|E) evaluates the degree of the importance of a
trigger to an event type. When the dependency graphs
of two rules of different event types are isomorphic to
each other, and two rules share a same event trigger,
we examine the P (ti|E) of each event type, and only
retain the rule for which the P (ti|E) is higher.
Compared to the ?once a trigger, always a trigger?
method employed in other work (Buyko et al, 2009;
Kilicoglu and Bergler, 2009), triggers are treated in a
more flexible way in our work. A token is not neces-
sarily always a trigger unless it appears in the appropri-
ate context. Also, the same token can serve as trigger
for different event types as long as it appears in the dif-
ferent context. A trigger will only be classified into a
fixed event type when it could serve as trigger for dif-
ferent event types in the same context.
5.1.3 Performance-based rule ranking
In addition to the process of refining rules across
event types, we proposed a performance-based rule
ranking method to evaluate each rule under one event
type. We matched each rule to sentences in the de-
velopment set using the subgraph matching approach.
For rules that produce at least one event prediction, we
ranked them by PRC(ri), the precision of each rule ri,
which is computed via Eq.(2).
PRC(ri) = #correctly predicted events by ri#predicted events by ri (2)
167
We manually examined the rules with low rank. In
our experiments, the PRC(ri) ratio of these rules is
bigger than 4:1. We removed the ones that are either in-
correct or ambiguous in semantics and syntactics based
on our domain knowledge. Our assumption is that these
rules will keep producing false positive events on the
testing data if they are retained in the rule set. For
rules that do not make any predictions on the develop-
ment data, we keep them in the set in the hope that they
may contribute to the event recognition from the testing
data. Without affecting much on the recall, this process
helps to improve the precision of the events extracted
from the development data.
5.1.4 GE Results on Development Set
In our previous work (Liu et al, 2010), the match-
ing criteria, ?E+P+T? and and ?E+P+A?, achieved the
highest F-score and the highest precision respectively
among all the investigated matching criteria. ?E+P+T?
requires that edge directions and labels of all edges (E)
be identical, POS tags (P) of all tokens be identical, and
tokens of only event triggers (T) be identical for the
edges and the nodes of a rule and a sentence to match
with each other. ?E+P+A? requires that edges (E), POS
tags (P) and all tokens (A) be exactly the same. In this
work, we focused on these two criteria and explored
to extend them for graph matching between event rules
and sentences.
We attempted to relax the matching criterion of POS
tags for nouns and verbs. For nouns, the plural form of
nouns is allowed to match with the singular form, and
proper nouns are allowed to match with regular nouns.
For verbs, past tense, present tense and base present
form are allowed to match with each other.
Next, letters of each token are transformed into lower
case, and tokens containing hyphens are normalized
into non-hyphenated forms. Lemmatization is then per-
formed on every pair of tokens to be matched using
WordNet (Fellbaum, 1998) as the lemmatizer to al-
low tokens that share a same lemma to match. Since
WordNet is a lexical database only for the general Eng-
lish language, the lemma of a fair amount of domain-
specific vocabulary cannot be found in WordNet, such
as ?Phosphorylation? and ?Methylation?. In this case,
a backup process is invoked to stem the tokens to
their root forms using the Porter?s stemming algorithm
(Porter, 1997) allowing the tokens derived from a same
root word to match.
To further generalize event rules, we extended
the matching criteria ?E+P*+A*? to ?E+P*+A*S?
to allow tokens to match if their lemmatized forms
have a common synonym in terms of the synsets
of WordNet. Since WordNet will relate verbs such
as ?induce? and ?receive? together as they share
a synonym ?have?, and allow nouns like ?expres-
sion? and ?aspect? to match as they share a syn-
onym ?face?, we limited this extension to only ad-
jective tokens to avoid too many false positive events
and allow tokens like ?crucial? and ?critical? to match.
Table 2 shows the event extraction results on the
development data based on different matching cri-
teria. The performance is evaluated by ?Approxi-
mate Span Matching/Approximate Recursive Match-
ing?, the primary evaluation measure of the shared task.
?E+P*+T*?, ?E+P*+A*? and ?E+P*+A*S? demon-
strate the performance of the extended criteria.
Feature Recall(%) Prec.(%) F-score(%)
E+P+A 28.03 66.74 39.48
E+P+T 31.17 52.38 39.09
E+P*+A* 31.45 63.51 42.07
E+P*+T* 35.71 46.26 40.31
E+P*+A*S 31.51 63.32 42.08
Table 2: GE results on development set using different
matching criteria
As the strictest matching criteria, ?E+P+A? performs
better than ?E+P+T? in both precision and F-score. Al-
though ?E+P+T? achieves a better recall, when relax-
ing the matching criteria from all tokens being the same
to only event trigger tokens having to be identical, the
precision of ?E+P+T? is decreased by a large margin,
nearly 14%. This indicates that a certain number of bi-
ological events are described in very similar ways in
the literature, involving same grammatical structures
and identical contextual contents. While producing
more incorrect events, ?E+P*+A*? and ?E+P*+T*?
significantly improve the recall, leading to a better
F-score over ?E+P+A? and ?E+P+T?. This confirms
the effectiveness of the POS relaxation and the to-
ken lemmatization on the generalization of event rules.
?E+P*+A*S? obtains a comparable performance with
?E+P*+A*? with only a 0.06% increase in recall and a
0.2% drop in precision.
5.1.5 GE Results on Testing Set
Table 3 shows our results of ?E+P*+A*? on the test-
ing data using the official metric. We are listed as
team ?CCP-BTMG?. Ranked by F-score, our perfor-
mance ranked 10th out of 15 participating groups. It
168
is worth noting that our result on the event type ?Pro-
tein catabolism? ranked 1st.
Event type Rec.(%) Prec.(%) F(%)
Gene expression 58.68 75.77 66.14
Transcription 39.08 51.91 44.59
Protein catabolism 66.67 83.33 74.07
Phosphorylation 63.78 85.51 73.07
Localization 29.32 91.80 44.44
Binding 22.61 49.12 30.96
Regulation 12.99 46.73 20.33
Positive regulation 21.90 44.51 29.35
Negative regulation 15.76 40.18 22.64
All total 31.57 58.99 41.13
Table 3: GE results of ?E+P*+A*? on testing set by ?Ap-
proximate Span /Approximate Recursive Matching?
The performance of our system on the testing set
is consistent with that of the development set. We
achieved a comparable precision with the top systems
and ranked 6th by precision. However, our recall was
lower, ranking 11th. This adversely impacted the over-
all F-score. The lower recall is not surprising because
the graph matching criteria ?E+P*+A*? strictly de-
mand that every lemmatized token in the patterns, other
than protein names represented as?BIO Entity?, has to
find its exact match in the input sentences. The detailed
analysis on the recall problem is presented in the ?Error
Classification? section.
While examining the false positives, we found that
for many cases our result matched the gold annotation
but for the trigger word. We believe that event type and
their arguments are more important biologically than
the trigger. We consulted some domain experts who
reinforced our intuition in many cases that different
words could be considered as trigger for the event in
question. Following this we contacted organizers and
they agreed to release a new evaluation scheme to ig-
nore the trigger match requirement in order to support
evaluation of the event extraction itself.
Table 4 shows our results of ?E+P*+A*? evaluated
by other official evaluation metrics of the task. The
strict matching scheme requires exact trigger span as
well as all its nested events to be recursively correct
for an event to be considered correctly extracted. Our
F-score in terms of the strict matching is only 2.65%
lower than the relaxed, primary measure, indicating
that most of the detected triggers are captured with cor-
rect text span. The organizers also provided the eval-
uation results on PubMed abstracts and PMC full text
articles separately. Our system performs consistently
on both abstracts and full papers and the difference be-
tween F-scores is less than 1% (41.39% vs. 40.47%)
mostly due to the small recall loss on full texts.
Measures R(%) P(%) F(%)
Strict Matching 29.55 55.13 38.48
Appr. SpanNoTrigger/Recur. 33.68 62.17 43.69
Appr. Span/Recur./Decomp. 32.56 66.20 43.65
Appr. Sp. No T./Recur./Decomp. 34.96 69.87 46.60
Appr. Span/Recur. (Abstract) 31.87 59.02 41.39
Appr. Span/Recur. (Full paper) 30.82 58.92 40.47
Table 4: GE results on testing set by other evaluation measures
5.2 EPI task
5.2.1 Preprocessing Results
Table 5 presents some statistics of the datasets. We
were able to build event rules for 1598 gold events. Af-
ter removing duplicate rules, we obtained 1,562 event
rules distributed over fifteen event types.
Attributes Counted Training Dev. Testing
Abstracts 600 200 440
Total sentences 6,411 2,218 4,640
Candidate sentences 1,054 1,241 2,839
Total events 1,738 582 1,194
Sentence-based events 1,643 536 hidden
Table 5: Statistics of EPI dataset
We processed the obtained rules following the
same rule refining and ranking processes of the GE
task. We experimented with two graph matching
criteria for extracting EPI events, ?E+P*+T*? and
?E+P*+A*?. From the preliminary results, we ob-
served that ?E+P*+A*? achieves a high precision over
80% but a lower recall around 33%. Compared to
the GE task results, ?E+P*+T*? achieves a better re-
call against a small tradeoff for precision. We consider
that this is because the event triggers themselves for
the EPI task such as ?acetylation?, ?deglycosylation?
and ?demethylation? are powerful enough to differen-
tiate among event types without the need to resort to
more contextual content of the patterns. Therefore, we
focused on using ?E+P*+T*? to extract events.
5.2.2 Recall-oriented rule merging
Since all the event types except Catalysis,
DNA methylation and DNA demethylation in the
169
EPI task involve addition or removal of biochemical
functional groups at a particular amino acid residue of
a protein (Hunter, 2009), common syntactic structures
of expressing the protein PTM events might be shared
across event types. To further improve the recall, we
proposed a rule merging strategy to take advantage of
the syntactic structures of rules across event types.
We first experimented with a ?pairwise flip? ap-
proach which combines rules of the pairwise, positive
and negative event types by flipping the type and the
trigger of event rules. For instance, the event rules
of Phosphorylation and Dephosphorylation are merged
together and then used to detect events of the two types
respectively.
Next, the ?pairwise flip? approach was extended to
an ?all in one? method. For one event type, the rules
of all other PTM event types are processed and merged
into the rules of the current type if the trigger of rules
of other types contains one of these 12 morphemes:
?acetyl?, ?glycosyl?, ?hydroxyl?, ?methyl?, ?phospho-
ryl?, ?ubiqui?, ?deacetyl?, ?deglycosyl?, ?dehydroxyl?,
?demethyl?, ?dephosphoryl?, ?deubiqui?. We consider
that event rules involving these morphemes in trigger
are more likely to discuss representative protein post-
translational modifications.
5.2.3 EPI Results on Development Set
Table 6 shows the event extraction results on the de-
velopment data using different matching criteria and
rule merging methods. The performance is evaluated
by the primary evaluation measure.
Feature Recall(%) Prec.(%) F(%)
E+P*+A* 32.65 79.83 46.34
E+P*+T* 38.14 73.51 50.23
E+P*+A*(pairwise) 35.22 80.39 48.98
E+P*+T*(pairwise) 40.89 77.52 53.54
E+P*+T*(all in one) 46.39 63.08 53.47
Table 6: EPI results on development set
The two rule merging methods using ?E+P*+T*?
outperform others in terms of F-score. The ?pairwise
flip? method achieves higher precision as the syntac-
tic structures of rules to describe the pairwise, positive
and negative events tend to be highly similar. However,
when merging all the rules across PTM event types,
although more events are captured, rules that involve
syntactic structures for expressing very specific events
of certain types may not generalize well on some other
types, resulting in incorrect events. Thus, the ?all in
one? approach significantly improves the recall while
producing many false positive events, leading to a F-
score comparable with the ?pairwise flip? method.
5.2.4 EPI Results on Testing Set
We conducted two runs on the testing data in terms
of ?E+P*+T*(pairwise)? and ?E+P*+T*(all in one)?.
Since the two rule merging methods achieve compara-
ble F-scores, we decided to submit a run with higher
recall. Table 7 shows our results of ?E+P*+T*? using
the ?all in one? approach on the official metrics. Only
7 teams participated in this task. For the core task, our
performance ranked 7th, only 0.16% lower in F-score
than the 6th team. When evaluating our results in terms
of the full task, we ranked 6th.
Feature Recall(%) Prec.(%) F(%)
E+P*+T*(core task) 45.06 63.37 52.67
E+P*+T*(full task) 23.44 37.93 28.97
Table 7: EPI results on testing set
Compared to the top teams, our F-score is mostly af-
fected by the lower recall. Although the run we submit-
ted achieves the highest recall among all our runs, our
recall is about 20% less than the best performing sys-
tem. Considering that most of the event types of the EPI
task tend to use tokens containing only a small fixed
set of domain-specific morphemes as triggers, the re-
call deficit is assumed to be lack of event rules that de-
scribe syntactic structures of expressing a fair amount
of EPI events.
5.3 Error Classification
Since the gold event annotation of the testing data is
hidden, we examined the event extraction results of the
development data to analyze the underlying errors. The
detailed analysis is reported in terms of false negative
and false positive events.
5.3.1 False negatives
It is shown that false negative events have a substan-
tial impact on the performance of all 15 participating
teams of the GE task. The best recall, 49.56%, cap-
tures less than half of the gold events in the testing set.
In our work, three major causes of false negatives are
determined for both tasks.
(1) Low coverage of rule set: For the GE task, the
graph matching criteria ?E+P*+A*? strictly asks every
lemmatized token in the patterns to find its exact match
in the input sentences. Although maintaining the pre-
cision at a high level, this directly limits the contextual
170
structure and content around the proteins and thus pre-
vents the recall from being higher.
Lemmatization helps to detect more events, however,
further generalization needs to be performed on the ex-
isting rules to relax the token matching requirement.
For instance, when ?lysine? appears in an event rule,
knowing that ?lysine? is an amino acid, the rule might
be further generalized to allow all amino acids to match
with each other in order to recognize more events.
For the EPI task, although ?E+P*+T*? requires to-
kens of only event triggers to be identical, we captured
less than half of the gold events. We noticed that many
trigger tokens in the development sentences do not ap-
pear as triggers in the training set. This leads to the
failure of extracting the corresponding events. Since
the training data is the only source of triggers in our
work, the coverage of triggers limits the generalization
power of event rules.
For both tasks, we found that many gold events are
described in grammatical structures that are not cov-
ered by the existing rules induced from the training sen-
tences. These structures tend to be more complex, in-
volving a long dependency path from the trigger to ar-
guments in the graphs of sentences. Events that consist
of these structures are not recognized as no matched
rules will be returned from the subgraph matching.
In order to further improve the recall, some post-
processing steps are necessary to be performed on the
raw dependency graphs of both rules and sentences in-
stead of using them in the graph matching directly. By
eliminating semantically unimportant nodes and group-
ing lexically connected nodes together, the rules can
be generalized to retain only their skeleton structures
while complex sentences can be syntactically simpli-
fied to allow event rules to match them.
(2) Compound error effect: In both tasks, reg-
ulation and catalysis event types can take sub-events
as arguments. Therefore, if the nested sub-events are
not correctly identified, the main events will not be ex-
tracted due to the compound error effect.
(3) Anaphora and coreference: Since our system
focuses on extracting events from sentences, events that
contain protein names spanning multiple sentences will
not be captured. Recognition of these events requires
the ability to do anaphora and coreference resolution in
biological text (Gasperin and Briscoe, 2008).
5.4 False positives
Three major causes of false positives are generalized
from our analysis.
(1) Assignment of overlapping event rules: The
conditional probability-based method to assign over-
lapped rules of different event types effectively reduces
the number of event candidates but leads to errors. For
instance, ?methylation? is used as the trigger for two
overlapping rules of DNA methylation and Methyla-
tion. Based on the P (ti|E), ?methylation? is classified
into DNA methylation. An erroneous DNA methylation
event is then detected from a development sentence in-
stead of the gold Methylation event. Although the trig-
ger and the participant are all identified correctly, the
event type is assigned wrongly.
In fact, the same contextual structure and con-
tent appear in both DNA methylation and Methylation
events in the training data. According to the EPI
task (Ohta et al, 2011), Methylation is to abbreviate
for ?protein methylation? and thus is different from
DNA methylation. In this case, the only way to dis-
tinguish between the two types is to identify that the
biological entity mentioned in the sentence is a gene for
DNA methylation and a protein for Methylation. Since
genes and their products are uniformly annotated as
?Protein? in the task, it is not possible to assign a cor-
rect event type in this case from the perspective of the
event extraction itself.
(2) Lack of postprocessing rules: Some misiden-
tified events require customized postprocessing rules.
For instance, a Gene expression event is detected from
the phrase ?Tax expression vector? of a development
sentence. However, since ?Tax expression? is only
used as an adjective to describe ?vector? in this context,
the identified Gene expression event is not appropriate.
Likewise, ?Sp1 transcription? should not be identified
as an event in the context of ?Sp1 transcription factors?.
(4) Inconsistencies in gold annotation: Some ex-
tracted events are considered biologically meaningful
but evaluated as false positives due to the inconsisten-
cies in the gold annotation. In Table 4, the 3.2% in-
crease in precision of the no-trigger evaluation measure
over the primary evaluation scheme indicates that the
inconsistent gold annotations of event triggers.
6 Conclusion and future work
We used dependency graphs to automatically induce
biological event rules from annotated events. We ex-
plored methods such as performance-based rule rank-
ing to improve the accuracy of the obtained rules, and
we merged rules across multiple event types in order to
increase the coverage of the rules. The event extraction
process is treated as a subgraph matching problem to
171
search for the graph of an event rule within the graph of
a sentence. We tackled two main tasks of the BioNLP
Shared Task 2011. We achieved a 41.13% F-score in
detecting events across nine types in the Task 1 of the
GE task, and a 52.67% F-score in identifying events
across fifteen types in the core task of the EPI task.
In future work, we would like to explore the ap-
proaches of generalizing the raw dependency graphs of
both event rules and sentences in order to improve the
recall of our event extraction system. We also plan to
extend our system to tackle the other sub-tasks in GE
and EPI tasks, such as to extract events with additional
arguments like site and location, and to recognize nega-
tions and speculations regarding the extracted events.
References
Muhammad Abulaish and Lipika Dey. 2007. Biological re-
lation extraction and query answering from medline ab-
stracts using ontology-based text mining. Data & Knowl-
edge Engineering, 61(2):228?262.
Sophia Ananiadou and John Mcnaught. 2005. Text Mining
for Biology And Biomedicine. Artech House Publishers.
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for systems bi-
ology by text mining the literature. Trends in Biotechnol-
ogy, 28(7):381?390.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and Udo
Hahn. 2009. Event extraction from trimmed dependency
graphs. In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 19?27, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Luigi P. Cordella, Pasquale Foggia, Carlo Sansone, and
Mario Vento. 2004. A (sub)graph isomorphism algo-
rithm for matching large graphs. IEEE Trans. Pattern
Anal. Mach. Intell., 26(10):1367?1372.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest,
and Clifford Stein. 2001. Introduction to Algorithms.
The MIT Press.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. In COLING
?08: Proceedings of the 22nd International Conference on
Computational Linguistics, pages 257?264, Morristown,
NJ, USA. Association for Computational Linguistics.
Ge?rard P. Huet. 1975. A unification algorithm for typed
lambda-calculus. Theor. Comput. Sci., 1(1):27?57.
Lawrence Hunter. 2009. The Processes of Life: An Intro-
duction to Molecular Biology. The MIT Press.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic de-
pendency based heuristics for biological event extraction.
In Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing: Shared Task,
BioNLP ?09, pages 119?127.
Jin-Dong Kim, Yoshinobu Kano Tomoko Ohta,
Sampo Pyysalo, and Jun?ichi Tsujii. 2009. Overview of
bionlp?09 shared task on event extraction. In Proceedings
of the NAACL-HLT 2009 Workshop on Natural Language
Processing in Biomedicine (BioNLP?09), pages 1?9.
ACL.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview of BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Portland,
Oregon, June. Association for Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori
Yonezawa. 2011b. Overview of the Genia Event task in
BioNLP Shared Task 2011. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational Lin-
guistics.
Haibin Liu, Vlado Keselj, and Christian Blouin. 2010. Bio-
logical event extraction using subgraph matching. In Pro-
ceedings of the 4th International Symposium on Semantic
Mining in Biomedicine (SMBM-2010), October.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contribu-
tions of natural language parsers to protein?protein inter-
action extraction. Bioinformatics, 25(3):394?400.
Tomoko Ohta, Yuka Tateisi, and Junichi Tsujii. 2005. Syn-
tax annotation for the genia corpus. In Proceedings of the
IJCNLP 2005, pages 222?227.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational Mod-
ifications (EPI) task of BioNLP Shared Task 2011. In
Proceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. Associ-
ation for Computational Linguistics.
Mate Ongenaert, Leander Van Neste, Tim De Meyer, Ger-
ben Menschaert, Sofie Bekaert, and Wim Van Criekinge.
2007. Pubmeth: a cancer methylation database combin-
ing text-mining and expert annotation. Nucleic Acids Re-
search, pages 1?5.
Marcello Pelillo, Kaleem Siddiqi, and Steven W. Zucker.
1999. Matching hierarchical structures using associa-
tion graphs. IEEE Trans. Pattern Anal. Mach. Intell.,
21(11):1105?1120.
M. F. Porter. 1997. An algorithm for suffix stripping. pages
313?316.
Yuanyuan Tian, Richard C. Mceachin, Carlos Santos,
David J. States, and Jignesh M. Patel. 2007. Saga: a
subgraph matching tool for biological graphs. Bioinfor-
matics, 23(2):232?239.
J. R. Ullmann. 1976. An algorithm for subgraph isomor-
phism. J. ACM, 23(1):31?42.
Xifeng Yan, Feida Zhu, Jiawei Han, and Philip S. Yu. 2006.
Searching substructures with superimposed distance. In
ICDE ?06: Proceedings of the 22nd International Con-
ference on Data Engineering, page 88, Washington, DC,
USA. IEEE Computer Society.
172
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35?44,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Extracting Biomedical Events and Modifications Using Subgraph
Matching with Noisy Training Data
Andrew MacKinlay?, David Martinez?, Antonio Jimeno Yepes?,
Haibin Liu?, W. John Wilbur? and Karin Verspoor?
? NICTA Victoria Research Laboratory, University of Melbourne, Australia
{andrew.mackinlay, david.martinez}@nicta.com.au
{antonio.jimeno, karin.verspoor}@nicta.com.au
? National Center for Biotechnology Information, Bethesda, MD, USA
haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov
Abstract
The Genia Event (GE) extraction task of
the BioNLP Shared Task addresses the ex-
traction of biomedical events from the nat-
ural language text of the published litera-
ture. In our submission, we modified an
existing system for learning of event pat-
terns via dependency parse subgraphs to
utilise a more accurate parser and signifi-
cantly more, but noisier, training data. We
explore the impact of these two aspects of
the system and conclude that the change in
parser limits recall to an extent that cannot
be offset by the large quantities of training
data. However, our extensions of the sys-
tem to extract modification events shows
promise.
1 Introduction
In this paper, we describe our submission to the
Genia Event (GE) information extraction subtask
of the BioNLP Shared Task. This task requires the
development of systems that are capable of iden-
tifying bio-molecular events as those events are
expressed in full-text publications. The task rep-
resents an important contribution to the broader
problem of converting unstructured information
captured in the biomedical literature into struc-
tured information that can be used to index and
analyse bio-molecular relationships.
This year?s task builds on previous instantia-
tions of this task (Kim et al, 2009; Kim et al,
2012), with only minor changes in the task defini-
tion introduced for 2011. The task organisers pro-
vided full text publications annotated with men-
tions of biological entities including proteins and
genes, and asked participants to provide annota-
tions of simple events including gene expression,
binding, localization, and protein modification, as
well as higher-order regulation events (e.g., pos-
itive regulation of gene expression). In our sub-
mission, we built on a system originally developed
for the BioNLP-ST 2011 (Liu et al, 2011) and ex-
tended in more recent work (Liu et al, 2013a; Liu
et al, 2013b). This system learns to recognise sub-
graphs of syntactic dependency parse graphs that
express a given bio-molecular event, and matches
those subgraphs to new text using an algorithm
called Approximate Subgraph Matching.
Due to the method?s fundamental dependency
on the syntactic dependency parse of the text, in
this work we set out to explore the impact of
substituting the previously employed dependency
parsers with a different parser which has been
demonstrated to achieve higher performance than
other commonly used parsers for full-text biomed-
ical literature (Verspoor et al, 2012).
In addition, we aimed to address the relatively
lower recall of the method through incorporation
of large quantities of external training data, ac-
quired through integration of previously automat-
ically extracted bio-molecular events available in
a web repository of such extracted events, EVEX
(Van Landeghem et al, 2011; Van Landeghem
et al, 2012), and additional bio-molecular events
generated from a large sample of full text pub-
lications using one of the state-of-the-art event
extraction systems, TEES (Bjo?rne and Salakoski,
2011). Since the performance of the subgraph
matching method, as an instance-based learning
strategy (Alpaydin, 2004), is dependent on having
good training examples that express the events in a
range of syntactic structures, the motivation under-
lying this was to increase the amount of training
data available to the system, even if that data was
derived from a less-than-perfect source. The aug-
mentation of training corpora with external unla-
belled data that is automatically processed to gen-
erate additional labels has been explored for re-
training the same system, in an approach known as
self-training. This approach has been shown to be
35
very effective for improving parsing performance
(McClosky et al, 2006; McClosky and Charniak,
2008). Self-training of the TEES system has been
previously explored (Bjorne et al, 2012), with
somewhat mixed results, but with evidence sug-
gesting it could be useful with an appropriate strat-
egy for selecting training examples. Here, rather
than training our system with its own output over
external data, we explore a semi-supervised learn-
ing approach in which we train our system with the
outputs of a different system (TEES) over external
data.
2 Methodology
2.1 Base Event Extraction System
The event extraction algorithm is essentially the
same as the one used in Liu et al (2013b). A fuller
description can be found there, but we summarise
the most important aspects of it here.
2.1.1 Event Extraction with ASM
The principal method used in event extraction is
Approximate Subgraph Matching, or ASM (Liu et
al., 2013a). Broadly, we learn subgraph patterns
from the event structures in the training data, and
then apply them by looking for matches with the
patterns of the learned rules, using ASM to allow
for non-exact matches of the patterns.
The first stage in this is learning the rules which
link subgraphs to associated patterns. The input
is a set of dependency-parsed articles (the setup
is described in ?2.1.2), and a set of gold-standard
annotations of proteins and events in the shared
task format. Using the standoff annotations in the
training data, every protein and trigger is mapped
to one or more nodes in the corresponding depen-
dency graphs. In addition, the textual content of
every protein is replaced with a generic string en-
abling abstraction over individual protein names.
Then, for each event annotation in the training
data, we retrieve the nodes from the graph corre-
sponding to the associated trigger and protein en-
tities. We determine the shortest path (or paths, in
case of a tie) connecting the graph trigger to each
of the event argument nodes. For arguments which
are themselves events (e.g., for regulatory events),
the node corresponding to the trigger of the event
argument is used instead of a protein node. Where
there are multiple arguments, we take the union of
the shortest paths to each individual argument.
This path is then used as the pattern compo-
nent of an event rule. The rule also consists of an
event type, and a mapping from event arguments
to nodes from the pattern graph, or to an event
type/node pair for nested event arguments. Af-
ter processing all training documents, we get on
the order of a few thousand rules; this can be de-
creased slightly by removing rules with subgraphs
that are isomorphic to those of other rules.
In principle, this set of rules could then be di-
rectly applied to the test documents, by searching
for any matching subgraphs. However, in practice
doing so leads to very low recall, since the pat-
terns are not general enough to get a broad range of
matches on new data. We can alleviate this by re-
laxing the strictness of the subgraph matching pro-
cess. Most basically, we relax node matching. In-
stead of requiring an exact match between both the
token and the part-of-speech of the nodes of the
sentence graph and those from the rule subgraph,
we also allow a match on the basis of the lemma
(according to BioLemmatizer (Liu et al, 2012)),
and a coarse-grained POS-tag (where there is only
one POS-tag for nouns, verbs and adjectives).
More importantly, we also relax the require-
ments on how closely the graphs must match, by
using ASM. ASM defines distances measures be-
tween subgraphs, based on structure, edge labels
and edge directions, and uses a set of specified
weights to combine them into an overall subgraph
distance. We have a pre-configured set of distance
thresholds for each event type, and for each sen-
tence/rule pairing, we extract events for any rules
with subgraphs under the given threshold.
The problem with this approximate matching is
that some rules now match too broadly, and pre-
cision is reduced. This is mitigated by adding
an iterative optimisation phase. In each iteration,
we run the event extraction using the current rule
set over some dataset ? usually the training set,
or a subset of it. We check the contribution of
each rule in terms of postulated events and actual
events which match the gold standard. If the ra-
tio of matched to postulated events is too low (for
the work reported here, the threshold is 0.25), the
rule is discarded. This process is repeated until no
more rules are discarded. This can take multiple
iterations since the rules are interdependent due to
the presence of nested event arguments.
The optimisation step is by far the most time-
consuming step of our process, especially for the
large rule sets produced in some configurations.
36
We were able to improve optimisation times some-
what by parallelising the event extraction, and
temporarily removing documents with long ex-
traction times from the optimisation process un-
til as late as possible, but it remained the primary
bottleneck in our experimentation.
2.1.2 Parsing Pipeline
In our parsing pipeline, we first split sentences
using the JULIE Sentence Boundary Detector, or
JSBD (Tomanek et al, 2007). We then parse
using a version of clearnlp1 (Choi and McCal-
lum, 2013), a successor to ClearParser (Choi and
Palmer, 2011), which was shown to have state-
of-the-art performance over the CRAFT corpus
of full-text biomedical articles (Verspoor et al,
2012). We use dependency and POS-tagging mod-
els trained on the CRAFT corpus (except where
noted); these pre-trained models are provided with
clearnlp. Our fork of clearnlp integrates to-
ken span marking into the parsing process, so the
dependency nodes can easily be matched to the
standoff annotations provided with the shared task
data. This pipeline is not dependent on any pre-
annotated data, so can thus be trivially applied to
extra data not provided as part of the shared task.
In addition the parsing is fast, requiring roughly 46
wall-clock seconds (processing serially) to parse
the 5059 sentences from the training and develop-
ment sets of the 2013 GE task ? an average of 9 ms
per sentence. The ability to apply the same pars-
ing configuration to new text was useful for adding
extra training data, as discussed in ?2.2.
The usage of clearnlp as the parser is the pri-
mary point of difference between our system and
that of Liu et al (2013b), who use the Charniak-
Johnson parser with the McClosky biomedical
model (CJM; McClosky and Charniak (2008)), al-
though there are other minor differences in tokeni-
sation and sentence splitting. We expected that the
higher accuracy of clearnlp over biomedical text
would translate into increased accuracy of event
detection in the shared task; we consider this ques-
tion in some detail below.
2.2 Adding Noisy Training Data
One of the limitations of the ASM approach is that
the high precision comes at the cost of lower re-
call. Our hypothesis is that adding extra training
instances, even if some are errors, will raise re-
call and improve overall performance. We utilised
1https://code.google.com/p/clearnlp/
two sources of automatically-annotated data: the
EVEX database, and running an automatic event
annotator over documents from PubMed Central
(PMC) and MEDLINE.
To test our hypothesis, we utilise one of the
best performing automatic event extractors in pre-
vious BioNLP tasks: TEES (Turku Event Extrac-
tion System)2 (Bjo?rne et al, 2011). We expand our
pool of training examples by adding the highest-
confidence events TEES identifies in unlabelled
text. We explored different approaches to ranking
events based on classifier confidence empirically.
TEES relies on multi-class SVMs both for trig-
ger and event classification, and produces confi-
dence scores for each prediction. We explored
ranking events according to: (i) score of the trig-
ger prediction, (ii) score of the event-type predic-
tion, and (iii) sum of trigger and event type predic-
tions. We also compared the performance when
selecting the top-k events overall, versus choos-
ing the top-k events for each event type. We also
tested adding as many instances per event-type as
there were in the manually-annotated dataset, with
different multiplying factors. Finally, we evalu-
ated the effect of using different splits of the data
for the evaluation and optimisation steps of ASM.
This is the full list of parameters that we tested
over held-out data:
? Original confidence scores: we ranked events
according to the three SVM scores mentioned
above: trigger prediction, event-type predic-
tion, and combined.
? Overall top-k: we selected the top 1,000,
5,000, 10,000, 20,000, 30,000, 40,000, and
50,000 for the different experimental runs.
? Top-k per type: for each event type, we se-
lected the top 400, 1,000, and 2,000.
? Training bias per type: we add as many in-
stances from EVEX per type as there are in
the manually annotated data. We experiment
with adding up to 6 times as many as in man-
ually annotated data.
? Training/optimisation split: we combine
manually and automatically annotated data
for training. For optimisation we tested
different options: manually annotated only,
manual + automatic, manual + top-100
events, and manual + top-1000 events.
2http://jbjorne.github.com/TEES/
37
We did not explore all these settings exhaus-
tively due to time constraints, and we report here
the most promising settings. It is worth mention-
ing that most of the configurations contributed to
improve the baseline performance. We only ob-
served drops when using automatically-annotated
data in the optimisation step.
2.2.1 Data from EVEX
Conveniently, the developers of TEES have re-
leased the output of their tool over the full 2009
collection of MEDLINE, consisting of abstracts of
biomedical articles, in a collection known as the
EVEX dataset. We used the full EVEX dataset as
provided by the University of Turku, and explored
different ways of ranking the full list of events as
described above.
2.2.2 Data from TEES
To augment the training data, we annotated two
data sets with TEES based on MEDLINE and
PubMed Central (PMC). The developers of TEES
released a trained model for the GE 2013 training
data that we utilised.
Due to the long pre-processing time of TEES,
which includes gene named entity recognition,
part-of-speech tagging and parsing, we used the
EVEX pre-processed MEDLINE, which required
some adaptation of the EVEX XML to the XML
format accepted by TEES. Once this adaptation
was finished, the files were processed by TEES.
Then, we have selected articles from PMC us-
ing a query containing specific MeSH headings
related to the GE task and limiting the result to
only the Open Access part of PMC. From the al-
most 600k articles from the PMC Open Access set,
we reduced the total number of articles to around
155k. The PMC query is the following:
(Genetic Phenomena[MH] OR Metabolic
Phenomena[MH] OR Cell Physiological
Phenomena[MH] OR Biochemical
Processes[MH]) AND open access[filter]
Furthermore, the articles were split into sections
and specific sections from the full text like Intro-
duction, Background and Methods were removed
to reduce the quantity of text to be annotated by
TEES. The PMC files produced by this filtering
were processed by TEES on the NICTA cluster.
2.3 Modification Detection
To evaluate the utility of ASM for a diverse range
of tasks, we also applied it to the task of detect-
ing modification (SPECULATION or NEGATION)
NEGATION cues
? Basic: not, no, never, nor, only, neither, fail, cease,
stop, terminate, end, lacking, missing, absent, absence,
failure, negative, unlikely, without, lack, unable
? Data-derived: any, prevention, prevent, disrupt, dis-
ruption
SPECULATION cues:
? Basic: analysis, whether, may, should, can, could, un-
certain, questionable, possible, likely, probable, prob-
ably, possibly, conceivable, conceivably, perhaps, ad-
dress, analyze, analyse, assess, ask, compare, consider,
enquire, evaluate, examine, experiment, explore, inves-
tigate, test, research, study, speculate
? Data-derived: measure, measurement, suggest, sug-
gestion, value, quantify, quantification, determine, de-
termination, detect, detection, calculate, calculation
Table 1: Modification cues
of events. In event detection, triggers are explic-
itly annotated, so the linguistic cue which indi-
cates that an event is occurring is easy to identify.
As described in Section 3.2, these triggers are im-
portant for learning event patterns.
The event extraction method is based on paths
between dependency graph nodes, so it is neces-
sary to have at least two relevant graph nodes be-
fore we can determine a path between them. For
learning modification rules, one graph node is the
trigger of the event which is subjec to modifica-
tion. However here we needed a method to deter-
mine another node in the sentence which provided
evidence that NEGATION or SPECULATION was
occurring, and could thus form an endpoint for a
semantically relevant graph pattern. To achieve
this, we specified a set cue lemmas for NEGATION
and SPECULATION. The basic set of cue lemmas
came from a variety of sources. Some were man-
ually specified and some were derived from previ-
ous work on modification detection (Cohen et al,
2011; MacKinlay et al, 2012). We manually ex-
panded this cue list to include obvious derivational
variants. This gave us a basic set of 34 SPECULA-
TION and 21 NEGATION cues.
We also used a data-driven strategy to find ad-
ditional lemmas indicative of modification. We
adapted the method of Rayson and Garside (2000)
which uses log-likelihood for finding words that
characterise differences between corpora. Here,
the ?corpora? are sentences attached to all events
in the training set, and sentences attached to events
which are subject to NEGATION or SPECULATION
(treated separately). We build a frequency distri-
bution over lemmas in each set of sentences, and
calculate the log-likelihood for all lemmas, us-
38
ing the observed frequency from the modification
events and the expected frequency over all events.
Sorting by decreasing log-likelihood, we get a
list of lemmas which are most strongly associated
with NEGATION or SPECULATION. We manually
examined the highest-ranked lemmas from these
two lists and noted lemmas which may occur,
according to human judgment, in phrases which
would denote the relevant modification type. We
found seven extra SPECULATION cues and three
extra NEGATION cues. Expanding with morpho-
logical variants as described above yielded 47
SPECULATION cues and 26 NEGATION cues to-
tal. These cues are shown, divided into basic and
data-derived, in Table 1.
For every node N with a lemma in the appro-
priate set of cue lemmas, we create a rule based
on the shortest path between the cue lemma node
N and the event trigger node. The trigger lem-
mas are replaced with generic lemmas which only
reflect the POS-tag of the trigger, to broaden the
range of possible matches. Each rule thus consists
of the POS-tag of an event trigger, and a subgraph
pattern including the abstracted event trigger node.
At modification detection time, the rules are ap-
plied in a similar way to the event rules. After
detecting events, we look for matches of each ex-
tracted event with every modification rule. A rule
R is considered to match if the event trigger node
POS tag matches the POS tag of the rule, and the
subgraph pattern of the rule matches the graph of
the sentence, including a node corresponding to
the event trigger node. If R is found to match
for a given event and sentence, any events which
have the trigger defined in the rule are marked as
SPECULATION or NEGATION as appropriate. As
in event extraction, we use ASM to allow a looser
match between graphs, but initial experimentation
showed that increasing the match thresholds be-
yond a relatively small distance was detrimental.
We have not yet added an optimisation phase for
modification, which might allow larger ASM dis-
tance threshold to have more benefit.
3 Results
We present our results over development data,
and the official test. We report the Approximate
Span/Approximate Recursive metric in all our ta-
bles, for easy comparison of scores. We describe
the data split used for development, explain our
event extraction results, and finally describe our
performance in modification detection.
3.1 Data division for development
In the data provided by the task organisers, the
split of data between training and development
sets, with 249 and 222 article sections respec-
tively, was fairly even. If we had used such a split,
we would have had an unfeasibly small amount
of data to train from during development, and
possible unexpected effects when we sharply in-
creased the amount of training data for running
over the held-out test set. We instead used our
own data set split during development, pooling
the provided training and development sets, and
randomly selecting six PMC articles (PMC IDs
2626671, 2674207, 3062687, 3148254, 3333881
and 3359311) for the development set, with the
remainder available for training. We respected ar-
ticle boundaries in the new split to avoid training
and testing on sentences taken from different sec-
tions of the same article. Results over the devel-
opment set reported in this section are over this
data split. We will refer to our training subset as
GE13tr, and to the testing subset as GE13dev.
For our runs over the official test of this chal-
lenge, we merged all the manually annotated data
from 2013 to be used as training. We also per-
formed some experiments with adding the exam-
ples from the 2011 GE task to our training data.
3.2 Event Extraction
For our first experiment, we evaluated the contri-
bution of the automatically annotated data over us-
ing GE13tr data only. We performed a set of ex-
periments to explore the parameters described in
Section 2.2 over two sources of extra examples:
EVEX and TEES.
Using EVEX data in training resulted in clear
improvements in performance when only manu-
ally annotated data was consulted for optimisa-
tion. The increase was mainly due to the better
recall, with small variations in precision over the
baseline for the majority of experiments. Our best
run over the GE13dev data followed this setting:
rank events according to trigger scores, include all
top-30000 events (without considering the types of
the events), and use only manually annotated data
for the optimisation step. Other settings also per-
formed well, as we will see below.
For TEES, we selected noisy examples from
MEDLINE and PMC to be used as additional
39
System Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+TEES 59.27 29.89 39.74
+TEES +EVEX (top5k) 46.93 30.78 37.18
+TEES +EVEX (top20k) 56.32 31.90 40.73
+TEES +EVEX (top30k) 55.34 32.48 40.93
+TEES +EVEX (pt1k) 58.54 30.96 40.50
+TEES +EVEX (trx4) 57.83 31.23 40.56
Table 2: Impact of adding extra training data to the
ASM method. top5k,20k,30k: using the top 5,000,
20,000, and 30,000 events. pt1k: using the top
1,000 events per event-type. trx4: following the
training bias of events, with a multiplying factor
of four. For TEES we always use the top 10,000
events. Evaluated over GE13dev.
training data. Initial results showed that when us-
ing only MEDLINE annotated data in the train-
ing step, the performance decreased compared to
not using any additional data. This might have
been due to differences between the EVEX pre-
processed data that we used and what TEES was
expecting, so the MEDLINE set was not consid-
ered for further experimentation. Using PMC ar-
ticles annotated with TEES in the training step se-
lected by the evidence score of TEES shows an in-
crease of recall while slightly decreasing the pre-
cision, which was expected. We selected the top
10000 events from the PMC set based on the evi-
dence score as additional training data.
Table 2 summarises the results of combin-
ing different settings of EVEX with TEES. We
achieve a considerable boost in recall, at the cost
of precision for most configurations. The only set-
ting where there is a slight drop in F-score is the
experiment with only 5000 events from EVEX; in
the remaining runs we are able to alleviate the drop
in precision, and improve the F-score. Consider-
ing the addition of top-events according to their
type, the increment in recall is slightly lower, but
these runs are able to reach similar F-score to the
best ones, using less training data. Results with
TEES might be slightly overoptimistic since the
PMC annotation is based on a TEES model trained
on the 2013 GE data and our configurations are
evaluated on a subset of this data.
For our next experiment, we tested the contribu-
tion of adding the dataset from the 2011 GE task
to the training dataset. We use this data both in
the training and optimisation steps. The results are
Train Prec. Rec. F-sc.
GE13tr 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
Table 3: Adding GE11 data to the training and op-
timisation steps. Evaluated over GE13dev.
Parser Train Prec. Rec. F-sc.
clearnlp
GE13 60.40 27.02 37.34
+GE11 53.41 32.62 40.50
CJM
GE13 60.96 33.11 42.91
+GE11 64.11 38.93 48.44
Table 4: Performance depending on the applied
parsing pipeline (clearnlp for this work against
the CJM pipeline of Liu et al (2013b)) over
GE13dev. For each run, the available data was
used both in training and optimisation.
given in Table 3, where we can observe a boost in
recall at the cost of precision. Overall, the im-
proved F-score suggests that this dataset would
make a useful contribution to the system.
We also compared our system to that of Liu
et al (2013b), where the primary difference
(although not the only difference, as noted in
?2.1.2) is the use of clearnlp instead of the CJM
(Charniak-Johnson/McClosky) pipeline. It is thus
somewhat surprising to see in Table 4 that the
CJM pipeline outperforms our clearnlp pipeline
by 5.5?8% in F-score, depending on the train-
ing data. For the smaller GE13-only training set,
the gap is smaller, and the precision figures are
in fact comparable. However, the recall is uni-
formly lower, suggesting that the rules learned
from clearnlp parses are for some reason less gen-
erally applicable. Another interesting difference
is that our clearnlp pipeline gets a smaller benefit
from the addition of the GE11 training data. We
consider possible reasons for this in ?4.1.
Table 5 contains the evaluation of different ex-
periments on the official test data. We tested the
baseline system using the training and develop-
ment data from 2011 and 2013 GE tasks and the
addition of TEES and EVEX data. The additional
data improves the recall slightly compared to not
using it, while, as expected, it decreases the pre-
cision. Table 5 also shows the results for our of-
ficial submission (+TEES+EVEX sub), which due
to time constraints was a combination of the opti-
mised rules of different data splits and has a lower
40
Train Prec. Rec. F-sc.
GE11, GE13 65.71 32.57 43.55
+TEES+EVEX 63.67 33.50 43.91
+TEES+EVEX * 50.68 36.99 42.77
Table 5: Test set results, always optimised over
gold data only. * denotes the official submission.
performance compared to the other results.
3.3 Modification Detection
We show results for selected modification detec-
tion experiments in Table 6. In all cases we used
all of the available gold training data from the
GE11 and GE13 datasets. To assess the impact of
modification cues, we show results using the basic
set as well as with the addition of the data-derived
cues. It has often been noted (MacKinlay et al,
2012; Cohen et al, 2011) that modification detec-
tion accuracy is strongly dependent on the quality
of the upstream event annotation, so we provide an
oracle evaluation, using gold-standard event anno-
tations rather than automatic output.
The performance over the automatically-
annotated runs is respectable, given that the recall
is fundamentally limited by the recall of the input
event annotations, which is only around 30% for
the configurations shown. With the oracle event
annotations, the results improve substantially,
with considerable gains in precision, and recall
increasing by a factor of 4?6. This boost in recall
in particular is more than we would naively expect
from the roughly threefold increase in recall over
the events. It seems that many of the modification
rules we learned were even more effective over
events which our pipeline was unable to detect.
The modification rules were learned from oracle
event data, but this does not fully explain the
discrepancy. Regardless, our algorithm for mod-
ification detection showed excellent performance
over the oracle annotations. Over the 2009 version
of the BioNLP shared task data, MacKinlay et al
(2012) report F-scores of 54.6% for NEGATION
and 51.7% for SPECULATION. These are not
directly comparable with those in Table 6, but
running our newer algorithm over the same 2009
data gives F-scores of 84.2% for NEGATION and
69.1% for SPECULATION.
For the official run, which conflates event
extraction and modification detection accuracy,
our system was ranked third for NEGATION and
SPECULATION out of the three competing teams,
although the other teams had event extraction F-
scores of roughly 8% higher than our system. For
SPECULATION, our system had the highest preci-
sion of 34.15%, while the F-score of 20.22% was
close to the best result of 23.92%. Our NEGA-
TION detection was less competitive, with an F-
score of 20.94% ? roughly 6% lower than the other
teams. We cannot extrapolate directly from the or-
acle evaluation in Table 6, but it seems to indicate
that an increase in event extraction accuracy would
have flow-on benefits in modification detection.
4 Discussion
4.1 Detrimental Effects of Parser Choice
The biggest surprise here was that clearnlp, a
more accurate dependency parser for the biomed-
ical domain, as evaluated on the CRAFT tree-
bank, gave a substantially lower event extrac-
tion F-score than the CJM parser. To determine
whether preprocessing caused the differences, we
replaced the existing modules (sentence-splitting
from JSBD and tokenisation/POS-tagging from
clearnlp) with the BioC-derived versions from the
CJM pipeline, but this yielded only an insignifi-
cant decrease in accuracy.
Over the same training data, the optimised rules
from CJM have an average of 2.6 nodes per sub-
graph path, compared to 3.9 nodes per path using
clearnlp. A longer path is less likely to match
than a shorter path, so this may help to explain
the lower generalisability of the clearnlp-derived
rules. While it is possible for a longer subgraph
to match just as generally, if the test sentences
are parsed consistently, in general there are more
nodes and edges which can fail to match due to mi-
nor surface variations. One way to mitigate this is
to raise the ASM distance thresholds to compen-
sate for this; preliminary experiments suggest it
would provide a small (? 1%) boost in F-score but
this would not close the gap between the parsers.
Both parsers produce outputs with Stanford
Dependency labels (de Marneffe and Manning,
2008), so we might naively expect similar graph
topology and subgraph pattern lengths. However,
the CJM pipeline produces graphs in the ?CCpro-
cessed? SD format, which are simpler and denser.
If a node N has a link to a node O with a conjunc-
tion link to another node P (from e.g. and), an ex-
tra link with the same label is added directly from
N to P in the CCprocessed format. This means
41
NEGATION SPECULATION
Eval Events (F-sc) Cues P / R / F P / R / F
Dev
GE13+TEES+EVEX (40.93) Basic 32.69 / 13.71 / 19.32 37.04 / 14.49 / 20.83
GE13+TEES+EVEX (40.93) B + Data 32.69 / 12.88 / 18.48 39.71 / 17.20 / 24.00
Oracle (100.0) B + Data 82.48 / 71.07 / 76.35 78.79 / 67.71 / 72.83
Test
GE11+GE13 (43.55) B + Data 39.53 / 13.99 / 20.66 50.00 / 13.85 / 21.69
GE11+GE13+TEES+EVEX * (42.77) B + Data 32.76 / 15.38 / 20.94 34.15 / 14.36 / 20.22
Table 6: Results for SPECULATION and NEGATION using automatically-annotated events (showing the
F-score of the configuration), as well as using oracle event annotations from the gold standard, over our
development set and the official test set. Rules are learned from GE13+GE11 gold data (excluding any
test data). Cues for learning rules are either the basic manually-specified set (34 SPEC/21 NEG) or the
augmented set with data-driven additions (47 SPEC/26 NEG). * denotes the official submission.
there are more direct links in the graph, match-
ing the semantics more closely. The shortest path
fromN to P is now direct, instead of viaO, which
could enable the CJM pipeline to produce more
general rules.
To evaluate how much this detrimentally af-
fects the clearnlp pipeline, as a post hoc in-
vestigation, we implemented a conversion mod-
ule. Using Stanford Dependency parser code,
we replicated the CCprocessed conversion on the
clearnlp graphs, reducing the average subgraph
pattern length to 2.8, and slightly improving ac-
curacy. Over our development set, compared to
the results in Table 3 it gave a 0.7% absolute F-
score boost over using GE13 training-data only,
and 1.1% over using GE11 and GE13 training data
(in both cases improving recall). Over the test
set, the improvement was greater, with a P/R/F
of 35.66/64.99/46.05, a 2.5% increase in F-score
compared to the results in Table 5 and only 2.9%
less than the official Liu et al (2012) submission.
Clearly some of the inter-parser discrepancies
are due to surface features and post-processing,
and as noted above, we can also achieve small im-
provements by relaxing ASM thresholds, so some
problems may be caused by the default parameters
being suboptimal for the parser. However, the ac-
curacy is still lower where we would expect it to
be higher, and this remaining discrepancy is diffi-
cult to explain without performing a detailed error
analysis, which we leave for future work.
4.2 Effect of additional data
Our initial intuition that using additional noisy
training data during the training of the system
would improve the performance is supported by
the results in Table 2. Table 3 shows that us-
ing a larger set of manually annotated data based
on 2011 GE task data also improves performance.
However, these tables also indicate that adding
manually annotated data produces an increase in
performance comparable to adding the noisy data,
despite its smaller size, and when using this man-
ually annotated set together with the noisy data,
the improvement resulting from the noisy data is
smaller (Table 5). Noisy data was only used dur-
ing training, which limits its effectiveness?any
rule extracted from automatically acquired anno-
tations that are not seen during optimisation of the
rule set will have a lower weight. On the other
hand, we found that using noisy data for optimi-
sation seemed to decrease performance. Together,
these results suggest that studying strategies, pos-
sibly self-training, for selection of events from the
noisy data to be used during rule set optimisation
in the ASM method are warranted.
5 Conclusion
Using additional training data, whether manually
annotated or noisy, improves the performance of
our baseline event extraction system. The gains
that we achieved by adding training data, however,
were outweighed by a loss of performance due to
our parser substitution, with longer dependency
subgraphs limiting rule generalisability the most
likely explanation. Our experiments demonstrate
that while a given parser might be ?better? in one
evaluation context, that advantage may not trans-
late to improved performance in a downstream
task that depends strongly on the parser output.
We presented an extension of the subgraph match-
ing methodology to extract modification events
which, when based on a good core event extrac-
tion system, shows very promising results.
42
Acknowledgments
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program. This research was
supported in part by the Intramural Research Pro-
gram of the NIH, NLM.
References
Ethem Alpaydin. 2004. Introduction to Machine
Learning. MIT Press.
Jari Bjo?rne and T. Salakoski. 2011. Generalizing
biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191.
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2011. Ex-
tracting contextualized complex biological events
with rich graph-based features sets. Computational
Intelligence, 27(4):541?557.
Jari Bjorne, Filip Ginter, and Tapio Salakoski. 2012.
University of turku in the bionlp?11 shared task.
BMC Bioinformatics, 13(Suppl 11):S4.
Jinho D. Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Jinho D. Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 687?692, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
K.B. Cohen, K. Verspoor, H.L. Johnson, C. Roeder,
P.V. Ogren, W.A. Baumgartner, E. White, H. Tip-
ney, and L. Hunter. 2011. High-precision biological
event extraction: Effects of system and data. Com-
putational Intelligence, 27(4):681701, November.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ?08: Coling 2008: Pro-
ceedings of the workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
J.D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsu-
jii. 2009. Overview of bionlp?09 shared task on
event extraction. Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 1?9.
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The genia event and protein coreference tasks of
the bionlp shared task 2011. BMC Bioinformatics,
13(Suppl 11):S1.
H. Liu, R. Komandur, and K. Verspoor. 2011. From
graphs to events: A subgraph matching approach for
information eextraction from biomedical text. ACL
HLT 2011, page 164.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLoS ONE, 8(4):e60954, 04.
Haibin Liu, Karin Verspoor, Don Comeau, Andrew
MacKinlay, and W. John Wilbur. 2013b. General-
izing an approximate subgraph matching-based sys-
tem to extract events in molecular biology and can-
cer genetics. In Proceedings of the 2013 BioNLP
Workshop Companion Volume for the Shared Task.
Andrew MacKinlay, David Martinez, and Timo-
thy Baldwin. 2012. Detecting modification of
biomedical events using a deep parsing approach.
BMC Medical Informatics and Decision Making,
12(Suppl 1):S4.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the Association for Computational Linguistics (ACL
2008, short papers).
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
conference of the North American chapter of the
ACL, pages 152?159.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In The Workshop
on Comparing Corpora, pages 1?6, Hong Kong,
China, October. Association for Computational Lin-
guistics.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on con-
ditional random fields. In Proceedings of the 10th
Conference of the Pacific Association for Compu-
tational Linguistics, pages 49?57, Melbourne, Aus-
tralia.
S. Van Landeghem, F. Ginter, Y. Van de Peer, and
T. Salakoski. 2011. EVEX: A pubmed-scale re-
source for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37.
43
S. Van Landeghem, K. Hakala, S. Ro?nnqvist,
T. Salakoski, Y. Van de Peer, and F. Ginter. 2012.
Exploring biomolecular literature with EVEX: Con-
necting genes through events, homology and indirect
associations. Advances in Bioinformatics, Special
issue Literature-Mining Solutions for Life Science
Research:ID 582765.
Karin Verspoor, K. Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer, ,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natural
language processing tools. BMC Bioinformatics.
44
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 76?85,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Generalizing an Approximate Subgraph Matching-based System to Extract
Events in Molecular Biology and Cancer Genetics
Haibin Liu
haibin.liu@nih.gov
NCBI, Bethesda, MD, USA
Karin Verspoor
karin.verspoor@nicta.com.au
NICTA, Melbourne, VIC, Australia
Donald C. Comeau
comeau@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Andrew MacKinlay
andrew.mackinlay@nicta.com.au
NICTA, Melbourne, VIC, Australia
W. John Wilbur
wilbur@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Abstract
We participated in the BioNLP 2013 shared
tasks, addressing the GENIA (GE) and the Can-
cer Genetics (CG) event extraction tasks. Our
event extraction is based on the system we re-
cently proposed for mining relations and events
involving genes or proteins in the biomedical
literature using a novel, approximate subgraph
matching-based approach. In addition to han-
dling the GE task involving 13 event types uni-
formly related to molecular biology, we gener-
alized our system to address the CG task tar-
geting a challenging set of 40 event types re-
lated to cancer biology with various arguments
involving 18 kinds of biological entities. More-
over, we attempted to integrate a distributional
similarity model into our system to extend the
graph matching scheme for more events. In ad-
dition, we evaluated the impact of using paths of
all possible lengths among event participants as
key contextual dependencies to extract potential
events as compared to using only the shortest
paths within the framework of our system.
We achieved a 46.38% F-score in the CG task
and a 48.93% F-score in the GE task, ranking
3rd and 4th respectively. The consistent perfor-
mance confirms that our system generalizes well
to various event extraction tasks and scales to
handle a large number of event and entity types.
1 Introduction
Understanding the sophisticated interactions between
various components of biological systems and conse-
quences of these biological processes on the function
and behavior of the systems provides profound im-
pacts on translational biomedical research, leading to
more rapid development of new therapeutics and vac-
cines for combating diseases. For the past five years,
the BioNLP shared task series has served as an in-
strumental platform to promote the development of
text mining methodologies and resources for the au-
tomatic extraction of semantic events involving genes
or proteins such as gene expression, binding, or reg-
ulatory events from the biomedical literature (Kim et
al., 2009; Kim et al, 2011). An event typically cap-
tures the association of multiple participants of vary-
ing numbers and with diverse semantic roles (Anani-
adou et al, 2010). Since events often serve as partic-
ipants in other events, the extraction of such nested
event structures provides an integrated, network view
of these biological processes.
Previous shared tasks focused exclusively on
events at the molecular and sub-cellular level. How-
ever, biological processes at higher levels of organi-
zation are equally important, such as cell prolifer-
ation, organ growth and blood vessel development.
While preserving the classic event extraction tasks
such as the GE task, the BioNLP-ST 2013 broad-
ens the scope of application domains by introducing
many new issues in biology such as cancer genetics
and pathway curation. On behalf of NCBI (National
Center for Biotechnology Information), our team par-
ticipated in the GENIA (GE) task and the Cancer Ge-
netics (CG) task. Compared to the GE task that aims
for 13 types of events concerning the protein NF-?B,
the CG task targets a challenging set of 40 types of
biological processes related to the development and
progression of cancer involving 18 entity types. This
additionally requires that event extraction systems be
able to associate entities and events at the molecular
level with anatomy level effects and organism level
outcomes of cancer biology.
Our event extraction is based on the system we re-
cently proposed for mining relations and events in-
volving genes or proteins in the biomedical litera-
ture using a novel, Approximate Subgraph Matching-
based (ASM) approach (Liu et al, 2013a). When
evaluated on the GE task of the BioNLP-ST 2011, its
performance is comparable to the top systems in ex-
tracting 9 types of biological events. In the BioNLP-
76
ST 2013, we generalized our system to investigate
both CG and GE tasks. Moreover, we attempted to in-
tegrate a distributional similarity model into the sys-
tem to extend the graph matching scheme for more
events. The graph representation that considers paths
of all possible lengths (all-paths) between any two
nodes has been encoded in graph kernels used in
conjunction with Support Vector Machines (SVM),
and led to state-of-the-art performance in extracting
protein-protein (Airola et al, 2008) and drug-drug in-
teractions (Zhang et al, 2012). Borrowing from the
idea of the all-paths representation, in addition, we
evaluated the impact of using all-paths among event
participants as key contextual dependencies to extract
potential events as compared to using only the short-
est paths within the framework of our system.
The rest of the paper is organized as follows: In
Section 2, we briefly introduce our ASM-based event
extraction system. Section 3 describes our experi-
ments aiming to extend our system. Section 4 elab-
orates some implementation details and Section 5
presents our results and discussion. Finally, Section
6 summarizes the paper and introduces future work.
2 ASM-based Event Extraction
The underlying assumption of our event extraction
approach is that the contextual dependencies of each
stated biological event represent a typical context for
such events in the biomedical literature. Our ap-
proach falls into the machine learning category of
instance-based reasoning (Alpaydin, 2004). Specif-
ically, the key contextual structures are learned from
each labeled positive instance in a set of train-
ing data and maintained as event rules in the form
of subgraphs. Extraction of events is performed
by searching for an approximate subgraph isomor-
phism between key dependencies and input sen-
tence graphs using an approximate subgraph match-
ing (ASM) algorithm designed for literature-based
relational knowledge extraction (Liu et al, 2013a).
By introducing error tolerance into the graph match-
ing process, our approach is capable of retrieving
events encoded within complex dependency contexts
while maintaining the extraction precision at a high
level. The ASM algorithm has been released as open
source software1. See (Liu et al, 2013a) for more de-
tails on the ASM algorithm, its complexity and the
comparison with existing graph distance metrics.
Figure 1 illustrates the overall architecture of our
ASM-based system with three core components high-
1http://asmalgorithm.sourceforge.net
lighted: rule induction, sentence matching and rule
set optimization. Our approach focuses on extract-
ing events expressed within the boundaries of a single
sentence. It is also assumed that entities involved in
the target event have been annotated. Next, we briefly
describe the core components of the system.
Rule Induction
Preprocessing
Sentence Matching
Postprocessing
Training data Testing data
Rule Set
Optimization
Figure 1: ASM-based Event Extraction Framework
2.1 Rule Induction
Event rules are learned automatically using the fol-
lowing method. Starting with the dependency graph
of each training sentence, for each annotated event,
the shortest dependency path connecting the event
trigger to each event argument in the undirected ver-
sion of the graph is selected. While additional in-
formation such as individual words in each sentence
(bag-of-words), sequences of words (n-grams) and
semantic concepts is typically used in the state-of-
the-art supervised learning-based systems to cover a
broader context (Airola et al, 2008; Buyko et al,
2009; Bjo?rne et al, 2012), the shortest path be-
tween two tokens in the dependency graph is par-
ticularly likely to carry the most valuable informa-
tion about their mutual relationship (Bunescu and
Mooney, 2005a; Thomas et al, 2011b; Rinaldi et
al., 2010). In case there exists more than one short-
est path, all of them are considered. For multi-token
event triggers, the shortest path connecting every trig-
ger token to each event argument is extracted, and the
union of the paths is then computed for each trigger.
For regulatory events that take a sub-event as an ar-
gument, the shortest path is extracted so as to connect
the trigger of the main event to that of the sub-event.
For complex events that involve multiple argu-
ments, we computed the dependency path union of
all shortest paths from trigger to each event argument,
resulting in a graph in which all event participants are
jointly depicted. Individual dependency paths con-
necting triggers to each argument are also considered
to determine event arguments independently. If the
77
resulting arguments share the same event trigger, they
are grouped together to form a potential event. In our
approach, the individual paths aim to retrieve more
potential events while the path unions retain the pre-
cision advantage of joint inference.
While the dependencies of such paths are used as
the graph representation of the event, a detailed de-
scription records the participants of the event, their
semantic role labels and the associated nodes in the
graph. All participating biological entities are re-
placed with a tag denoting their entity type, e.g. ?Pro-
tein? or ?Organism?, to ensure generalization of the
learned rules. As a result, each annotated event is
generalized and transformed into a generic graph-
based rule. The resulting event rules are categorized
into different target event types.
2.2 Sentence Matching
Event extraction is achieved by matching the induced
rules to each testing sentence and applying the de-
scriptions of rule tokens (e.g. role labels) to the cor-
responding sentence tokens. Since rules and sentence
parses all possess a graph representation, event recog-
nition becomes a subgraph matching problem. We
introduced a novel approximate subgraph matching
(ASM) algorithm (Liu et al, 2013a) to identify a sub-
graph isomorphic to a rule graph within the graph of
a testing sentence. The ASM problem is defined as
follows.
Definition 1. An event rule graph Gr =
(Vr, Er) is approximately isomorphic to a subgraph
Ss of a sentence graph Gs = (Vs, Es), denoted
by Gr ?=t Ss ? Gs, if there is an injective
mapping f : Vr ? Vs such that, for a given
threshold t, t ? 0, the subgraph distance be-
tween Gr and Gs satisfies 0 ? subgraphDistf (Gr,
Gs) ? t, where subgraphDistf (Gr, Gs) = ws ?
structDistf (Gr, Gs) + wl ? labelDistf (Gr, Gs) +
wd ? directionalityDistf (Gr, Gs).
The subgraph distance is proposed to be the
weighted summation of three penalty-based measures
for a candidate match between the two graphs. The
measure structDist compares the distance between
each pair of matched nodes in one graph to the
distance between corresponding nodes in the other
graph, and accumulates the structural differences.
The distance in rule graphs is defined as the length
of the shortest path between two nodes. The distance
in sentence graphs is defined as the length of the path
between corresponding nodes that leads to minimum
structural difference with the distance in rule graphs.
Because dependency graphs are edge-labeled, ori-
ented graphs, the measures labelDist and direction-
alityDist evaluate respectively the overall differences
in edge labels and directionalities on the compared
path between each pair of matched nodes in the two
graphs. The real numbers ws, wl and wd are non-
negative weights associated with the measures.
The weights ws, wl and wd are defaulted to be
equal but can be tuned to change the emphasis of the
overall distance function. The distance threshold t
controls the isomorphism quality of the retrieved sub-
graphs from sentences. A smaller t allows only lim-
ited variations and always looks for a sentence sub-
graph as closely isomorphic to the rule graph as pos-
sible. A larger t enables the extraction of events de-
scribed in complicated dependency contexts, thus in-
creasing the chance of retrieving more events. How-
ever, it can incur a bigger search cost due to the eval-
uation of more potential solutions.
An iterative, bottom-up matching process is used
to ensure the extraction of complex and nested events.
Starting with the extraction of simple events, simple
event rules are first matched with a testing sentence.
Next, as potential arguments of higher level events,
obtained simple events continue to participate in the
subsequent matching process between complex event
rules and the sentence to initiate the iterative process
for detecting complex events with nested structures.
The process terminates when no new candidate event
is generated for the testing sentence.
During the matching phase we relax the event
rules that contain sub-event arguments such that any
matched event can substitute for the sub-event. We
believe that the contextual structures linking anno-
tated sub-events of a certain type are generalizable
to other event types. This relaxation increases the
chance of extracting complex events with nested
structures but still takes advantage of the contextual
constraints encoded in the rule graphs.
2.3 Rule Set Optimization
Typical of instance-based reasoners, the accuracy of
rules with which to compare an unseen sentence is
crucial to the success of our approach. For instance, a
Transcription rule encoding a noun compound mod-
ification dependency between ?TNF? and ?mRNA?
derived from an event context ?expression of TNF
mRNA? should not produce a Transcription event
for the general phrase ?level of TNF mRNA? even
though they share a matchable dependency. Such
matches result in false positive events.
78
Therefore, we measured the accuracy of each rule
ri in terms of its prediction result via Eq.(1). For rules
that produce at least one prediction, we ranked them
byAcc(ri) and excluded the ones with aAcc(ri) ratio
lower than an empirical threshold, e.g. 1:4.
Acc(ri) =
#correct predictions by ri
#total predictions by ri
(1)
Because of nested event structures, the removal
of some rules might incur a propagating effect on
rules relying on them to produce arguments for the
extraction of higher order events. Therefore, an it-
erative rule set optimization process, in which each
iteration performs sentence matching, rule ranking
and rule removal sequentially, is conducted, lead-
ing to a converged, optimized rule set. While the
ASM algorithm aims to extract more potential events,
this performance-based evaluation component en-
sures the precision of our event extraction framework.
3 Extensions to Event Extraction System
In the BioNLP-ST 2013, we attempted two different
ways to extend the current event extraction system:
(1) integrate a distributional similarity model into the
system to extend the graph matching scheme for more
events; (2) use paths of all possible lengths (all-paths)
among event participants as key contextual depen-
dencies to extract events. We next elaborate these
system extensions in detail.
3.1 Integrating Distributional Similarity Model
The proposed subgraph distance measure of the ASM
algorithm focuses on capturing differences in the
overall graph structure, edge labels and directional-
ities. However, when determining the injective node
mapping between graphs, the matching remains at the
surface word level.
In the current setting, various node features can be
considered when comparing two graph nodes, result-
ing in different matching criteria. The features in-
clude POS tags (P), event trigger (T), token lemmas
(L) and tokens themselves (A). For instance, a match-
ing criterion, ?P*+L?, requires that the relaxed POS
tags (P*) and the lemmatized form (L) of tokens be
identical for each rule node to match with a sentence
node. The relaxed POS allows the plural form of
nouns to match with the singular form, and the con-
jugations of verbs to match with each other. How-
ever, the inability to go beyond surface level match-
ing prevents node tokens that share similar meaning
but possess distinct orthography from matching with
each other. For instance, a mismatch between rule
token ?crucial? and a sentence token ?critical? could
lead to an undiscovered Positive regulation event.
We attempted to use only POS information in the
node matching scheme and observed a nearly 14%
increase in recall (Liu et al, 2013b). However, the
precision drops sharply, resulting in an undesirable
F-score. This indicates that the lexical information
is a critical supplement to the contextual dependency
constraints in accurately capturing events within the
framework of our system. Moreover, we attempted to
extend the node matching using the synsets of Word-
Net (Fellbaum, 1998) to allow tokens to match with
their synonyms (Liu et al, 2011). However, since
WordNet is developed for the general English lan-
guage, it relates biomedical terms e.g., ?expression?
with general words such as ?aspect? and ?face?, thus
leading to incorrect events.
In this work, we integrated a distributional simi-
larity model (DSM) into our node matching scheme
to further improve the generalization of event rules.
A distributional similarity model is constructed
based on the distributional hypothesis (Harris, 1954):
words that occur in the same contexts tend to share
similar meanings. We expect that the incorporation
of DSM will enable our system to capture matching
tokens in testing sentences that do not appear in the
training data while maintaining the extraction pre-
cision at a high level. There have been many ap-
proaches to compute the similarity between words
based on their distribution in a corpus (Landauer and
Dumais, 1997; Pantel and Lin, 2002). The output is a
ranked list of similar words to each word. We reim-
plemented the model proposed by (Pantel and Lin,
2002) in which each word is represented by a fea-
ture vector and each feature corresponds to a context
where the word appears. The value of the feature
is the pointwise mutual information (Manning and
Schu?tze, 1999) between the feature and the word. Let
c be a context and Fc(w) be the frequency count of a
word w occurring in context c. The pointwise mutual
information, miw,c between c and w is defined as:
miw,c =
Fc(w)
N?
i
Fi(w)
N ?
?
j
Fc(j)
N
(2)
where N =
?
i
?
j
Fi(j) is the total frequency count
of all words and their contexts.
Since mutual information is known to be biased
towards infrequent words/features, the above mutual
79
information value is multiplied by a discounting fac-
tor as described in (Pantel and Lin, 2002). The simi-
larity between two words is then computed using the
cosine coefficient (Salton and McGill, 1986) of their
mutual information vectors.
We experimented with two different approaches to
integrate the DSM into our event extraction system.
First, the model is directly embedded into the node
matching scheme. Once a match cannot be deter-
mined by surface tokens, the DSM is invoked to allow
a match if the sentence token appears in the list of the
top M most similar words to the rule token. Sec-
ond, additional event rules are generated by replac-
ing corresponding rule tokens with their top M most
similar words, rather than allow DSM to participate
in the node matching. While the first method mea-
sures the consolidated extraction ability of an event
rule by combining its DSM-generalized performance,
the second approach provides a chance to evaluate the
impact of each DSM-introduced similar word indi-
vidually on event extraction.
3.2 Adopting All-paths for Event Rules
Airola et al proposed an all-paths graph (APG) ker-
nel for extracting protein-protein interactions (PPI),
in which the kernel function counts weighted shared
dependency paths of all possible lengths (Airola et
al., 2008). Thomas et al adopted this kernel as
one of the three models used in the ensemble learn-
ing for extracting drug-drug interactions (Thomas et
al., 2011a) and won the recent DDIExtraction 2011
challenge (Segura-Bedmar et al, 2011). The JULIE
lab adapted the APG kernel to event extraction us-
ing syntactically pruned and semantically enriched
dependency graphs (Buyko et al, 2009).
The graph representation of the kernel consists of
two sub-representations: the full dependency parse
and the surface word sequence of the sentence where
a pair of interacting entities occurs. At the expense
of computational complexity, this representation en-
ables the kernel to explore broader contexts of an
interaction, thus taking advantage of the entire de-
pendency graph of the sentence. When comparing
two interaction instances, instead of using only the
shortest path that might not always provide suffi-
cient syntactic information about relations, the ker-
nel considers paths of all possible lengths between
any two nodes. More recently, a hash subgraph pair-
wise (HSP) kernel-based approach was also proposed
for drug-drug interactions and adopts the same graph
representation as the APG kernel (Zhang et al, 2012).
In contrast, the graph representation that our ASM
algorithm searches in a sentence is inherently re-
stricted to the shortest path among target entities in
event rules, as described in Section 2.2. Borrowing
from the idea of the all-path graph representation, in
this work we attempted to explore contexts beyond
the shortest paths to enrich our rule set. We evalu-
ated within the framework of our system the impact
of using acyclic paths of all possible lengths among
event participants as key contextual dependencies to
populate the event rule set as compared to using only
the shortest paths in the current system setting.
4 Implementation
4.1 Preprocessing
We employed the preprocessed data in the
BioC (Comeau et al, 2013) compliant XML format
provided by the shared task organizers as supporting
resources. The BioC project attempts to address
the interoperability among existing natural language
processing tools by providing a unified BioC XML
format. The supporting analyses include tokeniza-
tion, sentence segmentation, POS tagging and
lemmatization. Different syntactic parsers analyze
text based on different underlying methodologies, for
instances, the Stanford parser (Klein and Manning,
2003) performs joint inference over the product of an
unlexicalized Probabilistic Context-Free Grammar
(PCFG) parser and a lexicalized dependency parser
while the McClosky-Charniak-Johnson (Charniak)
parser (McClosky and Charniak, 2008) is based on
N -best parse reranking over a lexicalized PCFG
model. In order to take advantage of multiple aspects
of structural analysis of sentences, both Stanford
parser and Charniak parser, which are among the best
performing parsers trained on the GENIA Treebank
corpus, are used to parse the training sentences and
produce dependency graphs for learning event rules.
Only the Charniak parser is used on the testing
sentences in the event extraction phase.
4.2 ASM Parameter Setting
The GE task includes 13 different event types. Since
each type possesses its own event contexts, an indi-
vidual threshold te is assigned to each type. Together
with the 3 distance function weights ws, wl and wd,
the ASM requires 16 parameters for the GE event ex-
traction task. Similarly, the ASM requires 43 param-
eters to cater to the 40 diverse event types of the CG
task. As reported in (Liu et al, 2013a), we used a
genetic algorithm (GA) (Cormen et al, 2001) to au-
80
tomatically determine values of the 12 ASM param-
eters for the 2011 GE task using the training data.
We inherited these previously determined parameters
and adapted them into the 2013 tasks according to
the event type and its argument configuration. For in-
stance, ?Pathway? events in the CG task is assigned
the same te as the ?Binding? events in the GE task as
they possess similar argument configurations.
Table 1 shows the parameter setting for the 2013
GE task with the equal weights ws = wl = wd con-
straint. The graph node matching criterion ?P*+L?
that requires the relaxed POS tags and the token lem-
mas to be identical is used in the ASM.
Parameter Value Parameter Value
tGene expression 8 tUbiquitination 3
tTranscription 7 tBinding 7
tProtein catabolism 10 tRegulation 3
tPhosphorylation 8 tPositive regulation 3
tLocalization 8 tNegative regulation 3
tAcetylation 3 ws 10
tDeacetylation 3 wl 10
tProteinmodification 3 wd 10
Table 1: ASM parameter setting for the 2013 GE task
4.3 Distributional Similarity Model
In our implementation, we made following improve-
ments to the original Pantel model (Pantel and Lin,
2002): (1) lemmas of words generated by the Bi-
oLemmatizer (Liu et al, 2012) are used to achieve
generalization. The POS information is combined
with each lemmatized word to disambiguate its cat-
egory. (2) instead of the linear context where a
word occurs, we take advantage of dependency con-
texts inferred from dependency graphs. For instance,
?toxicity?amod? is extracted as a feature of the to-
ken ?nonhematopoietic JJ?. It captures the dependent
token, the type and the directionality of the depen-
dency. (3) the resulting miw,c is scaled into the [0, 1]
range by
? ?miw,c
1 + ? ?miw,c
to avoid greater miw,c values
dominating the similarity calculation between words.
An empirical ? = 0.01 is used. (4) while only the
immediate dependency contexts of a word are used
in our model, our implementation is flexible so that
contexts of various dependency depths could be taken
into consideration.
In order to cover a wide range of words and capture
the diverse usages of them in biomedical texts, in-
stead of resorting to an existing corpus, our distribu-
tional similarity model is built based on a random se-
lection of 5 million abstracts from the entire PubMed.
When computing miw,c, we filtered out contexts of
each word where the word occurs less than 5 times.
Eventually, the model contains 2.8 million distinct to-
kens and 0.4 million features. When it is queried with
an amino acid, e.g, ?lysine?, the top 15 tokens in the
resulting ranked list are all correct amino acid names.
5 Results and Discussion
This section reports our results on the GE and the CG
tasks respectively, including the attempted extensions
to our ASM-based event extraction system.
5.1 GE task
5.1.1 Datasets
The 2013 GE task dataset is composed of full-text
articles from PubMed Central, which are divided into
smaller segments by the task organizers according to
various sections of the articles. Table 2 presents some
statistics of the GE dataset.
Attributes Counted Training Development Testing
Full article segments 222 249 305
Proteins 3,571 4,138 4,359
Annotated events 2,817 3,199 3,301
Table 2: Statistics of BioNLP-ST 2013 GE dataset
As distributed, the development set is bigger than
the training set. For better system generalization, we
randomly reshuffled the data and created a 353/118
training/development division, a roughly 3:1 ratio
consistent with the settings in previous GE tasks.
The results reported on the training/development data
thereafter are based on our new data partition.
5.1.2 GE Results on Development Set
Table 3 shows the event extraction results on the 118
development documents based on event rules derived
from different parsers. Only the numbers of unique,
optimized rules are reported and those that possess
isomorphic graph representations determined by an
Exact Subgraph Matching (ESM) algorithm (Liu et
al., 2013b) are removed. The ensemble rule set com-
bines rules derived from both parsers and achieves
a better performance than that of using individual
parsers. It makes sense that the Charniak parser is
favored and leads to a performance close to the en-
semble performance because sentences from which
events are extracted are parsed by the Charniak parser
as well. However, we retained the additional rules
from the Stanford parser in the hope that they may
contribute to the testing data.
When embedding the distributional similarity
model (DSM) directly into the graph node matching
81
Parser Type Event Rule Recall Precision F-score
Charniak 2,923 47.01% 66.01% 54.91%
Stanford 3,305 43.66% 67.67% 53.08%
Ensemble 4,617 47.45% 65.65% 55.09%
Table 3: Performance of using different parsers
scheme, we performed the DSM on all rule tokens ex-
cept biological entities, meaning that for each rule to-
ken, if a match will be granted if a rule token appears
in the top M most similar word list of a sentence to-
ken, e.g., ?DSM 3? denotes the top 3 similar words
determined by the DSM. We further performed DSM
only on trigger tokens for comparison, as presented
in Table 4.
All Tokens Recall Precision F-score
DSM 1 47.98% 52.56% 50.17%
DSM 3 48.68% 35.07% 40.77%
DSM 10 53.43% 19.38% 28.44%
Trigger Tokens Recall Precision F-score
DSM 1 48.06% 54.22% 50.95%
DSM 3 48.59% 37.00% 42.01%
DSM 10 53.35% 24.65% 33.72%
Table 4: Performance of integrated DSM
Even though the DSM helps to substantially in-
crease the recall to 53.43%, we observed a significant
precision drop which leads to an inferior F-score to
the ensemble baseline in Table 3. A close evaluation
of the generated graph matches reveals that antonyms
produced by the DSM contributes to most of the false
positive events. For instance, the most similar words
for the verb ?increase? and the adjective ?high? re-
turned by the model are ?decrease? and ?low? be-
cause they tend to occur in the same contexts. Fur-
ther investigation is needed to automatically filter out
the antonyms. When generating additional rules us-
ing the top M most similar words from the DSM,
since all the rules undergo the optimization process,
the event extraction precision is ensured. However,
the recall increase from simple events is diluted by
the counter effect of the introduced false positives in
detecting regulation-related complex events, result-
ing in a comparable performance to the baseline.
Table 5 gives the performance comparison of us-
ing all-paths and the shortest paths in our event ex-
traction system. Using all-paths does not bring in a
significant improvement in F-score but takes 27 it-
erations to optimize as compared to the 5-iteration
optimization on shortest paths. Most of the rules in-
duced from all-paths are eventually discarded by the
optimization process. The all-paths graph represen-
tation was motivated by the observation that short-
est paths between candidate entities often exclude
relation-signaling words when detecting binary re-
lationships (Airola et al, 2008). Exploring broader
contexts ensures such words to be considered. In the
event extraction task, however, since triggers have
been annotated, they are naturally incorporated into
the shortest paths connecting trigger to each event ar-
gument. This in part explains why contexts beyond
shortest paths did not bring in an appreciable benefit.
All Tokens Recall Precision F-score
All-paths 48.77% 64.64% 55.59%
Shortest paths 47.45% 65.65% 55.09%
Table 5: Performance of using all-paths
5.1.3 GE Results on Testing Set
Since integrating the DSM and all-paths do not pro-
vide significant performance improvements to our
system, we decided to retain the original settings in
the ASM when extracting events from the testing
data. While most of the 2011 shared task datasets are
composed of PubMed abstracts compared to full-text
articles in the 2013 GE task, our system focuses on
extracting events expressed within the boundaries of
a single sentence. Therefore, in order to take advan-
tage of existing annotated resources, we incorporated
the annotated data of 2011 GE task and EPI (Epi-
genetics and Post-translational Modifications) task to
enrich the training instances of corresponding event
types of the 2013 GE task. Eventually, we obtained a
total of 14,448 rules of different event types from our
training data. In practice, it takes the ASM less than a
second to match the entire rule set with one document
and return results.
Our submitted system achieves a 48.93% F-score
on the 305 testing documents of the GE task, ranking
4th among 12 participating teams. Table 6 presents
the performance of the top eight systems.
System Recall Precision F-score
EVEX 45.44% 58.03% 50.97%
TEES 2.1 46.17% 56.32% 50.74%
BioSEM 42.47% 62.83% 50.68%
NCBI 40.53% 61.72% 48.93%
DlutNLP 40.81% 57.00% 47.56%
HDS4NLP 37.11% 51.19% 43.03%
NICTANLM 36.99% 50.68% 42.77%
USheff 31.69% 63.28% 42.23%
Table 6: Performance of top 8 systems in GE task
Our performance is within a reasonable mar-
gin from the best-performing system ?EVEX?, and
shows an overall superior precision over most partic-
ipating teams; only two of the top 5 systems obtained
82
a precision in the 60% range. Particularly for the
regulation-related complex events, we are the only
team that achieved a precision over 55% among all
12 participating systems. This indicates that event
rules automatically learned and optimized over train-
ing data generalize well to the unseen text, and have
the ability to identify precisely corresponding events.
We further evaluated the impact of the additonal
training instances from 2011 tasks and the ensemble
rule set derived from different parsers as presented
in Table 7. With the help from the 2011 data, our
F-score is increased by 3% and we became the only
team that detected ?Ubiquitination? events from test-
ing data. In addition, rules derived from the Stanford
parser do not provide additional benefits on the test-
ing data compared to using the Charniak parser alone.
System Attribute Recall Precision F-score
Ensemble 2013 + 2011 data 40.53% 61.72% 48.93%
Ensemble 2013 data 35.63% 63.91% 45.75%
Charniak 2013 data 35.29% 65.71% 45.92%
Table 7: Impact of 2011 data and ensemble rule set
5.2 CG task
5.2.1 Datasets
The CG task dataset is prepared based on a previ-
ously released corpus of angiogenesis domain ab-
stracts (Wang et al, 2011). It targets a challenging
set of 40 types of biological processes related to the
development and progression of cancer involving 18
entity types (Pyysalo et al, 2012). Table 8 presents
some statistics of the CG dataset.
Attributes Counted Training Development Testing
Abstracts 300 100 200
Entities 10,935 3,634 6,955
Annotated events 8,803 2,915 5,972
Table 8: Statistics of BioNLP-ST 2013 CG dataset
5.2.2 CG Results on Testing Set
We generalized our event extraction system to the CG
task and the corresponding annotated data of the 2011
tasks is also incorporated in the training phase to ob-
tain the optimized event rule set. Due to time con-
straints, the impact of integrating the DSM and all-
paths is not evaluated on the CG task. We achieved
a 46.38% F-score on the 200 testing documents of
the CG task, ranking 3rd among the 6 participating
teams. Table 9 gives the primary evaluation results of
the 6 participating teams; only ?TEES-2.1? and we
participated in both GE and CG tasks. The detailed
results of each of the targeted 40 event types is avail-
able from the official CG task website.
Team Recall Precision F-score
TEES-2.1 48.76% 64.17% 55.41%
NaCTeM 48.83% 55.82% 52.09%
NCBI 38.28% 58.84% 46.38%
RelAgent 41.73% 49.58% 45.32%
UET-NII 19.66% 62.73% 29.94%
ISI 16.44% 47.83% 24.47%
Table 9: Performance of all systems in 2013 CG task
Inconsistent with other biological entities, the en-
tity annotation for the optional ?Site? argument in-
volved in events such as ?Binding?, ?Mutation? and
?Phosphorylation? are not provided by the task orga-
nizers. We consider that detecting ?Site? entities is
related to entity detection and we would like to focus
our system on the event extraction itself. Thus, we
decided to ignore the ?Site? argument in our system.
However, a problem will arise that even though the
other arguments are correctly identified for an event,
it might still be evaluated as false positive if a ?Site?
argument is not detected. This results in both false
positive and false negative events. In addition, since
we did not perform the secondary task which requires
us to detect modifications of the predicted events, in-
cluding negation and speculation, about 7.5% anno-
tated instances in the testing data are thus missed,
causing damage to our recall in the overall evalua-
tion. The organizers have agreed to issue an additonal
evaluation that will focus on core event extraction tar-
gets excluding optional arguments such as ?Site? and
the secondary task. We will conduct more detailed
analysis on the results once they are made available.
6 Conclusion and Future Work
In the BioNLP-ST 2013, we generalized our ASM-
based system to address both GE and CG tasks.
We attempted to integrate a distributional similarity
model into our system to extend the graph match-
ing scheme. We also evaluated the impact of using
paths of all possible lengths among event participants
as key contextual dependencies to extract potential
events as compared to using only the shortest paths
within the framework of our system.
We achieved a 46.38% F-score in the CG task and
a 48.93% F-score in the GE task, ranking 3rd and
4th respectively. While the distributional similarity
model did not improve the overall performance of our
system in the tasks, we would like to further investi-
gate the antonym problem introduced by the model in
our future work.
83
Acknowledgments
This research was supported by the Intramural Re-
search Program of the NIH, NLM.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski1. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9 Suppl 11:s2.
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press.
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for systems
biology by text mining the literature. Trends in Biotech-
nology, 28(7):381?390.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012. Uni-
versity of turku in the BioNLP?11 shared task. BMC
Bioinformatics, 13 Suppl 11:S4.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, pages 724?731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Information
Processing Systems (NIPS). Vancouver, BC, December.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In BioNLP ?09: Proceedings of the
Workshop on BioNLP, pages 19?27, Morristown, NJ,
USA. Association for Computational Linguistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Verspoor,
Thomas C. Wiegers, Cathy H. Wu, and W. John Wilbur.
2013. BioC: A minimalist approach to interoperability
for biomedical text processing. submitted.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. The MIT Press.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. Bradford Books.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP Shared Task 2009 Workshop, pages
1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6. As-
sociation for Computational Linguistics, June.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 423?430. Association for Computa-
tional Linguistics.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211?240.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching ap-
proach for information extraction from biomedical text.
In Proceedings of BioNLP Shared Task 2011 Work-
shop, pages 164?172. Association for Computational
Linguistics, June.
Haibin Liu, Tom Christiansen, William A Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a lemmati-
zation tool for morphological processing of biomedical
text. Journal of Biomedical Semantics, 3:3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLOS ONE, 8:4 e60954.
Haibin Liu, Vlado Keselj, and Christian Blouin. 2013b.
Exploring a subgraph matching approach for extracting
biological events from literature. Computational Intel-
ligence.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language processing.
MIT Press, Cambridge, MA, USA.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
Association for Computational Linguistics, pages 101?
104, Columbus, Ohio. The Association for Computer
Linguistics.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?02, pages 613?619,
New York, NY, USA. ACM.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol
Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012.
Event extraction across multiple levels of biological or-
ganization. Bioinformatics, 28:i575?i581.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon
Clematide, Thrse Vachon, and Martin Romacker. 2010.
Ontogene in BioCreative II.5. IEEE/ACM Trans. Com-
put. Biology Bioinform., 7(3):472?480.
84
Gerard Salton and Michael J. McGill. 1986. Introduction
to Modern Information Retrieval. McGraw-Hill, Inc.,
New York, NY, USA.
Isabel Segura-Bedmar, Paloma Martinez, and Daniel
Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011
Challenge Task: Extraction of Drug-Drug Interactions
from Biomedical Texts. In Proceedings of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction 2011,
pages 1?9.
Philippe Thomas, Mariana Neves, Illes Solt, Domonkos
Tikk, and Ulf Leser. 2011a. Relation extraction for
drug-drug interactions using ensemble learning. In Pro-
ceedings of DDIExtraction-2011 challenge task, pages
11?18.
Philippe Thomas, Stefan Pietschmann, Ille?s Solt,
Domonkos Tikk, and Ulf Leser. 2011b. Not all
links are equal: Exploiting dependency types for the
extraction of protein-protein interactions from text. In
Proceedings of BioNLP 2011 Workshop, pages 1?9.
Association for Computational Linguistics, June.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehensive
benchmark of kernel methods to extract protein?protein
interactions from literature. PLoS Computational Biol-
ogy, 6:e1000837, July.
Xinglong Wang, Iain McKendrick, Ian Barrett, Ian Dix,
Tim French, Jun?ichi Tsujii, and Sophia Ananiadou.
2011. Automatic extraction of angiogenesis bioprocess
from text. Bioinformatics, 27(19):2730?2737.
Yijia Zhang, Hongfei Lin, Zhihao Yang, Jian Wang, and
Yanpeng Li. 2012. A single kernel-based approach to
extract drug-drug interactions from biomedical litera-
ture. PLOS ONE, 7(11): e48901.
85
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 12?22,
Dublin, Ireland, August 23rd 2014.
Integrating UIMA with Alveo, a human communication science virtual 
laboratory 
 
Dominique Estival 
 U. of Western Sydney 
d.estival@uws.edu.au  
Steve Cassidy 
Macquarie University 
steve.cassidy@mq.edu.au 
Karin Verspoor 
University of Melbourne 
karin.verspoor@unimelb.edu.au 
                           Andrew MacKinlay                            Denis Burnham 
       RMIT                                      U. of Western Sydney  
                 andrew.mackinlay@rmit.edu.au                d.burnham@uws.edu.au 
Abstract 
This paper describes two aspects of Alveo, a new virtual laboratory for human communication science 
(HCS). As a platform for HCS researchers, the integration of the Unstructured Information Management 
Architecture (UIMA) with Alveo was one of the aims during the development phase and we report on 
the choices that were made for the implementation. User acceptance testing (UAT) constituted an inte-
gral part of the development and evolution of Alveo and we present the distributed testing organisation, 
the test development process and the evolution of the tests. We conclude with some lessons learned re-
garding multi-site collaborative work on the development and deployment of HLT research infrastruc-
ture. 
 
1 Introduction 
The Alveo Virtual Laboratory provides a new platform for collaborative research in human communi-
cation science (HCS). 1 Funded by the Australian Government National eResearch Collaboration Tools 
and Resources (NeCTAR) program, it involves partners from 16 institutions in a range of disciplines: 
linguistics, natural language processing, speech science, psychology, as well as music and acoustic 
processing. The goal of the platform is to provide easy access to a variety of databases and a range of 
analysis tools, in order to foster inter-disciplinary research and facilitate the discovery of new methods 
for solving old problems or the application of known methods to new datasets (Estival, Cassidy, 
Sefton, & Burnham, 2013). The platform integrates a number of tools and enables non-technical users 
to process communication resources (including not only text and speech corpora but also music re-
cordings and videos) using these tools in a straightforward manner. In this paper, we report on the re-
cent integration of the Unstructured Information Management Architecture (UIMA) with Alveo. This 
integration is bi-directional, in that existing resources and annotations captured over those resources in 
Alveo can flow to a UIMA process, and new annotations produced by a UIMA process can be con-
sumed and persisted by Alveo. We also introduce the general approach to user acceptance testing 
(UAT) of Alveo, focussing on the organisation and process acceptance adopted to meet the acceptance 
criteria required for the project and to ensure user uptake within the research community. Finally, we 
demonstrate the application of the testing process for acceptance of the UIMA integration. 
Section 2 briefly describes Alveo and its components, in particular the tools and corpora already 
available on the platform and the workflow engine, then Section 3 describes the Alveo-UIMA integra-
tion. Section 4 describes the UAT requirement and the organisation of the testing among the Alveo 
project partners, outlines the actual testing process and gives examples of the tests, among them the 
UIMA tests, which were developed for the project. Section 5 discusses alternative strategies and we 
conclude with some lessons learned regarding multi-site collaborative work on the development and 
deployment of HLT research infrastructure. 
                                                 
1  This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0. 12
2 The Alveo Virtual Laboratory 
Alveo provides easy access to a range of databases relevant to human communication science disci-
plines, including speech, text, audio and video, some of which would previously have been difficult 
for researchers to access or even know about. The system implements a uniform and secure license 
management system for the diverse licensing and user agreement conditions required. Browsing, 
searching and dataset manipulation are also functionalities which are available in a consistent manner 
across the data collections through the web-based Discovery Interface. The first phase of the project 
(December 2012 ? June 2014) saw the inclusion of the collections shown in Table 1. 
 
1. PARADISEC (Pacific and Regional Archive for Digital Sources in Endangered Cultures): audio, video, 
text and image resources for Australian and Pacific Island languages (Thieberger, Barwick, Billington, & 
Vaughan, 2011) 
2. AusTalk, audio-visual speech corpus  of Australian English (Burnham et al., 2011) 
3. The Australian National Corpus  (S. Cassidy, Haugh, Peters, & Fallu, 2012) comprising: Australian Cor-
pus of English (ACE); Australian Radio Talkback (ART); AustLit; Braided Channels; Corpus of Oz Early 
English (COOEE); Griffith Corpus of Spoken English (GCSAusE); International Corpus of English (ICE-
AUS); Mitchell & Delbridge corpus; Monash Corpus of Spoken English (Musgrave & Haugh, 2009). 
4. AVOZES, a visual speech corpus (Goecke & Millar, 2004) 
5. UNSW Pixar Emotional Music Excerpts: Pixar movie theme music expressing different emotions 
6. Sydney University Room Impulse Responses: environmental audio samples which, through convolution 
with speech or music, can create the effect of that speech or music in that acoustic environment 
7. Macquarie University Battery of Emotional Prosody: sung sentences with different prosodic patterns 
8. Colloquial Jakartan Indonesian corpus: audio and text, recorded in Jakarta in the early 1990?s (ANU) 
9. The ClueWeb dataset (http://lemurproject.org/clueweb12/). 
  Table 1: Alveo Data Collections 
 
Through the web-based Discovery interface, the user can select items based on the results of faceted 
search across the collections and can organise selected data in Items Lists. Beyond browsing and 
searching, Alveo offers the possibility of analysing and processing the data with a range of tools. In 
the first phase of the project, the tools listed in Table 2 were integrated within Alveo.  
 
1. EOPAS (PARADISEC tool) for text interlinear text and media analysis 
2. NLTK (Natural Language Toolkit) for text analytics with linguistic data (Bird, Klein, & Loper, 2009) 
3. EMU, for search, speech analysis and interactive labelling of spectrograms and waveforms (Steve Cassidy 
& Harrington, 2000) 
4. AusNC Tools: KWIC, Concordance, Word Count, statistical summary and statistical analysis 
5. Johnson-Charniak parser, to generate full parse trees for text sentences (Charniak & Johnson, 2005) 
6. ParseEval, to evaluate the syllabic parse of consonant clusters (Shaw & Gafos, 2010) 
7. HTK-modifications, a patch to HTK (Hidden Markov Model Toolkit, http://htk.eng.cam.ac.uk/) to enable 
missing data recognition 
8. DeMoLib, for video analysis (http://staff.estem-uc.edu.au/roland/research/demolib-home/) 
9. PsySound3, for physical and psycho-acoustical analysis of complex visual and auditory scenes (Cabrera, 
Ferguson, & Schubert, 2007) 
10. ParGram, grammar for Indonesian (Arka, 2012) 
11. INDRI, for information retrieval with large data sets (http://www.lemurproject.org/indri/) 
Table 2: Alveo Tools 
 
Most of these tools require significant expertise to set up and one of the Alveo project goals is to make 
this easier for non-technical researchers. The Alveo Workflow Engine is built around the Galaxy open 
source workflow management system (Goecks, Nekrutenko, Taylor, & Team, 2010), which was origi-
nally designed for use in the life sciences to support researchers in running pipelines of tools to ma-
nipulate data. Workflows in Galaxy can be stored, shared and published, and we hope this will also 
become a way for human communication science researchers to codify and exchange common anal-
yses.  
A number of the tools listed in Table 2 have been packaged as Python scripts, for instance NLTK 
based scripts to carry out part-of-speech tagging, stemming and parsing. Other tools are implemented 13
in R, e.g. EMU/R and ParseEval. An API is provided to mediate access to data, ensuring that permis-
sions are respected, and providing a way to access individual items, and 'mount' datasets for fast ac-
cess (Steve Cassidy, Estival, Jones, Burnham, & Burghold, 2014). An instance of the Galaxy Work-
flow engine is run on a virtual machine in the NeCTAR Research Cloud, a secure platform for Aus-
tralian research, funded by the same government program (https://www.nectar.org.au/research-cloud).  
Finally, a UIMA interface has been developed to enable the conversion of Alveo items, as well as their 
associated annotations, into UIMA CAS documents, for analysis in a conventional UIMA pipeline. 
Conversely annotations from a UIMA pipeline can be associated with a document in Alveo. Figure 1 
gives an overview of the architecture. 
 
 
Figure 1: The architecture of the Alveo Virtual Laboratory 
 
2.1 Annotations in Alveo 
Annotations in Alveo are stored in a standoff format based on the model described in the ISO Linguis-
tic Annotation Framework (ISO-LAF).  Internally, annotations are represented as RDF using the DA-
DA (Steve Cassidy, 2010). Each annotation is identified by a distinct URI and references into the 
source documents are stored as offsets either using character positions, times or frame counts for au-
dio/video data.  Annotations have an associated type attribute that denotes the kind of annotation 
(speaker turn, part of speech, phonetic segment) and a label that defines a simple string value associat-
ed with the annotation.  Annotations may also have other properties defined as standard RDF proper-
ties and values.   
The API exposes a direct interface to the annotation store in RDF via a SPARQL endpoint, but the 
normal mode of access is via the REST API where each item (document) has a corresponding URI 
that returns the collection of annotations on that item in a JSON-LD format.  JSON-LD allows us to 
represent the full RDF namespaces of properties and values in a concise format that is easily processed 
using standard JSON tools.  An example annotation delivered in this format is shown in Figure 2.  The 
same JSON-LD format can be used to upload new annotations to be stored in Alveo. 
 
{ 
 "@context": "https://app.alveo.edu.au/schema/json-ld", 
 "commonProperties": { 
    "alveo:annotates":"https://app?AusE08/document/GCSAusE07.mp3" 
 }, 
 "alveo:annotations": [ 
 { 
  "@id": "http://ns.ausnc.org.au/corpora/gcsause/annotation/535958", 
  "type": "http://ns.ausnc.org.au/schemas/annotation/conversation/micropause", 
  "@type": "dada:TextAnnotation", 
  "end": "34", 
  "start": "33" 
 }, 
?} 
Figure 2: An example of the Alveo JSON-LD annotation format 
 
 
 14
 3 UIMA 
3.1 Background 
The Unstructured Information Management Architecture (UIMA) is an Apache open source project 
(http://uima.apache.org) (D. Ferrucci & Lally, 2004) that provides an architecture for developing ap-
plications involving analysis of large volumes of unstructured information. This framework allows 
development of modular pipelines for analysing the sorts of data available in Alveo, including speech, 
text and video. A number of groups around the world have adopted UIMA to enable easier interopera-
bility and sharing of language technology components. This is true particularly in the biomedical natu-
ral language processing community; several groups have made tools available as UIMA modules. 
Most OpenNLP modules have been wrapped for use within UIMA, and the UIMA community more 
broadly has a range of language technology tools available in UIMA-compliant modules (David 
Ferrucci et al., 2010). 
Each component in a UIMA application implements interfaces defined by the framework and pro-
vides self-describing metadata via XML descriptor files. The framework manages these components 
and the data flow between them. Since UIMA applications are defined in terms of descriptors that 
clearly specify both the component modules of the application and the configuration parameter set-
tings for executing the application, they are ?re-runnable, re-usable procedures? of the kind that Alveo 
aims to capture.  
Given the objective of Alveo to facilitate access to analysis tools, and the UIMA objective of mak-
ing such analysis tools interoperable, bringing the two frameworks together made sense. Thus the ob-
jective of the integration was to build a bidirectional translation layer between Alveo and any standard 
UIMA pipeline. In other words, the translation component was required to: 1) read corpus data includ-
ing associated annotations stored in Alveo into a UIMA pipeline and 2) store annotations produced by 
a UIMA pipeline in Alveo. 
3.2 Overview of the Conversion Layer Architecture 
We opted for the most straightforward connection between the two frameworks, i.e. communicating 
directly with the Alveo REST API. The approach involves allowing annotations and documents to 
flow from Alveo, be processed externally to Alveo in a specially-configured UIMA pipeline, and then 
providing a mechanism for new annotations over the documents to be returned to Alveo for storage. 
The Alveo REST API provides access to item metadata, documents and annotations using a JSON-LD 
based interchange format.  The API supports most actions that are available via the web interface in-
cluding meta-data queries and retrieval of documents either individually or in batches.  
We built the UIMA-Alveo conversion layer, denoted Alveo-UIMA. It allows reading a group of 
documents from Alveo, and converting the documents along with their associated annotations into 
UIMA Common Annotation Structure (CAS) instances. It also allows annotations produced by a UI-
MA Collection Processing Engine (CPE) pipeline on a set of CASes to be uploaded to an Alveo serv-
er. Alveo-UIMA is built on top of a native Java wrapper for the Alveo REST API. It is implemented in 
Java and is distributed as an open-source package.2 The conversion layer exposes the Alveo data as 
native Java data structures and is also available as a standalone package,3 providing, as a side effect, a 
method to access the Alveo REST API without needing to invoke the UIMA machinery. Similar pack-
ages are also available for Python and R in the Alveo repository. 
The first component of the UIMA interface, for reading existing items, is implemented as UIMA 
Collection Reader. This takes as parameters an Alveo item list ID, corresponding to a user-created list 
of documents, and server credentials. It converts the Alveo items from that item list, as well as their 
associated annotations, into UIMA CAS documents, which can then be used as part of a conventional 
UIMA pipeline. The UIMA processing pipeline can then take advantage of the annotations download-
ed from Alveo (e.g. by using a speaker turn annotation to demarcate a sentence). 
                                                 
2 https://github.com/Alveo; https://github.com/Alveo/alveo-uima 
3 https://github.com/Alveo/alveo-java-rest-api 
15
The second component is for the opposite direction, i.e. taking annotations from a UIMA pipeline 
and associating them with the document in Alveo. There is no capability yet to add new documents 
that derive from outside Alveo, as this is not currently possible using Alveo?s REST API. This means 
that documents for which we are uploading UIMA annotations must have originated in Alveo and 
have come from the Collection Reader, ensuring that each document has appropriate identifying 
metadata for Alveo.  
Annotations need to be converted from Alveo to UIMA and vice versa, since the annotation formats 
are not identical. Some attributes, such as textual character offsets, are directly convertible, while oth-
ers require more work. Every document retrieved from the Alveo server has metadata (e.g. data 
source, recording date and language) and annotations (e.g. POS tags) associated with it. Converting 
metadata from Alveo to UIMA is straightforward, as the expected metadata fields can be directly 
mapped to a customised UIMA type. Converting annotations from between the frameworks requires 
more work, due to the required use of a type system in UIMA. This is discussed further below. 
3.3 Conversion from Alveo to UIMA 
An annotation in Alveo consists of a beginning and ending offset, which can correspond to a character 
span for textual data or a time span for audio-visual data, a human-readable plain-text label indicating 
additional attributes of the data, and a type (a URI indicating the kind of entity to which the annotation 
corresponds, e.g. ?speaker-turn?, ?intonation? or ?part-of-speech?). Since UIMA also encodes text 
annotations as character spans, these can be straightforwardly converted into the UIMA CAS (audio-
visual data can be similarly treated, but we have focused on text in the current work).  
UIMA allows the definition of custom data types with specific fields for storing salient values. We 
add a generic Alveo annotation type (inheriting from the standard UIMA Annotation, which means it 
still has spans attached). The label of the annotation in Alveo is a non-decomposable string, so this 
top-level type has a field to store the label as a string.  
Handling annotation types requires more care. Types in Alveo are encoded as fully-qualified URIs, 
while types in UIMA are more strictly defined. In particular, UIMA has a notion of a fully-specified 
type system associated with each pipeline specification, including a full inheritance hierarchy up to a 
root type and features corresponding to attributes of each type.  There are also minor differences be-
tween the encoding of the type names  (instead of a URI, UIMA uses a ?big-endian? qualified type 
name similar to a Java package, such as com.example.nlp.Sentence). 
In addition, UIMA component specification requires specifying in advance the type system to 
which all annotations represented within UIMA must conform. All these requirements are handled in a 
type system generation phase triggered when the Alveo-UIMA collection reader is created.4 During 
this phase, the reader requests an enumeration of all known type URIs from the Alveo server. Since 
we have no explicit additional information about type inheritance from the URIs, we make as few as-
sumptions as possible by having all types inherit from a generic Alveo annotation parent type. The 
Alveo URIs are automatically converted to UIMA types names, essentially by reversing the compo-
nents of the domain name, and replacing the ?/? character in the path component of the URI with ?.?, 
with some extra handling of non-alphanumeric characters, giving conversions such as the following: 
 http://example.com/nlp/sentence  ? com.example.nlp.Sentence  
In addition, the type URI is stored as an attribute of the UIMA annotation, providing an explicit record 
of the original Alveo type. Because it is far more natural to work with UIMA types than comparing 
string values when manipulating and filtering annotations in a UIMA pipeline, the automatically gen-
erated type system is very beneficial. 
We note that there have been proposals to simplify the internal UIMA type system through the use 
of a generic Referent type which refers to an external source of domain semantics (D. Ferrucci, 
Lally, Verspoor, & Nyberg, 2009) This would be a good strategy to pursue here, so that the UIMA 
annotations could refer explicitly to the Alveo URIs as an external type system. However, it has been 
noted previously that this representational choice has consequences for the indexing and reasoning 
over the semantics of annotations in UIMA analysis engines (Verspoor, Baumgartner Jr, Roeder, & 
                                                 
4 If the framework users are using UIMA canonically, where the type systems are described by pre-existing XML descriptors, 
they can explicitly request generation of the appropriate XML. The wrapper was primarily developed using UIMAFit 
(https://uimafit.apache.org/uimafit), which allows a more dynamic approach. 
16
Hunter, 2009). The current version of UIMA does not provide direct support for this model and hence 
our strategy is in line with current technical practice. These proposals aim to not replicate the full ex-
ternal type structure within the UIMA type system definition, and this is the practice we follow here, 
although since Alveo does not currently have a strong notion of type hierarchy this was not a signifi-
cant consideration. 
3.4 Conversion from UIMA to Alveo 
The annotation upload component is implemented as a UIMA CAS Consumer, i.e. a component which 
accepts CASes and performs some action with them. To upload annotations, as noted above, we ex-
pect the supplied CAS to derive originally from the Alveo server, with annotations added to that CAS 
by UIMA processing components. The original metadata from Alveo is used to determine the URL of 
the original item, and otherwise ignored.  
The first step in annotation upload is to retrieve the original source document and remove from the 
set of annotations to be uploaded those annotations which already appear in the version found on the 
server. In addition, since processing pipelines may produce a wide variety of annotations which may 
not all be appropriate or relevant for uploading to Alveo, the annotation type must occur in a precon-
figured whitelist. Converting each annotation from UIMA to Alveo is in some ways the inverse of the 
operation described in the previous section, although there are some intricacies to the process.  
The character spans can be directly converted as before; again the type and label require more work. 
For type conversion, some sensible default behaviours are used. A configurable list of UIMA features 
are inspected on the UIMA annotations, and the first match found is used as the Alveo annotation 
type. This list of features naturally includes the default feature for storing the type URI, ensuring that 
annotations which derive from Alveo originally can be matched back to the original annotation. If no 
matches are found, a type URI is inferred from the fully-qualified UIMA annotation type name, using 
the inverse operation to that described above.  
Alveo annotations also have labels, as noted in the previous section. As with the annotation types, 
there is a similar list of UIMA feature names which can be used to populate the label attribute on the 
Alveo annotation, defaulting to the empty string if no feature name is found. If these strategies do not 
produce the desired behaviour when uploading, it is possible to customise them by implementing a 
Java interface. Alternatively, it is also possible to insert a custom UIMA component into the pipeline 
to convert the added UIMA annotations so that the Alveo conversion works as desired. 
4 Alveo User Acceptance Testing 
As it was a requirement of the funding agency for the project to provide evidence of User Acceptance 
Testing (UAT) and acceptance of the results of these tests by the project governing body (the Steering 
Committee), the project was organised from the start around these requirements, with all the partners 
bidding for participation in tests of specific components or versions of the system. A testing schedule 
was developed to accompany the system development, with the aim of gathering feedback from the 
project partners during development to provide targeted input for improvement. A sub-committee of 
the Steering Committee was designated to oversee the tests distributed to the testers, examine the re-
ports summarising the results of those tests and recommend acceptance. 
Alveo was designed and implemented in partnership with Intersect, a commercial software devel-
opment company specialised in the support of academic eResearch. This partnership afforded exten-
sive professional support during development, using the Agile process (Beck & al, 2001) as well as 
thorough regression testing and debugging. In other projects of this type, Intersect provided UAT or 
managed the UAT process in-house. For the Alveo project, since user testing was the main way in 
which the academic partners were involved in the project, UAT was organised by the academic part-
ners with technical support from Intersect. The central team at the lead institution oversaw the creation 
of the tests (see section 4.2), distributed the tests and monitored the results.    
4.1 The Alveo UAT process 
During development, Alveo was deployed incrementally on separate servers. While the Production 
Server remained stable between versions, the Staging 1 server was reserved for UAT and only updated 
17
when new functionalities were added; the Staging 2 Server was used for development and frequently 
updated. This rarely caused problems, even with the distributed nature of the testing process. 
 Each partner site engaged High Degree Researchers (HDRs), generally Masters and Doctoral 
students but also Post-Doctoral Fellows or project members, who had an interest in a particular do-
main or tool, or who could provide critical comments about the functionalities. Some Testers were 
Linguistics students with no computing background, some were Computer Science students with lim-
ited linguistic knowledge. At some sites, the Testers were Research Assistants who had worked on the 
tools or corpora contributed by their institutions, while others were the tool developers themselves. 
This variety of backgrounds and skills ensured coverage of the main domains and functionalities ex-
pected of the Alveo Virtual Lab. Some sites had undertaken to conduct large amounts of testing 
throughout the development, while other partners only chose to perform limited or more targeted test-
ing, with commitments varying from 10 to 200 hours. Over 30 Testers participated at various times 
during of the project and a total of more than 300 hours has been spent on testing during Phase I. 
4.2 Evolution of the tests 
For each version of the system during development (Prototype, Version 1, 2, and 3) a series of tests 
were developed and posted on a Google Form. To record the results, the Testers filled out a Google 
form which was monitored by the central team. The first tests developed were very directive, giving 
very specific instructions as to what actions the user was asked to perform and what results were ex-
pected for each action, as shown in Figure 3, one of the tests for Version 1. 
 
 
Figure 3: Test for Alveo Version 1 
 
Gradually the tests became more open-ended, giving less guidance and gathering more informative 
feedback. The latest round of testing asked Testers to log in and to carry out a small research task, as 
shown in Figure 4, the instructions for the open form testing of Version 3. 
 
 
Figure 4: Open form test for Alveo Version 3 
 
Some of the early tests, such as the one shown in Figure 3, have become tutorials provided on the Al-
veo web page and are now available as help from within the Virtual Lab. 
Test 2 - Browsing COOEE 
1. Login to the main website. 
2. In the list of facets on the left, click Corpus, this should show a list of corpus names. 
3. From the list of corpus names click cooee, the page should update to show 1354 results, listing the first 10 
matching items from the COOEE corpus. 
4. In the list of facets on the left click Created, this should show a list of decades. 
5. From the list of decades click 1870-1879, the page should update to show 61 results which are COOEE items 
from the 1870s. 
6. From the list of matching items, click on the first item, the page should update to show the details of this 
item. Verify that the Created date is within the 1870s and that the isPartOf field shows cooee. 
7. Scroll down to the bottom of the page where you should see links to the documents in this item.  Click on 
the document marked Text(Original), you should see the text of the document including some markup at 
the start and end of the file. 
8. Use the Back button in your browser to return to the item display page, click on the document marked 
Text(Plain), you should see the text of the document with no markup. 
9. Use the Back button in your browser to return to the item display page. 
10. When you are finished, click on the HCSvLab logo on the top left of the page to return to the home page and 
reset your search. 
Based on your own research interests and based on what you've seen of the HCS vLab platform, please try to make 
use of the virtual lab to carry out a small research task. Use the form below to tell us about what you tried to do: the 
collections and tools that you used, a description of your task, the outcomes and any problems that you faced. 
18
4.3 UIMA Testing 
In order to test the Alveo-UIMA implementation and provide an example of how it can be used, we 
created a tutorial application5 available from the Alveo github repository. This tutorial shows an ex-
ample of instantiating a UIMA CPE pipeline which reads documents from an Alveo item list, aug-
ments it with part-of-speech annotations and uploads them to the Alveo server. A UIMA pipeline con-
sists of a collection reader, and one or more CAS annotators. The UIMA tutorial pipeline includes the 
standard Collection Reader from Alveo-UIMA, a basic POS-tagging CAS annotator from DKPro-
Core,6 and the annotation uploading CAS annotator from Alveo-UIMA. An advanced version also 
demonstrates implementing an interface which remaps the POS tag types from those automatically-
derived from DKPro.outputs. 
5 Discussion 
5.1 Related Work 
There are several frameworks that have been developed to enable development and evaluation of text 
processing workflows, and UIMA has been used as the backbone for a few such frameworks due to its 
support for processing module interoperability. The Argo web service (Rak, Rowley, & Ananiadou, 
2012; Rak, Rowley, Carter, & Ananiadou, 2013) is a recent web application that enables development 
of UIMA-based text processing workflows through an on-line graphical interface. In contrast to Al-
veo, documents are uploaded to the system within an individual user space, and resulting annotations 
are not persisted outside of the UIMA data structures; although they can be serialised and stored for 
subsequent re-use in processing pipelines, or exported as RDF, they are not directly accessible within 
the framework itself. The repository contains a wide range of NLP components, e.g., modules to per-
form sentence splitting, POS tagging, parsing, and a number of information extraction tasks targeted to 
biomedical text. 
The U-Compare system (Kano et al., 2009; Kano, Dorado, McCrohon, Ananiadou, & Tsujii, 2010) 
also supports evaluation and performance comparison of UIMA-based automated annotation tools. It 
was designed with UIMA in mind from the ground up, enabling UIMA workflow creation and execu-
tion through a GUI. Therefore it assumes that all analysis of collections is performed with a set of 
UIMA components, and indeed provides a substantial number of such components in their repository, 
although other components can be added. The system is launched locally via Java Web Start; given 
recent changes to how browsers interact with Java, this no longer works reliably and off-line use (after 
downloading and installing) is likely necessary, although interaction with web service-based pro-
cessing components is possible (Kontonasios, Korkontzelos, Kolluru, & Ananiadou, 2011). 
A competing framework based on the GATE architecture (Cunningham, Maynard, Bontcheva, & 
Tablan, 2002) is the cloud-based AnnoMarket platform. This framework provides access to natural 
language processing (NLP) components, and a limited number of existing resources (one at the time of 
writing7, with the facility to upload user-specific data) on a fee-for-service basis (passing along costs 
of using the Amazon cloud services). Results of NLP studies of this data can be downloaded, or in-
dexed and made available for search. There are a wide array of annotation services and pre-configured 
pipelines available within the AnnoMarket that can be applied to a user?s document collection, either 
directly through the on-line application or via a web service API. 
5.2 Alternative strategies for UIMA integration with Alveo 
There were several possible places where the UIMA-Alveo translation layer could have been inserted, 
and indeed several possible architectures were considered for integrating UIMA with Alveo.  
Since Alveo was already working with the workflow engine Galaxy, one option was to create a 
compatibility layer to bridge UIMA with Galaxy, for instance to enable a pre-configured UIMA pipe-
line to be instantiated via a Galaxy wrapper. The technical details for accomplishing this were not im-
mediately obvious, and it was decided that this approach would add substantial complexity to the con-
version of annotations in the conversion layer. 
                                                 
5 http://github.com/Alveo/alveo-uima-tutorial 
6 https://code.google.com/p/dkpro-core-asl/ 
7 https://annomarket.com/dataSources, accessed 29 May 2014 
19
Another option that was considered was to allow for dynamic construction of UIMA workflows 
from UIMA components directly through the Alveo web interface. UIMA is a workflow engine analo-
gous to Galaxy, in that it enables dynamic configuration of pipelines from the available set of UIMA 
components set up in a given environment. In principle, therefore, it would be possible to enable spec-
ification, instantiation, and execution of UIMA pipelines from a set of UIMA components made avail-
able via Alveo. However, this would have required a substantial development effort specifically tar-
geted towards hosting UIMA components and manipulating UIMA pipelines; it was decided that a 
more general approach to integrating a broader range of tools was more appropriate for Alveo. Given 
the recent availability of the Argo web application, an Alveo/Argo integration could be considered that 
would enable users to create UIMA workflows with Argo but execute them from Alveo, and on doc-
uments or corpora stored in Alveo (Rak et al., 2012; Rak et al., 2013). The current web service-based 
architecture of the Alveo-UIMA integration lends itself well to this possibility. This could be explored 
in future work. 
The current implementation assumes that an Alveo user will have the knowledge to create and run 
UIMA pipelines externally to Alveo. A complementary strategy, possible now that the conversion lay-
er is in place, would be to make complete, pre-configured UIMA pipelines available as tools that can 
be applied to Alveo corpora/data. A number of such services, e.g. services aimed at annotation of text 
with one of a set of biomedically-relevant entity types (diseases, genes, chemicals) have been built 
(MacKinlay & Verspoor, 2013). Each such service is run as a separate UIMA instance that is accessed 
via a web service.  Text is passed in via the REST interface, handed over to the UIMA instance, pro-
cessed, and annotations are returned. This basic model could be replicated for a number of UIMA 
pipelines that do standard text-related processing (e.g. split sentences, perform part of speech tagging 
and parsing, etc.) such that text extracted from Alveo could be processed by the UIMA-based service 
and annotations returned. This approach has been criticised for its inability to be extended or adapted 
(Tablan, Bontcheva, Roberts, Cunningham, & Dimitrov, 2013) although it is suitable where pre-
packaged pipelines can be applied to accomplish tasks of broad interest. 
6 Conclusions 
The development of Alveo presented a number of challenges, some technical, such as the integration 
of UIMA with the platform, and others more logistic, such as the distributed nature of testing during 
development. In this paper, we described the solution and the choices we made for the implementation 
of UIMA pipelines, given the constraints regarding the organisation of items, documents and their as-
sociated annotations in Alveo. One of the conditions of success of such a project is that the platform 
be used by researchers for their own projects and on their own data. The organisation of the User Ac-
ceptance Testing, requiring partners to contribute during the development, and providing exposure to 
the tools and the datasets to a large group of diverse researchers is expected to lead to a much wider 
uptake of Alveo as a platform for HCS research in Australia. We plan to open it to users outside the 
original project partners during Phase II (2014-2016). We will also continue to explore further interac-
tions with complementary frameworks, such that the data and annotation storage available in Alveo 
can be enhanced via processing and tools from external services to supplement the functionality that is 
currently directly integrated. 
 
Acknowledgements 
We gratefully acknowledge funding from the Australian Government National eResearch Collabora-
tion Tools and Resources (NeCTAR) and thank all our collaborating partners in the Alveo Virtual La-
boratory project: University of Western Sydney, Macquarie University, RMIT, University of Mel-
bourne, Australian National University, University of Western Australia, University of Sydney, Uni-
versity of New England, University of Canberra, Flinders University, University of New South Wales, 
University of La Trobe, University of Tasmania, ASSTA, AusNC Inc., NICTA, and Intersect. 
 
 
 
20
References 
Arka, I. W. (2012). Developing a Deep Grammar of Indonesian within the ParGram Framework: Theoretical 
and Implementational Challenges Paper presented at the 26th Pacific Asia Conference on 
Language,Information and Computation.  
Beck, K., et al. (2001). Manifesto for Agile Software Development. http://agilemanifesto.org/ 
Bird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python - Analyzing Text with the 
Natural Language Toolkit: O'Reilly Media. 
Burnham, D., Estival, D., Fazio, S., Cox, F., Dale, R., Viethen, J., . . . Wagner, M. (2011). Building an audio-
visual corpus of Australian English: large corpus collection with an economical portable and 
replicable Black Box. Paper presented at the Interspeech 2011, Florence, Italy.  
Cabrera, D., Ferguson, S., & Schubert, E. (2007). 'Psysound3': Software for Acoustical and Psychoacoustical 
Analysis of Sound Recordings. Paper presented at the International Community on Auditory Display.  
Cassidy, S. (2010). An RDF Realisation of LAF in the DADA Annotation Server. Paper presented at the ISA-5, 
Hong Kong.  
Cassidy, S., Estival, D., Jones, T., Burnham, D., & Burghold, J. (2014). The Alveo Virtual Laboratory: A Web 
Based Repository API. Paper presented at the 9th Language Resources and Evaluation Conference 
(LREC 2014), Reykjavik, Iceland.  
Cassidy, S., & Harrington, J. (2000). Multi-level Annotation in the Emu Speech Database Management System. 
Speech Communication, 33, 61?77.  
Cassidy, S., Haugh, M., Peters, P., & Fallu, M. (2012). The Australian National Corpus : national infrastructure 
for language resources. Paper presented at the LREC.  
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. Paper 
presented at the 43rd Annual Meeting on Association for Computational Linguistics.  
Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V. (2002). GATE: A Framework and Graphical 
Development Environment for Robust NLP Tools and Applications. Paper presented at the 40th 
Anniversary Meeting of the Association for Computational Linguistics (ACL'02), Philadelphia, USA.  
Estival, D., Cassidy, S., Sefton, P., & Burnham, D. (2013). The Human Communication Science Virtual Lab. 
Paper presented at the 7th eResearch Australasia Conference, Brisbane, Australia.  
Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A. A., . . . Welty, C. (2010). Building 
Watson: An Overview of the DeepQA Project. AI Magazine, 31(3), 59-79. doi: 
http://dx.doi.org/10.1609/aimag.v31i3.2303  
Ferrucci, D., & Lally, A. (2004). UIMA: an architectural approach to unstructured information processing in the 
corporate research environme. Natural Language Engineering, 10(3-4), 327-348.  
Ferrucci, D., Lally, A., Verspoor, K., & Nyberg, A. (2009). Unstructured Information Management Architecture 
(UIMA) Version 1.0 Oasis Standard. 
Goecke, R., & Millar, J. B. (2004). The Audio-Video Australian English Speech Data Corpus AVOZES. Paper 
presented at the 8th International Conference on Spoken Language Processing (INTERSPEECH 2004 - 
ICSLP), Jeju, Korea.  
Goecks, J., Nekrutenko, A., Taylor, J., & Team, T. G. (2010). Galaxy: a comprehensive approach for supporting 
accessible, reproducible, and transparent computational research in the life sciences. Genome Biology, 
11(8), R86.  
Kano, Y., Baumgartner, W. A., McCrohon, L., Ananiadou, S., Cohen, K. B., Hunter, L., & Tsujii, J. I. (2009). U-
Compare: share and compare text mining tools with UIMA. Bioinformatics, 25(15), 1997-1998.  
Kano, Y., Dorado, R., McCrohon, L., Ananiadou, S., & Tsujii, J. (2010). U-Compare: An Integrated Language 
Resource Evaluation Platform Including a Comprehensive UIMA Resource Library. Paper presented at 
the LREC. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.8878&rep=rep1&type=pdf 
Kontonasios, G., Korkontzelos, I., Kolluru, B., & Ananiadou, S. (2011). Adding text mining workflows as web 
services to the BioCatalogue. Paper presented at the Proceedings of the 4th International Workshop on 
Semantic Web Applications and Tools for the Life Sciences (SWAT4LS '11 ).  
MacKinlay, A., & Verspoor, K. (2013). A Web Service Annotation Framework for CTD Using the UIMA 
Concept Mapper. Paper presented at the Fourth BioCreative Challenge Evaluation Workshop. 
http://www.biocreative.org/media/store/files/2013/bc4_v1_14.pdf 
Musgrave, S., & Haugh, M. (2009). The AusNC Project: Plans, Progress and Implications for Language 
Technology. Paper presented at the ALTA 2009, Sydney.  
Rak, R., Rowley, A., & Ananiadou, S. (2012). Collaborative Development and Evaluation of Text-processing 
Workflows in a UIMA-supported Web-based Workbench. Paper presented at the LREC. 
http://www.lrec-conf.org/proceedings/lrec2012/pdf/960_Paper.pdf 
Rak, R., Rowley, A., Carter, J., & Ananiadou, S. (2013). Development and Analysis of NLP Pipelines in Argo. 
Paper presented at the ACL. http://aclweb.org/anthology//P/P13/P13-4020.pdf 21
Shaw, J. A., & Gafos, A. I. (2010). Quantitative evaluation of competing syllable parses. Paper presented at the 
11th Meeting of the Association for Computational Linguistics. Special Interest Group on 
Computational Morphology and Phonology, Uppsala, Sweden.  
Tablan, V., Bontcheva, K., Roberts, I., Cunningham, H., & Dimitrov, M. (2013). AnnoMarket: An Open Cloud 
Platform for NLP. Paper presented at the 51st Annual Meeting of the Association for Computational 
Linguistics (ACL 2013), Sofia, Bulgaria.  
Thieberger, N., Barwick, L., Billington, R., & Vaughan, J. (Eds.). (2011). Sustainable data from digital 
research: Humanities perspectives on digital scholarship. A PARDISEC Conference: Custom Book 
Centre. http://ses.library.usyd.edu.au/handle/2123/7890. 
Verspoor, K., Baumgartner Jr, W., Roeder, C., & Hunter, L. (2009). Abstracting the types away from a UIMA 
type system From Form to Meaning: Processing Texts Automatically (pp. 249-256). 
 
22
