Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 127?135,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Multilingual Subjectivity Analysis Using Machine Translation
Carmen Banea and Rada Mihalcea
University of North Texas
carmenb@unt.edu, rada@cs.unt.edu
Janyce Wiebe
University of Pittsburgh
wiebe@cs.pitt.edu
Samer Hassan
University of North Texas
samer@unt.edu
Abstract
Although research in other languages is in-
creasing, much of the work in subjectivity
analysis has been applied to English data,
mainly due to the large body of electronic re-
sources and tools that are available for this lan-
guage. In this paper, we propose and evalu-
ate methods that can be employed to transfer a
repository of subjectivity resources across lan-
guages. Specifically, we attempt to leverage
on the resources available for English and, by
employing machine translation, generate re-
sources for subjectivity analysis in other lan-
guages. Through comparative evaluations on
two different languages (Romanian and Span-
ish), we show that automatic translation is a
viable alternative for the construction of re-
sources and tools for subjectivity analysis in
a new target language.
1 Introduction
We have seen a surge in interest towards the ap-
plication of automatic tools and techniques for the
extraction of opinions, emotions, and sentiments in
text (subjectivity). A large number of text process-
ing applications have already employed techniques
for automatic subjectivity analysis, including auto-
matic expressive text-to-speech synthesis (Alm et
al., 2005), text semantic analysis (Wiebe and Mihal-
cea, 2006; Esuli and Sebastiani, 2006), tracking sen-
timent timelines in on-line forums and news (Lloyd
et al, 2005; Balog et al, 2006), mining opinions
from product reviews (Hu and Liu, 2004), and ques-
tion answering (Yu and Hatzivassiloglou, 2003).
A significant fraction of the research work to date
in subjectivity analysis has been applied to English,
which led to several resources and tools available for
this language. In this paper, we explore multiple
paths that employ machine translation while lever-
aging on the resources and tools available for En-
glish, to automatically generate resources for sub-
jectivity analysis for a new target language. Through
experiments carried out with automatic translation
and cross-lingual projections of subjectivity annota-
tions, we try to answer the following questions.
First, assuming an English corpus manually an-
notated for subjectivity, can we use machine trans-
lation to generate a subjectivity-annotated corpus in
the target language? Second, assuming the availabil-
ity of a tool for automatic subjectivity analysis in
English, can we generate a corpus annotated for sub-
jectivity in the target language by using automatic
subjectivity annotations of English text and machine
translation? Finally, third, can these automatically
generated resources be used to effectively train tools
for subjectivity analysis in the target language?
Since our methods are particularly useful for lan-
guages with only a few electronic tools and re-
sources, we chose to conduct our initial experiments
on Romanian, a language with limited text process-
ing resources developed to date. Furthermore, to
validate our results, we carried a second set of ex-
periments on Spanish. Note however that our meth-
ods do not make use of any target language specific
knowledge, and thus they are applicable to any other
language as long as a machine translation engine ex-
ists between the selected language and English.
127
2 Related Work
Research in sentiment and subjectivity analysis has
received increasingly growing interest from the nat-
ural language processing community, particularly
motivated by the widespread need for opinion-based
applications, including product and movie reviews,
entity tracking and analysis, opinion summarization,
and others.
Much of the work in subjectivity analysis has
been applied to English data, though work on other
languages is growing: e.g., Japanese data are used
in (Kobayashi et al, 2004; Suzuki et al, 2006;
Takamura et al, 2006; Kanayama and Nasukawa,
2006), Chinese data are used in (Hu et al, 2005),
and German data are used in (Kim and Hovy, 2006).
In addition, several participants in the Chinese
and Japanese Opinion Extraction tasks of NTCIR-
6 (Kando and Evans, 2007) performed subjectivity
and sentiment analysis in languages other than En-
glish.
In general, efforts on building subjectivity analy-
sis tools for other languages have been hampered by
the high cost involved in creating corpora and lexical
resources for a new language. To address this gap,
we focus on leveraging resources already developed
for one language to derive subjectivity analysis tools
for a new language. This motivates the direction of
our research, in which we use machine translation
coupled with cross-lingual annotation projections to
generate the resources and tools required to perform
subjectivity classification in the target language.
The work closest to ours is the one reported in
(Mihalcea et al, 2007), where a bilingual lexicon
and a manually translated parallel text are used to
generate the resources required to build a subjectiv-
ity classifier in a new language. In that work, we
found that the projection of annotations across par-
allel texts can be successfully used to build a cor-
pus annotated for subjectivity in the target language.
However, parallel texts are not always available for
a given language pair. Therefore, in this paper we
explore a different approach where, instead of rely-
ing on manually translated parallel corpora, we use
machine translation to produce a corpus in the new
language.
3 Machine Translation for Subjectivity
Analysis
We explore the possibility of using machine transla-
tion to generate the resources required to build sub-
jectivity annotation tools in a given target language.
We focus on two main scenarios. First, assuming a
corpus manually annotated for subjectivity exists in
the source language, we can use machine translation
to create a corpus annotated for subjectivity in the
target language. Second, assuming a tool for auto-
matic subjectivity analysis exists in the source lan-
guage, we can use this tool together with machine
translation to create a corpus annotated for subjec-
tivity in the target language.
In order to perform a comprehensive investiga-
tion, we propose three experiments as described be-
low. The first scenario, based on a corpus manu-
ally annotated for subjectivity, is exemplified by the
first experiment. The second scenario, based on a
corpus automatically annotated with a tool for sub-
jectivity analysis, is subsequently divided into two
experiments depending on the direction of the trans-
lation and on the dataset that is translated.
In all three experiments, we use English as a
source language, given that it has both a corpus man-
ually annotated for subjectivity (MPQA (Wiebe et
al., 2005)) and a tool for subjectivity analysis (Opin-
ionFinder (Wiebe and Riloff, 2005)).
3.1 Experiment One: Machine Translation of
Manually Annotated Corpora
In this experiment, we use a corpus in the source
language manually annotated for subjectivity. The
corpus is automatically translated into the target lan-
guage, followed by a projection of the subjectivity
labels from the source to the target language. The
experiment is illustrated in Figure 1.
We use the MPQA corpus (Wiebe et al, 2005),
which is a collection of 535 English-language news
articles from a variety of news sources manually an-
notated for subjectivity. Although the corpus was
originally annotated at clause and phrase level, we
use the sentence-level annotations associated with
the dataset (Wiebe and Riloff, 2005). From the total
of 9,700 sentences in this corpus, 55% of the sen-
tences are labeled as subjective while the rest are
objective. After the automatic translation of the cor-
128
Figure 1: Experiment one: machine translation of man-
ually annotated training data from source language into
target language
pus and the projection of the annotations, we obtain
a large corpus of 9,700 subjectivity-annotated sen-
tences in the target language, which can be used to
train a subjectivity classifier.
3.2 Experiment Two: Machine Translation of
Source Language Training Data
In the second experiment, we assume that the only
resources available are a tool for subjectivity anno-
tation in the source language and a collection of raw
texts, also in the source language. The source lan-
guage text is automatically annotated for subjectiv-
ity and then translated into the target language. In
this way, we produce a subjectivity annotated cor-
pus that we can use to train a subjectivity annotation
tool for the target language. Figure 2 illustrates this
experiment.
In order to generate automatic subjectivity anno-
tations, we use the OpinionFinder tool developed by
(Wiebe and Riloff, 2005). OpinionFinder includes
two classifiers. The first one is a rule-based high-
precision classifier that labels sentences based on the
presence of subjective clues obtained from a large
lexicon. The second one is a high-coverage classi-
fier that starts with an initial corpus annotated us-
ing the high-precision classifier, followed by several
bootstrapping steps that increase the size of the lex-
icon and the coverage of the classifier. For most of
our experiments we use the high-coverage classifier.
Figure 2: Experiment two: machine translation of raw
training data from source language into target language
Table 1 shows the performance of the two Opinion-
Finder classifiers as measured on the MPQA corpus
(Wiebe and Riloff, 2005).
P R F
high-precision 86.7 32.6 47.4
high-coverage 79.4 70.6 74.7
Table 1: Precision (P), Recall (R) and F-measure (F) for
the two OpinionFinder classifiers, as measured on the
MPQA corpus
As a raw corpus, we use a subset of the SemCor
corpus (Miller et al, 1993), consisting of 107 docu-
ments with roughly 11,000 sentences. This is a bal-
anced corpus covering a number of topics in sports,
politics, fashion, education, and others. The reason
for working with this collection is the fact that we
also have a manual translation of the SemCor docu-
ments from English into one of the target languages
used in the experiments (Romanian), which enables
comparative evaluations of different scenarios (see
Section 4).
Note that in this experiment the annotation of sub-
jectivity is carried out on the original source lan-
guage text, and thus expected to be more accurate
than if it were applied on automatically translated
text. However, the training data in the target lan-
guage is produced by automatic translation, and thus
likely to contain errors.
129
3.3 Experiment Three: Machine Translation of
Target Language Training Data
The third experiment is similar to the second one,
except that we reverse the direction of the transla-
tion. We translate raw text that is available in the
target language into the source language, and then
use a subjectivity annotation tool to label the auto-
matically translated source language text. After the
annotation, the labels are projected back into the tar-
get language, and the resulting annotated corpus is
used to train a subjectivity classifier. Figure 3 illus-
trates this experiment.
Figure 3: Experiment three: machine translation of raw
training data from target language into source language
As before, we use the high-coverage classifier
available in OpinionFinder, and the SemCor corpus.
We use a manual translation of this corpus available
in the target language.
In this experiment, the subjectivity annotations
are carried out on automatically generated source
text, and thus expected to be less accurate. How-
ever, since the training data was originally written
in the target language, it is free of translation errors,
and thus training carried out on this data should be
more robust.
3.4 Upper bound: Machine Translation of
Target Language Test Data
For comparison purposes, we also propose an ex-
periment which plays the role of an upper bound on
the methods described so far. This experiment in-
volves the automatic translation of the test data from
the target language into the source language. The
source language text is then annotated for subjectiv-
ity using OpinionFinder, followed by a projection of
the resulting labels back into the target language.
Unlike the previous three experiments, in this
experiment we only generate subjectivity-annotated
resources, and we do not build and evaluate a stan-
dalone subjectivity analysis tool for the target lan-
guage. Further training of a machine learning algo-
rithm, as in experiments two and three, is required in
order to build a subjectivity analysis tool. Thus, this
fourth experiment is an evaluation of the resources
generated in the target language, which represents
an upper bound on the performance of any machine
learning algorithm that would be trained on these re-
sources. Figure 4 illustrates this experiment.
Figure 4: Upper bound: machine translation of test data
from target language into source language
4 Evaluation and Results
Our initial evaluations are carried out on Romanian.
The performance of each of the three methods is
evaluated using a dataset manually annotated for
subjectivity. To evaluate our methods, we generate a
Romanian training corpus annotated for subjectivity
on which we train a subjectivity classifier, which is
then used to label the test data.
We evaluate the results against a gold-standard
corpus consisting of 504 Romanian sentences man-
ually annotated for subjectivity. These sentences
represent the manual translation into Romanian of
a small subset of the SemCor corpus, which was
removed from the training corpora used in experi-
ments two and three. This is the same evaluation
dataset as used in (Mihalcea et al, 2007). Two
Romanian native speakers annotated the sentences
individually, and the differences were adjudicated
130
through discussions. The agreement of the two an-
notators is 0.83% (? = 0.67); when the uncertain an-
notations are removed, the agreement rises to 0.89
(? = 0.77). The two annotators reached consensus
on all sentences for which they disagreed, resulting
in a gold standard dataset with 272 (54%) subjective
sentences and 232 (46%) objective sentences. More
details about this dataset are available in (Mihalcea
et al, 2007).
In order to learn from our annotated data, we ex-
periment with two different classifiers, Na??ve Bayes
and support vector machines (SVM), selected for
their performance and diversity of learning method-
ology. For Na??ve Bayes, we use the multinomial
model (McCallum and Nigam, 1998) with a thresh-
old of 0.3. For SVM (Joachims, 1998), we use the
LibSVM implementation (Fan et al, 2005) with a
linear kernel.
The automatic translation of the MPQA and of
the SemCor corpus was performed using Language
Weaver,1 a commercial statistical machine transla-
tion software. The resulting text was post-processed
by removing diacritics, stopwords and numbers. For
training, we experimented with a series of weight-
ing schemes, yet we only report the results obtained
for binary weighting, as it had the most consistent
behavior.
The results obtained by running the three experi-
ments on Romanian are shown in Table 2. The base-
line on this data set is 54.16%, represented by the
percentage of sentences in the corpus that are sub-
jective, and the upper bound (UB) is 71.83%, which
is the accuracy obtained under the scenario where
the test data is translated into the source language
and then annotated using the high-coverage Opin-
ionFinder tool.
Perhaps not surprisingly, the SVM classifier out-
performs Na??ve Bayes by 2% to 6%, implying that
SVM may be better fitted to lessen the amount of
noise embedded in the dataset and provide more ac-
curate classifications.
The first experiment, involving the automatic
translation of the MPQA corpus enhanced with man-
ual annotations for subjectivity at sentence level,
does not seem to perform well when compared to the
experiments in which automatic subjectivity classi-
1http://www.languageweaver.com/
Romanian
Exp Classifier P R F
E1 Na??ve Bayes 60.91 60.91 60.91
SVM 66.07 66.07 66.07
E2 Na??ve Bayes 63.69 63.69 63.69
SVM 69.44 69.44 69.44
E3 Na??ve Bayes 65.87 65.87 65.87
SVM 67.86 67.86 67.86
UB OpinionFinder 71.83 71.83 71.83
Table 2: Precision (P), Recall (R) and F-measure (F) for
Romanian experiments
fication is used. This could imply that a classifier
cannot be so easily trained on the cues that humans
use to express subjectivity, especially when they are
not overtly expressed in the sentence and thus can
be lost in the translation. Instead, the automatic
annotations produced with a rule-based tool (Opin-
ionFinder), relying on overt mentions of words in
a subjectivity lexicon, seems to be more robust to
translation, further resulting in better classification
results. To exemplify, consider the following sub-
jective sentence from the MPQA corpus, which does
not include overt clues of subjectivity, but was an-
notated as subjective by the human judges because
of the structure of the sentence: It is the Palestini-
ans that are calling for the implementation of the
agreements, understandings, and recommendations
pertaining to the Palestinian-Israeli conflict.
We compare our results with those obtained by
a previously proposed method that was based on
the manual translation of the SemCor subjectivity-
annotated corpus. In (Mihalcea et al, 2007), we
used the manual translation of the SemCor corpus
into Romanian to form an English-Romanian par-
allel data set. The English side was annotated us-
ing the Opinion Finder tool, and the subjectivity la-
bels were projected on the Romanian text. A Na??ve
Bayes classifier was then trained on the subjectivity
annotated Romanian corpus and tested on the same
gold standard as used in our experiments. Table 3
shows the results obtained in those experiments by
using the high-coverage OpinionFinder classifier.
Among our experiments, experiments two and
three are closest to those proposed in (Mihalcea
et al, 2007). By using machine translation, from
131
OpinionFinder classifier P R F
high-coverage 67.85 67.85 67.85
Table 3: Precision (P), Recall (R) and F-measure (F) for
subjectivity analysis in Romanian obtained by using an
English-Romanian parallel corpus
English into Romanian (experiment two) or Roma-
nian into English (experiment three), and annotating
this dataset with the high-coverage OpinionFinder
classifier, we obtain an F-measure of 63.69%, and
65.87% respectively, using Na??ve Bayes (the same
machine learning classifier as used in (Mihalcea et
al., 2007)). This implies that at most 4% in F-
measure can be gained by using a parallel corpus as
compared to an automatically translated corpus, fur-
ther suggesting that machine translation is a viable
alternative to devising subjectivity classification in a
target language leveraged on the tools existent in a
source language.
As English is a language with fewer inflections
when compared to Romanian, which accommodates
for gender and case as a suffix to the base form of a
word, the automatic translation into English is closer
to a human translation (experiment three). Therefore
labeling this data using the OpinionFinder tool and
projecting the labels onto a fully inflected human-
generated Romanian text provides more accurate
classification results, as compared to a setup where
the training is carried out on machine-translated Ro-
manian text (experiment two).
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 5: Experiment two: Machine learning F-measure
over an incrementally larger training set
We also wanted to explore the impact that the cor-
 0.5
 0.55
 0.6
 0.65
 0.7
 0.2  0.4  0.6  0.8  1
F-
m
ea
su
re
Percentage of corpus
NB
SVM
Figure 6: Experiment three: Machine learning F-measure
over an incrementally larger training set
pus size may have on the accuracy of the classifiers.
We re-ran experiments two and three with 20% cor-
pus size increments at a time (Figures 5 and 6). It
is interesting to note that a corpus of approximately
6000 sentences is able to achieve a high enough F-
measure (around 66% for both experiments) to be
considered viable for training a subjectivity classi-
fier. Also, at a corpus size over 10,000 sentences, the
Na??ve Bayes classifier performs worse than SVM,
which displays a directly proportional trend between
the number of sentences in the data set and the ob-
served F-measure. This trend could be explained
by the fact that the SVM classifier is more robust
with regard to noisy data, when compared to Na??ve
Bayes.
5 Portability to Other Languages
To test the validity of the results on other languages,
we ran a portability experiment on Spanish.
To build a test dataset, a native speaker of Span-
ish translated the gold standard of 504 sentences into
Spanish. We maintain the same subjectivity anno-
tations as for the Romanian dataset. To create the
training data required by the first two experiments,
we translate both the MPQA corpus and the Sem-
Cor corpus into Spanish using the Google Transla-
tion service,2 a publicly available machine transla-
tion engine also based on statistical machine transla-
tion. We were therefore able to implement all the ex-
periments but the third, which would have required
2http://www.google.com/translate t
132
a manually translated version of the SemCor corpus.
Although we could have used a Spanish text to carry
out a similar experiment, due to the fact that the
dataset would have been different, the results would
not have been directly comparable.
The results of the two experiments exploring the
portability to Spanish are shown in Table 4. Inter-
estingly, all the figures are higher than those ob-
tained for Romanian. We assume this occurs be-
cause Spanish is one of the six official United Na-
tions languages, and the Google translation engine
is using the United Nations parallel corpus to train
their translation engine, therefore implying that a
better quality translation is achieved as compared to
the one available for Romanian. We can therefore
conclude that the more accurate the translation en-
gine, the more accurately the subjective content is
translated, and therefore the better the results. As it
was the case for Romanian, the SVM classifier pro-
duces the best results, with absolute improvements
over the Na??ve Bayes classifier ranging from 0.2%
to 3.5%.
Since the Spanish automatic translation seems to
be closer to a human-quality translation, we are not
surprised that this time the first experiment is able
to generate a more accurate training corpus as com-
pared to the second experiment. The MPQA corpus,
since it is manually annotated and of better quality,
has a higher chance of generating a more reliable
data set in the target language. As in the experiments
on Romanian, when performing automatic transla-
tion of the test data, we obtain the best results with
an F-measure of 73.41%, which is also the upper
bound on our proposed experiments.
Spanish
Exp Classifier P R F
E1 Na??ve Bayes 65.28 65.28 65.28
SVM 68.85 68.85 68.85
E2 Na??ve Bayes 62.50 62.50 62.50
SVM 62.70 62.70 62.70
UB OpinionFinder 73.41 73.41 73.41
Table 4: Precision (P), Recall (R) and F-measure (F) for
Spanish experiments
6 Discussion
Based on our experiments, we can conclude that ma-
chine translation offers a viable approach to gener-
ating resources for subjectivity annotation in a given
target language. The results suggest that either a
manually annotated dataset or an automatically an-
notated one can provide sufficient leverage towards
building a tool for subjectivity analysis.
Since the use of parallel corpora (Mihalcea et al,
2007) requires a large amount of manual labor, one
of the reasons behind our experiments was to asses
the ability of machine translation to transfer subjec-
tive content into a target language with minimal ef-
fort. As demonstrated by our experiments, machine
translation offers a viable alternative in the construc-
tion of resources and tools for subjectivity classifica-
tion in a new target language, with only a small de-
crease in performance as compared to the case when
a parallel corpus is available and used.
To gain further insights, two additional experi-
ments were performed. First, we tried to isolate the
role played by the quality of the subjectivity anno-
tations in the source-language for the cross-lingual
projections of subjectivity. To this end, we used the
high-precision OpinionFinder classifier to annotate
the English datasets. As shown in Table 1, this clas-
sifier has higher precision but lower recall as com-
pared to the high-coverage classifier we used in our
previous experiments. We re-ran the second exper-
iment, this time trained on the 3,700 sentences that
were classified by the OpinionFinder high-precision
classifier as either subjective or objective. For Ro-
manian, we obtained an F-measure of 69.05%, while
for Spanish we obtained an F-measure of 66.47%.
Second, we tried to isolate the role played by
language-specific clues of subjectivity. To this end,
we decided to set up an experiment which, by com-
parison, can suggest the degree to which the lan-
guages are able to accommodate specific markers for
subjectivity. First, we trained an English classifier
using the SemCor training data automatically anno-
tated for subjectivity with the OpinionFinder high-
coverage tool. The classifier was then applied to the
English version of the manually labeled test data set
(the gold standard described in Section 4). Next, we
ran a similar experiment on Romanian, using a clas-
sifier trained on the Romanian version of the same
133
SemCor training data set, annotated with subjectiv-
ity labels projected from English. The classifier was
tested on the same gold standard data set. Thus, the
two classifiers used the same training data, the same
test data, and the same subjectivity annotations, the
only difference being the language used (English or
Romanian).
The results for these experiments are compiled in
Table 5. Interestingly, the experiment conducted on
Romanian shows an improvement of 3.5% to 9.5%
over the results obtained on English, which indi-
cates that subjective content may be easier to learn
in Romanian versus English. The fact that Roma-
nian verbs are inflected for mood (such as indicative,
conditional, subjunctive, presumptive), enables an
automatic classifier to identify additional subjective
markers in text. Some moods such as conditional
and presumptive entail human judgment, and there-
fore allow for clear subjectivity annotation. More-
over, Romanian is a highly inflected language, ac-
commodating for forms of various words based on
number, gender, case, and offering an explicit lex-
icalization of formality and politeness. All these
features may have a cumulative effect in allowing
for better classification. At the same time, English
entails minimal inflection when compared to other
Indo-European languages, as it lacks both gender
and adjective agreement (with very few notable ex-
ceptions such as beautiful girl and handsome boy).
Verb moods are composed with the aid of modals,
while tenses and expressions are built with the aid
of auxiliary verbs. For this reason, a machine learn-
ing algorithm may not be able to identify the same
amount of information on subjective content in an
English versus a Romanian text. It is also interesting
to note that the labeling of the training set was per-
formed using a subjectivity classifier developed for
English, which takes into account a large, human-
annotated, subjectivity lexicon also developed for
English. One would have presumed that any clas-
sifier trained on this annotated text would therefore
provide the best results in English. Yet, as explained
earlier, this was not the case.
7 Conclusion
In this paper, we explored the use of machine trans-
lation for creating resources and tools for subjec-
Exp Classifier P R F
En Na??ve Bayes 60.32 60.32 60.32
SVM 60.32 60.32 60.32
Ro Na??ve Bayes 67.85 67.85 67.85
SVM 69.84 69.84 69.84
Table 5: Precision (P), Recall (R) and F-measure (F) for
identifying language specific information
tivity analysis in other languages, by leveraging on
the resources available in English. We introduced
and evaluated three different approaches to generate
subjectivity annotated corpora in a given target lan-
guage, and exemplified the technique on Romanian
and Spanish.
The experiments show promising results, as they
are comparable to those obtained using manually
translated corpora. While the quality of the trans-
lation is a factor, machine translation offers an effi-
cient and effective alternative in capturing the sub-
jective semantics of a text, coming within 4% F-
measure as compared to the results obtained using
human translated corpora.
In the future, we plan to explore additional
language-specific clues, and integrate them into the
subjectivity classifiers. As shown by some of our
experiments, Romanian seems to entail more subjec-
tivity markers compared to English, and this factor
motivates us to further pursue the use of language-
specific clues of subjectivity.
Our experiments have generated corpora of about
20,000 sentences annotated for subjectivity in Ro-
manian and Spanish, which are available for down-
load at http://lit.csci.unt.edu/index.php/Downloads,
along with the manually annotated data sets.
Acknowledgments
The authors are grateful to Daniel Marcu and Lan-
guageWeaver for kindly providing access to their
Romanian-English and English-Romanian machine
translation engines. This work was partially sup-
ported by a National Science Foundation grant IIS-
#0840608.
134
References
C. Ovesdotter Alm, D. Roth, and R. Sproat. 2005.
Emotions from text: Machine learning for text-based
emotion prediction. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354, Vancouver,
Canada.
K. Balog, G. Mishne, and M. de Rijke. 2006. Why are
they excited? identifying and explaining spikes in blog
mood levels. In Proceedings of the 11th Meeting of
the European Chapter of the Association for Compu-
tational Linguistics (EACL-2006).
A. Esuli and F. Sebastiani. 2006. Determining term sub-
jectivity and term orientation for opinion mining. In
Proceedings the 11th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), pages 193?200, Trento, IT.
R. Fan, P. Chen, and C. Lin. 2005. Working set selection
using the second order information for training svm.
Journal of Machine Learning Research, 6:1889?1918.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing 2004 (KDD 2004), pages 168?177, Seattle, Wash-
ington.
Y. Hu, J. Duan, X. Chen, B. Pei, and R. Lu. 2005. A new
method for sentiment classification in text retrieval. In
IJCNLP, pages 1?9.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
H. Kanayama and T. Nasukawa. 2006. Fully automatic
lexicon expansion for domain-oriented sentiment anal-
ysis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 355?363, Sydney, Australia.
N. Kando and D. Kirk Evans, editors. 2007. Proceed-
ings of the Sixth NTCIR Workshop Meeting on Evalua-
tion of Information Access Technologies: Information
Retrieval, Question Answering, and Cross-Lingual In-
formation Access, 2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo 101-8430, Japan, May. National Institute of In-
formatics.
S.-M. Kim and E. Hovy. 2006. Identifying and ana-
lyzing judgment opinions. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
pages 200?207, New York, New York.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expres-
sions for opinion extraction. In Proceedings of the 1st
International Joint Conference on Natural Language
Processing (IJCNLP-04).
L. Lloyd, D. Kechagias, and S. Skiena. 2005. Lydia: A
system for large-scale news analysis. In String Pro-
cessing and Information Retrieval (SPIRE 2005).
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Association for Com-
putational Linguistics, Prague, Czech Republic.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
Y. Suzuki, H. Takamura, and M. Okumura. 2006. Ap-
plication of semi-supervised learning to evaluative ex-
pression classification. In Proceedings of the 7th In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2006),
pages 502?513, Mexico City, Mexico.
H. Takamura, T. Inui, and M. Okumura. 2006. Latent
variable models for semantic orientations of phrases.
In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2006), Trento, Italy.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proceedings of COLING-ACL 2006.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2005) ( invited paper), Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2003), pages
129?136, Sapporo, Japan.
135
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976?983,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Multilingual Subjective Language via Cross-Lingual Projections
Rada Mihalcea and Carmen Banea
Department of Computer Science
University of North Texas
rada@cs.unt.edu, carmenb@unt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
This paper explores methods for generating
subjectivity analysis resources in a new lan-
guage by leveraging on the tools and re-
sources available in English. Given a bridge
between English and the selected target lan-
guage (e.g., a bilingual dictionary or a par-
allel corpus), the methods can be used to
rapidly create tools for subjectivity analysis
in the new language.
1 Introduction
There is growing interest in the automatic extraction
of opinions, emotions, and sentiments in text (sub-
jectivity), to provide tools and support for various
natural language processing applications. Most of
the research to date has focused on English, which
is mainly explained by the availability of resources
for subjectivity analysis, such as lexicons and man-
ually labeled corpora.
In this paper, we investigate methods to auto-
matically generate resources for subjectivity analy-
sis for a new target language by leveraging on the
resources and tools available for English, which in
many cases took years of work to complete. Specif-
ically, through experiments with cross-lingual pro-
jection of subjectivity, we seek answers to the fol-
lowing questions.
First, can we derive a subjectivity lexicon for a
new language using an existing English subjectivity
lexicon and a bilingual dictionary? Second, can we
derive subjectivity-annotated corpora in a new lan-
guage using existing subjectivity analysis tools for
English and a parallel corpus? Finally, third, can we
build tools for subjectivity analysis for a new target
language by relying on these automatically gener-
ated resources?
We focus our experiments on Romanian, selected
as a representative of the large number of languages
that have only limited text processing resources de-
veloped to date. Note that, although we work with
Romanian, the methods described are applicable to
any other language, as in these experiments we (pur-
posely) do not use any language-specific knowledge
of the target language. Given a bridge between En-
glish and the selected target language (e.g., a bilin-
gual dictionary or a parallel corpus), the methods
can be applied to other languages as well.
After providing motivations, we present two ap-
proaches to developing sentence-level subjectivity
classifiers for a new target language. The first uses a
subjectivity lexicon translated from an English one.
The second uses an English subjectivity classifier
and a parallel corpus to create target-language train-
ing data for developing a statistical classifier.
2 Motivation
Automatic subjectivity analysis methods have been
used in a wide variety of text processing applica-
tions, such as tracking sentiment timelines in on-
line forums and news (Lloyd et al, 2005; Balog
et al, 2006), review classification (Turney, 2002;
Pang et al, 2002), mining opinions from product
reviews (Hu and Liu, 2004), automatic expressive
text-to-speech synthesis (Alm et al, 2005), text se-
mantic analysis (Wiebe and Mihalcea, 2006; Esuli
and Sebastiani, 2006), and question answering (Yu
and Hatzivassiloglou, 2003).
976
While much recent work in subjectivity analysis
focuses on sentiment (a type of subjectivity, namely
positive and negative emotions, evaluations, and
judgments), we opt to focus on recognizing subjec-
tivity in general, for two reasons.
First, even when sentiment is the desired focus,
researchers in sentiment analysis have shown that
a two-stage approach is often beneficial, in which
subjective instances are distinguished from objec-
tive ones, and then the subjective instances are fur-
ther classified according to polarity (Yu and Hatzi-
vassiloglou, 2003; Pang and Lee, 2004; Wilson et
al., 2005; Kim and Hovy, 2006). In fact, the prob-
lem of distinguishing subjective versus objective in-
stances has often proved to be more difficult than
subsequent polarity classification, so improvements
in subjectivity classification promise to positively
impact sentiment classification. This is reported in
studies of manual annotation of phrases (Takamura
et al, 2006), recognizing contextual polarity of ex-
pressions (Wilson et al, 2005), and sentiment tag-
ging of words and word senses (Andreevskaia and
Bergler, 2006; Esuli and Sebastiani, 2006).
Second, an NLP application may seek a wide
range of types of subjectivity attributed to a per-
son, such as their motivations, thoughts, and specu-
lations, in addition to their positive and negative sen-
timents. For instance, the opinion tracking system
Lydia (Lloyd et al, 2005) gives separate ratings for
subjectivity and sentiment. These can be detected
with subjectivity analysis but not by a method fo-
cused only on sentiment.
There is world-wide interest in text analysis appli-
cations. While work on subjectivity analysis in other
languages is growing (e.g., Japanese data are used in
(Takamura et al, 2006; Kanayama and Nasukawa,
2006), Chinese data are used in (Hu et al, 2005),
and German data are used in (Kim and Hovy, 2006)),
much of the work in subjectivity analysis has been
applied to English data. Creating corpora and lexical
resources for a new language is very time consum-
ing. In general, we would like to leverage resources
already developed for one language to more rapidly
create subjectivity analysis tools for a new one. This
motivates our exploration and use of cross-lingual
lexicon translations and annotation projections.
Most if not all work on subjectivity analysis has
been carried out in a monolingual framework. We
are not aware of multi-lingual work in subjectivity
analysis such as that proposed here, in which subjec-
tivity analysis resources developed for one language
are used to support developing resources in another.
3 A Lexicon-Based Approach
Many subjectivity and sentiment analysis tools rely
on manually or semi-automatically constructed lex-
icons (Yu and Hatzivassiloglou, 2003; Riloff and
Wiebe, 2003; Kim and Hovy, 2006). Given the suc-
cess of such techniques, the first approach we take
to generating a target-language subjectivity classi-
fier is to create a subjectivity lexicon by translating
an existing source language lexicon, and then build
a classifier that relies on the resulting lexicon.
Below, we describe the translation process and
discuss the results of an annotation study to assess
the quality of the translated lexicon. We then de-
scribe and evaluate a lexicon-based target-language
classifier.
3.1 Translating a Subjectivity Lexicon
The subjectivity lexicon we use is from Opinion-
Finder (Wiebe and Riloff, 2005), an English sub-
jectivity analysis system which, among other things,
classifies sentences as subjective or objective. The
lexicon was compiled from manually developed re-
sources augmented with entries learned from cor-
pora. It contains 6,856 unique entries, out of which
990 are multi-word expressions. The entries in the
lexicon have been labeled for part of speech, and for
reliability ? those that appear most often in subjec-
tive contexts are strong clues of subjectivity, while
those that appear less often, but still more often than
expected by chance, are labeled weak.
To perform the translation, we use two bilingual
dictionaries. The first is an authoritative English-
Romanian dictionary, consisting of 41,500 entries,1
which we use as the main translation resource for the
lexicon translation. The second dictionary, drawn
from the Universal Dictionary download site (UDP,
2007) consists of 4,500 entries written largely by
Web volunteer contributors, and thus is not error
free. We use this dictionary only for those entries
that do not appear in the main dictionary.
1Unique English entries, each with multiple Romanian
translations.
977
There were several challenges encountered in the
translation process. First, although the English sub-
jectivity lexicon contains inflected words, we must
use the lemmatized form in order to be able to trans-
late the entries using the bilingual dictionary. How-
ever, words may lose their subjective meaning once
lemmatized. For instance, the inflected form of
memories becomes memory. Once translated into
Romanian (as memorie), its main meaning is ob-
jective, referring to the power of retaining informa-
tion as in Iron supplements may improve a woman?s
memory.
Second, neither the lexicon nor the bilingual dic-
tionary provides information on the sense of the in-
dividual entries, and therefore the translation has to
rely on the most probable sense in the target lan-
guage. Fortunately, the bilingual dictionary lists the
translations in reverse order of their usage frequen-
cies. Nonetheless, the ambiguity of the words and
the translations still seems to represent an impor-
tant source of error. Moreover, the lexicon some-
times includes identical entries expressed through
different parts of speech, e.g., grudge has two sepa-
rate entries, for its noun and verb roles, respectively.
On the other hand, the bilingual dictionary does not
make this distinction, and therefore we have again
to rely on the ?most frequent? heuristic captured by
the translation order in the bilingual dictionary.
Finally, the lexicon includes a significant number
(990) of multi-word expressions that pose transla-
tion difficulties, sometimes because their meaning
is idiomatic, and sometimes because the multi-word
expression is not listed in the bilingual dictionary
and the translation of the entire phrase is difficult
to reconstruct from the translations of the individual
words. To address this problem, when a translation
is not found in the dictionary, we create one using
a word-by-word approach. These translations are
then validated by enforcing that they occur at least
three times on the Web, using counts collected from
the AltaVista search engine. The multi-word expres-
sions that are not validated in this process are dis-
carded, reducing the number of expressions from an
initial set of 990 to a final set of 264.
The final subjectivity lexicon in Romanian con-
tains 4,983 entries. Table 1 shows examples of en-
tries in the Romanian lexicon, together with their
corresponding original English form. The table
Romanian English attributes
??nfrumuset?a beautifying strong, verb
notabil notable weak, adj
plin de regret full of regrets strong, adj
sclav slaves weak, noun
Table 1: Examples of entries in the Romanian sub-
jectivity lexicon
also shows the reliability of the expression (weak or
strong) and the part of speech ? attributes that are
provided in the English subjectivity lexicon.
Manual Evaluation.
We want to assess the quality of the translated lexi-
con, and compare it to the quality of the original En-
glish lexicon. The English subjectivity lexicon was
evaluated in (Wiebe and Riloff, 2005) against a cor-
pus of English-language news articles manually an-
notated for subjectivity (the MPQA corpus (Wiebe et
al., 2005)). According to this evaluation, 85% of the
instances of the clues marked as strong and 71.5% of
the clues marked as weak are in subjective sentences
in the MPQA corpus.
Since there is no comparable Romanian corpus,
an alternate way to judge the subjectivity of a Ro-
manian lexicon entry is needed.
Two native speakers of Romanian annotated the
subjectivity of 150 randomly selected entries. Each
annotator independently read approximately 100 ex-
amples of each drawn from the Web, including a
large number from news sources. The subjectivity
of a word was consequently judged in the contexts
where it most frequently appears, accounting for its
most frequent meanings on the Web.
The tagset used for the annotations consists of
S(ubjective), O(bjective), and B(oth). A W(rong) la-
bel is also used to indicate a wrong translation. Table
2 shows the contingency table for the two annota-
tors? judgments on this data.
S O B W Total
S 53 6 9 0 68
O 1 27 1 0 29
B 5 3 18 0 26
W 0 0 0 27 27
Total 59 36 28 27 150
Table 2: Agreement on 150 entries in the Romanian
lexicon
Without counting the wrong translations, the
agreement is measured at 0.80, with a Kappa ? =
978
0.70, which indicates consistent agreement. After
the disagreements were reconciled through discus-
sions, the final set of 123 correctly translated entries
does include 49.6% (61) subjective entries, but fully
23.6% (29) were found in the study to have primar-
ily objective uses (the other 26.8% are mixed).
Thus, this study suggests that the Romanian sub-
jectivity clues derived through translation are less re-
liable than the original set of English clues. In sev-
eral cases, the subjectivity is lost in the translation,
mainly due to word ambiguity in either the source
or target language, or both. For instance, the word
fragile correctly translates into Romanian as fragil,
yet this word is frequently used to refer to breakable
objects, and it loses its subjective meaning of del-
icate. Other words, such as one-sided, completely
lose subjectivity once translated, as it becomes in
Romanian cu o singura latura?, meaning with only
one side (as of objects).
Interestingly, the reliability of clues in the English
lexicon seems to help preserve subjectivity. Out of
the 77 entries marked as strong, 11 were judged to be
objective in Romanian (14.3%), compared to 14 ob-
jective Romanian entries obtained from the 36 weak
English clues (39.0%).
3.2 Rule-based Subjectivity Classifier Using a
Subjectivity Lexicon
Starting with the Romanian lexicon, we developed
a lexical classifier similar to the one introduced by
(Riloff and Wiebe, 2003). At the core of this method
is a high-precision subjectivity and objectivity clas-
sifier that can label large amounts of raw text using
only a subjectivity lexicon. Their method is further
improved with a bootstrapping process that learns
extraction patterns. In our experiments, however, we
apply only the rule-based classification step, since
the extraction step cannot be implemented without
tools for syntactic parsing and information extrac-
tion not available in Romanian.
The classifier relies on three main heuristics to la-
bel subjective and objective sentences: (1) if two
or more strong subjective expressions occur in the
same sentence, the sentence is labeled Subjective;
(2) if no strong subjective expressions occur in a
sentence, and at most two weak subjective expres-
sions occur in the previous, current, and next sen-
tence combined, then the sentence is labeled Objec-
tive; (3) otherwise, if none of the previous rules ap-
ply, the sentence is labeled Unknown.
The quality of the classifier was evaluated on a
Romanian gold-standard corpus annotated for sub-
jectivity. Two native Romanian speakers (Ro1 and
Ro2) manually annotated the subjectivity of the sen-
tences of five randomly selected documents (504
sentences) from the Romanian side of an English-
Romanian parallel corpus, according to the anno-
tation scheme in (Wiebe et al, 2005). Agreement
between annotators was measured, and then their
differences were adjudicated. The baseline on this
data set is 54.16%, which can be obtained by as-
signing a default Subjective label to all sentences.
(More information about the corpus and annotations
are given in Section 4 below, where agreement be-
tween English and Romanian aligned sentences is
also assessed.)
As mentioned earlier, due to the lexicon projec-
tion process that is performed via a bilingual dictio-
nary, the entries in our Romanian subjectivity lex-
icon are in a lemmatized form. Consequently, we
also lemmatize the gold-standard corpus, to allow
for the identification of matches with the lexicon.
For this purpose, we use the Romanian lemmatizer
developed by Ion and Tufis? (Ion, 2007), which has
an estimated accuracy of 98%.2
Table 3 shows the results of the rule-based classi-
fier. We show the precision, recall, and F-measure
independently measured for the subjective, objec-
tive, and all sentences. We also evaluated a vari-
ation of the rule-based classifier that labels a sen-
tence as objective if there are at most three weak ex-
pressions in the previous, current, and next sentence
combined, which raises the recall of the objective
classifier. Our attempts to increase the recall of the
subjective classifier all resulted in significant loss in
precision, and thus we kept the original heuristic.
In its original English implementation, this sys-
tem was proposed as being high-precision but low
coverage. Evaluated on the MPQA corpus, it has
subjective precision of 90.4, subjective recall of
34.2, objective precision of 82.4, and objective re-
call of 30.7; overall, precision is 86.7 and recall is
32.6 (Wiebe and Riloff, 2005). We see a similar be-
havior on Romanian for subjective sentences. The
subjective precision is good, albeit at the cost of low
2Dan Tufis?, personal communication.
979
Measure Subjective Objective All
subj = at least two strong; obj = at most two weak
Precision 80.00 56.50 62.59
Recall 20.51 48.91 33.53
F-measure 32.64 52.52 43.66
subj = at least two strong; obj = at most three weak
Precision 80.00 56.85 61.94
Recall 20.51 61.03 39.08
F-measure 32.64 58.86 47.93
Table 3: Evaluation of the rule-based classifier
recall, and thus the classifier could be used to har-
vest subjective sentences from unlabeled Romanian
data (e.g., for a subsequent bootstrapping process).
The system is not very effective for objective classi-
fication, however. Recall that the objective classifier
relies on the weak subjectivity clues, for which the
transfer of subjectivity in the translation process was
particularly low.
4 A Corpus-Based Approach
Given the low number of subjective entries found in
the automatically generated lexicon and the subse-
quent low recall of the lexical classifier, we decided
to also explore a second, corpus-based approach.
This approach builds a subjectivity-annotated cor-
pus for the target language through projection, and
then trains a statistical classifier on the resulting
corpus (numerous statistical classifiers have been
trained for subjectivity or sentiment classification,
e.g., (Pang et al, 2002; Yu and Hatzivassiloglou,
2003)). The hypothesis is that we can eliminate
some of the ambiguities (and consequent loss of sub-
jectivity) observed during the lexicon translation by
accounting for the context of the ambiguous words,
which is possible in a corpus-based approach. Ad-
ditionally, we also hope to improve the recall of the
classifier, by addressing those cases not covered by
the lexicon-based approach.
In the experiments reported in this section, we
use a parallel corpus consisting of 107 documents
from the SemCor corpus (Miller et al, 1993) and
their manual translations into Romanian.3 The cor-
pus consists of roughly 11,000 sentences, with ap-
proximately 250,000 tokens on each side. It is a bal-
anced corpus covering a number of topics in sports,
politics, fashion, education, and others.
3The translation was carried out by a Romanian native
speaker, student in a department of ?Foreign Languages and
Translations? in Romania.
Below, we begin with a manual annotation study
to assess the quality of annotation and preservation
of subjectivity in translation. We then describe the
automatic construction of a target-language training
set, and evaluate a classifier trained on that data.
Annotation Study.
We start by performing an agreement study meant
to determine the extent to which subjectivity is pre-
served by the cross-lingual projections. In the study,
three annotators ? one native English speaker (En)
and two native Romanian speakers (Ro1 and Ro2) ?
first trained on 3 randomly selected documents (331
sentences). They then independently annotated the
subjectivity of the sentences of two randomly se-
lected documents from the parallel corpus, account-
ing for 173 aligned sentence pairs. The annotators
had access exclusively to the version of the sen-
tences in their language, to avoid any bias that could
be introduced by seeing the translation in the other
language.
Note that the Romanian annotations (after all dif-
ferences between the Romanian annotators were ad-
judicated) of all 331 + 173 sentences make up the
gold standard corpus used in the experiments re-
ported in Sections 3.2 and 4.1.
Before presenting the results of the annotation
study, we give some examples. The following are
English subjective sentences and their Romanian
translations (the subjective elements are shown in
bold).
[en] The desire to give Broglio as many starts as
possible.
[ro] Dorint?a de a-i da lui Broglio ca?t mai multe
starturi posibile.
[en] Suppose he did lie beside Lenin, would it be
permanent ?
[ro] Sa? presupunem ca? ar fi as?ezat ala?turi de Lenin,
oare va fi pentru totdeauna?
The following are examples of objective parallel
sentences.
[en]The Pirates have a 9-6 record this year and the
Redbirds are 7-9.
[ro] Pirat?ii au un palmares de 9 la 6 anul acesta si
Pa?sa?rile Ros?ii au 7 la 9.
[en] One of the obstacles to the easy control of a
2-year old child is a lack of verbal communication.
[ro] Unul dintre obstacolele ??n controlarea unui
copil de 2 ani este lipsa comunica?rii verbale.
980
The annotators were trained using the MPQA
annotation guidelines (Wiebe et al, 2005). The
tagset consists of S(ubjective), O(bjective) and
U(ncertain). For the U tags, a class was also given;
OU means, for instance, that the annotator is uncer-
tain but she is leaning toward O. Table 4 shows the
pairwise agreement figures and the Kappa (?) calcu-
lated for the three annotators. The table also shows
the agreement when the borderline uncertain cases
are removed.
all sentences Uncertain removed
pair agree ? agree ? (%) removed
Ro1 & Ro2 0.83 0.67 0.89 0.77 23
En & Ro1 0.77 0.54 0.86 0.73 26
En & Ro2 0.78 0.55 0.91 0.82 20
Table 4: Agreement on the data set of 173 sentences.
Annotations performed by three annotators: one na-
tive English speaker (En) and two native Romanian
speakers (Ro1 and Ro2)
When all the sentences are included, the agree-
ment between the two Romanian annotators is mea-
sured at 0.83 (? = 0.67). If we remove the border-
line cases where at least one annotator?s tag is Un-
certain, the agreement rises to 0.89 with ? = 0.77.
These figures are somewhat lower than the agree-
ment observed during previous subjectivity anno-
tation studies conducted on English (Wiebe et al,
2005) (the annotators were more extensively trained
in those studies), but they nonetheless indicate con-
sistent agreement.
Interestingly, when the agreement is conducted
cross-lingually between an English and a Romanian
annotator, the agreement figures, although some-
what lower, are comparable. In fact, once the
Uncertain tags are removed, the monolingual and
cross-lingual agreement and ? values become al-
most equal, which suggests that in most cases the
sentence-level subjectivity is preserved.
The disagreements were reconciled first between
the labels assigned by the two Romanian annotators,
followed by a reconciliation between the resulting
Romanian ?gold-standard? labels and the labels as-
signed by the English annotator. In most cases, the
disagreement across the two languages was found
to be due to a difference of opinion about the sen-
tence subjectivity, similar to the differences encoun-
tered in monolingual annotations. However, there
are cases where the differences are due to the sub-
jectivity being lost in the translation. Sometimes,
this is due to several possible interpretations for the
translated sentence. For instance, the following sen-
tence:
[en] They honored the battling Billikens last night.
[ro] Ei i-au celebrat pe Billikens seara trecuta?.
is marked as Subjective in English (in context, the
English annotator interpreted honored as referring
to praises of the Billikens). However, the Romanian
translation of honored is celebrat which, while cor-
rect as a translation, has the more frequent interpre-
tation of having a party. The two Romanian annota-
tors chose this interpretation, which correspondingly
lead them to mark the sentence as Objective.
In other cases, in particular when the subjectivity
is due to figures of speech such as irony, the trans-
lation sometimes misses the ironic aspects. For in-
stance, the translation of egghead was not perceived
as ironic by the Romanian annotators, and conse-
quently the following sentence labeled Subjective in
English is annotated as Objective in Romanian.
[en] I have lived for many years in a Connecti-
cut commuting town with a high percentage of [...]
business executives of egghead tastes.
[ro] Am tra?it mult?i ani ??ntr-un oras? din apropiere de
Connecticut ce avea o mare proport?ie de [...] oa-
meni de afaceri cu gusturi intelectuale.
4.1 Translating a Subjectivity-Annotated
Corpus and Creating a Machine Learning
Subjectivity Classifier
To further validate the corpus-based projection of
subjectivity, we developed a subjectivity classifier
trained on Romanian subjectivity-annotated corpora
obtained via cross-lingual projections.
Ideally, one would generate an annotated Roma-
nian corpus by translating English documents man-
ually annotated for subjectivity such as the MPQA
corpus. Unfortunately, the manual translation of this
corpus would be prohibitively expensive, both time-
wise and financially. The other alternative ? auto-
matic machine translation ? has not yet reached a
level that would enable the generation of a high-
quality translated corpus. We therefore decided to
use a different approach where we automatically
annotate the English side of an existing English-
Romanian corpus, and subsequently project the an-
notations onto the Romanian side of the parallel cor-
981
Precision Recall F-measure
high-precision 86.7 32.6 47.4
high-coverage 79.4 70.6 74.7
Table 5: Precision, recall, and F-measure for the
two OpinionFinder classifiers, as measured on the
MPQA corpus.
pus across the sentence-level alignments available in
the corpus.
For the automatic subjectivity annotations, we
generated two sets of the English-side annotations,
one using the high-precision classifier and one using
the high-coverage classifier available in the Opinion-
Finder tool. The high-precision classifier in Opin-
ionFinder uses the clues of the subjectivity lexicon
to harvest subjective and objective sentences from
a large amount of unannotated text; this data is then
used to automatically identify a set of extraction pat-
terns, which are then used iteratively to identify a
larger set of subjective and objective sentences.
In addition, in OpinionFinder, the high-precision
classifier is used to produce an English labeled data
set for training, which is used to generate its Naive
Bayes high-coverage subjectivity classifier. Table
5 shows the performance of the two classifiers on
the MPQA corpus as reported in (Wiebe and Riloff,
2005). Note that 55% of the sentences in the MPQA
corpus are subjective ? which represents the baseline
for this data set.
The two OpinionFinder classifiers are used to la-
bel the training corpus. After removing the 504 test
sentences, we are left with 10,628 sentences that
are automatically annotated for subjectivity. Table
6 shows the number of subjective and objective sen-
tences obtained with each classifier.
Classifier Subjective Objective All
high-precision 1,629 2,334 3,963
high-coverage 5,050 5,578 10,628
Table 6: Subjective and objective training sentences
automatically annotated with OpinionFinder.
Next, the OpinionFinder annotations are pro-
jected onto the Romanian training sentences, which
are then used to develop a probabilistic classifier for
the automatic labeling of subjectivity in Romanian
sentences.
Similar to, e.g., (Pang et al, 2002), we use a
Naive Bayes algorithm trained on word features co-
occurring with the subjective and the objective clas-
sifications. We assume word independence, and we
use a 0.3 cut-off for feature selection. While re-
cent work has also considered more complex syn-
tactic features, we are not able to generate such fea-
tures for Romanian as they require tools currently
not available for this language.
We create two classifiers, one trained on each
data set. The quality of the classifiers is evaluated
on the 504-sentence Romanian gold-standard corpus
described above. Recall that the baseline on this data
set is 54.16%, the percentage of sentences in the cor-
pus that are subjective. Table 7 shows the results.
Subjective Objective All
projection source: OF high-precision classifier
Precision 65.02 69.62 64.48
Recall 82.41 47.61 64.48
F-measure 72.68 56.54 64.68
projection source: OF high-coverage classifier
Precision 66.66 70.17 67.85
Recall 81.31 52.17 67.85
F-measure 72.68 56.54 67.85
Table 7: Evaluation of the machine learning classi-
fier using training data obtained via projections from
data automatically labeled by OpinionFinder (OF).
Our best classifier has an F-measure of 67.85,
and is obtained by training on projections from
the high-coverage OpinionFinder annotations. Al-
though smaller than the 74.70 F-measure obtained
by the English high-coverage classifier (see Ta-
ble 5), the result appears remarkable given that no
language-specific Romanian information was used.
The overall results obtained with the machine
learning approach are considerably higher than
those obtained from the rule-based classifier (except
for the precision of the subjective sentences). This
is most likely due to the lexicon translation process,
which as mentioned in the agreement study in Sec-
tion 3.1, leads to ambiguity and loss of subjectivity.
Instead, the corpus-based translations seem to better
account for the ambiguity of the words, and the sub-
jectivity is generally preserved in the sentence trans-
lations.
5 Conclusions
In this paper, we described two approaches to gener-
ating resources for subjectivity annotations for a new
982
language, by leveraging on resources and tools avail-
able for English. The first approach builds a target
language subjectivity lexicon by translating an exist-
ing English lexicon using a bilingual dictionary. The
second generates a subjectivity-annotated corpus in
a target language by projecting annotations from an
automatically annotated English corpus.
These resources were validated in two ways.
First, we carried out annotation studies measuring
the extent to which subjectivity is preserved across
languages in each of the two resources. These stud-
ies show that only a relatively small fraction of the
entries in the lexicon preserve their subjectivity in
the translation, mainly due to the ambiguity in both
the source and the target languages. This is con-
sistent with observations made in previous work
that subjectivity is a property associated not with
words, but with word meanings (Wiebe and Mihal-
cea, 2006). In contrast, the sentence-level subjectiv-
ity was found to be more reliably preserved across
languages, with cross-lingual inter-annotator agree-
ments comparable to the monolingual ones.
Second, we validated the two automatically gen-
erated subjectivity resources by using them to build
a tool for subjectivity analysis in the target language.
Specifically, we developed two classifiers: a rule-
based classifier that relies on the subjectivity lexi-
con described in Section 3.1, and a machine learn-
ing classifier trained on the subjectivity-annotated
corpus described in Section 4.1. While the highest
precision for the subjective classification is obtained
with the rule-based classifier, the overall best result
of 67.85 F-measure is due to the machine learning
approach. This result is consistent with the anno-
tation studies, showing that the corpus projections
preserve subjectivity more reliably than the lexicon
translations.
Finally, neither one of the classifiers relies on
language-specific information, but rather on knowl-
edge obtained through projections from English. A
similar method can therefore be used to derive tools
for subjectivity analysis in other languages.
References
Alina Andreevskaia and Sabine Bergler. Mining wordnet for
fuzzy sentiment: Sentiment tag extraction from WordNet
glosses. In Proceedings of EACL 2006.
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat. 2005.
Emotions from text: Machine learning for text-based emo-
tion prediction. In Proceedings of HLT/EMNLP 2005.
Krisztian Balog, Gilad Mishne, and Maarten de Rijke. 2006.
Why are they excited? identifying and explaining spikes in
blog mood levels. In EACL-2006.
Andrea Esuli and Fabrizio Sebastiani. 2006. Determining term
subjectivity and term orientation for opinion mining. In Pro-
ceedings the EACL 2006.
Minqing Hu and Bing Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of ACM SIGKDD.
Yi Hu, Jianyong Duan, Xiaoming Chen, Bingzhen Pei, and
Ruzhan Lu. 2005. A new method for sentiment classifi-
cation in text retrieval. In Proceedings of IJCNLP.
Radu Ion. 2007. Methods for automatic semantic disambigua-
tion. Applications to English and Romanian. Ph.D. thesis,
The Romanian Academy, RACAI.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented sentiment anal-
ysis. In Proceedings of EMNLP 2006.
Soo-Min Kim and Eduard Hovy. 2006. Identifying and ana-
lyzing judgment opinions. In Proceedings of HLT/NAACL
2006.
Levon Lloyd, Dimitrios Kechagias, and Steven Skiena. 2005.
Lydia: A system for large-scale news analysis. In Proceed-
ings of SPIRE 2005.
George Miller, Claudia Leacock, Tangee Randee, and Ross
Bunker. 1993. A semantic concordance. In Proceedings
of the DARPA Workshop on Human Language Technology.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL 2004.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP 2002.
Ellen Riloff and Janyce Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of EMNLP
2003.
Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2006.
Latent variable models for semantic orientations of phrases.
In Proceedings of EACL 2006.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of ACL 2002.
Universal Dictionary. 2007. Available at
www.dicts.info/uddl.php.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of COLING-ACL 2006.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts. In
Proceedings of CICLing 2005 (invited paper). Available at
www.cs.pitt.edu/mpqarequest.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2/3):164?
210. Available at www.cs.pitt.edu/mpqa.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of HLT/EMNLP 2005.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In Pro-
ceedings of EMNLP 2003.
983
Workshop on TextGraphs, at HLT-NAACL 2006, pages 53?60,
New York City, June 2006. c?2006 Association for Computational Linguistics
Random-Walk Term Weighting
for Improved Text Classification
Samer Hassan and Carmen Banea
Department of Computer Science
University of North Texas
Denton, TX 76203
samer@unt.edu, carmen@unt.edu
Abstract
This paper describes a new approach for
estimating term weights in a text classifi-
cation task. The approach uses term co-
occurrence as a measure of dependency
between word features. A random walk
model is applied on a graph encoding
words and co-occurrence dependencies,
resulting in scores that represent a quan-
tification of how a particular word feature
contributes to a given context. We argue
that by modeling feature weights using
these scores, as opposed to the traditional
frequency-based scores, we can achieve
better results in a text classification task.
Experiments performed on four standard
classification datasets show that the new
random-walk based approach outperforms
the traditional term frequency approach to
feature weighting.
1 Introduction
Term frequency has long been adapted as a measure
of term significance in a specific context (Robert-
son and Jones, 1997). The logic behind it is that the
more a certain term is encountered in a certain con-
text, the more it carries or contributes to the mean-
ing of the context. Due to this belief, term frequency
has been a major factor in estimating the probabilis-
tic distribution of features using maximum likeli-
hood estimates and hence has been incorporated in a
broad spectrum of tasks ranging from feature selec-
tion techniques (Yang and Pedersen, 1997; Schutze
et al, 1995) to language models (Bahl et al, 1983).
In this paper we introduce a new measure of term
weighting, which integrates the locality of a term
and its relation to the surrounding context. We
model this local contribution using a co-occurrence
relation in which terms that co-occur in a certain
context are likely to share between them some of
their importance (or significance). Note that in this
model the relation between a given term and its con-
text is not linear, since the context itself consists of
a collection of other terms, which in turn have a
dependency relation with their own context, which
might include the original given term. In order to
model this recursive relation we use a graph-based
ranking algorithm, namely the PageRank random-
walk algorithms (Brin and Page, 1998), and its Text-
Rank adaption to text processing applications (Mi-
halcea and Tarau, 2004). TextRank takes as in-
put a set of textual entities and relations between
them, and uses a graph-based ranking algorithm
(also known as random walk algorithm) to produce
a set of scores that represent the accumulated weight
or rank for each textual entity in their context. The
TextRank model was so far evaluated on three nat-
ural language processing tasks: document summa-
rization, word sense disambiguation, and keyword
extraction, and despite being fully unsupervised, it
has been shown to be competitive with other some-
time supervised state-of-the-art algorithms.
In this paper, we show how TextRank can be
used to model the probabilistic distribution of word
features in a document, by making further use of
the scores produced by the random-walk model.
53
Through experiments performed on a text classifi-
cation task, we show that these random walk scores
outperform the traditional term frequencies typically
used to model the feature weights for this task.
2 Graph-based Ranking Algorithms
The basic idea implemented by an iterative graph-
based ranking algorithm is that of ?voting? or ?rec-
ommendation?. When one vertex links to another
one, it is basically casting a vote for that other ver-
tex. The higher the number of votes that are cast
for a vertex, the higher the importance of the ver-
tex. Moreover, the importance of the vertex casting
a vote determines how important the vote itself is,
and this information is also taken into account by
the ranking algorithm. Hence, the score associated
with a vertex is determined based on the votes that
are cast for it, and the scores of the vertices casting
these votes.
While there are several graph-based ranking algo-
rithms previously proposed in the literature (Herings
et al, 2001), we focus on only one such algorithm,
namely PageRank (Brin and Page, 1998), as it was
previously found successful in a number of applica-
tions, including Web link analysis (Brin and Page,
1998), social networks (Dom et al, 2003), citation
analysis, and more recently in several text process-
ing applications (Mihalcea and Tarau, 2004), (Erkan
and Radev, 2004).
Given a graph G = (V,E), let In(Va) be the
set of vertices that point to vertex Va (predecessors),
and let Out(Va) be the set of vertices that vertex Va
points to (successors). The PageRank score associ-
ated with the vertex Va is then defined using a recur-
sive function that integrates the scores of its prede-
cessors:
S(Va) = (1 ? d) + d ?
?
Vb?In(Va)
S(Vb)
|Out(Vb)|
(1)
where d is a parameter that is set between 0 and 11.
The score of each vertex is recalculated upon each
iteration based on the new weights that the neighbor-
ing vertices have accumulated. The algorithm termi-
nates when the convergence point is reached for all
the vertices, meaning that the error rate for each ver-
tex falls below a pre-defined threshold. Formally,
1The typical value for d is 0.85 (Brin and Page, 1998), and
this is the value we are also using in our implementation.
for a vertex Vi let Sk(Vi) be the rank or the score
at iteration k and Sk+1(Vi) be the score at iteration
k + 1. The error rate ER is defined as:
ER = Sk+1(Vi) ? Sk(Vi) (2)
This vertex scoring scheme is based on a ran-
dom walk model, where a walker takes random steps
on the graph G, with the walk being modeled as
a Markov process ? that is, the decision on what
edge to follow is solely based on the vertex where
the walker is currently located. Under certain con-
ditions, this model converges to a stationary dis-
tribution of probabilities, associated with vertices
in the graph. Based on the Ergodic theorem for
Markov chains (Grimmett and Stirzaker, 1989), the
algorithm is guaranteed to converge if the graph is
both aperiodic and irreducible. The first condition is
achieved for any graph that is a non-bipartite graph,
while the second condition holds for any strongly
connected graph ? property achieved by PageRank
through the random jumps introduced by the (1?d)
factor. In matrix notation, the PageRank vector of
stationary probabilities is the principal eigenvector
for the matrix Arow, which is obtained from the ad-
jacency matrix A representing the graph, with all
rows normalized to sum to 1: (P = ATrowP ).
Intuitively, the stationary probability associated
with a vertex in the graph represents the probability
of finding the walker at that vertex during the ran-
dom walk, and thus it represents the importance of
the vertex within the graph. In the context of se-
quence data labeling, the random walk is performed
on the label graph associated with a sequence of
words, and thus the resulting stationary distribution
of probabilities can be used to decide on the most
probable set of labels for the given sequence.
2.1 TextRank
Given a natural language processing task, the Text-
Rank model includes four general steps for the
application of a graph-based ranking algorithm to
graph structures derived from natural language texts:
1. Identify text units that best define the proposed
task and add them as vertices in the graph.
2. Identify relations that connect such test units,
and use these relations to draw edges between
54
vertices in the graph. Edges can be directed or
undirected, weighted or un-weighted.
3. Iterate the graph ranking algorithm to conver-
gence.
4. Sort vertices based on their final score. Use the
values attached to each vertex for ranking.
The strength of this model lies in the global repre-
sentation of the context and its ability to model how
the co-occurrence between features might propagate
across the context and affect other distant features.
While TextRank has already been applied to sev-
eral language processing tasks, we focus here on the
keyword extraction task, since it best relates to our
approach. The goal of a keyword extraction tool is
to find a set of words or phrases that best describe a
given document. The co-occurrence relation within
a specific window is used to portray the correlation
between words, which are represented as vertices in
the graph. Two vertices are connected if their cor-
responding lexical units co-occur within a window
of at most N words, where N can be set to any
value greater than two. The TextRank application
to keyword extraction has also used different syn-
tactic filters for vertex selection, including all open
class words, nouns and verbs, nouns and adjectives,
and others. The algorithm was found to provide the
best results using nouns and adjectives with a win-
dow size of two.
Our approach follows the same main steps as used
in the TextRank keyword extraction application. We
are however incorporating a larger number of lexical
units, and we use different window sizes, as we will
show in the following section.
3 TextRank for Term Weighting
The goal of the work reported in this paper is to
study the ranking scores obtained using TextRank,
and evaluate their potential usefulness as a new mea-
sure of term weighting.
To understand how the random-walk weights
(rw) might be a good replacement for the traditional
term frequency weights (tf ), consider the example
in Figure 1. The example represents a sample doc-
ument from the Reuters collection. A graph is con-
structed as follows. If a term has not been previously
seen, then a node is added to the graph to represent
this term. A term can only be represented by one
node in the graph. An undirected edge is drawn be-
tween two nodes if they co-occur within a certain
window size. This example assumes a window size
of two, corresponding to two consecutive terms in
the text (e.g. London is linked to based).
London-based sugar operator Kaines Ltd con-
firmed it sold two cargoes of white sugar to India
out of an estimated overall sales total of four or five
cargoes in which other brokers participated. The
sugar, for April/May and April/June shipment, was
sold at between 214 and 218 dlrs a tonne cif, it said.
Figure 1: Sample Reuters document
London
based
sugar
operator
Kaines
confirmed
sold
cargoes
white
Indiaestimatedsales
total
brokers
participated
April
MayJune
shipment
dlrs
tonne
cif
Figure 2: Sample graph
Table 1 shows the tf and rw weights, also plotted
in Figure 3. By analyzing the rw weights, we can
observe a non-linear correlation with the tf weights,
with an emphasis given to terms surrounding impor-
tant key term like e.g. ?sugar? or ?cargoes.? This
spatial locality has resulted in higher ranks for terms
like ?operator? compared to other terms like ?lon-
don?2.
2All the missing words (e.g. ?Ltd,? ?it?) that are not shown
in the graph are common-words that were eliminated in the pre-
processing phase.
55
Term rw tf
sugar 2.248 3
sold 1.594 2
april 1.407 2
cargoes 1.542 2
cif 0.600 1
sales 0.891 1
london 0.546 1
tonne 1.059 1
shipment 0.829 1
based 0.933 1
estimated 0.888 1
dlrs 0.938 1
kaines 0.871 1
confirmed 0.859 1
total 0.856 1
white 0.796 1
india 0.846 1
operator 0.839 1
brokers 0.826 1
june 0.801 1
participated 0.819 1
Table 1: tf & rw scores
0
0.5
1
1.5
2
2.5
3
3.5
su
ga
r
so
ld
ca
rg
oe
s
ap
ril
to
nn
e
dl
rs
ba
se
d
sa
le
s
es
tim
at
ed
ka
in
es
co
nf
irm
ed
to
ta
l
in
di
a
op
er
at
or
sh
ip
m
en
t
br
ok
er
s
pa
rti
ci
pa
te
d
ju
ne
wh
ite ci
f
lo
nd
on
Fr
eq
ue
nc
y
r.w
t.f
Figure 3: tf & rw plots
4 Experimental Setup
To evaluate our random-walk based approach to fea-
ture weighting, we integrate it in a text classification
algorithm, and evaluate its performance on several
standard text classification data sets.
4.1 Random-Walk Term Weighting
Starting with a given document, we determine a
ranking over the words in the document by using the
approach described in Section 3.
First, we tokenize the document for punctuation,
special symbols, word abbreviations. We also re-
move the common words, using a list of approx-
imately 500 frequently used words as used in the
Smart retrieval system 3.
Next, the resulting text is processed to extract both
tf and rw weights for each term in the document.
Note that we do not apply any syntactic filters, as
it was previously done in applications of TextRank.
Instead, we consider each word as a potential fea-
ture. To determine tf we simply count the frequen-
cies of each word in the document. To determine
rw, all the terms are added as vertices in a graph
representing the document. A co-occurrence scan-
ner is then applied to the text to relate the terms that
co-occur within a given window size . For a given
term, all the terms that fall in the vicinity of this
term are considered dependent terms. This is rep-
resented by a set of edges that connect this term to
all the other terms in the window. Experiments are
performed for window sizes of 2, 4, 6, and 8. Once
the graph is constructed and the edges are in place,
the TextRank algorithm is applied4. The result of the
ranking process is a list of all input terms and their
corresponding rw scores.
We then calculate tf.idf and rw.idf as follows:
tf.idf = tf ? logNDn
where ND represent the total number of documents
in the collection and n is the number of documents
in which the target term appeared at least once.
Similarly,
rw.idf = rw ? logNDn
These term weights (tf.idf or rw.idf ) are then
used to create a feature vector for each document.
The vectors are fed to a traditional text classifica-
tion system, using one of the learning algorithms de-
scribed below. The results obtained using tf.idf will
act as a baseline in our evaluation.
4.2 Text Classification
Text classification is a problem typically formulated
as a machine learning task, where a classifier learns
how to distinguish between categories in a given set
3ftp://ftp.cs.cornell.edu/pub/smart.
4We use an implementation where the maximum number of
iterations is limited to 100, the damping factor is set to 0.85, and
convergence threshold to 0.0001. Each graph node is assigned
with an initial weight of 0.25.
56
using features automatically extracted from a collec-
tion of training documents. There is a large body
of algorithms previously tested on text classification
problems, due also to the fact that this task is one
of the testbeds of choice for machine learning algo-
rithms. In the experiments reported here, we com-
pare results obtained with four frequently used text
classifiers ? Rocchio, Na??ve Bayes, Nearest Neigh-
bor, and Support Vector Machines, selected based on
their diversity of learning methodologies.
Na??ve Bayes. The basic idea in a Na??ve Bayes
text classifier is to estimate the probability of a
category given a document using joint probabili-
ties of words and documents. Na??ve Bayes as-
sumes word independence, which means that the
conditional probability of a word given a category
is assumed to be independent of the conditional
probability of other words given the same category.
Despite this simplification, Na??ve Bayes classifiers
were shown to perform surprisingly well on text
classification (Joachims, 1997), (Schneider, 2004).
While there are several versions of Na??ve Bayes
classifiers (variations of multinomial and multivari-
ate Bernoulli), we use the multinomial model (Mc-
Callum and Nigam, 1998), which was shown to be
more effective.
Rocchio. This is an adaptation of the relevance
feedback method developed in information retrieval
(Rocchio, 1971). It uses standard tf.idf weighted
vectors to represent documents, and builds a pro-
totype vector for each category by summing up the
vectors of the training documents in each category.
Test documents are then assigned to the category
that has the closest prototype vector, based on a
cosine similarity. Text classification experiments
with different versions of the Rocchio algorithm
showed competitive results on standard benchmarks
(Joachims, 1997), (Moschitti, 2003).
KNN. K-Nearest Neighbor is one of the earliest text
categorization approaches (Makoto and Takenobu,
1995; Masand et al, 1992). The algorithm classifies
a test document based on the best class label identi-
fied for the nearest K-neighbors in the training doc-
uments. The best class label is chosen by weighting
the class of each similar training document with its
similarity to the target test document.
SVM. Support Vector Machines (Vapnik, 1995) is
a state-of-the-art machine learning approach based
on decision plans. The algorithm defines the best
hyper-plan which separates set of points associated
with different class labels with a maximum-margin.
The unlabeled examples are then classified by de-
ciding in which side of the hyper-surface they re-
side. The hyper-plan can be a simple linear plan as
first proposed by Vapnik, or a non-linear plan such
as e.g. polynomial, radial, or sigmoid. In our eval-
uation we used the linear kernel since it was proved
to be as powerful as the other kernels when tested on
text classification data sets (Yang and Liu, 1999).
4.3 Data Sets
In our experiments we use Reuters-21578,
WebKB, 20Newsgroups, and LingSpam
datasets. These datasets are commonly used for text
classification evaluations (Joachims, 1996; Craven
et al, 1998; Androutsopoulos et al, 2000; Mihalcea
and Hassan, 2005).
Reuter-21578. This is a publicly available subset of
the Reuters news, containing about 120 categories.
We use the standard ModApte data split (Apte et
al., 1994). The unlabeled documents were discarded
and only the documents with one or more class la-
bels were used in the classification experiments.
WebKB. This is a data set collected from com-
puter science departments of various universities by
the CMU text learning group. The dataset contains
seven class labels which are Project, Student, De-
partment, Faculty, Staff, Course, and Other. The
Other label was removed from the dataset for evalu-
ation purposes. Most of the evaluations in the liter-
ature have been performed on only four of the cate-
gories (Project, Student, Faculty, and Course) since
they represent the largest categories. However, since
we wanted to see how our system behaves when only
a few training examples were available as e.g. in the
Staff and the Department classes, we performed our
evaluations on two versions of WebKB: one with
the four categories version (WebKB4) and one with
the six categories (WebKB6).
20-Newsgroups. This is a collection of 20,000 mes-
sages from 20 different newsgroups, corresponding
to different topics or subjects. Each newsgroup has
about 1000 message split into 400 test and 600 train
documents.
LingSpam. This is a spam corpus, consisting of
email messages organized in 10 collections to al-
57
low for 10-fold cross validation. Each collection has
roughly 300 spam and legitimate messages. There
are four versions of the corpus standing for bare,
stop-word filtered, lemmatized, and stop-word and
lemmatized. We use the bare collection with a stan-
dard 10-fold cross validation.
4.4 Performance Measures
To evaluate the classification system we used the tra-
ditional accuracy measure defined as the number of
correct predictions divided with the number of eval-
uated examples.
We also use the correlation coefficient (?) as
a diversity measure to evaluate the dissimilarity
between the weighting models. Pairwise diver-
sity measures have been traditionally used to mea-
sure the statistical independence among ensemble of
classifiers (Kuncheva and Whitaker, 2003). Here,
we use them to measure the correlation between our
random-walk approach and the traditional term fre-
quency approach. The typical setting in which the
pairwise diversity measures are used is a set of dif-
ferent classifiers which are used to classify the same
set of feature vectors or documents over a given
dataset. In our evaluation we use the same classifier
to evaluate two different sets of feature vectors that
are produced by different weighting features: the rw
random walk weighting, and the tf term frequency
weighting. Since the two feature vector collections
are evaluated by one classifier at a time, the resulted
diversity scores will reflect the diversity of the two
systems.
Let Di and Dj be two feature weighting models
with the following contingency table.
Dj correct=Y Dj correct=N
Di correct=Y a b
Di correct=N c d
Table 2: Di & Dj Contingency table
The correlation coefficient (?) is defined as:
?ij =
ad ? bc
?
(a + b)(c + d)(a + c)(b + d)
5The symbol ?indicates a statistically significant result using
Table 3: Naive Bayes Results5
N.B. tf rw2 rw4 rw6 rw8
WebKB4 81.9 81.9 82.8 82.7 81.2
WebKB6 71.7 73.0 74.2? 74.4? 73.5
Reuter 83.2 82.5 82.9 83.0 82.8
20NG 81.7 82.0 82.3? 82.3? 82.1?
LSpam 99.3 99.4 99.3 99.3 99.3
Table 4: Rocchio Results
ROC tf rw2 rw4 rw6 rw8
WebKB4 71.9 77.5? 78.6? 80.8? 80.9?
WebKB6 58.3 69.6? 72.0? 76.5? 76.2?
Reuter 78.2 80.8? 81.1? 81.0? 81.4?
20NG 76.2 77.3? 77.1? 77.2? 77.4?
LSpam 97.5 97.8 97.8 97.7 97.8
5 Evaluation and Discussion
Tables 3, 4, 5, 6 show the classification results for
WebKB4, WebKB6, LingSpam, Reuter, and
20Newsgroups respectively. The rw2, rw4, rw6,
and rw8 represent the accuracies achieved using
random-walk weighting under window sizes of 2,
4, 6, and 8 respectively. The tf column represents
the results obtained with a term frequency weighting
scheme.
By examining the results we can see that the
rw.idf model outperforms the tf.idf model on all
the classifiers and datasets with only one excep-
tion in the case of a Na??ve Bayes classifier under
Reuter. The error reductions range from 3.5% as in
{20Newsgroups, NaiveBayes, rw4} to 44% as in
the case of {WebKB6, Rocchio, rw6}. The system
gives, in its worst performance, a comparable result
to the tf.idf baseline. The system shows a consis-
tent performance with different window sizes, with
no clear cut window size that would give the best
result. By further analyzing the results using statis-
tical paired t-tests we can see that windows of size
4 and 6 supply the most significant results across all
the classifiers as well as the datasets.
Comparing WebKB4 and WebKB6 fine-grained
results, we found that both systems failed to pre-
dict the class Staff; however the significant improve-
a paired t-test, with p < 0.05. The result is marked by ? when
p < 0.001.
58
Table 5: KNN Results
KNN tf rw2 rw4 rw6 rw8
WebKB4 59.2 68.6? 67.0? 64.6? 66.6?
WebKB6 55.8 63.7? 55.8 59.9? 61.0?
Reuter 73.6 76.9? 78.1? 78.5? 78.5?
20NG 70.3 76.1? 76.5? 77.2? 77.8?
LSpam 97.5 97.8 97.8 98.1? 97.9
Table 6: SVM Results
SVM tf rw2 rw4 rw6 rw8
WebKB4 87.7 87.9 87.9 89? 88.5
WebKB6 82.5 84.5? 85.2? 85.2? 84.6?
Reuter 83.2 84.5? 84.4? 84.6? 84.1?
20NG 95.2 95.5? 95.6? 95.6? 95.4?
LSpam 95.6 96.4? 96.4? 96.2? 96.3?
ment was over the class Department, in which our
rw model scores an accuracy of 47% compared to
4% in using tf.idf . This indicates how successful
rw.idf model is in cases where there are few train-
ing examples. This could be due to the ability of the
model to extract more realistic and smoother distri-
bution of terms as seen in the rw curve plotted in
Figure 3, hence reducing the feature bias imposed
by the limited number of training examples.
Table 7: Naive Bayes Correlation ?
N.B. rw2 rw4 rw6 rw8
WebKB4 0.68 0.70 0.70 0.66
WebKB6 0.71 0.71 0.71 0.65
Reuter 0.86 0.87 0.87 0.85
20NG 0.82 0.84 0.83 0.82
LSpam 0.89 0.89 0.92 0.92
By also examining the diversity of the classifi-
cation systems based on rw and tf weighting, as
shown in Table 7, 8, 9, 10, we can see an inter-
esting property of the system. The two models are
generally more diverse and less correlated when us-
ing windows of size 6 and 8 than using windows of
size 2 and 4. This could be due to the increasing
drift from the feature independence assumption that
is implied by tf.idf . However increasing the depen-
dency is not always desirable as seen in the reported
accuracies. We expect that at a certain window size
the system performance will degrade to tf.idf . This
Table 8: Rocchio Correlation ?
ROC rw2 rw4 rw6 rw8
WebKB4 0.49 0.51 0.53 0.54
WebKB6 0.40 0.40 0.41 0.42
Reuter 0.75 0.77 0.75 0.71
20NG 0.77 0.77 0.77 0.77
LSpam 0.82 0.85 0.81 0.78
Table 9: KNN Correlation ?
KNN rw2 rw4 rw6 rw8
WebKB4 0.35 0.32 0.36 0.37
WebKB6 0.35 0.35 0.37 0.37
Reuter 0.74 0.70 0.68 0.67
20NG 0.62 0.64 0.63 0.59
LSpam 0.66 0.69 0.63 0.57
threshold window size will be equal to the document
size. In such a case each term will depend on all the
remaining terms resulting in an almost completely
connected graph. Consequently, each feature contri-
bution to the surrounding will be equal resulting in
similar rw scores to all the features.
6 Conclusions and Future Work
Based on results obtained in text classification ex-
periments, the TextRank random-walk model to
term weighting was found to achieve error rate re-
ductions of 3.5?44% as compared to the traditional
frequency-based approach. The evaluation results
have shown that the system performance varies de-
pending on window size, dataset, as well as classi-
fier, with the greatest boost in performance recorded
for KNN ,Rocchio, and SVM. We believe that these
results support our claim that random-walk models
can accurately estimate term weights, and can be
used as a technique to model the probabilistic dis-
tribution of features in a document.
The evaluations reported in this paper has shown
that the TextRank model can accurately provide uni-
gram probabilities for a sequence of words. In future
work we will try to extend the TextRank model and
use it to define a formal language model in which
we can estimate the probability of entire sequences
of words (n-grams).
59
Table 10: SVM Correlation ?
SVM rw2 rw4 rw6 rw8
WebKB4 0.73 0.77 0.78 0.82
WebKB6 0.73 0.76 0.78 0.80
Reuter 0.80 0.83 0.82 0.82
20NG 0.80 0.78 0.82 0.83
LSpam 0.86 0.88 0.88 0.89
References
I. Androutsopoulos, J. Koutsias, K. V. Chandrinos,
G. Paliouras, and C. D. Spyropoulos. 2000. An eval-
uation of naive bayesian anti-spam filtering. In Pro-
ceedings of the workshop on Machine Learning in the
New Information Age.
C. Apte, F. Damerau, and S. M. Weiss. 1994. Towards
language independent automated learning of text cat-
egorisation models. In Proceedings of the 17th ACM
SIGIR Conference on Research and Development in
Information Retrieval.
L. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum
likelihood approach to continuous speech recognition.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 5(2).
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7).
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to extract symbolic knowledge from the World
Wide Web. In Proceedings of the 15th Conference of
the American Association for Artificial Intelligence.
B. Dom, I. Eiron, A. Cozzi, and Y. Shang. 2003. Graph-
based ranking algorithms for e-mail expertise analysis.
In Proceedings of the 8th ACM SIGMOD workshop on
Research issues in data mining and knowledge discov-
ery, San Diego, California.
G. Erkan and D. Radev. 2004. Lexpagerank: Prestige in
multi-document text summarization. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain, July.
G. Grimmett and D. Stirzaker. 1989. Probability and
Random Processes. Oxford University Press.
P.J. Herings, G. van der Laan, and D. Talman. 2001.
Measuring the power of nodes in digraphs. Technical
report, Tinbergen Institute.
T. Joachims. 1996. A probabilistic analysis of the roc-
chio algorithm with tf.idf for text categorization. In
Proceedings of the 14th International Conference on
Machine Learning.
T. Joachims. 1997. A probabilistic analysis of the Roc-
chio algorithm with TFIDF for text categorization. In
Proceedings of ICML-97, 14th International Confer-
ence on Machine Learning, Nashville, US.
L. Kuncheva and C. Whitaker. 2003. Measures of diver-
sity in classifier ensembles and their relationship with
the ensemble accuracy. Machine Learning, 51.
I. Makoto and T. Takenobu. 1995. Cluster-based text cat-
egorization: A comparison of category search starte-
gies. In Proceedings of the 18th ACM International
Conference on Research and Development in Informa-
tion Retrieval.
B. Masand, G. Linoff, and D. Waltz. 1992. Classify-
ing news stories using memory based reasoning. In
Proceedings of the 15th International Conference on
Research and Development in information Retrieval.
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea and S. Hassan. 2005. Using the essence of
texts to improve document classification. In Proceed-
ings of the Conference on Recent Advances in Natural
Language Processing (RANLP), Borovetz, Bulgaria.
R. Mihalcea and P. Tarau. 2004. TextRank ? bringing
order into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2004), Barcelona, Spain.
A. Moschitti. 2003. A study on optimal paramter tun-
ing for Rocchio text classifier. In Proceedings of the
European Conference on Information Retrieval, Pisa,
Italy.
R. Robertson and K. Sparck Jones. 1997. Simple, proven
approaches to text retrieval. Technical report.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
K. Schneider. 2004. A new feature selection score for
multinomial naive bayes text classification based on
kl-divergence. In The Companion Volume to the Pro-
ceedings of 42st Annual Meeting of the Association for
Computational Linguistics, Barcelona, Spain, July.
H. Schutze, D. A. Hull, and J. O. Pedersen. 1995. A
comparison of classifiers and document representa-
tions for the routing problem. In Proceedings of the
18th annual international ACM SIGIR conference on
Research and development in information retrieval,
Seattle, Washington.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
Y. Yang and X. Liu. 1999. A reexamination of text cate-
gorization methods. In Proceedings of the 22nd ACM
SIGIR Conference on Research and Development in
Information Retrieval.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the 14th International Conference on Machine
Learning, Nashville, US.
60
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 410?413,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT: SubFinder: Combining Knowledge Sources for
Automatic Lexical Substitution
Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, Rada Mihalcea?
Department of Computer Science and Engineering
University of North Texas
samer@unt.edu, csomaia@unt.edu, carmenb@unt.edu, rss0089@unt.edu, rada@cs.unt.edu
Abstract
This paper describes the University of North
Texas SUBFINDER system. The system is
able to provide the most likely set of sub-
stitutes for a word in a given context, by
combining several techniques and knowl-
edge sources. SUBFINDER has successfully
participated in the best and out of ten (oot)
tracks in the SEMEVAL lexical substitution
task, consistently ranking in the first or sec-
ond place.
1 Introduction
Lexical substitution is defined as the task of identify-
ing the most likely alternatives (substitutes) for a tar-
get word, given its context (McCarthy, 2002). Many
natural language processing applications can bene-
fit from the availability of such alternative words,
including word sense disambiguation, lexical ac-
quisition, machine translation, information retrieval,
question answering, text simplification, and others.
The task is closely related to the problem of word
sense disambiguation, with the substitutes acting as
synonyms for the input word meaning. Unlike word
sense disambiguation however, lexical substitution
is not performed with respect to a given sense inven-
tory, but instead candidate synonyms are generated
?on the fly? for a given word occurrence. Thus, lexi-
cal substitution can be regarded in a way as a hybrid
task that combines word sense disambiguation and
distributional similarity, targeting the identification
of semantically similar words that fit the context.
2 A system for lexical substitution
SUBFINDER is a system able to provide the most
likely set of substitutes for a word in a given context.
?Contact author.
In SUBFINDER, the lexical substitution task is car-
ried out as a sequence of two steps. First, candidates
are extracted from a variety of knowledge sources;
so far, we experimented with WordNet (Fellbaum,
1998), Microsoft Encarta encyclopedia, Roget, as
well as synonym sets generated from bilingual dic-
tionaries, but additional knowledge sources can be
integrated as well. Second, provided a list of candi-
dates, a number of ranking methods are applied in
a weighted combination, resulting in a final list of
lexical substitutes ranked by their semantic fit with
both the input target word and the context.
3 Candidate Extraction
Candidates are extracted using several lexical re-
sources, which are combined into a larger compre-
hensive resource.
WordNet: WordNet is a large lexical database of
English, with words grouped into synonym sets
called synsets. A problem we encountered with this
resource is that often times the only candidate in the
synset is the target word itself. Thus, to enlarge the
set of candidates, we use both the synonyms and the
hypernyms of the target word. We also remove the
target word from the synset, to ensure that only vi-
able candidates are considered.
Microsoft Encarta encyclopedia: The Microsoft
Encarta is an online encyclopedia and thesaurus re-
source, which provides for each word the part of
speech and a list of synonyms. Using the part of
speech as identified in the context, we are able to ex-
tract synsets for the target word. An important fea-
ture in the Encarta Thesaurus is that the first word
in the synset acts as a definition for the synset, and
therefore disambiguates the target word. This defi-
nition is maintained as a separate entry in the com-
410
prehensive resource, and it is also added to its corre-
sponding synset.
Other Lexical Resources: We have also experi-
mented with two other lexical resources, namely the
Roget thesaurus and a thesaurus built using bilingual
dictionaries. In evaluations carried out on the devel-
opment data set, the best results were obtained using
only WordNet and Encarta, and thus these are the
resources used in the final SUBFINDER system.
All these resources entail different forms of synset
clustering. In order to merge them, we use the
largest overlap among them. It is important to note
that the choice of the first resource considered has
a bearing on the way the synsets are clustered. In
experiments ran on the development data set, the
best results were obtained using a lexical resource
constructed starting with the Microsoft Encarta The-
saurus and then mapping the WordNet synsets to it.
4 Candidate Ranking
Several ranking methods are used to score the can-
didate substitutes, as described below.
Lexical Baseline (LB): In this approach we use
the pre-existing lexical resources to provide a rank-
ing over the candidate substitutes. We rank the can-
didates based on their occurrence in the two selected
lexical resources WordNet and Encarta, with those
occurring in both resources being assigned a higher
ranking. This technique emphasizes the resources
annotators? agreement that the candidates belong in-
deed to the same synset.
Machine Translation (MT): We use machine
translation to translate the test sentences back-and-
forth between English and a second language. From
the resulting English translation, we extract the re-
placement that the machine translation engine pro-
vides for the target word. To locate the translated
word we scan the translation for any of the can-
didates (and their inflections) as obtained from the
comprehensive resource, and score the candidate
synset accordingly.
We experimented with a range of languages such
as French, Italian, Spanish, Simplified Chinese, and
German, but the best results obtained on the devel-
opment data were based on the French translations.
This could be explained because French is part of
the Romance languages family and synonyms to En-
glish words often find their roots in Latin. If we
consider again the word bright, it was translated
into French as intelligent and then translated back
into English as intelligent for obvious reasons. In
one instance, intelligent was the best replacement
for bright in the trial data. Despite the fact that we
also used Italian and Spanish (which are both Latin-
based) we can only assume that French worked bet-
ter because translation engines are better trained on
French. From the resulting English translation, we
extract the replacement that the machine translation
engine provides for the target word. To locate the
translated word we scan the translation for any of the
candidates (and their inflections) as obtained from
the comprehensive resource, and score the candidate
synset accordingly. The translation process was car-
ried out using Google and AltaVista translation en-
gines resulting in two systems MTG and MTA re-
spectively. The translation systems feature high pre-
cision when a candidate is found (about 20% of the
time), at the cost of low recall. The lexical baseline
method is therefore used when no candidates are re-
turned by the translation method.
Most Common Sense (MCS): Another method
we use for ranking candidates is to consider the
first word appearing in the first synset returned by
WordNet. When no words other than the target
word are available in this synset, the method recur-
sively searches the next synset available for the tar-
get word. In order to guarantee a sufficient number
of candidates, we use the lexical baseline method as
a baseline.
Language Model (LM): We model the semantic
fit of a candidate substitute within the given context
using a language model, expressed using the condi-
tional probability:
P (c|g) = P (c, g)/P (g) ? Count(c, g) (1)
where c represents a possible candidate and g rep-
resents the context. The probability P (g) of the
context is the same for all the candidates, hence we
can ignore it and estimate P (c|g) as the N-gram fre-
quency of the context where the target word is re-
placed by the proposed candidate. To avoid skewed
counts that can arise from the different morpholog-
ical inflections of the target word or the candidate
and the bias that the context might have toward any
specific inflection, we generalize P (c|g) to take into
account all the inflections of the selected candidate
as shown in equation 2.
Pn(c|g) ?
n
?
i=1
Count(ci, g) (2)
where n is the number of possible inflections for the
candidate c.
We use the Google N-gram dataset to calculate the
term Count(ci g). The Google N-gram corpus is a
411
collection of English N-grams, ranging from one to
five N-grams, and their respective frequency counts
observed on the Web (Brants and Franz, 2006). In
order for the model to give high preference to the
longer N-grams, while maintaining the relative fre-
quencies of the shorter N-grams (typically more fre-
quent), we augment the counts of the higher order
N-grams with the maximum counts of the lower or-
der N-grams, hence guaranteeing that the score as-
signed to an N-gram of order N is higher than the
the score of an N-gram of order N ? 1.
Semantic Relatedness using Latent Semantic
Analysis (LSA): We expect to find a strong se-
mantic relationship between a good candidate and
the target context. A relatively simple and efficient
way to measure such a relatedness is the Latent Se-
mantic Analysis (Landauer et al, 1998). Documents
and terms are mapped into a 300 dimensional latent
semantic space, providing the ability to measure the
semantic relatedness between two words or a word
and a context. We use the InfoMap package from
Stanford University?s Center for the Study of Lan-
guage and Information, trained on a collection of
approximately one million Wikipedia articles. The
rank of a candidate is given by its semantic related-
ness to the entire context sentence.
Information Retrieval (IR): Although the Lan-
guage Model approach is successful in ranking the
candidates, it suffers from the small N-gram size im-
posed by using the Google N-grams corpus. Such
a restriction is obvious in the following 5-gram ex-
ample who was a bright boy in which the context
is not sufficient to disambiguate between happy and
smart as possible candidates. As a result, we adapt
an information retrieval approach which uses all the
content words available in the given context. Similar
to the previous models, the target word in the con-
text is replaced by all the generated inflections of
the selected candidate and then queried using a web
search engine. The resulting rank represents the sum
of the total number of pages in which the candidate
or any of its inflections occur together with the con-
text. This also reflects the semantic relatedness or
the relevance of the candidate to the context.
Word Sense Disambiguation (WSD): Since pre-
vious work indicated the usefulness of word sense
disambiguation systems in lexical substitution (Da-
gan et al, 2006), we use the SenseLearner word
sense disambiguation tool (Mihalcea and Csomai,
2005) to disambiguate the target word and, accord-
ingly, to propose its synonyms as candidates.
Final System: Our candidate ranking methods are
aimed at different aspects of what constitutes a good
candidate. On one hand, we measure the semantic
relatedness of a candidate with the original context
(the LSA and WSD methods fall under this cate-
gory). On the other hand, we also want to ensure
that the candidate fits the context and leads to a well
formed English sentence (e.g., the language model
method). Given that the methods described earlier
aim at orthogonal aspects of the problem, it is ex-
pected that a combination of these will provide a
better overall ranking.
We use a voting mechanism, where we consider
the reciprocal of the rank of each candidates as given
by one of the described methods. The final score of
a candidate is given by the decreasing order of the
weighted sum of the reciprocal ranks:
score (ci) =
?
m?rankings
?m
1
rmci
To determine the weight ? of each individual
ranking we run a genetic algorithm on the develop-
ment data, optimized for the mode precision and re-
call. Separate sets of weights are obtained for the
best and oot tasks. Table 1 shows the weights of
the individual ranking methods. As expected, for
the best task, the language model type of methods
obtain higher weights, whereas for the oot task, the
semantic methods seem to perform better.
5 Results and Discussion
The SUBFINDER system participated in the best and
the oot tracks of the lexical substitution task. The
best track calls for any number of best guesses,
with the most promising one listed first. The credit
for each correct guess is divided by the number of
guesses. The oot track allows systems to make up to
10 guesses, without penalizing, and without being of
any benefit if less than 10 substitutes are provided.
The ordering of guesses in the oot metric is unim-
portant.
For both tracks, the evaluation is carried out using
precision and recall, calculated based on the number
of matching responses between the system and the
human annotators, respectively. A ?mode? evalua-
tion is also conducted, which measures the ability of
the systems to capture the most frequent response
(the ?mode?) from the gold standard annotations.
For details, please refer to the official task descrip-
tion document (McCarthy and Navigli, 2007).
Tables 2 and 3 show the results obtained by SUB-
FINDER in the best and oot tracks respectively. The
tables also show a breakdown of the results based
412
on: only target words that were not identified as
multiwords (NMWT); only substitutes that were not
identified as multiwords (NMWS); only items with
sentences randomly selected from the Internet cor-
pus (RAND); only items with sentences manually se-
lected from the Internet corpus (MAN).
WSD LSA IR LB MCS MTA MTG LM
best 34 2 64 63 56 69 38 97
oot 6 82 7 28 46 14 32 68
Table 1: Weights of the individual ranking methods
P R Mode P Mode R
OVERALL 12.77 12.77 20.73 20.73
Further Analysis
NMWT 13.46 13.46 21.63 21.63
NMWS 13.79 13.79 21.59 21.59
RAND 12.85 12.85 20.18 20.18
MAN 12.69 12.69 21.35 21.35
Baselines
WORDNET 9.95 9.95 15.28 15.28
LIN 8.84 8.53 14.69 14.23
Table 2: BEST results
P R Mode P Mode R
OVERALL 49.19 49.19 66.26 66.26
Further Analysis
NMWT 51.13 51.13 68.03 68.03
NMWS 54.01 54.01 70.15 70.15
RAND 51.71 51.71 68.04 68.04
MAN 46.26 46.26 64.24 64.24
Baselines
WORDNET 29.70 29.35 40.57 40.57
LIN 27.70 26.72 40.47 39.19
Table 3: OOT results
Compared to other systems participating in this
task, our system consistently ranks on the first or
second place. SUBFINDER clearly outperforms all
the other systems for the ?mode? evaluation, show-
ing the ability of the system to find the substitute
most often preferred by the human annotators. In
addition, the system exceeds by a large margin all
the baselines calculated for the task, which select
substitutes based on existing lexical resources (e.g.,
WordNet or Lin distributional similarity).
Separate from the ?official? submission, we ran
a second experiment where we optimized the com-
bination weights targeting high precision and recall
(rather than high mode). An evaluation of the system
using this new set of weights yields a precision and
recall of 13.34 with a mode of 21.71 for the best task,
surpassing the best system according to the anony-
mous results report. For the oot task, the precision
and recall increased to 50.30, still maintaining sec-
ond place.
6 Conclusions
The lexical substitution task goes beyond simple
word sense disambiguation. To approach such a
task, we first need a good comprehensive and precise
lexical resource for candidate extraction. Secondly,
we need to semantically filter the highly diverse and
ambiguous set of candidates, while taking into ac-
count their fitness in the context in order to form
a proper linguistic expression. To accomplish this,
we built a system that incorporates lexical, semantic,
and probabilistic methods to capture both the seman-
tic similarity with the target word and the semantic
fit in the context. Compared to other systems partic-
ipating in this task, our system consistently ranks on
the first or second place. SUBFINDER clearly out-
performs all the other systems for the ?mode? eval-
uation, proving its ability to find the substitute most
often preferred by the human annotators.
Acknowledgments
This work was supported in part by the Texas Ad-
vanced Research Program under Grant #003594.
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for many useful discussions and
feedback on this work.
References
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings of the In-
ternational Conference on Computational Linguistics
ACL/COLING 2006.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
D. McCarthy and R. Navigli. 2007. The semeval English
lexical substitution task. In Proceedings of the ACL
Semeval workshop.
D. McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
413
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 28?36,
Beijing, August 2010
Multilingual Subjectivity: Are More Languages Better?
Carmen Banea, Rada Mihalcea
Department of Computer Science
University of North Texas
carmenbanea@my.unt.edu
rada@cs.unt.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Abstract
While subjectivity related research in
other languages has increased, most of the
work focuses on single languages. This
paper explores the integration of features
originating from multiple languages into
a machine learning approach to subjectiv-
ity analysis, and aims to show that this
enriched feature set provides for more ef-
fective modeling for the source as well
as the target languages. We show not
only that we are able to achieve over
75% macro accuracy in all of the six lan-
guages we experiment with, but also that
by using features drawn from multiple
languages we can construct high-precision
meta-classifiers with a precision of over
83%.
1 Introduction
Following the terminology proposed by (Wiebe
et al, 2005), subjectivity and sentiment analysis
focuses on the automatic identification of private
states, such as opinions, emotions, sentiments,
evaluations, beliefs, and speculations in natural
language. While subjectivity classification labels
text as either subjective or objective, sentiment or
polarity classification adds an additional level of
granularity, by further classifying subjective text
as either positive, negative or neutral.
To date, a large number of text processing ap-
plications have used techniques for automatic sen-
timent and subjectivity analysis, including auto-
matic expressive text-to-speech synthesis (Alm et
al., 1990), tracking sentiment timelines in on-line
forums and news (Balog et al, 2006; Lloyd et al,
2005), and mining opinions from product reviews
(Hu and Liu, 2004). In many natural language
processing tasks, subjectivity and sentiment clas-
sification has been used as a first phase filtering to
generate more viable data. Research that benefited
from this additional layering ranges from ques-
tion answering (Yu and Hatzivassiloglou, 2003),
to conversation summarization (Carenini et al,
2008), and text semantic analysis (Wiebe and Mi-
halcea, 2006; Esuli and Sebastiani, 2006a).
Although subjectivity tends to be preserved
across languages ? see the manual study in (Mi-
halcea et al, 2007), (Banea et al, 2008) hypoth-
esize that subjectivity is expressed differently in
various languages due to lexicalization, formal
versus informal markers, etc. Based on this obser-
vation, our research seeks to answer the following
questions. First, can we reliably predict sentence-
level subjectivity in languages other than English,
by leveraging on a manually annotated English
dataset? Second, can we improve the English sub-
jectivity classification by expanding the feature
space through the use of multilingual data? Sim-
ilarly, can we also improve the classifiers in the
other target languages? Finally, third, can we ben-
efit from the multilingual subjectivity space and
build a high-precision subjectivity classifier that
could be used to generate subjectivity datasets in
the target languages?
The paper is organized as follows. We intro-
duce the datasets and the general framework in
Section 2. Sections 3, 4, and 5 address in turn each
of the three research questions mentioned above.
Section 6 describes related literature in the area
of multilingual subjectivity. Finally, we draw our
conclusions in Section 7.
2 Multilingual Datasets
Corpora that are manually annotated for subjec-
tivity, polarity, or emotion, are available in only
select languages, since they require a consider-
able amount of human effort. Due to this im-
pediment, the focus of this paper is to create a
method for extrapolating subjectivity data devel-
28
SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
90.4% 34.2% 46.6% 82.4% 30.7% 44.7% 86.7% 32.6% 47.4%
Table 1: Results obtained with a rule-based subjectivity classifier on the MPQA corpus (Wiebe and
Riloff, 2005)
oped in a source language and to transfer it to
other languages. Multilingual feature spaces are
generated to create even better subjectivity classi-
fiers, outperforming those trained on the individ-
ual languages alone.
We use the Multi-Perspective Question An-
swering (MPQA) corpus, consisting of 535
English-language news articles from a variety
of sources, manually annotated for subjectivity
(Wiebe et al, 2005). Although the corpus is an-
notated at the clause and phrase levels, we use
the sentence-level annotations associated with the
dataset in (Wiebe and Riloff, 2005). A sentence
is labeled as subjective if it has at least one pri-
vate state of strength medium or higher. Other-
wise the sentence is labeled as objective. From the
approximately 9700 sentences in this corpus, 55%
of them are labeled as subjective, while the rest
are objective. Therefore, 55% represents the ma-
jority baseline on this corpus. (Wiebe and Riloff,
2005) apply both a subjective and an objective
rule-based classifier to the MPQA corpus data and
obtain the results presented in Table 1.1
In order to generate parallel corpora to MPQA
in other languages, we rely on the method we pro-
posed in (Banea et al, 2008). We experiment with
five languages other than English (En), namely
Arabic (Ar), French (Fr), German (De), Roma-
nian (Ro) and Spanish (Es). Our choice of lan-
guages is motivated by several reasons. First,
we wanted languages that are highly lexicalized
and have clear word delimitations. Second, we
were interested to cover languages that are simi-
lar to English as well as languages with a com-
pletely different etymology. Consideration was
given to include Asian languages, such as Chi-
nese or Japanese, but the fact that their script with-
1For the purpose of this paper we follow this abbreviation
style: Subj stands for subjective, Obj stands for objective,
and All represents overall macro measures, computed over
the subjective and objective classes; P, R, F, and MAcc cor-
respond to precision, recall, F-measure, and macro-accuracy,
respectively.
out word-segmentation preprocessing does not di-
rectly map to words was a deterrent. Finally, an-
other limitation on our choice of languages is the
need for a publicly available machine translation
system between the source language and each of
the target languages.
We construct a subjectivity annotated corpus
for each of the five languages by using machine
translation to transfer the source language data
into the target language. We then project the orig-
inal sentence level English subjectivity labeling
onto the target data. For all languages, other than
Romanian, we use the Google Translate service,2
a publicly available machine translation engine
based on statistical models. The reason Roma-
nian is not included in this group is that, at the
time when we performed the first experiments,
Google Translate did not provide a translation ser-
vice for this language. Instead, we used an al-
ternative statistical translation system called Lan-
guageWeaver,3 which was commercially avail-
able, and which the company kindly allowed us
to use for research purposes.
The raw corpora in the five target lan-
guages are available for download at
http://lit.csci.unt.edu/index.php/Downloads,
while the English MPQA corpus can be obtained
from http://www.cs.pitt.edu/mpqa.
Given the specifics of each language, we em-
ploy several preprocessing techniques. For Ro-
manian, French, English, German and Spanish,
we remove all the diacritics, numbers and punc-
tuation marks except - and ?. The exceptions are
motivated by the fact that they may mark contrac-
tions, such as En: it?s or Ro: s-ar (may be), and
the component words may not be resolved to the
correct forms. For Arabic, although it has a dif-
ferent encoding, we wanted to make sure to treat
it in a way similar to the languages with a Roman
2http://www.google.com/translate t
3http://www.languageweaver.com/
29
alphabet. We therefore use a library4 that maps
Arabic script to a space of Roman-alphabet letters
supplemented with punctuation marks so that they
can allow for additional dimensionality.
Once the corpora are preprocessed, each sen-
tence is defined by six views: one in the origi-
nal source language (English), and five obtained
through automatic translation in each of the tar-
get languages. Multiple datasets that cover all
possible combinations of six languages taken one
through six (a total of 63 combinations) are gen-
erated. These datasets feature a vector for each
sentence present in MPQA (approximately 9700).
The vector contains only unigram features in one
language for a monolingual dataset. For a mul-
tilingual dataset, the vector represents a cumu-
lation of monolingual unigram features extracted
from each view of the sentence. For example, one
of the combinations of six taken three is Arabic-
German-English. For this combination, the vector
is composed of unigram features extracted from
each of the Arabic, German and English transla-
tions of the sentence.
We perform ten-fold cross validation and train
Na??ve Bayes classifiers with feature selection on
each dataset combination. The top 20% of the fea-
tures present in the training data are retained. For
datasets resulting from combinations of all lan-
guages taken one, the classifiers are monolingual
classifiers. All other classifiers are multilingual,
and their feature space increases with each addi-
tional language added. Expanding the feature set
by encompassing a group of languages enables us
to provide an answer to two problems that can ap-
pear due to data sparseness. First, enough training
data may not be available in the monolingual cor-
pus alone in order to correctly infer labeling based
on statistical measures. Second, features appear-
ing in the monolingual test set may not be present
in the training set and therefore their information
cannot be used to generate a correct classification.
Both of these problems are further explained
through the examples below, where we make the
simplifying assumption that the words in italics
are the only potential carriers of subjective con-
tent, and that, without them, their surrounding
4Lingua::AR::Word PERL library.
contexts would be objective. Therefore, their as-
sociation with an either objective or subjective
meaning imparts to the entire segment the same
labeling upon classification.
To explore the first sparseness problem, let us
consider the following two examples extracted
from the English version of the MPQA dataset,
followed by their machine translations in German:
?En 1: rights group Amnesty Interna-
tional said it was concerned about the
high risk of violence in the aftermath?
?En 2: official said that US diplomats
to countries concerned are authorized
to explain to these countries?
?De 1: Amnesty International sagte, es
sei besorgt u?ber das hohe Risiko von
Gewalt in der Folgezeit?
?De 2: Beamte sagte, dass US-
Diplomaten betroffenen La?nder
berechtigt sind, diese La?nder zu
erkla?ren?
We focus our discussion on the word con-
cerned, which in the first example is used in its
subjective sense, while in the second it carries an
objective meaning (as it refers to a group of coun-
tries exhibiting a particular feature defined ear-
lier on in the context). The words in italics in
the German contexts represent the translations of
concerned into German, which are functionally
different as they are shaped by their surrounding
context. By training a classifier on the English ex-
amples alone, under the data sparseness paradigm,
the machine learning model may not differentiate
between the word?s objective and subjective uses
when predicting a label for the entire sentence.
However, appending the German translation to the
examples generates additional dimensions for this
model and allows the classifier to potentially dis-
tinguish between the senses and provide the cor-
rect sentence label.
For the second problem, let us consider two
other examples from the English MPQA and their
respective translations into Romanian:
?En 3: could secure concessions on Tai-
wan in return for supporting Bush on is-
sues such as anti-terrorism and?
30
Lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF MAcc
En 74.01% 83.64% 78.53% 75.89% 63.68% 69.25% 74.95% 73.66% 73.89% 74.72%
Ro 73.50% 82.06% 77.54% 74.08% 63.40% 68.33% 73.79% 72.73% 72.94% 73.72%
Es 74.02% 82.84% 78.19% 75.11% 64.05% 69.14% 74.57% 73.44% 73.66% 74.44%
Fr 73.83% 83.03% 78.16% 75.19% 63.61% 68.92% 74.51% 73.32% 73.54% 74.35%
De 73.26% 83.49% 78.04% 75.32% 62.30% 68.19% 74.29% 72.90% 73.12% 74.02%
Ar 71.98% 81.47% 76.43% 72.62% 60.78% 66.17% 72.30% 71.13% 71.30% 72.22%
Table 2: Na??ve Bayes learners trained on six individual languages
?En 4: to the potential for change
from within America. Supporting our
schools and community centres is a
good?
?Ro 3: ar putea asigura concesii cu
privire la Taiwan, ??n schimb pentru
sust?inerea lui Bush pe probleme cum ar
fi anti-terorismului s?i?
?Ro 4: la potent?ialul de schimbare din
interiorul Americii. Sprijinirea s?colile
noastre s?i centre de comunitate este un
bun?
In this case, supporting is used in both English ex-
amples in senses that are both subjective; the word
is, however, translated into Romanian through two
synonyms, namely sust?inerea and sprijinirea. Let
us assume that sufficient training examples are
available to strengthen a link between support-
ing and sust?inerea, and the classifier is presented
with a context containing sprijinirea, unseen in
the training data. A multilingual classifier may be
able to predict a label for the context using the co-
occurrence metrics based on supporting and ex-
trapolate a label when the context contains both
the English word and its translation into Roma-
nian as sprijinirea. For a monolingual classifier,
such an inference is not possible, and the fea-
ture is discarded. Therefore a multi-lingual classi-
fier model may gain additional strength from co-
occurring words across languages.
3 Question 1
Can we reliably predict sentence-level sub-
jectivity in languages other than English, by
leveraging on a manually annotated English
dataset?
In (Banea et al, 2008), we explored several meth-
ods for porting subjectivity annotated data from
a source language (English) to a target language
(Romanian and Spanish). Here, we focus on the
transfer of manually annotated corpora through
the usage of machine translation by projecting the
original sentence level annotations onto the gener-
ated parallel text in the target language. Our aim
is not to improve on that method, but rather to ver-
ify that the results are reliable across a number of
languages. Therefore, we conduct this experiment
in several additional languages, namely French,
German and Arabic, and compare the results with
those obtained for Spanish and Romanian.
Table 2 shows the results obtained using Na??ve
Bayes classifiers trained in each language individ-
ually, with a macro accuracy ranging from 71.30%
(for Arabic) to 73.89% (for English).5 As ex-
pected, the English machine learner outperforms
those trained on other languages, as the original
language of the annotations is English. However,
it is worth noting that all measures do not deviate
by more than 3.27%, implying that classifiers built
using this technique exhibit a consistent behavior
across languages.
4 Question 2
Can we improve the English subjectivity clas-
sification by expanding the feature space
through the use of multilingual data? Simi-
larly, can we also improve the classifiers in the
other target languages?
We now turn towards investigating the impact on
subjectivity classification of an expanded feature
space through the inclusion of multilingual data.
In order to methodically assess classifier behavior,
we generate multiple datasets containing all pos-
5Note that the experiments conducted in (Banea et al,
2008) were made on a different test set, and thus the results
are not directly comparable across the two papers.
31
No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%
2 74.59% 83.14% 78.63% 75.70% 64.97% 69.92% 75.15% 74.05% 74.28%
3 75.04% 83.27% 78.94% 76.06% 65.75% 70.53% 75.55% 74.51% 74.74%
4 75.26% 83.36% 79.10% 76.26% 66.10% 70.82% 75.76% 74.73% 74.96%
5 75.38% 83.45% 79.21% 76.41% 66.29% 70.99% 75.90% 74.87% 75.10%
6 75.43% 83.66% 79.33% 76.64% 66.30% 71.10% 76.04% 74.98% 75.21%
Table 3: Average measures for a particular number of languages in a combination (from one through
six) for Na??ve Bayes classifiers using a multilingual space
sible combinations of one through six languages,
as described in Section 2. We then train Na??ve
Bayes learners on the multilingual data and av-
erage our results per each group comprised of a
particular number of languages. For example, for
one language, we have the six individual classi-
fiers described in Section 3; for the group of three
languages, the average is calculated over 20 pos-
sible combinations; and so on.
Table 3 shows the results of this experiment.
We can see that the overall F-measure increases
from 73.08% ? which is the average over one lan-
guage ? to 75.21% when all languages are taken
into consideration (8.6% error reduction). We
measured the statistical significance of these re-
sults by considering on one side the predictions
made by the best performing classifier for one lan-
guage (i.e., English), and on the other side the
predictions made by the classifier trained on the
multilingual space composed of all six languages.
Using a paired t-test, the improvement was found
to be significant at p = 0.001. It is worth men-
tioning that both the subjective and the objective
precision measures increase to 75% when more
than 3 languages are considered, while the overall
recall level stays constant at 74%.
To verify that the improvement is due indeed
to the addition of multilingual features, and it is
not a characteristic of the classifier, we also tested
two other classifiers, namely KNN and Rocchio.
Figure 1 shows the average macro-accuracies ob-
tained with these classifiers. For all the classi-
fiers, the accuracies of the multilingual combina-
tions exhibit an increasing trend, as a larger num-
ber of languages is used to predict the subjectivity
annotations. The Na??ve Bayes algorithm has the
best performance, and a relative error rate reduc-
 0.6
 0.65
 0.7
 0.75
 0.8
 1  2  3  4  5  6
Number of languages
NBKNNRocchio
Figure 1: Average Macro-Accuracy per group of
languages (combinations of 6 taken one through
six)
tion in accuracy of 8.25% for the grouping formed
of six languages versus one, while KNN and Roc-
chio exhibit an error rate reduction of 5.82% and
9.45%, respectively. All of these reductions are
statistically significant.
In order to assess how the proposed multilin-
gual expansion improves on the individual lan-
guage classifiers, we select one language at a time
to be the reference, and then compute the aver-
age accuracies of the Na??ve Bayes learner across
all the language groupings (from one through six)
that contain the language. The results from this
experiment are illustrated in Figure 2. The base-
line in this case is represented by the accuracy ob-
tained with a classifier trained on only one lan-
guage (this corresponds to 1 on the X-axis). As
more languages are added to the feature space,
we notice a steady improvement in performance.
When the language of reference is Arabic, we ob-
tain an error reduction of 15.27%; 9.04% for Ro-
32
 0.72
 0.73
 0.74
 0.75
 0.76
 1  2  3  4  5  6
Number of languages
ArDeEnEsFrRo
Figure 2: Average macro-accuracy progression
relative to a given language
manian; 7.80% for German; 6.44% for French;
6.06% for Spanish; and 4.90 % for English. Even
if the improvements seem minor, they are consis-
tent, and the use of a multilingual feature set en-
ables every language to reach a higher accuracy
than individually attainable.
In terms of the best classifiers obtained for
each grouping of one through six, English pro-
vides the best accuracy among individual clas-
sifiers (74.71%). When considering all possible
combinations of six classifiers taken two, German
and Spanish provide the best results, at 75.67%.
Upon considering an additional language to the
mix, the addition of Romanian to the German-
Spanish classifier further improves the accuracy
to 76.06%. Next, the addition of Arabic results
in the best performing overall classifier, with an
accuracy of 76.22%. Upon adding supplemental
languages, such as English or French, no further
improvements are obtained. We believe this is
the case because German and Spanish are able to
expand the dimensionality conferred by English
alone, while at the same time generating a more
orthogonal space. Incrementally, Romanian and
Arabic are able to provide high quality features
for the classification task. This behavior suggests
that languages that are somewhat further apart are
more useful for multilingual subjectivity classifi-
cation than intermediary languages.
5 Question 3
Can we train a high precision classifier with a
good recall level which could be used to gen-
erate subjectivity datasets in the target lan-
guages?
Since we showed that the inclusion of multilingual
information improves the performance of subjec-
tivity classifiers for all the languages involved, we
further explore how the classifiers? predictions can
be combined in order to generate high-precision
subjectivity annotations. As shown in previous
work, a high-precision classifier can be used to
automatically generate subjectivity annotated data
(Riloff and Wiebe, 2003). Additionally, the data
annotated with a high-precision classifier can be
used as a seed for bootstrapping methods, to fur-
ther enrich each language individually.
We experiment with a majority vote meta-
classifier, which combines the predictions of the
monolingual Na??ve Bayes classifiers described in
Section 3. For a particular number of languages
(one through six), all possible combinations of
languages are considered. Each combination sug-
gests a prediction only if its component classifiers
agree, otherwise the system returns an ?unknown?
prediction. The averages are computed across all
the combinations featuring the same number of
languages, regardless of language identity.
The results are shown in Table 4. The
macro precision and recall averaged across groups
formed using a given number of languages are
presented in Figure 3. If the average monolingual
classifier has a precision of 74.07%, the precision
increases as more languages are considered, with
a maximum precision of 83.38% obtained when
the predictions of all six languages are consid-
ered (56.02% error reduction). It is interesting to
note that the highest precision meta-classifier for
groups of two languages includes German, while
for groups with more than three languages, both
Arabic and German are always present in the top
performing combinations. English only appears
in the highest precision combination for one, five
and six languages, indicating the fact that the pre-
dictions based on Arabic and German are more
robust.
We further analyze the behavior of each lan-
guage considering only those meta-classifiers that
include the given language. As seen in Figure 4,
all languages experience a boost in performance
33
No lang SubjP SubjR SubjF ObjP ObjR ObjF AllP AllR AllF
1 73.43% 82.76% 77.82% 74.70% 62.97% 68.33% 74.07% 72.86% 73.08%
2 76.88% 76.39% 76.63% 80.17% 54.35% 64.76% 78.53% 65.37% 70.69%
3 78.56% 72.42% 75.36% 82.58% 49.69% 62.02% 80.57% 61.05% 68.69%
4 79.61% 69.50% 74.21% 84.07% 46.54% 59.89% 81.84% 58.02% 67.05%
5 80.36% 67.17% 73.17% 85.09% 44.19% 58.16% 82.73% 55.68% 65.67%
6 80.94% 65.20% 72.23% 85.83% 42.32% 56.69% 83.38% 53.76% 64.46%
Table 4: Average measures for a particular number of languages in a combination (from one through
six) for meta-classifiers
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 1  2  3  4  5  6
Number of languages
Macro-PrecisionMacro-Recall
Figure 3: Average Macro-Precision and Recall
across a given number of languages
as a result of paired language reinforcement. Ara-
bic gains an absolute 11.0% in average precision
when considering votes from all languages, as
compared to the 72.30% baseline consisting of the
precision of the classifier using only monolingual
features; this represents an error reduction in pre-
cision of 66.71%. The other languages experi-
ence a similar boost, including English which ex-
hibits an error reduction of 50.75% compared to
the baseline. Despite the fact that with each lan-
guage that is added to the meta-classifier, the re-
call decreases, even when considering votes from
all six languages, the recall is still reasonably high
at 53.76%.
The results presented in table 4 are promis-
ing, as they are comparable to the ones obtained
in previous work. Compared to (Wiebe et al,
2005), who used a high-precision rule-based clas-
sifier on the English MPQA corpus (see Table 1),
our method has a precision smaller by 3.32%, but
a recall larger by 21.16%. Additionally, unlike
 0.71
 0.72
 0.73
 0.74
 0.75
 0.76
 0.77
 0.78
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 1  2  3  4  5  6
Number of languages
ArDeEnEsFrRo
Figure 4: Average Macro-Precision relative to a
given language
(Wiebe et al, 2005), which requires language-
specific rules, making it applicable only to En-
glish, our method can be used to construct a high-
precision classifier in any language that can be
connected to English via machine translation.
6 Related Work
Recently, resources and tools for sentiment anal-
ysis developed for English have been used as
a starting point to build resources in other lan-
guages, via cross-lingual projections or mono-
lingual and multi-lingual bootstrapping. Several
directions were followed, focused on leveraging
annotation schemes, lexica, corpora and auto-
mated annotation systems. The English annota-
tion scheme developed by (Wiebe et al, 2005)
for opinionated text lays the groundwork for the
research carried out by (Esuli et al, 2008) when
annotating expressions of private state in the Ital-
ian Content Annotation Bank. Sentiment and
subjectivity lexica such as the one included with
34
the OpinionFinder distribution (Wiebe and Riloff,
2005), the General Inquirer (Stone et al, 1967), or
the SentiWordNet (Esuli and Sebastiani, 2006b)
were transfered into Chinese (Ku et al, 2006; Wu,
2008) and into Romanian (Mihalcea et al, 2007).
English corpora manually annotated for subjec-
tivity or sentiment such as MPQA (Wiebe et al,
2005), or the multi-domain sentiment classifica-
tion corpus (Blitzer et al, 2007) were subjected
to experiments in Spanish, Romanian, or Chinese
upon automatic translation by (Banea et al, 2008;
Wan, 2009). Furthermore, tools developed for En-
glish were used to determine sentiment or sub-
jectivity labeling for a given target language by
transferring the text to English and applying an
English classifier on the resulting data. The labels
were then transfered back into the target language
(Bautin et al, 2008; Banea et al, 2008). These ex-
periments are carried out in Arabic, Chinese, En-
glish, French, German, Italian, Japanese, Korean,
Spanish, and Romanian.
The work closest to ours is the one proposed
by (Wan, 2009), who constructs a polarity co-
training system by using the multi-lingual views
obtained through the automatic translation of
product-reviews into Chinese and English. While
this work proves that leveraging cross-lingual in-
formation improves sentiment analysis in Chinese
over what could be achieved using monolingual
resources alone, there are several major differ-
ences with respect to the approach we are propos-
ing here. First, our training set is based solely
on the automatic translation of the English corpus.
We do not require an in-domain dataset available
in the target language that would be needed for
the co-training approach. Our method is therefore
transferable to any language that has an English-to
target language translation engine. Further, we fo-
cus on using multi-lingual data from six languages
to show that the results are reliable and replicable
across each language and that multiple languages
aid not only in conducting subjectivity research in
the target language, but also in improving the ac-
curacy in the source language as well. Finally,
while (Wan, 2009) research focuses on polarity
detection based on reviews, our work seeks to de-
termine sentence-level subjectivity from raw text.
7 Conclusion
Our results suggest that including multilingual
information when modeling subjectivity can not
only extrapolate current resources available for
English into other languages, but can also improve
subjectivity classification in the source language
itself. We showed that we can improve an English
classifier by using out-of-language features, thus
achieving a 4.90% error reduction in accuracy
with respect to using English alone. Moreover, we
also showed that languages other than English can
achieve an F-measure in subjectivity annotation
of over 75%, without using any manually crafted
resources for these languages. Furthermore, by
combining the predictions made by monolingual
classifiers using a majority vote learner, we are
able to generate sentence-level subjectivity anno-
tated data with a precision of 83% and a recall
level above 50%. Such high-precision classifiers
may be later used not only to create subjectivity-
annotated data in the target language, but also to
generate the seeds needed to sustain a language-
specific bootstrapping.
To conclude and provide an answer to the ques-
tion formulated in the title, more languages are
better, as they are able to complement each other,
and together they provide better classification re-
sults. When one language cannot provide suffi-
cient information, another one can come to the
rescue.
Acknowledgments
This material is based in part upon work supported
by National Science Foundation awards #0917170
and #0916046. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of the National Science
Foundation.
References
Alm, Cecilia Ovesdotter, Dan Roth, and Richard Sproat.
1990. Emotions from text: machine learning for text-
based emotion prediction. Intelligence.
Balog, Krisztian, Gilad Mishne, and Maarten De Rijke.
2006. Why Are They Excited? Identifying and Explain-
ing Spikes in Blog Mood Levels. In Proceedings of the
35
11th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-2006), Trento,
Italy.
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and Samer
Hassan. 2008. Multilingual Subjectivity Analysis Using
Machine Translation. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2008), pages 127?135, Honolulu.
Bautin, Mikhail, Lohit Vijayarenu, and Steven Skiena. 2008.
International Sentiment Analysis for News and Blogs. In
Proceedings of the International Conference on Weblogs
and Social Media (ICWSM-2008), Seattle, Washington.
Blitzer, John, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders: Do-
main Adaptation for Sentiment Classification. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational (ACL-2007), pages 440?447, Prague,
Czech Republic. Association for Computational Linguis-
tics.
Carenini, Giuseppe, Raymond T Ng, and Xiaodong Zhou.
2008. Summarizing Emails with Conversational Cohe-
sion and Subjectivity. In Proceedings of the Association
for Computational Linguistics: Human Language Tech-
nologies (ACL- HLT 2008), pages 353?361, Columbus,
Ohio.
Esuli, Andrea and Fabrizio Sebastiani. 2006a. Determining
Term Subjectivity and Term Orientation for Opinion Min-
ing. In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguistics
(EACL-2006), volume 2, pages 193?200, Trento, Italy.
Esuli, Andrea and Fabrizio Sebastiani. 2006b. SentiWord-
Net: A Publicly Available Lexical Resource for Opinion
Mining. In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation, pages 417?422.
Esuli, Andrea, Fabrizio Sebastiani, and Ilaria C Urciuoli.
2008. Annotating Expressions of Opinion and Emotion
in the Italian Content Annotation Bank. In Proceedings
of the Sixth International Language Resources and Eval-
uation (LREC-2008), Marrakech, Morocco.
Hu, Minqing and Bing Liu. 2004. Mining and Summariz-
ing Customer Reviews. In Proceedings of ACM Confer-
ence on Knowledge Discovery and Data Mining (ACM-
SIGKDD-2004), pages 168?177, Seattle, Washington.
Ku, Lun-wei, Yu-ting Liang, and Hsin-hsi Chen. 2006.
Opinion Extraction, Summarization and Tracking in
News and Blog Corpora. In Proceedings of AAAI-2006
Spring Symposium on Computational Approaches to An-
alyzing Weblogs, number 2001, Boston, Massachusetts.
Lloyd, Levon, Dimitrios Kechagias, and Steven Skiena,
2005. Lydia : A System for Large-Scale News Analysis
( Extended Abstract ) News Analysis with Lydia, pages
161?166. Springer, Berlin / Heidelberg.
Mihalcea, Rada, Carmen Banea, and Janyce Wiebe. 2007.
Learning Multilingual Subjective Language via Cross-
Lingual Projections. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguistics
(ACL-2007), pages 976?983, Prague, Czech Republic.
Riloff, Ellen and Janyce Wiebe. 2003. Learning Extrac-
tion Patterns for Subjective Expressions. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2003), pages 105?112, Sap-
poro, Japan.
Stone, Philip J, Marshall S Smith, Daniel M Ogilivie, and
Dexter C Dumphy. 1967. The General Inquirer: A Com-
puter Approach to Content Analysis. /. The MIT Press,
1st edition.
Wan, Xiaojun. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natural
Language Processing (ACL-IJCNLP 2009), Singapore.
Wiebe, Janyce and Rada Mihalcea. 2006. Word Sense and
Subjectivity. In Proceedings of the joint conference of
the International Committee on Computational Linguis-
tics and the Association for Computational Linguistics
(COLING-ACL-2006), Sydney, Australia.
Wiebe, Janyce and Ellen Riloff. 2005. Creating Subjec-
tive and Objective Sentence Classifiers from Unannotated
Texts. In Proceeding of CICLing-05, International Con-
ference on Intelligent Text Processing and Computational
Linguistics, pages 486?497, Mexico City, Mexico.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation, 39(2-
3):165?210.
Wu, Yejun. 2008. Classifying attitude by topic aspect for
English and Chinese document collections.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from opin-
ions and identifying the polarity of opinion sentence. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-2003), pages
129?136, Sapporo, Japan.
36
Tutorial Abstracts of ACL 2012, page 4,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Multilingual Subjectivity and Sentiment Analysis
Rada Mihalcea
University of North Texas
Denton, Tx
rada@cs.unt.edu
Carmen Banea
University of North Texas
Denton, Tx
carmenbanea@my.unt.edu
Janyce Wiebe
University of Pittsburgh
Pittsburgh, Pa
wiebe@cs.pitt.edu
Abstract
Subjectivity and sentiment analysis focuses on
the automatic identification of private states,
such as opinions, emotions, sentiments, evalu-
ations, beliefs, and speculations in natural lan-
guage. While subjectivity classification labels
text as either subjective or objective, sentiment
classification adds an additional level of gran-
ularity, by further classifying subjective text as
either positive, negative or neutral.
While much of the research work in this
area has been applied to English, research
on other languages is growing, including
Japanese, Chinese, German, Spanish, Ro-
manian. While most of the researchers in
the field are familiar with the methods ap-
plied on English, few of them have closely
looked at the original research carried out in
other languages. For example, in languages
such as Chinese, researchers have been look-
ing at the ability of characters to carry sen-
timent information (Ku et al, 2005; Xiang,
2011). In Romanian, due to markers of po-
liteness and additional verbal modes embed-
ded in the language, experiments have hinted
that subjectivity detection may be easier to
achieve (Banea et al, 2008). These addi-
tional sources of information may not be avail-
able across all languages, yet, various arti-
cles have pointed out that by investigating a
synergistic approach for detecting subjectiv-
ity and sentiment in multiple languages at the
same time, improvements can be achieved not
only in other languages, but in English as
well. The development and interest in these
methods is also highly motivated by the fact
that only 27% of Internet users speak En-
glish (www.internetworldstats.com/stats.htm,
Oct 11, 2011), and that number diminishes
further every year, as more people across the
globe gain Internet access.
The aim of this tutorial is to familiarize the
attendees with the subjectivity and sentiment
research carried out on languages other than
English in order to enable and promote cross-
fertilization. Specifically, we will review work
along three main directions. First, we will
present methods where the resources and tools
have been specifically developed for a given
target language. In this category, we will
also briefly overview the main methods that
have been proposed for English, but which can
be easily ported to other languages. Second,
we will describe cross-lingual approaches, in-
cluding several methods that have been pro-
posed to leverage on the resources and tools
available in English by using cross-lingual
projections. Finally, third, we will show how
the expression of opinions and polarity per-
vades language boundaries, and thus methods
that holistically explore multiple languages at
the same time can be effectively considered.
References
C. Banea, R. Mihalcea, and J. Wiebe. 2008. A Boot-
strapping method for building subjectivity lexicons for
languages with scarce resources. In Proceedings of
LREC 2008, Marrakech, Morocco.
L. W. Ku, T. H. Wu, L. Y. Lee, and H. H. Chen. 2005.
Construction of an Evaluation Corpus for Opinion Ex-
traction. In Proceedings of NTCIR-5, Tokyo, Japan.
L. Xiang. 2011. Ideogram Based Chinese Sentiment
Word Orientation Computation. Computing Research
Repository, page 4, October.
4
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 20?29,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Measuring Semantic Relatedness using Multilingual Representations
Samer Hassan
University of North Texas
Denton, TX
samer@unt.edu
Carmen Banea
University of North Texas
Denton, TX
carmenbanea@my.unt.edu
Rada Mihalcea
University of North Texas
Denton, TX
rada@cs.unt.edu
Abstract
This paper explores the hypothesis that se-
mantic relatedness may be more reliably in-
ferred by using a multilingual space, as com-
pared to the typical monolingual representa-
tion. Through evaluations using several state-
of-the-art semantic relatedness systems, ap-
plied on standard datasets, we show that a
multilingual approach is better suited for this
task, and leads to improvements of up to 47%
with respect to the monolingual baseline.
1 Introduction
Semantic relatedness is the task of quantifying the
strength of the semantic connection between tex-
tual units, be they words, sentences, or documents.
For instance, one may want to determine how se-
mantically related are two words such as car and
automobile, or two pieces of text such as I love an-
imals and I own a pet. It is one of the main tasks
explored in the field of natural language processing,
as it lies at the core of a large number of applica-
tions such as information retrieval (Ponte and Croft,
1998), query reformulation (Metzler et al, 2007;
Yih and Meek, 2007; Sahami and Heilman, 2006;
Broder et al, 2008), image retrieval (Leong and Mi-
halcea, 2009; Goodrum, 2000), plagiarism detection
(Hoad and Zobel, 2003; Shivakumar and Garcia-
Molina, 1995; Broder et al, 1997; Heintze, 1996;
Brin et al, 1995; Manber, 1994), information flow
(Metzler et al, 2005), sponsored search (Broder et
al., 2008), short answer grading (Mohler and Mihal-
cea, 2009a; Pulman and Sukkarieh, 2005; Mitchell
et al, 2002), and textual entailment (Dagan et al,
2005).
The typical approach to semantic relatedness is to
either measure the distance between the constituent
words by using a knowledge base such as Word-
Net or Roget (e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Jarmasz and Szpakowicz, 2003; Peder-
sen et al, 2004)), or to calculate the similarity be-
tween the word distributions in very large corpora
(e.g., (Landauer et al, 1991; Lin, 1998; Gabrilovich
and Markovitch, 2007)). With almost no exception,
these methods have been applied on one language at
a time ? English, most of the time, although mea-
sures of relatedness have also been explored on lan-
guages such as German (Zesch et al, 2007), Chinese
(Li et al, 2005), Japanese (Kazama et al, 2010), and
others.
In this paper, we take a step further and ex-
plore a joint multilingual semantic relatedness met-
ric, which aggregates semantic relatedness scores
measured on several different languages. Specifi-
cally, in our method, in order to measure the re-
latedness of two textual units, we first determine
their relatedness in multiple languages, and conse-
quently infer a final relatedness score by averaging
the scores calculated in the individual languages.
Our hypothesis is that a multilingual representa-
tion can enrich the relatedness space and address
relevant issues such as polysemy (i.e., find that two
occurrences of the same word in language L1 rep-
resent two different meanings because of different
translations in language L2) and synonymy (i.e., find
that two words in language L1 are related because
they have the same translation in language L2). We
show that by measuring relatedness in a multilingual
space, we are able to improve over a traditional re-
latedness measure that relies exclusively on a mono-
lingual representation.
Through experiments using several state-of-the-
art measures of relatedness, applied on a multilin-
gual space including English, Arabic, Spanish, and
Romanian, we aim to answer the following research
20
questions: (1) Does the task of semantic relatedness
benefit from a multilingual representation, as com-
pared to a monolingual one? (2) Does the translation
quality affect the results? and (3) Do the findings
hold for different relatedness datasets?
The paper is organized as follows. First, we
overview related work on word and text related-
ness, and on multilingual natural language process-
ing. We then briefly describe three corpus-based
measures of relatedness, and present several word
and text datasets that have been used in the past to
evaluate relatedness. We then present evaluations
and experiments addressing each of the three re-
search questions, and discuss our findings.
2 Related Work
Semantic relatedness. The approaches for seman-
tic relatedness that have been considered to date
can be grouped into knowledge-based and corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to mea-
sure definitional overlap (Lesk, 1986), term dis-
tance within a graphical taxonomy (Leacock and
Chodorow, 1998), term depth in the taxonomy as a
measure of specificity (Wu and Palmer, 1994), and
others. The application of such measures to a lan-
guage other than English requires the availability of
the lexical resource in that language; furthermore,
even though taxonomies such as WordNet (Miller,
1995) are available in a number of languages1, their
coverage is still limited, and often times they are not
publicly available. For these reasons, in multilingual
settings, these measures often become untractable.
On the other side, corpus-based measures
such as Latent Semantic Analysis (LSA) (Lan-
dauer et al, 1991), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011), Pointwise Mutual Information (PMI)
(Church and Hanks, 1990), PMI-IR (Turney, 2001),
Second Order PMI (Islam and Inkpen, 2006), Hy-
perspace Analogues to Language (HAL) (Burgess
et al, 1998) and distributional similarity (Lin, 1998)
employ probabilistic approaches to decode the se-
mantics of words. They consist of unsupervised
methods that utilize the contextual information and
patterns observed in raw text to build semantic pro-
files of words, and thus they can be easily transferred
1http://www.illc.uva.nl/EuroWordNet/
to a new language provided that a large corpus in that
language is available.
Multilingual natural language processing. Also
relevant is the work done on multilingual text pro-
cessing, which attempts to improve the performance
of different natural language processing tasks by
integrating information drawn from multiple lan-
guages. For instance, (Cohn and Lapata, 2007) ex-
plore the use of triangulation for machine transla-
tion, where multiple translation models are learned
using multilingual parallel corpora. The model was
found especially beneficial for languages where the
training dataset was small, thus suggesting that this
method may be particularly useful for languages
with scarce resources. (Davidov and Rappoport,
2009) experiment with the use of multiple languages
to enhance an existing lexicon. In their experiments,
using three source languages and 45 intermediate
languages, they find that the multilingual resources
can lead to significant improvements in concept ex-
pansion. (Banea et al, 2010) explore the use of
parallel multilingual corpora to improve subjectivity
classification in a target language, finding that the
use of multilingual representations for subjectivity
analysis improves over the monolingual classifiers.
Similarly, (Banea and Mihalcea, 2011) investigate
the use of multilingual contexts for word sense dis-
ambiguation. By leveraging on the translations of
the annotated contexts in multiple languages, a mul-
tilingual thematic space emerges that better disam-
biguates target words.
Finally, there are two lines of work that explore
semantic distances in a multilingual space. First,
(Besanc?on and Rajman, 2002) examine the notion
that the distances between document vectors within
a language correlate with the distances between their
corresponding vectors in a parallel corpus. These
findings provide clues about the possibility of reli-
able semantic knowledge transfer across language
boundaries. Second, (Hassan and Mihalcea, 2009)
propose a framework to compute semantic relat-
edness between two words in different languages,
by considering Wikipedia articles in multiple lan-
guages. The method differs from the one proposed
here, as we aggregate relatedness over monolingual
spaces rather than measuring cross-lingual related-
ness, and we do not specifically use the inter-wiki
links between Wikipedia pages.
21
3 Measures of Text Relatedness
In this work, we focus on corpus-based metrics
because of their unsupervised nature, their flexi-
bility, scalability, and portability to different lan-
guages. Specifically, we utilize three popular mod-
els, LSA (Landauer et al, 1991), ESA (Gabrilovich
and Markovitch, 2007), and SSA (Hassan and Mi-
halcea, 2011). In these models, the semantic profile
of a word is expressed in terms of the explicit (ESA),
implicit (LSA), or salient (SSA) concepts. All three
models are trained on the Wikipedia 2010 corpora
corresponding to the four languages of interest (En-
glish, Arabic, Spanish, Romanian).
Explicit Semantic Analysis. ESA (Gabrilovich
and Markovitch, 2007) uses encyclopedic knowl-
edge in an information retrieval framework to gen-
erate a semantic interpretation of words. Since en-
cyclopedic knowledge is typically organized into
concepts (or topics), each concept is described us-
ing definitions and examples. ESA relies on the
distribution of words inside the encyclopedic de-
scriptions. It builds semantic representations for
a given word using a word-document association,
where each document represents a Wikipedia article.
In this vector representation, the semantic interpre-
tation of a text can be modeled as an aggregation of
the semantic vectors of its individual words.
Latent Semantic Analysis. In LSA (Landauer et
al., 1991), term-context associations are captured by
means of a dimensionality reduction operated by a
singular value decomposition (SVD) on the term-by-
context matrix T, where the matrix is induced from
a large corpus. This reduction entails the abstraction
of meaning by collapsing similar contexts and dis-
counting noisy and irrelevant ones, hence transform-
ing the real world term-context space into a word-
latent-concept space which achieves a much deeper
and concrete semantic representation of words.
Salient Semantic Analysis. SSA (Hassan and
Mihalcea, 2011) incorporates a similar semantic
abstraction and interpretation of words, by using
salient concepts gathered from encyclopedic knowl-
edge, where a concept is defined as an unambigu-
ous word or phrase with a concrete meaning, which
can afford an encyclopedic definition. The links
available between Wikipedia articles, obtained ei-
ther through manual annotation by the Wikipedia
users or using an automatic annotation process, are
regarded as clues or salient features within the text
that help define and disambiguate its context. This
method seeks to determine the semantic relatedness
of words by measuring the distance between their
concept-based profiles, where a profile consists of
co-occurring salient concepts found within a given
window size in a very large corpus.
4 Datasets
To evaluate the representation strength of a multilin-
gual semantic relatedness model we employ several
standard word-to-word and text-to-text datasets. For
each of these datasets, we make use of their repre-
sentation in the four languages of interest.
4.1 Word Relatedness
We construct our multilingual word-to-word
datasets building upon three word relatedness
datasets that have been widely used in the past.
Rubenstein and Goodenough (Rubenstein and
Goodenough, 1965) (RG65) consists of 65 word
pairs ranging from synonymy pairs (e.g., car -
automobile) to completely unrelated words (e.g.,
noon - string). The participating terms in all the
pairs are non-technical nouns annotated by 51 hu-
man judges on a scale from 0 (unrelated) to 4 (syn-
onyms).
Miller-Charles (Miller and Charles, 1991) (MC30)
is a subset of RG65, consisting of 30 word pairs an-
notated for relatedness by 38 human subjects, using
the same 0 to 4 scale.
WordSimilarity-353 (Finkelstein et al, 2001)
(WS353), also known as Finkelstein-353, consists
of 353 word pairs annotated by 13 human experts,
on a scale from 0 (unrelated) to 10 (synonyms).
While containing the MC30 set, it poses an addi-
tional degree of difficulty by also including phrases
(e.g., ?Wednesday news?), proper names and tech-
nical terms.
To enable a multilingual representation, we use
the multilingual datasets introduced by (Hassan and
Mihalcea, 2009), which are based upon MC30 and
WS353. These multilingual datasets are built us-
ing manual translations, following the same guide-
lines adopted for the generation and the annotation
of their original English counterparts. These manu-
ally translated collections, available in Arabic, Span-
ish, and Romanian, allow us to infer an upper bound
for the multilingual semantic relatedness model.
Moreover, in order to provide a more realistic
scenario, where manual translations are not avail-
able, we also create multilingual datasets by auto-
matically translating the three English datasets into
22
Arabic, Spanish and Romanian.2 Similar to how the
manually translated datasets were created by provid-
ing the bilingual speakers with one word pair at a
time, for the automatic translation each word pair is
processed as a single query to the translation engine.
Thus, the co-occurrence metrics derived from large
corpora are able to play a role in providing a dis-
ambiguated translation instead of defaulting to the
most frequently used sense if the words were to be
processed individually. This allows for the embed-
ded word pair relatedness to be transferred to other
languages as well.
4.2 Text Relatedness
We use three standard text-to-text datasets.
Lee50 (Lee and Welsh, 2005) is a compilation of
50 documents collected from the Australian Broad-
casting Corporation?s news mail service. Each doc-
ument is scored by ten annotators on a scale from 1
(unrelated) to 5 (alike) based on its semantic related-
ness to all the other documents. The users? annota-
tion is then averaged per document pair, resulting in
2,500 document pairs annotated with their similarity
scores. Since it was found that there was no signif-
icant difference between annotations given a differ-
ent order of the documents in a pair (Lee and Welsh,
2005), the evaluations are carried out on only 1225
document pairs after ignoring duplicates.
Li30 (Li et al, 2006) is a sentence pair similar-
ity dataset obtained by replacing each of the RG65
word-pairs with their respective definitions extracted
from the Collins Cobuild dictionary (Sinclair, 2001).
Each sentence pair was scored between 0 (unrelated)
to 4 (alike) by 32 native English speakers, and their
annotations were averaged. Due to the skew in the
scores toward low similarity sentence-pairs, they se-
lected a subset of 30 sentences from the 65 sentence
pairs to maintain an even relatedness distribution.
AG400 (Mohler and Mihalcea, 2009b) is a domain
specific dataset from the field of computer science,
used to evaluate the application of semantic relat-
edness measures to real world applications such as
short answer grading. We employ the version pro-
posed by (Hassan and Mihalcea, 2011) which con-
sists of 400 student answers along with the corre-
sponding questions and correct instructor answers.
Each student answer was graded by two judges on
a scale from 0 (completely wrong) to 5 (perfect an-
swer). The correlation between human judges was
2For all the automatic translations we used the Google
Translate service.
measured at 0.64.
First, we construct a multilingual, manually trans-
lated text-to-text relatedness dataset based on the
standard Li30 corpus.3 Native speakers of Spanish,
Romanian and Arabic, who were also highly profi-
cient in English, were asked to translate the entries
drawn from the English collection. They were pre-
sented with one sentence at a time, and asked to pro-
vide the appropriate translation into their native lan-
guage. Since we had five Spanish, two Arabic, and
two Romanian translators, an arbitrator (native to the
language) was charged with merging the candidate
translations by proposing one sentence per language.
Furthermore, to test the abstraction of semantics
from the choice of underlying language, we asked
three different Spanish human experts to re-score the
Spanish text-pair translations on the same scale used
in the construction of the English collection. The
correlation between the relatedness scores assigned
during this experiment and the scores assigned to the
original English experiment was 0.77 ? 0.86, indi-
cating that the translations provided by the bilingual
judges were correct and preserved the semantics of
the original English text-pairs. As was the case
for the manually constructed word-to-word datasets
previously described, the metrics obtained on the
manually translated Li30 dataset will also act as an
upper bound for the text-to-text evaluations.
Finally, for a more sensible scenario where the
text fragments do not require manual translations
in order to compute their semantic relatedness, we
create a multilingual version of the three English
datasets by employing statistical machine translation
to translate the texts into the other three languages.
Each text pair was processed through two separate
queries to the translation engine, since the two text
fragments contain sufficient information to prompt
an in-context translation on their own.
5 Framework
We generate SSA, LSA and ESA vectorial models
for English, Romanian, Arabic, and Spanish, using
the same Wikipedia 2010 versions for all the sys-
tems (e.g., the SSA, LSA and ESA relatedness
measures for Spanish are all trained on the same
Spanish Wikipedia version).
We construct a multilingual model by considering
a word- or text-pair from a source language along
3Dataset is available for download at lit.csci.unt.
edu/index.php?P=research/downloads
23
with its translations in the other languages. To eval-
uate this multilingual model in a way that reduces
the bias that may arise from choosing one language
over the other, we do the following: we start from a
source language and generate all the possible combi-
nations of this language with the available language
set {ar, en, es, ro}. Within each combination, we
average the monolingual model scores for the lan-
guages in this combination with respect to the target
word- or text-pair into a final relatedness score.
For example, let us consider Spanish as the source
language, then the possible combinations of the lan-
guages that include the source language will be
{{es}, {es, ar}, {es, ro}, {es, en}, {es, ar, en},
{es, ar, ro}, {es, en, ro}, and {es, ar, en, ro}}.
For each possible combination, we aggregate the
scores of the languages in that combination. In this
setting, a combination of size (cardinality) one will
always be the source language and will serve as the
baseline. For every combination (e.g. {es, ar}),
we average the individual monolingual relatedness
scores for a given word- or text-pair in this set.
Finally, to calculate the overall correlation of
these generated multilingual models (one system per
combination size) with the human scores, we av-
erage the correlation scores achieved over all the
datasets in a given combination (e.g., {es, ar}) with
all correlation scores achieved under other combina-
tions of the same size (e.g., {es, ro}, {es, en}). This
in effect allows us to observe the cumulative perfor-
mance irrespective of language choice, as we extend
the multilingual model to include more languages.
Formally, let N be the number of languages, Cn
be the set of all language combinations of size n, and
ci be one of the possible combinations of size n,
Cn = {ci | |ci| = n, 0 < i <
(
N
n
)
} (1)
then the relatedness of a word- or text-pair p from
the dataset P under this combination can be repre-
sented as:
Simci(p) =
1
|ci|
?
l?ci
Siml(p) (2)
where Siml(p) is the relatedness score of the word-
or text-pair p in the monolingual model of language
l. To evaluate the performance of the multilingual
model, let Di be the generated relatedness distribu-
tion for the dataset P using the combination ci:
Di = {?p, Simci(p)? | p ? P}. (3)
Then, the correlation between the gold standard
distribution G and the generated scores can be cal-
culated as follows:
CorrelCn(D,G) =
1
|Cn|
?
ci?Cn
Correlci(Di, G),
(4)
where Correl can stand for Pearson (r), Spearman
(?), or their harmonic mean (?), as also reported in
(Hassan and Mihalcea, 2011).
6 Evaluations
In this section we revisit the questions formulated in
the introduction, and based on different experiment
setups following the framework introduced in Sec-
tion 5, we provide an answer to each one of them.
Does the task of semantic relatedness benefit
from a multilingual representation? We evalu-
ate the three semantic relatedness models, namely
LSA, ESA and SSA on our manually constructed
multilingual word relatedness (MC30, WS353)
and text relatedness datasets (LI30), as described in
Section 4.
Figure 1 plots the correlation scores achieved
across all the languages against the gold stan-
dard and then averaged across all the multilingual
datasets. The figure shows a clear and steady im-
provement (25% - 28% with respect to the mono-
lingual baseline) achieved when more languages are
incorporated into the relatedness model. It is worth
noting that both the Pearson and Spearman correla-
tions exhibit the same improvement pattern, which
confirms our hypothesis that adding more languages
has a positive impact on the relatedness scores. The
fact that this trend is visible across all the systems
supports the idea that a multilingual representation
constitutes a better model for determining semantic
relatedness. Furthermore, we notice that SSA is the
best performing system under these settings, with a
correlation improvement of approximately 15%.
To further analyze the role of the multilingual
model and to explore whether some languages ben-
efit from using this abstraction more than others,
we plot the correlation scores achieved by the indi-
vidual languages averaged over all the systems and
the datasets in Figure 2. We notice a sharp rise in
performance associated with the addition of more
languages to the Arabic (42%) and the Romanian
(47%) models, and a slower rise for Spanish (23%).
The performance of English is also affected, but on
a smaller scale (4%) when compared to the other
24
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ESA
LSA
SSA
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 1: Manual translation - average correlation (?,
r, ?) obtained from incorporating scores from models in
other languages
languages. Not surprisingly, this correlates with the
size of each corpus, where Arabic and Romanian are
the smallest, while English is the largest.
The results support the notion that resource poor
languages can benefit from languages with richer
and larger resources, such as English or Spanish.
Furthermore, incorporating additional languages to
English also leads to small improvements, which in-
dicates that the benefit, while disproportionate, is
mutual.
Does the quality of translations affect the results?
As a natural next step, we investigate the role played
by the manual translations in the performance of the
multilingual model. Since the previous evaluations
require the availability of the word- or text-pairs
in multiple languages, we attempt to see if we can
eliminate this restriction by automating the trans-
lation process using statistical machine translation
(MT). Therefore, for a multilingual model employ-
ing automated settings, the manual models proposed
previously constitute an upper bound.
We use the Google MT engine4 to translate our
multilingual datasets into the target languages (en,
es, ar, and ro). We then repeat all the evaluations
using the newly constructed datasets.
Figure 3 shows the correlation scores achieved
across all the languages and averaged across all the
multilingual datasets constructed using automatic
translation. We again see a clear and steady im-
4This API is now offered as a paid service; Microsoft or
Babelfish automatic translation services are publicly available.
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ar
en
es
ro
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 2: Manual translation - average correlation (?,
r, ?) obtained by supplementing a source language with
scores from other languages
provement (12% - 35% with respect to the mono-
lingual baseline) similar to the observed pattern in
the corresponding manual evaluations (Figure 1).
While the overall achieved performance for SSA
has dropped (from ? = 0.793 to ? = 0.71) when
compared to the manual settings, we are still able
to improve over the baseline (? = 0.635). LSA
seems to experience the highest relative improve-
ment (35%), which might be due to its ability to
handle noise in these automatic settings. Over-
all Pearson and Spearman correlations exhibit the
same improvement pattern, which supports the no-
tion that even with the possibility of introducing
noise through miss-translations, the models overall
benefit from the additional clues provided by the
multilingual representation.
To explore the effect of automatic translation on
the individual languages, we plot the correlation
scores achieved vis-a`-vis a reference language, and
average over all the systems and the automatically
translated datasets in Figure 4, in a similar fashion
to Figure 2.
We notice the similar rise in performance asso-
ciated with the addition of more languages to the
Arabic (20%) and the Romanian (37%) models, and
a slower rise for Spanish (16%) and English (8%).
The effect of the automatic translation quality is ev-
ident for the Arabic language where the automatic
translation seems to slow down the improvement
when compared to the manual translations (Figure
2). A similar behavior is also observed in Spanish
and Romanian but on a lower scale.
25
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ESA
LSA
SSA
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 3: Automatic translation - average correlation (?,
r, ?) obtained from incorporating scores from models in
other languages
A very interesting consideration is that English
experiences a stronger improvement when using au-
tomatic translations (8%) compared to manual trans-
lations (4%). This can be attributed to the trans-
lation engine quality in transferring English text to
other languages and to the fact that the statistical
translation (when accurate) can lead to a transla-
tion that makes use of more frequently used words,
which contribute to more robust relatedness mea-
sures. When presented with a word pair, human
judges may provide a translation influenced by the
form/root of the word in the source language, which
may not be as commonly used as the output of a
MT system. For example, when presented with the
pair ?coast - shore,? a Romanian translator may be
tempted to provide ?coasta?? as a translation candi-
date for the first word in the pair, as it resembles the
English word in form. However, the Romanian word
is highly ambiguous, and in an authoritative Roma-
nian dictionary5 its primary sense is that of rib, fol-
lowed by side, slope, and ultimately coast. Thus, a
MT system using a statistical inference may provide
a stronger translation such as ?t?a?rm? that is far less
ambiguous, and whose primary meaning is the one
intended by the original pair.
Overall, the trend is positive and follows the
pattern previously observed on the manually con-
structed datasets. This suggests that an automatic
translation, even if more noisy, is beneficial and pro-
vides a way to reinforce semantic relatedness in a
5http://dexonline.ro/definitie/coasta
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1  2  3  4
Number of Languages
?
ar
en
es
ro
 0.4
 0.6
 0.8
 1  2  3  4
r
 0.4
 0.6
 0.8
 1  2  3  4
?
Figure 4: Automatic translation - average correlation (?,
r, ?) obtained by supplementing a source language with
scores from other languages
given language with information coming from mul-
tiple languages with no manual effort.
Do our findings hold for different relatedness
datasets? At last, encouraged by the small perfor-
mance difference between the use of manual ver-
sus automatic translations, we seek to explore how
this multilingual model behaves under the different
paradigms dictated by word relatedness versus text
relatedness scenarios. Since our previous experi-
ments were constrained to collections for which we
also had a manual translation, we perform a larger
scale evaluation by including automatically trans-
lated word relatedness (RG65) and text relatedness
(LEE50 and AG400) datasets into all the languages
in our language set, and repeat all the word-to-word
and text-to-text evaluations.
Table 1 shows the correlation scores achieved us-
ing automatic translations on the word relatedness
datasets. Most models on most datasets benefit from
the multilingual representation (as shown by the fig-
ures in bold). Specifically, the SSA model has an
improvement in ? of 26% for WS353 and 15% for
MC30. This improvement is most evident in the
case of the largest dataset WS353, where all the
multilingual models exhibit a consistent and strong
performance.
Table 2 reports the results obtained for the text
relatedness datasets using automatic translation.
While the ESA performance suffers in the multi-
lingual model, it is overshadowed by the improve-
ment experienced by LSA and SSA. The multilin-
26
r ? ?
Models MC30 RG65 WS353 MC30 RG65 WS353 MC30 RG65 WS353
ESAen 0.645 0.644 0.487 0.742 0.768 0.525 0.690 0.701 0.506
ESAml 0.723 0.741 0.515 0.766 0.759 0.519 0.744 0.75 0.517
LSAen 0.509 0.450 0.435 0.525 0.499 0.436 0.517 0.473 0.436
LSAml 0.538 0.566 0.487 0.484 0.569 0.517 0.510 0.567 0.502
SSAen 0.771 0.824 0.543 0.688 0.772 0.553 0.727 0.797 0.548
SSAml 0.873 0.807 0.674 0.803 0.795 0.713 0.836 0.801 0.693
Table 1: Automatic translation - r, ?, ? correlations on the word relatedness datasets using multilingual models.
r ? ?
Models LI30 LEE50 AG400 LI30 LEE50 AG400 LI30 LEE50 AG400
ESAen 0.792 0.756 0.434 0.797 0.48 0.392 0.795 0.587 0.412
ESAml 0.776 0.648 0.382 0.742 0.339 0.358 0.759 0.445 0.369
LSAen 0.829 0.776 0.400 0.824 0.523 0.359 0.826 0.625 0.379
LSAml 0.856 0.765 0.46 0.855 0.502 0.404 0.856 0.606 0.43
SSAen 0.840 0.744 0.520 0.843 0.371 0.501 0.841 0.495 0.510
SSAml 0.829 0.743 0.539 0.87 0.41 0.521 0.849 0.528 0.53
Table 2: Automatic translation - r, ?, ? correlations on the text relatedness datasets using multilingual models.
gual model reports some of the best scores in the
literature, such as a correlations of r = 0.856 and
? = 0.87 for LI30 achieved by LSA and SSA, re-
spectively. Not surprisingly, SSA is still a top con-
tender, achieving the highest scores for AG400 and
LI30. In AG400, SSA reports a ? of 0.53 which
represents a 4% improvement over the English SSA
model (? = 0.51) and a 16% improvement over the
best knowledge-based system J&C (? = 0.457).
It is important to note that the evaluation in Ta-
bles 1 and 2 are restricted to data translated from En-
glish into a target language. English, as a resource-
rich language, has an extensive and robust monolin-
gual model, yet it can still be enhanced with addi-
tional clues originating from other languages. Ac-
cordingly, we only expected small improvements in
these two experiments, unlike the cases where we
start from resource-poor languages such as Roma-
nian or Arabic (see Figures 2 and 4).
7 Conclusion
In this paper, we showed how a semantic relatedness
measure computed in a multilingual space is able
to acquire and leverage additional information from
the multilingual representation, and thus be strength-
ened as more languages are taken into considera-
tion. Our experiments seem to suggest that combi-
nations of multiple languages supply additional in-
formation to derive a semantic relatedness between
texts in an automatic framework. Since establishing
semantic relatedness requires us to employ cogni-
tive processes that are in large part independent of
the language that we speak, it comes at no surprise
that using relatedness clues originating from more
than one language allows for a better identification
of relationships between texts. While efficiency may
be a concern, it is worth noting that the method is
highly parallelizable, as the individual relatedness
measures obtained before the aggregation step can
be calculated in parallel.
Notably, all the relatedness measures that we ex-
perimented with exhibited the same improvement
trend. While this framework allows languages with
scarce electronic resources, such as Romanian and
Arabic, to obtain very large improvements in seman-
tic relatedness as compared to the monolingual mea-
sures, improvements are also noticed for languages
with richer resources such as English.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
27
References
C. Banea and R. Mihalcea. 2011. Word sense disam-
biguation with multilingual features. In International
Conference on Semantic Computing, Oxford, UK.
C. Banea, R. Mihalcea, and J. Wiebe. 2010. Multilingual
subjectivity: Are more languages better? In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 28?36, Bei-
jing, China, August.
R. Besanc?on and M. Rajman. 2002. Evaluation of a vec-
tor space similarity measure in a multilingual frame-
work. In Proceedings of the Third International Con-
ference on Language Resource and Evaluation (LREC
2002), Las Palmas, Spain.
S. Brin, J. Davis, and H. Garcia-Molina. 1995. Copy de-
tection mechanisms for digital documents. In ACM In-
ternational Conference on Management of Data (SIG-
MOD 1995).
A. Z. Broder, S. C. Glassman, M. S. Manasse, and
G. Zweig. 1997. Syntactic clustering of the web.
Comput. Netw. ISDN Syst., 29(8-13):1157?1166.
A Z. Broder, P. Ciccolo, M. Fontoura, E. Gabrilovich,
V. Josifovski, and L. Riedel. 2008. Search advertising
using web relevance feedback. In CIKM ?08: Pro-
ceeding of the 17th ACM conference on Information
and knowledge management, pages 1013?1022, New
York, NY, USA. ACM.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: making effective use of multi-parallel
corpora. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
728?735, Prague, Czech Republic.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL recognising textual entailment challenge. In Pro-
ceedings of the PASCAL Workshop.
D. Davidov and A. Rappoport. 2009. Enhancement
of lexical concepts using cross-lingual web mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 852?
861, Singapore.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2001. Plac-
ing search in context: the concept revisited. In ACM
Press, editor, The Tenth International World Wide Web
Conference, pages 406?414, Hong Kong.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
A. Goodrum. 2000. Image information retrieval: An
overview of current research. Informing Science,
3(2):63?66.
S. Hassan and R. Mihalcea. 2009. Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192?
1201, Singapore. Association for Computational Lin-
guistics.
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
N. Heintze. 1996. Scalable document fingerprinting. In
In Proc. USENIX Workshop on Electronic Commerce.
T. C. Hoad and J. Zobel. 2003. Methods for identifying
versioned and plagiarized documents. J. Am. Soc. Inf.
Sci. Technol., 54(3):203?215.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s thesaurus
and semantic similarity. In Proceedings of the confer-
ence on Recent Advances in Natural Language Pro-
cessing RANLP-2003, Borovetz, Bulgaria, September.
J. Kazama, S. De Saeger, K. Kuroda, M. Murata, and
K. Torisawa. 2010. A bayesian method for robust
estimation of distributional similarities. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1991. How well can passage meaning be
derived without using word order? A comparison of
latent semantic analysis and humans. In Proceedings
of the 19th annual meeting of the Cognitive Science
Society, pages 412?417, Mawhwah, N. Erlbaum.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332.
M. D. Lee and M. Welsh. 2005. An empirical evaluation
of models of text document similarity. In Proceedings
of the 27th annual meeting of the Cognitive Science
Society, pages 1254?1259, Stresa, Italy.
C. W. Leong and R. Mihalcea. 2009. Explorations in
automatic image annotation using textual features. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 56?59, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
28
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries. In Proceedings of the
5th annual international conference on Systems docu-
mentation - SIGDOC ?86, pages 24?26, Toronto, On-
tario. ACM Press.
W. Li, Q. Lu, and R. Xu. 2005. Similarity based chinese
synonym collocation extraction. International Journal
of Computational Linguistics and Chinese Language
Processing, 10(1).
Y. Li, D. McLean, Z. A. Bandar, J. D. O?Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?
1150, August.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
U. Manber. 1994. Finding similar files in a large file sys-
tem. In USENIX WINTER 1994 TECHNICAL CON-
FERENCE, pages 1?10.
D. Metzler, Y. Bernstein, W. Bruce Croft, A. Moffat,
and J. Zobel. 2005. Similarity measures for track-
ing information flow. In CIKM ?05: Proceedings
of the 14th ACM international conference on Infor-
mation and knowledge management, pages 517?524,
New York, NY, USA. ACM.
D. Metzler, S. T. Dumais, and C. Meek. 2007. Similarity
measures for short segments of text. In Giambattista
Amati, Claudio Carpineto, and Giovanni Romano, edi-
tors, ECIR, volume 4425 of Lecture Notes in Computer
Science, pages 16?27. Springer.
G. A. Miller and W. G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. In roceedings of the 6th Interna-
tional Computer Assisted Assessment (CAA) Confer-
ence, Loughborough, UK. Loughborough University.
M. Mohler and R. Mihalcea. 2009a. Text-to-text seman-
tic similarity for automatic short answer grading. In
EACL, pages 567?575. The Association for Computer
Linguistics.
M. Mohler and R. Mihalcea. 2009b. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 567?575, Stroudsburg, PA, USA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - measuring the relatedness of
concepts. In Proceedings of the Nineteenth Na-
tional Conference on Artificial Intelligence (AAAI-04),
demonstrations, San Jose, CA.
J. Ponte and W. Croft. 1998. A language modeling ap-
proach to information retrieval. In Proceedings of the
Annual International SIGIR Conference on Research
and Development in Information Retrieval, pages 275?
281, Melbourne, Australia.
S. G. Pulman and J. Z. Sukkarieh. 2005. Automatic
short answer marking. In EdAppsNLP 05: Proceed-
ings of the second workshop on Building Educational
Applications Using NLP, pages 9?16, Morristown, NJ,
USA. Association for Computational Linguistics.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633, October.
M. Sahami and T. D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In WWW ?06: Proceedings of the 15th inter-
national conference on World Wide Web, pages 377?
386, New York, NY, USA. ACM.
N. Shivakumar and H. Garcia-Molina. 1995. Scam: A
copy detection mechanism for digital documents. In
2nd International Conference in Theory and Practice
of Digital Libraries (DL 1995).
J. Sinclair. 2001. Collins cobuild English dictionary for
advanced learners. Harper Collins, 3rd edition.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
W. T. Yih and C. Meek. 2007. Improving similarity mea-
sures for short segments of text. In AAAI?07: Pro-
ceedings of the 22nd national conference on Artificial
intelligence, pages 1489?1494. AAAI Press.
T. Zesch, I. Gurevych, and M. Mu?hlha?user. 2007. Com-
paring Wikipedia and German Wordnet by Evaluating
Semantic Relatedness on Multiple Datasets. In Pro-
ceedings of Human Language Technologies: The An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics.
29
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635?642,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNT: A Supervised Synergistic Approach
to Semantic Text Similarity
Carmen Banea, Samer Hassan, Michael Mohler, Rada Mihalcea
University of North Texas
Denton, TX, USA
{CarmenBanea,SamerHassan,MichaelMohler}@my.unt.edu, rada@cs.unt.edu
Abstract
This paper presents the systems that we par-
ticipated with in the Semantic Text Similar-
ity task at SEMEVAL 2012. Based on prior
research in semantic similarity and related-
ness, we combine various methods in a ma-
chine learning framework. The three varia-
tions submitted during the task evaluation pe-
riod ranked number 5, 9 and 14 among the 89
participating systems. Our evaluations show
that corpus-based methods display a more ro-
bust behavior on the training data, yet com-
bining a variety of methods allows a learning
algorithm to achieve a superior decision than
that achievable by any of the individual parts.
1 Introduction
Measures of text similarity have been used for a
long time in applications in natural language pro-
cessing and related areas. One of the earliest ap-
plications of text similarity is perhaps the vector-
space model used in information retrieval, where the
document most relevant to an input query is deter-
mined by ranking documents in a collection in re-
versed order of their similarity to the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and more recently for
extractive summarization (Salton et al, 1997), and
methods for automatic evaluation of machine trans-
lation (Papineni et al, 2002) or text summarization
(Lin and Hovy, 2003). Measures of text similarity
were also found useful for the evaluation of text co-
herence (Lapata and Barzilay, 2005).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stop-word removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is
an obvious similarity between the text segments I
own a dog and I have an animal, but most of the
current text similarity metrics will fail in identifying
any kind of connection between these texts.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus measures
such as Latent Semantic Analysis (Landauer et al,
1997), Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), or Salient Semantic Analysis
(Hassan and Mihalcea, 2011).
In this paper, we describe the system with which
635
we participated in the SEMEVAL 2012 task on se-
mantic text similarity (Agirre et al, 2012). The sys-
tem builds upon our earlier work on corpus-based
and knowledge-based methods of text semantic sim-
ilarity (Mihalcea et al, 2006; Hassan and Mihal-
cea, 2011; Mohler et al, 2011), and combines all
these previous methods into a meta-system by us-
ing machine learning. The framework provided by
the task organizers also enabled us to perform an in-
depth analysis of the various components used in our
system, and draw conclusions concerning the role
played by the different resources, features, and al-
gorithms in building a state-of-the-art semantic text
similarity system.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
The system we proposed for the SEMEVAL 2012
Semantic Textual Similarity task builds upon both
knowledge- and corpus-based methods previously
described in (Mihalcea et al, 2006; Hassan and Mi-
halcea, 2011; Mohler et al, 2011). The predictions
of these independent systems, paired with additional
salient features, are leveraged by a meta-system that
employs machine learning. In this section, we will
elaborate further on the resources we use, our fea-
tures, and the components of our machine learning
system. We will start by describing the task setup.
3.1 Task Setup
The training data released by the task organiz-
ers consists of three datasets showcasing two sen-
tences per line and a manually assigned similarity
score ranging from 0 (no relation) to 5 (semanti-
cally equivalent). The datasets1 provided are taken
from the Microsoft Research Paraphrase Corpus
(MSRpar), the Microsoft Research Video Descrip-
tion Corpus (MSRvid), and the WMT2008 devel-
opment dataset (Europarl section)(SMTeuroparl);
they each consist of about 750 sentence pairs with
the class distribution varying with each dataset. The
testing data contains additional sentences from the
same collections as the training data as well as
from two additional unknown sets (OnWN and
SMTnews); they range from 399 to 750 sentence
pairs. The reader may refer to (Agirre et al, 2012)
for additional information regarding this task.
3.2 Resources
Wikipedia2 is a free on-line encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
web page, and this ?freedom of contribution? has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential mis-
takes are quickly corrected within the collaborative
environment) of this on-line resource. The basic en-
try in Wikipedia is an article which describes an en-
tity or an event, and which, in addition to untagged
1http://www.cs.york.ac.uk/semeval-2012/
task6/data/uploads/datasets/train-readme.
txt
2www.wikipedia.org
636
content, also consists of hyperlinked text to other
pages within or outside of Wikipedia. These hyper-
links are meant to guide the reader to pages that pro-
vide additional information / clarifications, so that
a better understanding of the primary concept can
be achieved. The structure of Wikipedia in terms of
pages and hyperlinks is exploited directly by seman-
tic similarity methods such as ESA (Gabrilovich and
Markovitch, 2007), or SSA (Hassan and Mihalcea,
2011).
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
between basic units of meaning, or synsets. A synset
groups together senses of different words that share
a very similar meaning, which act in a particu-
lar context as synonyms. Each synset is accompa-
nied by a gloss or definition, and one or two ex-
amples illustrating usage in the given context. Un-
like a traditional thesaurus, the structure of Word-
Net is able to encode additional relationships be-
side synonymy, such as antonymy, hypernymy, hy-
ponymy, meronymy, entailment, etc., which vari-
ous knowledge-based methods use to derive seman-
tic similarity.
3.3 Features
Our meta-system uses several features, which can
be grouped into knowledge-based, corpus-based,
and bipartite graph matching, as described below.
The abbreviations appearing between parentheses
by each method allow for easy cross-referencing
with the evaluations provided in Table 1.
3.3.1 Knowledge-based Semantic Similarity
Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for
each open-class word in one of the input texts, we
compute the maximum semantic similarity (using
the WordNet::Similarity package (Pedersen et al,
2004)) that can be obtained by pairing it with any
open-class word in the other input text. All the
word-to-word similarity scores obtained in this way
are summed and normalized to the length of the two
input texts. We provide below a short description
for each of the similarity metrics employed by this
system3.
The shortest path (Path) similarity is determined
as:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting (including
the end nodes).
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) similarity is determined
as:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
3We point out that the similarity metric proposed by Hirst &
St. Onge was not considered due to the time constraints associ-
ated with the STS task.
637
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
Each of the measures listed above is used as a fea-
ture by our meta-system.
3.3.2 Corpus-based Semantic Similarity
Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
stand out as different, since they rely on a concept-
space representation. In these methods, the semantic
profile of a word is expressed in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch. In the
experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia
download from October 2008, with approximately
6 million articles, and more than 9.5 million hyper-
links.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. Since
encyclopedic knowledge is typically organized into
concepts (or topics), each concept is further de-
scribed using definitions and examples. ESA relies
on the distribution of words inside the encyclopedic
descriptions. It builds semantic representations for
a given word using a word-document association,
where the document represents a Wikipedia article
(concept). ESA is in effect a Vector Space Model
(VSM) built using Wikipedia corpus, where vectors
represents word-articles association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction and interpretation of words as ESA,
yet it uses salient concepts gathered from encyclo-
pedic knowledge, where a ?concept? represents an
unambiguous word or phrase with a concrete mean-
ing, and which affords an encyclopedic definition.
Saliency in this case is determined based on the
word being hyperlinked (either trough manual or au-
tomatic annotations) in context, implying that they
are highly relevant to the given text. SSA is an ex-
ample of Generalized Vector Space Model (GVSM),
where vectors represent word-concepts associations.
In order to determine the similarity of two text
fragments , we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail below.
Both variations were paired with the LSA, ESA,
and SSA systems resulting in six similarity scores
that were used as features by our meta-system,
namely LSAcos, LSAalign, ESAcos, ESAalign,
SSAcos, and SSAalign.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a + b
(8)
where ? is the number of shared terms between the
text fragments and ?i is the similarity score for the
ith pairing.
3.3.3 Bipartite Graph Matching
In an attempt to move beyond the bag-of-words
paradigm described thus far, we attempt to compute
638
a set of dependency graph alignment scores based
on previous work in automatic short-answer grading
(Mohler et al, 2011). This score, computed in two
stages, is used as a feature by our meta-system.
In the first stage, the system is provided with the
dependency graphs for each pair of sentences4. For
each node in one dependency graph, we compute a
similarity score for each node in the other depen-
dency graph based upon a set of lexical, semantic,
and syntactic features applied to both the pair of
nodes and their corresponding subgraphs (i.e. the set
of nodes reachable from a given node by following
directional governor-to-dependant links). The scor-
ing function is trained on a small set of manually
aligned graphs using the averaged perceptron algo-
rithm.
We define a total of 64 features5 to be used to train
a machine learning system to compute subgraph-
subgraph similarity. Of these, 32 are based upon the
bag-of-words semantic similarity of the subgraphs
using the metrics described in Section 3.3.1 as well
as a Wikipedia-trained LSA model. The remaining
32 features are lexico-syntactic features associated
with the parent nodes of the subgraphs and are de-
scribed in more detail in our earlier paper.
We then calculate weights associated with these
features using an averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002) trained on a set of 32 manually annotated
instructor/student answer pairs selected from the
short-answer grading corpus (MM2011). These
pairs contain 7303 node pairs (656 matches, 6647
non-matches). Once the weights are calculated, a
similarity score for each pair of nodes can be com-
puted by taking the dot product of the feature vector
with the weights.
In the second stage, the node similarity scores cal-
culated in the previous step are used to find an op-
timal alignment for the pair of dependency graphs.
We begin with a bipartite graph where each node
in one graph is represented by a node on the left
side of the bipartite graph and each node in the other
4We here use the output of the Stanford Dependency Parser
in collapse/propagate mode with some modifications as de-
scribed in our earlier work.
5With the exception of the four features based upon the Hirst
& St.Onge similarity metric, these are equivalent to the features
used in previous work.
graph is represented by a node on the right side. The
weight associated with each edge is the score com-
puted for each node-node pair in the previous stage.
The bipartite graph is then augmented by adding
dummy nodes to both sides which are allowed to
match any node with a score of zero. An optimal
alignment between the two graphs is then computed
efficiently using the Hungarian algorithm. Note that
this results in an optimal matching, not a mapping,
so that an individual node is associated with at most
one node in the other answer. After finding the opti-
mal match, we produce four alignment-based scores
by optionally normalizing by the number of nodes
and/or weighting the node-alignments according to
the idf scores of the words.6 This results in four
alignment scores listed as graphnone, graphnorm,
graphidf , graphidfnorm.
3.3.4 Baselines
As a baseline, we also utilize several lexical bag-
of-words approaches where each sentence is repre-
sented by a vector of tokens and the similarity of the
two sentences can be computed by finding the co-
sine of the angle between their representative vectors
using term frequency (tf ) or term frequency mul-
tiplied by inverse document frequency (tf.idf )6, or
by using simple overlap between the vectors? dimen-
sions (overlap).
3.4 Machine Learning
3.4.1 Algorithms
All the systems described above are used to gen-
erate a score for each training and test sample (see
Section 3.1). These scores are then aggregated per
sample, and used in a supervised learning frame-
work. We decided to use a regression model, instead
of classification, since the requirements for the task
specify that we should provide a score in the range of
0 to 5. We could have used classification paired with
bucketed ranges, yet classification does not take into
consideration the underlying ordinality of the scores
(i.e. a score of 4.5 is closer to either 4 or 5, but
farther away from 0), which is a noticeable handi-
cap in this scenario. We tried both linear and sup-
6The document frequency scores were taken from the British
National Corpus (BNC).
639
port vector regression7 by performing 10 fold cross-
validation on the train data, yet the latter algorithm
consistently performs better, no matter what kernel
was chosen. Thus we decided to use support vec-
tor regression (Smola and Schoelkopf, 1998) with a
Pearson VII function-based kernel.
Due to its different learning methodology, and
since it is suited for predicting continuous classes,
our second system uses the M5P decision tree al-
gorithm (Quinlan, 1992; Wang and Witten, 1997),
which outperforms support vector regression on the
10 fold cross-validation performed on the SMTeu-
roparl train set, while providing competitive results
on the other train sets (within .01 Pearson correla-
tion).
3.4.2 Setup
We submitted three system variations, namely
IndividualRegression, IndividualDecTree,
and CombinedRegression. The first word de-
scribes the training data; for individual, for the
known test sets we trained on the corresponding
train sets, while for the unknown test sets we trained
on all the train sets combined; for combined,
for each test set we trained on all the train sets
combined. The second word refers to the learning
methodology, where Regression stands for support
vector regression, and DecTree stands for M5P
decision tree.
4 Results and Discussion
We include in Table 1 the Pearson correlations ob-
tained by comparing the predictions of each fea-
ture to the gold standard for the three train datasets.
We notice that the corpus based metrics display a
consistent performance across the three train sets,
when compared to the other methods, including
knowledge-based. Furthermore, the best alignment
strategy (align) for corpus based models outper-
forms similarity scores based on traditional cosine
similarity. It is interesting to note that simple base-
lines such as tf , tf.idf and overlap offer signifi-
cant correlations with all the train sets without ac-
cess to additional knowledge inferred by knowledge
or corpus-based methods. In the case of the bipar-
7Implementations provided through the Weka framework
(Hall et al, 2009).
System MSRpar MSRvid SMTeuroparl
Path 0.49 0.62 0.50
LCH 0.48 0.49 0.45
Lesk 0.48 0.59 0.50
WUP 0.46 0.38 0.42
RES 0.47 0.55 0.48
Lin 0.49 0.54 0.48
JCN 0.49 0.63 0.51
LSAalign 0.44 0.57 0.61
LSAcos 0.37 0.74 0.56
ESAalign 0.52 0.70 0.62
ESAcos 0.30 0.71 0.53
SSAalign 0.46 0.61 0.65
SSAcos 0.22 0.63 0.39
graphnone 0.42 0.50 0.21
graphnorm 0.48 0.43 0.59
graphidf 0.16 0.67 0.16
graphidfnorm 0.08 0.60 0.19
tf.idf 0.45 0.63 0.41
tf 0.45 0.69 0.51
overlap 0.44 0.69 0.27
Table 1: Correlation of individual features for the training
sets with the gold standards
tite graph matching, the graphnorm variation pro-
vides the strongest correlation results across all the
datasets.
We include the evaluation results provided by the
task organizers in Table 2. They indicate that our in-
tuition in using a support vector regression strategy
was correct. While the IndividualRegression was
our strongest system on the training data, the same
ranking applies to the test data (including the addi-
tional two surprise datasets) as well, earning it the
fifth place among the 89 participating systems, with
a Pearson correlation of 0.7846.
Regarding the decision tree based learning
(IndividualDecTree), despite its more robust be-
havior on the train sets, it achieved slightly lower
outcome on the test data, at 0.7677 correlation. We
believe this happened because decision trees have a
tendency to overfit training data, as they generate a
rigid structure which is unforgiving to minor devia-
tions in the test data. Nonetheless, this second vari-
ation still ranks in the top 10% of the submitted sys-
tems.
As an alternative approach to handle unknown test
data (e.g. different distributions, genres), we opted
640
Run ALL Rank Mean RankMean MSRpar MSRvid SMTeuroparl OnWN SMTnews
IndividualRegression 0.7846 5 0.6162 13 0.5353 0.8750 0.4203 0.6715 0.4033
IndividualDecTree 0.7677 9 0.5947 25 0.5693 0.8688 0.4203 0.6491 0.2256
CombinedRegression 0.7418 14 0.6159 14 0.5032 0.8695 0.4797 0.6715 0.4033
Table 2: Evaluation results and ranking published by the task organizers
to also include the CombinedRegression strategy
as our third variation. This seems to have been fruit-
ful for MSRvid, SMTeuroparl, and the two sur-
prise datasets (ONWn and SMTnews). In the
case of SMTeuroparl, this expanded training set
achieves a better performance than learning from
the corresponding training set alne, gaining an im-
provement of 0.0776 correlation points. Unfortu-
nately, the variation has some losses, particularly for
the MSRpar dataset (0.0321), yet it is able to con-
sistently model and handle a wider variety of text
types.
5 Conclusion
This paper describes the three system variations our
team participated with in the Semantic Text Similar-
ity task in SEMEVAL 2012. Our focus has been to
produce a synergistic approach, striving to achieve a
superior result than attainable by each system indi-
vidually. We have considered a variety of methods
for inferring semantic similarity, including knowl-
edge and corpus-based methods. These were lever-
aged in a machine-learning framework, where our
preferred learning algorithm is support vector re-
gression, due to its ability to deal with continuous
classes and to dampen the effect of noisy features,
while augmenting more robust ones. While it is al-
ways preferable to use similar test and train sets,
when information regarding the test dataset is un-
available, we show that a robust performance can
be achieved by combining all train data from dif-
ferent sources into a single set and allowing a ma-
chine learner to make predictions. Overall, it was
interesting to note that corpus-based methods main-
tain strong results on all train datasets in compari-
son to knowledge-based methods. Our three systems
ranked number 5, 9 and 14 among the 89 systems
participating in the task.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
641
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1997. How well can passage meaning be
derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M.E. Lesk, 1971. The SMART Retrieval
System: Experiments in Automatic Document Process-
ing, chapter Computer evaluation of indexing and text
processing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
642
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560?565,
Dublin, Ireland, August 23-24, 2014.
SimCompass: Using Deep Learning Word Embeddings
to Assess Cross-level Similarity
Carmen Banea, Di Chen,
Rada Mihalcea
?
University of Michigan
Ann Arbor, MI
Claire Cardie
Cornell University
Ithaca, NY
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
Abstract
This article presents our team?s partici-
pating system at SemEval-2014 Task 3.
Using a meta-learning framework, we
experiment with traditional knowledge-
based metrics, as well as novel corpus-
based measures based on deep learning
paradigms, paired with varying degrees of
context expansion. The framework en-
abled us to reach the highest overall per-
formance among all competing systems.
1 Introduction
Semantic textual similarity is one of the key
components behind a multitude of natural lan-
guage processing applications, such as informa-
tion retrieval (Salton and Lesk, 1971), relevance
feedback and text classification (Rocchio, 1971),
word sense disambiguation (Lesk, 1986; Schutze,
1998), summarization (Salton et al., 1997; Lin
and Hovy, 2003), automatic evaluation of machine
translation (Papineni et al., 2002), plagiarism de-
tection (Nawab et al., 2011), and more.
To date, semantic similarity research has pri-
marily focused on comparing text snippets of simi-
lar length (see the semantic textual similarity tasks
organized during *Sem 2013 (Agirre et al., 2013)
and SemEval 2012 (Agirre et al., 2012)). Yet,
as new challenges emerge, such as augmenting a
knowledge-base with textual evidence, assessing
similarity across different context granularities is
gaining traction. The SemEval Cross-level seman-
tic similarity task is aimed at this latter scenario,
and is described in more details in the task paper
(Jurgens et al., 2014).
?
{carmennb,chenditc,mihalcea}@umich.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness us-
ing methods that are either knowledge-based or
corpus-based. Knowledge-based methods derive
a measure of relatedness by utilizing lexical re-
sources and ontologies such as WordNet (Miller,
1995) or Roget (Rog, 1995) to measure defi-
nitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy
as a measure of specificity. There are many
knowledge-based measures that were proposed in
the past, e.g., (Leacock and Chodorow, 1998;
Lesk, 1986; Resnik, 1995; Jiang and Conrath,
1997; Lin, 1998; Jarmasz and Szpakowicz, 2003;
Hughes and Ramage, 2007).
On the other side, corpus-based measures such
as Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997), Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007),
Salient Semantic Analysis (SSA) (Hassan and
Mihalcea, 2011), Pointwise Mutual Informa-
tion (PMI) (Church and Hanks, 1990), PMI-IR
(Turney, 2001), Second Order PMI (Islam and
Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al., 1998) and distributional
similarity (Lin, 1998) employ probabilistic ap-
proaches to decode the semantics of words. They
consist of unsupervised methods that utilize the
contextual information and patterns observed in
raw text to build semantic profiles of words. Un-
like knowledge-based methods, which suffer from
limited coverage, corpus-based measures are able
to induce the similarity between any two words, as
long as they appear in the corpus used for training.
3 System Description
3.1 Generic Features
Our system employs both knowledge and corpus-
based measures as detailed below.
560
Knowledge-based features
Knowledge-based metrics were shown to provide
high correlation scores with the goldstandard in
text similarity tasks (Agirre et al., 2012; Agirre et
al., 2013). We used three WordNet-based simi-
larity measures that employ information content.
We chose these metrics because they are able to
incorporate external information derived from a
large corpus: Resnik (Resnik, 1995) (RES), Lin
(Lin, 1998) (LIN ), and Jiang & Conrath (Jiang
and Conrath, 1997) (JCN ).
Corpus based features
Our corpus based features are derived from a
deep learning vector space model that is able to
?understand? word meaning without human in-
put. Distributed word embeddings are learned us-
ing a skip-gram recurrent neural net architecture
running over a large raw corpus (Mikolov et al.,
2013b; Mikolov et al., 2013a). A primary advan-
tage of such a model is that, by breaking away
from the typical n-gram model that sees individual
units with no relationship to each other, it is able to
generalize and produce word vectors that are simi-
lar for related words, thus encoding linguistic reg-
ularities and patterns (Mikolov et al., 2013b). For
example, vec(Madrid)-vec(Spain)+vec(France) is
closer to vec(Paris) than any other word vec-
tor (Mikolov et al., 2013a). We used the pre-
trained Google News word2vec model (WTV )
built over a 100 billion words corpus, and con-
taining 3 million 300-dimension vectors for words
and phrases. The model is distributed with the
word2vec toolkit.
1
Since the methods outlined above provide similar-
ity scores at the sense or word level, we derive text
level metrics by employing two methods.
VectorSum. We add the vectors corresponding to
the non-stopwords tokens in bag of words (BOW)
A and B, resulting in vectors V
A
and V
B
, respec-
tively. The assumption is that these vectors are
able to capture the semantic meaning associated
with the contexts, enabling us to gauge their relat-
edness using cosine similarity.
Align. Given two BOW A and B as input, we
compare them using a word-alignment-based sim-
ilarity measure (Mihalcea et al., 2006). We calcu-
late the pairwise similarity between the words in
A and B, and match each word in A with its most
similar counterpart in B. For corpus-based fea-
1
https://code.google.com/p/word2vec/
tures, the similarity measure represents the aver-
age over these scores, while for knowledge-based
measures, we consider the top 40% ranking pairs.
We use the DKPro Similarity package (B?ar et
al., 2013) to compute knowledge-based metrics,
and the word2vec implementation from the Gen-
sim toolkit (Rehurek and Sojka, 2010).
3.2 Feature Variations
Since our system participated in all four lexical
levels evaluations, we describe below the modifi-
cations pertaining to each.
word2sense. At the word2sense level, we em-
ploy both knowledge and corpus-based features.
Since the information available in each pair is ex-
tremely limited (only a word and a sense key)
we infuse contextual information by drawing on
WordNet (Miller, 1995). In WordNet, the sense
of each word is encapsulated in a uniquely iden-
tifiable synset, consisting of the definition (gloss),
usage examples and its synonyms. We can derive
three variations (where the word and sense com-
ponents are represented by BOW A and B, respec-
tively): a) no expansion (A={word}, B={sense}),
b) expand right (R) (A={word}, B={sense gloss
& example}), c) expand left (L) & right (R)
(A={word glosses & examples}, B={sense gloss
& example}). After applying the Align method,
we obtain measures JNC, LIN , RES and
WTV 1; VectorSum results in WTV 2.
phrase2word. As this lexical level also suf-
fers from low context, we adapt the above vari-
ations, where the phrase and word components
are represented by BOW A and BOW B, re-
spectively. Thus, we have: a) no expan-
sion (A={phrase}, B={word}), b) expand R
(A={phrase}, B={word glosses and examples}),
c) expand L & R (A={phrase glosses & exam-
ples}, B={word glosses and examples}). We ex-
tract the same measures as for word2sense.
sentence2phrase. For this variation, we use only
corpus based measures; BOW A represents the
sentence component, B, the phrase. Since there is
sufficient context available, we follow the no ex-
pansion variation, and obtain metrics WTV 1 (by
applying Align) and WTV 2 (using VectorSum).
paragraph2sentence. At this level, due to the
long context that entails one-to-many mappings
between the words in the sentence and those in
the paragraph, we use a text clustering technique
prior to calculating the features? weights.
561
a) no clustering. We use only corpus based mea-
sures, where the paragraph represents BOW A,
and the sentence represents BOW B. Then we ap-
ply Align and VectorSum, resulting in WTV 1 and
WTV 2, respectively.
b) paragraph centroids extraction. Since the
longer text contains more information compared
to the shorter one, we extract k topic vectors after
K-means clustering the left context.
2
These cen-
troids are able to model topics permeating across
sentences, and by comparing them with the word
vectors pertaining to the short text, we seek to cap-
ture how much of the information is covered in the
shorter text. Each word is paired with the centroid
that it is closest to, and the average is computed
over these scores, resulting in WTV 3.
c) sentence centroids extraction. Under a dif-
ferent scenario, assuming that one sentence cov-
ers only a few strongly expressed topics, unlike
a paragraph that may digress and introduce unre-
lated noise, we apply clustering on the short text.
The centroids thus obtained are able to capture
the essence of the sentence, so when compared to
every word in the paragraph, we can gauge how
much of the short text is reflected in the longer
one. Each centroid is paired with the word that it is
most similar to, and we average these scores, thus
obtaining WTV 4. In a way, methods b) and c)
provide a macro, respectively micro view of how
the topics are reflected across the two spans of text.
3.3 Meta-learning
The measures of similarity described above pro-
vide a single score per each long text - short text
pair in the training and test data. These scores then
become features for a meta-learner, which is able
to optimize their impact on the prediction process.
We experimented with multiple regression algo-
rithms by conducting 10 fold cross-validation on
the training data. The strongest performer across
all lexical levels was Gaussian processes with a
radial basis function (RBF) kernel. Gaussian pro-
cesses regression is an efficient probabilistic pre-
diction framework that assumes a Gaussian pro-
cess prior on the unobservable (latent) functions
and a likelihood function that accounts for noise.
An individual classifier
3
was trained for each lex-
ical level and applied to the test data sets.
2
Implementation provided in the Scikit library (Pedregosa
et al., 2011), where k is set to 3.
3
Implementation available in the WEKA machine learn-
ing software (Hall et al., 2009) using the default parameters.
4 Evaluations & Discussion
Our system participated in all cross-level subtasks
under the name SimCompass, competing with 37
other systems developed by 20 teams.
Figure 1 highlights the Pearson correlations at
the four lexical levels between the gold standard
and each similarity measure introduced in Section
3, as well as the predictions ensuing as a result
of meta-learning. The left and right histograms in
each subfigure present the scores obtained on the
train and test data, respectively.
In the case of word2sense train data, we no-
tice that expanding the context provides additional
information and improves the correlation results.
For corpus-based measures, the correlations are
stronger when the expansion involves only the
right side of the tuple, namely the sense. We
notice an increase of 0.04 correlation points for
WTV1 and 0.09 for WTV2. As soon as the word
is expanded as well, the context incorporates too
much noise, and the correlation levels drop. In
the case of knowledge-based measures, expanding
the context does not seem to impact the results.
However, these trends do not carry out to the test
data, where the corpus-based features without ex-
pansion reach a correlation higher than 0.3, while
the knowledge-based features score significantly
lower (by 0.16). Once all these measures are used
as features in a meta learner (All) using Gaus-
sian processes regression (GP), the correlation in-
creases over the level attained by the best perform-
ing individual feature, reaching 0.45 on the train
data and 0.36 on the test data. SimCompass ranks
second in this subtask?s evaluations, falling short
of the leading system by 0.025 correlation points.
Turning now to the phrase2word subfigure, we
notice that the context already carries sufficient
information, and expanding it causes the perfor-
mance to drop (the more extensive the expan-
sion, the steeper the drop). Unlike the scenario
encountered for word2sense, the trend observed
here on the training data also gets mirrored in the
test data. Same as before, knowledge-based mea-
sures have a significantly lower performance, but
deep learning-based features based on word2vec
(WTV) only show a correlation variation by at
most 0.05, proving their robustness. Leveraging
all the features in a meta-learning framework en-
ables the system to predict stronger scores for both
the train and the test data (0.48 and 0.42, respec-
tively). Actually, for this variation, SimCompass
562
 0
 0.1
 0.2
 0.3
 0.4
 0.5
w
o
rd
2s
en
se
Train Test
 0
 0.1
 0.2
 0.3
 0.4
 0.5
BL JNC
LIN
RES
W
TV
1
W
TV
2
G
P
JN
C
LIN
RES
W
TV
1
W
TV
2
G
P
ph
ra
se
2w
or
d
No expansion Expansion R Expansion L&R All
 0
 0.2
 0.4
 0.6
 0.8
s
e
n
te
n
ce
2p
hr
as
e
Train Test
BL WTV
1
W
TV
2
W
TV
3
W
TV
4
G
P
W
TV
1
W
TV
2
W
TV
3
W
TV
4
G
P
 0
 0.2
 0.4
 0.6
 0.8
pa
ra
gr
ap
h2
se
nt
en
ce
Figure 1: Pearson correlation of individual measures on the train and test data sets. As these measures be-
come features in a regression algorithm (GP), prediction correlations are included as well. BL represents
the baseline computed by the organizers.
obtains the highest score among all competing sys-
tems, surpassing the second best by 0.10.
Noticing that expansion is not helpful when suf-
ficient context is available, for the next variations
we use the original tuples. Also, due to the re-
duced impact of knowledge-based features on the
training outcome, we only focus on deep learning
features (WTV1, WTV2, WTV3, WTV4).
Shifting to sentence2phrase, WTV2 (con-
structed using VectorSum) is the top perform-
ing feature, surpassing the baseline by 0.19,
and attaining 0.69 and 0.73 on the train and
test sets, respectively. Despite also considering
a lower performing feature (WTV1), the meta-
learner maintains high scores, surpassing the cor-
relation achieved on the train data by 0.04 (from
0.70 to 0.74). In this variation, our system ranks
fifth, at 0.035 from the top system.
For the paragraph2sentence variation, due to
the availability of longer contexts, we introduce
WTV3 and WTV4 that are based on clustering the
left and the right sides of the tuple, respectively.
WTV2 fares slightly better than WTV3 and WTV4.
WTV1 surpasses the baseline this time, leaving its
mark on the decision process. When training the
GP learner on all features, we obtain 0.78 correla-
tion on the train data, and 0.81 on test data, 0.10
higher than those attained by the individual fea-
tures alone. SimCompass ranks seventh in perfor-
mance on this subtask, at 0.026 from the first.
Considering the overall system performance,
SimCompass is remarkably versatile, ranking
among the top at each lexical level, and taking the
first place in the SemEval Task 3 overall evalu-
ation with respect to both Pearson (0.58 average
correlation) and Spearman correlations.
5 Conclusion
We described SimCompass, the system we partic-
ipated with at SemEval-2014 Task 3. Our exper-
iments suggest that traditional knowledge-based
features are cornered by novel corpus-based word
meaning representations, such as word2vec, which
emerge as efficient and strong performers under
a variety of scenarios. We also explored whether
context expansion is beneficial to the cross-level
similarity task, and remarked that only when the
context is particularly short, this enrichment is vi-
able. However, in a meta-learning framework, the
information permeating from a set of similarity
measures exposed to varying context expansions
can attain a higher performance than possible with
individual signals. Overall, our system ranked first
among 21 teams and 38 systems.
Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation CAREER
award #1361274 and IIS award #1018613 and
by DARPA-BAA-12-47 DEFT grant #12475008.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
563
of the National Science Foundation or the Defense
Advanced Research Projects Agency.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012), Montreal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pi-
lot on Typed-Similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013).
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121?126,
Sofia, Bulgaria.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: words, sentences,
discourse. Discourse Processes, 25(2):211?257.
Kenneth Church and Patrick Hanks. 1990. Word as-
sociation norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606?1611, Hyderabad, India.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Samer Hassan and Rada Mihalcea. 2011. Measuring
semantic relatedness using salient encyclopedic con-
cepts. Artificial Intelligence, Special Issue.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic knowledge with random graph walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Prague, Czech
Republic.
Aminul Islam and Diana Zaiu Inkpen. 2006. Second
order co-occurrence PMI for determining the seman-
tic similarity of words. In Proceedings of the Fifth
Conference on Language Resources and Evaluation,
volume 2, pages 1033?1038, Genoa, Italy, July.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Proceedings
of the conference on Recent Advances in Natural
Language Processing RANLP-2003, Borovetz, Bul-
garia, September.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceeding of the International Confer-
ence Research on Computational Linguistics (RO-
CLING X), Taiwan.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review, 104.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database. The MIT Press.
Michael E. Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Pro-
ceedings of the SIGDOC Conference 1986, Toronto,
June.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of Human Lan-
guage Technology Conference (HLT-NAACL 2003),
Edmonton, Canada, May.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In American
Association for Artificial Intelligence (AAAI-2006),
Boston, MA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Distributed Representations of Words
and Phrases and their Compositionality . In NIPS,
pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In NAACL HLT, pages 746?
751, Atlanta, GA, USA.
George A. Miller. 1995. WordNet: a Lexical database
for English. Communications of the Association for
Computing Machinery, 38(11):39?41.
Rao Muhammad Adeel Nawab, Mark Stevenson, and
Paul Clough. 2011. External plagiarism detection
using information retrieval and sequence alignment:
564
Notebook for PAN at CLEF 2011. In Proceedings
of the 5th International Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2011).
Kishore. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825?2830.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45?50, Valletta,
Malta, May. ELRA.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence, pages 448?453, Montreal,
Quebec, Canada. Morgan Kaufmann Publishers Inc.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
1995. Roget?s II: The New Thesaurus. Houghton Mif-
flin.
Gerard Salton and Michael E. Lesk, 1971. The SMART
Retrieval System: Experiments in Automatic Doc-
ument Processing, chapter Computer evaluation of
indexing and text processing. Prentice Hall, Ing. En-
glewood Cliffs, New Jersey.
Gerard Salton, Amit Singhal, Mandar Mitra, and Chris
Buckley. 1997. Automatic text structuring and sum-
marization. Information Processing and Manage-
ment, 2(32).
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
124.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
565
Word Sense Disambiguation with Multilingual Features
Carmen Banea and Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
carmenbanea@my.unt.edu, rada@cs.unt.edu
Abstract
This paper explores the role played by a multilingual feature representation for the task of word
sense disambiguation. We translate the context of an ambiguous word in multiple languages, and
show through experiments on standard datasets that by using a multilingual vector space we can
obtain error rate reductions of up to 25%, as compared to a monolingual classifier.
1 Introduction
Ambiguity is inherent to human language. In particular, word sense ambiguity is prevalent in all natural
languages, with a large number of the words in any given language carrying more than one meaning.
For instance, the English noun plant can mean green plant or factory; similarly the French word feuille
can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context
where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of
automatically assigning the most appropriate meaning to a polysemous word within a given context.
Among the various knowledge-based (Lesk, 1986; Mihalcea et al, 2004) and data-driven (Yarowsky,
1995; Ng and Lee, 1996) word sense disambiguation methods that have been proposed to date, supervised
systems have been constantly observed as leading to the highest performance. In these systems, the sense
disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence
of a particular word is transformed into a feature vector which is then used in an automatic learning
process. One of the main drawbacks associated with these methods is the fact that their performance is
closely connected to the amount of labeled data available at hand.
In this paper, we investigate a new supervised word sense disambiguation method that is able to take
additional advantage of the sense-labeled examples by exploiting the information that can be obtained
from a multilingual representation. We show that by representing the features in a multilingual space,
we are able to improve the performance of a word sense disambiguation system by a significant margin,
as compared to a traditional system that uses only monolingual features.
2 Related Work
Despite the large number of word sense disambiguation methods that have been proposed so far, targeting
the resolution of word ambiguity in different languages, there are only a few methods that try to explore
more than one language at a time. The work that is perhaps most closely related to ours is the bilin-
gual bootstrapping method introduced in (Li and Li, 2002), where word translations are automatically
disambiguated using information iteratively drawn from two languages. Unlike that approach, which
iterates between two languages to select the correct translation for a given target word, in our method we
simultaneously use the features extracted from several languages. In fact, our method can handle more
than two languages at a time, and we show that the accuracy of the disambiguation algorithm increases
with the number of languages used.
There have also been a number of attempts to exploit parallel corpora for word sense disambiguation
(Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al, 2003), but in that line of work the parallel
25
texts were mainly used as a way to induce word senses or to create sense-tagged corpora, rather than as
a source of additional multilingual views for the disambiguation features. Another related technique is
concerned with the selection of correct word senses in context using large corpora in a second language
(Dagan and Itai, 1994), but as before, the additional language is used to help distinguishing between the
word senses in the original language, and not as a source of additional information for the disambiguation
context.
Also related is the recent SEMEVAL task that has been proposed for cross-lingual lexical substitution,
where the word sense disambiguation task was more flexibly formulated as the identification of cross-
lingual lexical substitutes in context (Mihalcea et al, 2010). A number of different approaches have been
proposed by the teams participating in the task, and although several of them involved the translation of
contexts or substitutes from one language to another, none of them attempted to make simultaneous use
of the information available in the two languages.
Finally, although the multilingual subjectivity classifier proposed in Banea et al (2010) is not directly
applicable to the disambiguation task we address in this paper, their findings are similar to ours. In that
paper, the authors showed how a natural language task can benefit from the use of features drawn from
multiple languages, thus supporting the hypothesis that multilingual features can be effectively used to
improve the accuracy of a monolingual classifier.
3 Motivation
Our work seeks to explore the expansion of a monolingual feature set with features drawn from multiple
languages in order to generate a more robust and more effective vector-space representation that can be
used for the task of word sense disambiguation. While traditional monolingual representations allow a
supervised learning systems to achieve a certain accuracy, we try to surpass this limitation by infusing
additional information in the model, mainly in the form of features extracted from the machine translated
view of the monolingual data. A statistical machine translation (MT) engine does not only provide
a dictionary-based translation of the words surrounding a given ambiguous word, but it also encodes
the translation knowledge derived from very large parallel corpora, thus accounting for the contextual
dependencies between the words.
In order to better explain why a multilingual vector space provides for a better representation for
the word sense disambiguation task, consider the following examples centered around the ambiguous
verb build.1 For illustration purposes, we only show examples for four out of the ten possible meanings
in WordNet (Fellbaum, 1998), and we only show the translations in one language (French). All the
translations are performed using the Google Translate engine.
En 1: Telegraph Co. said it will spend $20 million to build a factory in Guadalajara, Mex-
ico, to make telephone answering machines. (sense id 1)
Fr 1: Telegraph Co. a annonce? qu?il de?pensera 20 millions de dollars pour construire une
usine a? Guadalajara, au Mexique, pour faire re?pondeurs te?le?phoniques.
En 2: A member in the House leadership and skilled legislator, Mr. Fazio nonetheless found
himself burdened not only by California?s needs but by Hurricane Hugo amendments he ac-
cepted in a vain effort to build support in the panel. (sense id 3)
Fr 2: Un membre de la direction de la Chambre et le le?gislateur compe?tent, M. Fazio a
ne?anmoins conclu lui-me?me souffre, non seulement par les besoins de la Californie, mais
par l?ouragan Hugo amendements qu?il a accepte? dans un vain effort pour renforcer le sou-
tien dans le panneau.
En 3: Burmah Oil PLC, a British independent oil and specialty-chemicals marketing con-
cern, said SHV Holdings N.V. has built up a 7.5% stake in the company. (sense id 3)
1The sentences provided and their annotations are extracted from the SEMEVAL corpus.
26
Fr 3: Burmah Oil PLC, une huile inde?pendant britannique et le souci de commercialisation
des produits chimiques de spe?cialite?, a de?clare? SHV Holdings NV a acquis une participation
de 7,5% dans la socie?te?.
En 4: Plaintiffs? lawyers say that buildings become ?sick? when inadequate fresh air and
poor ventilation systems lead pollutants to build up inside. (sense id 2)
Fr 4: Avocats des plaignants disent que les ba?timents tombent malades quand l?insuffisance
d?air frais et des syste`mes de ventilation insuffisante de plomb polluants de s?accumuler a`
l?inte?rieur.
As illustrated by these examples, the multilingual representation helps in two important ways. First,
it attempts to disambiguate the target ambiguous word by assigning it a different translation depending
on the context where it occurs. For instance, the first example includes a usage for the verb build in its
most frequent sense, namely that of construct (WordNet: make by combining materials and parts), and
this sense is correctly translated into French as construire. In the second sentence, build is used as part
of the verbal expression build support where it means to form or accumulate steadily (WordNet), and
it is accurately translated in both French sentences as renforcer. For sentences three and four, build is
followed by the adverb up, yet in the first case, its sense id in WordNet is 3, build or establish something
abstract, while in the second one is 2, form or accumulate steadily. Being able to infer from the co-
occurrence of additional words appearing the context, the MT engine differentiates the two usages in
French, translating the first occurrence as acquis and the second one as accumuler.
Second, the multilingual representation also significantly enriches the feature space, by adding fea-
tures drawn from multiple languages. For instance, the feature vector for the first example will not only
include English features such as factory and make, but it will also include additional French features
such as usine and faire. Similarly, the second example will have a feature vector including words such
as buildings and systems, and also ba?timents and syste`mes. While this multilingual representation can
sometime result in redundancy when there is a one-to-one translation between languages, in most cases
however the translations will enrich the feature space, by either indicating that two features in English
share the same meaning (e.g., the words manufactory and factory will both be translated as usine in
French), or by disambiguating ambiguous English features using different translations (e.g., the context
word plant will be translated in French as usine or plante, depending on its meaning).
Appending therefore multilingual features to the monolingual vector generates a more orthogonal
vector space. If, previously, the different senses of buildwere completely dependent on their surrounding
context in the source language, now they are additionally dependent on the disambiguated translation of
build given its context, as well as the context itself and the translation of the context.
4 Multilingual Vector Space Representations for WSD
4.1 Datasets
We test our model on two publicly available word sense disambiguation datasets. Each dataset includes
a number of ambiguous words. For each word, a number of sample contexts were extracted and then
manually labeled with their correct sense. Therefore, both datasets follow a Zipfian distribution of senses
in context, given their natural usage. Note also that senses do not cross part-of-speech boundaries.
The TWA2 (two-way ambiguities) dataset contains sense tagged examples for six words that have
two-way ambiguities (bass, crane, motion, palm, plant, tank). These are words that have been previously
used in word sense disambiguation experiments reported in (Yarowsky, 1995; Mihalcea, 2003). Each
word has approximately 100 to 200 examples extracted from the British National Corpus. Since the
words included in this dataset have only two homonym senses, the classification task is easier.
2http://www.cse.unt.edu/?rada/downloads.html\#twa
27
Expanded multilingual features 
De 
Es 
Fr En 
English features 
En: suppose you let me 
explain actually 
Es: supongamos que 
vamos explicar la 
verdad 
Fr: supposons que vous 
laissez moi vous 
expliquer en fait 
De: angenommen sie 
lassen mir eigentlich 
erkl?ren 
Figure 1: Construction of a multilingual vector (combinations of target languages C(3, k), where k = 0..3
The second dataset is the SEMEVAL corpus 2007 (Pradhan et al, 2007),3 consisting of a sample of 35
nouns and 65 verbs with usage examples extracted from the Penn Treebank as well as the Brown corpus,
and annotated with OntoNotes sense tags (Hovy et al, 2006). These senses are more coarse grained
when compared to the traditional sense repository encoded in the WordNet lexical database. While
OntoNotes attains over 90% inter-annotator agreement, rendering it particularly useful for supervised
learning approaches, WordNet is too fine grained even for human judges to agree (Hovy et al, 2006).
The number of examples available per word and per sense varies greatly; some words have as few as
50 examples, while some others can have as many as 2,000 examples. Some of these contexts are
considerably longer than those appearing in TWA, containing around 200 words. For the experiments
reported in this paper, given the limitations imposed by the number of contexts that can be translated by
the online translation engine,4 we randomly selected a subset of 31 nouns and verbs from this dataset.
4.2 Model
In order to generate a multilingual representation for the TWA and SEMEVAL datasets, we rely on the
method proposed in Banea et al (2010) and use Google Translate to transfer the data from English into
several other languages and produce multilingual representations. We experiment with three languages,
namely French (Fr), German (De) and Spanish (Es). Our choice is motivated by the fact that when
Google made public their statistical machine translation system in 2007, these were the only languages
covered by their service, and we therefore assume that the underlying statistical translation models are
also the most robust. Upon translation, the data is aligned at instance level, so that the original English
context is augmented with three mirroring contexts in French, German, and Spanish, respectively.
We extract the word unigrams from each of these contexts, and then generate vectors that consist of
the original English unigrams followed by the multilingual portion resulted from all possible combina-
tions of the three languages taken 0 through 3 at a time, or more formally C(3, k), where k = 0..3 (see
Figure 1). For instance, a vector resulting from C(3, 0) is the traditional monolingual vector, whereas a
vector built from the combination C(3, 3) contains features extracted from all languages.
3http://nlp.cs.swarthmore.edu/semeval/tasks/task17/description.shtml
4We use Google Translate (http://translate.google.com/), which has a limitation of 1,000 translations per day.
28
0.00
0.50
1.00
1.50
2.00
2.50
3.00
3.50
4.00
Fe
at
ur
e 
W
ei
gh
t 
         ND ?2 = 5 A=1 
         ND ?2 = 5 A=20 
Figure 2: Example of sentence whose words are weighted based on a normal distribution with variance of 5, and
an amplification factor of 20
4.2.1 Feature Weighting
For weighting, we use a parametrized weighting based on a normal distribution scheme, to better leverage
the multilingual features. Let us consider the following sentence:
We made the non-slip surfaces by stippling the tops with a <head> bass </head> broom a
fairly new one works best.
Every instance in our datasets contains an XML-marking before and after the word to be disam-
biguated (also known as a headword), in order to identify it from the context. For instance, in the
example above, the headword is bass. The position of this headword in the context can be considered
the mean of a normal distribution. When considering a ?2 = 5, five words to the left and right of the
mean are activated with a value above 10?2 (see the dotted line in Figure 2). However, all the features
are actually activated by some amount, allowing this weighting model to capture a continuous weight
distribution across the entire context. In order to attain a higher level of discrepancy between the weight
of consecutive words, we amplify the normal distribution curve by an empirically determined factor of
20, effectively mapping the values to an interval ranging from 0 to 4. We apply this amplified activation
to every occurrence of a headword in a context. If two activation curves overlap, meaning that a given
word has two possible weights, the final weight is set to the highest (generated by the closest headword in
context). Similar weighting is also performed on the translated contexts, allowing for the highest weight
to be attained by the headword translated into the target language, and a decrementally lower weight for
its surrounding context.
This method therefore allows the vector-space model to capture information pertaining to both the
headword and its translations in the other languages, as well as a language dependent gradient of the
neighboring context usage. While a traditional bigram or trigram model only captures an exact expres-
sion, a normal distribution based model is able to account for wild cards, and transforms the traditionally
sparse feature space into one that is richer and more compact at the same time.
4.3 Adjustments
We encountered several technical difficulties in translating the XML-formatted datasets, which we will
expand on in this section.
29
4.3.1 XML-formatting and alignment
First of all, as every instance in our datasets contains an XML-marked headword (as shown in Section
4.2.1), the tags interfere with the MT system, and we had to remove them from the context before
proceeding with the translation. The difficulty came from the fact that the translated context provides
no way of identifying the translation of the original headword. In order to acquire candidate translations
of the English headword we query the Google Multilingual Dictionary5 (setting the dictionary direction
from English to the target language) and consider only the candidates listed under the correct part-of-
speech. We then scan the translated context for any of the occurrences mined from the dictionary, and
locate the candidates.
In some of the cases we also identify candidate headwords in the translated context that do not mirror
the occurrence of a headword in the English context (i.e., the number of candidates is higher than the
number of headwords in English). We solve this problem by relying on the assumption that there is an
ideal position for a headword candidate, and this ideal position should reflect the relative position of the
original headword with regard to its context. This alignment procedure is supported by the fact that the
languages we use follow a somewhat similar sentence structure; given parallel paragraphs of text, these
cross-lingual ?context anchors? will lie in close vicinity. We therefore create two lists: the first list is
the reference English list, and contains the indexes of the English headwords (normalized to 100); the
second list contains the normalized indexes of the candidate headwords in the target language context.
For each candidate headword in the target language, we calculate the shortest distance to a headword
appearing in the reference English list. Once the overall shortest distance is found, both the candidate
headword?s index in the target language and its corresponding English headword?s index are removed
from their respective list. The process continues until the reference English list is empty.
4.3.2 Inflections
There are also cases when we are not able to identify a headword due to the fact that we are trying to find
the lemma (extracted from the multilingual dictionary) in a fully inflected context, where most probably
the candidate translation is inflected as well. As French, German and Spanish are all highly inflected
languages, we are faced with two options: to either lemmatize the contexts in each of the languages,
which requires a lemmatizer tuned for each language individually, or to stem them. We chose the latter
option, and used the Lingua::Stem::Snowball,6 which is a publicly available implementation of the Porter
stemmer in multiple languages.
To summarize, all the translations are stemmed to obtain maximum coverage, and alignment is performed
when the number of candidate entries found in a translated context does not match the frequency of
candidate headwords in the reference English context. Also, all the contexts are processed to remove any
special symbols and numbers.
5 Results and Discussion
5.1 Experimental Setup
In order to determine the effect of the multilingual expanded feature space on word sense disambiguation,
we conduct several experiments using the TWA and SEMEVAL datasets. The results are shown in Tables
1 and 2.
Our proposed model relies on a multilingual vector space, where each individual feature is weighted
using a scheme based on a modified normal distribution (Section 4.2.1). As eight possible combinations
are available when selecting one main language (English) and combinations of three additional languages
5http://www.google.com/dictionary
6http://search.cpan.org/dist/Lingua-Stem-Snowball/lib/Lingua/Stem/Snowball.pm
30
taken 0 through 3 at a time (Spanish, French and German), we train eight Na??ve Bayes learners7 on the
resulted datasets: one monolingual (En), three bilingual (En-De, En-Fr, En-Es), three tri-lingual (En-
De-Es, En-De-Fr, En-Fr-Es), and one quadri-lingual (En-Fr-De-Es). Each dataset is evaluated using ten
fold cross-validation; the resulting micro-accuracy measures are averaged across each of the language
groupings and they appear in Tables 1 and 2 in ND-L1 (column 4), ND-L2 (column 5), ND-L3 (column
6), and ND-L4 (column 7), respectively. Our hypothesis is that as more languages are added to the mix
(and therefore the number of features increases), the learner will be able to distinguish better between
the various senses.
5.2 Baselines
Our baseline consists of the predictions made by a majority class learner, which labels all examples with
the predominant sense encountered in the training data.8 Note that the most frequent sense baseline
is often times difficult to surpass because many of the words exhibit a disproportionate usage of their
main sense (i.e., higher than 90%), such as the noun bass or the verb approve. Despite the fact that the
majority vote learner provides us with a supervised baseline, it does not take into consideration actual
features pertaining to the instances. We therefore introduce a second, more informed baseline that relies
on binary-weighted features extracted from the English view of the datasets and we train a multinomial
Na??ve Bayes learner on this data. For every word included in our datasets, the binary-weighted Na??ve
Bayes learner achieves the same or higher accuracy as the most frequent sense baseline.
5.3 Experiments
Comparing the accuracies obtained when training on the monolingual data, the binary weighted baseline
surpasses the normal distribution-based weighting model in only three out of six cases on the TWA
dataset (difference ranging from .5% to 4.81%), and in 6 out of 31 cases on the SEMEVAL dataset
(difference ranging from .53% to 7.57%, where for 5 of the words, the difference is lower than 3%). The
normal distribution-based model is thus able to activate regions around a particular headword, and not
an entire context, ensuring more accurate sense boundaries, and allowing this behavior to be expressed
in multilingual vector spaces as well (as seen in columns 7-9 in Tables 1 and 2).
When comparing the normal distribution-based model using one language versus more languages,
5 out of 6 words in TWA score highest when the expanded feature space includes all languages, and
one scores highest for combinations of 3 languages (only .17% higher than the accuracy obtained for
all languages). We notice the same behavior in the SEMEVAL dataset, where 18 of the words exhibit
their highest accuracy when all four languages are taken into consideration, and 3 achieve the highest
score for three-language groupings (at most .37% higher than the accuracy obtained for the four language
grouping). While the model displays a steady improvement as more languages are added to the mix, four
of the SEMEVAL words are unable to benefit from this expansion, namely the verbs buy (-0.61%), care
(-1.45%), feel (-0.29%) and propose (-2.94%). Even so, we are able to achieve error rate reductions
ranging from 6.52% to 63.41% for TWA, and from 3.33% to 34.62% for SEMEVAL.
To summarize the performance of the model based on the expanded feature set and the proposed
baselines, we aggregate all the accuracies from Tables 1 and 2, and present the results obtained in Table 3.
The monolingual modified normal-distribution model is able to exceed the most common sense baseline
and the binary-weighted Na??ve Bayes learner for both datasets, proving its superiority as compared to
a purely binary-weighted model. Furthermore, we notice a consistent increase in accuracy as more
languages are added to the vector space, displaying an average increment of 1.7% at every step for
TWA, and 0.67% for SEMEVAL. The highest accuracy is achieved when all languages are taken into
consideration: 86.02% for TWA and 83.36% for SEMEVAL, corresponding to an error reduction of
25.96% and 10.58%, respectively.
7We use the multinomial Na??ve Bayes implementation provided by the Weka machine learning software (Hall et al, 2009).
8Our baseline it is not the same as the traditional most common sense baseline that uses WordNet?s first sense heuristic,
because our data sets are not annotated with WordNet senses.
31
1 2 3 4 5 6 7 8 9 10
Word # Inst # Senses MCS BIN-L1 ND-L1 ND-L2 ND-L3 ND-L4 Error Red.
bass.n 107 2 90.65 90.65 90.65 91.28 91.90 92.52 20.00
crane.n 95 2 75.79 75.79 76.84 76.14 76.49 78.95 9.09
motion.n 201 2 70.65 81.09 79.60 86.73 89.88 92.54 63.41
palm.n 201 2 71.14 73.13 87.06 88.89 89.72 89.55 19.23
plant.n 187 2 54.55 79.14 74.33 77.90 81.82 83.96 37.50
tank.n 201 2 62.69 77.61 77.11 76.29 76.45 78.61 6.52
Table 1: Accuracies obtained on the TWA dataset; Columns: 1 - words contained in the corpus, 2 - number of
examples for a given word, 3 - number of senses covered by the examples, 4 - micro-accuracy obtained when
using the most common sense (MCS), 5 - micro-accuracy obtained using the multinomial Na??ve Bayes classifier
on binary weighted monolingual features in English, 6 - 9 - average micro-accuracy computed over all possible
combinations of English and 3 languages taken 0 through 3 at a time, resulted from features weighted following
a modified normal distribution with ?2 = 5 and an amplification factor of 20 using a multinomial Na??ve Bayes
learner, where 6 - one language, 7 - 2 languages, 8 - 3 languages, 9 - 4 languages, 10 - error reduction calculated
between ND-L1 (6) and ND-L4 (9)
6 Conclusion
This paper explored the cumulative ability of features originating from multiple languages to improve
on the monolingual word sense disambiguation task. We showed that a multilingual model is suited to
better leverage two aspects of the semantics of text by using a machine translation engine. First, the
various senses of a target word may be translated into other languages by using different words, which
constitute unique, yet highly salient features that effectively expand the target word?s space. Second, the
translated context words themselves embed co-occurrence information that a translation engine gathers
from very large parallel corpora. This information is infused in the model and allows for thematic spaces
to emerge, where features from multiple languages can be grouped together based on their semantics,
leading to a more effective context representation for word sense disambiguation. The average micro-
accuracy results showed a steadily increasing progression as more languages are added to the vector
space. Using two standard word sense disambiguation datasets, we showed that a classifier based on a
multilingual representation can lead to an error reduction ranging from 10.58% (SEMEVAL) to 25.96%
(TWA) as compared to the monolingual classifier.
Acknowledgments
This material is based in part upon work supported by the National Science Foundation CAREER award
#0747340 and IIS award #1018613. Any opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessarily reflect the views of the National
Science Foundation.
References
Banea, C., R. Mihalcea, and J. Wiebe (2010, August). Multilingual subjectivity: Are more languages
better? In Proceedings of the 23rd International Conference on Computational Linguistics (Coling
2010), Beijing, China, pp. 28?36.
Dagan, I. and A. Itai (1994). Word sense disambiguation using a second language monolingual corpus.
Computational Linguistics 20(4), 563?596.
Diab, M. and P. Resnik (2002, July). An unsupervised method for word sense tagging using parallel
corpora. In Proceedings of the 40st Annual Meeting of the Association for Computational Linguistics
(ACL 2002), Philadelphia, PA.
32
1 2 3 4 5 6 7 8 9 10
Word # Inst # Senses MCS BIN-L1 ND-L1 ND-L2 ND-L3 ND-L4 Error Red.
approve.v 53 2 94.34 94.34 94.34 94.34 95.60 96.23 33.33
ask.v 348 6 64.94 68.39 72.41 73.66 74.71 75.00 9.37
bill.n 404 3 65.10 88.12 90.59 91.75 92.41 92.82 23.68
buy.v 164 5 78.66 78.66 78.05 77.64 77.44 77.44 -2.78
capital.n 278 4 92.81 92.81 92.81 92.81 93.17 93.53 10.00
care.v 69 3 78.26 78.26 86.96 86.47 85.99 85.51 -11.11
effect.n 178 3 82.02 82.02 84.83 85.96 86.33 85.96 7.41
exchange.n 363 5 71.90 73.83 78.51 82.37 84.85 85.95 34.62
explain.v 85 2 88.24 88.24 88.24 88.24 88.24 88.24 0.00
feel.v 347 3 82.13 82.13 82.13 82.04 81.94 81.84 -1.61
grant.v 19 2 63.16 73.68 73.68 71.93 71.93 78.95 20.00
hold.v 129 8 34.88 45.74 43.41 43.41 43.41 43.41 0.00
hour.n 187 4 84.49 84.49 83.96 83.78 83.78 84.49 3.33
job.n 188 3 74.47 74.47 80.32 80.67 82.62 84.04 18.92
part.n 481 4 81.91 81.91 82.12 83.30 84.13 85.45 18.60
people.n 754 4 91.11 91.11 91.11 91.29 92.22 93.37 25.37
point.n 469 9 71.64 73.99 77.61 82.09 83.51 84.22 29.52
position.n 268 7 27.61 60.82 61.19 66.17 68.91 68.66 19.23
power.n 251 3 47.81 84.46 76.89 81.94 82.87 83.27 27.59
president.n 879 3 86.23 89.87 87.14 88.28 89.34 90.79 28.32
promise.v 50 2 88.00 88.00 86.00 86.67 87.33 88.00 14.29
propose.v 34 2 85.29 85.29 88.24 87.25 86.27 85.29 -25.00
rate.n 1009 2 84.64 86.92 87.02 88.07 88.64 89.30 17.56
remember.v 121 2 99.17 99.17 99.17 99.17 99.17 99.17 0.00
rush.v 28 2 92.86 92.86 92.86 92.86 92.86 92.86 0.00
say.v 2161 5 97.78 97.78 97.78 97.78 97.78 97.78 0.00
see.v 158 6 44.94 47.47 49.37 51.05 51.69 52.53 6.25
state.n 617 3 83.14 83.95 85.25 85.25 85.47 85.74 3.30
system.n 450 5 55.56 72.44 74.00 73.85 75.26 75.78 6.84
value.n 335 3 89.25 89.25 89.25 89.35 89.45 89.85 5.56
work.v 230 7 64.78 65.65 66.96 68.26 68.99 68.70 5.26
Table 2: Accuracies obtained on the SEMEVAL dataset; Columns: 1 - words contained in the corpus, 2 - number
of examples for a given word, 3 - number of senses covered by the examples, 4 - micro-accuracy obtained when
using the most common sense (MCS), 5 - micro-accuracy obtained using the multinomial Na??ve Bayes classifier
on binary weighted monolingual features in English, 6 - 9 - average micro-accuracy computed over all possible
combinations of English and 3 languages taken 0 through 3 at a time, resulted from features weighted following
a modified normal distribution with ?2 = 5 and an amplification factor of 20 using a multinomial Na??ve Bayes
learner, where 6 - one language, 7 - 2 languages, 8 - 3 languages, 9 - 4 languages, 10 - error reduction calculated
between ND-L1 (6) and ND-L4 (9)
1 2 3 4 5 6 7 8
Dataset MCS BIN-L1 ND-L1 ND-L2 ND-L3 ND-L4 Error Red.
TWA 70.91 79.57 80.93 82.87 84.38 86.02 25.96
SEMEVAL 75.71 80.52 81.36 82.18 82.78 83.36 10.58
Table 3: Aggregate accuracies obtained on the TWA and SEMEVAL datasets; Columns: 1 - dataset, 2 - average
micro-accuracy obtained when using the most common sense (MCS), 3 - average micro-accuracy obtained using
the multinomial Na??ve Bayes classifier on binary weighted monolingual features in English, 4 - 7 - average micro-
accuracy computed over all possible combinations of English and 3 languages taken 0 through 3 at a time, resulted
from features weighted following a modified normal distribution with ?2 = 5 and an amplification factor of 20
using a multinomial Na??ve Bayes learner, where 4 - one language, 5 - 2 languages, 6 - 3 languages, 7 - 4 languages,
8 - error reduction calculated between ND-L1 (4) and ND-L4 (7)
33
Fellbaum, C. (1998). WordNet, An Electronic Lexical Database. The MIT Press.
Hall, M., E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten (2009). The weka data
mining software: An update. SIGKDD Explorations 11(1).
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). Ontonotes: the 90In Proceed-
ings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers
on XX, NAACL ?06, Morristown, NJ, USA, pp. 57?60. Association for Computational Linguistics.
Lesk, M. (1986, June). Automatic sense disambiguation using machine readable dictionaries: How to
tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference 1986, Toronto.
Li, C. and H. Li (2002). Word translation disambiguation using bilingual bootstrapping. In Proceedings
of 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Pennsylvania.
Mihalcea, R. (2003, September). The role of non-ambiguous words in natural language disambiguation.
In Proceedings of the conference on Recent Advances in Natural Language Processing RANLP-2003,
Borovetz, Bulgaria.
Mihalcea, R., R. Sinha, and D. McCarthy (2010). Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the ACL Workshop on Semantic Evaluations, Uppsala, Sweden.
Mihalcea, R., P. Tarau, and E. Figa (2004). PageRank on semantic networks, with application to word
sense disambiguation. In Proceedings of the 20st International Conference on Computational Lin-
guistics (COLING 2004), Geneva, Switzerland.
Ng, H. and H. Lee (1996). Integrating multiple knowledge sources to disambiguate word sense: An
examplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 1996), Santa Cruz.
Ng, H., B. Wang, and Y. Chan (2003, July). Exploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of the 41st Annual Meeting of the Association for Computational
Linguistics (ACL 2003), Sapporo, Japan.
Pradhan, S., E. Loper, D. Dligach, and M. Palmer (2007, June). Semeval-2007 task-17: English lex-
ical sample, srl and all words. In Proceedings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech Republic.
Resnik, P. and D. Yarowsky (1999). Distinguishing systems and distinguishing senses: new evaluation
methods for word sense disambiguation. Natural Language Engineering 5(2), 113?134.
Yarowsky, D. (1995, June). Unsupervised word sense disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL 1995),
Cambridge, MA.
34
