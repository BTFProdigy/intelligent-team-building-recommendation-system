Semi-Automatic Practical Ontology Construction by Using a 
Thesaurus, Computational Dictionaries, and Large Corpora 
Sin-Jae Kang and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering, Pohang University of Science and Technology
San 31 Hyoja-Dong, Nam-Gu, Pohang 790-784 
Republic of KOREA 
sjkang@postech.ac.kr, jhlee@postech.ac.kr 
 
Abstract 
This paper presents the semi-automatic 
construction method of a practical 
ontology by using various resources. In 
order to acquire a reasonably practical 
ontology in a limited time and with less 
manpower, we extend the Kadokawa 
thesaurus by inserting additional 
semantic relations into its hierarchy, 
which are classified as case relations 
and other semantic relations. The 
former can be obtained by converting 
valency information and case frames 
from previously-built computational 
dictionaries used in machine translation. 
The latter can be acquired from concept 
co-occurrence information, which is 
extracted automatically from large 
corpora. The ontology stores rich 
semantic constraints among 1,110 
concepts, and enables a natural 
language processing system to resolve 
semantic ambiguities by making 
inferences with the concept network of 
the ontology. In our practical machine 
translation system, our ontology-based 
word sense disambiguation method 
achieved an 8.7% improvement over 
methods which do not use an ontology 
for Korean translation. 
1 Introduction 
An ontology is a knowledge base with 
information about concepts existing in the world 
or domain, their properties, and how they relate 
to each other. The principal reasons to use an 
ontology in machine translation (MT) are to 
enable source language analyzers and target 
language generators to share knowledge, to store 
semantic constraints, and to resolve semantic 
ambiguities by making inferences using the 
concept network of the ontology (Mahesh, 1996; 
Nirenburg et al, 1992). An ontology is different 
from a thesaurus in that it contains only 
language independent information and many 
other semantic relations, as well as taxonomic 
relations. 
In general, to build a high-quality semantic 
knowledge base, manual processing is 
indispensable. Previous attempts were mostly 
performed manually, or were developed without 
considering the context of a practical situation 
(Mahesh, 1996; Lenat et al, 1990). Therefore, it 
is difficult to construct a practical ontology with 
limited time and manpower resources. To solve 
this problem, we propose a semi-automatic 
ontology construction method, which takes full 
advantage of already existing knowledge 
resources and practical usages in large corpora. 
First, we define our ontology representation 
language (ORL) by modifying the most suitable 
among previously developed ORLs, and then 
design a language-independent and practical 
(LIP) ontology structure based on the defined 
ORL. Afterwards, we construct a practical 
ontology by the semi-automatic construction 
method given below. 
We extend the existing Kadokawa thesaurus 
(Ohno & Hamanishi, 1981) by inserting 
additional semantic relations into the hierarchy 
of the thesaurus. Uramoto (1996) and Tokunaga 
(1997) propose thesaurus extension methods for 
positioning unknown words in an existing 
thesaurus. Our approach differs in that the 
objects inserted are not words but semantic 
relations. 
Additional semantic relations can be 
classified as case relations and other semantic 
relations. The former can be obtained by 
converting the established valency information 
in bilingual dictionaries of COBALT-J/K 
(Collocation-Based Language Translator from 
Japanese to Korean) and COBALT-K/J 
(Collocation-Based Language Translator from 
Korean to Japanese) (Moon & Lee, 2000)  MT 
systems, as well as from the case frame in the 
Sejong electronic dictionary1. The latter can be 
acquired from concept co-occurrence 
information, which is extracted automatically 
from a corpus (Li et al, 2000). 
The remainder of this paper is organized as 
follows. We describe the principles of ontology 
design and an ORL used to represent our LIP 
ontology in the next section. In Section 3, we 
describe the semi-automatic ontology 
construction methodology in detail. An 
ontology-based word sense disambiguation 
(WSD) algorithm is given in Section 4. 
Experimental results are presented and analyzed 
in Section 5. Finally, we make a conclusion and 
indicate the direction of our future work in 
Section 6. 
2 Ontology Design 
2.1    Basic Principles 
Although no formal principles exist to determine 
the structure or content of our ontology, we can 
suggest some principles underlying our 
methodology. Firstly, an ontology for natural 
language processing (NLP) must provide 
concepts for representing word meanings in the 
lexicon and store selectional constraints of 
concepts, which enable inferences using the 
network of an ontology (Onyshkevych, 1997). 
These inferences can assist in metaphor and 
metonymy processing, as well as word sense 
disambiguation. For these reasons, an ontology 
becomes an essential knowledge source for high 
quality NLP, although it is difficult and time-
consuming to construct. Secondly, an ontology 
can be effortlessly shared by any application and 
in any domain (Gruber, 1993; Karp et al, 1999; 
Kent, 1999). More than two different ontologies 
in a certain domain can produce a semantic 
mismatch problem between concepts. Further, if 
                                                          
1 The Sejong electronic dictionary has been developed by 
several Korean linguistic researchers, funded by Ministry 
of Culture and Tourism, Republic of Korea. 
(http://www.sejong.or.kr) 
you wish to apply an existing ontology to a new 
application, it will often be necessary to convert 
the structure of the ontology to a new one. 
Thirdly, an ontology must support language 
independent features, because constructing 
ontologies for each language is inefficient. 
Fourthly, an ontology must have capabilities for 
users to easily understand, search, and browse. 
Therefore, we define a suitable ORL to support 
these principles. 
2.2    Ontology Representation Language 
Many knowledge representation languages are 
built specifically to share knowledge among 
different knowledge representation systems. 
Five types of ORLs were reviewed, such as 
FRAMEKIT (Nirenburg et al, 1992), 
Ontolingua (Gruber, 1993), CycL (Lenat et al, 
1990), XOL (Karp et al, 1999), and Ontology 
Markup Language (OML) (Kent, 1999). 
According to their semantics, FRAMEKIT and 
XOL adopt frame representation, CycL and 
Ontolingua use an extended first order predicate 
calculus, and the OML is based on conceptual 
graphs (CGs). Excepting FRAMEKIT and CycL, 
the other ORLs have not yet been applied to 
build any large-scale ontology. 
Among this variety of ORLs, we chose the 
simplified OML as the ORL of our LIP ontology, 
which is based on Extensible Markup Language 
(XML) and CGs. Since XML has a well-
established syntax, it is reasonably simple to 
parse, and XML will be widely used, because it 
has many software tools for parsing and 
manipulating, and a human readable 
representation. We intend to leave room for 
improvement by adopting the semantics of CGs, 
because the present design of our LIP ontology 
is for the specific purpose of disambiguating 
word senses. In future, however, we must extend 
its structure and content to build an interlingual 
meaning representation during semantic analysis 
in machine translation. Sowa's CGs (1984) is a 
widely-used knowledge representation language, 
consisting of logic structures with a graph 
notation and several features integrated from 
semantic net and frame representation. Globally, 
many research teams are working on the 
extension and application of CGs in many 
domains. 
 
3 Ontology Construction 
Many ontologies are developed for purely 
theoretical purposes, or are constructed as 
language-dependent computational resources, 
such as WordNet and EDR. However, they are 
seldom constructed as a language-independent 
computational resource. 
To construct a language-independent and 
practical ontology, we developed two strategies. 
First, we introduced the same number and grain 
size of concepts of the Kadokawa thesaurus and 
its taxonomic hierarchy into the LIP ontology. 
The thesaurus has 1,110 Kadokawa semantic 
categories and a 4-level hierarchy as a 
taxonomic relation (see Figure 1). This approach 
is a moderate shortcut to construct a practical 
ontology which easily enables us to utilize its 
results, since some resources are readily 
available, such as bilingual dictionaries of 
COBALT-J/K and COBALT-K/J. In these 
bilingual dictionaries, nominal and verbal words 
are already annotated with concept codes from 
the Kadokawa thesaurus. By using the same 
sense inventories of these MT systems, we can 
easily apply and evaluate our LIP ontology 
without additional lexicographic works. In 
addition, the Kadokawa thesaurus has proven to 
be useful for providing a fundamental 
foundation to build lexical disambiguation 
knowledge in COBALT-J/K and COBALT-K/J 
MT systems (Li et al, 2000). 
The second strategy to construct a practical 
ontology is to extend the hierarchy of the 
Kadokawa thesaurus by inserting additional 
semantic relations into its hierarchy. The 
additional semantic relations can be classified as 
case relations and other semantic relations. Thus 
far, case relations have been used occasionally 
to disambiguate lexical ambiguities in the form 
of valency information and case frame, but other 
semantic relations have not, because of the 
problem of discriminating them from each other, 
making them difficult to recognize. We define a 
total of 30 semantic relation types for WSD by 
referring mainly to the Sejong electronic 
dictionary and the Mikrokosmos ontology 
(Mahesh, 1996), as shown in Table 1. These 
semantic relation types cannot express all 
possible semantic relations existing among 
concepts, but experimental results demonstrated 
their usefulness for WSD. 
Two approaches are used to obtain these 
additional semantic relations, which will be 
inserted into the LIP ontology. The first imports 
relevant semantic information from existing 
computational dictionaries. The second applies 
the semi-automatic corpus analysis method (Li 
et al, 2000). Both approaches are explained in 
Section 3.1 and 3.2, respectively. 
Figure 2 displays the overall constructing 
flow of the LIP ontology. First, we build an 
initial LIP ontology by importing the existing 
Kadokawa thesaurus. Each concept inserted into 
the initial ontology has a Kadokawa code, a 
Korean name, an English name, a timestamp, 
and a concept definition. Although concepts can 
be uniquely identified by the Kadokawa concept 
codes, their Korean and English names are 
Root concept
nature    character  change action     feeling     human  disposition  society institute things 
0           1   2   3   4      5     6     7       8       9
astro- calen- wea- geog- sight plant  ani- physi- subs- pheno-
nomy    dar    ther  raphy                   mal  ology tance  mena
00      01       02     03     04       05     06     07     08 09
goods drugs food clothes buil- furni- statio- mark tools mach-
ding   ture    nary                       ine
90      91       92     93     94       95     96     97     98 99
orga- ani- fish insect organ foot&  sin- intes- egg   sex
nism  mal                               tail   ews   tine    
060    061  062   063   064    065   066   067   068   069
supp- writing- count- binder  toy  doll  recreation- sports- music- bell
lies      tool      book                             thing    tool  instrument
960     961      962     963   964  965     966           967 968       969
?????
?????
 
Figure 1. Concept hierarchy of the Kadokawa 
thesaurus 
 
Table 1. Sematic relation types in the LIP 
ontology 
 
Types Relation Lists 
Taxonomic 
relation 
is-a 
Case relation agent, theme, experiencer, 
accompanier, instrument,  
location, source, destination, 
reason, appraisee, criterion, 
degree, recipient 
Other 
Semantic 
relation 
has-member, has-element, 
contains, material-of, headed-
by, operated-by,  
controls, owner-of, represents, 
symbol-of, name-of,  
producer-of, composer-of, 
inventor-of, make, measured-in
 
inserted for the readability and convenience of 
the ontology developer. 
3.1    Dictionary Resources Utilization 
Case relations between concepts can be 
primarily derived from semantic information in 
the Sejong electronic dictionary 2  and the 
bilingual dictionaries of MT systems, which are 
COBALT-J/K and COBALT-K/J.  
We obtained 7,526 case frames from verb and 
adjective sub-dictionaries, which contain 3,848 
entries. Automatically converting lexical words 
in the case frame into the Kadokawa concept 
codes by using COBALT-K/J (see Figure 33), 
we extracted a total of 6,224 case relation 
instances. 
The bilingual dictionaries, which contain 
20,580 verb and adjective entries, have 16,567 
instances of valency information. Semi-
automatically converting syntactic relations into 
semantic relations by using specific rules and 
human intuition (see Figure 4), we generated 
15,956 case relation instances. The specific rules, 
as shown in Figure 5, are inferred from training 
samples, which are explained in Section 4.1. 
These obtained instances may overlap each 
other, but all instances are inserted only once 
into the initial LIP ontology. 
                                                          
2 The Sejong electronic dictionary has sub-dictionaries, 
such as noun, verb, pronoun, adverb, and others. 
3 The Yale Romanization is used to represent Korean 
lexical words. 
3.2    Corpus Analysis 
For the automatic construction of a sense-tagged 
corpus, we used the COBALT-J/K, which is a 
high-quality practical MT system developed by 
POSTECH in 1996. The entire system has been 
used successfully at POSCO (Pohang Iron and 
Steel Company), Korea, to translate patent 
materials on iron and steel subjects. We 
performed a slight modification on COBALT-
J/K so that it can produce Korean translations 
from Japanese texts with all nominal and verbal 
words tagged with the specific concept codes of 
the Kadokawa thesaurus. As a result, a Korean 
sense-tagged corpus, which has two hundred and 
fifty thousand sentences, can be obtained from 
Japanese texts. Unlike English, the Korean 
language has almost no syntactic constraints on 
word order as long as a verb appears in the final 
position. So we defined 12 local syntactic 
patterns (LSPs) using syntactically-related 
words in a sentence. Frequently co-occurring 
words in a sentence have no syntactic relations 
to homographs but may control their meaning. 
Such words are retrieved as unordered co-
occurring words (UCWs). Case relations are 
obtained from LSPs, and other semantic 
Case frames
Cilmwunha-ta (question) agent Salam(person) 
Cungkasikh-ta (increase) theme Kakyek (price) 
Kyeyhoykha-ta (plan) theme Pepan (bill) 
Seywu-ta (construct) theme Saep (business)
346 agent 5
743 theme 171
344 theme 419
394 theme 369
Case relations
 
Figure 3. Example of conversion from case 
frames in the Sejong dictionary 
381 (exercise) ul/lul (object) 449 (right)
071 (live) wa/kwa (adverb) 06|05|5 (living thing)
217 (soar) lo/ulo (adverb) 002 (sky)
712 (join) i/ka (subject) 5 (person)
381 theme 449
071 accompanier 06|05|5
217 destination 002
712 agent 5
Valency information
Case relations
 
Figure 4. Example of conversion from 
valency information in the bilingual 
dictionaries 
Step 1: From Kadokawa
thesaurus
Step 2: From existing
computational dictionaries
LIP Ontology
Kadokawa
Thesaurus
?
? ?
Import 
concepts
& taxonomic
relations
Sejong 
Electronic Dic.
(Case frame)
K-J & J-K
Bilingual Dic.
(Valency Info.)
Import 
case
relations
Semi-Automatic
Relations or Words
Mapping
Step 3: From a corpus
Japanese Raw Corpus
COBALT J/K
Japanese-to-Korean Translation
Sense Tagged
Korean Corpus
Partial Parsing
& Statistical Processing
Generalized Concept
Co-occurrence Information
Import case 
& other semantic
relations
Semi-Automatic 
Relations Mapping
 
Figure 2. Ovreall constructing flow of the LIP 
ontology 
relations are acquired from UCWs. Concept co-
occurrence information (CCI), which is 
composed of LSPs and UCWs, can be extracted 
by partial parsing and scanning. To select the 
most probable concept types, Shannon's entropy 
model is adopted to define the noise of a concept 
type to discriminate the homograph. Although it 
processes for concept type discrimination, many 
co-occurring concept types, which must be 
further selected, remain in each LSP and UCW. 
To solve this problem, some statistical 
processing was automatically applied (Li et al, 
2000). Finally, manual processing was 
performed to generate the ontological relation 
instances from the generalized CCI, similar to 
the previous valency information. The results 
obtained include approximately about 3,701 
case relations and 1,650 other semantic relations 
from 9,245 CCI, along with their frequencies. 
The obtained instances are inserted into the 
initial LIP ontology. Table 2 shows the number 
of relation instances imported into the LIP 
ontology from the Kadokawa thesaurus, 
computational dictionaries, and a corpus. 
4 Ontology Application 
The LIP ontology is applicable to many NLP 
applications. In this paper, we propose to use the 
ontology to disambiguate word senses. All 
approaches to WSD make use of words in a 
sentence to mutually disambiguate each other. 
The distinctions between various approaches lie 
in the source and type of knowledge made by 
the lexical units in a sentence. 
Our WSD approach is a hybrid method, 
which combines the advantages of corpus-based 
and knowledge-based methods. We use the LIP 
ontology as an external knowledge source and 
secured dictionary information as context 
information. Figure 6 shows our overall WSD 
algorithm. First, we apply the previously-
secured dictionary information to select correct 
senses of some ambiguous words with high 
precision, and then use the LIP ontology to 
disambiguate the remaining ambiguous words. 
The following are detailed descriptions of the 
procedure for applying the LIP ontology to 
WSD work. 
4.1    Measure of Concept Association 
To measure concept association, we use an 
association ratio based on the information 
theoretic concept of mutual information (MI), 
which is a natural measure of the dependence 
071 (life)
072 (upbringing)
073 (disease)
YES
NO
agent / theme
295 (influence)
370 (giving & receiving)
YES
NO
recipient
49 (joy & sorrow)
62 (figure)
YES
NO
experiencer
201 (stable)
202 (vibration)
1 (natural condition)
07 (physiology)
YES
NO
theme
2 (change)
3 (action)
YES
NO
agent
Manual mapping by human intuition
 
Figure 5. Example of subject relation 
mapping rules with governer concept codes
 
Apply secured dictionary information with high precision
Verb?s valency information
Success?
Local syntactic patterns
YES
NO
Success? YES
NO
Unordered co-occurring words patterns
Success?
Infer with the LIP ontology
YES
NO
Success?
Answer
YES
NO
Set the answer to the most frequently appearing sense
 
Figure 6. The proposed WSD algorithm 
Table 2. Imported relation instances 
 
Types Number 
Taxonomic relations 1,100
Case relations 19,459
Other semantic relations 1,650
Total 22,209
between random variables (Church & Hanks, 
1989). Resnik (1995) suggested a measure of 
semantic similarity in an IS-A taxonomy, based 
on the notion of information content. However, 
his method differs from ours in that we consider 
all semantic relations in the ontology, not 
taxonomy relations only. To implement this idea, 
we bind source concepts (SC) and semantic 
relations (SR) into one entity, since SR is mainly 
influenced by SC, not the destination concepts 
(DC). Therefore, if two entities, < SC, SR>, and 
DC have probabilities P(<SC, SR>) and P(DC), 
then their mutual information I(<SC, SR>, DC) 
is defined as: 
 
???
?
???
? +><
><=>< 1
)(),(
),,(log),,( 2 DCPSRSCP
DCSRSCPDCSRSCI  
 
The MI between concepts in the LIP ontology 
must be calculated before using the ontology as 
knowledge for disambiguating word senses. 
Figure 7 shows the construction process for 
training data in the form of <SC (governer), SR, 
DC (dependent), frequency> and the calculation 
of MI between the LIP ontology concepts. We 
performed a slight modification on COBALT-
K/J and COBALT-J/K to enable them to 
produce sense-tagged valency information 
instances with the specific concept codes of the 
Kadokawa thesaurus. After producing the 
instances, we converted syntactic relations into 
semantic relations using the specific rules (see 
Figure 5) and human intuition. As a result, we 
extracted sufficient training data from the 
Korean raw corpus: KIBS (Korean Information 
Base System, '94-'97) is a large-scale corpus of 
70 million words, and the Japanese raw corpus, 
which has eight hundred and ten thousand 
sentences. During this process, more specific 
semantic relation instances are obtained when 
compared with previous instances obtained in 
Section 3. Since such specific instances reflect 
the context of a practical situation, they are also 
imported into the LIP ontology. Table 3 shows 
the final number of semantic relations inserted 
into the LIP ontology. 
 
Table 3. Final relation instances in the LIP 
ontology 
 
Types Number 
Taxonomic relations 1,100
Case relations 112,746
Other semantic relations 2,093
Total 115,939
 
4.2    Locate the Least Weighted Path from 
One Ontology Concept to Other Concept 
If we regard MI as a weight between ontology 
concepts, we can treat the LIP ontology as a 
graph with weighted edges. All edge weights are 
non-negative and weights are converted into 
penalties by the below formula Pe. c indicate a 
constant, maximum MI between concepts of the 
LIP ontology. 
 
),,(),,( DCSRSCIcDCSRSCPe ><?=><  
 
So we use the formula below to locate the 
least weighted path from one concept to the 
other concept. The score function S is defined 
as: 
 
( )
( )
??
??
?
??
??
?
?
????
+
????
><
=
=
??
.,
),(),(min
,
),,(min
,1
),(
}{
j
R
kji
jkkiCCC
j
R
iji
jpip
ji
ji
CCandCCif
CCSCCS
CCandCCif
CRCPe
CCif
CCS
p
jik
p
 
 
Here C and R indicate concepts and semantic 
relations, respectively. By applying this formula, 
we can verify how well selectional constraints 
between concepts are satisfied. In addition, if 
there is no direct semantic relation between 
concepts, this formula provides a relaxation 
procedure, which enables it to approximate their 
semantic relations. This characteristic enables us 
Apply valency information with high precision
Japanese 
Raw Corpus
COBALT J/K
Japanese-to-Korean
Translation
Applied ValencyPatterns
<SC, synRel, DC, frequency>
Semi-Automatic Relation Mapping
Calculating MI 
btw <SC, SR> & DC
Semantic Relation Instances
<SC, SR, DC, frequency>
Korean
Raw Corpus
COBALT K/J
Korean-to-Japanese
Translation
LIP Ontology
Importing semantic
relation instances
 
Figure 7. Construction flow of ontology 
training data 
to obtain hints toward resolving metaphor and 
metonymy expressions. For example, when 
there is no direct semantic relation between 
concepts such as ?school? and ?inform,? the 
inferring process is as follows. The concept 
?school? is a ?facility?, and the ?facility? has 
?social human? as its members. The concept 
?inform? has ?social human? as its agent. Figure 
8 presents an example of the best path between 
these concepts, which is shown with bold lines. 
To locate the best path, the search mechanism of 
our LIP ontology applies heuristics as follows. 
Firstly, a taxonomic relation must be treated as 
exceptional from other semantic relations, 
because they inherently lack frequencies 
between parent and child concepts. So we assign 
a fixed weight to those edges experimentally. 
Secondly, the weight given to an edge is 
sensitive to the context of prior edges in the path. 
Therefore, our mechanism restricts the number 
of times that a particular relation can be 
traversed in one path. Thirdly, this mechanism 
avoids an excessive change in the gradient. 
5 Experimental Evaluation 
For experimental evaluation, eight ambiguous 
Korean nouns were selected, along with a total 
of 404 test sentences in which one of the 
homographs appears. The test sentences were 
randomly selected from the KIBS. Out of 
several senses for each ambiguous word, we 
considered only two senses that are most 
frequently used in the corpus. We performed 
three experiments: The first experiment, BASE, 
is the case where the most frequently used 
senses are always taken as the senses of test 
words. The purpose of this experiment is to 
show the baseline for WSD work. The second, 
PTN, uses only secured dictionary information, 
such as the selectional restriction of verbs, local 
syntactic patterns, and unordered co-occurring 
word patterns in disambiguating word senses. 
This is a general method without an ontology. 
The third, LIP, shows the results of our WSD 
method using the LIP ontology. The 
experimental results are shown in Table 4. In 
these experiments, the LIP method achieved an 
8.7% improvement over the PTN method for 
Korean analysis. The main reason for these 
results is that, in the absence of secured 
dictionary information (see Figure 7) about an 
ambiguous word, the ontology provides a 
generalized case frame (i.e. semantic restriction) 
by the concept code of the word. In addition, 
when there is no direct semantic restriction 
between concepts, our search mechanism 
provides a relaxation procedure (see Figure 8). 
Therefore, the quality and usefulness of the LIP 
ontology were proved indirectly by these results. 
6 Conclusion 
In this paper we have proposed a semi-automatic 
construction method of the LIP ontology and an 
ontology-based WSD algorithm. The LIP 
Table 4. Experimental results of WSD (%) 
 
Homograph Sense BASE PTN LIP 
father & 
child Pwuca 
rich man 
76.9 69.2 86.0
liver Kancang 
soy sauce 
67.3 87.8 91.8
housework 
Kasa words of 
song 
48.1 88.5 96.1
shoe 
Kwutwu word of 
mouth 
79.6 85.7 95.9
eye Nwun 
snow 
82.0 96.0 92.0
courage Yongki 
container 
62.0 74.0 82.0
expenses Kyengpi 
defense 
74.5 78.4 90.2
times Kyeongki 
match 
52.9 80.4 95.6
Average Precision 67.9 82.5 91.2
Root
Concept
nature
0
character
1
change
2
action
3
feeling
4
human
5
disposition
6
society
7
institute
8
things
9
school
722
facility
72
is-a
is-a
is-a
social
human
507
person
50
is-a
is-a has
member
inform
750
report
75
is-a
is-a
agent
?
?
?
?
?
?
 
Figure 8. Example of the best path between 
concepts ?school? and ?inform? in the LIP 
ontology 
 
ontology includes substantial semantic relations 
between concepts, and differs from many of the 
resources in that there is no language-dependent 
knowledge in the resource, which is a network 
of concepts, not words. Semantic relations of the 
LIP ontology are generated by considering two 
different languages, Korean and Japanese. In 
addition, we can easily apply the ontology 
without additional lexicographic works, since 
large-scale bilingual dictionaries have words 
already annotated with concept codes of the LIP 
ontology. Therefore, our LIP ontology is a 
language independent and practical knowledge 
base. You can apply this ontology for other 
languages, if one merely inserts Kadokawa 
concept codes for each entry into the dictionary. 
Our ontology construction method requires 
manual processing, i.e., mapping from syntactic 
relations to semantic relations by specific rules 
and human intuition. However, this is necessary 
for building a high-quality semantic knowledge 
base. Our construction method is quite effective 
in comparison with other methods. 
We plan further research on how to 
effectively divide the grain size of ontology 
concepts to best express the whole world 
knowledge, and how to utilize the LIP ontology 
in a full semantic analysis process. 
Acknowledgements 
The authors would like to thank the Ministry of 
Education of Korea for its financial support 
toward the Electrical and Computer Engineering 
Division at POSTECH through its BK21 
program. 
References 
Church, K. and P. Hanks. 1989. Word association 
norms, mutual information, and lexicography. In 
Proceedings of the 27th Annual Meeting of the 
Association for Computational Linguistics, pages 
76-83, Vancouver, Canada. 
Gruber, Thomas R. 1993. A Translation Approach to 
Portable Ontology Specification. Knowledge 
Acquisition 5(2):199-220. 
Karp, P. D., V. K. Chaudhri, and J. F. Thomere. 1999. 
XOL: An XML-Based Ontology Exchange 
Language. Technical Note 559, AI Center, SRI 
International, July. 
Kent, Robert E. 1999. Conceptual Knowledge 
Markup Language: The Central Core. In the 
Electronic Proceedings of the Twelfth Workshop 
on Knowledge Acquisition, Modeling and 
Management(KAW`99). Banff, Alberta, Canada, 
October. 
Lenat, D. B. et al 1990. Cyc: toward programs with 
common sense. Communications of the ACM, 
33(8):30-49. 
Li, Hui-Feng et al 2000. Lexical Transfer Ambiguity 
Resolution Using Automatically-Extracted 
Concept Co-occurrence Information. International 
Journal of Computer Processing of Oriental 
Languages, World Scientific Pub., 13(1):53-68. 
Mahesh, Kavi. 1996. Ontology Development for 
Machine Translation: Ideology and Methodology. 
Technical Report MCCS 96-292, Computing 
Research Laboratory, New Mexico State 
University, Las Cruces, NM. 
Moon, Kyunghi and Jong-Hyeok Lee. 2000. 
Representation and Recognition Method for 
Multi-Word Translation Units in Korean-to-
Japanese MT System. COLING 2000, pages 544-
550, Germany. 
Nirenburg, Sergei, Jaime Carbonell, Masaru Tomita, 
and Kenneth Goodman. 1992. Machine 
Translation: A Knowledge-Based Approach, 
Morgan Kaufmann Pub., San Mateo, California. 
Ohno, S. and M. Hamanishi. 1981. New Synonyms 
Dictionary, Kadogawa Shoten, Tokyo. (Written in 
Japanese). 
Onyshkevych, Boyan A. 1997. An Ontological-
Semantic Framework for Text Analysis. Ph.D. 
dissertation, Program in Language and 
Information Technologies, School of Computer 
Science, Carnegie Mellon University, CMU-LTI-
97-148. 
Resnik, Philip. 1995. Using Information Content to 
Evaluate Semantic Similarity in a Taxonomy. In 
Proceedings of IJCAI-95, 1995, pages 448-453, 
Montreal, Canada. 
Sowa, John F. 1984. Conceptual Structures: 
Information Processing in Mind and Machine, 
Addison-Wesley Pub., MA. 
Takenobu, Tokunaga et al 1997. Extending a 
thesaurus by classifying words. In Proceedings of 
the ACL-EACL Workshop on Automatic 
Information Extraction and Building of Lexical 
Semantic Resources, pages 16-21, Madrid, Spain. 
Uramoto, Naohiko. 1996. Positioning Unknown 
Word in a Thesaurus by using Information 
Extracted from a Corpus. In Proceedings of 
COLING-96, pages 956-961, Copenhagen, 
Denmark. 
Word Sense Disambiguation in a Korean-to-Japanese 
MT System Using Neural Networks 
You-Jin Chung, Sin-Jae Kang, Kyong-Hi Moon, and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering, Pohang University of Science and Technology (POSTECH) 
and Advanced Information Technology Research Center(AlTrc) 
San 31, Hyoja-dong, Nam-gu, Pohang, R. of KOREA, 790-784 
{prizer,sjkang,khmoon,jhlee}@postech.ac.kr 
 
Abstract  
This paper presents a method to resolve 
word sense ambiguity in a 
Korean-to-Japanese machine translation 
system using neural networks. The 
execution of our neural network model is 
based on the concept codes of a thesaurus. 
Most previous word sense disambiguation 
approaches based on neural networks have 
limitations due to their huge feature set size. 
By contrast, we reduce the number of 
features of the network to a practical size by 
using concept codes as features rather than 
the lexical words themselves. 
Introduction 
Korean-to-Japanese machine translation (MT) 
employs a direct MT strategy, where a Korean 
homograph may be translated into a different 
Japanese equivalent depending on which sense 
is used in a given context. Thus, word sense 
disambiguation (WSD) is essential to the 
selection of an appropriate Japanese target word. 
Much research on word sense disambiguation 
has revealed that several different types of 
information can contribute to the resolution of 
lexical ambiguity. These include surrounding 
words (an unordered set of words surrounding a 
target word), local collocations (a short sequence 
of words near a target word, taking word order 
into account), syntactic relations (selectional 
restrictions), parts of speech, morphological 
forms, etc (McRoy, 1992, Ng and Zelle, 1997). 
Some researchers use neural networks in 
their word sense disambiguation systems 
Because of its strong capability in classification 
(Waltz et al, 1985, Gallant, 1991, Leacock et al, 
1993, and Mooney, 1996). Since, however, most 
such methods require a few thousands of 
features or large amounts of hand-written data 
for training, it is not clear that the same neural 
network models will be applicable to real world 
applications. 
We propose a word sense disambiguation 
method that combines both the neural net-based 
approach and the work of Li et al(2000), 
especially focusing on the practicality of the 
method for application to real world MT 
systems. To reduce the number of input features 
of neural networks to a practical size, we use 
concept codes of a thesaurus as features. 
In this paper, Yale Romanization is used to 
represent Korean expressions. 
1 System Architecture 
Our neural network method consists of two 
phases. The first phase is the construction of the 
feature set for the neural network; the second 
phase is the construction and training of the 
neural network. (see Figure 1.) 
For practical reasons, a reasonably small 
number of features is essential to the design of a 
neural network. To construct a feature set of a 
reasonable size, we adopt Li?s method (2000), 
based on concept co-occurrence information 
(CCI). CCI are concept codes of words which 
co-occur with the target word for a specific 
syntactic relation. 
In accordance with Li?s method, we 
automatically extract CCI from a corpus by 
constructing a Korean sense-tagged corpus. To 
accomplish this, we apply a Japanese-to-Korean 
MT system. Next, we extract CCI from the 
constructed corpus through partial parsing and 
scanning. To eliminate noise and to reduce the 
number of CCI, refinement proceesing is applied 
Japanese Corpus
COBALT-J/K
(Japanese-to-Korean
MT system)
Sense Tagged
Korean Corpus
Partial Parsing
& Pattern Scanning
Raw CCI
CCI Refinement
Processing
Refined CCI
Feature Set Construction Neural Net Construction
Feature Set
Network
Construction
Neural Network
Network
Learning
Stored in
MT Dictionary
Network
Parameters
Figure 1. System Architecture 
noun
nature  character          society institute  things 
0            1                     7          8          9
astro- calen- animal        pheno-
nomy    dar                          mena
00      01              06             09
goods drugs  food       stationary    machine
90      91       92             96             99
orga- ani- sin- intes- egg    sex
nism  mal              ews   tine    
060    061               066   067    068   069
supp- writing- count- bell
lies      tool      book
960     961      962               969
?????
?????
?????
????? ?????
?????
?????
?????
?????
?????
L1
L2
L3
L4
Figure 2. Concept hierarchy of the Kadokawa
thesaurus
to the extracted raw CCI. After completing 
refinement processing, we use the remaining 
CCI as features for the neural network. The 
trained network parameters are stored in a 
Korean-to-Japanese MT dictionary for WSD in 
translation. 
2 Construction of Refined Feature Set 
2.1 Automatic Construction of Sense-tagged 
Corpus 
For automatic construction of the sense-tagged 
corpus, we used a Japanese-to-Korean MT 
system called COBALT-J/K1. In the transfer 
dictionary of COBALT-J/K, nominal and verbal 
words are annotated with concept codes of the 
Kadokawa thesaurus (Ohno and Hamanishi, 
1
1
C
d
C
t
n
c
a
1
J
N
 
1
T
p
The quality of the constructed sense-tagged 
corpus is a critical issue. To evaluate the quality, 
we collected 1,658 sample sentences (29,420 
eojeols2) from the corpus and checked their 
precision. The total number of errors was 789, 
and included such errors as morphological 
analysis, sense ambiguity resolution and 
unknown words. It corresponds to the accuracy 
of 97.3% (28,631 / 29,420 eojeols). 
Because almost all Japanese common nouns 
represented by Chinese characters are 
monosemous little transfer ambiguity is 
exhibited in Japanese-to-Korean translation. In 
our test, the number of ambiguity resolution 
errors was 202 and it took only 0.69% of the 
overall corpus (202 / 29,420 eojeols). 
Considering the fact that the overall accuracy of 
the constructed corpus exceeds 97% and only a 
few sense ambiguity resolution errors were 
 
 
 
 
 
. 
, 
 
 
 
 981), which has a 4-level hierarchy of about 
,100 semantic classes, as shown in Figure 2. 
oncept nodes in level L1, L2 and L3 are further 
ivided into 10 subclasses. 
We made a slight modification of 
OBALT-J/K to enable it to produce Korean 
ranslations from a Japanese text, with all 
ominal words tagged with specific concept 
odes at level L4 of the Kadokawa thesaurus. As 
 result, a Korean sense-tagged corpus of 
,060,000 sentences can be obtained from the 
apanese corpus (Asahi Shinbun, Japanese 
ewspaper of Economics, etc.). 
                                                    
 COBALT-J/K (Collocation-Based Language 
ranslator from Japanese to Korean) is a high-quality 
ractical MT system developed by POSTECH.                                                      
found in the Japanese-to-Korean translation of
nouns, we regard the generated sense-tagged
corpus as highly reliable. 
2.2 Extraction of Raw CCI 
Unlike English, Korean has almost no syntactic
constraints on word order as long as the verb
appears in the final position. The variable word
order often results in discontinuous constituents
Instead of using local collocations by word order
Li et al (2000) defined 13 patterns of CCI for
homographs using syntactically related words in
a sentence. Because we are concerned only with
2 An Eojeol is a Korean syntactic unit consisting of a
content word and one or more function words. 
Table 2. Concept codes and frequencies in CFP 
({<Ci,fi>}, type2, nwun(eye)) 
Code Freq. Code Freq. Code Freq. Code Freq.
103 4 107 8 121 7 126 4 
143 8 160 5 179 7 277 4 
320 8 331 6 416 7 419 12
433 4 501 13 503 10 504 11
505 6 507 12 508 27 513 5 
530 6 538 16 552 4 557 7 
573 5 709 5 718 5 719 4 
733 5 819 4 834 4 966 4 
987 9 other* 210     
? ?other? in the table means the set of concept codes 
with the frequencies less than 4. 
Table 1. Structure of CCI Patterns 
CCI type Structure of pattern 
type0 unordered co-occurrence words 
type1 noun + noun  or  noun + noun 
type2 noun + uy + noun 
type3 noun + other particles + noun 
type4 noun + lo/ulo + verb 
type5 noun + ey + verb 
type6 noun + eygey + verb 
type7 noun + eyse + verb 
type8 noun + ul/lul + verb 
type9 noun + i/ka + verb 
type10 verb + relativizer + noun 
noun homographs, we adopt 11 patterns from 
them excluding verb patterns, as shown in Table 
1. The words in bold indicate the target 
homograph and the words in italic indicate 
Korean particles. 
For a homograph W, concept frequency 
patterns (CFPs), i.e., ({<C1,f1>,<C2,f2>, ... , 
<Ck,fk>}, typei, W(Si)), are extracted from the 
sense-tagged training corpus for each CCI type i 
by partial parsing and pattern scanning, where k 
is the number of concept codes in typei, fi is the 
frequency of concept code Ci appearing in the 
corpus, typei is an CCI type i, and W(Si) is a 
homograph W with a sense Si. All concepts in 
CFPs are three-digit concept codes at level L4 in 
the Kadokawa thesaurus. Table 2 demonstrates 
an example of CFP that can co-occur with the 
homograph ?nwun(eye)? in the form of the CCI 
type2 and their frequencies. 
2.3 CCI Refinement Processing 
The extracted CCI are too numerous and too 
noisy to be used in a practical system, and must 
to be further selected. To eliminate noise and to 
reduce the number of CCI to a practical size, we 
apply the refinement processing to the extracted 
CCI. CCI refinement processing is composed of 
2 processes: concept code discrimination and 
concept code generalization. 
2.3.1 Concept Code Discrimination 
In the extracted CCI, the same concept code may 
appear for determining the different meanings of 
a homograph. To select the most probable 
concept codes, which frequently co-occur with 
the target sense of a homograph, Li defined the 
discrimination value of a concept code using 
Shannon?s entropy (Shannon, 1951). A concept 
code with a small entropy has a large 
discrimination value. If the discrimination value 
of the concept code is larger than a threshold, 
the concept code is selected as useful 
information for deciding the word sense. 
Otherwise, the concept code is discarded. 
2.3.2 Concept Code Generalization 
After concept discrimination, co-occurring 
concept codes in each CCI type must be further 
selected and the code generalized. To perform 
code generalization, Li adopted to Smadja?s 
work (Smadja, 1993) and defined the code 
strength using a code frequency and a standard 
deviation in each level of the concept hierarchy. 
The generalization filter selects the concept 
codes with a strength larger than a threshold. We 
perform this generalizaion processing on the 
Kadokawa thesaurus level L4 and L3. 
After processing, the system stores the 
re
ty
re
fe
s
e
3
3
B
th
u
s
refined conceptual patterns ({C1, C2, C3, ...}, 
pei, W(Si)) as a knowledge source for WSD of 
al texts. These refined CCI are used as input 
atures for the neural network. The more 
pecific description of the CCI extraction is 
xplained in Li (2000). 
 Construction of Neural Network 
.1 Neural Network Architecture 
ecause of its strong capability for classification, 
e multilayer feedforward neural network is 
sed in our sense classification system. As 
hown in Figure 3, each node in the input layer 
presents a concept code in CCI of a target 
. .
CCI type i2
CCI type i1
input CCI type 0
input
CCI type 1
input
CCI type 8
input
CCI type 2
input
74
26
022
078
080
50
696
028
419
38
23
239
323
nwun1 (snow)
nwun2 (eye)
...
Figure 5. The Resulting Network for ?nwun? 
w
r
n
n
To determine a good topology for the network, 
we implemented a 2-layer (no hidden layer) and 
a 3-layer (with a single hidden layer of 5 nodes) 
network and compared their performance. The 
comparison result is given in Section 5. 
Each homograph has a network of its own. 
Figure 43 demonstrates a construction example 
of the input layer for the homograph ?nwun? 
with the sense ?snow? and ?eye?. The left side is 
the extracted CCI for each sense after refinement 
processing. We construct the input layer for 
?nwun? by merely integrating the concept codes 
in both senses. The resulting input layer is 
partitioned into several subgroups depending on 
their CCI types, i.e., type 0, type 1, type 2 and 
type 8. Figure 5 shows the overall network 
architecture for ?nwun?. 
 
3  
f  
c
3.2 Network Learning 
We selected 875 Korean homographs requring 
the WSD processing in a Korean-to-Japanese 
translation. Among the selected nouns, 736 
nouns (about 84%) had two senses and the other 
139 nouns had more than 3 senses. Using the 
extracted CCI, we constructed neural networks 
and trained network parameters for each 
homograph. The training patterns were also 
extracted from the previously constructed 
sense-tagged corpus. 
The average number of input features (i.e. 
input nodes) of the constructed networks was 
approximately 54.1 and the average number of 
senses (i.e. output nodes) was about 2.19. In the 
case of a 2-layer network, the total number of 
parameters (synaptic weights) needed to be 
trained is about 118 (54.1?2.19) for each 
homograph. This means that we merely need 
storage for 118 floating point numbers (for 
s
fe
re
? CCI type 0 : {26, 022}
? CCI type 1 : {080, 696}
nwun1 (snow)
CCI type 0
input
CCI type 1
74
26
022
078
080
Refined CCI
4
O
c
k
o
s
F
K                                                    
 The concept codes in Figure 4 are simplified ones
or the ease of illustration. In reality there are 87
? CCI type 8 : {38, 239}
Total 13 concept codes
integrate input
CCI type 8
input
CCI type 2
input
13 nodes
nwun2 (eye)
? CCI type 0 : {74, 078}
? CCI type 2 : {50, 028, 419}
? CCI type 8 : {23, 323}
50
696
028
419
38
23
239
323
Figure 4. Construction of Input layer for ?nwun?...
..
Output
(senses of the 
target word)
Inputs Outputs
..
Hidden
Layers
input
CCI type ik
input
...
Figure 3. Topology of Neural Network 
ord and each node in the output layer 
epresents the sense of a target word. The 
umber of hidden layers and the number of 
odes in a hidden layer are another crucial issue. oncept codes for ?nwun?. Cynaptic weights) and 54 integers (for input 
atures) for each homograph, which is a 
asonable size to be used in real applications. 
 Word Sense Disambiguation 
ur WSD approach is a hybrid method, which 
ombines the advantage of corpus-based and 
nowledge-based methods. Figure 6 shows our 
verall WSD algorithm. For a given homograph, 
ense disambiguation is performed as follows. 
irst, we search a collocation dictionary. The 
orean-to-Japanese translation system 
OBALT-K/J has an MWTU (Multi-Word 
{078}
CCI type 0 CCI type 0
input
CCI type 1
input
CCI type 8
input
CCI type 2
input
CCI type 1
nwunmwul-i   katuk-han   kunye-uy   nwun-ul   po-mye
input               : ??? ??? ??? ?? ?? ?
[078] [274]concept code  : [503] [331]targetword
CCI type        : (type 0) (type 0) (type 2) (type 8)
CCI type 2
CCI type 8
{none}
{503}
{331}
078
022
74
26
028
50
696
080
239
38
23
419
323
Input Layer
Similarity
Calculation
{274}
(0.000)
(0.285)
(0.250)
(1.000)
(0.000)
(0.857)
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.000)
(0.250)
similarity values
Figure 7. Construction of Input Pattern by Using
Concept Similarity Calculation 
Neural Networks
Select the most frequent sense
Success
Success
Answer
NO
NO
NO
YES
YES
YES
Selectional Restrictions of the Verb
Collocation Dictionary
Success
 
Figure 6. The Proposed WSD Algorithm 
Translation Units) dictionary, which contains 
idioms, compound words, collocations, etc. If a 
collocation of the target word exists in the 
MWTU dictionary, we simply determine the 
sense of the target word to the sense found in the 
dictionary. This method is based on the idea of 
?one sense per collocation?. Next, we verify the 
selectional restriction of the verb described in 
the dictionary. If we cannot find any matched 
patterns for selectional restrictions, we apply the 
neural network approach. WSD in the neural 
network stage is performed in the following 3 
steps. 
Step 1. Extract CCI from the context of the 
target word. The window size of the context is a 
single sentence. Consider, for example, the 
sentence in Figure 7 which has the meaning of 
?Seeing her eyes filled with tears, ??. The 
target word is the homograph ?nwun?. We 
extract its CCI from the sentence by partial 
parsing and pattern scanning. In Figure 7, the 
words ?nwun? and ?kunye(her)? with the concept 
code 503 have the relation of <noun + uy + 
noun>, which corresponds to ?CCI type 2? in 
Table 1. There is no syntactic relation between 
the words ?nwun? and ?nwunmul(tears)? with the 
concept code 078, so we assign ?CCI type 0? to 
the concept code 078. 
Similarly, we can obtain all pairs of CCI types 
and their concept codes appearing in the context. 
All the extracted <CCI-type: concept codes> 
pairs are as follows: {<type 0: 078,274>, <type 
2: 503>, <type 8: 331>}. 
Step 2. Obtain the input pattern for the 
network by calculating concept similarities 
between the features of the input nodes and the 
concept code in the extracted <CCI-type: 
concept codes>. Concept similarity calculation 
is performed only between the concept codes 
with the same CCI-type. The calculated concept 
similarity score is assigned to each input node as 
the input value to the network. 
Csim(Ci, Pj) in Equation 1 is used to calculate 
the concept similarity between Ci and Pj, where 
MSCA(Ci, Pj) is the most specific common 
ancestor of concept codes Ci and Pj, and weight 
is a weighting factor reflecting that Ci as a 
descendant of Pj is preferable to other cases. 
That is, if Ci is a descendant of Pj, we set weight 
to 1. Otherwise we set weight to 0.5. 
weight
PlevelClevel
PCMSCAlevel
PCCsim
ji
ji
ji ?+
?=
)()(
)),((2
),(  (1)
The similarity values between the target 
(all 0.000)
(0.375)
(0.857)
(0.667)
(0.285)
(0.250) (0.250)
L1
L2
L3
L4
?
Ci
P1
P2
P3
P4
P5 P5
TOP
Figure 8. Concept Similarity on the Kadokawa
Thesaurus Hierarchy 
concept Ci and each Pj on the Kadokawa 
thesaurus hierarchy are shown in Figure 8. 
These similarity values are computed using 
Equation 1. For example, in ?CCI-type 0? part 
calculation, the relation between the concept 
codes 274 and 26 corresponds to the relation 
between Ci and P4 in Figure 8. So we assign the 
similarity 0.285 to the input node labeled by 26. 
As another example, the concept codes 503 and 
50 have a relation between Ci and P2 and we 
obtain the similarity 0.857. If more than two 
concept codes exist in one CCI-type, such as 
<CCI-type 0: 078, 274>, the maximum 
similarity value among them is assigned to the 
input node, as in Equation 2. 
In Equation 2, Ci is the concept code of the 
input node, and Pj is the concept codes in the 
<CCI-type: concept codes> pair which has the 
same CCI-type as Ci. 
By adopting this concept similarity calculation, 
we can achieve a broad coverage of the method. 
If we use the exact matching scheme instead of 
concept similarity, we may obtain only a few 
concept codes matched with the features. 
Consequently, sense disambiguation would fail 
because of the absence of clues. 
Step 3. Feed the obtained input pattern to the 
neural network and compute activation strengths 
for each output node. Next, select the sense of 
the node that has a larger activation value than 
all other output node. If the activation strength is 
lower than the threshold, it will be discarded and 
his 
5 Experimental Evaluation 
For an experimental evaluation, 10 ambiguous 
Korean nouns were selected, along with a total 
of 500 test sentences in which one homograph 
appears. In order to follow the ambiguity 
distribution described in Section 3.2, we set the 
number of test nouns with two senses to 8 (80%). 
The test sentences were randomly selected from 
the KIBS (Korean Information Base System) 
corpus. 
The experimental results are shown in Table 3, 
where result A is the case when the most 
frequent sense was taken as the answer. To 
compare it with our approach (result C), we also 
performed the experiment using Li?s method 
(result B). For sense disambiguation, Li?s 
method features which are similar to our method. 
However, unlike our method, which combines 
all features by using neural networks, Li 
considers only one clue at each decision step. As 
shown in the table, our approach exceeded Li?s 
)),((max)( jiPi PCCsimCInputVal i
=    (2)
Table 3. Comparison of WSD Results 
Precision (%) Word Sense No (A) (B) (C)
father & child 33 pwuca
rich man 17 
66 64 72
liver 37 kancang
soy source 13 
74 84 74
housework 39 kasa
words of song 11 
78 68 82
shoes 45 kwutwu
word of mouth 5 
90 70 92
eye 42 nwun
snow 8 
84 80 86
container 41 yongki 82 72 88
the network will not make any decisions. T
process is represented in Figure 9. courage 9 
doctor 27 uysa
intention 23 
54 80 84
district 27 cikwu
the earth 23 
54 84 92
whole body 39 
one?s past 6 censin
telegraph 5 
78 84 80
one?s best 27 
military strength 13 
electric power 7 
cenlyek
past record 3 
54 50 72
Average Precision 71.4 73.6 82.2
? (A) : Baseline   (B) : Li?s method 
(C) : Proposed method (using a 2-layer NN) 
nwun1 (snow)
nwun2 (eye)
...
threshold
(0.000)
(0.285)
(0.250)
(1.000)
(0.000)
(0.857)
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.000)
(0.250)
Figure 9. Sense Disambiguation for ?nwun? 
in most of the results except ?kancang? and 
?censin?. This result shows that word sense  
disambiguation can be improved by combining 
several clues together (e.g. neural networks) 
rather than using them independently (e.g. Li?s 
method). 
The performance for each stage of the 
proposed method is shown in Table 4. Symbols 
COL, VSR, NN and MFS in the table indicate 4 
stages of our method in Figure 6, respectively. 
In the NN stage, the 3-layer model did not show 
a performance superior  to the 2-layer model 
because of the lack of training samples. Since 
the 2-layer model has fewer parameters to be 
trained, it is more efficient to generalize for 
limited training corpora than the 3-layer model. 
Conclusion 
To resolve sense ambiguities in 
Korean-to-Japanese MT, this paper has proposed 
a practical word sense disambiguation method 
using neural networks. Unlike most previous 
approaches based on neural networks, we reduce 
the number of features for the network to a 
practical size by using concept codes rather than 
lexical words. In an experimental evaluation, the 
proposed WSD model using a 2-layer network 
achieved an average precision of 82.2% with an 
improvement over Li?s method by 8.6%. This 
result is very promising for real world MT 
systems. 
We plan further research to improve precision 
and to expand our method for verb homograph 
disambiguation. 
Acknowledgements 
This work was supported by the Korea Science 
and Engineering Foundation (KOSEF)  through 
the Advanced Information Technology Research 
Center(AITrc). 
Table 4. Average Precision and Coverage 
for Each Stage of thePproposed Method 
 
<Case 1 : 2-layer NN> 
 COL VSR NN MFS 
Avg. Prec 100.0% 91.2% 86.3% 56.1%
Avg. Cov 3.6% 6.8% 73.2% 16.4%
 
<Case 2 : 3-layer NN> 
 COL VSR NN MFS 
Avg. Prec 100.0% 91.2% 87.1% 56.0%
Avg. Cov 3.6% 6.8% 72.5% 17.1%
 
References 
Gallant S. (1991) A Practical Approach for 
Representing Context and for Performing Word 
Sense Disambiguation Using Neural Networks. 
Neural Computation, 3/3, pp. 293-309 
Leacock C., Twell G. and Voorhees E. (1993) 
Corpus-based Statistical Sense Resolution. In 
Proceedings of the ARPA Human Language 
Technology Workshop, San Francisco, Morgan 
Kaufman, pp. 260-265 
Li H. F., Heo N. W., Moon K. H., Lee J. H. and Lee 
G. B. (2000) Lexical Transfer Ambiguity 
Resolution Using Automatically-Extracted Concept 
Co-occurrence Information. International Journal 
of Computer Processing of Oriental Languages, 
13/1, pp. 53-68  
McRoy S. (1992) Using Multiple Knowledge Sources 
for Word Sense Discrimination. Computational 
Linguistics, 18/1, pp. 1-30 
Mooney R. (1996) Comparative Experiments on 
Disambiguating Word Senses: An Illustration of 
the Role of Bias in Machine Learning. In 
Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, 
Philadelphia, PA, pp. 82-91 
Ng, H. T. and Zelle J. (1997) Corpus-Based 
Approaches to Semantic Interpretation in Natural 
Language Processing. AI Magazine, 18/4, pp. 
45-64 
Ohno S. and Hamanishi M. (1981) New Synonym 
Dictionary. Kadokawa Shoten, Tokyo 
Smadja F. (1993) Retrieving Collocations from Text: 
Xtract. Computational Linguistics, 19/1, pp. 
143-177 
Waltz D. L. and Pollack J. (1985) Massively Parallel 
Parsing: A Strongly Interactive Model of Natural 
Language Interpretation. Cognitive Science, 9, pp. 
51-74 
 
