Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 19?24,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Iterative Approach for Joint  
Dependency Parsing and Semantic Role Labeling 
 
 
Qifeng Dai 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
daiqifeng001@126.com  
Enhong Chen 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
cheneh@ustc.edu.cn 
Liu Shi 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
shiliu@ustc.edu 
 
 
 
Abstract 
We propose a system to carry out the joint pars-
ing of syntactic and semantic dependencies in 
multiple languages for our participation in the 
shared task of CoNLL-2009. We present an it-
erative approach for dependency parsing and 
semantic role labeling. We have participated in 
the closed challenge, and our system achieves 
73.98% on labeled macro F1 for the complete 
problem, 77.11% on labeled attachment score 
for syntactic dependencies, and 70.78% on la-
beled F1 for semantic dependencies. The cur-
rent experimental results  show that our method 
effectively improves system performance. 
1 Introduction 
In this paper we describe the system submitted to 
the closed challenge of the CoNLL-2009 shared 
task on joint parsing of syntactic and semantic de-
pendencies in multiple languages.  
Give a sentence, the task of dependency parsing 
is to identify the syntactic head of each word in the 
sentence and classify the relation between the de-
pendent and its head. The task of semantic role 
labeling is to label the senses of predicates in the 
sentence and labeling the semantic role of each 
word in the sentence relative to each predicate. 
The difficulty of this shared task is to perform 
joint task on dependency parsing and semantic role 
labeling. We split the shared task into four sub-
problems: syntactic dependency parsing, syntactic 
dependency label classification, word sense disam-
biguation, and semantic role labeling. And we pro-
pose a novel iterative approach to perform the joint 
task. In the first step, the system performs depend-
ency parsing and semantic role labeling in a pipe-
lined manner and the four sub-problems extract 
features based on the known information. In the 
iterative step, the system performs the four tasks in 
a pipelined manner but uses features extracted 
from the previous parsing result. 
The remainder of the paper is structured as fol-
lows. Section 2 presents the technical details of our 
system. Section 3 presents experimental results and 
the performance analysis. Section 4 looks into a 
few issues concerning our forthcoming work for 
this shared task, and concludes the paper. 
2 System description 
This section briefly describes the main components 
of our system: a) system flow; b) syntactic parsing; 
c) semantic role labeling; d) an iterative approach 
to perform joint syntactic-semantic parsing. 
2.1 System flow 
As many systems did in CoNLL Shared Task 2008, 
the most direct way for such task is pipeline ap-
proach. First, Split the system into four subtasks: 
syntactic dependency parsing, syntactic depend-
ency relation labeling, predicate sense labeling and 
semantic role labeling. Then, execute them one by 
one. In our system, we extend this pipeline system 
to an iterative system so that it can do a joint label-
ing to improve the performance. 
Our iterative system is based on the pipeline 
system. For the first iteration (original step), we 
use the pipeline system to parse and label the 
19
whole sentence. For the rest iterations (iterative 
step), we use another pipeline system to parse and 
label it. The structure of this pipeline is the same as 
the original one, but each subtask can have much 
more features than the original subtask. Because 
the whole sentence has been labeled in the original 
step, all information is available for every subtask. 
For example, when doing syntactic dependency 
relation labeling, we can add some features about 
sense and semantic role. It seems like using syntac-
tic results to do semantic labeling, then using se-
mantic results to improve syntactic labeling. This 
is the core idea of our joint system. Figure 1 shows 
the main flow of our system. 
 
 
Figure 1. The main flow of iteration system 
 
 
 
 
2.2 Dependency Parsing 
In the dependency parsing step, we split the task 
into two sub-problems: syntactic dependency pars-
ing and syntactic dependency relation labeling. 
In the syntactic dependency parsing stage, 
MSTParser1, a dependency parser that searches for 
maximum spanning trees over directed graphs, is 
applied. Due to the differences between the seven 
languages, we use different parameters to train a 
parsing model. Specifically, as Czech and German 
languages are none-projective and the others are 
projective, we train Czech and German languages 
with parameter ?none-projective? and the others 
with ?projective?. 
On the syntactic dependency label classification 
step, we used the max-entropy classification algo-
rithm to train the model. This step contains two 
processes. In the first process the sub-problem 
trains the model with the following basic features: 
Start 
End 
Syntactic dependency 
parsing 
Syntactic dependency 
relation labeling 
Set count = iterate times 
Set isIterStep = false 
Predicate sense label-
ing 
Semantic role labeling 
count -- 
isIterStep = true 
count = 0 
Y
 
Get fea-
tures: 
this step 
return the 
feature of 
system 
judge by 
the type of 
sub task  
and the 
parameter 
isIterStep. 
N
? FORM1: FORM of the head. 
? LEMMA1: LEMMA of the head. 
? STEM1 (English only): STEM of the head. 
? POS1: POS of the head. 
? IS_PRED1: the value of FILLPRED of the 
head. 
? FEAT1: FEAT of the head. 
? LM_STEM1 (English only): the left-most 
modifier?s STEM of head. 
? LM_POS1: the left-most modifier?s POS 
of head. 
? L_NUM1: number of the head?s left modi-
fiers. 
? RM_STEM1 (English only): the right-
most modifier?s STEM of head. 
? RM_POS1: the right-most modifier?s POS 
of head. 
? M_NUM1: number of modifiers of the 
head. 
? SUFFIX1 (English only): suffix of the 
head. 
? FORM2: FORM of the dependent. 
? LEMMA2: LEMMA of the dependent. 
? STEM2 (English only): STEM of the de-
pendent. 
? POS2: POS of the dependent. 
? IS_PRED2: the value of FILLPRED of the 
dependent. 
                                                          
1 http://sourceforge.net/projects/mstparser 
20
? FEAT2: FEAT of the dependent. 
? LM_STEM2 (English only): the left-most 
modifier?s STEM of dependent. 
? LM_POS2: the left-most modifier?s POS 
of dependent. 
? L_NUM2:  number of the dependent?s left 
modifiers. 
? RM_STEM2 (English only): the right-
most modifier?s STEM of dependent. 
? RM_POS2: the right-most modifier?s POS 
of dependent. 
? M_NUM2: number of modifiers of the de-
pendent. 
? SUFFIX2 (English only): suffix of the de-
pendent. 
? DEP_PATH_ROOT_POS2: POS list from 
dependent to tree?s root through the syn-
tactic dependency path. 
? DEP_PATH_ROOT_LEN2: length from 
dependent to tree?s root through the syn-
tactic dependency path.  
? POSITION: The position of the word with 
respect to its predicate. It has three values, 
?before?, ?is? and ?after?, for the predicate. 
In the iterative step, in addition to the features 
mentioned above, the sub-task trains the model 
with the following features: 
? DEP_PATH_ROOT_POS1: POS list from 
head to tree?s root through the syntactic 
dependency path. 
? DEP_PATH_ROOT_REL1: length from 
dependent to tree?s root through the syn-
tactic dependency path. 
? PRED_POS: POS list of all predicates in 
the sentence. 
? FORM2 + DEP_PATH_REL: component 
of FORM2 and the POS list from head to 
the dependent through the syntactic de-
pendency path. 
? POSITION + FORM2 
? STEM1 + FORM2 (English only) 
? STEM1 + STEM2 (English only) 
? POSITION + POS2 
? ROLE_LIST2: list of APRED when the 
dependent is a predicate. 
? ROLE: list of APRED and PRED when 
the head is predicate. 
? L_ROLE: the nearest semantic role in its 
left side when head is a predicate. 
? R_ROLE: the nearest semantic role in its 
right side when head is a predicate. 
? IS_ROLE1: whether dependent is a se-
mantic role of head when head is a predi-
cate. 
2.3 Semantic role labeling 
Unlike CoNLL-2008 shared task, this shared task 
does not need to identify predicates. So the main 
task of this step is to label the sense of each predi-
cate and label the semantic role for each predicate. 
When labeling the sense of each predicate, we 
build a classification model for each predicate. As 
the senses of different predicates are usually unre-
lated even if they have the same sense label, this 
makes it difficult for us to use only one classifier to 
label them. But this approach leads to another issue. 
The set of predicates in the training set cannot 
cover all predicates. For new predicates in the test 
set, no classification model can be found for them, 
and we build a most common sense for them. The 
features we used are as follow: 
? DEPREL1: DEPREL of the predicate. 
? STEM1 
? POS1 
? RM_STEM1 (English only) 
? RM_POS1 
? FORM2 
? POS2 
? SUFFIX2 
? VOICE (English only): VOICE of predi-
cate. 
? POSITION + POS2 
? L_POS1 + POS1 + R_POS1: component 
of left word?s POS and predicate POS and 
right word?s POS. 
? FORM2 + DEP_PATH_REL 
? DEP_PATH_ROOT_POS1 
? DEP_PATH_ROOT_REL1 
When labeling the semantic role, we use a simi-
lar approach as we did in CoNLL Shared Task 
2008. However, as the frames information is not 
supplied for all languages, we do not use it in this 
task. The features we use are as follows: 
? DEPREL1 
? STEM1 (English only) 
? POS1 
? RM_STEM1 (English only) 
? RM_POS1 
21
? FORM2 
? POS2 
? SUFFIX2 
? VOICE2 (English only) 
? POSITION 
? DEP_PATH_REL 
? DEP_PATH_POS 
? SENSE2 
? SENSE2 + VOICE2 
? POSITION +  VOICE2 
? DEP_PATH_LEN 
? DEP_PATH_ROOT_REL1 
Moreover, we build an iterative model in this 
shared task. When doing an iterative labeling, the 
previous labeling results are known. So we can 
design some new features for checking the previ-
ous results in a global view. The features we add 
for the iterative model are as follows: 
? SENSE1: SENSE of the predicate. 
? SENSE1 + VOICE1: component of the 
SENSE + VOICE of predicate. 
? VOICE1 + FORM1: component of VOICE 
and FORM. 
? ROLE_LIST1: list of APRED of predicate. 
2.4 Iterative Approach 
As described above, some subtasks have two 
groups of features. One is for the pipeline model, 
and the other is for the iterative model. The usage 
of these two types of model is the same. The only 
difference is that they use different features. The 
iterative model can get more information, so they 
can use more features. These additional features 
can contain some joint and global (like frame and 
global structure) information. The performance 
may be improved because the viewer is extended. 
Some structural error and semantic conflict can be 
fixed. 
Although the usage of the two types of model is 
the same, there are some differences when building 
the models. 
In the iterative step, all information is available 
for doing parsing and labeling. For example, when 
doing syntactic dependency relation labeling in the 
iterative step, the fields ?HEAD?, ?DEPREL?, 
?PRED? and ?APREDs? are filled by the pervious 
iteration. So all these information can be used in 
the iterative step. This will cause one issue: use 
?HEAD1? to label ?HEAD2?. When training the 
model, ?HEAD1? is golden. The classifier will 
build a model directly and let ?HEAD2? equal to 
?HEAD1?. However, in the iterative step, 
?HEAD1? is not golden, but such model makes it 
impossible to change the results.. The iterative step 
will be useless. 
We design a simple method to avoid this issue.  
? Firstly, split the training set into N (N>1) 
subsets.  
? Secondly, for each subset, use the left N-1 
subsets to build an original sub-model (use 
features in the pipeline step). 
? Thirdly, use each sub-model to label the 
corresponding subset. 
? Lastly, use these labeled N subsets to ex-
tract samples (use features in the iterative 
step) for building the iterative model. 
In this way, the ?HEAD1? is not golden any 
more. And for each sub-task, we can use the simi-
lar method to build the original model and the it-
erative model.  
Moreover, in our system, we only build the it-
erative models for syntactic dependency relation 
labeling and semantic role labeling. For syntactic 
dependency parsing, we use an approach with very 
high time and space complexity, so it is not added 
to the iterative step. Thus, its results will not be 
changed in the iterative step. For sense labeling, 
we build classification models for every predicate. 
There are too many models and each model con-
tains only a few classes. We think they are not 
suitable for building the iterative model. But, as its 
previous sub-task (syntactic dependency relation 
labeling) is added to the iterative step, it is useful 
to add it to the iterative step. Though we do not 
build an iterative model for sense labeling, we can 
directly use its pipeline model. This is another ad-
vantage of our iterative model: if one subtask is not 
suitable for doing iterative labeling/parsing, we can 
use its pipeline model instead. 
3 Experiments and Results 
We have tested our system with the test set and 
obtained official results as shown in Table 1. We 
have tried to find how the iterative step influences 
syntactic dependency parsing and semantic role 
labeling. For syntactic dependency parsing and 
semantic role labeling, we do experiments on the 
test set. 
  
22
 Macro F1 Score 
Average 73.98 
Catalan 72.09 
Chinese 72.72 
Czech 67.14 
English 81.89 
German 75.00 
Japanese 80.89 
Spanish 68.14 
Table 1. The Macro F1 Score of every languages and 
the average value. 
3.1 Syntactic Dependency Parsing 
Dependency Parsing can be split into two sub-
problems: syntactic dependency parsing and syn-
tactic dependency label classification. We use the 
iterative method on syntactic dependency label 
classification. We do experiments on the test set.  
On the test set, we do two group experiments. In 
the first group, we build a subtest to test this sub-
task only. All other information is given, and we 
just label the dependency relation. The results are 
shown in Table 2. The row of ?Initial step? shows 
the results of this sub task in the original step. The 
left two rows show the results in the iterative step 
with iterating once and twice. The table shows that 
the iterative approach improves the performance. 
Especially for Catalan, the performance increases 
by 2.89%. 
Certainly, in the whole system, this subtask can-
not get golden information about sense and seman-
tic roles. So we test it in the whole system (joint 
test) on the test set in the second group of experi-
ments. As shown in Table 3, the iterative step is 
not as good as previous test. But it is still useful for 
some languages. The reason that some languages 
have no improvements on the iterative step is that 
the result of the initial step is not so good. 
3.2 Semantic Role Labeling 
Like syntactic dependency parsing, we do two tests 
on Semantic Role Labeling. This result is not con-
sistent with the official data because we have add-
ed some features of the subtask. The results of 
subtest can be found in Table 4. And Table 5 
shows the results of the joint test. These two 
groups of results show that the advantage of the 
iterative step is not as good as that of syntactic de-
pendency labeling in subtest. But it improves the 
performance for most languages. The iterative step 
improves the performance in both two tests.  
3.3 Analysis of Results  
From the experimental results, we can see that the 
effect of each part of the iterative step depends on 
the overall labeling result of the previous step. And 
the labeling effect varies with different languages. 
Iterative approach can improve the performance of 
the system but it strongly depends on the initial 
labeling result.  
4 Conclusion and Future Work  
This paper has presented a simple discriminative 
system submitted to the CoNLL-2009 shared task 
to address the learning task of syntactic and seman-
tic dependencies. The paper first describes how to 
carry out syntactic dependency parsing and seman-
tic role labeling, and then a new iterative approach 
is presented for joint parsing. The experimental 
results show that the iterative process can improve 
the labeling accuracy on syntactic and semantic 
analysis. However, this approach probably depends 
on the accuracy of the initial labeling results. The 
results of the initial labeling results will affect the 
effect of the iterative process.  
Because of time constraints and inadequate ex-
perimental environment, our first results do not 
meet our expectation, and the effect of the iterative 
step is not so clear. Next, we will strive to refine 
our approach to produce good results for the syn-
tactic dependency parsing, since it has a great im-
pact on the final parsing results. 
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. This work was supported 
by National Natural Science Foundation of China 
(No.60573077, No.60775037) and the National 
High Technology Research and Development Pro-
gram of China (863 Program) (grant no. 
2009AA01Z123). We also thank the High-
Performance Center of USTC for providing us 
with the experimental platform. 
 
 
 
 
 
 
 
 
23
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 93.64 95.66 95.01 88.10 88.10 96.79 92.98 96.41 89.71* 98.17 95.48
Iteration 1 94.60 98.56* 96.08* 88.59 88.29 97.31* 94.57* 96.63* 89.31 98.34 98.30
Iteration 2 94.65 98.55 96.08* 88.68* 88.45* 97.29 94.56 96.63* 89.53 98.35* 98.33*
Table 2. The subtest result of Labeled Syntactic Accuracy of each language and the average performance value 
on test set. (* denotes the best score for the system) 
 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 74.02 77.75 73.81 58.69* 55.50* 84.75 78.85 82.45 66.27* 90.45* 71.64
Iteration 1 73.90 77.82 73.86* 58.17 54.95 84.81 78.95 82.51* 65.78 90.43 71.68
Iteration 2 73.94 77.85* 73.86* 58.31 55.13 84.82* 79.02* 82.46 65.85 90.45* 71.69*
Table 3. The joint test result of Labeled Syntactic Accuracy of each language and the average performance value 
on test set. (* denotes the best score for the system) 
 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 83.83 88.56 85.86 88.08 86.20* 86.23 82.09 80.98 78.82 74.32* 87.45
Iteration 1 84.34 89.02* 87.14* 87.88 86.09 86.66 82.07 83.66* 79.28* 74.06 87.59
Iteration 2 84.36 89.02* 87.01 88.10* 86.17 86.78* 82.34* 83.15 79.18 74.06 87.81*
Table 4. The sub test result of Semantic Labeled F1 of each language and the average performance value on test 
set. (* denotes the best score for the system) 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 70.01 66.87 71.63 75.50 75.71 78.97 69.87 67.50 58.47 70.91* 64.64
Iteration 1 70.15 67.12 71.98 75.54 75.68 79.40 70.17* 68.08* 58.55* 70.69 64.32
Iteration 2 70.20 67.33* 71.99* 75.65* 75.90* 79.47* 69.98 67.98 58.33 70.70 64.65*
Table 5. The joint test result of Semantic Labeled F1 of each language and the average performance value on test 
set. (* denotes the best score for the system) 
References  
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2009). Boulder, Colorado, USA. June 4-
5. pp. 3-22. 
Mariona Taul?, Maria Ant?nia Mart? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora 
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC-2008). Marrakech, Morocco. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank.  Natural Language 
Engineering, 15(1):143-172. 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie Miku-
lov? and Zden?k ?abokrtsk?. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data 
Consortium, Philadelphia, Pennsylvania, USA. ISBN 
1-58563-370-4. LDC Cat. No. LDC2006T01. URL: 
http://ldc.upenn.edu.  
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of 
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2006). Genoa, Italy. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp. 2008-2013. 
McDonald, Ryan. 2006. Discriminative learning and 
Spanning Tree Algorithms for Dependency    parsing. 
Ph.D. thesis, University of Pennyslvania. 
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus- 
sian prior for smoothing maximum entropy models. 
Technical. Report CMU-CS-99-108. 
24
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 263?267
Manchester, August 2008
Probabilistic Model for Syntactic and Semantic Dependency Parsing 
Enhong Chen 
Department of Computer 
Science, University of Sci-
ence and Technology of 
China, Hefei, China 
cheneh@ustc.edu.cn 
Liu Shi 
Department of Computer 
Science, University of Sci-
ence and Technology of 
China, Hefei, China 
shiliu@ustc.edu 
Dawei Hu 
Department of Computer 
Science, University of Sci-
ence and Technology of 
China, Hefei, China 
dwhu@mail.ustc.edu.cn
 
Abstract 
This paper proposes a novel method to 
analyze syntactic dependencies and label 
semantic dependencies around both the 
verbal predicates and the nouns. In this 
method, a probabilistic model is designed 
to obtain a global optimal result. More-
over, a predicate identification model and 
a disambiguation model are proposed to 
label predicates and their senses. The ex-
perimental results obtained on the wsj 
and brown test sets show that our system 
obtains 77% of labeled macro F1 score 
for the whole task, 84.47% of labeled at-
tachment score for syntactic dependency 
task, and 69.45% of labeled F1 score for 
semantic dependency task. 
1 Introduction 
There are two difficulties in the CoNLL 2008 
shared task. One is how to label semantic role on 
a dependency-based representation and how to 
label verbal predicates and nouns. The other one 
is how to combine the syntactic task with the 
semantic task together. 
On the basis of statistical analysis of labeling 
results, we optimize the traditional approaches of 
syntactic dependency parsing and semantic role 
labeling. Moreover, we design a predicate 
identification model and a disambiguation model, 
which will be described in section 2.3, for 
labeling predicates and their senses. In the 
disambiguation model, an exhaustion method is 
used to find the best sense which is 
corresponding to a frame of predicate. In order to 
obtain a global optimization result for every 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
sentence, a probabilistic model is designed to 
combine all subtasks. 
The rest of this paper is organized as follows:   
our system is described in section 2; and section 
3 reports our results on development and test sets; 
at last we conclude the paper in section 4. 
2 A Probabilistic Model for Syntactic 
and Semantic Dependency Labeling 
Compared with previous tasks, this shared task is 
more complex. It aims to merge both syntactic 
and semantic dependencies under a unified 
representation. Obviously, it can be divided into 
two subtasks: syntactic dependency parsing and 
semantic dependency labeling. For the second 
subtask, predicates and their senses should be 
labeled before semantic arguments for predicates 
are labeled. Since many predicates have only one 
sense, it is inefficient to build a multi-label 
classifier to classify each predicate. When a 
classification approach is used, it is mandatory to 
consider multiple senses for those predicates 
with only one or two senses. To prevent 
assigning irrelevant senses to predicates, we do 
not adopt classification approach. Instead, two 
more subtasks, i.e., predicate identification and 
predicate sense labeling, are introduced in this 
paper. The predicate sense labeling and semantic 
dependency labeling are performed together with 
a disambiguation model. 
To ensure that we can get an optimal overall 
syntactic and semantic dependency results 
through integrating the above steps, a probability 
model is proposed. The probabilistic model is 
described in Equation (1), where the score P  
of a sentence labeling is the combined 
conditional probability of its all subtasks,  is 
the probability of syntactic dependency parsing, 
 is the probability of predicate 
identification,  is the probability of 
sent
synP
predP
)(iPsem
263
semantic dependency labeling for the ith-
predicate, and n is the number of predicates. 
?
=
=
n
1i
sempredsynsent )(** iPPPP            (1) 
For each sentence, its top-N candidates using 
syntactic dependency parsing are obtained. Then 
for each candidate, predicates and semantic ar-
guments are labeled. At last, the best one with 
the highest  is chosen as final labeling result. sentP
2.1 Syntactic Dependency Parsing 
There are several approaches for syntactic de-
pendency parsing, as demonstrated in the CoNLL 
2007 shared task. A commonly used LR algo-
rithm is applied to this task. Unlike the best-first 
probabilistic shift-reduce LR algorithm used by 
(Kenji and Jun, 2007), here a combined probabil-
ity of all parsing steps is used to evaluate parsing 
results, and the best one is obtained as the final 
result. The probability of syntactic dependency 
parsing is defined in Equation (2). 
?= j actsyn iPP )(
=i 1
act
                      (2)  
where  is the probability of every LR ac-
tion act at step i, and j is the number of all steps. 
)(iP
As the search space of LR parser is exponen-
tial growth with the word number, the maximum 
size of candidate states is limited to 50. 
The features that we use are similar to (Kenji 
and Jun, 2007). Hence we do not describe them 
in this paper. 
2.2 Predicate Identification 
In this subtask, a MaxEnt model is adopted for 
classification. The features we used are as follow: 
? Base info: FORM, LEMMA, POS (GPOS 
if available, or is PPOS), SPLIT_FORM, 
SPLIT_LEMMA, PPOSS. 
? Base syntactic dependency info:  
o Number of modifiers; 
o Number of modifiers of the previous word; 
o Number of modifiers of the next word; 
o PPOSS of left-most modifier; 
o Deprel of left-most modifier; 
o PPOSS of right-most modifier; 
o Deprel of right-most modifier. 
? Modifiers info 
o POS list of all modifiers: if GPOS is avail-
able, POS is GPOS. Otherwise it is PPOS. 
o DEPREL list of all modifiers; 
o SPLIT_LEMMA list of all modifiers; 
o PPOSS list of all modifiers. 
? Head?s base info 
? Head?s base syntactic dependency info 
? Head?s modifiers info 
? Deprel:  the syntactic dependency relation 
to head. 
? Word stem 
? Stem of right-most modifier 
? PPOSS of right-most modifier 
? Suffix: The suffix of the word. We use the 
last 3 characters as this feature. 
? Voice: Check if the word is a verb and is 
passive voice. 
? Previous word info: Check if the previous 
word is a predicate. 
? Pos path to ROOT: PPOSS list from 
word to ROOT through the syntactic de-
pendency path. 
? Deprel path to ROOT: DEPREL list from 
word to ROOT through the syntactic de-
pendency path. 
Through statistical analysis, we find that 
PPOSS of nearly all predicates are in a particular 
category which contains NN, NNP, NNS, VB, 
VBD, VBG, VBN, VBP, VBZ, and JJ. Hence we 
ignore the words without these PPOSS to reduce 
the number of samples and speed up the process 
of training and recognition. Meanwhile, we also 
ignore the words having no relational frame in 
PropBank or NomBank. 
2.3 Predicate Sense Labeling 
In this subtask, we label the sense of each predi-
cate. Different predicates are usually unrelated 
even if they have the same sense number, which 
makes us hardly use a classifier to label them. 
Hence, we design a disambiguation model to 
solve this problem.  
Firstly, for each word which has been identi-
fied to be a predicate, we find out all of its prob-
able sense forms (corresponding to the field of 
?PRED?). According to statistical analysis, only 
about 0.05% PREDs are not described in 
PropBank frames or NomBank frames. So it is 
reasonable to assume that all PREDs could be 
found in PropBank or NomBank. Moreover, we 
find that about 96% PREDs are formed as 
?SPLIT_LEMMA + .sense? or ?LEMMA 
+ .sense?. As a result, when a word is identified 
to be a predicate, we use its LEMMA and 
SPLIT_LEMMA to find all possible PREDs 
from PropBank and NomBank. Furthermore, if 
some special words are unsuitable for these two 
forms, we should convert them into their original 
forms first and then find their possible PREDs. 
264
For the rest anomalistic words, we build a map-
ping dictionary from training data. 
Secondly, for each possible sense form, we la-
bel semantic argument for all words. If a word is 
not a semantic argument, it would be labeled as 
?_?. The score of the current possible sense form 
is calculated as the combination of all probability 
of each labeling. More details about semantic 
dependency labeling will be described in section 
2.4. 
Thirdly, we choose the sense form and its se-
mantic arguments with the highest score. The 
above steps will be repeated until all predicates 
have definite senses. 
2.4 Semantic Dependency Labeling 
Unlike CoNLL-2005 shared task, this shared 
task performing Semantic Role Labeling on a 
dependency-based representation (DSRL). It is a 
novel way for SRL and the traditional SRL 
methods can not directly be used here. 
Constituent-based SRL model needs to find out 
all probable constituents, while DSRL only 
considers the semantic dependency between 
word and predicate. Moreover, DSRL uses 
syntactic dependency parsing tree instead of 
traditional full syntactic parsing tree. As a result, 
the traditional features need to be amended 
accordingly. The features we used are as follows: 
? Deprel 
? Word stem 
? POS: if GPOS is available, POS is GPOS. 
Otherwise it is PPOS. 
? Stem of right-most modifier 
? PPOSS of right-most modifier 
? Predicate: the FORM of predicate. 
? PPOSS of predicate 
? Suffix of predicate 
? Voice: voice of predicate 
? Position: The position of the word with re-
spect to its predicate. It has three values, 
?before?, ?is? and ?after?, for the predicate.  
? Deprel path to predicate: DEPREL list 
from word to its predicate through the syn-
tactic dependency path. 
? Length of syntactic dependency path to 
predicate 
? Sense: the sense of predicate 
Moreover, we try to find more features with 
frames. Since the PropBank and NomBank are 
available and all predicates with senses are avail-
able for this subtask. Statistical analysis shows 
that nearly all core semantic arguments (AA, A0, 
A1, A2 ?) of a predicate are described in the 
frame of predicate. But it is incorrect contrarily. 
Based on these observations, we design features 
the following features for five frequently used 
core arguments: 
? A0 is in predicate?s frame: Have two 
values: ?YES? and ?NO?. 
? A1 is in predicate?s frame 
? A2 is in predicate?s frame 
? A3 is in predicate?s frame 
? A4 is in predicate?s frame 
Because the other core semantic arguments are 
rare, we do not need to design features for them. 
With this method, the labeling efficiency is im-
proved while the precision almost keeps un-
changed. 
As the frame information has been used in fea-
tures, we do not add any valency check on the 
labeling result. 
3 Experiments and Analysis 
3.1 Data and Environment 
The data provided for this Closed Challenge of 
shared task is part of TreeBank and Brown cor-
pus. Training set covers sections 02-21 of Tree-
Bank. Development set covers section 24 of 
TreeBank. Wsj test set covers section 23 of 
TreeBank. Brown test set covers sections ck01, 
ck02, and ck03 of the Brown corpus. 
The maximum entropy classier (Berger et al 
1996) used is Le Zhang's Maximum Entropy 
Modeling Toolkit and the L-BFGS parameter 
estimation algorithm with gaussian prior smooth-
ing (Chen and Rosenfeld, 1999). The gaussian 
prior is set to 2 and the iteration count is set to 
500. All results we list here are post-evaluated 
because there are some small modifications. 
The experiments are performed on a PC with 
AMD Athlon? 64 x2 4400+ CPU and 2GB 
main memory running Microsoft Windows XP 
with sp2.  Our system is developed using C++. 
In our experimental analysis, the abbreviations 
used are listed as follows: 
? LAS1: Labeled attachment score 
? UAS: Unlabeled attachment score 
? LAS2: Label accuracy score 
? LP: Labeled precision 
? LR: Labeled recall 
? LF1: Labeled F1 
? UP: Unlabeled precision 
? UR: Unlabeled recall 
? UF1: Unlabeled F1 
265
3.2 Syntactic Dependency Parsing 
We trained two LR models for syntactic depend-
ency parsing. The first LR model uses MaxEnt 
classification to determine possible parser actions 
and their probabilities. The second LR model 
also uses MaxEnt classification, but parsing is 
performed backwards simply by reversing the 
sentence before parsing starts.  
For a sentence, each model can label top-N 
candidates and calculate the probability for every 
result. We join these two models by finding the 
candidate with the highest probability from all 
candidates as the final result for the sentence. 
Table 1 shows the results of each model and joint 
model. We can see that the two LR models ob-
tain similar results. The joint model can obtain 
better result and increase almost one percentage. 
The processing time of joint model is twice more 
than that of the two other models. 
 
 LR Model 
LR-back 
Model 
Joint 
Model
LAS1 83.05 83.38 84.43 
UAS 86.36 86.74 87.74 dev 
LAS2 89.15 89.63 90.08 
LAS1 84.84 84.06 85.48 
UAS 87.60 86.74 88.13 wsj 
LAS2 90.70 90.47 91.21 
LAS1 77.29 76.95 78.91 
UAS 82.75 82.61 84.38 brown 
LAS2 85.00 84.82 85.76 
LAS1 84.00 83.27 84.75 
UAS 87.06 86.28 87.71 
wsj + 
brown LAS2 90.07 89.84 90.6 
Speed (sec/sent) 0.49 0.42 0.92 
Table 1:  Syntactic dependency parsing results 
3.3 Predicate Identification 
Our predicate identification approach is de-
scribed in section 2.2. We use the gold HEAD 
and DEPREL fields to test our approach. The 
results are shown in Table 2. The labeling for 
each sentence spends about 14ms.  
 
 dev wsj brown 
Precision 93.56 93.61 87.51 
Recall 93.24 93.39 89.04 
F1 93.40 93.50 88.27 
Table 2:  Predicate identification results 
3.4 Semantic Dependency Labeling 
Semantic dependency labeling is the last sub-
task. Our DSRL model uses MaxEnt classifica-
tion to determine the semantic dependency be-
tween each word and its corresponding predicate. 
The gold HEAD and DEPREL and PRED fields 
is used to test the model. 
Statistical analysis shows that, for about 99% 
semantic argument labels, the length of syntactic 
dependency path from word to predicate is less 
than 7. So we ignore the words with the length of 
7 or more. 
The final results of semantic dependency la-
beling are shown in Table 3. The labeling for 
each sentence spends about 10ms. 
Brown set is an out-of-domain set and wsj set 
is an in-domain set. Usually, the results on wsj 
are much better than those on brown. But here 
we found that the unlabeled scores are nearly the 
same between wsj and brown. It shows that our 
model performs well at unlabeled labeling on 
out-of-domain set, and should be improved at 
labeled labeling. 
 
 dev wsj brown 
LP 80.50 82.47 77.29 
LR 70.73 73.58 67.16 
LF1 75.30 77.77 71.87 
UP 92.10 92.65 92.87 
UR 80.92 82.65 80.69 
UF1 86.15 87.36 86.35 
Table 3:  Semantic dependency labeling results 
3.5 Overall Result 
As described in section 2, we use a probabilistic 
model to integrate all subtasks. In the probabilis-
tic model, syntactic dependency parsing should 
parse top-N candidate results. We do the rest 
parsing for each candidate result and get N inte-
grated results. Then, for each integrated result, its 
 is calculated and the best one is chose as 
the final result. 
sentP
The DSRL results around verbal predicates 
and nouns on wsj set are shown in Table 4. It 
shows that verbal predicates are labeled much 
better than nouns. 
 
 Unlabeled Predicate
Labeled 
Predicate 
Labeled Semantic 
Arguments 
NN* 87.79 79.52 58.09 
VB* 96.85 80.25 73.77 
Table 4:  The F1 values of DSRL around verbal 
predicates and nouns on wsj  
 
Table 5 shows the overall results with differ-
ent N.  The results are improved when N changes 
from 1 to 2. However, there is nearly no im-
provement by increasing N from 2 to 3. So N is 
set to be 2 in our system. Meanwhile, the effect 
of this approach is not obvious. We find that 
266
there are nearly only one or two different points 
between the top-2 candidate dependency parsing 
results. This leads to that the DSRL results with 
these top-2 candidate results are almost the same. 
This is the probable reason that the approach is 
not much improved with the increase of N. In the 
future it would be necessary for us to consider 
the number of different points when finding the 
top-N dependency results. 
 
 N=1 N=2 N=3 
LP 78.58 78.93 79.01 
LR 75.58 75.52 75.33 
LF1 77.05 77.19 77.13 
UP 86.56 86.95 87.07 
UR 83.04 82.94 82.75 
dev 
UF1 84.76 84.90 84.85 
LP 79.41 79.76 79.96 
LR 76.67 76.59 76.49 
LF1 78.02 78.15 78.19 
UP 86.59 86.92 87.11 
UR 83.40 83.25 83.10 
wsj 
UF1 84.97 85.04 85.06 
LP 70.52 70.95 70.79 
LR 68 67.88 67.54 
LF1 69.24 69.38 69.13 
UP 81.87 82.39 82.28 
UR 78.65 78.47 78.14 
brown 
UF1 80.23 80.39 80.16 
LP 78.45 78.8 78.96 
LR 75.72 75.64 75.5 
LF1 77.06 77.18 77.19 
UP 86.08 86.43 86.59 
UR 82.89 82.73 82.56 
wsj + 
brown 
UF1 84.45 84.54 84.53 
Speed (sec/sent) 0.93 0.94 0.95 
Table 5: Overall macro scores (Wsem = 0.50) 
4 Conclusion 
We divide this shared task into four subtasks: 
syntactic dependency parsing, predicate identifi-
cation, predicate sense labeling and semantic 
dependency labeling. Then, we design a prob-
abilistic model to combine them. The purpose of 
our system is to find a global optimal result for 
every sentence. If a syntactic dependency parsing 
result has the highest probability but it is unrea-
sonable, it would be difficult to get a semantic 
parsing result with high probability again. Hence, 
a more reasonable result may be found with 
lower syntactic dependency parsing probability. 
In our system, we have not distinguished be-
tween nouns and verbal predicates. The experi-
mental results show that the results of verbal 
predicates are much better than those of nouns. 
In the future, it is necessary for us to deal with 
them separately. 
Acknowledgments 
This work was supported by National Natural 
Science Foundation of China (No.60573077, 
No.60775037), Specialized Research Fund for 
the Doctoral Program of Higher Education 
(No.2007105), and Program for New Century 
Excellent Talents in University (No.NCET-05-
0549). 
References 
Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. 
A maximum entropy approach to naturallanguage 
processing. Computational Linguistics, 22(1):39?
71. 
Che Wanxiang, Ting Liu, Sheng Li, Yuxuan Hu, and 
Huaijun Liu. 2005. Semantic role labeling system 
using maximum entropy classifier. In Proceedings 
of Computational Natural Language Learning 
(CoNLL-2005). 
Gildea Daniel and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Duan Xiangyu, Zhao Jun  and Xu Bo. 2007. Probabil-
istic Parsing Action Models for Multi-Lingual De-
pendency Parsing. In Proceedings of the CoNLL 
Shared Task Session of EMNLP-CoNLL 2007. 
Hacioglu K. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of COLING-
2004. 
Johansson R. and Nugues P. 2007. Extended Con-
stituent-to-dependency Conversion for English. In 
Proceedings of NODALIDA 2007. 
Sagae, Kenji  and  Tsujii, Jun'ichi. 2007. Dependency 
Parsing and Domain Adaptation with LR Models 
and Parser Ensembles. In Proceedings of the 
CoNLL Shared Task Session of EMNLP-CoNLL 
2007. 
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models. 
Technical Report CMU-CS-99-108. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings 
of the 12th Conference on Computational Natural 
Language Learning (CoNLL-2008).  
Tsai Tzong-Han, Chia-Wei Wu, Yu-Chun Lin, and 
Wen-Lian Hsu. 2005. Exploiting full parsing in-
formation to label semantic roles using an ensem-
ble of me and svm via integer linear programming. 
In Proceedings of Computational Natural Lan-
guage Learning (CoNLL-2005). 
267
