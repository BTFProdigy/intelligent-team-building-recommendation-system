Chunking Japanese Compound Functional Expressions
by Machine Learning
Masatoshi Tsuchiya? and Takao Shime? and Toshihiro Takagi?
Takehito Utsuro?? and Kiyotaka Uchimoto?? and Suguru Matsuyoshi?
Satoshi Sato?? and Seiichi Nakagawa??
?Computer Center / ??Department of Information and Computer Sciences,
Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN
??Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
??National Institute of Information and Communications Technology,
3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
The Japanese language has various types
of compound functional expressions,
which are very important for recogniz-
ing the syntactic structures of Japanese
sentences and for understanding their
semantic contents. In this paper, we
formalize the task of identifying Japanese
compound functional expressions in a
text as a chunking problem. We apply a
machine learning technique to this task,
where we employ that of Support Vector
Machines (SVMs). We show that the pro-
posed method significantly outperforms
existing Japanese text processing tools.
1 Introduction
As in the case of other languages, the Japanese
language has various types of functional words
such as post-positional particles and auxiliary
verbs. In addition to those functional words,
the Japanese language has much more compound
functional expressions which consist of more than
one words including both content words and func-
tional words. Those single functional words as
well as compound functional expressions are very
important for recognizing the syntactic structures
of Japanese sentences and for understanding their
semantic contents. Recognition and understanding
of them are also very important for various kinds
of NLP applications such as dialogue systems, ma-
chine translation, and question answering. How-
ever, recognition and semantic interpretation of
compound functional expressions are especially
difficult because it often happens that one com-
pound expression may have both a literal (in other
words, compositional) content word usage and
a non-literal (in other words, non-compositional)
functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni)???
(tsuite)?, which consists of a post-positional par-
ticle ?? (ni)?, and a conjugated form ????
(tsuite)? of a verb ??? (tsuku)?. In the sentence
(A), the compound expression functions as a case-
marking particle and has a non-compositional
functional meaning ?about?. On the other hand,
in the sentence (B), the expression simply corre-
sponds to a literal concatenation of the usages of
the constituents: the post-positional particle ??
(ni)? and the verb ???? (tsuite)?, and has a
content word meaning ?follow?. Therefore, when
considering machine translation of those Japanese
sentences into English, it is necessary to precisely
judge the usage of the compound expression ??
(ni)??? (tsuite)?, as shown in the English trans-
lation of the two sentences in Table 1.
There exist widely-used Japanese text process-
ing tools, i.e., pairs of a morphological analysis
tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. How-
ever, they process those compound expressions
only partially, in that their morphological analy-
sis dictionaries list only limited number of com-
pound expressions. Furthermore, even if certain
expressions are listed in a morphological analysis
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman-e.html
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
25
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 2: Classification of Functional Expressions based on Grammatical Function
# of major # of
Grammatical Function Type expressions variants Example
subsequent to predicate 36 67 ????
post-positional / modifying predicate (to-naru-to)
particle subsequent to nominal 45 121 ?????
type / modifying predicate (ni-kakete-ha)
subsequent to predicate, nominal 2 3 ???
/ modifying nominal (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
dictionary, those existing tools often fail in resolv-
ing the ambiguities of their usages, such as those
in Table 1. This is mainly because the frame-
work of those existing tools is not designed so as
to resolve such ambiguities of compound (possi-
bly functional) expressions by carefully consider-
ing the context of those expressions.
Considering such a situation, it is necessary
to develop a tool which properly recognizes and
semantically interprets Japanese compound func-
tional expressions. In this paper, we apply a ma-
chine learning technique to the task of identify-
ing Japanese compound functional expressions in
a text. We formalize this identification task as a
chunking problem. We employ the technique of
Support Vector Machines (SVMs) (Vapnik, 1998)
as the machine learning technique, which has been
successfully applied to various natural language
processing tasks including chunking tasks such
as phrase chunking (Kudo and Matsumoto, 2001)
and named entity chunking (Mayfield et al, 2003).
In the preliminary experimental evaluation, we fo-
cus on 52 expressions that have balanced distribu-
tion of their usages in the newspaper text corpus
and are among the most difficult ones in terms of
their identification in a text. We show that the pro-
posed method significantly outperforms existing
Japanese text processing tools as well as another
tool based on hand-crafted rules. We further show
that, in the proposed SVMs based framework, it is
sufficient to collect and manually annotate about
50 training examples per expression.
2 Japanese Compound Functional
Expressions and their Example
Database
2.1 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) examine
450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their exam-
ple sentences. Compared with those two collec-
tions, Gendaigo Hukugouji Youreishu (National
Language Research Institute, 2001) (henceforth,
denoted as GHY) concentrates on 125 major func-
tional expressions which have non-compositional
usages, as well as their variants5 (337 expressions
in total), and collects example sentences of those
expressions. As a first step of developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants. In this paper, we take
an approach of regarding each of those variants as
a fixed expression, rather than a semi-fixed expres-
sion or a syntactically-flexible expression (Sag et
al., 2002). Then, we focus on evaluating the ef-
fectiveness of straightforwardly applying a stan-
5For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) in-
sertion/deletion/alternation of certain particles, ii) alternation
of synonymous words, iii) normal/honorific/conversational
forms, iv) base/adnominal/negative forms.
26
Table 3: Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ???? ????????????? ????
??????
functional
(to-naru-to) (The situation is serious if it is not effec-
tive against this disease.)
(???? (to-naru-to) = if)
(2) ???? ???????????????
???? ????????
content
(to-naru-to) (They think that it will become a require-
ment for him to be the president.)
(????? (to-naru-to)
= that (something) becomes ?)
(3) ????? ???????? ????? ????
??????????
functional
(ni-kakete-ha) (He has a great talent for earning money.) (?????? (ni-kakete-ha)
= for ?)
(4) ????? ???? ????? ???? content
(ni-kakete-ha) (I do not worry about it.)
( (??)??????
((?)-wo-ki-ni-kakete-ha)
= worry about ?)
(5) ??? ??????? ??? ??????
??
functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ????????????? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this
discussion.)
(???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2, according to their grammat-
ical functions, those 337 expressions in total
are roughly classified into post-positional particle
type, and auxiliary verb type. Functional expres-
sions of post-positional particle type are further
classified into three subtypes: i) those subsequent
to a predicate and modifying a predicate, which
mainly function as conjunctive particles and are
used for constructing subordinate clauses, ii) those
subsequent to a nominal, and modifying a predi-
cate, which mainly function as case-marking parti-
cles, iii) those subsequent to a nominal, and modi-
fying a nominal, which mainly function as adnom-
inal particles and are used for constructing adnom-
inal clauses. For each of those types, Table 2 also
shows the number of major expressions as well as
that of their variants listed in GHY, and an exam-
ple expression. Furthermore, Table 3 gives exam-
ple sentences of those example expressions as well
as the description of their usages.
2.2 Issues on Identifying Compound
Functional Expressions in a Text
The task of identifying Japanese compound func-
tional expressions roughly consists of detecting
candidates of compound functional expressions in
a text and of judging the usages of those can-
didate expressions. The class of Japanese com-
pound functional expressions can be regarded as
closed and their number is at most a few thousand.
27
Table 4: Examples of Detecting more than one Candidate Expression
Expression Example sentence (English translation) Usage
(9) ??? ????? ??? ???????? functional
(to-iu) (That?s why a match is not so easy.) (NP1??? (to-iu)NP2
= NP
2
called as NP
1
)
(10) ?????? ??? ?????? ???????? functional
(to-iu-mono-no) (Although he won, the score is bad.)
(???????
(to-iu-mono-no)
= although ?)
Therefore, it is easy to enumerate all the com-
pound functional expressions and their morpheme
sequences. Then, in the process of detecting can-
didates of compound functional expressions in a
text, the text are matched against the morpheme
sequences of the compound functional expressions
considered.
Here, most of the 125 major functional expres-
sions we consider in this paper are compound ex-
pressions which consist of one or more content
words as well as functional words. As we intro-
duced with the examples of Table 1, it is often
the case that they have both a compositional con-
tent word usage as well as a non-compositional
functional usage. For example, in Table 3, the
expression ????? (to-naru-to)? in the sen-
tence (2) has the meaning ? that (something) be-
comes ??, which corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ???, the verb ????, and the
post-positional particle ???, and can be regarded
as a content word usage. On the other hand, in
the case of the sentence (1), the expression ???
?? (to-naru-to)? has a non-compositional func-
tional meaning ?if?. Based on this discussion, we
classify the usages of those expressions into two
classes: functional and content. Here, functional
usages include both non-compositional and com-
positional functional usages, although most of the
functional usages of those 125 major expressions
can be regarded as non-compositional. On the
other hand, content usages include compositional
content word usages only.
More practically, in the process of detecting
candidates of compound functional expressions in
a text, it can happen that more than one can-
didate expression is detected. For example, in
Table 4, both of the candidate compound func-
tional expressions ???? (to-iu)? and ????
??? (to-iu-mono-no)? are detected in the sen-
tence (9). This is because the sequence of the two
morphemes ?? (to)? and ??? (iu)? constituting
the candidate expression ???? (to-iu)? is a sub-
sequence of the four morphemes constituting the
candidate expression ??????? (to-iu-mono-
no)? as below:
Morpheme sequence
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression??? (to-iu)
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression?????? (to-iu-mono-no)
? (to) ?? (iu) ?? (mono) ? (no)
This is also the case with the sentence (10).
Here, however, as indicated in Table 4, the sen-
tence (9) is an example of the functional usage of
the compound functional expression ???? (to-
iu)?, where the sequence of the two morphemes ?
? (to)? and ??? (iu)? should be identified and
chunked into a compound functional expression.
On the other hand, the sentence (10) is an ex-
ample of the functional usage of the compound
functional expression ??????? (to-iu-mono-
no)?, where the sequence of the four morphemes ?
? (to)?, ??? (iu)?, ??? (mono)?, and ?? (no)?
should be identified and chunked into a compound
functional expression. Actually, in the result of
our preliminary corpus study, at least in about 20%
of the occurrences of Japanese compound func-
tional expressions, more than one candidate ex-
pression can be detected. This result indicates that
it is necessary to consider more than one candidate
expression in the task of identifying a Japanese
compound functional expression, and also in the
task of classifying the functional/content usage of
a candidate expression. Thus, in this paper, based
on this observation, we formalize the task of iden-
tifying Japanese compound functional expressions
as a chunking problem, rather than a classification
problem.
28
Table 5: Number of Sentences collected from
1995 Mainichi Newspaper Texts (for 337 Expres-
sions)
# of expressions
50 ? # of sentences 187 (55%)
0 < # of sentences < 50 117 (35%)
# of sentences = 0 33 (10%)
2.3 Developing an Example Database
We developed an example database of Japanese
compound functional expressions, which is used
for training/testing a chunker of Japanese com-
pound functional expressions (Tsuchiya et al,
2005). The corpus from which we collect example
sentences is 1995 Mainichi newspaper text corpus
(1,294,794 sentences, 47,355,330 bytes). For each
of the 337 expressions, 50 sentences are collected
and chunk labels are annotated according to the
following procedure.
1. The expression is morphologically analyzed
by ChaSen, and its morpheme sequence6 is
obtained.
2. The corpus is morphologically analyzed by
ChaSen, and 50 sentences which include the
morpheme sequence of the expression are
collected.
3. For each sentence, every occurrence of the
337 expressions is annotated with one of the
usages functional/content by an annotator7.
Table 5 classifies the 337 expressions accord-
ing to the number of sentences collected from the
1995 Mainichi newspaper text corpus. For more
than half of the 337 expressions, more than 50 sen-
tences are collected, although about 10% of the
377 expressions do not appear in the whole cor-
pus. Out of those 187 expressions with more than
50 sentences, 52 are those with balanced distribu-
tion of the functional/content usages in the news-
paper text corpus. Those 52 expressions can be re-
garded as among the most difficult ones in the task
of identifying and classifying functional/content
6For those expressions whose constituent has conjugation
and the conjugated form also has the same usage as the ex-
pression with the original form, the morpheme sequence is
expanded so that the expanded morpheme sequences include
those with conjugated forms.
7For the most frequent 184 expressions, on the average,
the agreement rate between two human annotators is 0.93 and
the Kappa value is 0.73, which means allowing tentative con-
clusions to be drawn (Carletta, 1996; Ng et al, 1999). For
65% of the 184 expressions, the Kappa value is above 0.8,
which means good reliability.
usages. Thus, this paper focuses on those 52 ex-
pressions in the training/testing of chunking com-
pound functional expressions. We extract 2,600
sentences (= 52 expressions ? 50 sentences) from
the whole example database and use them for
training/testing the chunker. The number of the
morphemes for the 2,600 sentences is 92,899. We
ignore the chunk labels for the expressions other
than the 52 expressions, resulting in 2,482/701
chunk labels for the functional/content usages, re-
spectively.
3 Chunking Japanese Compound
Functional Expressions with SVMs
3.1 Support Vector Machines
The principle idea of SVMs is to find a separate
hyperplane that maximizes the margin between
two classes (Vapnik, 1998). If the classes are not
separated by a hyperplane in the original input
space, the samples are transformed in a higher di-
mensional features space.
Giving x is the context (a set of features) of
an input example; xi and yi(i = 1, ..., l, xi ?
Rn, yi?{1,?1}) indicate the context of the train-
ing data and its category, respectively; The deci-
sion function f in SVM framework is defined as:
f(x) = sgn
( l
?
i=1
?iyiK(xi,x) + b
)
(1)
where K is a kernel function, b ? R is a thresh-
old, and ?i are weights. Besides, the weights ?i
satisfy the following constraints:
0 ? ?i ? C (i = 1, ..., l) (2)
?l
i=1 ?iyi = 0 (3)
where C is a misclassification cost. The xi with
non-zero ?i are called support vectors. To train
an SVM is to find the ?i and the b by solving the
optimization problem; maximizing the following
under the constraints of (2) and (3):
L(?) =
l
?
i=1
?i?
1
2
l
?
i,j=1
?i?jyiyjK(x
i
,x
j
) (4)
The kernel function K is used to transform the
samples in a higher dimensional features space.
Among many kinds of kernel functions available,
we focus on the d-th polynomial kernel:
K(x,y) = (x ? y + 1)d (5)
29
Through experimental evaluation on chunking
Japanese compound functional expressions, we
compared polynomial kernels with d = 1, 2, and
3. Kernels with d = 2 and 3 perform best, while
the kernel with d = 3 requires much more compu-
tational cost than that with d = 2. Thus, through-
out the paper, we show results with the quadratic
kernel (d = 2).
3.2 Chunking with SVMs
This section describes details of formalizing the
chunking task using SVMs. In this paper, we use
an SVMs-based chunking tool YamCha8 (Kudo
and Matsumoto, 2001). In the SVMs-based
chunking framework, SVMs are used as classi-
fiers for assigning labels for representing chunks
to each token. In our task of chunking Japanese
compound functional expressions, each sentence
is represented as a sequence of morphemes, where
a morpheme is regarded as a token.
3.2.1 Chunk Representation
For representing proper chunks, we employ
IOB2 representation, one of those which have
been studied well in various chunking tasks of nat-
ural language processing (Tjong Kim Sang, 1999;
Kudo and Matsumoto, 2001). This method uses
the following set of three labels for representing
proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
As we described in section 2.2, given a candi-
date expression, we classify the usages of the ex-
pression into two classes: functional and content.
Accordingly, we distinguish the chunks of the two
types: the functional type chunk and the content
type chunk. In total, we have the following five la-
bels for representing those chunks: B-functional,
I-functional, B-content, I-content, and O. Ta-
ble 6 gives examples of those chunk labels rep-
resenting chunks.
Finally, as for exending SVMs to multi-class
classifiers, we experimentally compare the pair-
wise method and the one vs. rest method, where
the pairwise method slightly outperformed the one
vs. rest method. Throughout the paper, we show
results with the pairwise method.
8http://chasen.org/?taku/software/
yamcha/
3.2.2 Features
For the feature sets for training/testing of
SVMs, we use the information available in the sur-
rounding context, such as the morphemes, their
parts-of-speech tags, as well as the chunk labels.
More precisely, suppose that we identify the chunk
label ci for the i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, mi is the morpheme appearing at i-th po-
sition, Fi is the feature set at i-th position, and ci
is the chunk label for i-th morpheme. Roughly
speaking, when identifying the chunk label ci for
the i-th morpheme, we use the feature sets Fi?2,
Fi?1, Fi, Fi+1, Fi+2 at the positions i ? 2, i ? 1,
i, i + 1, i + 2, as well as the preceding two chunk
labels ci?2 and ci?1.
The detailed definition of the feature set Fi at i-
th position is given below. The feature set Fi is de-
fined as a tuple of the morpheme feature MF (mi)
of the i-th morpheme mi, the chunk candidate fea-
ture CF (i) at i-th position, and the chunk context
feature OF (i) at i-th position.
Fi = ? MF (mi), CF (i), OF (i) ?
The morpheme feature MF (mi) consists of the
lexical form, part-of-speech, conjugation type and
form, base form, and pronunciation of mi.
The chunk candidate feature CF (i) and the
chunk context feature OF (i) are defined consid-
ering the candidate compound functional expres-
sion, which is a sequence of morphemes includ-
ing the morpheme mi at the current position i. As
we described in section 2, the class of Japanese
compound functional expressions can be regarded
as closed and their number is at most a few thou-
sand. Therefore, it is easy to enumerate all the
compound functional expressions and their mor-
pheme sequences. Chunk labels other than O
should be assigned to a morpheme only when it
constitutes at least one of those enumerated com-
pound functional expressions. Suppose that a se-
quence of morphemes mj . . . mi . . . mk including
mi at the current position i constitutes a candidate
functional expression E as below:
m
j?2
m
j?1
m
j
. . . m
i
. . . m
k
m
k+1
m
k+2
candidate E of
a compound
functional expression
where the morphemes mj?2, mj?1, mk+1, and
mk+2 are at immediate left/right contexts of E.
Then, the chunk candidate feature CF (i) at i-th
position is defined as a tuple of the number of mor-
phemes constituting E and the position of mi in
E. The chunk context feature OF (i) at i-th posi-
tion is defined as a tuple of the morpheme features
30
Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features
(a) Sentence (7) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
?? (giron) (discussion) O ? ?
? (ga) (NOM) O ? ?
???(owatt) (finish) O ? ?
?? (tara) (after) O ? ?
?? (kyuukei) (break) O ? ?
? (shi) (have) O ? ?
? (te) (may) B-functional ?2, 1? ? MF (?? (kyuukei)), ?, MF (? (shi)), ?,
?? (ii) I-functional ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
(b) Sentence (8) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
??? (bag) (discussion) O ? ?
? (ha) (TOP) O ? ?
??? (ookiku) (big) O ? ?
? (te) (because) B-content ?2, 1? ? MF (? (ha)), ?, MF (??? (ookiku)), ?,
?? (ii) (nice) I-content ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
as well as the chunk candidate features at immedi-
ate left/right contexts of E.
CF (i) = ? length of E, position of m
i
in E ?
OF (i) = ? MF (m
j?2
), CF (j ? 2),
MF (m
j?1
), CF (j ? 1),
MF (m
k+1
), CF (k + 1),
MF (m
k+2
), CF (k + 2) ?
Table 6 gives examples of chunk candidate fea-
tures and chunk context features
It can happen that the morpheme at the cur-
rent position i constitutes more than one candidate
compound functional expression. For example,
in the example below, the morpheme sequences
mi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-
tute candidate expressions E
1
, E
2
, and E
3
, respec-
tively.
Morpheme sequence m
i?1
m
i
m
i+1
m
i+2
Candidate E
1
m
i?1
m
i
m
i+1
Candidate E
2
m
i?1
m
i
Candidate E
3
m
i
m
i+1
m
i+2
In such cases, we prefer the one starting with the
leftmost morpheme. If more than one candidate
expression starts with the leftmost morpheme, we
prefer the longest one. In the example above, we
prefer the candidate E
1
and construct the chunk
candidate features and chunk context features con-
sidering E
1
only.
4 Experimental Evaluation
The detail of the data set we use in the experimen-
tal evaluation was presented in section 2.3. As we
show in Table 7, performance of our SVMs-based
chunkers as well as several baselines including ex-
isting Japanese text processing tools is evaluated
in terms of precision/recall/F?=1 of identifying
functional chunks. Performance is evaluated also
in terms of accuracy of classifying detected can-
didate expressions into functional/content chunks.
Among those baselines, ?majority ( = functional)?
always assigns functional usage to the detected
candidate expressions. ?Hand-crafted rules? are
manually created 145 rules each of which has con-
ditions on morphemes constituting a compound
functional expression as well as those at immedi-
ate left/right contexts. Performance of our SVMs-
based chunkers is measured through 10-fold cross
validation.
As shown in Table 7, our SVMs-based chunkers
significantly outperform those baselines both in
F?=1 and classification accuracy9. We also evalu-
ate the effectiveness of each feature set, i.e., the
morpheme feature, the chunk candidate feature,
and the chunk context feature. The results in the
table show that the chunker with the chunk candi-
date feature performs almost best even without the
chunk context feature10.
9Recall of existing Japanese text processing tools is low,
because those tools can process only 50?60% of the whole
52 compound functional expressions, and for the remaining
40?50% expressions, they fail in identifying all of the occur-
rences of functional usages.
10It is also worthwhile to note that training the SVMs-
based chunker with the full set of features requires computa-
tional cost three times as much as training without the chunk
31
Table 7: Evaluation Results (%)
Identifying Acc. of classifying
functional chunks functional/content
Prec. Rec. F?=1 chunks
majority ( = functional) 78.0 100 87.6 78.0
Baselines Juman/KNP 89.2 49.3 63.5 55.8
ChaSen/CaboCha 89.0 45.6 60.3 53.2
hand-crafted rules 90.7 81.6 85.9 79.1
SVM morpheme 88.0 91.0 89.4 86.5
(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0
set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2
Figure 1: Change of F?=1 with Different Number
of Training Instances
For the SVMs-based chunker with the chunk
candidate feature with/without the chunk context
feature, Figure 1 plots the change of F?=1 when
training with different number of labeled chunks
as training instances. With this result, the increase
in F?=1 seems to stop with the maximum num-
ber of training instances, which supports the claim
that it is sufficient to collect and manually annotate
about 50 training examples per expression.
5 Concluding Remarks
The Japanese language has various types of com-
pound functional expressions, which are very im-
portant for recognizing the syntactic structures of
Japanese sentences and for understanding their se-
mantic contents. In this paper, we formalized
the task of identifying Japanese compound func-
tional expressions in a text as a chunking prob-
lem. We applied a machine learning technique
to this task, where we employed that of Sup-
port Vector Machines (SVMs). We showed that
the proposed method significantly outperforms ex-
isting Japanese text processing tools. The pro-
context feature.
posed framework has advantages over an approach
based on manually created rules such as the one in
(Shudo et al, 2004), in that it requires human cost
to manually create and maintain those rules. On
the other hand, in our framework based on the ma-
chine learning technique, it is sufficient to collect
and manually annotate about 50 training examples
per expression.
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistic. Computational Linguistics,
22(2):249?254.
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten.
Kuroshio Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support
vector machines. In Proc. 2nd NAACL, pages 192?199.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named entity
recognition using hundreds of thousands of features. In
Proc. 7th CoNLL, pages 184?187.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo
Hukugouji Youreishu. (in Japanese).
H. T. Ng, C. Y. Lim, and S. K. Foo. 1999. A case study on
inter-annotator agreement for word sense disambiguation.
In Proc. ACL SIGLEXWorkshop on Standardizing Lexical
Resources, pages 9?13.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc.
2nd ACL Workshop on Multiword Expressions: Integrat-
ing Processing, pages 32?39.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. 9th EACL, pages 173?179.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2005. A corpus for classifying usages of Japanese
compound functional expressions. In Proc. PACLING,
pages 345?350.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
32
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 65?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Dependency Relations of
Japanese Compound Functional Expressions
Takehito Utsuro? and Takao Shime? and Masatoshi Tsuchiya??
Suguru Matsuyoshi?? and Satoshi Sato??
?Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
?NEC Corporation
??Computer Center, Toyohashi University of Technology,
Tenpaku-cho, Toyohashi, 441?8580, JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
This paper proposes an approach of process-
ing Japanese compound functional expressions
by identifying them and analyzing their depen-
dency relations through a machine learning tech-
nique. First, we formalize the task of identify-
ing Japanese compound functional expressions
in a text as a machine learning based chunking
problem. Next, against the results of identify-
ing compound functional expressions, we apply
the method of dependency analysis based on the
cascaded chunking model. The results of ex-
perimental evaluation show that, the dependency
analysis model achieves improvements when ap-
plied after identifying compound functional ex-
pressions, compared with the case where it is ap-
plied without identifying compound functional
expressions.
1 Introduction
In addition to single functional words, the Japanese
language has many more compound functional ex-
pressions which consist of more than one word in-
cluding both content words and functional words.
They are very important for recognizing syntactic
structures of Japanese sentences and for understand-
ing their semantic content. Recognition and under-
standing of them are also very important for vari-
ous kinds of NLP applications such as dialogue sys-
tems, machine translation, and question answering.
However, recognition and semantic interpretation of
compound functional expressions are especially dif-
ficult because it often happens that one compound
expression may have both a literal (i.e. compo-
sitional) content word usage and a non-literal (i.e.
non-compositional) functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni) ???
(tsuite)?, which consists of a post-positional particle
?? (ni)?, and a conjugated form ???? (tsuite)? of
a verb ??? (tsuku)?. In the sentence (A), the com-
pound expression functions as a case-marking parti-
cle and has a non-compositional functional meaning
?about?. On the other hand, in the sentence (B), the
expression simply corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ?? (ni)? and the verb ????
(tsuite)?, and has a content word meaning ?follow?.
Therefore, when considering machine translation of
these Japanese sentences into English, it is neces-
sary to judge precisely the usage of the compound
expression ?? (ni)??? (tsuite)?, as shown in the
English translation of the two sentences in Table 1.
There exist widely-used Japanese text processing
tools, i.e. combinations of a morphological analy-
sis tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. However,
they process those compound expressions only par-
tially, in that their morphological analysis dictionar-
ies list only a limited number of compound expres-
sions. Furthermore, even if certain expressions are
listed in a morphological analysis dictionary, those
existing tools often fail in resolving the ambigui-
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
65
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
Correct English Translation:
( As a means of solving the problem, USA recommended the activity of OSCE in which Russia participates.)
(1) Correct Dependency Relation by Identifying Compound Functional Expression: ??????
with a Case Marking Particle Usage.
(2)  Incorrect Dependency Relation without Identifying Compound Functional Expression: ??????,
which Literally Consists of a Post-positional Particle ??? (with) and a Conjugation Form ????
of a Verb ???? (do).
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP as a means for solution       Russia-NOM also             participate in                                of  OSCE activity-ACC          recommended
??? ???? ? ??? ??? ? ?? ?? ?? ????????? ??? ?????
USA-TOP with a means for Russia-NOM also             participate in                            of  OSCE activity-ACC       recommended
solution
Figure 1: Example of Improving Dependency Analysis of Compound Functional Expressions by Identifying
them before Dependency Analysis
ties of their usages, such as those in Table 1. This
is mainly because the framework of these existing
tools is not designed so as to resolve such ambigu-
ities of compound (possibly functional) expressions
by carefully considering the context of those expres-
sions.
Actually, as we introduce in the next section, as a
first step towards studying computational processing
of compound functional expressions, we start with
125 major functional expressions which have non-
compositional usages, as well as their variants (337
expressions in total). Out of those 337 expressions,
111 have both a content word usage and a functional
usage. However, the combination of JUMAN+KNP
is capable of distinguishing the two usages only for
43 of the 111 expressions, and the combination of
ChaSen+CaboCha only for 40 of those 111 expres-
sions. Furthermore, the failure in distinguishing the
two usages may cause errors of syntactic analysis.
For example, (1) of Figure 1 gives an example of
identifying a correct modifiee of the second bunsetsu
segment 5 ???????? (as a means for solu-
tion)? including a Japanese compound functional ex-
pression ???? (as)?, by appropriately detecting
the compound functional expression before depen-
dency analysis. On the other hand, (2) of Figure 1
gives an example of incorrectly indicating an erro-
neous modifiee of the third bunsetsu ????, which
actually happens if we do not identify the compound
functional expression ???? (as)? before depen-
dency analysis of this sentence.
Considering such a situation, it is necessary to
develop a tool which properly recognizes and se-
mantically interprets Japanese compound functional
expressions. This paper proposes an approach of
processing Japanese compound functional expres-
sions by identifying them and analyzing their de-
pendency relations through a machine learning tech-
nique. The overall flow of processing compound
functional expressions in a Japanese sentence is il-
5A Japanese bunsetsu segment is a phrasal unit which con-
sits of at least one content word and zero or more functional
words.
66
( As a means of solving the 
problem, USA recommended the 
activity of OSCE in which Russia 
participates.)
???????????
????????????
??????????
?????
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
??
(solution)
??
(means)
???
(as)
? ? ?
? ? ?
morphological 
analysis
by ChaSen
??
(solution)
??
(means)
?
(with)
?
(do)
?
(and)
? ? ?
? ? ?
compound
functional 
expression
Identifying
compound
functional
expression
chunking
bunsetsu
segmentation
&
dependency
analysis
bunsetsu
segment
dependency
relation
Figure 2: Overall Flow of Processing Compound Functional Expressions in a Japanese Sentence
lustrated in Figure 2. First of all, we assume a
sequence of morphemes obtained by a variant of
ChaSen with all the compound functional expres-
sions removed from its outputs, as an input to our
procedure of identifying compound functional ex-
pressions and analyzing their dependency relations.
We formalize the task of identifying Japanese com-
pound functional expressions in a text as a machine
learning based chunking problem (Tsuchiya et al,
2006). We employ the technique of Support Vec-
tor Machines (SVMs) (Vapnik, 1998) as the ma-
chine learning technique, which has been success-
fully applied to various natural language process-
ing tasks including chunking tasks such as phrase
chunking and named entity chunking. Next, against
the results of identifying compound functional ex-
pressions, we apply the method of dependency anal-
ysis based on the cascaded chunking model (Kudo
and Matsumoto, 2002), which is simple and efficient
because it parses a sentence deterministically only
deciding whether the current bunsetsu segment mod-
ifies the one on its immediate right hand side. As
we showed in Figure 1, identifying compound func-
tional expressions before analyzing dependencies in
a sentence does actually help deciding dependency
relations of compound functional expressions.
In the experimental evaluation, we focus on 59
expressions having balanced distribution of their us-
ages in the newspaper text corpus and are among the
most difficult ones in terms of their identification in
a text. We first show that the proposed method of
chunking compound functional expressions signifi-
cantly outperforms existing Japanese text processing
tools. Next, we further show that the dependency
analysis model of (Kudo and Matsumoto, 2002) ap-
plied to the results of identifying compound func-
tional expressions significantly outperforms the one
applied to the results without identifying compound
functional expressions.
2 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) exam-
ine 450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their example
sentences. Compared with those two collections,
Gendaigo Hukugouji Youreishu (National Language
Research Institute, 2001) (henceforth, denoted as
GHY) concentrates on 125 major functional expres-
sions which have non-compositional usages, as well
as their variants6, and collects example sentences of
those expressions. As we mentioned in the previous
section, as a first step towards developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants (337 expressions in to-
6For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) inser-
tion/deletion/alternation of certain particles, ii) alternation of
synonymous words, iii) normal/honorific/conversational forms,
iv) base/adnominal/negative forms.
67
(a) Classification of Compound Functional Expressions based on Grammatical Function
Grammatical Function Type # of major expressions # of variants Example
post-positional conjunctive particle 36 67 ??? (kuse-ni)
particle type case-marking particle 45 121 ??? (to-shite)
adnominal particle 2 3 ??? (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
(b) Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ??? ??????? ??? ???????????????? functional
(kuse-ni) (To my brother, (someone) gave money, while (he/she) did noth-
ing to me but just sent a letter.)
(??? (kuse-ni) = while)
(2) ??? ???? ??? ??????? content
(kuse-ni) (They all were surprised by his habit.) (???? (kuse-ni)
= by one?s habit
(3) ??? ?????????? ??? ??????? functional
(to-shite) (He is known as an expert of the problem.) (???? (to-shite)
= as ?)
(4) ??? ?????????????? ??? ???? content
(to-shite) (Please make it clear whether this is true or not.) (?? ???? (to-shite)
= make ? ?
(5) ??? ??????? ??? ???????? functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ?????????? ??? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this discussion.) (???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
Table 2: Classification and Example Usages of Compound Functional Expressions
tal). In this paper, following (Sag et al, 2002), we
regard each variant as a fixed expression, rather than
a semi-fixed expression or a syntactically-flexible
expression 7. Then, we focus on evaluating the
effectiveness of straightforwardly applying a stan-
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2 (a), according to their grammat-
ical functions, those 337 expressions in total are
roughly classified into post-positional particle type,
and auxiliary verb type. Functional expressions of
post-positional particle type are further classified
into three subtypes: i) conjunctive particle types,
which are used for constructing subordinate clauses,
ii) case-marking particle types, iii) adnominal parti-
cle types, which are used for constructing adnominal
7Compound functional expressions of auxiliary verb types
can be regarded as syntactically-flexible expressions.
clauses. Furthermore, for examples of compound
functional expressions listed in Table 2 (a), Table 2
(b) gives their example sentences as well as the de-
scription of their usages.
3 Identifying Compound Functional
Expressions by Chunking with SVMs
This section describes summaries of formalizing the
chunking task using SVMs (Tsuchiya et al, 2006).
In this paper, we use an SVMs-based chunking tool
YamCha8 (Kudo and Matsumoto, 2001). In the
SVMs-based chunking framework, SVMs are used
as classifiers for assigning labels for representing
chunks to each token. In our task of chunking
Japanese compound functional expressions, each
8http://chasen.org/?taku/software/
yamcha/
68
sentence is represented as a sequence of morphemes,
where a morpheme is regarded as a token.
3.1 Chunk Representation
For representing proper chunks, we employ IOB2
representation, which has been studied well in var-
ious chunking tasks of natural language processing.
This method uses the following set of three labels
for representing proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
Given a candidate expression, we classify the us-
ages of the expression into two classes: functional
and content. Accordingly, we distinguish the chunks
of the two types: the functional type chunk and the
content type chunk. In total, we have the follow-
ing five labels for representing those chunks: B-
functional, I-functional, B-content, I-content, and
O. Finally, as for extending SVMs to multi-class
classifiers, we experimentally compare the pairwise
method and the one vs. rest method, where the pair-
wise method slightly outperformed the one vs. rest
method. Throughout the paper, we show results with
the pairwise method.
3.2 Features
For the feature sets for training/testing of SVMs, we
use the information available in the surrounding con-
text, such as the morphemes, their parts-of-speech
tags, as well as the chunk labels. More precisely,
suppose that we identify the chunk label c
i
for the
i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, m
i
is the morpheme appearing at i-th posi-
tion, F
i
is the feature set at i-th position, and c
i
is
the chunk label for i-th morpheme. Roughly speak-
ing, when identifying the chunk label c
i
for the i-th
morpheme, we use the feature sets F
i?2
, F
i?1
, F
i
,
F
i+1
, F
i+2
at the positions i ? 2, i ? 1, i, i + 1,
i+2, as well as the preceding two chunk labels c
i?2
and c
i?1
. The detailed definition of the feature set
F
i
at i-th position is given in (Tsuchiya et al, 2006),
which mainly consists of morphemes as well as in-
formation on the candidate compound functional ex-
pression at i-th position.
4 Learning Dependency Relations of
Japanese Compound Functional
Expressions
4.1 Japanese Dependency Analysis using
Cascaded Chunking
4.1.1 Cascaded Chunking Model
First of all, we define a Japanese sen-
tence as a sequence of bunsetsu segments
B = ?b
1
, b
2
, . . . , b
m
? and its syntactic struc-
ture as a sequence of dependency patterns
D = ?Dep(1), Dep(2), . . . , Dep(m ? 1)?, where
Dep(i) = j means that the bunsetsu segment b
i
depends on (modifies) bunsetsu segment b
j
. In
this framework, we assume that the dependency
sequence D satisfies the following two constraints:
1. Japanese is a head-final language. Thus, except
for the rightmost one, each bunsetsu segment
modifies exactly one bunsetsu segment among
those appearing to its right.
2. Dependencies do not cross one another.
Unlike probabilistic dependency analysis models
of Japanese, the cascaded chunking model of Kudo
and Matsumoto (2002) does not require the proba-
bilities of dependencies and parses a sentence de-
terministically. Since Japanese is a head-final lan-
guage, and the chunking can be regarded as the cre-
ation of a dependency between two bunsetsu seg-
ments, this model simplifies the process of Japanese
dependency analysis as follows: 9
1. Put an O tag on all bunsetsu segments. The O
tag indicates that the dependency relation of the
current segment is undecided.
2. For each bunsetsu segment with an O tag, de-
cide whether it modifies the bunsetsu segment
on its immediate right hand side. If so, the O
tag is replaced with a D tag.
3. Delete all bunsetsu segments with a D tag that
immediately follows a bunsetsu segment with
an O tag.
9The O and D tags used in this section have no relation to
those chunk reppresentation tags introduced in section 3.1.
69
Initialization
?? ??? ??? ??? ?????
( He was moved by her warm heart. )
He her warm heart be moved
Input:
Tag:
?? ??? ??? ??? ?????
O O O O O
Input:
Tag:
?? ??? ??? ??? ?????
O O D D O
Deleted
Input:
Tag:
?? ??? ??? ?????
O D D O
Deleted
Input:
Tag:
?? ??? ?????
O D O
Input:
Tag:
?? ?????
O
Deleted
Input:
Tag:
?????
O
Finish
D
Deleted
Figure 3: Example of the Parsing Process with Cas-
caded Chunking Model
4. Terminate the algorithm if a single bunsetsu
segment remains, otherwise return to the step
2 and repeat.
Figure 3 shows an example of the parsing process
with the cascaded chunking model.
4.1.2 Features
As a Japanese dependency analyzer based on the
cascaded chunking model, we use the publicly avail-
able version of CaboCha (Kudo and Matsumoto,
2002), which is trained with the manually parsed
sentences of Kyoto text corpus (Kurohashi and Na-
gao, 1998), that are 38,400 sentences selected from
the 1995 Mainichi newspaper text.
The standard feature set used by CaboCha con-
sists of static features and dynamic features. Static
features are those solely defined once the pair
of modifier/modifiee bunsetsu segments is speci-
fied. For the pair of modifier/modifiee bunsetsu
segments, the following are used as static fea-
tures: head words and their parts-of-speech tags,
inflection-types/forms, functional words and their
parts-of-speech tags, inflection-types/forms, inflec-
tion forms of the words that appear at the end
of bunsetsu segments. As for features between
modifier/modifiee bunsetsu segments, the distance
of modifier/modifiee bunsetsu segments, existence
of case-particles, brackets, quotation-marks, and
punctuation-marks are used as static features. On the
other hand, dynamic features are created during the
parsing process, so that, when a certain dependency
relation is determined, it can have some influence
on other dependency relations. Dynamic features in-
clude bunsetsu segments modifying the current can-
didate modifiee (see Kudo and Matsumoto (2002)
for the details).
4.2 Coping with Compound Functional
Expressions
As we show in Figure 2, a compound functional ex-
pression is identified as a sequence of several mor-
phemes and then chunked into one morpheme. The
result of this identification process is then trans-
formed into the sequence of bunsetsu segments. Fi-
nally, to this modified sequence of bunsetsu seg-
ments, the method of dependency analysis based on
the cascaded chunking model is applied.
Here, when chunking a sequence of several mor-
phemes constituting a compound functional expres-
sion, the following two cases may exist:
(A) As in the case of the example (A) in Table 1, the
two morphemes constituting a compound func-
tional expression ?? (ni)??? (tsuite)? over-
laps the boundary of two bunsetsu segments.
In such a case, when chunking the two mor-
phemes into one morpheme corresponding to
a compound functional expression, those two
bunsetsu segments are concatenated into one
bunsetsu segment.
? ?
kare ni
(he)
???
tsuite
=?
? ????
kare ni-tsuite
(he) (about)
(B) As we show below, a compound functional ex-
pression ??? (koto)? (ga)?? (aru)? over-
laps the boundary of two bunsetsu segments,
though the two bunsetsu segments concatenat-
ing into one bunsetsu segment does include no
content words. In such a case, its immedi-
ate left bunsetsu segment (???(itt)? (ta)? in
the example below), which corresponds to the
content word part of ??? (koto)? (ga)??
(aru)?, has to be concatenated into the bunsetsu
segment ??? (koto)? (ga)?? (aru)?.
70
?? ?
itt ta
(went)
?? ?
koto ga
??
aru
=?
?? ? ?????
itt ta koto-ga-aru
(have been ?)
Next, to the compound functional expression, we
assign one of the four grammatical function types
listed in Table 2 as its POS tag. For example,
the compound functional expression ?? (ni)???
(tsuite)? in (A) above is assigned the grammatical
function type ?case-marking particle type?, while ?
?? (koto) ? (ga) ?? (aru)? in (B) is assigned
?auxiliary verb type?.
These modifications cause differences in the final
feature representations. For example, let us compare
the feature representations of the modifier bunsetsu
segments in (1) and (2) of Figure 1. In (1), the mod-
ifier bunsetsu segment is ????????? which
has the compound functional expression ?????
in its functional word part. On the other hand, in
(2), the modifier bunsetsu segment is ????, which
corresponds to the literal verb usage of a part of the
compound functional expression ?????. In the
final feature representations below, this causes the
following differences in head words and functional
words / POS of the modifier bunsetsu segments:
(1) of Figure 1 (2) of Figure 1
head word ?? (means) ?? (do)
functional word ??? (as) ? (and)
POS subsequent to nominal conjunctive
/ modifying predicate particle
5 Experimental Evaluation
5.1 Training/Test Data Sets
For the training of chunking compound functional
expressions, we collected 2,429 example sentences
from the 1995 Mainichi newspaper text corpus. For
each of the 59 compound functional expressions for
evaluation mentioned in section 1, at least 50 ex-
amples are included in this training set. For the
testing of chunking compound functional expres-
sions, as well as training/testing of learning depen-
dencies of compound functional expressions, we
used manually-parsed sentences of Kyoto text cor-
pus (Kurohashi and Nagao, 1998), that are 38,400
sentences selected from the 1995 Mainichi newspa-
per text (the 2,429 sentences above are selected so
that they are exclusive of the 37,400 sentences of
Kyoto text corpus.). To those data sets, we manually
annotate usage labels of the 59 compound functional
expressions (details in Table 3).
Usages # of
functional content total sentences
for chunker
training 1918 1165 3083 2429
Kyoto text corpus 5744 1959 7703 38400
Table 3: Statistics of Data Sets
Identifying
functional chunks
Acc. of
classifying
functional /
content
Prec. Rec. F
?=1
chunks
majority ( = functional) 74.6 100 85.5 74.6
Juman/KNP 85.8 40.5 55.0 58.4
ChaSen/CaboCha 85.2 26.7 40.6 51.1
SVM 91.4 94.6 92.9 89.3
Table 4: Evaluation Results of Chunking (%)
5.2 Chunking
As we show in Table 4, performance of our SVMs-
based chunkers as well as several baselines includ-
ing existing Japanese text processing tools is evalu-
ated in terms of precision/recall/F
?=1
of identifying
all the 5,744 functional chunks included in the test
data (Kyoto text corpus in Table 3). Performance is
evaluated also in terms of accuracy of classifying de-
tected candidate expressions into functional/content
chunks. Among those baselines, ?majority ( = func-
tional)? always assigns functional usage to the de-
tected candidate expressions. Performance of our
SVMs-based chunkers is measured through 10-fold
cross validation. Our SVMs-based chunker signif-
icantly outperforms those baselines both in F
?=1
and classification accuracy. As we mentioned in
section 1, existing Japanese text processing tools
process compound functional expressions only par-
tially, which causes damage in recall in Table 4.
5.3 Analyzing Dependency Relations
We evaluate the accuracies of judging dependency
relations of compound functional expressions by the
variant of CaboCha trained with Kyoto text cor-
pus annotated with usage labels of compound func-
tional expressions. This performance is measured
through 10-fold cross validation with the modified
version of the Kyoto text corpus. In the evaluation
phase, according to the flow of Figure 2, first we ap-
ply the chunker of compound functional expressions
trained with all the 2,429 sentences in Table 3 and
obtain the results of chunked compound functional
expressions with about 90% correct rate. Then, bun-
setsu segmentation and dependency analysis are per-
71
modifier modifiee
baselines CaboCha (w/o FE) 72.5 88.0
CaboCha (public) 73.9 87.6
chunker + CaboCha (proposed) 74.0 88.0
reference + CaboCha (proposed) 74.4 88.1
Table 5: Accuracies of Identifying Modi-
fier(s)/Modifiee (%)
formed by our variant of CaboCha, where accu-
racies of identifying modifier(s)/modifiee of com-
pound functional expressions are measured as in Ta-
ble 5 (?chunker + CaboCha (proposed)? denotes that
inputs to CaboCha (proposed) are with 90% correct
rate, while ?reference + CaboCha (proposed)? de-
notes that they are with 100% correct rate). Here,
?CaboCha (w/o FE)? denotes a baseline variant of
CaboCha, with all the compound functional expres-
sions removed from its inputs (which are outputs
from ChaSen), while ?CaoboCha (public)? denotes
the publicly available version of CaboCha, which
have some portion of the compound functional ex-
pressions included in its inputs.
For the modifier accuracy, the difference of
?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is statistically significant at a level of
0.05. Identifying compound functional expressions
typically contributes to improvements when the lit-
eral constituents of a compound functional expres-
sion include a verb. In such a case, for bunsetsu
segments which usually modifies a verb, an incor-
rect modifee candidate is removed, which results in
improvements in the modifier accuracy. The dif-
ference between ?CaoboCha (public)? and ?chunker
+ CaboCha (proposed)? is slight because the pub-
licly available version of CaboCha seems to include
compound functional expressions which are dam-
aged in identifying their modifiers with ?CaboCha
(w/o FE)?. For the modifiee accuracy, the difference
of ?chunker + CaboCha (proposed)? and ?CaboCha
(w/o FE)? is zero. Here, more than 100 instances of
improvements like the one in Figure 1 are observed,
while almost the same number of additional fail-
ures are also observed mainly because of the sparse-
ness problem. Furthermore, in the case of the modi-
fiee accuracy, it is somehow difficult to expect im-
provement because identifying modifiees of func-
tional/content bunsetsu segments mostly depends on
features other than functional/content distinction.
6 Concluding Remarks
We proposed an approach of processing Japanese
compound functional expressions by identifying
them and analyzing their dependency relations
through a machine learning technique. This ap-
proach is novel in that it has never been applied
to any language so far. Experimental evaluation
showed that the dependency analysis model applied
to the results of identifying compound functional ex-
pressions significantly outperforms the one applied
to the results without identifying compound func-
tional expressions. The proposed framework has ad-
vantages over an approach based on manually cre-
ated rules such as the one in (Shudo et al, 2004), in
that it requires human cost to create manually and
maintain those rules. Related works include Nivre
and Nilsson (2004), which reports improvement of
Swedish parsing when multi word units are manu-
ally annotated.
References
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten. Kuroshio
Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support vec-
tor machines. In Proc. 2nd NAACL, pages 192?199.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency ana-
lyisis using cascaded chunking. In Proc. 6th CoNLL, pages
63?69.
S. Kurohashi and M. Nagao. 1998. Building a Japanese parsed
corpus while improving the parsing system. In Proc. 1st
LREC, pages 719?724.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo Huku-
gouji Youreishu. (in Japanese).
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic
parsing. In Proc. LRECWorkshop, Methodologies and Eval-
uation of Multiword Units in Real-World Applications, pages
39?46.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc. 2nd
ACL Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 32?39.
M. Tsuchiya, T. Shime, T. Takagi, T. Utsuro, K. Uchimoto,
S. Matsuyoshi, S. Sato, and S. Nakagawa. 2006. Chunk-
ing Japanese compound functional expressions by machine
learning. In Proc. Workshop on Multi-Word-Expressions in
a Multilingual Context, pages 25?32.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
72
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 150?153,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Annotating Semantic Relations Combining Facts and Opinions
Koji Murakami? Shouko Masuda?? Suguru Matsuyoshi?
Eric Nichols? Kentaro Inui? Yuji Matsumoto?
?Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192 JAPAN
?Osaka Prefecture University
1-1, Gakuen, Naka-ku, Sakai, Osaka 599-8531 JAPAN
{kmurakami,shouko,matuyosi,eric-n,inui,matsu}@is.naist.jp
Abstract
As part of the STATEMENT MAP project,
we are constructing a Japanese corpus an-
notated with the semantic relations bridg-
ing facts and opinions that are necessary
for online information credibility evalua-
tion. In this paper, we identify the se-
mantic relations essential to this task and
discuss how to efficiently collect valid ex-
amples from Web documents by splitting
complex sentences into fundamental units
of meaning called ?statements? and an-
notating relations at the statement level.
We present a statement annotation scheme
and examine its reliability by annotating
around 1,500 pairs of statements. We are
preparing the corpus for release this win-
ter.
1 Introduction
The goal of the STATEMENT MAP project (Mu-
rakami et al, 2009) is to assist internet users with
evaluating the credibility of online information by
presenting them with a comprehensive survey of
opinions on a topic and showing how they relate
to each other. However, because real text on the
Web is often complex in nature, we target a sim-
pler and more fundamental unit of meaning which
we call the ?statement.? To summarize opinions
for the statement map users, we first convert all
sentences into statements and then, organize them
into groups of agreeing and conflicting opinions
that show the logical support for each group.
For example, a user who is concerned about po-
tential connections between vaccines and autism
would be presented with a visualization of the
opinions for and against such a connection to-
gether with the evidence supporting each view as
shown in Figure 1.
When the concerned user in our example looks
at this STATEMENT MAP, he or she will see that
some opinions support the query ?Do vaccines
cause autism?? while other opinions do not, but
it will also show what support there is for each of
these viewpoints. So, STATEMENT MAP can help
user come to an informed conclusion.
2 Semantic Relations between
Statements
2.1 Recognizing Semantic Relations
To generate STATEMENT MAPs, we need to an-
alyze a lot of online information retrieved on a
given topic, and STATEMENT MAP shows users
a summary with three major semantic relations.
AGREEMENT to group similar opinions
CONFLICT to capture differences of opinions
EVIDENCE to show support for opinions
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment (RTE).
A major task of the RTE Challenge (Dagan et al,
2005) is the identification of [ENTAILMENT] or
[CONTRADICTION] between Text (T) and Hy-
pothesis (H). For this task, several corpora have
been constructed over the past few years, and an-
notated with thousands of (T,H) pairs.
While our research objective is to recognize se-
mantic relations as well, our target domain is text
from Web documents. The definition of contradic-
tion in RTE is that T contradicts H if it is very un-
likely that both T and H can be true at the same
time. However, in real documents on the Web,
there are many examples which are partially con-
tradictory, or where one statement restricts the ap-
plicability of another like in the example below.
(1) a. Mercury-based vaccines actually cause autism in
children.
150
!Mercury-based vaccine preservatives actually have caused autism in 
children. 
!It?s biologically plausible that the MMR vaccine causes autism. 
VACCINES CAUSE AUTISM 
!There is no valid scientific evidence that vaccines  
cause autism. 
!The weight of the evidence indicates that vaccines  
are not associated with autism. 
VACCINES DON?T CAUSE AUTISM 
!My son then had the MMR, and then when he was three he was 
diagnosed with autism. 
!He then had the MMR, and then when he was three he was  
diagnosed with autism. 
MY CHILD WAS DIAGNOSED WITH AUTISM 
RIGHT AFTER THE VACCINE 
!Vaccinations are given around the same time  
children can be first diagnosed. 
!The plural of anecdote is not data.  
ANECDOTES ARE NOT EVIDENCE 
[CONFLICT]!
[FOCUS]!
[EVIDENCE]!
[EVIDENCE]!
Query : Do vaccines cause autism?!
[CONFLICT]!
Figure 1: An example STATEMENT MAP for the query ?Do vaccines cause autism??
b. Vaccines can trigger autism in a vulnerable subset of
children.
While it is difficult to assign any relation to this
pair in an RTE framework, in order to construct
statement maps we need to recognize a contradic-
tion between (1a) and (1b).
There is another task of recognizing relations
between sentences, CST (Cross-Document Struc-
ture Theory) which was developed by Radev
(2000). CST is an expanded rhetorical structure
analysis based on RST (Mann and Thompson,
1988), and attempts to describe relations between
two or more sentences from both single and mul-
tiple document sets. The CSTBank corpus (Radev
et al, 2003) was constructed to annotate cross-
document relations. CSTBank is divided into clus-
ters in which topically-related articles are gath-
ered. There are 18 kinds of relations in this corpus,
including [EQUIVALENCE], [ELABORATION],
and [REFINEMENT].
2.2 Facts and Opinions
RTE is used to recognize logical and factual re-
lations between sentences in a pair, and CST is
used for objective expressions because newspa-
per articles related to the same topic are used as
data. However, the task specifications of both RTE
and CST do not cover semantic relations between
opinions and facts as illustrated in the following
example.
(2) a. There must not be a connection between vaccines
and autism.
b. I do believe that there is a link between vaccinations
and autism.
Subjective statements, such as opinions, are re-
cently the focus of many NLP research topics,
such as review analysis, opinion extraction, opin-
ion QA, or sentiment analysis. In the corpus con-
structed by the MPQA Project (Multi-Perspective
Question Answering) (Wiebe et al, 2005), indi-
vidual expressions are marked that correspond to
explicit mentions of private states, speech events,
and expressive subjective elements.
Our goal is to annotate instances of the three
major relation classes: [AGREEMENT], [CON-
FLICT] and [EVIDENCE], between pairs of state-
ments in example texts. However, each relation
has a wide range, and it is very difficult to define
a comprehensive annotation scheme. For exam-
ple, different kinds of information can act as clues
to recognize the [AGREEMENT] relations. So,
we have prepared a wide spectrum of semantic re-
lations depending on different types of informa-
tion regarded as clues to identify a relation class,
such as [AGREEMENT] or [CONFLICT]. Table 1
shows the semantic relations needed for carry-
ing out the anotation. Although detecting [EVI-
DENCE] relations is also essential to the STATE-
MENT MAP project, we do not include them in our
current corpus construction.
3 Constructing a Japanese Corpus
3.1 Targeting Semantic Relations Between
Statements
Real data on the Web generally has complex sen-
tence structures. That makes it difficult to rec-
ognize semantic relations between full sentences.
but it is possible to annotate semantic relation be-
tween parts extracted from each sentence in many
cases. For example, the two sentences A and B
in Figure 2 cannot be annotated with any of the
semantic relations in Table 1, because each sen-
tence include different types of information. How-
ever, if two parts extracted from these sentences C
and D are compared, the parts can be identified as
[EQUIVALENCE] because they are semantically
close and each extracted part does not contain a
different type of information. So, we attempt to
break sentences from the Web down into reason-
able text segments, which we call ?statements.?
When a real sentence includes several pieces of se-
151
Table 1: Definition of semantic relations and example in the corpus
Relation Class Relation Label Example
AGREEMENT
Equivalence A: The overwhelming evidence is that vaccines are unrelated to autism.B: There is no link between the MMR vaccine and autism.
Equivalent Opinion
A: We think vaccines cause autism.
B: I am the mother of a 6 year old that regressed into autism because of his 18
month vaccinations.
Specific A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines cause autism.
CONFLICT
Contradiction A: Mercury-based vaccine preservatives actually have caused autism in children.B: Vaccines don?t cause autism.
Confinement A: Vaccines can trigger autism in a vulnerable subset of children.B: Mercury-based vaccine actually have caused autism in children.
Conflicting Opinion A: I don?t think vaccines cause autism.B: I believe vaccines are the cause of my son?s autism.
According to Department 
of Medicine, there is no 
link between the MMR 
vaccine and autism.!
There is no link between the 
MMR vaccine and autism.!
The weight of the 
evidence indicates that 
vaccines are not 
associated with autism.!
Vaccines are not 
associated with autism.!
(A) Real sentence (1) (B) Real sentence (2)!
(C) Statement (1)! (D) Statement (2)!(E) [EQUIVALENCE]!
Figure 2: Extracting statements from sentences
and annotating a semantic relation between them
mantic segments, more than one statement can be
extracted. So, a statement can reflect the writer?s
affirmation in the original sentence. If the ex-
tracted statements lack semantic information, such
as pronouns or other arguments, human annota-
tors manually add the missing information. Fi-
nally we label pairs of statements with either one
of the semantic relations from Table 1 or with ?NO
RELATION,? which means that two sentences (1)
are not semantically related, or (2) have a relation
other than relations defined in Table 1.
3.2 Corpus Construction Procedure
We automatically gather sentences on related top-
ics by following the procedure below:
1. Retrieve documents related to a set number of
topics using a search engine
2. Extract real sentences that include major sub-
topic words which are detected based on TF or
DF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate.
Dolan and Brockett (2005) proposed a method
to narrow the range of sentence pair candidates
and collect candidates of sentence-level para-
phrases which correspond [EQUIVALENCE] in
[AGREEMENT] class in our task. It worked well
for collecting valid sentence pairs from a large
cluster which was constituted by topic-related sen-
tences. The method also seem to work well for
[CONFLICT] relations, because lexical similar-
ity based on bag-of-words (BOW) can narrow the
range of candidates with this relation as well.
We calculate the lexical similarity between the
two sentences based on BOW. We also used hy-
ponym and synonym dictionaries (Sumida et al,
2008) and a database of relations between predi-
cate argument structures (Matsuyoshi et al, 2008)
as resources. According to our preliminary exper-
iments, unigrams of KANJI and KATAKANA ex-
pressions, single and compound nouns, verbs and
adjectives worked well as features, and we calcu-
late the similarity using cosine distance. We did
not use HIRAGANA expressions because they are
also used in function words.
4 Analyzing the Corpus
Five annotators annotated semantic relations ac-
cording to our specifications in 22 document sets
as targets. We have annotated target statement
pairs with either [AGREEMENT], [CONFLICT]
or [NO RELATION]. We provided 2,303 real
sentence pairs to human annotators, and they
identified 1,375 pairs as being invalid and 928
pairs as being valid. The number of annotated
statement pairs are 1,505 ([AGREEMENT]:862,
[CONFLICT]:126, [NO RELATION]:517).
Next, to evaluate inter annotator agreement, 207
randomly selected statement pairs were annotated
by two human annotators. The annotators agreed
in their judgment for 81.6% of the examples,
which corresponds to a kappa level of 0.49. The
annotation results are evaluated by calculating re-
call and precision in which one annotation result
is treated as a gold standard and the other?s as the
output of the system, as shown in Talbe 2.
152
Table 2: Inter-annotator agreement for 2 annota-
tors
Annotator A
AGR. CON. NONE TOTAL
AGR. 146 7 9 162
Anno- CON. 0 13 1 14
tator B NONE 17 4 10 31
TOTAL 163 24 20 207
5 Discussion
The number of sentence pairs that annotators iden-
tified as invalid examples shows that around 60%
of all pairs were invalid, showing that there is still
room to improve our method of collecting sen-
tence pairs for the annotators. Developing more
effective methods of eliminating sentences pairs
that are unlikely to contain statements with plau-
sible relations is important to improve annotator
efficiency. We reviewed 50 such invalid sentence
pairs, and the results indicate two major consider-
ations: (1) negation, or antonyms have not been re-
garded as key information, and (2) verbs in KANJI
have to be handled more carefully. The polarities
of sentences in all pairs were the same although
there are sentences which can be paired up with
opposite polarities. So, we will consider the po-
larity of words and sentences as well as similarity
when considering candidate sentence pairs.
In Japanese, the words which consist of
KATAKANA expressions are generally nouns, but
those which contain KANJI can be nouns, verbs,
or adjectives. Sharing KATAKANA words was
the most common way of increasing the simi-
larity between sentences. We need to assign a
higher weight to verbs and adjectives that contain
KANJI, to more accurately calculate the similarity
between sentences.
Another approach to reducing the search space
for statement pairs is taken by Nichols et al
(2009), who use category tags and in-article hyper-
links to organize scientific blog posts into discus-
sions on the same topic, making it easier to iden-
tify relevant statements. We are investigating the
applicability of these methods to the construction
of our Japanese corpus but suffer from the lack of
a richly-interlinked data source comparable to En-
glish scientific blogs.
6 Conclusion
In this paper, we described the ongoing construc-
tion of a Japanese corpus consisting of statement
pairs annotated with semantic relations for han-
dling web arguments. We designed an annotation
scheme complete with the necessary semantic re-
lations to support the development of statement
maps that show [AGREEMENT], [CONFLICT],
and [EVIDENCE] between statements for assist-
ing users in analyzing credibility of information
in Web. We discussed the revelations made from
annotating our corpus, and discussed future direc-
tions for refining our specifications of the corpus.
We are planning to annotate relations for more
than 6,000 sentence pairs in this summer, and the
finished corpus will consist of around 10,000 sen-
tence pairs. The first release of our annotation
specifications and the corpus will be made avail-
able on the Web1 this winter.
Acknowledgments
This work is supported by the National Institute
of Information and Communications Technology
Japan.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
Proc. of the PASCAL Challenges Workshop on RTE.
Bill Dolan and Chris Brockett. 2005. Automatical ly con-
structing a corpus of sentential paraphrases. In Proc. of
the IWP 2005, pages 9?16.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: towards a functional theory of text or-
ganization. Text, 8(3):243?281.
Suguru Matsuyoshi, Koji Murakami, Yuji Matsumoto, , and
Kentaro Inui. 2008. A database of relations between
predicate argument structures for recognizing textual en-
tailment and contradiction. In Proc. of the ISUC 2008.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka
Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information
credibility analysis by visualizing arguments. In Proc. of
the WICOW 2009, pages 43?50.
Eric Nichols, Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Constructing a scientific blog corpus for
information credibility analysis. In Proc. of the Annual
Meeting of ANLP.
Dragomir Radev, Jahna Otterbacher, and Zhu Zhang.
2003. CSTBank: Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Dragomir R. Radev. 2000. Common theory of informa-
tion fusion from multiple text sources step one: Cross-
document structure. In Proc. of the 1st SIGdial workshop
on Discourse and dialogue, pages 74?83.
Asuka Sumida, Naoki Yoshinaga, and Kentaro Torisawa.
2008. Boosting precision and recall of hyponymy rela-
tion acquisition from hierarchical layouts in wikipedia. In
Proc. of the LREC 2008.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in lan-
guage. Language Resources and Evaluation, 39(2-
3):165?210.
1http://cl.naist.jp/stmap/corpus/ja
153
Automatic Paraphrasing of Japanese Functional Expressions
Using a Hierarchically Organized Dictionary
Suguru Matsuyoshi?,? Satoshi Sato?
? Graduate School of Informatics, Kyoto University, Japan
? Graduate School of Engineering, Nagoya University, Japan
{s matuyo,ssato}@nuee.nagoya-u.ac.jp
Abstract
Automatic paraphrasing is a transformation
of expressions into semantically equivalent
expressions within one language. For gener-
ating a wider variety of phrasal paraphrases
in Japanese, it is necessary to paraphrase
functional expressions as well as content ex-
pressions. We propose a method of para-
phrasing of Japanese functional expressions
using a dictionary with two hierarchies: a
morphological hierarchy and a semantic hi-
erarchy. Our system generates appropriate
alternative expressions for 79% of source
phrases in Japanese in an open test. It also
accepts style and readability specifications.
1 Introduction
Automatic paraphrasing is a transformation of ex-
pressions into semantically equivalent expressions
within one language. It is expected for various ap-
plications, such as information retrieval, machine
translation and a reading/writing aid.
Automatic paraphrasing of Japanese text has been
studied by many researchers after the first interna-
tional workshop on automatic paraphrasing (Sato
and Nakagawa, 2001). Most of them focus on para-
phrasing of content words, such as noun phrases and
verb phrases. In contrast, paraphrasing of functional
expressions has less attention. A functional expres-
sion is a function word or a multi-word expression
that works as a function word. For generating a
wider variety of phrasal paraphrases in Japanese, as
shown in Fig. 1, it is necessary to paraphrase func-
tional expressions as well as content expressions, be-
cause almost all phrases in Japanese include one or
more functional expressions. In this paper, we focus
on paraphrasing of Japanese functional expressions.
In several applications, such as a reading aid,
in paraphrasing of Japanese functional expressions,
control of readability of generated text is impor-
tant, because functional expressions are critical units
that determine sentence structures and meanings. In
case a reader does not know a functional expres-
sion, she fails to understand the sentence meaning.
If the functional expression can be paraphrased into
an easier one, she may know it and understand the
sentence meaning. It is desirable to generate expres-
sions with readability suitable for a reader because
easier functional expressions tend to have more than
one meaning.
A remarkable characteristic of Japanese func-
tional expressions is that each functional expression
has many different variants. Each variant has one of
four styles. In paraphrasing of Japanese functional
expressions, a paraphrasing system should accept
style specification, because consistent use in style is
required. For example, the paraphrase (b) in Fig. 1
is not appropriate for a document in normal style be-
cause the expression has polite style.
Paraphrasing a functional expression into a se-
mantically equivalent one that satisfies style and
readability specifications can be realized as a com-
bination of the following two processes:
1. Transforming a functional expression into an-
other one that is semantically equivalent to it,
often with changing readability.
691
A phrase keQtei-se / zaru-wo-e-nai
 
Paraphrasing of
content expressions
Paraphrasing of
functional expressions
kimeru
sadameru
keQtei-wo-suru
:
shika-nai
shika-ari-mase-N
yori-hoka-nai
:
?
?
Phrasal
paraphrases
(a) kimeru shika-nai
(b) kimeru shika-ari-mase-N
(c) sadameru shika-nai
(d) sadameru yori-hoka-nai
:
Figure 1: Generation of a wider variety of phrasal
paraphrases.
2. Rewriting a functional expression to a variant
of it, often with changing style.
We propose a method of paraphrasing of Japanese
functional expressions using a dictionary with two
hierarchies: a morphological hierarchy and a se-
mantic hierarchy. The former hierarchy provides
a list of all variants specified with style for each
functional expression, which is required for the
above process 2. The latter hierarchy provides se-
mantic equivalence classes of functional expressions
and readability level for each functional expression,
which are required for the above process 1.
2 Related Work
A few studies on paraphrasing of Japanese func-
tional expressions have been conducted. In order
to implement automatic paraphrasing, some stud-
ies (Iida et al, 2001; Tsuchiya et al, 2004) use a
set of paraphrasing rules, and others (Tanabe et al,
2001; Shudo et al, 2004) use semantic equivalence
classes.
All of these studies do not handle variants in a
systematic way. In case a system paraphrases a func-
tional expression f into f ?, it also should generate all
variants of f ? in potential. However, any proposed
system does not guarantee this requirement. Output
selection of variants should be determined accord-
ing to the given style specification. Any proposed
system does not have such selection mechanism.
Controlling readability of generated text is not a
central issue in previous studies. An exception is
a study by Tsuchiya et al (Tsuchiya et al, 2004).
Level Num
L
1 Headword 341
L
2 Headwords with unique meaning 435
L
3 Derivations 555
L
4 Alternations of function words 774
L
5 Phonetic variations 1,187
L
6 Insertion of particles 1,810
L
7 Conjugation forms 6,870
L
8 Normal or desu/masu forms 9,722
L
9 Spelling variations 16,801
Table 1: Nine levels of the morphological hierarchy.
Their system paraphrases a functional expression
into an easier one. However, it does not accept the
readability specification, e.g. for learners of begin-
ner course or intermediate course of Japanese.
3 A Hierarchically Organized Dictionary
of Japanese Functional Expressions
3.1 Morphological hierarchy
In order to organize many different variants of func-
tional expressions, we have designed a morpho-
logical hierarchy with nine abstraction levels (Mat-
suyoshi et al, 2006). Table 1 summarizes these nine
levels. The number of entries in L1 (headwords) is
341, and the number of leaf nodes in L9 (surface
forms) is 16,801. For each surface form in the hier-
archy, we specified one of four styles (normal, po-
lite, colloquial, and stiff) and connectability (what
word can be to the left and right of the expression).
3.2 Semantic hierarchy
There is no available set of semantic equivalence
classes of Japanese functional expressions for para-
phrasing. Some sets are described in books in lin-
guistics (Morita and Matsuki, 1989; Tomomatsu et
al., 1996; Endoh et al, 2003), but these are not for
paraphrasing. Others are proposed for paraphrasing
in natural language processing (Tanabe et al, 2001;
Shudo et al, 2004), but these are not available in
public.
For 435 entries in L2 (headwords with unique
meaning) of the morphological hierarchy, from the
viewpoint of paraphrasability, we have designed a
semantic hierarchy with three levels according to the
semantic hierarchy proposed by a book (Morita and
Matsuki, 1989). The numbers of classes in the top,
middle and bottom levels are 45, 128 and 199, re-
692
spectively. For each entry in L2, we specified one of
readability levels of A1, A2, B, C, and F according
to proficiency level in a book (Foundation and of In-
ternational Education, Japan, 2002), where A1 is the
most basic level and F is the most advanced level.
3.3 Producing all surface forms that satisfy
style and readability specifications
For a given surface form of a functional expression,
our dictionary can produce all variants of semanti-
cally equivalent functional expressions that satisfy
style and readability specifications. The procedure
is as follows:
1. Find the functional expression in L2 for a given
surface form according to the morphological
hierarchy.
2. Obtain functional expressions that are seman-
tically equivalent to the functional expression
according to the semantic hierarchy.
3. Exclude the functional expressions that do not
satisfy readability specification.
4. Enumerate all variants (surface forms) of the
remaining functional expressions according to
the morphological hierarchy.
5. Exclude the surface forms that do not satisfy
style specification.
4 Formulation of Paraphrasing of
Japanese Functional Expressions
As a source expression of paraphrasing, we select a
phrase (or Bunsetsu) in Japanese because it is a base
unit that includes functional expressions. In this pa-
per, we define a phrase as follows. Let c
i
be a con-
tent word, and f
j
a functional expression. Then, a
phrase is formulated as the following:
Phrase = c
1
c
2
? ? ? c
m
f
1
f
2
? ? ? f
n
, (1)
where c
1
c
2
? ? ? c
m
is the content part of the phrase
and f
1
f
2
? ? ? f
n
is the functional part of it.
Paraphrasing of a functional part of a phrase is
performed as a combination of the following five
types of paraphrasing:
1?1 Substituting a functional expression with an-
other functional expression (f ? f ?).
Paraphrasing type Num
1?1 only 214 (61%)
1?N (and 1?1) 69 (20%)
N?1 (and 1?1) 18 ( 5%)
M?N (and 1?1) 8 ( 2%)
Otherwise 44 (12%)
Sum 353 (100%)
Table 2: Number of paraphrases produced by a na-
tive speaker of Japanese.
1?N Substituting a functional expression with a
sequence of functional expressions (f ?
f
?
1
f
?
2
? ? ? f
?
N
).
N?1 Substituting a sequence of functional ex-
pressions with one functional expression
(f
1
f
2
? ? ? f
N
? f
?).
M?N Substituting a sequence of functional ex-
pressions with another sequence of functional
expressions (f
1
f
2
? ? ? f
M
? f
?
1
f
?
2
? ? ? f
?
N
).
f?c Substituting a functional expression with an
expression including one or more content
words.
In a preliminary experiment, we investigated
which type of the above a native speaker of Japanese
tended to use in paraphrasing a functional part. Ta-
ble 2 shows the classification result of 353 para-
phrases produced by the subject for 238 source
phrases.1 From this table, it was found out that para-
phrasing of ?1?1? type was major in that it was
used for producing 61% of paraphrases.
Because of dominance of paraphrasing of ?1?1?
type, we construct a system that paraphrases
Japanese functional expressions in a phrase by sub-
stituting a functional expression with a semantically
equivalent expression. This system paraphrases a
phrase defined as the form in Eq. (1) into the fol-
lowing form:
Alternative = c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
f
?
2
? ? ? f
?
n
,
where c?
m
is c
m
or a conjugation form of c
m
, f ?
j
is a
functional expression that is semantically equivalent
to f
j
, and w is a null string or a function word that
is inserted for connecting f ?
1
to c?
m
properly.
1These source phrases are the same ones that we use in a
closed test in section 6.
693
INPUT
- kiku ya-ina-ya
(as soon as I hear)
Readability
specification:
A1, A2, B


Analysis

c
1
= kiku
f
1
= ya-ina-ya

Paraphrase
generation





Dictionary

- kiku to-sugu-ni
- kiku to-douzi-ni
- kii ta-totaN
:

Ranking

OUTPUT
1. kiku to-douzi-ni
2. kii ta-totaN
3. kiku to-sugu
:
Figure 2: Overview of our system.
The combination of simple substitution of a func-
tional expression and insertion of a function word
covers 22% (15/69) of the paraphrases by paraphras-
ing of ?1?N (and 1?1)? type in Table 2. There-
fore, our system theoretically covers 65% (229/353)
of the paraphrases in Table 2.
5 System
We have implemented a system that paraphrases
Japanese functional expressions using a hierarchi-
cally organized dictionary, by substituting a func-
tional expression with another functional expression
that is semantically equivalent to it. The system ac-
cepts a phrase in Japanese and generates a list of
ranked alternative expressions for it. The system
also accepts style and readability specifications.
Fig. 2 shows an overview of our system. This sys-
tem consists of three modules: analysis, paraphrase
generation, and ranking.
5.1 Analysis
Some methods have been proposed for detecting
Japanese functional expressions based on a set of
detection rules (Tsuchiya and Sato, 2003) and ma-
chine learning (Uchimoto et al, 2003; Tsuchiya et
al., 2006). However, because these methods detect
only a limited number of functional expressions (and
their variants), we cannot apply them to the analysis
of a phrase. Another method is to add a list of about
17,000 surface forms of functional expressions to a
dictionary of an existing morphological analyzer and
determine connecting costs based on machine learn-
ing. However, it is infeasible because there is no
large corpus in which all of these surface forms have
been tagged.
Instead of these methods, we use a different
method of decomposing a given phrase into a se-
quence of content words and functional expressions.
Our method uses two analyzers.
We constructed a functional-part analyzer (FPA).
This is implemented using a morphological analyzer
MeCab2 with a special dictionary containing only
functional expressions. FPA can decompose a func-
tional part (string) into a sequence of functional ex-
pressions, but fails to decompose a string when the
string includes one or more content words. In order
to extract a functional part from a given string, we
use original MeCab.
First, original MeCab decomposes a given string
into a sequence of morphemes m
1
m
2
? ? ?m
k
.
Next, we suppose that m
1
is a content part
and m
2
m
3
? ? ?m
k
is a functional part. If FPA
can decompose m
2
m
3
? ? ?m
k
into a sequence of
functional expressions f
1
f
2
? ? ? f
n
, then we obtain
c
1
f
1
f
2
? ? ? f
n
as shown in Eq. (1) as an analyzed
result, where c
1
= m
1
. Otherwise, we sup-
pose that m
1
m
2
is a content part and m
3
m
4
? ? ?m
k
is a functional part. If FPA can decompose
m
3
m
4
? ? ?m
k
into a sequence of functional expres-
sions f
1
f
2
? ? ? f
n
, then we obtain c
1
c
2
f
1
f
2
? ? ? f
n
as
an analyzed result, where c
1
= m
1
and c
2
= m
2
.
This procedure is continued until FPA succeeds in
decomposition.
5.2 Paraphrase generation
This module accepts an analyzed result
c
1
c
2
? ? ? c
m
f
1
f
2
? ? ? f
n
and generates a list of
alternative expressions for it.
First, the module obtains a surface form f ?
1
that
is semantically equivalent to f
1
from the dictionary
in section 3. Next, it constructs c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
by connecting f ?
1
to c
1
c
2
? ? ? c
m
by the method de-
scribed in section 4. Then, it obtains a surface
form f ?
2
that is semantically equivalent to f
2
and
constructs c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
f
?
2
in similar fashion.
This process proceeds analogously, and finally, the
module constructs c
1
c
2
? ? ? c
m?1
c
?
m
wf
?
1
f
?
2
? ? ? f
?
n
as
an alternative expression.
Because in practice the module obtains more than
one surface form that is semantically equivalent to
2http://mecab.sourceforge.net/
694
Top 1 Top 1 to 2 Top 1 to 3 Top 1 to 4 Top 1 to 5
Closed 177 (74%) 197 (83%) 210 (88%) 213 (90%) 213 (90%)
Closed (Perfect analysis) 196 (82%) 211 (89%) 219 (92%) 221 (93%) 221 (93%)
Open 393 (63%) 461 (73%) 496 (79%) 500 (80%) 501 (80%)
Open (Perfect analysis) 453 (72%) 508 (81%) 531 (85%) 534 (85%) 534 (85%)
Table 3: Evaluation of paraphrases generated by the paraphrasing system
f
j
by the method described in subsection 3.3, it gen-
erates more than one alternative expression by con-
sidering all possible combinations of these surface
forms and excluding candidates that include two ad-
jacent components that cannot be connected prop-
erly.
If the module generates no alternative expression,
it uses the semantic equivalence classes in the upper
level reluctantly.
5.3 Ranking
Because a functional expression seems to be more
standard and common as it appears more frequently
in newspaper corpus, we use frequencies of func-
tional expressions (strings) in newspaper corpus in
order to rank alternative expressions. We define a
scoring function as the product of frequencies of
functional expressions in a phrase.
6 Evaluation
We evaluate paraphrases generated by our para-
phrasing system for validating our semantic equiva-
lence classes, because the dictionary that the system
uses guarantees by the method described in subsec-
tion 3.3 that the system can generate all variants of a
functional expression and accept style and readabil-
ity specifications.
6.1 Methodology
We evaluated paraphrases generated by our para-
phrasing system from the viewpoint of an applica-
tion to a writing aid, where a paraphrasing system
is expected to output a few good alternative expres-
sions for a source phrase.
We evaluated the top 5 alternative expressions
generated by the system for a source phrase by clas-
sifying them into the following three classes:
Good Good alternative expression for the source
phrase.
Intermediate Expression that keeps the meaning
roughly that the source phrase has.
Bad Inappropriate expression.
Then, we counted source phrases for which at least
one of the alternative expressions of the top 1 to
n was judged as ?Good?. One of the authors per-
formed the judgment according to books (Morita
and Matsuki, 1989; Endoh et al, 2003).
As a closed test set, we used 238 example phrases
for 140 functional expressions extracted from a book
(Foundation and of International Education, Japan,
2002), which we had used for development of our
semantic equivalence classes. As an open test set,
we used 628 example phrases for 184 functional ex-
pressions extracted from a book (Tomomatsu et al,
1996). We used the Mainichi newspaper text corpus
(1991-2005, about 21 million sentences, about 1.5
gigabytes) for ranking alternative expressions.
6.2 Results
Table 3 shows the results. The rows with ?Perfect
analysis? in the table show the results in analyzing
source phrases by hand. Because the values in every
row of the table are nearly saturated in ?Top 1 to 3?,
we discuss the results of the top 1 to 3 hereafter.
Our system generated appropriate alternative ex-
pressions for 88% (210/238) and 79% (496/628) of
source phrases in the closed and the open test sets,
respectively. We think that this performance is high
enough.
We analyzed the errors made by the system. In the
closed and the open tests, it was found out that para-
phrasing of ?1?1? type could not generate alterna-
tive expressions for 7% (16/238) and 7% (41/628)
of source phrases, respectively. These values define
the upper limit of our system.
In the closed and the open tests, it was found out
that the system failed to analyze 3% (8/238) and 3%
(21/628) of source phrases, respectively, and that
695
ambiguity in meaning caused inappropriate candi-
dates to be ranked higher for 1% (2/238) and 4%
(23/628) of source phrases, respectively. The rows
with ?Perfect analysis? in Table 3 show that almost
all of these problems are solved in analyzing source
phrases by hand. Improvement of the analysis mod-
ule can solve these problems.
In the open test, insufficiency of semantic equiv-
alence classes and too rigid connectability caused
only 3% (19/628) and 3% (16/628) of source phrases
to have no good candidates, respectively. The small-
ness of the former value validates our semantic
equivalence classes.
The remaining errors were due to low frequencies
of good alternatives in newspaper corpus.
7 Conclusion and Future Work
We proposed a method of paraphrasing Japanese
functional expressions using a dictionary with two
hierarchies. Our system can generate all variants of a
functional expression and accept style and readabil-
ity specifications. The system generated appropriate
alternative expressions for 79% of source phrases in
an open test.
Tanabe et al have proposed paraphrasing rules
of ?1?N?, ?N?1?, and ?M?N? types (Tanabe
et al, 2001). For generating a wider variety of
phrasal paraphrases, future work is to incorporate
these rules into our system and to combine several
methods of paraphrasing of content expressions with
our method.
References
Orie Endoh, Kenji Kobayashi, Akiko Mitsui, Shinjiro
Muraki, and Yasushi Yoshizawa, editors. 2003. A
Dictionary of Synonyms in Japanese (New Edition).
Shogakukan. (in Japanese).
The Japan Foundation and Association of International
Education, Japan, editors. 2002. Japanese Language
Proficiency Test: Test Content Specifications (Revised
Edition). Bonjinsha. (in Japanese).
Ryu Iida, Yasuhiro Tokunaga, Kentaro Inui, and Junji
Etoh. 2001. Exploration of clause-structural and
function-expressional paraphrasing using KURA. In
Proceedings of the 63rd National Convention of Infor-
mation Processing Society of Japan, volume 2, pages
5?6. (in Japanese).
Suguru Matsuyoshi, Satoshi Sato, and Takehito Utsu-
ro. 2006. Compilation of a dictionary of Japanese
functional expressions with hierarchical organization.
In Proceedings of the 21st International Conference
on Computer Processing of Oriental Languages (IC-
CPOL), Lecture Notes in Computer Science, volume
4285, pages 395?402. Springer.
Yoshiyuki Morita and Masae Matsuki. 1989. Nihongo
Hyougen Bunkei, volume 5 of NAFL Sensho (Ex-
pression Patterns in Japanese). ALC Press Inc. (in
Japanese).
Satoshi Sato and Hiroshi Nakagawa, editors. 2001. Auto-
matic Paraphrasing: Theories and Applications, The
6th Natural Language Processing Pacific Rim Sympo-
sium (NLPRS) Post-Conference Workshop.
Kosho Shudo, Toshifumi Tanabe, Masahito Takahashi,
and Kenji Yoshimura. 2004. MWEs as non-
propositional content indicators. In Proceedings of the
2nd ACL Workshop on Multiword Expressions: Inte-
grating Processing (MWE-2004), pages 32?39.
Toshifumi Tanabe, Kenji Yoshimura, and Kosho Shudo.
2001. Modality expressions in Japanese and their au-
tomatic paraphrasing. In Proceedings of the 6th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS), pages 507?512.
Etsuko Tomomatsu, Jun Miyamoto, and Masako Wakuri.
1996. 500 Essential Japanese Expressions: A Guide
to Correct Usage of Key Sentence Patterns. ALC Press
Inc. (in Japanese).
Masatoshi Tsuchiya and Satoshi Sato. 2003. Automatic
detection of grammar elements that decrease readabil-
ity. In Proceedings of 41st Annual Meeting of the As-
sociation for Computational Linguistics, pages 189?
192.
Masatoshi Tsuchiya, Satoshi Sato, and Takehito Utsuro.
2004. Automatic generation of paraphrasing rules
from a collection of pairs of equivalent sentences in-
cluding functional expressions. In Proceedings of the
10th Annual Meeting of the Association for Natural
Language Processing, pages 492?495. (in Japanese).
Masatoshi Tsuchiya, Takao Shime, Toshihiro Takagi,
Takehito Utsuro, Kiyotaka Uchimoto, Suguru Mat-
suyoshi, Satoshi Sato, and Seiichi Nakagawa. 2006.
Chunking Japanese compound functional expressions
by machine learning. In Proceedings of the workshop
on Multi-word-expressions in a multilingual context,
EACL 2006 Workshop, pages 25?32.
Kiyotaka Uchimoto, Chikashi Nobata, Atsushi Yamada,
Satoshi Sekine, and Hitoshi Isahara. 2003. Morpho-
logical analysis of a large spontaneous speech corpus
in Japanese. Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 479?488.
696
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474?478,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Text Classification from Positive and Unlabeled Data using Misclassified
Data Correction
Fumiyo Fukumoto and Yoshimi Suzuki and Suguru Matsuyoshi
Interdisciplinary Graduate School of Medicine and Engineering
University of Yamanashi, Kofu, 400-8511, JAPAN
{fukumoto,ysuzuki,sugurum}@yamanashi.ac.jp
Abstract
This paper addresses the problem of deal-
ing with a collection of labeled training
documents, especially annotating negative
training documents and presents a method
of text classification from positive and un-
labeled data. We applied an error detec-
tion and correction technique to the re-
sults of positive and negative documents
classified by the Support Vector Machines
(SVM). The results using Reuters docu-
ments showed that the method was compa-
rable to the current state-of-the-art biased-
SVM method as the F-score obtained by
our method was 0.627 and biased-SVM
was 0.614.
1 Introduction
Text classification using machine learning (ML)
techniques with a small number of labeled data has
become more important with the rapid increase in
volume of online documents. Quite a lot of learn-
ing techniques e.g., semi-supervised learning, self-
training, and active learning have been proposed.
Blum et al proposed a semi-supervised learn-
ing approach called the Graph Mincut algorithm
which uses a small number of positive and nega-
tive examples and assigns values to unlabeled ex-
amples in a way that optimizes consistency in a
nearest-neighbor sense (Blum et al, 2001). Cabr-
era et al described a method for self-training text
categorization using the Web as the corpus (Cabr-
era et al, 2009). The method extracts unlabeled
documents automatically from the Web and ap-
plies an enriched self-training for constructing the
classifier.
Several authors have attempted to improve clas-
sification accuracy using only positive and unla-
beled data (Yu et al, 2002; Ho et al, 2011). Liu
et al proposed a method called biased-SVM that
uses soft-margin SVM as the underlying classi-
fiers (Liu et al, 2003). Elkan and Noto proposed
a theoretically justified method (Elkan and Noto,
2008). They showed that under the assumption
that the labeled documents are selected randomly
from the positive documents, a classifier trained on
positive and unlabeled documents predicts proba-
bilities that differ by only a constant factor from
the true conditional probabilities of being positive.
They reported that the results were comparable to
the current state-of-the-art biased SVM method.
The methods of Liu et al and Elkan et al model
a region containing most of the available positive
data. However, these methods are sensitive to the
parameter values, especially the small size of la-
beled data presents special difficulties in tuning
the parameters to produce optimal results.
In this paper, we propose a method for elimi-
nating the need for manually collecting training
documents, especially annotating negative train-
ing documents based on supervised ML tech-
niques. Our goal is to eliminate the need for manu-
ally collecting training documents, and hopefully
achieve classification accuracy from positive and
unlabeled data as high as that from labeled posi-
tive and labeled negative data. Like much previous
work on semi-supervised ML, we apply SVM to
the positive and unlabeled data, and add the classi-
fication results to the training data. The difference
is that before adding the classification results, we
applied the MisClassified data Detection and Cor-
rection (MCDC) technique to the results of SVM
learning in order to improve classification accu-
racy obtained by the final classifiers.
2 Framework of the System
The MCDC method involves category error cor-
rection, i.e., correction of misclassified candidates,
while there are several strategies for automati-
cally detecting lexical/syntactic errors in corpora
(Abney et al, 1999; Eskin, 2000; Dickinson and
474
training
UPP1 N1N1
training
SVM
MCDC
N1RC1
U?N1
CP1
CN1SVM
MCDC
? Final results
SVMtraining
selection
classification
PCPN1RC1
N1RC2
N1CN
MCDC
Figure 1: Overview of the system
Meurers., 2005; Boyd et al, 2008) or categorical
data errors (Akoglu et al, 2013). The method first
detects error candidates. As error candidates, we
focus on support vectors (SVs) extracted from the
training documents by SVM. Training by SVM is
performed to find the optimal hyperplane consist-
ing of SVs, and only the SVs affect the perfor-
mance. Thus, if some training document reduces
the overall performance of text classification be-
cause of an outlier, we can assume that the docu-
ment is a SV.
Figure 1 illustrates our system. First, we ran-
domly select documents from unlabeled data (U )
where the number of documents is equal to that of
the initial positive training documents (P1). We set
these selected documents to negative training doc-
uments (N1), and apply SVM to learn classifiers.
Next, we apply the MCDC technique to the re-
sults of SVM learning. For the result of correction
(RC1)1, we train SVM classifiers, and classify the
remaining unlabeled data (U \ N1). For the re-
sult of classification, we randomly select positive
(CP1) and negative (CN1) documents classified
by SVM and add to the SVM training data (RC1).
We re-train SVM classifiers with the training doc-
uments, and apply the MCDC. The procedure is
repeated until there are no unlabeled documents
judged to be either positive or negative. Finally,
the test data are classified using the final classi-
fiers. In the following subsections, we present the
MCDC procedure shown in Figure 2. It consists
of three steps: extraction of misclassified candi-
dates, estimation of error reduction, and correction
of misclassified candidates.
1The manually annotated positive examples are not cor-
rected.
Extraction
of miss-
classified
candidates
Training data D
test
learning
D ?SV (Support vectors)
Estimation of 
error reduction
classification
SV label
?
NB label
D ?Error candidates
Correction of
misclassified candidates
D1
D2
Final results
Error candidates
SVM NB
Loss function
Judgment using loss values
Figure 2: The MCDC procedure
2.1 Extraction of misclassified candidates
Let D be a set of training documents and xk ?
{x1, x2, ? ? ?, xm} be a SV of negative or positive
documents obtained by SVM.We remove ?mk=1xk
from the training documents D. The resulting
D \ ?mk=1xk is used for training Naive Bayes
(NB) (McCallum, 2001), leading to a classifica-
tion model. This classification model is tested on
each xk, and assigns a positive or negative label.
If the label is different from that assigned to xk,
we declare xk an error candidate.
2.2 Estimation of error reduction
We detect misclassified data from the extracted
candidates by estimating error reduction. The es-
timation of error reduction is often used in ac-
tive learning. The earliest work is the method of
Roy and McCallum (Roy and McCallum, 2001).
They proposed a method that directly optimizes
expected future error by log-loss or 0-1 loss, using
the entropy of the posterior class distribution on
a sample of unlabeled documents. We used their
method to detect misclassified data. Specifically,
we estimated future error rate by log-loss function.
It uses the entropy of the posterior class distribu-
tion on a sample of the unlabeled documents. A
loss function is defined by Eq (1).
EP?D2?(xk,yk) = ?
1
| X |
?
x?X
?
y?Y
P (y|x)
? log(P?D2?(xk,yk)(y|x)). (1)
Eq (1) denotes the expected error of the learner.
P (y | x) denotes the true distribution of out-
put classes y ? Y given inputs x. X denotes a
475
set of test documents. P?D2?(xk,yk)(y | x) shows
the learner?s prediction, and D2 denotes the train-
ing documents D except for the error candidates
?lk=1xk. If the value of Eq (1) is sufficiently
small, the learner?s prediction is close to the true
output distribution.
We used bagging to reduce variance of P (y | x)
as it is unknown for each test document x. More
precisely, from the training documents D, a dif-
ferent training set consisting of positive and nega-
tive documents is created2. The learner then cre-
ates a new classifier from the training documents.
The procedure is repeated m times3, and the final
class posterior for an instance is taken to be the un-
weighted average of the class posteriori for each of
the classifiers.
2.3 Correction of misclassified candidates
For each error candidate xk, we calculated the ex-
pected error of the learner, EP?D2?(xk,yk old) and
EP?D2?(xk,yk new) by using Eq (1). Here, yk old
refers to the original label assigned to xk, and
yk new is the resulting category label estimated by
NB classifiers. If the value of the latter is smaller
than that of the former, we declare the document
xk to be misclassified, i.e., the label yk old is an
error, and its true label is yk new. Otherwise, the
label of xk is yk old.
3 Experiments
3.1 Experimental setup
We chose the 1996 Reuters data (Reuters, 2000)
for evaluation. After eliminating unlabeled doc-
uments, we divided these into three. The data
(20,000 documents) extracted from 20 Aug to 19
Sept is used as training data indicating positive
and unlabeled documents. We set the range of ?
from 0.1 to 0.9 to create a wide range of scenar-
ios, where ? refers to the ratio of documents from
the positive class first selected from a fold as the
positive set. The rest of the positive and negative
documents are used as unlabeled data. We used
categories assigned to more than 100 documents
in the training data as it is necessary to examine
a wide range of ? values. These categories are 88
in all. The data from 20 Sept to 19 Nov is used
2We set the number of negative documents extracted ran-
domly from the unlabeled documents to the same number of
positive training documents.
3We set the number of m to 100 in the experiments.
as a test set X, to estimate true output distribu-
tion. The remaining data consisting 607,259 from
20 Nov 1996 to 19 Aug 1997 is used as a test data
for text classification. We obtained a vocabulary
of 320,935 unique words after eliminating words
which occur only once, stemming by a part-of-
speech tagger (Schmid, 1995), and stop word re-
moval. The number of categories per documents is
3.21 on average. We used the SVM-Light package
(Joachims, 1998)4. We used a linear kernel and set
all parameters to their default values.
We compared our method, MCDC with three
baselines: (1) SVM, (2) Positive Example-Based
Learning (PEBL) proposed by (Yu et al, 2002),
and (3) biased-SVM (Liu et al, 2003). We chose
PEBL because the convergence procedure is very
similar to our framework. Biased-SVM is the
state-of-the-art SVM method, and often used for
comparison (Elkan and Noto, 2008). To make
comparisons fair, all methods were based on a lin-
ear kernel. We randomly selected 1,000 positive
and 1,000 negative documents classified by SVM
and added to the SVM training data in each itera-
tion5. For biased-SVM, we used training data and
classified test documents directly. We empirically
selected values of two parameters, ?c? (trade-off
between training error and margin) and ?j?, i.e.,
cost (cost-factor, by which training errors on posi-
tive examples) that optimized the F-score obtained
by classification of test documents.
The positive training data in SVM are assigned
to the target category. The negative training data
are the remaining data except for the documents
that were assigned to the target category, i.e., this
is the ideal method as we used all the training data
with positive/negative labeled documents. The
number of positive training data in other three
methods depends on the value of ?, and the rest
of the positive and negative documents were used
as unlabeled data.
3.2 Text classification
Classification results for 88 categories are shown
in Figure 3. Figure 3 shows micro-averaged F-
score against the ? value. As expected, the re-
sults obtained by SVM were the best among all
? values. However, this is the ideal method
that requires 20,000 documents labeled posi-
tive/negative, while other methods including our
4http://svmlight.joachims.org
5We set the number of documents up to 1,000.
476
SVM PEBL Biased-SVM MCDC
Level (# of Cat) Cat F Cat F (Iter) Cat F (Iter) Cat F (Iter)
Best GSPO .955 GSPO .802 (26) CCAT .939 GSPO .946 (9)
Top (22) Worst GODD .099 GODD .079 (6) GODD .038 GODD .104 (4)
Avg .800 .475 (19) .593 .619 (8)
Best M14 .870 E71 .848 (7) M14 .869 M14 .875 (9)
Second (32) Worst C16 .297 E14 .161 (14) C16 .148 C16 .150 (3)
Avg .667 .383 (22) .588 .593 (7)
Best M141 .878 C174 .792 (27) M141 .887 M141 .885 (8)
Third (33) Worst G152 .102 C331 .179 (16) G155 .130 C331 .142 (6)
Avg .717 .313 (18) .518 .557 (8)
Fourth (1) ? C1511 .738 C1511 .481 (16) C1511 .737 C1511 .719 (4)
Micro Avg F-score .718 .428 (19) .614 .627 (8)
Table 1: Classification performance (? = 0.7)
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
F-
sc
or
e
Delta Value
SVM
PEBL
Biased-SVM
MCDC
Figure 3: F-score against the value of ?
method used only positive and unlabeled docu-
ments. Overall performance obtained by MCDC
was better for those obtained by PEBL and biased-
SVM methods in all ? values, especially when the
positive set was small, e.g., ? = 0.3, the improve-
ment of MCDC over biased-SVM and PEBL was
significant.
Table 1 shows the results obtained by each
method with a ? value of 0.7. ?Level? indi-
cates each level of the hierarchy and the numbers
in parentheses refer to the number of categories.
?Best? and ?Worst? refer to the best and the low-
est F-scores in each level of a hierarchy, respec-
tively. ?Iter? in PEBL indicates the number of it-
erations until the number of negative documents
is zero in the convergence procedure. Similarly,
?Iter? in the MCDC indicates the number of it-
erations until no unlabeled documents are judged
to be either positive or negative. As can be seen
clearly from Table 1, the results with MCDC were
better than those obtained by PEBL in each level
of the hierarchy. Similarly, the results were bet-
? SV Ec Err Correct
Prec Rec F
0.3 227,547 54,943 79,329 .693 .649 .670
0.7 141,087 34,944 42,385 .712 .673 .692
Table 2: Miss-classified data correction results
ter than those of biased-SVM except for the fourth
level, ?C1511?(Annual results). The average num-
bers of iterations with MCDC and PEBL were 8
and 19 times, respectively. In biased-SVM, it is
necessary to run SVM many times, as we searched
?c? and ?j?. In contrast, MCDC does not require
such parameter tuning.
3.3 Correction of misclassified candidates
Our goal is to achieve classification accuracy from
only positive documents and unlabeled data as
high as that from labeled positive and negative
data. We thus applied a miss-classified data de-
tection and correction technique for the classifica-
tion results obtained by SVM. Therefore, it is im-
portant to examine the accuracy of miss-classified
correction. Table 2 shows detection and correction
performance against all categories. ?SV? shows
the total number of SVs in 88 categories in all iter-
ations. ?Ec? refers to the total number of extracted
error candidates. ?Err? denotes the number of doc-
uments classified incorrectly by SVM and added
to the training data, i.e., the number of documents
that should be assigned correctly by the correction
procedure. ?Prec? and ?Rec? show the precision
and recall of correction, respectively.
Table 2 shows that precision was better than re-
call with both ? values, as the precision obtained
by ? value = 0.3 and 0.7 were 4.4% and 3.9%
improvement against recall values, respectively.
These observations indicated that the error candi-
dates extracted by our method were appropriately
477
corrected. In contrast, there were still other doc-
uments that were miss-classified but not extracted
as error candidates. We extracted error candidates
using the results of SVM and NB classifiers. En-
semble of other techniques such as boosting and
kNN for further efficacy gains seems promising to
try with our method.
4 Conclusion
The research described in this paper involved text
classification using positive and unlabeled data.
Miss-classified data detection and correction tech-
nique was incorporated in the existing classifica-
tion technique. The results using the 1996 Reuters
corpora showed that the method was comparable
to the current state-of-the-art biased-SVM method
as the F-score obtained by our method was 0.627
and biased-SVM was 0.614. Future work will in-
clude feature reduction and investigation of other
classification algorithms to obtain further advan-
tages in efficiency and efficacy in manipulating
real-world large corpora.
References
S. Abney, R. E. Schapire, and Y. Singer. 1999. Boost-
ing Applied to Tagging and PP Attachment. In Proc.
of the Joint SIGDAT Conference on EMNLP and
Very Large Corpora, pages 38?45.
L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos.
2013. Fast and Reliable Anomaly Detection in Cate-
gorical Data. In Proc. of the CIKM, pages 415?424.
A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy.
2001. Learning from Labeled and Unlabeled Data
using Graph Mincuts. In Proc. of the 18th ICML,
pages 19?26.
A. Boyd, M. Dickinson, and D. Meurers. 2008. On
Detecting Errors in Dependency Treebanks. Re-
search on Language and Computation, 6(2):113?
137.
R. G. Cabrera, M. M. Gomez, P. Rosso, and L. V.
Pineda. 2009. Using the Web as Corpus for
Self-Training Text Categorization. Information Re-
trieval, 12(3):400?415.
M. Dickinson and W. D. Meurers. 2005. Detecting
Errors in Discontinuous Structural Annotation. In
Proc. of the ACL?05, pages 322?329.
C. Elkan and K. Noto. 2008. Learning Classifiers from
Only Positive and Unlabeled Data. In Proc. of the
KDD?08, pages 213?220.
E. Eskin. 2000. Detectiong Errors within a Corpus us-
ing Anomaly Detection. In Proc. of the 6th ANLP
Conference and the 1st Meeting of the NAACL,
pages 148?153.
C. H. Ho, M. H. Tsai, and C. J. Lin. 2011. Active
Learning and Experimental Design with SVMs. In
Proc. of the JMLRWorkshop on Active Learning and
Experimental Design, pages 71?84.
T. Joachims. 1998. SVM Light Support Vector Ma-
chine. In Dept. of Computer Science Cornell Uni-
versity.
B. Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu. 2003.
Building Text Classifiers using Positive and Unla-
beled Examples. In Proc. of the ICDM?03, pages
179?188.
A. K. McCallum. 2001. Multi-label Text Classifica-
tion with a Mixture Model Trained by EM. In Re-
vised Version of Paper Appearing in AAAI?99 Work-
shop on Text Learning, pages 135?168.
Reuters. 2000. Reuters Corpus Volume1 English Lan-
guage. 1996-08-20 to 1997-08-19 Release Date
2000-11-03 Format Version 1.
N. Roy and A. K. McCallum. 2001. Toward Optimal
Active Learning through Sampling Estimation of Er-
ror Reduction. In Proc. of the 18th ICML, pages
441?448.
H. Schmid. 1995. Improvements in Part-of-Speech
Tagging with an Application to German. In Proc. of
the EACL SIGDAT Workshop, pages 47?50.
H. Yu, H. Han, and K. C-C. Chang. 2002. PEBL: Pos-
itive Example based Learning for Web Page Classi-
fication using SVM. In Proc. of the ACM Special
Interest Group on Knowledge Discovery and Data
Mining, pages 239?248.
478
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 21?30,
Beijing, August 2010
Automatic Classification of Semantic Relations
between Facts and Opinions
Koji Murakami? Eric Nichols? Junta Mizuno?? Yotaro Watanabe?
Hayato Goto? Megumi Ohki? Suguru Matsuyoshi? Kentaro Inui? Yuji Matsumoto?
?Nara Institute of Science and Technology
?Tohoku University
{kmurakami,matuyosi,hayato-g,megumi-o,matsu}@is.naist.jp
{eric-n,junta-m,inui}@ecei.tohoku.ac.jp
Abstract
Classifying and identifying semantic re-
lations between facts and opinions on
the Web is of utmost importance for or-
ganizing information on the Web, how-
ever, this requires consideration of a
broader set of semantic relations than are
typically handled in Recognizing Tex-
tual Entailment (RTE), Cross-document
Structure Theory (CST), and similar
tasks. In this paper, we describe the con-
struction and evaluation of a system that
identifies and classifies semantic rela-
tions in Internet data. Our system targets
a set of semantic relations that have been
inspired by CST but that have been gen-
eralized and broadened to facilitate ap-
plication to mixed fact and opinion data
from the Internet. Our system identi-
fies these semantic relations in Japanese
Web texts using a combination of lexical,
syntactic, and semantic information and
evaluate our system against gold stan-
dard data that was manually constructed
for this task. We will release all gold
standard data used in training and eval-
uation of our system this summer.
1 Introduction
The task of organizing the information on the In-
ternet to help users find facts and opinions on
their topics of interest is increasingly important
as more people turn to the Web as a source of
important information. The vast amounts of re-
search conducted in NLP on automatic summa-
rization, opinion mining, and question answer-
ing are illustrative of the great interest in mak-
ing relevant information easier to find. Provid-
ing Internet users with thorough information re-
quires recognizing semantic relations between
both facts and opinions, however the assump-
tions made by current approaches are often in-
compatible with this goal. For example, the
existing semantic relations considered in Rec-
ognizing Textual Entailment (RTE) (Dagan et
al., 2005) are often too narrow in scope to be
directly applicable to text on the Internet, and
theories like Cross-document Structure Theory
(CST) (Radev, 2000) are only applicable to facts
or second-hand reporting of opinions rather than
relations between both.
As part of the STATEMENT MAP project we
proposed the development of a system to sup-
port information credibility analysis on the Web
(Murakami et al, 2009b) by automatically sum-
marizing facts and opinions on topics of inter-
est to users and showing them the evidence and
conflicts for each viewpoint. To facilitate the de-
tection of semantic relations in Internet data, we
defined a sentence-like unit of information called
the statement that encompasses both facts and
opinions, started compiling a corpus of state-
ments annotated with semantic relations (Mu-
rakami et al, 2009a), and begin constructing a
system to automatically identify semantic rela-
tions between statements.
In this paper, we describe the construction and
evaluation of a prototype semantic relation iden-
tification system. We build on the semantic rela-
tions proposed in RTE and CST and in our pre-
vious work, refining them into a streamlined set
of semantic relations that apply across facts and
opinions, but that are simple enough to make
automatic recognition of semantic relations be-
tween statements in Internet text possible.Our
semantic relations are [AGREEMENT], [CON-
FLICT], [CONFINEMENT], and [EVIDENCE].
[AGREEMENT] and [CONFLICT] are expansions
of the [EQUIVALENCE] and [CONTRADICTION]
21
relations used in RTE. [CONFINEMENT] and
[EVIDENCE] are new relations between facts
and opinions that are essential for understanding
how statements on a topic are inter-related.
Our task differs from opinion mining and sen-
timent analysis which largely focus on identify-
ing the polarity of an opinion for defined param-
eters rather than identify how facts and opinions
relate to each other, and it differs from web doc-
ument summarization tasks which focus on ex-
tracting information fromweb page structure and
contextual information from hyperlinks rather
than analyzing the semantics of the language on
the webpage itself.
We present a system that automatically iden-
tifies semantic relations between statements in
Japanese Internet texts. Our system uses struc-
tural alignment to identify statement pairs that
are likely to be related, then classifies seman-
tic relations using a combination of lexical, syn-
tactic, and semantic information. We evaluate
cross-statement semantic relation classification
on sentence pairs that were taken from Japanese
Internet texts on several topics and manually an-
notated with a semantic relation where one is
present. In our evaluation, we look closely at the
impact that each of the resources has on semantic
relation classification quality.
The rest of this paper is organized as follows.
In Section 2, we discuss related work in summa-
rization, semantic relation classification, opinion
mining, and sentiment analysis, showing how
existing classification schemes are insufficient
for our task. In Section 3, we introduce a set of
cross-sentential semantic relations for use in the
opinion classification needed to support informa-
tion credibility analysis on the Web. In Section
4, we present our cross-sentential semantic re-
lation recognition system, and discuss the algo-
rithms and resources that are employed. In Sec-
tion 5, we evaluate our system in a semantic rela-
tion classification task. In Section 6, we discuss
our findings and conduct error analysis. Finally,
we conclude the paper in Section 7.
2 Related Work
2.1 Recognizing Textual Entailment
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment, the
task of deciding whether the meaning of one
text is entailed from another text. A major
task in the RTE Challenge (Recognizing Tex-
tual Entailment Challenge) is classifying the se-
mantic relation between a Text (T) and a Hy-
pothesis (H) into [ENTAILMENT], [CONTRA-
DICTION], or [UNKNOWN]. Over the last sev-
eral years, several corpora annotated with thou-
sands of (T,H) pairs have been constructed for
this task. In these corpora, each pair was tagged
indicating its related task (e.g. Information Ex-
traction, Question Answering, Information Re-
trieval or Summarization).
The RTE Challenge has successfully em-
ployed a variety of techniques in order to rec-
ognize instances of textual entailment, including
methods based on: measuring the degree of lex-
ical overlap between bag of words (Glickman
et al, 2005; Jijkoun and de Rijke, 2005), the
alignment of graphs created from syntactic or se-
mantic dependencies (Marsi and Krahmer, 2005;
MacCartney et al, 2006), statistical classifiers
which leverage a wide range of features (Hickl
et al, 2005), or reference rule generation (Szpek-
tor et al, 2007). These approaches have shown
great promise in RTE for entailment pairs in the
corpus, but more robust models of recognizing
logical relations are still desirable.
The definition of contradiction in RTE is that
T contradicts H if it is very unlikely that both T
and H can be true at the same time. However, in
real documents on the Web, there are many pairs
of examples which are contradictory in part, or
where one statement confines the applicability of
another, as shown in the examples in Table 1.
2.2 Cross-document Structure Theory
Cross-document Structure Theory (CST), devel-
oped by Radev (2000), is another task of rec-
ognizing semantic relations between sentences.
CST is an expanded rhetorical structure analy-
sis based on Rhetorical Structure Theory (RST:
(William and Thompson, 1988)), and attempts
to describe the semantic relations that exist
between two or more sentences from differ-
ent source documents that are related to the
same topic, as well as those that come from
a single source document. A corpus of cross-
document sentences annotated with CST rela-
tions has also been constructed (The CSTBank
Corpus: (Radev et al, 2003)). CSTBank is
organized into clusters of topically-related ar-
ticles. There are 18 kinds of semantic rela-
tions in this corpus, not limited to [EQUIVA-
LENCE] or [CONTRADICTION], but also includ-
ing [JUDGEMENT], [ELABORATION], and [RE-
22
Query Matching sentences Output
??????????????
???
?????????????????????????????????? ??
The cavity-prevention effects are greater the more Xylitol is included. [AGREEMENT].
????????????????????????????? ??
Xylitol is effective at preventing
cavities.
Xylitol shows effectiveness at maintaining good oral hygiene and preventing cavities. [AGREEMENT]
???????????????????????????????????
?????????????
??
There are many opinions about the cavity-prevention effectiveness of Xylitol, but it
is not really effective.
[CONFLICT]
?????????
???????????????????????????????? ??
Reduced water, which has weak alkaline ions, supports the health of you and your
family.
[AGREEMENT]
?????????????????????????????????? ??
Reduced water is good for the
health.
Reduced water is said to remove active oxygen from the body, making it effective at
promoting good health.
[AGREEMENT]
??????????????????????? ??
Even if oxidized water tastes good, it does not help one?s health. [CONFLICT]
??????????????
???
???????????????????????????????????
?????????
??
Isoflavone is effective at
maintaining good health.
Taking too much soy isoflavone as a supplement will have a negative effect on one?s
health
[CONFINEMENT]
Table 1: Example semantic relation classification.
FINEMENT]. Etoh et al (Etoh and Okumura,
2005) constructed a Japanese Cross-document
Relation Corpus, and they redefined 14 kinds of
semantic relations in their corpus.
CST was designed for objective expressions
because its target data is newspaper articles re-
lated to the same topic. Facts, which can be ex-
tracted from newspaper articles, have been used
in conventional NLP research, such as Informa-
tion Extraction or Factoid Question Answering.
However, there are a lot of opinions on the Web,
and it is important to survey opinions in addition
to facts to give Internet users a comprehensive
view of the discussions on topics of interest.
2.3 Cross-document Summarization Based
on CST Relations between Sentences
Zhang and Radev (2004) attempted to classify
CST relations between sentence pairs extracted
from topically related documents. However, they
used a vector space model and tried multi-class
classification. The results were not satisfactory.
This observation may indicate that the recog-
nition methods for each relation should be de-
veloped separately. Miyabe et al (2008) at-
tempted to recognize relations that were defined
in a Japanese cross-document relation corpus
(Etoh and Okumura, 2005). However, their tar-
get relations were limited to [EQUIVALENCE]
and [TRANSITION]; other relations were not tar-
geted. Recognizing [EVIDENCE] is indispens-
able for organizing information on the Internet.
We need to develop satisfactory methods of [EV-
IDENCE] recognition.
2.4 Opinion Mining and Sentiment Analysis
Subjective statements, such as opinions, have
recently been the focus of much NLP re-
search including review analysis, opinion ex-
traction, opinion question answering, and senti-
ment analysis. In the corpus constructed in the
Multi-Perspective Question Answering (MPQA)
Project (Wiebe et al, 2005), individual expres-
sions are tagged that correspond to explicit men-
tions of private states, speech event, and expres-
sive subjective elements.
The goal of opinion mining to extract expres-
sions with polarity from texts, not to recognize
semantic relations between sentences. Sentiment
analysis also focus classifying subjective expres-
sions in texts into positive/negative classes. In
comparison, although we deal with sentiment in-
formation in text, our objective is to recognize
semantic relations between sentences. If a user?s
query requires positive/negative information, we
will also need to extract sentences including sen-
timent expression like in opinion mining, how-
ever, our semantic relation, [CONFINEMENT], is
more precise because it identifies the condition
or scope of the polarity. Queries do not neces-
sarily include sentiment information; we also ac-
cept queries that are intended to be a statement
of fact. For example, for the query ?Xylitol is
effective at preventing cavities.? in Table 1, we
extract a variety of sentences from the Web and
recognize semantic relations between the query
and many kinds of sentences.
23
3 Semantic Relations between
Statements
In this section, we define the semantic relations
that we will classify in Japanese Internet texts as
well as their corresponding relations in RTE and
CST. Our goal is to define semantic relations that
are applicable over both fact and opinions, mak-
ing them more appropriate for handling Internet
texts. See Table 1 for real examples.
3.1 [AGREEMENT]
A bi-directional relation where statements have
equivalent semantic content on a shared topic.
Here we use topic in a narrow sense to mean that
the semantic contents of both statements are rel-
evant to each other.
The following is an example of [AGREE-
MENT] on the topic of bio-ethanol environmental
impact.
(1) a. Bio-ethanol is good for the environment.
b. Bio-ethanol is a high-quality fuel, and it
has the power to deal with the environ-
ment problems that we are facing.
Once relevance has been established,
[AGREEMENT] can range from strict logi-
cal entailment or identical polarity of opinions.
Here is an example of two statements that
share a broad topic, but that are not classified as
[AGREEMENT] because preventing cavities and
tooth calcification are not intuitively relevant.
(2) a. Xylitol is effective at preventing cavities.
b. Xylitol advances tooth calcification.
3.2 [CONFLICT]
A bi-directional relation where statements have
negative or contradicting semantic content on a
shared topic. This can range from strict logical
contradiction to opposite polarity of opinions.
The next pair is a [CONFLICT] example.
(3) a. Bio-ethanol is good for our earth.
b. There is a fact that bio-ethanol further the
destruction of the environment.
3.3 [EVIDENCE]
A uni-directional relation where one statement
provides justification or supporting evidence for
the other. Both statements can be either facts or
opinions. The following is a typical example:
(4) a. I believe that applying the technology of
cloning must be controlled by law.
b. There is a need to regulate cloning, be-
cause it can be open to abuse.
The statement containing the evidence con-
sists of two parts: one part has a [AGREEMENT]
or [CONFLICT] with the other statement, the
other part provides support or justification for it.
3.4 [CONFINEMENT]
A uni-directional relation where one statement
provides more specific information about the
other or quantifies the situations in which it ap-
plies. The pair below is an example, in which
one statement gives a condition under which the
other can be true.
(5) a. Steroids have side-effects.
b. There is almost no need to worry about
side-effects when steroids are used for lo-
cal treatment.
4 Recognizing Semantic Relations
In order to organize the information on the
Internet, we need to identify the [AGREE-
MENT], [CONFLICT], [CONFINEMENT], and
[EVIDENCE] semantic relations. Because iden-
tification of [AGREEMENT] and [CONFLICT] is
a problem of measuring semantic similarity be-
tween two statements, it can be cast as a sen-
tence alignment problem and solved using an
RTE framework. The two sentences do not need
to be from the same source.
However, the identification of [CONFINE-
MENT] and [EVIDENCE] relations depend on
contextual information in the sentence. For ex-
ample, conditional statements or specific dis-
course markers like ?because? act as important
cues for their identification. Thus, to identify
these two relations across documents, we must
first identify [AGREEMENT] or [CONFLICT] be-
tween sentences in different documents and then
determine if there is a [CONFINEMENT] or [EV-
IDENCE] relation in one of the documents.
Furthermore, the surrounding text often con-
tains contextual information that is important for
identifying these two relations. Proper handling
of surrounding context requires discourse analy-
sis and is an area of future work, but our basic
detection strategy is as follows:
1. Identify a [AGREEMENT] or [CONFLICT] re-
lation between the Query and Text
2. Search the Text sentence for cues that iden-
tify [CONFINEMENT] or [EVIDENCE]
24
3. Infer the applicability of the [CONFINE-
MENT] or [EVIDENCE] relations in the Text
to the Query
4.1 System Overview
We have finished constructing a prototype sys-
tem that detects semantic relation between state-
ments. It has a three-stage architecture similar to
the RTE system of MacCartney et al (2006):
1. Linguistic analysis
2. Structural alignment
3. Feature extraction for detecting [EVIDENCE]
and [CONFINEMENT]
4. Semantic relation classification
However, we differ in the following respects.
First, our relation classification is broader than
RTE?s simple distinction between [ENTAIL-
MENT], [CONTRADICTION], and [UNKNOWN];
in place of [ENTAILMENT] and [CONTRA-
DICTION, we use broader [AGREEMENT] and
[CONFLICT] relations. We also consider cover
gradations of applicability of statements with the
[CONFINEMENT] relation.
Second, we conduct structural alignment with
the goal of aligning semantic structures. We do
this by directly incorporating dependency align-
ments and predicate-argument structure informa-
tion for both the user query and the Web text
into the alignment scoring process. This allows
us to effectively capture many long-distance
alignments that cannot be represented as lexical
alignments. This contrasts with MacCartney et
al. (2006), who uses dependency structures for
the Hypothesis to reduce the lexical alignment
search space but do not produce structural align-
ments and do not use the dependencies in detect-
ing entailment.
Finally, we apply several rich semantic re-
sources in alignment and classification: extended
modality information that helps align and clas-
sify structures that are semantically similar but
divergent in tense or polarity; and lexical simi-
larity through ontologies like WordNet.
4.2 Linguistic Analysis
In order to identify semantic relations between
the user query (Q) and the sentence extracted
from Web text (T), we first conduct syntactic and
semantic linguistic analysis to provide a basis for
alignment and relation classification.
For syntactic analysis, we use the Japanese
dependency parser CaboCha (Kudo and Mat-
!"#$%&'!"#$!%&'()*'+$! ()*!,'-!.-'/'0+1!#$)23#!+,-!$4$50*$!./!%&!6! 7!!"#$%&'!"#$!%&'()*'+$!01234!'&3$'.'-'&%&!
567!*)-%'8&!
897:;!&85#!)&!9!3-$)3/$+3!
<=>?@A/!#)&!:$$+!&#';+!
()BC-! B! C!?!
)!
5!:!J!3'!#)*$!!#$)23#!)..2%5)0'+&!
C!J!
J!
B!
B!C!
Figure 1: An example of structural alignment
sumoto, 2002) and the predicate-argument struc-
ture analyzer ChaPAS (Watanabe et al, 2010).
CaboCha splits the Japanese text into phrase-like
chunks and represents syntactic dependencies
between the chunks as edges in a graph. Cha-
PAS identifies predicate-argument structures in
the dependency graph produced by CaboCha.
We also conduct extended modality analysis
using the resources provided by Matsuyoshi et
al. (2010), focusing on tense, modality, and po-
larity, because such information provides impor-
tant clues for the recognition of semantic rela-
tions between statements.
4.3 Structural Alignment
In this section, we describe our approach to
structural alignment. The structural alignment
process is shown in Figure 1. It consists of the
following two phases:
1. lexical alignment
2. structural alignment
We developed a heuristic-based algorithm to
align chunk based on lexical similarity infor-
mation. We incorporate the following informa-
tion into an alignment confidence score that has
a range of 0.0-1.0 and align chunk whose scores
cross an empirically-determined threshold.
? surface level similarity: identical content
words or cosine similarity of chunk contents
? semantic similarity of predicate-argument
structures
predicates we check for matches in predi-
cate entailment databases (Hashimoto et
al., 2009; Matsuyoshi et al, 2008) consid-
ering the default case frames reported by
ChaPAS
arguments we check for synonym or hy-
pernym matches in the Japanese WordNet
(2008) or the Japanese hypernym collec-
tion of Sumida et al (2008)
25
>?@???????????????????AB?C????DEF)!
>?'???????????????????AB?C????GHF)!I!
T :!
H :!
(field) (in)!(agricultural chemicals) (ACC)! (use)!
(field) (on)!(agricultural chemicals) (ACC)! (spray)!
Figure 2: Determining the compatibility of se-
mantic structures
We compare the predicate-argument structure
of the query to that of the text and determine
if the argument structures are compatible. This
process is illustrated in Figure 2 where the T(ext)
?Agricultural chemicals are used in the field.? is
aligned with the H(ypothesis) ?Over the field,
agricultural chemicals are sprayed.? Although
the verbs used and sprayed are not directly se-
mantically related, they are aligned because they
share the same argument structures. This lets up
align predicates for which we lack semantic re-
sources. We use the following information to de-
termine predicate-argument alignment:
? the number of aligned children
? the number of aligned case frame arguments
? the number of possible alignments in a win-
dow of n chunk
? predicates indicating existence or quantity.
E.g. many, few, to exist, etc.
? polarity of both parent and child chunks us-
ing the resources in (Higashiyama et al,
2008; Kobayashi et al, 2005)
We treat structural alignment as a machine
learning problem and train a Support Vector Ma-
chine (SVM) model to decide if lexically aligned
chunks are semantically aligned.
We train on gold-standard labeled alignment
of 370 sentence pairs. This data set is described
in more detail in Section 5.1. As features for our
SVM model, we use the following information:
? the distance in edges in the dependency graph
between parent and child for both sentences
? the distance in chunks between parent and
child in both sentences
? binary features indicating whether each
chunk is a predicate or argument according
to ChaPAS
? the parts-of-speech of first and last word in
each chunk
? when the chunk ends with a case marker, the
case of the chunk , otherwise none
? the lexical alignment score of each chunk
pair
4.4 Feature Extraction for Detecting
Evidence and Confinement
Once the structural alignment system has iden-
tified potential [AGREEMENT] or [CONFLICT]
relations, we need to extract contextual cues in
the Text as features for detecting [CONFINE-
MENT] and [EVIDENCE] relations. Conditional
statements, degree adverbs, and partial negation,
which play a role in limiting the scope or degree
of a query?s contents in the statement, can be im-
portant cues for detecting the these two semantic
relations. We currently use a set of heuristics to
extract a set of expressions to use as features for
classifying these relations using SVM models.
4.5 Relation Classification
Once the structural alignment is successfully
identified, the task of semantic relation classi-
fication is straightforward. We also solve this
problem with machine learning by training an
SVM classifier. As features, we draw on a com-
bination of lexical, syntactic, and semantic infor-
mation including the syntactic alignments from
the previous section. The feature set is:
alignments We define two binary function,
ALIGNword(qi, tm) for the lexical align-
ment and ALIGNstruct((qi, qj), (tm, tk))
for the structural alignment to be true if and
only if the node qi, qj ? Q has been seman-
tically and structurally aligned to the node
tm, tk ? T . Q and T are the (Q)uery and the
(T)ext, respectively. We also use a separate
feature for a score representing the likelihood
of the alignment.
modality We have a feature that encodes all of
the possible polarities of a predicate node
from modality analysis, which indicates the
utterance type, and can be assertive, voli-
tional, wish, imperative, permissive, or in-
terrogative. Modalities that do not repre-
sent opinions (i.e. imperative, permissive and
interrogative) often indicate [OTHER] rela-
tions.
antonym We define a binary function
ANTONYM(qi, tm) that indicates if
the pair is identified as an antonym. This
information helps identify [CONFLICT].
26
Relation Measure 3-class Cascaded 3-class ?
[AGREEMENT] precision 0.79 (128 / 162) 0.80 (126 / 157) +0.01
[AGREEMENT] recall 0.86 (128 / 149) 0.85 (126 / 149) -0.01
[AGREEMENT] f-score 0.82 0.82 -
[CONFLICT] precision 0 (0 / 5) 0.36 (5 / 14) +0.36
[CONFLICT] recall 0 (0 / 12) 0.42 (5 / 12) +0.42
[CONFLICT] f-score 0 0.38 +0.38
[CONFINEMENT] precision 0.4 (4 / 10) 0.8 (4 / 5) +0.4
[CONFINEMENT] recall 0.17 (4 / 23) 0.17 (4 / 23) -
[CONFINEMENT] f-score 0.24 0.29 +0.05
Table 2: Semantic relation classification results comparing 3-class and cascaded 3-class approaches
negation To identify negations, we primar-
ily rely on a predicate?s Actuality value,
which represents epistemic modality and
existential negation. If a predicate pair
ALIGNword(qi, tm) has mismatching actu-
ality labels, the pair is likely a [OTHER].
contextual cues This set of features is used to
mark the presence of any contextual cues
that identify of [CONFINEMENT] or [EVI-
DENCE] relations in a chunk . For example,
??? (because)? or ??? (due to)? are typ-
ical contextual cues for [EVIDENCE], and ?
?? (when)? or ???? (if)? are typical for
[CONFINEMENT].
5 Evaluation
5.1 Data Preparation
In order to evaluate our semantic relation clas-
sification system on realistic Web data, we con-
structed a corpus of sentence pairs gathered from
a vast collection of webpages (2009a). Our basic
approach is as follows:
1. Retrieve documents related to a set number
of topics using the Tsubaki1 search engine
2. Extract real sentences that include major sub-
topic words which are detected based on
TF/IDF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate
5. Annotate corresponding sentences with
[AGREEMENT], [CONFLICT], [CONFINE-
MENT], or [OTHER]
1http://tsubaki.ixnlp.nii.ac.jp/
Although our target semantic relations in-
clude [EVIDENCE], they difficult annotate con-
sistently, so we do not annotate them at this
time. Expanding our corpus and semantic re-
lation classifier to handle [EVIDENCE] remains
and area of future work.
The data that composes our corpus comes
from a diverse number of sources. A hand sur-
vey of a random sample of the types of domains
of 100 document URLs is given below. Half of
the URL domains were not readily identifiable,
but the known URL domains included govern-
mental, corporate, and personal webpages. We
believe this distribution is representative of in-
formation sources on the Internet.
type count
academic 2
blogs 23
corporate 10
governmental 4
news 5
press releases 4
q&a site 1
reference 1
other 50
We have made a partial release of our corpus
of sentence pairs manually annotated with the
correct semantic relations2. We will fully release
all the data annotated semantic relations and with
gold standard alignments at a future date.
5.2 Experiment Settings
In this section, we present results of empiri-
cal evaluation of our proposed semantic rela-
tion classification system on the dataset we con-
structed in the previous section. For this experi-
ment, we use SVMs as described in Section 4.5
2http://stmap.naist.jp/corpus/ja/
index.html (in Japanese)
27
to classify semantic relations into one of the four
classes: [AGREEMENT], [CONFLICT], [CON-
FINEMENT], or [OTHER] in the case of no re-
lation. As data we use 370 sentence pairs that
have been manually annotated both with the cor-
rect semantic relation and with gold standard
alignments. Annotations are checked by two na-
tive speakers of Japanese, and any sentence pair
where annotation agreement is not reached is
discarded. Because we have limited data that is
annotated with correct alignments and semantic
relations, we perform five-fold cross validation,
training both the structural aligner and semantic
relation classifier on 296 sentence pairs and eval-
uating on the held out 74 sentence pairs. The
figures presented in the next section are for the
combined results on all 370 sentence pairs.
5.3 Results
We compare two different approaches to classi-
fication using SVMs:
3-class semantic relations are directly classified
into one of [AGREEMENT], [CONFLICT],
and [CONFINEMENT] with all features de-
scribed in 4.5
cascaded 3-class semantic relations are first
classified into one of [AGREEMENT], [CON-
FLICT] without contextual cue features. Then
an additional judgement with all features de-
termines if [AGREEMENT] and [CONFLICT]
should be reclassified as [CONFINEMENT]
Initial results using the 3-class classifica-
tion model produced high f-scores for [AGREE-
MENT] but unfavorable results for [CONFLICT]
and [CONFINEMENT]. We significantly im-
proved classification of [CONFLICT] and [CON-
FINEMENT] by adopting the cascaded 3-class
model. We present these results in Table 2 and
successfully recognized examples in Table 1.
6 Discussion and Error Analysis
We constructed a prototype semantic relation
classification system by combining the compo-
nents described in the previous section. While
the system developed is not domain-specific and
capable of accepting queries on any topic, we
evaluate its semantic relation classification on
several queries that are representative of our
training data.
Figure 3 shows a snapshot of the semantic re-
lation classification system and the various se-
mantic relations it recognized for the query.
Baseline Structural Upper-boundAlignment
Precision 0.44 0.52 0.74(56/126) (96/186) (135/183)
Recall 0.30 0.52 0.73(56/184) (96/184) (135/184)
F1-score 0.36 0.52 0.74
Table 3: Comparison of lexical, structural, and
upper-bound alignments on semantic relation
classification
In the example (6), recognized as [CONFINE-
MENT] in Figure 3, our system correctly identi-
fied negation and analyzed the description ?Xyl-
itol alone can not completely? as playing a role
of requirement.
(6) a. ?????????????????
(Xylitol is effective at preventing cavi-
ties.)
b. ?????????????????
????
(Xylitol alone can not completely prevent
cavities.)
Our system correctly identifies [AGREE-
MENT] relations in other examples about re-
duced water from Table 1 by structurally align-
ing phrases like ?promoting good health? and
?supports the health? to ?good for the health.?
These examples show how resources like
(Matsuyoshi et al, 2010) and WordNet (Bond et
al., 2008) have contributed to the relation clas-
sification improvement of structural alignment
over them baseline in Table 3. Focusing on sim-
ilarity of syntactic and semantic structures gives
our alignment method greater flexibility.
However, there are still various examples
which the system cannot recognized correctly.
In examples on cavity prevention, the phrase
?effective at preventing cavities? could not be
aligned with ?can prevent cavities? or ?good for
cavity prevention,? nor can ?cavity prevention?
and ?cavity-causing bacteria control.?
The above examples illustrate the importance
of the role played by the alignment phase in the
whole system?s performance.
Table 3 compares the semantic relation classi-
fication performance of using lexical alignment
only (as the baseline), lexical alignment and
structural alignment, and, to calculate the maxi-
mum possible precision, classification using cor-
rect alignment data (the upper-bound). We can
28
Figure 3: Alignment and classification example for the query ?Xylitol is effective at preventing
cavities.?
see that structural alignment makes it possible to
align more words than lexical alignment alone,
leading to an improvement in semantic relation
classification. However, there is still a large gap
between the performance of structural alignment
and the maximum possible precision. Error anal-
ysis shows that a big cause of incorrect classifi-
cation is incorrect lexical alignment. Improving
lexical alignment is a serious problem that must
be addressed. This entails expanding our cur-
rent lexical resources and finding more effective
methods of apply them in alignment.
The most serious problem we currently face is
the feature engineering necessary to find the op-
timal way of applying structural alignments or
other semantic information to semantic relation
classification. We need to conduct a quantita-
tive evaluation of our current classification mod-
els and find ways to improve them.
7 Conclusion
Classifying and identifying semantic relations
between facts and opinions on the Web is of ut-
most importance to organizing information on
the Web, however, this requires consideration of
a broader set of semantic relations than are typi-
cally handled in RTE, CST, and similar tasks. In
this paper, we introduced a set of cross-sentential
semantic relations specifically designed for this
task that apply over both facts and opinions. We
presented a system that identifies these semantic
relations in Japanese Web texts using a combina-
tion of lexical, syntactic, and semantic informa-
tion and evaluated our system against data that
was manually constructed for this task. Prelimi-
nary evaluation showed that we are able to detect
[AGREEMENT] with high levels of confidence.
Our method also shows promise in [CONFLICT]
and [CONFINEMENT] detection. We also dis-
cussed some of the technical issues that need to
be solved in order to identify [CONFLICT] and
[CONFINEMENT].
Acknowledgments
This work is supported by the National Institute
of Information and Communications Technology
Japan.
References
Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and
Kiyotaka Uchimoto. 2008. Boot-strapping a
wordnet using multiple existing wordnets. In Proc.
of the 6th International Language Resources and
Evaluation (LREC?08).
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proc. of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Etoh, Junji and Manabu Okumura. 2005. Cross-
document relationship between sentences corpus.
29
In Proc. of the 14th Annual Meeting of the Associa-
tion for Natural Language Processing, pages 482?
485. (in Japanese).
Glickman, Oren, Ido Dagan, and Moshe Koppel.
2005. Web based textual entailment. In Proc. of
the First PASCAL Recognizing Textual Entailment
Workshop.
Hashimoto, Chikara, Kentaro Torisawa, Kow
Kuroda, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition
from the web. In Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP2009), pages 1172?1181.
Hickl, Andrew, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2005. Recog-
nizing textual entailment with lcc?s groundhog sys-
tem. In Proc. of the Second PASCAL Challenges
Workshop.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring noun polarity knowl-
edge using selectional preferences. In Proc. of the
14th Annual Meeting of the Association for Natu-
ral Language Processing.
Jijkoun, Valentin and Maarten de Rijke. 2005. Rec-
ognizing textual entailment using lexical similar-
ity. In Proc. of the First PASCAL Challenges Work-
shop.
Kobayashi, Nozomi, Kentaro Inui, Yuji Matsumoto,
Kenji Tateishi, and Toshikazu Fukushima. 2005.
Collecting evaluative expressions for opinion ex-
traction. Journal of natural language processing,
12(3):203?222.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc of CoNLL 2002, pages 63?69.
MacCartney, Bill, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D.
Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proc. of
HLT/NAACL 2006.
Marsi, Erwin and Emiel Krahmer. 2005. Classifi-
cation of semantic relations by humans and ma-
chines. In Proc. of ACL-05 Workshop on Empiri-
cal Modeling of Semantic Equivalence and Entail-
ment, pages 1?6.
Matsuyoshi, Suguru, Koji Murakami, Yuji Mat-
sumoto, and Kentaro Inui. 2008. A database of re-
lations between predicate argument structures for
recognizing textual entailment and contradiction.
In Proc. of the Second International Symposium
on Universal Communication, pages 366?373, De-
cember.
Matsuyoshi, Suguru, Megumi Eguchi, Chitose Sao,
Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2010. Annotating event mentions in text
with modality, focus, and source information. In
Proc. of the 7th International Language Resources
and Evaluation (LREC?10), pages 1456?1463.
Miyabe, Yasunari, Hiroya Takamura, and Manabu
Okumura. 2008. Identifying cross-document re-
lations between sentences. In Proc. of the 3rd In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-08), pages 141?148.
Murakami, Koji, Shouko Masuda, Suguru Mat-
suyoshi, Eric Nichols, Kentaro Inui, and Yuji Mat-
sumoto. 2009a. Annotating semantic relations
combining facts and opinions. In Proceedings of
the Third Linguistic Annotation Workshop, pages
150?153, Suntec, Singapore, August. Association
for Computational Linguistics.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009b. Statement map: Assist-
ing information credibility analysis by visualizing
arguments. In Proc. of the 3rd ACM Workshop
on Information Credibility on the Web (WICOW
2009), pages 43?50.
Radev, Dragomir, Jahna Otterbacher,
and Zhu Zhang. 2003. CSTBank:
Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Radev, Dragomir R. 2000. Common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In Proc. of the 1st SIG-
dial workshop on Discourse and dialogue, pages
74?83.
Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in wikipedia. In Proc. of the 6th International
Language Resources and Evaluation (LREC?08).
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
456?463.
Watanabe, Yotaro, Masayuki Asahara, and Yuji Mat-
sumoto. 2010. A structured model for joint learn-
ing of argument roles and predicate senses. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (to appear).
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2-3):165?210.
William, Mann and Sandra Thompson. 1988.
Rhetorical structure theory: towards a functional
theory of text organization. Text, 8(3):243?281.
Zhang, Zhu and Dragomir Radev. 2004. Combin-
ing labeled and unlabeled data for learning cross-
document structural relationships. In Proc. of the
Proceedings of IJC-NLP.
30
Recognizing Confinement in Web Texts
Megumi Ohki?
Suguru Matsuyoshi?
Junta Mizuno?
Kentaro Inui?
Nara Insutitute of Science and Technology?
Eric Nichols?
Koji Murakami??
Shouko Masuda?
Yuji Matsumoto?
Tohoku University?
{megumi-o,shouko,matuyosi,matsu}@is.naist.jp
{eric,junta-m,inui}@tohoku.ac.jp
koji.murakami@mail.rakuten.co.jp
Abstract
In the Recognizing Textual Entailment (RTE) task, sentence pairs are classified into one of three se-
mantic relations: ENTAILMENT, CONTRADICTION or UNKNOWN. While we find some sentence pairs
hold full entailments or contradictions, there are a number of pairs that partially entail or contradict one
another depending on a specific situation. These partial contradiction sentence pairs contain useful infor-
mation for opinion mining and other such tasks, but it is difficult for Internet users to access this knowledge
because current frameworks do not differentiate between full contradictions and partial contradictions. In
this paper, under current approaches to semantic relation recognition, we define a new semantic relation
known as CONFINEMENT in order to recognize this useful information. This information is classified as
either CONTRADICTION or ENTAILMENT. We provide a series of semantic templates to recognize CON-
FINEMENT relations in Web texts, and then implement a system for recognizing CONFINEMENT between
sentence pairs. We show that our proposed system can obtains a F-score of 61% for recognizing CON-
FINEMENT in Japanese-language Web texts, and it outperforms a baseline which does not use a manually
compiled list of lexico-syntactic patterns to instantiate the semantic templates.
1 Introduction
On the Internet, there are various kinds of documents, and they often include conflicting opinions or
differing information on a single topic. Collecting and organizing this diverse information is an important
part of multi-document summarization.
When searching with a particular query on the Internet, we want information that tells us what other
people think about the query: e.g. do they believe it is true or not; what are the necessary conditions
for it to apply. For example, consider the hypothetical search results for the query given in (1). You get
opinion (2a), which supports the query, and opinion (2b) which opposes it.
(1) Xylitol is effective at preventing tooth decay.
(2) a. Xylitol can prevent tooth decay.
b. Xylitol is not effective at all at preventing tooth decay.
A major task in the Recognizing Textual Entailment (RTE) Challenge (Giampiccolo et al (2007)) is
classifying the semantic relation between a Text and a Hypothesis into ENTAILMENT, CONTRADICTION,
or UNKNOWN. Murakami et al (2009) report on the STATEMENT MAP project, the goal of which is
to help Internet users evaluate the credibility of information sources by analyzing supporting evidence
from a variety of viewpoints on their topics of interest and presenting them to users together with the
supporting evidence in a way that makes it clear how they are related. A variety of techniques have been
successfully employed in the RTE Challenge in order to recognize instances of textual entailment.
?Current afflication: Rakuten Institute of Technology
215
However, as far as we know, there have been no studies on recognizing sentences which specify condi-
tions under which a query applies, despite the fact that these relations are useful information for Internet
users. Such useful sentences are plentiful on the Web. Consider the following examples of CONTRA-
DICTION and ENTAILMENT.
(3) a. Xylitol can not prevent tooth decay if it not at least 50%.
b. The effect of Xylitol on preventing tooth decay is limited.
In example (3a), the necessary condition to prevent tooth decay by Xylitol is ?it contains at least fifty
percent Xylitol?. That condition is expressed by the phrase in bold in (3a). This sentence informs users
that if they want to prevent tooth decay, the products they use must contain a certain amount of Xylitol to
be effective. In example (3b), we obtain information on uncertainty of Xylitol?s tooth decay prevention
effectiveness from the phrase ?is limited?. It tells that Xylitol is not necessarily effective at preventing
tooth decay, and thus it is not completely in agreement with or contradiction to the original sentence (1).
It is important to recognize the semantic relation shown in (3) because it provides more specific infor-
mation about the query or specifies the conditions under which the statement holds or does not. This is
valuable information for Internet users and needs to be distinguished from fully contradicting or agreeing
opinions.
We call this semantic relation CONFINEMENT because it confines the situation under which a query
applies. In this paper, we give a language independent definition of the CONFINEMENT relation in pred-
icate logic and provide a framework for detecting the relation through a series of semantic templates that
take logical and semantic features as input. We implement a system that detects CONFINEMENT rela-
tions between sentence pairs in Japanese by instantiating the semantic templates using rules and a list of
lexico-semantic patterns. Finally, we conduct empirical evaluation of recognition of the CONFINEMENT
relation between queries and sentences in Japanese-language Web texts.
2 Related Work
In RTE research, only three types of relations are defined: ENTAILMENT, CONTRADICTION, and
UNKNOWN. RTE is an important task and has been the target of much research (Szpektor et al (2007);
Sammons et al (2009)). However, none of the previous research has introduced relations corresponding
to CONFINEMENT.
Cross-document Structure Theory (CST, Radev (2000)) is another approach to recognizing semantic
relations between sentences. CST is an extended rhetorical structure analysis based on Rhetorical Struc-
ture Theory (RST). It attempts to describe the semantic relations between two or more sentences from
different source documents that are related to the same topic. It defines 18 kinds of semantic relations
between sentences. Etoh and Okumura (2005) constructed a Japanese Cross-document Relation Corpus
and defined 14 kinds of semantic relations. It is difficult to consider CONFINEMENT relations in the
CST categorical semantic relations because it focuses on comparing sentences in terms of equivalence
and difference between sentences. At first glance, CONFINEMENT may seem to be defined in terms of
difference between sentences, but this approach does not capture the idea of restriction on a sentence?s
applicability. Thus, it is beyond the scope of CST.
In the field of linguistics, Nakagawa and Mori (1995) discussed restrictions as represented in the
four Japanese subordinate clause patterns. Abe (1996) researched the role of quantifiers in quantitative
restrictions and the role of ??? (only).? There is much other researches on expressions representing
?confinement? in a sentence in linguistics. These expressions are useful in order to recognize phrases
which contradict each other. However, as far as we know, there is no research on the relation of CON-
FINEMENT between two sentences in the linguistics literature. The absence of related research makes
defining and recognizing CONFINEMENT a very challenging task.
3 The CONFINEMENT Relation
We present the definition of the CONFINEMENT relation and describe its differences from ENTAIL-
MENT and CONTRADICTION. In essence, a pair of sentences is in the CONFINEMENT relation if either
the premise or consequent of the second sentence has a certain condition or restriction, and without such
condition or restriction the pair is equivalent to either ENTAILMENT or CONTRADICTION.
216
Consider an example of CONFINEMENT setence pair: (2a) and (3a). The statement ?it (Xylitol) is not at
least 50%? is a condition of the statement ?Xylitol can not prevent tooth decay.? It is a CONTRADICTION
if the conditional statement is satisfied. Because the truth value of the whole statement depends on
various conditions to be satisfied, it is important to properly define a framework to define them.
3.1 A Logical Definition of CONFINEMENT
We present a definition of CONFINEMENT in predicate logic. We define CONFINEMENT as a semantic
relation between two sentences, where the first sentence corresponds to RTE?s Hypothesis, or the user
Query, and the second sentence corresponds to RTE?s Text that has some semantic relation with the
Query, which we want to identify.
Here we consider sentence pairs where the Query matches the logical pattern ?x(P (x) ? C(x)),
where we call P (x) the Premise and C(x) the Consequence. There are many ways of representing
sentences as logical expressions, and we think that the logical pattern (?(P (x) ? C(x))) can cover a
variety of queries. For example, the sentence ?Xylitol is effective at preventing tooth decay.? can be
represented as ?x(isXylitol(x) ? effectiveAtPreventingToothDecay(x)). Consider the case where one
sentence contains only a Consequence. This case can be regarded as a special case of the above formula.
We write such a sentence as ?x(T ? C(x)) showing that the Premise is always True.
In this paper, we limit discussion of the CONFINEMENT relation to the Query matching to the above
logical pattern. Recognizing CONFINEMENT between the Text and the Query having more complex
semantic patterns is an area of future work. Here, we split the definition of CONFINEMENT into subtypes
according to: (i) conditions to satisfy in addition to the Premise, and (ii) limitations on the degree of the
Consequence.
Premise side Additional conditions for achieving the Consequence
Explicit constraint
Some conditional sentences use an expression correspoinding to logical ?only if,? which explicitly
means two way conditions as the following formula.
?x((P (x) ?AdditionalCondition(x) ? C(x)) (1)
?(P (x) ? ?AdditionalCondition(x) ? ?C(x)))
For example, S1 in Table 1, ?Xylitol is effective at preventing cavities only when it is 100%?,
explicitly specify that Xylitol is effective if it is 100% and is not effective if it is not 100%. So,
we assume the form of the above formula for this type of statement.
Implicit constraint
This type of sentence specifies an additional condition on the Premise and is represented by the
following formula. The Premise needs to be satisfied for the consequence to be achieved.
?x((P (x) ?AdditionalCondition(x) ? C(x)) (2)
Example S5 in Table 1 says ?Xylitol is effective at preventing tooth decay if it is 100%?, which
is assumed by Formula (2). S5 does not contain an expression such as ?only (??)?, which
explicitly specifies that C(x) does not hold when an additional condition is not satisfied. One may
understand that it implicitly means ?Xylitol is not effective at preventing tooth decay if it is not
100%,? but S5 does not structly require this.
Consequence side Constraints on the degree of achieving the Consequence
There are sentences in partial entailment or contradiction where the degree of achieving of the Con-
sequence is limited. To represent these limitations on the Consequence side, we define a CONFINE-
MENT relation where the degrees of the Consequence are limited as in Example (3b). We define the
following formula to represent these limitations on the Consequence side.
?x((P (x) ? Cr(x)) (3)
Cr(x) represents C(x) with additional restriction. For example, S3 in Table 1 says that Xylitol is
somewhat effective at preventing tooth decay, which means that there are cases in which Xylitol can
not prevent tooth decay. In the case of S3, Cr(x) is ?is a bit effective?. This type of CONFINE-
MENT provides valuable information about Xylitol?s limited ability to promote dental hygiene in S3.
217
All CONFINEMENTs on the Consequence side are of type EXPLICIT CONFINEMENT, because they
explicitly mean that a part of the Consequence is achieved but no other parts are achieved.
3.2 Semantic Templates
We propose a series of semantic templates to classify sentence pairs into one of the CONFINEMENT
relation subtypes we define. The semantic templates take a set of features as input and use their values
to categorize the sentence pair. In Section 4, we evaluate the coverage of the semantic templates by
classifying a small set of sentence pairs using manually set feature values. In Section 6, we provide
more realistic evaluation by using a proposed system to set the feature values automatically and classify
sentence pairs as ENTAILMENT / CONTRADICTION, or CONFINEMENT.
We assume that each sentence consists of a Premise and Consequence, and that each sentence pair
which has a CONFINEMENT relation contains at least one additional condition or one additional limitation
as defined in Section 3.1.
We know that there are a variety of expressions that indicate the presence of a CONFINEMENT relation.
For example, both ?Only 100% pure Xylitol is effective at preventing tooth decay.? and ?Xylitol is not
effective at preventing tooth decay unless it is 100% pure.? are CONFINEMENTs of ?Xylitol is effective
at preventing tooth decay.? Since it is impossible to handle all possible expressions that indicate CON-
FINEMENT, we focus on covering as many as possible with three features: (1) the type of constraint, (2)
the type of Premise, and (3) the type of Consequence. The features are defined in more detail below.
IF-Constraint This feature indicates the type of logical constraint in the Text sentence. Its values can
be ?IF,? ?ONLY-IF.?
Premise This feature indicates the type of Premise in the Text sentence. The value ?P+A? or ?notP+A?
means there is an Additional Condition on the Premise. The value ?P? or ?notP? means there is just
a Premise. ?not? represents the Premise have a negation.
Consequence This feature indicates the type of Consequence. Its possible values are ?C? (just a Conse-
quence), ?notC? (negated Consequence), ?Cr? or ?notCr? (certain partial Consequence).
Semantic templates consist of a tuple of four feature values and a mapping to the confinement type they
indicate. A full list of templates is given in Table 1. In the templates, a wildcard asterisk ?*? indicates
that any feature value can match in that slot of the template. The abbreviations ENT, CONT and CONF
stand for ENTAILMENT, CONFINEMENT and CONFINEMENT respectively.
Semantic templates are applied in turn from top pattern by determining the value of each feature and
looking up the corresponding relation type in Table 1. We give a classification examples below. The user
query is sentence S0. Sentences S1 are Web texts.
Query : S0. Xylitol is effective at preventing tooth decay.
Text [ONLY-IF P(x) ? AC(x) then C(x) ]: S1. Xylitol is effective at preventing tooth decay when you
take it every day without fail.
In Example, IF-Constraint is ?ONLY-IF?, Premise is ?P+A?, and the type of Consequence is ?C?.
This instance has an additional condition and the Consequence matches the Query, so it is identified as
an EXPLICIT CONFINEMENT.
4 Verifying Semantic Templates
In this section, we verify the effectiveness of semantic templates in recognizing CONFINEMENT rela-
tions by testing them on real-world data in Japanese. To directly evaluate the quality of the templates,
we construct a small data set of sentence pairs and manually annotate them with the correct values for
each of the features defined in Section 3.2.
4.1 Data
We constructed the Development set and the Open-test set of sample Japanese user queries and Inter-
net text pairs following the methodology of Murakami et al (2009). However, Murakami et al (2009)
annotated Query-Text pairs with coarse-grained AGREEMENT and CONFLICT relations that subsume the
218
Table 1: Semantic templates for recognizing CONFINEMENT
Semantic features Relation Number of Number of Example
IF-constraint Premise Consequence positive negative S0:?????????????????.
example example Xylitol is effective at preventing tooth decay.
ONLY-IF P+A * EXPLICIT 8 0 S1:??????? 100%?????????????????.
CONF Xylitol is effective at preventing tooth decay only when it is 100%.
ONLY-IF notP+A * EXPLICIT 0 0 S2:??????? 50%??????????????????????.
CONF Xylitol is effective at preventing tooth decay only when it is not under 50%.
* * Cr EXPLICIT 11 0 S3:??????????????????????.
CONF Xylitol is a bit effective at preventing tooth decay.
* * notCr EXPLICIT 12 0 S4:????????????????????????.
CONF Xylitol is not almost of effective at preventing tooth decay.
IF P+A * IMPLICT 62 0 S5:??????? 100%???????????????.
CONF Xylitol is effective at preventing tooth decay if it is 100%.
IF notP+A * IMPLICIT 1 0 S6:??????? 100%???????????????????.
CONF Xylitol is not effective at preventing tooth decay if it is not 100%
IF P C ENT 279 0 S7:???????????????????????.
Xylitol is effective at preventing tooth decay if it is eaten.
IF notP C CONT 0 0 S8:????????????????????????.
Xylitol is effective at preventing tooth decay if it is not eaten.
IF P notC CONT 13 0 S9:????????????????????????.
Xylitol is not effective at preventing tooth decay if it is eaten.
IF notP notC ENT 0 0 S10:????????????????????????.
Xylitol is not effective at preventing tooth decay if it is not eaten.
ONLY-IF P C ENT 3 0 S11:??????????????????????????.
Xylitol is effective at preventing tooth decay only when it is eaten.
ONLY-IF notP C CONT 0 0 S12:?????????????????????????????.
Xylitol is effective at preventing tooth decay only when it is not eaten.
ONLY-IF P notC CONT 0 0 S13:??????????????????????????.
Xylitol is effective at preventing tooth decay only when it is eaten.
ONLY-IF notP notC ENT 0 0 S14:?????????????????????????????.
Xylitol is not effective at preventing tooth decay only when it is not eaten.
Table 2: Data set (Counts of sentences out of parenthesis and statements in parentheses)
Entailment Contradiction Confinement All
Development 258 (282) 8 (13) 79 (94) 345 (389)
Open-test 230 170 200 600
RTE relations of ENTAILMENT and CONTRADICTION. As our task is to discriminate between CON-
FINEMENT and RTE relations, we annotate each sentence pair or each statement1 pair with one of the
following relations instead: ENTAILMENT, CONTRADICTION, or CONFINEMENT. In the case of CON-
FINEMENT, we annotate Query-Text pairs which are not full ENTAILMENT or CONTRADICTION but
these Text partially agrees and disagrees with the Query. Annotations were checked by two native speak-
ers of Japanese, and any sentence pair where annotation agreement is not reached was discarded. Table
2 shows that how many sentences or statements are in each data set. Annotated statements counts are
written in parentheses. We use the Development set for evaluation of verifying semantic templates and
develop list of lexical and syntactic patterns for semantic features extraction, and the Open-test set for
evaluation in Section 6.
4.2 Verification Result
After the data was prepared, we annotated it with the correct feature values for use with the semantic
templates. This was done by manually checking for words or phrases in the sentences that indicated one
of the features in Table 1. Once the features were set, we used them to classify each sentence pair.
We give the numbers of instances that we could confirm for each pattern in the sixth column of Table
1 and the numbers of negative instances in the seventh column, which satisfy semantic template but does
not agree Relation values in the fifth column. As a result we find that there were no statement pairs that
could not be successfully classified. We grasp CONFINEMENT relation with semantic templates for the
most part. This verification data does not cover all combinations of patterns in our semantic templates, so
we can not rule out the possibility of existence of an exception that cannot be classified by the semantic
templates. However, we find these results to be an encouraging indication of the usefulness of semantic
templates. Here are some example classifications found in the verification data.
Coordinate clauses Combining multiple of IMPLICIT CONFINEMENTs results in an EXPLICIT CON-
FINEMENT relation
(4)S0. ????????????.
Steroid has side-effects.
S1. ???????????????????????????????????????
1Murakami et al define a ?statement? as the smallest unit that can convey a complete thought or viewpoint. In practice, this
can be a sentence or something smaller such as a clause.
219
???????????????????.
Long-term use of steroid causes side-effects, but there is no need to worry about side-effects
in short-term usage.
In Example (4), S1 is an EXPLICIT CONFINEMENT for S0. This is derived from the combination of
CONFINEMENT of the two coordinate clauses of S1: the former phrase ?Long-term use of steroid causes
side-effects? of S1 is an IMPLICIT CONFINEMENT for S0 by our semantic templates and the latter phrase
is an IMPLICIT CONFINEMENT for S0.
Additional information for whole Query Combining of a CONTRADICTION and an IMPLICIT CON-
FINEMENT result in an EXPLICIT CONFINEMENT
(5)S0. ????????????????.
Xylitol is effective at preventing tooth decay.
S1. ??????????????????????,???????????????????????????????????????.
Tooth decay can not be prevented with Xylitol alone, but it can be fundamentally prevented
with an appropriate diet and by taking Xylitol after every meal.
The first clause before the comma in S1 of Example (5) corresponds to the entire sentence of S0. The
second clause after the comma helps us recognize that it is a CONFINEMENT relation. This instance
is also a combination of semantic templates, so we need to recognize negation of each statement and
adversative conjunction but we do not need to add new features to Table 1.
5 Proposed System
We propose a system which uses semantic templates for recognizing CONFINEMENT consists of six
steps: (I) linguistic analysis, (II) structural alignment, (III) Premise and Consequence identification,
(IV) semantic feature extraction, (V) adversative conjunction identification, and (VI) semantic template
application. Figure 1 shows the work flow of the system. This system takes as input corresponding to S0
and S1, and return a semantic relation.
5.1 I. Linguistic Analysis
In linguistic analysis, we conduct word segmentation, POS tagging, dependency parsing, and extended
modality analysis. This linguistic analysis acts as the basis for alignment and semantic feature extrac-
tion. For syntactic analysis, we identify words and POS tags with the Japanese morphological analyser
Mecab2, and we use the Japanese dependency parser CaboCha (Kudo and Matsumoto (2002)) to pro-
duce dependency trees. We also conduct extended modality analysis using the resources provided by
Matsuyoshi et al (2010).
5.2 II. Structural Alignment
To identify the consequence of S0 in S1, we use Structural Alignment (Mizuno et al (2010)). In Struc-
tural Alignment, dependency parent-child links are aligned across sentences using a variety of resources
to ensure semantic relatedness.
5.3 III. Premise and Consequence identification
In this step, we identify the Premise and the Consequence in S1. When a sentence pair satisfies all
items is satisfying, we can identify a focused chunk as the Consequence in S1:
1. A chunk?s modality in S0 is assertion, this chunk is the Consequence in S0
2. A chunk in S1 align with the Consequence in S0
We identify the Premise in S1 when a sentence pair satisfies first, and either second or third item of
the following conditions:
1. A case particle of chunks in S0 is either ?? (agent marker)? or ?? (topic marker)? and these chunks
are children of the Consequence in S0?s dependency tree
2. The subject in S0 aligns with the subject of S1
3. All of the dependants of the expression ??? (to, for)? have alignments in S0 dependency tree
2http://chasen.org/taku/ software/mecab/.
220
Figure 1: An overview of a proposal system to recognize CONFINEMENT
5.4 IV. Semantic Feature Extraction
We extract features for the semantic templates using a list of lexical and syntactic patterns. These
patterns were manually compiled using the development data set introduced in Section 4. Features for
the semantic templates are then automatically extracted by applying these patterns to input sentence
pairs. The following overviews our extraction approach for each feature.
5.4.1 IF-Constraint Feature Extraction
Using CaboCha, we manually constructed lists of words and their POS that are indicators of the
semantic condition under which a Premise occurs. We extract as features any words in the input sentences
that appear in the list with the corresponding POS. The ?IF? lexical type lists conjunctions that are the
results of a conditional chunk or noun phrases that indicate a case or situation. The ?ONLY-IF? lexical
type is used to represent the most constraining situations. The following is our list of expressions.
? IF: ?? (in case),?/??/? (when),?/??/?? (if),? (with)
? ONLY-IF: ??/??? (for this time), ??/??/?? (only), ??? (for the first time), ?? (to,
for)
5.4.2 Premise Feature Extraction
We treat the words or phrases which are extracted from the constraint as conditions, and need to decide
whether a given condition is the Premise or an additional condition for the Premise. The Premise is set
to ?P? when first step and either the second or third step of the following conditions are satisfied, and it
is set to ?P+A? otherwise:
1. ? The condition have children in the S1?s dependency tree or the condition?s children are not aligned
to chunks in S0
2. ? The condition?s parent in S0?s dependency tree has any chunk with a child aligned with the Conse-
quence in S0, or the condition?s parent is not aligned with chunks in S0
3. ? The condition?s parent does not have any expression with the meaning of ?use? in the S0?s depen-
dency tree
When these step are satisfied and negation exists in conditional chunks, Premise is set to ?notP+A,? if
these step are not satisfied, Premise is set to ?notP.? In the third step, we identify expressions with the
meaning of ?use? with our lexical list. For example ?? (use), ??? (eat), ?? (take) and so on. If
the condition?s parent has words in our lexical list, we identify that ?Xylitol? and ?eating Xylitol? and
?using Xylitol? are equivalent.
5.4.3 Consequence Feature Extraction
This feature is used to indicate the semantic relationship between Consequences of the sentences pair.
Sentences with Consequences that share a certain amount of similarity in polarity and syntax are judged
to have ENTAILMENT, otherwise they are in CONTRADICTION. In order to be judged as ENTAILMENT,
the following conditions must all be true:
1. The modality of the Consequences must be identical.
2. The polarity of the Consequences must be identical as indicated by the resources in (Sumida et al
(2008))
3. The Premises of both sentences must align with each other
221
4. ? The sentences must not contain expressions that limit range or degree such as ????? (almost)?
or ??? (degree)?
When all item are satisfied, the Consequence is set to ?C?, otherwise it is set to ?notC.? We identify
whether the consequence has expressions which limit the degree or not. The Consequence is set to ?Cr?
or ?notCr? when the following all conditions is satisfied:
1. Any of the children of the Consequence align with a chunk in S0?s dependency tree.
2. ? There are expressions limiting the degree of the Consequence or the siblings in S1?s dependency
tree
When this two steps are satisfied and the all four steps to judge whether sentence pairs is ENTAILMENT
or not are not satisfied, Consequence is set to ?notCr.?
5.5 V. Adversative Conjunction Identification
We manually compiled a list of target expressions including conjunctions such as ?? (but).? When a
S1 chunk containing an adversative conjunction that aligns with the Premise of S0 or the S0?s Premise
depends on S1 chunk containing an adversative conjunction, we set each feature set in a chunk before an
adversative conjunction and after an adversative conjunction to semantic templates.
5.6 VI. Semantic Template Application
We apply semantic feature extracted in Step IV to semantic templates. If S1 matches multiple semantic
templates with an adversative conjunction from Step V, we combine the semantic templates. We get a
relation for a sentence pair in this step.
5.7 Example of Semantic Features Extraction
Feature extraction is illustrated in greater detail in the examples S0 which is the query and S1 in
Table 1. First, we identify words represented IF-Constraint is ?ONLY-IF?: ?? (when)? is in S1 and the
conditional chunk has a word ??? (only).? Next, we evaluate each the type of Premise of each chunk to
determine if it is a premise or an additional condition. The subject word ?Xylitol? align between S0 and
S1, and the conditional chunk?s sibling in dependency tree of S1 is a chunk which has the subject. And
the conditional chunk have a child which is not aligned any chunk in S0, it is ?100%? (100%).? And the
conditional chunk has no negations. So, Premise is set to ?P+A.? Finally, we check if the consequences
to the conditions are aligned to the verbs and nouns indicating consequences in S0: ?prevent? and ?is
effective? are aligned, the modality and polarity of the Consequence are identical, these depended on by
the condition, and the Consequence has no expressions which limited range or degree. Consequence is
set to ?C.?We set the semantic template features and get a result which the sentences relation is EXPLICIT
CONFINEMENT. Ideally patterns for setting semantic feature for semantic templates should be learned
automatically, but this remains an area of future work. Nonetheless, our current experiment gives a good
measure of the effectiveness of semantic templates in recognizing CONFINEMENT relations.
6 Evaluation
In Section 4, we verified that the semantic templates defined in Section 3.2 can successfully classify
semantic relations as CONFINEMENT given the correct feature values. In this Section, we present the
results of an experiment in a more realistic setting by using semantic templates together with the features
automatically extracted as described with our proposed system in Section 5 to determine whether or not
a sentence pair has a CONFINEMENT relation.
6.1 Setting up Evaluation
While more research on recognizing ENTAILMENT or CONTRADICTION between sentences pairs is
necessary, it is important to recognize new relations that cannot be analysed in existing frameworks in
order to provide Internet users with the information they need. Thus, We assume that unrelated sentence
pairs will be discarded before classification, in this experiment we focus only on the recognition of
CONFINEMENT relations. So our goal in this experiment is to classify between CONFINEMENT and NOT
CONFINEMENT. We will evaluate determining whether CONFINEMENT sentence pairs are Explicit or
Implicit in future. In our experiment, we used a gold data for structural alignment to evaluate semantic
feature extraction.
222
Table 3: Results of recognizing confinement relations with our proposal system
Recall Precision F-Score
proposed system 0.65(129/200) 0.57(129/225) 0.61
baseline system 0.96(192/200) 0.34(192/562) 0.50
Table 4: Instances of incorrect classification
S0 S1
A ???????????????. ??????????????????????????????.
False A person can regain their health with isoflavon. Excess intake of isoflavon to boost its health effects is prohibited.
Negative B ?????????????????. ?????????????????????????????????????????????????????????.
Xylitol has effects on preventing tooth decay. The use of xylitol is effective at preventing tooth decay when done while eating properly and brushing one?s
teeth regularly.
C ????????????????????. ?????????????????????????????????????.
Xylitol can prevent tooth decay. It is a big mistake to think that one can prevent tooth decay if they put Xylitol in ? their mouth.
False D ??????????????. ???????????????????????????.
Positive Steroids can cure illnesses. Atrophic dermatitis will heal completely if steroid use is stopped.
E ???????????????. ?????????????????????????????????????????????.
Side effects are a worry for steroids. The amount of steroids or period of time that causes side effects differs from person to person.
6.2 Baseline System
We developed a baseline system that does not use our manually-compiled lexico-syntactic patterns
in order to act as a point of comparison for the proposed system in evaluating their contribution to
CONFINEMENT recognition.
The baseline system consists of performing all of the steps from of our proposed system that do not
rely on manually compiled lexico-syntactic patterns. Step relying on these resources are marked with a
? in Section 5 and are skipped in the baseline. Essentially, we conduct Steps I, II, and III, the parts of
Step IV that can be done without manually-compiled patterns, and, finally, Step VI.
In Step IV, we determine if there are any limitations on the Consequence in the Consequence Feature
subset, but we do not judge whether the Consequence is ENTAILMENT or CONTRADICTION in the
baseline system.
6.3 Result and Error Analysis
The results are given in Table 3. We find that our system has much higher precision than the baseline,
improving by over 20%. In our system, the list of semantic patterns is effective at recognizing CON-
FINEMENT. On the other hand recall has gone down compared to the baseline. The baseline judged that
almost sentences are CONFINEMENT, so the list of semantic patterns employed in our rule-based system
is useful at eliminating false positives. Table 4 shows some instances of incorrect classification. Each
instance is a pair (S0, S1).
Example A-S1 means ?Excess intake of isoflavon can not boost one?s health? and ?excess intake? is
an additional condition for A-S1. In this case ?excess? is a lexical specifier of the specific condition and
is indicated by the particle ???. The particle ?? (topic marker)? is not currently used as a feature in the
semantic templates since it is very noisy, so this instance can not be detected. We need to expand our
method of acquiring semantic patterns to better handle such cases.
The additional condition phrase in Example B-S1 modifies ?The use of Xylitol? instead of ?is effective
at preventing tooth decay?, preventing us from properly recognizing the limiting condition in this case.
We need to conduct deeper scopal analysis to determine when the modifier of an embedded chunk should
be considered as an additional condition.
Example C-S1 is an instance where the system fails to recognize that ?put in their mouth? is an expres-
sion meaning ?use? since our lists of lexical words for features did not have it. We should increase our
ability to recognize synonyms of ?to use? by automatically mining data for paraphrases or approaching
it as a machine learning task in order to handle examples like C-S1. On the other hands ?if steroid use
is stopped? in example D-S1 is the premise which should indicate an IF condition and Negation exists,
however we can not recognize it correctly since the phrase lacks negation. We will make a list of words
and phrases that are antonyms of ?use? in order to recognize such instances.
The condition in example E-S1 is about how side-effects appear, and not a condition for the other
sentence example E-S0. This instance requires detailed semantic analysis and cannot be solved with
alignment-based approaches. It represents a very difficult class of problems.
223
7 Conclusion
On theWeb, much of the information and opinions we encounter indicates the conditions or limitations
under which a statement is true. This information is important to Internet users who are interested in
determining the validity of a query of interest, but such information cannot be represented under the
prevalent RTE framework containing only ENTAILMENT and CONTRADICTION.
In this paper, we provided a logical definition of the CONFINEMENT relation and showed how it
could be used to represent important information that is omitted under an RTE framework. We also
proposed a set of semantic templates that use set of features extracted from sentences pairs to recognize
CONFINEMENT relations between two sentences. Preliminary investigations showed that given correct
feature input, semantic templates could effectively recognize CONFINEMENT relations.
In addition, we presented empirical evaluation of the effectiveness of semantic templates and
automatically-extracted features at recognizing CONFINEMENT between user queries and Web text pairs,
and conducted error analysis of the results. Currently, our system does not deal with unknown instances
well since it extracts features for semantic template using manually constructed lexical patterns. In fu-
ture work, we will learn features for the semantic templates directly from data to better handle unknown
instances.
Acknowledgment
This work is supported by the National Institute of Information and Communications Technology
Japan.
References
Abe, T. (1996). Restriction with? dake ?and modification with quantifier. Tsukuba Japanese Research 1, 4?20. in Japanese.
Etoh, J. and M. Okumura (2005). Cross-document relationship between sentences corpus. In Proceedings of the 14th Annual
Meeting of the Association for Natural Language Processing, pp. 482?485. (in Japanese).
Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan (2007). The third pascal recognizing textual entailment challenge. In
Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE ?07, Morristown, NJ, USA, pp.
1?9. Association for Computational Linguistics.
Kudo, T. and Y. Matsumoto (2002). Japanese dependency analysis using cascaded chunking. In CoNLL 2002: In Proceedings
of the 6th Conference on Natural Language Learning 2002 (COLING 2002 Post-Conference Workshops), pp. 63?69.
Matsuyoshi, S., M. Eguchi, C. Sao, K. Murakami, K. Inui, and Y. Matsumoto (2010). Annotating event mentions in text
with modality, focus, and source information. In Proceedings of the 7th International Language Resources and Evaluation
(LREC?10).
Mizuno, J., H. Goto, Y. Watanabe, K. Murakami, K. Inui, and Y. Matsumoto (2010). Local Structural Alignment for Recogniz-
ing Semantic Relations between Sentences. In Proceedings of IPSJ-NL196. (in Japanese).
Murakami, K., S. Matsuyoshi, K. Inui, and Y. Matsumoto (2009). A corpus of statement pairs with semantic relations in
Japanese. In Proceedings of the 15th Annual Meeting of the Association for Natural Language Processing.
Murakami, K., E. Nichols, S. Matsuyoshi, A. Sumida, S. Masuda, K. Inui, and Y. Matsumoto (2009). Statement map: As-
sisting information credibility analysis by visualizing arguments. In Proceedings of the 3rd ACM Workshop on Information
Credibility on the Web (WICOW 2009), pp. 43?50.
Nakagawa, H. and T. Mori (1995). Pragmatic analysis of aspect morphemes in manual sentences in Japanese. The Association
for Natural Language Processing 2(4), 19 ? 36. in Japanese.
Radev, D. R. (2000). Common theory of information fusion from multiple text sources step one: Cross-document structure. In
Proceedings of the 1st SIGdial workshop on Discourse and dialogue, pp. 74?83.
Sammons, M., V. G. V. Vydiswaran, T. Vieira, N. Johri, M.-W. Chang, D. Goldwasser, V. Srikumar, G. Kundu, Y. Tu, K. Small,
J. Rule, Q. Do, and D. Roth (2009). Relation alignment for textual entailment recognition. In Proceedings of Recognizing
Textual Entailment 2009.
Sumida, A., N. Yoshinaga, and K. Torisawa (2008). Boosting precision and recall of hyponymy relation acquisition from
hierarchical layouts in Wikipedia. In Proceedings of the 6th International Language Resources and Evaluation (LREC?08).
Szpektor, I., E. Shnarch, and I. Dagan (2007). Instance-based evaluation of entailment rule acquisition. In Proceedings of the
45th Annual Meeting of the Association of Computational Linguistics, pp. 456?463.
224
