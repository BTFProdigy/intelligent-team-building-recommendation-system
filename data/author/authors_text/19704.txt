Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1059?1070, Dublin, Ireland, August 23-29 2014.
Automatic Classification of Communicative Functions of Definiteness
Archna Bhatia
?,?
Chu-Cheng Lin
?
Nathan Schneider
?
Yulia Tsvetkov
?
Fatima Talib Al-Raisi
?
Laleh Roostapour
?
Jordan Bender
?
Abhimanu Kumar
?
Lori Levin
?
Mandy Simons
?
Chris Dyer
?
?
Carnegie Mellon University
?
University of Pittsburgh
Pittsburgh, PA 15213 Pittsburgh, PA 15260
?archnab@cs.cmu.edu
Abstract
Definiteness expresses a constellation of semantic, pragmatic, and discourse properties?the
communicative functions?of an NP. We present a supervised classifier for English NPs that
uses lexical, morphological, and syntactic features to predict an NP?s communicative function in
terms of a language-universal classification scheme. Our classifiers establish strong baselines for
future work in this neglected area of computational semantic analysis. In addition, analysis of
the features and learned parameters in the model provides insight into the grammaticalization of
definiteness in English, not all of which is obvious a priori.
1 Introduction
Definiteness is a morphosyntactic property of noun phrases (NPs) associated with semantic and pragmatic
characteristics of entities and their discourse status. Lyons (1999), for example, argues that definite
markers prototypically reflect identifiability (whether a referent for the NP can be identified by the
discourse participants or not); other aspects identified in the literature include uniqueness of the entity
in the world and whether the hearer is already familiar with the entity given the context and preceding
discourse (Roberts, 2003; Abbott, 2006). While some morphosyntactic forms of definiteness are employed
by all languages?namely, demonstratives, personal pronouns, and possessives?languages display a vast
range of variation with respect to the form and meaning of definiteness. For example, while languages
like English make use of definite and indefinite articles to distinguish between the discourse status of
various entities (the car vs. a car vs. cars), many other languages?including Czech, Indonesian, and
Russian?do not have articles (although they do have demonstrative determiners). Sometimes definiteness
is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in
Chinese (a language without articles), where the existential construction can be used to express indefinite
subjects and the ba- construction can be used to express definite direct objects (Chen, 2004).
Aside from this variation in the form of (in)definite NPs within and across languages, there is also vari-
ability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites
expressing these functions. We refer to these as communicative functions of definiteness, following
Bhatia et al. (2014). Croft (2003, pp. 6?7) shows that even when two languages have access to the
same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite
or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French
translations (both languages use definite as well as indefinite articles) such as:
(1) He showed extreme care. (unmarked)
Il montra un soin extr?me. (indef.)
(2) I love artichokes and asparagus. (unmarked)
J?aime les artichauts et les asperges. (def.)
(3) His brother became a soldier. (indef.)
Son fr?re est devenu soldat. (unmarked)
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
1059
? NONANAPHORA [?A,?B] 999
? UNIQUE [+U] 287
*
UNIQUE_HEARER_OLD [+F,?G,+S] 251
? UNIQUE_PHYSICAL_COPRESENCE [+R] 13
? UNIQUE_LARGER_SITUATION [+R] 237
? UNIQUE_PREDICATIVE_IDENTITY [+P] 1
*
UNIQUE_HEARER_NEW [?F] 36
? NONUNIQUE [?U] 581
*
NONUNIQUE_HEARER_OLD [+F] 169
? NONUNIQUE_PHYSICAL_COPRESENCE [?G,+R,+S] 39
? NONUNIQUE_LARGER_SITUATION [?G,+R,+S] 117
? NONUNIQUE_PREDICATIVE_IDENTITY [+P] 13
*
NONUNIQUE_HEARER_NEW_SPEC [?F,?G,+R,+S] 231
*
NONUNIQUE_NONSPEC [?G,?S] 181
? GENERIC [+G,?R] 131
*
GENERIC_KIND_LEVEL 0
*
GENERIC_INDIVIDUAL_LEVEL 131
? ANAPHORA [+A] 1574
? BASIC_ANAPHORA [?B,+F] 795
*
SAME_HEAD 556
*
DIFFERENT_HEAD 329
? EXTENDED_ANAPHORA [+B] 779
*
BRIDGING_NOMINAL [?G,+R,+S] 43
*
BRIDGING_EVENT [+R,+S] 10
*
BRIDGING_RESTRICTIVE_MODIFIER [?G,+S] 614
*
BRIDGING_SUBTYPE_INSTANCE [?G] 0
*
BRIDGING_OTHER_CONTEXT [+F] 112
? MISCELLANEOUS [?R] 732
? PLEONASTIC [?B,?P] 53
? QUANTIFIED 248
? PREDICATIVE_EQUATIVE_ROLE [?B,+P] 58
? PART_OF_NONCOMPOSITIONAL_MWE 100
? MEASURE_NONREFERENTIAL 125
? OTHER_NONREFERENTIAL 148
+ ? 0 + ? 0 + ? 0 + ? 0
Anaphoric 1574 999 732 Generic 131 1476 1698 Predicative 72 53 3180 Specific 1305 181 1819
Bridging 779 1905 621 Familiar 1327 267 1711 Referential 690 863 1752 Unique 287 581 2437
Figure 1: CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the
corpus. Internal (non-leaf) labels are in bold; these are not annotated or predicted. +/? values are shown
for ternary attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique; these are inherited from supercategories, but otherwise default to 0. Thus, for example, the
full attribute specification for UNIQUE_PHYSICAL_COPRESENCE is [?A,?B,+F,?G,0P,+R,+S,+U].
Counts for these attributes are shown in the table at bottom.
A cross-linguistic classification of communicative functions should be able to characterize the aspects
of meaning that account for the different patterns of definiteness marking exhibited in (1?3): e.g., that
(2) concerns a generic class of entities while (3) concerns a role filled by an individual. For more on
communicative functions, see ?2.
This paper develops supervised classifiers to predict communicative function labels for English NPs
using lexical, morphological, and syntactic features. The contribution of our work is in both the output of
the classifiers and the models themselves (features and weights). Each classifier predicts communicative
function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. Such
functions are useful in a variety of language processing applications. For example, they should usually be
preserved in translation, even when the grammatical mechanisms for expressing them are different. The
communicative function labels also represent the discourse status of entities, making them relevant for
entity tracking, knowledge base construction, and information extraction.
Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphological
features to properties of communicative functions. The learned weights of this model can, e.g., gener-
ate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorously
through controlled experiments. This hypothesis generation is linguistically significant as it indicates new
grammatical mechanisms beyond the obvious a and the articles that are used for expressing definiteness
in English.
To build our models, we leverage a cross-lingual definiteness annotation scheme (?2) and annotated
English corpus (?3) developed in prior work (Bhatia et al., 2014). The classifiers, ?4, are supervised
models with features that combine lexical and morphosyntactic information and the prespecified attributes
or groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in fig. 1) to
predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (?5) include one that
exploits these label groupings to award partial credit according to relatedness. ?6 presents experiments
comparing several models and discussing their strengths and weaknesses; computational work and
applications related to definiteness are addressed in ?7.
1060
2 Annotation scheme
The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoric-
ity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel
et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell,
1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define
it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003)
proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite
descriptions. However, possessive definite descriptions (John?s daughter) and the weak definites (the son
of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they
are spoken. In contrast to the reductionist approaches are approaches to grammaticalization (Hopper and
Traugott, 2003) in which grammar develops over time in such a way that each grammatical construction
has some prototypical communicative functions, but may also have many non-prototypical communica-
tive functions. The scheme we are adopting for this work?the annotation scheme for Communicative
Functions of Definiteness (CFD) as described in Bhatia et al. (2014)?assumes that there may be multiple
functions to definiteness. CFD is based on a combination of these functions and is summarized in fig. 1. It
was developed by annotating texts in two languages (English and Hindi) for four different genres?namely
TED talks, a presidential inaugural speech, news articles, and fictional narratives?keeping in mind the
communicative functions that have been associated with definiteness in the linguistic literature.
CFD is hierarchically organized. This hierarchical organization serves to reduce the number of decisions
that an annotator needs to make for speed and consistency. We now highlight some of the major distinctions
in the hierarchy.
At the highest level, the distinction is made between Anaphora, Nonanaphora, and Miscellaneous
functions of an NP (the annotatable unit). Anaphora and Nonanaphora respectively describe whether
an entity is old or new in the discourse; the Miscellaneous function is mainly assigned to various kinds of
nonreferential NPs.
The Anaphora category has two subcategories: Basic_Anaphora and Extended_Anaphora. Ba-
sic_Anaphora applies to NPs referring to entities that have been mentioned before. Extended_Anaphora
applies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentioned
entity. For example, after mentioning a wedding, the bride, the groom, and the cake are considered to be
Extended_Anaphora.
Within the Nonanaphora category, a first distinction is made between Unique, Nonunique, and
Generic. The Unique function applies to NPs whose referent becomes unique in a context for any of
several reasons. For example, Obama can safely be considered unique in contemporary political discourse
in the United States. The function Nonunique applies to NPs that start out with multiple possible referents
and that may or may not become identifiable in a speech situation. For example, a little riding hood of
red velvet in fig. 2 could be annotated with the label Nonunique. Finally, Generic NPs refer to classes
or types of entities rather than specific entities. For example, Dinosaurs in Dinosaurs are extinct. is a
Generic NP.
Another important distinction CFD makes is between Hearer_Old for references to entities that are
familiar to the hearer (e.g., if they are physically present in the speech situation), versus Hearer_New
for nonfamiliar references. This distinction cuts across the two subparts of the hierarchy, Anaphora
and Nonanaphora; thus, labels marking Hearer_Old or Hearer_New also encode other distinctions
(e.g., Unique_Hearer_Old, Unique_Hearer_New, Nonunique_Hearer_Old). For further details on
the annotation scheme, see fig. 1 and Bhatia et al. (2014).
Because the ordering of distinctions determines the tree structure of the hierarchy, the same commu-
nicative functions could have been organized in a superficially different way. In fact, Komen (2013) has
proposed a hierarchy with similar leaf nodes, but different internal structure. Since it is possible that
some natural groupings of labels are not reflected in the hierarchy we used, we also decompose each
label into fundamental communicative functions, which we call attributes. Each label type is associated
with values for attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique. These attributes can have values of +, ?, or 0, as shown in fig. 1. For instance, with the Anaphoric
1061
Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother,
and there was nothing that she would not have given to the child.
Once she
SAME_HEAD
gave her
DIFFERENT_HEAD
a little riding hood of red velvet
OTHER_NONREFERENTIAL
NONUNIQUE_HEARER_NEW_SPEC
, which suited her
SAME_HEAD
so well that
she
SAME_HEAD
would never wear anything else
QUANTIFIED
; so she
SAME_HEAD
was always called ?Little Red Riding Hood
UNIQUE_HEARER_NEW
.?
Figure 2: An annotated sentence from ?Little Red Riding Hood.? The previous sentence is shown for
context.
attribute, a value of + applies to labels that can never mark NPs new to the discourse, ? applies to labels
that can only apply if the NP is new in the discourse, and 0 applies to labels such as Pleonastic (where
anaphoricity is not applicable because there is no discourse referent).
3 Data
We use the English definiteness corpus of Bhatia et al. (2014), which consists of texts from multiple genres
annotated with the scheme described in ?2.
1
The 17 documents consist of prepared speeches (TED talks
and a presidential address), published news articles, and fictional narratives. The TED data predominates
(75% of the corpus);
2
the presidential speech represents about 16%, fictional narratives 5%, and news
articles 4%. All told, the corpus contains 13,860 words (868 sentences), with 3,422 NPs (the annotatable
units). Bhatia et al. (2014) report high inter-annotator agreement, estimating Cohen?s ? = 0.89 within the
TED genre as well as for all genres.
Figure 2 is an excerpt from the ?Little Red Riding Hood? annotated with the CFD scheme.
4 Classification framework
To model the relationship between the grammar of definiteness and its communicative functions in a
data-driven fashion, we work within the supervised framework of feature-rich discriminative classification,
treating the functional categories from ?2 as output labels y and various lexical, morphological, and
syntactic characteristics of the language as features of the input x. Specifically, we learn two kinds
of probabilistic models. The first is a log-linear model similar to multiclass logistic regression, but
deviating in that logistic regression treats each output label (response) as atomic, whereas we decompose
each into attributes based on their linguistic definitions, enabling commonalities between related labels
to be recognized. Each weight in the model corresponds to a feature that mediates between percepts
(characteristics of the input NP) and attributes (characteristics of the label). This is aimed at attaining
better predictive accuracy as well as feature weights that better describe the form?function interactions we
are interested in recovering. We also train a random forest model on the hypothesis that it would allow us
to sacrifice interpretability of the learned parameters for predictive accuracy.
Our setup is formalized below, where we discuss the mathematical models and linguistically motivated
features.
4.1 Models
We experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemble
model. Due to their consistency and interpretability, linear models are a valuable tool for quantifying and
analyzing the effects of individual features. Non-linear models, while less interpretable, often outperform
logistic regression (Perlich et al., 2003), and thus could be desirable when the predictions are needed for a
downstream task.
1
The data can be obtained from http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus.
2
The TED talks are from a large parallel corpus obtained from http://www.ted.com/talks/.
1062
4.1.1 Log-linear model
At test time, we model the probability of communicative function label y conditional on an NP x as
follows:
p
?
(y?x) = log
exp?
?
f(x,y)
?
y
??Y exp?
?
f(x,y?)
(1)
where ? ?Rd is a vector of parameters (feature weights), and f ?X ?Y ?Rd is the feature function over
input?label pairs. The feature function is defined as follows:
f(x,y) = ? (x)? ??(y) (2)
where the percept function ? ?X ?Rc produces a vector of real-valued characteristics of the input, and
the attribute function
?
? ?Y ? {0,1}a encodes characteristics of each label. There is a feature for every
percept?attribute pairing: so d = c ?a and f(i?1)a+ j(x,y) = ?i(x) ?? j(y),1 ? i ? c,1 ? j ? a.
3
The contents of
the percept and attribute functions are detailed in ?4.2 and ?4.3 respectively.
For prediction, having learned weights
?
? we use the Bayes-optimal decision rule for minimizing
misclassification error, selecting the y that maximizes this probability:
y?? argmax
y?Y
p
?
?
(y?x) (3)
Training optimizes
?
? so as to maximize a convex L
2
-regularized
4
learning objective over the training data
D:
?
? = argmax
?
?? ??? ??
2
2
+ ?
?x,y??D
log
exp?
?
f(x,y)
?
y
??Y exp(?
?
f(x,y?))
(4)
With
?
?(y) = the identity of the label, this reduces to standard logistic regression.
4.1.2 Non-linear model
We employ a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learned
from many independent subsamples of the training data. Given an input, each tree classifier assigns a
probability to each label; those probabilities are averaged to compute the probability distribution across
the ensemble.
An important property of the random forests, in addition to being an effective tool in prediction, is
their immunity to overfitting: as the number of trees increases, they produce a limiting value of the
generalization error.
5
Thus, no hyperparameter tuning is required. Random forests are known to be
robust to sparse data and to label imbalance (Chen et al., 2004), both of which are challenges with the
definiteness dataset.
4.2 Percepts
The characteristics of the input that are incorporated in the model, which we call percepts to distinguish
them from model features linking inputs to outputs, see ?4.1, are intended to capture the aspects of English
morphosyntax that may be relevant to the communicative functions of definiteness.
After preprocessing the text with a dependency parser and coreference resolver, which is described in
?6.1, we extract several kinds of percepts for each NP.
4.2.1 Basic
Words of interest. These are the head within the NP, all of its dependents, and its governor (external to
the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing
the dependency path upward from the head. For each of these words, we have separate percepts capturing:
the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a
3
Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection.
4
As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are
excluded from regularization.
5
See Theorem 1.2 in Breiman (2001) for details.
1063
binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we
have additional features specific to the first and the last one. Moreover, to better capture tense, aspect
and modality, we collect the attached verb?s auxiliaries. We also make note of the negative particle (with
dependency label neg) if it is a dependent of the verb.
Structural. The structural percepts are: the path length from the head up to the root, and to the attached
verb. We also have percepts for the number of dependents, and the number of dependency relations that
link non-neighbors. Integer values were binarized with thresholding.
Positional. These percepts are the token length of the NP, the NP?s location in the sentence (first or
second half), and the attached verb?s position relative to the head (left or right). 12 additional percept
templates record the POS and lemma of the left and right neighbors of the head, governor, and attached
verb.
4.2.2 Contextual NPs
When extracting features for a given NP (call it the ?target?), we also consider NPs in the following
relationship with the target NP: its immediate parent, which is the smallest NP whose span fully subsumes
that of the target; the immediate child, which is the largest NP subsumed within the target; the immediate
precedent and immediate successor within the sentence; and the nearest preceding coreferent mention.
For each of these related NPs, we include all of their basic percepts conjoined with the nature of the
relation to the target.
4.3 Attributes
As noted above, though CFD labels are organized into a tree hierarchy, there are actually several dimensions
of commonality that suggest different groupings. These attributes are encoded as ternary characteristics;
for each label (including internal labels), every one of the 8 attributes is assigned a value of +, ?, or 0
(refer to fig. 1). In light of sparse data, we design features to exploit these similarities via the attribute
vector function
?(y) = [y,A(y),B(y),F(y),G(y),P(y),R(y),S(y),U(y)]
?
(5)
where A ?Y ? {+,?,0} returns the value for Anaphoric, B(y) for Bridging, etc. The identity of the label
is also included in the vector so that different labels are always recognized as different by the attribute
function. The categorical components of this vector are then binarized to form
?
?(y); however, instead
of a binary component that fires for the 0 value of each ternary attribute, there is a component that fires
for any value of the attribute?a sort of bias term. The weights assigned to features incorporating + or ?
attribute values, then, are easily interpreted as deviations relative to the bias.
5 Evaluation
The following measures are used to evaluate our predictor against the gold standard for the held-out
evaluation (dev or test) set E :
? Exact Match: This accuracy measure gives credit only where the predicted and gold labels are identical.
? By leaf label: We also compute precision and recall of each leaf label to determine which categories
are reliably predicted.
? Soft Match: This accuracy measure gives partial credit where the predicted and gold labels are
related. It is computed as the proportion of attributes-plus-full-label whose (categorical) values match:
??(y)??(y?)?/9.
6 Experiments
6.1 Experimental Setup
Data splits. The annotated corpus of Bhatia et al. (2014) (?3) contains 17 documents in 3 genres:
13 prepared speeches (mostly TED talks),
6
2 newspaper articles, and 2 fictional narratives. We arbitrarily
choose some documents to hold out from each genre; the resulting test set consists of 2 TED talks
6
We have combined the TED talks and presidential speech genres since both involved prepared speeches.
1064
Condition ?? ? ? Exact Match Acc. Soft Match Acc.
Majority baseline ? ? 12.1 47.8
Log-linear classifier, attributes only 473,064 100 38.7 77.1
Log-linear classifier, labels only 413,931 100 40.8 73.6
Full log-linear classifier (labels + attributes) 926,417 100 43.7 78.2
Random forest classifier 20,363 ? 49.7 77.5
Table 1: Classifiers and baseline, as measured on the test set. The first two columns give the number of
parameters and the tuned regularization hyperparameter, respectively; the third and fourth columns give
accuracies as percentages. The best in each column is bolded.
(?Alisa_News?, ?RobertHammond_park?), 1 newspaper article (?crime1_iPad_E?), and 1 narrative
(?Little Red Riding Hood?). The test set then contains 19,28 tokens (111 sentences), in which there are
511 annotated NPs; while the training set contains 2,911 NPs among 11,932 tokens (757 sentences).
Preprocessing. Automatic dependency parses and coreference information were obtained with the
parser and coreference resolution system in Stanford CoreNLP v. 3.3.0 (Socher et al., 2013; Recasens
et al., 2013) for use in features (?4.2). Syntactic features were extracted from the Basic dependencies
output by the parser. To evaluate the performance of Stanford system on our data, we manually inspected
the dependencies and coreference information for a subset of sentences from our corpus (using texts
from TED talks and fictional narratives genres) and recorded the errors. We found that about 70% of the
sentences had all correct dependencies, and only about 0.04% of the total dependencies were incorrect
for our data. However, only 62.5% of the coreference links were correctly identified by the coreference
resolver. The rest of them were either missing or incorrectly identified. We believe this may have caused a
portion of the classifier errors while predicting the Ananphoic labels.
Throughout our experiments (training as well as testing), we use the gold NP boundaries identified by
the human annotators. The automatic dependency parses are used to extract percepts for each gold NP.
If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extracting
misleading percepts, we assign a default value.
Learning. The log-linear model variants are trained with an in-house implementation of supervised
learning with L
2
-regularized AdaGrad (Duchi et al., 2011). Hyperparameters are tuned on a development
set formed by holding out every tenth instance from the training set (test set experiments use the full
training set): the power of 10 giving the highest Soft Match accuracy was chosen for ? .
7
The Python
scikit-learn toolkit (Pedregosa et al., 2011) was used for the random forest classifier.8
6.2 Results
Measurements of overall classification performance appear in table 1. While far from perfect, our
classifiers achieve promising accuracy levels given the small size of the training data and the number of
labels in the annotation scheme. The random forest classifier is the most accurate in Exact Match, likely
due to the robustness of that technique under conditions where the data are small and the frequencies
of individual labels are imbalanced. By the Soft Match measure, our attribute-aware log-linear models
perform very well. The most successful of the log-linear models is the richest model, which combines the
fine-grained communicative function labels with higher-level attributes of those labels. But notably the
attribute-only model, which decomposes the semantic labels into attributes without directly considering
the full label, performs almost as well as the random forest classifier in Soft Match. This is encouraging
because it suggests that the model has correctly exploited known linguistic generalizations to account for
the grammaticalization of definiteness in English.
Table 2 reports the precision and recall of each leaf label predicted. Certain leaf labels are found
to be easier for the classifier to predict: e.g., the communicative function label Pleonastic has a high
F
1
score. This is expected as the Ploenastic CFD for English is quite regular and captured by the EX
7
Preliminary experiments with cross-validation on the training data showed that the value of ? was stable across folds.
8
Because it is a randomized algorithm, the results may vary slightly between runs; however, a cross-validation experiment on
the training data found very little variance in accuracy.
1065
Leaf label N P R F
1
Leaf label N P R F
1
Pleonastic 44 100 78 88 Part_of_Noncompositional_MWE 88 20 17 18
Bridging_Restrictive_Modifier 552 58 84 68 Bridging_Nominal 33 33 10 15
Quantified 213 57 57 57 Generic_Individual_Level 113 14 11 13
Unique_Larger_Situation 97 52 58 55 Nonunique_Nonspec 173 9 25 13
Same_Head 452 41 41 41 Bridging_Other_Context 96 33 6 11
Measure_Nonreferential 98 88 26 40 Bridging_Event 9 ? 0 ?
Nonunique_Hearer_New_Spec 190 36 46 40 Nonunique_Physical_Copresence 36 0 0 ?
Other_Nonreferential 134 39 36 37 Nonunique_Predicative_Identity 10 ? 0 ?
Different_Head 271 32 33 32 Predicative_Nonidentity 57 0 0 ?
Nonunique_Larger_Situation 97 29 25 27 Unique_Hearer_New 26 ? 0 ?
Table 2: Number of training set instances and precision, recall, and F
1
percentages for leaf labels.
part-of-speech tag. The classifier finds predictions of certain CFD labels, such as Bridging_Event,
Bridging_Nominal and Nonunique_Nonspecific, to be more difficult due to data sparseness: it appears
that there were not enough training instances for the classifier to learn the generalizations corresponding
to these CFDs. Bridging_Other_Context was hard to predict as this was a category which referred not
to the entities previously mentioned but to the whole speech event from the past. There seem to be no
clear morphosyntactic cues associated with this CFD, so to train a classifier to predict this category label,
we would need to model more complex semantic and discourse information. This also applies to the
classifier confusion between the Same_Head and Different_Head, since both of these labels share all
the semantic attributes used in this study.
An advantage of log-linear models is that inspecting the learned feature weights can provide useful
insights into the model?s behavior. Figure 3 lists 10 features that received the highest positive weights
in the full model for the + and ? values of the Specific attribute. These confirm some known properties
of English definites and indefinites. The definite article, possessives (PRP$), proper nouns (NNP), and the
second person pronoun are all associated with specific NPs, while the indefinite article is associated with
nonspecific NPs. The model also seems to have picked up on the less obvious but well-attested tendency
of objects to be nonspecific (Aissen, 2003).
In addition to confirming known grammaticalization patterns of definiteness, we can mine the highly-
weighted features for new hypotheses: e.g., in figs. 3 and 4, the model thinks that objects of ?from? are
especially likely to be Specific, and that NPs with comparative adjectives (JJR) are especially likely to be
nonspecific (fig. 3). From fig. 3, we also know that Num. of dependents, dependent?s POS: 1,PRP$ has
a higher weight than, say, Num. of dependents, dependent?s POS: 2,PRP$. This observation suggests a
hypothesis that in English the NPs which have possessive pronouns immediately preceding the head are
more likely to be specific than the NPs which have intervening words between the possessive pronoun
and the head. Similarly, looking at another example in fig. 4, the following two percepts get high weights
for the NP the United States of America to be Specific: last dependent?s POS: NNP and first dependent?s
lemma: the. Since frequency and other factors affect the feature weights learned by the classifier, these
differences in weights may or may not reflect an inherent association with Specificity. Whether these
are general trends, or just an artifact of the sentences that happened to be in the training data and our
statistical learning procedure, will require further investigation, ideally with additional datasets and more
rigorous hypothesis testing.
Finally, we can remove features to test their impact on predictive performance. Notably, in experiments
ablating features indicating articles?the most obvious exponents of definiteness in English?we see
a decrease in performance, but not a drastic one. This suggests that the expression of communicative
functions of definiteness is in fact much richer than morphological definiteness.
Errors. Several labels are unattested or virtually unattested in the training data, so the models unsurpris-
ingly fail to predict them correctly at test time. Same_Head and Different_Head, though both common,
are confused quite frequently. Whether the previous coreferent mention has the same or different head is a
simple distinction for humans; low model accuracy is likely due to errors propagated from coreference
resolution. This problem is so frequent that merging these two categories and retraining the random
forest model improves Exact Match accuracy by 8% absolute and Soft Match accuracy by 5% absolute.
1066
Percepts
+Specific ?Specific
First dependent?s POS PRP$ First dependent?s lemma a
Head?s left neighbor?s POS PRP$ Last dependent?s lemma a
Last dependent?s lemma you Num. of dependents, dependent?s lemma 1,a
Num. of dependents, dependent?s lemma 1,you Head?s left neighbor?s POS JJR
Num. of dependents, dependent?s POS 1,PRP$ Last dependent?s POS JJR
Governor?s right neighbor?s POS PRP$ Num. of dependents, dependent?s lemma 2,a
Last dependent?s POS NNP First dependent?s lemma new
Last dependent?s POS PRP$ Last dependent?s lemma new
First dependent?s lemma the Num. of dependents, dependent?s POS 2,JJR
Governor?s lemma from Governor?s left neighbor?s POS VB
Figure 3: Percepts receiving highest positive weights in association with values of the Specific attribute.
Example Relevant percepts from fig. 3 CFD annotation
This is just for the United States of America. Last dependent?s POS: NNP
First dependent?s lemma: the
Unique_Larger_Situation
We were driving from our home in Nashville
to a little farm we have 50 miles east of
Nashville ? driving ourselves.
First dependent?s POS: PRP$
Head?s left neighbor?s POS: PRP$
Governor?s right neighbor?s POS: PRP$
Governor?s lemma: from
Bridging_Restrictive_Modifier
Figure 4: Sentences from our corpus illustrating percepts fired for gold NPs and their CFD annotations.
Another common confusion is between the highly frequent category Unique_Larger_Situation and the
rarer category Unique_Hearer_New; the latter is supposed to occur only for the first occurrence of a
proper name referring to a entity that is not already part of the knowledge of the larger community. In
other words, this distinction requires world knowledge about well-known entities, which could perhaps be
mined from the Web or other sources.
7 Related Work
Because semantic/pragmatic analysis of referring expressions is important for many NLP tasks, a compu-
tational model of the communicative functions of definiteness has the potential to leverage diverse lexical
and grammatical cues to facilitate deeper inferences about the meaning of linguistic input. We have used
a coreference resolution system to extract features for modeling definiteness, but an alternative would be
to predict definiteness functions as input to (or jointly with) the coreference task. Applications such as
information extraction and dialogue processing could be expected to benefit not only from coreference
information, but also from some of the semantic distinctions made in our framework, including specificity
and genericity.
Better computational processing of definiteness in different languages stands to help machine translation
systems. It has been noted that machine translation systems face problems when the source and the target
language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov
et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either
(a) preprocessing the source language to make it look more like the target language (Collins et al., 2005;
Habash, 2007; Nie?en and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine
translation output to match the target language, (e.g., Popovi
?
c et al., 2006). Attempts have also been made
to use syntax on the source and/or the target sides to capture the syntactic differences between languages
(Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite
articles has been found beneficial in a variety of applications, including postediting of MT output (Knight
and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction
of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013)
trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and
used this classifier to improve the quality of statistical machine translation.
While definiteness morpheme prediction has been thoroughly studied in computational linguistics,
1067
studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit
linguistically-motivated features in a supervised approach to distinguish between generic and specific
NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve
the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong
et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been
conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness
more broadly.
Our work is related to research in linguistics on the modeling of syntactic constructions such as dative
shift and the expression of possession with ?of? or ??s?. Bresnan and Ford (2010) used logistic regression
with semantic features to predict syntactic constructions. Although we are doing the opposite (using
syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as
mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper
and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in
multiple communicative functions for each grammatical construction. Other attempts have also been made
to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have
been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation,
Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by
prepositions.
8 Conclusion
We have presented a data-driven approach to modeling the relationship between universal communicative
functions associated with (in)definiteness and their lexical/grammatical realization in a particular language.
Our feature-rich classifiers can give insights into this relationship as well as predict communicative
functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear
classifier compares favorably to the random forest classifier in Soft Match accuracy. Further improvements
to the classifier may come from additional features or better preprocessing. This work has focused on
English, but in future work we plan to build similar models for other languages?including languages
without articles, under the hypothesis that such languages will rely on other, subtler devices to encode
many of the functions of definiteness.
Acknowledgments
This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533. We thank the reviewers for their useful comments.
References
Barbara Abbott. 2006. Definite and indefinite. In Keith Brown, editor, Encyclopedia of Language and Linguistics,
pages 3?392. Elsevier.
Judith Aissen. 2003. Differential object marking: iconicity vs. economy. Natural Language & Linguistic Theory,
21(3):435?483.
Archna Bhatia, Mandy Simons, Lori Levin, Yulia Tsvetkov, Chris Dyer, and Jordan Bender. 2014. A unified anno-
tation scheme for the semantic/pragmatic components of definiteness. In Proc. of LREC. Reykjav?k, Iceland.
Betty Birner and Gregory Ward. 1994. Uniqueness, familiarity and the definite article in English. In Proc. of the
Twentieth Annual Meeting of the Berkeley Linguistics Society, pages 93?102.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Joan Bresnan and Marilyn Ford. 2010. Predicting syntax: Processing dative constructions in American and Aus-
tralian varieties of English. Language, 86(1):168?213.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proc. of EMNLP, pages 1677?1687. Seattle, Washington, USA.
1068
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Using random forest to learn imbalanced data. University of
California, Berkeley.
Ping Chen. 2004. Identifiability and definiteness in Chinese. Linguistics, 42:1129?1184.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation.
In Proc. of ACL, pages 531?540. Ann Arbor, Michigan.
Cleo Condoravdi. 1992. Strong and weak novelty and familiarity. In Proc. of SALT II, pages 17?37.
William Croft. 2003. Typology and Universals. Cambridge University Press.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121?2159.
Michael Elhadad. 1993. Generating argumentative judgment determiners. In Proc. of AAAI, pages 344?349.
Gareth Evans. 1977. Pronouns, quantifiers and relative clauses. Canadian Journal of Philosophy, 7(3):46.
Gareth Evans. 1980. Pronouns. Linguistic Inquiry, 11.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1988. The generation and interpretation of demonstrative
expressions. In Proc. of XIIth International Conference on Computational Linguistics, pages 216?221.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1993. Cognitive status and the form of referring expres-
sions in discourse. Language, 69:274?307.
Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In MT Summit XI, pages 215?222.
Copenhagen.
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?129.
Irene Heim. 1990. E-type pronouns and donkey anaphora. Linguistics and Philosophy, 13:137?177.
Iris Hendrickx, Orph?e De Clercq, and V?ronique Hoste. 2011. Analysis and reference resolution of bridge
anaphora across different text genres. In Iris Hendrickx, Sobha Lalitha Devi, Antonio Horta Branco, and Ruslan
Mitkov, editors, DAARC, volume 7099 of Lecture Notes in Computer Science, pages 1?11. Springer.
Paul J. Hopper and Elizabeth Closs Traugott. 2003. Grammaticalization. Cambridge University Press.
Nirit Kadmon. 1987. On unique and non-unique reference and asymmetric quantification. Ph.D. thesis, University
of Massachusetts.
Nirit Kadmon. 1990. Uniqueness. Linguistics and Philosophy, 13:273?324.
Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proc. of the National Conference
on Artificial Intelligence, pages 779?779. Seattle, WA.
Erwin Ronald Komen. 2013. Finding focus: a study of the historical development of focus in English. LOT,
Utrecht.
Fang Kong, Guodong Zhou, Longhua Qian, and Qiaoming Zhu. 2010. Dependency-driven anaphoricity determi-
nation for coreference resolution. In Proc. of COLING, pages 599?607. Beijing, China.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proc. of COLING/ACL, pages 609?616. Sydney, Australia.
Christopher Lyons. 1999. Definiteness. Cambridge University Press.
Guido Minnen, Francis Bond, and Ann Copestake. 2000. Memory-based learning for article generation. In Proc. of
1069
the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language
Learning, pages 43?48.
Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING. Taipei, Taiwan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proc. of
COLING, pages 1081?1085.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, M. Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree induction vs. logistic regression: a learning-
curve analysis. Journal of Machine Learning Research, 4:211?255.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of German compound words.
In Advances in Natural Language Processing, pages 616?624. Springer.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen
Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowd-
sourcing. In Proc. of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,
ExProM ?12, pages 57?64.
Ellen F. Prince. 1992. The ZPG letter: Subjects, definiteness and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a fund raising text, pages 295?325. John Benjamins.
Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse
entities: identifying singleton mentions. In Proc. of NAACL-HLT, pages 627?633. Atlanta, Georgia, USA.
Roi Reichart and Ari Rappoport. 2010. Tense sense disambiguation: A new syntactic polysemy task. In Proc. of
EMNLP, EMNLP ?10, pages 325?334.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proc. of ACL, pages 40?49. Uppsala,
Sweden.
Craig Roberts. 2003. Uniqueness in definite noun phrases. Linguistics and Philosophy, 26:287?350.
Alla Rozovskaya and Dan Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Proc.
of NAACL-HLT, pages 154?162.
Bertrand Russell. 1905. On denoting. Mind, New Series, 14:479?493.
Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector
grammars. In Proc. of ACL, pages 455?465. Sofia, Bulgaria.
Vivek Srikumar and Dan Roth. 2013. An inventory of preposition relations. CoRR, abs/1305.5785.
Sara Stymne. 2009. Definite noun phrases in statistical machine translation into Danish. In Proc. of Workshop on
Extracting and Using Constructions in NLP, pages 4?9.
Yulia Tsvetkov, Chris Dyer, Lori Levi, and Archna Bhatia. 2013. Generating English determiners in phrase-based
translation with synthetic translation options. In Proc. of WMT.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proc. of ACL, pages 303?310.
Philadelphia, Pennsylvania, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved chunk-level reordering for statistical machine
translation. In IWSLT 2007: International Workshop on Spoken Language Translation, pages 21?28.
1070
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001?1012,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Dependency Parser for Tweets
Lingpeng Kong Nathan Schneider Swabha Swayamdipta
Archna Bhatia Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{lingpenk,nschneid,swabha,archna,cdyer,nasmith}@cs.cmu.edu
Abstract
We describe a new dependency parser for
English tweets, TWEEBOPARSER. The
parser builds on several contributions: new
syntactic annotations for a corpus of tweets
(TWEEBANK), with conventions informed
by the domain; adaptations to a statistical
parsing algorithm; and a new approach to
exploiting out-of-domain Penn Treebank
data. Our experiments show that the parser
achieves over 80% unlabeled attachment
accuracy on our new, high-quality test set
and measure the benefit of our contribu-
tions.
Our dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
1 Introduction
In contrast to the edited, standardized language of
traditional publications such as news reports, social
media text closely represents language as it is used
by people in their everyday lives. These informal
texts, which account for ever larger proportions of
written content, are of considerable interest to re-
searchers, with applications such as sentiment anal-
ysis (Greene and Resnik, 2009; Kouloumpis et al.,
2011). However, their often nonstandard content
makes them challenging for traditional NLP tools.
Among the tools currently available for tweets are
a POS tagger (Gimpel et al., 2011; Owoputi et al.,
2013) and a named entity recognizer (Ritter et al.,
2011)?but not a parser.
Important steps have been taken. The English
Web Treebank (Bies et al., 2012) represents an
annotation effort on web text?which likely lies
somewhere between newspaper text and social me-
dia messages in formality and care of editing?that
was sufficient to support a shared task (Petrov and
McDonald, 2012). Foster et al. (2011b) annotated
a small test set of tweets to evaluate parsers trained
on the Penn Treebank (Marcus et al., 1993), aug-
mented using semi-supervision and in-domain data.
Others, such as Soni et al. (2014), have used exist-
ing Penn Treebank?trained models on tweets.
In this work, we argue that the Penn Treebank
approach to annotation?while well-matched to
edited genres like newswire?is poorly suited to
more informal genres. Our starting point is that
rapid, small-scale annotation efforts performed
by imperfectly-trained annotators should provide
enough evidence to train an effective parser. We
see this starting point as a necessity, given observa-
tions about the rapidly changing nature of tweets
(Eisenstein, 2013), the attested difficulties of do-
main adaptation for parsing (Dredze et al., 2007),
and the expense of creating Penn Treebank?style
annotations (Marcus et al., 1993).
This paper presents TWEEBOPARSER, the first
syntactic dependency parser designed explicitly for
English tweets. We developed this parser follow-
ing current best practices in empirical NLP: we
annotate a corpus (TWEEBANK) and train the pa-
rameters of a statistical parsing algorithm. Our
research contributions include:
? a survey of key challenges posed by syntactic
analysis of tweets (by humans or machines) and
decisions motivated by those challenges and by
our limited annotation-resource scenario (?2);
? our annotation process and quantitative mea-
sures of the quality of the annotations (?3);
? adaptations to a statistical dependency parsing
algorithm to make it fully compatible with the
above, and also to exploit information from out-
of-domain data cheaply and without a strong
commitment (?4); and
? an experimental analysis of the parser?s unla-
beled attachment accuracy?which surpasses
80%?and contributions of various important
components (?5).
The dataset and parser can be found at http://www.
ark.cs.cmu.edu/TweetNLP.
1001
2 Annotation Challenges
Before describing our annotated corpus of tweets
(?3), we illustrate some of the challenges of syn-
tactic analysis they present. These challenges moti-
vate an approach to annotation that diverges signif-
icantly from conventional approaches to treebank-
ing. Figure 1 presents a single example illustrating
four of these: token selection, multiword expres-
sions, multiple roots, and structure within noun
phrases. We discuss each in turn.
2.1 Token Selection
Many elements in tweets have no syntactic function.
These include, in many cases, hashtags, URLs, and
emoticons. For example:
RT @justinbieber : now Hailee get a twitter
The retweet discourse marker, username, and colon
should not, we argue, be included in the syntactic
analysis. By contrast, consider:
Got #college admissions questions ? Ask them
tonight during #CampusChat I?m looking
forward to advice from @collegevisit
http://bit.ly/cchOTk
Here, both the hashtags and the at-mentioned user-
name are syntactically part of the utterances, while
the punctuation and the hyperlink are not. In the
example of Figure 1, the unselected tokens include
several punctuation tokens and the final token #be-
lieber, which marks the topic of the tweet.
Typically, dependency parsing evaluations ig-
nore punctuation token attachment (Buchholz and
Marsi, 2006), and we believe it is a waste of an-
notator (and parser) time to decide how to attach
punctuation and other non-syntactic tokens. Ma
et al. (2014) recently proposed to treat punctua-
tion as context features rather than dependents, and
found that this led to state-of-the-art performance
in a transition-based parser. A small adaptation
to our graph-based parsing approach, described in
?4.2, allows a similar treatment.
Our approach to annotation (?3) forces annota-
tors to explicitly select tokens that have a syntactic
function. 75.6% tokens were selected by the anno-
tators. Against the annotators? gold standard, we
found that a simple rule-based filter for usernames,
hashtags, punctuation, and retweet tokens achieves
95.2% (with gold-standard POS tags) and 95.1%
(with automatic POS tags) average accuracy in the
task of selecting tokens with a syntactic function
in a ten-fold cross-validation experiment. To take
context into account, we developed a first-order
sequence model and found that it achieves 97.4%
average accuracy (again, ten-fold cross-validated)
with either gold-standard or automatic POS tags.
Features include POS; shape features that recog-
nize the retweet marker, hashtags, usernames, and
hyperlinks; capitalization; and a binary feature
for tokens that include punctuation. We trained
the model using the structured perceptron (Collins,
2002).
2.2 Multiword Expressions
We consider multiword expressions (MWEs) of
two kinds. The first, proper names, have been
widely modeled for information extraction pur-
poses, and even incorporated into parsing (Finkel
and Manning, 2009). (An example found in Fig-
ure 1 is LA Times.) The second, lexical idioms,
have been a ?pain in the neck? for many years (Sag
et al., 2002) and have recently received shallow
treatment in NLP (Baldwin and Kim, 2010; Con-
stant and Sigogne, 2011; Schneider et al., 2014).
Constant et al. (2012), Green et al. (2012), Candito
and Constant (2014), and Le Roux et al. (2014)
considered MWEs in parsing. Figure 1 provides
LA Times and All the Rage as examples.
Penn Treebank?style syntactic analysis (and de-
pendency representations derived from it) does
not give first-class treatment to this phenomenon,
though there is precedent for marking multiword
lexical units and certain kinds of idiomatic relation-
ships (Haji
?
c et al., 2012; Abeill? et al., 2003).
1
We argue that internal analysis of MWEs is not
critical for many downstream applications, and
therefore annotators should not expend energy on
developing and respecting conventions (or mak-
ing arbitrary decisions) within syntactically opaque
or idiosyncratic units. We therefore allow annota-
tors to decide to group words as explicit MWEs,
including: proper names (Justin Bieber, World
Series), noncompositional or entrenched nominal
compounds (belly button, grilled cheese), connec-
tives (as well as), prepositions (out of), adverbials
(so far), and idioms (giving up, make sure).
From an annotator?s perspective, a MWE func-
tions as a single node in the dependency parse,
with no internal structure. For idioms whose in-
ternal syntax is easily characterized, the parse can
be used to capture compositional structure, an at-
1
The popular Stanford typed dependencies (de Marneffe
and Manning, 2008) scheme includes a special dependency
type for multiwords, though this is only applied to a small list.
1002
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
? #belieber
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
Figure 1: Parse tree for a (constructed) example illustrating annotation challenges discussed in ?2. Colors highlight token
selection (gray; ?2.1), multiword expressions (blue; ?2.2), multiple roots (red; ?2.3), coordination (dotted arcs, green; ?3.2), and
noun phrase internal structure (orange; ?2.4). The internal structure of multiword expressions (dashed arcs below the sentence)
was predicted automatically by a parser, as described in ?2.2.
tractive property from the perspective of semantic
processing.
To allow training a fairly conventional statisti-
cal dependency parser from these annotations, we
find it expedient to apply an automatic conversion
to the MWE annotations, in the spirit of Johnson
(1998). We apply an existing dependency parser,
the first-order TurboParser (Martins et al., 2009)
trained on the Penn Treebank, to parse each MWE
independently, assigning structures like those for
LA Times and All the Rage in Figure 1. Arcs
involving the MWE in the annotation are then re-
connected to the MWE-internal root, so that the re-
sulting tree respects the original tokenization. The
MWE-internal arcs are given a special label so that
the transformation can be reversed and MWEs re-
constructed from parser output.
2.3 Multiple Roots
For news text such as that found in the Penn Tree-
bank, sentence segmentation is generally consid-
ered a very easy task (Reynar and Ratnaparkhi,
1997). Tweets, however, often contain multiple
sentences or fragments, which we call ?utterances,?
each with its own syntactic root disconnected from
the others. The selected tokens in Figure 1 com-
prise four utterances.
Our approach to annotation allows multiple ut-
terances to emerge directly from the connectedness
properties of the graph implied by an annotator?s
decisions. Our parser allows multiple attachments
to the ?wall? symbol, so that multi-rooted analyses
can be predicted.
2.4 Noun Phrase Internal Structure
A potentially important drawback of deriving de-
pendency structures from phrase-structure annota-
tions, as is typically done using the Penn Treebank,
is that flat annotations lead to loss of information.
This is especially notable for noun phrases in the
Penn Treebank (Vadas and Curran, 2007). Consider
Teen Pop Star Heartthrob in Figure 1; Penn Tree-
bank conventions would label this as a single NP
with four NN children and no internal structure. De-
pendency conversion tools would likely attach the
first three words in the NP to Heartthrob. Direct de-
pendency annotation (rather than phrase-structure
annotation followed by automatic conversion) al-
lows a richer treatment of such structures, which is
potentially important for semantic analysis (Vecchi
et al., 2013).
3 A Twitter Dependency Corpus
In this section, we describe the TWEEBANK cor-
pus, highlighting data selection (?3.1), the annota-
tion process (?3.2), important convention choices
(?3.3), and measures of quality (?3.4).
3.1 Data Selection
We added manual dependency parses to 929 tweets
(12,318 tokens) drawn from the POS-tagged Twit-
ter corpus of Owoputi et al. (2013), which are tok-
enized and contain manually annotated POS tags.
Owoputi et al.?s data consists of two parts. The
first, originally annotated by Gimpel et al. (2011),
consists of tweets sampled from a particular day,
October 27, 2010?this is known as OCT27. Due
to concerns about overfitting to phenomena specific
to that day (e.g., tweets about a particular sports
game), Owoputi et al. (2013) created a new set of
547 tweets (DAILY547) consisting of one random
English tweet per day from January 2011 through
June 2012.
Our corpus is drawn roughly equally from
OCT27 and DAILY547.
2
Despite its obvious tem-
poral skew, there is no reason to believe this sample
is otherwise biased; our experiments in ?5 suggest
that this property is important.
3.2 Annotation
Unlike a typical treebanking project, which may
take years and involve thousands of person-hours
of work by linguists, most of TWEEBANK was built
in a day by two dozen annotators, most of whom
had only cursory training in the annotation scheme.
2
This results from a long-term goal to fully annotate both.
1003
(1) RT @FRIENDSHlP : Friendship is love without
kissing ...
Friendship > is < love < without < kissing
(2) bieber is an alien ! :O he went down to earth .
bieber > is** < alien < an
he > [went down]** < to < earth
(3) RT @YourFavWhiteGuy : Helppp meeeee . I?mmm
meltiiinngggg ? http://twitpic.com/316cjg
Helppp** < meeeee
I?mmm** < meltiiinngggg
Figure 2: Examples of GFL annotations from the corpus.
Our annotators used the Graph Fragment Lan-
guage (GFL), a text-based notation that facilitates
keyboard entry of parses (Schneider et al., 2013). A
Python Flask web application allows the annotator
to validate and visualize each parse (Mordowanec
et al., 2014). Some examples are shown in Fig-
ure 2. Note that all of the challenges in ?2 are
handled easily by GFL notation: ?retweet? infor-
mation, punctuation, and a URL are not selected by
virtue of their exclusion from the GFL expression;
in (2) went down is annotated as a MWE using
GFL?s square bracket notation; in (3) the tokens
are grouped into two utterances whose roots are
marked by the ** symbol.
Schneider et al.?s GFL offers some additional fea-
tures, only some of which we made use of in this
project. One important feature allows an annotator
to leave the parse underspecified in some ways. We
allowed our annotators to make use of this feature;
however, we excluded from our training and test-
ing data any parse that was incomplete (i.e., any
parse that contained multiple disconnected frag-
ments with no explicit root, excluding unselected
tokens). Learning to parse from incomplete anno-
tations is a fascinating topic explored in the past
(Hwa, 2001; Pereira and Schabes, 1992) and, in the
case of tweets, left for future work.
An important feature of GFL that we did use is
special notation for coordination structures. For
the coordination structure in Figure 1, for example,
the notation is:
$a :: {? want} :: {&}
where $a creates a new node in the parse tree as it is
visualized for the annotator, and this new node at-
taches to the syntactic parent of the conjoined struc-
ture, avoiding the classic forced choice between
coordinator and conjunct as parent. For learning to
parse, we transform GFL?s coordination structures
into specially-labeled dependency parses collaps-
ing nodes like $a with the coordinator and labeling
the attachments specially for postprocessing, fol-
lowing Schneider et al. (2013). In our evaluation
(?5), these are treated like other attachments.
3.3 Annotation Conventions
A wide range of dependency conventions are in use;
in many cases these are conversion conventions
specifying how dependency trees can be derived
from phrase-structure trees. For English, the most
popular are due to Yamada and Matsumoto (2003)
and de Marneffe and Manning (2008), known as
?Yamada-Matsumoto? (YM) and ?Stanford? depen-
dencies, respectively. The main differences be-
tween them are in whether the auxiliary is the par-
ent of the main verb (or vice versa) and whether the
preposition or its argument heads a prepositional
phrase (Elming et al., 2013).
A full discussion of our annotation conventions
is out of scope. We largely followed the conven-
tions suggested by Schneider et al. (2013), which in
turn are close to those of YM. Auxiliary verbs are
parents of main verbs, and prepositions are parents
of their arguments. The key differences from YM
are in coordination structures (discussed in ?3.2;
YM makes the first conjunct the head) and posses-
sive structures, in which the possessor is the child
of the clitic, which is the child of the semantic head,
e.g., the > king > ?s > horses.
3.4 Intrinsic Quality
Our approach to developing this initial corpus of
syntactically annotated tweets was informed by an
aversion to making the perfect the enemy of the
good; that is, we sought enough data of sufficient
quality to build a usable parser within a relatively
short amount of time. If our research goals had
been to develop a replicable process for annotation,
more training and more quality control would have
been called for. Under our budgeted time and anno-
tator resources, this overhead was simply too costly.
Nonetheless, we performed a few analyses that give
a general picture of the quality of the annotations.
Inter-annotator agreement. 170 of the tweets
were annotated by multiple users. By the softCom-
Prec measure (Schneider et al., 2013),
3
the agree-
ment rate on dependencies is above 90%.
Expert linguistic judgment. A linguist co-
author examined a stratified sample (balanced
3
softComPrec is a generalization of attachment accuracy
that handles unselected tokens and MWEs.
1004
across annotators) of 60 annotations and rated their
quality on a 5-point scale. 30 annotations were
deemed to have ?no obvious errors,? 15 only minor
errors, 3 a major error (i.e., clear violation of an-
notation guidelines),
4
4 a major error and at least
one minor error, and 8 as containing multiple major
errors. Thus, 75% are judged as having no major
errors. We found this encouraging, considering that
this sample is skewed in favor of people who anno-
tated less (including many of the less experienced
and/or lower-proficiency annotators).
Pairwise ranking. For 170 of the doubly anno-
tated tweets, an experienced annotator examined
whether one or the other was markedly better. In
100 cases the two annotations were of comparable
quality (neither was obviously better) and did not
contain any obvious major errors. In only 7 pairs
did both of the annotations contain a serious error.
Qualitatively, we found several unsurprising
sources of error or disagreement, including em-
bedded/subordinate clauses, subject-auxiliary in-
version, predeterminers, and adverbial modifiers
following a modal/auxiliary verb and a main verb.
Clarification of the conventions, or even explicit
rule-based checking in the validation step, might
lead to quality improvements in further annotation
efforts.
4 Parsing Algorithm
For parsing, we start with TurboParser, which is
open-source and has been found to perform well on
a range of parsing problems in different languages
(Martins et al., 2013; Kong and Smith, 2014). The
underlying model allows for flexible incorporation
of new features and changes to specification in the
output space. We briefly review the key ideas in
TurboParser (?4.1), then describe decoder modifi-
cations required for our problem (?4.2). We then
discuss features we added to TurboParser (?4.3).
4.1 TurboParser
Let an input sentence be denoted by x and the set
of possible dependency parses for x be denoted by
Y
x
. A generic linear scoring function based on a
4
What we deemed major errors included, for example,
an incorrect dependency relation between an auxiliary verb
and the main verb (like ima > [have to]). Minor errors
included an incorrect attachment between two modifiers of
the same head, as in the > only > [grocery store]?the
correct annotation would have two attachments to a single
head, i.e. the > [grocery store] < only (or equivalent).
feature vector representation g is used in parsing
algorithms that seek to find:
parse
?(x) = argmax
y?Y
x
w
?
g(x,y) (1)
The score is parameterized by a vector w of
weights, which are learned from data (most com-
monly using MIRA, McDonald et al., 2005a).
The decomposition of the features into local
?parts? is a critical choice affecting the computa-
tional difficulty of solving Eq. 1. The most aggres-
sive decomposition leads to an ?arc-factored? or
?first-order? model, which permits exact, efficient
solution of Eq. 1 using spanning tree algorithms
(McDonald et al., 2005b) or, with a projectivity
constraint, dynamic programming (Eisner, 1996).
Second- and third-order models have also been
introduced, typically relying on approximations,
since less-local features increase the computational
cost, sometimes to the point of NP-hardness (Mc-
Donald and Satta, 2007). TurboParser attacks the
parsing problem using a compact integer linear pro-
gramming (ILP) representation of Eq. 1 (Martins
et al., 2009), then employing alternating directions
dual decomposition (AD
3
; Martins et al., 2011).
This enables inclusion of second-order features
(e.g., on a word with its sibling or grandparent;
Carreras, 2007) and third-order features (e.g., a
word with its parent, grandparent, and a sibling, or
with its parent and two siblings; Koo and Collins,
2010).
For a collection of (possibly overlapping) parts
for input x, S
x
(which includes the union of all
parts of all trees in Y
x
), we will use the following
notation. Let
g(x,y) = ?
s?S
x
f
s
(x,y), (2)
where f
s
only considers part s and is nonzero only
if s is present in y. In the ILP framework, each s
has a corresponding binary variable z
s
indicating
whether part s is included in the output. A col-
lection of constraints relating z
s
define the set of
feasible vectors z that correspond to valid outputs
and enfore agreement between parts that overlap.
Many different versions of these constraints have
been studied (Riedel and Clarke, 2006; Smith and
Eisner, 2008; Martins et al., 2009, 2010).
A key attraction of TurboParser is that many
overlapping parts can be handled, making use of
separate combinatorial algorithms for efficiently
handling subsets of constraints. For example, the
constraints that force z to encode a valid tree can
be exploited within the framework by making calls
1005
to classic arborescence algorithms (Chu and Liu,
1965; Edmonds, 1967). As a result, when describ-
ing modifications to TurboParser, we need only to
explain additional constraints and features imposed
on parts.
4.2 Adapted Parse Parts
The first collection of parts we adapt are simple
arcs, each consisting of an ordered pair of indices
of words in x; arc(p,c) corresponds to the attach-
ment of x
c
as a child of x
p
(iff z
arc(p,c) = 1). Our rep-
resentation explicitly excludes some tokens from
being part of the syntactic analysis (?2.1); to han-
dle this, we constrain z
arc(i, j) = 0 whenever xi or x j
is excluded.
The implication is that excluded tokens are still
?visible? to feature functions that involve other
edges. For example, some conventional first-order
features consider the tokens occurring between a
parent and child. Even if a token plays no syntactic
role of its own, it might still be informative about
the syntactic relationships among other tokens. We
note three alternative methods:
1. We might remove all unselected tokens from
x before running the parser. In ?5.6 we find
this method to fare 1.7?2.3% worse than our
modified decoding algorithm.
2. We might remove unselected tokens but use
them to define new features, so that they still
serve as evidence. This is the approach taken
by Ma et al. (2014) for punctuation. We judge
our simple modification to the decoding algo-
rithm to be more expedient, and leave the trans-
lation of existing context-word features into that
framework for future exploration.
3. We might incorporate the token selection deci-
sions into the parser, performing joint inference
for selection and parsing. The AD
3
algorithm
within TurboParser is well-suited to this kind
of extension: z-variables for each token?s se-
lection could be added, and similar scores to
those of our token selection sequence model
(?2.1) could be integrated into parsing. Given,
however, that the sequence model achieves over
97% accuracy, and that perfect token selection
would gain only 0.1?1% in parsing accuracy (re-
ported in ?5.5), we leave this option for future
work as well.
For first-order models, the above change is all
that is necessary. For second- and third-order
models, TurboParser makes use of head automata,
in particular ?grand-sibling head automata? that
assign scores to word tuples of x
g
, its child x
p
,
and two of x
p
?s adjacent children, x
c
and x
?
c
(Koo
et al., 2010). The second-order models in our
experiments include parts for sibling(p,c,c?) and
grandparent(p,c,g) and use the grand-sibling head
automaton to reason about these together. Au-
tomata for an unselected x
p
or x
g
, and transitions
that consider unselected tokens as children, are
eliminated. In order to allow the scores to depend
on unselected tokens between x
c
and x
?
c
, we added
the binned counts of unselected tokens (mostly
punctuation) joint with the word form and POS
tag of x
p
and the POS tag of x
c
and x
?
c
as features
scored in the sibling(p,c,c?) part. The changes dis-
cussed above comprise the totality of adaptations
we made to the TurboParser algorithm; we refer to
them as ?parsing adaptations? in the experiments.
4.3 Additional Features
Brown clusters. Owoputi et al. (2013) found that
Brown et al. (1992) clusters served as excellent fea-
tures in Twitter POS tagging. Others have found
them useful in parsing (Koo et al., 2008) and other
tasks (Turian et al., 2010). We therefore follow
Koo et al. in incorporating Brown clusters as fea-
tures, making use of the publicly available Twitter
clusters from Owoputi et al.
5
We use 4 and 6 bit
cluster representations to create features wherever
POS tags are used, and full bit strings to create
features wherever words were used.
Penn Treebank features. A potential danger of
our choice to ?start from scratch? in developing
a dependency parser for Twitter is that the result-
ing annotation conventions, data, and desired out-
put are very different from dependency parses de-
rived from the Penn Treebank. Indeed, Foster et al.
(2011a) took a very different approach, applying
Penn Treebank conventions in annotation of a test
dataset for evaluation of a parser trained using Penn
Treebank trees. In ?5.4, we replicate, for depen-
dencies, their finding that a Penn Treebank?trained
parser is hard to beat on their dataset, which was
not designed to be topically representative of En-
glish Twitter. When we turn to a more realistic
dataset like ours, we find the performance of the
Penn Treebank?trained parser to be poor.
Nonetheless, it is hard to ignore such a large
amount of high-quality syntactic data. We there-
5http://www.ark.cs.cmu.edu/TweetNLP/clusters/
50mpaths2
1006
fore opted for a simple, stacking-inspired incor-
poration of Penn Treebank information into our
model.
6
We define a feature on every candidate arc
whose value is the (quantized) score of the same arc
under a first-order model trained on the Penn Tree-
bank converted using head rules that are as close
as possible to our conventions (discussed in more
detail in ?5.1). This lets a Penn Treebank model
literally ?weigh in? on the parse for a tweet, and
lets the learning algorithm determine how much
consideration it deserves.
5 Experiments
Our experiments quantify the contributions of vari-
ous components of our approach.
5.1 Setup
We consider two test sets. The first, TEST-NEW,
consists of 201 tweets from our corpus annotated
by the most experienced of our annotators (one
of whom is a co-author of this work). Given very
limited data, we believe using the highest quality
data for measuring performance, and lower-quality
data for training, is a sensibly realistic choice.
Our second test set, TEST-FOSTER, is the dataset
annotated by Foster et al. (2011b), which consists
of 250 sentences. Recall that their corpus was
annotated with phrase structures according to Penn
Treebank conventions. Conversion to match our
annotation conventions was carried out as follows:
1. We used the PennConverter tool with head rule
options selected to approximate our annotation
conventions as closely as possible.
7
2. An experienced annotator manually modified
the automatically converted trees by:
(a) Performing token selection (?2.1) to remove
the tokens which have no syntactic function.
(b) Grouping MWEs (?2.2). Here, most of the
MWEs are named entities such as Manch-
ester United.
(c) Attaching the roots of the utterance in tweets
to the ?wall? symbol (?2.3).
8
6
Stacking is a machine learning method where the predic-
tions of one model are used to create features for another. The
second model may be from a different family. Stacking has
been found successful for dependency parsing by Nivre and
McDonald (2008) and Martins et al. (2008). Johansson (2013)
describes further advances that use path features.
7http://nlp.cs.lth.se/software/treebank_
converter; run with -rightBranching=false
-coordStructure=prague -prepAsHead=true
-posAsHead=true -subAsHead=true -imAsHead=true
-whAsHead=false.
8
This was infrequent; their annotations split most multi-
TRAIN TEST-NEW TEST-FOSTER
tweets 717 201 < 250?
unique tweets 569 201 < 250?
tokens 9,310 2,839 2,841
selected tokens 7,015 2,158 2,366
types 3,566 1,461 1,230
utterances 1,473 429 337
multi-root tweets 398 123 60
MWEs 387 78 109
Table 1: Statistics of our datasets. (A tweet with k annotations
in the training set is counted k times for the totals of tokens,
utterances, etc.).
?
TEST-FOSTER contains 250 manually split
sentences. The number of tweets should be smaller but is not
recoverable from the data release.
(d) Recovering the internal structure of the noun
phrases.
(e) Fixing a difference in conventions with re-
spect to subject-auxiliary inversion.
9
We consider two training sets. TRAIN-NEW con-
sists of the remaining 717 tweets from our corpus
(?3) annotated by the rest of the annotators. Some
of these tweets have annotations from multiple an-
notators; 11 annotations for tweets that also oc-
curred in TEST-NEW were excluded. TRAIN-PTB
is the conventional training set from the Penn Tree-
bank (?2?21). The PennConverter tool was used
to extract dependencies, with head rule options se-
lected to approximate our annotation conventions
as closely as possible (see footnote 7). The result-
ing annotations lack the same attention to noun
phrase?internal structure (?2.4) and handle subject-
auxiliary inversions differently than our data. Part-
of-speech tags were coarsened to be compatible
with the Twitter POS tags, using the mappings spec-
ified by Gimpel et al. (2011).
Statistics for the in-domain datasets are given in
Table 1. As we can see in the table, more than half
of the tweets in our corpus have multiple utterances.
The out-of-vocabulary rate for our TRAIN/TEST-
NEW split is 33.7% by token and 62.5% by type;
for TRAIN/TEST-FOSTER it is 41.4% and 64.6%
respectively. These are much higher than the 2.5%
and 13.2% in the standard Penn Treebank split.
All evaluations here are on unlabeled attachment
F
1
scores.
10
Our parser provides labels for coordi-
nation structures and MWEs (?2), but we do not
present detailed evaluations of those due to space
constraints.
utterance tweets into separate sentence-instances.
9
For example, in the sentence Is he driving, we attached
he to driving while PennConverter attaches it to Is.
10
Because of token selection, precision and recall may not
be equal.
1007
5.2 Preprocessing
Because some of the tweets in our test set were
also in the training set of Owoputi et al. (2013),
we retrained their POS tagger on all the annotated
data they have minus the 201 tweets in our test
set. Its tagging accuracy was 92.8% and 88.7% on
TEST-NEW and TEST-FOSTER, respectively. The
token selection model (?2.1) achieves 97.4% on
TEST-NEW with gold or automatic POS tagging;
and on TEST-FOSTER, 99.0% and 99.5% with gold
and automatic POS tagging, respectively.
As noted in ?4.3, Penn Treebank features were
developed using a first-order TurboParser trained
on TRAIN-PTB; Brown clusters were included in
computing these Penn Treebank features if they
were available in the parser to which the features
(i.e. Brown clusters) were added.
5.3 Main Parser
The second-order TurboParser described in ?4,
trained on TRAIN-NEW (default hyperparameter
values), achieves 80.9% unlabeled attachment ac-
curacy on TEST-NEW and 76.1% on TEST-FOSTER.
The experiments consider variations on this main
approach, which is the version released as TWEE-
BOPARSER.
The discrepancy between the two test sets is
easily explained: as noted in ?3.1, the dataset
from which our tweets are drawn was designed
to be representative of English on Twitter. Fos-
ter et al. (2011b) selected tweets from Berming-
ham and Smeaton?s (2010) corpus, which uses fifty
predefined topics like politics, business, sports,
and entertainment?in short, topics not unlike
those found in the Penn Treebank. Relative to
the Penn Treebank training set, the by-type out-
of-vocabulary rates are 45.2% for TEST-NEW and
only 21.6% for TEST-FOSTER (cf. 13.2% for the
Penn Treebank test set).
Another mismatch is in the handling of utter-
ances. In our corpus, utterance segmentation
emerges from multi-rooted annotations (?2.3). Fos-
ter et al. (2011b) manually split each tweet into
utterances and treat those as separate instances in
their corpus, so that our model trained on often
multi-rooted tweets from TRAIN is being tested
only on single-rooted utterances.
5.4 Experiment: Which Training Set?
We consider the direct use of TRAIN-PTB instead
of TRAIN-NEW. Table 2 reports the results on both
Unlabeled Attachment F
1
(%)
mod. POS POS as-is
TEST-NEW
Baseline 73.0 73.5
+ Brown 73.7 73.3
+ Brown & PA 72.9 73.1
TEST-FOSTER
Baseline 76.3 75.2
+ Brown 75.5 76.7
+ Brown & PA 76.9 77.0
Table 2: Performance of second-order TurboParser trained on
TRAIN-PTB, with various preprocessing options. The main
parser (?5.3) achieves 80.9% and 76.1% on the two test sets,
respectively; see ?5.4 for discussion.
test sets, with various options. ?Baseline? is off-
the-shelf second-order TurboParser. We consider
augmenting it with Brown cluster features (?4.3;
?+ Brown?) and then also with the parsing adapta-
tions of ?4.2 (?+ Brown & PA?). Another choice
is whether to modify the POS tags at test time; the
modified version (?mod. POS?) maps at-mentions
to pronoun, and hashtags and URLs to noun.
We note that comparing these scores to our main
parser (?5.3) conflates three very important inde-
pendent variables: the amount of training data
(39,832 Penn Treebank sentences vs. 1,473 Twitter
utterances), the annotation method, and the source
of the data. However, we are encouraged that, on
what we believe is the superior test set (TEST-NEW),
our overall approach obtains a 7.8% gain with an
order of magnitude less annotated data.
5.5 Experiment: Effect of Preprocessing
Table 3 (second block, italicized) shows the per-
formance of the main parser on both test sets with
gold-standard and automatic POS tagging and to-
ken selection. On TEST-NEW, with either gold-
standard POS tags or gold-standard token selection,
performance increases by 1.1%; with both, it in-
creases by 2.3%. On TEST-FOSTER, token selec-
tion matters much less, but POS tagging accounts
for a drop of more than 6%. This is consistent with
Foster et al.?s finding: using a fine-grained Penn
Treebank?trained POS tagger (achieving around
84% accuracy on Twitter), they saw 5?8% improve-
ment in unlabeled dependency attachment accuracy
using gold-standard POS tags.
5.6 Experiment: Ablations
We ablated each key element of our main parser?
PTB features, Brown features, second order fea-
tures and decoding, and the parsing adaptations of
1008
0.76
0.78
0.8
0.82
Unla
beled
 Atta
chme
nt F 1
 
 
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
First?OrderSecond?Order
(a) TEST-NEW
0.7
0.72
0.74
0.76
Unla
beled
 Atta
chme
nt F 1
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
 
 First?OrderSecond?Order
(b) TEST-FOSTER
Figure 3: Feature ablations; these charts present the same scores shown in Table 3 and more variants of the first-order model.
Unlabeled Attachment F
1
(%)
TEST-NEW TEST-FOSTER
Main parser 80.9 76.1
Gold POS and TS 83.2 82.8
Gold POS, automatic TS 82.0 82.3
Automatic POS, gold TS 82.0 76.2
Single ablations:
? PTB 80.2 72.6
? Brown 81.2 75.4
? 2nd order 80.1 75.6
? PA 79.2 73.7
Double ablations:
? PTB, ? Brown 79.5 72.8
? PTB, ? 2nd order 78.5 72.2
? PTB, ? PA 77.4 69.6
? Brown, ? 2nd order 80.7 74.5
? Brown, ? PA 78.2 73.7
? 2nd order, ? PA 77.7 73.5
Baselines:
Second order 76.5 70.4
First order 76.1 70.4
Table 3: Effects of gold-standard POS tagging and token
selection (TS; ?5.5) and of feature ablation (?5.6). The ?base-
lines? are TurboParser without the parsing adaptations in ?4.2
and without Penn Treebank or Brown features. The best result
in each column is bolded. See also Figure 3.
?4.2?as well as each pair of these. These condi-
tions use automatic POS tags and token selection.
The ?? PA? condition, which ablates parsing adap-
tations, is accomplished by deleting punctuation
(in training and test data) and parsing using Turbo-
Parser?s existing algorithm.
Results are shown in Table 3. Further results
with first- and second-order TurboParsers are plot-
ted in Figure 3. Notably, a 2?3% gain is obtained by
modifying the parsing algorithm, and our stacking-
inspired use of Penn Treebank data contributes in
both cases, quite a lot on TEST-FOSTER (unsur-
prisingly given that test set?s similarity to the Penn
Treebank). More surprisingly, we find that Brown
cluster features do not consistently improve perfor-
mance, at least not as instantiated here, with our
small training set.
6 Conclusion
We described TWEEBOPARSER, a dependency
parser for English tweets that achieves over 80%
unlabeled attachment score on a new, high-quality
test set. This is on par with state-of-the-art re-
ported results for news text in Turkish (77.6%;
Koo et al., 2010) and Arabic (81.1%; Martins
et al., 2011). Our contributions include impor-
tant steps taken to build the parser: a considera-
tion of the challenges of parsing tweets that in-
formed our annotation process, the resulting new
TWEEBANK corpus, adaptations to a statistical
parsing algorithm, a new approach to exploiting
data in a better-resourced domain (the Penn Tree-
bank), and experimental analysis of the decisions
we made. The dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgments
The authors thank the anonymous reviewers
and Andr? Martins, Yanchuan Sim, Wang Ling,
Michael Mordowanec, and Alexander Rush for
helpful feedback, as well as the annotators Waleed
Ammar, Jason Baldridge, David Bamman, Dallas
Card, Shay Cohen, Jesse Dodge, Jeffrey Flanigan,
Dan Garrette, Lori Levin, Wang Ling, Bill Mc-
Dowell, Michael Mordowanec, Brendan O?Connor,
Rohan Ramanath, Yanchuan Sim, Liang Sun, Sam
Thomson, and Dani Yogatama. This research was
supported in part by the U. S. Army Research Lab-
oratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533 and by
NSF grants IIS-1054319 and IIS-1352440.
1009
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Treebanks,
pages 165?187. Springer.
Timonthy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Handbook of Natural Lan-
guage Processing, Second Edition. CRC Press, Tay-
lor and Francis Group.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proc. of CIKM.
Ann Bies, Justin Mott, Colin Warner, and Seth
Kulick. 2012. English Web Treebank. Techni-
cal Report LDC2012T13, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T13.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie Candito and Matthieu Constant. 2014. Strate-
gies for contiguous multiword expression analysis
and dependency parsing. In Proc. of ACL.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proc. of
EMNLP-CoNLL.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest
arborescence of a directed graph. Scientia Sinica,
14(10):1396.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to
the Real World.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate mul-
tiword expression recognition and parsing. In Proc.
of ACL.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Proc. of COLING Workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proc. of EMNLP-
CoNLL.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(233-240):160.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL-HLT.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, H?ctor Mart?nez Alonso, and
Anders S?gaard. 2013. Down-stream effects of tree-
to-dependency conversions. In Proc. of NAACL-
HLT.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc
of ACL-HLT.
Jennifer Foster, ?zlem ?etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011a.
#hardtoparse: POS tagging and parsing the Twitter-
verse. In Proc. of AAAI Workshop on Analyzing Mi-
crotext.
Jennifer Foster, ?zlem ?etino?glu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011b. From news to comment:
resources and benchmarks for parsing the language
of Web 2.0. In Proc. of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2012. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Stephan Greene and Philip Resnik. 2009. Syntac-
tic packaging and implicit sentiment. In Proc. of
NAACL.
1010
Jan Haji?c, Eva Haji?cov?, Jarmila Panevov?, Petr Sgall,
Silvie Cinkov?, Eva Fu?c?kov?, Marie Mikulov?,
Petr Pajas, Jan Popelka, Ji?r? Semeck`y, Jana
?indlerov?, Jan ?t?ep?nek, Josef Toman, Zde?nka
Ure?ov?, and Zden?ek ?abokrtsk?. 2012. Prague
Czech-English Dependency Treebank 2.0. Techni-
cal Report LDC2012T08, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T08.
Rebecca Hwa. 2001. Learning Probabilistic Lexical-
ized Grammars for Natural Language Processing.
Ph.D. thesis, Harvard University.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In Proc. of NAACL-HLT.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Lingpeng Kong and Noah A. Smith. 2014. An empiri-
cal comparison of parsing methods for Stanford de-
pendencies. ArXiv:1404.4314.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In
Proc. of ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual de-
composition for parsing with non-projective head au-
tomata. In Proc. of EMNLP.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. of ICWSM.
Joseph Le Roux, Matthieu Constant, and Antoine
Rozenknop. 2014. Syntactic parsing and compound
recognition via dual decomposition: application to
French. In Proc. of COLING.
Ji Ma, Yue Zhang, and Jingbo Zhu. 2014. Punctua-
tion processing for projective dependency parsing.
In Proc. of ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional linguistics, 19(2):313?330.
Andr? F.T. Martins, Miguel Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proc. of ACL.
Andr? F.T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, Pedro M.Q.
Aguiar, and M?rio A.T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proc. of ACL-
IJCNLP.
Andr? F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and M?rio A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji
?
c. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proc. of IWPT.
Michael T. Mordowanec, Nathan Schneider, Chris.
Dyer, and Noah A. Smith. 2014. Simplified depen-
dency annotations with GFL-Web. In Proc. of ACL,
demonstration track.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL-HLT.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL-HLT.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proc. of ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. of ANLP.
1011
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In Proc. of EMNLP.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Proc. of
CICLing.
Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. Transactions of the Association for Compu-
tational Linguistics, 2:193?206.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of EMNLP.
Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob
Eisenstein. 2014. Modeling factuality judgments in
social media text. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proc. of
ACL.
Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-
roni. 2013. Studying the recursive behaviour of
adjectival modification with compositional distribu-
tional semantics. In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
1012
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 82?90,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PropBank Annotation of Multilingual Light Verb Constructions 
 
 
Jena D. Hwang1, Archna Bhatia3, Clare Bonial1, Aous Mansouri1,  
Ashwini Vaidya1, Nianwen Xue2, and Martha Palmer1 
1Department of Linguistics, University of Colorado at Boulder, Boulder CO 80309 
2Department of Computer Science, Brandeis University, Waltham MA 02453 
3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801 
{hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer} 
@colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu 
 
  
 
Abstract 
In this paper, we have addressed the task 
of PropBank annotation of light verb 
constructions, which like multi-word 
expressions pose special problems. To 
arrive at a solution, we have evaluated 3 
different possible methods of annotation. 
The final method involves three passes: 
(1) manual identification of a light verb 
construction, (2) annotation based on the 
light verb construction?s Frame File, and 
(3) a deterministic merging of the first 
two passes. We also discuss how in 
various languages the light verb 
constructions are identified and can be 
distinguished from the non-light verb 
word groupings.  
1 Introduction  
One of the aims in natural language processing, 
specifically the task of semantic role labeling 
(SRL), is to correctly identify and extract the 
different semantic relationships between words 
in a given text. In such tasks, verbs are 
considered important, as they are responsible for 
assigning and controlling the semantic roles of 
the arguments and adjuncts around it. Thus, the 
goal of the SRL task is to identify the arguments 
of the predicate and label them according to their 
semantic relationship to the predicate (Gildea 
and Jurafsky, 2002; Pradhan et al, 2003).  
To this end, PropBank (Palmer et. al., 2005) 
has developed semantic role labels and labeled 
large corpora for training and testing of 
supervised systems. PropBank identifies and 
labels the semantic arguments of the verb on a 
verb-by-verb basis, creating a separate Frame 
File that includes verb specific semantic roles to 
account for each subcategorization frame of the 
verb. It has been shown that training supervised 
systems with PropBank?s semantic roles for 
shallow semantic analysis yield good results (see 
CoNLL 2005 and 2008).  
However, semantic role labeling tasks are 
often complicated by multiword expressions 
(MWEs) such as idiomatic expressions (e.g., 
?Stop pulling my leg!?), verb particle 
constructions (e.g., ?You must get over your 
shyness.?), light verb constructions (e.g., ?take a 
walk?, ?give a lecture?), and other complex 
predicates (e.g., V+V predicates such as Hindi?s 
???? ??? nikal gayaa, lit. ?exit went?, means 
?left? or ?departed?). MWEs that involve verbs 
are especially challenging because the 
subcategorization frame of the predicate is no 
longer solely dependent on the verb alone. 
Rather, in many of these cases the argument 
structure is assigned by the union of two 
predicating elements. Thus, it is important that 
the manual annotation of semantic roles, which 
will be used by automatic SRL systems, define 
and label these MWEs in a consistent and 
effective manner. 
In this paper we focus on the PropBank 
annotation of light verb constructions (LVCs). 
We have developed a multilingual schema for 
annotating LVCs that takes into consideration the 
similarities and differences shared by the 
construction as it appears in English, Arabic, 
Chinese, and Hindi. We also discuss in some 
detail the practical challenges involved in the 
crosslinguistic analysis of LVCs, which we hope 
will bring us a step closer to a unified 
crosslinguistic analysis.    
Since NomBank, as a companion to 
PropBank, provides corresponding semantic role 
82
labels for noun predicates (Meyers et al, 2004), 
we would like to take advantage of NomBank?s 
existing nominalization Frame Files and 
annotations as much as possible.  A question that 
we must therefore address is, ?Are 
nominalization argument structures exactly the 
same whether or not they occur within an LVC?? 
as will be discussed in section 6.1. 
2 Identifying Light Verb Constructions 
Linguistically LVCs are considered a type of a 
complex predicate. Many studies from differing 
angles and frameworks have characterized 
complex predicates as a fusion of two or more 
predicative elements. For example, Rosen (1997) 
treats complex structures as complementation 
structures, where the argument structure of 
elements in a complex predicate are fused 
together.  Goldberg (1993) takes a constructional 
approach to complex predicates and arrives at an 
analysis that is comparable to viewing complex 
predicates as a single lexical item. Similarly, 
Mohanan (1997) assumes different levels of 
linguistic representation for complex predicates 
in which the elements, such as the noun and the 
light verb, functionally combine to give a single 
clausal nucleus. Alsina (1997) and Butt (1997) 
suggest that complex predicates may be formed 
by syntactically independent elements whose 
argument structures are brought together by a 
predicate composition mechanism.  
While there is no clear-cut definition of LVCs, 
let alne the whole range of complex predicates, 
for the purposes of this study, we have adapted 
our approach largely from Butt?s (2004) criteria 
for defining LVCs. LVCs are characterized by a 
light verb and a predicating complement 
(henceforth, true predicate) that ?combine to 
predicate as a single element.? (Ibid.) In LVC, 
the verb is considered semantically bleached in 
such a way that the verb does not hold its full 
predicating power. Thus, the light verb plus its 
true predicate can often be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression. For example, 
the light verb ?gave? and the predicate ?lecture? 
in ?gave a lecture?, together form a single 
predicating unit such that it can be paraphrased 
by ?lectured?. 
True predicates in LVCs can be a noun (the 
object of the verb or the object of the preposition 
in a prepositional phrase), an adjective, or a verb. 
One light verb plus true predicate combination 
found commonly across all our PropBank 
languages (i.e., English, Arabic, Chinese, and 
Hindi) is the noun as the object of the verb as in 
?Sara took [a stroll] along the beach?. In Hindi, 
true predicates can be adjectives or verbs, in 
addition to the nouns. 
??? ? ??? [?????]  ???         (Adjective) 
to-me  you [nice]  seem 
lit. ?You seem nice to me? 
'You (are) liked to me (=I like you).' 
?????  ?? ???  [??] ????   (Verb) 
I-ERG everything  [do] took 
lit. ?I took do everything? 
'I have done everything.' 
As for Arabic, the LVCs come in verb+noun 
pairings. However, they surface in two syntactic 
forms. It can either be the object of the verb just 
like in English: 
 
 ???? ????]?????? ] ????? ??  
gave.he Georges [lecture] PREP Lebanon 
lit.'Georges gave a lecture about Lebanon' 
?Georges lectured about Lebanon? 
or the complement can be the object of a 
preposition: 
 
 ??????]????? ]????? ????? 
conduct.I [PREP-visit] our.saint Ilias 
lit. ?I will conduct with visit Saint Ilias?s? 
?I will visit Saint Ilias?s? 
3 Standard PropBank  
Annotation Procedure 
The PropBank annotation process can be broken 
down into two major steps: creation of the Frame 
Files for verbs occurring in the data and 
annotation of the data using the Frame Files. 
During the creation of the Frame Files, the 
usages of the verbs in the data are examined by 
linguists (henceforth, ?framers?). Based on these 
observations, the framers create a Frame File for 
each verb containing one or more framesets, 
which correspond to coarse-grained senses of the 
predicate lemma. Each frameset specifies the 
PropBank labels (i.e., ARG0, ARG1,?ARG5) 
corresponding to the argument structure of the 
verb. Additionally, illustrative examples are 
included for each frameset, which will later be 
referenced by the annotators. These examples 
also include the use of the ARGM labels. 
Thus, the framesets are based on the 
examination of the data, the framers? linguistic 
knowledge and native-speaker intuition. At 
83
times, we also make use of the syntactic and 
semantic behavior of the verb as described by 
certain lexical resources. These resources include 
VerbNet (Kipper et. al., 2006) and FrameNet 
(Baker et. al., 1998) for English, a number of 
monolingual and bilingual dictionaries for 
Arabic, and Hindi WordNet and DS Parses 
(Palmer et. al., 2009) for Hindi. Additionally, if 
available, we consult existing framesets of words 
with similar meanings across different languages. 
The data awaiting annotation are passed onto 
the annotators for a double-blind annotation 
process using the previously created framesets. 
The double annotated data is then adjudicated by 
a third annotator, during which time the 
differences of the two annotations are resolved to 
produce the Gold Standard. 
Two major guiding considerations during the 
framing and annotating process are data 
consistency and annotator productivity. During 
the frameset creation process, verbs that share 
similar semantic and syntactic characteristics are 
framed similarly. During the annotation process, 
the data is organized by verbs so that each verb is 
tackled all at once. In doing so, we firstly ensure 
that the framesets of similar verbs, and in turn, 
the annotation of the verbs, will both be 
consistent across the data. Secondly, by tackling 
annotation on verb-by-verb basis, the annotators 
are able to concentrate on a single verb at a time, 
making the process easier and faster for the 
annotators. 
4 Annotating LVC 
A similar process must be followed when 
annotating light verb constructions The first step 
is to create consistent Frame Files for light verbs. 
Then in order to make the annotation process 
produce consistent data at a reasonable speed, we 
have decided to carry out the light verb 
annotation in three passes (Table 1):  (1) annotate 
the light verb, (2) annotate the true predicate, and 
(3) merge the two annotations into one. 
The first pass involves the identification of the 
light verb. The most important parts of this step 
are to identify a verb as having bleached 
meaning, thereafter assign a generic light verb 
frameset and identify the true predicating 
expression of the sentence, which would be 
marked with ARG-PRX (i.e., ARGument-
PRedicating eXpression). For English, for 
example, annotators were instructed to use Butt?s 
(2004) criteria as described in Section 2. These 
criteria required that annotators be able to 
recognize whether or not the complement of a 
potential light verb was itself a predicating 
element. To make this occasionally difficult 
judgment, annotators used a simple heuristic test 
of whether or not the complement was headed by 
an element that has a verbal counterpart.  If so, 
the light verb frameset was selected. 
The second pass involves the annotation of the 
sentence with the true predicate as the relation. 
During this pass, the true predicate is annotated 
with an appropriate frameset. In the third pass, 
the arguments and the modifiers of the two 
previous passes are reconciled and merged into a 
single annotation. In order to reduce the number 
of hand annotation, it is preferable for this last 
pass, the Pass 3, to be done automatically. 
Since the nature of the light verb is different 
from that of other verbs as described in Section 
2, the advantage of doing the annotation of the 
light verb and the true predicate on separate 
passes is that in the light verb pass the annotators 
will be able to quickly dispose of the verb as a 
light verb and in the second pass, they will be 
allowed to solely focus on the annotation of the 
light verb?s true predicate. 
The descriptions of how the arguments and 
modifiers of the light verbs and their true 
predicates are annotated are mentioned in Table 
1, but notably, none of the examples in it 
currently include the annotation of arguments 
 Pass 1: Pass 2: Pass 3: 
 Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
and 
Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the light verb are annotated 
- Arguments and modifiers of 
the true predicate are annotated 
- Arguments and modifiers 
found in the two passes are 
merged, preferably 
automatically. 
Frameset Light verb frameset True predicate?s frameset LVC?s frameset 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG-MNR: brisk  
REL: walk 
REL: took walk 
ARG-MNR: brisk 
Table 1. Preliminary Annotation Scheme 
84
and modifiers.  This is intentional, as coming to 
an agreement concerning the details of what 
exactly each of the three passes looks like while 
meeting the needs of the four PropBank 
languages is quite challenging. Thus, for the rest 
of the paper we will discuss the strengths and 
weaknesses of the two trial methods of 
annotation we have considered and discarded in 
Section 5, as well as the final annotation scheme 
we chose in Section 6. 
5 Trials 
5.1 Method 1 
As our first attempt, the annotation of argument 
and adjuncts was articulated in the following 
manner (Table 2). 
Pass 1: Pass 2: 
Light verb True predicate 
- Predicating expression 
is labeled ARG-PRX 
- Annotate the Subject 
argument of the light 
verb as the Arg0. 
- Annotate the rest of the 
arguments and modifiers 
of the light verb with 
ARGM labels. 
- Annotate arguments 
and modifiers of the 
true predicate within 
its domain of locality. 
Generic light verb Frame 
File 
True predicate?s 
Frame File 
?-RKQ WRRN D EULVN ZDON WKURXJK WKH SDUN? 
ARG0: John 
REL: took 
ARG-PRX: a brisk walk 
ARG-DIR: through the park 
ARG-MNR: brisk  
REL: walk 
Table 2. Method 1 for annotation for Passes 1 and 2. 
Revised information is in italics. 
In Pass 1, in addition to annotating the 
predicating expression of the light verb with 
ARG-PRX, the subject argument was marked 
with an ARG0. The choice of ARG0, which 
corresponds to a proto-typical agent, was guided 
by the observation that English LVCs tend to 
lend a component of agentivity to the subject 
even in cases where the true predicate would not 
necessarily assign an agent as its subject. The 
rest of the arguments and modifiers were labeled 
with corresponding ARGM (i.e., modifier) 
labels. The assumption here is that the arguments 
of the light verb will also be the arguments of the 
true predicate.   
In Pass 2, then, the annotation of the 
arguments of the true predicate was restricted to 
its domain of locality (i.e., the span of the ARG-
PRX as marked in Pass1). That is, in the example 
?John took a brisk walk through the park?, the 
labeled spans for the true predicate would be 
limited to the NP ?a brisk walk? and neither 
?John? nor through the park? would be annotated 
as the arguments of the true predicate ?walk?. 
Frame Files: This method would require three 
Frame Files: a generic light verb Frame File, a 
true predicate Frame File, and an LVC Frame 
File. The Frame File for the light verb would not 
be specific to the form of the light verb (e.g., 
same frame for take and make). Rather, it would 
indicate a skeletal argument structure in order to 
reduce the amount of Frame Files made, 
including only Arg0 as its argument1.  
5.2 Weakness of Method 1 
This method has one glaring problem: the 
assumption that the semantic roles of the 
arguments as assigned by the light verb 
uniformly coincide with those assigned by the 
true predicate does not always hold. Consider the 
following English sentence2. 
whether Wu Shu-Chen would make another 
[appearance] in court was subject to observation 
In this example, ?Wu Shu-Chen? is the agent 
argument (Arg0) of the light verb ?make? and is 
the theme or patient argument (Arg1) of a typical  
?appearance? event. Also consider the following 
example from Hindi.  
It is possible that in a light verb construction, 
the light verb actually modifies the standard 
underlying semantics of a nominalization like 
appearance.  In any event, we cannot assume that 
the expected argument labels for the light verb 
and for the standard interpretation of the 
nominalization will always coincide. Thus, we 
could say that Pass 2?s true predicate annotation 
is only partial and is not representative of the 
complete argument structure. In particular, we 
are left with a very difficult merging problem, 
because the argument labels of the two separate 
passes conflict as seen in the above examples. 
5.3 Method 2 
In order to remedy the problem of conflicting 
argument labels, we revised Method 1?s Pass 2 
annotation scheme. This is shown in Table 3. 
Pass 1 remains unchanged from Method 1. 
In this method, both the light verb and the true 
predicate of the sentence receive complete sets of 
                                                          
1 This is why the rest of the argument/modifiers would be 
annotated using ARGM modifier labels. 
2  The light verb is in boldface, the true predicate is in bold 
and square brackets, and the argument/adjunct under 
consideration is underlined. 
85
argument and modifier labels. In Pass 2, the 
limitation of annotating within the domain of 
locality is removed. That is, the arguments and 
modifiers inside and outside the true predicate?s 
domain of control are annotated with respect to 
their semantic relationship to the true predicate 
(e.g., in the English example of Section 5.2, ?Wu 
Shu-Chen? would be considered ARG1 of 
?appearance?).  
Frame Files: This method would also require 
three Frame Files. The major difference is that 
with this method the Frame File for the true 
predicate includes arguments that are sisters to 
the light verb.  
5.4 Weaknesses of Method 2 
If in Method 1 we have committed the error of 
semantic unfaithfulness due to omission, in 
Method 2 we are faced with the problem of 
including too much. In the following sentence, 
consider the role of the underlined adjunct: 
A New York audience ? gave it a big round 
of applause when the music started to play. 
By the annotation in Method 2, the underlined 
temporal adjunct ?when the music started to 
play? is labeled as both the argument of ?give? 
and of ?applause?. The question here is does the 
argument apply to both the giving and the 
applauding event? In other words, does the 
adjunct play an equal role in both passes?  
 Since it could be easily said that the temporal 
phrase applies to both the applauding and the 
giving of the applause events, this example may 
not be particularly compelling. However, what if 
a syntactic complement of the light verb is a 
semantic argument of the true predicate and the 
true predicate only? This is seen more frequently 
in the cases where the light verb is less bleached 
than in the case of ?give? above. Consider the 
following Arabic example. 
 
 ????? ??]???????? ] ????? ????????? ??????? ??????  
took.we PREP DEF-consideration PREP 
prepertations.our possibility sustain.their losses 
?We took into [consideration] during our prepa-
rations the possibility of them sustaining losses? 
 
Here, even though the constituent ?of them 
sustaining losses? is the syntactic complement of 
the verb ?to take;? semantically, it modifies only 
the nominal object of the PP ?consideration.?  
There are similar phenomena in Chinese light 
verb constructions. Syntactic modifiers of the 
light verb are semantic arguments of the true 
predicate, which is usually a nominalization that 
serves as its complement.  
 
?? ?  ?    ? ? ??    [??]    ?? ? 
we now regarding this CL issue [conduct] discussion. 
lit.?We are conducting a discussion on this issue.? 
 ?We are discussing this issue.? 
 
The prepositional phrase ????? ?regarding 
this issue? is a sister to the light verb but 
semantically it is an argument of the nominalized 
predicate ?? ?discussion?. 
The logical next question would be: does the 
annotation of the arguments, adjuncts and 
modifiers have to be all or nothing? It could 
conceivably be possible to assign a selected set 
of arguments at the light verb or true predicate 
level. For example, in the Chinese sentence, the 
modifier ?regarding this CL issue?, though a 
syntactic adjunct to the light verb, could be left 
out from the semantic annotation in Pass 1 and 
included only in the Pass 2. 
However, the objection to this treatment 
comes from a more practical need. As mentioned 
above, in order to keep the manual annotation to 
a minimum, it would be necessary to keep Pass 3 
completely deterministic. As is, with the 
unmodified Method 2, there would be the need to 
choose between Pass 1 or Pass 2 annotation to 
when doing the automatic Pass 3. If we modify 
Method 2 by annotating only a selected set of 
syntactic arguments for the light verb or the true 
predicate, then this issue is exacerbated. In such 
a case there we would have to develop with strict 
rules for which arguments of which pass should 
be included in Pass 3. Pass 3 would no longer be 
automatic, and should be done manually.  
Pass 2: 
True predicate 
- Annotate the Subject argument of the light verb 
with the appropriate role of the true predicate 
- Annotate arguments and modifiers of the true 
predicate without limitation as to the domain of 
locality. 
True predicate?s Frame File 
?+H PDGH DQRWKHU DSSHDUDQFH DW WKH SDUW\? 
ARG1: He 
ARG-ADV: another 
REL: appearance 
ARG-DIR: at court 
Table 3. Method 2 for annotation for Pass 2. Pass 
1 as presented in Table 2 remains unchanged. 
Revised information for Pass 2 is in italics 
 
86
6 Final Annotation Scheme 
6.1 Semantic Fidelity 
Many of the objections so far to Methods 1 and 2 
have centered on the issue of semantic fidelity 
during the annotation of each of the two passes. 
The debate of whether both passes should be 
annotated and to what extent has practical 
implications for the third Pass, as described 
above. However, more importantly it comes 
down to whether or not the semantics of the final 
light verb plus true predicate combination is 
indeed distinct from the semantics of its parts 
(i.e. light verb and true predicate, separately). 
This may be a fascinating linguistic question, but 
it is not something our annotators can be 
debating for each and every instance.   
Instead, we argue that the semantic argument 
structure of the light verb plus true predicate 
combination can in practice be different from 
that of the expressions taken independently as 
has been proposed by various studies (Butt, 
2004; Rosen, 1997; Grimshaw & Mester, 1988). 
Thus, we resolve the cases in which the 
differences in argument roles as assigned by the 
light verb and the nominalization (Section 5.2) 
by handling the argument structure of the 
standard nominalization separately from that of 
the nominalization participating in the LVC. In 
the example ?Chen made another appearance in 
court?, we annotate ?Chen? as the Agent (ARG0) 
of the full predicate ?[make] [appearance]?, 
which is different from the argument structure of 
the standard nominalization which would label 
?Chen? to be the Patient argument (ARG1). 
6.2 Method 3: Final Method 
Our final method of light verb annotation reflects 
the notion that the noun, verb, or adjective as a 
true predicate within an LVC can have a 
different argument structure from that of the 
word alone. Table 4 shows the final annotation 
scheme for light verb construction.  
During Pass 1, the LVCs and their predicating 
expressions are identified in the data. Instances 
identified as LVCs in Pass 1 are then manually 
annotated during Pass 2, annotating the 
arguments and adjuncts of the light verb and the 
true predicate with roles that reflect their 
semantic relationships to the light verb plus true 
predicate. In practice, Pass 1 becomes a way of 
simply manually identifying the light verb 
usages. It is in Pass 2 that we make the final 
choice of argument labels for all of the 
arguments. Thus in Pass 3, the light verb and the 
true predicate lemmas from Pass 1 and 2 are 
joined into a single unit (e.g., in the example 
found in Table 4, the light verb ?took? would be 
joined with the true predicate ?walk? into 
?took+walk?) 3. In this final method, Pass 3 can 
be achieved completely deterministically. 
The major difference in this annotation 
scheme from that of Methods 1 and 2 is that 
instead of annotating in terms of the semantics of 
the bare noun, adjective or verb, the argument 
structure is determined for the entire predicate or 
the full event: semantics of the light verb plus the 
true predicate. This means that for the sentences 
where the argument roles of the verb and the 
nominalization disagree like ?Chen? in ?Chen 
                                                          
3 The order of Pass 2 and Pass 3 as presented in Table 4 is 
arguably a product of how the annotation tools for 
PropBank are set up for Arabic, Chinese, and English. That 
is, the order of the Pass 2 and Pass 3 could potentially be 
flipped provided that the tools and procedures of annotation 
support it, as is the case for Hindi PropBank. After the LVC 
and ARG-PRX are identified in Pass 1, the light verb and 
the true predicate can be deterministically joined into a 
single relation in Pass 2, leaving the manual annotation of 
LVC for Pass 3.  The advantage of this alternative ordering 
is that because the annotation of LVC is done around light 
verb plus the true predicate as a single relation, rather than 
the true predicate alone as in Table 4, the argument 
annotation may in actuality be more intuitive for annotators 
even with less training. 
 Pass 1: Pass 2:  Pass 3: 
 Light Verb Identification LVC Annotation Deterministic relation merge 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
& Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the LVCs are annotated 
- Arguments and modifiers 
are taken from Pass 2 
Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG0: John 
ARG-MNR: brisk  
REL: walk 
ARGM-DIR: through the park 
ARG0: John 
ARG-MNR: brisk  
REL: [took][walk] 
ARGM-DIR: through the park 
Table 4. Final Annotation Scheme 
87
made another4 appearance in court?, we label the 
argument with the role that is consistent with the 
entire predicate (i.e. Agent, ARG0).  
Frame Files: The final advantage to this 
method is that only one Frame File is needed. 
Since Pass 1 is an identification round, no Frame 
File is required. A single Frame File for LVC 
that includes the argument structure with respect 
to the light verb plus true predicate combination 
will suffice for Pass 2 and Pass 3. 
7 Distinguishing LVCs from MWEs 
As we have discussed in Section 2, we adapted 
our approach from Butt?s (2004) definition of 
LVCs. That is, an LVC is characterized by a 
semantically bleached light verb and a true 
predicate. These elements combine as a single 
predicating unit, in such a way that the light verb 
plus its true predicate can be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression (e.g. 
?lectured? for ?gave a lecture?). Also, as 
discussed in Section 6.1, our approach advocates 
the notion that the semantic argument structure 
of the light verb plus true predicate is different 
from that of the expressions taken independently 
(as also proposed by Butt, 2004; Rosen, 1997; 
Grimshaw & Mester, 1988 among others). 
While these definitions are appropriate for the 
PropBank annotation task as we have presented 
it, there are still cases that merit closer attention. 
Even English with a rather limited set of verbs 
that are commonly cited as LVCs, includes a 
problematic mixture of what could arguably be 
termed either LVCs or idiomatic expressions: 
?make exception?, ?take charge?. This difficulty 
in part is the effect of frequency and 
entrenchment of particular constructions.  The 
light verbs themselves do not diminish in form 
over time in a manner similar to auxiliaries (Butt, 
2004), although the complements of common 
LVCs can change over time such that it is no 
longer clear that the complement is a predicating 
element.   
In the case of English, the expressions ?take 
charge? may be more commonly found today as a 
LVC than independently in its verbal form.  As 
we discovered with our annotators, native 
English speakers are uncomfortable using the 
verb ?charge? (i.e. to burden with a 
                                                          
4 The adjective ?another? is annotated as the modifier of the 
full predicate ?[make][appearance]? as it can be interpreted 
to mean that the make appearance event happened a 
previous appearance has been made. 
responsibility) as an independent matrix verb. A 
similar phenomenon can be seen in Arabic, 
where the predicate ??? ???? lit. ?release name? 
exemplifies a prototypical LVC that means ?to 
name?. However, in our data we see cases in 
which the complement is missing, while the 
semantics of the LVC remains intact: 
 ???? ???? ?? ??????? ??????  
CONJ REL be released.he PREP-him/it  
DEF-sector DEF-public 
lit ?Or what is released to it ?the public sector?? 
?Or what is called/named ?the public sector.?? 
This raises the question of: when does a 
construction that may have once been an LVC 
become more properly defined as an idiomatic 
expression due to such entrenchment?  Idiomatic 
expressions can potentially be distinguished from 
LVCs through judgments of how fixed or 
syntactically variable a construction is, and on 
the basis of how semantically transparent or 
decomposable the construction is (Nunberg et. 
al., 1994). However, sometimes the dividing line 
is hard to draw.  
A similar problem arises in determining 
whether a construction is a case of an LVC or 
simply a usage with a distinct sense of the verb. 
Take, for example, the following Arabic 
sentence. 
 ?????? ????? 
   take.he DEF-food 
lit. ?(he) took food? 
?he ate? 
Here, the Arabic word ???? ?food? is the noun 
derivation of the root shared by the verb ???? ?to 
eat?, in such a way that the sentence could be 
rephrased as ???? ?(he) ate?. This example falls 
neatly into the LVC category. However, further 
examples suggest that the example is a case of a 
distinct sense of ?to take orally? where the 
restrictions on the object are that the theme must 
be something that can be taken by mouth: 
?????? ????? 
take.he DEF-medicine 
?he took medicine? 
?????? ????? 
take.he DEF-soup 
?he took soup? 
Finally, determining the appropriate criteria to 
distinguish between a truly semantically 
bleached verb and verbs that seem to be 
participating in complex predication but 
contribute more to the semantics of the 
construction is a challenge for all languages. For 
example, in English data, there are potential 
LVCs with verbs that are not often thought of as 
light verbs, such as ?produce an alteration? and 
88
?issue a complaint?.  Although most English 
speakers would agree that the verbs in these 
constructions do not contribute to the semantics 
of the construction (e.g. ?issue a complaint? can 
be paraphrased to ?to complain?), there are 
similar constructions such as ?register a 
complaint,? wherein the verb cannot be 
considered light. For the purposes of annotation, 
where it is necessary for annotators to understand 
clear criteria for distinguishing light verbs, such 
cases are highly problematic because there is no 
deterministic way to measure the extent to which 
the verbal element contributes to the semantics 
of the construction.  In turn, there is not a good 
way to distinguish some of these borderline 
verbs from their normal, heavy usages.  
Such problems can be resolved by establishing 
language-specific semantic or syntactic tests that 
can be used for taking care of the borderline 
cases of LVCs. However, there is one other 
plausible manner we have identified that could 
help in detecting such atypical LVCs. This can 
be done by focusing on the argument structures 
of predicating complements rather than focusing 
on the verbs themselves.  Grimshaw & Mester 
(1988) suggest that the formation of LVCs 
involves argument transfer from the predicating 
complement to the verb, which is semantically 
bleached and thematically incomplete and 
assigns no thematic roles itself.  Similarly, 
Stevenson et al (2004) suggest that the 
acceptability of a potential LVC depends on the 
semantic properties of the complement.  Thus, 
atypical LVCs, such as the English construction 
?issue a complaint,? can potentially be detected 
during the annotation of eventive nouns, planned 
for all PropBank languages.  
This process will make our treatment of LVCs 
more comprehensive. Used with our language-
specific semantic and syntactic criteria relating to 
both the verb and the predicating complement, it 
will help us to more effectively capture as many 
types of LVCs as possible, including those of the 
V+ADJ and V+V varieties. 
8 Usefulness of our Approach 
Two basic approaches have previously been 
taken to handle all types of MWEs, including 
LVCs in natural language processing 
applications. The first is to treat MWEs quite 
simply as fixed expressions or long strings of 
words with spaces in between; the second is to 
treat MWEs as purely compositional (Sag et al, 
2002). The words-with-spaces approach is 
adequate for handling fixed idiomatic 
expressions, but issues of lexical proliferation 
and flexibility quickly arise when this approach 
is applied to light verbs, which are syntactically 
flexible and can number in the tens of thousands 
for a given language (Stevenson et al, 2004; Sag 
et al, 2002).  Nonetheless, large-scale lexical 
resources such as FrameNet (Baker et al, 1998) 
and WordNet (Fellbaum, 1999) continue to 
expand with entries that are MWEs.   
The purely compositional approach is also 
problematic for light verbs because it is 
notoriously difficult to predict which light verbs 
can grammatically combine with other 
predicating elements; thus, this approach leads to 
problems of overgeneration (Sag et al, 2002).  In 
order to overcome this problem, Stevenson et al 
(2004) attempted to determine which 
nominalizations could form a valid complement 
to the English light verbs take, give and make, 
using Levin?s (1993) verb classes to group 
similar nominalizations.  This approach was 
rather successful for take and give, but 
inconclusive for the verb make.  
Our approach can help to develop a resource 
that is useful whether one takes a words-with-
spaces approach or a compositional approach. 
Specifically, for those implementing a words-
with-spaces approach, the resulting PropBank 
annotation can serve as a lexical resource listing 
for LVCs. For those interested in implementing a 
compositional approach the PropBank annotation 
can serve to assist in predicting likely 
combinations. Moreover, information in the 
PropBank Frame Files can be used to generalize 
across classes of nouns that can occur with a 
given light verb with the help of lexical resources 
such as WordNet (Fellbaum, 1998), FrameNet 
(Baker et. al., 1998), and VerbNet (Kipper-
Schuler, 2005) (in a manner similar to the 
approach of Stevenson et al (2004)). 
Acknowledgements 
We also gratefully acknowledge the support of the 
National Science Foundation Grant CISE-CRI 
0709167, Collaborative: A Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu, and a 
grant from the Defense Advanced Research Projects 
Agency (DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No HR0011-06-C-0022, 
subcontract from BBN, Inc.  
Any opinions, findings, and conclusions or 
recommendations expressed in this material are those 
of the authors and do not necessarily reflect the views 
of the National Science Foundation. 
89
Reference 
Alsina, A. 1997. Causatives in Bantu and Romance. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 203-246. 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of the 17th International Conference 
on Computational Linguistics (COLING/ACL-98), 
pages 86?90, Montreal. ACL. 
Butt, M. 2004.  The Light Verb Jungle. In G. Aygen, 
C. Bowern & C. Quinn eds.  Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in 
Linguistics, p. 1-50.   
Butt, M. 1997. Complex Predicates in Urdu. In A. 
Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 107-149. 
Fellbaum, Christine, ed.: 1998, WordNet: An 
Electronic Lexical Database, Cambridge, MA: 
MIT Press.  
Grimshaw, J., and A. Mester. 1988. Light verbs and 
?-marking. Linguistic Inquiry 19(2):205?232. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics 28:3, 245-288. 
Goldberg, Adele E. 2003.  ?Words by Default: 
Inheritance and the Persian Complex Predicate 
Construction.? In E. Francis and L. Michaelis 
(eds). Mismatch: Form-Function Incongruity and 
the Architecture of Grammar. CSLI Publications.  
84-112. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad 
coverage, comprehensive verb lexicon. Ph.D. 
thesis, University of Pennsylvania. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
Chicago: Chicago Univ. Press.  
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, and R. Grishman. 2004. The 
NomBank Project: An interim report. In 
Proceedings of the HLT-NAACL 2004 Workshop: 
Frontiers in Corpus Annotation, pages 24- 31, 
Boston, MA. pages 430?437, Barcelona, Spain. 
Mohanan, T. 1997. Multidimensionality of 
Representation: NV Complex Predicates in Hindi. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 431-471. 
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, Fei Xia, 
Hindi Syntax: Annotating Dependency, Lexical 
Predicate-Argument Structure, and Phrase 
Structure, In the Proceedings of the 7th 
International Conference on Natural Language 
Processing, ICON-2009, Hyderabad, India, Dec 
14-17, 2009 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1):71?106. 
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, Daniel Jurafsky. 
2004. Shallow Semantic Parsing using Support 
Vector Machines. University of Colorado 
Technical Report: TR-CSLR 2003-03. 
Rosen, C. 1997. Auxiliation and Serialization: On 
Discerning the Difference. In A. Alsina, J. 
Bresnan, and P. Sells eds. Complex Predicates. 
Stanford, California: CSLI Publications, p. 175-
202. 
Sag, I., Baldwin, T. Bond, F., Copestake, A., 
Flickinger, D. 2002.  Multiword expressions: A 
pain in the neck for NLP.  In Proceedings of teh 
Third International Conference on Intelligent Text 
processing and Computatinal Linguistics 
(CICLING 2002), p. 1-15, Mexico City, Mexico. 
ACL. 
Stevenson, S., Fazly, A., and North, R. (2004). 
Statistical measures of the semi-productivity of 
light verb constructions. In Proceedings of the 
ACL-04 Workshop on Multiword Expressions: 
Integrating Processing, p. 1?8. 
 
90
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271?280,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Generating English Determiners in Phrase-Based Translation with
Synthetic Translation Options
Yulia Tsvetkov Chris Dyer Lori Levin Archna Bhatia
Language Technologies Institute
Carnegie Mellon University
Pittspurgh, PA, 15213, USA
{ytsvetko, cdyer, lsl, archna}@cs.cmu.edu
Abstract
We propose a technique for improving
the quality of phrase-based translation
systems by creating synthetic translation
options?phrasal translations that are gen-
erated by auxiliary translation and post-
editing processes?to augment the de-
fault phrase inventory learned from par-
allel data. We apply our technique to
the problem of producing English deter-
miners when translating from Russian and
Czech, languages that lack definiteness
morphemes. Our approach augments the
English side of the phrase table using a
classifier to predict where English arti-
cles might plausibly be added or removed,
and then we decode as usual. Doing
so, we obtain significant improvements in
quality relative to a standard phrase-based
baseline and to a to post-editing complete
translations with the classifier.
1 Introduction
Phrase-based translation works as follows. A set
of candidate translations for an input sentence is
created by matching contiguous spans of the in-
put against an inventory of phrasal translations,
reordering them into a target-language appropri-
ate order, and choosing the best one according to a
discriminative model that combines features of the
phrases used, reordering patterns, and target lan-
guage model (Koehn et al, 2003). This relatively
simple approach to translation can be remarkably
effective, and, since its introduction, it has been
the basis for further innovations, including devel-
oping better models for distinguishing the good
translations from bad ones (Chiang, 2012; Gim-
pel and Smith, 2012; Cherry and Foster, 2012;
Eidelman et al, 2013), improving the identifica-
tion of phrase pairs in parallel data (DeNero et al,
2008; DeNero and Klein, 2010), and formal gen-
eralizations to gapped rules and rich nonterminal
types (Chiang, 2007; Galley et al, 2006). This
paper proposes a different mechanism for improv-
ing phrase-based translation: the use of synthetic
translation options to supplement the standard
phrasal inventory used in phrase-based translation
systems.
In the following, we argue that phrase tables ac-
quired in usual way will be expected to have gaps
in their coverage in certain language pairs and
that supplementing these with synthetic translation
options is a priori preferable to alternative tech-
niques, such as post processing, for generalizing
beyond the translation pairs observable in training
data (?2). As a case study, we consider the prob-
lem of producing English definite/indefinite arti-
cles (the, a, and an) when translating from Russian
and Czech, two languages that lack overt definite-
ness morphemes (?3). We develop a classifier that
predicts the presence and absence of English arti-
cles (?4). This classifier is used to generate syn-
thetic translation options that are used to augment
phrase tables used the usual way (?5). We eval-
uate their performance relative to post-processing
approach and to a baseline phrase-based system,
finding that synthetic translation options reliably
outperform the other approaches (?6). We then
discuss how our approach relates to previous work
(?7) and conclude by discussing further applica-
tions of our technique (?8).
2 Why Synthetic Translation Options?
Before turning to the problem of generating En-
glish articles, we give arguments for why syn-
thetic translation options are a useful extension of
271
standard phrase-based translation approaches, and
why this technique might be better than some al-
ternative proposals that been made for generaliz-
ing beyond translation examples directly observ-
able in the training data.
In language pairs that are typologically sim-
ilar (i.e., when both languages lexicalize the
same kinds of semantic and syntactic informa-
tion), words and phrases map relatively directly
from source to target languages, and the standard
approach to learning phrase pairs is quite effec-
tive.1 However, in language pairs in which in-
dividual source language words have many dif-
ferent possible translations (e.g., when the target
language word could have many different inflec-
tions or could be surrounded by different func-
tion words that have no direct correspondence in
the source language), we can expect the standard
phrasal inventory to be incomplete, except when
very large quantities of parallel data are available
or for very frequent words. There simply will not
be enough examples from which to learn the ideal
set of translation options. Therefore, since phrase
based translation can only generate input/output
word pairs that were directly observed in the train-
ing corpus, the decoder?s only hope for produc-
ing a good output is to find a fluent, meaning-
preserving translation using incomplete transla-
tion lexicons. Synthetic translation option genera-
tion seeks to fill these gaps using secondary gener-
ation processes that produce possible phrase trans-
lation alternatives that are not directly extractable
from the training data. We hypothesize that by
filling in gaps in the translation options, discrim-
inative translation models will be more effective
(leading to better translation quality).
The creation of synthetic translation options can
be understood as a kind of translation or post-
editing of phrasal units/translations. This raises
a question: if we have the ability to post-edit a
phrasal translation or retranslate a source phrase
so as to fill in gaps in the phrasal inventory, we
should be able to use the same technique to trans-
late the sentence; why not do this? While the ef-
fectiveness of this approach will ultimately be as-
sessed empirically, translation option generation is
appealing because the translation option synthe-
sizer need not produce only single-best guesses?
1When translating from a language with a richer lexical
inventory to a simpler one, approximate matching or backing
off to (e.g.) morphologically simpler forms likewise reliably
produces good translations.
??????
?
?????
saw +1SG +PST cat+ACC1SG+NOM
I
saw
saw a
saw the
cat
a
the cat
cat
saw the cat
saw a cat
I saw
I saw a
I saw the
Figure 1: Russian-English phrase-based transla-
tion example. Since Russian lacks a definiteness
morpheme the determiners a, the must be part of
a translation option containing ?????? or ?????
in order to be present in the right place in the En-
glish output. Translation options that are in dashed
boxes should exist but were not observed in the
training data. This work seeks to produce such
missing translation options synthetically.
if multiple possibilities appear to be equally good
(say, multiple inflections of a translated lemma),
then multiple translation options may be synthe-
sized. Ultimately, of course, the global translation
model must select one translation for every phrase
it uses, but the decoder will have access to global
information that it can use to pick better transla-
tion options.
3 Case Study: English Definite Articles
We now turn to a translation problem that we will
use to assess the value of synthetic translation op-
tions: generating English in/definite articles when
translating from Russian.
Definiteness is a semantic property of noun
phrases that expresses information such as iden-
tifiability, specificity, familiarity and unique-
ness (Lyons, 1999). In English, it is expressed
through the use of article determiners and non-
article determiners. Although languages may ex-
press definiteness through such morphemes, many
languages use alternative mechanisms. For exam-
ple they may use noncanonical word orders (Mo-
hanan, 1994)2 or different constructions such as
existentials, differential object marking (Aissen,
2003), and the ba (?) construction in Chinese
2See pp. 11?12 for an example in Hindi, a language with-
out articles.
272
(Chen, 2004). While these languages lack arti-
cles, they may use demonstratives and the quan-
tifier one to emphasize definiteness and indefinite-
ness, respectively.
Russian and Czech are examples of languages
that use non-lexical means to express definiteness.
As such, in Russian to English translation systems,
we expect that most Russian nouns should have at
least three translation options?the bare noun, the
noun preceded by the, and the noun preceded a/an.
Fig. 1 illustrates how the definiteness mismatch
between Russian and English can result in ?gaps?
in the phrasal inventory learned from a relatively
large parallel corpus. The Russian input should
translate (depending on context) as either I saw a
cat or I saw the cat; however, the phrase table we
learned is only able to generate the former.3
4 Predicting English Definite Articles
Although English articles express semantic con-
tent, their use is largely predictable in context,
both for native English speakers and for automated
systems (Knight and Chander, 1994).4 In this sec-
tion we describe a classifier that uses local contex-
tual features to predict whether an article belongs
in a particular position in a sequence of words, and
if so, whether it is definite or indefinite (the form
of the indefinite article is deterministic given the
pronunciation of the following word).
4.1 Model
The classifier takes an English word sequence w =
?w1, w2, . . . , w|w|?with missing articles and an in-
dex i and predicts whether no article, a definite ar-
ticle, or an indefinite article should appear before
wi. We parameterize the classifier as a multiclass
3The phrase table for this example was extracted from the
WMT 2013 shared task training data consisting of 1.2M sen-
tence pairs.
4An interesting contribution of this work is a discussion
on lower and upper bounds that can be achieved by native
English speakers in predicting determiners. 67% is a lower
bound, obtained by guessing the for every instance. The up-
per bound was obtained experimentally, and was measured on
noun phrases (NP) without context, in a context of 4 words
(2 before and 2 after NP), and given full context. Human
subjects achieved an accuracy of 94-96% given full context,
83-88% for NPs in a context of 4 words, and 79-80% for NPs
without context. Since in the current state-of-the-art building
an automated determiners prediction in a full context (repre-
senting meaning computationally) is not a feasible task, we
view 83-88% accuracy as our goal, and 88% as an upper
bound for our method.
logistic regression:
p(y | w, i) ? exp?
j
?jhj(y,w, i),
where hj(?) are feature functions, ?j are the corre-
sponding weights, and y ? {D,I,N} refer, respec-
tively, to the outputs: definite article, indefinite ar-
ticle, and no article.5
4.2 Features
The English article system is extremely com-
plex (as non-native English speakers will surely
know!): in addition to a general placement rule
that articles must precede a noun or its modifiers
in an NP, multiple other factors can also affect ar-
ticle selection, including countability of the head
noun, syntactic properties of an adjective modi-
fying a noun (superlative, ordinal), discourse fac-
tors, general knowledge, etc. In this section, we
define morphosyntactic features aimed at reflect-
ing basic grammatical rules, we define statistical,
semantic and shallow lexical features to capture
additional regular and idiosyncratic usages of def-
inite and indefinite articles in English. Below we
provide brief details of the features and their mo-
tivation.
Lexical. Because training data can be con-
structed inexpensively (from any unannotated En-
glish corpus), n-gram indicator features, such as
[[wi?1ywiwi+1 = with y lot of]], can be es-
timated reliably and capture construction-specific
article use.
Morphosyntactic. We used part-of-speech
(POS) tags produced by the Stanford POS tagger
(Toutanova and Manning, 2000) to capture gen-
eral article patterns. These are relevant features
in the prediction of articles as we observe certain
constraints regarding the use of articles in the
neighborhood of certain POS tags. For example,
we do not expect to predict an article following an
adjective (JJ).
Semantic. We extract further information indi-
cating whether a named entity, as identified by the
Stanford NE Recognizer (Finkel et al, 2005) be-
gins at wi. These features are relevant as there
5Realization of the classes D and N as lexical items is
straightforward. To convert I into a or an, we use the
CMU pronouncing dictionary (http://www.speech.
cs.cmu.edu/cgi-bin/cmudict) and select an if wi
starts with a phonetic vowel.
273
is, in general, a constraint on the co-occurrence
of articles with named entities which can help us
predict the use of articles in such constructions.
For example, proper nouns do not tend to co-
occur with articles in English. Although there are
some proper nouns that have an article included in
them, such as the Netherlands, the United States
of America, but these are fixed expressions and the
model is easily able to capture such cases with lex-
ical features.
Statistical. Statistical features capture probabil-
ity of co-occurrences of a sample with each of
the determiner classes, e.g., for wi?1ywi we
collect probabilities of wi?1Iwi, wi?1Dwi, and
wi?1Nwi.6
4.3 Training and evaluation
We employ the creg regression modeling frame-
work to train a ternary logistic regression classi-
fier.7 All features were computed for the target-
side of the Russian-English TED corpus (Cettolo
et al, 2012); from 117,527 sentences we removed
5K sentences used as tuning and test sets in the
MT system. We extract statistical features from
monolingual English corpora released for WMT-
11 (Callison-Burch et al, 2011).
In the training corpus there are 65,075 I in-
stances, 114,571 D instances, and 2,435,287 N in-
stances. To create a balanced training set we
randomly sample 65K instances from each set of
collected instances.8 This training set of feature
vectors has 142,604 features and 285,210 param-
eters. To minimize the number of free parame-
ters in our model we use `1 regularization. We
perform 10-fold cross validation experiments with
various feature combinations, evaluating the clas-
sifier accuracy for all classes and for each class
independently. The performance of the classifier
on individual classes and consolidated results for
all classes are listed in Table 1.
We observe that morphosyntactic and lexical
features are highly significant, reducing the er-
ror rate of statistical features by 25%. A combi-
6Although statistical features are de rigeur in NLP, they
are arguably justified for this problem on linguistic grounds
since human subjects use frequency-based in addition to their
grammatical knowledge. For example, we say He is at school
rather than He is at the school, but Americans say He is in
the hospital while UK English speakers might prefer He is in
hospital.
7https://github.com/redpony/creg
8Preliminary experiments indicated that the excess of N
labels resulted in poor performance.
Feature combination All I D N
Statistical 0.80 0.76 0.79 0.87
Lexical 0.82 0.79 0.80 0.87
Morphosyntactic 0.75 0.71 0.64 0.86
Semantic 0.35 0.99 0.02 0.04
Statistical+Lexical 0.85 0.83 0.82 0.89
+ Morphosyntactic 0.87 0.86 0.83 0.92
+ Semantic 0.87 0.86 0.83 0.92
Table 1: 10-fold cross validation accuracy of the
classifier over all and by class.
nation of morphosyntactic, lexical, and statistical
features is also helpful, reducing 13% more errors.
Semantic features do not contribute to the classi-
fier accuracy (we believe, mainly due to the feature
sparsity).
5 Experimental Setup
Our experimental workflow includes the follow-
ing steps. First, we select a phrase table PTsource
from which we generate synthetic phrases. For
each phrase pair ?f, e? in PTsource we generate n
synthetic variants of the target side phrase e which
we then append to PTbaseline. We annotate both
the original and synthetic phrases with additional
translation features in PTbaseline.
For this language pair, we have several options
for how to construct PTsource. The most straight-
forward way is to extract the phrasal inventory as
usual; a second option is to extract phrases from
training data from which definite articles have
been removed (since we will rely on the classifier
to reinsert them where they belong).
To synthesize phrases, we employ two differ-
ent techniques: LM-based and classifier-based.
We use a LM for one- or two-word phrases or
an auxiliary classifier for longer phrases and cre-
ate a new phrase in which we insert, remove or
substitute an article between each adjacent pair of
words in the original phrase. Such distinction be-
tween short and longer phrases has clear motiva-
tion: phrases without context may allow alterna-
tive, equally plausible options for article selection,
therefore we can just rely on a LM, trained on
large monolingual corpora, to identify phrases un-
observed in MT training corpus. Longer context
restricts determiners usage and statistical model
decisions are less prone to generating ungrammat-
ical synthetic phrases.
LM-based method is applied to phrases shorter
than three words. These phrases are numerous,
roughly 20% of a phrase table, and extracted from
274
many sites in the training data. For each short (tar-
get) phrase we add all possible alternative entries
observed in the LM and not observed in the orig-
inal translation model. For example, for a short
target phrase a cat we extract the cat.
We apply an auxiliary classifier to longer
phrases, containing three or more words. Based
on the classifier prediction, we use the maximally
probable class to insert, remove or substitute an
article between each adjacent pair of words in
the original phrase. Synthetic phrases are gener-
ated by linguistically-informed features and can
introduce alternative grammatically-correct trans-
lations of source phrases by adding or removing
existing articles (since the English article selection
in a local context is often ambiguous and not cat-
egorical). We add a synthetic phrase only if the
phrase pair not observed in the original model.
We compare two possible applications of a clas-
sifier: one-pass and iterative prediction. With
one-pass prediction we decide on the prediction
for each position independently of other deci-
sions. With iterative update we adopt the best
first (greedy) strategy, selecting in each iteration
the update-location in which the classifier obtains
highest confidence score. In each iteration we in-
corporate a prediction in a target phrase, and in the
next iteration the best first decision is made on an
updated phrase. Iterative prediction stops when no
updates are introduced.
Synthetic phrases are added to a phrase table
with the five standard phrasal translation features
that were found in the source phrase, and with sev-
eral new features. First, we add a boolean fea-
ture indicating the origin of a phrase: synthetic or
original. Second, we experiment with a posterior
probability of a classifier averaged over all loca-
tions where it could be extracted from the training
data. The next feature is derived from this score:
it is a boolean feature indicating a confidence of
the classifier: the feature value is 1 iff the average
classifier score is higher than some threshold.
Consider again a phrase I saw a cat discussed
in Section 1. Synthetic entry generation from the
original phrase table entry is illustrated in Fig-
ure 2.
6 Translation Results
We now review the results of experiments using
synthetic translation options in a machine trans-
lation system. We use the Moses toolkit (Koehn
et al, 2007) to train a baseline phrase-based SMT
system. Each configuration we compare has a dif-
ferent phrase table, with synthetic phrases gen-
erated with best-first or iterative strategies, from
a phrase table with- or without-determiners, with
variable number of translation features. To verify
that system improvement is consistent, and is not a
result of optimizer instability (Clark et al, 2011),
we replicate each experimental setup three times,
and then estimate the translation quality of the me-
dian MT system using the MultEval toolkit.9
The corpus is the same as in Section 4.3:
the training part contains 112,527 sentences from
Russian-English TED corpus, randomly sampled
3K sentences are used for tuning and a disjoint set
of 2K sentences is used for test. We lowercase
both sides, and use Stanford CoreNLP10 tools to
tokenize the corpora. We employ SRILM toolkit
(Stolcke, 2002) to linearly interpolate the target
side of the training corpus with the WMT En-
glish corpus, optimizing towards the MT tuning
set. This LM is used in all experiments.
The rest of this section is organized as follows.
First, we compare two approaches to the deter-
miners classifier application. Then, we provide
detailed description of experiments with synthetic
phrases. We evaluate various aspects of synthetic
phrases generation and summarize all the results
in Table 3. In Table 5 we show examples of im-
proved translations.
Classifier application: one-pass vs. iterative.
First, as an intrinsic evaluation of the prediction
strategy we remove definite and indefinite articles
from the reference translations (2K test sentences)
and then employ the determiners classifier to re-
produce the original sentences. In Table 2 we re-
port on the word error rate (WER) derived from
the Levenshtein distance between the original sen-
tences and the sentences (1) without articles, (2)
with articles recovered using one-pass prediction,
and (3) articles recovered using iterative predic-
tion. The WER is averaged over all test sentences.
Both one-pass and iterative approaches are effec-
tive in the task of determiners prediction, reducing
the number of errors by 44%. The iterative ap-
proach yields slightly lower WER, hence we em-
ploy the iterative prediction in the future experi-
ments with synthetic phrases.
9https://github.com/jhclark/multeval
10http://nlp.stanford.edu/software/corenlp.shtml
275
the
? ?????? ????? ||| i saw the cat ||| f0 f1 f2 f3 f4 exp(1) exp(0) |||
<s>      I      saw   a   cat </s>
None
None
? ?????? ????? ||| i saw a cat ||| f0 f1 f2 f3 f4 exp(0) exp(0) |||
original phrase
post-processing
synthetic phrase
is synthetic
is no-context
Figure 2: Synthetic entry generation example. The original parallel phrase has two additional boolean
features (set to false) indicating that this is not a synthetic phrase and not a short phrase. We apply
our determiners classifier to predict an article at each location marked with a dashed box. Based on a
classifier prediction we derive a new phrase I saw the cat. Since corresponding parallel entry is not in
the original phrase table, we set the synthetic indicator feature to 1.
Post-processing WER
None 5.6%
One-pass 3.2%
Iterative 3.1%
Table 2: WER (lower is better) of reference trans-
lations without articles and of post-processed ref-
erence translations. Both one-pass and iterative
approaches are effective in the task of determin-
ers prediction.
MT output post-processing. We then evaluate
the post-processing strategy directly on the MT
output. We experiment with one-pass and itera-
tive post-processing of two variants of the base-
line system outputs: original output and the out-
put without articles (we remove the articles prior
to post-processing). The results are listed in Ta-
ble 3. Interestingly, we do not obtain any improve-
ments applying the determiners classifier in a con-
ventional way of a MT output post-processing. It
is the combination of linguistically-motivated fea-
tures with synthetic phrases that contribute to the
best performance.
LM-based synthetic phrases. As discussed
above, LM-based (short) phrases are shorter than
3 tokens and their synthetic variants contain same
words with articles inserted or deleted between
each adjacent pair of words. The phrase table
of the baseline system contains 2,441,678 phrase
pairs. There are 518,453 original short phrases,
and our technique yields 842,252 new synthetic
entries which we append to the baseline phrase ta-
ble. Table 3 shows the evaluation of the median
SMT system (derived from three systems) with
short phrases. In these systems the five phrasal
translation features are the same as in the base-
line systems. Improvement in the BLEU score
(Papineni et al, 2002) is statistically significant
(p < .05), compared to the baseline system
Classifier-generated synthetic phrases We ap-
ply classifier with the iterative prediction directly
on the baseline phrase table entries and synthe-
size 944,145 new parallel phrases, increasing the
phrase table size by 38%. The phrasal transla-
tion features in each synthetic phrase are the same
as in the phrase it was derived from. The BLEU
score of the median SMT system with synthetic
phrases is 22.9 ? .1, the improvement is statisti-
cally significant (p < .01). Post-processing of a
phrase table created from corpora without articles
and adding synthetic phrases to the baseline phrase
table yielded similar results.
Translation features for synthetic phrases In
the following experiments we aim to establish the
optimal set of translation features that should be
used with synthetic phrases. We train several SMT
systems, each containing synthetic phrases derived
from the original phrase table by iterative classifi-
cation, and with LM-based short phrases. Each
synthetic phrase has five translation features as an
original phrase it was derived from. The additional
features that we evaluate are:
1. Boolean feature for LM-based synthetic
phrases
276
MT System BLEU
Baseline 22.6? .1
MT output post-processing
one-pass, MT output with articles 20.8
one-pass, MT output without articles 19.7
iterative, MT output with articles 22.6
iterative, MT output without articles 21.8
With synthetic phrases
LM-based phrases 22.9? .1
+ classifier-generated phrases 22.9? .1
+ features 1,2 23.0 ? .1
+ features 1,2,3 22.8? .1
+ features 1,2,3,4 22.8? .1
+ feature 5 22.9? .1
Table 3: Summary of experiments with MT out-
put post-processing and with synthetic translation
options in a phrase table. Post-processing of the
MT output do not improve translations. Best per-
forming system with synthetic phrases has five
original phrase translation features and two addi-
tional boolean features indicating if the phrase is
LM-based or not, is classifier-generated or not. All
the synthetic systems are significantly better than
the baseline system.
2. Boolean feature for classifier-generated syn-
thetic phrases
3. Classifier confidence: posterior probability of
the classifier averaged over all samples in a tar-
get phrase.
4. Boolean feature indicating a confidence of the
classifier: the feature value is 1 iff the Fea-
ture 3 scores higher than some threshold. The
threshold was set to 0.8, we did not experiment
with other values.
5. Boolean feature for a synthetic phrase of any
type: LM-based or classifier-generated
Table 3 details the change in the BLEU score
of each experimental setup. The best perform-
ing system has five original phrase translation fea-
tures and two additional boolean features indicat-
ing if the phrase is LM-based or not, is classifier-
generated or not. Note that all the synthetic sys-
tems are significantly better than the baseline.
Czech-English. Our technique was developed
using Russian-English system in the TED domain,
so we want to see how our method generalizes to a
different domain when translating from a different
language. We therefore applied our most success-
ful configuration to a Czech-English news transla-
tion task.11 For training, we use the WMT Czech-
English parallel corpus CzEng0.7; we tune using
the WMT2011 test set and test on the WMT2012
test set. The LM is trained on the target side of the
training corpus. Determiners classifier, re-trained
on the English side of this corpus, with statistical,
lexical, morphosyntactic and dependency features
obtained an accuracy of 88%.
In Table 4, we report the results of evaluat-
ing the performance of the Russian-to-English
and Czech-to-English MT systems with synthetic
phrases. The results of both systems show a statis-
tically significant (p < .01) improvement in terms
of BLEU score.
Russian Czech
Baseline 22.6? .1 16.0? .05
Synthetic 23.0? .1 16.2? .03
Table 4: BLEU score of Russian-to-English
and Czech-to-English MT systems with synthetic
phrases and features 1 and 2 show a significant im-
provement.
Qualitative analysis. Table 5 shows some ex-
amples from the output of our Russian-to-English
systems. Although both systems produce compre-
hensible translations, the system augmented with
determiner classifier is more fluent. The first ex-
ample represents a case where a singular count
noun (piece) is present which requires an article.
The baseline is not able to identify this require-
ment and hence does not insert the article an be-
fore the phrase extraordinary engineering piece.
Our system, however, correctly identifies the con-
struction requiring an article and thus provides an
appropriate form of the article (an- Indefinite arti-
cle for lexical items beginning with a vowel). Thus
we see that our system is able to capture the lin-
guistic requirement of the singular count nouns to
co-occur with an article. In the second row, the
lexical item poor is used as an adjective. The base-
line has inserted an article in front of it, chang-
ing it to a noun. Our system, however, is able to
maintain the status of poor as an adjective since
it has the option not to insert an article. Thus we
see that besides fluency, our system also does bet-
ter in maintaining the grammatical category of a
lexical item. In the third row, the phrase three
11Like Russian, Czech is a Slavic language that does not
have definite or indefinite articles.
277
Source: ?? ??? ?? ????? , ??? ?????????? ???????????? ??????????? ????????? .
Reference: but nonetheless , it ?s an extraordinary piece of engineering .
Baseline: but nevertheless , it ?s extraordinary engineering piece of art .
Ours: but nevertheless , it ?s an extraordinary piece of engineering art .
Source: ? ?? ?????? ?????????? ??? ??? ?? ?????? .
Reference: and by many definitions she is no longer poor .
Baseline: and in a lot definitions , it ?s not a poor .
Ours: and in a lot definitions she ?s not poor .
Source: ??? ????? ????????? ??? ????????? ????????? ??????? .
Reference: we must feed three billion people in cities .
Baseline: we need to feed the three billion urban hundreds of them .
Ours: we need to feed three billion people in the city .
Table 5: Examples of translations with improved articles handling.
billion people refers to a nonidentifiable referent.
The baseline inserts the definite article the. If a
human subject reads this translation, it would mis-
lead him/her to interpret the object three billion
people as referring to a specific identifiable set.
Our system, on the other hand, correctly selects
the determiner class N and hence does not insert an
article. Thus we see that our system does not just
add fluency but it also captures a semantic distinc-
tion, namely identifiability, that a human subject
makes when producing or interpreting a phrase.
7 Related Work
Automated determiner prediction has been found
beneficial in a variety of applications, including
postediting of MT output (Knight and Chander,
1994), text generation (Elhadad, 1993; Minnen
et al, 2000), and more recently identification and
correction of ESL errors (Han et al, 2006; De Fe-
lice and Pulman, 2008; Gamon et al, 2009; Ro-
zovskaya and Roth, 2010). Our work on determin-
ers extends previous studies in several dimensions.
While all previous approaches were tested only on
NP constructions, we evaluate our classifier on any
sequence of tokens.
To the best of our knowledge, the only stud-
ies that directly address generation of synthetic
phrase table entries was conducted by Chen et al
(2011) and Koehn and Hoang (2007). The former
find semantically similar source phrases and pro-
duce ?fabricated? translations by combining these
source phrases with a set of their target phrases;
however, they do not observe improvements. The
later work integrates the synthesis of translation
options into the decoder. While related in spirit,
their method only supports a limited set of gen-
erative processes for producing the candidate set
(lacking, for instance, the simple and effective
phrase post-editing process we have used), and
their implementation has been plagued by compu-
tational challenges.
Post-processing techniques have been ex-
tremely popular. These can be understood as using
a translation model to generate a translation skele-
ton (or k-best skeletons) and then post-editing
these in various ways. These have been applied
to translation into morphologically rich languages,
such as Japanese, German, Turkish, and Finnish
(de Gispert et al, 2005; Suzuki and Toutanova,
2006; Suzuki and Toutanova, 2007; Fraser et al,
2012; Clifton and Sarkar, 2011; Oflazer and Dur-
gar El-Kahlout, 2007).
8 Conclusions and future work
The contribution of this work is twofold. First, we
propose a new supervised method to predict defi-
nite and indefinite articles. Our log-linear model
trained on a linguistically-motivated set of fea-
tures outperforms previously reported results, and
obtains an upper bound of an accuracy achieved
by human subjects given a context of four words.
However, more important result of this work is the
experimentally verified idea of improving phrase-
based SMT via synthetic phrases. While we have
focused on a limited problem in this paper, there
are numerous alternative applications including
translation into morphologically rich languages, as
a method for incorporating (source) contextual in-
formation in making local translation decisions,
enriching the target language lexicon using lexical
translation resources, and many others.
Acknowledgments
We are grateful to Shuly Wintner for insightful sugges-
tions and support. This work was supported in part by the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-1-
0533.
278
References
J. Aissen. 2003. Differential object marking: Iconic-
ity vs. economy. Natural Language and Linguistic
Theory, 21(3):435?483.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
22?64, Edinburgh, Scotland, July. Association for
Computational Linguistics.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web inventory of transcribed and translated talks. In
Proceedings of the 16th Conference of the European
Association for Machine Translation (EAMT), pages
261?268, Trento, Italy, May.
B. Chen, R. Kuhn, and G. Foster. 2011. Semantic
smoothing and fabrication of phrase pairs for SMT.
In Proceedings of the International Workshop on
Spoken Lanuage Translation (IWSLT-2011).
P. Chen. 2004. Identifiability and definiteness in chi-
nese. Linguistics, 42(6):1129?1184.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proceedings of
HLT-NAACL 2012, volume 12, pages 34?35.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. The Jour-
nal of Machine Learning Research, 98888:1159?
1187.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In In Proc. of ACL.
A. Clifton and A. Sarkar. 2011. Combining
morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
ACL.
R. De Felice and S. G. Pulman. 2008. A classifier-
based approach to preposition and determiner error
correction in L2 English. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 169?176. Association
for Computational Linguistics.
A. de Gispert, J. B. Marin?o, and J. M. Crego. 2005.
Improving statistical machine translation by classi-
fying and generalizing inflected verb forms. In Pro-
ceedings of InterSpeech.
J. DeNero and D. Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1453?
1463. Association for Computational Linguistics.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008.
Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 314?323. Association for Com-
putational Linguistics.
V. Eidelman, Y. Marton, and P. Resnik. 2013. Online
relative margin maximization for statistical machine
translation. In Proceedings of ACL.
M. Elhadad. 1993. Generating argumentative judg-
ment determiners. In AAAI, pages 344?349.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 363?
370, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
A. Fraser, M. Weller, A. Cahill, and F. Cap. 2012.
Modeling inflection and word-formation in SMT. In
Proceedings of EACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic transla-
tion models. In ACL-44: Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 961?968,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2009. Us-
ing contextual speller techniques and language mod-
eling for ESL error correction. Urbana, 51:61801.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Pro-
ceedings of 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies HLT-
NAACL 2012, Montreal, Canada.
N.-R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers.
K. Knight and I. Chander. 1994. Automated poste-
diting of documents. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
779?779, Seattle, WA.
P. Koehn and H. Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
279
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54. Association for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
C. Lyons. 1999. Definiteness. Cambridge University
Press.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based learning for article generation. In
Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Compu-
tational natural language learning-Volume 7, pages
43?48. Association for Computational Linguistics.
T. Mohanan. 1994. Argument Structure in Hindi.
CSLI Publications.
K. Oflazer and I. Durgar El-Kahlout. 2007. Explor-
ing different representational units in English-to-
Turkish statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 25?32, Prague, Czech Republic,
June. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
A. Rozovskaya and D. Roth. 2010. Training
paradigms for correcting errors in grammar and us-
age. Urbana, 51:61801.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Procedings of International
Conference on Spoken Language Processing, pages
901?904.
H. Suzuki and K. Toutanova. 2006. Learning to pre-
dict case markers in Japanese. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 1049?
1056. Association for Computational Linguistics.
H. Suzuki and K. Toutanova. 2007. Generating case
markers in machine translation. In Proceedings of
HLT-NAACL 2007, pages 49?56.
K. Toutanova and C. D. Manning. 2000. Enriching
the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the 2000
Joint SIGDAT conference on Empirical methods in
natural language processing and very large corpora,
pages 63?70, Morristown, NJ, USA. Association for
Computational Linguistics.
280
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142?149,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2014
Austin Matthews Waleed Ammar Archna Bhatia Weston Feely
Greg Hanneman Eva Schlinger Swabha Swayamdipta Yulia Tsvetkov
Alon Lavie Chris Dyer
?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?
Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submitted
to the 2014 WMT shared translation task.
We participated in two language pairs,
German?English and Hindi?English. Our
innovations include: a label coarsening
scheme for syntactic tree-to-tree transla-
tion, a host of new discriminative features,
several modules to create ?synthetic trans-
lation options? that can generalize beyond
what is directly observed in the training
data, and a method of combining the out-
put of multiple word aligners to uncover
extra phrase pairs and grammar rules.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute partici-
pated in two language pairs for the 2014 Workshop
on Machine Translation shared translation task:
German?English and Hindi?English. Our systems
showcase our multi-phase approach to translation,
in which synthetic translation options supple-
ment the default translation rule inventory that is
extracted from word-aligned training data.
In the German?English system, we used our
compound splitter (Dyer, 2009) to reduce data
sparsity, and we allowed the translator to back
off to translating lemmas when it detected case-
inflected OOVs. We also demonstrate our group?s
syntactic system with coarsened nonterminal types
(Hanneman and Lavie, 2011) as a contrastive
German?English submission.
In both the German?English and Hindi?English
systems, we used an array of supplemental ideas to
enhance translation quality, ranging from lemma-
tization and synthesis of inflected phrase pairs to
novel reordering and rule preference features.
2 Core System Components
The decoder infrastructure we used was cdec
(Dyer et al., 2010). For our primary systems,
all data was tokenized using cdec?s tokenization
tool. Only the constrained data resources pro-
vided for the shared task were used for training
both the translation and language models. Word
alignments were generated using both FastAlign
(Dyer et al., 2013) and GIZA++ (Och and Ney,
2003). All our language models were estimated
using KenLM (Heafield, 2011). Translation model
parameters were chosen using MIRA (Eidelman,
2012) to optimize BLEU (Papineni et al., 2002)
on a held-out development set.
Our data was filtered using qe-clean
(Denkowski et al., 2012), with a cutoff of
two standard deviations from the mean. All
data was left in fully cased form, save the first
letter of each segment, which was changed to
whichever form the first token more commonly
used throughout the data. As such, words like The
were lowercased at the beginning of segments,
while words like Obama remained capitalized.
Our primary German?English and Hindi?
English systems were Hiero-based (Chiang,
2007), while our contrastive German?English sys-
tem used cdec?s tree-to-tree SCFG formalism.
Before submitting, we ran cdec?s implementa-
tion of MBR on 500-best lists from each of our
systems. For both language pairs, we used the
Nelder?Mead method to optimize the MBR pa-
rameters. In the German?English system, we ran
MBR on 500 hypotheses, combining the output of
the Hiero and tree-to-tree systems.
The remainder of the paper will focus on our
primary innovations in the two language pairs.
142
3 Common System Improvements
A number of our techniques were used for both our
German?English and Hindi?English primary sub-
missions. These techniques each fall into one of
three categories: those that create translation rules,
those involving language models, or those that add
translation features. A comparison of these tech-
niques and their performance across the two lan-
guage pairs can be found in Section 6.
3.1 Rule-Centric Enhancements
While many of our methods of enhancing the
translation model with extra rules are language-
specific, three were shared between language
pairs.
First, we added sentence-boundary tokens <s>
and </s> to the beginning and end of each line in
the data, on both the source and target sides.
Second, we aligned all of our training data us-
ing both FastAlign and GIZA++ and simply con-
catenated two copies of the training corpus, one
aligned with each aligner, and extracted rules from
the resulting double corpus.
Third, we hand-wrote a list of rules that trans-
form numbers, dates, times, and currencies into
well-formed English equivalents, handling differ-
ences such as the month and day reversal in dates
or conversion from 24-hour time to 12-hour time.
3.2 Employed Language Models
Each of our primary systems uses a total of three
language models.
The first is a traditional 4-gram model gen-
erated by interoplating LMs built from each of
the available monolingual corpora. Interpolation
weights were calculated used the SRILM toolkit
(Stolcke, 2002) and 1000 dev sentences from the
Hindi?English system.
The second is a model trained on word clus-
ters instead of surface forms. For this we mapped
the LM vocabulary into 600 clusters based on the
algorithm of Brown et al. (1992) and then con-
structed a 7-gram LM over the resulting clusters,
allowing us to capture more context than our tra-
ditional surface-form language model.
The third is a bigram model over the source side
of each language?s respective bitext. However, at
run time this LM operates on the target-side out-
put of the translator, just like the other two. The
intuition here is that if a source-side LM likes our
output, then we are probably passing through more
than we ought to.
Both source and target surface-form LM used
modified Kneser-Ney smoothing (Kneser and Ney,
1995), while the model over Brown clusters
(Brown et al., 1992) used subtract-0.5 smoothing.
3.3 New Translation Features
In addition to the standard array of features, we
added four new indicator feature templates, lead-
ing to a total of nearly 150,000 total features.
The first set consists of target-side n-gram fea-
tures. For each bigram of Brown clusters in the
output string generated by our translator, we fire
an indicator feature. For example, if we have the
sentence, Nato will ihren Einfluss im Osten st?arken
translating as NATO intends to strengthen its influ-
ence in the East, we will fire an indicator features
NGF C367 C128=1, NGF C128 C31=1, etc.
The second set is source-language n-gram fea-
tures. Similar to the previous feature set, we fire
an indicator feature for each ngram of Brown clus-
ters in the output. Here, however, we use n = 1,
and we use the map of source language words to
Brown clusters, rather than the target language?s,
despite the fact that this is examining target lan-
guage output. The intuition here is to allow this
feature to penalize passthroughs differently de-
pending on their source language Brown cluster.
For example, passing through the German word
zeitung (?newspaper?) is probably a bad idea, but
passing through the German word Obama proba-
bly should not be punished as severely.
The third type of feature is source path features.
We can imagine translation as a two-step process
in which we first permute the source words into
some order, then translate them phrase by phrase.
This set of features examines that intermediate
string in which the source words have been per-
muted. Again, we fire an indicator feature for each
bigram in this intermediate string, this time using
surface lexical forms directly instead of first map-
ping them to Brown clusters.
Lastly, we create a new type of rule shape fea-
ture. Traditionally, rule shape features have indi-
cated, for each rule, the sequence of terminal and
non-terminal items on the right-hand side. For ex-
ample, the rule [X] ? der [X] :: the [X] might
have an indicator feature Shape TN TN, where
T represents a terminal and N represents a non-
terminal. One can also imagine lexicalizing such
rules by replacing each T with its surface form.
We believe such features would be too sparse, so
instead of replacing each terminal by its surface
form, we instead replace it with its Brown cluster,
143
creating a feature like Shape C37 N C271 N.
4 Hindi?English Specific Improvements
In addition to the enhancements common to the
two primary systems, our Hindi?English system
includes improved data cleaning of development
data, a sophisticated linguistically-informed tok-
enization scheme, a transliteration module, a syn-
thetic phrase generator that improves handling of
function words, and a synthetic phrase generator
that leverages source-side paraphrases. We will
discuss each of these five in turn.
4.1 Development Data Cleaning
Due to a scarcity of clean development data, we
augmented the 520 segments provided with 480
segments randomly drawn from the training data
to form our development set, and drew another
random 1000 segments to serve as a dev test set.
After observing large discrepencies between the
types of segments in our development data and the
well-formed news domain sentences we expected
to be tested on, we made the decision to prune our
tuning set by removing any segment that did not
appear to be a full sentence on both the Hindi and
English sides. While this reduced our tuning set
from 1000 segments back down to 572 segments,
we believe it to be the single largest contributor to
our success on the Hindi?English translation task.
4.2 Nominal Normalization
Another facet of our system was normalization of
Hindi nominals. The Hindi nominal system shows
much more morphological variation than English.
There are two genders (masculine and feminine)
and at least six noun stem endings in pronuncia-
tion and 10 in writing.
The pronominal system also is much richer than
English with many variants depending on whether
pronouns appear with case markers or other post-
positions.
Before normalizing the nouns and pronouns, we
first split these case markers / postpositions from
the nouns / pronouns to result in two words in-
stead of the original combined form. If the case
marker was n (ne), the ergative case marker in
Hindi, we deleted it as it did not have any trans-
lation in English. All the other postpositions were
left intact while splitting from and normalizing the
nouns and pronouns.
These changes in stem forms contribute to the
sparsity in data; hence, to reduce this sparsity, we
construct for each input segment an input lattice
that allows the decoder to use the split or original
forms of all nouns or pronouns, as well as allowing
it to keep or delete the case marker ne.
4.3 Transliteration
We used the 12,000 Hindi?English transliteration
pairs from the ACL 2012 NEWS workshop on
transliteration to train a linear-chained CRF tag-
ger
1
that labels each character in the Hindi token
with a sequence of zero or more English characters
(Ammar et al., 2012). At decoding, unseen Hindi
tokens are fed to the transliterator, which produces
the 100 most probable transliterations. We add
a synthetic translation option for each candidate
transliteration.
In addition to this sophisticated transliteration
scheme, we also employ a rule-based translitera-
tor that specifically targets acronyms. In Hindi,
many acronyms are spelled out phonetically, such
as NSA being rendered as enese (en.es.e). We
detected such words in the input segments and
generated synthetic translation options both with
and without periods (e.g. N.S.A. and NSA).
4.4 Synthetic Handling of Function Words
In different language pairs, individual source
words may have many different possible trans-
lations, e.g., when the target language word has
many different morphological inflections or is sur-
rounded by different function words that have no
direct counterpart in the source language. There-
fore, when very large quantities of parallel data
are not available, we can expect our phrasal inven-
tory to be incomplete. Synthetic translation option
generation seeks to fill these gaps using secondary
generation processes that exploit existing phrase
pairs to produce plausible phrase translation alter-
natives that are not directly extractable from the
training data (Tsvetkov et al., 2013; Chahuneau et
al., 2013).
To generate synthetic phrases, we first remove
function words from the source and target sides
of existing non-gappy phrase pairs. We manually
constructed English and Hindi lists of common
function words, including articles, auxiliaries, pro-
nouns, and adpositions. We then employ the
SRILM hidden-ngram utility (Stolcke, 2002) to re-
store missing function words according to an n-
gram language model probability, and add the re-
sulting synthetic phrases to our phrase table.
1
https://github.com/wammar/transliterator
144
4.5 Paraphrase-Based Synthetic Phrases
We used a graph-based method to obtain transla-
tion distributions for source phrases that are not
present in the phrase table extracted from the par-
allel corpus. Monolingual data is used to construct
separate similarity graphs over phrases (word se-
quences or n-grams), using distributional features
extracted from the corpora. The source similar-
ity graph consists of phrase nodes representing se-
quences of words in the source language. In our
instance, we restricted the phrases to bigrams, and
the bigrams come from both the phrase table (the
labeled phrases) and from the evaluation set but
not present in the phrase table (unlabeled phrases).
The labels for these source phrases, namely the
target phrasal inventory, can also be represented
in a graph form, where the distributional features
can also be computed from the target monolingual
data. Translation information is then propagated
from the labeled phrases to the unlabeled phrases
in the source graph, proportional to how similar
the phrases are to each other on the source side,
as well as how similar the translation candidates
are to each other on the target side. The newly
acquired translation distributions for the unlabeled
phrases are written out to a secondary phrase table.
For more information, see Saluja et al. (2014).
5 German?English Specific
Improvements
Our German?English system also had its own
suite of tricks, including the use of ?pseudo-
references? and special handling of morphologi-
cally inflected OOVs.
5.1 Pseudo-References
The development sets provided have only a sin-
gle reference, which is known to be sub-optimal
for tuning of discriminative models. As such,
we use the output of one or more of last year?s
top performing systems as pseudo-references dur-
ing tuning. We experimented with using just one
pseudo-reference, taken from last year?s Spanish?
English winner (Durrani et al., 2013), and with
using four pseudo-references, including the out-
put of last year?s winning Czech?English, French?
English, and Russian?English systems (Pino et al.,
2013).
5.2 Morphological OOVs
Examination of the output of our baseline sys-
tems lead us to conclude that the majority of our
system?s OOVs were due to morphologically in-
flected nouns in the input data, particularly those
in genitive case. As such, for each OOV in the
input, we attempt to remove the German genitive
case marker -s or -es. We then run the resulting
form f through our baseline translator to obtain a
translation e of the lemma. Finally, we add two
translation rules to our translation table: f ? e,
and f ? e?s.
6 Results
As we added each feature to our systems, we
first ran a one-off experiment comparing our base-
line system with and without each individual fea-
ture. The results of that set of experiments are
shown in Table 1 for Hindi?English and Table 2
for German?English. Features marked with a *
were not included in our final system submission.
The most surprising result is the strength of
our Hindi?English baseline system. With no extra
bells or whistles, it is already half a BLEU point
ahead of the second best system submitted to this
shared task. We believe this is due to our filter-
ing of the tuning set, which allowed our system to
generate translations more similar in length to the
final test set.
Another interesting result is that only one fea-
ture set, namely our rule shape features based on
Brown clusters, helped on the test set in both lan-
guage pairs. No feature hurt the BLEU score on
the test set in both language pairs, meaning the
majority of features helped in one language and
hurt in the other.
If we compare results on the tuning sets, how-
ever, some clearer patterns arise. Brown cluster
language models, n-gram features, and our new
rule shape features all helped.
Furthermore, there were a few features, such as
the Brown cluster language model and tuning to
Meteor (Denkowski and Lavie, 2011), that helped
substantially in one language pair while just barely
hurting the other. In particular, the fact that tuning
to Meteor instead of BLEU can actually help both
BLEU and Meteor scores was rather unexpected.
7 German?English Syntax System
In addition to our primary German?English sys-
tem, we also submitted a contrastive German?
English system showcasing our group?s tree-to-
tree syntax-based translation formalism.
145
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 15.7 25.3 68.0 11.4 22.9 70.3
*Meteor Tuning 15.2 25.8 71.3 12.8 23.7 71.3
Sentence Boundaries 15.2 25.4 69.1 12.1 23.4 70.0
Double Aligners 16.1 25.5 66.6 11.9 23.1 69.2
Manual Number Rules 15.7 25.4 68.5 11.6 23.0 70.3
Brown Cluster LM 15.6 25.1 67.3 11.5 22.7 69.8
*Source LM 14.2 25.1 72.1 11.3 23.0 72.3
N-Gram Features 15.6 25.2 67.9 12.2 23.2 69.2
Src N-Gram Features 15.3 25.2 68.9 12.0 23.4 69.5
Src Path Features 15.8 25.6 68.8 11.9 23.3 70.4
Brown Rule Shape 15.9 25.4 67.2 11.8 22.9 69.6
Lattice Input 15.2 25.8 71.3 11.4 22.9 70.3
CRF Transliterator 15.7 25.7 69.4 12.1 23.5 70.1
Acronym Translit. 15.8 25.8 68.8 12.4 23.4 70.2
Synth. Func. Words 15.7 25.3 67.8 11.4 22.8 70.4
Source Paraphrases 15.6 25.2 67.7 11.5 22.7 69.9
Final Submission 16.7
Table 1: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero Hindi?
English system. Each line is the baseline plus that one feature, non-cumulatively. Lines marked with a *
were not included in our final WMT submission.
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 25.3 30.4 52.6 26.2 31.3 53.6
*Meteor Tuning 26.2 31.3 53.1 26.9 32.2 54.4
Sentence Boundaries 25.4 30.5 52.2 26.1 31.4 53.3
Double Aligners 25.2 30.4 52.5 26.0 31.3 53.6
Manual Number Rules 25.3 30.3 52.5 26.1 31.4 53.4
Brown Cluster LM 26.4 31.0 51.9 27.0 31.8 53.2
*Source LM 25.8 30.6 52.4 26.4 31.5 53.4
N-Gram Features 25.4 30.4 52.6 26.7 31.6 53.0
Src N-Gram Features 25.3 30.5 52.5 26.2 31.5 53.4
Src Path Features 25.0 30.1 52.6 26.0 31.2 53.3
Brown Rule Shape 25.5 30.5 52.4 26.3 31.5 53.2
One Pseudo Ref 25.5 30.4 52.6 34.4 32.7 49.3
*Four Psuedo Refs 22.6 29.2 52.6 49.8 35.0 46.1
OOV Morphology 25.5 30.5 52.4 26.3 31.5 53.3
Final Submission 27.1
Table 2: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero
German?English system. Each line is the baseline plus that one feature, non-cumulatively.
Dev (2013) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 20.98 29.81 58.47 18.65 28.72 61.80
+ Label coarsening 23.07 30.71 56.46 20.43 29.34 60.16
+ Meteor tuning 23.48 30.90 56.18 20.96 29.60 59.87
+ Brown LM + Lattice + Synthetic 24.46 31.41 56.66 21.50 30.28 60.51
+ Span limit 15 24.20 31.25 55.48 21.75 29.97 59.18
+ Pseudo-references 24.55 31.30 56.22 22.10 30.12 59.73
Table 3: BLEU, Meteor, and TER results for experiments conducted in the tree-to-tree German?English
system. The system in the bottom line was submitted to WMT as a contrastive entry.
7.1 Basic System Construction
Since all training data for the tree-to-tree system
must be parsed in addition to being word-aligned,
we prepared separate copies of the training, tun-
ing, and testing data that are more suitable for in-
put into constituency parsing. Importantly, we left
the data in its original mixed-case format. We used
the Stanford tokenizer to replicate Penn Treebank
tokenization on the English side. On the German
side, we developed new in-house normalization
and tokenization script.
We filtered tokenized training sentences by sen-
146
tence length, token length, and sentence length ra-
tio. The final corpus for parsing and word align-
ment contained 3,897,805 lines, or approximately
86 percent of the total training resources released
under the WMT constrained track. Word align-
ment was carried out using FastAlign (Dyer et
al., 2013), while for parsing we used the Berke-
ley parser (Petrov et al., 2006).
Given the parsed and aligned corpus, we ex-
tracted synchronous context-free grammar rules
using the method of Hanneman et al. (2011).
In addition to aligning subtrees that natively ex-
ist in the input trees, our grammar extractor also
introduces ?virtual nodes.? These are new and
possibly overlapping constituents that subdivide
regions of flat structure by combining two adja-
cent sibling nodes into a single nonterminal for
the purposes of rule extraction. Virtual nodes
are similar in spirit to the ?A+B? extended cate-
gories of SAMT (Zollmann and Venugopal, 2006),
and their nonterminal labels are constructed in the
same way, but with the added restriction that they
do not violate any existing syntactic structure in
the parse tree.
7.2 Improvements
Nonterminals in our tree-to-tree grammar are
made up of pairs of symbols: one from the source
side and one from the target side. With virtual
nodes included, this led to an initial German?
English grammar containing 153,219 distinct non-
terminals ? a far larger set than is used in SAMT,
tree-to-string, string-to-tree, or Hiero systems. To
combat the sparsity introduce by this large nonter-
minal set, we coarsened the label set with an ag-
glomerative label-clustering technique(Hanneman
and Lavie, 2011; Hanneman and Lavie, 2013).
The stopping point was somewhat arbitrarily cho-
sen to be a grammar of 916 labels.
Table 3 shows a significant improvement in
translation quality due to coarsening the label set:
approximately +1.8 BLEU, +0.6 Meteor, and ?1.6
TER on our dev test set, newtest2012.
2
In the MERT runs, however, we noticed that the
length of the MT output can be highly variable,
ranging on the tuning set from a low of 92.8% of
the reference length to a high of 99.1% in another.
We were able to limit this instability by tuning to
Meteor instead of BLEU. Aside from a modest
2
We follow the advice of Clark et al. (2011) and eval-
uate our tree-to-tree experiments over multiple independent
MERT runs. All scores in Table 3 are averages of two or
three runs, depending on the row.
score improvement, we note that the variability in
length ratio is reduced from 6.3% to 2.8%.
Specific difficulties of the German?English lan-
guage pair led to three additional system compo-
nents to try to combat them.
First, we introduced a second language model
trained on Brown clusters instead of surface forms.
Next we attempted to overcome the sparsity
of German input by making use of cdec?s lattice
input functionality introduce compound-split ver-
sions of dev and test sentences.
Finally, we attempted to improve our grammar?s
coverage of new German words by introducing
synthetic rules for otherwise out-of-vocabulary
items. Each token in a test sentence that the gram-
mar cannot translate generates a synthetic rule al-
lowing the token to be translated as itself. The left-
hand-side label is decided heuristically: a (coars-
ened) ?noun? label if the German OOV starts with
a capital letter, a ?number? label if the OOV con-
tains only digits and select punctuation characters,
an ?adjective? label if the OOV otherwise starts
with a lowercase letter or a number, or a ?symbol?
label for anything left over.
The effect of all three of these improvements
combined is shown in the fourth row of Table 3.
By default our previous experiments were per-
formed with a span limit of 12 tokens. Increasing
this limit to 15 has a mixed effect on metric scores,
as shown in the fifth row of Table 3. Since two out
of three metrics report improvement, we left the
longer span limit in effect in our final system.
Our final improvement was to augment our tun-
ing set with the same set of pseudo-references
as our Hiero systems. We found that using one
pseudo-reference versus four pseudo-references
had negligible effect on the (single-reference) tun-
ing scores, but four produced a better improve-
ment on the test set.
The best MERT run of this final system (bottom
line of Table 3) was submitted to the WMT 2014
evaluation as a contrastive entry.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
147
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine transla-
tion systems for european language pairs.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 406?414. Association for Computational Lin-
guistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-
pert, Federico Flego, and William Byrne. 2013.
The university of cambridge russian-english system
at wmt13.
148
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Baltimore, Maryland, June.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
149
