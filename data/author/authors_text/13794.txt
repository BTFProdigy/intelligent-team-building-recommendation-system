R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 827 ? 837, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Principles of Non-stationary Hidden Markov Model  
and Its Applications to Sequence Labeling Task 
Xiao JingHui, Liu BingQuan, and Wang XiaoLong 
School of Computer Science and Techniques,  
Harbin Institute of Technology, Harbin, 150001, China 
{xiaojinghui, liubq, wangxl}@insun.hit.edu.cn 
Abstract. Hidden Markov Model (Hmm) is one of the most popular language 
models. To improve its predictive power, one of Hmm hypotheses, named 
limited history hypothesis, is usually relaxed. Then Higher-order Hmm is built 
up. But there are several severe problems hampering the applications of high-
order Hmm, such as the problem of parameter space explosion, data sparseness 
problem and system resource exhaustion problem. From another point of view, 
this paper relaxes the other Hmm hypothesis, named stationary (time invariant) 
hypothesis, makes use of time information and proposes a non-stationary Hmm 
(NSHmm). This paper describes NSHmm in detail, including its definition, the 
representation of time information, the algorithms and the parameter space and 
so on. Moreover, to further reduce the parameter space for mobile applications, 
this paper proposes a variant form of NSHmm (VNSHmm). Then NSHmm and 
VNSHmm are applied to two sequence labeling tasks: pos tagging and pinyin-to-
character conversion. Experiment results show that compared with Hmm, 
NSHmm and VNSHmm can greatly reduce the error rate in both of the two 
tasks, which proves that they have much more predictive power than Hmm does. 
1   Introduction 
Statistical language model plays an important role in natural language processing and 
great efforts are devoted to the research of language modeling. Hidden Markov Model 
(Hmm) is one of the most popular language models. It was first proposed by IBM in 
speech recognition [1] and achieved great success. Then Hmm has a wide range of 
applications in many domains, such as OCR [2], handwriting recognition [3], machine 
translation [4], Chinese pinyin-to-character conversion [5] and so on. 
To improve Hmm?s predictive power, one of Hmm hypotheses [6] named limited 
history hypothesis, is usually relaxed and higher-order Hmm is proposed. But as the 
order of Hmm increases, its parameter space explodes at an exponential rate, which 
may result in several severe problems, such as data sparseness problem [7], system 
resource exhaustion problem and so on. From another point of view, this paper 
relaxes the other Hmm hypothesis, named stationary hypothesis, makes use of time 
information and proposes non-stationary Hmm (NSHmm). This paper first defines 
NSHmm in a formalized form, and then discusses how to represent time information 
in NSHmm. After that, the algorithms of NSHmm are provided and the parameter 
space complexity is calculated. Moreover, to further reduce the parameter space, a 
828 J. Xiao, B. Liu, and X. Wang 
variant form of NSHmm (VNSHmm) is proposed later. At last, NSHmm and 
VNSHmm are applied to two sequence labeling tasks: pos tagging and pinyin-to-
character conversion. As the experiment results show, compared with Hmm, NSHmm 
and VNSHmm can greatly reduce the error rate in the both two tasks.  
The rest of this paper is structured as follows: in section 2 we briefly review the 
definition of standard Hmm. In section 3, NSHmm is proposed and the relative 
questions are discussed in detail. Experiments and results are discussed in section 4. 
Finally, we give our conclusions in section 5 and plan the further work in section 6. 
2   Hidden Markov Model 
Hmm is a function of Markov process and can be mathematically defined as a five-
tuple M = <?, ?, ?, ?, ?> which consists of: 
1. A finite set of (hidden) states ?. 
2. A finite set of (observed) symbols ?. 
3. A state transition function ?: ?? ?-> [0, 1]. 
4. A symbol emission function ?: ?? ?-> [0, 1]. 
5. And an initial state probability function ?: Omega -> [0, 1]. 
The functions of ?, ? and ? are usually estimated by MLE principle on large scale 
corpus. Based on the above definition, Hmm makes two hypotheses at the same time: 
1. Limited history hypothesis: the current state is completely decided by the last 
state before, but irrelative to the entire state history. 
2. Stationary hypothesis: the state transition function ? is completely determined 
by states, but irrelative to the time when state transition occurs. So it is with the 
symbol emission function. 
There are three fundamental questions and a series of corresponding algorithms for 
Hmm: 
1. Given Hmm, how to calculate the probability of a sequence observation? 
Forward algorithm and backward algorithm can handle that question. 
2. Given Hmm and an observation sequence, how to find the best state sequence to 
explain the observation? Viterbi algorithm can fulfill that task. 
3. Given an observation sequence, how to estimate the parameters of Hmm to best 
explain the observed data? Baum-Welch algorithm can solve that problem. 
Hmm is a popular language model and has been applied to many tasks in natural 
language processing. For example, in pos tagging, the word sequence is taken as the 
observation of Hmm, and the pos sequence as the hidden state chain. Viterbi 
algorithm can find the best pos sequence corresponding to the word sequence. 
3   Non-stationary Hidden Markov Model 
3.1   Motivation 
There are many approaches to improve the predictive power of Hmm in practice. For 
example, factorial Hmm [8] is proposed by decomposing the hidden state 
 Principles of Non-stationary Hidden Markov Model and Its Applications 829 
representation into multiple independent Markov chains. In speech recognition, a 
factorial Hmm can represent the combination of multiple signals which are produced 
independently and the characteristics of each signal are described by a distinct 
Markov chain. And some Hmms use neural networks to estimate phonetic posterior 
probability in speech recognition [9]. The input layer of the network typically covers 
both the past states and the further states. However, from the essential definition of 
Hmm, there are two ways to improve the predictive power of Hmm. One approach is 
to relax the limited history hypothesis and involve more history information into 
language model. The other is to relax the stationary hypothesis and make use of time 
information. In recent years, much research focuses on the first approach [10] and 
higher-order Hmm is built up. But as the order increases, the parameter space 
explodes at such an exponential rate that training corpus becomes too sparse and 
system resource exhausts soon. This paper adopts the second approach and tries to 
make good use of time information. Then NSHmm is proposed. Since there is no 
theoretical conflict between NSHmm and high-order Hmm, the two models can be 
combined together in proper conditions.  
3.2   Definition for NSHmm 
Similarly with Hmm, NSHmm is also mathematically defined as a five-tuple M = 
<?, ?, ??, ??, ??> which consists of: 
1. A finite set of (hidden) states ?. 
2. A finite set of (observed) symbols ?. 
3. A state transition function ??: ?? ?? t -> [0, 1]. 
4. A symbol emission function ??: ?? ?? t -> [0, 1]. 
5. And an initial state probability function ??: ?? t -> [0, 1]. 
In the above definition, t is the time variable indicating when state transition or 
symbol emission occurs. Different from Hmm?s definition, ??, ?? and ?? are all the 
functions of t. And they can still be estimated by MLE principle on large scale corpus. 
This key question of NSHmm is how to represent time information. We?ll discuss that 
question in the next section.  
3.3   Representation of Time Information 
Since time information is to describe when the events of Hmm (e.g. state transition or 
symbol emission) occur, a natural way is to use the event index in Markov chain to 
represent the time information. But there are two serious problems with that method. 
Firstly, index has different meanings in the Markov chains of different length. 
Secondly, since a Markov chain may have arbitrary length, the event index can be any 
natural number. However, computer system can only deal with finite value. A refined 
method is to use the ratio of the event index and the length of Markov chain which is 
a real number of the range [0, 1]. But there are infinite real numbers in the range [0, 
1]. In this paper, we divide the range [0, 1] into several equivalence classes (bins) and 
each class share the same time information. When training NSHmm, the functions of 
??, ?? and ?? should be estimated in each bin respectively according to their time 
information. And when they are accessed, they should also get the value in the 
830 J. Xiao, B. Liu, and X. Wang 
according bin. For example, the state transition function ?? can be estimated by the 
formula below:  
( , , )
( , )ijt
C i j tp
C i t
=  (1) 
where ( , , )C i j t is the co-occurrence frequency of state i and state j at time t and it can 
be estimated by counting the co-occurrence times of state i and state j in the tth bin in 
each sentence of corpus. ( , )C i t is the frequency of state i at time t and can be 
estimated by counting the occurrence times of state i in the tth bin in the sentence of 
corpus. And the result ijtP is the transition probability between state i and j at time t. 
It?s similar to estimate the functions of ?? and ??.  
3.4   Algorithms on Non-stationary Hidden Markov Model 
The three fundamental questions of Hmm also exist in NSHmm. The corresponding 
algorithms, such as forward algorithm, viterbi algorithm and Baum-Welch algorithm, 
can work well in NSHmm, except that they have to first calculate the time 
information and then compute the function values of ??, ?? and ?? according to the 
statistical information in the corresponding bins. 
3.5   Space Complexity Analysis 
In this section, we will analyze the space complexity of NSHmm. Compared with 
Hmm, some conclusions can be drawn at the end of this section. For simplicity and 
convenience, we define some notations below: 
? The hidden state number n 
? The observed symbol number m 
? The bin number for NSHmm k 
In Hmm and NSHmm, all system parameters are devoted to simulate the three 
functions of ?, ? and ?. For Hmm, a vector of size n is usually used to store the initial 
probability of each state. An n ? n matrix is adopted to store the transition 
probabilities between every two states, and n ? m matrix to record the emission 
probabilities between states and observed symbols. The space complexity for Hmm is 
the sum of these three parts which is ( )n n n n m? + ? + ? . For NSHmm, since ??, ?? 
and ?? are all the functions of time t, time information should be counted in. An n ? k 
matrix is used to store the initial probability of each state at different time. An n ? n? 
k matrix is used to store the transition probability between each state at different time 
and n ? m ? k matrix to keep the emission probability. Thus, the space complexity of 
NSHmm is (( ) )n n n n m k? + ? + ? ? which is k times than that of Hmm. As the 
analysis shows, the space complexity of NSHmm increases at a linear speed with k, 
rather than at an exponential speed as high-order Hmm dose. Moreover, as k is 
usually far below than n, NSHmm is much easier to avoid the problem of parameter 
space explosion. 
 Principles of Non-stationary Hidden Markov Model and Its Applications 831 
3.6   Variant Form of NSHmm 
In this section, this paper proposes a variant form of NSHmm (VNSHmm). It?s based 
on these facts: for some applications, such as on mobile platform, there is not enough 
system resource to build up a whole NSHmm. Then NSHmm has to be compressed. 
This paper constructs some statistical variables for time information and uses these 
statistical variables to substitute concrete time information in NSHmm. When 
computing the probability in VNSHmm, these statistical variables are combined 
together to calculate a coefficient for normal probability of Hmm. 
Two statistical variables, expectation and variance of time information, are adopted 
in VNSHmm. And such assumptions are made that more weight should be awarded if 
the time of event occurring fits better with the training corpus, and less weight vice 
versa. The probability function in VNSHmm is defined as below: 
2(( ) )1 V t E
tp e pZ
? ?? ? +
= ?  (2) 
where Z is a normalizing factor, and is defined as: 
2(( ) )
1
t k
V t E
t
Z e p? ?
=
? ? +
=
= ??  (3) 
The notations in the formulation (2) and (3) are described in the following: 
? Current time information t 
? Expectation of time information E 
? Variance of time information V 
? State transition probability ( or symbol emission probability ) p 
? Adjusted coefficients ? and  ? 
pt is descendent with the term t-E which defines the difference between current 
time and time expectation in training corpus. As the value of t-E decreases, t fits for 
training corpus better and more weight is added to pt. For example, we take a 
Chinese sentence as a state chain of Markov process. The word ????(first of all) 
usually leads a sentence in training corpus. For test corpus, more weight should be 
given to pt if ??(first of all) appears at the beginning of the sentence, whereas less 
weight if at the sentence end. pt is ascendant with the variance V. The item V is 
mainly used to balance the value of term t-E for some active states. For example, in 
Chinese, some adjectives, such as ????(beautiful), can appear at any position of 
the sentence. Then it?s unreasonable to decrease pt just because the term t-E 
increases. In such a situation, the value of item V for ????(beautiful) is usually 
bigger than that of those inactive states (e.g.??(first of all)). Then the item V can 
provide a balance for the value of t-E.  
Since VNSHmm just makes use of expectation and variance, rather than the whole 
time information, its space complexity is equal to that of the NSHmm with only two 
bins, which is (( ) 2)n n n n m? + ? + ? ? . 
832 J. Xiao, B. Liu, and X. Wang 
4   Experiments 
In the experiments, NSHmm and VNSHmm have been applied to two sequence 
labeling tasks: pos tagging and pinyin-to-character conversion. This paper will 
describe them in detail in the following two sections. 
4.1   Pos Tagging 
For pos tagging, this paper chooses the People?s Daily corpus in 1998 which has been 
labeled by Peking University [11]. The first 5 month corpus is taken as training 
corpus and the 6th month as test corpus. Since most of pos-taggers are based on 2-
order Hmm (trigram), 2-order NSHmm and 2-order VNSHmm are constructed 
respectively in the experiments.  
We first calculate KL distances between the emission probability distribution of 
Hmm and the distributions of NSHmm at different time. Only when the distances are 
great, could NSHmm be expected to outperform Hmm; otherwise NSHmm would 
have similar performance as Hmm has. Since there are totally k different distance 
values for NSHmm with k bins, we just calculate the average distance for each 
NSHmm. The results are presented in table 1 as below: 
Table 1. Average KL Distances between Emission Probability Distributions of NSHmm and 
Hmm  
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Aver KL Dis 0 0.08 0.12 0.15 0.17 0.19 0.21 0.22 
From table 1 we can see that as the bin number increases, the average KL distance 
become bigger and bigger, which indicates there is more and more difference between 
the emission probability distributions of Hmm and that of NSHmm. Similar results can 
be gotten by comparing state-transition-probability distributions of the two models. 
And as time information increases, we expect more predictive power for NSHmm.  
To prove the effectiveness of NSHmm and VNSHmm, in the rest of this section, 
two sets of experiments, close test and open test, are performed. The results of close 
test are showed in table 2, figure 1 and the results of open test are presented in table 3, 
figure 2 as below. 
Table 2. Pos Tagging Close Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 6.04% --- --- --- --- --- --- --- 
Error 
Rate 
6.04% 5.63% 5.55% 5.52% 5.47% 5.44% 5.42% 5.47% NSHmm 
Reduction --- 6.79% 8.11% 8.61% 9.44% 9.93% 10.26% 9.43% 
Error 
Rate 
6.04% 5.85% 5.85% 5.85% 5.85% 5.85% 5.85% 5.85% VNSHmm 
Reduction --- 3.15% 3.15% 3.15% 3.15% 3.15% 3.15% 3.15% 
 Principles of Non-stationary Hidden Markov Model and Its Applications 833 
0 1 2 3 4 5 6 7 8 9
5.2
5.4
5.6
5.8
6.0
6.2
Er
ro
r 
Ra
te
 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 1. Pos Tagging Close Test 
Table 3. Pos Tagging Open Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 6.99% --- --- --- --- --- --- --- 
Error  
Rate 
6.99% 6.44% 6.39% 6.42% 6.40% 6.43% 6.47% 6.58% NSHmm 
Reduction --- 7.87% 8.58% 8.15% 8.44% 8.01% 7.44% 5.87% 
Error  
Rate 
6.99% 6.59% 6.59% 6.59% 6.59% 6.59% 6.59% 6.59% VNSHmm 
Reduction --- 5.72% 5.72% 5.72% 5.72% 5.72% 5.72% 5.72% 
0 1 2 3 4 5 6 7 8 9
6.4
6.6
6.8
7.0
7.2
Er
ro
r R
at
e 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 2. Pos Tagging Open Test 
As table 2 and table 3 have showed, no matter in close test or in open test, NSHmm 
and VNSHmm achieve much lower error rates than Hmm. NSHmm gets at most 
10.26% error rate reduction and VNSHmm obtains 3.15% reduction in close test; and 
834 J. Xiao, B. Liu, and X. Wang 
they achieve 8.58% and 5.72% reductions respectively in open test. These facts prove 
that NSHmm and VNSHmm have much more predictive power than Hmm has. From 
figure 1 we can see that in close test, as the bin number increases, the error rate of 
NSHmm is decreased constantly, which proves that the improvement of NSHmm is 
due to the increasing time information. But in the open test as figure 2 shows, the 
error rate stops decreasing after k=3. That is because of the overfitting problem. As a 
consequence, this paper suggests k=3 in NSHmm for pos tagging task. From figure 1 
and figure 2, VNSHmm performs stably after k=2, which indicates a small number of 
parameters are enough to stat reliable statistical variables for VNSHmm and get 
improved performance.  
4.2   Pinyin-to-Character Conversion 
For the experiments of pinyin-to-character conversion, this paper adopts the same 
training corpus and test corpus as in pos tagging experiments. And 6763 Chinese 
frequent characters are chosen as the lexicon. This paper firstly converts all raw 
Chinese corpuses to the pinyin corpuses. Then based on the both kinds of corpuses, 
Hmm, NSHmm and VNSHmm are built up.  
In the experiments, we first calculate KL distances between the state-transition-
probability distributions of Hmm and the distributions of NSHmm at different time. 
As we have done in the pos tagging experiments, we just calculate the average KL 
distance for each NSHmm. The results are presented in table 4. 
Table 4. Average KL Distances between State-Transition-Probability Distributions of NSHmm 
and Hmm 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Aver KL Dis 0 0.08 0.12 0.15 0.17 0.18 0.19 0.21 
From table 4 we can see that as the bin number increases, the average KL distance 
become bigger and bigger and more predictive power is expected for NSHmm. And 
similar results can be gotten by comparing emission probability distributions of the 
two models. Then in the rest of this section, we perform the pinyin-to-character 
conversion experiments. Close test and open test are performed respectively. The 
results of close test are showed in table 5, figure 3 and the results of open test are 
presented in table 6, figure 4 respectively. 
Table 5. Pinyin-to-Character Conversion Close Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 8.30% --- --- --- --- --- --- --- 
Error  
Rate 
8.30% 7.17% 6.55% 6.08% 5.74% 5.43% 5.19% 4.98% NSHmm 
Reduction --- 13.61% 21.08% 26.75% 30.84% 34.58% 37.47% 40.00% 
Error  
Rate 
8.30% 8.28% 8.27% 8.28% 8.28% 8.28% 8.28% 8.28% VNSHmm 
Reduction --- 0.24% 0.24% 0.24% 0.24% 0.24% 0.24% 0.24% 
 Principles of Non-stationary Hidden Markov Model and Its Applications 835 
0 1 2 3 4 5 6 7 8 9
4.5
5.0
5.5
6.0
6.5
7.0
7.5
8.0
8.5
9.0
Er
ro
r 
R
at
e 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 3. Pinyin-to-Character Conversion Close Test 
Table 6. Pinyin-to-Character Conversion Open Test 
Bin Number K=1 K=2 K=3 K=4 K=5 K=6 K=7 K=8 
Hmm (baseline) 14.97% --- --- --- --- --- --- --- 
Error  
Rate 
14.97%12.62% 13.16% 13.61% 13.93% 14.23% 14.52% 14.81% NSHmm 
Reduction --- 15.70% 12.09% 9.08% 6.95% 4.94% 3.01% 1.07% 
Error  
Rate 
14.97% 11.98% 11.96% 11.96% 11.96% 11.97% 11.97% 11.97% VNSHmm 
Reduction --- 19.97% 20.11% 20.11% 20.11% 20.04% 20.04% 20.04% 
0 1 2 3 4 5 6 7 8 9
12
13
14
15
16
Er
ro
r R
at
e 
(%
)
Bins Number: K
 Hmm
 NSHmm
 VNSHmm
 
Fig. 4. Pinyin-to-Character Conversion Open Test 
836 J. Xiao, B. Liu, and X. Wang 
In the experiments of pinyin-to-character conversion, the results are very similar to 
those in the pos tagging experiments. NSHmm and VNSHmm show much more 
predictive power than Hmm does. NSHmm gets at most 40% error rate reduction and 
VNSHmm obtains 0.24% reduction in close test; and they achieve 15.7% and 20.11% 
reductions respectively in open test. As time information increases, the error rate of 
NSHmm decreases drastically in close test as it dose in pos tagging task. And the 
overfitting problem arises after k=2 in open test.  
However, different from the results of pos tagging experiments, VNSHmm 
outperforms NSHmm in open test. Since 6763 characters are adopted as states set in 
pinyin-to-character conversion system, which is much larger than the states set in pos 
tagging system, data sparseness problem is more likely to occur. VNSHmm can be 
view as a natural smoothing technique for NSHmm. Thus it works better. We also 
notice that the improvements in pinyin-to-character conversion experiments are more 
significant than those in pos-tagging experiments. In pinyin-to-character conversion 
task, the state chain is the Chinese sentence. Intuitively, some Chinese characters and 
words are much more likely to occur at some certain positions in the sentence, for 
instance, the beginning or the end of a sentence. As we discuss in section 3.3, in 
practice the time information of events in NSHmm is defined as the position 
information where the events occur. Then NSHmm and VNSHmm can capture those 
characteristics straightforwardly. But in pos-tagging, the state chain is the pos tag 
stream. Pos is a more abstract concept than word, and their positional characteristics 
are not as apparent as words?. Henceforth, the improvements in pos-tagging 
experiments are less significant than those in pinyin-to-character conversion 
experiments. But NSHmm and VNSHmm can still model and make good use of those 
positional characteristics, and notable improvements have been achieved. 
In a word, NSHmm and VNSHmm achieve much lower error rates in both of the 
two sequence labeling tasks and show much more predictive power than Hmm. 
5   Conclusions 
To improve Hmm?s predictive power and meanwhile avoid the problems of high-
order Hmm, this paper relaxes the stationary hypothesis of Hmm, makes use of time 
information and proposes NSHmm. Moreover, to further reduce NSHmm?s parameter 
space for mobile applications, VNSHmm is proposed by constructing statistical 
variables on the time information of NSHmm. Then NSHmm and VNSHmm are 
applied to two sequence labeling tasks: pos tagging and pinyin-to-character 
conversion. From the experiment results, we can draw three conclusions in this paper: 
? Firstly, NSHmm and VNSHmm achieve much lower error rates than Hmm in 
both of the two tasks and thus have more predictive power. 
? Secondly, the improvement of NSHmm is due to the increasing time 
information. 
? Lastly, a small number of parameters are enough to stat the statistical variables 
for VNSHmm. 
 Principles of Non-stationary Hidden Markov Model and Its Applications 837 
6   Further Research 
Since NSHmm is an enhanced Hmm, some problems of Hmm also exist in NSHmm. 
For example, data sparseness problem is arising as time information increases in 
NSHmm. Some smoothing algorithms should be designed to solve it in our further 
work. Also it?s difficult to describe long distance constraint for NSHmm and further 
research should be devoted to this problem. To construct more compact NSHmm, 
proper prone techniques should be further studied and be compared with VNSHmm. 
Acknowledgements 
This investigation was supported emphatically by the National Natural Science 
Foundation of China (No.60435020) and the High Technology Research and 
Development Programme of China (2002AA117010-09). 
We especially thank the three anonymous reviewers for their valuable suggestions 
and comments.  
References 
1. F. Jelinek. Self-Organized Language Modeling for Speech Recognition. IEEE ICASSP, 
1989.  
2. George Nagy. At the Frontier of OCR. Processing of IEEE. 1992, 80(7).  
3. ZhiMing Xu, XiaoLong Wang, Kai Zhang, Yi Guan. A Post Processing Method for Online 
Handwritten Chinese Character recognition. Journal of Computer Research and 
Development. Vol.36, No. 5, May 1999. 
4. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 
The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational 
Linguistics. 1992, 19(2). 
5. Liu Bingquan, Wang Xiaolong and Wang Yuying, Incorporating Linguistic Rules in 
Statistical Chinese Language Model for Pinyin-to-Character Conversion. High Technology 
Letters. Vol.7 No.2, June 2001, P:8-13 
6. Christopher D. Manning and Hinrich Schutze. Foundation of Statistic Natural Language 
Processing. The MIT Press. 1999.  
7. Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. 
Mercer. Class-based n-gram models of natural language. Computational Linguistics, 
18(4):467-479. 1992.  
8. Z. Ghahramani and M. Jordan. Factorial hidden Markov models. Machine Learning, 29, 
1997.  
9. J. Fritsch. ACID/HNN: A framework for hierarchical connectionist acoustic modeling. In 
Proc. IEEE ASRU, Santa Barbara, December 1997.  
10. Goodman, J. A bit of progress in language modeling. Computer Speech and Language, 
403-434. 2001.  
11. http://www.icl.pku.edu.cn 
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1230?1238,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Modeling Semantic Relevance for Question-Answer Pairs
in Web Social Communities
Baoxun Wang, Xiaolong Wang, Chengjie Sun, Bingquan Liu, Lin Sun
School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China
{bxwang, wangxl, cjsun, liubq, lsun}@insun.hit.edu.cn
Abstract
Quantifying the semantic relevance be-
tween questions and their candidate an-
swers is essential to answer detection in
social media corpora. In this paper, a deep
belief network is proposed to model the
semantic relevance for question-answer
pairs. Observing the textual similarity
between the community-driven question-
answering (cQA) dataset and the forum
dataset, we present a novel learning strat-
egy to promote the performance of our
method on the social community datasets
without hand-annotating work. The ex-
perimental results show that our method
outperforms the traditional approaches on
both the cQA and the forum corpora.
1 Introduction
In natural language processing (NLP) and infor-
mation retrieval (IR) fields, question answering
(QA) problem has attracted much attention over
the past few years. Nevertheless, most of the QA
researches mainly focus on locating the exact an-
swer to a given factoid question in the related doc-
uments. The most well known international evalu-
ation on the factoid QA task is the Text REtrieval
Conference (TREC)1, and the annotated questions
and answers released by TREC have become im-
portant resources for the researchers. However,
when facing a non-factoid question such as why,
how, or what about, however, almost no automatic
QA systems work very well.
The user-generated question-answer pairs are
definitely of great importance to solve the non-
factoid questions. Obviously, these natural QA
pairs are usually created during people?s com-
munication via Internet social media, among
which we are interested in the community-driven
1http://trec.nist.gov
question-answering (cQA) sites and online fo-
rums. The cQA sites (or systems) provide plat-
forms where users can either ask questions or de-
liver answers, and best answers are selected man-
ually (e.g., Baidu Zhidao2 and Yahoo! Answers3).
Comparing with cQA sites, online forums have
more virtual society characteristics, where people
hold discussions in certain domains, such as tech-
niques, travel, sports, etc. Online forums contain
a huge number of QA pairs, and much noise infor-
mation is involved.
To make use of the QA pairs in cQA sites and
online forums, one has to face the challenging
problem of distinguishing the questions and their
answers from the noise. According to our investi-
gation, the data in the community based sites, es-
pecially for the forums, have two obvious charac-
teristics: (a) a post usually includes a very short
content, and when a person is initializing or re-
plying a post, an informal tone tends to be used;
(b) most of the posts are useless, which makes
the community become a noisy environment for
question-answer detection.
In this paper, a novel approach for modeling the
semantic relevance for QA pairs in the social me-
dia sites is proposed. We concentrate on the fol-
lowing two problems:
1. How to model the semantic relationship be-
tween two short texts using simple textual fea-
tures? As mentioned above, the user generated
questions and their answers via social media are
always short texts. The limitation of length leads
to the sparsity of the word features. In addition,
the word frequency is usually either 0 or 1, that is,
the frequency offers little information except the
occurrence of a word. Because of this situation,
the traditional relevance computing methods based
on word co-occurrence, such as Cosine similarity
and KL-divergence, are not effective for question-
2http://zhidao.baidu.com
3http://answers.yahoo.com
1230
answer semantic modeling. Most researchers try
to introduce structural features or users? behavior
to improve the models performance, by contrast,
the effect of textual features is not obvious.
2. How to train a model so that it has good per-
formance on both cQA and forum datasets? So
far, people have been doing QA researches on the
cQA and the forum datasets separately (Ding et
al., 2008; Surdeanu et al, 2008), and no one has
noticed the relationship between the two kinds of
data. Since both the cQA systems and the online
forums are open platforms for people to commu-
nicate, the QA pairs in the cQA systems have sim-
ilarity with those in the forums. In this case, it is
highly valuable and desirable to propose a train-
ing strategy to improve the model?s performance
on both of the two kinds of datasets. In addition,
it is possible to avoid the expensive and arduous
hand-annotating work by introducing the method.
To solve the first problem, we present a deep
belief network (DBN) to model the semantic rel-
evance between questions and their answers. The
network establishes the semantic relationship for
QA pairs by minimizing the answer-to-question
reconstructing error. Using only word features,
our model outperforms the traditional methods on
question-answer relevance calculating.
For the second problem, we make our model
to learn the semantic knowledge from the solved
question threads in the cQA system. Instead of
mining the structure based features from cQA
pages and forum threads individually, we con-
sider the textual similarity between the two kinds
of data. The semantic information learned from
cQA corpus is helpful to detect answers in forums,
which makes our model show good performance
on social media corpora. Thanks to the labels for
the best answers existing in the threads, no manual
work is needed in our strategy.
The rest of this paper is organized as follows:
Section 2 surveys the related work. Section 3 in-
troduces the deep belief network for answer de-
tection. In Section 4, the homogenous data based
learning strategy is described. Experimental result
is given in Section 5. Finally, conclusions and fu-
ture directions are drawn in Section 6.
2 Related Work
The value of the naturally generated question-
answer pairs has not been recognized until recent
years. Early studies mainly focus on extracting
QA pairs from frequently asked questions (FAQ)
pages (Jijkoun and de Rijke, 2005; Riezler et al,
2007) or service call-center dialogues (Berger et
al., 2000).
Judging whether a candidate answer is seman-
tically related to the question in the cQA page
automatically is a challenging task. A frame-
work for predicting the quality of answers has
been presented in (Jeon et al, 2006). Bernhard
and Gurevych (2009) have developed a transla-
tion based method to find answers. Surdeanu et
al. (2008) propose an approach to rank the an-
swers retrieved by Yahoo! Answers. Our work is
partly similar to Surdeanu et al (2008), for we also
aim to rank the candidate answers reasonably, but
our ranking algorithm needs only word informa-
tion, instead of the combination of different kinds
of features.
Because people have considerable freedom to
post on forums, there are a great number of irrel-
evant posts for answering questions, which makes
it more difficult to detect answers in the forums.
In this field, exploratory studies have been done by
Feng et al (2006) and Huang et al (2007), who ex-
tract input-reply pairs for the discussion-bot. Ding
et al(2008) and Cong et al(2008) have also pre-
sented outstanding research works on forum QA
extraction. Ding et al (2008) detect question con-
texts and answers using the conditional random
fields, and a ranking algorithm based on the au-
thority of forum users is proposed by Cong et al
(2008). Treating answer detection as a binary clas-
sification problem is an intuitive idea, thus there
are some studies trying to solve it from this view
(Hong and Davison, 2009; Wang et al, 2009). Es-
pecially Hong and Davison (2009) have achieved
a rather high precision on the corpora with less
noise, which also shows the importance of ?social?
features.
In order to select the answers for a given ques-
tion, one has to face the problem of lexical gap.
One of the problems with lexical gap embedding
is to find similar questions in QA achieves (Jeon et
al., 2005). Recently, the statistical machine trans-
lation (SMT) strategy has become popular. Lee et
al. (2008) use translate models to bridge the lexi-
cal gap between queries and questions in QA col-
lections. The SMT based methods are effective on
modeling the semantic relationship between ques-
tions and answers and expending users? queries in
answer retrieval (Riezler et al, 2007; Berger et al,
1231
2000; Bernhard and Gurevych, 2009). In (Sur-
deanu et al, 2008), the translation model is used
to provide features for answer ranking.
The structural features (e.g., authorship, ac-
knowledgement, post position, etc), also called
non-textual features, play an important role in an-
swer extraction. Such features are used in (Ding
et al, 2008; Cong et al, 2008), and have signifi-
cantly improved the performance. The studies of
Jeon et al (2006) and Hong et al (2009) show that
the structural features have even more contribution
than the textual features. In this case, the mining
of textual features tends to be ignored.
There are also some other research topics in this
field. Cong et al (2008) and Wang et al (2009)
both propose the strategies to detect questions in
the social media corpus, which is proved to be a
non-trivial task. The deep research on question
detection has been taken by Duan et al (2008).
A graph based algorithm is presented to answer
opinion questions (Li et al, 2009). In email sum-
marization field, the QA pairs are also extracted
from email contents as the main elements of email
summarization (Shrestha and McKeown, 2004).
3 The Deep Belief Network for QA pairs
Due to the feature sparsity and the low word fre-
quency of the social media corpus, it is difficult
to model the semantic relevance between ques-
tions and answers using only co-occurrence fea-
tures. It is clear that the semantic link exists be-
tween the question and its answers, even though
they have totally different lexical representations.
Thus a specially designed model may learn se-
mantic knowledge by reconstructing a great num-
ber of questions using the information in the cor-
responding answers. In this section, we propose
a deep belief network for modeling the seman-
tic relationship between questions and their an-
swers. Our model is able to map the QA data into
a low-dimensional semantic-feature space, where
a question is close to its answers.
3.1 The Restricted Boltzmann Machine
An ensemble of binary vectors can be modeled us-
ing a two-layer network called a ?restricted Boltz-
mann machine? (RBM) (Hinton, 2002). The di-
mension reducing approach based on RBM ini-
tially shows good performance on image process-
ing (Hinton and Salakhutdinov, 2006). Salakhut-
dinov and Hinton (2009) propose a deep graphical
model composed of RBMs into the information re-
trieval field, which shows that this model is able to
obtain semantic information hidden in the word-
count vectors.
As shown in Figure 1, the RBM is a two-layer
network. The bottom layer represents a visible
vector v and the top layer represents a latent fea-
ture h. The matrix W contains the symmetric in-
teraction terms between the visible units and the
hidden units. Given an input vector v, the trained
Figure 1: Restricted Boltzmann machine
RBM model provides a hidden feature h, which
can be used to reconstruct v with a minimum er-
ror. The training algorithm for this paper will be
described in the next subsection. The ability of the
RBM suggests us to build a deep belief network
based on RBM so that the semantic relevance be-
tween questions and answers can be modeled.
3.2 Pretraining a Deep Belief Network
In the social media corpora, the answers are al-
ways descriptive, containing one or several sen-
tences. Noticing that an answer has strong seman-
tic association with the question and involves more
information than the question, we propose to train
a deep belief network by reconstructing the ques-
tion using its answers. The training object is to
minimize the error of reconstruction, and after the
pretraining process, a point that lies in a good re-
gion of parameter space can be achieved.
Firstly, the illustration of the DBN model is
given in Figure 2. This model is composed of
three layers, and here each layer stands for the
RBM or its variant. The bottom layer is a variant
form of RBM?s designed for the QA pairs. This
layer we design is a little different from the classi-
cal RBM?s, so that the bottom layer can generate
the hidden features according to the visible answer
vector and reconstruct the question vector using
the hidden features. The pre-training procedure of
this architecture is practically convergent. In the
bottom layer, the binary feature vectors based on
the statistics of the word occurrence in the answers
are used to compute the ?hidden features? in the
1232
Figure 2: The Deep Belief Network for QA Pairs
hidden units. The model can reconstruct the ques-
tions using the hidden features. The processes can
be modeled as follows:
p(h j = 1|a) = ?(b j +
?
i
wi jai) (1)
p(qi = 1|h) = ?(bi +
?
j
wi jh j) (2)
where ?(x) = 1/(1 + e?x), a denotes the visible
feature vector of the answer, qi is the ith element
of the question vector, and h stands for the hid-
den feature vector for reconstructing the questions.
wi j is a symmetric interaction term between word
i and hidden feature j, bi stands for the bias of the
model for word i, and b j denotes the bias of hidden
feature j.
Given the training set of answer vectors, the bot-
tom layer generates the corresponding hidden fea-
tures using Equation 1. Equation 2 is used to re-
construct the Bernoulli rates for each word in the
question vectors after stochastically activating the
hidden features. Then Equation 1 is taken again
to make the hidden features active. We use 1-step
Contrastive Divergence (Hinton, 2002) to update
the parameters by performing gradient ascent:
?wi j = (< qih j >qData ? < qih j >qRecon) (3)
where < qih j >qData denotes the expectation of
the frequency with which the word i in a ques-
tion and the feature j are on together when the
hidden features are driven by the question data.
< qih j >qRecon defines the corresponding expec-
tation when the hidden features are driven by the
reconstructed question data.  is the learning rate.
The classical RBM structure is taken to build
the middle layer and the top layer of the network.
The training method for the higher two layer is
similar to that of the bottom one, and we only have
to make each RBM to reconstruct the input data
using its hidden features. The parameter updates
still obeying the rule defined by gradient ascent,
which is quite similar to Equation 3. After train-
ing one layer, the h vectors are then sent to the
higher-level layer as its ?training data?.
3.3 Fine-tuning the Weights
Notice that a greedy strategy is taken to train each
layer individually during the pre-training proce-
dure, it is necessary to fine-tune the weights of the
entire network for optimal reconstruction. To fine-
tune the weights, the network is unrolled, taking
the answers as the input data to generate the corre-
sponding questions at the output units. Using the
cross-entropy error function, we can then tune the
network by performing backpropagation through
it. The experiment results in section 5.2 will show
fine-tuning makes the network performs better for
answer detection.
3.4 Best answer detection
After pre-training and fine-tuning, a deep belief
network for QA pairs is established. To detect the
best answer to a given question, we just have to
send the vectors of the question and its candidate
answers into the input units of the network and
perform a level-by-level calculation to obtain the
corresponding feature vectors. Then we calculate
the distance between the mapped question vector
and each candidate answer vector. We consider the
candidate answer with the smallest distance as the
best one.
4 Learning with Homogenous Data
In this section, we propose our strategy to make
our DBN model to detect answers in both cQA and
forum datasets, while the existing studies focus on
one single dataset.
4.1 Homogenous QA Corpora from Different
Sources
Our motivation of finding the homogenous
question-answer corpora from different kind of so-
cial media is to guarantee the model?s performance
and avoid hand-annotating work.
In this paper, we get the ?solved question? pages
in the computer technology domain from Baidu
Zhidao as the cQA corpus, and the threads of
1233
Figure 3: Comparison of the post content lengths in the cQA and the forum datasets
ComputerFansClub Forum4 as the online forum
corpus. The domains of the corpora are the same.
To further explain that the two corpora are ho-
mogenous, we will give the detail comparison on
text style and word distribution.
As shown in Figure 3, we have compared the
post content lengths of the cQA and the forum
in our corpora. For the comparison, 5,000 posts
from the cQA corpus and 5,000 posts from the fo-
rum corpus are randomly selected. The left panel
shows the statistical result on the Baidu Zhidao
data, and the right panel shows the one on the fo-
rum data. The number i on the horizontal axis de-
notes the post contents whose lengths range from
10(i? 1) + 1 to 10i bytes, and the vertical axis rep-
resents the counts of the post contents. From Fig-
ure 3 we observe that the contents of most posts
in both the cQA corpus and the forum corpus are
short, with the lengths not exceeding 400 bytes.
The content length reflects the text style of the
posts in cQA systems and online forums. From
Figure 3 it can be also seen that the distributions
of the content lengths in the two figures are very
similar. It shows that the contents in the two cor-
pora are both mainly short texts.
Figure 4 shows the percentage of the concurrent
words in the top-ranked content words with high
frequency. In detail, we firstly rank the words by
frequency in the two corpora. The words are cho-
sen based on a professional dictionary to guarantee
that they are meaningful in the computer knowl-
edge field. The number k on the horizontal axis in
Figure 4 represents the top k content words in the
4http://bbs.cfanclub.net/
corpora, and the vertical axis stands for the per-
centage of the words shared by the two corpora in
the top k words.
Figure 4: Distribution of concurrent content words
Figure 4 shows that a large number of meaning-
ful words appear in both of the two corpora with
high frequencies. The percentage of the concur-
rent words maintains above 64% in the top 1,400
words. It indicates that the word distributions of
the two corpora are quite similar, although they
come from different social media sites.
Because the cQA corpus and the forum corpus
used in this study have homogenous characteris-
tics for answer detecting task, a simple strategy
may be used to avoid the hand-annotating work.
Apparently, in every ?solved question? page of
Baidu Zhidao, the best answer is selected by the
user who asks this question. We can easily extract
the QA pairs from the cQA corpus as the training
1234
set. Because the two corpora are similar, we can
apply the deep belief network trained by the cQA
corpus to detect answers on both the cQA data and
the forum data.
4.2 Features
The task of detecting answers in social media cor-
pora suffers from the problem of feature sparsity
seriously. High-dimensional feature vectors with
only several non-zero dimensions bring large time
consumption to our model. Thus it is necessary to
reduce the dimension of the feature vectors.
In this paper, we adopt two kinds of word fea-
tures. Firstly, we consider the 1,300 most fre-
quent words in the training set as Salakhutdinov
and Hinton (2009) did. According to our statis-
tics, the frequencies of the rest words are all less
then 10, which are not statistically significant and
may introduce much noise.
We take the occurrence of some function words
as another kind of features. The function words
are quite meaningful for judging whether a short
text is an answer or not, especially for the non-
factoid questions. For example, in the answers to
the causation questions, the words such as because
and so are more likely to appear; and the words
such as firstly, then, and should may suggest the
answers to the manner questions. We give an ex-
ample for function word selection in Figure 5.
Figure 5: An example for function word selection
For this reason, we collect 200 most frequent
function words in the answers of the training set.
Then for every short text, either a question or an
answer, a 1,500-dimensional vector can be gener-
ated. Specifically, all the features we have adopted
are binary, for they only have to denote whether
the corresponding word appears in the text or not.
5 Experiments
To evaluate our question-answer semantic rele-
vance computing method, we compare our ap-
proach with the popular methods on the answer
detecting task.
5.1 Experiment Setup
Architecture of the Network: To build the deep
belief network, we use a 1500-1500-1000-600 ar-
chitecture, which means the three layers of the net-
work have individually 1,500?1,500, 1,500?1,000
and 1,000?600 units. Using the network, a 1,500-
dimensional binary vector is finally mapped to a
600-dimensional real-value vector.
During the pretraining stage, the bottom layer
is greedily pretrained for 200 passes through the
entire training set, and each of the rest two layers is
greedily pretrained for 50 passes. For fine-tuning
we apply the method of conjugate gradients5, with
three line searches performed in each pass. This
algorithm is performed for 50 passes to fine-tune
the network.
Dataset: we have crawled 20,000 pages of
?solved question? from the computer and network
category of Baidu Zhidao as the cQA corpus. Cor-
respondingly we obtain 90,000 threads from Com-
puterFansClub, which is an online forum on com-
puter knowledge. We take the forum threads as
our forum corpus.
From the cQA corpus, we extract 12,600 human
generated QA pairs as the training set without any
manual work to label the best answers. We get the
contents from another 2,000 cQA pages to form
a testing set, each content of which includes one
question and 4.5 candidate answers on average,
with one best answer among them. To get another
testing dataset, we randomly select 2,000 threads
from the forum corpus. For this training set, hu-
man work are necessary to label the best answers
in the posts of the threads. There are 7 posts in-
cluded in each thread on average, among which
one question and at least one answer exist.
Baseline: To show the performance of our
method, three main popular relevance computing
methods for ranking candidate answers are con-
sidered as our baselines. We will briefly introduce
them:
Cosine Similarity. Given a question q and its
candidate answer a, their cosine similarity can be
computed as follows:
cos(q, a) =
?n
k=1 wqk ? wak??n
k=1 w2qk ?
??n
k=1 w2ak
(4)
where wqk and wak stand for the weight of the kth
word in the question and the answer respectively.
5Code is available at
http://www.kyb.tuebingen.mpg.de/bs/people/carl/code/minimize/
1235
The weights can be get by computing the product
of term frequency (tf ) and inverse document fre-
quency (idf )
HowNet based Similarity. HowNet6 is an elec-
tronic world knowledge system, which serves as
a powerful tool for meaning computation in hu-
man language technology. Normally the similar-
ity between two passages can be calculated by
two steps: (1) matching the most semantic-similar
words in each passages greedily using the API?s
provided by HowNet; (2) computing the weighted
average similarities of the word pairs. This strat-
egy is taken as a baseline method for computing
the relevance between questions and answers.
KL-divergence Language Model. Given a ques-
tion q and its candidate answer a, we can con-
struct unigram language model Mq and unigram
language model Ma. Then we compute KL-
divergence between Mq and Ma as below:
KL(Ma||Mq) =
?
w
p(w|Ma) log(p(w|Ma)/p(w|Mq))
(5)
5.2 Results and Analysis
We evaluate the performance of our approach for
answer detection using two metrics: Precision@1
(P@1) and Mean Reciprocal Rank (MRR). Ap-
plying the two metrics, we perform the baseline
methods and our DBN based methods on the two
testing set above.
Table 1 lists the results achieved on the forum
data using the baseline methods and ours. The ad-
ditional ?Nearest Answer? stands for the method
without any ranking strategies, which returns the
nearest candidate answer from the question by po-
sition. To illustrate the effect of the fine-tuning for
our model, we list the results of our method with-
out fine-tuning and the results with fine-tuning.
As shown in Table 1, our deep belief network
based methods outperform the baseline methods
as expected. The main reason for the improve-
ments is that the DBN based approach is able to
learn semantic relationship between the words in
QA pairs from the training set. Although the train-
ing set we offer to the network comes from a dif-
ferent source (the cQA corpus), it still provide
enough knowledge to the network to perform bet-
ter than the baseline methods. This phenomena in-
dicates that the homogenous corpora for training is
6Detail information can be found in:
http://www.keenage.com/
effective and meaningful.
Method P@1 (%) MRR (%)
Nearest Answer 21.25 38.72
Cosine Similarity 23.15 43.50
HowNet 22.55 41.63
KL divergence 25.30 51.40
DBN (without FT) 41.45 59.64
DBN (with FT) 45.00 62.03
Table 1: Results on Forum Dataset
We have also investigated the reasons for the un-
satisfying performance of the baseline approaches.
Basically, the low precision is ascribable to the
forum corpus we have obtained. As mentioned
in Section 1, the contents of the forum posts are
short, which leads to the sparsity of the features.
Besides, when users post messages in the online
forums, they are accustomed to be casual and use
some synonymous words interchangeably in the
posts, which is believed to be a significant situ-
ation in Chinese forums especially. Because the
features for QA pairs are quite sparse and the con-
tent words in the questions are usually morpholog-
ically different from the ones with the same mean-
ing in the answers, the Cosine Similarity method
become less powerful. For HowNet based ap-
proaches, there are a large number of words not
included by HowNet, thus it fails to compute the
similarity between questions and answers. KL-
divergence suffers from the same problems with
the Cosine Similarity method. Compared with
the Cosine Similarity method, this approach has
achieved the improvement of 9.3% in P@1, but
it performs much better than the other baseline
methods in MRR.
The baseline results indicate that the online fo-
rum is a complex environment with large amount
of noise for answer detection. Traditional IR
methods using pure textual features can hardly
achieve good results. The similar baseline results
for forum answer ranking are also achieved by
Hong and Davison (2009), which takes some non-
textual features to improve the algorithm?s perfor-
mance. We also notice that, however, the baseline
methods have obtained better results on forum cor-
pus (Cong et al, 2008). One possible reason is that
the baseline approaches are suitable for their data,
since we observe that the ?nearest answer? strat-
egy has obtained a 73.5% precision in their work.
Our model has achieved the precision of
1236
45.00% in P@1 and 62.03% in MRR for answer
detecting on forum data after fine-tuning, while
some related works have reported the results with
the precision over 90% (Cong et al, 2008; Hong
and Davison, 2009). There are mainly two rea-
sons for this phenomena: Firstly, both of the pre-
vious works have adopt non-textual features based
on the forum structure, such as authorship, po-
sition and quotes, etc. The non-textual (or so-
cial based) features have played a significant role
in improving the algorithms? performance. Sec-
ondly, the quality of corpora influences the results
of the ranking strategies significantly, and even
the same algorithm may perform differently when
the dataset is changed (Hong and Davison, 2009).
For the experiments of this paper, large amount of
noise is involved in the forum corpus and we have
done nothing extra to filter it.
Table 2 shows the experimental results on the
cQA dataset. In this experiment, each sample is
composed of one question and its following sev-
eral candidate answers. We delete the ones with
only one answer to confirm there are at least two
candidate answers for each question. The candi-
date answers are rearranged by post time, so that
the real answers do not always appear next to the
questions. In this group of experiment, no hand-
annotating work is needed because the real an-
swers have been labeled by cQA users.
Method P@1 (%) MRR (%)
Nearest Answer 36.05 56.33
Cosine Similarity 44.05 62.84
HowNet 41.10 58.75
KL divergence 43.75 63.10
DBN (without FT) 56.20 70.56
DBN (with FT) 58.15 72.74
Table 2: Results on cQA Dataset
From Table 2 we observe that all the approaches
perform much better on this dataset. We attribute
the improvements to the high quality QA corpus
Baidu Zhidao offers: the candidate answers tend to
be more formal than the ones in the forums, with
less noise information included. In addition, the
?Nearest Answer? strategy has reached 36.05% in
P@1 on this dataset, which indicates quite a num-
ber of askers receive the real answers at the first
answer post. This result has supported the idea of
introducing position features. What?s more, if the
best answer appear immediately, the asker tends
to lock down the question thread, which helps to
reduce the noise information in the cQA corpus.
Despite the baseline methods? performances
have been improved, our approaches still outper-
form them, with a 32.0% improvement in P@1
and a 15.3% improvement in MRR at least. On
the cQA dataset, our model shows better perfor-
mance than the previous experiment, which is ex-
pected because the training set and the testing set
come from the same corpus, and the DBN model
is more adaptive to the cQA data.
We have observed that, from both of the two
groups of experiments, fine-tuning is effective for
enhancing the performance of our model. On the
forum data, the results have been improved by
8.6% in P@1 and 4.0% in MRR, and the improve-
ments are 3.5% and 3.1% individually.
6 Conclusions
In this paper, we have proposed a deep belief net-
work based approach to model the semantic rel-
evance for the question answering pairs in social
community corpora.
The contributions of this paper can be summa-
rized as follows: (1) The deep belief network we
present shows good performance on modeling the
QA pairs? semantic relevance using only word fea-
tures. As a data driven approach, our model learns
semantic knowledge from large amount of QA
pairs to represent the semantic relevance between
questions and their answers. (2) We have stud-
ied the textual similarity between the cQA and the
forum datasets for QA pair extraction, and intro-
duce a novel learning strategy to make our method
show good performance on both cQA and forum
datasets. The experimental results show that our
method outperforms the traditional approaches on
both the cQA and the forum corpora.
Our future work will be carried out along two
directions. Firstly, we will further improve the
performance of our method by adopting the non-
textual features. Secondly, more research will be
taken to put forward other architectures of the deep
networks for QA detection.
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. Special
thanks to Deyuan Zhang, Bin Liu, Beidong Liu
and Ke Sun for insightful suggestions. This work
is supported by NSFC (60973076).
1237
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the lexi-
cal chasm: Statistical approaches to answer-finding.
In In Proceedings of the 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 192?199.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining lexical semantic resources with question &
answer archives for translation-based answer find-
ing. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 728?736, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ?08: Proceed-
ings of the 31st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 467?474, New York, NY,
USA. ACM.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract contexts and answers of questions from online
forums. In Proceedings of ACL-08: HLT, pages
710?718, Columbus, Ohio, June. Association for
Computational Linguistics.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of
ACL-08: HLT, pages 156?164, Columbus, Ohio,
June. Association for Computational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard H.
Hovy. 2006. An intelligent discussion-bot for an-
swering student queries in threaded discussions. In
Ccile Paris and Candace L. Sidner, editors, IUI,
pages 171?177. ACM.
G. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Georey E. Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14.
Liangjie Hong and Brian D. Davison. 2009. A
classification-based approach to question answering
in discussion boards. In SIGIR ?09: Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 171?178, New York, NY, USA. ACM.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion
forums. In IJCAI?07: Proceedings of the 20th in-
ternational joint conference on Artifical intelligence,
pages 423?428, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In CIKM ?05, pages 84?90, New
York, NY, USA. ACM.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In SIGIR ?06,
pages 228?235, New York, NY, USA. ACM.
Valentin Jijkoun and Maarten de Rijke. 2005. Retriev-
ing answers from frequently asked questions pages
on the web. In CIKM ?05, pages 76?83, New York,
NY, USA. ACM.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and
Hae-Chang Rim. 2008. Bridging lexical gaps be-
tween queries and questions on large online q&a
collections with compact translation models. In
EMNLP ?08: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 410?418, Morristown, NJ, USA. Association
for Computational Linguistics.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with
random walks on graphs. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
737?745, Suntec, Singapore, August. Association
for Computational Linguistics.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical machine translation for query expansion
in answer retrieval. In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics, pages 464?471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Semantic hashing. Int. J. Approx. Reasoning,
50(7):969?978.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. In Proceedings of Coling 2004, pages 889?
895, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online QA collections. In Proceedings of ACL-08:
HLT, pages 719?727, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Baoxun Wang, Bingquan Liu, Chengjie Sun, Xiao-
long Wang, and Lin Sun. 2009. Extracting chinese
question-answer pairs from online forums. In SMC
2009: Proceedings of the IEEE International Con-
ference on Systems, Man and Cybernetics, 2009.,
pages 1159?1164.
1238
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 843?847,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Multimodal DBN for Predicting High-Quality Answers in cQA portals
Haifeng Hu, Bingquan Liu, Baoxun Wang, Ming Liu, Xiaolong Wang
School of Computer Science and Technology
Harbin Institute of Technology, China
{hfhu, liubq, bxwang, mliu, wangxl}@insun.hit.edu.cn
Abstract
In this paper, we address the problem for
predicting cQA answer quality as a clas-
sification task. We propose a multimodal
deep belief nets based approach that op-
erates in two stages: First, the joint rep-
resentation is learned by taking both tex-
tual and non-textual features into a deep
learning network. Then, the joint repre-
sentation learned by the network is used
as input features for a linear classifier. Ex-
tensive experimental results conducted on
two cQA datasets demonstrate the effec-
tiveness of our proposed approach.
1 Introduction
Predicting the quality of answers in communi-
ty based Question Answering (cQA) portals is a
challenging task. One straightforward approach
is to use textual features as a text classification
task (Agichtein et al, 2008). However, due to
the word over-sparsity and inherent noise of user-
generated content, the classical bag-of-words rep-
resentation, is not appropriate to estimate the qual-
ity of short texts (Huang et al, 2011). Another typ-
ical approach is to leverage non-textual features to
automatically identify high quality answers (Jeon
et al, 2006; Zhou et al, 2012). However, in this
way, the mining of meaningful textual features
usually tends to be ignored.
Intuitively, combining both textual and non-
textual information extracted from answers is
helpful to improve the performance for predict-
ing the answer quality. However, textual and non-
textual features usually have different kinds of rep-
resentations and the correlations between them are
highly non-linear. Previous study (Ngiam et al,
2011) has shown that it is hard for a shallow model
to discover the correlations over multiple sources.
To this end, a deep learning approach, called
multimodal deep belief nets (mDBN), is intro-
duced to address the above problems to predict the
answer quality. The approach includes two stages:
feature learning and supervised training. In the
former stage, a specially designed deep network is
given to learn the unified representation using both
textual and non-textual information. In the latter
stage, the outputs of the network are then used as
inputs for a linear classifier to make prediction.
The rest of this paper is organized as follows:
The related work is surveyed in Section 2. Then,
the proposed approach and experimental results
are presented in Section 3 and Section 4 respec-
tively. Finally, conclusions and future directions
are drawn in Section 5.
2 Related Work
The typical way to predict the answer quality is
exploring various features and employing machine
learning methods. For example, Jeon et al (2006)
have proposed a framework to predict the qual-
ity of answers by incorporating non-textual fea-
tures into a maximum entropy model. Subsequent-
ly, Agichtein et al (2008) and Bian et al (2009)
both leverage a larger range of features to find high
quality answers. The deep research on evaluating
answer quality has been taken by Shah and Pomer-
antz (2010) using the logistic regression model.
We borrow some of their ideas in this paper.
In deep learning field, extensive studies have
been done by Hinton and his co-workers (Hin-
ton et al, 2006; Hinton and Salakhutdinov, 2006;
Salakhutdinov and Hinton, 2009), who initial-
ly propose the deep belief nets (DBN). Wang
et.al (2010; 2011) firstly apply the DBNs to model
semantic relevance for qa pairs in social communi-
ties. Meanwhile, the feature learning for disparate
sources has also been the hot research topic. Lee
et al (2009) demonstrate that the hidden represen-
tations computed by a convolutional DBN make
excellent features for visual recognition.
843
3 Approach
We consider the problem of high-quality answer
prediction as a classification task. Figure 1 sum-
marizes the framework of our proposed approach.
First, textual features and non-textual features ex-
TextualFeatures Non-textualFeaturesCQAArchives
ClassifierFusion Representation
FeatureLearning  Supervised Training
High-qualityAnswers
Figure 1: Framework of our proposed approach.
tracted from cQA portals are used to train two DB-
N models to learn the high-level representation-
s independently for answers. The two high-level
representations learned by the deep architectures
are then joined together to train a RBM model.
Finally, a linear classifier is trained with the final
shared representation as input to make prediction.
In this section, a deep network for the cQA an-
swer quality prediction is presented. Textual and
non-textual features are typically characterized by
distinct statistical properties and the correlations
between them are highly non-linear. It is very dif-
ficult for a shallow model to discover these corre-
lations and form an informative unified represen-
tation. Our motivation of proposing the mDBN is
to tackle these problems using an unified represen-
tation to enhance the classification performance.
3.1 The Restricted Boltzmann Machines
The basic building block of our feature leaning
component is the Restricted Boltzmann Machine
(RBM). The classical RBM is a two-layer undi-
rected graphical model with stochastic visible u-
nits v and stochastic hidden units h.The visible
layer and the hidden layer are fully connected to
the units in the other layer by a symmetric matrix
w. The classical RBM has been used effectively in
modeling distributions over binary-value data. As
for real-value inputs, the gaussian RBM (Bengio
et al, 2007) can be employed. Different from the
former, the hypothesis for the visible unit in the
gaussian RBM is the normal distribution.
3.2 Feature Learning
The illustration of the feature learning model is
given by Figure 2. Basically, the model consists
of two parts.
In the bottom part (i.e., V -H1, H1-H2), each
data modality is modeled by a two-layer DBN sep-
arately. For clarity, we take the textual modality
as an example to illustrate the construction of the
mDBN in this part. Given a textual input vector v,
the visible layer generates the hidden vector h, by
p(hj = 1|v) = ?(cj +
?
iwijvi).
Then the conditional distribution of v given h, is
p(vi = 1|h) = ?(bi +
?
j wijhj).
where ?(x) = (1 + e?x)?1 denotes the logistic
function. The parameters are updated by perform-
ing gradient ascent using Contrastive Divergence
(CD) algorithm (Hinton, 2002).
After learning the RBMs in the bottom layer,
we treat the activation probabilities of its hidden
units driven by the inputs, as the training data for
training a new layer. The construction procedures
for the non-textual modality are similar to the tex-
tual one, except that we use the gaussian RBM to
model the real-value inputs in the bottom layer.
Finally, we combine the two models by adding
an additional layer, H3, on the top of them to form
the mDBN. The training method is also similar to
the bottom?s, but the input vector is the concatena-
tion of the mapped textual vector and the mapped
non-textual vector.
Figure 2: mDBN for Feature Learning
It should be noted in the network, the bottom
part is essential to form the joint representation
because the correlations between the textual and
non-textual features are highly non-linear. It is
hard for a RBM directly combining the two dis-
parate sources to learn their correlations.
3.3 Supervised Training and Classification
After the above steps, a deep network for feature
learning between textual and non-textual data is
established. Classifiers, either support vector ma-
chine (SVM) or logistic regression (LR), can then
be trained with the unified representation (Ngiam
844
et al, 2011; Srivastava and Salakhutdinov, 2012).
Specifically, the LR classifier is used to make the
final prediction in our experiments since it keeps
to deliver the best performance.
3.4 Basic Features
Textual Features: The textual features ex-
tract from 1,500 most frequent words in the train-
ing dataset after standard preprocessing steps,
namely word segmentation, stopwords removal
and stemming1. As a result, each answer is repre-
sented as a vector containing 1,500 distinct terms
weighted by binary scheme.
Non-textual Features: Referring to
the previous work (Jeon et al, 2006; Shah and
Pomerantz, 2010), we adopt some features used
in theirs and also explore three additional features
marked by ? sign. The complete list is described
in Table 1.
Features Type
Length of question title (description) Integer
Length of answer Integer
Number of unique words for the answer ? Integer
Ratio of the qa length ? Float
Answer?s relative position ? Integer
Number of answers for the question Integer
Number of comments for the question Integer
Number of questions asked by asker (answerer) Integer
Number of questions resolved by asker (answerer) Integer
Asker?s (Answerer?s) total points Integer
Asker?s (Answerer?s) level Integer
Asker?s (Answerer?s) total stars Integer
Asker?s (Answerer?s) best answer ratio Float
Table 1: Summary of non-textual features.
4 Experiments
4.1 Experiment Setup
Datasets: We carry out experiments on two
datasets. One dataset comes from Baidu Zhi-
dao2, which contains 33,740 resolved questions
crawled by us from the ?travel? category. The oth-
er dataset is built by Chen and Nayak (2008) from
Yahoo! Answers3. We refer to these two dataset-
s as ZHIDAO and YAHOO respectively and ran-
domly sample 10,000 questions from each to form
our experimental datasets. According to the us-
er name, we have crawled all the user profile web
pages for non-textual feature collection. To allevi-
ate unnecessary noise, we only select those ques-
tions with number of answers no less than 3 (one
1The stemming step is only used in English corpus.
2http://zhidao.baidu.com
3http://answers.yahoo.com
best answer among them), and those answers at
least have 10 tokens. The statistics on the datasets
used for experiments are summarized in Table 2.
Statistics Items YAHOO ZHIDAO
# of questions 6841 5368
# of answers 74485 22435
# of answers per question 10.9 4.1
# of users 28812 12734
Table 2: Statistics on experimental datasets.
Baselines and Evaluation Metrics: We com-
pare against the following methods as our base-
lines. (1) Logistic Regression (LR): We imple-
ment the approach used by Shah and Pomer-
antz (2010) with textual features LR-T, non-
textual features LR-N and their simple combina-
tion LR-C. (2) DBN: Similar to the mDBN, the
outputs of the last hidden layer by the DBN are
used as inputs for LR model. Based on the fea-
ture sets, we have DBN-T for textual features and
DBN-N for non-textual features.
Since we mainly focus on the high quality an-
swers, the precision, recall and f1 for positive class
and the overall accuracy for both classes are em-
ployed as our evaluation metrics.
Model Architecture and Training Details: To
create the mDBN architecture, we use the classi-
cal RBM with 1500 visible units followed by 2
hidden layers with 1000 and 800 units respective-
ly for the textual branch, and the gaussian RBM
with 20 visible units followed by 2 hidden layers
with 100 and 200 units respectively for the non-
textual branch. On the joint layer of the network,
the layer contains 1000 real-value units.
Each RBM is trained using 1-step CD algorith-
m. During the training stage, a small weight-cost
of 0.0002 is used, and the learning rate for textu-
al data modal is 0.05 while the non-textual data is
0.001. We also adopt a monument of 0.5 for the
first five epochs and 0.9 for the rest epochs. In
addition, all non-textual data vectors are normal-
ized to have zero mean and unit standard variance.
More details for training the deep architecture can
be found in Hinton (2012).
4.2 Results and Analysis
In the first experiment, we compare the perfor-
mance of mDBN with different methods. To make
a fare comparison, we use the liblinear toolkit4 for
logistic regression model with L2 regularization
and randomly select 70% QA pairs as training data
4available at http://www.csie.ntu.edu.tw/ cjlin/liblinear
845
and the rest 30% as testing data. Table 3 and Ta-
ble 4 summarize the average results of the 5 round
experiments on YAHOO and ZHIDAO respectively.
Methods P R F1 Accu.
LR-T 0.374 0.558 0.448 0.542
LR-N 0.524 0.614 0.566 0.686
LR-C 0.493 0.557 0.523 0.662
DBN-T 0.496 0.571 0.531 0.663
DBN-N 0.505 0.578 0.539 0.670
mDBN 0.534 0.631 0.579 0.694
Table 3: Comparing results on YAHOO
It is promising to see that the proposed mDBN
method notably outperforms almost all the other
methods on both datasets over all the metrics as
expected, except for the recall on ZHIDAO. The
main reason for the improvements is that the joint
representation learned by mDBN is able to com-
plement each modality perfectly. In addition, the
mDBN can extract stronger representation through
modeling semantic relationship between textual
and non-textual information, which can effectively
help distinguish more complicated answers from
high quality to low quality.
Methods P R F1 Accu.
LR-T 0.380 0.540 0.446 0.553
LR-N 0.523 0.735 0.611 0.688
LR-C 0.537 0.695 0.606 0.698
DBN-T 0.527 0.730 0.612 0.692
DBN-N 0.539 0.760 0.631 0.703
mDBN 0.590 0.755 0.662 0.743
Table 4: Comparing results on ZHIDAO
The classification performance of the textu-
al features are worse on average compared with
non-textual features, even when the feature learn-
ing strategy is employed. More interestingly, we
find the simple combinations of textual and non-
textual features don?t improve the classification
results compared with using non-textual features
alone.We conjecture that there are mainly three
reasons for the phenomena: First, this is due to the
fact that user-generated content is inherently noisy
with low word frequency, resulting in the sparsity
of employing textual feature. Second, non-textual
features (e.g., answer length) usually own strongly
statistical properties and feature sparsity problem
can be better relieved to some extent. Finally, s-
ince correlations between the textual features and
non-textual features are highly non-linear, con-
catenating these features simply sometimes can
submerge classification performance. In contrast,
mDBN enjoys the advantage of the shared repre-
sentation between textual features and non-textual
features using the deep learning architecture.
We also note that neither the mDBN nor other
approaches perform very well in predicting answer
quality across the two datasets. The best precision
on ZHIDAO and YAHOO are respectively 59.0%
and 53.4%, which means that there are nearly half
of the high quality answers not effectively identi-
fied. One of the possible reason is that the quali-
ty of the corpora influences the result significant-
ly. As shown in Table 2, each question on aver-
age receives more than 4 answers on ZHIDAO and
more than 10 on YAHOO. Therefore, it is possi-
ble that there are several answers with high quali-
ty to the same question. Selecting only one as the
high quality answer is relatively difficult for our
humans, not to mention for the models.
100 500 1000 2000 5000
# iterations
0.50
0.55
0.60
0.65
0.70
0.75
0.80
Precision Recall F1 Accuracy
Figure 3: Influences of iterations for mDBN
In the second experiment, we intend to exam-
ine the performance of mDBN with different num-
ber of iterations. Figure 3 depicts the metrics on
ZHIDAO when the iteration number is varied from
100 to 5000. From the result, the first observa-
tion is that increasing the number of iterations the
performance of mDBN can improve significant-
ly, obtaining the best results for iteration of 1000.
This clearly shows the representation power of the
mDBN again. However, after a large number of it-
erations (large than 1000), the mDBN has a detri-
mental performance. This may be explained by
with large number of iterations, the deep learning
architecture is easier to be overfitting. The similar
trend is also observed on YAHOO.
5 Conclusions and Future work
In this paper, we have provided a new perspec-
tive to predict the cQA answer quality: learning
an informative unified representation between tex-
tual and non-textual features instead of concate-
nating them simply. Specifically, we have pro-
posed a multimodal deep learning framework to
846
form the unified representation. We compare this
with the basic features both in isolation and in
combination. Experimental results have demon-
strated that our proposed approach can capture the
complementarity between textual and non-textual
features, which is helpful to improve the perfor-
mance for cQA answer quality prediction.
For the future work, we plan to explore more se-
mantic analysis to approach the issue for short tex-
t quality evaluation. Additionally, more research
will be taken to put forward other approaches for
learning multimodal representation.
Acknowledgments
The authors are grateful to the anonymous re-
viewers for their constructive comments. Spe-
cial thanks to Chengjie Sun and Deyuan Zhang
for insightful suggestions. This work is supported
by National Natural Science Foundation of China
(NSFC) via grant 61272383 and 61100094.
References
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. 2008. Finding high-quality content in
social media. In Proceedings of the internation-
al conference on Web search and web data mining,
pages 183?194. ACM.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems, pages 153?160.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World wide
web, pages 51?60. ACM.
L. Chen and R. Nayak. 2008. Expertise analysis in a
question answer portal for author ranking. In Inter-
national Conference on Web Intelligence and Intel-
ligent Agent Technology, volume 1, pages 134?140.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
G.E. Hinton, S. Osindero, and Y.W. Teh. 2006. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18(7):1527?1554.
G.E. Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural compu-
tation, 14(8):1771?1800.
G.E. Hinton. 2012. A practical guide to training re-
stricted boltzmann machines. Lecture Notes in Com-
puter Science, pages 599?619.
Minlie Huang, Yi Yang, and Xiaoyan Zhu. 2011.
Quality-biased ranking of short texts in microblog-
ging services. In Proceedings of the 5th Internation-
al Joint Conference on Natural Language Process-
ing, pages 373?382.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with
non-textual features. In Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 228?235. ACM.
H. Lee, R. Grosse, R. Ranganath, and A.Y. Ng. 2009.
Convolutional deep belief networks for scalable un-
supervised learning of hierarchical representation-
s. In Proceedings of the 26th Annual International
Conference on Machine Learning, pages 609?616.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A.Y.
Ng. 2011. Multimodal deep learning. In Proceed-
ings of the 28th International Conference on Ma-
chine Learning (ICML), pages 689?696.
R. Salakhutdinov and G.E. Hinton. 2009. Deep boltz-
mann machines. In Proceedings of the internation-
al conference on artificial intelligence and statistics,
volume 5, pages 448?455.
C. Shah and J. Pomerantz. 2010. Evaluating and pre-
dicting answer quality in community qa. In Pro-
ceeding of the 33rd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 411?418.
N. Srivastava and R. Salakhutdinov. 2012. Multi-
modal learning with deep boltzmann machines. In
Advances in Neural Information Processing System-
s, pages 2231?2239.
B. Wang, X. Wang, C. Sun, B. Liu, and L. Sun. 2010.
Modeling semantic relevance for question-answer
pairs in web social communities. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1230?1238. ACL.
B. Wang, B. Liu, X. Wang, C. Sun, and D. Zhang.
2011. Deep learning approaches to semantic rele-
vance modeling for chinese question-answer pairs.
ACM Transactions on Asian Language Information
Processing, 10(4):21:1?21:16.
Z.M. Zhou, M. Lan, Z.Y. Niu, and Y. Lu. 2012. Ex-
ploiting user profile information for answer ranking
in cqa. In Proceedings of the 21st international con-
ference on World Wide Web, pages 767?774. ACM.
847
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 25?30,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
WINGS: Writing with Intelligent Guidance and Suggestions 
 
Xianjun Dai, Yuanchao Liu*, Xiaolong Wang, Bingquan Liu 
School of Computer Science and Technology 
Harbin Institute of Technology, China 
{xjdai, lyc, wangxl, liubq}@insun.hit.edu.cn 
 
 
 
Abstract 
Without inspirations, writing may be a 
frustrating task for most people. In this study, 
we designed and implemented WINGS, a 
Chinese input method extended on 
IBus-Pinyin with intelligent writing assistance. 
In addition to supporting common Chinese 
input, WINGS mainly attempts to spark users? 
inspirations by recommending both word 
level and sentence level writing suggestions. 
The main strategies used by WINGS, 
including providing syntactically and 
semantically related words based on word 
vector representation and recommending 
contextually related sentences based on LDA, 
are discussed and described. Experimental 
results suggest that WINGS can facilitate 
Chinese writing in an effective and creative 
manner. 
1 Introduction 
Writing articles may be a challenging task, as we 
usually have trouble in finding the suitable words 
or suffer from lack of ideas. Thus it may be very 
helpful if some writing reference information, 
e.g., words or sentences, can be recommended 
while we are composing an article. 
On the one hand, for non-english users, e.g., 
Chinese, the Chinese input method is our first 
tool for interacting with a computer. Nowadays, 
the most popular Chinese input methods are 
Pinyin-based ones, such as Sougou Pinyin1 and 
Google Pinyin 2 . These systems only present 
accurate results of Pinyin-to-Character 
conversion. Considering these systems? lack of 
suggestions for related words, they hardly 
provide writers with substantial help in writing. 
On the other hand, try to meet the need of writing 
assistance, more and more systems facilitating 
Chinese writing have been available to the public, 
                                                          
* Corresponding author 
1 http://pinyin.sogou.com 
2 http://www.google.com/intl/zh-CN/ime/pinyin 
such as WenXin Super Writing Assistant3 and 
BigWriter4, and among others. However, due to 
their shortcomings of building examples library 
manually and lack of corpus mining techniques, 
most of the time the suggestions made by these 
systems are not creative or contextual. 
  Thus, in this paper, we present Writing with 
INtelligent Guidance and Suggestions (WINGS)5, 
a Chinese input method extended with intelligent 
writing assistance. Through WINGS, users can 
receive intelligent, real-time writing suggestions, 
including both word level and sentence level. 
Different from existing Chinese writing assistants, 
WINGS mainly attempts to spark users? writing 
inspirations from two aspects: providing diverse 
related words to expand users? minds and 
recommending contextual sentences according to 
their writing intentions. Based on corpus mining 
with Natural Language Processing techniques, 
e.g., word vector representation and LDA model, 
WINGS aims to facilitate Chinese writing in an 
effective and creative manner. 
  For example, when using WINGS to type 
?xuxurusheng?, a sequence of Chinese Pinyin 
characters for ?????? (vivid/vividly), the 
Pinyin-to-Character Module will generate ???
??? and some other candidate Chinese words. 
Then the Words Recommending Module 
generates word recommendations for ????
? ?. The recommended words are obtained 
through calculating word similarities based on 
word vector representations as well as rule-based 
strategy (POS patterns). 
In the Sentences Recommending Module, we 
first use ????? ? to retrieve example 
sentences from sentences library. Then the topic 
similarities between the local context and the 
candidate sentences are evaluated for contextual 
                                                          
3 http://www.xiesky.com 
4 http://www.zidongxiezuo.com/bigwriter_intro.php 
5 The DEB package for Ubuntu 64 and recorded video of 
our system demonstration can be accessed at this URL: 
http://yunpan.cn/Qp4gM3HW446Rx (password:63b3) 
25
Chinese Pinyin Sequence
Recommended Words
Recommended Sentences
Pinyin-to-Character results (Original Words)
 
Figure 1. Screenshot of WINGS.  
 
sentence recommendations. 
At last in consideration of users? feedback, we 
introduce a User Feedback Module to our system. 
The recorded feedback data will in turn influence 
the scores of words and sentences in 
Recommending Modules above. 
Figure 1 shows a screenshot of WINGS. 
2 Related Work 
2.1 Input Method 
Chinese input method is one of the most 
important tools for Chinese PC users. Nowadays, 
Pinyin-based input method is the most popular 
one. The main strategy that Pinyin-based input 
method uses is automatically converting Pinyin 
to Chinese characters (Chen and Lee, 2000).  
In recent years, more and more intelligent 
strategies have been adopted by different input 
methods, such as Triivi 6 , an English input 
method that attempts to increase writing speed 
by suggesting words and phrases, and PRIME 
(Komatsu et al., 2005), an English/Japanese 
input system that utilizes visited documents to 
predict the user?s next word to be input. 
In our system the basic process was Pinyin ? 
Characters (words) ? Writing Suggestions 
(including words and sentences). We mainly 
focused on writing suggestions from Characters 
(words) in this paper. As the Pinyin-to-Character 
was the underlining work, we developed our 
system directly on the open source framework of 
the IBus (an intelligent input Bus for Linux and 
Unix OS) and IBus-Pinyin7 input method. 
2.2 Writing Assistant 
As previously mentioned, several systems are 
available in supporting Chinese writing, such as 
WenXin Super Writing Assistant and Big Writer. 
                                                          
6 http://baike.baidu.com/view/4849876.htm 
7 https://code.google.com/p/ibus 
These systems are examples of a retrieval-based 
writing assistant, which is primarily based on a 
large examples library and provides users with a 
search function. 
In contrast, other writing assistants employ 
special NLP strategies. Liu et al. (2011, 2012) 
proposed two computer writing assistants: one 
for writing love letters and the other for blog 
writing. In these two systems, some special 
techniques were used, including text generation, 
synonym substitution, and concept expansion. 
PENS (Liu et al., 2000) and FLOW (Chen et al., 
2012) are two writing assistants designed for 
students of English as a Foreign Language (EFL) 
practicing writing, which are mainly based on 
Statistical Machine Translation (SMT) strategies. 
Compared with the above mentioned systems, 
WINGS is closer to retrieval-based writing 
assistants in terms of function. However, WINGS 
can provide more intelligent suggestions because 
of the introduction of NLP techniques, e.g., word 
vector representation and topic model. 
2.3 Word Representations in Vector Space 
Recently, Mikolov et al. (2013) proposed novel 
model architectures to compute continuous 
vector representations of words obtained from 
very large data sets. The quality of these 
representations was assessed through a word 
similarity task, and according to their report, the 
word vectors provided state-of-the-art 
performance for measuring syntactic and 
semantic word similarities in their test set. Their 
research produced the open source tool 
word2vec8. 
In our system, we used word2vec to train the 
word vectors from a corpus we processed 
beforehand. For the Words Recommending 
Module, these vectors were used to determine the 
similarity among different words. 
                                                          
8 https://code.google.com/p/word2vec 
26
2.4 Latent Dirichlet Allocation 
The topic model Latent Dirichlet Allocation 
(LDA) is a generative probabilistic model of a 
corpus. In this model, documents are represented 
as random mixtures of latent topics, where each 
topic is characterized by the distribution of 
words (Blei et al., 2003). Each document can 
thus be represented as a distribution of topics. 
Gibbs Sampling is a popular and efficient 
strategy used for LDA parameter estimation and 
inference. This technique is used in 
implementing several open sourcing LDA tools, 
such as GibbsLDA++9 (Phan and Nguyen, 2007), 
which was used in this paper. 
In order to generate contextual sentence 
suggestions, we ensured that the sentences 
recommended to the user were topic related to 
the local context (5-10 words previously input) 
based on the LDA model.  
3 Overview of WINGS 
Figure 2 illustrates the overall architecture of 
WINGS.  
Start
Pinyin to Character
Convert pinyin to Chinese words 
(Original words)
Words Recommending 1
1. Calculate similarity between focused 
original word and the rest words in the 
dictionary
2. Get top 200 most similar words as 
the candidate words
Words and word 
vectors
Sentences 
index
Sentences Recommending 1
Use the focused original or 
recommended word to retrieve at most  
200 sentences by Clucene from 
sentences index.
S ntenc s and 
their 
topic vector 
Sentences Recommending 2
1. Infer the topic vector of the local 
context by Gibbs Sammpling. Calculate 
the KL divergence between the local 
context and candidate sentences.
2. The sentence has been used before 
will get a boost in score.
1. Select word or sentence as input
2. Save feedback(User Feedback)
LDA train result 
for inference
Input Pinyin
Pinyin-Character 
mapping data,etc.
Words and 
s ntences 
selected info
YES
End
Continue
NO
Words Recommending 2
1.Boost in score: 1).Whether the 
original and recommended word 
match one of the specified patterns, 
such as A-N, V-N and etc. 2). Whether 
The word has been used before
2. Re-rank candidate words.
 
Figure 2. Overall architecture of WINGS. 
3.1 System Architecture 
Our system is composed of four different 
                                                          
9 http://gibbslda.sourceforge.net 
modules: Pinyin-to-Character Module, Words 
Recommending Module, Sentences 
Recommending Module, and User Feedback 
Module. The following sub-sections discuss 
these modules in detail. 
3.2 Pinyin-to-Character Module 
Our system is based on the open sourcing input 
framework IBus and extended on the 
IBus-Pinyin input method. Thus, the 
Pinyin-to-Character module is adopted from the 
original IBus-Pinyin system. This module 
converts the input Chinese Pinyin sequence into 
a list of candidate Chinese words, which we refer 
to as original words. 
3.3 Words Recommending Module 
? Words vector representations 
In this preparatory step for word 
recommendation, words vector representations 
are obtained using the word2vec tool. This will 
be described in detail in Section 4. 
? Obtain the most related words 
Our system will obtain the focused original 
word and calculate the cosine similarities 
between this word and the rest of the words in 
the dictionary. Thus, we can obtain the top 200 
most similar words according to their cosine 
values. These words are referred to as 
recommended words. According to Mikolov et 
al. (2013), these words are syntactically and 
semantically similar to the original word. 
? Re-rank the recommended words 
In order to further improve word recommending, 
we introduce several special POS patterns (Table 
1). If the POS of the original word and the 
recommended word satisfy one of the POS 
patterns we specified, the score (based on the 
cosine similarity) of the recommended word will 
be boosted. In addition, the score of the word 
selected by the user before will also be boosted. 
Therefore, these words will be ranked higher in 
the recommended words list. 
POS of  
original word 
POS of  
recommended word 
N (noun) A (adjective) 
A (adjective) N (noun) 
N (noun) V (verb) 
Any POS Same with the original word 
Any POS L (idiom) 
Table 1. Special POS patterns. 
3.4 Sentences Recommending Module 
? Sentences topic distribution 
In this preparatory step for sentence 
27
recommendation, sentences topic distribution 
vectors and other parameters are trained using 
the GibbsLDA++. This step will be discussed in 
Section 4. 
? Retrieve relative sentences via CLucene 
The focused original or recommended word will 
be used to search the most related sentences in 
the sentences index via CLucene10. At most 200 
sentences will be taken as candidates, which will 
be called recommended sentences. 
? Re-rank the recommended sentences 
To ensure that the recommended sentences are 
topic related to our local input context (5-10 
words previously input), we use Gibbs Sampling 
to infer the topic vector of the local context, and 
calculate the KL divergence between the local 
context and each recommended sentence. Finally, 
the recommended sentences will be re-ranked 
based on their KL divergences value with respect 
to the local context and the boost score derived 
from the feedback information. 
3.5 User Feedback Module 
This module saves the users? feedback 
information, particularly the number of times 
when users select the recommended words and 
sentences. This information will be used as a 
boost factor for the Words and Sentences 
Recommending Modules. Our reasons for 
introducing this module are two-fold: the users? 
feedback reflects their preference, and at the 
same time, this information can somewhat 
indicate the quality of the words and sentences. 
4 Data Pre-processing 
In this section, the procedure of our data 
pre-processing is discussed in detail. Firstly, our 
raw corpus was crawled from DiYiFanWen11, a 
Chinese writing website that includes all types of 
writing materials. After extracting useful 
composition examples from each raw html file, 
we merged all articles into a single file named 
large corpus. Finally, a total of 324,302 articles 
were merged into the large corpus (with a total 
size of 320 MB). 
For words recommending, each of the articles 
in our large corpus was segmented into words by 
ICTCLAS 12  with POS tags. Subsequently, 
word2vec tool was used on the words sequence 
(with useless symbols filtered). Finally, the 
words, their respective vector representations and 
                                                          
10 http://sourceforge.net/projects/clucene 
11 http://www.diyifanwen.com 
12 http://ictclas.nlpir.org 
main POS tags were combined, and we built 
these data into one binary file. 
For sentences recommending, the large corpus 
was segmented into sentences based on special 
punctuations. Sentences that were either too long 
or too short were discarded. Finally, 2,567,948 
sentences were left, which we called original 
sentences. An index was created on these 
sentences using CLucene. Moreover, we 
segmented these original sentences and filtered 
the punctuations and stop words. Accordingly, 
these new sentences were named segmented 
sentences. We then ran GibbsLDA++ on the 
segmented sentences, and the Gibbs sampling 
result and topic vector of each sentence were 
thus obtained. Finally, we built the original 
sentence and their topic vectors into a binary file. 
The Gibbs sampling data used for inference was 
likewise saved into a binary file. 
  Table 2 lists all information on the resources 
of WINGS.  
Items Information 
Articles corpus size 320 MB 
Articles total count 324,302 
Words total count 101,188 
Sentences total count 2,567,948 
Table 2. Resources information. 
5 Experimental Results 
This section discusses the experimental results of 
WINGS. 
5.1 Words Recommending 
The top 20 recommended words for the sample 
word ???? (teacher) are listed in Table 3. 
Compared with traditional methods (using Cilin, 
Hownet, and so forth.), using the word vectors to 
determine related words will identify more 
diverse and meaningful related words and this 
quality of WINGS is shown in Table 4. With the 
diversity of recommended words, writers? minds 
can be expanded easily.  
1-10: ??(student), ??(conduct class), ??
?(Chinese class), ????(with sincere words 
and earnest wishes), ????(affability), ??
(guide), ?? (lecture), ?? (dais), ????
(patient), ??(the whole class) 
11-20: ??(finish class), ???(remarks), ?
??(math class), ???(be absent-minded), ?
? (ferule), ??? (class adviser), ????
(restless), ??(remember), ????????
(excel one?s master), ??(listen to) 
Table 3. Top 20 recommended words for ???? 
(teacher). 
28
Words about Words 
Person ??, ???, ?? 
Quality ????, ????, ?
??? 
Course ???, ??? 
Teaching ??, ??, ??, ?? 
Teaching facility ??, ?? 
Student behaviour ??, ???, ???? 
Special idiom ???????? 
Others ??, ??? 
Table 4. Diversity of recommended words for 
???? (teacher). 
5.2 Sentences Recommending 
By introducing the topic model LDA, the 
sentences recommended by WINGS are related to 
the topic of the local context. Table 5 presents 
the top 5 recommended sentences for the word 
?????? (vivid/vividly) in two different local 
contexts: one refers to characters in books; the 
other refers to statues and sculptures. Most 
sentences in the first group are related to the first 
context, and most from the second group are 
related to the second context. 
In order to assess the performance of WINGS 
in sentence recommendation, the following 
evaluation was implemented. A total of 10 
Chinese words were randomly selected, and each 
word was given two or three different local 
contexts as above (contexts varied for different 
words). Finally, we obtained a total of 24 groups 
of data, each of which included an original word, 
a local context, and the top 10 sentences 
recommended by WINGS. To avoid the influence 
of personal preferences, 12 students were invited 
to judge whether each sentence in the 24 
different groups was related to their respective 
local context. We believed that a sentence was 
related to its context only when at least 70% of 
the evaluators agreed. The Precision@10 
measure in Information Retrieval was used, and 
the total average was 0.76, as shown in Table 6. 
Additionally, when we checked the sentences 
which were judged not related to their respective 
local context, we found that these sentences were 
generally too short after stop words removal, and 
as a result the topic distributions inferred from 
Gibbs Sampling were not that reliable. 
Context 1 is about characters in books:  
?? (story), ?? (character), ?? (image), 
??(works) 
1??????????????? 
(The characters of this book are depicted 
vividly) 
2???????????????????
?  
(The characters of this book are depicted vividly 
and the story is impressive narrative) 
3????????????  
(The characters of this story are depicted 
vividly) 
4???????????????????
??? 
(His works are full of plot twists, vivid 
characters, and surprising endings) 
5??????????????????  
(The characters in the book are depicted vividly 
by Jing Zhuge) 
Context 2 is about statues and sculptures:  
?? (statue), ?? (sculpture), ?? (stone 
inscription), ??(temple) 
1??????????????  
(The walls are painted with mighty and vivid 
dragons) 
2????????????????  
(On both sides there are standing 18 vivid Arhats 
with different manners) 
3?????????????????  
(the Great Buddha Hall is grand and the statues 
there are vivid) 
4????????????  
(Each statue is vivid and lifelike) 
5???????????????????
??????  
(On each of the eave angles there are 7 vivid 
statues of animals and birds with special 
meanings) 
Table 5. Top 5 recommended sentences for ??
???? (vivid/vividly) in two different local 
contexts.  
 
 
Local 
Context 
word 
1 
word 
2 
word 
3 
word 
4 
word 
5 
word 
6 
word 
7 
word 
8 
word 
9 
word 
10 
1 0.9 0.3 0.9 0.6 0.7 0.8 0.6 0.8 1.0 0.9 
2 0.4 0.7 1.0 0.9 0.9 0.7 1.0 0.5 0.9 0.5 
3 0.9 N/A N/A N/A N/A 0.9 0.8 N/A N/A 0.7 
Average Precision@10 value of the 24 groups data                0.76 
Table 6. Precision@10 value of each word under their respective context and the total average. 
29
5.3 Real Time Performance 
In order to ensure the real time process for each 
recommendation, we used CLucene to index and 
retrieve sentences and memory cache strategy to 
reduce the time cost of fetching sentences? 
information. Table 7 shows the average and max 
responding time of each recommendation of 
randomly selected 200 different words (Our test 
environment is 64-bit Ubuntu 12.04 LTS OS on 
PC with 4GB memory and 3.10GHz Dual-Core 
CPU). 
 
Item Responding time 
Average 154 ms 
Max 181 ms 
Table 7. The average and max responding time 
of 200 different words? recommending process 
6 Conclusion and Future Work 
In this paper, we presented WINGS, a Chinese 
input method extended with writing assistance 
that provides intelligent, real-time suggestions 
for writers. Overall, our system provides 
syntactically and semantically related words, as 
well as recommends contextually related 
sentences to users. As for the large corpus, on 
which the recommended words and sentences are 
based, and the corpus mining based on NLP 
techniques (e.g., word vector representation and 
topic model LDA), experimental results show 
that our system is both helpful and meaningful. 
In addition, given that the writers? feedback is 
recorded, WINGS will become increasingly 
effective for users while in use. Thus, we believe 
that WINGS will considerably benefit writers. 
  In future work, we will conduct more user 
experiments to understand the benefits of our 
system to their writing. For example, we can 
integrate WINGS into a crowdsourcing system 
and analyze the improvement in our users? 
writing. Moreover, our system may still be 
improved further. For example, we are interested 
in adding a function similar to Google Suggest, 
which is based on the query log of the search 
engine, in order to provide more valuable 
suggestions for users. 
 
 
 
 
 
 
 
 
References 
David M. Blei, Andrew Y. Ng and Michael I. Jordan. 
2003. Latent dirichlet allocation. the Journal of 
machine Learning research, 3, pages 993-1022. 
Mei-Hua Chen, Shih-Ting Huang, Hung-Ting Hsieh, 
Ting-Hui Kao and Jason S. Chang. 2012. FLOW: a 
first-language-oriented writing assistant system. In 
Proceedings of the ACL 2012 System 
Demonstrations, pages 157-162. 
Zheng Chen and Kai-Fu Lee. 2000. A new statistical 
approach to Chinese Pinyin input. In Proceedings 
of the 38th annual meeting on association for 
computational linguistics, pages 241-247. 
Hiroyuki Komatsu, Satoru Takabayash and Toshiyuki 
Masui. 2005. Corpus-based predictive text input. In 
Proceedings of the 2005 international conference 
on active media technology, pages 75?80. 
Chien-Liang Liu, Chia-Hoang Lee, Ssu-Han Yu and 
Chih-Wei Chen. 2011. Computer assisted writing 
system. Expert Systems with Applications, 38(1), 
pages 804-811. 
Chien-Liang Liu, Chia-Hoang Lee and Bo-Yuan Ding. 
2012. Intelligent computer assisted blog writing 
system. Expert Systems with Applications, 39(4), 
pages 4496-4504. 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and 
Changning Huang. 2000. PENS: A machine-aided 
English writing system for Chinese users. In 
Proceedings of the 38th Annual Meeting on 
Association for Computational Linguistics, pages 
529-536. 
Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey 
Dean. 2013. Efficient estimation of word 
representations in vector space. arXiv:1301.3781. 
Xuan-Hieu Phan and Cam-Tu Nguyen. 2007. 
GibbsLDA++: A C/C++ implementation of latent 
Dirichlet allocation (LDA). 
30
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 100?105,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Detect Hedges and their Scope Using CRF 
Qi Zhao, Chengjie Sun, Bingquan Liu, Yong Cheng 
Harbin Institute of Technology, HIT 
Harbin, PR China 
{qzhao, cjsun, liubq, ycheng}@insun.hit.edu.cn 
 
Abstract 
Detecting speculative assertions is essential 
to distinguish the facts from uncertain 
information for biomedical text. This paper 
describes a system to detect hedge cues and 
their scope using CRF model. HCDic feature 
is presented to improve the system perfor-
mance of detecting hedge cues on BioScope 
corpus. The feature can make use of cross-
domain resources.  
1 Introduction 
George Lakoff (1972) first introduced linguistic 
hedges which indicate that speakers do not back 
up their opinions with facts. Later other linguists 
followed the social functions of hedges closely. 
Interestingly, Robin Lakoff (1975) introduces 
that hedges might be one of the ?women?s 
language features? as they have higher frequency 
in women?s languages than in men?s. 
In the natural language processing domain, 
hedges are very important, too. Along with the 
rapid development of computational and 
biological technology, information extraction 
from huge amount of biomedical resource 
becomes more and more important. While the 
uncertain information can be a noisy factor 
sometimes, affecting the performance of 
information extraction. Biomedical articles are 
rich in speculative, while 17.70% of the 
sentences in the abstracts section of the 
BioScope corpus and 19.44% of the sentences in 
the full papers section contain hedge cues 
(Vincze et al, 2008). In order to distinguish facts 
from uncertain information, detecting speculative 
assertions is essential in biomedical text.  
Hedge detection is paid attention to in the 
biomedical NLP field. Some researchers regard 
the problem as a text classification problem (a 
sentence is speculative or not) using simple 
machine learning techniques. Light et al (2004) 
use substring matching to annotate speculation in 
biomedical text. Medlock and Briscoe (2007) 
create a hedging dataset and use an SVM 
classifier and get to a recall/precision Break-
Even Point (BEP) of 0.76. They report that the 
POS feature performs badly, while lemma 
feature works well. Szarvas (2008) extends the 
work of Medlock and Briscoe with feature 
selection, and further improves the result to a 
BEP of 0.85 by using an external dictionary. 
Szarvas concludes that scientific articles contain 
multiword hedging cues more commonly, and 
the portability of hedge classifiers is limited. 
Halil Kilicoglu and Sabine Bergler (2008) 
propose an algorithm to weight hedge cues, 
which are used to evaluate the speculative 
strength of sentences. Roser Morante and Walter 
Daelemans (2009) introduce a metalearning 
approach to process the scope of negation, and 
they identify the hedge cues and their scope with 
a CRF classifier based on the original work. 
They extract a hedge cues dictionary as well, but 
do not combine it with the CRF model. 
In the CoNLL-2010 shared task (Farkas et al, 
2010), there are two subtasks for worldwide 
participants to choose: 
? Task 1: learning to detect sentences 
contain-ing uncertainty.  
? Task 2: learning to resolve the in-
sentence scope of hedge cues.  
This paper describes a system using CRF 
model for the task, which is partly based on 
Roser Morante and Walter Daelemans? work. 
2 Hedges in the training dataset of 
BioScope and Wikipedia Corpus 
Two training datasets, the BioScope and Wiki-
pedia corpus are provided in the CoNLL-2010 
shared task. BioScope consists of two parts, full 
articles and abstracts collected from biomedical 
papers. The latter is analyzed for having larger 
scale and more information of hedges.  
In Table 1, the percentage of the speculative 
sentences in the abstracts section of BioScope 
corpus is the same as Vincze et al (2008) 
reported. We can estimate 1.28 cue words per 
sentence, meaning that each sentence usually just 
has one hedge cue. The statistics in Table 1 also 
100
indicate that a hedge cue appears 26.7 times on 
average. 
 
Dataset ITEM # 
Sentences 11871 
Certain sentences 9770 
Uncertain 
sentences 
2101 
(17.7%) 
Hedge cues 2694 
cues# per sentence 1.28 
Different hedge 
cues 
143 
Abstracts 
of 
BioScope 
Max length of the 
cues 
4 
Sentences 11111 
Certain sentences 8627 
Uncertain 
sentences 
2484 
(22.4%) 
weasel cues 3133 
Different weasel 
cues 
1984 
Wikipedia 
Max length of the 
cues 
13 words 
 
Table 1: Statistics about the abstracts section of 
the BioScope corpus and Wikipedia corpus. 
 
We extract all the hedge cues from the 
abstracts section of BioScope corpus, getting 143 
different hedge cues and 101 cues with ignoring 
morphological changes. The maximum length of 
the cues is 4, with 1.44 words per hedge cue. 
This suggests that most hedge cues happen to be 
a single word. We assume that hedge cues set is 
a limited one in BioScope corpus. Most hedge 
cues could be identified if the known dataset of 
hedge cues is large enough. The cue words 
collected from the BioScope corpus play an 
important role in the speculative sentences 
detection. 
In contrast to the biomedical abstracts, the 
weasel cues on Wikipedia corpus make a little 
difference. Most weasel cues consist of more 
than one word, and usually appear once. This 
leads to different results in our test. 
A hedge cue word may appear in the non-
speculative sentences. Occurrences of the four 
typical words in speculative and non-speculative 
sentences are counted. 
As shown in Table 2, the cue words can be 
divided into two classes generally. The hedge 
cue words ?feel? and ?suggesting?, which are 
grouped as one class, only act as hedge cues with 
never appearing in the non-speculative sentences. 
While ?may? and ?or? appear both in the 
speculative and non-speculative sentences, which 
are regard as the other one. Moreover, we treat 
the words ?may? and ?or? in the same class 
differently, while ?may? is more likely to be a 
hedge cue than ?or?. The treatment is also 
unequal between ?feel? and ?suggesting?. In the 
training datasets, the non-S#/S# ratio can give a 
weight to distinguish the words in each class. 
After all, we can divide the hedge cues into 4 
groups. 
 
word S# non-S# 
feel 1 0 
suggesting 150 0 
may 516 1 
or 118 6218 
 
Table 2: Statistics of cue words. (S# short for the 
occurrence times in speculative sentences, non-
S# for the count in non-speculative ones) 
3 Methods 
Conditional random fields (CRF) model was 
firstly introduced by Lafferty et al (2001). CRF 
model can avoid the label bias problem of 
HMMs and other learning approaches. It was 
applied to solve sequence-labeling problems, and 
has shown good performance in NER task. We 
consider hedge cues detection as some kind of 
sequence-labeling problem, and the model will 
contribute to a good result.  
We use CRF++ (version 0.51) to implement 
the CRF model. Cheng Yong, one of our team 
members has evaluated the several widespread 
used CRF tool kits, and he points out that 
CRF++ has better precision and recall but longer 
training time. Fortunately, the training time cost 
of BioScope corpus is acceptable. In our system, 
all the data training and testing processing step 
can be completed within 8 minutes (Intel Xeon 
2.0GHz CPU, 6GB RAM). It is likely due to the 
small scale of the training dataset and the limited 
types of the annotation. 
To identify sentences in the biomedical texts 
that contain unreliable or uncertain information 
(CoNLL-2010 shared task1), we start with hedge 
cues detection: 
? If one or more than one hedge cues are 
detected in the sentence, then it will be 
annotated ?uncertain? 
? If not, the sentence will be tagged as 
?certain?. 
101
3.1 Detecting hedge cues 
The BioScope corpus annotation guidelines 1 
show that most typical instances of keywords can 
be grouped into 4 types as Auxiliaries, Verbs of 
hedging or verbs with speculative content, 
Adjectives or adverbs, and Conjunctions. So the 
POS (part-of-speech) is thought to be the feature 
reasonably. Lemma feature of the word and 
chunk features are also considered to improve 
system performance. Chunk features may help to 
the recognition of biomedical entity boundaries. 
GENIA Tagger (Tsuruoka et al, 2005) is em-
ployed to obtain part-of-speech (POS) features, 
chunk features and lemma features. It works well 
for biomedical documents. 
In the biomedical abstracts section of Bio-
Scope corpus, the hedge cues are collected into a 
dictionary (HCDic, short for the Hedge Cues 
Dictionary). As mentioned in section 2, one 
hedge cue appears 26.7 times on average, and we 
assume the set of hedge cues is limited. The 
HCDic consist of 143 different hedge cues 
extracted from the abstracts. The dictionary 
(HCDic) extracted from the corpus is very 
valuable for the system. We can focus on 
whether the word such as ?or? listed in table 2 is 
a hedge cue or not. The cue words in HCDic are 
divided into 4 different levels with the non-S#/S# 
ratio. 
The four types are described as ?L?, ?H?, 
?FL? and ?FH?. ?L? shows low confidence of 
the cue word being a hedge cue, while ?H? 
indicates high confidence about it. The prefix ?F? 
for ?FL?/?FH? shows false negatives may 
happen to the cue word in HCDic. The threshold 
for the non-S#/S# ratio to distinguish ?FL? type 
from ?FH? is set 1.0. As the non-S#/S# ratio of 
?L? and ?H? is always zero, we set the hedge cue 
whose S# is more than 5 as ?H? type as shown in 
table 3. The four types are added into the HCDic 
along with the hedge cues,  
In our experiment, HCDic types of word 
sequence are tagged as follows: 
? If words are found in HCDic using 
maximum matching method, label them 
with their types in HCDic. For hedges of 
multi-word, label them with BI scheme 
which will be described later. 
? If not, tag the words as ?O? type.  
                                                 
1
 http://www.inf.u-szeged.hu/rgai/bioscope 
The processing assigns each token of a 
sentence with an HCDic type. The BIO types for 
each token are involved as features for the CRF. 
The HCDic can be expanded to a larger scale. 
Hedge cues extracted from different corpora can 
be added into HCDic, and regular expression of 
hedge cues can be used, too. This will be helpful 
to the usage of cross-domain resources. 
 
word S# non-S# type  
feel 1 0 L 
suggesting 150 0 H 
may 516 1 FH 
or 118 6218 FL 
 
Table 3: Types of the HCDic words. (S# and 
non-S# have the same meaning as in Table 2) 
 
The features F (F stands for all the Features) 
including unigram, bigram, and trigram types is 
used for CRF as follows: 
 
F(n)(n=-2,-1,0,+1,+2) 
F(n-1)F(n)(n=-1,0,+1,+2) 
F(n-2)F(n-1)F(n) (n=0,+1,+2) 
Where F(0) is the current feature, F(-1) is the 
previous one, F(1) is the following one, etc. 
 
We regard each word in a sentence as a token 
and each token is tagged with a cue-label. The 
BIO scheme is used for tagging multiword hedge 
cues, such as ?whether or not? in our HCDic. 
where B-cue (tag for ?whether?) represents that 
the token is the start of a hedge cue, I-cue (tag 
for ?or?, ?not?) stands for the inside of a hedge 
cue, and O (tag for the other words in the 
sentence) indicates that the token does not 
belong to any hedge cue. 
We also have the method tested on Wikipedia 
corpus with a preprocessing of the HCDic. 
Section 2 reports that most weasel cues in 
Wikipedia corpus are multiword, and usually 
appear once. Different from our assumption in 
BioScope corpus, the set of weasel cues seems 
numerous. The HCDic of Wikipedia would be 
not so valuable if it tags few tokens for a new 
given text. To prevent these from happening, a 
preprocessing of the HCDic is taken. 
Most of the hedge cues in Wikipedia corpus 
accord with the structure of ?adjective + noun? 
e.g. ?many persons?. Although most cue words 
appear just once, the adjective usually happens to 
be the same, and we call them core words. 
Therefore, the hedge cue dictionary (HCDic) can 
be simplified with the core words. It helps to 
102
reduce the scale of the hedges cues from 1984 
cues down to 170. Then, we process the 
Wikipedia text the same way as the BioScope 
corpus. 
3.2 Detecting scope of hedge cues  
This phase (for CoNLL-2010 shared task 2) is 
based on Roser Morante and Walter Daelemans? 
scope detection system. 
CRF model is applied in this part, too. The 
word, POS, lemma, chunk and HCDic tags are 
also applied to be the features as in the step of 
hedge cues detection. In section 3.1, we can 
obtain the hedge cues in a sentence. The scope 
relies on its cue vary much. We make the BIO 
schema of detected hedge cues to be the 
important features of this part. Besides, the 
sentences tagged as ?certain? type are neglected 
in this step. 
Here is an example of golden standard of 
scope label.  
 
<sentence id="S5.149"> We <xcope id="X5.149. 
3"><cue ref="X5.149.3" type= "specula-tion"> 
propose </cue> that IL-10-producing Th1 cells 
<xcope id="X5.149.2"> <cue ref="X5.149.2" 
type= "speculation" >may</cue> be the essential 
regulators of acute infection-induced inflammation 
</xcope> and that such ?self-regulating? Th1 cells 
<xcope id= "X5.149.1"> <cue ref= "X5.149.1" 
type= "speculation" >may</cue> be essential for 
the infection to be cleared without inducing 
immune-mediated pathology </xcope> </xcope>. 
 
As shown, each scope is a block with a 
beginning and an end, and we refer to the 
beginning of scope as scope head (<xcope?>), 
and the end of the scope as scope tail 
(</xcope>). 
The types of the scope are labeled as: 
 
1. Label the token next to scope head as 
?xcope-H? ( e.g. propose, may ) 
2. Tag the token before scope tail as ?xcope-
T?(e.g. pathology for both scopes)  
3. The other words tag ?O? , including the 
words inside the scope and out of it. This 
is very different from the BIO scheme. 
 
The template for each feature is the same as in 
section 3.1. 
Following are our rules to form the scope of a 
hedge: 
 
1. Most hedge cues have only one scope tag, 
meaning there is one-to-one relationship 
between hedge cue and its scope. 
2. The scope labels may be nested. 
3. The scope head of the cue words appears 
nearest before hedge cue. 
4. The scope tail appears far from the cue 
word. 
5. The most frequent head/tail positions of the 
scope are shown in Table 4. 
a) The scope head usually is just before 
the cue words. 
b) The scope tail appears in the end of the 
sentence frequently. 
 
Scopes of hedge cues in BioScope corpus 
should be found for the shared task. The training 
dataset of abstract part is analyzed for its larger 
scale  
 
item Following strings  
with high frequency % 
1 
scope 
head 
<cue...>(cue words) 0.861 
?.?(sentence end) 0.695 
</xcope> 
(another scope tail) 0.144 
2 
scope 
tail 
?,?  ?;?  ?:? 0.078 
 
Table 4: Statistics of the strings nearby the scope 
head and tail. Item 1 shows the word follow 
scope head, and item 2 shows the frequent words 
next to the scope tail. 
 
We analyze the words around the scope head 
and the scope tail. The item 1 in Table 4 shows 
that 86.1% of the following words of the scope 
head are hedge cues. Other following words not 
listed are less than 1%, according to our 
statistics. The item 2 lists the strings with high 
frequency next to the scope tail as well. The first 
2 words in item 2 can be combined sometimes, 
so the percentage of scope tail at the end of the 
sentence can be more than 80%. The strings 
ahead of scope head and tail not listed are also 
counted, but they do not give such valuable 
information as the two items listed in Table 4. 
Therefore, when the CRF model gives low 
confidence, we just set the most probable 
positions of scope head and tail. 
For the one-to-one relationship between hedge 
cues and their scopes, we make rules to insure 
each cue has only one scope, including the scope 
head and scope tail. 
103
Rule 1: if more than one scope heads or tails 
are predicted, we get rid of the farther head or 
nearer tail. 
Rule 2: if none of scope head or tail is pre-
dicted, the head is set to the word just before the 
cue words; the tail is set at the end of the 
sentence. 
Rule 3: if one scope head and one tail are 
predicted, we consider them the result of scope 
detection. 
4 Results 
Our experiments are based on the CoNLL-2010 
shared task?s datasets, including BioScope and 
Wikipedia corpus. All the experiments for 
BioScope use abstracts and full papers for 
training data and the provided evaluation for 
testing. 
We employ CRF model to detect the hedge 
cues in the BioScope. The experiments are 
carried out on different feature sets: words 
sequence with the chunk feature only, lemma 
feature only and POS feature only. The effect of 
the HCDic feature is also evaluated. 
 
Features prec. recall F-score 
Chunk only 0.7236 0.6275 0.6721 
Lemma only 0.7278 0.6103 0.6639 
POS only 0.7320 0.6208 0.6718 
Without 
HCDic 
0.7150 0.6447 0.6781 
ALL 0.7671 0.7393 0.7529 
 
Table 5: Results at hedge cue-level 
 
As described in section 1 of this paper, the 
feature of POS may be not so significant as the 
lemma, but we do not agree with this point of 
view for given POS feature's better performance 
in F-score (in Table 5). The interesting cue-level 
result does not go into for time limitations. The 
F-score of the three features, chunk, lemma and 
POS are approximately equal. When all of the 
three features are used for CRF model, the 
performance is not improved so significantly. 
The recall rate is a bit low in the experiment 
without HCDic features. As shown in Table 5, 
the feature of HCDic is effective to get a better 
score both in precision rate and in recall rate. As 
our assumption, hedges in the evaluation dataset 
are limited, too. Most of them along with some 
non-hedges can be tagged with HCDic. Then the 
tag could contribute to a good recall. It also helps 
the classifier to focus on whether the words with 
?L?, ?FL?, and ?FH? are hedge cues or not, 
which will be good for a better precision. 
With detected hedge cues, we can get senten-
ces containing uncertainty for the shared task 1. 
A sentence is tagged as ?uncertain? type if any 
hedge cue is found in it.  
 
 precision recall F-score 
Without 
HCDic 0.8965 0.7898 0.8398 
ALL  0.8344 0.8481 0.8412 
 
Table 6: Evaluation result of task 1 
 
Statistics in Table 6 show that even poor 
performance in cue-level test can get a 
satisfactory F-score of speculative sentences 
detection as well. It seems that hedges detection 
at cue-level is not proportionate to the sentence-
level. Think about instance of more than one 
cues in a sentence such as the example of golden 
standard in section 3.2, the sentence will be 
tagged even if only one hedge cue has been 
identified (lower recall at cue-level). Moreover, 
in the speculative sentence with one hedge cue, 
false positives (lower precision at cue-level) can 
also lead to the correct result at sentence-level. 
The method is also tested on Wikipedia corpus, 
using provided training dataset and evaluation 
data. The method has a bad performance in our 
close test. The results are listed in Table 7. 
As talked in section 2, hedges in Wikipedia 
corpus are very different from in BioScope 
corpus. Besides, the string matching method for 
simplified HCDic is not so effective. The useful-
ness of HCDic is not so significant for a good 
recall in Wikipedia corpus.  
 
dataset precision recall F-score 
Wikipedia 0.7075 0.2001 0.3120 
BioScope 0.7671 0.7393 0.7529 
 
Table 7: Results of weasel/hedge detection in 
Wikipedia and BioScope corpus. 
 
In CoNLL-2010 shared task 2, the evaluation 
result shows our precision, recall and F-score are 
34.8%, 41% and 37.6%. The performance of 
identifying the scope relies on the cue-level 
detection. Therefore, the false positive and false 
negatives of hedge cues can lead to recognition 
errors. The result shows that our lexical-level 
method for the semantic problem is limited. For 
the time constraints, we do not probe deeply. 
104
5 Conclusions 
This paper presents an approach for extracting 
the hedge cues and their scopes in BioScope 
corpus using two CRF models for CoNLL-2010 
shared task. In the first task, the HCDic feature is 
proposed to improve the system performances, 
getting better performance (84.1% in F-score) 
than the baseline. The HCDic feature is also 
helpful to make use of cross-domain resources. 
The comparison of our methods based on 
between BioScope and Wikipedia corpus is 
given, which shows that ours are good at hedge 
cues detection in BioScope corpus but short at 
the in Wikipedia corpus. To detect the scope of 
hedge cues, we make rules to post process the 
text. For future work, we will look forward to 
constructing regulations for the HCDic to 
improve our system.  
References 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of the Fourteenth Conference 
on Computational Natural Language Learning 
(CoNLL-2010): Shared Task, pages 1?12, 
Uppsala, Sweden, July. Association for 
Computational Linguistics. 
Halil Kilicoglu, and Sabine Bergler. 2008. 
Recognizing speculative language in biomedical 
research articles: a linguistically motivated 
perspective. BMC Bioinformatics, 9(Suppl 
11):S10. 
John Lafferty, Andrew K. McCallum, and Fernando 
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling 
sequence data. In ICML, pages 282?289. 
George Lakoff. 1972. Hedges: a study in meaning 
criteria and the logic of fuzzy concepts. Chicago 
Linguistics Society Papers, 8:183?228. 
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 
2004 The language of bioscience: 
facts,speculations, and statements in between. In 
BioLINK 2004: Linking Biological Literature, 
Ontologies and Databases, pages 17?24. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proceedings of ACL 
2007, pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the BioNLP 2009 
Workshop, pages 28-36, Boulder, Colorado, June 
2009. Association for Computational Linguistics. 
Roser Morante, and Walter Daelemans. 2009. A 
metalearning approach to processing the scope of 
negation. In Proceedings of CoNLL-2009. 
Boulder, Colorado. 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised 
selection of keywords. In Proceedings of ACL 
2008, pages 281?289, Columbus, Ohio, USA. 
ACL. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun?ichi Tsujii. 2005. Developing a robust 
part-of-speech tagger for biomedical text. In: 
Advances in Informatics, PCI 2005, pages 382?
392. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The 
BioScope corpus: biomedical texts annotated for 
uncertainty, negation and their scopes. BMC 
Bioinformatics, 9(Suppl 11):S9. 
105
CRF tagging for head recognition based on Stanford parser 
 
Yong Cheng, Chengjie Sun, Bingquan Liu, Lei Lin 
Harbin Institute of Technology  
{ycheng, cjsun, linl,liubq}@insun.hit.edu.cn 
 
   
Abstract 
Chinese parsing has received more and 
more attention, and in this paper, we use 
toolkit to perform parsing on the data of 
Tsinghua Chinese Treebank (TCT) used in 
CIPS, and we use Conditional Random 
Fields (CRFs) to train specific model for the 
head recognition. At last, we compare 
different results on different POS results. 
1 Introduction 
    In the past decade, Chinese parsing has 
received more and more attention, it is the 
core of Chinese information processing 
technology, and it is also the cornerstone for 
deep understanding of Chinese.  
    Parsing is to identify automatically 
syntactic units in the sentence and give the 
relationship between these units. It is based 
on a given grammar. The results of parsing 
are usually structured syntax tree. For 
example, the parsing result of sentence "?
???????" is as following. 
                           (ROOT 
(dj (nS ??) 
(vp (v ?) 
(np 
                              (np (m ?) (n ??)) 
                           (n ??))))) 
With the development of Chinese 
economy, Chinese information processing 
has become a worldwide hot spot, and 
parsing is an essential task. However, 
parsing is a recognized research problem, 
and it is so difficult to meet the urgent needs 
of industrial applications in accuracy, 
robustness, speed. So the study of Chinese 
grammar and syntax analysis algorithm are 
still the focus of Chinese information 
processing.  
In all the parsing technology research, 
English parsing research is the most in-depth, 
and there are three main aspects of research 
in statistical parsing, they are  parsing model, 
parsing algorithm, and corpus construction. 
As for the parsing model, currently there are 
four commonly used parsing models, PCFG 
model [1], the model based on historical, 
Hierarchical model of progressive, head-
driven model [2]. 
 Since parsing is mostly a data driven 
process, its performance is determined by 
the amount of data in a Treebank on which a 
parser is trained. Much more data for 
English than for any other languages have 
been available so far. Thus most researches 
on parsing are concentrated on English. It is 
unrealistic to directly apply any existing 
parser trained on an English Treebank for 
Chinese sentences. But the methodology is, 
without doubt, highly applicable. Even for 
those corpora with special format and 
information integrated some modification 
and enhancement on a well-performed parser 
to fit the special structure for the data could 
help to obtain a good performance.  
    This paper presents our solution for the 
shared Task 2 of CIPS2010-Chinese Parsing. 
We exploit an existing powerful parser, 
Stanford parser, which has showed its 
effectiveness on English, with necessary 
modifications for parsing Chinese for the 
shared task. Since the corpus used in CIPS is 
from TCT, and the sentence contains the 
head-word information, but for the Stanford 
parser, it can't recognize the head 
constituents. So we apply a sequence tagging 
method to label head constituents based on 
the data extracted from the TCT corpus, In 
section 2 and section 3, we will present the  
Table 1. Training data with different formats 
 
details of our approach, and In section 4, we 
present the details of experiment. 
 
2 Parsing 
    Since English parsing has made many 
achievements, so we investigated some 
statistical parsing models designed for 
English. There are three open source 
constituent parsers, Stanford parser [3], 
Berkeley parser [4] and Bikel's parser [5]. 
Bikel's parser is an implementation of 
Collins' head-driven statistical model [6]. 
The Stanford parser is based on the factored 
model described in [7]. Berkeley parser is 
based on unlexicalized parsing model, as 
described in [8]. 
All the three parsers are claimed to be 
multilingual parsers but only accept training 
data in UPenn Treebank format. To adapt 
these parsers to Tsinghua Chinese Treebank 
(TCT) used in CIP, we firstly transform the 
TCT training data into UPenn format. Then, 
some slight modifications have been made to 
the three parsers. So that they could fulfill 
the needs in our task. 
In our work, we use Stanford parser to 
train our model by change the training data 
to three parts with different formats, one for 
training parsing model, one for training POS 
model, and the last for training head-
recognition model. Table 1 shows the three 
different forms. 
 
3 Head recognition 
    Head recognition is to find the head 
word in a clause, for example, 'np-1' express 
that in the clause, the word with index '1' is 
the key word. 
    To recognize the head constituents, and 
extra step is needed since Stanford parsing 
could not provide a straight forward way for 
this. Consider that head constituents are 
always determined by their syntactic symbol 
and their neighbors, whose order and 
relations strongly affects the head labeling. 
Like chunking [9], it is natural to apply a 
sequence labeling strategy to tackle this 
problem. We adopt the linear-chain CRF 
[10], one of the most successful sequence 
labeling framework so far, for the head 
recognition is this stage.  
    
4 Experiment 
4.1 Data 
    The training data is from Tsinghua 
Chinese Treebank (TCT), and our task is to 
perform full parsing on them. There are 
37218 lines in official released training data, 
As the Table 1 show; we change the data 
into three parts for different models. 
The testing data doesn?t contain POS 
labels, and there are 1000 lines in official 
released testing data. 
 
 
Parsing model 
1.(ROOT (np-0-2 (n ?
???) (cC ??) (np-
0-1 (n ? ? ) (n ?
?) ) ) ) 
2.(ROOT (vp-1 (pp-1 (p 
?) (np-0-2 (np-1 (n ?
?) (n ??) ) (cC ?
? ) (np-2 (a ? ? ) 
(uJDE ?) (np-1 (n ?
?) (np-1 (n ??) (n ?
?) ) ) ) ) ) (vp-1 (d ?
?) (vp-1 (d ??) (v ?
?) ) ) ) ) 
POS model 
1. ??/nS  ??/a  ?
?/n 
2.??/nS  ?/vC  ?/a  
??/n  ??/n  ?/wP  
??/nR  ??/n  ?/vC  
??/m  ?/m  ?/qN  
??/n  ?/uJDE  ??
/n  ?/wE   
Head-recognition 
model 
a O n np 0 
n a O np 1 
 
nS O np np 0 
np nS O np 1 
Table 2. Different POS tagging results 
 original new 
pos accuracy 80.40 94.82 
 
4.2 Models training 
4.2.1 Parsing model training 
    As for training parsing model with 
Stanford parser, since there are little 
parameters need to set, so we directly use the 
Stanford parser to train a model without any 
parameter setting. 
4.2.2 POS model training 
    In this session of the evaluation, POS 
tagging is no longer as a separate task, so we 
have to train our own POS tagging model. In 
the evaluation process, we didn't fully 
consider the POS tagging results' impact on 
the overall results, so we didn't train the POS 
model specially, we directly use the POS 
function in Stanford parser toolkit. This has 
led to relatively poor results in POS tagging, 
and it also affects the overall parsing result. 
After the evaluation, we train a specific 
model to improve the POS tagging results. 
As the table 1 shows, we extract training 
data from the original corpus and adopt the 
linear-chain CRF to train a POS tagging 
model. Table 2 shows the original POS 
tagging results and new results.  
4.2.3 Head recognition model training 
As the table 1 shows, we extract specific 
training data from original corpus.  
Table 3.  Training data formats for Head-
recognition 
original corpus 1.[vp-0 ??/v  [np-1 
??/n  ??/n  ] ]  
temp corpus 1.[np-1 ??/n  ??
/n  ] 
2.[vp-0 ??/v  [np-1 
??/n  ??/n  ] ] 
final corpus n O n np 0 
n n O np 1 
 
v O np vp 1 
np v O vp 0 
Table 4. Statistics the frequency of the words in 
each clause 
number of word statistics number 
< 1 160 
2 50834 
3 12592 
4 56 
5 664 
>5 360 
 
And for head-word recognition, since the 
adjacent clause has little effect on the 
recognition of head-word, so we set the 
clause as the smallest unit. We chose CRF to 
train our model. However, for getting the 
proper format of data for training in CRF, 
We have to do further processing on the data. 
As the table 3 shows, the final data set word 
as the unit. 
For example, the line 'n O np vp 1?, the 
meaning from beginning to end is POS or 
clause mark of current word or clause, POS 
or clause mark of previous word, POS or 
clause mark of latter word, the clause mark 
of current word, and the last mean that if 
current word or clause is headword 1 
represents YES, 0 represents NO. 
4.4 Result and Conclusion 
As we mention before, in evaluation, we 
didn't train specific POS tagging model, So 
we re-train our pos model, and the new 
results is shown in table 6, it can be seen that, 
with the increase of POS result, there is a 
corresponding increase in the overall results. 
Table 5. Performance of head recognition and 
the template for model training 
Boundary + 
Constituent 70.58 
 Boundary + 
Constituent + Head 66.97 
template 
U00:%x[0,0] 
U01:%x[-1,0] 
U02:%x[1,0] 
U04:%x[0,0]/%x[-1,0]
U05:%x[0,0]/%x[1,0]
U06:%x[-1,0]/%x[1,0]
 
 
Table 6. Overall results on different POS results 
 POS Boundary + 
Constituent 
original 80.40 67.00 
new 94.82 74.28 
 
Through our evaluation results, we can 
see that it is not appropriate to directly use 
English parser toolkit to process Chinese. 
And it is urgent to development parsing 
model based on the characteristics of 
Chinese. 
 
 
 
References 
[1] T. L. Booth and R. A. Thompson. Applying 
Probability Measures  to Abstract Languages. 
IEEE Transactions on Computers, 1973, C-
22(5):422-450. 
[2] M. Collins. Three Generative, Lexicalised 
Models for Statistical Parsing. In Proceedings 
of the 35th annual meeting of the association 
for computational linguistics. 
[3] http://nlp.stanford.edu/software/lex-parser.html 
[4] http://code.google.com/p/berkeleyparser 
[5] http://www.cis.upenn.edu/~dbikel/download 
[6] Michael Collins. 1999. Head-Driven Statistical Models for  
     Natural Language Parsing. Ph.D. thesis. 
University of Pennsylvania. 
[7] Dan Klein and Christopher D. Manning 
Accurate unlixicalized parsing. In Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics. 
[8] S Petrov and D Klein. Improved inference for 
unlexicalized parsing. In Proceedings of 
NAACL HLT 2007. 
[9] Fei Sha and Fernando Pereira. 2003. Shallow 
parsing with conditional random fields. In 
Proceedings of HLT-NAACL 2003, pages 
213-220, Edmonton. Canada. 
[10] John Lafferty. Andrew McCallum. And 
Fernando Pereira. 2001. Conditional random 
fields: Probabilistic models for segmenting and 
labeling sequence data. In Proceedings of 
ICML 2001, pages 282-289, Williams College, 
Williamstown, MA, USA. 
