Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 468?476, Prague, June 2007. c?2007 Association for Computational Linguistics
Smoothed Bloom filter language models: Tera-Scale LMs on the Cheap
David Talbot and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
d.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract
A Bloom filter (BF) is a randomised data
structure for set membership queries. Its
space requirements fall significantly below
lossless information-theoretic lower bounds
but it produces false positives with some
quantifiable probability. Here we present
a general framework for deriving smoothed
language model probabilities from BFs.
We investigate how a BF containing n-gram
statistics can be used as a direct replacement
for a conventional n-gram model. Recent
work has demonstrated that corpus statistics
can be stored efficiently within a BF, here
we consider how smoothed language model
probabilities can be derived efficiently from
this randomised representation. Our pro-
posal takes advantage of the one-sided error
guarantees of the BF and simple inequali-
ties that hold between related n-gram statis-
tics in order to further reduce the BF stor-
age requirements and the error rate of the
derived probabilities. We use these models
as replacements for a conventional language
model in machine translation experiments.
1 Introduction
Language modelling (LM) is a crucial component in
statistical machine translation (SMT). Standard n-
gram language models assign probabilities to trans-
lation hypotheses in the target language, typically
as smoothed trigram models (Chiang, 2005). Al-
though it is well-known that higher-order language
models and models trained on additional monolin-
gual corpora can significantly improve translation
performance, deploying such language models is not
trivial. Increasing the order of an n-gram model can
result in an exponential increase in the number of
parameters; for the English Gigaword corpus, for
instance, there are 300 million distinct trigrams and
over 1.2 billion distinct five-grams. Since a language
model is potentially queried millions of times per
sentence, it should ideally reside locally in memory
to avoid time-consuming remote or disk-based look-
ups.
Against this background, we consider a radically
different approach to language modelling. Instead
of explicitly storing all distinct n-grams from our
corpus, we create an implicit randomised represen-
tation of these statistics. This allows us to drastically
reduce the space requirements of our models. In
this paper, we build on recent work (Talbot and Os-
borne, 2007) that demonstrated how the Bloom filter
(Bloom (1970); BF), a space-efficient randomised
data structure for representing sets, could be used to
store corpus statistics efficiently. Here, we propose
a framework for deriving smoothed n-gram models
from such structures and show via machine trans-
lation experiments that these smoothed Bloom filter
language modelsmay be used as direct replacements
for standard n-gram models in SMT.
The space requirements of a Bloom filter are quite
spectacular, falling significantly below information-
theoretic error-free lower bounds. This efficiency,
however, comes at the price of false positives: the fil-
ter may erroneously report that an item not in the set
is a member. False negatives, on the other hand, will
468
never occur: the error is said to be one-sided. Our
framework makes use of the log-frequency Bloom
filter presented in (Talbot and Osborne, 2007), and
described briefly below, to compute smoothed con-
ditional n-gram probabilities on the fly. It takes
advantage of the one-sided error guarantees of the
Bloom filter and certain inequalities that hold be-
tween related n-gram statistics drawn from the same
corpus to reduce both the error rate and the compu-
tation required in deriving these probabilities.
2 The Bloom filter
In this section, we give a brief overview of the
Bloom filter (BF); refer to Broder andMitzenmacher
(2005) for a more in detailed presentation. A BF rep-
resents a set S = {x1, x2, ..., xn} with n elements
drawn from a universe U of size N . The structure is
attractive when N  n. The only significant stor-
age used by a BF consists of a bit array of size m.
This is initially set to hold zeroes. To train the filter
we hash each item in the set k times using distinct
hash functions h1, h2, ..., hk. Each function is as-
sumed to be independent from each other and to map
items in the universe to the range 1 to m uniformly
at random. The k bits indexed by the hash values
for each item are set to 1; the item is then discarded.
Once a bit has been set to 1 it remains set for the life-
time of the filter. Distinct items may not be hashed
to k distinct locations in the filter; we ignore col-
lisons. Bits in the filter can, therefore, be shared by
distinct items allowing significant space savings but
introducing a non-zero probability of false positives
at test time. There is no way of directly retrieving or
ennumerating the items stored in a BF.
At test time we wish to discover whether a given
item was a member of the original set. The filter is
queried by hashing the test item using the same k
hash functions. If all bits referenced by the k hash
values are 1 then we assume that the item was a
member; if any of them are 0 then we know it was
not. True members are always correctly identified,
but a false positive will occur if all k corresponding
bits were set by other items during training and the
item was not a member of the training set.
The probability of a false postive, f , is clearly the
probability that none of k randomly selected bits in
the filter are still 0 after training. Letting p be the
proportion of bits that are still zero after these n ele-
ments have been inserted, this gives,
f = (1? p)k.
As n items have been entered in the filter by hashing
each k times, the probability that a bit is still zero is,
p
?
=
(
1?
1
m
)kn
? e?
kn
m
which is the expected value of p. Hence the false
positive rate can be approximated as,
f = (1? p)k ? (1? p
?
)k ?
(
1? e?
kn
m
)k
.
By taking the derivative we find that the number of
functions k? that minimizes f is,
k? = ln 2 ?
m
n
,
which leads to the intuitive result that exactly half
the bits in the filter will be set to 1 when the optimal
number of hash functions is chosen.
The fundmental difference between a Bloom fil-
ter?s space requirements and that of any lossless rep-
resentation of a set is that the former does not depend
on the size of the (exponential) universe N from
which the set is drawn. A lossless representation
scheme (for example, a hash map, trie etc.) must de-
pend on N since it assigns a distinct representation
to each possible set drawn from the universe.
3 Language modelling with Bloom filters
Recent work (Talbot and Osborne, 2007) presented a
scheme for associating static frequency information
with a set of n-grams in a BF efficiently.1
3.1 Log-frequency Bloom filter
The efficiency of the scheme for storing n-gram
statistics within a BF presented in Talbot and Os-
borne (2007) relies on the Zipf-like distribution of
n-gram frequencies: most events occur an extremely
small number of times, while a small number are
very frequent. We assume that raw counts are quan-
tised and employ a logarithmic codebook that maps
counts, c(x), to quantised counts, qc(x), as follows,
qc(x) = 1 + blogb c(x)c. (1)
1Note that as described the Bloom filter is not an associative
data structure and provides only a Boolean function character-
ising the set that has been stored in it.
469
Algorithm 1 Training frequency BF
Input: Strain, {h1, ...hk} and BF = ?
Output: BF
for all x ? Strain do
c(x)? frequency of n-gram x in Strain
qc(x)? quantisation of c(x) (Eq. 1)
for j = 1 to qc(x) do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
BF [hi(x)]? 1
end for
end for
end for
return BF
The precision of this codebook decays exponentially
with the raw counts and the scale is determined by
the base of the logarithm b; we examine the effect
of this parameter on our language models in experi-
ments below.
Given the quantised count qc(x) for an n-gram
x, the filter is trained by entering composite events
consisting of the n-gram appended by an integer
counter j that is incremented from 1 to qc(x) into
the filter. To retrieve an n-gram?s frequency, the n-
gram is first appended with a counter set to 1 and
hashed under the k functions; if this tests positive,
the counter is incremented and the process repeated.
The procedure terminates as soon as any of the k
hash functions hits a 0 and the previous value of the
counter is reported. The one-sided error of the BF
and the training scheme ensure that the actual quan-
tised count cannot be larger than this value. As the
counts are quantised logarithmically, the counter is
usually incremented only a small number of times.
We can then approximate the original frequency
of the n-gram by taking its expected value given the
quantised count retrieved,
E[c(x)|qc(x) = j] =
bj?1 + bj ? 1
2
. (2)
These training and testing routines are repeated here
as Algorithms 1 and 2 respectively.
As noted in Talbot and Osborne (2007), errors for
this log-frequency BF scheme are one-sided: fre-
quencies will never be underestimated. The prob-
ability of overestimating an item?s frequency decays
Algorithm 2 Test frequency BF
Input: x, MAXQCOUNT , {h1, ...hk} and BF
Output: Upper bound on c(x) ? Strain
for j = 1 to MAXQCOUNT do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
if BF [hi(x)] = 0 then
return E[c(x)|qc(x) = j ? 1] (Eq. 2)
end if
end for
end for
exponentially with the size of the overestimation er-
ror d (i.e. as fd for d > 0) since each erroneous
increment corresponds to a single false positive and
d such independent events must occur together.
The efficiency of the log-frequency BF scheme
can be understood from an entropy encoding per-
spective under the distribution over frequencies of
n-gram types: the most common frequency (the sin-
gleton count) is assigned the shortest code (length k)
while rarer frequencies (those for more common n-
grams) are assigned increasingly longer codes (k ?
qc(x)).
3.2 Smoothed BF language models
A standard n-gram language model assigns condi-
tional probabilities to target words given a certain
context. In practice, most standard n-gram language
models employ some form of interpolation whereby
probabilities conditioned on the most specific con-
text consisting usually of the n ? 1 preceding to-
kens are combined with more robust estimates based
on less specific conditioning events. To compute
smoothed language model probabilities, we gener-
ally require access to the frequencies of n-grams of
length 1 to n in our training corpus. Depending on
the smoothing scheme, we may also need auxiliary
statistics regarding the number of distinct suffixes
for each n-gram (e.g., Witten-Bell and Kneser-Ney
smoothing) and the number of distinct prefixes or
contexts in which they appear (e.g., Kneser-Ney).
We can use a single BF to store these statistics but
need to distinguish each type of event (e.g., raw
counts, suffix counts, etc.). Here we use a distinct
set of k hash functions for each such category.
Our motivation for storing the corpus statistics
470
directly rather than precomputed probabilities is
twofold: (i) the efficiency of the scheme described
above for storing frequency information together
with items in a BF relies on the frequencies hav-
ing a Zipf-like distribution; while this is definitely
true for corpus statistics, it may well not hold for
probabilities estimated from them; (ii) as will be-
come apparent below, by using the corpus statistics
directly, we will be able to make additional savings
in terms of both space and error rate by using simple
inequalities that hold for related information drawn
consistently from the same corpus; it is not clear
whether such bounds can be established for proba-
bilities computed from these statistics.
3.2.1 Proxy items
There is a potential risk of redundancy if we rep-
resent related statistics using the log-frequency BF
scheme presented in Talbot and Osborne (2007). In
particular, we do not need to store information ex-
plicitly that is necessarily implied by the presence
of another item in the training set, if that item can
be identified efficiently at query time when needed.
We use the term proxy item to refer to items whose
presence in the filter implies the existence of another
item and that can be efficiently queried given the im-
plied item. In using a BF to store corpus statistics
for language modelling, for example, we can use the
event corresponding to an n-gram and the counter
set to 1 as a proxy item for a distinct prefix, suffix or
context count of 1 for the same n-gram since (ignor-
ing sentence boundaries) it must have been preceded
and followed by at least one distinct type, i.e.,
qc(w1, ..., wn) ? 1 ? BF ? s(w1, ..., wn) ? 1,
where s(?) is the number of the distinct types follow-
ing this n-gram in the training corpus. We show be-
low that such lower bounds allow us to significantly
reduce the memory requirements for a BF language
model.
3.2.2 Monotonicity of n-gram event space
The error analysis in Section 2 focused on the
false positive rate of a BF; if we deploy a BF within
an SMT decoder, however, the actual error rate will
also depend on the a priori membership probability
of items presented to it. The error rate Err is,
Err = Pr(x /? Strain|Decoder)f.
This implies that, unlike a conventional lossless data
structure, the model?s accuracy depends on other
components in system and how it is queried.
Assuming that statistics are entered consistently
from the same corpus, we can take advantage of the
monotonicity of the n-gram event space to place up-
per bounds on the frequencies of events to be re-
trieved from the filter prior to querying it, thereby
reducing the a priori probability of a negative and
consequently the error rate.
Specifically, since the log-frequency BF scheme
will never underestimate an item?s frequency, we
can apply the following inequality recursively and
bound the frequency of an n-gram by that of its least
frequent subsequence,
c(w1, ..., wn) ? min {c(w1, ..., wn?1), c(w2, ..., wn)}.
We use this to reduce the error rate of an interpolated
BF language model described below.
3.3 Witten-Bell smoothed BF LM
As an example application of our framework, we
now describe a scheme for creating and querying
a log-frequency BF to estimate n-gram language
model probabilities using Witten-Bell smoothing
(Bell et al, 1990). Other smoothing schemes, no-
tably Kneser-Ney, could be described within this
framework using additional proxy relations for infix
and prefix counts.
In Witten-Bell smoothing, an n-gram?s probabil-
ity is discounted by a factor proportional to the num-
ber of times that the n ? 1-gram preceding the cur-
rent word was observed preceding a novel type in
the training corpus. It is defined recursively as,
Pwb(wi|w
i?1
i?n+1) = ?wi?1i?n+1
Pml(wi|w
i?1
i?n+1)
+(1??wi?1i?n+1
)Pwb(wi|w
i?1
i?n+2)
where ?x is defined via,
1? ?x =
c(x)
s(x) + c(x)
,
and Pml(?) is the maximum likelihood estimator cal-
culated from relative frequencies.
The statistics required to compute the Witten-Bell
estimator for the conditional probability of an n-
gram consist of the counts of all n-grams of length
471
1 to n as well as the counts of the number of distinct
types following all n-grams of length 1 to n ? 1.
In practice we use the c(w1, ..., wi) = 1 event as a
proxy for s(w1, ..., wi) = 1 and thereby need not
store singleton suffix counts in the filter.
Distinct suffix counts of 2 and above are stored
by subtracting this proxy count and converting to the
log quantisation scheme described above, i.e.,
qs(x) = 1 + blogb(s(x)? 1)c
In testing for a suffix count, we first query the item
c(w1, ..., wn) = 1 as a proxy for s(w1, ..., wn) =
1 and, if found, query the filter for incrementally
larger suffix counts, taking the reconstructed suffix
count of an n-gram with a non-zero n-gram count to
be the expected value, i.e.,
E[s(x)|qs(x) = j ? j > 0] = 1 +
(bj?1 + bj ? 1)
2
Having created a BF containing these events, the
algorithm we use to compute the interpolated WB
estimate makes use of the inequalities described
above to reduce the a priori probability of querying
for a negative. In particular, we bound the count of
each numerator in the maximum likelihood term by
the count of the corresponding denominator and the
count of distinct suffixes of an n-gram by its respec-
tive token frequency.
Unlike more traditional LM formulations that
back-off from the highest-order to lower-order mod-
els, our algorithm works up from the lowest-order
model. Since the conditioning context increases in
specificity at each level, each statistic is bound from
above by its corresponding value at the previous less
specific level. The bounds are applied by passing
them as the parameter MAXQCOUNT to the fre-
quency test routine shown as Algorithm 2. We ana-
lyze the effect of applying such bounds on the per-
formance of the model within an SMT decoder in
the experiments below. Working upwards from the
lower-order models also allows us to truncate the
computation before the highest level if the denomi-
nator in the maximum likelihood term is found with
a zero count at any stage (no higher-order terms can
be non-zero given this).
4 Experiments
We conducted a range of experiments to explore
the error-space trade-off of using a BF-based model
as a replacement for a conventional n-gram model
within an SMT system and to assess the benefits of
specific features of our framework for deriving lan-
guage model probabilities from a BF.
4.1 Experimental set-up
All of our experiments use publically available re-
sources. Our main experiments use the French-
English section of the Europarl (EP) corpus for par-
allel data and language modelling (Koehn, 2003).
Decoding is carried-out using the Moses decoder
(Koehn and Hoang, 2007). We hold out 1,000 test
sentences and 500 development sentences from the
parallel text for evaluation purposes. The parame-
ters for the feature functions used in this log-linear
decoder are optimised using minimum error rate
(MER) training on our development set unless other-
wise stated. All evaluation is in terms of the BLEU
score on our test set (Papineni et al, 2002).
Our baseline language models were created us-
ing the SRILM toolkit (Stolcke, 2002). We built 3,
4 and 5-gram models from the Europarl corpus us-
ing interpolated Witten-Bell smoothing (WB); no n-
grams are dropped from these models or any of the
BF-LMs. The number of distinct n-gram types in
these baseline models as well as their sizes on disk
and as compressed by gzip are given in Table 1; the
gzip figures are given as an approximate (and opti-
mistic) lower bound on lossless representations of
these models.2
The BF-LM models used in these experiments
were all created from the same corpora following the
scheme outlined above for storing n-gram statistics.
Proxy relations were used to reduce the number of
items that must be stored in the BF; in addition, un-
less specified otherwise, we take advantage of the
bounds described above that hold between related
statistics to avoid presenting known negatives to the
filter. The base of the logarithm used in quantization
is specified on all figures.
The SRILM and BF-based models are both
queried via the same interface in the Moses decoder.
2Note, in particular, that gzip compressed files do not sup-
port direct random access as required by in language modelling.
472
n Types Mem. Gzip?d BLEU
3 5.9M 174Mb 51Mb 28.54
4 14.1M 477Mb 129Mb 28.99
5 24.2M 924Mb 238Mb 29.07
Table 1: WB-smoothed SRILM baseline models.
We assign a small cache to the BF-LM models (be-
tween 1 and 2MBs depending on the order of the
model) to store recently retrieved statistics and de-
rived probabilities. Translation takes between 2 to 5
times longer using the BF-LMs as compared to the
corresponding SRILM models.
4.2 Machine translation experiments
Our first set of experiments examines the relation-
ship between memory allocated to the BF-LM and
translation performance for a 3-gram and a 5-gram
WB smoothed BF-LM. In these experiments we use
the log-linear weights of the baseline model to avoid
variation in translation performance due to differ-
ences in the solutions found by MER training: this
allows us to focus solely on the quality of each BF-
LM?s approximation of the baseline. These exper-
iments consider various settings of the base for the
logarithm used during quantisation (b in Eq. (1)).
We also analyse these results in terms of the re-
lationships between BLEU score and the underlying
error rate of the BF-LM and the number of bits as-
signed per n-gram in the baseline model.
MER optimised BLEU scores on the test set are
then given for a range of BF-LMs.
4.3 Mean squared error experiments
Our second set of experiments focuses on the accu-
racy with which the BF-LM can reproduce the base-
line model?s distribution. Unfortunately, perplex-
ity or related information-theoretic quantities are not
applicable in this case since the BF-LM is not guar-
anteed to produce a properly normalised distribu-
tion. Instead we evaluate the mean squared error
(MSE) between the log-probabilites assigned by the
baseline model and by BF-LMs to n-grams in the
English portion of our development set; we also con-
sider the relation between MSE and the BLEU score
from the experiments above.
 
22
 
24
 
26
 
28
 
30
 
32
 
0.02
 
0.017
5
 
0.015
 
0.012
5
 
0.01
 
0.007
5
 
0.005
 
0.002
5
BLEU Score
Memo
ry in G
B
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
SRILM
 Witte
n-Bell
 3-gra
m (174
MB)
Figure 1: WB-smoothed 3-gram model (Europarl).
4.4 Analysis of BF-LM framework
Our third set of experiments evaluates the impact of
the use of upper bounds between related statistics on
translation performance. Here the standard model
that makes use of these bounds to reduce the a pri-
ori negative probability is compared to a model that
queries the filter in a memoryless fashion.3
We then present details of the memory savings ob-
tained by the use of proxy relations for the models
used here.
5 Results
5.1 Machine translation experiments
Figures 1 and 2 show the relationship between trans-
lation performance as measured by BLEU and the
memory assigned to the BF respectively for WB-
smoothed 3-gram and 5-gram BF-LMs. There is a
clear degradation in translation performance as the
memory assigned to the filter is reduced. Models
using a higher quantisation base approach their opti-
mal performance faster; this is because these more
coarse-grained quantisation schemes store fewer
items in the filter and therefore have lower underly-
ing false positive rates for a given amount of mem-
ory.
Figure 3 presents these results in terms of the re-
lationship between translation performance and the
false positive rate of the underlying BF. We can see
that for a given false positive rate, the more coarse-
grained quantisation schemes (e.g., base 3) perform
3In both cases we apply ?sanity check? bounds to ensure that
none of the ratios in the WB formula (Eq. 3) are greater than 1.
473
 
22
 
24
 
26
 
28
 
30
 
32
 
0.07
 
0.06
 
0.05
 
0.04
 
0.03
 
0.02
 
0.01
BLEU Score
Memo
ry in G
B
WB-s
mooth
ed BF
-LM 5
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
SRILM
 Witte
n-Bell
 5-gra
m (924
MB)
Figure 2: WB-smoothed 5-gram model (Europarl).
 
22
 
23
 
24
 
25
 
26
 
27
 
28
 
29
 
30  0.0
1
 
0.1
 
1
BLEU Score
False
 posit
ive ra
te (prob
ability)
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
Figure 3: False positive rate vs. BLEU .
worse than the more fine-grained schemes.4
Figure 4 presents the relationship in terms of the
number of bits per n-gram in the baseline model.
This suggests that between 10 and 15 bits is suf-
ficient for the BF-LM to approximate the baseline
model. This is a reduction of a factor of between 16
and 24 on the plain model and of between 4 and 7
on gzip compressed model.
The results of a selection of BF-LM models with
decoder weights optimised using MER training are
given in Table 2; these show that the models perform
consistently close to the baseline models that they
approximate.
5.2 Mean squared error experiments
Figure 5 shows the relationship between memory as-
signed to the BF-LMs and the mean squared error
4Note that in this case the base 3 scheme will use approxi-
mately two-thirds the amount of memory required by the base
1.5 scheme.
 
20
 
22
 
24
 
26
 
28
 
30
 
32
 
19
 
17
 
15
 
13
 
11
 
9
 
7
 
5
 
3
 
1
BLEU Score
Bits p
er n-g
ram
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
Figure 4: Bits per n-gram vs. BLEU.
n Memory Bits / n-gram base BLEU
3 10MB 14 bits 1.5 28.33
3 10MB 14 bits 2.0 28.47
4 20MB 12 bits 1.5 28.63
4 20MB 12 bits 2.0 28.63
5 40MB 14 bits 1.5 28.53
5 40MB 14 bits 2.0 28.72
5 50MB 17 bits 1.5 29.31
5 50MB 17 bits 2.0 28.67
Table 2: MERT optimised WB-smoothed BF-LMS.
(MSE) of log-probabilities that these models assign
to the development set compared to those assigned
by the baseline model. This shows clearly that the
more fine-grained quantisation scheme (e.g. base
1.1) can reach a lower MSE but also that the more
coarse-grained schemes (e.g., base 3) approach their
minimum error faster.
Figure 6 shows the relationship between MSE
between the BF-LM and the baseline model and
BLEU. The MSE appears to be a good predictor of
BLEU score across all quantisation schemes. This
suggests that it may be a useful tool for optimising
BF-LM parameters without the need to run the de-
coder assuming a target (lossless) LM can be built
and queried for a small test set on disk. An MSE of
below 0.05 appears necessary to achieve translation
performance matching the baseline model here.
5.3 Analysis of BF-LM framework
We refer to (Talbot and Osborne, 2007) for empiri-
cal results establishing the performance of the log-
frequency BF-LM: overestimation errors occur with
474
 
0.01
 
0.025 0.05 0.1 0.25 0.5
 
0.03
 
0.02
 
0.01
 
0.005
 
0.002
5
 
0.001
Mean squared error of log probabilites
Memo
ry in G
B
MSE 
betwe
en WB
 3-gra
m SR
ILM a
nd BF
-LMs
Base
 3
Base
 1.5
Base
 1.1
Figure 5: MSE between SRILM and BF-LMs
 
22
 
23
 
24
 
25
 
26
 
27
 
28
 
29
 
30  0.0
1
 
0.1
 
1
BLEU Score
Mean
 squa
red er
ror
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 1.1
BF-LM
 base
 1.5
BF-LM
 base
 3
Figure 6: MSE vs. BLEU for WB 3-gram BF-LMs
a probability that decays exponentially in the size of
the overestimation error.
Figure 7 shows the effect of applying upper
bounds to reduce the a priori probability of pre-
senting a negative event to the filter in our in-
terpolation algorithm for computing WB-smoothed
probabilities. The application of upper bounds im-
proves translation performance particularly when
the amount of memory assigned to the filter is lim-
ited. Since both filters have the same underlying
false positive rate (they are identical), we can con-
clude that this improvement in performance is due
to a reduction in the number of negatives that are
presented to the filter and hence errors.
Table 3 shows the amount of memory saved by
the use of proxy items to avoid storing singleton
suffix counts for the Witten-Bell smoothing scheme.
The savings are given as ratios over the amount of
memory needed to store the statistics without proxy
items. These models have the same underlying false
 
22
 
24
 
26
 
28
 
30
 
32
 
0.01
 
0.007
5
 
0.005
 
0.002
5
BLEU Score
Memo
ry in G
B
WB-s
mooth
ed BF
-LM 3
-gram
 mode
l
BF-LM
 base
 2 wit
h bou
nds
BF-LM
 base
 2 wit
hout b
ounds
Figure 7: Effect of upper bounds on BLEU
n-gram order Proxy space saving
3 0.885
4 0.783
5 0.708
Table 3: Space savings via proxy items .
positive rate (0.05) and quantisation base (2). Sim-
ilar savings may be anticipated when applying this
framework to infix and prefix counts for Kneser-Ney
smoothing.
6 Related Work
Previous work aimed at reducing the size of n-gram
language models has focused primarily on quanti-
sation schemes (Whitaker and Raj, 2001) and prun-
ing (Stolcke, 1998). The impact of the former seems
limited given that storage for the n-gram types them-
selves will generally be far greater than that needed
for the actual probabilities of the model. Pruning
on the other hand could be used in conjunction with
the framework proposed here. This holds also for
compression schemes based on clustering such as
(Goodman and Gao, 2000). Our approach, however,
avoids the significant computational costs involved
in the creation of such models.
Other schemes for dealing with large language
models include per-sentence filtering of the model
or its distribution over a cluster. The former requires
time-consuming adaptation of the model for each
sentence in the test set while the latter incurs sig-
nificant overheads for remote calls during decoding.
Our framework could, however, be used to comple-
ment either of these approaches.
475
7 Conclusions and Future Work
We have proposed a framework for computing
smoothed language model probabilities efficiently
from a randomised representation of corpus statis-
tics provided by a Bloom filter. We have demon-
strated that models derived within this framework
can be used as direct replacements for equivalent
conventional language models with significant re-
ductions in memory requirements. Our empirical
analysis has also demonstrated that by taking advan-
tage of the one-sided error guarantees of the BF and
simple inequalities that hold between related n-gram
statistics we are able to further reduce the BF stor-
age requirements and the effective error rate of the
derived probabilities.
We are currently implementing Kneser-Ney
smoothing within the proposed framework. We hope
the present work will, together with Talbot and Os-
borne (2007), establish the Bloom filter as a practi-
cal alternative to conventional associative data struc-
tures used in computational linguistics. The frame-
work presented here shows that with some consider-
ation for its workings, the randomised nature of the
Bloom filter need not be a significant impediment to
is use in applications.
Acknowledgements
References
T.C. Bell, J.G. Cleary, and I.H. Witten. 1990. Text Compres-
sion. Prentice Hall, Englewood Cliffs, NJ.
B. Bloom. 1970. Space/time tradeoffs in hash coding with
allowable errors. CACM, 13:422?426.
A. Broder and M. Mitzenmacher. 2005. Network applications
of Bloom filters: A survey. Internet Mathematics, 1(4):485?
509.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 263?270, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
J. Goodman and J. Gao. 2000. Language model size reduction
by pruning and clustering. In ICSLP?00, Beijing, China.
Philipp Koehn and Hieu Hoang. 2007. Factored translation
models. In Proc. of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP/Co-NLL).
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation, draft. Available
at:http://people.csail.mit.edu/ koehn/publications/europarl.ps.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU:
a method for automatic evaluation of machine translation.
In ACL-2002: 40th Annual meeting of the Association for
Computational Linguistics.
Andreas Stolcke. 1998. Entropy-based pruning of back-off lan-
guage models. In Proc. DARPA Broadcast News Transcrip-
tion and Understanding Workshop, pages 270?274.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Proc. Intl. Conf. on Spoken Language Processing.
D. Talbot and M. Osborne. 2007. Randomised language mod-
elling for statistical machine translation. In 45th Annual
Meeting of the Association of Computational Linguists (To
appear).
E. Whitaker and B. Raj. 2001. Quantization-based language
model compression (tr-2001-41). Technical report, Mit-
subishi Electronic Research Laboratories.
476
Statistical Machine Translation with
Word- and Sentence-Aligned Parallel Corpora
Chris Callison-Burch David Talbot Miles Osborne
School on Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
callison-burch@ed.ac.uk
Abstract
The parameters of statistical translation models are
typically estimated from sentence-aligned parallel
corpora. We show that significant improvements in
the alignment and translation quality of such mod-
els can be achieved by additionally including word-
aligned data during training. Incorporating word-
level alignments into the parameter estimation of
the IBM models reduces alignment error rate and
increases the Bleu score when compared to training
the same models only on sentence-aligned data. On
the Verbmobil data set, we attain a 38% reduction
in the alignment error rate and a higher Bleu score
with half as many training examples. We discuss
how varying the ratio of word-aligned to sentence-
aligned data affects the expected performance gain.
1 Introduction
Machine translation systems based on probabilistic
translation models (Brown et al, 1993) are gener-
ally trained using sentence-aligned parallel corpora.
For many language pairs these exist in abundant
quantities. However for new domains or uncommon
language pairs extensive parallel corpora are often
hard to come by.
Two factors could increase the performance of
statistical machine translation for new language
pairs and domains: a reduction in the cost of cre-
ating new training data, and the development of
more efficient methods for exploiting existing train-
ing data. Approaches such as harvesting parallel
corpora from the web (Resnik and Smith, 2003)
address the creation of data. We take the second,
complementary approach. We address the prob-
lem of efficiently exploiting existing parallel cor-
pora by adding explicit word-level alignments be-
tween a number of the sentence pairs in the train-
ing corpus. We modify the standard parameter esti-
mation procedure for IBM Models and HMM vari-
ants so that they can exploit these additional word-
level alignments. Our approach uses both word- and
sentence-level alignments for training material.
In this paper we:
1. Describe how the parameter estimation frame-
work of Brown et al (1993) can be adapted to
incorporate word-level alignments;
2. Report significant improvements in alignment
error rate and translation quality when training
on data with word-level alignments;
3. Demonstrate that the inclusion of word-level
alignments is more effective than using a bilin-
gual dictionary;
4. Show the importance of amplifying the contri-
bution of word-aligned data during parameter
estimation.
This paper shows that word-level alignments im-
prove the parameter estimates for translation mod-
els, which in turn results in improved statistical
translation for languages that do not have large
sentence-aligned parallel corpora.
2 Parameter Estimation Using
Sentence-Aligned Corpora
The task of statistical machine translation is to
choose the source sentence, e, that is the most prob-
able translation of a given sentence, f , in a for-
eign language. Rather than choosing e? that di-
rectly maximizes p(e|f), Brown et al (1993) apply
Bayes? rule and select the source sentence:
e? = argmax
e
p(e)p(f |e). (1)
In this equation p(e) is a language model probabil-
ity and is p(f |e) a translation model probability. A
series of increasingly sophisticated translation mod-
els, referred to as the IBM Models, was defined in
Brown et al (1993).
The translation model, p(f |e) defined as a
marginal probability obtained by summing over
word-level alignments, a, between the source and
target sentences:
p(f |e) =
?
a
p(f ,a|e). (2)
While word-level alignments are a crucial com-
ponent of the IBM models, the model parame-
ters are generally estimated from sentence-aligned
parallel corpora without explicit word-level align-
ment information. The reason for this is that
word-aligned parallel corpora do not generally ex-
ist. Consequently, word level alignments are treated
as hidden variables. To estimate the values of
these hidden variables, the expectation maximiza-
tion (EM) framework for maximum likelihood esti-
mation from incomplete data is used (Dempster et
al., 1977).
The previous section describes how the trans-
lation probability of a given sentence pair is ob-
tained by summing over all alignments p(f |e) =
?
a p(f ,a|e). EM seeks to maximize the marginal
log likelihood, log p(f |e), indirectly by iteratively
maximizing a bound on this term known as the ex-
pected complete log likelihood, ?log p(f ,a|e)?q(a),1
log p(f |e) = log
?
a
p(f ,a|e) (3)
= log
?
a
q(a)
p(f ,a|e)
q(a)
(4)
?
?
a
q(a) log
p(f ,a|e)
q(a)
(5)
= ?log p(f ,a|e)?q(a) + H(q(a))
where the bound in (5) is given by Jensen?s inequal-
ity. By choosing q(a) = p(a|f , e) this bound be-
comes an equality.
This maximization consists of two steps:
? E-step: calculate the posterior probability
under the current model of every permissi-
ble alignment for each sentence pair in the
sentence-aligned training corpus;
? M-step: maximize the expected log like-
lihood under this posterior distribution,
?log p(f ,a|e)?q(a), with respect to the model?s
parameters.
While in standard maximum likelihood estima-
tion events are counted directly to estimate param-
eter settings, in EM we effectively collect frac-
tional counts of events (here permissible alignments
weighted by their posterior probability), and use
these to iteratively update the parameters.
1Here ? ??q(?) denotes an expectation with respect to q(?).
Since only some of the permissible alignments
make sense linguistically, we would like EM to use
the posterior alignment probabilities calculated in
the E-step to weight plausible alignments higher
than the large number of bogus alignments which
are included in the expected complete log likeli-
hood. This in turn should encourage the parame-
ter adjustments made in the M-step to converge to
linguistically plausible values.
Since the number of permissible alignments for
a sentence grows exponentially in the length of the
sentences for the later IBM Models, a large num-
ber of informative example sentence pairs are re-
quired to distinguish between plausible and implau-
sible alignments. Given sufficient data the distinc-
tion occurs because words which are mutual trans-
lations appear together more frequently in aligned
sentences in the corpus.
Given the high number of model parameters and
permissible alignments, however, huge amounts of
data will be required to estimate reasonable transla-
tion models from sentence-aligned data alone.
3 Parameter Estimation Using Word- and
Sentence-Aligned Corpora
As an alternative to collecting a huge amount of
sentence-aligned training data, by annotating some
of our sentence pairs with word-level alignments
we can explicitly provide information to highlight
plausible alignments and thereby help parameters
converge upon reasonable settings with less training
data.
Since word-alignments are inherent in the IBM
translation models it is straightforward to incorpo-
rate this information into the parameter estimation
procedure. For sentence pairs with explicit word-
level alignments marked, fractional counts over all
permissible alignments need not be collected. In-
stead, whole counts are collected for the single hand
annotated alignment for each sentence pair which
has been word-aligned. By doing this the expected
complete log likelihood collapses to a single term,
the complete log likelihood (p(f ,a|e)), and the E-
step is circumvented.
The parameter estimation procedure now in-
volves maximizing the likelihood of data aligned
only at the sentence level and also of data aligned
at the word level. The mixed likelihood function,
M, combines the expected information contained
in the sentence-aligned data with the complete in-
formation contained in the word-aligned data.
M =
Ns?
s=1
(1? ?)?log p(fs,as|es)?q(as)
+
Nw?
w=1
? log p(fw,aw|ew) (6)
Here s and w index the Ns sentence-aligned sen-
tences and Nw word-aligned sentences in our cor-
pora respectively. Thus M combines the expected
complete log likelihood and the complete log likeli-
hood. In order to control the relative contributions
of the sentence-aligned and word-aligned data in
the parameter estimation procedure, we introduce a
mixing weight ? that can take values between 0 and
1.
3.1 The impact of word-level alignments
The impact of word-level alignments on parameter
estimation is closely tied to the structure of the IBM
Models. Since translation and word alignment pa-
rameters are shared between all sentences, the pos-
terior alignment probability of a source-target word
pair in the sentence-aligned section of the corpus
that were aligned in the word-aligned section will
tend to be relatively high.
In this way, the alignments from the word-aligned
data effectively percolate through to the sentence-
aligned data indirectly constraining the E-step of
EM.
3.2 Weighting the contribution of
word-aligned data
By incorporating ?, Equation 6 becomes an interpo-
lation of the expected complete log likelihood pro-
vided by the sentence-aligned data and the complete
log likelihood provided by word-aligned data.
The use of a weight to balance the contributions
of unlabeled and labeled data in maximum like-
lihood estimation was proposed by Nigam et al
(2000). ? quantifies our relative confidence in the
expected statistics and observed statistics estimated
from the sentence- and word-aligned data respec-
tively.
Standard maximum likelihood estimation (MLE)
which weighs all training samples equally, corre-
sponds to an implicit value of lambda equal to the
proportion of word-aligned data in the whole of
the training set: ? = NwNw+Ns . However, having
the total amount of sentence-aligned data be much
larger than the amount of word-aligned data implies
a value of ? close to zero. This means that M can be
maximized while essentially ignoring the likelihood
of the word-aligned data. Since we believe that the
explicit word-alignment information will be highly
effective in distinguishing plausible alignments in
the corpus as a whole, we expect to see benefits by
setting ? to amplify the contribution of the word-
aligned data set particularly when this is a relatively
small portion of the corpus.
4 Experimental Design
To perform our experiments with word-level aligne-
ments we modified GIZA++, an existing and freely
available implementation of the IBM models and
HMM variants (Och and Ney, 2003). Our modifi-
cations involved circumventing the E-step for sen-
tences which had word-level alignments and incor-
porating these observed alignment statistics in the
M-step. The observed and expected statistics were
weighted accordingly by ? and (1? ?) respectively
as were their contributions to the mixed log likeli-
hood.
In order to measure the accuracy of the predic-
tions that the statistical translation models make un-
der our various experimental settings, we choose
the alignment error rate (AER) metric, which is de-
fined in Och and Ney (2003). We also investigated
whether improved AER leads to improved transla-
tion quality. We used the alignments created during
our AER experiments as the input to a phrase-based
decoder. We translated a test set of 350 sentences,
and used the Bleu metric (Papineni et al, 2001) to
automatically evaluate machine translation quality.
We used the Verbmobil German-English parallel
corpus as a source of training data because it has
been used extensively in evaluating statistical trans-
lation and alignment accuracy. This data set comes
with a manually word-aligned set of 350 sentences
which we used as our test set.
Our experiments additionally required a very
large set of word-aligned sentence pairs to be in-
corporated in the training set. Since previous work
has shown that when training on the complete set
of 34,000 sentence pairs an alignment error rate as
low as 6% can be achieved for the Verbmobil data,
we automatically generated a set of alignments for
the entire training data set using the unmodified ver-
sion of GIZA++. We wanted to use automatic align-
ments in lieu of actual hand alignments so that we
would be able to perform experiments using large
data sets. We ran a pilot experiment to test whether
our automatic would produce similar results to man-
ual alignments.
We divided our manual word alignments into
training and test sets and compared the performance
of models trained on human aligned data against
models trained on automatically aligned data. A
Size of training corpus
Model .5k 2k 8k 16k
Model 1 29.64 24.66 22.64 21.68
HMM 18.74 15.63 12.39 12.04
Model 3 26.07 18.64 14.39 13.87
Model 4 20.59 16.05 12.63 12.17
Table 1: Alignment error rates for the various IBM
Models trained with sentence-aligned data
100-fold cross validation showed that manual and
automatic alignments produced AER results that
were similar to each other to within 0.1%.2
Having satisfied ourselves that automatic align-
ment were a sufficient stand-in for manual align-
ments, we performed our main experiments which
fell into the following categories:
1. Verifying that the use of word-aligned data has
an impact on the quality of alignments pre-
dicted by the IBM Models, and comparing the
quality increase to that gained by using a bilin-
gual dictionary in the estimation stage.
2. Evaluating whether improved parameter esti-
mates of alignment quality lead to improved
translation quality.
3. Experimenting with how increasing the ratio of
word-aligned to sentence-aligned data affected
the performance.
4. Experimenting with our ? parameter which al-
lows us to weight the relative contributions
of the word-aligned and sentence-aligned data,
and relating it to the ratio experiments.
5. Showing that improvements to AER and trans-
lation quality held for another corpus.
5 Results
5.1 Improved alignment quality
As a staring point for comparison we trained
GIZA++ using four different sized portions of the
Verbmobil corpus. For each of those portions we
output the most probable alignments of the testing
data for Model 1, the HMM, Model 3, and Model
2Note that we stripped out probable alignments from our
manually produced alignments. Probable alignments are large
blocks of words which the annotator was uncertain of how to
align. The many possible word-to-word translations implied by
the manual alignments led to lower results than with the auto-
matic alignments, which contained fewer word-to-word trans-
lation possibilities.
Size of training corpus
Model .5k 2k 8k 16k
Model 1 21.43 18.04 16.49 16.20
HMM 14.42 10.47 9.09 8.80
Model 3 20.56 13.25 10.82 10.51
Model 4 14.19 10.13 7.87 7.52
Table 2: Alignment error rates for the various IBM
Models trained with word-aligned data
4,3 and evaluated their AERs. Table 1 gives align-
ment error rates when training on 500, 2000, 8000,
and 16000 sentence pairs from Verbmobil corpus
without using any word-aligned training data.
We obtained much better results when incorpo-
rating word-alignments with our mixed likelihood
function. Table 2 shows the results for the differ-
ent corpus sizes, when all of the sentence pairs have
been word-aligned. The best performing model in
the unmodified GIZA++ code was the HMM trained
on 16,000 sentence pairs, which had an alignment
error rate of 12.04%. In our modified code the
best performing model was Model 4 trained on
16,000 sentence pairs (where all the sentence pairs
are word-aligned) with an alignment error rate of
7.52%. The difference in the best performing mod-
els represents a 38% relative reduction in AER. In-
terestingly, we achieve a lower AER than the best
performing unmodified models using a corpus that
is one-eight the size of the sentence-aligned data.
Figure 1 show an example of the improved
alignments that are achieved when using the word
aligned data. The example alignments were held
out sentence pairs that were aligned after training on
500 sentence pairs. The alignments produced when
the training on word-aligned data are dramatically
better than when training on sentence-aligned data.
We contrasted these improvements with the im-
provements that are to be had from incorporating a
bilingual dictionary into the estimation process. For
this experiment we allowed a bilingual dictionary
to constrain which words can act as translations of
each other during the initial estimates of translation
probabilities (as described in Och and Ney (2003)).
As can be seen in Table 3, using a dictionary reduces
the AER when compared to using GIZA++ without
a dictionary, but not as dramatically as integrating
the word-alignments. We further tried combining a
dictionary with our word-alignments but found that
the dictionary results in only very minimal improve-
ments over using word-alignments alone.
3We used the default training schemes for GIZA++, and left
model smoothing parameters at their default settings.
Th
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(a) Sentence-aligned
T
h
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(b) Word-aligned
T
h
e
n
a
s
s
u
m
e
.
Dann
reserviere
ich
zwei
Einzelzimmer
I
w
i
l
l
r
e
s
e
r
v
e
t
w
o
s
i
n
g
l
e
,
nehme
r
o
o
m
s
, I
ich
mal
an
.
(c) Reference
Figure 1: Example alignments using sentence-aligned training data (a), using word-aligned data (b), and a
reference manual alignment (c)
Size of training corpus
Model .5k 2k 8k 16k
Model 1 23.56 20.75 18.69 18.37
HMM 15.71 12.15 9.91 10.13
Model 3 22.11 16.93 13.78 12.33
Model 4 17.07 13.60 11.49 10.77
Table 3: The improved alignment error rates when
using a dictionary instead of word-aligned data to
constrain word translations
Sentence-aligned Word-aligned
Size AER Bleu AER Bleu
500 20.59 0.211 14.19 0.233
2000 16.05 0.247 10.13 0.260
8000 12.63 0.265 7.87 0.278
16000 12.17 0.270 7.52 0.282
Table 4: Improved AER leads to improved transla-
tion quality
5.2 Improved translation quality
The fact that using word-aligned data in estimat-
ing the parameters for machine translation leads to
better alignments is predictable. A more signifi-
cant result is whether it leads to improved transla-
tion quality. In order to test that our improved pa-
rameter estimates lead to better translation quality,
we used a state-of-the-art phrase-based decoder to
translate a held out set of German sentences into
English. The phrase-based decoder extracts phrases
from the word alignments produced by GIZA++,
and computes translation probabilities based on the
frequency of one phrase being aligned with another
(Koehn et al, 2003). We trained a language model
AER when when
Ratio ? = Standard MLE ? = .9
0.1 11.73 9.40
0.2 10.89 8.66
0.3 10.23 8.13
0.5 8.65 8.19
0.7 8.29 8.03
0.9 7.78 7.78
Table 5: The effect of weighting word-aligned data
more heavily that its proportion in the training data
(corpus size 16000 sentence pairs)
using the 34,000 English sentences from the train-
ing set.
Table 4 shows that using word-aligned data leads
to better translation quality than using sentence-
aligned data. Particularly, significantly less data is
needed to achieve a high Bleu score when using
word alignments. Training on a corpus of 8,000 sen-
tence pairs with word alignments results in a higher
Bleu score than when training on a corpus of 16,000
sentence pairs without word alignments.
5.3 Weighting the word-aligned data
We have seen that using training data consisting
of entirely word-aligned sentence pairs leads to
better alignment accuracy and translation quality.
However, because manually word-aligning sentence
pairs costs more than just using sentence-aligned
data, it is unlikely that we will ever want to label
an entire corpus. Instead we will likely have a rel-
atively small portion of the corpus word aligned.
We want to be sure that this small amount of data
labeled with word alignments does not get over-
whelmed by a larger amount of unlabeled data.
 
0.07
 
0.075 0.08
 
0.085 0.09
 
0.095 0.1
 
0.105 0.11
 
0.115 0.12
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9
 
1
Alignment Error Rate
Lamb
da20% w
ord-a
ligned
50% 
word-
aligne
d
70% 
word-
aligne
d
100%
 word
-align
ed
Figure 2: The effect on AER of varying ? for a train-
ing corpus of 16K sentence pairs with various pro-
portions of word-alignments
Thus we introduced the ? weight into our mixed
likelihood function.
Table 5 compares the natural setting of ? (where
it is proportional to the amount of labeled data in the
corpus) to a value that amplifies the contribution of
the word-aligned data. Figure 2 shows a variety of
values for ?. It shows as ? increases AER decreases.
Placing nearly all the weight onto the word-aligned
data seems to be most effective.4 Note this did not
vary the training data size ? only the relative contri-
butions between sentence- and word-aligned train-
ing material.
5.4 Ratio of word- to sentence-aligned data
We also varied the ratio of word-aligned to
sentence-aligned data, and evaluated the AER and
Bleu scores, and assigned high value to ? (= 0.9).
Figure 3 shows how AER improves as more
word-aligned data is added. Each curve on the graph
represents a corpus size and shows its reduction in
error rate as more word-aligned data is added. For
example, the bottom curve shows the performance
of a corpus of 16,000 sentence pairs which starts
with an AER of just over 12% with no word-aligned
training data and decreases to an AER of 7.5% when
all 16,000 sentence pairs are word-aligned. This
curve essentially levels off after 30% of the data is
word-aligned. This shows that a small amount of
word-aligned data is very useful, and if we wanted
to achieve a low AER, we would only have to label
4,800 examples with their word alignments rather
than the entire corpus.
Figure 4 shows how the Bleu score improves as
more word-aligned data is added. This graph also
4At ? = 1 (not shown in Figure 2) the data that is only
sentence-aligned is ignored, and the AER is therefore higher.
 
0.06
 
0.08 0.1
 
0.12
 
0.14
 
0.16
 
0.18 0.2
 
0.22  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Alignment error rate
Ratio
 of wo
rd-alig
ned to
 sente
nce-a
ligned
 data
500 s
enten
ce pa
irs
2000 
sente
nce p
airs
8000 
sente
nce p
airs
16000
 sente
nce p
airs
Figure 3: The effect on AER of varying the ratio of
word-aligned to sentence-aligned data
 
0.2
 
0.21
 
0.22
 
0.23
 
0.24
 
0.25
 
0.26
 
0.27
 
0.28
 
0.29  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Bleu Score
Ratio
 of wo
rd-alig
ned to
 sente
nce-a
ligned
 data
500 s
enten
ce pa
irs
2000 
sente
nce p
airs
8000 
sente
nce p
airs
16000
 sente
nce p
airs
Figure 4: The effect on Bleu of varying the ratio of
word-aligned to sentence-aligned data
reinforces the fact that a small amount of word-
aligned data is useful. A corpus of 8,000 sentence
pairs with only 800 of them labeled with word align-
ments achieves a higher Bleu score than a corpus of
16,000 sentence pairs with no word alignments.
5.5 Evaluation using a larger training corpus
We additionally tested whether incorporating word-
level alignments into the estimation improved re-
sults for a larger corpus. We repeated our experi-
ments using the Canadian Hansards French-English
parallel corpus. Figure 6 gives a summary of the im-
provements in AER and Bleu score for that corpus,
when testing on a held out set of 484 hand aligned
sentences.
On the whole, alignment error rates are higher
and Bleu scores are considerably lower for the
Hansards corpus. This is probably due to the dif-
ferences in the corpora. Whereas the Verbmobil
corpus has a small vocabulary (<10,000 per lan-
Sentence-aligned Word-aligned
Size AER Bleu AER Bleu
500 33.65 0.054 25.73 0.064
2000 25.97 0.087 18.57 0.100
8000 19.00 0.115 14.57 0.120
16000 16.59 0.126 13.55 0.128
Table 6: Summary results for AER and translation
quality experiments on Hansards data
guage), the Hansards has ten times that many vocab-
ulary items and has a much longer average sentence
length. This made it more difficult for us to create a
simulated set of hand alignments; we measured the
AER of our simulated alignments at 11.3% (which
compares to 6.5% for our simulated alignments for
the Verbmobil corpus).
Nevertheless, the trend of decreased AER and in-
creased Bleu score still holds. For each size of train-
ing corpus we tested we found better results using
the word-aligned data.
6 Related Work
Och and Ney (2003) is the most extensive analy-
sis to date of how many different factors contribute
towards improved alignments error rates, but the in-
clusion of word-alignments is not considered. Och
and Ney do not give any direct analysis of how
improved word alignments accuracy contributes to-
ward better translation quality as we do here.
Mihalcea and Pedersen (2003) described a shared
task where the goal was to achieve the best AER. A
number of different methods were tried, but none
of them used word-level alignments. Since the best
performing system used an unmodified version of
Giza++, we would expected that our modifed ver-
sion would show enhanced performance. Naturally
this would need to be tested in future work.
Melamed (1998) describes the process of manu-
ally creating a large set of word-level alignments of
sentences in a parallel text.
Nigam et al (2000) described the use of weight
to balance the respective contributions of labeled
and unlabeled data to a mixed likelihood function.
Corduneanu (2002) provides a detailed discussion
of the instability of maximum likelhood solutions
estimated from a mixture of labeled and unlabeled
data.
7 Discussion and Future Work
In this paper we show with the appropriate modifi-
cation of EM significant improvement gains can be
had through labeling word alignments in a bilingual
corpus. Because of this significantly less data is re-
quired to achieve a low alignment error rate or high
Bleu score. This holds even when using noisy word
alignments such as our automatically created set.
One should take our research into account when
trying to efficiently create a statistical machine
translation system for a language pair for which a
parallel corpus is not available. Germann (2001)
describes the cost of building a Tamil-English paral-
lel corpus from scratch, and finds that using profes-
sional translations is prohibitively high. In our ex-
perience it is quicker to manually word-align trans-
lated sentence pairs than to translate a sentence, and
word-level alignment can be done by someone who
might not be fluent enough to produce translations.
It might therefore be possible to achieve a higher
performance at a fraction of the cost by hiring a non-
professional produce word-alignments after a lim-
ited set of sentences have been translated.
We plan to investigate whether it is feasible to
use active learning to select which examples will
be most useful when aligned at the word-level. Sec-
tion 5.4 shows that word-aligning a fraction of sen-
tence pairs in a training corpus, rather than the entire
training corpus can still yield most of the benefits
described in this paper. One would hope that by se-
lectively sampling which sentences are to be manu-
ally word-aligned we would achieve nearly the same
performance as word-aligning the entire corpus.
Acknowledgements
The authors would like to thank Franz Och, Her-
mann Ney, and Richard Zens for providing the
Verbmobil data, and Linear B for providing its
phrase-based decoder.
References
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Adrian Corduneanu. 2002. Stable mixing of complete
and incomplete information. Master?s thesis, Mas-
sachusetts Institute of Technology, February.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1?38, Nov.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France,
July 7.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the HLT/NAACL.
I. Dan Melamed. 1998. Manual annotation of trans-
lational equivalence: The blinker project. Cognitive
Science Technical Report 98/07, University of Penn-
sylvania.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Rada Mihalcea and
Ted Pedersen, editors, HLT-NAACL 2003 Workshop:
Building and Using Parallel Texts.
Kamal Nigam, Andrew K. McCallum, Sebastian Thrun,
and Tom M. Mitchell. 2000. Text classification from
labeled and unlabeled documents using EM. Machine
Learning, 39(2/3):103?134.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. IBM Research Report
RC22176(W0109-022), IBM.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380, September.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 969?976,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Modelling lexical redundancy for machine translation
David Talbot and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
d.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract
Certain distinctions made in the lexicon
of one language may be redundant when
translating into another language. We
quantify redundancy among source types
by the similarity of their distributions over
target types. We propose a language-
independent framework for minimising
lexical redundancy that can be optimised
directly from parallel text. Optimisation
of the source lexicon for a given target lan-
guage is viewed as model selection over a
set of cluster-based translation models.
Redundant distinctions between types may
exhibit monolingual regularities, for ex-
ample, inflexion patterns. We define a
prior over model structure using a Markov
random field and learn features over sets
of monolingual types that are predictive
of bilingual redundancy. The prior makes
model selection more robust without the
need for language-specific assumptions re-
garding redundancy. Using these mod-
els in a phrase-based SMT system, we
show significant improvements in transla-
tion quality for certain language pairs.
1 Introduction
Data-driven machine translation (MT) relies on
models that can be efficiently estimated from par-
allel text. Token-level independence assumptions
based on word-alignments can be used to decom-
pose parallel corpora into manageable units for pa-
rameter estimation. However, if training data is
scarce or language pairs encode significantly dif-
ferent information in the lexicon, such as Czech
and English, additional independence assumptions
may assist the model estimation process.
Standard statistical translation models use sep-
arate parameters for each pair of source and target
types. In these models, distinctions in either lex-
icon that are redundant to the translation process
will result in unwarranted model complexity and
make parameter estimation from limited parallel
data more difficult. A natural way to eliminate
such lexical redundancy is to group types into ho-
mogeneous clusters that do not differ significantly
in their distributions over types in the other lan-
guage. Cluster-based translation models capture
the corresponding independence assumptions.
Previous work on bilingual clustering has fo-
cused on coarse partitions of the lexicon that
resemble automatically induced part-of-speech
classes. These were used to model generic
word-alignment patterns such as noun-adjective
re-ordering between English and French (Och,
1998). In contrast, we induce fine-grained parti-
tions of the lexicon, conceptually closer to auto-
matic lemmatisation, optimised specifically to as-
sign translation probabilities. Unlike lemmatisa-
tion or stemming, our method specifically quanti-
fies lexical redundancy in a bilingual setting and
does not make language-specific assumptions.
We tackle the problem of redundancy in the
translation lexicon via Bayesian model selection
over a set of cluster-based translation models. We
search for the model, defined by a clustering of
the source lexicon, that maximises the marginal
likelihood of target tokens in parallel data. In this
optimisation, source types are combined into clus-
ters if their distributions over target types are too
similar to warrant distinct parameters.
Redundant distinctions between types may ex-
hibit regularities within a language, for instance,
inflexion patterns. These can be used to guide
model selection. Here we show that the inclusion
of a model ?prior? over the lexicon structure leads
to more robust translation models. Although a pri-
ori we do not know which monolingual features
characterise redundancy for a given language pair,
by defining a model over the prior monolingual
969
space of source types and cluster assignments, we
can introduce an inductive bias that allows cluster-
ing decisions in different parts of the lexicon to in-
fluence one another via monolingual features. We
use an EM-type algorithm to learn weights for a
Markov random field parameterisation of this prior
over lexicon structure.
We obtain significant improvements in transla-
tion quality as measured by BLEU, incorporating
these optimised model within a phrase-based SMT
system for three different language pairs. The
MRF prior improves the results and picks up fea-
tures that appear to agree with linguistic intuitions
of redundancy for the language pairs considered.
2 Lexical redundancy between languages
In statistical MT, the source and target lexicons
are usually defined as the sets of distinct types ob-
served in the parallel training corpus for each lan-
guage. Such models may not be optimal for cer-
tain language pairs and training regimes.
A word-level statistical translation model ap-
proximates the probability Pr(E|F ) that a source
type indexed by F will be translated as a target
type indexed by E. Standard models, e.g. Brown
et al (1993), consist of discrete probability distri-
butions with separate parameters for each unique
pairing of a source and target types; no attempt is
made to leverage structure within the event spaces
E and F during parameter estimation. This results
in a large number of parameters that must be esti-
mated from limited amounts of parallel corpora.
We refer to distinctions made between lexical
types in one language that do not result in different
distributions over types in the other language as
lexically redundant for the language pair. Since
the role of the translation model is to determine a
distribution over target types given a source type,
when the corresponding target distributions do not
vary significantly over a set of source types, the
model gains nothing by maintaining a distinct set
of parameters for each member of this set.
Lexical redundancy may arise when languages
differ in the specificity with which they refer to the
same concepts. For instance, colours of the spec-
trum may be partitioned differently (e.g. blue in
English v.s. sinii and goluboi in Russian). It will
also arise when languages explicitly encode differ-
ent information in the lexicon. For example, trans-
lating from French to English, a standard model
would treat the following pairs of source and tar-
get types as distinct events with entirely unre-
lated parameters: (vert, green), (verte, green),
(verts, green) and (vertes, green). Here the
French types differ only in their final suffixes due
to adjectival agreement. Since there is no equiva-
lent mechanism in English, these distinctions are
redundant with respect to this target language.
Distinctions that are redundant in the source
lexicon when translating into one language may,
however, be significant when translating into an-
other. For instance, the French adjectival number
agreement (the addition of an s) may be significant
when translating to Russian which also marks ad-
jectives for number (the inflexion to -ye).
We can remove redundancy from the translation
model by conflating redundant types, e.g. vert .=
{vert, verte, verts, vertes}, and averaging bilin-
gual statistics associated with these events.
3 Eliminating redundancy in the model
Redundancy in the translation model can be
viewed as unwarranted model complexity. A
cluster-based translation model defined via a hard-
clustering of the lexicon can reduce this com-
plexity by introducing additional independence as-
sumptions: given the source cluster label, cj , the
target type, ei, is assumed to be independent of the
exact source type, fj , observed, i.e., p(ei|fj) ?
p(ei|cj). Optimising the model for lexical redun-
dancy can be viewed as model selection over a set
of such cluster-based translation models.
We formulate model search as a maximum a
posteriori optimisation: the data-dependent term,
p(D|C), quantifies evidence provided for a model,
C, by bilingual training data, D, while the prior,
p(C), can assert a preference for a particular
model structure (clustering of the source lexicon)
on the basis of monolingual features. Both terms
have parameters that are estimated from data. For-
mally, we search for C?,
C? = argmaxC p(C|D)
= argmaxC p(C)p(D|C). (1)
Evaluating the data-dependent term, p(D|C), for
different partitions of the source lexicon, we can
compare how well different models predict the tar-
get tokens aligned in a parallel corpus. This term
will prefer models that group together source types
with similar distributions over target types. By
using the marginal likelihood (integrating out the
parameters of the translation model) to calculate
970
p(D|C), we can account explicitly for the com-
plexity of the translation model and compare mod-
els with different numbers of clusters as well as
different assignments of types to clusters.
In addition to an implicit uniform prior over
cluster labels as in k-means clustering (e.g. Chou
(1991)), we also consider a Markov random field
(MRF) parameterisation of the p(C) term to cap-
ture monolingual regularities in the lexicon. The
MRF induces dependencies between clustering
decisions in different parts of the lexicon via a
monolingual feature space biasing the search to-
wards models that exhibit monolingual regulari-
ties. Rather than assuming a priori knowledge of
redundant distinctions in the source language, we
use an EM algorithm to update parameters for fea-
tures defined over sets of source types on the basis
of existing cluster assignments. While initially the
model search will be guided only by information
from the bilingual statistics in p(D|C), monolin-
gual regularities in the lexicon, such as inflexion
patterns, may gradually be propagated through the
model as p(C) becomes informative. Our exper-
iments suggest that the MRF prior enables more
robust model selection.
As stated, the model selection procedure ac-
counts for redundancy in the source lexicon us-
ing the target distributions. The target lexicon
can be optimised analogously. Clustering target
types allows the implementation of independence
assumptions asserting that the exact specification
of a target type is independent of the source type
given knowledge of the target cluster label. For ex-
ample, when translating an English adjective into
French it may be more efficient to use the trans-
lation model to specify only that the translation
lies within a certain set of French adjectives, corre-
sponding to a single lemma, and have the language
model select the exact form. Our experiments sug-
gest that it can be useful to account for redundancy
in both languages in this way; this can be incorpo-
rated simply within our optimisation procedure.
In Section 3.1 we describe the bilingual
marginal likelihood, p(D|C), clustering proce-
dure; in Section 3.2 we introduce the MRF param-
eterisation of the prior, p(C), over model struc-
ture; and in Section 3.3, we describe algorithmic
approximations.
3.1 Bilingual model selection
Assume we are optimising the source lexicon (the
target lexicon is optimised analogously). A clus-
tering of the lexicon is a unique mapping CF :
F ? CF defined for all f ? F where, in addition
to all source types observed in the parallel training
corpus, F may include items seen in other mono-
lingual corpora (and, in the case of the source lex-
icon only, the development and test data). The
standard SMT lexicon can be viewed as a cluster-
ing with each type observed in the parallel training
corpus assigned to a distinct cluster and all other
types assigned to a single ?unknown word? cluster.
We optimise a conditional model of target to-
kens from word-aligned parallel corpora, D =
{Dc0 , ..., DcN }, where Dci represents the set of
target words that were aligned to the set of source
types in cluster ci. We assume that each target to-
ken in the corpus is generated conditionally i.i.d.
given the cluster label of the source type to which
it is aligned. Sufficient statistics for this model
consist of co-occurrence counts of source and tar-
get types summed across each source cluster,
#cf (e)
.=
?
f ??cf
#(e, f ?). (2)
Maximising the likelihood of the data under this
model would require us to specify the number of
clusters (the size of the lexicon) in advance. In-
stead we place a Dirichlet prior parameterised by
?1 over the translation model parameters of each
cluster, ?cf ,e, defining the conditional distribu-
tions over target types. Given a clustering, the
Dirichlet prior, and independent parameters, the
distribution over data and parameters factorises,
p(D,?|CF , ?) =
?
cf?CF
p(Dcf , ?cf |cf , ?)
?
?
cf?CF
?
e?E
?
??1+#cf (e)
cf ,e
We optimise cluster assignments with respect to
the marginal likelihood which averages the like-
lihood of the set of counts assigned to a cluster,
Dcf , under the current model over the prior,
p(Dcf |?, cf ) =
?
p(?cf |?)p(Dcf |?cf , cf )d?cf .
This can be evaluated analytically for a Dirichlet
prior with multinomial parameters.
Assuming a (fixed) uniform prior over model
structure, p(C), model selection involves itera-
tively re-assigning source types to clusters such
as to maximise the marginal likelihood. Re-
assignments may alter the total number of clusters
1Distinct from the prior over model structure, p(C).
971
at any point. Updates can be calculated locally, for
instance, given the sets of target tokens Dci and
Dcj aligned to source types currently in clusters
ci and cj , the change in log marginal likelihood if
clusters ci and cj are merged into cluster c? is,
?ci,cj?c? = log
p(Dc?|?, c?)
p(Dci |?, ci)p(Dcj |?, cj)
, (3)
which is a Bayes factor in favour of the hypothe-
sis that Dci and Dcj were sampled from the same
distribution (Wolpert, 1995). Unlike its equivalent
in maximum likelihood clustering, Eq.(3) may as-
sume positive values favouring a smaller number
of clusters when the data does not support a more
complex hypothesis. The more complex model,
with ci and cj modelled separately, is penalised
for being able to model a wider range of data sets.
The hyperparameter, ?, is tied across clusters
and taken to be proportional to the marginal (the
?background?) distribution over target types in the
corpus. Under this prior, source types aligned to
the same target types, will be clustered together
more readily if these target types are less frequent
in the corpus as a whole.
3.2 Markov random field model prior
As described above we consider a Markov random
field (MRF) parameterisation of the prior over
model structure, p(C). This defines a distribution
over cluster assignments of the source lexicon as a
whole based solely on monolingual characteristics
of the lexical types and the relations between their
respective cluster assignments.
Viewed as graph, each variable in the MRF is
modelled as conditionally independent of all other
variables given the values of its neighbours (the
Markov property; (Geman and Geman, 1984)).
Each variable in the MRF prior corresponds to a
lexical source type and its cluster assignment. Fig.
1 shows a section of the complete model including
the MRF prior for a Welsh source lexicon; shad-
ing denotes cluster assignments and English tar-
get tokens are shown as directed nodes.2 From the
Markov property it follows that this prior decom-
poses over neighbourhoods,
pMRF(C)? e
?
?
f?F
?
f ??Nf
?
i
?i?i(f,f ?,cf ,c?f )
Here Nf is the set of neighbours of source type f ;
i indexes a set of functions ?i(?) that pick out fea-
tures of a clique; each function has a parameter ?i
2The plates represent repeated sampling; each Welsh
source type may be aligned to multiple English tokens.
Figure 1: Model with Markov random field prior
#(f)
#(f)
#(f) #(f)
car
car
#(f)
wales
wales
car
gar cymru
gymru
bar
mar
that we learn from the data; these are tied across
the graph. ? is a free parameter used to control the
overall contribution of the prior in Eq. (1). Here
features are defined over pairs of types but higher-
order interactions can also be modelled. We only
consider ?positive? prior knowledge that is indica-
tive of redundancy among source types. Hence all
features are non-zero only when their arguments
are assigned to the same cluster.
Features can be defined over any aspects of the
lexicon; in our experiments we use binary features
over constrained string edits between types. The
following feature would be 1, for instance, if the
Welsh types cymru and gymru (see Fig. 1), were
assigned to the same cluster.3
?1(fi = (c ?) ? fj = (g ?) ? ci = cj)
Setting the parameters of the MRF prior over
this feature space by hand would require a priori
knowledge of redundancies for the language pair.
In the absence of such knowledge, we use an it-
erative EM algorithm to update the parameters on
the basis of the previous solution to the bilingual
clustering procedure. EM parameter estimation
forces the cluster assignments of the MRF prior to
agree with those obtained on the basis of bilingual
data using monolingual features alone. Since fea-
tures are tied across the MRF, patterns that char-
acterise redundant relations between types will be
re-enforced across the model. For instance (see
Fig. 1), if cymru and gymru are clustered to-
gether, the parameter for feature ?1, shown above,
may increase. This induces a prior preference for
car and gar to form a cluster on subsequent it-
erations. A similar feature defined for mar and
gar in the a priori string edit feature space, on
the other hand, may remain uninformative if not
observed frequently on pairs of types assigned to
the same clusters. In this way, the model learns to
3Here? matches a common substring of both arguments.
972
generalise language-specific redundancy patterns
from a large a priori feature space. Changes in the
prior due to re-assignments can be calculated lo-
cally and combined with the marginal likelihood.
3.3 Algorithmic approximations
The model selection procedure is an EM algo-
rithm. Each source type is initially assigned to
its own cluster and the MRF parameters, ?i, are
initialised to zero. A greedy E-step iteratively re-
assigns each source type to the cluster that max-
imises Eq. (1); cluster statistics are updated af-
ter any re-assignment. To reduce computation, we
only consider re-assignments that would cause at
least one (non-zero) feature in the MRF to fire, or
to clusters containing types sharing target word-
alignments with the current type; types may also
be re-assigned to a cluster of their own at any iter-
ation. When clustering both languages simultane-
ously, we average ?target? statistics over the num-
ber of events in each ?target? cluster in Eq. (2).
We re-estimate the MRF parameters after each
pass through the vocabulary. These are updated
according to MLE using a pseudolikelihood ap-
proximation (Besag, 1986). Since MRF parame-
ters can only be non-zero for features observed on
types clustered together during an E-step, we use
lazy instantiation to work with a large implicit fea-
ture set defined by a constrained string edit.
The algorithm has two free parameters: ? deter-
mining the strength of the Dirichlet prior used in
the marginal likelihood, p(D|C), and ? which de-
termines the contribution of pMRF(C) to Eq. (1).
4 Experiments
Phrase-based SMT systems have been shown to
outperform word-based approaches (Koehn et al,
2003). We evaluate the effects of lexicon model
selection on translation quality by considering two
applications within a phrase-based SMT system.
4.1 Applications to phrase-based SMT
A phrase-based translation model can be estimated
in two stages: first a parallel corpus is aligned at
the word-level and then phrase pairs are extracted
(Koehn et al, 2003). Aligning tokens in paral-
lel sentences using the IBM Models (Brown et
al., 1993), (Och and Ney, 2003) may require less
information than full-blown translation since the
task is constrained by the source and target tokens
present in each sentence pair. In the phrase-level
translation table, however, the model must assign
Source Tokens Types Singletons Test OOV
Czech 468K 54K 29K 6K 469
French 5682K 53K 19K 16K 112
Welsh 4578K 46K 18K 15K 64
Table 1: Parallel corpora used in the experiments.
probabilities to a potentially unconstrained set of
target phrases. We anticipate the optimal model
sizes to be different for these two tasks.
We can incorporate an optimised lexicon at the
word-alignment stage by mapping tokens in the
training corpus to their cluster labels. The map-
ping will not change the number of tokens in a
sentence, hence the word-alignments can be asso-
ciated with the original corpus (see Exp. 1).
To extrapolate a mapping over phrases from our
type-level models we can map each type within
a phrase to its corresponding cluster label. This,
however, results in a large number of distinct
phrases being collapsed down to a single ?clus-
tered phrase?. Using these directly may spread
probability mass too widely. Instead we use
them to smooth the phrase translation model (see
Exp. 2). Here we consider a simple interpolation
scheme; they could also be used within a backoff
model (Yang and Kirchhoff, 2006).
4.2 Experimental set-up
The system we use is described in (Koehn,
2004). The phrase-based translation model in-
cludes phrase-level and lexical weightings in both
directions. We use the decoder?s default behaviour
for unknown words copying them verbatim to the
output. Smoothed trigram language models are es-
timated on training sections of the parallel corpus.
We used the parallel sections of the Prague
Treebank (Cmejrek et al, 2004), French and En-
glish sections of the Europarl corpus (Koehn,
2005) and parallel text from the Welsh Assem-
bly4 (see Table1). The source languages, Czech,
French and Welsh, were chosen on the basis that
they may exhibit different degrees of redundancy
with respect to English and that they differ mor-
phologically. Only the Czech corpus has explicit
morphological annotation.
4.3 Models
All models used in the experiments are defined as
mappings of the source and target vocabularies.
The target vocabulary includes all distinct types
4This Welsh-English parallel text is in the public domain.
Contact the first author for details.
973
seen in the training corpus; the source vocabu-
lary also includes types seen only in development
and test data. Free parameters were set to max-
imize our evaluation metric, BLEU, on develop-
ment data. The results are reported on the test sets
(see Table 1). The baseline mappings used were:
? standard: the identity mapping;
? max-pref : a prefix of no more than n letters;
? min-freq: a prefix with a frequency of at least
n in the parallel training corpus.
? lemmatize: morphological lemmas (Czech)
standard corresponds to the standard SMT lexi-
con. max-pref and min-freq are both simple stem-
ming algorithms that can be applied to raw text.
These mappings result in models defined over
fewer distinct events that will have higher frequen-
cies; min-freq optimises the latter directly. We
optimise over (possibly different) values of n for
source and target languages. The lemmatize map-
ping which maps types to their lemmas was only
applicable to the Czech corpus.
The optimised lexicon models define mappings
directly via their clusterings of the vocabulary. We
consider the following four models:
? src: clustered source lexicon;
? src+mrf : as src with MRF prior;
? src+trg: clustered source and target lexicons;
? src+trg+mrf : as src+trg with MRF priors.
In each case we optimise over ? (a single value for
both languages) and, when using the MRF prior,
over ? (a single value for both languages).
4.4 Experiments
The two sets of experiments evaluate the base-
line models and optimised lexicon models dur-
ing word-alignment and phrase-level translation
model estimation respectively.
? Exp. 1: map the parallel corpus, perform
word-alignment; estimate the phrase transla-
tion model using the original corpus.
? Exp. 2: smooth the phrase translation model,
p(e|f) =
#(e, f) + ?#(ce, cf )
#(f) + ?#(cf )
Here e, f and ce, cf are phrases mapped un-
der the standard model and the model be-
ing tested respectively; ? is set once for all
experiments on development data. Word-
alignments were generated using the optimal
max-pref mapping for each training set.
5 Results
Table 2 shows the changes in BLEU when we in-
corporate the lexicon mappings during the word-
alignment process. The standard SMT lexicon
model is not optimal, as measured by BLEU, for
any of the languages or training set sizes consid-
ered. Increases over this baseline, however, di-
minish with more training data. For both Czech
and Welsh, the explicit model selection procedure
that we have proposed results in better translations
than all of the baseline models when the MRF
prior is used; again these increases diminish with
larger training sets. We note that the stemming
baseline models appear to be more effective for
Czech than for Welsh. The impact of the MRF
prior is also greater for smaller training sets.
Table 3 shows the results of using these models
to smooth the phrase translation table.5 With the
exception of Czech, the improvements are smaller
than for Exp 1. For all source languages and mod-
els we found that it was optimal to leave the tar-
get lexicon unmapped when smoothing the phrase
translation model.
Using lemmatize for word-alignment on the
Czech corpus gave BLEU scores of 32.71 and
37.21 for the 10K and 21K training sets respec-
tively; used to smooth the phrase translation model
it gave scores of 33.96 and 37.18.
5.1 Discussion
Model selection had the largest impact for smaller
data sets suggesting that the complexity of the
standard model is most excessive in sparse data
conditions. The larger improvements seen for
Czech and Welsh suggest that these languages en-
code more redundant information in the lexicon
with respect to English. Potential sources could be
grammatical case markings (Czech) and mutation
patterns (Welsh). The impact of the MRF prior for
smaller data sets suggests it overcomes sparsity in
the bilingual statistics during model selection.
The location of redundancies, in the form of
case markings, at the ends of words in Czech as
assumed by the stemming algorithms may explain
why these performed better on this language than
5The standard model in Exp. 2 is equivalent to the opti-
mised max-pref in Exp. 1.
974
Table 2: BLEU scores with optimised lexicon applied during word-alignment (Exp. 1)
Czech-English French-English Welsh-English
Model 10K sent. 21K 10K 25K 100K 250K 10K 25K 100K 250K
standard 32.31 36.17 20.76 23.17 26.61 27.63 35.45 39.92 45.02 46.47
max-pref 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11
min-freq 33.95 36.98 21.22 23.77 26.74 27.98 36.23 40.65 45.38 46.35
src 33.95 37.27 21.43 24.42 26.99 27.82 36.98 40.98 45.81 46.45
src+mrf 33.97 37.89 21.63 24.38 26.74 28.39 37.36 41.13 46.50 46.56
src+trg 34.24 38.28 22.05 24.02 26.53 27.80 36.83 41.31 45.22 46.51
src+trg+mrf 34.70 38.44 22.33 23.95 26.69 27.75 37.56 42.19 45.18 46.48
Table 3: BLEU scores with optimised lexicon used to smooth phrase-based translation model (Exp. 2)
Czech-English French-English Welsh-English
Model 10K sent. 21K 10K 25K 100K 250K 10K 25K 100K 250K
(standard)5 34.18 37.34 21.63 23.94 26.45 28.25 35.88 41.03 44.82 46.11
max-pref 35.63 38.81 22.49 24.10 26.99 28.26 37.31 40.09 45.57 46.41
min-freq 34.65 37.75 21.14 23.41 26.29 27.47 36.40 40.84 45.75 46.45
src 34.38 37.98 21.28 24.17 26.88 28.35 36.94 39.99 45.75 46.65
src+mrf 36.24 39.70 22.02 24.10 26.82 28.09 37.81 41.04 46.16 46.51
Table 4: System output (Welsh 25K; Exp. 2)
Src ehangu o ffilm i deledu.
Ref an expansion from film into television.
standard expansion of footage to deledu.
max-pref expansion of ffilm to television.
src+mrf expansion of film to television.
Src yw gwarchod cymru fel gwlad brydferth
Ref safeguarding wales as a picturesque country
standard protection of wales as a country brydferth
max-pref protection of wales as a country brydferth
src+mrf protecting wales as a beautiful country
Src cynhyrchu canlyniadau llai na pherffaith
Ref produces results that are less than perfect
standard produce results less than pherffaith
max-pref produce results less than pherffaith
src+mrf generates less than perfect results
Src y dynodiad o graidd y broblem
Ref the identification of the nub of the problem
standard the dynodiad of the heart of the problem
max-pref the dynodiad of the heart of the problem
src+mrf the identified crux of the problem
on Welsh. The highest scoring features in the
MRF (see Table 5) show that Welsh redundancies,
on the other hand, are primarily between initial
characters. Inspection of system output confirms
that OOV types could be mapped to known Welsh
words with the MRF prior but not via stemming
(see Table 4). For each language pair the MRF
learned features that capture intuitively redundant
patterns: adjectival endings for French, case mark-
ings for Czech, and mutation patterns for Welsh.
The greater improvements in Exp. 1 were mir-
rored by higher compression rates for these lex-
icons (see Table. 6) supporting the conjecture
that word-alignment requires less information than
full-blown translation. The results of the lemma-
Table 5: Features learned by MRF prior
Czech French Welsh
(?,? m) (?,? s) (c ?, g ?)
(?,? u) (?,? e) (d ?, dd ?)
(?,? a) (?,? es) (d ?, t ?)
(?,? ch) (? e,? es) (b ?, p ?)
(?,? ho) (? e,? er) (c ?, ch ?)
(? a,? u) (? e,? ent) (b ?, f ?)
Note: Features defined over pairs of source types assigned to
the same cluster; here ? matches a common substring.
Table 6: Optimal lexicon size (ratio of raw vocab.)
Czech French Welsh
Word-alignment 0.26 0.22 0.24
TM smoothing 0.28 0.38 0.51
tizemodel on Czech show the model selection pro-
cedure improving on a simple supervised baseline.
6 Related Work
Previous work on automatic bilingual word clus-
tering has been motivated somewhat differently
and not made use of cluster-based models to as-
sign translation probabilities directly (Wang et
al., 1996), (Och, 1998). There is, however, a
large body of work using morphological analy-
sis to define cluster-based translation models sim-
ilar to ours but in a supervised manner (Zens and
Ney, 2004), (Niessen and Ney, 2004). These
approaches have used morphological annotation
(e.g. lemmas and part of speech tags) to pro-
vide explicit supervision. They have also involved
manually specifying which morphological distinc-
975
tions are redundant (Goldwater and McClosky,
2005). In contrast, we attempt to learn both equiv-
alence classes and redundant relations automat-
ically. Our experiments with orthographic fea-
tures suggest that some morphological redundan-
cies can be acquired in an unsupervised fashion.
The marginal likelihood hard-clustering algo-
rithm that we propose here for translation model
selection can be viewed as a Bayesian k-means al-
gorithm and is an application of Bayesian model
selection techniques, e.g., (Wolpert, 1995). The
Markov random field prior over model structure
extends the fixed uniform prior over clusters im-
plicit in k-means clustering and is common in
computer vision (Geman and Geman, 1984). Re-
cently Basu et al (2004) used an MRF to embody
hard constraints within semi-supervised cluster-
ing. In contrast, we use an iterative EM algo-
rithm to learn soft constraints within the ?prior?
monolingual space based on the results of cluster-
ing with bilingual statistics.
7 Conclusions and Future Work
We proposed a framework for modelling lexical
redundancy in machine translation and tackled op-
timisation of the lexicon via Bayesian model se-
lection over a set of cluster-based translation mod-
els. We showed improvements in translation qual-
ity incorporating these models within a phrase-
based SMT sytem. Additional gains resulted from
the inclusion of an MRF prior over model struc-
ture. We demonstrated that this prior could be
used to learn weights for monolingual features that
characterise bilingual redundancy. Preliminary
experiments defining MRF features over morpho-
logical annotation suggest this model can also
identify redundant distinctions categorised lin-
guistically (for instance, that morphological case
is redundant on Czech nouns and adjectives with
respect to English, while number is redundant only
on adjectives). In future work we will investigate
the use of linguistic resources to define feature sets
for the MRF prior. Lexical redundancy would ide-
ally be addressed in the context of phrases, how-
ever, computation and statistical estimation may
then be significantly more challenging.
Acknowledgements
The authors would like to thank Philipp Koehn for providing
training scripts used in this work; and Steve Renals, Mirella
Lapata and members of the Edinburgh SMT Group for valu-
able comments. This work was supported by an MRC Prior-
ity Area Studentship to the School of Informatics, University
of Edinburgh.
References
Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney.
2004. A probabilistic framework for semi-supervised
clustering. In Proc. of the 10th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining (KDD-2004).
Julian Besag. 1986. The statistical analysis of dirty pictures.
Journal of the Royal Society Series B, 48(2):259?302.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and
Robert Mercer. 1993. The mathematics of machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
Philip A. Chou. 1991. Optimal partitioning for classification
and regression trees. IEEE Trans. on Pattern Analysis and
Machine Intelligence, 13(4).
M. Cmejrek, J. Curin, J. Havelka, J. Hajic, and V. Kubon.
2004. Prague Czech-English dependency treebank: Syn-
tactically annotated resources for machine translation. In
4th International Conference on Language Resources and
Evaluation, Lisbon, Portugal
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of im-
ages. IEEE Trans. on Pattern Analysis and Machine Intel-
ligence, 6:721?741.
Sharon Goldwater and David McClosky. 2005. Improving
statistical MT through morphological analysis. In Proc.
of the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002).
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of the
HLT/NAACL 2003.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of the AMTA 2004.
Philipp Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit 2005.
S. Niessen and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic infor-
mation. Computational Linguistics, 30(2):181?204.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F.-J. Och. 1998. An efficient method for determining bilin-
gual word classes. In Proc. of the European Chapter of
the Association for Computational Linguistics 1998.
Ye-Yi Wang, John Lafferty, and Alex Waibel. 1996. Word
clustering with parallel spoken language corpora. In Proc.
of 4th International Conference on Spoken Language Pro-
cessing, ICSLP 96, Philadelphia, PA.
D.H. Wolpert. 1995. Determining whether two data sets are
from the same distribution. In 15th international work-
shop on Maximum Entropy and Bayesian Methods.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected lan-
guages. In Proc. of the the European Chapter of the Asso-
ciation for Computational Linguistics 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-based
statistical machine translation. In Proc. of the Human
Language Technology Conference (HLT-NAACL 2004).
976
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 512?519,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Randomised Language Modelling for Statistical Machine Translation
David Talbot and Miles Osborne
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
d.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract
A Bloom filter (BF) is a randomised data
structure for set membership queries. Its
space requirements are significantly below
lossless information-theoretic lower bounds
but it produces false positives with some
quantifiable probability. Here we explore the
use of BFs for language modelling in statis-
tical machine translation.
We show how a BF containing n-grams can
enable us to use much larger corpora and
higher-order models complementing a con-
ventional n-gram LM within an SMT sys-
tem. We also consider (i) how to include ap-
proximate frequency information efficiently
within a BF and (ii) how to reduce the er-
ror rate of these models by first checking for
lower-order sub-sequences in candidate n-
grams. Our solutions in both cases retain the
one-sided error guarantees of the BF while
taking advantage of the Zipf-like distribution
of word frequencies to reduce the space re-
quirements.
1 Introduction
Language modelling (LM) is a crucial component in
statistical machine translation (SMT). Standard n-
gram language models assign probabilities to trans-
lation hypotheses in the target language, typically as
smoothed trigram models, e.g. (Chiang, 2005). Al-
though it is well-known that higher-order LMs and
models trained on additional monolingual corpora
can yield better translation performance, the chal-
lenges in deploying large LMs are not trivial. In-
creasing the order of an n-gram model can result in
an exponential increase in the number of parameters;
for corpora such as the English Gigaword corpus, for
instance, there are 300 million distinct trigrams and
over 1.2 billion 5-grams. Since a LMmay be queried
millions of times per sentence, it should ideally re-
side locally in memory to avoid time-consuming re-
mote or disk-based look-ups.
Against this background, we consider a radically
different approach to language modelling: instead
of explicitly storing all distinct n-grams, we store a
randomised representation. In particular, we show
that the Bloom filter (Bloom (1970); BF), a sim-
ple space-efficient randomised data structure for rep-
resenting sets, may be used to represent statistics
from larger corpora and for higher-order n-grams to
complement a conventional smoothed trigrammodel
within an SMT decoder. 1
The space requirements of a Bloom filter are quite
spectacular, falling significantly below information-
theoretic error-free lower bounds while query times
are constant. This efficiency, however, comes at the
price of false positives: the filter may erroneously
report that an item not in the set is a member. False
negatives, on the other hand, will never occur: the
error is said to be one-sided.
In this paper, we show that a Bloom filter can be
used effectively for language modelling within an
SMT decoder and present the log-frequency Bloom
filter, an extension of the standard Boolean BF that
1For extensions of the framework presented here to stand-
alone smoothed Bloom filter language models, we refer the
reader to a companion paper (Talbot and Osborne, 2007).
512
takes advantage of the Zipf-like distribution of cor-
pus statistics to allow frequency information to be
associated with n-grams in the filter in a space-
efficient manner. We then propose a mechanism,
sub-sequence filtering, for reducing the error rates
of these models by using the fact that an n-gram?s
frequency is bound from above by the frequency of
its least frequent sub-sequence.
We present machine translation experiments us-
ing these models to represent information regarding
higher-order n-grams and additional larger mono-
lingual corpora in combination with conventional
smoothed trigram models. We also run experiments
with these models in isolation to highlight the im-
pact of different order n-grams on the translation
process. Finally we provide some empirical analysis
of the effectiveness of both the log frequency Bloom
filter and sub-sequence filtering.
2 The Bloom filter
In this section, we give a brief overview of the
Bloom filter (BF); refer to Broder andMitzenmacher
(2005) for a more in detailed presentation. A BF rep-
resents a set S = {x1, x2, ..., xn} with n elements
drawn from a universe U of size N . The structure is
attractive when N  n. The only significant stor-
age used by a BF consists of a bit array of size m.
This is initially set to hold zeroes. To train the filter
we hash each item in the set k times using distinct
hash functions h1, h2, ..., hk. Each function is as-
sumed to be independent from each other and to map
items in the universe to the range 1 to m uniformly
at random. The k bits indexed by the hash values
for each item are set to 1; the item is then discarded.
Once a bit has been set to 1 it remains set for the life-
time of the filter. Distinct items may not be hashed
to k distinct locations in the filter; we ignore col-
lisons. Bits in the filter can, therefore, be shared by
distinct items allowing significant space savings but
introducing a non-zero probability of false positives
at test time. There is no way of directly retrieving or
ennumerating the items stored in a BF.
At test time we wish to discover whether a given
item was a member of the original set. The filter is
queried by hashing the test item using the same k
hash functions. If all bits referenced by the k hash
values are 1 then we assume that the item was a
member; if any of them are 0 then we know it was
not. True members are always correctly identified,
but a false positive will occur if all k corresponding
bits were set by other items during training and the
item was not a member of the training set. This is
known as a one-sided error.
The probability of a false postive, f , is clearly the
probability that none of k randomly selected bits in
the filter are still 0 after training. Letting p be the
proportion of bits that are still zero after these n ele-
ments have been inserted, this gives,
f = (1? p)k.
As n items have been entered in the filter by hashing
each k times, the probability that a bit is still zero is,
p
?
=
(
1?
1
m
)kn
? e?
kn
m
which is the expected value of p. Hence the false
positive rate can be approximated as,
f = (1? p)k ? (1? p
?
)k ?
(
1? e?
kn
m
)k
.
By taking the derivative we find that the number of
functions k? that minimizes f is,
k? = ln 2 ?
m
n
.
which leads to the intuitive result that exactly half
the bits in the filter will be set to 1 when the optimal
number of hash functions is chosen.
The fundmental difference between a Bloom fil-
ter?s space requirements and that of any lossless rep-
resentation of a set is that the former does not depend
on the size of the (exponential) universe N from
which the set is drawn. A lossless representation
scheme (for example, a hash map, trie etc.) must de-
pend on N since it assigns a distinct representation
to each possible set drawn from the universe.
3 Language modelling with Bloom filters
In our experiments we make use of both standard
(i.e. Boolean) BFs containing n-gram types drawn
from a training corpus and a novel BF scheme, the
log-frequency Bloom filter, that allows frequency
information to be associated efficiently with items
stored in the filter.
513
Algorithm 1 Training frequency BF
Input: Strain, {h1, ...hk} and BF = ?
Output: BF
for all x ? Strain do
c(x)? frequency of n-gram x in Strain
qc(x)? quantisation of c(x) (Eq. 1)
for j = 1 to qc(x) do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
BF [hi(x)]? 1
end for
end for
end for
return BF
3.1 Log-frequency Bloom filter
The efficiency of our scheme for storing n-gram
statistics within a BF relies on the Zipf-like distribu-
tion of n-gram frequencies in natural language cor-
pora: most events occur an extremely small number
of times, while a small number are very frequent.
We quantise raw frequencies, c(x), using a loga-
rithmic codebook as follows,
qc(x) = 1 + blogb c(x)c. (1)
The precision of this codebook decays exponentially
with the raw counts and the scale is determined by
the base of the logarithm b; we examine the effect of
this parameter in experiments below.
Given the quantised count qc(x) for an n-gram x,
the filter is trained by entering composite events con-
sisting of the n-gram appended by an integer counter
j that is incremented from 1 to qc(x) into the filter.
To retrieve the quantised count for an n-gram, it is
first appended with a count of 1 and hashed under
the k functions; if this tests positive, the count is in-
cremented and the process repeated. The procedure
terminates as soon as any of the k hash functions hits
a 0 and the previous count is reported. The one-sided
error of the BF and the training scheme ensure that
the actual quantised count cannot be larger than this
value. As the counts are quantised logarithmically,
the counter will be incremented only a small number
of times. The training and testing routines are given
here as Algorithms 1 and 2 respectively.
Errors for the log-frequency BF scheme are one-
sided: frequencies will never be underestimated.
Algorithm 2 Test frequency BF
Input: x, MAXQCOUNT , {h1, ...hk} and BF
Output: Upper bound on qc(x) ? Strain
for j = 1 to MAXQCOUNT do
for i = 1 to k do
hi(x)? hash of event {x, j} under hi
if BF [hi(x)] = 0 then
return j ? 1
end if
end for
end for
The probability of overestimating an item?s fre-
quency decays exponentially with the size of the
overestimation error d (i.e. as fd for d > 0) since
each erroneous increment corresponds to a single
false positive and d such independent events must
occur together.
3.2 Sub-sequence filtering
The error analysis in Section 2 focused on the false
positive rate of a BF; if we deploy a BF within an
SMT decoder, however, the actual error rate will also
depend on the a priori membership probability of
items presented to it. The error rate Err is,
Err = Pr(x /? Strain|Decoder)f.
This implies that, unlike a conventional lossless data
structure, the model?s accuracy depends on other
components in system and how it is queried.
We take advantage of the monotonicity of the n-
gram event space to place upper bounds on the fre-
quency of an n-gram prior to testing for it in the filter
and potentially truncate the outer loop in Algorithm
2 when we know that the test could only return pos-
tive in error.
Specifically, if we have stored lower-order n-
grams in the filter, we can infer that an n-gram can-
not present, if any of its sub-sequences test nega-
tive. Since our scheme for storing frequencies can
never underestimate an item?s frequency, this rela-
tion will generalise to frequencies: an n-gram?s fre-
quency cannot be greater than the frequency of its
least frequent sub-sequence as reported by the filter,
c(w1, ..., wn) ? min {c(w1, ..., wn?1), c(w2, ..., wn)}.
We use this to reduce the effective error rate of BF-
LMs that we use in the experiments below.
514
3.3 Bloom filter language model tests
A standard BF can implement a Boolean ?language
model? test: have we seen some fragment of lan-
guage before? This does not use any frequency in-
formation. The Boolean BF-LM is a standard BF
containing all n-grams of a certain length in the
training corpus, Strain. It implements the following
binary feature function in a log-linear decoder,
?bool(x) ? ?(x ? Strain)
Separate Boolean BF-LMs can be included for
different order n and assigned distinct log-linear
weights that are learned as part of a minimum error
rate training procedure (see Section 4).
The log-frequency BF-LM implements a multino-
mial feature function in the decoder that returns the
value associated with an n-gram by Algorithm 2.
?logfreq(x) ? qc(x) ? Strain
Sub-sequence filtering can be performed by using
the minimum value returned by lower-order models
as an upper-bound on the higher-order models.
By boosting the score of hypotheses containing n-
grams observed in the training corpus while remain-
ing agnostic for unseen n-grams (with the exception
of errors), these feature functions have more in com-
mon with maximum entropy models than conven-
tionally smoothed n-gram models.
4 Experiments
We conducted a range of experiments to explore the
effectiveness and the error-space trade-off of Bloom
filters for language modelling in SMT. The space-
efficiency of these models also allows us to inves-
tigate the impact of using much larger corpora and
higher-order n-grams on translation quality. While
our main experiments use the Bloom filter models in
conjunction with a conventional smoothed trigram
model, we also present experiments with these mod-
els in isolation to highlight the impact of different
order n-grams on the translation process. Finally,
we present some empirical analysis of both the log-
frequency Bloom filter and the sub-sequence filter-
ing technique which may be of independent interest.
Model EP-KN-3 EP-KN-4 AFP-KN-3
Memory 64M 99M 1.3G
gzip size 21M 31M 481M
1-gms 62K 62K 871K
2-gms 1.3M 1.3M 16M
3-gms 1.1M 1.0M 31M
4-gms N/A 1.1M N/A
Table 1: Baseline and Comparison Models
4.1 Experimental set-up
All of our experiments use publically available re-
sources. We use the French-English section of the
Europarl (EP) corpus for parallel data and language
modelling (Koehn, 2003) and the English Giga-
word Corpus (LDC2003T05; GW) for additional
language modelling.
Decoding is carried-out using the Moses decoder
(Koehn and Hoang, 2007). We hold out 500 test sen-
tences and 250 development sentences from the par-
allel text for evaluation purposes. The feature func-
tions in our models are optimised using minimum
error rate training and evaluation is performed using
the BLEU score.
4.2 Baseline and comparison models
Our baseline LM and other comparison models are
conventional n-gram models smoothed using modi-
fied Kneser-Ney and built using the SRILM Toolkit
(Stolcke, 2002); as is standard practice these models
drop entries for n-grams of size 3 and above when
the corresponding discounted count is less than 1.
The baseline language model, EP-KN-3, is a trigram
model trained on the English portion of the parallel
corpus. For additional comparisons we also trained a
smoothed 4-gram model on this Europarl data (EP-
KN-4) and a trigram model on the Agence France
Press section of the Gigaword Corpus (AFP-KN-3).
Table 1 shows the amount of memory these mod-
els take up on disk and compressed using the gzip
utility in parentheses as well as the number of dis-
tinct n-grams of each order. We give the gzip com-
pressed size as an optimistic lower bound on the size
of any lossless representation of each model.2
2Note, in particular, that gzip compressed files do not sup-
port direct random access as required by our application.
515
Corpus Europarl Gigaword
1-gms 61K 281K
2-gms 1.3M 5.4M
3-gms 4.7M 275M
4-gms 9.0M 599M
5-gms 10.3M 842M
6-gms 10.7M 957M
Table 2: Number of distinct n-grams
4.3 Bloom filter-based models
To create Bloom filter LMs we gathered n-gram
counts from both the Europarl (EP) and the whole
of the Gigaword Corpus (GW). Table 2 shows the
numbers of distinct n-grams in these corpora. Note
that we use no pruning for these models and that
the numbers of distinct n-grams is of the same or-
der as that of the recently released Google Ngrams
dataset (LDC2006T13). In our experiments we cre-
ate a range of models referred to by the corpus used
(EP or GW), the order of the n-gram(s) entered into
the filter (1 to 10), whether the model is Boolean
(Bool-BF) or provides frequency information (Freq-
BF), whether or not sub-sequence filtering was used
(FTR) and whether it was used in conjunction with
the baseline trigram (+EP-KN-3).
4.4 Machine translation experiments
Our first set of experiments examines the relation-
ship between memory allocated to the BF and BLEU
score. We present results using the Boolean BF-
LM in isolation and then both the Boolean and log-
frequency BF-LMS to add 4-grams to our baseline
3-gram model.Our second set of experiments adds
3-grams and 5-grams from the Gigaword Corpus to
our baseline. Here we constrast the Boolean BF-
LM with the log-frequency BF-LM with different
quantisation bases (2 = fine-grained and 5 = coarse-
grained). We then evaluate the sub-sequence fil-
tering approach to reducing the actual error rate of
these models by adding both 3 and 4-grams from the
Gigaword Corpus to the baseline. Since the BF-LMs
easily allow us to deploy very high-order n-gram
models, we use them to evaluate the impact of dif-
ferent order n-grams on the translation process pre-
senting results using the Boolean and log-frequency
BF-LM in isolation for n-grams of order 1 to 10.
Model EP-KN-3 EP-KN-4 AFP-KN-3
BLEU 28.51 29.24 29.17
Memory 64M 99M 1.3G
gzip size 21M 31M 481M
Table 3: Baseline and Comparison Models
4.5 Analysis of BF extensions
We analyse our log-frequency BF scheme in terms
of the additional memory it requires and the error
rate compared to a non-redundant scheme. The non-
redundant scheme involves entering just the exact
quantised count for each n-gram and then searching
over the range of possible counts at test time starting
with the count with maximum a priori probability
(i.e. 1) and incrementing until a count is found or
the whole codebook has been searched (here the size
is 16).
We also analyse the sub-sequence filtering
scheme directly by creating a BF with only 3-grams
and a BF containing both 2-grams and 3-grams and
comparing their actual error rates when presented
with 3-grams that are all known to be negatives.
5 Results
5.1 Machine translation experiments
Table 3 shows the results of the baseline (EP-KN-
3) and other conventional n-gram models trained on
larger corpora (AFP-KN-3) and using higher-order
dependencies (EP-KN-4). The larger models im-
prove somewhat on the baseline performance.
Figure 1 shows the relationship between space al-
located to the BF models and BLEU score (left) and
false positive rate (right) respectively. These experi-
ments do not include the baseline model. We can see
a clear correlation between memory / false positive
rate and translation performance.
Adding 4-grams in the form of a Boolean BF or a
log-frequency BF (see Figure 2) improves on the 3-
gram baseline with little additional memory (around
4MBs) while performing on a par with or above
the Europarl 4-gram model with around 10MBs;
this suggests that a lossy representation of the un-
pruned set of 4-grams contains more useful informa-
tion than a lossless representation of the pruned set.3
3An unpruned modified Kneser-Ney 4-gram model on the
Eurpoparl data scores slightly higher - 29.69 - while taking up
489MB (132MB gzipped).
516
 
29
 
28
 
27
 
26
 
25
 
10
 
8
 
6
 
4
 
2
 
1
 
0.8
 
0.6
 
0.4
 
0.2
 
0
BLEU Score
False positive rate
Mem
ory in
 MB
Europ
arl Bo
olean
 BF 4
-gram
 (alone
)
BLEU
 Scor
e Boo
l-BF-E
P-4
False
 posit
ive ra
te
Figure 1: Space/Error vs. BLEU Score.
 
30.5  30
 
29.5  29
 
28.5  28
 
9
 
7
 
5
 
3
 
1
BLEU Score
Mem
ory in
 MB
EP-B
ool-B
F-4 a
nd Fr
eq-BF
-4 (with
 EP-KN
-3)
EP-B
ool-B
F-4 +
 EP-K
N-3
EP-F
req-B
F-4 +
 EP-K
N-3
EP-K
N-4 c
ompa
rison 
(99M / 
31M gz
ip)
EP-K
N-3 b
aselin
e (64M
 / 21M 
gzip)
Figure 2: Adding 4-grams with Bloom filters.
As the false positive rate exceeds 0.20 the perfor-
mance is severly degraded. Adding 3-grams drawn
from the whole of the Gigaword corpus rather than
simply the Agence France Press section results in
slightly improved performance with signficantly less
memory than the AFP-KN-3 model (see Figure 3).
Figure 4 shows the results of adding 5-grams
drawn from the Gigaword corpus to the baseline. It
also contrasts the Boolean BF and the log-frequency
BF suggesting in this case that the log-frequency BF
can provide useful information when the quantisa-
tion base is relatively fine-grained (base 2). The
Boolean BF and the base 5 (coarse-grained quan-
tisation) log-frequency BF perform approximately
the same. The base 2 quantisation performs worse
 
30.5  30
 
29.5  29
 
28.5  28
 
27.5  27
 
1
 
0.8
 
0.6
 
0.4
 
0.2
 
0.1
BLEU Score
Mem
ory in
 GB
GW-B
ool-B
F-3 a
nd GW
-Freq
-BF-3
 (with E
P-KN-3
)
GW-B
ool-B
F-3 +
 EP-K
N-3
GW-F
req-B
F-3 +
 EP-K
N-3
AFP-
KN-3
 + EP
-KN-3
Figure 3: Adding GW 3-grams with Bloom filters.
 
30.5  30
 
29.5  29
 
28.5  28
 
1
 
0.8
 
0.6
 
0.4
 
0.2
 
0.1
BLEU Score
Mem
ory in
 GB
GW-B
ool-B
F-5 a
nd GW
-Freq
-BF-5
 (base 
2 and 5
) (with 
EP-KN
-3)
GW-B
ool-B
F-5 +
 EP-K
N-3
GW-F
req-B
F-5 (ba
se 2) +
 EP-KN
-3
GW-F
req-B
F-5 (ba
se 5) +
 EP-KN
-3
AFP-
KN-3
 + EP
-KN-3
Figure 4: Comparison of different quantisation rates.
for smaller amounts of memory, possibly due to the
larger set of events it is required to store.
Figure 5 shows sub-sequence filtering resulting in
a small increase in performance when false positive
rates are high (i.e. less memory is allocated). We
believe this to be the result of an increased a pri-
ori membership probability for n-grams presented
to the filter under the sub-sequence filtering scheme.
Figure 6 shows that for this task the most useful
n-gram sizes are between 3 and 6.
5.2 Analysis of BF extensions
Figure 8 compares the memory requirements of
the log-frequencey BF (base 2) and the Boolean
517
 
31
 
30
 
29
 
28
 
1
 
0.8
 
0.6
 
0.4
 
0.2
BLEU Score
Mem
ory in
 MB
GW-B
ool-B
F-3-4
-FTR 
and G
W-Bo
ol-BF
-3-4 (w
ith EP-
KN-3)
GW-B
ool-B
F-3-4
-FTR 
+ EP-
KN-3
GW-B
ool-B
F-3-4
 + EP
-KN-3
Figure 5: Effect of sub-sequence filtering.
 
27
 
26
 
25
 
24
 
10
 
9
 
8
 
7
 
6
 
5
 
4
 
3
 
2
 
1
BLEU Score
N-gra
m ord
er
EP-B
ool-B
F and
 EP-F
req-B
F with
 differ
ent or
der N
-gram
s (alon
e)
EP-B
ool-B
F
EP-F
req-B
F
Figure 6: Impact of n-grams of different sizes.
BF for various order n-gram sets from the Giga-
word Corpus with the same underlying false posi-
tive rate (0.125). The additional space required by
our scheme for storing frequency information is less
than a factor of 2 compared to the standard BF.
Figure 7 shows the number and size of frequency
estimation errors made by our log-frequency BF
scheme and a non-redundant scheme that stores only
the exact quantised count. We presented 500K nega-
tives to the filter and recorded the frequency of over-
estimation errors of each size. As shown in Section
3.1, the probability of overestimating an item?s fre-
quency under the log-frequency BF scheme decays
exponentially in the size of this overestimation er-
ror. Although the non-redundant scheme requires
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
16
 
15
 
14
 
13
 
12
 
11
 
10
 
9
 
8
 
7
 
6
 
5
 
4
 
3
 
2
 
1
Frequency (K)
Size o
f over
estim
ation 
error
Frequ
ency 
Estim
ation 
Errors
 on 50
0K Ne
gative
s
Log-fr
equen
cy BF
 (Bloom
 error =
 0.159)
Non-r
edund
ant sc
heme
 (Bloom
 error =
 0.076)
Figure 7: Frequency estimation errors.
 
0
 
100
 
200
 
300
 
400
 
500
 
600
 
700  1
 
2
 
3
 
4
 
5
 
6
 
7
Memory (MB)
N-gra
m ord
er (Gig
aword)
Mem
ory re
quirem
ents f
or 0.1
25 fal
se po
sitive 
rate
Bool-
BF
Freq-
BF (log
 base-2
 quanti
sation)
Figure 8: Comparison of memory requirements.
fewer items be stored in the filter and, therefore, has
a lower underlying false positive rate (0.076 versus
0.159), in practice it incurs a much higher error rate
(0.717) with many large errors.
Figure 9 shows the impact of sub-sequence filter-
ing on the actual error rate. Although, the false pos-
itive rate for the BF containing 2-grams, in addition,
to 3-grams (filtered) is higher than the false positive
rate of the unfiltered BF containing only 3-grams,
the actual error rate of the former is lower for mod-
els with less memory. By testing for 2-grams prior
to querying for the 3-grams, we can avoid perform-
ing some queries that may otherwise have incurred
errors using the fact that a 3-gram cannot be present
if one of its constituent 2-grams is absent.
518
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7  0.
5
 
1
 
1.5
 
2
 
2.5
 
3
 
3.5
Error rate
Mem
ory (MB
)
Error
 rate 
with s
ub-se
quenc
e filte
ring
Filter
ed fal
se po
sitive 
rate
Unfilt
ered 
false 
pos ra
te / ac
tual e
rror ra
te
Filter
ed ac
tual e
rror ra
te
Figure 9: Error rate with sub-sequence filtering.
6 Related Work
We are not the first people to consider building very
large scale LMs: Kumar et al used a four-gram
LM for re-ranking (Kumar et al, 2005) and in un-
published work, Google used substantially larger n-
grams in their SMT system. Deploying such LMs
requires either a cluster of machines (and the over-
heads of remote procedure calls), per-sentence fil-
tering (which again, is slow) and/or the use of some
other lossy compression (Goodman and Gao, 2000).
Our approach can complement all these techniques.
Bloom filters have been widely used in database
applications for reducing communications over-
heads and were recently applied to encode word
frequencies in information retrieval (Linari and
Weikum, 2006) using a method that resembles the
non-redundant scheme described above. Exten-
sions of the BF to associate frequencies with items
in the set have been proposed e.g., (Cormode and
Muthukrishn, 2005); while these schemes are more
general than ours, they incur greater space overheads
for the distributions that we consider here.
7 Conclusions
We have shown that Bloom Filters can form the ba-
sis for space-efficient language modelling in SMT.
Extending the standard BF structure to encode cor-
pus frequency information and developing a strat-
egy for reducing the error rates of these models by
sub-sequence filtering, our models enable higher-
order n-grams and larger monolingual corpora to be
used more easily for language modelling in SMT.
In a companion paper (Talbot and Osborne, 2007)
we have proposed a framework for deriving con-
ventional smoothed n-gram models from the log-
frequency BF scheme allowing us to do away en-
tirely with the standard n-gram model in an SMT
system. We hope the present work will help estab-
lish the Bloom filter as a practical alternative to con-
ventional associative data structures used in compu-
tational linguistics. The framework presented here
shows that with some consideration for its workings,
the randomised nature of the Bloom filter need not
be a significant impediment to is use in applications.
References
B. Bloom. 1970. Space/time tradeoffs in hash coding with
allowable errors. CACM, 13:422?426.
A. Broder and M. Mitzenmacher. 2005. Network applications
of bloom filters: A survey. Internet Mathematics, 1(4):485?
509.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 263?270, Ann Arbor, Michigan.
G. Cormode and S. Muthukrishn. 2005. An improved data
stream summary: the count-min sketch and its applications.
Journal of Algorithms, 55(1):58?75.
J. Goodman and J. Gao. 2000. Language model size reduction
by pruning and clustering. In ICSLP?00, Beijing, China.
Philipp Koehn and Hieu Hoang. 2007. Factored translation
models. In Proc. of the 2007 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP/Co-NLL).
P. Koehn. 2003. Europarl: A multilingual corpus for eval-
uation of machine translation philipp koehn, draft. Available
at:http://people.csail.mit.edu/ koehn/publications/europarl.ps.
S. Kumar, Y. Deng, and W. Byrne. 2005. Johns Hopkins Uni-
versity - Cambridge University Chinese-English and Arabic-
English 2005 NIST MT Evaluation Systems. In Proceedings
of 2005 NIST MT Workshop, June.
Alessandro Linari and Gerhard Weikum. 2006. Efficient peer-
to-peer semantic overlay networks based on statistical lan-
guage models. In Proceedings of the International Workshop
on IR in Peer-to-Peer Networks, pages 9?16, Arlington.
Andreas Stolcke. 2002. SRILM ? An extensible language mod-
eling toolkit. In Proc. of the Intl. Conf. on Spoken Lang.
Processing, 2002.
David Talbot and Miles Osborne. 2007. Smoothed Bloom fil-
ter language models: Tera-scale LMs on the cheap. In Pro-
ceedings of the 2007 Conference on Empirical Methods in
Natural Language Processing (EMNLP/Co-NLL), June.
519
Proceedings of ACL-08: HLT, pages 505?513,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Randomized Language Models via Perfect Hash Functions
David Talbot?
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh, UK
d.r.talbot@sms.ed.ac.uk
Thorsten Brants
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
brants@google.com
Abstract
We propose a succinct randomized language
model which employs a perfect hash func-
tion to encode fingerprints of n-grams and
their associated probabilities, backoff weights,
or other parameters. The scheme can repre-
sent any standard n-gram model and is easily
combined with existing model reduction tech-
niques such as entropy-pruning. We demon-
strate the space-savings of the scheme via ma-
chine translation experiments within a dis-
tributed language modeling framework.
1 Introduction
Language models (LMs) are a core component in
statistical machine translation, speech recognition,
optical character recognition and many other areas.
They distinguish plausible word sequences from a
set of candidates. LMs are usually implemented
as n-gram models parameterized for each distinct
sequence of up to n words observed in the train-
ing corpus. Using higher-order models and larger
amounts of training data can significantly improve
performance in applications, however the size of the
resulting LM can become prohibitive.
With large monolingual corpora available in ma-
jor languages, making use of all the available data
is now a fundamental challenge in language mod-
eling. Efficiency is paramount in applications such
as machine translation which make huge numbers
of LM requests per sentence. To scale LMs to larger
corpora with higher-order dependencies, researchers
?Work completed while this author was at Google Inc.
have considered alternative parameterizations such
as class-based models (Brown et al, 1992), model
reduction techniques such as entropy-based pruning
(Stolcke, 1998), novel represention schemes such as
suffix arrays (Emami et al, 2007), Golomb Coding
(Church et al, 2007) and distributed language mod-
els that scale more readily (Brants et al, 2007).
In this paper we propose a novel randomized lan-
guage model. Recent work (Talbot and Osborne,
2007b) has demonstrated that randomized encod-
ings can be used to represent n-gram counts for
LMs with signficant space-savings, circumventing
information-theoretic constraints on lossless data
structures by allowing errors with some small prob-
ability. In contrast the representation scheme used
by our model encodes parameters directly. It can
be combined with any n-gram parameter estimation
method and existing model reduction techniques
such as entropy-based pruning. Parameters that are
stored in the model are retrieved without error; how-
ever, false positives may occur whereby n-grams not
in the model are incorrectly ?found? when requested.
The false positive rate is determined by the space us-
age of the model.
Our randomized language model is based on the
Bloomier filter (Chazelle et al, 2004). We encode
fingerprints (random hashes) of n-grams together
with their associated probabilities using a perfect
hash function generated at random (Majewski et al,
1996). Lookup is very efficient: the values of 3 cells
in a large array are combined with the fingerprint of
an n-gram. This paper focuses on machine transla-
tion. However, many of our findings should transfer
to other applications of language modeling.
505
2 Scaling Language Models
In statistical machine translation (SMT), LMs are
used to score candidate translations in the target lan-
guage. These are typically n-gram models that ap-
proximate the probability of a word sequence by as-
suming each token to be independent of all but n?1
preceding tokens. Parameters are estimated from
monolingual corpora with parameters for each dis-
tinct word sequence of length l ? [n] observed in
the corpus. Since the number of parameters grows
somewhat exponentially with n and linearly with the
size of the training corpus, the resulting models can
be unwieldy even for relatively small corpora.
2.1 Scaling Strategies
Various strategies have been proposed to scale LMs
to larger corpora and higher-order dependencies.
Model-based techniques seek to parameterize the
model more efficiently (e.g. latent variable models,
neural networks) or to reduce the model size directly
by pruning uninformative parameters, e.g. (Stolcke,
1998), (Goodman and Gao, 2000). Representation-
based techniques attempt to reduce space require-
ments by representing the model more efficiently or
in a form that scales more readily, e.g. (Emami et al,
2007), (Brants et al, 2007), (Church et al, 2007).
2.2 Lossy Randomized Encodings
A fundamental result in information theory (Carter
et al, 1978) states that a random set of objects can-
not be stored using constant space per object as the
universe from which the objects are drawn grows
in size: the space required to uniquely identify an
object increases as the set of possible objects from
which it must be distinguished grows. In language
modeling the universe under consideration is the
set of all possible n-grams of length n for given
vocabulary. Although n-grams observed in natu-
ral language corpora are not randomly distributed
within this universe no lossless data structure that we
are aware of can circumvent this space-dependency
on both the n-gram order and the vocabulary size.
Hence as the training corpus and vocabulary grow, a
model will require more space per parameter.
However, if we are willing to accept that occa-
sionally our model will be unable to distinguish be-
tween distinct n-grams, then it is possible to store
each parameter in constant space independent of
both n and the vocabulary size (Carter et al, 1978),
(Talbot and Osborne, 2007a). The space required in
such a lossy encoding depends only on the range of
values associated with the n-grams and the desired
error rate, i.e. the probability with which two dis-
tinct n-grams are assigned the same fingerprint.
2.3 Previous Randomized LMs
Recent work (Talbot and Osborne, 2007b) has used
lossy encodings based on Bloom filters (Bloom,
1970) to represent logarithmically quantized cor-
pus statistics for language modeling. While the ap-
proach results in significant space savings, working
with corpus statistics, rather than n-gram probabil-
ities directly, is computationally less efficient (par-
ticularly in a distributed setting) and introduces a
dependency on the smoothing scheme used. It also
makes it difficult to leverage existing model reduc-
tion strategies such as entropy-based pruning that
are applied to final parameter estimates.
In the next section we describe our randomized
LM scheme based on perfect hash functions. This
scheme can be used to encode any standard n-gram
model which may first be processed using any con-
ventional model reduction technique.
3 Perfect Hash-based Language Models
Our randomized LM is based on the Bloomier filter
(Chazelle et al, 2004). We assume the n-grams and
their associated parameter values have been precom-
puted and stored on disk. We then encode the model
in an array such that each n-gram?s value can be re-
trieved. Storage for this array is the model?s only
significant space requirement once constructed.1
The model uses randomization to map n-grams
to fingerprints and to generate a perfect hash func-
tion that associates n-grams with their values. The
model can erroneously return a value for an n-gram
that was never actually stored, but will always return
the correct value for an n-gram that is in the model.
We will describe the randomized algorithm used to
encode n-gram parameters in the model, analyze the
probability of a false positive, and explain how we
construct and query the model in practice.
1Note that we do not store the n-grams explicitly and there-
fore that the model?s parameter set cannot easily be enumerated.
506
3.1 N -gram Fingerprints
We wish to encode a set of n-gram/value pairs
S = {(x1, v(x1)), (x2, v(x2)), . . . , (xN , v(xN ))}
using an array A of size M and a perfect hash func-
tion. Each n-gram xi is drawn from some set of pos-
sible n-grams U and its associated value v(xi) from
a corresponding set of possible values V .
We do not store the n-grams and their proba-
bilities directly but rather encode a fingerprint of
each n-gram f(xi) together with its associated value
v(xi) in such a way that the value can be retrieved
when the model is queried with the n-gram xi.
A fingerprint hash function f : U ? [0, B ? 1]
maps n-grams to integers between 0 and B ? 1.2
The array A in which we encode n-gram/value pairs
has addresses of size dlog2 Be hence B will deter-
mine the amount of space used per n-gram. There
is a trade-off between space and error rate since the
larger B is, the lower the probability of a false pos-
itive. This is analyzed in detail below. For now we
assume only that B is at least as large as the range
of values stored in the model, i.e. B ? |V|.
3.2 Composite Perfect Hash Functions
The function used to associate n-grams with their
values (Eq. (1)) combines a composite perfect hash
function (Majewski et al, 1996) with the finger-
print function. An example is shown in Fig. 1.
The composite hash function is made up of k in-
dependent hash functions h1, h2, . . . , hk where each
hi : U ? [0,M ? 1] maps n-grams to locations in
the array A. The lookup function is then defined as
g : U ? [0, B ? 1] by3
g(xi) = f(xi)?
(
k?
i=1
A[hi(xi)]
)
(1)
where f(xi) is the fingerprint of n-gram xi and
A[hi(xi)] is the value stored in location hi(xi) of the
array A. Eq. (1) is evaluated to retrieve an n-gram?s
parameter during decoding. To encode our model
correctly we must ensure that g(xi) = v(xi) for all
n-grams in our set S. Generating A to encode this
2The analysis assumes that all hash functions are random.
3We use ? to denote the exclusive bitwise OR operator.
Figure 1: Encoding an n-gram?s value in the array.
function for a given set of n-grams is a significant
challenge described in the following sections.
3.3 Encoding n-grams in the model
All addresses in A are initialized to zero. The proce-
dure we use to ensure g(xi) = v(xi) for all xi ? S
updates a single, unique location in A for each n-
gram xi. This location is chosen from among the k
locations given by hj(xi), j ? [k]. Since the com-
posite function g(xi) depends on the values stored at
all k locations A[h1(xi)], A[h2(xi)], . . . , A[hk(xi)]
in A, we must also ensure that once an n-gram xi
has been encoded in the model, these k locations
are not subsequently changed since this would inval-
idate the encoding; however, n-grams encoded later
may reference earlier entries and therefore locations
in A can effectively be ?shared? among parameters.
In the following section we describe a randomized
algorithm to find a suitable order in which to enter
n-grams in the model and, for each n-gram xi, de-
termine which of the k hash functions, say hj , can
be used to update A without invalidating previous
entries. Given this ordering of the n-grams and the
choice of hash function hj for each xi ? S, it is clear
that the following update rule will encode xi in the
array A so that g(xi) will return v(xi) (cf. Eq.(1))
A[hj(xi)] = v(xi)? f(xi)?
k?
i=1?i6=j
A[hi(xi)]. (2)
3.4 Finding an Ordered Matching
We now describe an algorithm (Algorithm 1; (Ma-
jewski et al, 1996)) that selects one of the k hash
507
functions hj , j ? [k] for each n-gram xi ? S and
an order in which to apply the update rule Eq. (2) so
that g(xi) maps xi to v(xi) for all n-grams in S.
This problem is equivalent to finding an ordered
matching in a bipartite graph whose LHS nodes cor-
respond to n-grams in S and RHS nodes correspond
to locations in A. The graph initially contains edges
from each n-gram to each of the k locations in A
given by h1(xi), h2(xi), . . . , hk(xi) (see Fig. (2)).
The algorithm uses the fact that any RHS node that
has degree one (i.e. a single edge) can be safely
matched with its associated LHS node since no re-
maining LHS nodes can be dependent on it.
We first create the graph using the k hash func-
tions hj , j ? [k] and store a list (degree one)
of those RHS nodes (locations) with degree one.
The algorithm proceeds by removing nodes from
degree one in turn, pairing each RHS node with
the unique LHS node to which it is connected. We
then remove both nodes from the graph and push the
pair (xi, hj(xi)) onto a stack (matched). We also
remove any other edges from the matched LHS node
and add any RHS nodes that now have degree one
to degree one. The algorithm succeeds if, while
there are still n-grams left to match, degree one
is never empty. We then encode n-grams in the order
given by the stack (i.e., first-in-last-out).
Since we remove each location in A (RHS node)
from the graph as it is matched to an n-gram (LHS
node), each location will be associated with at most
one n-gram for updating. Moreover, since we match
an n-gram to a location only once the location has
degree one, we are guaranteed that any other n-
grams that depend on this location are already on
the stack and will therefore only be encoded once
we have updated this location. Hence dependencies
in g are respected and g(xi) = v(xi) will remain
true following the update in Eq. (2) for each xi ? S.
3.5 Choosing Random Hash Functions
The algorithm described above is not guaranteed to
succeed. Its success depends on the size of the array
M , the number of n-grams stored |S| and the choice
of random hash functions hj , j ? [k]. Clearly we
require M ? |S|; in fact, an argument from Majew-
ski et al (1996) implies that if M ? 1.23|S| and
k = 3, the algorithm succeeds with high probabil-
Figure 2: The ordered matching algorithm: matched =
[(a, 1), (b, 2), (d, 4), (c, 5)]
ity. We use 2-universal hash functions (L. Carter
and M. Wegman, 1979) defined for a range of size
M via a prime P ? M and two random numbers
1 ? aj ? P and 0 ? bj ? P for j ? [k] as
hj(x) ? ajx + bj mod P
taken modulo M . We generate a set of k hash
functions by sampling k pairs of random numbers
(aj , bj), j ? [k]. If the algorithm does not find
a matching with the current set of hash functions,
we re-sample these parameters and re-start the algo-
rithm. Since the probability of failure on a single
attempt is low when M ? 1.23|S|, the probability
of failing multiple times is very small.
3.6 Querying the Model and False Positives
The construction we have described above ensures
that for any n-gram xi ? S we have g(xi) = v(xi),
i.e., we retrieve the correct value. To retrieve a value
given an n-gram xi we simply compute the finger-
print f(xi), the hash functions hj(xi), j ? [k] and
then return g(xi) using Eq. (1). Note that unlike the
constructions in (Talbot and Osborne, 2007b) and
(Church et al, 2007) no errors are possible for n-
grams stored in the model. Hence we will not make
errors for common n-grams that are typically in S.
508
Algorithm 1 Ordered Matching
Input : Set of n-grams S; k hash functions hj , j ? [k];
number of available locations M .
Output : Ordered matching matched or FAIL.
matched ? [ ]
for all i ? [0,M ? 1] do
r2li ? ?
end for
for all xi ? S do
l2ri ? ?
for all j ? [k] do
l2ri ? l2ri ? hj(xi)
r2lhj(xi) ? r2lhj(xi) ? xi
end for
end for
degree one ? {i ? [0,M ? 1] | |r2li| = 1}
while |degree one| ? 1 do
rhs ? POP degree one
lhs ? POP r2lrhs
PUSH (lhs, rhs) onto matched
for all rhs? ? l2rlhs do
POP r2lrhs?
if |r2lrhs? | = 1 then
degree one ? degree one ? rhs?
end if
end for
end while
if |matched| = |S| then
return matched
else
return FAIL
end if
On the other hand, querying the model with an n-
gram that was not stored, i.e. with xi ? U \ S we
may erroneously return a value v ? V .
Since the fingerprint f(xi) is assumed to be dis-
tributed uniformly at random (u.a.r.) in [0, B ? 1],
g(xi) is also u.a.r. in [0, B?1] for xi ? U\S . Hence
with |V| values stored in the model, the probability
that xi ? U \ S is assigned a value in v ? V is
Pr{g(xi) ? V|xi ? U \ S} = |V|/B.
We refer to this event as a false positive. If V is fixed,
we can obtain a false positive rate  by setting B as
B ? |V|/.
For example, if |V| is 128 then taking B = 1024
gives an error rate of  = 128/1024 = 0.125 with
each entry inA using dlog2 1024e = 10 bits. Clearly
B must be at least |V| in order to distinguish each
value. We refer to the additional bits allocated to
each location (i.e. dlog2 Be ? log2 |V| or 3 in our
example) as error bits in our experiments below.
3.7 Constructing the Full Model
When encoding a large set of n-gram/value pairs S,
Algorithm 1 will only be practical if the raw data
and graph can be held in memory as the perfect hash
function is generated. This makes it difficult to en-
code an extremely large set S into a single array A.
The solution we adopt is to split S into t smaller
sets S?i, i ? [t] that are arranged in lexicographic or-
der.4 We can then encode each subset in a separate
array A?i, i ? [t] in turn in memory. Querying each
of these arrays for each n-gram requested would be
inefficient and inflate the error rate since a false posi-
tive could occur on each individual array. Instead we
store an index of the final n-gram encoded in each
array and given a request for an n-gram?s value, per-
form a binary search for the appropriate array.
3.8 Sanity Checks
Our models are consistent in the following sense
(w1, w2, . . . , wn) ? S =? (w2, . . . , wn) ? S.
Hence we can infer that an n-gram can not be
present in the model, if the n? 1-gram consisting of
the final n ? 1 words has already tested false. Fol-
lowing (Talbot and Osborne, 2007a) we can avoid
unnecessary false positives by not querying for the
longer n-gram in such cases.
Backoff smoothing algorithms typically request
the longest n-gram supported by the model first, re-
questing shorter n-grams only if this is not found. In
our case, however, if a query is issued for the 5-gram
(w1, w2, w3, w4, w5) when only the unigram (w5) is
present in the model, the probability of a false posi-
tive using such a backoff procedure would not be  as
stated above, but rather the probability that we fail to
avoid an error on any of the four queries performed
prior to requesting the unigram, i.e. 1?(1?)4 ? 4.
We therefore query the model first with the unigram
working up to the full n-gram requested by the de-
coder only if the preceding queries test positive. The
probability of returning a false positive for any n-
gram requested by the decoder (but not in the model)
will then be at most .
4In our system we use subsets of 5 million n-grams which
can easily be encoded using less than 2GB of working space.
509
4 Experimental Set-up
4.1 Distributed LM Framework
We deploy the randomized LM in a distributed
framework which allows it to scale more easily
by distributing it across multiple language model
servers. We encode the model stored on each lan-
guagage model server using the randomized scheme.
The proposed randomized LM can encode param-
eters estimated using any smoothing scheme (e.g.
Kneser-Ney, Katz etc.). Here we choose to work
with stupid backoff smoothing (Brants et al, 2007)
since this is significantly more efficient to train and
deploy in a distributed framework than a context-
dependent smoothing scheme such as Kneser-Ney.
Previous work (Brants et al, 2007) has shown it to
be appropriate to large-scale language modeling.
4.2 LM Data Sets
The language model is trained on four data sets:
target: The English side of Arabic-English parallel
data provided by LDC (132 million tokens).
gigaword: The English Gigaword dataset provided
by LDC (3.7 billion tokens).
webnews: Data collected over several years, up to
January 2006 (34 billion tokens).
web: The Web 1T 5-gram Version 1 corpus provided
by LDC (1 trillion tokens).5
An initial experiment will use the Web 1T 5-gram
corpus only; all other experiments will use a log-
linear combination of models trained on each cor-
pus. The combined model is pre-compiled with
weights trained on development data by our system.
4.3 Machine Translation
The SMT system used is based on the framework
proposed in (Och and Ney, 2004) where translation
is treated as the following optimization problem
e? = argmax
e
M?
i=1
?i?i(e, f). (3)
Here f is the source sentence that we wish to trans-
late, e is a translation in the target language, ?i, i ?
[M ] are feature functions and ?i, i ? [M ] are
weights. (Some features may not depend on f .)
5N -grams with count < 40 are not included in this data set.
Full Set Entropy-Pruned
# 1-grams 13,588,391 13,588,391
# 2-grams 314,843,401 184,541,402
# 3-grams 977,069,902 439,430,328
# 4-grams 1,313,818,354 407,613,274
# 5-grams 1,176,470,663 238,348,867
Total 3,795,790,711 1,283,522,262
Table 1: Num. of n-grams in the Web 1T 5-gram corpus.
5 Experiments
This section describes three sets of experiments:
first, we encode the Web 1T 5-gram corpus as a ran-
domized language model and compare the result-
ing size with other representations; then we mea-
sure false positive rates when requesting n-grams
for a held-out data set; finally we compare transla-
tion quality when using conventional (lossless) lan-
guages models and our randomized language model.
Note that the standard practice of measuring per-
plexity is not meaningful here since (1) for efficient
computation, the language model is not normalized;
and (2) even if this were not the case, quantization
and false positives would render it unnormalized.
5.1 Encoding the Web 1T 5-gram corpus
We build a language model from the Web 1T 5-gram
corpus. Parameters, corresponding to negative loga-
rithms of relative frequencies, are quantized to 8-bits
using a uniform quantizer. More sophisticated quan-
tizers (e.g. (S. Lloyd, 1982)) may yield better results
but are beyond the scope of this paper.
Table 1 provides some statistics about the corpus.
We first encode the full set of n-grams, and then a
version that is reduced to approx. 1/3 of its original
size using entropy pruning (Stolcke, 1998).
Table 2 shows the total space and number of bytes
required per n-gram to encode the model under dif-
ferent schemes: ?LDC gzip?d? is the size of the files
as delivered by LDC; ?Trie? uses a compact trie rep-
resentation (e.g., (Clarkson et al, 1997; Church et
al., 2007)) with 3 byte word ids, 1 byte values, and 3
byte indices; ?Block encoding? is the encoding used
in (Brants et al, 2007); and ?randomized? uses our
novel randomized scheme with 12 error bits. The
latter requires around 60% of the space of the next
best representation and less than half of the com-
510
size (GB) bytes/n-gram
Full Set
LDC gzip?d 24.68 6.98
Trie 21.46 6.07
Block Encoding 18.00 5.14
Randomized 10.87 3.08
Entropy Pruned
Trie 7.70 6.44
Block Encoding 6.20 5.08
Randomized 3.68 3.08
Table 2: Web 1T 5-gram language model sizes with dif-
ferent encodings. ?Randomized? uses 12 error bits.
monly used trie encoding. Our method is the only
one to use the same amount of space per parameter
for both full and entropy-pruned models.
5.2 False Positive Rates
All n-grams explicitly inserted into our randomized
language model are retrieved without error; how-
ever, n-grams not stored may be incorrectly assigned
a value resulting in a false positive. Section (3) an-
alyzed the theoretical error rate; here, we measure
error rates in practice when retrieving n-grams for
approx. 11 million tokens of previously unseen text
(news articles published after the training data had
been collected). We measure this separately for all
n-grams of order 2 to 5 from the same text.
The language model is trained on the four data
sources listed above and contains 24 billion n-
grams. With 8-bit parameter values, the model
requires 55.2/69.0/82.7 GB storage when using
8/12/16 error bits respectively (this corresponds to
2.46/3.08/3.69 bytes/n-gram).
Using such a large language model results in a
large fraction of known n-grams in new text. Table
3 shows, e.g., that almost half of all 5-grams from
the new text were seen in the training data.
Column (1) in Table 4 shows the number of false
positives that occurred for this test data. Column
(2) shows this as a fraction of the number of unseen
n-grams in the data. This number should be close to
2?b where b is the number of error bits (i.e. 0.003906
for 8 bits and 0.000244 for 12 bits). The error rates
for bigrams are close to their expected values. The
numbers are much lower for higher n-gram orders
due to the use of sanity checks (see Section 3.8).
total seen unseen
2gms 11,093,093 98.98% 1.02%
3gms 10,652,693 91.08% 8.92%
4gms 10,212,293 68.39% 31.61%
5gms 9,781,777 45.51% 54.49%
Table 3: Number of n-grams in test set and percentages
of n-grams that were seen/unseen in the training data.
(1) (2) (3)
false pos. false posunseen
false pos
total
8 error bits
2gms 376 0.003339 0.000034
3gms 2839 0.002988 0.000267
4gms 6659 0.002063 0.000652
5gms 6356 0.001192 0.000650
total 16230 0.001687 0.000388
12 error bits
2gms 25 0.000222 0.000002
3gms 182 0.000192 0.000017
4gms 416 0.000129 0.000041
5gms 407 0.000076 0.000042
total 1030 0.000107 0.000025
Table 4: False positive rates with 8 and 12 error bits.
The overall fraction of n-grams requested for
which an error occurs is of most interest in applica-
tions. This is shown in Column (3) and is around a
factor of 4 smaller than the values in Column (2). On
average, we expect to see 1 error in around 2,500 re-
quests when using 8 error bits, and 1 error in 40,000
requests with 12 error bits (see ?total? row).
5.3 Machine Translation
We run an improved version of our 2006 NIST MT
Evaluation entry for the Arabic-English ?Unlimited?
data track.6 The language model is the same one as
in the previous section.
Table 5 shows baseline translation BLEU scores
for a lossless (non-randomized) language model
with parameter values quantized into 5 to 8 bits. We
use MT04 data for system development, with MT05
data and MT06 (?NIST? subset) data for blind test-
ing. As expected, results improve when using more
bits. There seems to be little benefit in going beyond
6See http://www.nist.gov/speech/tests/mt/2006/doc/
511
dev test test
bits MT04 MT05 MT06
5 0.5237 0.5608 0.4636
6 0.5280 0.5671 0.4649
7 0.5299 0.5691 0.4672
8 0.5304 0.5697 0.4663
Table 5: Baseline BLEU scores with lossless n-gram
model and different quantization levels (bits).
 0.554
 0.556
 0.558
 0.56
 0.562
 0.564
 0.566
 0.568
 0.57
 8  9  10  11  12  13  14  15  16
MT
05
 B
LE
U
Number of Error Bits
8 bit values
7 bit values
6 bit values
5 bit values
Figure 3: BLEU scores on the MT05 data set.
8 bits. Overall, our baseline results compare favor-
ably to those reported on the NIST MT06 web site.
We now replace the language model with a ran-
domized version. Fig. 3 shows BLEU scores for the
MT05 evaluation set with parameter values quan-
tized into 5 to 8 bits and 8 to 16 additional ?er-
ror? bits. Figure 4 shows a similar graph for MT06
data. We again see improvements as quantization
uses more bits. There is a large drop in performance
when reducing the number of error bits from 10 to
8, while increasing it beyond 12 bits offers almost
no further gains with scores that are almost identi-
cal to the lossless model. Using 8-bit quantization
and 12 error bits results in an overall requirement of
(8+12)?1.23 = 24.6 bits = 3.08 bytes per n-gram.
All runs use the sanity checks described in Sec-
tion 3.8. Without sanity checks, scores drop, e.g. by
0.002 for 8-bit quantization and 12 error bits.
Randomization and entropy pruning can be com-
bined to achieve further space savings with minimal
loss in quality as shown in Table (6). The BLEU
score drops by between 0.0007 to 0.0018 while the
 0.454
 0.456
 0.458
 0.46
 0.462
 0.464
 0.466
 0.468
 8  9  10  11  12  13  14  15  16
MT
06
 (N
IST) B
LE
U
Number of Error Bits
8 bit values
7 bit values
6 bit values
5 bit values
Figure 4: BLEU scores on MT06 data (?NIST? subset).
size dev test test
LM GB MT04 MT05 MT06
unpruned block 116 0.5304 0.5697 0.4663
unpruned rand 69 0.5299 0.5692 0.4659
pruned block 42 0.5294 0.5683 0.4665
pruned rand 27 0.5289 0.5679 0.4656
Table 6: Combining randomization and entropy pruning.
All models use 8-bit values; ?rand? uses 12 error bits.
model is reduced to approx. 1/4 of its original size.
6 Conclusions
We have presented a novel randomized language
model based on perfect hashing. It can associate
arbitrary parameter types with n-grams. Values ex-
plicitly inserted into the model are retrieved without
error; false positives may occur but are controlled
by the number of bits used per n-gram. The amount
of storage needed is independent of the size of the
vocabulary and the n-gram order. Lookup is very
efficient: the values of 3 cells in a large array are
combined with the fingerprint of an n-gram.
Experiments have shown that this randomized
language model can be combined with entropy prun-
ing to achieve further memory reductions; that error
rates occurring in practice are much lower than those
predicted by theoretical analysis due to the use of
runtime sanity checks; and that the same translation
quality as a lossless language model representation
can be achieved when using 12 ?error? bits, resulting
in approx. 3 bytes per n-gram (this includes one byte
to store parameter values).
512
References
B. Bloom. 1970. Space/time tradeoffs in hash coding
with allowable errors. CACM, 13:422?426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL 2007, Prague.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Larry Carter, Robert W. Floyd, John Gill, George
Markowsky, and Mark N. Wegman. 1978. Exact and
approximate membership testers. In STOC, pages 59?
65.
L. Carter and M. Wegman. 1979. Universal classes of
hash functions. Journal of Computer and System Sci-
ence, 18:143?154.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The Bloomier Filter: an efficient
data structure for static support lookup tables. In Proc.
15th ACM-SIAM Symposium on Discrete Algoritms,
pages 30?39.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with golomb
coding. In Proceedings of EMNLP-CoNLL 2007,
Prague, Czech Republic, June.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the CMU-Cambridge toolkit. In Pro-
ceedings of EUROSPEECH, vol. 1, pages 2707?2710,
Rhodes, Greece.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)
2007, Hawaii, USA.
J. Goodman and J. Gao. 2000. Language model size re-
duction by pruning and clustering. In ICSLP?00, Bei-
jing, China.
S. Lloyd. 1982. Least squares quantization in PCM.
IEEE Transactions on Information Theory, 28(2):129?
137.
B.S. Majewski, N.C. Wormald, G. Havas, and Z.J. Czech.
1996. A family of perfect hashing methods. British
Computer Journal, 39(6):547?554.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270?274.
D. Talbot andM. Osborne. 2007a. Randomised language
modelling for statistical machine translation. In 45th
Annual Meeting of the ACL 2007, Prague.
D. Talbot and M. Osborne. 2007b. Smoothed Bloom
filter language models: Tera-scale LMs on the cheap.
In EMNLP/CoNLL 2007, Prague.
513
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183?192,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Parser for Machine Translation Reordering
Jason Katz-Brown Slav Petrov Ryan McDonald Franz Och
David Talbot Hiroshi Ichikawa Masakazu Seno Hideto Kazawa
Google
{jasonkb|slav|ryanmcd|och|talbot|ichikawa|seno|kazawa}@google.com
Abstract
We propose a simple training regime that can
improve the extrinsic performance of a parser,
given only a corpus of sentences and a way
to automatically evaluate the extrinsic quality
of a candidate parse. We apply our method
to train parsers that excel when used as part
of a reordering component in a statistical ma-
chine translation system. We use a corpus of
weakly-labeled reference reorderings to guide
parser training. Our best parsers contribute
significant improvements in subjective trans-
lation quality while their intrinsic attachment
scores typically regress.
1 Introduction
The field of syntactic parsing has received a great
deal of attention and progress since the creation of
the Penn Treebank (Marcus et al, 1993; Collins,
1997; Charniak, 2000; McDonald et al, 2005;
Petrov et al, 2006; Nivre, 2008). A common?
and valid?criticism, however, is that parsers typi-
cally get evaluated only on Section 23 of the Wall
Street Journal portion of the Penn Treebank. This
is problematic for many reasons. As previously ob-
served, this test set comes from a very narrow do-
main that does not necessarily reflect parser perfor-
mance on text coming from more varied domains
(Gildea, 2001), especially web text (Foster, 2010).
There is also evidence that after so much repeated
testing, parsers are indirectly over-fitting to this set
(Petrov and Klein, 2007). Furthermore, parsing was
never meant as a stand-alone task, but is rather a
means to an end, towards the goal of building sys-
tems that can process natural language input.
This is not to say that parsers are not used in larger
systems. All to the contrary, as parsing technology
has become more mature, parsers have become ef-
ficient and accurate enough to be useful in many
natural language processing systems, most notably
in machine translation (Yamada and Knight, 2001;
Galley et al, 2004; Xu et al, 2009). While it has
been repeatedly shown that using a parser can bring
net gains on downstream application quality, it is of-
ten unclear how much intrinsic parsing accuracy ac-
tually matters.
In this paper we try to shed some light on this is-
sue by comparing different parsers in the context of
machine translation (MT). We present experiments
on translation from English to three Subject-Object-
Verb (SOV) languages,1 because those require ex-
tensive syntactic reordering to produce grammatical
translations. We evaluate parse quality on a num-
ber of extrinsic metrics, including word reordering
accuracy, BLEU score and a human evaluation of fi-
nal translation quality. We show that while there is
a good correlation between those extrinsic metrics,
parsing quality as measured on the Penn Treebank
is not a good indicator of the final downstream ap-
plication quality. Since the word reordering metric
can be computed efficiently offline (i.e. without the
use of the final MT system), we then propose to tune
parsers specifically for that metric, with the goal of
improving the performance of the overall system.
To this end we propose a simple training regime
1We experiment with Japanese, Korean and Turkish, but
there is nothing language specific in our approach.
183
which we refer to as targeted self-training (Sec-
tion 2). Similar to self-training, a baseline model
is used to produce predictions on an unlabeled data
set. However, rather than directly training on the
output of the baseline model, we generate a list of
hypotheses and use an external signal to select the
best candidate. The selected parse trees are added
to the training data and the model is then retrained.
The experiments in Section 5 show that this simple
procedure noticeably improves our parsers for the
task at hand, resulting in significant improvements
in downstream translation quality, as measured in a
human evaluation on web text.
This idea is similar in vein to McClosky. et al
(2006) and Petrov et al (2010), except that we use an
extrinsic quality metric instead of a second parsing
model for making the selection. It is also similar to
Burkett and Klein (2008) and Burkett et al (2010),
but again avoiding the added complexity introduced
by the use of additional (bilingual) models for can-
didate selection.
It should be noted that our extrinsic metric is com-
puted from data that has been manually annotated
with reference word reorderings. Details of the re-
ordering metric and the annotated data we used are
given in Sections 3 and 4. While this annotation re-
quires some effort, such annotations are much easier
to obtain than full parse trees. In our experiments
in Section 6 we show that we can obtain similar
improvements on downstream translation quality by
targeted self-training with weakly labeled data (in
form of word reorderings), as with training on the
fully labeled data (with full syntactic parse trees).
2 Targeted Self-Training
Our technique for retraining a baseline parser is an
extension of self-training. In standard parser self-
training, one uses the baseline parsing model to
parse a corpus of sentences, and then adds the 1-best
output of the baseline parser to the training data. To
target the self-training, we introduce an additional
step, given as Algorithm 1. Instead of taking the 1-
best parse, we produce a ranked n-best list of predic-
tions and select the parser which gives the best score
according to an external evaluation function. That
is, instead of relying on the intrinsic model score,
we use an extrinsic score to select the parse towards
Algorithm 1 Select parse that maximizes an extrin-
sic metric.
Input: baseline parser B
Input: sentence S
Input: function COMPUTEEXTRINSIC(parse P )
Output: a parse for the input sentence
Pn = {P1, . . . , Pn} ? n-best parses of S by B
maxScore = 0
bestParse = ?
for k = 1 to n do
extrinsicScore = COMPUTEEXTRINSIC(Pk)
if extrinsicScore > maxScore then
maxScore = extrinsicScore
bestParse = Pk
end if
end for
return bestParse
which to update. In the case of a tie, we prefer the
parse ranked most highly in the n-best list.
The motivation of this selection step is that good
performance on the downstream external task, mea-
sured by the extrinsic metric, should be predictive
of an intrinsically good parse. At the very least,
even if the selected parse is not syntactically cor-
rect, or even if it goes against the original treebank-
ing guidelines, it results in a higher extrinsic score
and should therefore be preferred.
One could imagine extending this framework by
repeatedly running self-training on successively im-
proving parsers in an EM-style algorithm. A recent
work by Hall et al (2011) on training a parser with
multiple objective functions investigates a similar
idea in the context of online learning.
In this paper we focus our attention on machine
translation as the final application, but one could en-
vision applying our techniques to other applications
such as information extraction or question answer-
ing. In particular, we explore one application of
targeted self-training, where computing the extrin-
sic metric involves plugging the parse into an MT
system?s reordering component and computing the
accuracy of the reordering compared to a reference
word order. We now direct our attention to the de-
tails of this application.
184
3 The MT Reordering Task
Determining appropriate target language word or-
der for a translation is a fundamental problem in
MT. When translating between languages with sig-
nificantly different word order such as English and
Japanese, it has been shown that metrics which ex-
plicitly account for word-order are much better cor-
related with human judgments of translation qual-
ity than those that give more weight to word choice,
like BLEU (Lavie and Denkowski, 2009; Isozaki et
al., 2010a; Birch and Osborne, 2010). This demon-
strates the importance of getting reordering right.
3.1 Reordering as a separately evaluable
component
One way to break down the problem of translat-
ing between languages with different word order
is to handle reordering and translation separately:
first reorder source-language sentences into target-
language word order in a preprocessing step, and
then translate the reordered sentences. It has been
shown that good results can be achieved by reorder-
ing each input sentence using a series of tree trans-
formations on its parse tree. The rules for tree
transformation can be manually written (Collins et
al., 2005; Wang, 2007; Xu et al, 2009) or auto-
matically learned (Xia and McCord, 2004; Habash,
2007; Genzel, 2010).
Doing reordering as a preprocessing step, sepa-
rately from translation, makes it easy to evaluate re-
ordering performance independently from the MT
system. Accordingly, Talbot et al (2011) present a
framework for evaluating the quality of reordering
separately from the lexical choice involved in trans-
lation. They propose a simple reordering metric
based on METEOR?s reordering penalty (Lavie and
Denkowski, 2009). This metric is computed solely
on the source language side. To compute it, one
takes the candidate reordering of the input sentence
and partitions it into a set C of contiguous spans
whose content appears contiguously in the same or-
der in the reference. The reordering score is then
computed as
?(esys, eref) = 1?
|C| ? 1
|e| ? 1 .
This metric assigns a score between 0 and 1 where 1
indicates that the candidate reordering is identical to
the reference and 0 indicates that no two words that
are contiguous in the candidate reordering are con-
tiguous in the reference. For example, if a reference
reordering is A B C D E, candidate reordering A
B E C D would get score 1?(3?1)/(5?1) = 0.5.
Talbot et al (2011) show that this reordering score
is strongly correlated with human judgment of trans-
lation quality. Furthermore, they propose to evalu-
ate the reordering quality of an MT system by com-
puting its reordering score on a test set consisting
of source language sentences and their reference re-
orderings. In this paper, we take the same approach
for evaluation, and in addition, we use corpora of
source language sentences and their reference re-
orderings for training the system, not just testing
it. We describe in more detail how the reference re-
ordering data was prepared in Section 4.1.
3.2 Reordering quality as predictor of parse
quality
Figure 1 gives concrete examples of good and bad
reorderings of an English sentence into Japanese
word order. It shows that a bad parse leads to a bad
reordering (lacking inversion of verb ?wear? and ob-
ject ?sunscreen?) and a low reordering score. Could
we flip this causality around, and perhaps try to iden-
tify a good parse tree based on its reordering score?
With the experiments in this paper, we show that in-
deed a high reordering score is predictive of the un-
derlying parse tree that was used to generate the re-
ordering being a good parse (or, at least, being good
enough for our purpose).
In the case of translating English to Japanese or
another SOV language, there is a large amount of
reordering required, but with a relatively small num-
ber of reordering rules one can cover a large pro-
portion of reordering phenomena. Isozaki et al
(2010b), for instance, were able to get impressive
English?Japanese results with only a single re-
ordering rule, given a suitable definition of a head.
Hence, the reordering task depends crucially on a
correct syntactic analysis and is extremely sensitive
to parser errors.
185
4 Experimental Setup
4.1 Treebank data
In our experiments the baseline training corpus is
the Wall Street Journal (WSJ) section of the Penn
Treebank (Marcus et al, 1993) using standard train-
ing/development/testing splits. We converted the
treebank to match the tokenization expected by our
MT system. In particular, we split tokens containing
hyphens into multiple tokens and, somewhat sim-
plistically, gave the original token?s part-of-speech
tag to all newly created tokens. In Section 6 we
make also use of the Question Treebank (QTB)
(Judge et al, 2006), as a source of syntactically an-
notated out-of-domain data. Though we experiment
with both dependency parsers and phrase structure
parsers, our MT system assumes dependency parses
as input. We use the Stanford converter (de Marneffe
et al, 2006) to convert phrase structure parse trees to
dependency parse trees (for both treebank trees and
predicted trees).
4.2 Reference reordering data
We aim to build an MT system that can accurately
translate typical English text that one finds on the
Internet to SOV langauges. To this end, we ran-
domly sampled 13595 English sentences from the
web and created Japanese-word-order reference re-
orderings for them. We split the sentences arbitrarily
into a 6268-sentence Web-Train corpus and a 7327-
sentence Web-Test corpus.
To make the reference alignments we used the
technique suggested by Talbot et al (2011): ask
annotators to translate each English sentence to
Japanese extremely literally and annotate which En-
glish words align to which Japanese words. Golden
reference reorderings can be made programmati-
cally from these annotations. Creating a large set
of reference reorderings is straightforward because
annotators need little special background or train-
ing, as long as they can speak both the source and
target languages. We chose Japanese as the target
language through which to create the English refer-
ence reorderings because we had access to bilingual
annotators fluent in English and Japanese.
Good parse
Reordered:
15 or greater of an SPF has that sunscreen Wear
Reordering score: 1.0 (matches reference)
Bad parse
Reordered:
15 or greater of an SPF has that Wear sunscreen
Reordering score: 0.78 (?Wear? is out of place)
Figure 1: Examples of good and bad parses and cor-
responding reorderings for translation from English to
Japanese. The good parse correctly identifies ?Wear? as
the main verb and moves it to the end of the sentence; the
bad parse analyses ?Wear sunscreen? as a noun phrase
and does not reorder it. This example was one of the
wins in the human evaluation of Section 5.2.
4.3 Parsers
The core dependency parser we use is an implemen-
tation of a transition-based dependency parser using
an arc-eager transition strategy (Nivre, 2008). The
parser is trained using the averaged perceptron algo-
rithm with an early update strategy as described in
Zhang and Clark (2008). The parser uses the fol-
lowing features: word identity of the first two words
on the buffer, the top word on the stack and the head
of the top word on the stack (if available); part-of-
speech identities of the first four words on the buffer
and top two words on the stack; dependency arc la-
bel identities for the top word on the stack, the left
and rightmost modifier of the top word on the stack,
and the leftmost modifier of the first word in the
buffer. We also include conjunctions over all non-
lexical features.
We also give results for the latent variable parser
(a.k.a. BerkeleyParser) of Petrov et al (2006). We
convert the constituency trees output by the Berke-
leyParser to labeled dependency trees using the same
procedure that is applied to the treebanks.
While the BerkeleyParser views part-of-speech
(POS) tagging as an integral part of parsing, our
dependency parser requires the input to be tagged
186
with a separate POS tagger. We use the TnT tag-
ger (Brants, 2000) in our experiments, because of
its efficiency and ease of use. Tagger and parser are
always trained on the same data.
For all parsers, we lowercase the input at train and
test time. We found that this improves performance
in parsing web text. In addition to general upper-
case/lowercase noisiness of the web text negatively
impacting scores, we found that the baseline case-
sensitive parsers are especially bad at parsing imper-
ative sentences, as discussed in Section 5.3.2.
4.4 Reordering rules
In this paper we focus on English to Japanese, Ko-
rean, and Turkish translation. We use a superset of
the reordering rules proposed by Xu et al (2009),
which flatten a dependency tree into SOV word or-
der that is suitable for all three languages. The rules
define a precedence order for the dependents of each
part of speech. For example, a slightly simplified
version of the precedence order of child labels for
a verbal head HEADVERB is: advcl, nsubj, prep,
[other children], dobj, prt, aux, neg, HEADVERB,
mark, ref, compl.
Alternatively, we could have used an automatic
reordering-rule learning framework like that of Gen-
zel (2010). Because the reordering accuracy met-
ric can be computed for any source/target language
pair, this would have made our approach language
completely independent and applicable to any lan-
guage pair. We chose to use manually written rules
to eliminate the variance induced by the automatic
reordering-rule learning framework.
4.5 MT system
We carried out all our translation experiments on a
state-of-the-art phrase-based statistical MT system.
During both training and testing, the system reorders
source-language sentences in a preprocessing step
using the above-mentioned rules. During decoding,
we used an allowed jump width of 4 words. In ad-
dition to the regular distance distortion model, we
incorporate a maximum entropy based lexicalized
phrase reordering model (Zens and Ney, 2006) as
a feature used in decoding.
Overall for decoding, we use between 20 to
30 features, whose weights are optimized using
MERT (Och, 2003). All experiments for a given lan-
guage pair use the same set of MERT weights tuned
on a system using a separate parser (that is neither
the baseline nor the experiment parser). This po-
tentially underestimates the improvements that can
be obtained, but also eliminates MERT as a pos-
sible source of improvement, allowing us to trace
back improvements in translation quality directly to
parser changes.2
For parallel training data, we use a custom collec-
tion of parallel documents. They come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. For all language pairs, we
trained on approximately 300 million source words
each.
5 Experiments Reordering Web Text
We experimented with parsers trained in three dif-
ferent ways:
1. Baseline: trained only on WSJ-Train.
2. Standard self-training: trained on WSJ-Train
and 1-best parse of the Web-Train set by base-
line parser.
3. Targeted self-training: trained on WSJ-Train
and, for each sentence in Web-Train, the parse
from the baseline parser?s 512-best list that
when reordered gives the highest reordering
score.3
5.1 Standard self-training vs targeted
self-training
Table 1 shows that targeted self-training on Web-
Train significantly improves Web-Test reordering
score more than standard self-training for both the
shift-reduce parser and for the BerkeleyParser. The
reordering score is generally divorced from the at-
tachment scores measured on the WSJ-Test tree-
bank: for the shift-reduce parser, Web-Test reorder-
ing score and WSJ-Test labeled attachment score
2We also ran MERT on all systems and the pattern of im-
provement is consistent, but sometimes the improvement is big-
ger or smaller after MERT. For instance, the BLEU delta for
Japanese is +0.0030 with MERT on both sides as opposed to
+0.0025 with no MERT.
3We saw consistent but diminishing improvements as we in-
creased the size of the n-best list.
187
Parser Web-Test reordering WSJ-Test LAS
Shift-reduce WSJ baseline 0.757 85.31%
+ self-training 1x 0.760 85.26%
+ self-training 10x 0.756 84.14%
+ targeted self-training 1x 0.770 85.19%
+ targeted self-training 10x 0.777 84.48%
Berkeley WSJ baseline 0.780 88.66%
+ self-training 1x 0.785 89.21%
+ targeted self-training 1x 0.790 89.32%
Table 1: English?Japanese reordering scores on Web-Test for standard self-training and targeted self-training on
Web-Train. Label ?10x? indicates that the self-training data was weighted 10x relative to the WSJ training data.
Bolded reordering scores are different from WSJ-only baseline with 95% confidence but are not significantly different
from each other within the same group.
English to BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.1777 0.1802 2.56 2.69 yes (at 95% level)
Korean 0.3229 0.3259 2.61 2.70 yes (at 90% level)
Turkish 0.1344 0.1370 2.10 2.20 yes (at 95% level)
Table 2: BLEU scores and human evaluation results for translation between three language pairs, varying only the
parser between systems. ?WSJ-only? corresponds to the baseline WSJ-only shift-reduce parser; ?Targeted? corre-
sponds to the Web-Train targeted self-training 10x shift-reduce parser.
(LAS) are anti-correlated, but for BerkeleyParser
they are correlated. Interestingly, weighting the self-
training data more seems to have a negative effect on
both metrics.4
One explanation for the drops in LAS is that some
parts of the parse tree are important for downstream
reordering quality while others are not (or only to
a lesser extent). Some distinctions between labels
become less important; for example, arcs labeled
?amod? and ?advmod? are transformed identically
by the reordering rules. Some semantic distinctions
also become less important; for example, any sane
interpretation of ?red hot car? would be reordered
the same, that is, not at all.
5.2 Translation quality improvement
To put the improvement of the MT system in terms
of BLEU score (Papineni et al, 2002), a widely used
metric for automatic MT evaluation, we took 5000
sentences from Web-Test and had humans gener-
ate reference translations into Japanese, Korean, and
4We did not attempt this experiment for the BerkeleyParser
since training was too slow.
Turkish. We then trained MT systems varying only
the parser used for reordering in training and decod-
ing. Table 2 shows that targeted self-training data
increases BLEU score for translation into all three
languages.
In addition to BLEU increase, a side-by-side hu-
man evaluation on 500 sentences (sampled from
the 5000 used to compute BLEU scores) showed
a statistically significant improvement for all three
languages (see again Table 2). For each sen-
tence, we asked annotators to simultaneously score
both translations from 0 to 6, with guidelines
that 6=?Perfect?, 4=?Most Meaning/Grammar?,
2=?Some Meaning/Grammar?, 0=?Nonsense?. We
computed confidence intervals for the average score
difference using bootstrap resampling; a difference
is significant if the two-sided confidence interval
does not include 0.
5.3 Analysis
As the divergence between the labeled attachment
score on the WSJ-Test data and the reordering score
on the WSJ-Test data indicates, parsing web text
188
Parser Click as N Click as V Imperative rate
case-sensitive shift-reduce WSJ-only 74 0 6.3%
case-sensitive shift-reduce + Web-Train targeted self-training 75 0 10.5%
case-insensitive shift-reduce WSJ-only 75 0 10.3%
case-insensitive shift-reduce + Web-Train targeted self-training 75 0 11.6%
Berkeley WSJ-only 35 35 11.9%
Berkeley + Web-Train targeted self-training 13 58 12.5%
(WSJ-Train) 1 0 0.7%
Table 3: Counts on Web-Test of ?click? tagged as a noun and verb and percentage of sentences parsed imperatively.
poses very different challenges compared to parsing
newswire. We show how our method improves pars-
ing performance and reordering performance on two
examples: the trendy word ?click? and imperative
sentences.
5.3.1 Click
The word ?click? appears only once in the train-
ing portion of the WSJ (as a noun), but appears many
times in our Web test data. Table 3 shows the distri-
bution of part-of-speech tags that different parsers
assign to ?click?. The WSJ-only parsers tag ?click?
as a noun far too frequently. The WSJ-only shift-
reduce parser refuses to tag ?click? as a verb even
with targeted self-training, but BerkeleyParser does
learn to tag ?click? more often as a verb.
It turns out that the shift-reduce parser?s stub-
bornness is not due to a fundamental problem of
the parser, but due to an artifact in TnT. To in-
crease speed, TnT restricts the choices of tags for
known words to previously-seen tags. This causes
the parser?s n-best lists to never hypothesize ?click?
as a verb, and self-training doesn?t click no matter
how targeted it is. This shows that the targeted self-
training approach heavily relies on the diversity of
the baseline parser?s n-best lists.
It should be noted here that it would be easy to
combine our approach with the uptraining approach
of Petrov et al (2010). The idea would be to use the
BerkeleyParser to generate the n-best lists; perhaps
we could call this targeted uptraining. This way, the
shift-reduce parser could benefit both from the gen-
erally higher quality of the parse trees produced by
the BerkeleyParser, as well as from the information
provided by the extrinsic scoring function.
5.3.2 Imperatives
As Table 3 shows, the WSJ training set contains
only 0.7% imperative sentences.5 In contrast, our
test sentences from the web contain approximately
10% imperatives. As a result, parsers trained exclu-
sively on the WSJ underproduce imperative parses,
especially a case-sensitive version of the baseline.
Targeted self-training helps the parsers to predict im-
perative parses more often.
Targeted self-training works well for generating
training data with correctly-annotated imperative
constructions because the reordering of main sub-
jects and verbs in an SOV language like Japanese
is very distinct: main subjects stay at the begin-
ning of the sentence, and main verbs are reordered
to the end of the sentence. It is thus especially easy
to know whether an imperative parse is correct or
not by looking at the reference reordering. Figure 1
gives an example: the bad (WSJ-only) parse doesn?t
catch on to the imperativeness and gets a low re-
ordering score.
6 Targeted Self-Training vs Training on
Treebanks for Domain Adaptation
If task-specific annotation is cheap, then it is rea-
sonable to consider whether we could use targeted
self-training to adapt a parser to a new domain as
a cheaper alternative to making new treebanks. For
example, if we want to build a parser that can reorder
question sentences better than our baseline WSJ-
only parser, we have these two options:
1. Manually construct PTB-style trees for 2000
5As an approximation, we count every parse that begins with
a root verb as an imperative.
189
questions and train on the resulting treebank.
2. Create reference reorderings for 2000 questions
and then do targeted self-training.
To compare these approaches, we created reference
reordering data for our train (2000 sentences) and
test (1000 sentences) splits of the Question Tree-
bank (Judge et al, 2006). Table 4 shows that both
ways of training on QTB-Train sentences give sim-
ilarly large improvements in reordering score on
QTB-Test. Table 5 confirms that this corresponds
to very large increases in English?Japanese BLEU
score and subjective translation quality. In the hu-
man side-by-side comparison, the baseline transla-
tions achieved an average score of 2.12, while the
targeted self-training translations received a score of
2.94, where a score of 2 corresponds to ?some mean-
ing/grammar? and ?4? corresponds to ?most mean-
ing/grammar?.
But which of the two approaches is better? In
the shift-reduce parser, targeted self-training gives
higher reordering scores than training on the tree-
bank, and in BerkeleyParser, the opposite is true.
Thus both approaches produce similarly good re-
sults. From a practical perspective, the advantage of
targeted self-training depends on whether the extrin-
sic metric is cheaper to calculate than treebanking.
For MT reordering, making reference reorderings is
cheap, so targeted self-training is relatively advanta-
geous.
As before, we can examine whether labeled at-
tachment score measured on the test set of the
QTB is predictive of reordering quality. Table 4
shows that targeted self-training raises LAS from
64.78?69.17%. But adding the treebank leads
to much larger increases, resulting in an LAS of
84.75%, without giving higher reordering score. We
can conclude that high LAS is not necessary to
achieve top reordering scores.
Perhaps our reordering rules are somehow defi-
cient when it comes to reordering correctly-parsed
questions, and as a result the targeted self-training
process steers the parser towards producing patho-
logical trees with little intrinsic meaning. To explore
this possibility, we computed reordering scores after
reordering the QTB-Test treebank trees directly. Ta-
ble 4 shows that this gives reordering scores similar
to those of our best parsers. Therefore it is at least
possible that the targeted self-training process could
have resulted in a parser that achieves high reorder-
ing score by producing parses that look like those in
the QuestionBank.
7 Related Work
Our approach to training parsers for reordering is
closely related to self/up-training (McClosky. et al,
2006; Petrov et al, 2010). However, unlike uptrain-
ing, our method does not use only the 1-best output
of the first-stage parser, but has access to the n-best
list. This makes it similar to the work of McClosky.
et al (2006), except that we use an extrinsic metric
(MT reordering score) to select a high quality parse
tree, rather than a second, reranking model that has
access to additional features.
Targeted self-training is also similar to the re-
training of Burkett et al (2010) in which they
jointly parse unannotated bilingual text using a mul-
tiview learning objective, then retrain the monolin-
gual parser models to include each side of the jointly
parsed bitext as monolingual training data. Our ap-
proach is different in that it doesn?t use a second
parser and bitext to guide the creation of new train-
ing data, and instead relies on n-best lists and an
extrinsic metric.
Our method can be considered an instance of
weakly or distantly supervised structured prediction
(Chang et al, 2007; Chang et al, 2010; Clarke et al,
2010; Ganchev et al, 2010). Those methods attempt
to learn structure models from related external sig-
nals or aggregate data statistics. This work differs
in two respects. First, we use the external signals
not as explicit constraints, but to compute an ora-
cle score used to re-rank a set of parses. As such,
there are no requirements that it factor by the struc-
ture of the parse tree and can in fact be any arbitrary
metric. Second, our final objective is different. In
weakly/distantly supervised learning, the objective
is to use external knowledge to build better struc-
tured predictors. In our case this would mean using
the reordering metric as a means to train better de-
pendency parsers. Our objective, on the other hand,
is to use the extrinsic metric to train parsers that are
specifically better at the reordering task, and, as a re-
sult, better suited for MT. This makes our work more
in the spirit of Liang et al (2006), who train a per-
190
Parser QTB-Test reordering QTB-Test LAS
Shift-reduce WSJ baseline 0.663 64.78%
+ treebank 1x 0.704 77.12%
+ treebank 10x 0.768 84.75%
+ targeted self-training 1x 0.746 67.84%
+ targeted self-training 10x 0.779 69.17%
Berkeley WSJ baseline 0.733 76.50%
+ treebank 1x 0.800 87.79%
+ targeted self-training 1x 0.775 80.64%
(using treebank trees directly) 0.788 100%
Table 4: Reordering and labeled attachment scores on QTB-Test for treebank training and targeted self-training on
QTB-Train.
English to QTB-Test BLEU Human evaluation (scores range 0 to 6)
WSJ-only Targeted WSJ-only Targeted Sig. difference?
Japanese 0.2379 0.2615 2.12 2.94 yes (at 95% level)
Table 5: BLEU scores and human evaluation results for English?Japanese translation of the QTB-Test corpus, varying
only the parser between systems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training
10x shift-reduce parser.
ceptron model for an end-to-end MT system where
the alignment parameters are updated based on se-
lecting an alignment from a n-best list that leads to
highest BLEU score. As mentioned earlier, this also
makes our work similar to Hall et al (2011) who
train a perceptron algorithm on multiple objective
functions with the goal of producing parsers that are
optimized for extrinsic metrics.
It has previously been observed that parsers of-
ten perform differently for downstream applications.
Miyao et al (2008) compared parser quality in the
biomedical domain using a protein-protein interac-
tion (PPI) identification accuracy metric. This al-
lowed them to compare the utility of extant depen-
dency parsers, phrase structure parsers, and deep
structure parsers for the PPI identification task. One
could apply the targeted self-training technique we
describe to optimize any of these parsers for the PPI
task, similar to how we have optimized our parser
for the MT reordering task.
8 Conclusion
We introduced a variant of self-training that targets
parser training towards an extrinsic evaluation met-
ric. We use this targeted self-training approach to
train parsers that improve the accuracy of the word
reordering component of a machine translation sys-
tem. This significantly improves the subjective qual-
ity of the system?s translations from English into
three SOV languages. While the new parsers give
improvements in these external evaluations, their in-
trinsic attachment scores go down overall compared
to baseline parsers trained only on treebanks. We
conclude that when using a parser as a component
of a larger external system, it can be advantageous
to incorporate an extrinsic metric into parser train-
ing and evaluation, and that targeted self-training is
an effective technique for incorporating an extrinsic
metric into parser training.
References
A. Birch and M. Osborne. 2010. LRscore for evaluating
lexical and reordering quality in MT. In ACL-2010
WMT.
T. Brants. 2000. TnT ? a statistical part-of-speech tagger.
In ANLP ?00.
D. Burkett and D. Klein. 2008. Two languages are better
than one (for syntactic parsing). In EMNLP ?08.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In CoNLL ?10.
191
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In ACL
?07.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In ICML ?10.
E. Charniak. 2000. A maximum?entropy?inspired
parser. In NAACL ?00.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
Driving semantic parsing from the world?s response.
In CoNLL ?10.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In ACL
?05.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL ?97.
M.-C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC ?06.
J. Foster. 2010. ?cba to check the spelling?: Investigat-
ing parser performance on discussion forum posts. In
NAACL ?10.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In HLT-NAACL ?04.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Genzel. 2010. Automatically learning source-side re-
ordering rules for large scale machine translation. In
COLING ?10.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP ?01.
N. Habash. 2007. Syntactic preprocessing for statistical
machine translation. In MTS ?07.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In EMNLP ?11.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada.
2010a. Automatic evaluation of translation quality for
distant language pairs. In EMNLP ?10.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
Head finalization: A simple reordering rule for SOV
languages. In ACL-2010 WMT.
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
Bank: creating a corpus of parse-annotated questions.
In ACL ?06.
A. Lavie and M. Denkowski. 2009. The Meteor metric
for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL ?06.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL ?06.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
?05.
Y. Miyao, R. S?tre, K. Sagae, T. Matsuzaki, and J. Tsu-
jii. 2008. Task-oriented evaluation of syntactic parsers
and their representations. In ACL ?08.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4).
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL ?02.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL ?07.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ?06.
S. Petrov, P. Chang, and M. Ringgaard H. Alshawi. 2010.
Uptraining for accurate deterministic question parsing.
In EMNLP ?10.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalua-
tion framework for machine translation reordering. In
EMNLP-2011 WMT.
C. Wang. 2007. Chinese syntactic reordering for statisti-
cal machine translation. In EMNLP ?07.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. In
Coling ?04.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In NAACL-HLT ?09.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In ACL ?01.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In NAACL-
06 WMT.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing. In EMNLP ?08.
192
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1363?1372,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Watermarking the Outputs of Structured Prediction with an
application in Statistical Machine Translation.
Ashish Venugopal1 Jakob Uszkoreit1 David Talbot1 Franz J. Och1 Juri Ganitkevitch2
1Google, Inc. 2Center for Language and Speech Processing
1600 Amphitheatre Parkway Johns Hopkins University
Mountain View, 94303, CA Baltimore, MD 21218, USA
{avenugopal,uszkoreit,talbot,och}@google.com juri@cs.jhu.edu
Abstract
We propose a general method to water-
mark and probabilistically identify the
structured outputs of machine learning al-
gorithms. Our method is robust to lo-
cal editing operations and provides well
defined trade-offs between the ability to
identify algorithm outputs and the qual-
ity of the watermarked output. Unlike
previous work in the field, our approach
does not rely on controlling the inputs to
the algorithm and provides probabilistic
guarantees on the ability to identify col-
lections of results from one?s own algo-
rithm. We present an application in statis-
tical machine translation, where machine
translated output is watermarked at mini-
mal loss in translation quality and detected
with high recall.
1 Motivation
Machine learning algorithms provide structured
results to input queries by simulating human be-
havior. Examples include automatic machine
translation (Brown et al, 1993) or automatic
text and rich media summarization (Goldstein
et al, 1999). These algorithms often estimate
some portion of their models from publicly avail-
able human generated data. As new services
that output structured results are made avail-
able to the public and the results disseminated
on the web, we face a daunting new challenge:
Machine generated structured results contam-
inate the pool of naturally generated human
data. For example, machine translated output
and human generated translations are currently
both found extensively on the web, with no auto-
matic way of distinguishing between them. Al-
gorithms that mine data from the web (Uszko-
reit et al, 2010), with the goal of learning to
simulate human behavior, will now learn mod-
els from this contaminated and potentially self-
generated data, reinforcing the errors commit-
ted by earlier versions of the algorithm.
It is beneficial to be able to identify a set of
encountered structured results as having been
generated by one?s own algorithm, with the pur-
pose of filtering such results when building new
models.
Problem Statement: We define a struc-
tured result of a query q as r = {z1 ? ? ? zL} where
the order and identity of elements zi are impor-
tant to the quality of the result r. The structural
aspect of the result implies the existence of alter-
native results (across both the order of elements
and the elements themselves) that might vary in
their quality.
Given a collection of N results, CN =
r1 ? ? ? rN , where each result ri has k ranked alter-
natives Dk(qi) of relatively similar quality and
queries q1 ? ? ? qN are arbitrary and not controlled
by the watermarking algorithm, we define the
watermarking task as:
Task. Replace ri with r?i ? Dk(qi) for some sub-
set of results in CN to produce a watermarked
collection C?N
such that:
? C?N is probabilistically identifiable as having
been generated by one?s own algorithm.
1363
? the degradation in quality from CN to the
watermarked C?N should be analytically con-
trollable, trading quality for detection per-
formance.
? C?N should not be detectable as water-
marked content without access to the gen-
erating algorithms.
? the detection of C?N should be robust to sim-
ple edit operations performed on individual
results r ? C?N .
2 Impact on Statistical Machine
Translation
Recent work(Resnik and Smith, 2003; Munteanu
and Marcu, 2005; Uszkoreit et al, 2010) has
shown that multilingual parallel documents can
be efficiently identified on the web and used as
training data to improve the quality of statisti-
cal machine translation.
The availability of free translation services
(Google Translate, Bing Translate) and tools
(Moses, Joshua), increase the risk that the con-
tent found by parallel data mining is in fact gen-
erated by a machine, rather than by humans. In
this work, we focus on statistical machine trans-
lation as an application for watermarking, with
the goal of discarding documents from training
if they have been generated by one?s own algo-
rithms.
To estimate the magnitude of the problem,
we used parallel document mining (Uszkoreit et
al., 2010) to generate a collection of bilingual
document pairs across several languages. For
each document, we inspected the page content
for source code that indicates the use of trans-
lation modules/plug-ins that translate and pub-
lish the translated content.
We computed the proportion of the content
within our corpus that uses these modules. We
find that a significant proportion of the mined
parallel data for some language pairs is gener-
ated via one of these translation modules. The
top 3 languages pairs, each with parallel trans-
lations into English, are Tagalog (50.6%), Hindi
(44.5%) and Galician (41.9%). While these
proportions do not reflect impact on each lan-
guage?s monolingual web, they are certainly high
enough to affect machine translations systems
that train on mined parallel data. In this work,
we develop a general approach to watermark
structured outputs and apply it to the outputs
of a statistical machine translation system with
the goal of identifying these same outputs on the
web. In the context of the watermarking task
defined above, we output selecting alternative
translations for input source sentences. These
translations often undergo simple edit and for-
matting operations such as case changes, sen-
tence and word deletion or post editing, prior to
publishing on the web. We want to ensure that
we can still detect watermarked translations de-
spite these edit operations. Given the rapid pace
of development within machine translation, it
is also important that the watermark be robust
to improvements in underlying translation qual-
ity. Results from several iterations of the system
within a single collection of documents should be
identifiable under probabilistic bounds.
While we present evaluation results for sta-
tistical machine translation, our proposed ap-
proach and associated requirements are applica-
ble to any algorithm that produces structured
results with several plausible alternatives. The
alternative results can arise as a result of inher-
ent task ambiguity (for example, there are mul-
tiple correct translations for a given input source
sentence) or modeling uncertainty (for example,
a model assigning equal probability to two com-
peting results).
3 Watermark Structured Results
Selecting an alternative r? from the space of al-
ternatives Dk(q) can be stated as:
r? = arg max
r?Dk(q)
w(r,Dk(q), h) (1)
where w ranks r ? Dk(q) based on r?s presen-
tation of a watermarking signal computed by a
hashing operation h. In this approach, w and
its component operation h are the only secrets
held by the watermarker. This selection crite-
rion is applied to all system outputs, ensuring
that watermarked and non-watermarked version
of a collection will never be available for compar-
ison.
1364
A specific implementation of w within our wa-
termarking approach can be evaluated by the
following metrics:
? False Positive Rate: how often non-
watermarked collections are falsely identi-
fied as watermarked.
? Recall Rate: how often watermarked col-
lections are correctly identified as water-
marked.
? Quality Degradation: how significantly
does C?N differ from CN when evaluated by
task specific quality metrics.
While identification is performed at the col-
lection level, we can scale these metrics based
on the size of each collection to provide more
task sensitive metrics. For example, in machine
translation, we count the number of words in
the collection towards the false positive and re-
call rates.
In Section 3.1, we define a random hashing
operation h and a task independent implemen-
tation of the selector function w. Section 3.2
describes how to classify a collection of water-
marked results. Section 3.3 and 3.4 describes re-
finements to the selection and classification cri-
teria that mitigate quality degradation. Follow-
ing a comparison to related work in Section 4,
we present experimental results for several lan-
guages in Section 5.
3.1 Watermarking: CN ? C?N
We define a random hashing operation h that is
applied to result r. It consists of two compo-
nents:
? A hash function applied to a structured re-
sult r to generate a bit sequence of a fixed
length.
? An optional mapping that maps a single
candidate result r to a set of sub-results.
Each sub-result is then hashed to generate
a concatenated bit sequence for r.
A good hash function produces outputs whose
bits are independent. This implies that we can
treat the bits for any input structured results
as having been generated by a binomial distri-
bution with equal probability of generating 1s
vs 0s. This condition also holds when accu-
mulating the bit sequences over a collection of
results as long as its elements are selected uni-
formly from the space of possible results. There-
fore, the bits generated from a collection of un-
watermarked results will follow a binomial dis-
tribution with parameter p = 0.5. This result
provides a null hypothesis for a statistical test
on a given bit sequence, testing whether it is
likely to have been generated from a binomial
distribution binomial(n, p) where p = 0.5 and n
is the length of the bit sequence.
For a collection CN = r1 ? ? ? rN , we can define
a watermark ranking function w to systemati-
cally select alternatives r?i ? Dk(q), such that
the resulting C?N is unlikely to produce bit se-
quences that follow the p = 0.5 binomial distri-
bution. A straightforward biasing criteria would
be to select the candidate whose bit sequence ex-
hibits the highest ratio of 1s. w can be defined
as:
w(r,Dk(q), h) =
#(1, h(r))
|h(r)| (2)
where h(r) returns the randomized bit sequence
for result r, and #(x, ~y) counts the number of
occurrences of x in sequence ~y. Selecting alter-
natives results to exhibit this bias will result in
watermarked collections that exhibit this same
bias.
3.2 Detecting the Watermark
To classify a collection CN as watermarked or
non-watermarked, we apply the hashing opera-
tion h on each element in CN and concatenate
the sequences. This sequence is tested against
the null hypothesis that it was generated by a
binomial distribution with parameter p = 0.5.
We can apply a Fisherian test of statistical sig-
nificance to determine whether the observed dis-
tribution of bits is unlikely to have occurred by
chance under the null hypothesis (binomial with
p = 0.5).
We consider a collection of results that rejects
the null hypothesis to be watermarked results
generated by our own algorithms. The p-value
under the null hypothesis is efficiently computed
1365
by:
p? value = Pn(X ? x) (3)
=
n?
i=x
(n
i
)
pi(1? p)n?i (4)
where x is the number of 1s observed in the col-
lection, and n is the total number of bits in the
sequence. Comparing this p-value against a de-
sired significance level ?, we reject the null hy-
pothesis for collections that have Pn(X ? x) <
?, thus deciding that such collections were gen-
erated by our own system.
This classification criteria has a fixed false
positive rate. Setting ? = 0.05, we know that
5% of non-watermarked bit sequences will be
falsely labeled as watermarked. This parameter
? can be controlled on an application specific ba-
sis. By biasing the selection of candidate results
to produce more 1s than 0s, we have defined
a watermarking approach that exhibits a fixed
false positive rate, a probabilistically bounded
detection rate and a task independent hashing
and selection criteria. In the next sections, we
will deal with the question of robustness to edit
operations and quality degradation.
3.3 Robustness and Inherent Bias
We would like the ability to identify water-
marked collections to be robust to simple edit
operations. Even slight modifications to the ele-
ments within an item r would yield (by construc-
tion of the hash function), completely different
bit sequences that no longer preserve the biases
introduced by the watermark selection function.
To ensure that the distributional biases intro-
duced by the watermark selector are preserved,
we can optionally map individual results into a
set of sub-results, each one representing some lo-
cal structure of r. h is then applied to each sub-
result and the results concatenated to represent
r. This mapping is defined as a component of
the h operation.
While a particular edit operation might af-
fect a small number of sub-results, the majority
of the bits in the concatenated bit sequence for
r would remain untouched, thereby limiting the
damage to the biases selected during watermark-
ing. This is of course no defense to edit opera-
tions that are applied globally across the result;
our expectation is that such edits would either
significantly degrade the quality of the result or
be straightforward to identify directly.
For example, a sequence of words r = z1 ? ? ? zL
can be mapped into a set of consecutive n-gram
sequences. Operations to edit a word zi in r will
only affect events that consider the word zi. To
account for the fact that alternatives in Dk(q)
might now result in bit sequences of different
lengths, we can generalize the biasing criteria to
directly reflect the expected contribution to the
watermark by defining:
w(r,Dk(q), h) = Pn(X ? #(1, h(r))) (5)
where Pn gives probabilities from binomial(n =
|h(r)|, p = 0.5).
Inherent collection level biases: Our null
hypothesis is based on the assumption that col-
lections of results draw uniformly from the space
of possible results. This assumption might not
always hold and depends on the type of the re-
sults and collection. For example, considering
a text document as a collection of sentences,
we can expect that some sentences might repeat
more frequently than others.
This scenario is even more likely when ap-
plying a mapping into sub-results. n-gram se-
quences follow long-tailed or Zipfian distribu-
tions, with a small number of n-grams contribut-
ing heavily toward the total number of n-grams
in a document.
A random hash function guarantees that in-
puts are distributed uniformly at random over
the output range. However, the same input will
be assigned the same output deterministically.
Therefore, if the distribution of inputs is heav-
ily skewed to certain elements of the input space,
the output distribution will not be uniformly
distributed. The bit sequences resulting from
the high frequency sub-results have the potential
to generate inherently biased distributions when
accumulated at the collection level. We want to
choose a mapping that tends towards generating
uniformly from the space of sub-results. We can
empirically measure the quality of a sub-result
mapping for a specific task by computing the
1366
false positive rate on non-watermarked collec-
tions. For a given significance level ?, an ideal
mapping would result in false positive rates close
to ? as well.
Figure 1 shows false positive rates from 4 al-
ternative mappings, computed on a large corpus
of French documents (see Table 1 for statistics).
Classification decisions are made at the collec-
tion level (documents) but the contribution to
the false positive rate is based on the number
of words in the classified document. We con-
sider mappings from a result (sentence) into its
1-grams, 1 ? 5-grams and 3 ? 5 grams as well
as the non-mapping case, where the full result
is hashed.
Figure 1 shows that the 1-grams and 1 ? 5-
gram generate sub-results that result in heav-
ily biased false positive rates. The 3 ? 5 gram
mapping yields false positive rates close to their
theoretically expected values. 1 Small devia-
tions are expected since documents make differ-
ent contributions to the false positive rate as a
function of the number of words that they repre-
sent. For the remainder of this work, we use the
3-5 gram mapping and the full sentence map-
ping, since the alternatives generate inherently
distributions with very high false positive rates.
3.4 Considering Quality
The watermarking described in Equation 3
chooses alternative results on a per result basis,
with the goal of influencing collection level bit
sequences. The selection criteria as described
will choose the most biased candidates available
in Dk(q). The parameter k determines the ex-
tent to which lesser quality alternatives can be
chosen. If all the alternatives in each Dk(q) are
of relatively similar quality, we expect minimal
degradation due to watermarking.
Specific tasks however can be particularly sen-
sitive to choosing alternative results. Discrimi-
native approaches that optimize for arg max se-
lection like (Och, 2003; Liang et al, 2006; Chi-
ang et al, 2009) train model parameters such
1In the final version of this paper we will perform sam-
pling to create a more reliable estimate of the false posi-
tive rate that is not overly influenced by document length
distributions.
that the top-ranked result is well separated from
its competing alternatives. Different queries also
differ in the inherent ambiguity expected from
their results; sometimes there really is just one
correct result for a query, while for other queries,
several alternatives might be equally good.
By generalizing the definition of the w func-
tion to interpolate the estimated loss in quality
and the gain in the watermarking signal, we can
trade-off the ability to identify the watermarked
collections against quality degradation:
w(r,Dk(q), fw) = ? ? gain(r,Dk(q), fw)
?(1? ?) ? loss(r,Dk(q))
(6)
Loss: The loss(r,Dk(q)) function reflects the
quality degradation that results from selecting
alternative r as opposed to the best ranked can-
didate in Dk(q)). We will experiment with two
variants:
lossrank(r,Dk(q)) = (rank(r)? k)/k
losscost(r,Dk(q)) = (cost(r)?cost(r1))/ cost(r1)
where:
? rank(r): returns the rank of r within Dk(q).
? cost(r): a weighted sum of features (not
normalized over the search space) in a log-
linear model such as those mentioned in
(Och, 2003).
? r1: the highest ranked alternative in Dk(q).
lossrank provides a generally applicable criteria
to select alternatives, penalizing selection from
deep within Dk(q). This estimate of the qual-
ity degradation does not reflect the generating
model?s opinion on relative quality. losscost con-
siders the relative increase in the generating
model?s cost assigned to the alternative trans-
lation.
Gain: The gain(r,Dk(q), fw) function reflects
the gain in the watermarking signal by selecting
candidate r. We simply define the gain as the
Pn(X ? #(1, h(r))) from Equation 5.
1367
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(a) 1-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(b) 1? 5-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(c) 3? 5-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(d) Full result hashing
Figure 1: Comparison of expected false positive rates against observed false positive rates for different
sub-result mappings.
4 Related Work
Using watermarks with the goal of transmitting
a hidden message within images, video, audio
and monolingual text media is common. For
structured text content, linguistic approaches
like (Chapman et al, 2001; Gupta et al, 2006)
use language specific linguistic and semantic
expansions to introduce hidden watermarks.
These expansions provide alternative candidates
within which messages can be encoded. Re-
cent publications have extended this idea to ma-
chine translation, using multiple systems and
expansions to generate alternative translations.
(Stutsman et al, 2006) uses a hashing function
to select alternatives that encode the hidden
message in the lower order bits of the transla-
tion. In each of these approaches, the water-
marker has control over the collection of results
into which the watermark is to be embedded.
These approaches seek to embed a hidden
message into a collection of results that is se-
lected by the watermarker. In contrast, we ad-
dress the condition where the input queries are
not in the watermarker?s control.
The goal is therefore to introduce the water-
mark into all generated results, with the goal of
probabilistically identifying such outputs. Our
approach is also task independent, avoiding the
need for templates to generate additional al-
ternatives. By addressing the problem directly
within the search space of a dynamic program-
ming algorithm, we have access to high quality
alternatives with well defined models of qual-
ity loss. Finally, our approach is robust to local
word editing. By using a sub-result mapping, we
increase the level of editing required to obscure
the watermark signal; at high levels of editing,
the quality of the results themselves would be
significantly degraded.
5 Experiments
We evaluate our watermarking approach applied
to the outputs of statistical machine translation
under the following experimental setup.
A repository of parallel (aligned source and
target language) web documents is sampled to
produce a large corpus on which to evaluate the
watermarking classification performance. The
1368
corpora represent translations into 4 diverse tar-
get languages, using English as the source lan-
guage. Each document in this corpus can be
considered a collection of un-watermarked struc-
tured results, where source sentences are queries
and each target sentence represents a structured
result.
Using a state-of-the-art phrase-based statisti-
cal machine translation system (Och and Ney,
2004) trained on parallel documents identified
by (Uszkoreit et al, 2010), we generate a set
of 100 alternative translations for each source
sentence. We apply the proposed watermarking
approach, along with the proposed refinements
that address task specific loss (Section 3.4) and
robustness to edit operations (Section 3.3) to
generate watermarked corpora.
Each method is controlled via a single param-
eter (like k or ?) which is varied to generate
alternative watermarked collections. For each
parameter value, we evaluate the Recall Rate
and Quality Degradation with the goal of find-
ing a setting that yields a high recall rate, min-
imal quality degradation. False positive rates
are evaluated based on a fixed classification sig-
nificance level of ? = 0.05. The false posi-
tive and recall rates are evaluated on the word
level; a document that is misclassified or cor-
rectly identified contributes its length in words
towards the error calculation. In this work, we
use ? = 0.05 during classification corresponding
to an expected 5% false positive rate. The false
positive rate is a function of h and the signifi-
cance level ? and therefore constant across the
parameter values k and ?.
We evaluate quality degradation on human
translated test corpora that are more typical for
machine translation evaluation. Each test cor-
pus consists of 5000 source sentences randomly
selected from the web and translated into each
respective language.
We chose to evaluate quality on test corpora
to ensure that degradations are not hidden by
imperfectly matched web corpora and are con-
sistent with the kind of results often reported for
machine translation systems. As with the clas-
sification corpora, we create watermarked ver-
sions at each parameter value. For a given pa-
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 
l
o
s
s
recall
FRENCH max K-BestFRENCH model cost gainFRENCH rank gain
Figure 2: BLEU loss against recall of watermarked
content for the baseline approach (max K-best),
rank and cost interpolation.
rameter value, we measure false positive and re-
call rates on the classification corpora and qual-
ity degradation on the evaluation corpora.
Table 1 shows corpus statistics for the classi-
fication and test corpora and non-watermarked
BLEU scores for each target language. All
source texts are in English.
5.1 Loss Interpolated Experiments
Our first set of experiments demonstrates base-
line performance using the watermarking crite-
ria in Equation 5 versus the refinements sug-
gested in Section 3.4 to mitigate quality degra-
dation. The h function is computed on the full
sentence result r with no sub-event mapping.
The following methods are evaluated in Figure 2.
? Baseline method (labeled ?max K-best?):
selects r? purely based on gain in water-
marking signal (Equation 5) and is param-
eterized by k: the number of alternatives
considered for each result.
? Rank interpolation: incorporates rank into
w, varying the interpolation parameter ?.
? Cost interpolation: incorporates cost into
w, varying the interpolation parameter ?.
The observed false positive rate on the French
classification corpora is 1.9%.
1369
Classification Quality
Target # words # sentences # documents # words # sentences BLEU %
Arabic 200107 15820 896 73592 5503 12.29
French 209540 18024 600 73592 5503 26.45
Hindi 183676 13244 1300 73409 5489 20.57
Turkish 171671 17155 1697 73347 5486 13.67
Table 1: Content statistics for classification and quality degradation corpora. Non-watermarked BLEU
scores are reported for the quality corpora.
We consider 0.2% BLEU loss as a thresh-
old for acceptable quality degradation. Each
method is judged by its ability to achieve high
recall below this quality degradation threshold.
Applying cost interpolation yields the best
results in Figure 2, achieving a recall of 85%
at 0.2% BLEU loss, while rank interpolation
achieves a recall of 76%. The baseline approach
of selecting the highest gain candidate within a
depth of k candidates does not provide sufficient
parameterization to yield low quality degrada-
tion. At k = 2, this method yields almost 90%
recall, but with approximately 0.4% BLEU loss.
5.2 Robustness Experiments
In Section 5.2, we proposed mapping results into
sub-events or features. We considered alterna-
tive feature mappings in Figure 1, finding that
mapping sentence results into a collection of 3-
5 grams yields acceptable false positive rates at
varied levels of ?.
Figure 3 presents results that compare mov-
ing from the result level hashing to the 3-5 gram
sub-result mapping. We show the impact of the
mapping on the baseline max K-best method as
well as for cost interpolation. There are sub-
stantial reductions in recall rate at the 0.2%
BLEU loss level when applying sub-result map-
pings in cases. The cost interpolation method
recall drops from 85% to 77% when using the
3-5 grams event mapping. The observed false
positive rate of the 3-5 gram mapping is 4.7%.
By using the 3-5 gram mapping, we expect
to increase robustness against local word edit
operations, but we have sacrificed recall rate due
to the inherent distributional bias discussed in
Section 3.3.
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 
l
o
s
s
recall
FRENCH nbest max K-best full sentenceFRENCH max K-best 3-5 gramsFRENCH cost interp. full sentenceFRENCH cost interp. 3-5 grams
Figure 3: BLEU loss against recall of watermarked
content for the baseline and cost interpolation meth-
ods using both result level and 3-5 gram mapped
events.
5.3 Multilingual Experiments
The watermarking approach proposed here in-
troduces no language specific watermarking op-
erations and it is thus broadly applicable to
translating into all languages. In Figure 4, we
report results for the baseline and cost interpola-
tion methods, considering both the result level
and 3-5 gram mapping. We set ? = 0.05 and
measure recall at 0.2% BLEU degradation for
translation from English into Arabic, French,
Hindi and Turkish. The observed false posi-
tive rates for full sentence hashing are: Arabic:
2.4%, French: 1.8%, Hindi: 5.6% and Turkish:
5.5%, while for the 3-5 gram mapping, they are:
Arabic: 5.8%, French: 7.5%, Hindi:3.5% and
Turkish: 6.2%. Underlying translation qual-
ity plays an important role in translation qual-
ity degradation when watermarking. Without
a sub-result mapping, French (BLEU: 26.45%)
1370
 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Arabic French Hindi Turkishrecall sentence-level3-to-5 grams
Figure 4: Loss of recall when using 3-5 gram mapping
vs sentence level mapping for Arabic, French, Hindi
and Turkish translations.
achieves recall of 85% at 0.2% BLEU loss, while
the other languages achieve over 90% recall at
the same BLEU loss threshold. Using a sub-
result mapping degrades quality for each lan-
guage pair, but changes the relative perfor-
mance. Turkish experiences the highest rela-
tive drop in recall, unlike French and Arabic,
where results are relatively more robust to using
sub-sentence mappings. This is likely a result of
differences in n-gram distributions across these
languages. The languages considered here all
use space separated words. For languages that
do not, like Chinese or Thai, our approach can
be applied at the character level.
6 Conclusions
In this work we proposed a general method
to watermark and probabilistically identify the
structured outputs of machine learning algo-
rithms. Our method provides probabilistic
bounds on detection ability, analytic control on
quality degradation and is robust to local edit-
ing operations. Our method is applicable to
any task where structured outputs are generated
with ambiguities or ties in the results. We ap-
plied this method to the outputs of statistical
machine translation, evaluating each refinement
to our approach with false positive and recall
rates against BLEU score quality degradation.
Our results show that it is possible, across sev-
eral language pairs, to achieve high recall rates
(over 80%) with low false positive rates (between
5 and 8%) at minimal quality degradation (0.2%
BLEU), while still allowing for local edit opera-
tions on the translated output. In future work
we will continue to investigate methods to mit-
igate quality loss.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Peter F. Brown, Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19:263?311.
Mark Chapman, George Davida, and Marc
Rennhardway. 2001. A practical and effec-
tive approach to large-scale automated linguistic
steganography. In Proceedings of the Information
Security Conference.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Lan-
guage Technologies (NAACL-HLT).
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics.
In Research and Development in Information Re-
trieval, pages 121?128.
Gaurav Gupta, Josef Pieprzyk, and Hua Xiong
Wang. 2006. An attack-localizing watermarking
scheme for natural language documents. In Pro-
ceedings of the 2006 ACM Symposium on Informa-
tion, computer and communications security, ASI-
ACCS ?06, pages 157?165, New York, NY, USA.
ACM.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein,
and Ben Taskar. 2006. An end-to-end dis-
criminative approach to machine translation. In
Proceedings of the Joint International Conference
on Computational Linguistics and Association of
Computational Linguistics (COLING/ACL, pages
761?768.
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2004. The
1371
alignment template approach to statistical ma-
chine translation. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 2003 Meeting of the Asssociation of Com-
putational Linguistics.
Philip Resnik and Noah A. Smith. 2003. The web as
a parallel corpus. computational linguistics. Com-
putational Linguistics.
Ryan Stutsman, Mikhail Atallah, Christian
Grothoff, and Krista Grothoff. 2006. Lost
in just the translation. In Proceedings of the 2006
ACM Symposium on Applied Computing.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 2010 COLING.
1372
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1395?1404,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language-independent Compound Splitting with Morphological Operations
Klaus Macherey1 Andrew M. Dai2 David Talbot1 Ashok C. Popat1 Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{kmach,talbot,popat,och}@google.com
2University of Edinburgh
10 Crichton Street
Edinburgh, UK EH8 9AB
a.dai@ed.ac.uk
Abstract
Translating compounds is an important prob-
lem in machine translation. Since many com-
pounds have not been observed during train-
ing, they pose a challenge for translation sys-
tems. Previous decompounding methods have
often been restricted to a small set of lan-
guages as they cannot deal with more complex
compound forming processes. We present a
novel and unsupervised method to learn the
compound parts and morphological operations
needed to split compounds into their com-
pound parts. The method uses a bilingual
corpus to learn the morphological operations
required to split a compound into its parts.
Furthermore, monolingual corpora are used to
learn and filter the set of compound part can-
didates. We evaluate our method within a ma-
chine translation task and show significant im-
provements for various languages to show the
versatility of the approach.
1 Introduction
A compound is a lexeme that consists of more than
one stem. Informally, a compound is a combina-
tion of two or more words that function as a single
unit of meaning. Some compounds are written as
space-separated words, which are called open com-
pounds (e.g. hard drive), while others are written
as single words, which are called closed compounds
(e.g. wallpaper). In this paper, we shall focus only
on closed compounds because open compounds do
not require further splitting.
The objective of compound splitting is to split a
compound into its corresponding sequence of con-
stituents. If we look at how compounds are created
from lexemes in the first place, we find that for some
languages, compounds are formed by concatenating
existing words, while in other languages compound-
ing additionally involves certain morphological op-
erations. These morphological operations can be-
come very complex as we illustrate in the following
case studies.
1.1 Case Studies
Below, we look at splitting compounds from 3 differ-
ent languages. The examples introduce in part the
notation used for the decision rule outlined in Sec-
tion 3.1.
1.1.1 English Compound Splitting
The word flowerpot can appear as a closed or open
compound in English texts. To automatically split
the closed form we have to try out every split point
and choose the split with minimal costs according to
a cost function. Let's assume that we already know
that flowerpot must be split into two parts. Then we
have to position two split points that mark the end of
each part (one is always reserved for the last charac-
ter position). The number of split points is denoted
by K (i.e. K = 2), while the position of split points
is denoted by n1 and n2. Since flowerpot consists of
9 characters, we have 8 possibilities to position split
point n1 within the characters c1, . . . , c8. The final
split point corresponds with the last character, that is,
n2 = 9. Trying out all possible single splits results
in the following candidates:
flowerpot ? f+ lowerpot
flowerpot ? fl+ owerpot
...
flowerpot ? flower+ pot
...
flowerpot ? flowerpo+ t
1395
If we associate each compound part candidate with
a cost that reflects how frequent this part occurs in a
large collection of English texts, we expect that the
correct split flower + pot will have the lowest cost.
1.1.2 German Compound Splitting
The previous example covered a casewhere the com-
pound is constructed by directly concatenating the
compound parts. While this works well for En-
glish, other languages require additional morpholog-
ical operations. To demonstrate, we look at the Ger-
man compound Verkehrszeichen (traffic sign) which
consists of the two nouns Verkehr (traffic) and Zei-
chen (sign). Let's assume that we want to split this
word into 3 parts, that is, K = 3. Then, we get the
following candidates.
Verkehrszeichen ? V+ e+ rkehrszeichen
Verkehrszeichen ? V+ er+ kehrszeichen
...
Verkehrszeichen ? Verkehr+ s+ zeichen
...
Verkehrszeichen ? Verkehrszeich+ e+ n
Using the same procedure as described before, we
can lookup the compound parts in a dictionary or de-
termine their frequency from large text collections.
This yields the optimal split points n1 = 7, n2 =
8, n3 = 15. The interesting part here is the addi-
tional s morpheme, which is called a linking mor-
pheme, because it combines the two compound parts
to form the compound Verkehrszeichen. If we have
a list of all possible linking morphemes, we can
hypothesize them between two ordinary compound
parts.
1.1.3 Greek Compound Splitting
The previous example required the insertion of a
linking morpheme between two compound parts.
We shall now look at a more complicated mor-
phological operation. The Greek compound
?????????? (cardboard box) consists of the two
parts ????? (paper) and ????? (box). Here, the
problem is that the parts ????? and ????? are not
valid words in Greek. To lookup the correct words,
we must substitute the suffix of the compound part
candidates with some other morphemes. If we allow
the compound part candidates to be transformed by
some morphological operation, we can lookup the
transformed compound parts in a dictionary or de-
termine their frequencies in some large collection of
Greek texts. Let's assume that we need only one split
point. Then this yields the following compound part
candidates:
?????????? ? ? + ?????????
?????????? ? ? + ????????? g2 : ? / ?
?????????? ? ? + ????????? g2 : ? / ?
...
?????????? ? ????? + ????? g1 : ? / ? ,
g2 : ? / ?...
?????????? ? ????????? + ? g1 : ? / ?
?????????? ? ????????? + ? g2 : ? / ?
Here, gk : s/t denotes the kth compound part which
is obtained by replacing string s with string t in the
original string, resulting in the transformed part gk.
1.2 Problems and Objectives
Our goal is to design a language-independent com-
pound splitter that is useful for machine translation.
The previous examples addressed the importance of
a cost function that favors valid compound parts ver-
sus invalid ones. In addition, the examples have
shown that, depending on the language, the morpho-
logical operations can become very complex. For
most Germanic languages like Danish, German, or
Swedish, the list of possible linking morphemes is
rather small and can be provided manually. How-
ever, in general, these lists can become very large,
and language experts who could provide such lists
might not be at our disposal. Because it seems in-
feasible to list the morphological operations explic-
itly, we want to find and extract those operations
automatically in an unsupervised way and provide
them as an additional knowledge source to the de-
compounding algorithm.
Another problem is how to evaluate the quality
of the compound splitter. One way is to compile
for every language a large collection of compounds
together with their valid splits and to measure the
proportion of correctly split compounds. Unfortu-
nately, such lists do not exist for many languages.
1396
While the training algorithm for our compound split-
ter shall be unsupervised, the evaluation data needs
to be verified by human experts. Since we are in-
terested in improving machine translation and to cir-
cumvent the problem of explicitly annotating com-
pounds, we evaluate the compound splitter within a
machine translation task. By decompounding train-
ing and test data of a machine translation system, we
expect an increase in the number of matching phrase
table entries, resulting in better translation quality
measured in BLEU score (Papineni et al, 2002).
If BLEU score is sensitive enough to measure the
quality improvements obtained from decompound-
ing, there is no need to generate a separate gold stan-
dard for compounds.
Finally, we do not want to split non-compounds
and named entities because we expect them to be
translated non-compositionally. For example, the
German wordDeutschland (Germany) could be split
into two parts Deutsch (German) + Land (coun-
try). Although this is a valid split, named entities
should be kept as single units. An example for a
non-compound is the German participle vereinbart
(agreed) which could be wrongly split into the parts
Verein (club) + Bart (beard). To avoid overly eager
splitting, we will compile a list of non-compounds in
an unsupervised way that serves as an exception list
for the compound splitter. To summarize, we aim to
solve the following problems:
? Define a cost function that favors valid com-
pound parts and rejects invalid ones.
? Learn morphological operations, which is im-
portant for languages that have complex com-
pound forming processes.
? Apply compound splitting to machine transla-
tion to aid in translation of compounds that have
not been seen in the bilingual training data.
? Avoid splitting non-compounds and named en-
tities as this may result in wrong translations.
2 Related work
Previous work concerning decompounding can be
divided into two categories: monolingual and bilin-
gual approaches.
Brown (2002) describes a corpus-driven approach
for splitting compounds in a German-English trans-
lation task derived from a medical domain. A large
proportion of the tokens in both texts are cognates
with a Latin or Greek etymological origin. While the
English text keeps the cognates as separate tokens,
they are combined into compounds in the German
text. To split these compounds, the author compares
both the German and the English cognates on a char-
acter level to find reasonable split points. The algo-
rithm described by the author consists of a sequence
of if-then-else conditions that are applied on the two
cognates to find the split points. Furthermore, since
the method relies on finding similar character se-
quences between both the source and the target to-
kens, the approach is restricted to cognates and can-
not be applied to split more complex compounds.
Koehn and Knight (2003) present a frequency-
based approach to compound splitting for German.
The compound parts and their frequencies are es-
timated from a monolingual corpus. As an exten-
sion to the frequency approach, the authors describe
a bilingual approach where they use a dictionary ex-
tracted from parallel data to find better split options.
The authors allow only two linking morphemes be-
tween compound parts and a few letters that can be
dropped. In contrast to our approach, those opera-
tions are not learned automatically, but must be pro-
vided explicitly.
Garera and Yarowsky (2008) propose an approach
to translate compounds without the need for bilin-
gual training texts. The compound splitting pro-
cedure mainly follows the approach from (Brown,
2002) and (Koehn and Knight, 2003), so the em-
phasis is put on finding correct translations for com-
pounds. To accomplish this, the authors use cross-
language compound evidence obtained from bilin-
gual dictionaries. In addition, the authors describe a
simple way to learn glue characters by allowing the
deletion of up to two middle and two end charac-
ters.1 More complex morphological operations are
not taken into account.
Alfonseca et al (2008b) describe a state-of-the-
art German compound splitter that is particularly ro-
bust with respect to noise and spelling errors. The
compound splitter is trained on monolingual data.
Besides applying frequency and probability-based
methods, the authors also take the mutual informa-
tion of compound parts into account. In addition, the
1However, the glue characters found by this procedure seem
to be biased for at least German and Albanian. A very frequent
glue morpheme like -es- is not listed, while glue morphemes
like -k- and -h- rank very high, although they are invalid glue
morphemes for German. Albanian shows similar problems.
1397
authors look for compound parts that occur in dif-
ferent anchor texts pointing to the same document.
All these signals are combined and the weights are
trained using a support vector machine classifier. Al-
fonseca et al (2008a) apply this compound splitter
on various other Germanic languages.
Dyer (2009) applies a maximum entropy model
of compound splitting to generate segmentation lat-
tices that serve as input to a translation system.
To train the model, reference segmentations are re-
quired. Here, we produce only single best segmen-
tations, but otherwise do not rely on reference seg-
mentations.
3 Compound Splitting Algorithm
In this section, we describe the underlying optimiza-
tion problem and the algorithm used to split a token
into its compound parts. Starting from Bayes' de-
cision rule, we develop the Bellman equation and
formulate a dynamic programming-based algorithm
that takes a word as input and outputs the constituent
compound parts. We discuss the procedure used to
extract compound parts from monolingual texts and
to learn themorphological operations using bilingual
corpora.
3.1 Decision Rule for Compound Splitting
Given a token w = c1, . . . , cN = cN1 consisting of a
sequence of N characters ci, the objective function
is to find the optimal number K? and sequence of split
points n?K?0 such that the subwords are the constituents
of the token, where2 n0 := 0 and nK := N :
w = cN1 ? (K?, n?K?0 ) =
= argmax
K,nK0
{
Pr(cN1 ,K, nK0 )
}
(1)
= argmax
K,nK0
{
Pr(K) ? Pr(cN1 , nK0 |K)
}
u argmax
K,nK0
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1|K)?
?p(nk|nk?1,K)} (2)
with p(n0) = p(nK |?) ? 1. Equation 2 requires that
token w can be fully decomposed into a sequence
2For algorithmic reasons, we use the start position 0 to rep-
resent a fictitious start symbol before the first character of the
word.
of lexemes, the compound parts. Thus, determin-
ing the optimal segmentation is sufficient for finding
the constituents. While this may work for some lan-
guages, the subwords are not valid words in general
as discussed in Section 1.1.3. Therefore, we allow
the lexemes to be the result of a transformation pro-
cess, where the transformed lexemes are denoted by
gK1 . This leads to the following refined decision rule:
w = cN1 ? (K?, n?K?0 , g?K?1 ) =
= argmax
K,nK0 ,gK1
{
Pr(cN1 ,K, nK0 , gK1 )
}
(3)
= argmax
K,nK0 ,gK1
{
Pr(K) ? Pr(cN1 , nK0 , gK1 |K)
}
(4)
u argmax
K,nK0 ,gK1
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1, gk|K)
? ?? ?
compound part probability
?
? p(nk|nk?1,K)
}
(5)
The compound part probability is a zero-order
model. If we penalize each split with a constant split
penalty ?, and make the probability independent of
the number of splits K, we arrive at the following
decision rule:
w = cN1 ? (K?, n?K?1 , g?K?1 )
= argmax
K,nK0 ,gK1
{
?K ?
K
?
k=1
p(cnknk?1+1, nk?1, gk)
}
(6)
3.2 Dynamic Programming
We use dynamic programming to find the optimal
split sequence. Each split infers certain costs that
are determined by a cost function. The total costs of
a decomposed word can be computed from the in-
dividual costs of the component parts. For the dy-
namic programming approach, we define the follow-
ing auxiliary function Q with nk = j:
Q(cj1) = max
nk0 ,gk1
{
?k ?
k
?
?=1
p(cn?n??1+1, n??1, g?)
}
that is, Q(cj1) is equal to the minimal costs (maxi-
mum probability) that we assign to the prefix string
cj1 where we have used k split points at positions nk1 .
This yields the following recursive equation:
Q(cj1) = maxnk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(7)
1398
Algorithm 1 Compound splitting
Input: input word w = cN1
Output: compound parts
Q(0) = 0
Q(1) = ? ? ? = Q(N) = ?
for i = 0, . . . , N ? 1 do
for j = i + 1, . . . , N do
split-costs = Q(i) + cost(cji+1, i, gj) +
split-penalty
if split-costs < Q(j) then
Q(j) = split-costs
B(j) = (i, gj)
end if
end for
end for
with backpointer
B(j) = argmax
nk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(8)
Using logarithms in Equations 7 and 8, we can inter-
pret the quantities as additive costs rather than proba-
bilities. This yields Algorithm 1, which is quadratic
in the length of the input string. By enforcing that
each compound part does not exceed a predefined
constant length `, we can change the second for loop
as follows:
for j = i + 1, . . . ,min(i + `,N) do
With this change, Algorithm 1 becomes linear in the
length of the input word, O(|w|).
4 Cost Function and Knowledge Sources
The performance of Algorithm 1 depends on
the cost function cost(?), that is, the probability
p(cnknk?1+1, nk?1, gk). This cost function incorpo-
rates knowledge about morpheme transformations,
morpheme positionswithin a compound part, and the
compound parts themselves.
4.1 Learning Morphological Operations using
Phrase Tables
Let s and t be strings of the (source) language al-
phabet A. A morphological operation s/t is a pair
of strings s, t ? A?, where s is replaced by t. With
the usual definition of the Kleene operator ?, s and
t can be empty, denoted by ?. An example for such
a pair is ?/es, which models the linking morpheme
es in the German compound Bundesagentur (federal
agency):
Bundesagentur ? Bund+ es+ Agentur .
Note that by replacing either s or t with ?, we can
model insertions or deletions of morphemes. The
explicit dependence on position nk?1 in Equation 6
allows us to determine if we are at the beginning,
in the middle, or at the end of a token. Thus, we
can distinguish between start, middle, or end mor-
phemes and hypothesize them during search.3 Al-
though not explicitly listed in Algorithm 1, we dis-
allow sequences of linking morphemes. This can
be achieved by setting the costs to infinity for those
morpheme hypotheses, which directly succeed an-
other morpheme hypothesis.
To learn the morphological operations involved
in compounding, we determine the differences be-
tween a compound and its compound parts. This can
be done by computing the Levenshtein distance be-
tween the compound and its compound parts, with
the allowable edit operations being insertion, dele-
tion, or substitution of one or more characters. If we
store the current and previous characters, edit opera-
tion and the location (prefix, infix or suffix) at each
position during calculation of the Levenshtein dis-
tance then we can obtain the morphological opera-
tions required for compounding. Applying the in-
verse operations, that is, replacing twith s yields the
operation required for decompounding.
4.1.1 Finding Compounds and their Parts
To learn the morphological operations, we need
compounds together with their compound parts. The
basic idea of finding compound candidates and their
compound parts in a bilingual setting are related to
the ideas presented in (Garera and Yarowsky, 2008).
Here, we use phrase tables rather than dictionaries.
Although phrase tablesmight containmore noise, we
believe that overall phrase tables cover more phe-
nomena of translations thanwhat can be found in dic-
tionaries. The procedure is as follows. We are given
a phrase table that provides translations for phrases
from a source language l into English and from En-
glish into l. Under the assumption that English does
not contain many closed compounds, we can search
3We jointly optimize over K and the split points nk, so we
know that cnKnK?1 is a suffix of w.
1399
the phrase table for those single-token source words
f in language l, which translate into multi-token En-
glish phrases e1, . . . , en for n > 1. This results
in a list of (f ; e1, . . . , en) pairs, which are poten-
tial compound candidates together with their English
translations. If for each pair, we take each token ei
from the English (multi-token) phrase and lookup
the corresponding translation for language l to get
gi, we should find entries that have at least some
partial match with the original source word f , if f
is a true compound. Because the translation phrase
table was generated automatically during the train-
ing of a multi-language translation system, there is
no guarantee that the original translations are cor-
rect. Thus, the bilingual extraction procedure is
subject to introduce a certain amount of noise. To
mitigate this, thresholds such as minimum edit dis-
tance between the potential compound and its parts,
minimum co-occurrence frequencies for the selected
bilingual phrase pairs and minimum source and tar-
get word lengths are used to reduce the noise at the
expense of finding fewer compounds. Those entries
that obey these constraints are output as triples of
form:
(f ; e1, . . . , en; g1, . . . , gn) (9)
where
? f is likely to be a compound,
? e1, . . . , en is the English translation, and
? g1, . . . , gn are the compound parts of f .
The following example for German illustrates the
process. Suppose that the most probable translation
for?berweisungsbetrag is transfer amount using the
phrase table. We then look up the translation back to
German for each translated token: transfer translates
to?berweisung and amount translates toBetrag. We
then calculate the distance between all permutations
of the parts and the original compound and choose
the one with the lowest distance and highest transla-
tion probability: ?berweisung Betrag.
4.2 Monolingual Extraction of Compound
Parts
The most important knowledge source required for
Algorithm 1 is a word-frequency list of compound
parts that is used to compute the split costs. The
procedure described in Section 4.1.1 is useful for
learning morphological operations, but it is not suffi-
cient to extract an exhaustive list of compound parts.
Such lists can be extracted frommonolingual data for
which we use language model (LM) word frequency
lists in combination with some filter steps. The ex-
traction process is subdivided into 2 passes, one over
a high-quality news LM to extract the parts and the
other over a web LM to filter the parts.
4.2.1 Phase 1: Bootstrapping pass
In the first pass, we generate word frequency lists de-
rived from news articles for multiple languages. The
motivation for using news articles rather than arbi-
trary web texts is that news articles are in general
less noisy and contain fewer spelling mistakes. The
language-dependent word frequency lists are filtered
according to a sequence of filter steps. These filter
steps include discarding all words that contain digits
or punctuations other than hyphen, minimum occur-
rence frequency, and a minimum length which we
set to 4. The output is a table that contains prelim-
inary compound parts together with their respective
counts for each language.
4.2.2 Phase 2: Filtering pass
In the second pass, the compound part vocabulary
is further reduced and filtered. We generate a LM
vocabulary based on arbitrary web texts for each lan-
guage and build a compound splitter based on the vo-
cabulary list that was generated in phase 1. We now
try to split every word of the web LM vocabulary
based on the compound splitter model from phase
1. For the compound parts that occur in the com-
pound splitter output, we determine how often each
compound part was used and output only those com-
pound parts whose frequency exceed a predefined
threshold n.
4.3 Example
Suppose we have the following word frequencies
output from pass 1:
floor 10k poll 4k
flow 9k pot 5k
flower 15k potter 20k
In pass 2, we observe the word flowerpot. With the
above list, the only compound parts used are flower
and pot. If we did not split any other words and
threshold at n = 1, our final list would consist of
flower and pot. This filtering pass has the advantage
of outputting only those compound part candidates
1400
which were actually used to split words from web
texts. The thresholding also further reduces the risk
of introducing noise. Another advantage is that since
the set of parts output in the first pass may contain a
high number of compounds, the filter is able to re-
move a large number of these compounds by exam-
ining relative frequencies. In our experiments, we
have assumed that compound part frequencies are
higher than the compound frequency and so remove
words from the part list that can themselves be split
and have a relatively high frequency. Finally, after
removing the low frequency compound parts, we ob-
tain the final compound splitter vocabulary.
4.4 Generating Exception Lists
To avoid eager splitting of non-compounds and
named entities, we use a variant of the procedure de-
scribed in Section 4.1.1. By emitting all those source
words that translate with high probability into single-
token English words, we obtain a list of words that
should not be split.4
4.5 Final Cost Function
The final cost function is defined by the following
components which are combined log-linearly.
? The split penalty ? penalizes each compound
part to avoid eager splitting.
? The cost for each compound part gk is com-
puted as ? logC(gk), where C(gk) is the un-
igram count for gk obtained from the news LM
word frequency list. Since we use a zero-order
model, we can ignore the normalization and
work with unigram counts rather than unigram
probabilities.
? Because Algorithm 1 iterates over the charac-
ters of the input token w, we can infer from the
boundaries (i, j) if we are at the start, in the
middle, or at the end of the token. Applying
a morphological operation adds costs 1 to the
overall costs.
Although the cost function is language dependent,
we use the same split penalty weight ? = 20 for all
languages except for German, where the split penalty
weight is set to 13.5.
5 Results
To show the language independence of the approach
within a machine translation task, we translate from
languages belonging to different language families
into English. The publicly available Europarl corpus
is not suitable for demonstrating the utility of com-
pound splitting because there are few unseen com-
pounds in the test section of the Europarl corpus.
The WMT shared translation task has a broader do-
main compared to Europarl but covers only a few
languages. Hence, we present results for German-
English using the WMT-07 data and cover other lan-
guages using non-public corporawhich contain news
as well as open-domain web texts. Table 1 lists the
various corpus statistics. The source languages are
grouped according to their language family.
For learning the morphological operations, we al-
lowed the substitution of at most 2 consecutive char-
acters. Furthermore, we only allowed at most one
morphological substitution to avoid introducing too
much noise. The found morphological operations
were sorted according to their frequencies. Those
which occurred less than 100 times were discarded.
Examples of extracted morphological operations are
given in Table 2. Because the extraction procedure
described in Section 4.1 is not purely restricted to the
case of decompounding, we found that many mor-
phological operations emitted by this procedure re-
flect morphological variations that are not directly
linked to compounding, but caused by inflections.
To generate the language-dependent lists of com-
pound parts, we used language model vocabulary
lists5 generated from news texts for different lan-
guages as seeds for the first pass. These lists were
filtered by discarding all entries that either con-
tained digits, punctuations other than hyphens, or se-
quences of the same characters. In addition, the in-
frequent entries were discarded as well to further re-
duce noise. For the second pass, we used the lists
generated in the first pass together with the learned
morphological operations to construct a preliminary
compound splitter. We then generated vocabulary
lists for monolingual web texts and applied the pre-
liminary compound splitter onto this list. The used
4Because we will translate only into English, this is not an
issue for the introductory example flowerpot.
5The vocabulary lists also contain the word frequencies. We
use the term vocabulary list synonymously for a word frequency
list.
1401
Family Src Language #Tokens Train src/trg #Tokens Dev src/trg #Tokens Tst src/trg
Germanic Danish 196M 201M 43, 475 44, 479 72, 275 74, 504
German 43M 45M 23, 151 22, 646 45, 077 43, 777
Norwegian 251M 255M 42, 096 43, 824 70, 257 73, 556
Swedish 201M 213M 42, 365 44, 559 70, 666 74, 547
Hellenic Greek 153M 148M 47, 576 44, 658 79, 501 74, 776
Uralic Estonian 199M 244M 34, 987 44, 658 57, 916 74, 765
Finnish 205M 246M 32, 119 44, 658 53, 365 74, 771
Table 1: Corpus statistics for various language pairs. The target language is always English. The source languages are
grouped according to their language family.
Language morpholog. operations
Danish -/?, s/?
German -/?, s/?, es/?, n/?, e/?, en/?
Norwegian -/?, s/?, e/?
Swedish -/?, s/?
Greek ?/?, ?/?, ?/?, ?/?, ?/?, ?/?
Estonian -/?, e/?, se/?
Finnish ?/n, n/?, ?/en
Table 2: Examples of morphological operations that were
extracted from bilingual corpora.
compound parts were collected and sorted according
to their frequencies. Those which were used at least
2 times were kept in the final compound parts lists.
Table 3 reports the number of compound parts kept
after each pass. For example, the Finnish news vo-
cabulary list initially contained 1.7M entries. After
removing non-alpha and infrequent words in the first
filter step, we obtained 190K entries. Using the pre-
liminary compound splitter in the second filter step
resulted in 73K compound part entries.
The finally obtained compound splitter was in-
tegrated into the preprocessing pipeline of a state-
of-the-art statistical phrase-based machine transla-
tion system that works similar to the Moses de-
coder (Koehn et al, 2007). By applying the com-
pound splitter during both training and decoding we
ensured that source language tokens were split in
the same way. Table 4 presents results for vari-
ous language-pairs with and without decompound-
ing. Both the Germanic and the Uralic languages
show significant BLEU score improvements of 1.3
BLEU points on average. The confidence inter-
vals were computed using the bootstrap resampling
normal approximation method described in (Noreen,
1989). While the compounding process for Ger-
manic languages is rather simple and requires only a
few linking morphemes, compounds used in Uralic
languages have a richer morphology. In contrast to
the Germanic and Uralic languages, we did not ob-
serve improvements for Greek. To investigate this
lack of performance, we turned off transliteration
and kept unknown source words in their original
script. We analyzed the number of remaining source
characters in the baseline system and the system us-
ing compound splitting by counting the number of
Greek characters in the translation output. The num-
ber of remaining Greek characters in the translation
output was reduced from 6, 715 in the baseline sys-
tem to 3, 624 in the systemwhich used decompound-
ing. In addition, a few other metrics like the number
of source words that consisted of more than 15 char-
acters decreased as well. Because we do not know
how many compounds are actually contained in the
Greek source sentences6 and because the frequency
of using compounds might vary across languages,
we cannot expect the same performance gains across
languages belonging to different language families.
An interesting observation is, however, that if one
language from a language family shows performance
gains, then there are performance gains for all the
languages in that family. We also investigated the ef-
fect of not using any morphological operations. Dis-
allowing all morphological operations accounts for
a loss of 0.1 - 0.2 BLEU points across translation
systems and increases the compound parts vocabu-
lary lists by up to 20%, which means that most of the
gains can be achieved with simple concatenation.
The exception lists were generated according to
the procedure described in Section 4.4. Since we
aimed for precision rather than recall when con-
structing these lists, we inserted only those source
6Quite a few of the remaining Greek characters belong to
rare named entities.
1402
Language initial vocab size #parts after 1st pass #parts after 2nd pass
Danish 918, 708 132, 247 49, 592
German 7, 908, 927 247, 606 45, 059
Norwegian 1, 417, 129 237, 099 62, 107
Swedish 1, 907, 632 284, 660 82, 120
Greek 877, 313 136, 436 33, 130
Estonian 742, 185 81, 132 36, 629
Finnish 1, 704, 415 190, 507 73, 568
Table 3: Number of remaining compound parts for various languages after the first and second filter step.
System BLEU[%] w/o splitting BLEU[%] w splitting ? CI 95%
Danish 42.55 44.39 1.84 (? 0.65)
German WMT-07 25.76 26.60 0.84 (? 0.70)
Norwegian 42.77 44.58 1.81 (? 0.64)
Swedish 36.28 38.04 1.76 (? 0.62)
Greek 31.85 31.91 0.06 (? 0.61)
Estonian 20.52 21.20 0.68 (? 0.50)
Finnish 25.24 26.64 1.40 (? 0.57)
Table 4: BLEU score results for various languages translated into English with and without compound splitting.
Language Split source translation
German no Die EU ist nicht einfach ein Freundschaftsclub. The EU is not just a Freundschaftsclub.
yes Die EU ist nicht einfach ein Freundschaft Club The EU is not simply a friendship club.
Greek no ?? ????? ??????????? ??????????; What ??????????? configuration?
yes ?? ????? ????? ?????? ??????????; What is pulse code modulation?
Finnish no Lis?vuodevaatteet ja pyyheliinat ovat kaapissa. Lis?vuodevaatteet and towels are in the closet.
yes Lis? Vuode Vaatteet ja pyyheliinat ovat kaapissa. Extra bed linen and towels are in the closet.
Table 5: Examples of translations into English with and without compound splitting.
words whose co-occurrence count with a unigram
translation was at least 1, 000 and whose translation
probability was larger than 0.1. Furthermore, we re-
quired that at least 70%of all target phrase entries for
a given source word had to be unigrams. All decom-
pounding results reported in Table 4 were generated
using these exception lists, which prevented wrong
splits caused by otherwise overly eager splitting.
6 Conclusion and Outlook
We have presented a language-independent method
for decompounding that improves translations for
compounds that otherwise rarely occur in the bilin-
gual training data. We learned a set of morpholog-
ical operations from a translation phrase table and
determined suitable compound part candidates from
monolingual data in a two pass process. This al-
lowed us to learn morphemes and operations for lan-
guages where these lists are not available. In addi-
tion, we have used the bilingual information stored
in the phrase table to avoid splitting non-compounds
as well as frequent named entities. All knowledge
sources were combined in a cost function that was
applied in a compound splitter based on dynamic
programming. Finally, we have shown this improves
translation performance on languages from different
language families.
The weights were not optimized in a systematic
way but set manually to their respective values. In
the future, the weights of the cost function should be
learned automatically by optimizing an appropriate
error function. Instead of using gold data, the devel-
opment data for optimizing the error function could
be collected without supervision using the methods
proposed in this paper.
1403
References
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008a. Decompounding query keywords from com-
pounding languages. In Proc. of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL): Human Language Technologies (HLT), pages
253--256, Columbus, Ohio, USA, June.
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008b. German decompounding in a difficult corpus.
In A. Gelbukh, editor, Lecture Notes in Computer Sci-
ence (LNCS): Proc. of the 9th Int. Conf. on Intelligent
Text Processing and Computational Linguistics (CI-
CLING), volume 4919, pages 128--139. Springer Ver-
lag, February.
Ralf D. Brown. 2002. Corpus-Driven Splitting of Com-
poundWords. In Proc. of the 9th Int. Conf. on Theoret-
ical andMethodological Issues inMachine Translation
(TMI), pages 12--21, Keihanna, Japan, March.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc. of
the Human Language Technologies (HLT): The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL), pages
406--414, Boulder, Colorado, June.
Nikesh Garera and David Yarowsky. 2008. Translating
Compounds by Learning Component Gloss Transla-
tion Models via Multiple Languages. In Proc. of the
3rd Internation Conference on Natural Language Pro-
cessing (IJCNLP), pages 403--410, Hyderabad, India,
January.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proc. of the 10th
Conf. of the European Chapter of the Association for
Computational Linguistics (EACL), volume 1, pages
187--193, Budapest, Hungary, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of the 44th
Annual Meeting of the Association for Computational
Linguistics (ACL), volume 1, pages 177--180, Prague,
Czech Republic, June.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311--318, Philadel-
phia, Pennsylvania, July.
1404
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12?21,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Lightweight Evaluation Framework for Machine Translation Reordering
David Talbot1 and Hideto Kazawa2 and Hiroshi Ichikawa2
Jason Katz-Brown2 and Masakazu Seno2 and Franz J. Och1
1 Google Inc. 2 Google Japan
1600 Amphitheatre Parkway Roppongi Hills Mori Tower
Mountain View, CA 94043 6-10-1 Roppongi, Tokyo 106-6126
{talbot, och}@google.com {kazawa, ichikawa}@google.com
{jasonkb, seno}@google.com
Abstract
Reordering is a major challenge for machine
translation between distant languages. Recent
work has shown that evaluation metrics that
explicitly account for target language word or-
der correlate better with human judgments of
translation quality. Here we present a simple
framework for evaluating word order indepen-
dently of lexical choice by comparing the sys-
tem?s reordering of a source sentence to ref-
erence reordering data generated from manu-
ally word-aligned translations. When used to
evaluate a system that performs reordering as
a preprocessing step our framework allows the
parser and reordering rules to be evaluated ex-
tremely quickly without time-consuming end-
to-end machine translation experiments. A
novelty of our approach is that the translations
used to generate the reordering reference data
are generated in an alignment-oriented fash-
ion. We show that how the alignments are
generated can significantly effect the robust-
ness of the evaluation. We also outline some
ways in which this framework has allowed our
group to analyze reordering errors for English
to Japanese machine translation.
1 Introduction
Statistical machine translation systems can perform
poorly on distant language pairs such as English
and Japanese. Reordering errors are a major source
of poor or misleading translations in such systems
(Isozaki et al, 2010). Unfortunately the stan-
dard evaluation metrics used by the statistical ma-
chine translation community are relatively insensi-
tive to the long-distance reordering phenomena en-
countered when translating between such languages
(Birch et al, 2010).
The ability to rapidly evaluate the impact of
changes on a system can significantly accelerate the
experimental cycle. In a large statistical machine
translation system, we should ideally be able to ex-
periment with separate components without retrain-
ing the complete system. Measures such as per-
plexity have been successfully used to evaluate lan-
guage models independently in speech recognition
eliminating some of the need for end-to-end speech
recognition experiments. In machine translation,
alignment error rate has been used with some mixed
success to evaluate word-alignment algorithms but
no standard evaluation frameworks exist for other
components of a machine translation system (Fraser
and Marcu, 2007).
Unfortunately, BLEU (Papineni et al, 2001) and
other metrics that work with the final output of a ma-
chine translation system are both insensitive to re-
ordering phenomena and relatively time-consuming
to compute: changes to the system may require the
realignment of the parallel training data, extraction
of phrasal statistics and translation of a test set. As
training sets grow in size, the cost of end-to-end ex-
perimentation can become significant. However, it is
not clear that measurements made on any single part
of the system will correlate well with human judg-
ments of the translation quality of the whole system.
Following Collins et al (2005a) and Wang (2007),
Xu et al (2009) showed that when translating from
English to Japanese (and to other SOV languages
such as Korean and Turkish) applying reordering as
12
a preprocessing step that manipulates a source sen-
tence parse tree can significantly outperform state-
of-the-art phrase-based and hierarchical machine
translation systems. This result is corroborated by
Birch et al (2009) whose results suggest that both
phrase-based and hierarchical translation systems
fail to capture long-distance reordering phenomena.
In this paper we describe a lightweight framework
for measuring the quality of the reordering compo-
nents in a machine translation system. While our
framework can be applied to any translation sys-
tem in which it is possible to derive a token-level
alignment from the input source tokens to the out-
put target tokens, it is of particular practical interest
when applied to a system that performs reordering
as a preprocessing step (Xia and McCord, 2004). In
this case, as we show, it allows for extremely rapid
and sensitive analysis of changes to parser, reorder-
ing rules and other reordering components.
In our framework we evaluate the reordering pro-
posed by a system separately from its choice of tar-
get words by comparing it to a reference reordering
of the sentence generated from a manually word-
aligned translation. Unlike previous work (Isozaki
et al, 2010), our approach does not rely on the sys-
tem?s output matching the reference translation lexi-
cally. This makes the evaluation more robust as there
may be many ways to render a source phrase in the
target language and we would not wish to penalize
one that simply happens not to match the reference.
In the next section we review related work on
reordering for translation between distant language
pairs and automatic approaches to evaluating re-
ordering in machine translation. We then describe
our evaluation framework including certain impor-
tant details of how our reference reorderings were
created. We evaluate the framework by analyz-
ing how robustly it is able to predict improvements
in subjective translation quality for an English to
Japanese machine translation system. Finally, we
describe ways in which the framework has facili-
tated development of the reordering components in
our system.
2 Related Work
2.1 Evaluating Reordering
The ability to automatically evaluate machine trans-
lation output has driven progress in statistical ma-
chine translation; however, shortcomings of the
dominant metric, BLEU (Papineni et al, 2001) , par-
ticularly with respect to reordering, have long been
recognized (Callison-burch and Osborne, 2006).
Reordering has also been identified as a major fac-
tor in determining the difficulty of statistical ma-
chine translation between two languages (Birch et
al., 2008) hence BLEU scores may be most unreli-
able precisely for those language pairs for which sta-
tistical machine translation is most difficult (Isozaki
et al, 2010).
There have been many results showing that met-
rics that account for reordering are better correlated
with human judgements of translation quality (Lavie
and Denkowski, 2009; Birch and Osborne, 2010;
Isozaki et al, 2010). Examples given in Isozaki et
al. (2010) where object and subject arguments are
reversed in a Japanese to English statistical machine
translation system demonstrate how damaging re-
ordering errors can be and it should therefore not
come as a surprise that word order is a strong pre-
dictor of translation quality; however, there are other
advantages to be gained by focusing on this specific
aspect of the translation process in isolation.
One problem for all automatic evaluation metrics
is that multiple equally good translations can be con-
structed for most input sentences and typically our
reference data will contain only a small fraction of
these. Equally good translations for a sentence may
differ both in terms of lexical choice and word or-
der. One of the potential advantages of designing a
metric that looks only at word order, is that it may,
to some extent, factor out variability along the di-
mension of the lexical choice. Previous work on au-
tomatic evaluation metrics that focus on reordering,
however, has not fully exploited this.
The evaluation metrics proposed in Isozaki et al
(2010) compute a reordering score by comparing
the ordering of unigrams and bigrams that appear
in both the system?s translation and the reference.
These scores are therefore liable to overestimate
the reordering quality of sentences that were poorly
translated. While Isozaki et al (2010) does propose
13
a work-around to this problem which combines the
reordering score with a lexical precision term, this
clearly introduces a bias in the metric whereby poor
translations are evaluated primarily on their lexical
choice and good translations are evaluated more on
the basis of their word order. In our experience
word order is particularly poor in those sentences
that have the lowest lexical overlap with reference
translations; hence we would like to be able to com-
pute the quality of reordering in all sentences inde-
pendently of the quality of their lexical choice.
Birch and Osborne (2010) are closer to our ap-
proach in that they use word alignments to induce a
permutation over the source sentence. They com-
pare a source-side permutation generated from a
word alignment of the reference translation with one
generated from the system?s using various permuta-
tion distances. However, Birch and Osborne (2010)
only demonstrate that these metrics are correlated
with human judgements of translation quality when
combined with BLEU score and hence take lexical
choice into account.
Birch et al (2010) present the only results we
are aware of that compute the correlation be-
tween human judgments of translation quality and
a reordering-only metric independently of lexical
choice. Unfortunately, the experimental set-up there
is somewhat flawed. The authors ?undo? reorderings
in their reference translations by permuting the ref-
erence tokens and presenting the permuted transla-
tions to human raters. While many machine trans-
lation systems (including our own) assume that re-
ordering and translation can be factored into sepa-
rate models, e.g. (Xia and McCord, 2004), and per-
form these two operations in separate steps, the lat-
ter conditioned on the former, Birch et al (2010) are
making a much stronger assumption when they per-
form these simulations: they are assuming that lexi-
cal choice and word order are entirely independent.
It is easy to find cases where this assumption does
not hold and we would in general be very surprised
if a similar change in the reordering component in
our system did not also result in a change in the lex-
ical choice of the system; an effect which their ex-
periments are unable to model.
Another minor difference between our evaluation
framework and (Birch et al, 2010) is that we use
a reordering score that is based on the minimum
number of chunks into which the candidate and ref-
erence permutations can be concatenated similar to
the reordering component of METEOR (Lavie and
Denkowski, 2009). As we show, this is better cor-
related with human judgments of translation quality
than Kendall?s ? . This may be due to the fact that
it counts the number of ?jumps? a human reader has
to make in order to parse the system?s order if they
wish to read the tokens in the reference word order.
Kendall?s ? on the other hand penalizes every pair
of words that are in the wrong order and hence has
a quadratic (all-pairs) flavor which in turn might ex-
plain why Birch et al (2010) found that the square-
root of this quantity was a better predictor of trans-
lation quality.
2.2 Evaluation Reference Data
To create the word-aligned translations from which
we generate our reference reordering data, we used
a novel alignment-oriented translation method. The
method (described in more detail below) seeks
to generate reference reorderings that a machine
translation system might reasonably be expected to
achieve. Fox (2002) has analyzed the extent to
which translations seen in a parallel corpus can be
broken down into clean phrasal units: they found
that most sentence pairs contain examples of re-
ordering that violate phrasal cohesion, i.e. the cor-
responding words in the target language are not
completely contiguous or solely aligned to the cor-
responding source phrase. These reordering phe-
nomena are difficult for current statistical transla-
tion models to learn directly. We therefore deliber-
ately chose to create reference data that avoids these
phenomena as much as possible by having a single
annotator generate both the translation and its word
alignment. Our word-aligned translations are cre-
ated with a bias towards simple phrasal reordering.
Our analysis of the correlation between reorder-
ing scores computed on reference data created from
such alignment-oriented translations with scores
computed on references generated from standard
professional translations of the same sentences sug-
gests that the alignment-oriented translations are
more useful for evaluating a current state-of-the-art
system. We note also that while prior work has con-
jectured that automatically generated alignments are
a suitable replacement for manual alignments in the
14
context of reordering evaluation (Birch et al, 2008),
our results suggest that this is not the case at least for
the language pair we consider, English-Japanese.
3 A Lightweight Reordering Evaluation
We now present our lightweight reordering evalu-
ation framework; this consists of (1) a method for
generating reference reordering data from manual
word-alignments; and (2) a reordering metric for
scoring a sytem?s proposed reordering against this
reference data; and (3) a stand-alone evaluation tool.
3.1 Generating Reference Reordering Data
We follow Birch and Osborne (2010) in using ref-
erence reordering data that consists of permuations
of source sentences in a test set. We generate these
from word alignments of the source sentences to
reference translations. Unlike previous work, how-
ever, we have the same annotator generate both the
reference translation and the word alignment. We
also explicitly encourage the translators to generate
translations that are easy to align even if this does
result in occasionally unnatural translations. For in-
stance in English to Japanese translation we require
that all personal pronouns are translated; these are
often omitted in natural Japanese. We insist that
all but an extremely small set of words (articles and
punctuation for English to Japanese) be aligned. We
also disprefer non-contiguous alignments of a sin-
gle source word and require that all target words be
aligned to at least one source token. In Japanese
this requires deciding how to align particles that
mark syntactic roles; we choose to align these to-
gether with the content word (jiritsu-go) of the cor-
responding constituent (bunsetsu). Asking annota-
tors to translate and perform word alignment on the
same sentence in a single session does not necessar-
ily increase the annotation burden over stand-alone
word alignment since it encourages the creation of
alignment-friendly translations which can be aligned
more rapidly. Annotators need little special back-
ground or training for this task, as long as they can
speak both the source and target languages.
To generate a permutation from word alignments
we rank the source tokens by the position of the first
target token to which they are aligned. If multiple
source tokens are aligned to a single target word
or span we ignore the ordering within these source
spans; this is indicated by braces in Table 2. We
place unaligned source words immediately before
the next aligned source word or at the end of the
sentence if there is none. Table 2 shows the ref-
erence reordering derived from various translations
and word alignments.
3.2 Fuzzy Reordering Score
To evaluate the quality of a system?s reordering
against this reference data we use a simple reorder-
ing metric related to METEOR?s reordering compo-
nent (Lavie and Denkowski, 2009) . Given the refer-
ence permutation of the source sentence ?ref and the
system?s reordering of the source sentence ?sys ei-
ther generated directly by a reordering component or
inferred from the alignment between source and tar-
get phrases used in the decoder, we align each word
in ?sys to an instance of itself in ?ref taking the first
unmatched instance of the word if there is more than
one. We then define C to be the number chunks of
contiguously aligned words. If M is the number of
words in the source sentence then the fuzzy reorder-
ing score is computed as,
FRS(?sys, ?ref) = 1?
C ? 1
M ? 1
. (1)
This metric assigns a score between 0 and 1 where
1 indicates that the system?s reordering is identical
to the reference. C has an intuitive interpretation as
the number of times a reader would need to jump in
order to read the system?s reordering of the sentence
in the order proposed by the reference.
3.3 Evaluation Tool
While the framework we propose can be applied to
any machine translation system in which a reorder-
ing of the source sentence can be inferred from the
translation process, it has proven particularly use-
ful applied to a system that performs reordering as
a separate preprocessing step. Such pre-ordering
approaches (Xia and McCord, 2004; Collins et al,
2005b) can be criticized for greedily committing to
a single reordering early in the pipeline but in prac-
tice they have been shown to perform extremely well
on language pairs that require long distance reorder-
ing and have been successfully combined with other
more integrated reordering models (Xu et al, 2009).
15
The performance of a parser-based pre-ordering
component is a function of the reordering rules and
parser; it is therefore desirable that these can be eval-
uated efficiently. Both parser and reordering rules
may be evaluated using end-to-end automatic met-
rics such as BLEU score or in human evaluations.
Parsers may also be evaluated using intrinsic tree-
bank metrics such as labeled accuracy. Unfortu-
nately these metrics are either expensive to compute
or, as we show, unpredictive of improvements in hu-
man perceptions of translation quality.
Having found that the fuzzy reordering score pro-
posed here is well-correlated with changes in human
judgements of translation quality, we established a
stand-alone evaluation tool that takes a set of re-
ordering rules and a parser and computes the re-
ordering scores on a set of reference reorderings.
This has become the most frequently used method
for evaluating changes to the reordering component
in our system and has allowed teams working on
parsing, for instance, to contribute significant im-
provements quite independently.
4 Experimental Set-up
We wish to determine whether our evaluation frame-
work can predict which changes to reordering com-
ponents will result in statistically significant im-
provements in subjective translation quality of the
end-to-end system. To that end we created a num-
ber of systems that differ only in terms of reorder-
ing components (parser and/or reordering rules). We
then analyzed the corpus- and sentence-level corre-
lation of our evaluation metric with judgements of
human translation quality.
Previous work has compared either quite separate
systems, e.g. (Isozaki et al, 2010), or systems that
are artificially different from each other (Birch et al,
2010). There has also been a tendency to measure
corpus-level correlation. We are more interested in
comparing systems that differ in a realistic manner
from one another as would typically be required in
development. We also believe sentence-level cor-
relation is more important than corpus-level corre-
lation since good sentence-level correlation implies
that a metric can be used for detailed analysis of a
system and potentially to optimize it.
4.1 Systems
We carried out all our experiments using a state-of-
the-art phrase-based statistical English-to-Japanese
machine translation system (Och, 2003). Dur-
ing both training and testing, the system reorders
source-language sentences in a preprocessing step
using a set of rules written in the framework pro-
posed by (Xu et al, 2009) that reorder an English
dependency tree into target word order. During de-
coding, we set the reordering window to 4 words.
In addition to the regular distance distortion model,
we incorporate a maximum entropy based lexical-
ized phrase reordering model (Zens and Ney, 2006).
For parallel training data, we use an in-house collec-
tion of parallel documents. These come from vari-
ous sources with a substantial portion coming from
the web after using simple heuristics to identify po-
tential document pairs. We trained our system on
about 300 million source words.
The reordering rules applied to the English de-
pendency tree define a precedence order for the chil-
dren of each head category (a coarse-grained part of
speech). For example, a simplified version of the
precedence order for child labels of a verbal head
HEADVERB is: advcl, nsubj, prep, [other children],
dobj, prt, aux, neg, HEADVERB, mark, ref, compl.
The dependency parser we use is an implementa-
tion of a transition-based dependency parser (Nivre,
2008). The parser is trained using the averaged per-
ceptron algorithm with an early update strategy as
described in Zhang and Clark (2008).
We created five systems using different parsers;
here targeted self-training refers to a training pro-
cedure proposed by Katz-Brown et al (2011) that
uses our reordering metric and separate reference re-
ordering data to pick parses for self-training: an n-
best list of parses is generated for each English sen-
tence for which we have reference reordering data
and the parse tree that results in the highest fuzzy
reordering score is added to our parser?s training set.
Parsers P3, P4 and P5 differ in how that framework
is applied and how much data is used.
? P1 Penn Treebank, perceptron, greedy search
? P2 Penn Treebank, perceptron, beam search
? P3 Penn Treebank, perceptron, beam search,
targeted self-training on web data
16
? P4 Penn Treebank, perceptron, beam search,
targeted self-training on web data
? P5 Penn Treebank, perceptron, beam search,
targeted self-training on web data, case insen-
sitive
We also created five systems using the fifth parser
(P5) but with different sets of reordering rules:
? R1 No reordering
? R2 Reverse reordering
? R3 Head final reordering with reverse reorder-
ing for words before the head
? R4 Head final reordering with reverse reorder-
ing for words after the head
? R5 Superset of rules from (Xu et al, 2009)
Reverse reordering places words in the reverse of the
English order. Head final reordering moves the head
of each dependency after all its children. Rules in R3
and R4 overlap significantly with the rules for noun
and verb subtrees respectively in R5. Otherwise all
systems were identical. The rules in R5 have been
extensively hand-tuned while R1 and R2 are rather
naive. System P5R5 was our best performing system
at the time these experiments were conducted.
We refer to systems by a combination of parser
and reordering rules set identifiers, for instance, sys-
tem P2R5, uses parser P2 with reordering rules R5.
We conducted two subjective evaluations in which
bilingual human raters were asked to judge trans-
lations on a scale from 0 to 6 where 0 indicates
nonsense and 6 is perfect. The first experiment
(Parsers) contrasted systems with different parsers
and the second (Rules) varied the reordering rules.
In each case three bilingual evaluators were shown
the source sentence and the translations produced by
all five systems.
4.2 Meta-analysis
We perform a meta-analysis of the following metrics
and the framework by computing correlations with
the results of these subjective evaluations of transla-
tion quality:
1. Evaluation metrics: BLEU score on final trans-
lations, Kendall?s ? and fuzzy reordering score
on reference reordering data
2. Evaluation data: both manually-generated and
automatically-generated word alignments on
both standard professional and alignment-
oriented translations of the test sentences
The automatic word alignments were generated us-
ing IBM Model 1 in order to avoid directional biases
that higher-order models such as HMMs have.
Results presented in square parentheses are 95
percent confidence intervals estimated by bootstrap
resampling on the test corpus (Koehn, 2004).
Our test set contains 500 sentences randomly
sampled from the web. We have both professional
and alignment-friendly translations for these sen-
tences. We created reference reorderings for this
data using the method described in Section 3.1.
The lack of a broad domain and publically avail-
able Japanese test corpus makes the use of this non-
standard test set unfortunately unavoidable.
The human raters were presented with the source
sentence, the human reference translation and the
translations of the various systems simultaneously,
blind and in a random order. Each rater was allowed
to rate no more than 3 percent of the sentences and
three ratings were elicited for each sentence. Rat-
ings were a single number between 0 and 6 where 0
indicates nonsense and 6 indicates a perfectly gram-
matical translation of the source sentence.
5 Results
Table 2 shows four reference reorderings generated
from various translations and word alignments. The
automatic alignments are significantly sparser than
the manual ones but in these examples the refer-
ence reorderings still seem reasonable. Note how the
alignment-oriented translation includes a pronoun
(translation for ?I?) that is dropped in the slightly
more natural standard translation to Japanese.
Table 1 shows the human judgements of transla-
tion quality for the 10 systems (note that P5R5 ap-
pears in both experiments but was scored differently
as human judgments are affected by which other
translations are present in an experiment). There is a
clear ordering of the systems in each experiment and
17
1. Parsers Subjective Score (0-6) 2. Rules Subjective Score (0-6)
P1R5 2.173 [2.086, 2.260] P5R1 1.258 [1.191, 1.325]
P2R5 2.320 [2.233, 2.407] P5R2 1.825 [1.746, 1.905]
P3R5 2.410 [2.321, 2.499] P5R3 1.849 [1.767, 1.931]
P4R5 2.453 [2.366, 2.541] P5R4 2.205 [2.118, 2.293]
P5R5 2.501 [2.413, 2.587] P5R5 2.529 [2.441, 2.619]
Table 1: Human judgements of translation quality for 1. Parsers and 2. Rules.
Metric Sentence-level correlation
r ?
Fuzzy reordering 0.435 0.448
Kendall?s ? 0.371 0.450
BLEU 0.279 0.302
Table 6: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at
sentence-level.
we see that both the choice of parser and reordering
rules clearly effects subjective translation quality.
We performed pairwise significance tests using
bootstrap resampling for each pair of ?improved?
systems in each experiment. Tables 3, 4 and 5
shows which pairs were judged to be statistically
significant improvements at either 95 or 90 percent
level under the different metrics. These tests were
computed on the same 500 sentences. All pairs
but one are judged to be statistically significant im-
provements in subjective translation quality. Sig-
nificance tests performed using the fuzzy reorder-
ing metric are identical to the subjective scores for
the Parsers experiment but differ on one pairwise
comparison for the Rules experiment. According to
BLEU score, however, none of the parser changes
are significant at the 95 percent level and only one
pairwise comparison (between the two most differ-
ent systems) was significant at the 90 percent level.
BLEU score appears more sensitive to the larger
changes in the Rules experiment but is still in dis-
agreement with the results of the human evaluation
on four pairwise comparisons.
Table 6 shows the sentence-level correlation of
different metrics with human judgments of transla-
tion quality. Here both the fuzzy reordering score
and Kendall?s ? are computed on the reference
reordering data generated as described in Section
3.1. Both metrics are computed by running our
Translation Alignment Sentence-level
r ?
Alignment-oriented Manual 0.435 0.448
Alignment-oriented Automatic 0.234 0.252
Standard Manual 0.271 0.257
Standard Automatic 0.177 0.159
Table 7: Pearson?s correlation (r) and Spearman?s rank
correlation (?) with subjective translation quality at the
sentence-level for different types of reordering reference
data: (i) alignment-oriented translation vs. standard, (ii)
manual vs. automatic alignment.
lightweight evaluation tool and involve no transla-
tion whatsoever. These lightweight metrics are also
more correlated with subjective quality than BLEU
score at the sentence level.
Table 7 shows how the correlation between fuzzy
reordering score and subjective translation quality
degrades as we move from manual to automatic
alignments and from alignment-oriented translations
to standard ones. The automatically aligned refer-
ences, in particular, are less correlated with subjec-
tive translation scores then BLEU; we believe this
may be due to the poor quality of word alignments
for languages such as English and Japanese due to
the long-distance reordering between them.
Finally we present some intrinsic evaluation met-
rics for the parsers used in the first of our experi-
ments. Table 8 demonstrates that certain changes
may not be best captured by standard parser bench-
marks. While the first four parser models improve
on the WSJ benchmarks as they improve subjective
translation quality the best parser according to sub-
jective translation qualtiy (P5) is actually the worst
under both metrics on the treebank data. We con-
jecture that this is due to the fact that P5 (unlike the
other parsers) is case insensitive. While this helps us
significantly on our test set drawn from the web, it
18
Standard / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        A Mortgage {{ Tax Deduction }} For I Qualify How Can ?Translation                        ?? ??? ?? ? ?? ? ?? ? ?? ? ? ?? ?? ? ?? ?? ? ?Alignment          6,6,7_8,4,3,3,3,3,3,0,0,0,0,0,1,1,9,9
Alignment-oriented / ManualSource              How Can I Qualify For A Mortgage Tax Deduction ?Reordering        I How A Mortgage {{ Tax Deduction }} For Qualify Can ?Translation                         ? ? ?? ? ?? ?? ??? ? ?? ? ?? ? ??? ?? ? ?? ?? ? ?Alignment         2,2,0,0,0,6,6,6,7_8,4,3,3,3,1,1,1,1,1,9
Standard / AutomaticSource              We do not claim to cure , prevent or treat any disease .Reordering        any disease cure , prevent or treat claim to We do not .Translation           ???? ?? ? ?? ,  ?? ,            ??? ?? ? ?? ?? ?? ? ? ?? ?? ? .Alignment         10,11,,5,6,7,,8,9,,,4,,,,2,2,2,12
Alignment-oriented / AutomaticSource            We do not claim to cure , prevent or treat any disease .Reordering        We any disease cure , prevent or treat claim to do not .Translation              ? ? ? ???? ?? ? ?? ,           ?? ???? ?? ? ?? ? ?? ? ?? ? .Alignment          0,0,,10,11,,5,6,7,8,9,,,,3,4,2,2,12
Table 2: Reference reordering data generated via various methods: (i) alignment-oriented vs. standard translation, (ii)
manual vs. automatic word alignment
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 +** +** +** +**
P2R5 +** +** +** P5R2 0 +** +**
P3R5 +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 3: Pairwise significance in subjective evaluation (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 +** +** +** +** P5R1 0 +** +** +**
P2R5 +** +** +** P5R2 +** +** +**
P3R5 +** +** P5R3 +** +**
P4R5 0 P5R4 +**
Table 4: Pairwise significance in fuzzy reordering score (0 = not significant, * = 90 percent, ** = 95 percent).
Exp. 1 Parsers Exp. 2 Reordering Rules
P2R5 P3R5 P4R5 P5R5 P5R2 P5R3 P5R4 P5R5
P1R5 0 0 +* +* P5R1 +** +** +** +**
P2R5 0 0 0 P5R2 0 +** +**
P3R5 0 0 P5R3 0 +*
P4R5 0 P5R4 0
Table 5: Pairwise significance in BLEU score (0 = not significant, * = 90 percent, ** = 95 percent).
19
Parser Labeled attachment POS accuracy
P1 0.807 0.954
P2 0.822 0.954
P3 0.827 0.955
P4 0.830 0.955
P5 0.822 0.944
Table 8: Intrinsic parser metrics on WSJ dev set.
Figure 1: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
hurts parsing performance on cleaner newswire.
6 Discussion
We have found that in practice this evaluation frame-
work is sufficiently correlated with human judg-
ments of translation quality to be rather useful for
performing detailed error analysis of our English-to-
Japanese system. We have used it in the following
ways in simple error analysis sessions:
? To identify which words are most frequently re-
ordered incorrectly
? To identify systematic parser and/or POS errors
? To identify the worst reordered sentences
? To evaluate individual reordering rules
Figures 1 and 2 show pairs of parse trees together
with their resulting reorderings and scores against
Figure 2: P1 and P5?s parse trees and automatic reorder-
ing (using R5 ruleset) and fuzzy score.
the reference. These are typical of the parser er-
rors that impact reordering and which are correctly
identified by our framework. In related joint work
(Katz-Brown et al, 2011) and (Hall et al, 2011), it
is shown that the framework can be used to optimize
reordering components automatically.
7 Conclusions
We have presented a lightweight framework for eval-
uating reordering in machine translation and demon-
strated that this is able to accurately distinguish sig-
nificant changes in translation quality due to changes
in preprocessing components such as the parser or
reordering rules used by the system. The sentence-
level correlation of our metric with judgements of
human translation quality was shown to be higher
than other standard evaluation metrics while our
evaluation has the significant practical advantage of
not requiring an end-to-end machine translation ex-
periment when used to evaluate a separate reorder-
ing component. Our analysis has also highlighted
the benefits of creating focused evaluation data that
attempts to factor out some of the phenomena found
in real human translation. While previous work has
provided meta-analysis of reordering metrics across
quite independent systems, ours is we believe the
first to provide a detailed comparison of systems
20
that differ only in small but realistic aspects such as
parser quality. In future work we plan to use the
framework to provide a more comprehensive analy-
sis of the reordering capabilities of a broad range of
machine translation systems.
References
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 327?332,
Uppsala, Sweden, July.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting success in machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 745?
754, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2009. A quantitative analysis of reordering phenom-
ena. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation, pages 197?205, Athens,
Greece, March.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for mt evaluation: evaluating reorder-
ing. Machine Translation, 24:15?26, March.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249?256.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005a. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 531?540, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005b. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 304?3111, July.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Comput. Linguist., 33:293?303, September.
Keith Hall, Ryan McDonald, and Jason Katz-Brown.
2011. Training dependency parsers by jointly optimiz-
ing multiple objective functions. In Proc. of EMNLP
2011.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944?
952, Cambridge, MA, October. Association for Com-
putational Linguistics.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz
Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno,
and Hideto Kazawa. 2011. Training a Parser for Ma-
chine Translation Reordering. In Proc. of EMNLP
2011.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105?115.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL ?03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation. In ACL ?02: Proceedings
of the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
Chao Wang. 2007. Chinese syntactic reordering for
statistical machine translation. In In Proceedings of
EMNLP, pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international con-
ference on Computational Linguistics, COLING ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
245?253, Boulder, Colorado, June.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings of the Workshop on Statistical Machine
Translation, pages 55?63.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
21
