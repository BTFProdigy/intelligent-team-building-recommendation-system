Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1128?1136,
Beijing, August 2010
Efficient Statement Identification for Automatic Market Forecasting
Henning Wachsmuth
Universita?t Paderborn
Software Quality Lab
hwachsmuth@slab.upb.de
Peter Prettenhofer and Benno Stein
Bauhaus-Universita?t Weimar
Web Technology & Information Systems
benno.stein@uni-weimar.de
Abstract
Strategic business decision making in-
volves the analysis of market forecasts.
Today, the identification and aggregation
of relevant market statements is done by
human experts, often by analyzing doc-
uments from the World Wide Web. We
present an efficient information extrac-
tion chain to automate this complex nat-
ural language processing task and show
results for the identification part. Based
on time and money extraction, we iden-
tify sentences that represent statements on
revenue using support vector classifica-
tion. We provide a corpus with German
online news articles, in which more than
2,000 such sentences are annotated by do-
main experts from the industry. On the
test data, our statement identification al-
gorithm achieves an overall precision and
recall of 0.86 and 0.87 respectively.
1 Introduction
Touch screen market to hit $9B by 2015. 50 sup-
pliers provide multi-touch screens, and that num-
ber is likely to rise.1
Strategic business decision making is a highly
complex process that requires experience as well
as an overall view of economics, politics, and
technological developments. Clearly, for the time
being this process cannot be done by a computer at
the level of a human expert. However, important
tasks may be automated such as market forecast-
ing, which relies on identifying and aggregating
relevant information from the World Wide Web
(Berekoven et. al., 2001). An analyst who inter-
prets the respective data can get a reasonable idea
about the future market volume, for example. The
1Adapted from http://industry.bnet.com.
problem is that a manually conducted Web search
is time-consuming and usually far from being ex-
haustive. With our research we seek to develop
an efficient system that finds and analyzes market
forecast information with retrieval, extraction and
natural language processing (NLP) techniques.
We contribute to the following situation. For a
given product, technology, or industry sector we
identify and aggregate statements on its market
development found on relevant websites. In par-
ticular, we extract time information (?by 2015?)
and money information (?$9B?) and use support
vector classification to identify sentences that rep-
resent market statements. The statements? sub-
jects (?touch screen?) are found by relating recog-
nized named entities to the time and money infor-
mation, which we then normalize and aggregate.
In this paper we report on results for the statement
identification. To the best of our knowledge no
data for the investigation of such market analysis
tasks has been made publicly available until now.
We provide such a corpus with statements on rev-
enue annotated in news articles from the Web; the
corpus was created in close collaboration with our
industry partner Resolto Informatik GmbH.
We pursue two objectives, namely, to support
human experts with respect to the effectiveness
and completeness of their analysis, and to estab-
lish a technological basis upon which more intri-
cate analysis tasks can be automated. To summa-
rize, the main contributions of this paper are:
1. We show how to decompose the identifi-
cation and aggregation of forecasts into re-
trieval, extraction, and normalization tasks.
2. We introduce a manually annotated German
corpus for computational linguistics research
on market information.
3. We offer empirical evidence that classifica-
tion and extraction techniques can be com-
1128
bined to precisely identify statements on rev-
enue.
1.1 Related Work
Stein et. al. (2005) were among the first to con-
sider information extraction for automatic mar-
ket forecasting. Unlike us, the authors put much
emphasis on retrieval aspects and applied depen-
dency grammar parsing to identify market state-
ments. As a consequence their approach suffers
from the limitation to a small number of prede-
fined sentence structures.
While we obtain market forecasts by extract-
ing expert statements from the Web, related ap-
proaches derive them from past market behavior
and quantitative news data. Koppel and Shtrim-
berg (2004) studied the effect of news on finan-
cial markets. Lavrenko et al (2000) used time-
series analysis and language models to predict
stock market prices and, similarly, Lerman et al
(2008) proposed a system for forecasting public
opinion based on concurrent modeling of news ar-
ticles and market history. Another related field is
opinion mining in the sense that it relies on the ag-
gregation of individual statements. Glance et al
(2005) inferred marketing intelligence from opin-
ions in online discussions. Liu et al (2007) exam-
ined the effect of Weblogs on box office revenues
and combined time-series with sentiment analysis
to predict the sales performance of movies.
The mentioned approaches are intended to re-
flect or to predict present developments and,
therefore, primarily help for operative decision
making. In contrast, we aim at predicting long-
term market developments, which are essential for
strategic decision making.
2 The Problem
Market forecasts depend on two parameters, the
topic of interest and the criterion to look at. A
topic is either an organization or a market. Under
a market we unite branches, products, and tech-
nologies, because the distinction between these is
not clear in general (e.g., for semiconductors). In
contrast, we define a criterion to be a metric at-
tribute that can be measured over time. Here we
are interested in financial criteria such as revenue,
profit, and the like. The ambitious overall task that
we want to solve is as follows:
Task description: Given a topic ? and a finan-
cial criterion ?, find information for ? on the de-
velopment of ?. Aggregate the found values on ?
with respect to time.
We omit the limitation to forecasts because we
could miss useful information otherwise:
(1) In 2008, the Egyptian automobile industry
achieved US$ 9.96bn in sales.
(2) Egypt?s automotive sales will rise by 97%
from 2008 to 2013.
Both sentences have the same topic. In Particu-
lar, the 2008 amount of money from example (1)
can be aggregated with the forecast in (2) to infer
the predicted amount in 2013.
As in these examples, market information can
often only be found in running text; the major
source for this is the Web. Thus, we seek to
find web pages with sentences that represent state-
ments on a financial criterion ? and to make
these statements processable. Conceptually, such
a statement is a 5-tuple S? = (S, g, T,M, td),
where S is the topical subject, which may have a
geographic scope g, T is a period of time, M con-
sists of a growth rate and/or an amount of money
to be achieved during T with respect to ?, and td
is the statement time, i.e., the point in time when
the statement was made.
3 Approach
Our goal is to find and aggregate statements on
a criterion ? for a topic ? . In close collaboration
with two companies from the semantic technology
field, we identified eight high-level subtasks in the
overall process as explained in the following. An
overview is given in Table 1.
3.1 Find Candidate Documents
To find web pages that are likely to contain state-
ments on ? and ? , we propose to perform a meta-
search by starting from a set of characteristic
terms of the domain and then using query expan-
sion techniques such as local context analysis (Xu
and Croft, 2000). As Stein et. al. (2005) describe,
1129
Subtask Applied technologies
1 Find candidate documents meta-search, query expansion, genre analysis
2 Preprocess content content extraction, sentence splitting, tokenization, POS tagging and chunking
3 Extract entities time and money extraction, named entity recognition of organizations and markets
4 Identify statements statistical classification based on lexical and distance features
5 Determine statement type relation extraction based on dependency parse trees, matching of word lists
6 Fill statement templates template filling, anaphora resolution, matching of word lists
7 Normalize values time and money normalization, coreference resolution
8 Aggregate information chronological merging and averaging, inference from subtopic to topic
Table 1: Subtasks of the identification and aggregation of market statements for a specified topic.
Experiments in this paper cover the subtasks written in black.
a genre analysis, which classifies a document with
respect to its form, style, and targeted audience,
may be deployed afterwards to further improve
the quality of the result list efficiently. In this way,
we only maintain candidate documents that look
promising on the surface.
3.2 Preprocess Content
Preprocessing is needed for accurate access to the
document text. Our overall task incorporates re-
lating information from different document areas,
so mixing up a web page?s main frame and side-
bars should be avoided. We choose Document
Slope Curve (DSC) for content detection, which
looks for plateaus in the HTML tag distribution.
Gottron (2007) has offered evidence that DSC
is currently the best algorithm in terms of pre-
cision. Afterwards, the sentences are split with
rules that consider the specific characteristics of
reports, press releases and the like, such as head-
lines between short paragraphs. In succeeding
subtasks, tokens as well as their Part-of-Speech
and chunk tags are also used, but we see no point
in not relying on standard algorithms here.
3.3 Extract Entities
The key to identify a statement S? on a finan-
cial criterion ? is the extraction of temporal and
monetary entities. Recent works report that sta-
tistical approaches to this task can compete with
hand-crafted rules (Ahn et. al., 2005; Cramer et.
al., 2007). In the financial domain, however, the
focus is only on dates and periods as time infor-
mation, along with currency numbers, currency
terms, or fractions as money information. We
found that with regular expressions, which rep-
resent the complex but finite structures of such
phrases, we can achieve nearly perfect recall in
recognition (see Section 5).
We apply named entity recognition (NER) of
organizations and markets in this stage, too, so we
can relate statements to the appropriate subjects,
later on. Note that market names do not follow a
unique naming scheme, but we observed that they
often involve similar phrase patterns that can be
exploited as features. NER is usually done by se-
quence labeling, and we use heuristic beam search
due to our effort to design a highly efficient overall
system. Ratinov and Roth (2009) have shown for
the CoNLL-2003 shared task that Greedy decod-
ing (i.e., beam search of width 1) is competitive
to the widely used Viterbi algorithm while being
over 100 times faster at the same time.
3.4 Identify Statements
Based on time and money information, sentences
that represent a statement S? can be identified.
Such a sentence gives us valuable hints on which
temporal and monetary entity stick together and
how to interpret them in relation. Additionally,
it serves as evidence for the statement?s correct-
ness (or incorrectness). Every sentence with at
least one temporal and one monetary entity is a
candidate. Criteria such as revenue usually imply
small core vocabularies Lpos, which indicate that
a sentence is on that criterion or which often ap-
pear close to it. On the contrary, there are sets of
words Lneg that suggest a different criterion. For
a given text collection with known statements on
?, both Lpos and Lneg can be found by computing
the most discriminant terms with respect to ?. A
reasonable first approach is then to filter sentences
1130
that contain terms from Lpos and lack terms from
Lneg, but problems arise when terms from differ-
ent vocabularies co-occur or statements on differ-
ent criteria are attached to one another.
Instead, we propose a statistical learning ap-
proach. Support Vector Machines (SVMs) have
been proven to yield very good performance in
both general classification and sentence extraction
while being immune to overfitting (Steinwart and
Christmann, 2008; Hirao et. al., 2001). For our
candidates, we compute lexical and distance fea-
tures based on Lpos, Lneg, and the time and money
information. Then we let an SVM use these fea-
tures to distinguish between sentences with state-
ments on ? and others. At least for online news
articles, this works reasonably well as we demon-
strate in Section 5. Note that classification is not
used to match the right entities, but to filter the
small set of sentences on ?.
3.5 Determine Statement Type
The statement type implies what information we
can process. If a sentence contains more than one
temporal or monetary entity, we need to relate the
correct T and M to each S?, now. The type of S?
then depends on the available money information,
its trend and the time direction.
We consider four types of money information.
? refers to a period of time that results in a new
amount A of money in contrast to its preceding
amount Ap. The difference between A and Ap
may be specified as an incremental amount ?A
or as a relative growth rate r. M can span any
combination of A, Ap, ?A and r, and at least A
and r constitute a reasonable entity on their own.
Sometimes the trend of r (i.e. decreasing or in-
creasing) cannot be derived from the given val-
ues. However, this information can mostly be ob-
tained from a nearby indicator word (e.g. ?plus? or
?decreased?) and, therefore, we address this prob-
lem with appropriate word lists. Once the trend is
known, any two types imply the others.
Though we are predominantly interested in
forecasts, statements also often represent a decla-
ration on achieved results. This distinction is es-
sential and can be based on time-directional indi-
cators (e.g. ?next?) and the tense of leading verbs.
For this, we test both feature and kernel methods
on dependency parse trees, thereby determining T
and M at the same time. We only parse the iden-
tified sentences, though. Hence, we avoid running
into efficiency problems.
3.6 Fill Statement Templates
The remaining subtasks are ongoing work, so we
only present basic concepts here.
Besides T and M , the subject S and the state-
ment time td have to be determined. S may be
found within the previously extracted named enti-
ties using the dependency parse tree from Section
3.5 or by anaphora resolution. Possible limitations
to a geographic scope g can be recognized with
word lists. In market analysis, the approximate
td suffices, and for most news articles td is simi-
lar to their release date. Thus, if no date is in the
parse tree, we search the extracted temporal enti-
ties for the release date, which is often mentioned
at the beginning or end of the document?s content.
We fill one template (S, g, T,M, td) for each S?
where we have at least S, T , and M .
3.7 Normalize Values
Since we base the extraction on regular expres-
sions, we can normalize most monetary entities
with a predefined set of rules. Section 3.5 implies
that M? = (A?, r?) is a reasonable normalized
form where A? is A specified in million US-$ and
r? is r as percentage with a fixed number of deci-
mals.2 Time normalization is more complex. Any
period should be transformed to T ? = (t?s, t?e)
consisting of the start date t?s and end date t?e .
Following Ahn et. al. (2005), we consider fully
qualified, deictic and anaphoric periods. While
normalization of fully qualified periods like ?from
Apr to Jun 1999? is straightforward, deictic (e.g.
?since 2005?, ?next year?) and anaphoric men-
tions (e.g. ?in the reported time?) require a refer-
ence time. Approaches to resolve such references
rely on dates or fully qualified periods in the pre-
ceding text (Saquete et. al., 2003; Mani and Wil-
son, 2000).3
2Translating the currency requires exchange rates at state-
ment time. We need access to such information or omit the
translation if only one currency is relevant.
3References to fiscal years even involve a whole search
problem if no look-up table on such data is available.
1131
without interference
with interference
0
2
4
A
A?p
A?
Ap
t 0
2
4
t
0
2
4
A
A?Ap=A?p
t 0
2
4
t
mill.US-$ mill.US-$
mill.US-$ mill.US-$
Figure 1: Example for merging monetary values.
9
10
11
A?
A??
A
t
mill.US-$
-10%
0%
10%
t
r
Figure 2: Example for the inference of relative in-
formation from absolute values.
If we cannot normalize M or T , we discard the
corresponding statement templates. For the oth-
ers, we have to resolve synonymous co-references
(e.g. ?Loewe AG? and ?Loewe?) before we can
proceed to the last step.
3.8 Aggregate Information
We can aggregate the normalized values in either
two or three dimensions depending on whether
to separate statements with respect to td. Aggre-
gation then incorporates two challenges, namely,
how to merge values and how to infer information
on a topic from values of a subtopic.
We say that two statements on the same topic
? and criterion ? interfere if the contained peri-
ods of time intersect and the according monetary
values do not coincide. In case of declarations,
this means that we extracted incorrect values or
extracted values incorrectly. For forecasts, on the
contrary, we are exactly onto such information.
In both cases, an intuitive solution is to compute
the average (or median) and deviations. Figure 1
graphically illustrates such merging. The subtopic
challenge is based on the assumption that a mean-
ingful number of statements on a certain subtopic
of ? implies relative information on ? , as shown in
Figure 2. One of the most interesting relations are
organizations as subtopics of markets they pro-
duce for, because it is quite usual to search for
Statements Total Forecasts Declarations
Complete corpus 2075 523 (25.2%) 1552 (74.8%)
Training set 1366 306 (22.4%) 1060 (77.6%)
Validation set 362 113 (31.2%) 249 (68.8%)
Test set 347 104 (30.0%) 243 (70.0%)
Table 2: Statements on revenue in the corpus.
information on a market, but only receive state-
ments on companies. Approaches to this relation
may rely e.g. on the web page co-occurrence and
term frequencies of the markets and companies.
Altogether, we return the aggregated values
linked to the sentences in which we found them.
In this way, we make the results verifiable and,
thereby, compensate for possible inaccuracies.
4 Corpus
To evaluate the given and related tasks, we built
a manually annotated corpus with online news ar-
ticles on the revenues of organizations and mar-
kets. The compilation aims at being representa-
tive for target documents, a search engine returns
to queries on revenue. The purpose of the corpus
is to investigate both the structure of sentences on
financial criteria and the distribution of associated
information over the text.
The corpus consists of 1,128 German news ar-
ticles from the years 2003 to 2009, which were
taken from 29 news websites like www.spiegel.de
or www.capital.de. The content of each document
comes as unicode plain text with appended URL
for access to the HTML source code. Annotations
are given in a standard XMI file preformatted for
the Unstructured Information Management Archi-
tecture (Ferrucci and Lally, 2004). We created a
split, in which 2/3 of the documents constitute the
training set and each 1/6 refers to the validation
and test set. To simulate real conditions, the train-
ing documents were randomly chosen from only
the seven most represented websites, while the
validation and test data both cover all 29 sources.
Table 2 shows some corpus statistics, which give
a hint that the validation and test set differ sig-
nificantly from the training set. The corpus is
free for scientific use and can be downloaded at
http://infexba.upb.de.
1132
Loewe AG: Vorla?ufige Neun-Monats-Zahlen
Kronach, [6. November 2007]REF ? Das Ergebnis vor
Zinsen und Steuern (EBIT) des Loewe Konzerns konnte
in den ersten 9 Monaten 2007 um 41% gesteigert wer-
den. Vor diesem Hintergrund hebt die [Loewe AG]ORG
ihre EBIT-Prognose fu?r das laufende Gescha?ftsjahr auf
20 Mio. Euro an. Beim Umsatz strebt Konzernchef
[Rainer Hecker]AUTH [fu?r das Gesamtjahr]TIME ein
ho?her als urspru?nglich geplantes [Wachstum]TREND
[von 10% auf ca. 380 Mio. Euro]MONEY an. (...)
Figure 3: An annotated document in the corpus.
The text is taken from www.boerse-online.de, but
has been modified for clarification.
4.1 Annotations
In each document, every sentence that includes a
temporal entity T and a monetary entity M and
that represents a forecast or declaration on the
revenue of an organization or market is marked
as such. T and M are annotated themselves and
linked to the sentence. Accordingly, the subject
is tagged (and linked) within the sentence bound-
aries if available, otherwise its last mention in the
preceding text. The same holds for optional en-
tities, namely a reference time, a trend indicator
and the author of a statement. Altogether, 2,075
statements are tagged in this way. As in Figure
3, only information that refers to a statement on
revenue (typed in bold face) is annotated. These
annotations may be spread across the text.
The source documents were manually selected
and prepared by our industrial partners, and two
of their employees annotated the plain document
text. With respect to the statement annotations,
a preceding pilot study yielded substantial inter-
annotator agreement, as indicated by the value
? = 0.79 of the conservative measure Cohen?s
Kappa (Carletta, 1996). Additionally, we per-
formed a manual correction process for each an-
notated document to improve consistency.
5 Experiments
We now present experiments for the statement
identification, which were conducted on our cor-
pus. The goal was to evaluate whether our com-
bined extraction and classification approach suc-
ceeds in the precise identification of sentences that
comprise a statement on revenue, while keeping
recall high. Only exact matches of the annotated
text spans were considered to be correct identifi-
cations. Unlike in Section 3, we only worked on
plain text, though.
5.1 Experimental Setup
To find candidate sentences, we implemented a
sentence splitter that can handle article elements
such as subheadings, URLs, or bracketed sen-
tences. We then constructed sophisticated, but
efficient regular expressions for time and money.
They do not represent correct language, in gen-
eral, but model the structure of temporal and mon-
etary entities, and use word lists provided by do-
main experts on the lowest level.4 For feature
computation, we assumed that the closest pair of
temporal and monetary entity refers to the enclos-
ing candidate sentence.5 Since only positive in-
stances IP of statements on revenue are annotated
in our corpus, we declared all candidates, which
have no counterpart in the annotated data, to con-
stitute the negative class IN , and balanced IP and
IN by ?randomly? (seed 42) removing instances
from IN .6
For the vocabularies Lpos = {P1, P2} we first
counted the frequencies of all words in the unbal-
anced sets IP and IN . From these, we deleted
named entities, numbers and adjectives. If the pre-
fix (e.g. ?Umsatz?) of a word (?Umsatzplus?) oc-
curred, we only kept the prefix. We then filtered
all terms that appeared in at least 1.25% of the in-
stances in IP and more than 3.5 times as much in
IP as in IN . The remaining words were manually
partitioned into two lists:
P1 = {umgesetzt, Umsatz, Umsa?tze, setzte} (all
of these are terms for revenue)
P2 = {Billionen, meldet, Mitarbeiter, Verband}
(trillions, announce, employee, association)
Lneg = {N1, N2} was built accordingly. In ad-
dition, we set up a list G1 with genitive pronouns
4More details are given at http://infexba.upb.de.
555% of the candidate sentences in the training set con-
tain more than one temporal and/or monetary entity, so this
assumption may lead to errors.
6We both tested undersampling and oversampling tech-
niques but saw no effective differences in the results.
1133
and determiners. Based on Lpos, Lneg and G1,
we computed the following 43 features for every
candidate sentence s:
? 1-8: Number of terms from P1 (N1) in s as
well as in the two preceding sentences and in
the following sentence.
? 9-10: Number of terms from P2 (N2) in s.
? 11: Occurrence of term from G1 next to the
monetary entity.
? 12-19: Forward (backward) distance in to-
kens between the monetary (temporal) entity
in s and a term from P1 (N1).
? 20-27: Forward (backward) distance in num-
ber of symbols from O1 = {?.?,???,?!?} be-
tween the monetary (temporal) entity in s
and a term from P1 (N1).
? 28-43: Same as 20-27 for O2 = {?:?,?;?} and
O3 = {?,?}, respectively.
We trained a linear SVM with cost parameter
C = 0.3 (selected during validation) on these fea-
tures using the Weka integration of LibSVM (Hall
et. al., 2009; Fan et. al., 2001). Further features
were evaluated, e.g. occurrences of contraposi-
tions or comparisons, but they did not improve the
classifier. Instead, we noticed that we can avoid
some complex cases when we apply two rules af-
ter entity extraction:
R1: Delete temporal and monetary entities that
are directly surrounded by brackets.
R2: Delete temporal entities that contain the
word ?Vorjahr? (?preceding year?).
Now, we evaluated the following five statement
identification algorithms:
? Na??ve: Simply return all candidate sentences
(to estimate the relative frequency of state-
ments on revenue in the corpus).
? Baseline: Return all candidate sentences that
contain a term from the list P1.
? NEG: Use the results from Baseline. Return
all sentences that lack terms from N1.
Recall Training Validation Test
Sentences 0.98 0.98 0.96
Temporal entities 0.97 (0.95) 0.97 (0.94) 0.98 (0.96)
Monetary entities 0.96 (0.96) 0.96 (0.96) 0.95 (0.94)
Table 3: Recall of sentence and entity extraction.
In brackets: Recall after applying R1 and R2.
? RB: Filter candidates using R1 and R2. Then
apply NEG.
? SVM: Filter candidates using R1 and R2.
Then classify sentences with the SVM.
5.2 Results
Table 3 shows that we found at least 95% of the
sentences, time and money information, which re-
fer to a statement on revenue, in all datasets.7 We
could not measure precision for these since not all
sentences and entities are annotated in the corpus,
as mentioned in Section 4.
Results for the statement identification are
given in Figure 4. Generally, the test values are
somewhat lower than the validation values, but
analog in distribution. Nearly all statements were
recognized by the Na??ve algorithm, but only with
a precision of 0.35. In contrast, both for Baseline
and NEG already around 80% of the found state-
ments were correct. The latter paid a small gain in
precision with a significant loss in recall. While
RB and SVM both achieved 86% precision on the
test set, SVM tends to be a little more precise as
suggested by the validation results. In terms of re-
call, SVM clearly outperformed RB with values
of 89% and 87% and was only a little worse than
the Baseline. Altogether, the F1-Measure values
show that SVM was the best performing algorithm
in our evaluation.
5.3 Error Analysis
To assess the influence of the sentence, time and
money extraction, we compared precision and re-
call of the classifier on the manually annotated and
the extracted data, respectively. Table 4 shows
7We intentionally did not search for unusual entities like
?am 1. Handelstag nach dem Erntedankfest? (?the 1st trading
day after Thanksgiving?) in order not to develop techniques
that are tailored to individual cases. Also, money amounts
that lack a currency term were not recognized.
1134
0,75
0,80
0,85
0,90
0,95
1,00
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
Recall
.89
.87
.83.83
0,3
0,4
0,5
0,6
0,7
0,8
0,9
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
Precision
.89 .90
.86 .86
0,5
0,6
0,7
0,8
0,9
TestValidation
N
a?
ve
N
a?
ve
B
as
el
in
e
B
as
el
in
e
N
E
G
N
E
G
R
B
R
BSV
M
SV
M
F1-Measure
.89
.86
.84
.86
.79 .77
.92
.89
.85
.83
Figure 4: Precision, recall and F1-Measure of the five evaluated statement identification algorithms.
SVM is best in precision both on validation and test data and outperforms RB in recall significantly.
that only recall differs significantly. We found that
false statement identifications referred to the fol-
lowing noteworthy error cases.
False match: Most false positives result from
matchings of temporal and monetary entities that
actually do not refer to the same statement.
Missing criterion: Some texts describe the de-
velopment of revenue without ever mentioning
revenue. Surrogate words like ?market? may be
used, but they are not discriminative enough.
Multiple criteria: Though we aimed at dis-
carding sentences, in which revenue is mentioned
without comprising a statement on it, in some
cases our features did not work out, mainly due
to intricate sentence structure.
Traps: Some sentences contain numeric values
on revenue, but not the ones looked for, as in ?10%
of the revenue?. We tackled these cases, but had
still some false classifications left.
Hidden boundaries: Finally, we did not find
all correct sentence boundaries, which can lead to
both false positives and false negatives. The pre-
dominant problem was to separate headlines from
paragraph beginnings and is partly caused by the
missing access to markup tags.
5.4 Efficiency
We ran the identification algorithm on the whole
corpus using a 2 GHz Intel Core 2 Duo MacBook
with 4 GB RAM. The 1,128 corpus documents
contain 33,370 sentences as counted by our algo-
rithm itself. Tokenization, sentence splitting, time
and money extraction took only 55.2 seconds, i.e.,
more than 20 documents or 600 sentences each
second. Since our feature computation is not op-
timized yet, the complete identification process is
a little less efficient with 7.35 documents or 218
Candidates Data Precision Recall
Annotated validation data 0.91 0.94
test data 0.87 0.93
Extracted validation data 0.90 0.89
test data 0.86 0.87
Table 4: Precision and recall of the statement
identification on manually annotated data and on
automatically extracted data, respectively.
sentences per second. However, it is fast enough
to be used in online applications, which was our
goal in the end.
6 Conclusion
We presented a multi-stage approach for the au-
tomatic identification and aggregation of market
statements and introduced a manually annotated
German corpus for related tasks. The approach
has been influenced by industry and is oriented
towards practical applications, but is, in general,
not specific to the German language. It relies on
efficient retrieval, extraction and NLP techniques.
By now, we can precisely identify most sentences
that represent statements on revenue. This already
allows for the support of strategists, e.g. by high-
lighting such sentences in web pages, which we
currently implement as a Firefox extension. The
overall problem is complex, though, and we are
aware that human experts can do better at present.
Nevertheless, time-consuming tasks can be auto-
mated and, in this respect, the results on our cor-
pus are very promising.
Acknowledgement: This work was funded by
the project ?InfexBA? of the German Federal Min-
istry of Education and Research (BMBF) under
contract number 01IS08007A.
1135
References
Ahn, David, Sisay F. Adafre, and Maarten de Rijke.
2005. Extracting Temporal Information from Open
Domain Text: A Comparative Exploration. Journal
of Digital Information Management, 3(1): 14?20.
Berekoven, Ludwig, Werner Eckert, and Peter El-
lenrieder. 2001. Marktforschung: Methodische
Grundlagen und praktische Anwendung, 9th Edi-
tion, Gabler, Wiesbaden, Germany.
Carletta, Jean. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22: 249?254.
Cramer, Irene M., Stefan Schacht, and Andreas
Merkel. 2007. Classifying Number Expressions in
German Corpora. In Proceedings of the 31st An-
nual Conference of the German Classification Soci-
ety on Data Analysis, Machine Learning, and Appli-
cations, pages 553?560.
Fan, Rong-En, Pai-Hsuen Chen, and Chih-Jen Lin.
2001. Working Set Selection Using Second Order
Information for Training Support Vector Machines.
Journal of Machine Learning Research, 6: 1889?
1918.
Ferrucci, David and Adam Lally. 2004. UIMA:
An Architectural Approach to Unstructured Infor-
mation Processing in the Corporate Research Envi-
ronment. Natural Language Engineering, 10(3?4):
pages 327?348.
Glance, Natalie, Matthew Hurst, Kamal Nigam,
Matthew Siegler, Robert Stockton, and Takashi
Tomokiyo. 2005. Deriving Marketing Intelligence
from Online Discussion. In Proceedings of the
Eleventh International Conference on Knowledge
Discovery in Data Mining, pages 419?428.
Gottron, Thomas. 2007. Evaluating Content Extrac-
tion on HTML Documents. In Proceedings of the
2nd International Conference on Internet Technolo-
gies and Applications, pages 123?132.
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1).
Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and
Yuji Matsumoto. 2002. Extracting Important Sen-
tences with Support Vector Machines. In Proceed-
ings of the 19th International Conference on Com-
putational linguistics, pages 342?348.
Koppel, Moshe and Itai Shtrimberg. 2004. Good
News or Bad News? Let the Market Decide. In Pro-
ceedings of the AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Appli-
cations, pages 86?88.
Lavrenko, Victor, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of Concurrent Text and Time Series. In
Proceedings of the 6th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining Workshop on Text Mining, pages 37?
44.
Lerman, Kevin, Ari Gilder, Mark Dredze, and Fer-
nando Pereira. 2008. Reading the Markets: Fore-
casting Public Opinion of Political Candidates by
News Analysis. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 473?480.
Liu, Yang, Xiangji Huang, Aijun An, and Xiaohui Yu.
2007. Arsa: A Sentiment-Aware Model for Predict-
ing Sales Performance Using Blogs. In Proceedings
of the 30th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 607?614.
Mani, Inderjeet and George Wilson. 2000. Ro-
bust Temporal Processing of News. In Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 69?76.
Ratinov, Lev and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 147?155.
Saquete, Estela, Rafael Mun?oz, and Patricio Mart??nez-
Barco. 2003. TERSEO: Temporal Expression Res-
olution System Applied to Event Ordering. Text,
Speech and Dialogue, Springer, Berlin / Heidelberg,
Germany, pages 220?228.
Stein, Benno, Sven Meyer zu Eissen, Gernot Gra?fe,
and Frank Wissbrock. 2005. Automating Market
Forecast Summarization from Internet Data. Fourth
International Conference on WWW/Internet, pages
395?402.
Steinwart, Ingo and Andreas Christmann. 2008. Sup-
port Vector Machines, Springer, New York, NY.
Xu, Jinxi and Bruce W. Croft 2000. Improving the ef-
fectiveness of information retrieval with local con-
text analysis. ACM Transactions on Information
Systems, 18(1): 79-112.
1136
Coling 2010: Poster Volume, pages 997?1005,
Beijing, August 2010
An Evaluation Framework for Plagiarism Detection
Martin Potthast Benno Stein
Web Technology & Information Systems
Bauhaus-Universit?t Weimar
{martin.potthast, benno.stein}@uni-weimar.de
Alberto Barr?n-Cede?o Paolo Rosso
Natural Language Engineering Lab?ELiRF
Universidad Polit?cnica de Valencia
{lbarron, prosso}@dsic.upv.es
Abstract
We present an evaluation framework for
plagiarism detection.1 The framework
provides performance measures that ad-
dress the specifics of plagiarism detec-
tion, and the PAN-PC-10 corpus, which
contains 64 558 artificial and 4 000 sim-
ulated plagiarism cases, the latter gener-
ated via Amazon?s Mechanical Turk. We
discuss the construction principles behind
the measures and the corpus, and we com-
pare the quality of our corpus to exist-
ing corpora. Our analysis gives empirical
evidence that the construction of tailored
training corpora for plagiarism detection
can be automated, and hence be done on a
large scale.
1 Introduction
The lack of an evaluation framework is a seri-
ous problem for every empirical research field.
In the case of plagiarism detection this short-
coming has recently been addressed for the first
time in the context of our benchmarking work-
shop PAN [15, 16]. This paper presents the eval-
uation framework developed in the course of the
workshop. But before going into details, we sur-
vey the state of the art in evaluating plagiarism de-
tection, which has not been studied systematically
until now.
1.1 A Survey of Evaluation Methods
We have queried academic databases and search
engines to get an overview of all kinds of con-
tributions to automatic plagiarism detection. Al-
together 275 papers were retrieved, from which
139 deal with plagiarism detection in text,
1The framework is available free of charge at
http://www.webis.de/research/corpora.
Table 1: Summary of the plagiarism detection
evaluations in 205 papers, from which 104 deal
with text and 101 deal with code.
Evaluation Aspect Text Code
Experiment Task
local collection 80% 95%
Web retrieval 15% 0%
other 5% 5%
Performance Measure
precision, recall 43% 18%
manual, similarity 35% 69%
runtime only 15% 1%
other 7% 12%
Comparison
none 46% 51%
parameter settings 19% 9%
other algorithms 35% 40%
Evaluation Aspect Text Code
Corpus Acquisition
existing corpus 20% 18%
homemade corpus 80% 82%
Corpus Size [# documents]
[1, 10) 11% 10%
[10, 102) 19% 30%
[102, 103) 38% 33%
[103, 104) 8% 11%
[104, 105) 16% 4%
[105, 106) 8% 0%
123 deal with plagiarism detection in code, and
13 deal with other media types. From the pa-
pers related to text and code we analyzed the
205 which present evaluations. Our analysis
covers the following aspects: experiment tasks,
performance measures, underlying corpora, and,
whether comparisons to other plagiarism detec-
tion approaches were conducted. Table 1 summa-
rizes our findings.
With respect to the experiment tasks the ma-
jority of the approaches perform overlap detec-
tion by exhaustive comparison against some lo-
cally stored document collection?albeit a Web
retrieval scenario is more realistic. We explain
this shortcoming by the facts that the Web can-
not be utilized easily as a corpus, and, that in the
case of code plagiarism the focus is on collusion
detection in student courseworks. With respect to
performance measures the picture is less clear: a
manual result evaluation based on similarity mea-
sures is used about the same number of times for
text (35%), and even more often for code (69%),
as an automatic computation of precision and re-
call. 21% and 13% of the evaluations on text and
code use custom measures or examine only the de-
997
tection runtime. This indicates that precision and
recall may not be well-defined in the context of
plagiarism detection. Moreover, comparisons to
existing research are conducted in less than half
of the papers, a fact that underlines the lack of an
evaluation framework.
The right-hand side of Table 1 overviews two
corpus-related aspects: the use of existing cor-
pora versus the use of handmade corpora, and the
size distribution of the used corpora. In particu-
lar, we found that researchers follow two strate-
gies to compile a corpus. Small corpora (<1 000
documents) are built from student courseworks or
from arbitrary documents into which plagiarism-
alike overlap is manually inserted. Large corpora
(>1 000 documents) are collected from sources
where overlap occurs more frequently, such as
rewritten versions of news wire articles, or from
consecutive versions of open source software. Al-
together, we see a need for an open, commonly
used plagiarism detection corpus.
1.2 Related Work
There are a few surveys about automatic plagia-
rism detection in text [7, 8, 14] and in code [12,
17, 19, 20]. These papers, as well as nearly all
papers of our survey, omit a discussion of evalua-
tion methodologies; the following 4 papers are an
exception.
In [21] the authors introduce graph-based per-
formance measures for code plagiarism detection
that are intended for unsupervised evaluations.
We argue that evaluations in this field should be
done in a supervised manner. An aside: the pro-
posed measures have not been adopted since their
first publication. In [15] we introduce preliminary
parts of our framework. However, the focus of
that paper is less on methodology but on the com-
parison of the detection approaches that were sub-
mitted to the first PAN benchmarking workshop.
In [9, 10] the authors report on an unnamed cor-
pus that comprises 57 cases of simulated plagia-
rism. We refer to this corpus as the Clough09 cor-
pus; a comparison to our approach is given later
on. Finally, a kind of related corpus is the ME-
TER corpus, which has been the only alternative
for the text domain up to now [11]. It comprises
445 cases of text reuse among 1 716 news articles.
Although the corpus can be used to evaluate pla-
giarism detection its design does not support this
task. This is maybe the reason why it has not been
used more often. Furthermore, it is an open ques-
tion whether or not cases of news reuse differ from
plagiarism cases where the plagiarists strive to re-
main undetected.
1.3 Contributions
Besides the above survey, the contributions of our
paper are threefold: Section 2 presents formal
foundations for the evaluation of plagiarism detec-
tion and introduces three performance measures.
Section 3 introduces methods to create artificial
and simulated plagiarism cases on a large scale,
and the PAN-PC-10 corpus in which these meth-
ods have been operationalized. Section 4 then
compares our corpus with the Clough09 corpus
and the METER corpus. The comparison reveals
important insights for the different kinds of text
reuse in these corpora.
2 Plagiarism Detection Performance
This section introduces measures to quantify the
precision and recall performance of a plagiarism
detection algorithm; we present a micro-averaged
and a macro-averaged variant. Moreover, the so-
called detection granularity is introduced, which
quantifies whether the contiguity between plagia-
rized text passages is properly recognized. This
concept is important: a low granularity simpli-
fies both the human inspection of algorithmically
detected passages as well as an algorithmic style
analysis within a potential post-process. The three
measures can be applied in isolation but also
be combined into a single, overall performance
score. A reference implementation of the perfor-
mance measures is distributed with our corpus.
2.1 Precision, Recall, and Granularity
Let dplg denote a document that contains pla-
giarism. A plagiarism case in dplg is a 4-tuple
s = ?splg, dplg, ssrc, dsrc?, where splg is a plagia-
rized passage in dplg, and ssrc is its original coun-
terpart in some source document dsrc. Likewise,
a plagiarism detection for document dplg is de-
noted as r = ?rplg, dplg, rsrc, d?src?; r associates
an allegedly plagiarized passage rplg in dplg with
998
a passage rsrc in d?src. We say that r detects s iff
rplg ? splg = ?, rsrc ? ssrc = ?, and d?src = dsrc.
With regard to a plagiarized document dplg it is as-
sumed that different plagiarized passages of dplg
do not intersect; with regard to detections for dplg
no such restriction applies. Finally, S and R de-
note sets of plagiarism cases and detections.
While the above 4-tuples resemble an intu-
itive view of plagiarism detection we resort to
an equivalent, more concise view to simplify the
subsequent notations: a document d is repre-
sented as a set of references to its characters d =
{(1, d), . . . , (|d|, d)}, where (i, d) refers to the
i-th character in d. A plagiarism case s can then be
represented as s = splg ? ssrc, where splg ? dplg
and ssrc ? dsrc. The characters referred to in splg
and ssrc form the passages splg and ssrc. Likewise,
a detection r can be represented as r = rplg?rsrc.
It follows that r detects s iff rplg ? splg = ? and
rsrc?ssrc = ?. Based on these representations, the
micro-averaged precision and recall of R under S
are defined as follows:
precmicro(S,R) =
|?(s,r)?(S?R)(s 	 r)|
|?r?R r|
, (1)
recmicro(S,R) =
|?(s,r)?(S?R)(s 	 r)|
|?s?S s|
, (2)
where s 	 r =
{
s ? r if r detects s,
? otherwise.
The macro-averaged precision and recall are
unaffected by the length of a plagiarism case; they
are defined as follows:
precmacro(S,R) =
1
|R|
?
r?R
|?s?S(s 	 r)|
|r| , (3)
recmacro(S,R) =
1
|S|
?
s?S
|?r?R(s 	 r)|
|s| , (4)
Besides precision and recall there is another
concept that characterizes the power of a detec-
tion algorithm, namely, whether a plagiarism case
s ? S is detected as a whole or in several pieces.
The latter can be observed in today?s commercial
plagiarism detectors, and the user is left to com-
bine these pieces to a consistent approximation
of s. Ideally, an algorithm should report detec-
tions R in a one-to-one manner to the true cases S.
To capture this characteristic we define the detec-
tion granularity of R under S:
gran(S,R) = 1|SR|
?
s?SR
|Rs|, (5)
where SR ? S are cases detected by detections
in R, and Rs ? R are the detections of a given s:
SR = {s | s ? S ? ?r ? R : r detects s},
Rs = {r | r ? R ? r detects s}.
The domain of gran(S,R) is [1, |R|], with 1
indicating the desired one-to-one correspondence
and |R| indicating the worst case, where a single
s ? S is detected over and over again.
Precision, recall, and granularity allow for a
partial ordering among plagiarism detection algo-
rithms. To obtain an absolute order they must be
combined to an overall score:
plagdet(S,R) = F?log2(1 + gran(S,R))
, (6)
where F? denotes the F?-Measure, i.e., the
weighted harmonic mean of precision and recall.
We suggest using ? = 1 (precision and recall
equally weighted) since there is currently no indi-
cation that either of the two is more important. We
take the logarithm of the granularity to decrease
its impact on the overall score.
2.2 Discussion
Plagiarism detection is both a retrieval task and
an extraction task. In light of this fact not only
retrieval performance but also extraction accuracy
becomes important, the latter of which being ne-
glected in the literature. Our measures incorpo-
rate both. Another design objective of our mea-
sures is the minimization of restrictions imposed
on plagiarism detectors. The overlap restriction
for plagiarism cases within a document assumes
that a certain plagiarized passage is unlikely to
have more than one source. Imprecision or lack
of evidence, however, may cause humans or algo-
rithms to report overlapping detections, e.g., when
being unsure about the true source of a plagia-
rized passage. The measures (1)-(4) provide for a
sensible treatment of this fact since the set-based
999
passage representations eliminate duplicate detec-
tions of characters. The macro-averaged vari-
ants allot equal weight to each plagiarism case,
regardless of its length. Conversely, the micro-
averaged variants favor the detection of long pla-
giarism passages, which are generally easier to be
detected. Which of both is to be preferred, how-
ever, is still an open question.
3 Plagiarism Corpus Construction
This section organizes and analyzes the practices
that are employed?most of the time implicitly?
for the construction of plagiarism corpora. We
introduce three levels of plagiarism authentic-
ity, namely, real plagiarism, simulated plagiarism,
and artificial plagiarism. It turns out that simu-
lated plagiarism and artificial plagiarism are the
only viable alternatives for corpus construction.
We propose a new approach to scale up the gen-
eration of simulated plagiarism based on crowd-
sourcing, and heuristics to generate artificial pla-
giarism. Moreover, based on these methods, we
compile the PAN plagiarism corpus 2010 (PAN-
PC-10) which is the first corpus of its kind that
contains both a large number and a high diversity
of artificial and simulated plagiarism cases.
3.1 Real, Simulated, and Artificial Plagiarism
Syntactically, a plagiarism case is the result of
copying a passage ssrc from a source document
into another document dplg. Since verbatim
copies can be detected easily, plagiarists often
rewrite ssrc to obfuscate their illegitimate act.
This behavior must be modeled when constructing
a training corpus for plagiarism detection, which
can be done at three levels of authenticity. Ide-
ally, one would secretly observe a large number
of plagiarists and use their real plagiarism cases;
at least, one could resort to plagiarism cases which
have been detected in the past. The following as-
pects object against this approach:
? The distribution of detected real plagiarism
is skewed towards ease of detectability.
? The acquisition of real plagiarism is expen-
sive since it is often concealed.
? Publishing real cases requires the consents
from the plagiarist and the original author.
? A public corpus with real cases is question-
able from an ethical and legal viewpoint.
? The anonymization of real plagiarism is dif-
ficult due to Web search engines and author-
ship attribution technology.
It is hence more practical to let people create
plagiarism cases by ?purposeful? modifications,
or to tap resources that contain similar kinds of
text reuse. We subsume these strategies under the
term simulated plagiarism. The first strategy has
often been applied in the past, though on a small
scale and without a public release of the corpora;
the second strategy comes in the form of the ME-
TER corpus [11]. Note that, from a psycholog-
ical viewpoint, people who simulate plagiarism
act under a different mental attitude than plagia-
rists. From a linguistic viewpoint, however, it is
unclear whether real plagiarism differs from sim-
ulated plagiarism.
A third possibility is to generate plagiarism al-
gorithmically [6, 15, 18], which we call artificial
plagiarism. Generating artificial plagiarism cases
is a non-trivial task if one requires semantic equiv-
alence between a source passage ssrc and the pas-
sage splg that is obtained by an automatic obfus-
cation of ssrc. Such semantics-preserving algo-
rithms are still in their infancy; however, the sim-
ilarity computation between texts is usually done
on the basis of document models like the bag of
words model and not on the basis of the original
text, which makes obfuscation amenable to sim-
pler approaches.
3.2 Creating Simulated Plagiarism
Our approach to scale up the creation of simu-
lated plagiarism is based on Amazon?s Mechani-
cal Turk, AMT, a commercial crowdsourcing ser-
vice [3]. This service has gathered considerable
interest, among others to recreate TREC assess-
ments [1], but also to write and translate texts [2].
We offered the following task on the Mechani-
cal Turk platform: Rewrite the original text found
below [on the task Web page] so that the rewritten
version has the same meaning as the original, but
with a different wording and phrasing. Imagine a
scholar copying a friend?s homework just before
class, or imagine a plagiarist willing to use the
1000
Table 2: Summary of 4 000 Mechanical Turk tasks
completed by 907 workers.
Worker Demographics
Age Education
18, 19 10% HS 11%
20?29 37% College 30%
30?39 16% BSc. 17%
40?49 7% MSc. 11%
50?59 4% Dr. 2%
60?69 1%
n/a 25% n/a 29%
Native Speaker Gender
yes 62% male 37%
no 14% female 39%
n/a 23% n/a 24%
Prof. Writer Plagiarized
yes 10% yes 16%
no 66% no 60%
n/a 24% n/a 25%
Task Statistics
Tasks per Worker
average 15
std. deviation 20
minimum 1
maximum 103
Work Time (minutes)
average 14
std. deviation 21
minimum 1
maximum 180
Compensation
pay per task 0.5 US$
rejected results 25%
original text without proper citation.
Workers were required to be fluent in English
reading and writing, and they were informed that
every result was to be reviewed. A questionnaire
displayed alongside the task description asked
about the worker?s age, education, gender, and na-
tive speaking ability. Further we asked whether
the worker is a professional writer, and whether
he or she has ever plagiarized. Completing the
questionnaire was optional in order to minimize
false answers, but still, these numbers have to
be taken with a grain of salt: the Mechanical
Turk is not the best environment for such sur-
veys. Table 2 overviews the worker demographics
and task statistics. The average worker appears
to be a well-educated male or female in the twen-
ties, whose mother tongue is English. 16% of the
workers claim to have plagiarized at least once,
and if at least the order of magnitude of the lat-
ter number can be taken seriously this shows that
plagiarism is a prevalent problem.
A number of pilot experiments were conducted
to determine the pay per task, depending on the
text length and the task completion time: for
50 US-cents about 500 words get rewritten in
about half an hour. We observed that decreasing
or increasing the pay per task has proportional ef-
fect on the task completion time, but not on the
result quality. This observation is in concordance
with earlier research [13]. Table 3 contrasts a
source passage ssrc and its rewritten, plagiarized
passage splg obtained via the Mechanical Turk.
3.3 Creating Artificial Plagiarism
To create artificial plagiarism, we propose three
obfuscation strategies. Given a source passage
ssrc a plagiarized passage splg can be created as
follows (see Table 4):
? Random Text Operations. splg is created
from ssrc by shuffling, removing, inserting,
or replacing words or short phrases at ran-
dom. Insertions and replacements are taken
from the document dplg where splg is to be
inserted.
? Semantic Word Variation. splg is created
from ssrc by replacing words by one of their
synonyms, antonyms, hyponyms, or hyper-
nyms, chosen at random. A word is kept if
none of them is available.
Table 3: Example of a simulated plagiarism case s, generated with Mechanical Turk.
Source Passage ssrc Plagiarized Passage splg
The emigrants who sailed with Gilbert were better fitted for a
crusade than a colony, and, disappointed at not at once find-
ing mines of gold and silver, many deserted; and soon there
were not enough sailors to man all the four ships. Accord-
ingly, the Swallow was sent back to England with the sick;
and with the remainder of the fleet, well supplied at St. John?s
with fish and other necessaries, Gilbert (August 20) sailed
south as far as forty-four degrees north latitude. Off Sable
Island a storm assailed them, and the largest of the ves-
sels, called the Delight, carrying most of the provisions, was
driven on a rock and went to pieces.
[Excerpt from ?Abraham Lincoln: A History? by John Nicolay and John Hay.]
The people who left their countries and sailed with Gilbert
were more suited for fighting the crusades than for leading a
settled life in the colonies. They were bitterly disappointed as
it was not the America that they had expected. Since they did
not immediately find gold and silver mines, many deserted.
At one stage, there were not even enough man to help sail
the four ships. So the Swallow was sent back to England
carrying the sick. The other fleet was supplied with fish and
the other necessities from St. John. On August 20, Gilbert
had sailed as far as forty-four degrees to the north latitude.
His ship known as the Delight, which bore all the required
supplies, was attacked by a violent storm near Sable Island.
The storm had driven it into a rock shattering it into pieces.
1001
Table 4: Examples of the obfuscation strategies.
Obfuscation Examples
Original Text
The quick brown fox jumps over the lazy dog.
Manual Obfuscation (by a human)
Over the dog which is lazy jumps quickly the fox which is brown.
Dogs are lazy which is why brown foxes quickly jump over them.
A fast auburn vulpine hops over an idle canine.
Random Text Operations
over The. the quick lazy dog <context word> jumps brown fox
over jumps quick brown fox The lazy. the
brown jumps the. quick dog The lazy fox over
Semantic Word Variation
The quick brown dodger leaps over the lazy canine.
The quick brown canine jumps over the lazy canine.
The quick brown vixen leaps over the lazy puppy.
POS-preserving Word Shuffling
The brown lazy fox jumps over the quick dog.
The lazy quick dog jumps over the brown fox.
The brown lazy dog jumps over the quick fox.
? POS-preserving Word Shuffling. The se-
quence of parts of speech in ssrc is deter-
mined and splg is created by shuffling words
at random while retaining the original POS
sequence.
To generate different degrees of obfuscation the
strategies can be adjusted by varying the number
of operations made on ssrc, and by limiting the
range of affected phrases within ssrc. For our cor-
pus, the strategies were combined and adjusted to
match an intuitive understanding of a ?low? and
a ?high? obfuscation. Of course other obfusca-
tion strategies are conceivable, e.g., based on au-
tomatic paraphrasing methods [4], but for perfor-
mance reasons simple strategies are preferred at
the expense of readability of the obfuscated text.
3.4 Overview of the PAN-PC-10
To compile the PAN plagiarism corpus 2010, sev-
eral other parameters besides the above plagiarism
obfuscation methods have been varied. Table 5
gives an overview.
The documents used in the corpus are derived
from books from the Project Gutenberg.2 Every
document in the corpus serves one of two pur-
poses: it is either used as a source for plagiarism
or as a document suspicious of plagiarism. The
latter documents divide into documents that actu-
ally contain plagiarism and documents that don?t.
2http://www.gutenberg.org
Table 5: Corpus statistics of the PAN-PC-10 for
its 27 073 documents and 68 558 plagiarism cases.
Document Statistics
Document Purpose
source documents 50%
suspicious documents
? with plagiarism 25%
? w/o plagiarism 25%
Intended Algorithms
external detection 70%
intrinsic detection 30%
Plagiarism per Document
hardly (5%-20%) 45%
medium (20%-50%) 15%
much (50%-80%) 25%
entirely (>80%) 15%
Document Length
short (1-10 pp.) 50%
medium (10-100 pp.) 35%
long (100-1000 pp.) 15%
Plagiarism Case Statistics
Topic Match
intra-topic cases 50%
inter-topic cases 50%
Obfuscation
none 40%
artificial
? low obfuscation 20%
? high obfuscation 20%
simulated (AMT) 6%
translated ({de,es} to en) 14%
Case Length
short (50-150 words) 34%
medium (300-500 words) 33%
long (3000-5000 words) 33%
The documents without plagiarism allow to deter-
mine whether or not a detector can distinguish pla-
giarism cases from overlaps that occur naturally
between random documents.
The corpus is split into two parts, correspond-
ing to the two paradigms of plagiarism detection,
namely external plagiarism detection and intrinsic
plagiarism detection. Note that in the case of in-
trinsic plagiarism detection the source documents
used to generate the plagiarism cases are omitted:
intrinsic detection algorithms are expected to de-
tect plagiarism in a suspicious document by an-
alyzing the document in isolation. Moreover, the
intrinsic plagiarism cases are not obfuscated in or-
der to preserve the writing style of the original au-
thor; the 40% of unobfuscated plagiarism cases in
the corpus include the 30% of the cases belonging
to the intrinsic part.
The fraction of plagiarism per document, the
lengths of the documents and plagiarism cases,
and the degree of obfuscation per case deter-
mine the difficulty of the cases: the corpus con-
tains short documents with a short, unobfuscated
plagiarism case, resulting in a 5% fraction of
plagiarism, but it also contains large documents
with several obfuscated plagiarism cases of vary-
ing lengths, drawn from different source docu-
ments and resulting in fractions of plagiarism up
to 100%. Since the true distributions of these pa-
rameters in real plagiarism are unknown, sensible
1002
estimations were made for the corpus. E.g., there
are more simple plagiarism cases than complex
ones, where ?simple? refers to short cases, hardly
plagiarism per document, and less obfuscation.
Finally, plagiarism cases were generated be-
tween topically related documents and between
unrelated documents. To this end, the source doc-
uments and the suspicious documents were clus-
tered into k = 30 clusters using bisecting k-
means [22]. Then an equal share of plagiarism
cases were generated for pairs of source docu-
ments and suspicious documents within as well
as between clusters. Presuming the clusters cor-
respond to (broad) topics, we thus obtained intra-
topic plagiarism and inter-topic plagiarism.
4 Corpus Validation
This section reports on validation results about
the ?quality? of the plagiarism cases created for
our corpus. We compare both artificial plagia-
rism cases and simulated plagiarism cases to cases
of the two corpora Clough09 and METER. Pre-
suming that the authors of these corpora put their
best efforts into case construction and annotation,
the comparison gives insights whether our scale-
up strategies are reasonable in terms of case qual-
ity. To foreclose the results, we observe that sim-
ulated plagiarism and, in particular, artificial pla-
giarism behave similar to the two handmade cor-
pora. In the light of the employed strategies to
construct plagiarism this result may or may not
be surprising?however, we argue that it is neces-
sary to run such a comparison in order to provide
a broadly accepted evaluation framework in this
sensitive area.
The experimental setup is as follows: given a
plagiarism case s = ?splg, dplg, ssrc, dsrc?, the pla-
giarized passage splg is compared to the source
passage ssrc using 10 different retrieval models.
Each model is an n-gram vector space model
(VSM) where n ranges from 1 to 10 words,
employing stemming, stop word removal, tf -
weighting, and the cosine similarity. Similarity
values are computed for all cases found in each
corpus, but since the corpora are of different sizes,
100 similarities are sampled from each corpus to
ensure comparability.
The rationale of this setup is as follows: a well-
known fact from near-duplicate detection is that
if two documents share only a few 8-grams?so-
called shingles?it is highly probable that they are
duplicates [5]. Another well-known fact is that
two documents which are longer than a few sen-
tences and which are exactly about the same topic
will, with a high probability, share a considerable
portion of their vocabulary. I.e., they have a high
1
0.8
0.6
0.4
0.2
0
n = 1 2 3 4 5 6 7 8 9 10
Si
m
ila
rit
y
n-gram VSM
Clough09
Artificial
Median
25% Quartile
75% Quartile
Simulated (AMT)
METER
Left to right:
Figure 1: Comparison of four corpora of text reuse and plagiarism: each box plot shows the middle
range of the measured similarities when comparing source passages to their rewritten versions. Basis is
an n-gram VSM, where n ? {1, 2, . . . , 10} words.
1003
similarity under a 1-gram VSM. It follows for pla-
giarism detection that a common shingle between
splg and ssrc pinpoints very accurately an unob-
fuscated portion of splg, while it is inevitable that
even a highly obfuscated splg will share a portion
of its vocabulary with ssrc. The same holds for all
other kinds of text reuse.
Figure 1 shows the obtained similarities, con-
trasting each n-gram VSM and each corpus. The
box plots show the middle 50% of the respective
similarity distributions as well as median similar-
ities. The corpora divide into groups with compa-
rable behavior: in terms of the similarity ranges
covered, the artificial plagiarism compares to the
METER corpus, except for n ? {2, 3}, while the
simulated plagiarism from the Clough09 corpus
behaves like that from our corpus, but with a dif-
ferent amplitude. In terms of median similarity,
METER, Clough09, and our simulated plagiarism
behave almost identical, while the artificial plagia-
rism differs. Also note that our simulated plagia-
rism as well as the Clough09 corpus contain some
cases which are hardly obfuscated.
We interpret these results as follows: (1) Dif-
ferent kinds of plagiarism and text reuse do not
differ very much under n-gram models. (2) Ar-
tificial plagiarism, if carefully generated, is a vi-
able alternative to simulated plagiarism cases and
real text reuse cases. (3) Our strategies to scale-up
the construction of plagiarism corpora works well
compared to existing, handmade corpora.
5 Summary
Current evaluation methodologies in the field
of plagiarism detection research have conceptual
shortcomings and allow only for a limited compa-
rability. Our research contributes right here: we
present tailored performance measures for plagia-
rism detection and the large-scale corpus PAN-
PC-10 for the controlled evaluation of detection
algorithms. The corpus features various kinds
of plagiarism cases, including obfuscated cases
that have been generated automatically and man-
ually. An evaluation of the corpus in relation to
previous corpora reveals a high degree of matu-
rity. Until now, 31 plagiarism detectors have been
compared using our evaluation framework. This
high number of systems has been achieved based
on two benchmarking workshops in which the
framework was employed and developed, namely
PAN?09 [15] and PAN?10 [16]. We hope that our
framework will be beneficial as a challenging and
yet realistic test bed for researchers in order to pin-
point the room for the development of better pla-
giarism detection systems.
Acknowledgements
We thank Andreas Eiselt for his devoted work
on the corpus over the past two years. This
work is partially funded by CONACYT-Mexico
and the MICINN project TEXT-ENTERPRISE
2.0 TIN2009-13391-C04-03 (Plan I+D+i).
Bibliography
[1] Omar Alonso and Stefano Mizzaro. Can
We Get Rid of TREC Assessors? Using
Mechanical Turk for Relevance
Assessment. In SIGIR?09: Proceedings of
the Workshop on The Future of IR
Evaluation, 2009.
[2] Vamshi Ambati, Stephan Vogel, and Jaime
Carbonell. Active learning and
crowd-sourcing for machine translation. In
Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike
Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh conference on
International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may
2010. European Language Resources
Association (ELRA). ISBN 2-9517408-6-7.
[3] Jeff Barr and Luis Felipe Cabrera. AI Gets
a Brain. Queue, 4(4):24?29, 2006. ISSN
1542-7730. doi:
10.1145/1142055.1142067.
[4] Regina Barzilay and Lillian Lee. Learning
to Paraphrase: An Unsupervised Approach
Using Multiple-Sequence Alignment. In
NAACL?03: Proceedings of the 2003
Conference of the North American Chapter
of the Association for Computational
Linguistics on Human Language
Technology, pages 16?23, Morristown, NJ,
USA, 2003. Association for Computational
Linguistics. doi:
10.3115/1073445.1073448.
[5] Andrei Z. Broder. Identifying and Filtering
Near-Duplicate Documents. In COM?00:
Proceedings of the 11th Annual Symposium
on Combinatorial Pattern Matching, pages
1004
1?10, London, UK, 2000. Springer-Verlag.
ISBN 3-540-67633-3.
[6] Manuel Cebrian, Manuel Alfonseca, and
Alfonso Ortega. Towards the Validation of
Plagiarism Detection Tools by Means of
Grammar Evolution. IEEE Transactions on
Evolutionary Computation, 13(3):477?485,
June 2009. ISSN 1089-778X.
[7] Paul Clough. Plagiarism in Natural and
Programming Languages: An Overview of
Current Tools and Technologies. Internal
Report CS-00-05, University of Sheffield,
2000.
[8] Paul Clough. Old and New Challenges in
Automatic Plagiarism Detection. National
UK Plagiarism Advisory Service,
http://ir.shef.ac.uk/cloughie/papers/pas_plagiarism.pdf,
2003.
[9] Paul Clough and Mark Stevenson. Creating
a Corpus of Plagiarised Academic Texts. In
Proceedings of Corpus Linguistics
Conference, CL?09 (to appear), 2009.
[10] Paul Clough and Mark Stevenson.
Developing A Corpus of Plagiarised Short
Answers. Language Resources and
Evaluation: Special Issue on Plagiarism
and Authorship Analysis (in press), 2010.
[11] Paul Clough, Robert Gaizauskas, and S. L.
Piao. Building and Annotating a Corpus for
the Study of Journalistic Text Reuse. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation (LREC-02), pages 1678?1691,
2002.
[12] Wiebe Hordijk, Mar?a L. Ponisio, and Roel
Wieringa. Structured Review of Code
Clone Literature. Technical Report
TR-CTIT-08-33, Centre for Telematics and
Information Technology, University of
Twente, Enschede, 2008.
[13] Winter Mason and Duncan J. Watts.
Financial Incentives and the "Performance
of Crowds". In HCOMP?09: Proceedings
of the ACM SIGKDD Workshop on Human
Computation, pages 77?85, New York, NY,
USA, 2009. ACM. ISBN
978-1-60558-672-4. doi:
10.1145/1600150.1600175.
[14] Hermann Maurer, Frank Kappe, and Bilal
Zaka. Plagiarism - A Survey. Journal of
Universal Computer Science, 12(8):
1050?1084, 2006.
[15] Martin Potthast, Benno Stein, Andreas
Eiselt, Alberto Barr?n-Cede?o, and Paolo
Rosso. Overview of the 1st International
Competition on Plagiarism Detection. In
Benno Stein, Paolo Rosso, Efstathios
Stamatatos, Moshe Koppel, and Eneko
Agirre, editors, SEPLN 2009 Workshop on
Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 09), pages
1?9. CEUR-WS.org, September 2009. URL
http://ceur-ws.org/Vol-502.
[16] Martin Potthast, Benno Stein, Andreas
Eiselt, Alberto Barr?n-Cede?o, and Paolo
Rosso. Overview of the 2nd International
Benchmarking Workshop on Plagiarism
Detection. In Benno Stein, Paolo Rosso,
Efstathios Stamatatos, and Moshe Koppel,
editors, Proceedings of PAN at CLEF 2010:
Uncovering Plagiarism, Authorship, and
Social Software Misuse, September 2010.
[17] Chanchal K. Roy and James R. Cordy.
Scenario-Based Comparison of Clone
Detection Techniques. In ICPC ?08:
Proceedings of the 2008 The 16th IEEE
International Conference on Program
Comprehension, pages 153?162,
Washington, DC, USA, 2008. IEEE
Computer Society. ISBN
978-0-7695-3176-2.
[18] Chanchal K. Roy and James R. Cordy.
Towards a Mutation-based Automatic
Framework for Evaluating Code Clone
Detection Tools. In C3S2E ?08:
Proceedings of the 2008 C3S2E conference,
pages 137?140, New York, NY, USA, 2008.
ACM. ISBN 978-1-60558-101-9.
[19] Chanchal K. Roy, James R. Cordy, and
Rainer Koschke. Comparison and
Evaluation of Code Clone Detection
Techniques and Tools: A Qualitative
Approach. Sci. Comput. Program., 74(7):
470?495, 2009. ISSN 0167-6423.
[20] Chanchal K. Roy and James R. Cordy. A
survey on software clone detection
research. Technical Report 2007-541,
School of Computing, Queen?s University
at Kingston, Ontario, Canada, 2007.
[21] Geoffrey R. Whale. Identification of
Program Similarity in Large Populations.
The Computer Journal, 33(2):140?146,
1990. doi: 10.1093/comjnl/33.2.140.
[22] Ying Zhao, George Karypis, and Usama
Fayyad. Hierarchical Clustering Algorithms
for Document Datasets. Data Min. Knowl.
Discov., 10(2):141?168, 2005. ISSN
1384-5810. doi:
10.1007/s10618-005-0361-3.
1005
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 553?564, Dublin, Ireland, August 23-29 2014.
Modeling Review Argumentation for Robust Sentiment Analysis
Henning Wachsmuth
Universit?at Paderborn
s-lab ? Software Quality Lab
Paderborn, Germany
henningw@upb.de
Martin Trenkmann, Benno Stein
Bauhaus-Universit?at Weimar
Webis Group
Weimar, Germany
<1st>.<last>@uni-weimar.de
Gregor Engels
Universit?at Paderborn
s-lab ? Software Quality Lab
Paderborn, Germany
engels@upb.de
Abstract
Most text classification approaches model text at the lexical and syntactic level only, lacking do-
main robustness and explainability. In tasks like sentiment analysis, such approaches can result in
limited effectiveness if the texts to be classified consist of a series of arguments. In this paper, we
claim that even a shallow model of the argumentation of a text allows for an effective and more
robust classification, while providing intuitive explanations of the classification results. Here, we
apply this idea to the supervised prediction of sentiment scores for reviews. We combine existing
approaches from sentiment analysis with novel features that compare the overall argumentation
structure of the given review text to a learned set of common sentiment flow patterns. Our evalu-
ation in two domains demonstrates the benefit of modeling argumentation for text classification
in terms of effectiveness and robustness.
1 Introduction
Text classification is a key technique in natural language processing and information retrieval that is ap-
plied for several tasks. Standard classification approaches map a text to a vector of lexical and shallow
syntactic surface-level features, from which class information is inferred using supervised learning (Man-
ning et al., 2008). Even though the results of such approaches can hardly be explained, they have proven
effective for narrow-domain texts with explicit class information (Joachims, 2001; Pang et al., 2002).
However, surface-level features often do not help to classify out-of-domain texts correctly, because
they tend to model the domain of the texts and not the classes to be inferred, as we observe in (Wachsmuth
and Bujna, 2011) among others. Moreover, they are likely to fail on texts where the class information
is implicitly represented by the argumentation of the writer. Such texts are in the focus of popular tasks
like authorship attribution, automatic essay grading, and, above all, sentiment analysis. As an example,
consider the short hotel review at the top and bottom of Figure 1. It contains more positive than negative
statements. Hence, a surface-level analysis would probably classify the review to have a positive overall
sentiment polarity. In fact, the argumentation of the review text reveals a clear negative sentiment.
The analysis of argumentation is recently getting more attention (cf. Section 2 for details). With respect
to sentiment, related approaches analyze discourse relations (Mukherjee and Bhattacharyya, 2012), iden-
tify the different aspects mentioned in a text (Lazaridou et al., 2013), or the like. While these approaches
can infer implicit class information from argumentative texts like reviews, they do not address the domain
dependency problem of sentiment analysis (Wu et al., 2010). In addition, they still lack explainability,
which limits end user acceptance in case of wrong results (Lim and Dey, 2009).
In this paper, we consider the question of how to capture the argumentation of reviews for a domain-
robust and explainable text classification. As Figure 1 illustrates, we rely on a shallow model of review
argumentation, which represents a text as a sequence of statements that express local sentiment on do-
main concepts and that are connected by discourse relations. We claim that, by focusing on features that
model the abstract argumentation structure of a text, a more robust sentiment analysis can be achieved.
At the same time, such an analysis can explain its results based on the underlying model.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
553
4statement
positive
negative
objective
We spent one night at that hotel.  Staff at the front desk was very nice,  the room was clean and cozy,
and the hotel lies in the city center...  but all this never justifies the price, which is outrageous!
background elaboration elaboration contrast
5
321
Figure 1: Illustration of our shallow model of review argumentation for a sample review text from the ho-
tel domain. Domain concepts, such as ?front desk?, are marked in bold. Each circle denotes a statement
with local sentiment. The statements are connected by directed discourse relations like ?elaboration?.
Concretely, here we address the supervised prediction of sentiment scores. To this end, we combine
a number of existing argumentation-related features with a novel approach that learns common patterns
in sequences of local sentiment through a cluster analysis in order to capture a review?s overall argu-
mentation structure. Inspired by explicit semantic analysis (Gabrilovich and Markovitch, 2007), we then
compute the similarity of a given review text to each of these sentiment flow patterns and we use these
similarities as features for sentiment scoring. To explain a predicted score and, hence, to increase user
acceptance, both the underlying model and the sentiment flow patterns can be visualized.
We evaluate our approach on reviews of the hotel domain and the movie domain. In comparison to
standard baselines, we demonstrate the effectiveness and robustness of modeling argumentation. Our
results suggest that especially the sentiment flow patterns learned in one domain generalize well to other
domains. Altogether, the contributions of this paper are:
1. A shallow model of review argumentation for text classification that enables a more domain-robust
and explainable sentiment analysis (Section 3).
2. A novel feature type named sentiment flow patterns that, for the first time, captures the abstract
overall argumentation structure of review texts, irrespective of their domain (Section 4).
3. Experimental evidence for the existence of common patterns in the argumentation structure of re-
view texts across domains (Section 5).
2 Related Work
Argumentation plays a key role in human communication and cognition. Its purpose is to provide persua-
sive information for or against a decision or claim. This involves the identification of facts and warrants
justified by a backing or countered by a rebuttal (Toulmin, 1958). Argumentation is studied in various
disciplines, such as logic, philosophy, and artificial intelligence. We consider the linguistics perspective,
where it is pragmatically viewed as a regulated sequence of speech or text (Walton and Godden, 2006).
In particular, we analyze monological argumentations in written text as opposed to dialogical argu-
mentations where participants persuade each other with arguments (Cabrio and Villata, 2012). In terms
of text, one of the most obvious forms of monological argumentation can be found in reviews. A review
comprises a positional argumentation, where an author collates and structures a choice of facts, pros,
and cons in order to inform intended recipients about his or her beliefs (Besnard and Hunter, 2008).
According to Mochales and Moens (2011), an argumentation analysis targets at ?the content of serial
arguments, their linguistic structure, the relationship between the preceding and following arguments,
recognizing the underlying conceptual beliefs, and understanding within the comprehensive coherence of
the specific topic.? The authors work on argumentation mining, i.e., the detection of different arguments
for justifying a conclusion as well as their interactions. Our model of argumentation matches the quoted
definition. Similar to the distinction between shallow and deep parsing (Jurafsky and Martin, 2009),
our approach can be seen as a shallow argumentation analysis in that we consider only the sequence of
arguments. This abstraction appears very promising to address text classification.
Unlike argumentative zoning (Teufel et al., 2009), which classifies segments of scientific articles ac-
cording to argumentative functions, we predict the sentiment scores of reviews from a sequence of clas-
sified segments. Sentiment scoring is tackled in both computational linguistics (Pang and Lee, 2005) and
554
information retrieval (Wang et al., 2010). Such kind of sentiment analysis benefits from modeling argu-
mentative discourse (Villalba and Saint-Dizier, 2012). Related works already employ discourse features
to detect sentiment polarity. Some rely on complex discourse parsing (Heerschop et al., 2011), whereas
others argue that a lightweight approach is more robust for noisy texts (Mukherjee and Bhattacharyya,
2012). We rather follow the latter, but we see discourse only as one part of review argumentation.
In accordance with Lazaridou et al. (2013) who address aspect-based sentiment analysis, we addition-
ally analyze the connection of local sentiment to domain concepts and discourse relations. Even more
important for us is the local sentiment flow in a text. This term was introduced by Mao and Lebanon
(2007), who infer a text?s global sentiment from its sequence of local (sentence) sentiments, classified
with conditional random fields. Their approach converts each sentiment in the sequence to a single fea-
ture and learns a mapping from the features to global sentiment. By that, it actually disregards the order-
ing of local sentiment. In contrast, our sentiment flow patterns measure the similarity between complete
sequences of local sentiment. This resembles explicit semantic analysis (Gabrilovich and Markovitch,
2007), which classifies texts based on their relatedness to concepts modeled by complete texts.
In (Wachsmuth et al., 2014), we reveal correlations between a review?s sentiment score and its local
sentiment flow. Similar to Socher et al. (2013), we therefore argue that global sentiment emanates from
the composition of local sentiment. The authors model the semantic compositionality of words in given
sentences, thus capturing the language of a given domain. Conversely, our sentiment flow patterns focus
on the structure of complete texts in order to reduce domain dependency, which is a general problem
in text classification (Wu et al., 2010). Among others, existing strategies to tackle this problem align
features of the source and the target domain, as we do in (Prettenhofer and Stein, 2010).
Given a vector of features, text classification approaches typically output only a class label (Manning
et al., 2008). This renders the understanding and debugging of classification results hard (Kulesza et al.,
2011). Instead, our approach explains results by making the argumentation of texts visible. Thereby, we
increase intelligibility and, thus, support user acceptance (Lim and Dey, 2009).
3 A Shallow Model of Review Argumentation
This section first sketches our general hypothesis. Then, we present our model of review argumentation.
3.1 Hypothesis behind Modeling Argumentation for Text Classification
Several text classification tasks relate to the argumentation of a text. As an obvious example, automated
essay scoring explicitly rates argumentative texts, mostly targeting at structural aspects (Dikli, 2006). In
genre identification, a central concept is the form of texts. Some genre-related tasks address argumenta-
tion, e.g. by classifying texts according to their function (Wachsmuth and Bujna, 2011). Criteria in text
quality assessment often measure structure (Anderka et al., 2012), while readability is connected to dis-
course (Pitler and Nenkova, 2008). Authorship attribution profits from argumentation clues like uncon-
sciously used function words (Stamatatos, 2009), and plagiarism detection, in the end, aims to check if
the argumentation in a fragment of a text refers to the author of the text (Potthast et al., 2013).
We hypothesize that in these and further tasks the class of a text is often decided by the structure of
its argumentation rather than by its content, while the content adapts the argumentation to the domain at
hand. Following Besnard and Hunter (2008), an argumentation consists of a composition of arguments
used to justify a decision or claim. Each argument can be seen as a statement with some evidence. Under
our hypothesis, an explicit model of statements and their composition hence supports the identification of
domain-independent patterns. Together with the content, the statements enable a fine-grained analysis,
while serving as the basis for an explanation. Since the relevant types of statements vary among tasks, we
argue that such a model should be task-specific. Below, we investigate reviews on products and services
from a sentiment analysis perspective. Because of its positional nature (cf. Section 2), review argumen-
tation makes its arguments explicit, i.e., facts and opinions on different product features and aspects.
3.2 Modeling Review Argumentation for Sentiment Analysis
We consider reviews that comprise a text about some product or service as well as a numerical overall
rating. Any other metadata that might be given for reviews is ignored in the following. Our assumption
555
x1 x i xn
C(x i) C'(x i)
R(x i, x j)
local sentiment
domain concepts
discourse relations
...
x j
......
...... ... ...
... ...
S(x1) S(x i) S(xn)S(x j)... ......
statements
Figure 2: Our shallow model of review argumen-
tation defined by a segmentation into statements
and by three functions based on the statements.
local 
sentiment
sentiment flow
patterns
discourse
relations
domain 
concepts
baseline distributions
word & POS n-grams, character trigrams, lengths, SentiWordNet scores
(
)
, , , ,
a1 a2 a3 a4
b1
baseline features
argumentation features
Figure 3: A vector with all five considered feature
types including the novel sentiment flow patterns.
Each found pattern becomes a single feature in a4.
is that the overall rating denotes a sentiment score y from a metric sentiment scale that quantifies the
possibly implicit conclusion of the review text in terms of its global sentiment.
Statements To capture a review?s argumentation, we model the review?s text as a sequence of n > 0
statements x
1
, . . . , x
n
. Here, we define a statement x syntactically to be a main clause together with
all its subordinate clauses. The notion behind is that, in our experience, such a text segment is usually
meaningful on its own while bearing at most one sentiment. Many sentences in reviews comprise series
of statements. For instance, the following excerpt from Figure 1 consists of two statements, x
4
and x
5
:
x
4
: and the hotel lies in the city center... x
5
: but all this never justifies the price, which is outrageous!
Based on the set of all statements X, we capture the structure and content of review texts as follows:
Local Sentiment We assume each statement to represent either an objective fact obj, a positive opin-
ion pos, or a negative opinion neg (for a wide applicability, we ignore sentiment intensity). So, there is
an unknown function that maps each statement to a local sentiment, e.g. x
4
to obj and x
5
to neg:
local sentiment : X ?
{
S(x) | S ? {pos, neg, obj}
}
Discourse Relations As for x
4
and x
5
, the composition of statements in a text is, in general, not co-
incidental. Rather, it implies a structure made up of an ordered choice of statements as well as of a
number of directed discourse relations. We define a discourse relation to have some type R of a set of
relation typesR and to relate two (typically neighboring) statements, e.g. contrast(x
5
, x
4
) in the example
above. The following function hence can be understood as a shallow version of the rhetorical structure
theory (Mann and Thompson, 1988):
discourse relations : X ? {R(x
i
, x
j
) | 1 ? i, j ? n ; R ? R}
Domain Concepts The argumentation structure of a text is bound to the domain at hand through the
text?s content. In particular, a review text discusses a subset of the domain conceptsC that are associated
to a product or service, each being referred to in one or more statements. For instance, x
5
discusses the
price of the hotel, i.e., price(x
5
). We capture the domain concepts in statements as follows:
domain concepts : X ? {C(x
i
) | 1 ? i ? n ; C ? C}
Altogether, our model represents a review text as a sequence of interrelated statements of certain types
and content. Figure 2 illustrates the defined functions. An instance of the model is visualized in Figure 1.
The model is an abstraction of argumentation, covering some information only implicitly if at all (e.g.
lexical or syntactic clues). However, it can be extended by further information, as we do below.
4 Features for Robust Sentiment Analysis and Explanation
We now present different types of features for supervised learning that capture both distributional and
structural aspects of review argumentation based on our shallow model. Here, we assume that all infor-
mation represented in the model is given, but Section 5 analyzes the effects of inferring the information
from a text. Figure 3 gives an overview of the vector with all feature types that we consider, including a
common set of baseline features (b1). The goal of all argumentation features (a1?a4) is twofold: (1) To
enable an effective and robust sentiment analysis. (2) To provide means to explain analysis results.
556
1 161 5
positive (1.0)
negative (0.0)
objective (0.5)
Figure 4: Illustration of a length-normalized ver-
sion (small circles) of the sample local sentiment
flow from Figure 1 (big circles) for length 16.
positive (1.0)
negative (0.0)
objective (0.5)
Figure 5: Sketch of the construction of a sentiment
flow pattern (dashed curve), here from two sample
local sentiment flows (circles and squares).
4.1 Quantification of Distributional Argumentation Aspects
In terms of the distributional aspects of the three functions introduced in Section 3, we combine a selec-
tion of ideas from existing sentiment analysis approaches that are related to review argumentation. Some
features of the types described in the following are selected only if they occur frequently in a given set of
training texts. Thus, the concrete numbers of features vary, as we see in the evaluation in Section 5.
Local Sentiment (a1) In (Wachsmuth et al., 2014), we stress the impact of the distribution of local sen-
timent. Accordingly, here we determine the frequencies of all types of local sentiment in the given text as
well as of series of statements with the same type and of changes from one type to another. Also, we have
features that denote the local sentiment at specific positions like the first and last two statements, and we
compute the average local sentiment. For the latter, we map pos to 1.0, obj to 0.5, and neg to 0.0.
In addition, we follow Mao and Lebanon (2007) in that we capture the local sentiment flow based on
the defined mapping. To preserve the original flows as far as possible, we length-normalize the sequence
of values using non-linear interpolation with subsequent sampling. Figure 4 shows an example.
Discourse Relations (a2) We count the occurrences of different discourse relation types from (Mann
and Thompson, 1988), e.g. contrast or elaboration (in Section 5, we distinguish a subset of ten types). To
model connections between sentiment and discourse, we do the same for all frequently occurring combi-
nations of discourse relation types and local sentiment of the related statements, e.g. contrast(pos, neg)
or contrast(neg, pos). By that, we imitate Lazaridou et al. (2013) to some extent.
Domain Concepts (a3) With the same intention, we determine the most frequent domain concepts in the
given training set and we compute how often each concept cooccurs with each type of local sentiment.
Examples from the sample text in Figure 1 are hotel(obj) or price(neg). Moreover, we count the num-
ber of different domain concepts as well as the instances of all possibly distinguished types of domain
concepts, which would be product (like ?hotel?) and product feature (like ?price?) in the given case.
Types a1?a3 refer to important characteristics of review argumentation. However, none of them cap-
tures a review?s overall argumentation structure. Even the local sentiment flow in a1 rather measures the
impact of local sentiment at different positions. The reason behind is that the flow positions are repre-
sented by individual features. Hence, common learning approaches like regression will naturally tend to
assign positive weights to all positions, not considering the sentiment flow as a whole.
4.2 Learning of Structural Argumentation Aspects
To capture the impact of the structure of an argumentation, we introduce a novel feature type based on the
local sentiment flows of texts only. The idea behind resembles explicit semantic analysis (Gabrilovich
and Markovitch, 2007) in that every single feature represents the similarity to a complete flow:
Sentiment Flow Patterns (a4) We first construct a set of common sentiment flow patterns from a set
of known training review texts, where each pattern denotes the average of a set of similar local senti-
ment flows of normalized length. Given an unknown review text, we then measure the similarity of its
normalized local sentiment flow to each constructed pattern. The set of these similarities forms a4.
Figure 5 exemplifies the pattern construction. Our hypothesis behind sentiment flow patterns is that
similar local sentiment flows entail similar sentiment scores. Accordingly, flows that construct a pattern
should be as similar as possible and flows of different patterns as dissimilar as possible. Therefore, we ap-
ply clustering (Manning et al., 2008) to partition the flows of all texts from the given training set based
on some flow similarity function (in Section 5, we use the manhattan distance). The centroid of each ob-
557
1 1 1 3 1 3 3 3 2 3 4 4 3 3 5 54 5 5 4
highest cuts 
with relaxed
purity ? 0.8
relaxed purity
values at cuts
0.8 0.889 1.0 1.0
flow clusters
at cuts
(a)
1.0
0.5
0.00.0 0.5 1.0 x1
x2
1
1
1
1
3
55
554
43
3
3
3 4
4
3
3
2
(b) sentiment flow patterns,i.e., the centroids of the 
flow clusters
local sentiment flow 
to be classified
manhattan distances to
sentiment flow patterns
?
Figure 6: (a) A purity threshold of 0.8 derives four
clusters from a hierarchical clustering of 20 local
sentiment flows represented by their scores from 1
to 5. (b) 2D plot of computing distances to the sen-
timent flow patterns, i.e., the clusters? centroids.
(b) positive
negative
objective
common pattern
of score 3-4
score 2.69
con-
trast
elabo-
ration
elabo-
ration
back-
ground
We spent one night at that hotel ... never justifies the price, which is outrageous!
We spent 
one night at 
that hotel.
Staff at the 
front desk was 
very nice,
the room 
looked clean 
and cozy,
and the hotel 
lies in the city 
center...
but all this never
justifies the price,
which is outrageous!
objective positive objectivepositive negative
score 2.69
product feature feature feature featureproduct
hotel hotelstaff front desk priceroom
(a)
common pattern
of score 2-3
Figure 7: Two possible explanations of scoring the
sample text from Figure 1: (a) Graph visualization
of our model of review argumentation. (b) Com-
parison of the local sentiment flow of the text with
the two most similar sentiment flow patterns.
tained cluster ? then becomes a sentiment flow pattern. Since we know the sentiment scores associated to
the flows in the training set, we can measure the purity of a cluster ?, which here denotes the fraction of
those flows ?
y
?
in ? whose score equals the majority score y
?
in ? (Manning et al., 2008). The original
purity definition, however, assumes exactly one correct score for each flow. Here, this would mean that
a flow alone decides a score. Instead, for larger sentiment scales, we propose to relax the purity measure
by assuming also the dominant neighbor of the majority score as correct:
relaxed purity(?) =
(
|?
y
?
|+ max(|?
y
?
?1
|,|?
y
?
+1
|)
) /
|?|
We seek for clusters with a high purity, because such clusters support that similarities between flows
and patterns indicate specific sentiment scores. At the same time, the number of clusters should be small
in order to achieve a high average cluster size and, thus, a high commonness of the patterns. For this
purpose, we rely on hierarchical clustering, where we can easily find a flat clustering with a certain
number of clusters through cuts at appropriate nodes in the binary tree of the associated hierarchy. Pattern
construction profits from compact clusters, suggesting to compute distances between clusters from their
group-average links (Manning et al., 2008). To minimize the number of clusters, we search for all nodes
closest to the tree?s root that represent clusters with a purity above some threshold, e.g. 0.8 in the example
in Figure 6(a). The centroids of these clusters become sentiment flow patterns, if they are made up of
some minimum number of flows.
At the end of the clustering process, we remain with one feature for each constructed sentiment flow
pattern. Given a review text to be classified, we then compute its normalized local sentiment flow and we
measure the flow?s distance to all patterns. Each distance represents one similarity in the feature type a4.
Figure 6(b) sketches the computation of the distances, mapped into two dimensions.
4.3 Comparison with Baseline Approaches
In the evaluation below, we compare the feature types a1 to a4 with the well-known sentiment scoring
approach of Pang and Lee (2005) in terms of effectiveness. Our focus, however, is the robustness of
modeling argumentation structure in contrast to standard text classification features employed in many
other approaches, such as n-grams or variations of them (Qu et al., 2010). To this end, we also integrate
the following baseline features, most of which model a text at the lexical and syntactic level:
Baseline Distributions (b1) We compute the distributions of all word and part-of-speech unigrams,
bigrams, and trigrams as well as of all character trigrams that frequently occur in a given training set.
In addition, we determine the length of the given text in different units and some average SentiWordNet
scores with respect to both the first and the average senses of its words (Baccianella et al., 2010).
558
4.4 Explanation of Sentiment Scores
We propose a shallow statistical argumentation analysis that learns to predict sentiment scores on a train-
ing set of review texts. After prediction, we can directly exploit the information captured in our model
as well as the values of the feature types a1?a4 in order to explain the predicted score. Two possible
explanations are visualized in Figure 7, while a combination of them is exemplified in Figure 1. We be-
lieve that such explanations can increase a user?s confidence in statistical analysis results and, hence, the
acceptance of corresponding applications. To demonstrate the analysis and explanation of review argu-
mentation, we provide a free-to-use tool and webservice at http://www.arguana.com.
5 Evaluation of Modeling Argumentation for Sentiment Scoring
In this section, we evaluate the effectiveness and domain robustness of modeling argumentation for
sentiment scoring with a focus on the sentiment flow patterns. The source code of the evaluation can be
found at http://www.arguana.com. Our experiments are based on two English text corpora with reviews
from the hotel domain and the movie domain, respectively. In both cases, we leave out the review titles
for generality, because our approach targets at arbitrary reviews including those without a title.
Text Corpora On the one hand, we process the ArguAna TripAdvisor corpus that we have introduced
in (Wachsmuth et al., 2014). This corpus compiles a collection of 2,100 reviews of hotels from seven
locations, balanced with respect to their sentiment scores between 1 and 5. All of the reviews? texts are
segmented into statements with an average of 14.8 statements per text. Each statement is annotated as an
objective fact, a positive, or a negative opinion. Moreover, all mentions of domain concepts are marked as
such. The corpus is also available at http://www.arguana.com, free for scientific use. In the experiments,
we rely on the provided corpus split with 900 reviews from three hotel locations in the training set, and
600 reviews from two locations in the validation set and test set each.
On the other hand, we use the Sentiment Scale dataset (Pang and Lee, 2005) consisting of 5,006 movie
reviews that are split into four text corpora according to their authors (Author a, b, c, and d). From these,
we have discarded eight reviews due to encoding problems. We choose the provided sentiment scale
from 0 to 2, so we can logically map the scale of the hotel reviews (1?5) to it for a domain transfer. In
particular, scores 1?2 are mapped to 0, 3 to 1, and 4?5 to 2. On average, the movie reviews are much
longer with 36.1 statements per text. Since no local sentiment annotations are given, we also process
the subjectivity dataset (Pang and Lee, 2004) and the sentence polarity dataset (Pang and Lee, 2005) in
order to develop classifiers for sentence sentiment. Accordingly, we assume each movie review sentence
to denote one statement. To directly compare our results to those of Pang and Lee (2005), we perform
10-fold cross-validation separately on the dataset of each single author, averaged over five runs.
Preprocessing For feature computations, we preprocess all texts with a tokenizer, a sentence splitter,
and the part-of-speech tagger from (Schmid, 1995). We employ lexicon-based extractors for discourse
relations and domain concepts, which aim at a high precision while not being able to recognize unseen
instances. The former resembles the lightweight approach of Mukherjee and Bhattacharyya (2012). Pri-
marily, it looks for conjunctions that indicate certain discourse relations, such as ?but? or ?because?. The
latter detects exactly those domain concepts that are annotated largely consistently in the training set of
the ArguAna TripAdvisor corpus. Thus, it helps only on the hotel reviews. These reviews are segmented
into statements with a respective algorithm that comes with the corpus.
For both domains, we have trained linear support vector machines (SVMs) from Chang and Lin (2011)
that classify the subjectivity of each statement (opinion or fact) and the polarity of each opinion (positive
or negative). They use 1k to 2k features of different types: word and part-of-speech unigrams, character
trigrams, SentiWordNet scores (Baccianella et al., 2010), and some special features like the first word of a
statement or its position in the text. On the test set of the hotel domain, the classifiers have an accuracy of
78.1% for subjectivity and of 80.4% for polarity. In the movie domain, we achieve a subjectivity accuracy
of 91.1%, but a polarity accuracy of only 73.8% (measured through 10-fold cross-validation).
Feature Computation We determine one distinct feature set for each evaluated text corpus made up
of the feature types presented in Section 4. Where necessary, we divide the computed feature values by
559
the length of the text (in tokens or statements, as appropriate), in order to ensure that all feature values
always lie between 0 and 1.
Local sentiment flows are normalized to length 30 in case of the hotel reviews and to length 60 in case
of the movie reviews, which allows us to represent most of the original flows without loss. Altogether,
feature type a1 sums up to 50 and 80 features, respectively. For a2, a3, and b1, we consider only those
features whose frequency in the training texts exceeds some specified threshold. For instance, a word
unigram is taken into account within b1 only if it occurs in at least 5% of the hotel reviews or 10% of
the movie reviews, respectively. As a result, the number of evaluated features varies depending on the
processed text corpus. Concretely, we obtain 64 to 78 features for discourse relations (a2), 78 to 114 for
domain concepts (a3), and 1026 to 2071 baseline features (b1). More details are given in the instruction
and configuration files that come with the provided source code.
To construct sentiment flow patterns (a4), we have developed an agglomerative hierarchical clusterer
that implements the approach from Section 4.2. After some tests with different settings, we decided to
measure flow and cluster similarity using group-average link clustering based on the manhattan distance
between the length-normalized local sentiment flows. For the hierarchy tree cuts, we use a purity thresh-
old of 0.8, where we take the relaxed purity for the sentiment scale 1?5 of the hotel reviews, but the
original purity for the movie reviews (because of the limited scale from 0 to 2). All centroids of clusters
with at least three flows become a sentiment flow pattern, resulting in 16 to 86 features in a4.
Sentiment Scoring On the hotel reviews, we compute the root mean squared error of linear sentiment
score regression trained using stochastic gradient descent (SGD) from Weka 3.7.5 (Hall et al., 2009).
Both the regularization parameter and the learning rate of SGD are set to 10
?5
, whereas we determine
the epochs parameter of SGD on the validation set. Then, we measure the error on the test set.
For the comparison to (Pang and Lee, 2005), we predict the scores of the movie reviews using classifi-
cation, which additionally stresses the domain change. In particular, we measure the accuracy of a linear
1-vs.-1 multi-class SVM with probability estimates and normalization. While we optimize the cost para-
meter of the SVMs in the in-domain task, we rely on the default value (1.0) for the domain transfer.
5.1 Effectiveness of Modeling Argumentation
First, we measure the theoretically possible scoring effectiveness of all feature types within one domain.
To this end, we compare the feature types based on the ground-truth annotations of the ArguAna Trip-
Advisor corpus. The column Corpus of Table 1 lists the resulting root mean squared errors. As can
be seen, all argumentation feature types clearly outperform the baseline distributions (b1) and improve
strongly over random guessing. The distributional local sentiment (a1) does best with an error of 0.77,
whereas the domain concepts perform worst among a1 to a4. Still, they result in an 0.12 lower root mean
squared error than the baseline distributions (b1). Overall, the lowest observed error is 0.75, achieved by
the SVM with all features as well as by two subsets of the argumentation features alone.
In practice, no ground-truth annotations are given, so we need to create annotations in the review texts
ourselves using the preprocessing described above. This in turn changes the feature set and the respective
values of the argumentation features. The third column of Table 1 (Self) shows that such a resort to self-
created annotations leads to a root mean squared error increase of 0.14 to 0.22 for the types a1 to a4.
Nevertheless, the argumentation features succeed over the baseline distributions with 0.94 as opposed
to 1.11, which demonstrates the effectiveness of modeling the argumentation of hotel reviews.
5.2 Robustness of Modeling Argumentation Structure
We hypothesize that the developed structure-based argumentation features are robust against domain
transfer to a wide extent. To investigate this, we classify sentiment scores using SVMs based either on
all or on one single feature type (except for a3, for lack of movie domain concept extractors) in two tasks
on the four movie datasets: (1) with training in the movie domain (through 10-fold cross-validation), and
(2) with training out-of-domain on the hotel review training set.
Figure 8 contrasts the accuracy results for the two tasks and compares them to the best SVM approach
of Pang and Lee (2005), i.e., ova (open squares). In the in-domain task, our SVM based on all feature
types (black squares) is significantly better than ova on one dataset (Author a) and a little worse on
560
Feature type Corpus Self
none Random guessing 1.41 1.41
a1 Local sentiment 0.77 0.99
a2 Discourse relations 0.84 1.01
a3 Domain concepts 0.99 1.13
a4 Sentiment flow patterns 0.86 1.07
b1 Baseline distributions 1.11 1.11
a1?a4 Argumentation features 0.76 0.94
a2, a3, a4 w/o local sentiment 0.79 0.99
a1, a3, a4 w/o discourse relations 0.76 0.97
a1, a2, a4 w/o domain concepts 0.75 0.95
a1, a2, a3 w/o sentiment flow patterns 0.75 0.95
all All features 0.75 0.93
Table 1: Root mean squared error of sentiment
score regression on the hotel review test set
for all evaluated features types and for different
combinations of these types. Features are com-
puted based on ground-truth annotations (Cor-
pus) or based on self-created annotations (Self).
70%
60%
50%
40%
30%
Author a Author cAuthor b Author d
 a
cc
ur
ac
y 
in
 m
ov
ie
 d
om
ai
n
68.2
57.4
72.0
58.3
45.5
42.7
50.7
31.0
a4
a2
a1
b1
a4
a2
a1
b1
a4
a2
a1
b1
a4
a2
a1
b1
in hotel domain
training
in movie domain
Figure 8: Sentiment scoring accuracy of the SVM
based on feature type a1, a2, a4, and b1 (black icons)
and of each SVM based on one of these types (or-
ange icons) on the four movie datasets, when trained
on movie reviews (squares) or on hotel reviews (cir-
cles). Open squares: ova from (Pang and Lee, 2005).
301
positive (1.0)
negative (0.0)
objective (0.5)
score 5-4 (based on 226 flows)score 2-3 (based on 24 flows)
score 1-2 (based on 155 flows)
1 60
1.0
0.0
0.5
(c) Sentiment Scale dataset, Author d (scale 0-2)(b) Sentiment Scale dataset, Author c (scale 0-2)
1.0
0.0
0.5
(a) ArguAna TripAdvisor corpus (scale 1-5)
1 60
statement
score 2
(155 flows)
score 0
(11 flows)
score 1
(7 flows)
score 2
(5 flows)
score 1
(15 flows)
score 0
(16 flows)
Figure 9: (a) The three most common sentiment flow patterns in the training set of the ArguAna TripAd-
visor corpus, labeled with their associated sentiment scores. (b?c) The according sentiment flow pattern
for each possible score of the texts of Author c and Author d in the Sentiment Scale dataset, respectively.
two other datasets (Author c and Author d). On all four datasets, a4 classifies sentiment scores more
accurately than both a1 and a2, but none of the argumentation feature types can compete with the baseline
distributions (b1). We suppose that the reason behind mainly lies in the limited effectiveness of our
opinion polarity classifier, which reduces the impact of all features that rely on statement sentiment.
Conversely, b1 fails completely in the out-of-domain task (from squares to circles) with accuracy drops
of up to 41% (on Author c). This indicates a large covariate shift (Shimodaira, 2000) in the distribution
of the baseline features. In contrast, a1, a2, and a4 suffer much less from the domain transfer. Especially
the accuracy of the sentiment flow patterns (a4) remains stable on three of the four datasets and, hence,
provides strong support for our hypothesis. In case of Author c, the SVM based on a4 alone even achieves
a significantly higher accuracy than the SVM based on all features (60.5% as opposed to 50.7%), thus
offering evidence for the decisiveness of the structure of an argumentation. Only on Author d, all four
evaluated feature types similarly fail when trained on hotel reviews with a4 being the worst. Apparently,
the argumentation structure in the texts of Author d differs from the others, which is reflected by the
found sentiment flow patterns and which we therefore finally analyze.
5.3 Insights into Sentiment Flow Patterns
In Figure 9, we plot the three most common sentiment flow patterns in the training set of the ArguAna
TripAdvisor corpus (with self-created annotations) as well as the respective patterns in the movie reviews
561
of Author c and Author d for each possible sentiment score. In total, we found 38 sentiment flow patterns
in the hotel reviews, meaning that a4 consists of 38 features in this case. As depicted in Figure 9(a), they
are constructed from the local sentiment flows of up to 226 texts. One of the 75 patterns of Author c
results from 155 flows, whereas each of the 41 patterns of Author d represents at most 16 flows.
With respect to the depicted sentiment flow patterns, the movie reviews show less clear sentiment but
more changes of local sentiment than the hotel reviews. While there appears to be a certain similarity
in the overall argumentation structure between the hotel reviews and the movie reviews of Author c, two
of the three patterns of Author d contain only little clear sentiment at all, especially in the middle parts.
The disparity of the Author d dataset is additionally emphasized by the different proportions of opinions
in the evaluated text corpora. In particular, 79.7% of all statements in the ArguAna TripAdvisor corpus
are opinions, but only 36.5% of the sentences of Author d are classified as subjective. The proportions
of the three other movie datasets at least range between 58.4% and 66.5%. These numbers also serve as
a general explanation for the limited accuracy of a1, a2, and a4 in the movie domain.
A solution to achieve higher accuracy and to further improve the domain robustness of the structure-
based argumentation features might be to construct flow patterns from the subjective statements or from
the changes of local sentiments only, which we leave for future work. Here, we conclude that our novel
feature type a4 does not yet solve the domain dependency problem, but it still defines a promising step
towards a more domain-robust sentiment analysis.
6 Conclusion
Text classification tasks like sentiment analysis are domain-dependent and tend to be hard on texts that
comprise an involved argumentation, such as reviews. To classify the sentiment scores of reviews, we
model a review?s text as a composition of local sentiment, discourse relations, and domain concepts.
Based on this shallow model of argumentation, we combine existing sentiment analysis approaches with
novel features that capture the abstract overall argumentation structure of reviews irrespective of their
domain and their linguistic style. In particular, we learn common sequences of local sentiment in re-
views through clustering in order to then compare a given review to each of these learned sentiment flow
patterns. Our evaluation on hotel and movie reviews suggests that the sentiment flow patterns generalize
well across domains and it indicates the effectiveness of modeling argumentation. In addition, both the
patterns and our model help to explain sentiment scoring results, as exemplified.
Due to errors in the preprocessing of texts, some obtained effectiveness gains are rather small, though.
In the future, we seek to develop features that are less affected from preprocessing. A promising variation
in this respect is e.g. to learn patterns based on the changes of local sentiment only. Also, we plan to
analyze common sequences of discourse relations in order to capture the argumentation structure of a
text in an even more domain- and language-independent manner. By that, we contribute to the general
research on robust and explainable text classification. As outlined in Section 3, many text classification
tasks can profit from modeling argumentation. For this purpose, other types of statements, relations, and
domain concepts will be needed as well as, in some cases, a deeper argumentation analysis.
Acknowledgments
This work was funded by the German Federal Ministry of Education and Research (BMBF) under con-
tract number 01IS11016A as part of the project ?ArguAna?, http://www.arguana.com.
References
Maik Anderka, Benno Stein, and Nedim Lipka. 2012. Predicting Quality Flaws in User-generated Content: The
Case of Wikipedia. In Proceedings of the 35th International ACM Conference on Research and Development
in Information Retrieval, pages 981?990.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical
Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh International Conference
on Language Resources and Evaluation, pages 2200?2204.
562
Philippe Besnard and Anthony Hunter. 2008. Elements of Argumentation. The MIT Press.
Elena Cabrio and Serena Villata. 2012. Combining Textual Entailment and Argumentation Theory for Supporting
Online Debates Interactions. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics: Short Papers, pages 208?212.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?27:27.
Semire Dikli. 2006. An Overview of Automated Scoring of Essays. Journal of Technology, Learning, and
Assessment, 5(1).
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artifical Intelligence,
pages 1606?1611.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The
WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10?18.
Bas Heerschop, Frank Goossen, Alexander Hogenboom, Flavius Frasincar, Uzay Kaymak, and Franciska de Jong.
2011. Polarity Analysis of Texts Using Discourse Structure. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Management, pages 1061?1070.
Thorsten Joachims. 2001. A Statistical Learning Model of Text Classification for Support Vector Machines.
In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, pages 128?136.
Daniel Jurafsky and James H. Martin. 2009. Speech and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, 2nd edition.
Todd Kulesza, Simone Stumpf, Weng-Keen Wong, Margaret M. Burnett, Stephen Perona, Andrew Ko, and Ian
Oberst. 2011. Why-oriented End-user Debugging of Naive Bayes Text Classification. ACM Transactions on
Interactive Intelligent Systems, 1(1):2:1?2:31.
Angeliki Lazaridou, Ivan Titov, and Caroline Sporleder. 2013. A Bayesian Model for Joint Unsupervised Induc-
tion of Sentiment, Aspect and Discourse Representations. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 1630?1639.
Brian Y. Lim and Anind K. Dey. 2009. Assessing Demand for Intelligibility in Context-aware Applications. In
Proceedings of the 11th International Conference on Ubiquitous Computing, pages 195?204.
William C. Mann and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of
Text Organization. Text, 8(3):243?281.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Yi Mao and Guy Lebanon. 2007. Isotonic Conditional Random Fields and Local Sentiment Flow. Advances in
Neural Information Processing Systems, 19:961?968.
Raquel Mochales and Marie-Francine Moens. 2011. Argumentation Mining. Artificial Intelligence and Law,
19(1):1?22.
Subhabrata Mukherjee and Pushpak Bhattacharyya. 2012. Sentiment Analysis in Twitter with Lightweight Dis-
course Analysis. In Proceedings of the 24th International Conference on Computational Linguistics, pages
1847?1864.
Bo Pang and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity. In Proceedings
of 42th Annual Meeting of the Association for Computational Linguistics, pages 271?278.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with
Respect to Rating Scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics, pages 115?124.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs Up?: Sentiment Classification Using Ma-
chine Learning Techniques. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Lan-
guage Processing - Volume 10, pages 79?86.
563
Emily Pitler and Ani Nenkova. 2008. Revisiting Readability: A Unified Framework for Predicting Text Quality.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186?195.
Martin Potthast, Matthias Hagen, Michael V?olske, and Benno Stein. 2013. Crowdsourcing Interaction Logs
to Understand Text Reuse from the Web. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1212?1221.
Peter Prettenhofer and Benno Stein. 2010. Cross-Language Text Classification using Structural Correspondence
Learning. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics, pages
1118?1127.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010. The Bag-of-Opinions Method for Review Rating
Prediction from Sparse Text Patterns. In Proceedings of the 23rd International Conference on Computational
Linguistics, pages 913?921.
Helmut Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
Hidetoshi Shimodaira. 2000. Improving Predictive Inference under Covariate Shift by Weighting the Log-
Likelihood Function. Journal of Statistical Planning and Inference, 90(2):227?244.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christo-
pher Potts. 2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631?1642.
Efstathios Stamatatos. 2009. A Survey of Modern Authorship Attribution Methods. Journal of American Society
for Information Science and Technology, 60(3):538?556.
Simone Teufel, Advaith Siddharthan, and Colin Batchelor. 2009. Towards Discipline-independent Argumentative
Zoning: Evidence from Chemistry and Computational Linguistics. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing, pages 1493?1502.
Stephen E. Toulmin. 1958. The Uses of Argument. Cambridge University Press.
Maria Paz Garcia Villalba and Patrick Saint-Dizier. 2012. Some Facets of Argument Mining for Opinion Analysis.
In Proceedings of the 2012 Conference on Computational Models of Argument, pages 23?34.
Henning Wachsmuth and Kathrin Bujna. 2011. Back to the Roots of Genres: Text Classification by Language
Function. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages
632?640.
Henning Wachsmuth, Martin Trenkmann, Benno Stein, Gregor Engels, and Tsvetomira Palakarska. 2014. A
Review Corpus for Argumentation Analysis. In Proceedings of the 15th International Conference on Intelligent
Text Processing and Computational Linguistics, pages 115?127.
Douglas Walton and David M. Godden, 2006. Considering Pragma-Dialectics, chapter The Impact of Argumen-
tation on Artificial Intelligence, pages 287?299. Erlbaum.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent Aspect Rating Analysis on Review Text Data: A
Rating Regression Approach. In Proceedings of the 16th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 783?792.
Qiong Wu, Songbo Tan, Miyi Duan, and Xueqi Cheng. 2010. A Two-Stage Algorithm for Domain Adaptation
with Application to Sentiment Transfer Problems. In Information Retrieval Technology, volume 6458 of Lecture
Notes in Computer Science, pages 443?453.
564
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 962?973, Dublin, Ireland, August 23-29 2014.
Improving Cloze Test Performance of Language Learners Using Web N-Grams
Martin Potthast Matthias Hagen Anna Beyer Benno Stein
Bauhaus-Universit?t Weimar, Germany
<first name>.<last name>@uni-weimar.de
Abstract
We study the effectiveness of search engines for common usage, a new category of search engines
that exploit n-gram frequencies on the web to measure the commonness of a formulation, and that
allow their users to submit wildcard queries about formulation uncertainties often encountered in
the process of writing. These search engines help to resolve questions on common prepositions
following verbs, common synonyms in given contexts, and word order difficulties, to name only
a few. Until now, however, it has never been shown that search engines for common usage have
a positive impact on writing performance.
Our contribution is a large-scale user study with 121 participants using the Netspeak search
engine to shed light on this issue for the first time. Via carefully designed cloze tests we show
that second language learners who have access to a search engine for common usage significantly
and effectively improve their test performance as opposed to not using them.
1 Introduction
When writing texts in a second language, uncertainties on specific formulations regularly come up. Even
experienced second language writers may sometimes be in doubt about the preposition following a verb
or what word order to choose. In this paper, we study search engines for common usage (usage search
engines, for short) that aim at assisting second language writers to cope with their uncertainties. These
search engines allow for phrasal queries that include wildcards at positions where a user is not sure what
to write. The search results typically consist of a list of phrases matching the query?s expression?the
wildcards filled with formulations. The returned phrases are ranked by their commonness of being used in
everyday writing, where a phrase?s commonness is estimated by its occurrence frequency in a collection
of web n-grams. The occurrence frequencies are usually not hidden from the user but displayed alongside
each phrase, either implicitly or explicitly. This way, the users of usage search engines have a way of
judging whether a phrase is commonly used by others. Figure 1 (left) shows an example search result.
Target audience of usage search engines is language learners who have mastered basic vocabulary
and grammar but whose language proficiency in terms of their feeling for language usage is still worse
than that of a native speaker. Until recently, there has been hardly any technological support for them,
so they could only resort to studying abstract style guides, consuming foreign language media, and
language study travels in order to improve their usage skills. Today, three public usage search engines
are available. The first one, called Netspeak (Stein et al., 2010), is developed at our group since 2008.
It was followed by PhraseUp and Linggle (Boisson et al., 2013), which have been released in 2011
and 2013.
1
Moreover, there is Google?s N-Gram Viewer prototype (Michel et al., 2011), which has a
different purpose and target audience but visualizes n-gram usage over time.
All of these search engines provide a way to quantify the commonness of a phrase and thus have the
potential to become important tools for second language learners. That is, if they work as advertised.
Until now, it has not at all been clear whether writers can actually benefit from the information distilled
from analyzing n-gram occurrence frequencies, or whether they are easily misled, for example, by noisy
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Netspeak is freely available at www.netspeak.org, PhraseUp at www.phraseup.com, and Linggle at www.linggle.com.
962
Figure 1: Netspeak?s two alternative interfaces: search results can either be displayed as textual ranked
list of phrases alongside frequencies (left), or as WordGraph visualization (right) (Riehmann et al., 2011),
where the frequencies determine various aspects of the visualization. The WordGraph is particularly
suited to handling multiple wildcards per query. The participants of our user study used primarily the
textual interface, since they did not require more than one or two wildcards for solving the cloze tests.
data. Our contribution is to shed light on this issue for the first time and to conduct a large-scale user study
with 121 language learners aged 14?18, measuring their performance when using our Netspeak search
engine to solve cloze tests. The study ascertains the positive impact of Netspeak and by extension, usage
search engines in general; moreover, it shows the low barrier to entry of Netspeak?s user interface.
The paper is organized as follows: after a detailed discussion of related work in Section 2, Netspeak?s
retrieval engine is formally described in Section 3 as background for the design of our user study and as
an example of how such search engines work internally. Section 4 reports on our user study and provides
a statistical analysis of our findings. The paper closes with a conclusion and an outlook into future work.
2 Related Work
Carrying out research and development on usage search engines is an interdisciplinary effort that requires
expertise from information retrieval, information visualization and interface design, as well as domain
knowledge from computer linguistics. Therefore, we divide our review of related work into four parts:
(1) existing search engines and web services, (2) retrieval engines and wildcard search from the perspec-
tive of information retrieval, (3) search result visualization, and, (4) writing support systems dedicated
to second language writers.
2.1 Public Search Engines and Web Services
There are currently three public search engines and one public prototype that fall into the category of
search engines for common usage, namely Netspeak (Stein et al., 2010), PhraseUp, Linggle (Boisson
et al., 2013), and the Google N-Gram Viewer (Michel et al., 2011). All of them index large n-gram
corpora, and their search interfaces are primarily dedicated to returning results that allow their users to
judge the commonness of a phrase compared to alternative phrases. We distinguish the former three
search engines from the latter mainly by its target audience. While the former target average web users,
the latter targets professional linguists and humanities researchers. To the best of our knowledge, our
paper is the first to investigate the effectiveness of such search engines for the use case of assisting
writers, thereby underpinning these efforts.
Moreover, a number of other linguistic search engines are available, such as WebCorp Live (Kehoe
and Renouf, 2002), WebAsCorpus (Fletcher, 2007), and the Linguist?s Search Engine (Resnik and Elkiss,
2005). These search engines cannot be readily used for usage search as defined above, since they work
more like concordancers in that they only retrieve usage examples and present them in context, disregard-
ing usage commonness. Again, their target audience is professional linguists rather than laymen users,
let alone second language learners. While they may still be applied in the context of language learning,
the search interfaces of these search engines are not sufficiently tailored to this domain.
963
Another category of related web services that are readily available to second language learners include
style and grammar checkers, such as Grammarly, PaperRater, SlickWrite, AfterTheDeadline (Mudge,
2010), the Hemingway App, GrammarBase, etc. From what can be said by analyzing their features, all
of these services are based on a collection of basic style and grammar rules that can be checked automati-
cally with some degree of confidence in their recommendations. However, none of the services we found
make any recommendations with regard to usage commonness, i.e., they do not identify uncommon
formulations or make recommendations for more common ones.
2.2 Information Retrieval Models and Indexes for Wildcard Search
The retrieval models employed by usage search engines are hardly ever discussed in the literature cited
above. One of the few exceptions is Netspeak (Stein et al., 2010), where the retrieval model has been a
contribution in itself since it is tailored specifically to its application domain. For the lack of discussion
of the finer details of how the above search engines work, it can be assumed that they do not employ a
specifically tailored retrieval approach. Nevertheless, when reviewing the information retrieval literature
for retrieval models that support linguistic queries or wildcard queries, a number of sources can be found.
Cafarella et al. (2005, 2007) study indexing methods that are particularly suited to support queries
comprising parts-of-speech as wildcards. They introduce so-called neighborhood indexes whose disk
accesses required to answer a query are on the order of the number of non-wildcard terms in a query.
Rafiei and Li (2009) develop a wildcard search engine that supports linguistically rich wildcards in
order to support information extraction from the web, which employs a preprocessor for queries, and a
postprocessor for search results on top of a traditional web search engine. The approach does not create
a tailored index but translates the wildcard queries into flat queries that can be answered by traditional
search engines. Sekine (2008) explores the trie data structure as an alternative to inverted indexes when
indexing large-scale n-gram corpora. The approach is limited to short n-grams (n < 10) to be feasible,
which can be a strong point in terms of retrieval speed. Netspeak?s retrieval engine is also intentionally
restricted to small values of n, but uses minimal perfect hash functions instead of tries to maximize
retrieval performance.
While all of the aforementioned approaches support shallow linguistic wildcards, or only basic wild-
cards, Tsang and Chawla (2011) propose a method to support regular expressions. Doing so involves
various trade-offs between retrieval performance and index size. Further, a search engine like this may
be only useful to experts, but not second language learners. Again, all of the aforementioned contri-
butions target either professional linguists or they are meant to facilitate automatic usage, instead of
supporting average writers.
2.3 Visualization of Usage Search Results
An important part of every search engine is its user interface. Since usage search engines are still in their
infancy, their user interfaces have not been studied in-depth, so far. As a first attempt to close this gap,
we developed and analyzed two alternative user interfaces for Netspeak in a previous work, one textual
interface and one using a tailored visualization that was specifically developed for usage search engines,
the so-called WordGraph (Riehmann et al., 2011). Figure 1 shows them side-by-side. The textual inter-
face displays search results in the form of a tabular list, where each row lists an n-gram matching the
wildcard query alongside its absolute and relative occurrence frequency. If a query comprises more than
one wildcard, situations arise where this linear ranking of n-grams is insufficient to grasp the true distri-
bution of formulations that may be used instead of the wildcards. The WordGraph therefore visualizes
the search results as a horizontal graph, so that the i-th word of an n-gram is displayed as a node on
the i-th level of the graph. Paths from left to right through the graph correspond to n-grams found in
the result set returned by Netspeak. A user study that investigated the fitness of the WordGraph to serve
as a user interface for specific search tasks found that study participants prefer the WordGraph over the
textual user interface when the number of wildcards increases (Riehmann et al., 2012). The user study
we report on in this paper is based solely on the textual user interface, since most of our cloze tests can
be solved by using one wildcard.
964
2.4 Writing Support for Second Language Learners
?For writers of English as a Second Language (ESL), useful editorial assistance geared to their needs is
surprisingly hard to come by,? and ?[...] there has been remarkably little progress in this area over the last
decade,? observe Brockett et al. (2006) about the state of the art. This is despite the fact that English is
the second language of most people who speak English today.
2
A recent overview of technology to detect
grammatical errors of language learners is given by Leacock et al. (2010), whereas computer feedback for
second language learners is mostly studied within pedagogical research under the label of computer-aided
language learning (CALL). There, classroom systems are being deployed on a small scale to measure
their effects on student learning performance. The development of usage search engines in general, our
Netspeak engine in particular, and the user study contributed in this paper may be considered first steps
toward the development of new, better technologies that specifically target the needs of second language
learners and writers.
3 Netspeak: A Search Engine for Common Usage Based on Web N-Grams
As a background for our user study and as an example of how usage search engines work internally, this
section briefly describes Netspeak and its retrieval engine.
3
The main building block of Netspeak is a
query processor tailored to the following task: given a wildcard query q and a set D of n-grams, retrieve
those n-grams D
q
? D that match the pattern defined by q. To solve this task, we have developed
an index-based wildcard query processor addressing the three steps indexing, retrieval, and filtering, as
illustrated in Figure 2 (middle).
3.1 Query Language
Netspeak utilizes a query language defined by the EBNF grammar shown in Figure 2 (left). A query is a
sequence of literal words and wildcard operators, wherein the literal words must occur in the expression
sought after, while the wildcard operators allow to specify uncertainties. Currently five operators are
supported:
? the question mark (?), which matches exactly one word;
? the asterisk (*), which matches any sequence of words;
? the tilde sign in front of a word (?<word>), which matches any of the word?s synonyms;
? the multiset operator ({<words>}), which matches any ordering of the enumerated words; and,
? the optionset operator ([<words>]), which matches any one word from a list of options.
The textual interface displays the search results for the given query as a ranked list of phrases, ordered
by decreasing absolute and relative occurrence frequencies. This way, the user can find confidence in
choosing a particular phrase by judging both its absolute and relative frequencies. For example, a phrase
may have a low relative frequency but a high absolute frequency, or vice versa, which in both cases
indicates that the phrase is not the worst of all choices. Furthermore, the textual web interface offers
example sentences for each phrase, which are retrieved on demand when clicking on a plus sign next to
a phrase. This allows users who are still in doubt to get an idea of the larger context of a phrase.
3.2 Retrieval Engine
The indexing step is done offline. Let V denote the set of all words found in the n-grams D, and
let D?denote the set of integer references to the storage positions of the n-grams in D on hard disk.
During indexing, an inverted index ? : V ? P(D )? is built that maps each word w ? V to a sorted
list ?(w) ? D ,? where ?(w) is comprised of exactly all references to the n-grams in D that contain w.
2
http://en.wikipedia.org/wiki/English language#Geographical distribution
3
Extended versions of this section can be found in previous publications on Netspeak?s WordGraph visualization (Riehmann
et al., 2011; Riehmann et al., 2012).
965
EBNF grammar of Netspeak?s query language
query = { word | wildcard }
5
1
word = ( [apostrophe] ( letter { alpha } ) ) | ? , ?
letter = ? a ? | ... | ? z ? | ? A ? | ... | ? Z ?
alpha = letter | ? 0 ? | ... | ? 9 ?
apostrophe = ? ? ?
wildcard = ? ? ? | ? * ? | synonyms | multiset | optionset
synonyms = ? ~ ? word
multiset = ?{ ? word { word } ?} ?
optionset = ? [ ? word { word } ? ] ?
Netspeak's retrieval engine
Retrieval Filtering
Inverted
index ?
Web
n-grams D
?
w?q  ?(w) = ?q DqDq'q
Sequential
access
Random
access
Indexing
online
offline
rotate about
around
once
on
the
axis
y
the z
on its
the
its
an
its own
<empty>
a vertical
Frequency
128,176      63.7%
36,615      18.2%
10,390        5.2%
4,091        2.0%
3,941        2.0%
3,323        1.7%
3,110        1.5%
2,574        1.3%
Phrase
i am waiting for
i am waiting to
i am waiting on
i am waiting.
i am waiting,
i am waiting impatiently
i am waiting ur
i am waiting until
Figure 2: Netspeak at a glance (Riehmann et al., 2012): the left table shows Netspeak?s query language
as an EBNF grammar, the middle figure overviews its retrieval engine, and the right figure shows an
example of search results as shown to its users. Given a query q, the intersection of relevant postlists
yields a tentative postlist ?
q
, which then is filtered and presented as a ranked list. The index ? exploits
essential characteristics that are known a-priori about possible queries and the n-gram set D.
The list ?(w) is referred to as posting list or postlist. Since D is invariant, ? can be implemented as
an external hash table with O(1)-access to ?(w). For ? being space-optimal, a minimal perfect hash
function based on the CHD algorithm is employed (Belazzougui et al., 2009).
The two online steps, retrieval and filtering, are taken successively when answering a query q. Within
the retrieval step, a tentative postlist ?
q
=
?
w?q
?(w) is constructed; ?
q
is the complete set of references
to n-grams in D that contain all words in q. The computation of ?
q
is done in increasing order of postlist
length, whereas each ?(w) is read sequentially from hard disk. Within the filtering step, a pattern matcher
is compiled from q, and D
q
is constructed as the set of those n-grams referenced in ?
q
that are accepted
by the pattern matcher. Constructing D
q
requires random hard disk access. Basically, this approach
corresponds to how web search engines retrieve documents for a given keyword query before ranking
them. In what follows, we briefly outline how the search in D is significantly narrowed down.
With an inverted index that also stores specific n-gram information along with the keywords, the
filtering of ?
q
can be avoided. In this regard, we distinguish the queries that can be formulated with
Netspeak?s query language into two classes: fixed-length queries and variable-length queries. A fixed-
length query contains only wildcard operators that represent an a-priori known number of words, while
a variable-length query contains at least one wildcard operator that expands to a variable number of
words. For example, the query fine ? me is a fixed-length query since only 3-grams in D match this
pattern, while the query fine
*
me is a variable-length query since n-grams of length 2, 3, 4, . . . match.
Obviously, fixed-length queries can be answered with less filtering effort than variable-length queries:
simply checking an n-gram?s length suffices to discard many non-matching queries. The query processor
first reformulates a variable-length query into a set of fixed-length queries, which then are processed in
parallel, merging the results.
Moreover, the retrieval engine employs pruning strategies so that only relevant parts of a postlist
are read during retrieval, presuming sorted postlists. Head pruning means to start reading a postlist at
some entry within, without compromising recall. Given a query q, let ? denote an upper bound for the
frequencies of the n-grams in q?s result set D
q
, i.e., d ? D
q
implies f(d) ? ? . Obviously, in all postlists
that are involved within the construction of D
q
, all entries whose n-gram frequencies are above ? can
safely be skipped, whereas ? is determined in a preprocessing step as the lowest occurrence frequency of
a sub-sequence of q that does not include wildcards. Up to this point, the retrieval of n-grams matching
a query q is exact?but, not all n-grams that match a query are of equal importance. We consider this
fact by applying tail pruning for postlists that are too long to be read at once into main memory. As a
consequence, less frequent n-grams that might match a given query can be missed.
3.3 The Web n-Gram Collection
To provide relevant suggestions, a wide cross-section of written text on the web is required. Currently,
Netspeak indexes the Google n-gram corpus ?Web 1T 5-gram Version 1? (Brants and Franz, 2006),
966
which consists of 42 GB of phrases up to a length of n = 5 words along with their occurrence frequencies
on the web in 2006. This corpus has been compiled from approximately 1 trillion words extracted from
the English portion of the web, totaling more than 3 billion n-grams. Two post-processing steps were
applied: case reduction and vocabulary filtering. For the latter, a white list vocabulary V was compiled
and only n-grams whose words appear in V were retained. The vocabulary V consists of the words
found in the Wiktionary and various other dictionaries, complemented by words from the 1-gram portion
of the Google corpus whose occurrence frequency exceeds 10 000. After post-processing, the size of the
corpus has been reduced by about 54%.
3.4 Retrieval Performance in Practice and Public Availability
In practice, the described techniques enable Netspeak to provide search results at a speed similar to
modern web search engines. Results are usually returned within a couple of milliseconds. Whenever a
user stops typing for more than 300 milliseconds, the current input is submitted as an ?instant? query
without need for a click. That way, the ?search experience? with Netspeak is similar to what users expect
from web search engines.
Netspeak is freely available online and has about 300 distinct users on a working day who submit about
2500 queries (half the workload on weekends). Most of its users are returning users. From their feedback
and from our own experience, we know that Netspeak helps to resolve uncertainties on formulations in
the daily process of writing papers, proposals, etc. However, in the following section we attempt to
capture Netspeak?s effectiveness in a controlled user study.
4 User Study on the Effectiveness of Usage Search Engines
It is generally assumed that usage search engines are useful, say, that they provide valuable feedback that
leads to improved writing. To empirically confirm this ?usefulness? assumption, we conduct systematic
tests with experienced language learners and analyze whether a usage search engine enables them to
improve their writing. We choose Netspeak as a representative of usage search engines for our study.
Our study?s underlying rationale is to model the use case of usage search engines by solving cloze
tests. In a cloze test, a word or a phrase is removed from a sentence and the participant has to replace
the missing words. Although we followed standard procedures on constructing cloze tests (Sachs et al.,
1997), it should be noted that our usage of cloze tests is not as originally intended (Taylor, 1953). We
do not assess a language learner?s reading skills, but use the cloze test to model word choice, which
resembles the use case of usage search engines very well. For each participant, we provide two different
cloze test questionnaires. The first has to be solved without any help, whereas for the second, participants
are allowed to use the search engine. Besides evaluating the answers, we also analyze the submitted
search queries.
4.1 Experiment 1: General Usage, Average Learners
In the first experiment, we examine whether the search engine in general can support users in resolving
uncertainties on formulations modeled by cloze tests. Our hypothesis is that using a usage search engine
helps to improve a human?s performance in such tests.
Experimental Design To test our hypothesis, we conduct an empirical study with a within-subjects
design (Lazar et al., 2010). This means that our participants are exposed to a cloze test without the help
of a search engine and then to another cloze test where our chosen usage search engine is allowed.
The to-be-solved cloze tests are carefully constructed under the guidance of a university-level English
teacher who is a native English speaker. From several language learner textbooks, we selected questions
in order to have an equal mix of two easy, four medium, and three hard questions for two different cloze
test questionnaires A and B (see Appendix A and B).
In order to have objectively comparable test cases, the English teacher provided four possible answers
for each of the nine questions from test A and B, from which participants had to choose one in each case.
This way, the participants do not have to rely on their subjective own vocabulary knowledge.
967
Table 1: Results of our user study on the impact of usage search engines on language learners.
Experiment Question Questions answered
difficulty manually with search engine available
but not used and used
X ? ? sum X ? ? X ? ? sum
easy 17 41 0 58 7 2 1 42 6 0 58
Average medium 61 100 3 164 25 16 1 88 34 0 164
Learners hard 37 72 2 111 4 22 2 18 62 1 111
all 115 213 5 333 36 40 4 149 102 1 333
Highly easy 11 5 0 16 10 1 0 4 1 0 16
Experienced medium 27 17 0 44 24 2 0 14 3 1 44
Learners hard 18 12 0 30 8 8 0 4 10 0 30
all 56 34 0 90 42 11 0 22 14 1 90
easy 147 29 1 177 28 2 1 135 11 0 177
Specific medium 117 57 3 177 20 6 1 123 24 3 177
Operators hard 135 40 2 177 31 5 2 130 18 1 177
all 399 126 6 531 79 13 4 378 53 4 531
Search engine not used Search engine used
Experiment Search engine used vs not used
p-value effect size
Average Learners 0.0000 0.73 large
Highly Exp. Learners 0.7030 0.12 small
Specific Operators 0.0000 0.58 large
In the left table,Xdenotes correct answers,
? denotes wrong answers, and ? denotes
unanswered questions.
To evaluate the statistical significance and the
effect size, we distinguished cloze test answers
for the conditions ?Search engine not used? and
?Search engine used? in the left table.
The brackets below the bottom row of the left
table indicate which cases fall under what
condition.
The English teacher first chose the questions independent of knowing the indexed n-grams of the
search engine. In a ?postprocessing? step, the chosen answers for the questions are checked for existence
in the n-gram vocabulary of the search engine. This always was the case, although sometimes the queries
required to retrieve them were different from the exact context around the cloze test?s missing word. This
check ensured that there was a chance of answering each individual question in the cloze tests with the
search engine.
During the experiment, the use or non-use of the search engine is the independent variable. The
dependent variable is the number of correct answers per questionnaire. There also are confounding
variables like whether our engine really was used when it was allowed, the time needed to type queries,
or the different numbers of answered questions with and without the search engine. We will further
elaborate on how we deal with these variables in the following description of the experimental process.
Experimental Process From three different local high schools, 43 German pupils (23 female, 20 male;
mean age 16.2, SD = 1.2) with five or more years of English courses participated in six groups. None
of the participants had any previous experience with any usage search engine.
When a group arrived in our lab, they were randomly assigned to a lab seat; questionnaire A or B
were distributed ensuring that neighboring participants had a different question set. This way, the test
distribution was random and the participants could not collaborate (which was also ensured by their
accompanying ?watchdog? teachers). After seven minutes, the first questionnaires were collected and
a short five minute introduction to the search engine and its operator set was given. To ensure that
the pupils really followed the introduction, we provided the chance of winning small prices based on
correctly answering a question on the underlying technique of usage search engines?the index?in an
exit questionnaire. After that, each participant had to solve the opposite questionnaire (A when the first
was B, and vice versa) but was allowed to use the search engine this time. In pilot studies, we noticed
that pupils of that age often need a lot of time for typing their search queries on a standard keyboard.
Thus, we allowed 10 minutes for the second questionnaire. This confounding variable of different timing
for the questionnaires could not be avoided. Otherwise, most participants would not have had the chance
to complete all questions. In order to check whether our participants actually used the search engine, we
logged their querying behavior and manually identified the questions which they had answered without
using the search engine.
Results and Discussion Since not all participants answered all questions for both cloze tests, we ex-
cluded the six participants from the following analyses, who had a difference of more than one between
the number of answered questions for either test.
The aggregated numbers on questionnaire performance for the remaining 37 individuals are given in
the first block of rows of Table 1 (?Average Learners?). Note that the ratio of correct vs. incorrect answers
goes up when the search engine was used: on average, an individual answered two more questions
correctly. Especially interesting is that the short five minute introduction was sufficient for that effect
968
which shows the strength of the textual interface. To statistically estimate the per-individual effect, we
compare the ratio of correct answers among all answers when the search engine was used to the ratio
when it was not used (note that this includes the questions where the engine was allowed but was not used;
i.e., columns ?manually? and ?but not used? in Table 1). According to the Shapiro-Wilk test (Razali and
Wah, 2011), the individual participants? ratios are not normally distributed for either condition (engine
used vs. not used) such that we choose a non-parametric significance test (Lazar et al., 2010). For our
within-subjects design with ratio data and two to-be-compared samples, the Wilcoxon signed rank test
is known as a suitable significance test (Lazar et al., 2010). For the 37 participants? ratios we get a p-
value below 0.001 and thus can reject the null hypothesis that the ratios? distributions are equal. Further
estimating the effect size for the Wilcoxon signed rank test, we obtain a value of 0.73 which corresponds
to a large effect (Cohen, 1988; Fritz et al., 2012). This result supports our prediction that the search
engine can help resolve writing uncertainties.
We also studied the query logs of our participants. Per cloze test question, they submitted 4?5 queries
with 2?3 terms on average (a wildcard is counted as a term). The last query in each such ?search session?
for a single question typically was 3?4 terms long. Almost all participants only used the ?-operator and
most participants chose the strategy of querying with context before and after the operator. Having only
context before or only after the operator are less successful strategies with higher error ratios.
4.2 Experiment 2: General Usage, Highly Experienced Learners
In our neighborhood, there also is an international high school, where German pupils have all their
classes taught in English. Obviously, such pupils have a much higher experience speaking and writing
English than our participants from Experiment 1. For a second experiment, we invited pupils from the
international school to our lab. Our hypothesis is that the pupils from the international school will have
to use the search engine less frequently but still can benefit from it for individual questions.
Experimental Design and Process We used the same questionnaires, time constraints, and logging
strategies as in Experiment 1. From the international school, 12 German pupils (7 female, 5 male; mean
age 16.5, SD = 0.7) participated in two groups. These pupils are taught all their courses in English
for five and more years. None of them had any previous experience with usage search engines. The
experimental process was as in Experiment 1.
Results and Discussion Again, not all participants answered all questions for both cloze tests; we ex-
cluded the two participants from the following analyses, who had a difference of more than one between
the number of answered questions for either test.
The aggregated numbers on questionnaire performance for the remaining 10 individuals are given in
the second block of rows of Table 1 (?Highly Experienced Learners?). As expected, the highly experi-
enced pupils used the search engine very rarely. This is not too surprising since our questionnaires were
designed with an average German pupil in mind; many questions seemed too easy to the internationals
which they also indicated in their exit questionnaires. Still, on a per-question basis, for the medium and
difficult questions where the pupils used the search engine, they slightly improved their performance.
However, the sample and the effect size are too small to draw any reliable conclusions.
The experiment shows that the highly experienced pupils indeed did not use our engine often. How-
ever, the predicted benefit for them cannot be confirmed from our small sample. It is thus an interesting
open task to conduct a larger study with highly experienced users and more difficult questions.
4.3 Experiment 3: Specific Operators, Average Learners
Our first experiment revealed that most participants used the ?-operator to solve the tasks. We thus
designed a third experiment specifically targeted at the options, synonyms, and word-order operators of
our Netspeak search engine. Our hypothesis is that each individual operator helps improve a human?s
performance in cloze tests targeted at the individual operator.
Experimental Design As in Experiment 1, we asked the university-level English teacher to design
two cloze test questionnaires (see Appendix C and D); for each operator with an easy, a medium, and
969
a hard question. Here, the questions for the option operator are of a similar kind as the questions from
Experiment 1. Four alternatives are given, but the participants are asked to use the option operator [] and
not the ?-operator. For synonyms, a complete sentence is given and for a specified word, the best among
four given potential synonyms is requested. As for the word order operator, a two-word phrase is missing
from the sentence and the two different word orders are provided as options. Like in Experiment 1 and 2,
the explicit answer options ensure that the test is objective and not subjective. In a second development
step, the questions were checked for solvability using the search engine just like in Experiment 1.
Experimental Process From three different local schools, 66 pupils (45 female, 21 male; mean
age 15.9, SD = 1.4) participated in six groups. None of the pupils participated in Experiment 1 or 2 nor
had they any previous experience with usage search engines. These pupils have learned English in their
schools for at least five years. The schedule was similar to Experiment 1 with an emphasis on the three
tested operators in the introductory explanations on Netspeak. In the questionnaires, the pupils were
asked to use only the specific operator for the respective queries. Logging their queries, we are able to
exclude solutions obtained by using a not-allowed operator.
Results and Discussion Again, not all participants answered all questions for both cloze tests; we
excluded the seven participants from the following analyses, who had a difference of more than one
between the number of answered questions for either test.
The aggregated numbers on questionnaire performance for the remaining 59 individuals are given in
the third block of rows of Table 1 (?Specific Operators?). Note that the ratio of correct vs. incorrect
answers goes up when the search engine was used: one to two more questions correctly answered on
average. As in Experiment 1, the short five minute introduction is sufficient for that effect which shows
the strength of our interface. To statistically estimate the per-individual effect, we compare the ratio
of correct answers among all answers when the search engine was used to the ratio when it was not
used (note that this includes the questions where the engine was allowed but was not used; i.e., columns
?manually? and ?but not used? in Table 1). For the 59 participants? ratios, we get a p-value below 0.001
and thus can reject the null hypothesis that the ratios? distributions are equal. Further, estimating the
effect size for the Wilcoxon signed rank test, we obtain a value of 0.58 which corresponds to a large
effect (Cohen, 1988; Fritz et al., 2012). Again, the result supports our prediction that usage search
engines can help resolve writing uncertainties.
However, a deeper analysis reveals that the large effect is due to the synonym operator. Only for
that operator, a statistically significant performance difference and a large effect size can be shown. For
the other two operators, the null hypothesis of no performance difference cannot be rejected. This is
in line with the exit questionnaire findings, where the pupils reported the synonym operator to be very
helpful while the other questions were perceived as rather easy. In the query log analyses, we found that
context before and after the wildcard had a similarly positive effect as before and was generally better
than adding context only before the wildcard.
5 Conclusion and Future Work
Search engines for common usage have the potential to become an important tool for second language
writers and learners. The possibility to check one?s language against what is commonly written forms a
unique opportunity to improve one?s writing on-the-fly. Such information has not been available at scale
so far. Our user study shows that usage search engines can indeed help second language writers solve
uncertainties about formulations. Modeling writing uncertainties by carefully designed cloze tests, we
are able to show a significant improvement when experienced language learners use the search engine.
Highly experienced language learners represented by our study participants from an international
school, however, did not use the search engine often enough to draw meaningful conclusions. This
can probably be attributed to the fact that the cloze tests were not tailored to their level of language pro-
ficiency. Therefore, the question of whether also highly experienced writers and learners, or even native
speakers, can benefit from such search engines remains open and is left for future work.
Another missing piece in determining the effectiveness of usage search engines is whether their users
970
actually learn something while using them, or whether users frequently submit the same or similar queries
again and again. Our user study was not designed to answer this question, since our participants were
only around for about 30 minutes for organizational reasons. Even measuring effects on short-term
memory is rendered infeasible in this time frame. A longitudinal study would be ideal, in this case, but
we also see an exciting, data-driven way to approach this. By analyzing the query logs of Netspeak,
which is currently being used hundreds of times per day, we can track returning users. We can then study
their online search behavior to determine if and how often they return to submit similar queries, which
allows us to draw conclusions about their learning success. More generally, the query logs of usage
search engines may form a unique opportunity to observe language learners ?in the wild? as opposed to
the laboratory.
Finally, regarding the user interface of usage search engines, our user study has revealed ways to
improve them. For example, the interface must be optimized for faster typing (especially on mobile de-
vices) as we observed that the pupils were not adept to entering special characters on standard keyboards,
which resulted in slow typing speed. Besides this, our user study also showed that the current state of
Netspeak?s textual user interface as well as the simplified wildcard query language is easy enough to
be understood in less than a minute by any newcomer, which demonstrates the low barrier to entry that
search engines for common usage have right now.
Acknowledgements
We thank the anonymous participants of our user study as well as Tim Gollub, Martin Trenkmann,
Michael V?lske, Howard Atkinson, Johannes Kiesel, Matthias Busse, and Alexander Herr for their help
in organizing the user study.
References
Djamal Belazzougui, Fabiano C. Botelho, and Martin Dietzfelbinger. 2009. Hash, Displace, and Compress. In
Proceedings of ESA 2009, pages 682?693.
Joanne Boisson, Ting-Hui Kao, Jian-Cheng Wu, Tzu-Hsi Yen, and Jason S. Chang. 2013. Linggle: A Web-scale
Linguistic Search Engine for Words in Context. In Proceedings of ACL 2013 (Demos), pages 139?144.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium LDC2006T13.
Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL Errors Using Phrasal SMT Tech-
niques. In Proceedings of ACL 2006, pages 249?256.
Michael J. Cafarella and Oren Etzioni. 2005. A Search Engine for Natural Language Applications. In Proceedings
of WWW 2005, pages 442?452.
Michael J. Cafarella, Christopher Re, Dan Suciu, and Oren Etzioni. 2007. Structured Querying of Web Text Data:
A Technical Challenge. In Proceedings of CIDR 2007, pages 225?234.
Jacob Cohen. 1988. Statistical Power Analysis for the Behavioral Sciences. Psychology Press.
William H. Fletcher. 2007. Implementing a BNC-Compare-able Web Corpus. In Proceedings of the 3rd Web as
Corpus Workshop, pages 43?56.
Catherine O. Fritz, Peter E. Morris, and Jennifer J. Richler. 2012. Effect Size Estimates: Current Use, Calculations,
and Interpretation. Journal of Experimental Psychology: General, 141(1):2.
Andrew Kehoe and Antoinette Renouf. 2002. WebCorp: Applying the Web to Linguistics and Linguistics to the
Web. In Proceedings of WWW 2002 (Posters).
Jonathan Lazar, Jinjuan Heidi Feng, and Harry Hochheiser. 2010. Research Methods in Human-Computer Inter-
action. Wiley Publishing.
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error
Detection for Language Learners. Morgan and Claypool Publishers.
Jean-Baptiste Michel, Yuan K. Shen, Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team,
Joseph P. Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak,
and Erez L. Aiden. 2011. Quantitative Analysis of Culture Using Millions of Digitized Books. Science,
331(6014):176?182.
971
Raphael Mudge. 2010. The Design of a Proofreading Software Service. In Proceedings of HLT 2010 Workshop
on Computational Linguistics and Writing, pages 24?32.
Davood Rafiei and Haobin Li. 2009. Data Extraction from the Web Using Wild Card Queries. In Proceedings of
CIKM 2009, pages 1939?1942.
Nornadiah Mohd Razali and Yap Bee Wah. 2011. Power Comparisons of Shapiro-Wilk, Kolmogorov-Smirnov,
Lilliefors and Anderson-Darling Tests. Journal of Statistical Modeling and Analytics, 2(1):21?33.
Philip Resnik and Aaron Elkiss. 2005. The Linguist?s Search Engine: An Overview. In Proceedings of ACL 2005
(Posters and Demos), pages 33?36.
Patrick Riehmann, Henning Gruendl, Bernd Froehlich, Martin Potthast, Martin Trenkmann, and Benno Stein.
2011. The NETSPEAK WORDGRAPH: Visualizing Keywords in Context. In Proceedings of PacificVis 2011,
pages 123?130.
Patrick Riehmann, Henning Gruendl, Martin Potthast, Martin Trenkmann, Benno Stein, and Bernd Froehlich.
2012. WORDGRAPH: Keyword-in-Context Visualization for NETSPEAK?s Wildcard Search. IEEE Transac-
tions on Visualization and Computer Graphics, 18(9):1411?1423.
J. Sachs, P. Tung, and R.Y.H. Lam. 1997. How to Construct a Cloze Test: Lessons from Testing Measurement
Theory Models. Perspectives, 9:145?160.
Satoshi Sekine. 2008. A Linguistic Knowledge Discovery Tool: Very Large N -gram Database Search with
Arbitrary Wildcards. In Proceedings of COLING 2008 (Demos), pages 181?184.
Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving Customary Web Language to Assist
Writers. In Proceedings of ECIR 2010, pages 631?635.
W. L. Taylor. 1953. Cloze Procedure: A New Tool for Measuring Readability. Journalism Quarterly, 30:415?433.
Dominic Tsang and Sanjay Chawla. 2011. A Robust Index for Regular Expression Queries. In Proceedings of
CIKM 2011, pages 2365?2368.
Appendix
A Questionnaire A from Experiments 1 and 2
1. I really prefer just anything watching television.
? against X to ? about ? on
2. Has Tony?s new book yet?
X come out ? published ? developed ? drawn up
3. If this plan off, I promise you you?ll get the credit for it.
? lets ? goes ? gets X comes
4. Helen had great admiration her history teacher.
? in ? to X for ? on
5. I just couldn?t over how well the team played!
X get ? turn ? make ? put
6. The problem stems the government?s lack of action.
? out X from ? under ? for
7. It?s too late to phone Jill at work, at any .
? case ? time ? situation X rate
8. I?m afraid I?m not very good children.
? about ? for X with ? at
9. We are no obligation to change goods which were not purchased here.
? with X under ? to ? at
B Questionnaire B from Experiments 1 and 2
1. Don?t worry about the lunch. I?ll to it.
? look ? prepare ? care X see
2. I am afraid that these regulations have to be with.
? provided X complied ? faced ? met
3. Our thoughts on our four missing colleagues.
? based X centred ? laid ? depended
4. Carol doesn?t have a very good relationship her mother.
X with ? at ? for ? to
972
5. It seems to be your boss who is fault in this case.
? under ? with X at ? for
6. Being rich doesn?t count much on a desert island.
? on ? to ? of X for
7. The policeman me off with a warning as it was Christmas.
? sent ? gave X let ? set
8. Tina is an authority Byzantine architecture.
X on ? for ? with ? in
9. I was the impression that you liked Indian food.
? at ? with ? of X under
C Questionnaire A from Experiment 3
Choose the word which fits best using the options operator [<words>].
1. If you spend so much money every day, you will out of money before the end of the month.
? pay ? use X run ? take
2. You need to take all your other clothes before you put on your swimming costume.
? down ? away ? out X off
3. I?m afraid I?m not very good history.
? about ? for X at ? with
Choose the best synonym for the underlined word using the synonym operator ?<word>.
4. I love studying geometry the most.
? hate ? absent X enjoy ? difficult
5. My ambition is to become a computer scientist.
? thought ? reward ? study X dream
6. Your action will have serious consequences.
X effects ? events ? reasons ? affects
Choose the correct word order using the word order operator {<words>}.
7. The bird! I?m going to help it!
X poor little ? little poor
8. She was wearing a dress.
? green beautiful X beautiful green
9. I plan on wearing my coat.
X long black ? black long
D Questionnaire B from Experiment 3
Choose the word which fits best using the options operator [<words>].
1. Sometimes Julia speaks very quickly so the other students have to ask her to slow .
X down ? up ? out ? off
2. The missing plane has apparently disappeared without a .
? sign ? news ? word X trace
3. When Gabriel?s credit card stopped, he cut it many small pieces.
? out X into ? apart ? in
Choose the best synonym for the underlined word using the synonym operator ?<word>.
4. I choose to study the differences between alligators and crocodiles.
? make ? buy X prefer ? wash
5. I cannot find my money. Can you get me my billfold?
X wallet ? pocket ? watch ? bag
6. This is a very rough environment for elephants to live in.
X harsh ? abrasive ? coarse ? beneficial
Choose the correct word order using the word order operator {<words>}.
7. She sold the chairs at a yard sale.
? wooden old X old wooden
8. The years were fantastic.
? two first X first two
9. It?s close to the building.
X big blue ? blue big
973
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2018?2029, Dublin, Ireland, August 23-29 2014.
Generating Acrostics via Paraphrasing and Heuristic Search
Benno Stein Matthias Hagen Christof Br
?
autigam
Bauhaus-Universit?at Weimar, Germany
<first name>.<last name>@uni-weimar.de
Abstract
We consider the problem of automatically paraphrasing a text in order to find an equivalent text that
contains a given acrostic. A text contains an acrostic, if the first letters of a range of consecutive
lines form a word or phrase. Our approach turns this paraphrasing task into an optimization
problem: we use various existing and also new paraphrasing techniques as operators applicable to
intermediate versions of a text (e.g., replacing synonyms), and we search for an operator sequence
with minimum text quality loss. The experiments show that many acrostics based on common
English words can be generated in less than a minute. However, we see our main contribution in
the presented technology paradigm: a novel and promising combination of methods from Natural
Language Processing and Artificial Intelligence. The approach naturally generalizes to related
paraphrasing tasks such as shortening or simplifying a given text.
1 Introduction
Given some text, paraphrasing means to rewrite it in order to improve readability or to achieve other
desirable properties while preserving the original meaning (Androutsopoulos and Malakasiotis, 2010).
The paper in hand focuses on a specific paraphrasing problem: rewriting a given text such that it encodes
a given acrostic. A text contains an acrostic if the first letters of a range of consecutive lines form a
word or phrase read from top to bottom. A prominent and very explicit example of former Governor
Schwarzenegger is shown in Figure 1 (see the third and fourth paragraphs). Schwarzenegger himself
characterized the appearance of that acrostic a ?wild coincidence?.
1
However, such a coincidence is
highly unlikely: Using the simplistic assumption that first letters of words are independent of each other
if more than ten words are in between (line length in the Schwarzenegger letter) and calculating with
the relative frequencies of first letters in the British National Corpus (Aston and Burnard, 1998), the
probability for the acrostic in Figure 1 can be estimated at 1.15 ? 10
?12
. Typically, a given text will not
contain a given acrostic but has to be reformulated using different wording or formatting to achieve the
desired effect. Thus we consider the purposeful generation of acrostics a challenging benchmark problem
for paraphrasing technology, which is subject to soft and hard constraints of common language usage.
The paper shows how heuristic search techniques are applied to solve the problem. Different paraphras-
ing techniques are modeled as operators applicable to paraphrased versions of a text. By pruning the
so-formed search space and by employing a huge corpus of text n-grams for the possible operators, we
are able to generate acrostics in given texts. Our algorithmic solution is a novel combination of techniques
from Natural Language Processing and Artificial Intelligence. We consider such combinations as a very
promising research direction (Stein and Curatolo, 2006; Sturtevant et al., 2012), and we point out that
the problem of acrostic generation serves as a serious demonstration object: the presented model along
with the heuristic search approach generalizes easily to other paraphrasing tasks such as text shortening,
improving readability, or e-journalism.
2 Related Work and Problem Definition
Rewriting a given text in order to ?encode? an acrostic is a paraphrasing problem, which in turn is studied
in the domain of computational linguistics and natural language processing. We review relevant literature
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
www.huffingtonpost.com/2009/10/30/schwarzenegger-f-bomb-in_n_340579.html, last accessed: June 12, 2014.
2018
To the Members of the Californian State Assembly:
I am returning Assembly Bill 1178 without my signature.
F or some time now I have lamented the fact that major issues are overlooked while many
u nnecessary bills come to me for consideration. Water reform, prison reform, and health
c are are major issues my Administration has brought to the table, but the Legislature just
k icks the can down the alley.
Y et another legislative year has come and gone without the major reforms Californians
o verwhelmingly deserve. In light of this, and after careful consideration, I believe it is
u nnecessary to sign this measure at this time.
Sincerely,
Arnold Schwarzenegger
Figure 1: Excerpt from a letter of former Governor Arnold Schwarzenegger to the Californian State
Assembly in October 2009. The third and fourth paragraphs contain the acrostic ?F??? You?.
of the topic and highlight techniques that will be employed in our work.
An important branch of the paraphrasing literature focuses on analyses with fixed corpora. Such corpora
typically are parallel in the sense that they contain different formulations of the same facts (Barzilay and
McKeown, 2001; Barzilay and Lee, 2003; Callison-Burch, 2008). These ?facts? can be news articles on
the same event (Clough et al., 2002; Dolan and Brockett, 2005), different translations of a source text to a
target language (Pang et al., 2003), or cross-lingual parallel corpora (Bannard and Callison-Burch, 2005).
As most of the early parallel corpora were constructed manually (especially the judgments of whether a
pair of sentences forms a paraphrase), there are two shortcomings. First, the obtained paraphrases are
usually specific to the domain covered in the corpus (e.g., showbiz news) and often do not generalize well.
Second and probably more severe is the fact that manually building parallel corpora is very expensive,
such that the available ones are rather small: the METER corpus contains only 1717 texts on legal issues
and showbiz (Clough et al., 2002), the MSRP corpus contains only 5801 sentence pairs (Dolan and
Brockett, 2005). Recently, new methods employ machine learning techniques to automatically build larger
paraphrase collections from parallel corpora (Ganitkevitch et al., 2011; Ganitkevitch et al., 2013; Metzler
et al., 2011; Metzler and Hovy, 2011). We include the paraphrase database (Ganitkevitch et al., 2013)?a
database of extracted patterns from such large scale corpora?as one source of potential paraphrases in
our algorithm.
Compared to the large body of literature that ?extracts? paraphrases from (parallel) corpora, there is
relatively little work on automatically paraphrasing a given text. Some of the early generation methods
are based on rules that encode situations wherein a reformulation is possible (Barzilay and Lee, 2003). A
problem with rules is that often the rather complicated patterns extracted from text corpora are hardly
applicable to a given to-be-paraphrased text: manually created corpora are simply too small and machine
generated paraphrasing rules often do not match in a given text. Other early methods use machine
translation (Quirk et al., 2004). However, the need for large and expensive parallel manual translation
corpora cannot be circumvented by using multiple resources (Zhao et al., 2008).
Another branch of paraphrasing methods is based on large thesaurus resources such as WordNet (Fell-
baum, 1998). The idea is to insert synonyms into a text when the context fits (Bolshakov and Gelbukh,
2004; Kauchak and Barzilay, 2006). The most recent approaches are statistics-based (Chevelu et al., 2009;
Chevelu et al., 2010; Zhao et al., 2009; Burrows et al., 2013).
Compared to the existing research, we have a more difficult use case here. Existing paraphrase
generation focuses on sentence paraphrasing, while we have to consider a complete text that has to be
rewritten/reformatted in order to contain a given acrostic. We will employ the above shown state-of-the-art
paraphrasing procedures as ?operators? in our approach. This new problem setting of applying different
operators to a complete text forms a search problem with a huge search space. In order to deal with
this search space, we apply powerful search heuristics from Artificial Intelligence. The combination of
heuristic search with established text level paraphrasing techniques represents a new approach to tackle
problems in computational linguistics. The acrostic generation problem is defined as follows:
2019
ACROSTIC GENERATION
Given: (1) A text T and an acrostic x .
(2) A lower bound l
min
and an upper bound l
max
on the desired line length.
Task: Find a paraphrased version T
?
of T in monospaced font that encodes x in some
of its lines when possible. Each line of T
?
has to meet the length constraints.
3 Modeling Paraphrasing as Search Problem
This section shows how to model paraphrasing in general and ACROSTIC GENERATION in particular as
a search problem (Pearl, 1984). The search space is a universe T of candidate texts for which we can
devise, at least theoretically, a systematic search strategy: if T is finite, each element n ? T is analyzed
exactly once. The elements in T represent states (nodes), and there is a limited and a-priori known set of
possibilities (edges, paraphrasing operators) to get from a node n to an adjacent or successor node n
i
. A
paraphrasing operator ? provides a number of parameters that control its application. Each state n ? T
is considered an acrostic (sub)problem; the dedicated state s ? T represents the original problem while
? ? T is the set of solution nodes that have no problem associated with. The following subsections
will outline important properties of the search space and introduce a suited cost measure to control the
exploration of T .
3.1 Search Space Structure
Solving an instance of ACROSTIC GENERATION under a so-called state-space representation means to
find a path from s, which represents the original text T , to some goal state ? ? ?. The problem of finding
an acrostic consists of tightly connected subproblems (finding subsequences of the acrostic) that cannot
be solved independently of each other. Most puzzles such as Rubik?s cube are of this nature: changing a
decision somewhere on the solution path will affect all subsequent decisions. By contrast, a so-called
problem-reduction representation will exploit the fact that subproblems can be solved independently of
each other. Many tasks of logical reasoning and theorem proving give rise to such a structure: given a set
of axioms, the lemmas required for a proof can be derived independently, which in turn means that the
sought solution (a plan or proof) is ideally represented by a tree.
Searching T under a state-space representation means to unfold a tree whose inner nodes link to
successor nodes that encode alternative decisions; hence these inner nodes are also called OR-nodes. Each
path from s that can be extended towards a goal state ? forms a solution candidate. Similarly, searching
T under a problem-reduction representation also means to unfold a tree?however, the tree?s inner nodes
must be distinguished as AND-nodes and OR-nodes, whereas the successors of an AND-node encode
subproblems all of which have to be solved. A solution candidate then is a tree comprised of (1) the root s,
(2) OR-nodes with a single successor, and (3) AND-nodes with as many successors as subproblems all of
which are characterized by the fact of being extensible towards goal states in ?. Figure 2 contrasts both
search space structures.
Under either representation, OR-graphs as well as AND-OR-graphs, the application of a sequence of
?
?
[T, x] Problem specification
Dead end: node with unsolvable problem
Goal state: no problem associated with node
OR-node AND-node
(a) (b)
Legend:
Solution
candidate
...
n4 n6n5
n1
s
n3n2
[T, x]
[T', y]
[T*]
...
... ? ?
n4 n6n5
n1
s
n2
[T, x]
[Ty , y]
?
...
[Tz , z]
?? [Ty*] *][Tz...
Figure 2: (a) State-space representation (OR-graph) versus (b) Problem-reduction representation (AND-
OR-graph). OR-nodes encode alternative decisions, while AND-nodes decompose a problem into sub-
problems all of which are to be solved.
2020
operators will easily lead to situations where states are revisited?precisely: are generated again. Search
algorithms maintain so-called OPEN- and CLOSED-lists to manage the exploration status of generated
nodes. However, because of the intricate state encoding, which must inform about the effect of all applied
operators from s to an arbitrary node, the exponentially growing number of nodes during search, and the
necessity of efficiently querying these lists, sophisticated data structures such as externalized hash tables
and key value stores are employed. Typically, these data structures are tailored to the problem domain
(here: to paraphrasing), and they model heuristic access strategies to operationalize a probabilistically
controlled trade-off between false positive and true negative answers to state queries.
We have outlined the different search space structures since ACROSTIC GENERATION may show an OR-
graph puzzle nature at first sight: paraphrasing at some position will affect all following text. Interestingly,
there is a limited possibility to introduce ?barriers? in the text, which allows for an AND-OR-graph
modeling and hence for an isolated subproblem treatment. Examples include paraphrasing operators that
do not affect line breaks, or acrostics consisting of several words and thus spanning several paragraphs.
Since in general the underlying linguistic considerations for the construction and maintenance of such
barriers are highly intricate and complex, we capture this structural constraint probabilistically as shown
in Equation (1). The equation models the problem difficulty or effort for acrostic generation, E, and
introduces P
y?z
(|x|), which quantifies the probability for the event that an acrostic x can be treated
independently as two partial acrostics y and z, where x = yz.
E(x) =
?
?
?
e(x) If |x| = 1.
P
y?z
(|x|) ?
(
E(y) + E(z)
)
+ (1? P
y?z
(|x|)) ? E(y) ? E(z) If |x| > 1.
(1)
Remarks. The effort for generating a single-letter acrostic of length 1 is e(x), with e(x) ? 1. Based on
e(x), we recursively model the true effort E(x) for generating an acrostic x = yz as follows: as additive
effort if the generation of the acrostics y and z can be done independently, and as multiplicative effort
otherwise. Observe how P
y?z
controls the search space structure: if P
y?z
= 0 for all partitionings of
x into y and z, one obtains a pure state-space representation for ACROSTIC GENERATION. Similarly,
the other extreme with P
y?z
= 1 results in a series of |x| letter generation problems that can be solved
independently of each other.
As an estimate e?(x) for e(x) we suggest the multiplicative inverse of the occurrence probabilities
of the first letters in the English language, as computed from the British National Corpus (BNC). The
BNC is a 100 million word collection of written and spoken language from a wide range of sources,
designed to represent a wide cross-section of current British English (Aston and Burnard, 1998). The
BNC probabilities vary between 0.115719 for the letter ?s? and 0.00005 for the letter ?z?. As an estimate
for P
y?z
we suggest the N(5, 0.5) distribution to model the paragraph lengths in T or, equivalently, the
number of characters of the (English) words in x. These choices give rise to
?
E(x), the estimated effort for
generating an acrostic x.
?
E(x) is used to assess the (residual) problem complexity and, under a maximum
likelihood approach, models the expected search effort. There is a close relation between the effort
estimate
?
E and the quality estimate
?
Q introduced below, which will be exploited later on, in Equation (3).
3.2 Cost Measure Structure
Cost measures?equivalently: merit measures?form the heart of systematic search strategies and de-
termine whether an acceptable solution of a complex problem can be heuristically constructed within
reasonable time. Here, we refer to general best-first search strategies as well as variants that relax strict
admissibility. As a working example consider the following text T about Alan Turing taken from the
English Wikipedia, where the task is to generate the acrostic x = Turing with l
min
= 55 and l
max
= 60.
Alan Mathison Turing was a British mathematician, logician,
cryptanalyst and computer scientist. He was highly influential in
the development of computer science, giving a formalization of the
concepts of algorithm and computation with the Turing machine,
which can be considered a model of a general purpose computer.
2021
A possible solution T
?
(a paragraph?s last line may be shorter than l
min
) :
T he British mathematician Alan Mathison Turing was also an
u nrivaledlogician,cryptanalystandcomputerscientist.He
r evolutionized the development of computer science, giv-
i ng a formalization of the concepts of algorithm and defi-
n ite computation with the Turing machine, which can be re-
g arded a model of a general purpose computer.
T
?
is of a high quality though it introduces an exaggerating tone, this way violating Wikipedia?s
neutrality standard. Also note that the applied paraphrasing operators vary in their quality, which is rooted
in both the kind and the context of the operators. Table 1 (left) shows a selection of the operators, some of
which are applied in a combined fashion. Section 4 introduces the operators in greater detail.
To further formalize the quantification of a cost measure C or a merit measure Q, we stipulate on the
following properties:
1. The quality of the original text T cannot be improved. Each paraphrasing operator ? introduces
unavoidable deficiencies in T .
2. The overall quality of a solution T
?
depends on the quality of all applied paraphrasing operators.
3. Following readability theory and relevant research, the severity of text deficiencies?here introduced
by a paraphrasing operator ??has a disproportionate impact on the text quality (Meyer, 2003).
4. To render different problems and solutions comparable, the achieved quality of a solution T
?
has to
be normalized.
Equation (2) below shows the basic structure of Q, the proposed, unnormalized merit measure. Its
optimization yields Q
?
. Q
?
(n) assigns to a node n ? T the maximum paraphrasing quality of a text T
?
that contains the partial acrostic associated with n. Likewise, Q
?
(s) characterizes the quality of the
optimum solution for solving ACROSTIC GENERATION.
1
Q
?
(n)
=
?
?
?
0 If n ? ?.
min
i
{
1
q(n, n
i
)
+
1
Q
?
(n
i
)
}
Otherwise.
(2)
Remarks. The state (node) n
i
denotes a direct successor of the state (node) n in the search space T .
Associated with n
i
is a text resulting from the application of a paraphrasing operator ? to the text associated
with n, whereas q(n, n
i
) quantifies the local quality achieved with ?. The measure in Equation (2) is
both of an additive form and formulated as a minimization problem. As shown in the following, it can
be reformulated for a best-first algorithm scheme, ensuring admissibility under a delayed termination
condition. Also note that the merit measure operationalizes the above Property 3 via the harmonic mean
computation. Accordingly, we obtain a normalized overall quality
?
Q
?
given an acrostic x as
?
Q
?
= |x|?Q
?
.
To turn Equation (2) into actionable knowledge, the quality q(n, n
i
) of a paraphrasing operator ? when
moving from n to a successor n
i
needs to be quantified. We employ for q the domain [0; 1], where 0 and 1
encode the worst and best achievable quality respectively. By construction the normalized quality
?
Q
?
will
then lie in the interval [0; 1] as well, thus greatly simplifying the interpretation of the measure.
Table 1 (right) shows values for the local quality of the operators in the Alan Turing example, which
are derived from linguistic quality considerations and the experimental analysis detailed in Section 5. The
comment column argues the linguistic meaningfulness. If we agree on q = 1.0 for the first two lines of
the generated acrostic x = Turing and recursively apply the merit measure defined in Equation (2), we
obtain Q = 0.127 as unnormalized and
?
Q = |x| ?Q = 0.76 as normalized overall quality.
To make Equation (2) applicable as cost estimation heuristic f(n) in a best-first algorithm scheme,
Equation (3) below unravels its recursive structure in the usual way as f(n) = g(n)+h(n). The semantics
is as follows: under an optimistic estimate h(n) (= underestimating costs or overestimating merits) the
2022
Table 1: Left: Paraphrasing operators in the Alan Turing example. Right: Values for the local quality of
the respective operators, which entail the normalized overall quality
?
Q = 0.76 for the example.
Line Operator ? Text? paraphrased text
3 synonym highly influential?revolutionized
4 hyphenation giving? giv-ing
5 tautology computation? defi-nite computation
6 synonym considered? re-garded
q(n, n
i
) Comment
0.9 stylistically well, exaggerating tone
0.6 unexpected hyphen for a short word
0.6 tautology arguable, hyphen unusual
0.7 synonym suited, hyphen acceptable
total cost (the overall quality) for solving ACROSTIC GENERATION via a path along node n is always
larger (smaller) than f(n). In particular, g(n) accumulates the true cost (the achieved quality) for the
partial acrostic via a concrete path s = n
0
, n
1
, . . . , n
k
= n, while h(n) gives an underestimation of the
cost (overestimation of the quality) for the remaining part of the acrostic. Observe that the additive form
of Equation (2) guarantees the parent discarding property (Pearl, 1984), which states that no decision on a
path from n to a goal state ? can change the value for g(n).
A tricky part is the construction of h(n), which, on the one hand, may ensure admissibility, while, on
the other hand, should be as close as possible to the real cost. Here, the measure E(x) for the problem
difficulty from Equation (1) comes into play, which models the problem decomposability and which
informs us about the largest remaining subproblem (= the depth of the deepest remaining OR-graph) when
solving x. Without loss of generality, admissibility is ensured if (a) the probability P
y?z
used in
?
E(x)
is biased towards decomposability, and if (b) we assume that the remaining acrostic x can be solved by
always applying the cheapest (maximum quality-preserving) operator q
max
.
1
?
Q(n)
? ?? ?
f(n)
=
k
?
i=1
1
q(n
i?1
, n
i
)
? ?? ?
g(n)
+ log
K
(
?
E(?(n))
)
?
1
q
max
? ?? ?
h(n)
, where n
0
= s, n
k
= n (3)
Remarks. ?(n) denotes the remaining acrostic x that is associated with node n ? T . The logarithm
base K serves for normalization purposes with regard to the BNC letter frequencies e?(x), |x| = 1,
which are used within
?
E(x) in Equation (1). We define K as the multiplicative inverse of the occurrence
probability of the least frequent letter in the remaining acrostic x = ?(n), which gives rise to the inequality
log
K
(
?
E(x)) ? |x|. This choice entails two properties: (1) it underestimates the remaining acrostic length
and hence ensures the admissibility characteristic of h(n), and, (2) it yields an increasing accuracy of
h(n) when approaching a goal state in ?. Finally, we can substitute 1.0 as an upper bound for q
max
, again
preserving the admissibility of h(n).
Admissibility, i.e., the guarantee of optimality during best-first search, may not be the ultimate goal: if
h(n) underestimates costs (overestimates merits) too rigorously, best-first search degenerates to a kind
of breadth-first search?precisely: to uniform-cost search. Especially if computing power is a scarce
resource, we may be better off with a depth-preferring strategy. Observe that the logarithm base K in
Equation (3) provides us a means to smoothly vary between the two extremes, namely by choosing K
from [K
min
;K
max
], where K
min
(K
max
) specifies the multiplicative inverse of the occurrence probability
of the most (least) frequent letter in the remaining acrostic x = ?(n).
4 Paraphrasing Operators
Most of the following operators used in our heuristic search process employ state-of-the-art linguistic
tools or are based on standard knowledge from Wikipedia. Table 2 shows information about the role of
individual operators in our experiments from Section 5; the table illustrates also the effort for preparing
(offline) and applying (online) the operators.
2023
4.1 Context-Independent Operators
Line break Since we are dealing with text that should spread over several lines, breaking between lines
is one of the most basic operators. Similarly, it is the most efficient operator, and Column 4 of Table 2
illustrates the performance of the others in relation to this operator. Line breaks are possible at the end of
sentences (i.e., a paragraph break), while a line break in between words is only possible if it falls in the
[l
min
; l
max
]-window given by the line length constraints.
Hyphenation Related to line breaks are hyphenations. We re-implemented and employ the standard
T
E
X hyphenation algorithm (Knuth, 1986). Analogous to line breaks, hyphenation is applicable if the line
after hyphenating (and line breaking) has a length in the [l
min
; l
max
]-window.
Function word synonym Specific groups of so-called synsemantic words can often be replaced by
each other without changing a text?s meaning. We have identified 40 such groups from a list of Sequence
Publishing
2
and the Paraphrase Database (Ganitkevitch et al., 2013). Examples are {can, may}, {so, thus,
therefore, consequently, as a result}, and {however, nevertheless, yet}.
Contraction and expansion Some local text changes can be achieved by contracting or expanding for-
mulations like ?he?ll? or ?won?t?. We have identified 240 such pairs from Wikipedia.
3
Other possibilities
are to spell out / contract standard abbreviations and acronyms. We have mined a list of several thousand
such acronyms from the Web.
4
Finally, also small numbers can be spelled out or be written as numerals
(e.g., ?five? instead of ?5?). It is interesting to note that this operator was hardly ever used on successful
paths in our experiments.
Spelling In principle, we want to generate text that is correctly spelled. In certain situations, however, it
can nevertheless be beneficial to introduce some slight mistakes in order to change word lengths or to
generate letters not present in the correctly spelled text. We employ a list of 3 000 common misspellings
mined from Wikipedia
5
(e.g., ?accidently? instead of ?accidentally?). We also include several standard
typos related to computer keyboards (e.g., an ?m? is often typed as an ?n? and vice versa) as well as
phonetic misspellings (e.g., ?f? and ?ph? often sound similar). Since the quality score of wrong spellings
tends to be low, this operator has to be treated with care. Especially at the beginning of words, typos are
less common than within words such that we allow typos only within words.
Wrong hyphenation Similar to wrong spellings is the purposeful choice of a wrong hyphenation. As
with wrong spellings the quality score is typically low. We thus employ this operator very carefully,
avoiding for instance syllables on a new line with just two letters. Analogous to correct hyphenation, the
line length has to be in the [l
min
; l
max
]-window to apply wrong hyphenation. Despite its questionable
quality, this operator is used pretty often in the experiments since it has a very high probability of
?generating? a desired letter.
4.2 Context-Dependent Operators
Synonym For identifying synonyms, WordNet?s synsets (Fellbaum, 1998) is used. Since only a small
subset of the synset members of a to-be-replaced word w is reasonable in the context around w in T ,
we check in the Google n-gram corpus (Brants and Franz, 2006) whether the synonym in fact fits in the
same context. In this regard the public and highly efficient Netspeak API (Stein et al., 2010; Riehmann
et al., 2012) is employed. For example, given ?hello world?, the most frequent phrase with a synonym
for world is ?hello earth?. The Google n-grams are up to five words long, such that at most four context
words can be checked before or after w. Previous studies showed that more context yields higher quality
synonyms (Metzler and Hovy, 2011; Pasca and Dienes, 2005), so that we use at least two words before or
after w. Higher quality scores are achieved if the context is matched before as well as after w.
2
sequencepublishing.com/academic.html\#function-words, last accessed: June 12, 2014
3
en.wikipedia.org/wiki/List_of_English_contractions\#English, last accessed: June 12, 2014,
en.wikipedia.org/wiki/English_auxiliaries_and_contractions, last accessed: June 12, 2014
4
www.acronymfinder.com, last accessed: June 12, 2014
5
en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines, last accessed: June 12, 2014
2024
Table 2: Statistics for the applicability, usage, and effort of single operators. ?Application probability?
reports whether an operator is applicable at all at some node, ?Usage? reports the application probability
on a solution path, ?Effort? reports the (online) application effort as multiple of the fastest operator (the
Line break operator), and ?Offline time? reports the preprocessing time in ms per word before the actual
search is started. All numbers are profiled within the experiment setup described in Section 5.
Paraphrasing operator Application probability Usage probability Effort Offline time
(in %) (in %) (multiple of Line break) (in ms per word)
Line break 16.14 21.13 1.00 0.00
Hyphenation 5.48 9.38 1.22 1.38
Function word synonym 1.43 2.57 1.33 0.01
Contraction and expansion 0.29 0.00 1.93 0.02
Spelling 6.98 1.57 1.18 0.11
Wrong hyphenation 9.53 37.63 1.07 1.38
Synonyms 16.69 2.47 1.66 23.24
Word insertion or deletion 43.46 25.24 2.46 42.75
Average 12.50 12.50 1.49 14.35
Word insertion or deletion Similar to the synonym replacement, the insertion or deletion of short
phrases is handled. For all positions of the given text, the Google n-grams are checked with Netspeak
(see above) for a word w that sufficiently often appears within the context of the text. Similarly, for each
word w in the text, it is checked whether there are sufficiently many n-grams without the word but the
same surrounding context. In both cases, w is a candidate to be inserted or deleted. Given ?hello world?,
the most frequently used intermediate word is ?cruel?, yielding the phrase ?hello cruel world.? Again,
as with synonyms, context size and quality are positively correlated. We thus use at least two words as
context and favor variants that match more context.
4.3 Further Operator Ideas
In pilot experiments, also the three operator ideas discussed below were analyzed. The ideas show
promising results for specific cases, but they easily lead to unexpected text flows due the introduction of
odd sentences or names. The operators require future work to better fit them in the given text?s context,
and they are not employed within the experiments in Section 5.
Tautology It is often possible to introduce entirely new phrases or sentences in a text, which may
confirm a previous sentence or which introduce a (nearly) arbitrary but true statement. We tested a small
list, including among others ?As a matter of fact this is true.? or ?I didn?t know that until now.? However,
due to improper context such tautologies may mess up a text significantly.
Sentence beginning The beginning of a sentence can often be modified without changing its meaning.
Possibilities include the addition of function words like ?in general? or ?actually?, but also the addition
of a prefix like ??someone? said that . . . ? or ??time? ?someone? said that . . . ? where ?someone? is to be
replaced by a person?s name or {I, he, she} depending on the full context of the text (e.g., author?s name
or gender of name mentioned before). The ?time? expression may expand to ?yesterday? or ?last week?,
etc. Especially with the usage of names, a whole bunch of letters can be generated. However, context is
more subtle for this operator compared to the usage of Google n-grams.
Full PPDB The paraphrase database (Ganitkevitch et al., 2013) comes in different sizes and quality
levels. Many synonymity relations for nouns are already covered by WordNet, and function word
replacements are already an operator on their own. Still, the rich variety of the full data set can form a
semantically strong operator. However, in our pilot experiments, the full PPDB patterns often decreased
text quality unacceptably, such that we refrained to use PPDB as a single operator in our experiments.
2025
5 Experimental Evaluation
Goal of the evaluation is to show that our approach is able to efficiently generate acrostics in different
situations. In this regard, we analyze the general success of acrostic generation, the influence of different
operators, and effects on the text quality.
5.1 Experiment Setup
To model different ?use cases? in which acrostics have to be inserted, we use texts of different genres:
newspaper articles, emails, and Wikipedia articles. We sample 50 newspaper articles from the Reuters
Corpus Volume 1 (Lewis et al., 2004), 50 emails from the Enron corpus (Klimt and Yang, 2004), and
50 articles from the English Wikipedia. Each text contains at least 150 words excluding tables and lists.
As target acrostics for all of the above text types, the 25 most common adjectives, nouns, prepositions,
verbs, and 50 other common English words are chosen (in total 150 words).
6
This scenario reflects the
inclusion of arbitrary words. Other target acrostics are formed by the 100 most common male and female
first names from the US (in total 200 words).
7
This models the standard poetry usage of acrostics where
often a writer?s name is encoded. For all input texts, we also model self-referentiality by using a text?s first
phrases as the target acrostics (in total 150 phrases for which at least the first word has to be generated). In
these cases, the first letter of the acrostic is also the first letter of the text?a fact that enables the controlled
evaluation of the importance of the producibility of the first letter.
The evaluation system is a standard quad-core PC running Ubuntu 12.04 with 16 GB of RAM. A
relevant subset of all operator application possibilities is preprocessed and stored in-memory (e.g., the
synonym n-gram frequencies for every word), whereas the preprocessing time (about one minute in total
per run) is not counted for the search process. We then conduct an A
?
search using the preprocessed
operator tables and an admissible instance of Equation (3). To safe runtime, we slightly transform the
problem setting and require the acrostic to start at the beginning of the given text. Pilot experiments show
that a good choice for line lengths is l
min
= 50 and l
max
= 70. Note that this is only slightly more flexible
than a standard line length between 55 and 65 characters (i.e., about 10-12 words) but eases acrostic
generation. The experiments also reveal that a successful run (the acrostic can be generated) usually takes
less than 30 seconds for the search part. An unsuccessful run (the acrostic cannot be generated) takes five
to ten minutes until its termination caused by the memory constraints for the open list.
5.2 Experiment Discussion
Given our hardware and time restrictions, about 20% of the runs are successful altogether. The producibil-
ity of the first letter is critical for the overall success: we observe an almost 90% success rate for the
self-referential acrostics compared to the about 20% for all others. Statistics for the successful runs are
given in Table 3. As can be seen, our system is able to generate about 90 000 nodes with 550 goal checks
per second. This yields reasonable answer times on the test acrostics: the average number of goal checks
needed when the acrostic can be generated is below 10 000 (about 20 seconds of runtime). Only very few
successful runs took more than 40 seconds; the self-referential acrostics that often are two or three words
long form the main exception. Not that surprisingly, shorter acrostics are on average generated faster than
longer ones. Interestingly, besides self-referential acrostics, male first names seem to be the most difficult
acrostics when taking the required runtime into account. Note in this regard that many of the (longer)
female names start with a more common first letter, which can be generated faster.
Since our approach is the first attempt at the problem of acrostic generation, we cannot compare to other
systems from the literature. Instead, we compare to a baseline system that can only use line breaking and
hyphenation as its operators. This also helps to further examine the effect of the producibility of the first
letter. Whenever the acrostic?s first letter is not the first letter of the text, the baseline fails right from the
start: recall that for our experiments we require the acrostic to start at the text?s beginning. For less than
1% of our test cases, the baseline can generate the acrostic. Most of these few cases are self-referential
first words. Even if the first letter is already present, usually the second or third one are not producible by
6
en.wikipedia.org/wiki/Common_English_words, last accessed: June 12, 2014.
7
www.ssa.gov/OACT/babynames/decades/century.html, last accessed: June 12, 2014.
2026
Table 3: Experimental results for complete acrostic generations. For each of the acrostic types (Col-
umn 1), several thousand runs were conducted for which we report averaged values of the acrostic
lengths in letters (Column 2). The columns 3-10 relate to successful generations and report averaged
values for the runtime in seconds, size of the explored search tree, generated nodes and goal checks per
second, used main memory, and the quality change according to the introduced measures.
Acrostic type Length Runtime Nodes Nodes Goal checks Memory Quality-related measures
(in letters) (total in s) (total) (per s) (per s) (in MByte)
? WFC ? ARI ? SMOG
Common English words
Adjective 4.36 3.25 286 960 88 269 578 270 -0.99 -1.61 -0.91
Noun 4.47 3.40 285 016 83 837 576 277 -0.39 -0.96 -0.50
Preposition 3.44 3.16 280 593 88 853 556 243 -1.59 -2.28 -1.29
Verb 3.59 2.76 251 161 90 898 595 236 -0.95 -1.60 -0.92
Other 3.29 2.41 218 974 90 755 601 206 -1.10 -2.05 -1.11
Common US first names
Male 6.00 9.32 851 665 91 368 554 649 -0.74 -1.87 -0.93
Female 6.07 7.82 740 418 94 693 546 575 -0.60 -1.77 -0.93
Self-referential
First words 10.33 36.09 3 164 873 87 690 518 1 985 -0.31 -0.09 0.20
Average 5.19 8.53 759 957 89 545 565 372 -0.83 -1.53 -0.80
the simple operators. Using all operators, our approach is able to produce a self-referential acrostic of
more than seven characters in 80% of the cases. On average, acrostics of ten characters are possible for the
self-referential cases. This further highlights the importance of the first letter: whenever it is producible,
the success ratio is much higher.
To compare the importance of the different operators, we count for the successful generations in the
experiments of Table 3, how often operators are used and how long the search paths are. Table 2 contains
information on the applicability of the different operators. About 21% of the operator applications are line
breaks, another 9% are hyphenations. Interestingly, about 38% of the operator applications are wrong
hyphenations despite the low quality of this operator. Even though our heuristic tries to avoid wrong
hyphenations, there are a lot of situations where all other operators fail. Although not reflected by standard
quality metrics (see next subsection), a wrong hyphenation usually is eye-catching for human readers,
which gives rise to a desirable further quality improvement that should be aimed for in future work. The
other operator usages are mostly word insertions and deletions (about 25%), synonym replacements (3%),
and function words (3%). The context-independent operators of contractions and spelling correspond to
only about 1% of all operator applications.
5.3 Quality-Related Analysis
Table 3 also contains information about the text quality before and after generating the acrostic. To
algorithmically measure text quality-related effects, we employ a word frequency class analysis and a
readability analysis.
The frequency class WFC(w) of a word w relates to its customariness in written language and has
successfully been employed for text genre analysis (Stein and Meyer zu Ei?en, 2008). Let ?(w) denote
the frequency of a word in a given corpus; then the Wortschatz
8
defines WFC(w) as blog
2
(?(w
?
)/?(w))c,
where w
?
denotes the most frequently used word in the respective corpus. Here we use as reference the
Google n-gram corpus (Brants and Franz, 2006) whose most frequent word is ?the?, which corresponds to
the word frequency class 0; the most uncommonly used words within this corpus have a word frequency
class of 26. The readability of the text before and after acrostic generation is quantified according to
the standard ARI (Smith and Senter, 1967) and SMOG (McLaughlin, 1969) measures, implemented
in the Phantom Readability Library.
9
Both measures have been designed to estimate the U.S. grade
8
wortschatz.uni-leipzig.de, last accessed: June 12, 2014.
9
http://niels.drni.de/s9y/pages/phantom.html, last accessed: June 12, 2014
2027
level equivalent to the education required for understanding a given text. Hence, larger readability
scores indicate more difficult texts. The Automated Readability Index ARI (Smith and Senter, 1967) is
designed for being easily automatable and uses only the number of characters (excluding whitespace and
punctuation), words, and sentences in the text (delimited by a period, an exclamation mark, a question
mark, a colon, a semicolon, or an ellipsis). The Simple Measure of Gobbledygook SMOG (McLaughlin,
1969) includes the number of words with more than three syllables, so-called polysyllables.
ARI = 4.71 ?
Characters
Words
+ 0.5 ?
Words
Sentences
? 21.43 SMOG = 1.0430 ?
?
30 ?
Polysyllables
Sentences
+ 3.1291
Of course, the above three measures cannot capture text quality as human judges would perceive it. Still,
they have their merits and can indicate interesting trends: On average, the texts after acrostic generation
use more common words (cf. the negative ? WFC) and are easier to read (cf. the negative ? ARI and
? SMOG) for almost all acrostic types. Thus, one may argue that the quality is not harmed too much; still
some issues like wrong hyphenation are ignored by the metrics (cf. the above discussion of individual
operators). A deeper analysis of operator quality and improved quality of paraphrased texts (e.g., further
operators or avoiding wrong hyphenations) constitute very promising directions for future work.
6 Conclusion and Outlook
We have presented the first algorithmic approach to acrostic generation. The experiments show that the
heuristic search approach is able to generate about 20% of the target acrostics in reasonable time, whereas
the producibility of the first letter plays a key role. Our solution successfully combines paraphrasing
techniques from Natural Language Processing with a heuristic search strategy from Artificial Intelligence.
This way, our problem modeling opens a novel and very promising research direction, and the application
of our framework to other paraphrasing problems is the most interesting line of future work. As for the
acrostic use case, the resulting text?s quality gives the most obvious possibility for improvements. We plan
to further analyze better quality measures for the individual operators and to develop more sophisticated
operators like changing a text?s tense or even anaphora exploitation (Schmolz et al., 2012).
References
Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A Survey of Paraphrasing and Textual Entailment
Methods. Journal of Artificial Intelligence Research, 38(1):135?187.
Guy Aston and Lou Burnard. 1998. The BNC Handbook. http://www.natcorp.ox.ac.uk.
Colin J. Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings
of ACL 2005, pages 597?604.
Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. In Proceedings of HLT 2003, pages 16?23.
Regina Barzilay and Kathleen McKeown. 2001. Extracting Paraphrases From a Parallel Corpus. In Proceedings
of ACL 2001, pages 50?57.
Igor A. Bolshakov and Alexander F. Gelbukh. 2004. Synonymous Paraphrasing Using WordNet and Internet. In
Proceedings of NLDB 2004, pages 312?323.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium LDC2006T13.
Steven Burrows, Martin Potthast, and Benno Stein. 2013. Paraphrase Acquisition via Crowdsourcing and Machine
Learning. ACM Transactions on Intelligent Systems and Technology, 4(3):43:1?43:21.
Chris Callison-Burch. 2008. Syntactic Constraints on Paraphrases Extracted from Parallel Corpora. In Proceed-
ings of EMNLP 2008, pages 196?205.
Jonathan Chevelu, Thomas Lavergne, Yves Lepage, and Thierry Moudenc. 2009. Introduction of a New Para-
phrase Generation Tool Based on Monte-Carlo Sampling. In Proceedings of ACL 2009, pages 249?252.
Jonathan Chevelu, Ghislain Putois, and Yves Lepage. 2010. The True Score of Statistical Paraphrase Generation.
In Proceedings of COLING 2010 (Posters), pages 144?152.
2028
Paul Clough, Robert Gaizauskas, Scott S. L. Piao, and Yorick Wilks. 2002. METER: MEasuring TExt Reuse. In
Proceedings of ACL 2002, pages 152?159.
William B. Dolan and Chris Brockett. 2005. Automatically Constructing a Corpus of Sentential Paraphrases. In
Proceedings of the Third International Workshop on Paraphrasing 2005, pages 1?8.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.
Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme. 2011. Learning Sentential
Paraphrases From Bilingual Parallel Corpora for Text-to-Text Generation. In Proceedings of EMNLP 2011,
pages 1168?1179.
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In
Proceedings of HLT 2013, pages 758?764.
David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of HLT 2006.
Bryan Klimt and Yiming Yang. 2004. The Enron Corpus: A New Dataset for Email Classification Research. In
Proceedings of ECML 2004, pages 217?226.
Donald E. Knuth. 1986. The T
E
Xbook. Addison-Wesley.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. 2004. RCV1: A New Benchmark Collection for Text
Categorization Research. The Journal of Machine Learning Research, 5:361?397.
G. Harry McLaughlin. 1969. SMOG Grading: A New Readability Formula. Journal of Reading, 12(8):639?646.
Donald Metzler and Eduard Hovy. 2011. Mavuno: A Scalable and Effective Hadoop-Based Paraphrase Acquisi-
tion System. In Proceedings of the Third Workshop on Large Scale Data Mining 2011, pages 3:1?3:8.
Donald Metzler, Eduard H. Hovy, and Chunliang Zhang. 2011. An Empirical Evaluation of Data-Driven Para-
phrase Generation Techniques. In Proceedings of ACL 2011 (Short Papers), pages 546?551.
Bonnie J. F. Meyer. 2003. Text Coherence and Readability. Topics in Language Disorders, 23(3):204?224.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-Based Alignment of Multiple Translations: Extracting
Paraphrases and Generating New Sentences. In Proceedings of HLT 2003, pages 102?109.
Marius Pasca and P?eter Dienes. 2005. Aligning Needles in a Haystack: Paraphrase Acquisition Across the Web.
In Proceedings of IJCNLP 2005, pages 119?130.
Judea Pearl. 1984. Heuristics. Addison-Wesley.
Chris Quirk, Chris Brockett, and William B. Dolan. 2004. Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of EMNLP 2004, pages 142?149.
Patrick Riehmann, Henning Gruendl, Martin Potthast, Martin Trenkmann, Benno Stein, and Bernd Froehlich.
2012. WORDGRAPH: Keyword-in-Context Visualization for NETSPEAK?s Wildcard Search. IEEE Transac-
tions on Visualization and Computer Graphics, 18(9):1411?1423.
Helene Schmolz, David Coquil, and Mario D?oller. 2012. In-Depth Analysis of Anaphora Resolution Require-
ments. In Proceedings of TIR 2012, pages 174?179.
Edgar A. Smith and R. J. Senter. 1967. Automated Readability Index. Technical Report AMRL-TR-6620, Wright-
Patterson Air Force Base.
Benno Stein and Daniel Curatolo. 2006. Phonetic Spelling and Heuristic Search. In Proceedings of ECAI 2006,
pages 829?830.
Benno Stein and Sven Meyer zu Ei?en. 2008. Retrieval Models for Genre Classification. Scandinavian Journal
of Information Systems (SJIS), 20(1):91?117.
Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving Customary Web Language to Assist
Writers. In Proceedings of ECIR 2010, pages 631?635.
Nathan Sturtevant, Ariel Felner, Maxim Likhachev, and Wheeler Ruml. 2012. Heuristic Search Comes of Age. In
Proceedings of AAAI 2012.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng Li. 2008. Combining Multiple Resources to Improve
SMT-Based Paraphrasing Model. In Proceedings of ACL 2008, pages 1021?1029.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009. Application-Driven Statistical Paraphrase Generation. In
Proceedings of ACL 2009, pages 834?842.
2029
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 570?579,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
The Impact of Spelling Errors on Patent Search
Benno Stein and Dennis Hoppe and Tim Gollub
Bauhaus-Universit?t Weimar
99421 Weimar, Germany
<first name>.<last name>@uni-weimar.de
Abstract
The search in patent databases is a risky
business compared to the search in other
domains. A single document that is relevant
but overlooked during a patent search can
turn into an expensive proposition. While
recent research engages in specialized mod-
els and algorithms to improve the effective-
ness of patent retrieval, we bring another
aspect into focus: the detection and ex-
ploitation of patent inconsistencies. In par-
ticular, we analyze spelling errors in the as-
signee field of patents granted by the United
States Patent & Trademark Office. We in-
troduce technology in order to improve re-
trieval effectiveness despite the presence of
typographical ambiguities. In this regard,
we (1) quantify spelling errors in terms of
edit distance and phonological dissimilarity
and (2) render error detection as a learn-
ing problem that combines word dissimi-
larities with patent meta-features. For the
task of finding all patents of a company,
our approach improves recall from 96.7%
(when using a state-of-the-art patent search
engine) to 99.5%, while precision is com-
promised by only 3.7%.
1 Introduction
Patent search forms the heart of most retrieval
tasks in the intellectual property domain?cf. Ta-
ble 1, which provides an overview of various user
groups along with their typical (?) and related (?)
tasks. The due diligence task, for example, is
concerned with legal issues that arise while inves-
tigating another company. Part of an investiga-
tion is a patent portfolio comparison between one
or more competitors (Lupu et al 2011). Within
all tasks recall is preferred over precision, a fact
which distinguishes patent search from general
web search. This retrieval constraint has produced
a variety of sophisticated approaches tailored to
the patent domain: citation analysis (Magdy and
Jones, 2010), the learning of section-specific re-
trieval models (Lopez and Romary, 2010), and au-
tomated query generation (Xue and Croft, 2009).
Each approach improves retrieval performance,
but what keeps them from attaining maximum ef-
fectiveness in terms of recall are the inconsisten-
cies found in patents: incomplete citation sets, in-
correctly assigned classification codes, and, not
least, spelling errors.
Our paper deals with spelling errors in an oblig-
atory and important field of each patent, namely,
the patent assignee name. Bibliographic fields are
widely used among professional patent searchers
in order to constrain keyword-based search ses-
sions (Joho et al 2010). The assignee name is
particularly helpful for patentability searches and
portfolio analyses since it determines the com-
pany holding the patent. Patent experts address
these search tasks by formulating queries contain-
ing the company name in question, in the hope of
finding all patents owned by that company. A for-
mal and more precise description of this relevant
search task is as follows: Given a query q which
specifies a company, and a set D of patents, de-
termine the set Dq ? D comprised of all patents
held by the respective company.
For this purpose, all assignee names in the
patents in D should be analyzed. Let A denote
the set of all assignee names in D, and let a ? q
denote the fact that an assignee name a ? A refers
to company q. Then in the portfolio search task,
all patents filed under a are relevant. The retrieval
of Dq can thus be rendered as a query expansion
570
Table 1: User groups and patent-search-related retrieval tasks in the patent domain (Hunt et al 2007).
User group
Analyst Attorney Manager Inventor Investor Researcher
Patentability ? ? ? ?
State of the art ? ?
Patent search task Infringement ?
Opposition ? ?
Due diligence ? ?
Portfolio ? ? ? ?
task, where q is expanded by the disjunction of
assignee names Aq with Aq = {a ? A | a ? q}.
While the trivial expansion of q by the entire
set A ensures maximum recall but entails an un-
acceptable precision, the expansion of q by the
empty set yields a reasonable baseline. The latter
approach is implemented in patent search engines
such as PatBase1 or FreePatentsOnline,2 which
return all patents where the company name q oc-
curs as a substring of the assignee name a. This
baseline is simple but reasonable; due to trade-
mark law, a company name q must be a unique
identifier (i.e. a key), and an assignee name a that
contains q can be considered as relevant. It should
be noted in this regard that |q| < |a| holds for
most elements in Aq, since the assignee names
often contain company suffixes such as ?Ltd?
or ?Inc?.
Our hypothesis is that due to misspelled as-
signee names a substantial fraction of relevant
patents cannot be found by the baseline ap-
proach. In this regard, the types of spelling er-
rors in assignee names given in Table 2 should
be considered.
Table 2: Types of spelling errors with increasing
problem complexity according to Stein and Curatolo
(2006). The first row refers to lexical errors, whereas
the last two rows refer to phonological errors. For each
type, an example is given, where a misspelled com-
pany name is followed by the correctly spelled variant.
Spelling error type Example
Permutations or dropped letters ? Whirpool Corporation
? Whirlpool Corporation
Misremembering spelling details ? Whetherford International
? Weatherford International
Spelling out the pronunciation ? Emulecks Corporation
? Emulex Corporation
In order to raise the recall for portfolio search
without significantly impairing precision, an ap-
1www.patbase.com
2www.freepatentsonline.com
proach more sophisticated than the standard re-
trieval approach, which is the expansion of q by
the empty set, is needed. Such an approach must
strive for an expansion of q by a subset of Aq,
whereby this subset should be as large as possible.
1.1 Contributions
The paper provides a new solution to the problem
outlined. This solution employs machine learn-
ing on orthographic features, as well as on patent
meta features, to reliably detect spelling errors. It
consists of two steps: (1) the computation of A+q ,
the set of assignee names that are in a certain edit
distance neighborhood to q; and (2) the filtering of
A+q , yielding the set A?q , which contains those as-
signee names from A+q that are classified as mis-
spellings of q. The power of our approach can be
seen from Table 3, which also shows a key result
of our research; a retrieval system that exploits
our classifier will miss only 0.5% of the relevant
patents, while retrieval precision is compromised
by only 3.7%.
Another contribution relates to a new, manu-
ally-labeled corpus comprising spelling errors in
the assignee field of patents (cf. Section 3). In
this regard, we consider the over 2 million patents
granted by the USPTO between 2001 and 2010.
Last, we analyze indications of deliberately in-
serted spelling errors (cf. Section 4).
Table 3: Mean average Precision, Recall, and F -
Measure (? = 2) for different expansion sets for q in
a portfolio search task, which is conducted on our test
corpus (cf. Section 3).
Expansion set for q Precision Recall F2
? (baseline) 0.993 0.967 0.968
A?q (machine learning) 0.956 0.995 0.980
A (trivial) 0.001 1.0 0.005
A+q (edit distance) 0.274 1.0 0.672
571
1.2 Causes for Inconsistencies in Patents
We identify the following six factors for inconsis-
tencies in the bibliographic fields of patents, in
particular for assignee names: (1) Misspellings
are introduced due to the lack of knowledge, the
lack of attention, and due to spelling disabili-
ties. Intellevate Inc. (2006) reports that 98%
of a sample of patents taken from the USPTO
database contain errors, most which are spelling
errors. (2) Spelling errors are only removed by the
USPTO upon request (U.S. Patent & Trademark
Office, 2010). (3) Spelling variations of inventor
names are permitted by the USPTO. The Manual
of Patent Examining Procedure (MPEP) states in
paragraph 605.04(b) that ?if the applicant?s full
name is ?John Paul Doe,? either ?John P. Doe? or
?J. Paul Doe? is acceptable.? Thus, it is valid to in-
troduce many different variations: with and with-
out initials, with and without a middle name, or
with and without suffixes. This convention ap-
plies to assignee names, too. (4) Companies of-
ten have branches in different countries, where
each branch has its own company suffix, e.g.,
?Limited? (United States), ?GmbH? (Germany),
or ?Kabushiki Kaisha? (Japan). Moreover, the
usage of punctuation varies along company suf-
fix abbreviations: ?L.L.C.? in contrast to ?LLC?,
for example. (5) Indexing errors emerge from
OCR processing patent applications, because sim-
ilar looking letters such as ?e? versus ?c? or ?l?
versus ?I? are likely to be misinterpreted. (6) With
the advent of electronic patent application filing,
the number of patent reexamination steps was re-
duced. As a consequence, the chance of unde-
tected spelling errors increases (Adams, 2010).
All of the mentioned factors add to a highly in-
consistent USPTO corpus.
2 Related Work
Information within a corpus can only be retrieved
effectively if the data is both accurate and unique
(M?ller and Freytag, 2003). In order to yield data
that is accurate and unique, approaches to data
cleansing can be utilized to identify and remove
inconsistencies. M?ller and Freytag (2003) clas-
sify inconsistencies, where duplicates of entities
in a corpus are part of a semantic anomaly. These
duplicates exist in a database if two or more dif-
ferent tuples refer to the same entity. With respect
to the bibliographic fields of patents, the assignee
names ?Howlett-Packard? and ?Hewett-Packard?
are distinct but refer to the same company. These
kinds of near-duplicates impede the identification
of duplicates (Naumann and Herschel, 2010).
Near-duplicate Detection The problem of
identifying near-duplicates is also known as
record linkage, or name matching; it is sub-
ject of active research (Elmagarmid et al 2007).
With respect to text documents, slightly modi-
fied passages in these documents can be identi-
fied using fingerprints (Potthast and Stein, 2008).
On the other hand, for data fields which con-
tain natural language such as the assignee name
field, string similarity metrics (Cohen et al
2003) as well as spelling correction technol-
ogy are exploited (Damerau, 1964; Monge and
Elkan, 1997). String similarity metrics com-
pute a numeric value to capture the similarity
of two strings. Spelling correction algorithms,
by contrast, capture the likelihood for a given
word being a misspelling of another word. In
our analysis, the similarity metric SoftTfIdf is
applied, which performs best in name matching
tasks (Cohen et al 2003), as well as the complete
range of spelling correction algorithms shown in
Figure 1: Soundex, which relies on similarity
hashing (Knuth, 1997), the Levenshtein distance,
which gives the minimum number of edits needed
to transform a word into another word (Leven-
shtein, 1966), and SmartSpell, a phonetic pro-
duction approach that computes the likelihood
of a misspelling (Stein and Curatolo, 2006). In
order to combine the strength of multiple met-
rics within a near-duplicate detection task, sev-
eral authors resort to machine learning (Bilenko
and Mooney, 2002; Cohen et al 2003). Christen
(2006) concludes that it is important to exploit all
kinds of knowledge about the type of data in ques-
tion, and that inconsistencies are domain-specific.
Hence, an effective near-duplicate detection ap-
proach should employ domain-specific heuristics
and algorithms (M?ller and Freytag, 2003). Fol-
lowing this argumentation, we augment various
word similarity assessments with patent-specific
meta-features.
Patent Search Commercial patent search en-
gines, such as PatBase and FreePatentsOnline,
handle near-duplicates in assignee names as fol-
lows. For queries which contain a company name
followed by a wildcard operator, PatBase suggests
572
Single word 
spelling
correction
Near similarity
hashing
Editing
Phonetic production 
approach
Edit-distance-based
Trigram-based
Rule-based
Collision-based
Neighborhood-based
Heuristic search
Hidden Markov
models
Figure 1: Classification of spelling correction methods
according to Stein and Curatolo (2006).
a set of additional companies (near-duplicates),
which can be considered alongside the company
name in question. These suggestions are solely
retrieved based on a trailing wildcard query. Each
additional company name can then be marked in-
dividually by a user to expand the original query.
In case the entire set of suggestions is consid-
ered, this strategy conforms to the expansion of
a query by the empty set, which equals a rea-
sonable baseline approach. This query expansion
strategy, however, has the following drawbacks:
(1) The strategy captures only inconsistencies that
succeed the given company name in the origi-
nal query. Thus, near-duplicates which contain
spelling errors in the company name itself are not
found. Even if PatBase would support left trailing
wildcards, then only the full combination of wild-
card expressions would cover all possible cases of
misspellings. (2) Given an acronym of a company
such as IBM, it is infeasible to expand the ab-
breviation to ?International Business Machines?
without considering domain knowledge.
Query Expansion Methods for Patent Search
To date, various studies have investigated query
expansion techniques in the patent domain that
focus on prior-art search and invalidity search
(Magdy and Jones, 2011). Since we are dealing
with queries that comprise only a company name,
existing methods cannot be applied. Instead, the
near-duplicate task in question is more related to a
text reuse detection task discussed by Hagen and
Stein (2011); given a document, passages which
also appear identical or slightly modified in other
documents, have to be retrieved by using standard
keyword-based search engines. Their approach is
guided by the user-over-ranking hypothesis intro-
duced by Stein and Hagen (2011). It states that
?the best retrieval performance can be achieved
with queries returning about as many results as
can be considered at user site.? If we make use
of their terminology, then we can distinguish the
query expansion sets (cf. Table 3) into two cate-
gories: (1) The trivial as well as the edit distance
expansion sets are underspecific, i.e., users cannot
cope with the large amount of irrelevant patents
returned; the precision is close to zero. (2) The
baseline approach, by contrast, is overspecific;
it returns too few documents, i.e., the achieved
recall is not optimal. As a consequence, these
query expansion sets are not suitable for portfolio
search. Our approach, on the other hand, excels
in both precision and recall.
Query Spelling Correction Queries which are
submitted to standard web search engines differ
from queries which are posed to patent search en-
gines with respect to both length and language
diversity. Hence, research in the field of web
search is concerned with suggesting reasonable
alternatives to misspelled queries rather than cor-
recting single words (Li et al 2011). Since stan-
dard spelling correction dictionaries (e.g. ASpell)
are not able to capture the rich language used in
web queries, large-scale knowledge sources such
as Wikipedia (Li et al 2011), query logs (Chen
et al 2007), and large n-gram corpora (Brants et
al., 2007) are employed. It should be noted that
the set of correctly written assignee names is un-
known for the USPTO patent corpus.
Moreover, spelling errors are modeled on the
basis of language models (Li et al 2011). Okuno
(2011) proposes a generative model to encounter
spelling errors, where the original query is ex-
panded based on alternatives produced by a small
edit distance to the original query. This strategy
correlates to the trivial query expansion set (cf.
Section 1). Unlike using a small edit distance, we
allow a reasonable high edit distance to maximize
the recall.
Trademark Search The trademark search is
about identifying registered trademarks which are
similar to a new trademark application. Sim-
ilarities between trademarks are assessed based
on figurative and verbal criteria. In the former
case, the focus is on image-based retrieval tech-
niques. Trademarks are considered verbally simi-
lar for a variety of reasons, such as pronunciation,
spelling, and conceptual closeness, e.g., swapping
letters or using numbers for words. The verbal
similarity of trademarks, on the other hand, can
be determined by using techniques comparable
to near-duplicate detection: phonological parsing,
573
fuzzy search, and edit distance computation (Fall
and Giraud-Carrier, 2005).
3 Detection of Spelling Errors
This section presents our machine learning ap-
proach to expand a company query q; the classi-
fier c delivers the set A?q = {a ? A | c(q, a) = 1},
an approximation of the ideal set of relevant as-
signee names Aq. As a classification technol-
ogy a support vector machine with linear kernel
is used, which receives each pair (q, a) as a six-
dimensional feature vector. For training and test
purposes we identified misspellings for 100 dif-
ferent company names. A detailed description of
the constructed test corpus and a report on the
classifiers performance is given in the remainder
of this section.
3.1 Feature Set
The feature set comprises six features, three of
them being orthographic similarity metrics, which
are computed for every pair (q, a). Each metric
compares a given company name q with the first
|q| words of the assignee name a:
1. SoftTfIdf. The SoftTfIdf metric is consid-
ered, since the metric is suitable for the com-
parison of names (Cohen et al 2003). The
metric incorporates the Jaro-Winkler met-
ric (Winkler, 1999) with a distance threshold
of 0.9. The frequency values for the similar-
ity computation are trained on A.
2. Soundex. The Soundex spelling correction
algorithm captures phonetic errors. Since the
algorithm computes hash values for both q
and a, the feature is 1 if these hash values
are equal, 0 otherwise.
3. Levenshtein distance. The Levenshtein dis-
tance for (q, a) is normalized by the charac-
ter length of q.
To obtain further evidence for a misspelling
in an assignee name, meta information about the
patents in D, to which the assignee name refers
to, is exploited. In this regard, the following three
features are derived:
1. Assignee Name Frequency. The number
of patents filed under an assignee name a:
FFreq (a) = Freq(a,D). We assume that the
probability of a misspelling to occur multi-
ple times is low, and thus an assignee name
with a misspelled company name has a low
frequency.
2. IPC Overlap. The IPC codes of a patent
specify the technological areas it applies
to. We assume that patents filed under the
same company name are likely to share the
same set of IPC codes, regardless whether
the company name is misspelled or not.
Hence, if we determine the IPC codes of
patents which contain q in the assignee
name, IPC(q), and the IPC codes of patents
filed under assignee name a, IPC(a), then
the intersection size of the two sets serves as
an indicator for a misspelled company name
in a:
FIPC (q, a) =
IPC(q) ? IPC(a)
IPC(q) ? IPC(a)
3. Company Suffix Match. The suffix match
relies on the company suffixes Suffixes(q)
that occur in the assignee names of A con-
taining q. Similar to the IPC overlap fea-
ture, we argue that if the company suffix
of a exists in the set Suffixes(q), a mis-
spelling in a is likely: FSuffixes(q, a) = 1
iff Suffixes(a) ? Suffixes(q).
3.2 Webis Patent Retrieval Assignee Corpus
A key contribution of our work is a new cor-
pus called Webis Patent Retrieval Assignee Cor-
pus 2012 (Webis-PRA-12). We compiled the cor-
pus in order to assess the impact of misspelled
companies on patent retrieval and the effective-
ness of our classifier to detect them.3 The corpus
is built on the basis of 2 132 825 patents D granted
by the USPTO between 2001 and 2010; the patent
corpus is provided publicly by the USPTO in
XML format. Each patent contains bibliographic
fields as well as textual information such as the
abstract and the claims section. Since we are in-
terested in the assignee name a associated with
each patent d ? D, we parse each patent and ex-
tract the assignee name. This yields the set A of
202 846 different assignee names. Each assignee
name refers to a set of patents, which size varies
from 1 to 37 202 (the number of patents filed
under ?International Business Machines Corpo-
ration?). It should be noted that for a portfolio
3The Webis-PRA-12 corpus is freely available via
www.webis.de/research/corpora
574
Table 4: Statistics of spelling errors for the 100 companies in the Webis-PRA-12 corpus. Considered are the
number of words and the number of letters in the company names, as well as the number of different company
suffixes that are used together with a company name (denoted as variants of q)
Total Num. of words in q Num. of letters in q Num. of variants of q
1 2 3-4 2-10 11-15 16-35 1-5 6-15 16-96
Number of companies in Q 100 36 53 11 30 35 35 45 32 23
Avg. num. of misspellings in A 3.79 2.13 3.75 9.36 1.16 2.94 6.88 0.91 3.81 9.39
search task the number of patents which refer to
an assignee name matters for the computation of
precision and recall. If we, however, isolate the
task of detecting misspelled company names, then
it is also reasonable to weight each assignee name
equally and independently from the number of
patents it refers to. Both scenarios are addressed
in the experiments.
Given A, the corpus construction task is to map
each assignee name a ? A to the company name
q it refers to. This gives for each company name
q the set of relevant assignee names Aq. For our
corpus, we do not construct Aq for all company
names but take a selection of 100 company names
from the 2011 Fortune 500 ranking as our set of
company names Q. Since the Fortune 500 rank-
ing contains only large companies, the test cor-
pus may appear to be biased towards these com-
panies. However, rather than the company size the
structural properties of a company name are de-
terminative; our sample includes short, medium,
and long company names, as well as company
names with few, medium, and many different
company suffixes. Table 4 shows the distribution
of company names in Q along these criteria in the
first row.
For each company name q ? Q, we ap-
ply a semi-automated procedure to derive the
set of relevant assignee names Aq . In a first
step, all assignee names in A which do not re-
fer to the company name q are filtered auto-
matically. From a preliminary evaluation we
concluded that the Levenshtein distance d(q, a)
with a relative threshold of |q|/2 is a reasonable
choice for this filtering step. The resulting sets
A+q = {a ? A | d(q, a) ? |q|/2) contain, in total
over Q, 14 189 assignee names. These assignee
names are annotated by human assessors within a
second step to derive the final set Aq for each q ?
Q. Altogether we identify 1 538 assignee names
that refer to the 100 companies in Q. With respect
to our classification task, the assignee names in
each Aq are positive examples; the remaining as-
signee names A+q \ Aq form the set of negative
examples (12 651 in total).
During the manual assessment, names of as-
signees which include the correct company name
q were distinguished from misspelled ones. The
latter holds true for 379 of the 1 538 assignee
names. These names are not retrievable by the
baseline system, and thus form the main target for
our classifier. The second row of Table 4 reports
on the distribution of the 379 misspelled assignee
names. As expectable, the longer the company
name, the more spelling errors occur. Compa-
nies which file patents under many different as-
signee names are likelier to have patents with mis-
spellings in the company name.
3.3 Classifier Performance
For the evaluation with the Webis-PRA-12 cor-
pus, we train a support vector machine,4 which
considers the six outlined features, and compare
it to the other expansion techniques. For the train-
ing phase, we use 2/3 of the positive examples
to form a balanced training set of 1 025 posi-
tive and 1 025 negative examples. After 10-fold
cross validation, the achieved classification accu-
racy is 95.97%.
For a comparison of the expansion techniques
on the test set, which contains the examples not
considered in the training phase, two tasks are
distinguished: finding near duplicates in assignee
names (cf. Table 5, Columns 3?5), and finding all
patents of a company (cf. Table 5, Columns 6?8).
The latter refers to the actual task of portfo-
lio search. It can be observed that the perfor-
mance improvements on both tasks are pretty sim-
ilar. The baseline expansion ? yields a recall
of 0.83 in the first task. The difference of 0.17
to a perfect recall can be addressed by consid-
ering query expansion techniques. If the triv-
ial expansion A is applied to the task the max-
imum recall can be achieved, which, however,
4We use the implementation of the WEKA toolkit with default
parameters.
575
Table 5: The search results (macro-averaged) for two retrieval tasks and various expansion techniques. Besides
Precision and Recall, the F-Measure with ? = 2 is stated.
Misspelling detection Task: assignee names Task: patents
P R F2 P R F2
Baseline (?) .975 .829 .838 .993 .967 .968
Trivial (A) .000 1.0 .001 .001 1.0 .005
Edit distance (A+q ) .274 1.0 .499 .412 1.0 .672
SVM (Levenshtein) .752 .981 .853 .851 .991 .911
SVM (SoftTfIdf) .702 .980 .796 .826 .993 .886
SVM (Soundex) .433 .931 .624 .629 .984 .759
SVM (orthographic features) .856 .975 .922 .942 .990 .967
SVM (A?q , all features) .884 .975 .938 .956 .995 .980
is bought with precision close to zero. Using
the edit distance expansion A+q yields a precision
of 0.274 while keeping the recall at maximum. Fi-
nally, the machine learning expansion A?q leads
to a dramatic improvement (cf. Table 5, bottom
lines), whereas the exploitation of patent meta-
features significantly outperforms the exclusive
use of orthography-related features; the increase
in recall which is achieved by A?q is statistically
significant (matched pair t-test) for both tasks (as-
signee names task: t = ?7.6856, df = 99,
p = 0.00; patents task: t = ?2.1113, df = 99,
p = 0.037). Note that when being applied as a
single feature none of the spelling metrics (Lev-
enshtein, SoftTfIdf, Soundex) is able to achieve
a recall close to 1 without significantly impairing
the precision.
4 Distribution of Spelling Errors
Encouraged by the promising retrieval results
achieved on the Webis-PRA-12 corpus, we ex-
tend the analysis of spelling errors in patents to
the entire USPTO corpus of granted patents be-
tween 2001 and 2010. The analysis focuses on
the following two research questions:
1. Are spelling errors an increasing issue in
patents? According to Adams (2010), the
amount of spelling errors should have been
increased in the last years due to the elec-
tronic patent filing process (cf. Section 1.2).
We address this hypothesis by analyzing the
distribution of spelling errors in company
names that occur in patents granted between
2001 and 2010.
2. Are misspellings introduced deliberately in
patents? We address this question by analyz-
ing the patents with respect to the eight tech-
nological areas based on the International
Patent Classification scheme IPC: A (Hu-
man necessities), B (Performing operations;
transporting), C (Chemistry; metallurgy),
D (Textiles; paper), E (Fixed constructions),
F (Mechanical engineering; lighting; heat-
ing; weapons; blasting), G (Physics), and
H (Electricity). If spelling errors are in-
troduced accidentally, then we expect them
to be uniformly distributed across all ar-
eas. A biased distribution, on the other
hand, indicates that errors might be in-
serted deliberately.
In the following, we compile a second corpus
on the basis of the entire set A of assignee names.
In order to yield a uniform distribution of the com-
panies across years, technological areas and coun-
tries, a set of 120 assignee names is extracted for
each dimension. After the removal of duplicates,
we revised these assignee names manually in or-
der to check (and correct) their spelling. Finally,
trailing business suffixes are removed, which re-
sults in a set of 3 110 company names. For each
company name q, we generate the set A?q as de-
scribed in Section 3.
The results of our analysis are shown in Table 6.
Table 6(a) refers to the first research question and
shows that the amount of misspellings in compa-
nies decreased over the years from 6.67% in 2001
to 4.74% in 2010 (cf. Row 3). These results let us
reject the hypothesis of Adams (2010). Neverthe-
less, the analysis provides evidence that spelling
errors are still an issue. For example, the company
identified with most spelling errors are ?Konin-
klijke Philips Electronics? with 45 misspellings
in 2008, and ?Centre National de la Recherche
Scientifique? with 28 misspellings in 2009. The
results are consistent with our findings with re-
576
Table 6: Distribution of spelling errors for 3 110 company identifiers in the USPTO patents. The mean of spelling
errors per company identifier and the standard deviation ? refer to companies with misspellings. The last row in
each table shows the number of patents that are additionally found if the original query q is expanded by A?q .
(a) Distribution of spelling errors between the years 2001 and 2010.
Year
2001 2002 2003 2004 2005 2006 2007 2008 2009 2010
M
ea
su
re
Number of companies 1 028 1 066 1 115 1 151 1 219 1 261 1 274 1 210 1 224 1 268
Number of companies with misspellings 67 63 53 65 65 60 65 64 53 60
Companies with misspellings (%) 6.52 5.91 4.75 5.65 5.33 4.76 5.1 5.29 4.33 4.73
Mean 2.78 2.35 2.23 2.28 2.18 2.48 2.23 3.0 2.64 2.8
Standard deviation ? 4.62 3.3 3.63 3.13 2.8 3.55 2.87 6.37 4.71 4.6
Maximum misspellings per company 24 12 16 12 10 18 12 45 28 22
Additional number of patents 7.1 7.21 7.43 7.68 7.91 8.48 7.83 8.84 8.92 8.92
(b) Distribution of spelling errors based on the IPC scheme.
IPC code
A B C D E F G H
M
ea
su
re
Number of companies 954 1 231 811 277 412 771 1 232 949
Number of companies with misspellings 59 70 51 7 10 33 83 63
Companies with misspellings (%) 6.18 5.69 6.29 2.53 2.43 4.28 6.74 6.64
Mean 3.0 2.49 3.57 1.86 2.8 1.88 3.29 4.05
Standard deviation ? 5.28 3.65 7.03 1.99 4.22 2.31 5.72 7.13
Maximum misspellings per company 32 14 40 3 12 6 24 35
Additional number of patents 9.25 9.67 11.12 4.71 4.6 4.79 8.92 12.84
spect to the Fortune 500 sample (cf. Table 4),
where company names that are longer and pre-
sumably more difficult to write contain more
spelling errors.
In contrast to the uniform distribution of mis-
spellings over the years, the situation with re-
gard to the technological areas is different (cf. Ta-
ble 6(b)). Most companies are associated with
the IPC sections G and B, which both refer to
technical domains (cf. Table 6(b), Row 1). The
percentage of misspellings in these sections in-
creased compared to the spelling errors grouped
by year. A significant difference can be seen for
the sections D and E. Here, the number of as-
signed companies drops below 450 and the per-
centage of misspellings decreases significantly
from about 6% to 2.5%. These findings might
support the hypothesis that spelling errors are in-
serted deliberately in technical domains.
5 Conclusions
While researchers in the patent domain concen-
trate on retrieval models and algorithms to im-
prove the search performance, the original aspect
of our paper is that it points to a different (and or-
thogonal) research avenue: the analysis of patent
inconsistencies. With the analysis of spelling er-
rors in assignee names we made a first yet consid-
erable contribution in this respect; searches with
assignee constraints become a more sensible op-
eration. We showed how a special treatment of
spelling errors can significantly raise the effec-
tiveness of patent search. The identification of
this untapped potential, but also the utilization of
machine learning to combine patent features with
typography, form our main contributions.
Our current research broadens the application
of a patent spelling analysis. In order to iden-
tify errors that are introduced deliberately we
investigate different types of misspellings (edit
distance versus phonological). Finally, we con-
sider the analysis of acquisition histories of com-
panies as promising research direction: since
acquired companies often own granted patents,
these patents should be considered while search-
ing for the company in question in order to further
increase the recall.
Acknowledgements
This work is supported in part by the German Sci-
ence Foundation under grants STE1019/2-1 and
FU205/22-1.
577
References
Stephen Adams. 2010. The Text, the Full Text and
nothing but the Text: Part 1 ? Standards for creating
Textual Information in Patent Documents and Gen-
eral Search Implications. World Patent Information,
32(1):22?29, March.
Mikhail Bilenko and Raymond J. Mooney. 2002.
Learning to Combine Trained Distance Metrics
for Duplicate Detection in Databases. Technical
Report AI 02-296, Artificial Intelligence Labora-
tory, University of Austin, Texas, USA, Austin,
TX, February.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language
Models in Machine Translation. In EMNLP-CoNLL
?07: Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, pages 858?867. ACL, June.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing Query Spelling Correction Using Web Search
Results. In EMNLP-CoNLL ?07: Proceedings of
the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 181?189. ACL,
June.
Peter Christen. 2006. A Comparison of Personal
Name Matching: Techniques and Practical Is-
sues. In ICDM ?06: Workshops Proceedings of
the sixth IEEE International Conference on Data
Mining, pages 290?294. IEEE Computer Society,
December.
William W. Cohen, Pradeep Ravikumar, and Stephen
E. Fienberg. 2003. A Comparison of String
Distance Metrics for Name-Matching Tasks. In
Subbarao Kambhampati and Craig A. Knoblock,
editors, IIWeb ?03: Proceedings of the IJCAI
workshop on Information Integration on the Web,
pages 73?78, August.
Fred J. Damerau. 1964. A Technique for Computer
Detection and Correction of Spelling Errors. Com-
munications of the ACM, 7(3):171?176.
Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and
Vassilios S. Verykios. 2007. Duplicate Record De-
tection: A Survey. IEEE Trans. Knowl. Data Eng.,
19(1):1?16.
Caspas J. Fall and Christophe Giraud-Carrier. 2005.
Searching Trademark Databases for Verbal Similar-
ities. World Patent Information, 27(2):135?143.
Matthias Hagen and Benno Stein. 2011. Candidate
Document Retrieval for Web-Scale Text Reuse De-
tection. In 18th International Symposium on String
Processing and Information Retrieval (SPIRE 11),
volume 7024 of Lecture Notes in Computer Science,
pages 356?367. Springer.
David Hunt, Long Nguyen, and Matthew Rodgers, ed-
itors. 2007. Patent Searching: Tools & Techniques.
Wiley.
Intellevate Inc. 2006. Patent Quality, a blog en-
try. http://www.patenthawk.com/blog/
2006/01/patent_quality.html, January.
Hideo Joho, Leif A. Azzopardi, and Wim Vander-
bauwhede. 2010. A Survey of Patent Users: An
Analysis of Tasks, Behavior, Search Functionality
and System Requirements. In IIix ?10: Proceed-
ing of the third symposium on Information Inter-
action in Context, pages 13?24, New York, NY,
USA. ACM.
Donald E. Knuth. 1997. The Art of Computer Pro-
gramming, Volume I: Fundamental Algorithms, 3rd
Edition. Addison-Wesley.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions and reversals.
Soviet Physics Doklady, 10(8):707?710. Original
in Doklady Akademii Nauk SSSR 163(4): 845-848.
Yanen Li, Huizhong Duan, and ChengXiang Zhai.
2011. CloudSpeller: Spelling Correction for Search
Queries by Using a Unified Hidden Markov Model
with Web-scale Resources. In Spelling Alteration
for Web Search Workshop, pages 10?14, July.
Patrice Lopez and Laurent Romary. 2010. Experi-
ments with Citation Mining and Key-Term Extrac-
tion for Prior Art Search. In Martin Braschler,
Donna Harman, and Emanuele Pianta, editors,
CLEF 2010 LABs and Workshops, Notebook Pa-
pers, September.
Mihai Lupu, Katja Mayer, John Tait, and Anthony J.
Trippe, editors. 2011. Current Challenges in Patent
Information Retrieval, volume 29 of The Informa-
tion Retrieval Series. Springer.
Walid Magdy and Gareth J. F. Jones. 2010. Ap-
plying the KISS Principle for the CLEF-IP 2010
Prior Art Candidate Patent Search Task. In Martin
Braschler, Donna Harman, and Emanuele Pianta,
editors, CLEF 2010 LABs and Workshops, Note-
book Papers, September.
Walid Magdy and Gareth J.F. Jones. 2011. A Study
on Query Expansion Methods for Patent Retrieval.
In PAIR ?11: Proceedings of the 4th workshop on
Patent information retrieval, AAAI Workshop on
Plan, Activity, and Intent Recognition, pages 19?
24, New York, NY, USA. ACM.
Alvaro E. Monge and Charles Elkan. 1997. An Ef-
ficient Domain-Independent Algorithm for Detect-
578
ing Approximately Duplicate Database Records.
In DMKD ?09: Proceedings of the 2nd workshop
on Research Issues on Data Mining and Knowl-
edge Discovery, pages 23?29, New York, NY,
USA. ACM.
Heiko M?ller and Johann-C. Freytag. 2003. Prob-
lems, Methods and Challenges in Comprehensive
Data Cleansing. Technical Report HUB-IB-164,
Humboldt-Universit?t zu Berlin, Institut f?r Infor-
matik, Germany.
Felix Naumann and Melanie Herschel. 2010. An In-
troduction to Duplicate Detection. Synthesis Lec-
tures on Data Management. Morgan & Claypool
Publishers.
Yoh Okuno. 2011. Spell Generation based on Edit
Distance. In Spelling Alteration for Web Search
Workshop, pages 25?26, July.
Martin Potthast and Benno Stein. 2008. New Is-
sues in Near-duplicate Detection. In Christine
Preisach, Hans Burkhardt, Lars Schmidt-Thieme,
and Reinhold Decker, editors, Data Analysis, Ma-
chine Learning and Applications. Selected papers
from the 31th Annual Conference of the German
Classification Society (GfKl 07), Studies in Classi-
fication, Data Analysis, and Knowledge Organiza-
tion, pages 601?609, Berlin Heidelberg New York.
Springer.
Benno Stein and Daniel Curatolo. 2006. Phonetic
Spelling and Heuristic Search. In Gerhard Brewka,
Silvia Coradeschi, Anna Perini, and Paolo Traverso,
editors, 17th European Conference on Artificial In-
telligence (ECAI 06), pages 829?830, Amsterdam,
Berlin, August. IOS Press.
Benno Stein and Matthias Hagen. 2011. Introducing
the User-over-Ranking Hypothesis. In Advances in
Information Retrieval. 33rd European Conference
on IR Resarch (ECIR 11), volume 6611 of Lecture
Notes in Computer Science, pages 503?509, Berlin
Heidelberg New York, April. Springer.
U.S. Patent & Trademark Office. 2010. Manual of
Patent Examining Procedure (MPEP), Eighth Edi-
tion, July.
William W. Winkler. 1999. The State of Record Link-
age and Current Research Problems. Technical re-
port, Statistical Research Division, U.S. Bureau of
the Census.
Xiaobing Xue and Bruce W. Croft. 2009. Automatic
Query Generation for Patent Search. In CIKM
?09: Proceeding of the eighteenth ACM conference
on Information and Knowledge Management, pages
2037?2040, New York, NY, USA. ACM.
579
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1118?1127,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Language Text Classification
using Structural Correspondence Learning
Peter Prettenhofer and Benno Stein
Bauhaus-Universita?t Weimar
D-99421 Weimar, Germany
{peter.prettenhofer,benno.stein}@uni-weimar.de
Abstract
We present a new approach to cross-
language text classification that builds on
structural correspondence learning, a re-
cently proposed theory for domain adap-
tation. The approach uses unlabeled doc-
uments, along with a simple word trans-
lation oracle, in order to induce task-
specific, cross-lingual word correspon-
dences. We report on analyses that reveal
quantitative insights about the use of un-
labeled data and the complexity of inter-
language correspondence modeling.
We conduct experiments in the field
of cross-language sentiment classification,
employing English as source language,
and German, French, and Japanese as tar-
get languages. The results are convincing;
they demonstrate both the robustness and
the competitiveness of the presented ideas.
1 Introduction
This paper deals with cross-language text classifi-
cation problems. The solution of such problems
requires the transfer of classification knowledge
between two languages. Stated precisely: We are
given a text classification task ? in a target lan-
guage T for which no labeled documents are avail-
able. ? may be a spam filtering task, a topic cate-
gorization task, or a sentiment classification task.
In addition, we are given labeled documents for
the identical task in a different source language S.
Such type of cross-language text classification
problems are addressed by constructing a clas-
sifier fS with training documents written in S
and by applying fS to unlabeled documents writ-
ten in T . For the application of fS under lan-
guage T different approaches are current practice:
machine translation of unlabeled documents from
T to S, dictionary-based translation of unlabeled
documents from T to S , or language-independent
concept modeling by means of comparable cor-
pora. The mentioned approaches have their pros
and cons, some of which are discussed below.
Here we propose a different approach to cross-
language text classification which adopts ideas
from the field of multi-task learning (Ando and
Zhang, 2005a). Our approach builds upon struc-
tural correspondence learning, SCL, a recently
proposed theory for domain adaptation in the
field of natural language processing (Blitzer et al,
2006).
Similar to SCL, our approach induces corre-
spondences among the words from both languages
by means of a small number of so-called pivots. In
our context a pivot is a pair of words, {wS , wT },
from the source language S and the target lan-
guage T , which possess a similar semantics. Test-
ing the occurrence of wS or wT in a set of unla-
beled documents from S and T yields two equiv-
alence classes across these languages: one class
contains the documents where eitherwS orwT oc-
cur, the other class contains the documents where
neither wS nor wT occur. Ideally, a pivot splits
the set of unlabeled documents with respect to the
semantics that is associated with {wS , wT }. The
correlation between wS or wT and other words w,
w 6? {wS , wT } is modeled by a linear classifier,
which then is used as a language-independent pre-
dictor for the two equivalence classes. As we will
see, a small number of pivots can capture a suffi-
ciently large part of the correspondences between
S and T in order to (1) construct a cross-lingual
representation and (2) learn a classifier fST for the
task ? that operates on this representation. Several
advantages follow from our approach:
? Task specificity. The approach exploits the
words? pragmatics since it considers?during
the pivot selection step?task-specific char-
acteristics of language use.
1118
? Efficiency in terms of linguistic resources.
The approach uses unlabeled documents
from both languages along with a small num-
ber (100 - 500) of translated words, instead
of employing a parallel corpus or an exten-
sive bilingual dictionary.
? Efficiency in terms of computing resources.
The approach solves the classification prob-
lem directly, instead of resorting to a more
general and potentially much harder problem
such as machine translation. Note that the use
of such technology is prohibited in certain sit-
uations (market competitors) or restricted by
environmental constraints (offline situations,
high latency, bandwidth capacity).
Contributions Our contributions to the outlined
field are threefold: First, the identification and uti-
lization of the theory of SCL to cross-language
text classification, which has, to the best of our
knowledge, not been investigated before. Sec-
ond, the further development and adaptation of
SCL towards a technology that is competitive with
the state-of-the-art in cross-language text classifi-
cation. Third, an in-depth analysis with respect
to important hyperparameters such as the ratio
of labeled and unlabeled documents, the number
of pivots, and the optimum dimensionality of the
cross-lingual representation. In this connection we
compile extensive corpora in the languages En-
glish, German, French, and Japanese, and for dif-
ferent sentiment classification tasks.
The paper is organized as follows: Section 2
surveys related work. Section 3 states the termi-
nology for cross-language text classification. Sec-
tion 4 describes our main contribution, a new ap-
proach to cross-language text classification based
on structural correspondence learning. Section 5
presents experimental results in the context of
cross-language sentiment classification.
2 Related Work
Cross-Language Text Classification Bel et al
(2003) belong to the first who explicitly consid-
ered the problem of cross-language text classi-
fication. Their research, however, is predated
by work in cross-language information retrieval,
CLIR, where similar problems are addressed
(Oard, 1998). Traditional approaches to cross-
language text classification and CLIR use linguis-
tic resources such as bilingual dictionaries or par-
allel corpora to induce correspondences between
two languages (Lavrenko et al, 2002; Olsson et
al., 2005). Dumais et al (1997) is considered as
seminal work in CLIR: they propose a method
which induces semantic correspondences between
two languages by performing latent semantic anal-
ysis, LSA, on a parallel corpus. Li and Taylor
(2007) improve upon this method by employing
kernel canonical correlation analysis, CCA, in-
stead of LSA. The major limitation of these ap-
proaches is their computational complexity and,
in particular, the dependence on a parallel cor-
pus, which is hard to obtain?especially for less
resource-rich languages. Gliozzo and Strappar-
ava (2005) circumvent the dependence on a par-
allel corpus by using so-called multilingual do-
main models, which can be acquired from com-
parable corpora in an unsupervised manner. In
(Gliozzo and Strapparava, 2006) they show for
particular tasks that their approach can achieve a
performance close to that of monolingual text clas-
sification.
Recent work in cross-language text classifica-
tion focuses on the use of automatic machine
translation technology. Most of these methods in-
volve two steps: (1) translation of the documents
into the source or the target language, and (2) di-
mensionality reduction or semi-supervised learn-
ing to reduce the noise introduced by the ma-
chine translation. Methods which follow this two-
step approach include the EM-based approach by
Rigutini et al (2005), the CCA approach by For-
tuna and Shawe-Taylor (2005), the information
bottleneck approach by Ling et al (2008), and the
co-training approach by Wan (2009).
Domain Adaptation Domain adaptation refers
to the problem of adapting a statistical classifier
trained on data from one (or more) source domains
(e.g., newswire texts) to a different target domain
(e.g., legal texts). In the basic domain adaptation
setting we are given labeled data from the source
domain and unlabeled data from the target domain,
and the goal is to train a classifier for the target
domain. Beyond this setting one can further dis-
tinguish whether a small amount of labeled data
from the target domain is available (Daume, 2007;
Finkel and Manning, 2009) or not (Blitzer et al,
2006; Jiang and Zhai, 2007). The latter setting is
referred to as unsupervised domain adaptation.
1119
Note that, cross-language text classification
can be cast as an unsupervised domain adapta-
tion problem by considering each language as a
separate domain. Blitzer et al (2006) propose
an effective algorithm for unsupervised domain
adaptation, called structural correspondence learn-
ing. First, SCL identifies features that general-
ize across domains, which the authors call pivots.
SCL then models the correlation between the piv-
ots and all other features by training linear clas-
sifiers on the unlabeled data from both domains.
This information is used to induce correspon-
dences among features from the different domains
and to learn a shared representation that is mean-
ingful across both domains. SCL is related to the
structural learning paradigm introduced by Ando
and Zhang (2005a). The basic idea of structural
learning is to constrain the hypothesis space of a
learning task by considering multiple different but
related tasks on the same input space. Ando and
Zhang (2005b) present a semi-supervised learning
method based on this paradigm, which generates
related tasks from unlabeled data. Quattoni et al
(2007) apply structural learning to image classifi-
cation in settings where little labeled data is given.
3 Cross-Language Text Classification
This section introduces basic models and termi-
nology.
In standard text classification, a document d
is represented under the bag-of-words model as
|V |-dimensional feature vector x ? X , where V ,
the vocabulary, denotes an ordered set of words,
xi ? x denotes the normalized frequency of word
i in d, and X is an inner product space. DS
denotes the training set and comprises tuples of
the form (x, y), which associate a feature vector
x ? X with a class label y ? Y . The goal is to
find a classifier f : X ? Y that predicts the la-
bels of new, previously unseen documents. With-
out loss of generality we restrict ourselves to bi-
nary classification problems and linear classifiers,
i.e., Y = {+1, -1} and f(x) = sign(wTx). w is a
weight vector that parameterizes the classifier, [?]T
denotes the matrix transpose. The computation of
w from DS is referred to as model estimation or
training. A common choice for w is given by a
vector w? that minimizes the regularized training
error:
w? = argmin
w?R|V |
?
(x,y)?DS
L(y, wTx) +
?
2
?w?2 (1)
L is a loss function that measures the quality
of the classifier, ? is a non-negative regulariza-
tion parameter that penalizes model complexity,
and ?w?2 = wTw. Different choices for L entail
different classifier types; e.g., when choosing the
hinge loss function for L one obtains the popular
Support Vector Machine classifier (Zhang, 2004).
Standard text classification distinguishes be-
tween labeled (training) documents and unlabeled
(test) documents. Cross-language text classifica-
tion poses an extra constraint in that training doc-
uments and test documents are written in different
languages. Here, the language of the training doc-
uments is referred to as source language S, and
the language of the test documents is referred to as
target language T . The vocabulary V divides into
VS and VT , called vocabulary of the source lan-
guage and vocabulary of the target language, with
VS ? VT = ?. I.e., documents from the training
set and the test set map on two non-overlapping
regions of the feature space. Thus, a linear classi-
fier fS trained on DS associates non-zero weights
only with words from VS , which in turn means that
fS cannot be used to classify documents written
in T .
One way to overcome this ?feature barrier? is
to find a cross-lingual representation for docu-
ments written in S and T , which enables the trans-
fer of classification knowledge between the two
languages. Intuitively, one can understand such
a cross-lingual representation as a concept space
that underlies both languages. In the following,
we will use ? to denote a map that associates the
original |V |-dimensional representation of a doc-
ument d written in S or T with its cross-lingual
representation. Once such a mapping is found the
cross-language text classification problem reduces
to a standard classification problem in the cross-
lingual space. Note that the existing methods for
cross-language text classification can be character-
ized by the way ? is constructed. For instance,
cross-language latent semantic indexing (Dumais
et al, 1997) and cross-language explicit semantic
analysis (Potthast et al, 2008) estimate ? using a
parallel corpus. Other methods use linguistic re-
sources such as a bilingual dictionary to obtain ?
(Bel et al, 2003; Olsson et al, 2005).
1120
4 Cross-Language
Structural Correspondence Learning
We now present a novel method for learning a
map ? by exploiting relations from unlabeled doc-
uments written in S and T . The proposed method,
which we call cross-language structural corre-
spondence learning, CL-SCL, addresses the fol-
lowing learning setup (see also Figure 1):
? Given a set of labeled training documentsDS
written in language S, the goal is to create a
text classifier for documents written in a dif-
ferent language T . We refer to this classifi-
cation task as the target task. An example for
the target task is the determination of senti-
ment polarity, either positive or negative, of
book reviews written in German (T ) given a
set of training reviews written in English (S).
? In addition to the labeled training docu-
ments DS we have access to unlabeled doc-
uments DS,u and DT ,u from both languages
S and T . Let Du denote DS,u ?DT ,u.
? Finally, we are given a budget of calls to a
word translation oracle (e.g., a domain ex-
pert) to map words in the source vocabu-
lary VS to their corresponding translations in
the target vocabulary VT . For simplicity and
without loss of applicability we assume here
that the word translation oracle maps each
word in VS to exactly one word in VT .
CL-SCL comprises three steps: In the first step,
CL-SCL selects word pairs {wS , wT }, called piv-
ots, where wS ? VS and wT ? VT . Pivots have to
satisfy the following conditions:
Confidence Both words, wS and wT , are predic-
tive for the target task.
Support Both words, wS and wT , occur fre-
quently in DS,u and DT ,u respectively.
The confidence condition ensures that, in the
second step of CL-SCL, only those correlations
are modeled that are useful for discriminative
learning. The support condition, on the other
hand, ensures that these correlations can be es-
timated accurately. Considering our sentiment
classification example, the word pair {excellentS ,
exzellentT } satisfies both conditions: (1) the
words are strong indicators of positive sentiment,
Words in V
S
Class
label
term frequencies
Negative class label
Positive class label
Words in V
T
... , x|V|)x = (x1 , ...
D
S
D
S,u
D
T,u
Du
No value
y
Figure 1: The document sets underlying CL-SCL.
The subscripts S , T , and u designate ?source lan-
guage?, ?target language?, and ?unlabeled?.
and (2) the words occur frequently in book reviews
from both languages. Note that the support of wS
andwT can be determined from the unlabeled data
Du. The confidence, however, can only be deter-
mined for wS since the setting gives us access to
labeled data from S only.
We use the following heuristic to form an or-
dered set P of pivots: First, we choose a subset
VP from the source vocabulary VS , |VP |  |VS |,
which contains those words with the highest mu-
tual information with respect to the class label of
the target task in DS . Second, for each word
wS ? VP we find its translation in the target vo-
cabulary VT by querying the translation oracle; we
refer to the resulting set of word pairs as the can-
didate pivots, P ? :
P ? = {{wS , TRANSLATE(wS)} | wS ? VP }
We then enforce the support condition by elim-
inating in P ? all candidate pivots {wS , wT } where
the document frequency of wS in DS,u or of wT
in DT ,u is smaller than some threshold ?:
P = CANDIDATEELIMINATION(P ?, ?)
Let m denote |P |, the number of pivots.
In the second step, CL-SCL models the corre-
lations between each pivot {wS , wT } ? P and all
other words w ? V \ {wS , wT }. This is done by
training linear classifiers that predict whether or
not wS or wT occur in a document, based on the
other words. For this purpose a training set Dl is
created for each pivot pl ? P :
Dl = {(MASK(x, pl), IN(x, pl)) | x ? Du}
1121
MASK(x, pl) is a function that returns a copy of
x where the components associated with the two
words in pl are set to zero?which is equivalent
to removing these words from the feature space.
IN(x, pl) returns +1 if one of the components of x
associated with the words in pl is non-zero and -1
otherwise. For each Dl a linear classifier, charac-
terized by the parameter vector wl, is trained by
minimizing Equation (1) on Dl. Note that each
training set Dl contains documents from both lan-
guages. Thus, for a pivot pl = {wS , wT } the vec-
tor wl captures both the correlation between wS
and VS \ {wS} and the correlation between wT
and VT \ {wT }.
In the third step, CL-SCL identifies correlations
across pivots by computing the singular value de-
composition of the |V |?m-dimensional parameter
matrix W, W =
[
w1 . . . wm
]
:
U?VT = SVD(W)
Recall that W encodes the correlation structure
between pivot and non-pivot words in the form
of multiple linear classifiers. Thus, the columns
of U identify common substructures among these
classifiers. Choosing the columns of U associated
with the largest singular values yields those sub-
structures that capture most of the correlation in
W. We define ? as those columns of U that are
associated with the k largest singular values:
? = UT[1:k, 1:|V |]
Algorithm 1 summarizes the three steps of CL-
SCL. At training and test time, we apply the pro-
jection ? to each input instance x. The vector v?
that minimizes the regularized training error for
DS in the projected space is defined as follows:
v? = argmin
v?Rk
?
(x,y)?DS
L(y, vT ?x) +
?
2
?v?2 (2)
The resulting classifier fST , which will operate
in the cross-lingual setting, is defined as follows:
fST (x) = sign(v
?T ?x)
4.1 An Alternative View of CL-SCL
An alternative view of cross-language structural
correspondence learning is provided by the frame-
work of structural learning (Ando and Zhang,
2005a). The basic idea of structural learning is
Algorithm 1 CL-SCL
Input: Labeled source data DS
Unlabeled data Du = DS,u ?DT ,u
Parameters: m, k, ?, and ?
Output: k ? |V |-dimensional matrix ?
1. SELECTPIVOTS(DS ,m)
VP = MUTUALINFORMATION(DS )
P ? = {{wS , TRANSLATE(wS)} | wS ? VP }
P = CANDIDATEELIMINATION(P ?, ?)
2. TRAINPIVOTPREDICTORS(Du,P )
for l = 1 to m do
Dl = {(MASK(x, pl), IN(x, pl)) | x ? Du}
wl= argmin
w?R|V |
?
(x,y)?Dl
L(y,wTx)) + ?2 ?w?
2
end for
W =
[
w1 . . . wm
]
3. COMPUTESVD(W, k)
U?VT = SVD(W)
? = UT[1:k, 1:|V |]
output {?}
to constrain the hypothesis space, i.e., the space of
possible weight vectors, of the target task by con-
sidering multiple different but related prediction
tasks. In our context these auxiliary tasks are rep-
resented by the pivot predictors, i.e., the columns
of W. Each column vector wl can be considered
as a linear classifier which performs well in both
languages. I.e., we regard the column space of W
as an approximation to the subspace of bilingual
classifiers. By computing SVD(W) one obtains
a compact representation of this column space in
the form of an orthonormal basis ?T .
The subspace is used to constrain the learning of
the target task by restricting the weight vector w to
lie in the subspace defined by ?T . Following Ando
and Zhang (2005a) and Quattoni et al (2007) we
choose w for the target task to be w? = ?Tv?,
where v? is defined as follows:
v? = argmin
v?Rk
?
(x,y)?DS
L(y, (?Tv)Tx) +
?
2
?v?2 (3)
Since (?Tv)T = vT ? it follows that this view
of CL-SCL corresponds to the induction of a new
feature space given by Equation 2.
1122
5 Experiments
We evaluate CL-SCL for the task of cross-
language sentiment classification using English
as source language and German, French, and
Japanese as target languages. Special emphasis is
put on corpus construction, determination of upper
bounds and baselines, and a sensitivity analysis of
important hyperparameters. All data described in
the following is publicly available from our project
website.1
5.1 Dataset and Preprocessing
We compiled a new dataset for cross-language
sentiment classification by crawling product re-
views from Amazon.{de | fr | co.jp}. The crawled
part of the corpus contains more than 4 million
reviews in the three languages German, French,
and Japanese. The corpus is extended with En-
glish product reviews provided by Blitzer et al
(2007). Each review contains a category label,
a title, the review text, and a rating of 1-5 stars.
Following Blitzer et al (2007) a review with >3
(<3) stars is labeled as positive (negative); other
reviews are discarded. For each language the la-
beled reviews are grouped according to their cate-
gory label, whereas we restrict our experiments to
three categories: books, dvds, and music.
Since most of the crawled reviews are posi-
tive (80%), we decide to balance the number of
positive and negative reviews. In this study, we
are interested in whether the cross-lingual repre-
sentation induced by CL-SCL captures the differ-
ence between positive and negative reviews; by
balancing the reviews we ensure that the imbal-
ance does not affect the learned model. Balancing
is achieved by deleting reviews from the major-
ity class uniformly at random for each language-
specific category. The resulting sets are split into
three disjoint, balanced sets, containing training
documents, test documents, and unlabeled docu-
ments; the respective set sizes are 2,000, 2,000,
and 9,000-50,000. See Table 1 for details.
For each of the nine target-language-category-
combinations a text classification task is created
by taking the training set of the product category in
S and the test set of the same product category in
T . A document d is described as normalized fea-
ture vector x under a unigram bag-of-words docu-
ment representation. The morphological analyzer
1http://www.webis.de/research/corpora/
webis-cls-10/
MeCab is used for Japanese word segmentation.2
5.2 Implementation
Throughout the experiments linear classifiers are
employed; they are trained by minimizing Equa-
tion (1), using a stochastic gradient descent (SGD)
algorithm. In particular, the learning rate schedule
from PEGASOS is adopted (Shalev-Shwartz et al,
2007), and the modified Huber loss, introduced by
Zhang (2004), is chosen as loss function L.3
SGD receives two hyperparameters as input: the
number of iterations T , and the regularization pa-
rameter ?. In our experiments T is always set to
106, which is about the number of iterations re-
quired for SGD to converge. For the target task,
? is determined by 3-fold cross-validation, testing
for ? all values 10?i, i ? [0; 6]. For the pivot pre-
diction task, ? is set to the small value of 10?5, in
order to favor model accuracy over generalizabil-
ity.
The computational bottleneck of CL-SCL is the
SVD of the dense parameter matrix W. Here we
follow Blitzer et al (2006) and set the negative
values in W to zero, which yields a sparse repre-
sentation. For the SVD computation the Lanczos
algorithm provided by SVDLIBC is employed.4
We investigated an alternative approach to obtain
a sparse W by directly enforcing sparse pivot pre-
dictors wl through L1-regularization (Tsuruoka et
al., 2009), but didn?t pursue this strategy due to
unstable results. Since SGD is sensitive to fea-
ture scaling the projection ?x is post-processed as
follows: (1) Each feature of the cross-lingual rep-
resentation is standardized to zero mean and unit
variance, where mean and variance are estimated
on DS ?Du. (2) The cross-lingual document rep-
resentations are scaled by a constant ? such that
|DS |
?1?
x?DS
???x? = 1.
We use Google Translate as word translation or-
acle, which returns a single translation for each
query word.5 Though such a context free transla-
tion is suboptimum we do not sanitize the returned
words to demonstrate the robustness of CL-SCL
with respect to translation noise. To ensure the re-
producibility of our results we cache all queries to
the translation oracle.
2http://mecab.sourceforge.net
3Our implementation is available at http://github.
com/pprett/bolt
4http://tedlab.mit.edu/?dr/SVDLIBC/
5http://translate.google.com
1123
T Category
Unlabeled data Upper Bound CL-MT CL-SCL
|DS,u| |DT ,u| ? ? ? ? ? ? ? ?
books 50,000 50,000 83.79 (?0.20) 79.68 (?0.13) 4.11 79.50 (?0.33) 4.29
German dvd 30,000 50,000 81.78 (?0.27) 77.92 (?0.25) 3.86 76.92 (?0.07) 4.86
music 25,000 50,000 82.80 (?0.13) 77.22 (?0.23) 5.58 77.79 (?0.02) 5.00
books 50,000 32,000 83.92 (?0.14) 80.76 (?0.34) 3.16 78.49 (?0.03) 5.43
French dvd 30,000 9,000 83.40 (?0.28) 78.83 (?0.19) 4.57 78.80 (?0.01) 4.60
music 25,000 16,000 86.09 (?0.13) 75.78 (?0.65) 10.31 77.92 (?0.03) 8.17
books 50,000 50,000 79.39 (?0.27) 70.22 (?0.27) 9.17 73.09 (?0.07) 6.30
Japanese dvd 30,000 50,000 81.56 (?0.28) 71.30 (?0.28) 10.26 71.07 (?0.02) 10.49
music 25,000 50,000 82.33 (?0.13) 72.02 (?0.29) 10.31 75.11 (?0.06) 7.22
Table 1: Cross-language sentiment classification results. For each task, the number of unlabeled docu-
ments from S and T is given. Accuracy scores (mean ? and standard deviation ? of 10 repetitions of
SGD) on the test set of the target language T are reported. ? gives the difference in accuracy to the
upper bound. CL-SCL uses m = 450, k = 100, and ? = 30.
5.3 Upper Bound and Baseline
To get an upper bound on the performance of
a cross-language method we first consider the
monolingual setting. For each target-language-
category-combination a linear classifier is learned
on the training set and tested on the test set. The
resulting accuracy scores are referred to as upper
bound; it informs us about the expected perfor-
mance on the target task if training data in the tar-
get language is available.
We chose a machine translation baseline
to compare CL-SCL to another cross-language
method. Statistical machine translation technol-
ogy offers a straightforward solution to the prob-
lem of cross-language text classification and has
been used in a number of cross-language senti-
ment classification studies (Hiroshi et al, 2004;
Bautin et al, 2008; Wan, 2009). Our baseline
CL-MT works as follows: (1) learn a linear clas-
sifier on the training data, and (2) translate the test
documents into the source language,6 (3) predict
6Again we use Google Translate.
the sentiment polarity of the translated test doc-
uments. Note that the baseline CL-MT does not
make use of unlabeled documents.
5.4 Performance Results and Sensitivity
Table 1 contrasts the classification performance of
CL-SCL with the upper bound and with the base-
line. Observe that the upper bound does not ex-
hibit a great variability across the three languages.
The average accuracy is about 82%, which is con-
sistent with prior work on monolingual sentiment
analysis (Pang et al, 2002; Blitzer et al, 2007).
The performance of CL-MT, however, differs con-
siderably between the two European languages
and Japanese: for Japanese, the average difference
between the upper bound and CL-MT (9.9%) is
about twice as much as for German and French
(5.3%). This difference can be explained by the
fact that machine translation works better for Eu-
ropean than for Asian languages such as Japanese.
Recall that CL-SCL receives three hyperparam-
eters as input: the number of pivots m, the di-
mensionality of the cross-lingual representation k,
Pivot
English German
Semantics Pragmatics Semantics Pragmatics
{beautifulS , scho?nT } amazing, beauty, picture, pattern, poetry, scho?ner (more beautiful), bilder (pictures),
lovely photographs, paintings traurig (sad) illustriert (illustrated)
{boringS , langweiligT } plain, asleep, characters, pages, langatmig (lengthy), charaktere (characters),
dry, long story einfach (plain), handlung (plot),
entta?uscht (disappointed) seiten (pages)
Table 2: Semantic and pragmatic correlations identified for the two pivots {beautifulS , scho?nT } and
{boringS , langweiligT } in English and German book reviews.
1124
Figure 2: Influence of unlabeled data and hyperparameters on the performance of CL-SCL. The rows
show the performance of CL-SCL as a function of (1) the ratio between labeled and unlabeled documents,
(2) the number of pivots m, and (3) the dimensionality of the cross-lingual representation k.
and the minimum support ? of a pivot in DS,u
and DT ,u. For comparison purposes we use fixed
values of m = 450, k = 100, and ? = 30.
The results show the competitiveness of CL-SCL
compared to CL-MT. Although CL-MT outper-
forms CL-SCL on most tasks for German and
French, the difference in accuracy can be consid-
ered as small (<1%); merely for French book and
music reviews the difference is about 2%. For
Japanese, however, CL-SCL outperforms CL-MT
on most tasks with a difference in accuracy of
about 3%. The results indicate that if the dif-
ference between the upper bound and CL-MT is
large, CL-SCL can circumvent the loss in accu-
racy. Experiments with language-specific settings
revealed that for Japanese a smaller number of piv-
ots (150<m<250) performs significantly better.
Thus, the reported results for Japanese can be con-
sidered as pessimistic.
Primarily responsible for the effectiveness of
CL-SCL is its task specificity, i.e., the ways in
which context contributes to meaning (pragmat-
ics). Due to the use of task-specific, unlabeled
data, relevant characteristics are captured by the
pivot classifiers. Table 2 exemplifies this with two
pivots for German book reviews. The rows of the
table show those words which have the highest
correlation with the pivots {beautifulS , scho?nT }
and {boringS , langweiligT }. We can distinguish
between (1) correlations that reflect similar mean-
ing, such as ?amazing?, ?lovely?, or ?plain?, and
(2) correlations that reflect the pivot pragmatics
with respect to the task, such as ?picture?, ?po-
etry?, or ?pages?. Note in this connection that au-
thors of book reviews tend to use the word ?beau-
tiful? to refer to illustrations or poetry. While the
first type of word correlations can be obtained by
methods that operate on parallel corpora, the sec-
ond type of correlation requires an understanding
of the task-specific language use.
In the following we discuss the sensitivity of
each hyperparameter in isolation while keeping
1125
the others fixed atm = 450, k = 100, and ? = 30.
The experiments are illustrated in Figure 2.
Unlabeled Data The first row of Figure 2 shows
the performance of CL-SCL as a function of the
ratio of labeled and unlabeled documents. A ratio
of 1 means that |DS,u| = |DT ,u| = 2,000, while
a ratio of 25 corresponds to the setting of Table 1.
As expected, an increase in unlabeled documents
results in an improved performance, however, we
observe a saturation at a ratio of 10 across all nine
tasks.
Number of Pivots The second row shows the in-
fluence of the number of pivots m on the perfor-
mance of CL-SCL. Compared to the size of the
vocabularies VS and VT , which is in 105 order
of magnitude, the number of pivots is very small.
The plots show that even a small number of piv-
ots captures a significant amount of the correspon-
dence between S and T .
Dimensionality of the Cross-Lingual Represen-
tation The third row shows the influence of the
dimensionality of the cross-lingual representation
k on the performance of CL-SCL. Obviously the
SVD is crucial to the success of CL-SCL if m
is sufficiently large. Observe that the value of k
is task-insensitive: a value of 75<k<150 works
equally well across all tasks.
6 Conclusion
The paper introduces a novel approach to cross-
language text classification, called cross-language
structural correspondence learning. The approach
uses unlabeled documents along with a word
translation oracle to automatically induce task-
specific, cross-lingual correspondences. Our con-
tributions include the adaptation of SCL for the
problem of cross-language text classification and
a well-founded empirical analysis. The analy-
sis covers performance and robustness issues in
the context of cross-language sentiment classifica-
tion with English as source language and German,
French, and Japanese as target languages. The re-
sults show that CL-SCL is competitive with state-
of-the-art machine translation technology while
requiring fewer resources.
Future work includes the extension of CL-SCL
towards a general approach for cross-lingual adap-
tation of natural language processing technology.
References
Rie-K. Ando and Tong Zhang. 2005a. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817?
1853.
Rie-K. Ando and Tong Zhang. 2005b. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of ACL-05, pages 1?
9, Ann Arbor.
Mikhail Bautin, Lohit Vijayarenu, and Steven Skiena.
2008. International sentiment analysis for news and
blogs. In Proceedings of ICWSM-08, pages 19?26,
Seattle.
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In Proceed-
ings of ECDL-03, pages 126?139, Trondheim.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural corre-
spondence learning. In Proceedings of EMNLP-06,
pages 120?128, Sydney.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL-07, pages 440?447,
Prague.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL-07, pages 256?263,
Prague.
Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic cross-language retrieval using latent semantic
indexing. In AAAI Symposium on CrossLanguage
Text and Speech Retrieval.
Jenny-R. Finkel and Christopher-D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of HLT/NAACL-09, pages 602?610, Boul-
der.
Blaz? Fortuna and John Shawe-Taylor. 2005. The use
of machine translation tools for cross-lingual text
mining. In Proceedings of the ICML Workshop on
Learning with Multiple Views.
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts.
Alfio Gliozzo and Carlo Strapparava. 2006. Exploit-
ing comparable corpora and bilingual dictionaries
for cross-language text categorization. In Proceed-
ings of ACL-06, pages 553?560, Sydney.
Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe
Hideo. 2004. Deeper sentiment analysis using
machine translation technology. In Proceedings of
COLING-04, pages 494?500, Geneva.
1126
Jing Jiang and Chengxiang Zhai. 2007. A two-stage
approach to domain adaptation for statistical classi-
fiers. In Proceedings of CIKM-07, pages 401?410,
Lisbon.
Victor Lavrenko, Martin Choquette, and W. Bruce
Croft. 2002. Cross-lingual relevance models. In
Proceedings of SIGIR-02, pages 175?182, Tampere.
Yaoyong Li and John S. Taylor. 2007. Advanced
learning algorithms for cross-language patent re-
trieval and classification. Inf. Process. Manage.,
43(5):1183?1199.
Xiao Ling, Gui-R. Xue, Wenyuan Dai, Yun Jiang,
Qiang Yang, and Yong Yu. 2008. Can chinese web
pages be classified with english data source? In Pro-
ceedings of WWW-08, pages 969?978, Beijing.
Douglas W. Oard. 1998. A comparative study of query
and document translation for cross-language infor-
mation retrieval. In Proceedings of AMTA-98, pages
472?483, Langhorne.
J. Scott Olsson, Douglas W. Oard, and Jan Hajic?. 2005.
Cross-language text classification. In Proceedings
of SIGIR-05, pages 645?646, Salvador.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP-02, pages 79?86, Philadelphia.
Martin Potthast, Benno Stein, and Maik Anderka.
2008. A wikipedia-based multilingual retrieval
model. In Proceedings of ECIR-08, pages 522?530,
Glasgow.
Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2007. Learning visual representations using images
with captions. In Proceedings of CVPR-07, pages
1?8, Minneapolis.
Leonardo Rigutini, Marco Maggini, and Bing Liu.
2005. An em based training algorithm for cross-
language text categorization. In Proceedings of WI-
05, pages 529?535, Compie`gne.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient
solver for svm. In Proceedings of ICML-07, pages
807?814, Corvalis.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL/AFNLP-09, pages
477?485, Singapore.
Xiaojun Wan. 2009. Co-training for cross-
lingual sentiment classification. In Proceedings of
ACL/AFNLP-09, pages 235?243, Singapore.
Tong Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In Proceedings of ICML-04, pages 116?
124, Banff.
1127
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1212?1221,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Crowdsourcing Interaction Logs to Understand Text Reuse from the Web
Martin Potthast Matthias Hagen Michael V?lske Benno Stein
Bauhaus-Universit?t Weimar
99421 Weimar, Germany
<first name>.<last name>@uni-weimar.de
Abstract
We report on the construction of the Webis
text reuse corpus 2012 for advanced re-
search on text reuse. The corpus compiles
manually written documents obtained from
a completely controlled, yet representative
environment that emulates the web. Each
of the 297 documents in the corpus is about
one of the 150 topics used at the TREC
Web Tracks 2009?2011, thus forming a
strong connection with existing evaluation
efforts. Writers, hired at the crowdsourc-
ing platform oDesk, had to retrieve sources
for a given topic and to reuse text from
what they found. Part of the corpus are
detailed interaction logs that consistently
cover the search for sources as well as the
creation of documents. This will allow for
in-depth analyses of how text is composed
if a writer is at liberty to reuse texts from a
third party?a setting which has not been
studied so far. In addition, the corpus pro-
vides an original resource for the evalua-
tion of text reuse and plagiarism detectors,
where currently only less realistic resources
are employed.
1 Introduction
The web has become one of the most common
sources for text reuse. When reusing text from
the web, humans may follow a three step ap-
proach shown in Figure 1: searching for appro-
priate sources on a given topic, copying of text
from selected sources, modification and paraphras-
ing of the copied text. A considerable body of
research deals with the detection of text reuse, and,
in particular, with the detection of cases of plagia-
rism (i.e., the reuse of text with the intent of disguis-
ing the fact that text has been reused). Similarly,
a large number of commercial software systems is
being developed whose purpose is the detection of
plagiarism. Both the developers of these systems as
well as researchers working on the subject matter
frequently claim their approaches to be searching
the entire web or, at least, to be scalable to web
size. However, there is hardly any evidence to
substantiate this claim?rather the opposite can be
observed: commercial plagiarism detectors have
not been found to reliably identify plagiarism from
the web (K?hler and Weber-Wulff, 2010), and the
evaluation of research prototypes even under lab-
oratory conditions shows that there is still a long
way to go (Potthast et al, 2010b). We explain the
disappointing state of the art by the lack of realistic,
large-scale evaluation resources.
With our work, we want to contribute to closing
the gap. In this regard the paper in hand introduces
the Webis text reuse corpus 2012 (Webis-TRC-12),
which, for the first time, emulates the entire process
of reusing text from the web, both at scale and in
a controlled environment. The corpus comprises a
number of features that set it apart from previous
ones: (1) the topic of each document in the corpus
is derived from a topic of the TREC Web Track,
and the sources to copy from have been retrieved
manually from the ClueWeb corpus. (2) The search
for sources is logged, including click-through and
browsing data. (3) A fine-grained edit history has
been recorded for each document. (4) A total of
297 documents were written with an average length
of about 5700 words, whereas diversity is ensured
via crowdsourcing. Altogether, this corpus forms
the current most realistic sample of writers reusing
text. The corpus is publicly available.1
1.1 Related Work
As organizers of the annual PAN plagiarism de-
tection competitions,2 we have introduced the first
standardized evaluation framework for that pur-
1http://www.webis.de/research/corpora
2http://pan.webis.de
1212
Search I?m Feeling Lucky
Search Copy & Paste Modification
Figure 1: The basic steps of reusing text from the web (Potthast, 2011).
pose (Potthast et al, 2010b). Among others, it com-
prises a series of corpora that consist of automat-
ically generated cases of plagiarism, provided in
the form of the PAN plagiarism corpora 2009-2011.
The corpora have been used to evaluate dozens of
plagiarism detection approaches within the respec-
tive competitions in these years;3 but even though
they have been adopted by the community, a num-
ber of shortcomings render them less realistic:
1. All plagiarism cases were generated by ran-
domly selecting text passages from documents
and inserting them at random positions in a
host document. This way, the reused passages
do not match the topic of the host document.
2. The majority of the reused passages were mod-
ified in order to obfuscate the reuse. However,
the applied modification strategies, again, are
basically random: shuffling, replacing, insert-
ing, or deleting words randomly. An effort
was made to avoid non-readable text, yet none
of it bears any semantics.
3. The corpus documents are parts of books from
the Project Gutenberg. Many of these books
are pretty old, whereas today the web is the
predominant source for text reuse.
To overcome the second issue, about 4 000 pas-
sages were rewritten manually via crowdsourcing
on Amazon?s Mechanical Turk for the 2011 cor-
pus. But, because of the first issue (random passage
insertion), a topic drift analysis can spot a reused
passage more easily than a search within the doc-
ument set containing the original source (Potthast
et al, 2011). From these observations it becomes
clear that there are limits for the automatic con-
struction of such kinds of corpora. The Webis text
reuse corpus 2012 addresses all of the mentioned
issues since it has been constructed manually.
3See (Potthast et al, 2009; Potthast et al, 2010a; Potthast
et al, 2011) for overviews of approaches and evaluation results
of each competition.
Besides the PAN corpora, there are two other
corpora that comprise ?genuinely reused? text: the
Clough09 corpus, and the Meter corpus. The for-
mer corpus consists of 57 answers to one of five
computer science questions that were reused from
a respective Wikipedia article (Clough and Steven-
son, 2011). While the text was genuinely written by
a number of volunteer students, the choice of topics
is narrow, and text lengths range from 200 to 300
words, which is hardly more than 2-3 paragraphs.
Also, the sources from which text was reused were
given up front, so that there is no data about their
retrieval. The Meter corpus annotates 445 cases
of text reuse among 1 716 news articles (Clough et
al., 2002). The cases of text reuse in this corpus
are realistic for the news domain; however, they
have not been created by the reuse process outlined
in Figure 1. Note that in the news domain, text is
often reused directly from a news wire without the
need for retrieval. Our new corpus complements
these two resources.
2 Corpus Construction
Two data sets form the basis for constructing our
corpus, namely (1) a set of topics to write about
and (2) a set of web pages to research about a given
topic. With regard to the former, we resort to topics
used at TREC, specifically to those used at the Web
Tracks 2009?2011. With regard to the latter, we em-
ploy the ClueWeb corpus from 20094 (and not the
?web in the wild?). The ClueWeb comprises more
than one billion documents from ten languages and
can be considered as a representative cross-section
of the real web. It is a widely accepted resource
among researchers and became one of the primary
resources to evaluate the retrieval performance of
search engines within several TREC tracks. Our
corpus?s strong connection to TREC will allow for
unforeseen synergies. Based on these decisions our
4http://lemurproject.org/clueweb09
1213
corpus construction steps can be summarized as
follows:
1. Rephrasing of the 150 topics used at the
TREC Web Tracks 2009?2011 so that they
explicitly invite people to write an essay.
2. Indexing of the ClueWeb corpus category A
(the entire English portion with about 0.5 bil-
lion documents) using the BM25F retrieval
model plus additional features.
3. Development of a search interface that allows
for answering queries within milliseconds and
that is designed along the lines of commercial
search interfaces.
4. Development of a browsing API for the
ClueWeb, which serves ClueWeb pages on
demand and which rewrites links of delivered
pages, now pointing to their corresponding
ClueWeb pages on our servers (instead of to
the originally crawled URL).
5. Recruiting 27 writers, 17 of whom with a
professional writing background, hired at the
crowdsourcing platform oDesk from a wide
range of hourly rates for diversity.
6. Instructing the writers to write one essay at
a time of at least 5000 words length (cor-
responding to an average student?s home-
work assignment) about an open topic of
their choice, using our search engine?hence
browsing only ClueWeb pages.
7. Logging all writers? interactions with the
search engine and the ClueWeb on a per-essay
basis at our site.
8. Logging all writers? edits to their essays in a
fine-grained edit log: a snapshot was taken
whenever a writer stopped writing for more
than 300ms.
9. Double-checking all of the essays for quality.
After having deployed the search engine and
completed various usability tests, the actual corpus
construction took nine months, from April 2012
through December 2012.
Obviously, the outlined experimental setup can
serve different lines of research and is publicly
available as well. The remainder of the section
presents elements of our setup in greater detail.
2.1 Topic Preparation
Since the topics used at the TREC Web Tracks were
not amenable for our purpose as is, we rephrased
them so that they ask for writing an essay instead of
searching for facts. Consider for example topic 001
of the TREC Web Track 2009:
Query. obama family tree
Description. Find information on Pres-
ident Barack Obama?s family history,
including genealogy, national origins,
places and dates of birth, etc.
Sub-topic 1. Find the TIME magazine
photo essay ?Barack Obama?s Family
Tree.?
Sub-topic 2. Where did Barack Obama?s
parents and grandparents come from?
Sub-topic 3. Find biographical informa-
tion on Barack Obama?s mother.
This topic is rephrased as follows:
Obama?s family. Write about President
Barack Obama?s family history, includ-
ing genealogy, national origins, places
and dates of birth, etc. Where did Barack
Obama?s parents and grandparents come
from? Also include a brief biography of
Obama?s mother.
In the example, Sub-topic 1 is considered too
specific for our purposes while the other sub-topics
are retained. TREC Web Track topics divide into
faceted and ambiguous topics. While topics of
the first kind can be directly rephrased into essay
topics, from topics of the second kind one of the
available interpretations was chosen.
2.2 A Controlled Web Search Environment
To give the oDesk writers a familiar search experi-
ence while maintaining reproducibility at the same
time, we developed a tailored search engine called
ChatNoir (Potthast et al, 2012b).5 Besides ours,
the only other public search engine for the ClueWeb
is Carnegie Mellon?s Indri,6 which, unfortunately,
is far from our efficiency requirements. Moreover,
its search interface does not follow the standard in
terms of result page design, and it does not give
access to interaction logs. Our search engine is
on the order of milliseconds in terms of retrieval
5http://chatnoir.webis.de
6http://lemurproject.org/clueweb09.php/index.php#Services
1214
time, its interface follows industry standards, and
it features an API that allows for user tracking.
ChatNoir is based on the BM25F retrieval
model (Robertson et al, 2004), uses the anchor
text list provided by (Hiemstra and Hauff, 2010),
the PageRanks provided by the Carnegie Mellon
University alongside the ClueWeb corpus, and the
Spam rank list provided by (Cormack et al, 2011).
ChatNoir comes with a proximity feature with
variable-width buckets as described by (Elsayed
et al, 2011). Our choice of retrieval model and
ranking features is intended to provide a reasonable
baseline performance. However, it is neither near
as mature as those of commercial search engines
nor does it compete with the best-performing mod-
els from TREC. Yet, it is among the most widely
accepted models in information retrieval, which
underlines our goal of reproducibility.
In addition to its retrieval model, ChatNoir im-
plements two search facets: text readability scoring
and long text search. The first facet, similar to that
provided by Google, scores the readability of a text
found on a web page via the well-known Flesch-
Kincaid grade level formula (Kincaid et al, 1975):
it estimates the number of years of education re-
quired in order to understand a given text. This
number is mapped onto the three categories ?Sim-
ple? (up to 5 years), ?Intermediate? (between 5 and
9 years) and ?Expert? (at least 9 years). The ?Long
Text? search facet omits search results which do
not contain at least one continuous paragraph of
text that exceeds 300 words. The two facets can be
combined with each other.
When clicking on a search result, ChatNoir does
not link into the real web but redirects into the
ClueWeb. Though the ClueWeb provides the orig-
inal URLs from which the web pages have been
obtained, many of these pages have gone or been
updated since. We hence set up an API that serves
web pages from the ClueWeb on demand: when
accessing a web page, it is pre-processed before
being shipped, removing automatic referrers and
replacing all links to the real web with links to
their counterpart inside the ClueWeb. This way,
the ClueWeb can be browsed as if surfing the real
web, whereas it becomes possible to track a user.
The ClueWeb is stored in the HDFS of our 40 node
Hadoop cluster, and web pages are fetched directly
from there with latencies of about 200ms. Chat-
Noir?s inverted index has been optimized to guaran-
tee fast response times, and it is deployed alongside
Hadoop on the same cluster.
Table 1: Demographics of the 12 Batch 2 writers.
Writer Demographics
Age Gender Native language(s)
Minimum 24 Female 67% English 67%
Median 37 Male 33% Filipino 25%
Maximum 65 Hindi 17%
Academic degree Country of origin Second language(s)
Postgraduate 41% UK 25% English 33%
Undergraduate 25% Philippines 25% French 17%
None 17% USA 17% Afrikaans, Dutch,
n/a 17% India 17% German, Spanish,
Australia 8% Swedish each 8%
South Africa 8% None 8%
Years of writing Search engines used Search frequency
Minimum 2 Google 92% Daily 83%
Median 8 Bing 33% Weekly 8%
Standard dev. 6 Yahoo 25% n/a 8%
Maximum 20 Others 8%
2.3 Two Batches of Writing
In order to not rely only on the retrieval model
implemented in our controlled web search envi-
ronment, we divided the task into two batches, so
that two essays had to be written for each of the
150 topics, namely one in each batch. In Batch 1,
our writers did not search for sources themselves,
but they were provided up front with an average
of 20 search results to choose from for each topic.
These results were obtained from the TREC Web
Track relevance judgments (so-called ?qrels?): only
documents that were found to be relevant or key
documents for a given topic by manual inspection
of the NIST assessors were provided to our writ-
ers. These documents result from the combined
wisdom of all retrieval models of the TREC Web
Tracks 2009?2011, and hence can be considered
as optimum retrieval results produced by the state
of the art in search engine technology. In Batch 2,
in order to obtain realistic search interaction logs,
our writers were instructed to search for source
documents using ChatNoir.
2.4 Crowdsourcing Writers
Our ideal writer has experience in writing, is ca-
pable of writing about a diversity of topics, can
complete a text in a timely manner, possesses de-
cent English writing skills, and is well-versed in
using the aforementioned technologies. After boot-
strapping our setup with 10 volunteers recruited at
our university, it became clear that, because of the
workload involved, accomplishing our goals would
not be possible with volunteers only. Therefore, we
resorted to hiring (semi-)professional writers and
made use of the crowdsourcing platform oDesk.7
Crowdsourcing has quickly become one of the
7http://www.odesk.com
1215
Table 2: Key figures of the Webis text reuse corpus 2012.
Corpus Distribution Total
characteristic min avg max stdev
Writers (Batch 1+2) 27
Essays (Topics) (Two essays per topic) 297 (150)
Essays / Writer 1 2 66 15.9
Queries (Batch 2) 13 655
Queries / Essay 4 91.0 616 83.1
Clicks (Batch 2) 16 739
Clicks / Essay 12 111.6 443 80.3
Clicks / Query 1 2.3 76 3.3
Irrelevant (Batch 2) 5 962
Irrelevant / Essay 1 39.8 182 28.7
Irrelevant / Query 0 0.5 60 1.4
Relevant (Batch 2) 251
Relevant / Essay 0 1.7 7 1.5
Relevant / Query 0 0.0 4 0.2
Key (Batch 2) 1 937
Key / Essay 1 12.9 46 7.5
Key / Query 0 0.2 22 0.7
Corpus Distribution Total
characteristic min avg max stdev
Search Sessions (Batch 2) 931
Sessions / Essay 1 12.3 149 18.9
Days (Batch 2) 201
Days / Essay 1 4.9 17 2.7
Hours (Batch 2) 2 068
Hours / Writer 3 129.3 679 167.3
Hours / Essay 3 7.5 10 2.5
Edits (Batch 1+2) 633 334
Edits / Essay 45 2 132.4 6 975 1 444.9
Edits / Day 5 2 959.5 8 653 1 762.5
Words (Batch 1+2) 1 704 354
Words / Essay 260 5 738.8 15 851 1 604.3
Words / Writer 2 078 63 124.2 373 975 89 246.7
Sources (Batch 1+2) 4 582
Sources / Essay 0 15.4 69 10.0
Sources / Writer 5 169.7 1 065 269.6
cornerstones for constructing evaluation corpora,
which is especially true for paid crowdsourcing.
Compared to Amazon?s Mechanical Turk (Barr
and Cabrera, 2006), which is used more frequently
than oDesk, there are virtually no workers at oDesk
submitting fake results because of its advanced rat-
ing features for workers and employers. Moreover,
oDesk tracks their workers by randomly taking
screenshots, which are provided to employers in or-
der to check whether the hours logged correspond
to work-related activity. This allowed us to check
whether our writers used our environment instead
of other search engines and editors.
During Batch 2, we have conducted a survey
among the twelve writers who worked for us at
that time. Table 1 gives an overview of the demo-
graphics of these writers, based on a questionnaire
and their resumes at oDesk. Most of them come
from an English-speaking country, and almost all
of them speak more than one language, which sug-
gests a reasonably good education. Two thirds of
the writers are female, and all of them have years
of writing experience. Hourly wages were negoti-
ated individually and range from 3 to 34 US dollars
(dependent on skill and country of residence), with
an average of about 12 US dollars. For ethical rea-
sons, we payed at least the minimum wage of the
respective countries involved. In total, we spent
20 468 US dollars to pay the writers?an amount
that may be considered large compared to other
scientific crowdsourcing efforts from the literature,
but small in terms of the potential of crowdsourcing
to make a difference in empirical science.
3 Corpus Analysis
This section presents selected results of a prelim-
inary corpus analysis. We overview the data and
shed some light onto the search and writing behav-
ior of writers.
3.1 Corpus Statistics
Table 2 shows key figures of the collected inter-
action logs, including the absolute numbers of
queries, relevance judgments, working times, num-
ber of edits, words, and retrieved sources, as well
as their relation to essays, writers, and work time,
where applicable. On average, each writer wrote
2 essays while the standard deviation is 15.9, since
one very prolific writer managed to write 66 essays.
From a total of 13 655 queries submitted by the
writers within Batch 2, each essay got an aver-
age of 91 queries. The average number of results
clicked per query is 2.3. For comparison, we com-
puted the average number of clicks per query in
the AOL query log (Pass et al, 2006), which is 2.0.
In this regard, the behavior of our writers on indi-
vidual queries does not differ much from that of
the average AOL user in 2006. Most of the clicks
that we recorded are search result clicks, whereas
2 457 of them are browsing clicks on web page
links. Among the browsing clicks, 11.3% are clicks
on links that point to the same web page (i.e., an-
chor links using the hash part of a URL). The
longest click trail contains 51 unique web pages,
but most trails are very short. This is a surprising
result, since we expected a larger proportion of
browsing clicks, but it also shows that our writers
1216
relied heavily on the ChatNoir?s ranking. Regard-
ing search facets, we observed that our writers used
them only for about 7% of their queries. In these
cases, the writers used either the ?Long Text? facet,
which retrieves web pages containing at least one
continuous passage of at least 300 words, or set the
desired reading level to ?Expert.?
The query log of each writer in Batch 2 divides
into 931 search sessions with an average of 12.3 ses-
sions per topic. Here, a session is defined as a se-
quence of queries recorded for a given topic which
is not divided by a break longer than 30 minutes.
Despite other claims in the literature (Jones and
Klinkner, 2008; Hagen et al, 2013) we argue that,
in our case, sessions can be reliably identified by
timeouts because we have a priori knowledge about
which query belongs to which essay. Typically,
completing an essay took 4.9 days, which includes
to a long-lasting exploration of the topic at hand.
The 297 essays submitted within the two batches
were written with a total of 633 334 edits. Each
topic was edited 2 132 times on average, whereas
the standard deviation gives an idea about how
diverse the modifications of the reused text were.
Writers were not specifically instructed to modify a
text as much as possible?rather they were encour-
aged to paraphrase in order to foreclose the detec-
tion by an automatic text reuse detector. This way,
our corpus captures each writer?s idea of the nec-
essary modification effort to accomplish this goal.
The average lengths of the essays is 5 739 words,
but there are also some short essays if hardly any
useful information could be found on the respective
topics. About 15 sources have been reused in each
essay, whereas some writers reused text from as
many as 69 unique documents.
3.2 Relevance Judgments
In the essays from Batch 2, writers reused texts
from web pages they found during their search.
This forms an interesting relevance signal which
allows us to separate web pages relevant to a given
topic from those which are irrelevant. Following
the terminology of TREC, we consider web pages
from which text is reused as key documents for
the respective essay?s topic, while web pages that
are on a click trail leading to a key document are
termed relevant. The unusually high number of
key documents compared to relevant documents
is explained by the fact that there are only few
click trails of this kind, whereas most web pages
Table 3: Confusion matrix of TREC judgments
versus writer judgments.
TREC Writer judgment
judgment irrelevant relevant key unjudged
spam (-2) 3 0 1 2 446
spam (-1) 64 4 18 16 657
irrelevant (0) 219 13 73 33 567
relevant (1) 114 8 91 10 676
relevant (2) 44 5 56 3 711
key (3) 12 0 8 526
unjudged 5 506 221 1 690 ?
have been retrieved directly. The remainder of web
pages that were viewed but discarded by our writers
are considered as irrelevant.
Each year, the NIST assessors employed for the
TREC conference manually review hundreds of
web pages that have been retrieved by experimental
retrieval systems that are submitted to the various
TREC tracks. This was also the case for the TREC
Web Tracks from which the topics of our corpus
are derived. We have compared the relevance judg-
ments provided by TREC for these tracks with the
implicit judgments from our writers. Table 3 con-
trasts the two judgment scales in the form of a con-
fusion matrix. TREC uses a six-point Likert scale
ranging from -2 (extreme Spam) to 3 (key docu-
ment). For 733 of the documents visited by our
writers, TREC relevance judgments can be found.
From these, 456 documents (62%) have been con-
sidered irrelevant for the purposes of reuse by our
writers, however, the TREC assessor disagree with
this judgment in 170 cases. Regarding the docu-
ments considered as key documents for reuse by
our writers, the TREC assessors disagree on 92 of
the 247 documents. An explanation for the dis-
agreement can be found in the differences between
the TREC ad hoc search task and our text reuse
task: the information nuggets (small chunks of
text) that satisfy specific factual information needs
from the original TREC topics are not the same as
the information ?ingots? (big chunks of text) that
satisfy our writers? needs.
3.3 Research Behavior
To analyze the writers? search behavior during es-
say writing in Batch 2, we have recorded detailed
search logs of their queries while they used our
search engine. Figure 2 shows for each of the
150 essays of this batch a curve of the percentage
of queries at times between a writer?s first query
and an essay?s completion. We have normalized
the time axis and excluded working breaks of more
1217
1653320161582105870113231811928271962334710924840148153113154319
642630182083524341142844652605248669750138364234703457
1206167410162326910641362810898474610555088481989421848198
11276201471701395610632370607410451423011116944150274489215599
241588418140135461181851429133611723782466803368121626076
6210822424275691814720830241731616522636960864427464
A
F
E
D
C
B
1 5 10 15 20 25
Figure 2: Spectrum of writer search behavior. Each grid cell corresponds to one of the 150 essays of
Batch 2 and shows a curve of the percentage of submitted queries (y-axis) at times between the first query
until the essay was finished (x-axis). The numbers denote the amount of queries submitted. The cells are
sorted by area under the curve, from the smallest area in cell A1 to the largest area in cell F25.
than five minutes. The curves are organized so as
to highlight the spectrum of different search behav-
iors we have observed: in row A, 70-90% of the
queries are submitted toward the end of the writ-
ing task, whereas in row F almost all queries are
submitted at the beginning. In between, however,
sets of queries are often submitted in the form of
?bursts,? followed by extended periods of writing,
which can be inferred from the steps in the curves
(e.g., cell C12). Only in some cases (e.g., cell C10)
a linear increase of queries over time can be ob-
served for a non-trivial amount of queries, which
indicates continuous switching between searching
and writing. From these observations, it can be
inferred that our writers sometimes conducted a
?first fit? search and reused the first texts they found
easily. However, as the essay progressed and the
low hanging fruit in terms of search were used up,
they had to search more intensively in order to com-
plete their essay. More generally, this data gives
an idea of how humans perform exploratory search
in order to learn about a given topic. Our current
research on this aspect focuses on the prediction
of search mission types, since we observe that the
search mission type does not simply depend on the
writer or the perceived topic difficulty.
3.4 Visualizing Edit Histories
To analyze the writers? writing style, that is to
say, how writers reuse texts and how the essay
is completed in both batches, we have recorded
the edit logs of their essays. Whenever a writer
stopped writing for more than 300ms, a new edit
was stored in a version control system at our site.
The edit logs document the entire text evolution,
from first the keystroke until an essay was com-
pleted. We have used the so-called history flow
visualization to analyze the writing process (Vi?-
gas et al, 2004). Figure 3 shows four examples
from the set of 297 essays. Based on these visu-
alizations, a number of observations can be made.
In general, we identify two distinct writing-style
types to perform text reuse, namely to build up an
essay during writing, or, to first gather material and
then to boil down a text until the essay is completed.
Later in this section, we will analyze this observa-
tion in greater detail. Within the plots, a number
of events can be spotted that occurred during writ-
ing: in the top left plot, encircled as area A, the
insertion of a new piece of text can be observed.
Though marked as original text at first, the writer
worked on this passage and then revealed that it
was reused from another source. At area B in the
top right plot, one can observe the reorganization of
two passages as they exchange places from one edit
to another. Area C in the bottom right plot shows
that the writer, shortly before completing this essay,
reorganized substantial parts. Area D in the same
plot shows how the writer went about boiling down
the text by incorporating contents from different
passages that have been collected beforehand and,
then, from one edit to another, discarded most of
the rest. The saw-tooth shaped pattern in area E
in the bottom left plot reveals that, even though
the writer of this essay adopts a build-up style, she
still pastes passages from her sources into the text
one at a time, and then individually boils down
each. Our visualizations also include information
about the text positions where writers have been
working at a given point in time; these positions
are shown as blue dots in the plots. In this regard
distinct writing patterns are discernible of writers
who go through a text linearly versus those who do
not. Future work will include an analysis of these
writing patterns.
1218
AB
C
D
E
Figure 3: Types of text reuse: build-up reuse (left) versus boil-down reuse (right). Each plot shows the text
length at text edit between first keystroke and essay completion; edits have been recorded during writing
whenever a writer stopped for more than 300ms. Colors encode different source documents. Original text
is white; blue dots indicate the text position of the writer?s last edit.
3.5 Build-up Reuse versus Boil-down Reuse
Based on the edit history visualizations, we have
manually classified the 297 essays of both batches
into two categories, corresponding to the two styles
build-up reuse and boil-down reuse. We found
that 40% are instances of build-up reuse, 45% are
instances of boil-down reuse, and 13% fall in be-
tween, excluding 2% of the essays as outliers due
to errors or for being too short. The in-between
cases show that a writer actually started one way
and then switched to the respective other style of
reuse so that the resulting essays could not be at-
tributed to a single category. An important question
that arises out of this observation is whether differ-
ent writers habitually exert different reuse styles
or whether they apply them at random. To obtain
a better overview, we envision the applied reuse
style of an essay by the skyline curve of its edit
history visualization (i.e., by the curve that plots
the length of an essay after each edit). Aggregating
these curves on a per-writer basis reveals distinct
Table 4: Contingency table: writers over reuse style.
Reuse Writer ID
Style A02A05A06A07A10A17A18A19A20A21A24
build-up 4 27 11 4 9 13 12 4 9 18 2
boil-down 52 5 0 14 2 13 11 3 0 0 24
mixed 10 3 0 1 1 7 6 0 0 3 1
patterns. For eight of our writers Figure 4 shows
this characteristic. The plots are ordered by the
shape of the averaged curve, starting from a linear
increase (left) to a compound of steep increase to
a certain length after which the curve levels out
(right). The former shape corresponds to writers
who typically apply build-up reuse, while the lat-
ter can be attributed to writers who typically apply
boil-down reuse.
When comparing the plots we notice a very in-
teresting effect: it appears that writers who conduct
boil-down reuse vary more wildly in their behavior.
The reuse style of some writers, however, falls in
between the two extremes. Besides the visual anal-
ysis, Table 4 shows the distribution of reuse styles
1219
Te
x
t l
en
gt
h 
(%)
Te
x
t l
en
gt
h 
(%)
A10 (12 essays) A18 (32 essays) A24 (27 essays)A21 (21 essays)
A06 (12 essays) A17 (33 essays) A02 (66 essays)A05 (37 essays)
Edits (%)Edits (%) Edits (%)Edits (%)
build up boil down
Text reuse style
Figure 4: Text reuse styles ranging from build-up reuse (left) to boil-down reuse (right). A gray curve
shows the normalized length of an essay over the edits that went into it during writing. Curves are grouped
by writers. The black curve marks the average of all other curves in a plot.
for the eleven writers who contributed at least five
essays. Most writers use one style for about 80%
of their essays, whereas two writers (A17, A18) are
exactly on par between the two styles. Based on
Pearson?s chi-squared test, one can safely reject the
null hypothesis that writers and text reuse styles
are independent: ?2 = 139.0 with p = 7.8 ? 10?20.
Since our sample of authors and essays is sparse,
Pearson?s chi-squared test may not be perfectly
suited which is why we have also applied Fisher?s
exact test, which computes probability p = 0.0005
that the null hypothesis is true.
4 Summary and Outlook
This paper details the construction of the Webis text
reuse corpus 2012 (Webis-TRC-12), a new corpus
for text reuse research that has been created en-
tirely manually on a large scale. We have recorded
consistent interaction logs of human writers with a
search engine as well as with the used text proces-
sor; these logs serve the purpose of studying how
texts from the web are being reused for essay writ-
ing. Our setup is entirely reproducible: we have
built a static web search environment consisting of
a search engine along with a means to browse a
large corpus of web pages as if it were the ?real?
web. Yet, in terms of scale, this environment is rep-
resentative of the real web. Besides our corpus also
this infrastructure is available to other researchers.
The corpus itself goes beyond existing resources in
that it allows for a much more fine-grained analysis
of text reuse, and in that it significantly improves
the realism of the data underlying evaluations of
automatic tools to detect text reuse and plagiarism.
Our analysis gives an overview of selected as-
pects of the new corpus. This includes corpus
statistics about important variables, but also ex-
ploratory studies of search behaviors and strategies
for reusing text. We present new insights about how
text is composed, revealing two types of writers:
those who build up a text as they go, and those who
first collect a lot of material which then is boiled
down until the essay is finished.
Parts of our corpus have been successfully em-
ployed to evaluate plagiarism detectors in the
PAN plagiarism detection competition 2012 (Pot-
thast et al, 2012a). Future work will include analy-
ses that may help to understand the state of mind of
writers when reusing text as well as of plagiarists.
We also expect insights with regard to the develop-
ment of algorithms for detection purposes and for
linguists studying the process of writing.
Acknowledgements
We thank our writers at oDesk and all volunteers
for their contribution. We also thank Jan Gra?egger
and Martin Tippmann who kept the search engine
up and running during corpus construction.
1220
References
Jeff Barr and Luis Felipe Cabrera. 2006. AI gets a
brain. Queue, 4(4):24?29.
Paul Clough and Mark Stevenson. 2011. Develop-
ing a corpus of plagiarised short answers. Language
Resources and Evaluation, 45:5?24.
Paul Clough, Robert Gaizauskas, Scott S. L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt Reuse.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2002),
Philadelphia, PA, USA, July 6?12, 2002, pages 152?
159.
Gordon V. Cormack, Mark D. Smucker, and Charles L.
A. Clarke. 2011. Efficient and effective spam filtering
and re-ranking for large web datasets. Information
Retrieval, 14(5):441?465.
Tamer Elsayed, Jimmy J. Lin, and Donald Metzler.
2011. When close enough is good enough: approxi-
mate positional indexes for efficient ranked retrieval.
In Proceedings of the 20th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2011),
Glasgow, United Kingdom, October 24?28, 2011,
pages 1993?1996.
Matthias Hagen, Jakob Gomoll, Anna Beyer, and
Benno Stein. 2013. From Search Session Detection to
Search Mission Detection. In Proceedings of the 10th
International Conference Open Research Areas in In-
formation Retrieval (OAIR 2013), Lisbon, Portugal,
May 22?24, 2013, to appear.
Djoerd Hiemstra and Claudia Hauff. 2010. MIREX:
MapReduce information retrieval experiments. Tech-
nical Report TR-CTIT-10-15, University of Twente.
Rosie Jones and Kristina Lisa Klinkner. 2008. Be-
yond the session timeout: automatic hierarchical seg-
mentation of search topics in query logs. In Proceed-
ings of the 17th ACM Conference on Information and
Knowledge Management (CIKM 2008), Napa Valley,
California, USA, October 26?30, 2008, pages 699?
708.
J. Peter Kincaid, Robert P. Fishburne, Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation of
new readability formulas (automated readability index,
Fog count and Flesch reading ease formula) for Navy
enlisted personnel. Research Branch Report 8-75,
Naval Air Station Memphis, Millington, TN.
Katrin K?hler and Debora Weber-Wulff. 2010. Pla-
giarism detection test 2010. http://plagiat.
htw-berlin.de/wp-content/uploads/
PlagiarismDetectionTest2010-final.pdf.
Greg Pass, Abdur Chowdhury, and Cayley Torgeson.
2006. A picture of search. In Proceedings of the 1st
International Conference on Scalable Information
Systems (Infoscale 2006), Hong Kong, May 30?June 1,
2006, paper 1.
Martin Potthast, Benno Stein, Andreas Eiselt, Alberto
Barr?n-Cede?o, and Paolo Rosso. 2009. Overview
of the 1st international competition on plagiarism
detection. In SEPLN 2009 Workshop on Uncovering
Plagiarism, Authorship, and Social Software Misuse
(PAN 2009), pages 1?9.
Martin Potthast, Alberto Barr?n-Cede?o, Andreas
Eiselt, Benno Stein, and Paolo Rosso. 2010a.
Overview of the 2nd international competition on
plagiarism detection. In Working Notes Papers of the
CLEF 2010 Evaluation Labs.
Martin Potthast, Benno Stein, Alberto Barr?n-Cede?o,
and Paolo Rosso. 2010b. An evaluation framework
for plagiarism detection. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING 2010), Beijing, China, August 23?27,
2010, pages 997?1005.
Martin Potthast, Andreas Eiselt, Alberto Barr?n-
Cede?o, Benno Stein, and Paolo Rosso. 2011.
Overview of the 3rd international competition on pla-
giarism detection. In Working Notes Papers of the
CLEF 2011 Evaluation Labs.
Martin Potthast, Tim Gollub, Matthias Hagen, Jan
Gra?egger, Johannes Kiesel, Maximilian Michel,
Arnd Oberl?nder, Martin Tippmann, Alberto Barr?n-
Cede?o, Parth Gupta, Paolo Rosso, and Benno Stein.
2012a. Overview of the 4th international competition
on plagiarism detection. In Working Notes Papers of
the CLEF 2012 Evaluation Labs.
Martin Potthast, Matthias Hagen, Benno Stein, Jan
Gra?egger, Maximilian Michel, Martin Tippmann, and
Clement Welsch. 2012b. ChatNoir: a search engine
for the ClueWeb09 corpus. In Proceedings of the
35th International ACM Conference on Research and
Development in Information Retrieval (SIGIR 2012),
Portland, OR, USA, August 12?16, 2012, page 1004.
Martin Potthast. 2011. Technologies for Reusing
Text from the Web. Dissertation, Bauhaus-Universit?t
Weimar.
Stephen E. Robertson, Hugo Zaragoza, and Michael J.
Taylor. 2004. Simple BM25 extension to multiple
weighted fields. In Proceedings of the 13th ACM Con-
ference on Information and Knowledge Management
(CIKM 2004), Washington, DC, USA, November 8?13,
2004, pages 42?49.
Fernanda B. Vi?gas, Martin Wattenberg, and Kushal
Dave. 2004. Studying cooperation and conflict be-
tween authors with history flow visualizations. In Pro-
ceedings of the 2004 Conference on Human Factors
in Computing Systems (CHI 2004), Vienna, Austria,
April 24?29, 2004, pages 575?582.
1221
