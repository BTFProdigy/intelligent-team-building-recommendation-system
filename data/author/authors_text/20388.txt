Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628?632,
Dublin, Ireland, August 23-24, 2014.
TeamX: A Sentiment Analyzer with Enhanced Lexicon Mapping and
Weighting Scheme for Unbalanced Data
Yasuhide Miura
Fuji Xerox Co., Ltd. / Japan
yasuhide.miura@fujixerox.co.jp
Shigeyuki Sakaki
Fuji Xerox Co., Ltd. / Japan
sakaki.shigeyuki@fujixerox.co.jp
Keigo Hattori
Fuji Xerox Co., Ltd. / Japan
keigo.hattori@fujixerox.co.jp
Tomoko Ohkuma
Fuji Xerox Co., Ltd. / Japan
ohkuma.tomoko@fujixerox.co.jp
Abstract
This paper describes the system that has
been used by TeamX in SemEval-2014
Task 9 Subtask B. The system is a senti-
ment analyzer based on a supervised text
categorization approach designed with fol-
lowing two concepts. Firstly, since lex-
icon features were shown to be effective
in SemEval-2013 Task 2, various lexicons
and pre-processors for them are introduced
to enhance lexical information. Secondly,
since a distribution of sentiment on tweets
is known to be unbalanced, an weighting
scheme is introduced to bias an output of a
machine learner. For the test run, the sys-
tem was tuned towards Twitter texts and
successfully achieved high scoring results
on Twitter data, average F
1
70.96 on Twit-
ter2014 and average F
1
56.50 on Twit-
ter2014Sarcasm.
1 Introduction
The growth of social media has brought a ris-
ing interest to make natural language technologies
that work with informal texts. Sentiment anal-
ysis is one such technology, and several work-
shops such as SemEval-2013 Task 2 (Nakov et
al., 2013), CLEF 2013 RepLab 2013 (Amig?o
et al., 2013), and TASS 2013 (Villena-Rom?an
and Garc??a-Morera, 2013) have recently targeted
tweets or cell phone messages as analysis text.
This paper describes a system that has submit-
ted a sentiment analysis result to Subtask B of
SemEval-2014 Task9 (Rosenthal et al., 2014).
SemEval-2014 Task9 is a rerun of SemEval-2013
Task 2 with different test data, and Subtask B is a
task of message polarity classification.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
The system we prepared is a sentiment ana-
lyzer based on a supervised text categorization
approach. Various features and their extraction
methods are integrated in the system following the
works presented in SemEval-2013 Task 2. Addi-
tionally to these features, we assembled following
notable functionalities to the system:
1. Processes to enhance word-to-lemma map-
ping.
(a) A spelling corrector to normalize out-of-
vocabulary words.
(b) Two Part-of-Speech (POS) taggers to
realize word-to-lemma mapping in two
perspectives.
(c) A word sense disambiguator to obtain
word senses and their confidence scores.
2. An weighting scheme to bias an output of a
machine learner.
Functionalities 1a to 1c are introduced to enhance
information based on lexical knowledge, since
features based on lexicons are shown to be ef-
fective in SemEval-2013 Task 2 (Mohammad et
al., 2013). Functionality 2 is introduced to make
the system adjustable to polarity unbalancedness
known to exists in Twitter data (Nakov et al.,
2013).
The accompanying sections of this papers are
organized as follows. Section 2 describes re-
sources such as labeled texts and lexicons used in
our system. Section 3 explains the details of the
system. Section 4 discusses the submission test
run and some extra test runs that we performed
after the test data release. Finally, section 5 con-
cludes the paper.
2 Resources
2.1 Sentiment Labeled Data
The system is a constrained system, therefore only
the sentiment labeled data distributed by the task
628
Type #Used #Full %
Twitter(train) 6949 9684 71.8
Twitter(dev) 1066 1654 64.4
Twitter(dev-test) 3269 3813 85.7
SMS(dev-test) 2094 2094 100
Table 1: The numbers of messages for each type.
?train?, ?dev?, and ?dev-test? denote training, devel-
opment, and development-test respectively. #Used
is the number of messages that we were able to
obtain, and #Full is the maximum number of mes-
sages that were provided.
Criterion Lexicon
General Inquirer
FORMAL MPQA Subjectivity Lexicon
SentiWordNet
AFINN-111
INFORMAL
Bing Liu?s Opinion Lexicon
NRC Hashtag Sentiment Lexicon
Sentiment140 Lexicon
Table 2: The seven sentiment lexicons and their
criteria.
organizers were used. However, due to accessibil-
ity changes in tweets, a subset of the training, the
development, and the development-test data were
used. Table 1 shows the numbers of messages for
each type.
2.2 Sentiment Lexicons
The system includes seven sentiment lexicons
namely: AFINN-111 (Nielsen, 2011), Bing Liu?s
Opinion Lexicon
1
, General Inquirer (Stone et al.,
1966), MPQA Subjectivity Lexicon (Wilson et al.,
2005), NRCHashtag Sentiment Lexicon (Moham-
mad et al., 2013), Sentiment140 Lexicon (Moham-
mad et al., 2013), and SentiWordNet (Baccianella
et al., 2010). We categorized these seven lexi-
cons to two criteria: ?FORMAL? and ?INFOR-
MAL?. Lexicons that include lemmas of erroneous
words (e.g. misspelled words) were categorized
to ?INFORMAL?. Table 2 illustrates the criteria of
the seven lexicons. These criteria are used in the
process of word-to-lemma mapping processes and
will be explained in Section 3.1.3.
3 System Details
The system is a modularized system consisting
of a variety of pre-processors, feature extractors,
1
http://www.cs.uic.edu/
?
liub/FBS/
sentiment-analysis.html
Text?Normalizer
Stanford?POS?Tagger
Word?Sense?Disambiguator Negation?Detector
word?senses FORMAL?lexicons
Pre?processors
Feature?Extractors
Input Spelling?Corrector
CMU?ARK?POS?Tagger
Negation?Detector
word?ngramscharacter?ngrams clusters
Machine?Learner Prediction?Adjuster
Output
INFORMAL?lexicons
Figure 1: An overview of the system?
and a machine learner. Figure 1 illustrates the
overview of the system.
3.1 Pre-processors
3.1.1 Text Normalizer
The text normalizer performs following three rule-
based normalization of an input text:
? Unicode normalization in form NFKC
2
.
? All upper case letters are converted to lower
case ones (ex. ?GooD? to ?good?).
? URLs are exchanged with string ?URL?s (ex.
?http://example.org? to ?URL?).
3.1.2 Spelling Corrector
A spelling corrector is included in the system to
normalize misspellings. We used Jazzy
3
, an open
source spell checker with US English dictionaries
provided along with Jazzy. Jazzy combines Dou-
bleMetaphone phonetic matching algorithm and a
near-miss match algorithm based on Levenshtein
distance to correct a misspelled word.
3.1.3 POS Taggers
The system includes two POS taggers to realize
word-to-lemma mapping in two perspectives.
Stanford POS Tagger Stanford Log-linear Part-
of-Speech Tagger (Toutanova et al., 2003) is
one POS tagger which is used to map words
2
http://www.unicode.org/reports/tr15/
3
http://jazzy.sourceforge.net/
629
to lemmas of ?FORMAL? criterion lexicons,
and to extract word sense features. A finite-
state transducer based lemmatizer (Minnen et
al., 2001) included in the POS tagger is used
to obtain lemmas of tokenized words.
CMU ARK POS Tagger A POS tagger for
tweets by CMU ARK group (Owoputi et al.,
2013) is another POS tagger used to map
words to lemmas of ?INFORMAL? criterion
lexicons, and to extract ngram features and a
cluster feature.
3.1.4 Word Sense Disambiguator
Aword sense disambiguator is included in the sys-
tem to determine a sense of a word. We used
UKB
4
which implements graph-based word sense
disambiguation based on Personalized PageRank
algorithm (Agirre and Soroa, 2009) on a lexical
knowledge base. As a lexical knowledge base,
WordNet 3.0 (Fellbaum, 1998) included in the
UKB package is used.
3.1.5 Negation Detector
The system includes a simple rule-based negation
detector. The detector is an implementation of the
algorithm on Christopher Potts? Sentiment Sym-
posium Tutorial
5
. The algorithm is a simple algo-
rithm that appends a negation suffix to words that
appear within a negation scope surrounded by a
negation key (ex. ?no?) and a certain punctuation
(ex. ?:?).
3.2 Features
The followings are the features used in the system.
word ngrams Contiguous 1, 2, 3, and 4 grams
of words, and non-contiguous 3 and 4 grams
of words are extracted from a given words.
Non-contiguous ngram are ngrams where one
of words are replaced with a wild card word
?*?. Example of contiguous 3 grams is
?by the way?, and the corresponding noncon-
tiguous variation is ?by * way?.
character ngrams Contiguous 3, 4, and 5 grams
of characters with in a word are extracted
from given words.
lexicons Words are mapped to seven lexicons of
section 2.2. For two sentiment labels (pos-
itive and negative) in each lexicon, follow-
ing four values are extracted: total matched
4
http://ixa2.si.ehu.es/ukb/
5
http://sentiment.christopherpotts.
net/lingstruc.html#negation
I?liked an?example.org?video http://example.org
Sense ID Score01824736?v 0.44231301777210?v 0.35567901776952?v 0.148101?
Sense?ID Score06277280?n 0.68865506277803?n 0.16334304534127?n 0.103199?
text
WSD?result
Feature Weight01824736?v 0.442313
Features 01777210?v 0.35567901776952?v 0.14810106277280?n 0.688655?
Figure 2: An example of word senses feature?
word count, total score, maximal score, and
last word score
6
. For lexicons without senti-
ment scores, score 1.0 is used for all entries.
Note that different POS taggers are used in
word-to-lemma mapping as described in Sec-
tion 3.1.3.
clusters Words are mapped to Twitter Word Clus-
ters of CMU ARK group
7
. The largest clus-
tering result consisting of 1000 clusters from
approximately 56 million tweets is used as
clusters.
word senses A result of the word sense disam-
biguator is extracted as weighted features ac-
cording to their scores. Figure 2 shows an
example of this feature.
The ngram features are introduced as basic bag-
of-words features in a supervised text categoriza-
tion approach. Lexicon features are designed to
strengthen the lexical features of Mohammad et
al. (2013) which have been shown to be effective
in the last year?s task. Cluster features are im-
plemented as an improvement for an supervised
NLP system following the work of Turian et al.
(2010). Word sense features are utilized to help
subjectivity analysis and contextual polarity anal-
ysis (Akkaya et al., 2009).
3.3 Machine Learner
Logistic Regression is utilized as an algorithm of
a supervised machine learning method. As an
implementation of Logistic Regression, LIBLIN-
EAR (Fan et al., 2008) is used. A Logistic Regres-
sion is trained using the features of Section 3.2
with the three polarities (positive, negative, and
neutral) as labels.
6
The total number of lexical features is 7? 2? 4 = 56.
7
http://www.ark.cs.cmu.edu/TweetNLP/
630
Parameters Sources
Parameter Selection Source
C w
pos
w
neg
LiveJournal SMS Twitter Twitter Twitter2014
2014 2013 2013 2014 Sarcasm
Twitter(train)+Twitter(dev) 0.07 1.7 2.6 71.23 62.33 71.28 70.40 53.32
Twitter(dev-test)* 0.03 2.4 3.3 69.44 57.36 72.12 70.96 56.50
SMS(dev-test) 0.80 1.1 1.2 72.99 68.92 65.65 66.66 48.24
SMS(dev-test)+Twitter(dev-test) 0.07 1.9 2.0 72.54 65.44 70.41 69.80 51.09
Table 3: The scores for each source in the test runs. The run with asterisk (*) denotes the submission
run. The values in the ?Sources? columns represent scores in SemEval-2014 Task 9 metric (the average
of positive F
1
and negative F
1
).
3.4 Prediction Adjuster
Since the labels in the tweets data are unbalanced
(Nakov et al., 2013), we prepared a prediction ad-
juster for Logistic Regression output. For each po-
larity l, an weighting factorw
l
that adjusts a proba-
bility output Pr(l) is introduced. An updated pre-
diction label is decided by selecting an l that max-
imizes score(l) which can be expressed as equa-
tion 1.
arg max
l?{pos,neg,neu}
score(l) = w
l
Pr(l) (1)
The approach we took in this prediction adjuster
is a simple approach to bias an output of Logistic
Regression, but may not be a typical approach to
handle unbalanced data. For instance, LIBLIN-
EAR includes the weighting option ?-wi? which
enables a use of different cost parameter C for dif-
ferent classes. One advantage of our approach is
that the change in w
l
does not require a training of
Logistic Regression. Various values of w
l
can be
tested with very low computational cost, which is
helpful in a situation like SemEval tasks where the
time for development is limited.
4 Test Runs
4.1 Submission Test Run
The system was trained using the 8,015 tweets in-
cluded in Twitter(train) and Twitter(dev) described
in Section 2.1. Three parameters: cost parameter
C of Logistic Regression, weight w
pos
of the pre-
diction adjuster, and weight w
neg
of the predic-
tion adjuster, were considered in the submission
test run. For the w
neu
of the prediction adjuster, a
fixed value of 1.0 was used.
Prior to the submission test run, the following
two steps were performed to select a parameter
combination for the submission run.
Step 1 The system with all combinations of C in
range of {0.01 to 0.09 by step 0.01, 0.1 to 0.9
by step 0.1, 1 to 10 by step 1}, w
pos
in range
of {1.0 to 5.0 by step 0.1}, and w
neg
in range
of {1.0 to 5.0 by step 0.1} were prepared
8
.
Step 2 The performances of the system for all
these parameter combinations were calcu-
lated using Twitter(dev-test) described in
Section 2.1.
As a result, the parameter combination C = 0.03,
w
pos
= 2.4, and w
neg
= 3.3 which performed
best in Twitter(dev-test) was selected as a parame-
ter combination for the submission run.
Finally, the system with the selected parameters
was applied to the test set of SemEval-2014 Task
9. ?Twitter(dev-test)? in Table 3 shows the val-
ues of this submission run. The system achieved
high performances on Twitter data: 72.12, 70.96,
and 56.50 on Twitter2013, Twitter2014, and Twit-
ter2014Sarcasm respectively.
4.2 Post-Submission Test Runs
The system performed quite well on Twitter
data but not so well on other data on the sub-
mission run. After the release of the gold
data of the 2014 test tun, we conducted sev-
eral test runs using different parameter combina-
tions. ?Twitter(train)+Twitter(dev)?, ?SMS(dev-
test)?, and ?SMS(dev-test)+Twitter(dev-test)? are
the results of test runs with different data sources
used for the parameter selection process. In ?Twit-
ter(train)+Twitter(dev)?, the parameter combina-
tion that maximizes a micro-average score of 5-
fold cross validation was chosen since the training
data and the parameter selection are equivalent.
The parameter combination selected with ?Twit-
ter(train)+Twitter(dev)? showed similar result as
the submission run, which is high performances
on Twitter data. In the case of ?SMS(dev-test)?, the
system performed well on ?LiveJournal2014? and
?SMS(dev-test)? namely 72.99 and 68.92. How-
8
The total number of parameter combination is 29?51?
51 = 75429.
631
ever, in this parameter combination the scores on
Twitter data were clearly lower than the submis-
sion run. Finally, ?SMS(dev-test)+Twitter(dev-
test)? resulted to a mid performing result, where
scores for each source marked in-between values
of ?Twitter(dev-test)? and ?SMS(dev-test)?.
5 Conclusion
We proposed a system that is designed to enhance
information based on lexical knowledge and to
be adjustable to unbalanced training data. With
parameters tuned towards Twitter data, the sys-
tem successfully achieved high scoring results on
Twitter data, average F
1
70.96 on Twitter2014 and
average F
1
56.50 on Twitter2014Sarcasm.
Additional test runs with different parameter
combination showed that the system can be tuned
to perform well on non-Twitter data such as blogs
or short messages. However, the limitation of our
approach to directly weight a machine learner?s
output was shown, since we could not find a
general purpose parameter combination that can
achieve high scores on any types of data.
Acknowledgements
We would like to thank the anonymous reviewers
for their valuable comments to improve this paper.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of EACL 2009, pages 33?41.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190?199.
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Mart??n, Edgar Meij, Maarten de Rijke, and Damiano
Spina. 2013. Overview of RepLab 2013: Evaluat-
ing online reputation monitoring systems. In CLEF
2013 Evaluation Labs and Workshop, Online Work-
ing Notes.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC 2010, pages 2200?2204.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. In Journal of
Machine Learning Research, volume 9, pages 1871?
1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Semantic
Evaluation Exercises (SemEval-2013), pages 321?
327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the seventh interna-
tional workshop on Semantic Evaluation Exercises
(SemEval-2013), pages 312?320.
Finn
?
Arup Nielsen. 2011. A new ANEW: Evalu-
ation of a word list for sentiment analysis in mi-
croblogs. In Proceedings of the ESWC2011 Work-
shop on ?Making Sense of Microposts?: Big things
come in small packages, pages 93?98.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL 2013, pages 380?390.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and
Veselin Stoyanov. 2014. SemEval-2014 task 9:
Sentiment analysis in Twitter. In Proceedings of the
eighth international workshop on Semantic Evalua-
tion Exercises (SemEval-2014).
Philip Stone, Dexter Dunphy, Marshall Smith, and
Daniel Ogilvie. 1966. General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252?
259.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL 2010, pages 384?394.
Julio Villena-Rom?an and Janine Garc??a-Morera. 2013.
TASS 2013 - Workshop on sentiment analysis at SE-
PLN 2013: An overview. In Proceedings of the
TASS workshop at SEPLN 2013.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP 2005, pages 347?354.
632
Proceedings of the 25th International Conference on Computational Linguistics, pages 54?61,
Dublin, Ireland, August 23-29 2014.
Twitter User Gender Inference Using Combined Analysis                     
of Text and Image Processing 
 
Shigeyuki Sakaki, Yasuhide Miura, Xiaojun Ma, Keigo Hattori, and Tomoko Ohkuma  
Fuji Xerox Co., Ltd. / Japan 
6-1, Minatomirai, Nishi-ku, Yokohama-shi, Kanagawa 
{sakaki.shigeyuki, yasuhide.miura, xiaojun.ma, keigo.hattori, 
ohkuma.tomoko}@fujixerox.co.jp 
Abstract 
Profile inference of SNS users is valuable for marketing, target advertisement, and opinion polls. Sev-
eral studies examining profile inference have been reported to date. Although information of various 
types is included in SNS, most such studies only use text information. It is expected that incorporating 
information of other types into text classifiers can provide more accurate profile inference. As de-
scribed in this paper, we propose combined method of text processing and image processing to im-
prove gender inference accuracy. By applying the simple formula to combine two results derived from 
a text processor and an image processor, significantly increased accuracy was confirmed. 
1 Introduction 
Recently, several researches on profile inference of Social Networking Services (SNS) user conducted 
by analyzing postings have been reported (Rao and Yarowsky, 2010; Han et al., 2013; Makazhanov et 
al., 2013). User profile information such as gender, age, residential area, and political preference have 
attracted attention because they are helpful for marketing, target advertisement, TV viewer rate calcu-
lations, and opinion polls. The major approach to this subject is building a machine learning classifier 
trained by text in postings. However, images posted by a user are rarely used in profile inference. Im-
ages in postings also include features of user profiles. For example, if a user posts many dessert imag-
es, then the user might be female. Therefore, we assumed that highly accurate profile inference will be 
available by analyzing image information and text information simultaneously. 
As described in this paper, we implement gender inference of Japanese Twitter user using text in-
formation and image information. We propose a combined method consisting of text processing and 
image processing, which accepts tweets as input data and outputs a gender probability score. The 
combined method comprises of two steps: step 1) two gender probability scores are inferred respec-
tively by a text processor and an image processor; step 2) the combined score is calculated by merging 
two gender scores with an appropriate ratio. This report is the first describing an attempt to apply the 
combined method of text processing and image processing to profile inference of an SNS user. 
This paper is presented as seven sections: section 2 presents a description of prior work; section 3 
presents a description of the annotation data prepared for this study; section 4 introduces the proposed 
method; section 5 explains preliminary experiments for optimizing the combined method parameter; 
section 6 presents the experimentally obtained result; section 7 summarizes the paper and discusses 
future work. 
2 Prior Work 
Many reports have described studies examining gender inference. The conventional approach to 
this theme is building a machine learning classifier such as Support Vector Machine (SVM) trained by 
text features (Burger et al., 2011; Liu et al., 2012). Most of these studies specifically examine im-
provement of the machine classification methodology rather than expanding features or combining 
features. Different from these studies, Liu et al. (2013) implemented gender inference with incorpora-
tion of a user name into the classifier based on text information. However, the expansion of features 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organisers. License details: http://creativecommons.org/licenses 
/by/4.0/ 
 
54
 remains in the text field. 
A few reports in the literature describe studies of systems that infer the SNS user gender with in-
formation aside from the text. Ikeda et al. (2013) leverages the accuracy of profile inference based on a 
text feature classifier by combining user cluster information. According to their study, the accuracy of 
classification that deals only with the user cluster is lower than that of the text classifier. The classifier 
using both text and cluster information of a user outperforms their text classifier. This research shows 
that information aside from the text is useful to leverage the performance of profile inference based on 
text and text information is necessary to achieve high accuracy. However, we introduce image infor-
mation that is not used by Ikeda et al (2013). 
Along with text information and cluster information, images are popular informative elements that 
are included in SNS postings. An image includes enough information to infer what is printed in itself, 
and researches to automatically annotate an image with semantic labels are already known (Zhang et 
al., 2012). Automatic image annotation is a machine learning technique that involves a process by 
which a computer system automatically assigns semantic labels to a digital image.  These studies suc-
ceeded in inferences of various objects, such as person, dog, bicycle, chair etc. We supposed that such 
objects in images posted by a user should be useful clues as to a profile inference of a twitter user. As 
a matter of fact, gender inference by image information is reported by Ma et al. (2014), which imple-
mented gender inference by processing images in tweets. Their study, which ignored text information, 
exhibited accuracy of less than 70%. It was much lower than most gender inference work using text 
feature. 
From results of these studies, we concluded that gender inference by text and image information 
invites further study. 
3 Proposed Method 
Our proposed method for combining text processing and image processing is presented in Figure 1. 
First, data of 200 tweets of a user are separated into text data and image data. Each of separated data is 
analyzed using a dedicated processor, a text processor, and an image processor. Both of the processors 
 
 
Figure 1. Combined method constitution. 
text
Gender classifier
G nd probability
c r  of a user
Image
Combined method
Combined gender probability score of a user
Text Processor Image Processor
Gender probability score of a user
Object Classifiers 
Object probability scores
Consolidation of scores
Food-male 
classifiers
Food-female 
classifiers
Pet-female 
classifiers
Toy male 
classifiers
Food-male
probability
score of an image
Food-female
probability
score of an image
Toy-male
probability
score of an image
Pet-female
probability
score of an image
200 tweets
posted by a user
55
 output a user?s gender probability score, the upper/lower ends of which respectively correspond to 
male and female labels. At the end of this process, the combined gender probability score is calculated 
using two probability scores. In this section, details of the two processors and the method of combin-
ing their two results are described. 
3.1 Text Processing 
The text processor is constructed from a text classifier, which accepts text data in tweets and outputs 
the gender probability score of a user. We defined the gender classifier in the text processor as an 
SVM binary classification of a male and female. The SVM classifier is trained based on unigram Bag-
of-words with a linear kernel. The cost parameter C is set to 1.0. Then LIBSVM (Chang and Lin, 
2001) is used as an implementation of SVM. Because words are not divided by spaces in a Japanese 
sentence, Kuromoji (Atilika, 2011), a morphological analysis program for Japanese, is used to obtain 
unigrams. 
To combine two results from the text processor and the image processor, it is necessary to calculate 
each result as a probability value. To retrieve probability scores, we used logistic regression. Logistic 
function converts a distance from a hyper plane to probability scores of 0.0?1.0. The text classifier is a 
male and female binary classification. Therefore, the upper and lower ends of the probability score 
respectively correspond to male and female data. If a score is close to 0.0, then the user has high prob-
ability of being male. If it is close to 1.0, then a user is probably female. 
3.2 Image Processing 
We first tried to infer a Twitter user gender directly by a two-class classifier trained by image feature 
vector calculated by all images posted by a user. However, with some preliminary experiments, we 
found that this approach does not work well, since the large variation of objects made the classifica-
tion difficult with single classifier setting. We, therefore, used the image processing method described 
by Ma et al. (2014) which uses automatic image annotation classifiers (Zhang et al., 2012) to model 
human recognition of different gender tendency in images. The method consists of two steps: step 1) 
annotating images by an image annotation technique at the image level; step 2) consolidating gender 
scores according to annotation results at the user level. 
In the first step, the image labels are defined as the combination of the following two information: 
the gender tendency in images of a user and the objects that images express. Ma et al. (2014) defined 
10 categories of objects in SNS images based on observation on a real dataset. The defined labels are 
cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, screen-
shot/capture, and other. They also indicated that gender tendency in images are coherent with user 
gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30 
labels constructed from object label and gender label (e.g. ?male-person?) are used in this paper, 
which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al., 
2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe, 
1999) and image features are encoded with a k-mean generated codebook with size of 2000. We ap-
plied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of 
image features. Then, the 30 SVM classifiers are trained based on the features of training images: each 
classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates im-
ages of a user by computing scores, and logistic function is applied to the outputs of the image classi-
fiers in order to obtain probability scores. Each of 30 probability scores shows how an image is close 
to the decision boundary of a particular label. 
In the second step, we integrated the 30 scores of labels assigned to images to yield comprehensive 
scores which imply a user?s gender. Two methods are suggested for the second step. One is computing 
the average of all scores output from each classifier for each of the categories of male and female for a 
user. The other is computing the mean value of only the highest scores of every image for each of the 
categories of male and female for a user. 
3.3 Combined method of Text Processing and Image Processing 
To combine two results derived from text processing and image processing, we used the function be-
low. Scoretext and Scoreimage respectively represent gender probability scores derived from the text pro-
56
 cessor and the image processor. In the function, ? is set as a ratio of the text score and an image score 
to combine two scores appropriately. We introduced ? as a reliability ratio parameter of the scores by 
the text processor and the scores by the image processor. 
 
? ??? ????? 1imagetextcombined ScoreScoreScore  
4 Data 
We prepared user annotation data and image annotation data that we used as training data and evalua-
tion data. User annotation data are input data for the text processor, whereas image annotation data are 
for the image processor. As it is required to prepare a huge number of annotated data as a training cor-
pus, the data is annotated by Yahoo Crowd Sourcing (Yahoo! Japan, 2013). Yahoo Crowd Sourcing is 
a Japanese crowd sourcing service similar to Amazon Mechanical Turk (Amazon, 2005). Therefore 
the annotation process aims to obtain annotation based on human recognition rather than to explore 
truth about users and images of twitter. 
4.1 User Annotation Data 
We first collected Japanese Twitter users according to their streaming tweets. We ignored heavy users 
and Twitter bots. A random sampling of tweets revealed that tweets from heavy users include much 
information that is not useful for profile inference such as short representations of their actions (e.g. 
?Going to bed now? and ?Just waking up?). A Twitter bot is also classed as an uninformative user be-
cause it is a program that automatically generates tweets. During data collection, we filtered out those 
users by setting conditions shown below in Table 1. Finally, we obtained 3976 Twitter users. We 
gathered tweets on each user up to 200. By executing the processes above, we obtained tweet data of 
each user corresponding to the user?s own 200 tweets. 
To obtain gender annotation for this large dataset, we used Yahoo! Crowd Sourcing. As shown in 
Figure 2(a), we set task for every Twitter user: please infer the gender of the user who posted the 
tweets in the URL below. In this task, after reading 200 tweets of a user, the gender label of male or 
female was asked of every Twitter user. To guarantee quality reliability, annotation tasks for one Twit-
ter user were duplicated 10 times by different workers; then a majority vote of 10 annotations was cal-
culated to obtain a gold label. 
As a result of the crowd sourcing tasks, 1733 users were reported as male; 2067 users were reported 
as female. There were 176 users whose votes were split equally between male and female. We re-
Table 1. Filtering conditions used to disqualify heavy users and Twitter bots 
User Types Definition for N Criteria 
Twitter bots Number of tweets posted from Twitter clients on PC/mobile 
by a user 
N<150 
Heavy 
Number of Friends or followers of a user N>200 
Number of Tweets posted in a day by a user N>10 
 
 
(a) User annotation task                                                                                  (b) Image annotation task 
Figure 2. Annotation tasks in crowd sourcing. 
http://www.abc.com/defg/index.html
An w r: ?Male
? Female
Qu stion :
Please infer the gender of the user
 p sted the tweets in the URL below.
?
Question 2:
Please choose the word most 
suitable to express the objects 
included in the image
Answer: ? Male
? Female
?Unknown
Question 1:
Please guess the gender of the 
user who uploaded the image
Answer: ? Cartoon/Illustration
? Food
? Memo/Leaflet
? Person
? Screenshot/Capture
? Famous person
? Goods
? Outdoor/Nature
? Pet
? Others
?
?
57
 moved balanced users from the data. The male and female populations of annotation assumed users 
are 45.6% and 54.4% respectively. This gender proportion tendency is consistent with those reported 
from an earlier study showing that Twitter participants are 55% female (Heli and Piskorski, 2009; 
Burger et al., 2011). Finally, we obtained gender annotation data of 3800 users. We divided these data 
equally between training data and evaluation data: 1900 users for training data and 1900 users for 
evaluation data. 
4.2 Image Annotation Data 
We first made a user list including 1523 users. After checking tweets from these users, we extracted 
9996 images. Image annotation processes were also executed by Yahoo Crowd Sourcing. 
Our image annotation process refers to rules proposed by Ma et al. (2014). As shown in Figure 2(b), 
a worker is requested to provide responses of two kinds for every image: Q1. Please guess the gender 
of the user who uploaded the image; Q2. Please choose the word most suitable to express the objects 
included in the image. The possible responses for Q1 were male, female, and unknown. Those for Q2 
were cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, 
screenshot/capture, and other. It is sometimes difficult to infer a gender of a user solely based on one 
image. Therefore, unknown is set for Q1. From those responses we obtained multiple labels for every 
image, such as ?male-person?. To avoid influence by poor-quality workers, each image was presented 
to 10 different workers. A summation of 10 annotations was executed to obtain gold label data. 
5 Preliminary Experiments 
5.1 Image Processing 
We compared two consolidation methods, computing the average of all scores and computing the av-
erage of the highest scores for 30 object scores. We applied the two method to the training data of the 
user annotation data, and tested them on the evaluation data. Results show that the accuracy of former 
method is 60.11. That of the latter is 65.42. The reason the latter method is superior to the former one 
is probably attributable to noise reduction effects of ignoring low scores. 
5.2 Combined method of Text Processing and Image Processing 
To estimate the optimal value of ?, we conducted a preliminary experiment of the combined method 
with training data. We first prepared text and image probability scores. The text score is obtained by 
executing five-fold cross validation of the text processor for training data. We used the probability 
score derived in section 5.1 as the image score. The accuracies were, respectively, 86.23 and 65.42. 
Next, the combined formula was applied to these probability scores with moving ? from 0 to 1. Figure 
3 shows the correlation between accuracy and ?. To obtain the ? value of the peak, we executed poly-
nomial fitting to a part of the correlation curve where ? is 0.1?0.4. By differentiating this function, we 
calculated the ? value of the peak as equal to 0.244 indicated by the arrow in Figure 3. The accuracy 
reaches 86.73% at the peak, which is 0.50 pt higher than that of the text processor. 
 
Figure 3. Correlation between accuracy and ? in training data. 
(Fitting curve function is 0.9519?3-0.9129?2+0.2756?+0.8409) 
0.85
0.855
0.86
0.865
0.87
0.875
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
c
c
u
r
a
c
y
 
[
%
]
?
58
 6 Experimental Results 
6.1 Comparing the Accuracies between Three Methods 
We executed an evaluation experiment assessing the three methods: text processing, image processing 
with a selected consolidation method, and the combined method with optimized ? (0.235). Each 
method is applied to evaluation data including 1900 gender-annotated data. Table 2 presents precision, 
recall, F-measure, and accuracy obtained through the evaluation experiments. The text processing ac-
curacy achieves 84.63%, and image processing accuracy is 64.21%. The combined method achieves 
85.11% accuracy, which is 0.48 pt higher than the text processing accuracy.1 We also confirmed that 
both the male and female F-measures become higher than text processing. We concluded that signifi-
cantly increased accuracy obtained using the method combining text processing and image processing. 
 
6.2 Discussion 
We expected the optimal value of ? to be large, since the accuracy of the text processor is explicitly 
higher than that of the image processor. However, the actual optimal ? resulted to the rather small val-
ue, 0.244. This small ? is thought to be caused by a characteristic of the image processor?s gender 
scores. Figure 4 (a) and (b) show the distributions of the gender scores derived by the text processor 
and the image processor. The horizontal axis corresponds to a gender score of a user, ranging from 0, 
highly probable female, to 1, highly probable male. The two distributions are clearly different from 
Table 2. Results obtained using text processing, image processing and combined method. 
 (P, precision; R, recall; F, F-measure; Acc., Accuracy) 
 Male Female Acc. 
P R F P R F  
Text processing 84.65 82.39 83.50 84.62 86.64 83.50 84.63 
Image processing 64.68 66.56 65.60 72.10 62.11 66.74 64.21 
Combined method (? = 0.244) 84.57 83.72 84.16 85.49 86.34 85.91 85.11 
 
  
(a) The distribution of the text processing scores          (b) The distribution of the image processing scores 
Figure 4. The distributions of the probability scores.  
 
 
Figure 5. The distribution of the ?male-person? score of training data of user annotation data. 
0
100
200
300
400
500
600
700
800
900
1000
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
N
u
m
b
e
r
 o
f
 u
s
e
r
s
Probability score
0
100
200
300
400
500
600
700
800
900
1000
1 2 3 4 5 6 0 7 0.8 0.9 1.0 
N
u
m
b
e
r
 o
f
 u
s
e
r
s
Pr bability score
0
1000
2000
3000
4000
5000
00
7000
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
N
u
m
b
e
r
 o
f
 l
a
b
e
l
s
Probability score
1Significance improvement with paired t-test (p=0.09<0.1). 
 
 
59
 each other: the variance of the image scores is much smaller than that of the text. From this character-
istic, the image scores were needed to be amplified in order to reflect them in the final result. In terms 
of ?, this amplification corresponds to a small value. 
The reason why the variance of the image scores became small is in its calculation process. In the 
image processor, the gender score of a user is calculated as the mean of the highest object scores ex-
tracted from each image. Figure 5 shows a distribution of ?male-person? label scores. Though a distri-
bution of each object probability scores centres not at 0.5, highest score selections and the averaging 
of them leads to a mid-range value, in this case 0.5. 
Our intuition behind the introduction of ? was to provide a reliability ratio parameter of the text 
processor and the image processor. But as a matter of fact, this parameter also worked to calibrate the 
scale difference between the two probability scores. From this observation, a function that includes a 
reliability parameter and a calibration parameter separately can be considered as an alternative to the 
proposed function. Using this kind of function will provide further insights about combining a text 
processing and image processing. 
7 Conclusion 
As described herein, we assembled two results retrieved by text and image processors respectively to 
enhance the Twitter user gender inference. Even though the gender inference accuracy already reached 
84.63 solely by the text classifier, we succeeded in improving efficiency further by 0.48 pt. Because 
the image processing in our method is completely independent from the text processing, this combined 
method is applicable to the other gender prediction methods, just like those of Burger and Liu (Burger, 
2011; Liu, 2013). Reported studies about SNS user profile inference targeted basic attributes such as 
gender, age, career, residential area, etc. More worthwhile attributes for marketing that directly indi-
cate user characteristics are desired to predict, for example, hobbies and lifestyles. Images in tweets 
are expected to include clues about these profiles aside from gender. As a subject for future work, we 
will apply our combined method to various profile attributes. 
As the combined method in this paper is simple linear consolidation and ignores a capability of ana-
lyzing both text and image information at the same time, exploring more suitable combined method is 
needed. The simplest way to analyze both text and image information simultaneously is early fusion 
that first creates the large multi-model feature vector constructed by both text and image features and 
then trains a classifier. Meta classifier which infers final class from the outputs of two modalities is 
also considerable method for this subject. Applying more sophisticated combined methods is another 
subject for future work. 
References 
Amazon. 2005. Amazon Mechanical Turk (2005), Available: http://www.mturk/welcom 
Atilika. 2011, Kuromoji. Available: http://www.atilika.org 
John D. Burger, John Henderson, Gerorge Kim, Guido Zarrella. 2011. Discriminating Gender on 
Twitter, In Proc. of the Conference on Empirical Methods in natural Language Processing 
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding methods, In Proc. of British Machine Vision Confer-
ence 2011 
Chih-Chung Chang, Chih-Jen Lin, 2001. LIBSVM: a Library for Support Vector Machines. Available: 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Bo Han, Paul Cook, and Timothy Baldwin. 2013. A Stacking-based Approach to Twitter User Geolo-
cation Prediction, In Proc. of the 51st Annual meeting of Association for Computational Linguistics, 
pages 7-12 
Bill Heli, Mikolaj Jan Piskorski. 2009. New Twitter Research: Men Follow Men and Nobody Tweets, 
Harvard Business Review, June 1. 
60
 Kazushi Ikeda, Gen Hattori, Chihiro Ono, Hideki Asoh, Teruo Higashino. 2013. Twitter User Profil-
ing Based on Text and Community Mining for Market Analysis, Knowledge Based Systems 51, 
pages 35-47. 
Svetlana Lazebnik, Cordelia Schmid, Jean Ponce, 2006. Beyond bags of features: Spatial Pyramid 
Matching for Recognizing Natural Scene Categories, In Proc. of Computer Vision and Pattern 
Recognition 2006, page 2169-2178 
Wendy Liu, Faiyaz Al Zamal, Derek Ruths. 2012. Using Social Media to Infer Gender Composition of 
Commuter Populations, In Proc. of the International Association for the Advancement of Artificial 
Intelligence Conference on Weblogs and Social 
Wendy Liu, Derek Ruths. 2013. What?s in a Name? Using First Names as Features for Gender Infer-
ence in Twitter, In Symposium on Analyzing Microtext 
David G. Lowe. 1999. Object recognition from local scale-invariant features, In Proc. of the Interna-
tional Conference on Computer Vision, pages 1150-1157 
Matt Lynley. 2012. Statistics That Reveal Instagram?s Mind-Blowing Success, Available: 
http://www.businessinsider.com/statistics-that-reveal-instagrams-mind-blowing-success-2012-4 
Xiaojun Ma, Yukihiro Tsuboshita, Noriji Kato. 2014. Gender Estimation for SNS User Profiling Au-
tomatic Image Annotation, In Proc. of the 1st International Workshop on Cross-media Analysis for 
Social Multimedia 
Aibek Makazhanov, Davood Refiei. 2013. Predicting Political Preference of Twitter Users, In Proc. of 
the 2013 IEEE/ACM International Conference on Advances in Social Network and Mining, pages 
298-305 
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka onnela, J. Hiels Rosenquist. 2011. Un-
dersanding the Demographics of Twitter Users, In Proc. of 5th International AAAI Conference on 
Weblogs and Social Media, pages 554-557 
Delip Rao and David Yarowsky. 2010. Detecting Latent User Properties in Social Media, In Proc. of 
the Neural Information Processing Systems Foundation workshop on Machine Learning for Social 
Networks 
Chih-Fong Tsai. 2012. Bag-of-Words Representation in Image Annotation: A Review, International 
Scholarly Research Notices Artificial Intelligence, Volume 2012, Article ID 376804, 19 pages 
Jinjun Wang, Jinchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, Yihong Gong. 2010. Locality-
constrained linear coding for image classification, In Proc. of  Computer Vision and Pattern Recog-
nition 2010, page 626 
Yahoo! Japan. 2013. Yahoo Crowd Sourcing. Available: http://crowdsourcing.yahoo.co.jp/ 
Dengsheng Zhang, Md Monirul Islam, Guojun Lu. 2012. A review on automatic image annotation, 
Pattern Recognition 45, pages 346-362 
61
