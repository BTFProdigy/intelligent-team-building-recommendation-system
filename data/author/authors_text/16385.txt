INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 31?39,
Utica, May 2012. c?2012 Association for Computational Linguistics
Generation for Grammar Engineering
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54000, France
claire.gardent@loria.fr
German Kruszewski
Inria, LORIA, UMR 7503
Villers-le`s-Nancy, F-54600, France
german.kruszewski@inria.fr
Abstract
While in Computer Science, grammar engi-
neering has led to the development of various
tools for checking grammar coherence, com-
pletion, under- and over-generation, in Natu-
ral Langage Processing, most approaches de-
veloped to improve a grammar have focused
on detecting under-generation and to a much
lesser extent, over-generation. We argue that
generation can be exploited to address other
issues that are relevant to grammar engineer-
ing such as in particular, detecting grammar
incompleteness, identifying sources of over-
generation and analysing the linguistic cover-
age of the grammar. We present an algorithm
that implements these functionalities and we
report on experiments using this algorithm to
analyse a Feature-Based Lexicalised Tree Ad-
joining Grammar consisting of roughly 1500
elementary trees.
1 Introduction
Grammar engineering, the task of developing large
scale computational grammars, is known to be er-
ror prone. As the grammar grows, the interactions
between the rules and the lexicon become increas-
ingly complex and the generative power of the gram-
mar becomes increasingly difficult for the grammar
writer to predict.
While in Computer Science, grammar engineer-
ing has led to the development of various tools for
checking grammar coherence, completion, under-
and over-generation (Klint et al, 2005), in Natu-
ral Langage Processing, most approaches developed
to improve a grammar have focused on detecting
under-generation (that is cases where the grammar
and/or the lexicon fails to provide an analysis for
a given, grammatical, input) and to a lesser degree
over-generation.
In this paper, we argue that generation can be ex-
ploited to address other issues that are relevant to
grammar engineering. In particular, we claim that it
can be used to:
? Check grammar completeness: for each gram-
mar rule, is it possible to derive a syntactically
complete tree ? That is, can each grammar rule
be used to derive a constituent.
? Analyse generation and over-generation: given
some time/recursion upper bounds, what does
the grammar generate? How much of the out-
put is over-generation? Which linguistic con-
structions present in a language are covered by
the grammar?
We present a generation algorithm called GRADE
(GRAmmar DEbugger) that permits addressing
these issues. In essence, this algorithm implements
a top-down grammar traversal guided with semantic
constraints and controlled by various parameteris-
able constraints designed to ensure termination and
linguistic control.
The GRADE algorithm can be applied to any gen-
erative grammar i.e., any grammar which uses a
start symbol and a set of production rules to gen-
erate the sentences of the language described by
that grammar. We present both an abstract descrip-
tion of this algorithm and a concrete implementation
which takes advantage of Definite Clause Grammars
31
to implement grammar traversal. We then present
the results of several experiments where we use the
GRADE algorithm to examine the output of SEM-
TAG, a Feature-Based Lexicalised Tree Adjoining
Grammar (FB-LTAG) for French.
The paper is structured as follows. Section 2
summarises related work. Section 3 presents the
GRADE algorithm. Section 4 introduces the gram-
mar used for testing and describes an implementa-
tion of GRADE for FB-LTAG. Section 5 presents
the results obtained by applying the GRADE algo-
rithm to SEMTAG. We show that it helps (i) to detect
sources of grammar incompleteness (i.e., rules that
do not lead to a complete derivation) and (ii) to iden-
tify overgeneration and analyse linguistic coverage.
Section 6 concludes.
2 Related Work
Two main approaches have so far been used to im-
prove grammars: treebank-based evaluation and er-
ror mining techniques. We briefly review this work
focusing first, on approaches that are based on pars-
ing and second, on those that exploit generation.
Debugging Grammars using Parsing Over the
last two decades, Treebank-Based evaluation has be-
come the standard way of evaluating parsers and
grammars. In this framework (Black et al, 1991),
the output of a parser is evaluated on a set of sen-
tences that have been manually annotated with their
syntactic parses. Whenever the parse tree produced
by the parser differs from the manual annotation, the
difference can be traced back to the parser (timeout,
disambiguation component), the grammar and/or to
the lexicon. Conversely, if the parser fails to re-
turn an output, undergeneration can be traced back
to missing or erroneous information in the grammar
or/and in the lexicon.
While it has supported the development of ro-
bust, large coverage parsers, treebank based evalu-
ation is limited to the set of syntactic constructions
and lexical items present in the treebank. It also
fails to directly identify the most likely source of
parsing failures. To bypass these limitations, error
mining techniques have been proposed which per-
mit detecting grammar and lexicon errors by pars-
ing large quantities of data (van Noord, 2004; Sagot
and de la Clergerie, 2006; de Kok et al, 2009). The
output of this parsing process is then divided into
two sets of parsed and unparsed sentences which are
used to compute the ?suspicion rate? of n-grams of
word forms, lemmas or part of speech tags whereby
the suspicion rate of an item indicates how likely a
given item is to cause parsing to fail. Error mining
was shown to successfully help detect errors in the
lexicon and to a lesser degree in the grammar.
Debugging Grammars using Generation Most
of the work on treebank-based evaluation and error
mining target undergeneration using parsing. Re-
cently however, some work has been done which ex-
ploits generation and more specifically, surface real-
isation to detect both under- and over-generation.
Both (Callaway, 2003) and the Surface Realisa-
tion (SR) task organised by the Generation Chal-
lenge (Belz et al, 2011) evaluate the output of sur-
face realisers on a set of inputs derived from the
Penn Treebank. As with parsing, these approaches
permit detecting under-generation in that an input
for which the surface realiser fails to produce a
sentence points to shortcomings either in the sur-
face realisation algorithm or in the grammar/lexicon.
The approach also permits detecting overgeneration
in that a low BLEU score points to these inputs
for which the realiser produced a sentence that is
markedly different from the expected answer.
Error mining approaches have also been devel-
oped using generation. (Gardent and Kow, 2007) is
similar in spirit to the error mining approaches de-
veloped for parsing. Starting from a set of manu-
ally defined semantic representations, the approach
consists in running a surface realiser on these repre-
sentations; manually sorting the generated sentences
as correct or incorrect; and using the resulting two
datasets to detect grammatical structures that sys-
tematically occur in the incorrect dataset. The ap-
proach however is only partially automatised since
both the input and the output need to be manually
produced/annotated. More recently, (Gardent and
Narayan, 2012) has shown how the fully automatic
error mining techniques used for parsing could be
adapted to mine for errors in the output of a surface
realiser tested on the SR input data. In essence, they
present an algorithm which enumerate the subtrees
in the input data that frequently occur in surface re-
alisation failure (the surface realiser fails to gener-
32
ate a sentence) and rarely occur in surface realisa-
tion success. In this way, they can identify subtrees
in the input that are predominantly associated with
generation failure.
In sum, tree-bank based evaluation permits de-
tecting over- and under-generation while error min-
ing techniques permits identifying sources of er-
rors; Treebank-based evaluation requires a refer-
ence corpus while error mining techniques require
a way to sort good from bad ouput; and in all cases,
generation-based grammar debugging requires input
to be provided (while for parsing, textual input is
freely available).
Discussion The main difference between the
GRADE approach and both error mining and tree-
bank based evaluation is that GRADE is grammar
based. No other input is required for the GRADE
algorithm to work than the grammar1. Whereas ex-
isting approaches identify errors by processing large
amounts of data, GRADE identifies errors by travers-
ing the grammar. In other words, while other ap-
proaches assess the coverage of a parser or a genera-
tor on a given set of input data, GRADE permits sys-
tematically assessing the linguistic coverage and the
precision of the constructs described by the grammar
independently of any input data.
Currently, the output of GRADE needs to be man-
ually examined and the sources of error manually
identified. Providing an automatic means of sorting
GRADE ?s output into good and bad sentences is de-
veloped however, it could be combined with error
mining techniques so as to facilitate interpretation.
3 The GraDE Algorithm
How can we explore the quirks and corners of a
grammar to detect inconsistencies and incorrect out-
put?
In essence, the GRADE algorithm performs a top-
down grammar traversal and outputs the sentences
generated by this traversal. It is grammar neutral in
that it can be applied to any generative grammar i.e.,
any grammar which includes a start symbol and a
set of production rules. Starting from the string con-
sisting of the start symbol, the GRADE algorithm
recursively applies grammar rules replacing one oc-
1Although some semantic input is possible.
currence of its left-hand side in the string by its right-
hand side until a string that contains neither the start
symbol nor designated nonterminal symbols is pro-
duced.
Since NL grammars describe infinite sets of sen-
tences however, some means must be provided to
control the search and output sets of sentences that
are linguistically interesting. Therefore, the GRADE
algorithm is controlled by several user-defined pa-
rameters designed to address termination (Given that
NL grammars usually describe an infinite set of sen-
tences, how can we limit sentence generation to
avoid non termination?), linguistic control (How can
we control sentence generation so that the sentences
produced cover linguistic variations that the linguist
is interested in ?) and readibility (How can we con-
strain sentence generation in such a way that the out-
put sentences are meaningful sentences rather than
just grammatical ones?).
3.1 Ensuring termination
To ensure termination, GRADE supports three user-
defined control parameters which can be used simul-
taneously or in isolation namely: a time out parame-
ter; a restriction on the number and type of recursive
rules allowed in any derivation; and a restriction on
the depth of the derivation tree.
Each of these restrictions is implemented as a re-
striction on the grammar traversal process as fol-
lows.
Time out. The process halts when the time bound
is reached.
Recursive Rules. For each type of recursive rule,
a counter is created which is initialised to the values
set by the user and decremented each time a recur-
sive rule of the corresponding type is used. When
all counters are null, recursive rules can no longer
be used. The type of a recursive rule is simply the
main category expanded by that rule namely, N, NP,
V, VP and S. In addition, whenever a rule is applied,
the GRADE algorithm arbitrarily divides up the re-
cursion quotas of a symbol among the symbol?s chil-
dren. If it happens to divide them a way that can-
not be fulfilled, then it fails, backtracks, and divides
them some other way.
33
Derivation Depth. A counter is used to keep track
of the depth of the derivation tree and either halts (if
no other rule applies) or backtracks whenever the set
depth is reached.
3.2 Linguistic Coverage and Output
Readibility
GRADE provides several ways of controlling the lin-
guistic coverage and the readibility of the output
sentences.
Modifiers. As we shall show in Section 5, the re-
cursivity constraints mentioned in the previous sec-
tion can be used to constrain the type and the number
of modifiers present in the output.
Root Rule. Second, the ?root rule? i.e., the rule
that is used to expand the start symbol can be con-
strained in several ways. The user can specify which
rule should be used; which features should label
the lhs of that rule; which subcategorisation type it
should model; and whether or not it is a recursive
rule. For instance, given the FB-LTAG we are using,
by specifying the root rule to be used, we can con-
strain the generated sentences to be sentences con-
taining an intransitive verb in the active voice com-
bining with a canonical nominal subject. If we only
specify the subcategorisation type of the root rule
e.g., transitive, we can ensure that the main verb of
the generated sentences is a transitive verb; And if
we only constrain the features of the root rule to in-
dicative mode and active voice, then we allow for
the generation of any sentence whose main verb is
in the indicative mode and active voice.
Input Semantics. Third, in those cases where the
grammar is a reversible grammar associating sen-
tences with both a syntactic structure and a seman-
tic representation, the content of the generated sen-
tences can be controlled by providing GRADE with
an input semantics. Whenever a core semantics
is specified, only rules whose semantics includes
one or more literal(s) in the core semantics can be
used. Determiner rules however are selected inde-
pendent of their semantics. In this way, it is possi-
ble to constrain the output sentences to verbalise a
given meaning without having to specify their full
semantics (the semantic representations used in re-
versible grammars are often intricate representations
which are difficult to specify manually) and while
allowing for morphological variations (tense, num-
ber, mode and aspect can be left unspecified and will
be informed by the calls to the lexicon embedded in
the DCG rules) as well as variations on determin-
ers2. For instance, the core semantics {run(E M),
man(M)} is contained in, and therefore will gen-
erate, the flat semantics for the sentences The man
runs, The man ran, A man runs, A man ran, This
man runs, My man runs, etc..
4 Implementation
In the previous section, we provided an abstract de-
scription of the GRADE algorithm. We now describe
an implementation of that algorithm tailored for FB-
LTAGs equipped with a unification-based composi-
tional semantics. We start by describing the gram-
mar used (SEMTAG), we then summarise the im-
plementation of GRADE for FB-LTAG.
4.1 SemTAG
For our experiments, we use the FB-LTAG described
in (Crabbe?, 2005; Gardent, 2008). This grammar,
called SEMTAG, integrates a unification-based se-
mantics and can be used both for parsing and for
generation. It covers the core constructs for non
verbal constituents and most of the verbal construc-
tions for French. The semantic representations built
are MRSs (Minimal Recursion Semantic representa-
tions, (Copestake et al, 2001)).
More specifically, a tree adjoining grammar
(TAG) is a tuple ??, N, I, A, S? with ? a set of ter-
minals, N a set of non-terminals, I a finite set of
initial trees, A a finite set of auxiliary trees, and S
a distinguished non-terminal (S ? N ). Initial trees
are trees whose leaves are labeled with substitution
nodes (marked with a downarrow) or terminal cate-
gories3. Auxiliary trees are distinguished by a foot
node (marked with a star) whose category must be
the same as that of the root node.
2The rules whose semantics is not checked during derivation
are specified as a parameter of the system and can be modified
at will e.g., to include adverbs or auxiliaries. Here we choosed
to restrict underspecification to determiners.
3Due to space limitation we here give a very sketchy defini-
tion of FB-LTAG. For a more detailed presentation, see (Vijay-
Shanker and Joshi, 1988).
34
Two tree-composition operations are used to com-
bine trees: substitution and adjunction. Substitu-
tion inserts a tree onto a substitution node of some
other tree while adjunction inserts an auxiliary tree
into a tree. In a Feature-Based Lexicalised TAG
(FB-LTAG), tree nodes are furthermore decorated
with two feature structures (called top and bottom)
which are unified during derivation; and each tree
is anchored with a lexical item. Figure 1 shows an
example toy FB-LTAG with unification semantics.
NPj
John
l0:proper q(c hr hs)
l1:named(j john)
qeq(hr l1)
Sb
NP?c VPba
Va
runs
lv:run(a,j)
VPx
often VP*x
lo:often(x)
? l0:proper q(c hr hs) l1:named(j john), qeq(hr, l1),
lv:run(a,j), lv:often(a)
Figure 1: MRS for ?John often runs?
4.2 GraDe for FB-LTAG
The basic FB-LTAG implementation of GRADE is
described in detail in (Gardent et al, 2011; Gar-
dent et al, 2010). In brief, this implementation
takes advantage of the top-down, left-to-right, gram-
mar traversal implemented in Definite Clause Gram-
mars by translating the FB-LTAG to a DCG. In the
DCG formalism, a grammar is represented as a set of
Prolog clauses and Prolog?s query mechanism pro-
vides a built-in top-down, depth-first, traversal of the
grammar. In addition, the DCG formalism allows
arbitrary Prolog goals to be inserted into a rule. To
implement a controlled, top-down grammar traver-
sal of SEMTAG, we simply convert SEMTAGto a
Definite Clause Grammar (DCG) wherein arbitrary
Prolog calls are used both to ground derivations with
lexical items and to control Prolog?s grammar traver-
sal so as to respect the user defined constraints on
recursion and on linguistic coverage. In addition,
we extended the approach to handle semantic con-
straints (i.e., to allow for an input semantic to con-
strain the traversal) as discussed in Section 3. That
is, for a subset of the grammar rules, a rule will only
be applied if its semantics subsumes a literal in the
input semantics.
For more details, on the FB-LTAG implementa-
tion of the GRADE algorithm and of the conversion
from FB-LTAG to DCG, we refer the reader to (Gar-
dent et al, 2011; Gardent et al, 2010).
5 Grammar Analysis
Depending on which control parameters are used,
the GRADE algorithm can be used to explore the
grammar from different viewpoints. In what fol-
lows, we show that it can be used to check grammar
completeness (Can all rules in the grammar be used
so as to derive a constituent?); to inspect the vari-
ous possible realisations of syntactic functors and of
their arguments (e.g., Are all possible syntactic real-
isations of the verb and of its arguments generated
and correct?); to explore the interactions between
basic clauses and modifiers; and to zoom in on the
morphological and syntactic variants of a given core
semantics (e.g., Does the grammar correctly account
for all such variants ?).
5.1 Checking for Grammar Completeness
We first use GRADE to check, for each grammar
rule, whether it can be used to derive a complete
constituent i.e., whether a derivation can be found
such that all leaves of the derivation tree are ter-
minals (words). Can all trees anchored by a verb
for instance, be completed to build a syntactically
complete clause? Trees that cannot yield a complete
constituent points to gaps or inconsistencies in the
grammar.
To perform this check, we run the GRADE algo-
rithm on verb rules, allowing for up to 1 adjunc-
tion on either a noun, a verb or a verb phrase and
halting when either a derivation has been found or
all possible rule combinations have been tried. Ta-
ble 1 shows the results per verb family4. As can be
seen, there are strong differences between the fam-
ilies with e.g., 80% of the trees failing to yield a
derivation in the n0Vs1int (Verbs with interrogative
sentential complement) family against 0% in the ilV
4The notational convention for verb types is from XTAG and
reads as follows. Subscripts indicate the thematic role of the
verb argument. n indicates a nominal, Pn a PP and s a sentential
argument. pl is a verbal particle. Upper case letters describe
the syntactic functor type: V is a verb, A an adjective and BE
the copula. For instance, n0Vn1 indicates a verb taking two
nominal arguments (e.g., like) .
35
Tree Family Trees Fails Fails/Trees
CopulaBe 60 1 1%
ilV 2 0 0%
n0V 10 0 0%
n0ClV 9 0 0%
n0ClVn1 45 2 4%
n0ClVden1 36 3 8%
n0ClVpn1 29 3 10%
n0Vn1 84 3 3%
n0Vn1Adj2 24 6 25%
n0Van1 87 3 3%
n0Vden1 38 3 7%
n0Vpn1 30 3 10%
ilVcs1 2 0 0%
n0Vcs1 30 23 74%
n0Vas1 15 10 66%
n0Vn1Adj2 24 0 0%
s0Vn1 72 9 12%
n0Vs1int 15 12 80%
n0Vn1n2 24 0 0%
n0Vn1an2 681 54 7%
Table 1: Checking for Gaps in the Grammar
(impersonal with expletive subject, ?it rains?) and
the n0V (intransitive, ?Tammy sings?). In total, ap-
proximatively 10% (135/1317) of the grammar rules
cannot yield a derivation.
5.2 Functor/Argument Dependencies
To check grammar completeness, we need only find
one derivation for any given tree. To assess the de-
gree to which the grammar correctly generates all
possible realisations associated with a given syn-
tactic functor however, all realisations generated by
the grammar need to be produced. To restrict the
output to sentences illustrating functor/argument de-
pendencies (no modifiers), we constrain adjunction
to the minimum required by each functor. In most
cases, this boils down to setting the adjunction coun-
ters to null for all categories. One exception are
verbs taking a sentential argument which require one
S adjunction. We also allow for one N-adjunction
and one V-adjunction to allow for determiners and
the inverted subject clitic (t?il). In addition, the lex-
icon is restricted to avoid lexical or morphological
variants.
We show below some of the well-formed sen-
tences output by GRADE for the n0V (intransitive
verbs) family.
Elle chante (She sings), La tatou chante-
t?elle? (Does the armadillo sing? ),
La tatou chante (The armadillo sings ),
La tatou qui chante (The armadillo which
sings ), Chacun chante -t?il (Does every-
one sing? ), Chacun chante (Everyone
sings ), Quand chante chacun? (When
does everyone sing? ), Quand chante la
tatou? (When does the armadillo sing?
) Quand chante quel tatou? (When does
which armadillo sing? ), Quand chante
Tammy? (When does Tammy sing? ),
Chante-t?elle? (Does she sing? ) Chante
-t?il? (Does he sing? ), Chante! (Sing!
), Quel tatou chante ? (Which armadillo
sing? ), Quel tatou qui chante ..? (Which
armadillo who sings ..? ) Tammy chante-
t?elle? (Does Tammy sing? ), Tammy
chante (Tammy sings ), une tatou qui
chante chante (An armadillo which sings
sings ), C?est une tatou qui chante (It is an
armadillo which sings ), ...
The call on this family returned 55 distinct MRSs
and 65 distinct sentences of which only 28 were cor-
rect. Some of the incorrect cases are shown below.
They illustrate the four main sources of overgener-
ation. The agreement between the inverted subject
clitic and the subject fails to be enforced (a); the in-
verted nominal subject fails to require a verb in the
indicative mode (b); the inverted subject clitic fails
to be disallowed in embedded clauses (c); the inter-
rogative determiner quel fails to constrain its nomi-
nal head to be a noun (d,e).
(a) Chacun chante-t?elle? (Everyone
sings?) (b) Chante?e chacun? (Sung every-
one?) (c) La tatou qui chante-t?elle? (The
armadillo which does she sing?) (d) Quel
chacun chante ? (Which everyone sings?)
(e) quel tammy chante ? (Which Tammy
sings?)
5.3 Interactions with Modifiers
Once basic functor/argument dependencies have
been verified, adjunction constraints can be used to
36
explore the interactions between e.g., basic clauses
and modification5. Allowing for N-adjunctions for
instance, will produce sentences including determin-
ers and adjectives. Similarly, allowing for V ad-
junction will permit for auxiliaries and adverbs to
be used; and allowing for VP or S adjunctions will
licence the use of raising verbs and verbs subcate-
gorising for sentential argument.
We queried GRADE for derivations rooted in n0V
(intransitive verbs) and with alternatively, 1N, 2N,
1V and 1VP adjunction. Again a restricted lexicon
was used to avoid structurally equivalent but lexi-
cally distinct variants. The following table shows
the number of sentences output for each query.
0 1S 1VP 1V 1N 2N
36 170 111 65 132 638
As the examples below show, the generated sen-
tences unveil two further shortcomings in the gram-
mar: the inverted subject clitic fails to be constrained
to occur directly after the verb (1) and the order and
compatibility of determiners are unrestricted (2).
(1) a. Semble-t?il chanter? / * Semble chanter
t?il? (Does he seems to sing?)
b. Chante-t?il dans Paris? / * Chante dans
Paris-t?il? (Does he sing in Paris?)
c. Chante-t?il beaucoup? / * Chante
beaucoup-t?il? (Does he sing a lot?)
d. Veut-t?il que Tammy chante? / * Veut que
Tammy chante-t?il? (Does he want that
Tammy sings?
(2) * Un quel tatou, *Quel cette tatou, Ma quelle
tatou (Un which armadillo, Which this ar-
madillo, My which armadillo)
5.4 Inspecting Coverage and Correctness
In the previous sections, GRADE was used to gen-
erate MRSs and sentences ex nihilo. As mentioned
above however, a core semantics can be used to re-
strict the set of output sentences to sentences whose
MRS include this core semantics. This is useful for
5Recall that in FB-LTAG, adjunction is the operation which
permits applying recursive rules (i.e., auxiliary trees). Hence
allowing for adjunctions amounts to allowing for modification
with the exception already noted above of certain verbs subcat-
egorising for sentential arguments.
Tree Family MRS Sent. S/MRS
ilV 7 52 7.4
n0V 65 161 2.4
n0ClV 30 62 2.0
n0ClVn1 20 25 1.25
n0ClVden1 10 15 1.5
n0ClVpn1 40 63 1.57
n0Vn1 20 110 5.5
n0Van1 30 100 3.33
n0Vden1 5 15 3.00
n0Vpn1 25 76 3.04
ilVcs1 1 1 1.00
n0Vcs1 200 660 3.3
n0Vas1 35 120 3.42
n0Vn1Adj2 10 15 1.5
s0Vn1 4 24 6.00
n0Vn1n2 10 48 4.80
n0Vn1an2 5 45 9.00
Table 2: Producing Variants
instance, to systematically inspect all variations out-
put by the grammar on a given input. These varia-
tions include all morphological variations supported
by the lexicon (number, tense, mode variations) and
the syntactic variations supported by the grammar
for the same MRSs (e.g., active/passive). It also in-
cludes the variations supported by GRADE in that
some rules are not checked for semantic compati-
bility thereby allowing for additional materials to be
added. In effect, GRADE allows for the inclusion of
arbitrary determiners and auxiliaries.
Table 2 shows the number of MRSs and sen-
tences output for each verb family given a match-
ing core semantics and a morphological lexicon in-
cluding verbs in all simple tenses (3rd person only)
and nouns in singular and plural6. The ratio S/M of
sentences on MRSs produced by one GRADE call
shows how the underspecified core semantics per-
mits exploring a larger number of sentences gener-
ated by the grammar than could be done by gener-
ating from fully specified MRSs. For the n0Vn1an2
class, for instance, the GRADE call permits generat-
ing 9 times more sentences in average than generat-
ing from a single MRS.
6The lexicon used in this experiment includes more mor-
phological variants than in the experiment of Section 5.2 where
the focus was on syntactic rather than morphological variants.
Hence the different number of generated sentences.
37
6 Conclusion
When using a grammar for generation, it is essen-
tial, not only that it has coverage (that it does not
undergenerate) but also that it be precise (that it
does not overgenerate). Nonetheless, relatively lit-
tle work has been done on how to detect overgener-
ation. In this paper, we presented an algorithm and
a methodology to explore the sentences generated
by a grammar; we described an implementation of
this algorithm based on DCGs (GRADE ); and we
illustrated its impact by applying it to an existing
grammar. We showed that GRADE could be used
to explore a grammar from different viewpoints: to
find gaps or inconsistencies in the rule system; to
systematically analyse the grammar account of func-
tor/argument dependencies; to explore the interac-
tion between base constructions and modifiers; and
to verify the completeness and correctness of syn-
tactic and morphological variants.
There are many directions in which to pursue
this research. One issue is efficiency. Unsurpris-
ingly, the computational complexity of GRADE is
formidable. For the experiments reported here, run-
times are fair (a few seconds to a few minutes de-
pending on how much output is required and on the
size of the grammar and of the lexicon). As the com-
plexity of the generated sentences and the size of the
lexicons grow, however, it is clear that runtimes will
become unpractical. We are currently using YAP
Prolog tabling mechanism for storing intermediate
results. It would be interesting however to compare
this with the standard tabulating algorithms used for
parsing and surface realisation.
Another interesting issue is that of the interac-
tion between GRADE and error mining. As men-
tioned in Section 2, GRADE could be usefully com-
plemented by error mining techniques as a means
to automatically identify the most probable causes
of errors highlighted by GRADE and thereby of im-
proving the grammar. To support such an integration
however, some means must be provided of sorting
GRADE ?s output into ?good? and ?bad? output i.e.,
into sentences that are grammatical and sentences
that are over-generated by the grammar. We plan to
investigate whether language models could be used
to identify those sentences that are most probably
incorrect. In a first step, simple and highly con-
strained input would be used to generate from the
grammar and the lexicon a set of correct sentences
using GRADE . Next these sentences would be used
to train a language model which could be used to
detect incorrect sentences produced by GRADE on
more complex, less constrained input.
Other issues we are currently pursueing are the
use of GRADE (i) for automating the creation of
grammar exercises for learners of french and (ii) for
creating a bank of MRSs to be used for the evalua-
tion and comparison of data-to-text generators. The
various degrees of under-specification supported by
GRADE permit producing either many sentences out
of few input (e.g., generate all basic clauses whose
verb is of a given subcategorisation type as illus-
trated in Section 5.2); or fewer sentences out a more
constrained input (e.g., producing all syntactic and
morphological variants verbalising a given input se-
mantics). We are currently exploring how seman-
tically constrained GRADE calls permit producing
variants of a given meaning; and how these vari-
ants can be used to automatically construct gram-
mar exercises which illustrate the distinct syntac-
tic and morphological configurations to be acquired
by second language learners. In contrast, more un-
derspecified GRADE calls can be used to automat-
ically build a bank of semantic representations and
their associated sentences which could form the ba-
sis for an evaluation of data-to-text surface realis-
ers. The semantics input to GRADE are simplified
representations of MRSs. During grammar traver-
sal, GRADE reconstructs not only a sentence and
its associated syntactic tree but also its full MRS.
As a result, it is possible to produce a generation
bank which, like the Redwook Bank, groups to-
gether MRSs and the sentences verbalising these
MRSs. This bank however would reflect the linguis-
tic coverage of the grammar rather than the linguis-
tic constructions present in the corpus parsed to pro-
duce the MRS. It would thus provide an alternative
way to test the linguistic coverage of existing surface
realisers.
Acknowledgments
The research presented in this paper was partially
supported by the European Fund for Regional De-
velopment within the framework of the INTERREG
IVA Allegro Project.
38
References
Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalua-
tion results. In Proc. of the 13th European Workshop
on Natural Language Generation.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, Ingria R., F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of english grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop,
page 306311.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In 18th IJCAI, pages
811?817, Aug.
Ann Copestake, Alex Lascarides, and Dan Flickinger.
2001. An algebra for semantic construction in
constraint-based grammars. In Proceedings of the
39th Annual Meeting of the Association for Compu-
tational Linguistics, Toulouse, France.
Benoit Crabbe?. 2005. Reprsentation informatique de
grammaires d?arbres fortement lexicalise?es : le cas de
la grammaire d?arbres adjoints. Ph.D. thesis, Nancy
University.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error mining
in parsing results. In ACL2009 Workshop Grammar
Engineering Across Frameworks (GEAF), Singapore.
Claire Gardent and Eric Kow. 2007. Spotting overgener-
ation suspect. In 11th European Workshop on Natural
Language Generation (ENLG).
Claire Gardent and Shashi Narayan. 2012. Error mining
on dependency trees. In Proceedings of ACL.
Claire Gardent, Benjamin Gottesman, and Laura Perez-
Beltrachini. 2010. Benchmarking surface realisers. In
COLING 2010 (Poster Session), Beijing, China.
Claire Gardent, Benjamin Gottesman, and Laura Perez-
Beltrachini. 2011. Using regular tree grammar to
enhance surface realisation. Natural Language En-
gineerin, 17:185?201. Special Issue on Finite State
Methods and Models in Natural Language Processing.
Claire Gardent. 2008. Integrating a unification-based
semantics in a large scale lexicalised tree adjoining
grammar for french. In COLING?08, Manchester, UK.
Paul Klint, Ralf La?mmel, and Chris Verhoef. 2005.
Toward an engineering discipline for grammarware.
ACM Transactions on Software Engineering Method-
ology, 14(3):331?380.
Benoit Sagot and Eric de la Clergerie. 2006. Error min-
ing in parsing results. In ACL, editor, Proceedings of
the ACL 2006, pages 329?336, Morristown, NJ, USA.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL, editor, Pro-
ceedings of the ACL 2004, pages 446?454, Morris-
town, NJ, USA.
K. Vijay-Shanker and Aravind Joshi. 1988. Feature
Structures Based Tree Adjoining Grammars. Proceed-
ings of the 12th conference on Computational linguis-
tics, 55:v2.
39
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 147?156,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Generating Grammar Exercises
Laura Perez-Beltrachini
Universite? de Lorraine
LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
laura.perez@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy
F-54500, France
claire.gardent@loria.fr
German Kruszewski
Inria, LORIA, UMR 7503
Villers-le`s-Nancy
F-54600, France
german.kruszewski@inria.fr
Abstract
Grammar exercises for language learning fall
into two distinct classes: those that are based
on ?real life sentences? extracted from exist-
ing documents or from the web; and those that
seek to facilitate language acquisition by pre-
senting the learner with exercises whose syn-
tax is as simple as possible and whose vo-
cabulary is restricted to that contained in the
textbook being used. In this paper, we in-
troduce a framework (called GramEx) which
permits generating the second type of gram-
mar exercises. Using generation techniques,
we show that a grammar can be used to
semi-automatically generate grammar exer-
cises which target a specific learning goal; are
made of short, simple sentences; and whose
vocabulary is restricted to that used in a given
textbook.
1 Introduction
Textbooks for language learning generally include
grammar exercises. Tex?s French Grammar 1 for in-
stance, includes at the end of each lecture, a set of
grammar exercises which target a specific pedagog-
ical goal such as learning the plural form of nouns
1Tex?s French Grammar http://www.laits.
utexas.edu/tex/ is an online pedagogical reference
grammar that combines explanations with surreal dialogues
and cartoon images. Tex?s French Grammar is arranged like
many other traditional reference grammars with the parts of
speech (nouns, verbs, etc.) used to categorize specific grammar
items (gender of nouns, irregular verbs). Individual grammar
items are carefully explained in English, then exemplified in a
dialogue, and finally tested in self-correcting, fill-in-the-blank
exercises.
or learning the placement of adjectives. Figure 1
shows the exercises provided by this book at the end
of the lecture on the plural formation of nouns. As
exemplified in this figure, these exercises markedly
differ from more advanced learning activities which
seek to familiarise the learner with ?real world sen-
tences?. To support in situ learning, this latter type
of activity presents the learner with sentences drawn
from the Web or from existing documents thereby
exposing her to a potentially complex syntax and to
a diverse vocabulary. In contrast, textbook grammar
exercises usually aim to facilitate the acquisition of
a specific grammar point by presenting the learner
with exercises made up of short sentences involving
a restricted vocabulary.
As shall be discussed in the next section, most ex-
isting work on the generation of grammar exercises
has concentrated on the automatic creation of the
first type of exercises i.e., exercises whose source
sentences are extracted from an existing corpus. In
this paper, we present a framework (called GramEx)
which addresses the generation of the second type of
grammar exercises used for language learning i.e.,
grammar exercises whose syntax and lexicon are
strongly controlled. Our approach uses generation
techniques to produce these exercises from an exist-
ing grammar describing both the syntax and the se-
mantics of natural language sentences. Given a ped-
agogical goal for which exercises must be produced,
the GramEx framework permits producing Fill in the
blank (FIB, the learner must fill a blank with an ap-
propriate form or phrase) and Shuffle (given a set of
lemmas or forms, the learner must use these to pro-
duce a phrase) exercises that target that specific goal.
147
Give the plural form of the noun indicated in parentheses.
Pay attention to both the article and the noun.
1. Bette aime _____ . (le bijoux)
2. Fiona aime ______ . (le cheval)
3. Joe-Bob aime ______ ame?ricaines. (la bie`re)
4. Tex n?aime pas ______ . (le choix)
5. Joe-Bob n?aime pas ______ difficiles. (le cours)
6. Tammy n?aime pas ______ . (l?ho?pital)
7. Eduard aime ______. (le tableau)
8. Bette aime ______ de Tex. (l?oeil)
9. Tex aime ______ franc?ais. (le poe`te)
10. Corey aime ______ fra??ches. (la boisson)
11. Tammy aime ______ ame?ricains. (le campus)
12. Corey n?aime pas ______ . (l?examen)
Figure 1: Grammar exercises from the Tex?s French Grammar textbook
The exercises thus generated use a simple syntax and
vocabulary similar to that used in the Tex?s French
Grammar textbook.
We evaluate the approach on several dimensions
using quantitative and qualitative metrics as well as a
small scale user-based evaluation. And we show that
the GramEx framework permits producing exercises
for a given pedagogical goal that are linguistically
and pedagogically varied.
The paper is structured as follows. We start by
discussing related work (Section 2). In Section 3,
we present the framework we developed to generate
grammar exercises. Section 4 describes the exper-
imental setup we used to generate exercise items.
Section 5 reports on an evaluation of the exercise
items produced and on the results obtained. Section
6 concludes.
2 Related Work
A prominent strand of research in Computer Aided
Language Learning (CALL) addresses the automa-
tion of exercise specifications relying on Natural
Language Processing (NLP) techniques (Mitkov et
al., 2006; Heilman and Eskenazi, 2007; Karama-
nis et al, 2006; Chao-Lin et al, 2005; Coniam,
1997; Sumita et al, 2005; Simon Smith, 2010; Lin
et al, 2007; Lee and Seneff, 2007). Mostly, this
work targets the automatic generation of so-called
objective test items i.e., test items such as multiple
choice questions, fill in the blank and cloze exercise
items, whose answer is strongly constrained and can
therefore be predicted and checked with high accu-
racy. These approaches use large corpora and ma-
chine learning techniques to automatically generate
the stems (exercise sentences), the keys (correct an-
swers) and the distractors (incorrect answers) that
are required by such test items.
Among these approaches, some proposals target
grammar exercises. Thus, (Chen et al, 2006) de-
scribes a system called FAST which supports the
semi-automatic generation of Multiple-Choice and
Error Detection exercises while (Aldabe et al, 2006)
presents the ArikiTurri automatic question genera-
tor for constructing Fill-in-the-Blank, Word Forma-
tion, Multiple Choice and Error Detection exercises.
These approaches are similar to the approach we
propose. First, a bank of sentences is built which are
automatically annotated with syntactic and morpho-
syntactic information. Second, sentences are re-
trieved from this bank based on their annotation and
on the linguistic phenomena the exercise is meant to
illustrate. Third, the exercise question is constructed
from the retrieved sentences. There are important
differences however.
First, in these approaches, the source sentences
used for building the test items are selected from
corpora. As a result, they can be very complex
and most of the generated test items are targeted
for intermediate or advanced learners. In addition,
some of the linguistic phenomena included in the
language schools curricula may be absent or insuf-
ficiently present in the source corpus (Aldabe et al,
2006). In contrast, our generation based approach
permits controlling both the syntax and the lexicon
of the generated exercices.
Second, while, in these approaches, the syntactic
and morpho-syntactic annotations associated with
the bank sentences are obtained using part-of-speech
tagging and chunking, in our approach, these are
obtained by a grammar-based generation process.
148
As we shall see below, the information thus asso-
ciated with sentences is richer than that obtained by
chunking. In particular, it contains detailed linguis-
tic information about the syntactic constructs (e.g.,
cleft subject) contained in the bank sentences. This
permits a larger coverage of the linguistic phenom-
ena that can be handled. For instance, we can re-
trieve sentences which contain a relativised cleft ob-
ject (e.g., This is the man whom Mary likes who
sleeps) by simply stipulating that the retrieved sen-
tences must be associated with the information Cleft
Object).
To sum up, our approach differs from most exist-
ing work in that it targets the production of syntac-
tically and lexically controlled grammar exercises
rather than producing grammar exercises based on
sentences extracted from an existing corpus.
3 Generating Exercises
Given a pedagogical goal (e.g., learning adjective
morphology), GramEx produces a set of exercise
items for practicing that goal. The item can be ei-
ther a FIB or a shuffle item; and GramEx produces
both the exercise question and the expected solution.
To generate exercise items, GramEx proceeds in
three main steps as follows. First, a generation
bank is constructed using surface realisation tech-
niques. This generation bank stores sentences that
have been generated together with the detailed lin-
guistic information associated by the generation al-
gorithm with each of these sentences. Next, sen-
tences that permit exercising the given pedagogical
goal are retrieved from the generation bank using a
constraint language that permits defining pedagog-
ical goals in terms of the linguistic properties as-
sociated by the generator with the generated sen-
tences. Finally, exercises are constructed from the
retrieved sentences using each retrieved sentence to
define FIB and Shuffle exercises; and the sentence
itself as the solution to the exercise.
We now discuss each of these steps in more detail.
3.1 Constructing a Generation bank
The generation bank is a database associating sen-
tences with a representation of their semantic con-
tent and a detailed description of their syntactic and
morphosyntactic properties. In other words, a gen-
Sentence realisation:
?Tammy a une voix douce?
Lemma-features pairs:
{?lemma?: ?Tammy?,
?lemma-features?: {anim:+,num:sg,det: +,wh:-,cat:n,
func:suj,xp: +, gen:f},
?trace?: {propername}},
{?lemma?: ?avoir?,
?lemma-features?: {aux-refl:-,inv:-,cat:v,pers:3,pron:-,
num:sg,mode:ind, aspect:indet,tense:pres,stemchange:-,
flexion:irreg},
?trace?: {CanonicalObject,CanonicalSubject,n0Vn1}},
{?lemma?: ?un?,
?lemma-features?: {wh:-,num:sg,mass:-,cat:d,
gen:f,def:+},
?trace?: {determiner}},
{?lemma?: ?voix?,
?lemma-features?: {bar:0,wh:-,cat:n,num:sg,
mass:-,gen:f,flexion:irreg,
?trace?: {noun}},
{?lemma?: ?doux?,
?lemma-features?: {num:sg,gen:f,flexion:irreg,cat:adj},
?trace?: {Epith,EpithPost}}
Figure 2: Morphosyntactic information associated by
GraDe with the sentence Tammy a un voix douce
eration bank is a set of (Si, Li, ?i) tuples where Si is
a sentence, Li is a set of linguistic properties true of
that sentence and ?i is its semantic representation.
To produce these tuples, we use the GraDe gram-
mar traversal algorithm described in (Gardent and
Kruszewski, 2012). Given a grammar and a set
of user-defined constraints, this algorithm gener-
ates sentences licensed by this grammar. The user-
defined constraints are either parameters designed to
constrain the search space and guarantee termina-
tion (e.g., upper-bound on the number and type of
recursive rules used or upper-bound on the depth of
the tree build by GraDe); or linguistic parameters
which permit constraining the output (e.g., by spec-
ifying a core semantics the output must verbalise or
by requiring the main verb to be of a certain type).
Here we use GraDe both to generate from manu-
ally specified semantic input; and from a grammar
(in this case an existing grammar is used and no
manual input need to be specified). As explained
in (Gardent and Kruszewski, 2012), when generat-
ing from a semantic representation, the output sen-
tences are constrained to verbalise that semantics but
the input semantics may be underspecified thereby
allowing for morpho-syntactic, syntactic and tem-
poral variants to be produced from a single se-
mantics. For instance, given the input semantics
149
L1:named(J bette n) A:le d(C RH SH) B:bijou n(C)
G:aimer v(E J C), GraDe will output among others
the following variants:
Bette aime le bijou (Bette likes the jewel),
Bette aime les bijoux (Bette likes the jewels),
C?est Bette qui aime le bijou (It is Bette who
likes the jewel), C?est Bette qui aime les bijoux
(It is Bette who likes the jewel), Bette aimait le
bijou (Bette liked the jewel), Bette aimait les
bijoux (Bette liked the jewels), ...
When generating from the grammar, the output
is even less constrained since all derivations com-
patible with the user-defined constraints will be pro-
duced irrespective of semantic content. For instance,
when setting GraDe with constraints restricting the
grammar traversal to only derive basic clauses con-
taining an intransitive verb, the output sentences in-
clude among others the following sentences:
Elle chante (She sings), La tatou chante-t?elle?
(Does the armadillo sing?), La tatou chante
(The armadillo sings), Chacun chante -t?il
(Does everyone sing? ), Chacun chante (Ev-
eryone sings), Quand chante la tatou? (When
does the armadillo sing?), ...
Figure 2 shows the linguistic properties associ-
ated with the sentence Tammy a une voix douce
(Tammy has a soft voice) by GraDe. To gener-
ate exercises, GramEx makes use of the morpho-
syntactic information associated with each lemma
i.e., the feature-value pairs occurring as values of the
lemma-features fields; and of their linguistic proper-
ties i.e., the items occurring as values of the trace
fields.
3.2 Retrieving Appropriate Sentences
To enable the retrieval of sentences that are appropri-
ate for a given pedagogical goal, we define a query
language on the linguistic properties assigned by
GraDe to sentences. We then express each peda-
gogical goal as a query in that language; and we use
these queries to retrieve from the generation bank
appropriate source sentences. For instance, to re-
trieve a sentence for building a FIB exercise where
the blank is a relative pronoun, we query the gen-
eration bank with the constraint RelativePronoun.
This will return all sentences in the generation bank
whose trace field contains the RelativePronoun
item i.e., all sentences containing a relative pronoun.
We then use this sentence to build both the exercise
question and its solution.
3.2.1 GramEx Query Language
We now define the query language used to retrieve
sentences that are appropriate to build an exercise
for a given pedagogical goal. Let B be a genera-
tion bank and let (Si, Li, ?i) be the tuples stored in
B. Then, a GramEx query q permits retrieving from
B the set of sentences Si ? (Si, Li, ?i) such that
Li satisfies q. In other words, GramEx queries per-
mit retrieving from the generation bank all sentences
whose linguistic properties satisfy those queries.
The syntax of the GramEx query language is as
follows:
BoolExpr ? BoolTerm
BoolTerm ? BoolFactor | BoolTerm ? BoolFactor
BoolFactor ? BoolUnary | BoolFactor ? BoolUnary
BoolUnary ? BoolPrimary | ? BoolPrimary
BoolPrimary ? PrimitiveCond | ( BoolExpr ) | [ BoolExpr ]
PrimitiveCond ? traceItem | feature = value
In words: the GramEx query language permits
defining queries that are arbitrary boolean con-
straints on the linguistic properties associated by
GraDe with each generated sentence. In addi-
tion, complex constraints can be named and reused
(macros); and expressions can be required to hold
on a single lexical item ([ BoolExpr] indicates that
BoolExpr should be satisfied by the linguistic prop-
erties of a single lexical item).
The signature of the language is the set of gram-
matical (traceItem) and morpho-syntactic proper-
ties (feature = value) associated by GraDe with
each generated sentence where traceItem is any
item occurring in the value of a trace field and
feature = value any feature/value pair occurring
in the value of a lemma-features field (cf. Fig-
ure 2). The Table below (Table 1) shows some of the
constraints that can be used to express pedagogical
goals in the GramEx query language.
3.2.2 Query Examples
The GramEx query language allows for very spe-
cific constraints to be expressed thereby providing
fine-grained control over the type of sentences and
therefore over the types of exercises that can be pro-
duced. The following example queries illustrate this.
150
Grammatical Properties (traceItem)
Argument Cleft, CleftSUbj, CleftOBJ, ...,
Realisation InvertedSubj
Questioned, QuSubj, ...
Relativised, RelSubj ...
Pronominalised, ProSubj, ...
Voice Active, Passive, Reflexive
Aux tse, modal, causal
Adjective Predicative,Pre/Post nominal
Adverb Sentential, Verbal
Morpho-Syntactic Properties (feature=value)
Tense present,future,past
Number mass, count, plural, singular
Inflexion reg,irreg
Table 1: Some grammatical and morpho-syntactic prop-
erties that can be used to specify pedagogical goals.
(1) a. EpithAnte
Tex pense que Tammy est une jolie tatou (Tex
thinks that Tammy is a pretty armadillo)
b. [Epith ? flexion: irreg]
Tex et Tammy ont une voix douce (Tex and
Tammy have a soft voice)
c. POBJinf ? CLAUSE
POBJinf ? (DE-OBJinf ? A-OBJinf)
CLAUSE ? Vfin??Mod ? ?CCoord? ?Sub
Tammy refuse de chanter (Tammy refuses to
sing)
Query (1a) shows a query for retrieving sentences
containing prenominal adjectives which uses the
grammatical (traceItem) property EpithAnte associ-
ated with preposed adjectives.
In contrast, Query (1b) uses both grammatical and
morpho-syntactic properties to retrieve sentences
containing a postnominal adjective with irregular in-
flexion. The square brackets in the query force the
conjunctive constraint to be satisfied by a single lex-
ical unit. That is, the query will be satisfied by sen-
tences containing a lexical item that is both a post-
nominal adjective and has irregular inflexion. This
excludes sentences including e.g., a postnominal ad-
jective and a verb with irregular inflexion.
Finally, Query (1c) shows a more complex case
where the pedagogical goal is defined in terms of
predefined macros themselves defined as GramEx
query expressions. The pedagogical goal is de-
fined as a query which retrieves basic clauses
(CLAUSE) containing a prepositional infinitival ob-
ject (POBJinf). A sentence containing a preposi-
tional infinitival object is in turn defined (second
line) as a prepositional object introduced either by
the de or the a` preposition. And a basic clause (3rd
line) is defined as a sentence containing a finite verb
and excluding modifiers, clausal or verb phrase co-
ordination (CCORD) and subordinated clauses2
3.3 Building Exercise Items
In the previous section, we saw the mechanism used
for selecting an appropriate sentence for a given
pedagogical goal. GramEx uses such selected sen-
tences as source or stem sentences to build exercise
items. The exercise question is automatically gen-
erated from the selected sentence based on its asso-
ciated linguistic properties. Currently, GramEx in-
cludes two main types of exercises namely, Fill in
the blank and Shuffle exercises.
FIB questions. FIB questions are built by remov-
ing a word from the target sentence and replacing it
with either: a blank (FIBBLNK), a lemma (FIBLEM)
or a set of features used to help the learner guess
the solution (FIBHINT). For instance, in an exercise
on pronouns, GramEx will use the gender, number
and person features associated with the pronoun by
the generation process and display them to specify
which pronominal form the learner is expected to
provide. The syntactic representation (cf. Figure 2)
associated by GraDe with the sentence is used to
search for the appropriate key word to be removed.
For instance, if the pedagogical goal is Learn Sub-
ject Pronouns and the sentence retrieved from the
generation bank is that given in (2a), GramEx will
produce the FIBHINT question in (2b) by search-
ing for a lemma with category cl (clitic) and feature
func=subj and using its gender value to provide the
learner with a hint constraining the set of possible
solutions.
(2) a. Elle adore les petits tatous
(She loves small armadillos)
b. ... adore les petits tatous (gender=fem)
Shuffle questions. Similarly to FIB questions,
shuffle exercise items are produced by inspecting
and using the target derivational information. More
specifically, lemmas are retrieved from the list of
2The expressions CCoord and Sub are themselves defined
rather than primitive expressions.
151
lemma-feature pairs. Function words are (option-
ally) deleted. And the remaining lemmas are ?shuf-
fled? (MSHUF). For instance, given the source sen-
tence (2a), the MSHUF question (2b) can be pro-
duced.
(3) a. Tammy adore la petite tatou
a. tatou / adorer / petit / Tammy
Note that in this case, there are several possible
solutions depending on which tense and number is
used by the learner. For such cases, we can either
use hints as shown above to reduce the set of pos-
sible solutions to one; or compare the learner?s an-
swer to the set of output produced by GraDe for the
semantics the sentence was produced from.
4 Experimental Setup
We carried out an experiment designed to assess the
exercises produced by GramEx. In what follows, we
describe the parameters of this experiment namely,
the grammar and lexicons used; the input and the
user-defined parameters constraining sentence gen-
eration; and the pedagogical goals being tested.
4.1 Grammar and Lexicon
The grammar used is a Feature-Based Lexicalised
Tree Adjoining Grammar for French augmented
with a unification-based compositional semantics.
This grammar contains around 1300 elementary
trees and covers auxiliaries, copula, raising and
small clause constructions, relative clauses, infini-
tives, gerunds, passives, adjuncts, wh-clefts, PRO
constructions, imperatives and 15 distinct subcate-
gorisation frames.
The syntactic and morpho-syntactic lexicons used
for generating were derived from various existing
lexicons, converted to fit the format expected by
GraDe and tailored to cover basic vocabulary as de-
fined by the lexicon used in Tex?s French Grammar.
The syntactic lexicon contains 690 lemmas and the
morphological lexicon 5294 forms.
4.2 Pedagogical Goals
We evaluate the approach on 16 pedagogical goals
taken from the Tex?s French Grammar book. For
each of these goals, we define the corresponding
linguistic characterization in the form of a GramEx
query. We then evaluate the exercises produced by
the system for each of these queries. The pedagog-
ical goals tested are the following (we indicate in
brackets the types of learning activity produced for
each teaching goal by the system):
? Adjectives: Adjective Order (MSHUF), Adjec-
tive Agreement (FIBLEM), Prenominal adjec-
tives (FIBLEM), Present and Past Participial
used as adjectives (FIBLEM), Regular and Ir-
regular Inflexion (FIBLEM), Predicative adjec-
tives (MSHUF)
? Prepositions: Prepositional Infinitival Object
(FIBBLNK), Modifier and Complement Prepo-
sitional Phrases (FIBBLNK)
? Noun: Gender (FIBLEM), Plural form (FI-
BLEM), Subject Pronoun (FIBHINT).
? Verbs: Pronominals (FIBLEM), -ir Verbs in
the present tense (FIBLEM), Simple past (FI-
BLEM), Simple future (FIBLEM), Subjunctive
Mode (FIBLEM).
4.3 GraDe?s Input and User-Defined
Parameters
GraDe?s configuration As mentioned in Sec-
tion 3, we run GraDe using two main configura-
tions. In the first configuration, GraDe search is con-
strained by an input core semantics which guides the
grammar traversal and forces the output sentence to
verbalise this core semantics. In this configuration,
GraDe will only produce the temporal variations
supported by the lexicon (the generated sentences
may be in any simple tense i.e., present, future,
simple past and imperfect) and the syntactic varia-
tions supported by the grammar for the same MRSs
(e.g., active/passive voice alternation and cleft argu-
ments).
Greater productivity (i.e., a larger output/input ra-
tio) can be achieved by providing GraDe with less
constrained input. Thus, in the second configura-
tion, we run GraDe not on core semantics but on the
full grammar. To constrain the search, we specify a
root constraint which requires that the main verb of
all output sentences is an intransitive verb. We also
set the constraints on recursive rules so as to exclude
the inclusion of modifiers. In sum, we ask GraDe to
produce all clauses (i) licensed by the grammar and
the lexicon; (ii) whose verb is intransitive; and (iii)
152
which do not include modifiers. Since the number
of sentences that can be produced under this con-
figuration is very large, we restrict the experiment
by using a lexicon containing a single intransitive
verb (chanter/To sing), a single common noun and a
single proper name. In this way, syntactically struc-
turally equivalent but lexically distinct variants are
excluded.
Input Semantics We use two different sets of in-
put semantics for the semantically guided configura-
tion: one designed to test the pedagogical coverage
of the system (Given a set of pedagogical goals, can
GramEx generate exercises that appropriately target
those goals?); and the other to illustrate linguistic
coverage (How much syntactic variety can the sys-
tem provide for a given pedagogical goal?).
The first set (D1) of semantic representations con-
tains 9 items representing the meaning of exam-
ple sentences taken from the Tex?s French Gram-
mar textbook. For instance, for the first item
in Figure 1, we use the semantic representation
L1:named(J bette n) A:le d(C RH SH) B:bijou n(C)
G:aimer v(E J C). With this first set of input seman-
tics, we test whether GramEx correctly produces the
exercises proposed in the Tex?s French Grammar
book. Each of the 9 input semantics corresponds to
a distinct pedagogical goal.
The second set (D2) of semantic representations
contains 22 semantics, each of them illustrating dis-
tinct syntactic configurations namely, intransitive,
transitive and ditransitive verbs; raising and control;
prepositional complements and modifiers; sentential
and prepositional subject and object complements;
pronominal verbs; predicative, attributive and par-
ticipial adjectives. With this set of semantics, we
introduce linguistically distinct material thereby in-
creasing the variability of the exercises i.e., making
it possible to have several distinct syntactic configu-
rations for the same pedagogical goal.
5 Evaluation, Results and Discussion
Using the experimental setup described in the previ-
ous section, we evaluate GramEx on the following
points:
? Correctness: Are the exercises produced by the
generator grammatical, meaningful and appro-
priate for the pedagogical goal they are associ-
ated with?
? Variability: Are the exercises produced linguis-
tically varied and extensive? That is, do the ex-
ercises for a given pedagogical goal instantiate
a large number of distinct syntactic patterns?
? Productivity: How much does GramEx support
the production, from a restricted number of se-
mantic input, of a large number of exercises?
Correctness To assess correctness, we randomly
selected 10 (pedagogical goal, exercise) pairs for
each pedagogical goal in Section 4.2 and asked two
evaluators to say for each pair whether the exer-
cise text and solutions were grammatical, meaning-
ful (i.e., semantically correct) and whether the ex-
ercise was adequate for the pedagogical goal. The
results are shown in Table 3 and show that the sys-
tem although not perfect is reliable. Most sources of
grammatical errors are cases where a missing word
in the lexicon fails to be inflected by the generator.
Cases where the exercise is not judged meaningful
are generally cases where a given syntactic construc-
tion seems odd for a given semantics content. For
instance, the sentence C?est Bette qui aime les bi-
joux (It is Bette who likes jewels) is fine but C?est
Bette qui aime des bijoux although not ungrammati-
cal sounds odd. Finally, cases judged inappropriate
are generally due to an incorrect feature being as-
signed to a lemma. For instance, avoir (To have) is
marked as an -ir verb in the lexicon which is incor-
rect.
Grammatical Meaningful Appropriate
91% 96% 92%
Table 3: Exercise Correctness tested on 10 randomly se-
lected (pedagogical goal, exercise pairs)
We also asked a language teacher to examine 70
exercises (randomly selected in equal number across
the different pedagogical goals) and give her judg-
ment on the following three questions:
? A. Do you think that the source sentence se-
lected for the exercise is appropriate to practice
the topic of the exercise? Score from 0 to 3 ac-
cording to the degree (0 inappropriate - 3 per-
fectly appropriate)
153
Nb. Ex. 1 2 4 5 6 12 17 18 20 21 23 26 31 37 138
Nb. Sem 1 4 6 1 4 3 1 1 1 1 1 1 1 1 1
Table 2: Exercise Productivity: Number of exercises produced per input semantics
? B. The grammar topic at hand together with
the complexity of the source sentence make
the item appropriate for which language level?
A1,A2,B1,B2,C13
? C. Utility of the exercise item: ambiguous (not
enough context information to solve it) / correct
For Question 1, the teacher graded 35 exercises as
3, 20 as 2 and 14 as 1 pointing to similar problems
as was independently noted by the annotators above.
For question B, she marked 29 exercises as A1/A2,
24 as A2, 14 as A2/B1 and 3 as A1 suggesting that
the exercises produced are non trivial. Finally, she
found that 5 out of the 70 exercises lacked context
and were ambiguously phrased.
Variability For any given pedagogical goal, there
usually are many syntactic patterns supporting learn-
ing. For instance, learning the gender of common
nouns can be practiced in almost any syntactic con-
figuration containing a common noun. We assess the
variability of the exercises produced for a given ped-
agogical goal by computing the number of distinct
morpho-syntactic configurations produced from a
given input semantics for a given pedagogical goal.
We count as distinct all exercise questions that are
derived from the same semantics but differ either
in syntax (e.g., passive/active distinction) or in mor-
phosyntax (determiner, number, etc.). Both types of
differences need to be learned and therefore produc-
ing exercises which, for a given pedagogical goal,
expose the learner to different syntactic and morpho-
syntactic patterns (all involving the construct to be
learned) is effective in supporting learning. How-
ever we did not take into account tense differences
as the impact of tense on the number of exercises
produced is shown by the experiment where we gen-
erate by traversing the grammar rather than from a
3A1, A2, B1, B2 and C1 are reference levels established
by the Common European Framework of Reference for
Languages: Learning, Teaching, Assessment (cf. http:
//en.wikipedia.org/wiki/Common_European_
Framework_of_Reference_for_Languages) for
grading an individual?s language proficiency.
semantics. Table 4 shows for each (input semantics,
teaching goal) pair the number of distinct patterns
observed. The number ranges from 1 to 21 distinct
patterns with very few pairs (3) producing a single
pattern, many (33) producing two patterns and a fair
number producing either 14 or 21 patterns.
Nb. PG 1 2 3 4 5 6
Nb. sent 213 25 8 14 10 6
Table 6: Pedagogical Productivity: Number of Teaching
Goals the source sentence produced from a given seman-
tics can be used for
Productivity When used to generate from seman-
tic representations (cf. Section 4.3), GramEx only
partially automates the production of grammar ex-
ercises. Semantic representations must be manually
input to the system for the exercises to be generated.
Therefore the issue arises of how much GramEx
helps automating exercise creation. Table 5 shows
the breakdown of the exercises produced per teach-
ing goal and activity type. In total, GramEx pro-
duced 429 exercises out of 28 core semantics yield-
ing an output/input ratio of 15 (429/28). Further, Ta-
ble 2 and 6 show the distribution of the ratio be-
tween (i) the number of exercises produced and the
number of input semantics and (ii) the number of
teaching goals the source sentences produced from
input semantics i can be used for. Table 6 (peda-
gogical productivity) shows that, in this first exper-
iment, a given input semantics can provide material
for exercises targeting up to 6 different pedagogi-
cal goals while Table 2 (exercise productivity) shows
that most of the input semantics produce between 2
and 12 exercises4.
When generating by grammar traversal, under the
constraints described in Section 4, from one input
4If the input semantics contains a noun predicate whose gen-
der is underspecified, the exercise productivity could be dou-
bled. This is the case for 4 of the input semantics in the dataset
D2; i.e. an input semantics containing the predicates tatou n(C)
petit a(C) will produce variations such as: la petite tatou (the
small armadillo (f)) and le petit tatou (the small armadillo (m)).
154
Nb. SP 1 2 3 4 5 6 7 8 9 10 14 21
(S,G) 3 33 16 7 2 4 6 1 4 1 2 6
Table 4: Variability: Distribution of the number of distinct sentential patterns that can be produced for a given peda-
gogical goal from a given input semantics
Pedagogical Goal FIBLEM FIBBLNK MSHUF FIBHINT
Preposition ? 28 ? ?
Prepositions with infinitives ? 8 ? ?
Subject pronouns?il ? ? ? 3
Noun number 11 ? ? ?
Noun gender ? 49 ? ?
Adjective order ? ? 30 ?
Adjective morphology 30 ? ? ?
Adjectives that precede the noun 24 ? ? ?
Attributive Adjectives ? ? 28 ?
Irregular adjectives 4 ? ? ?
Participles as adjectives 4 ? ? ?
Simple past 78 ? ? ?
Simple future 90 ? ? ?
-ir verbs in present 18 ? ? ?
Subjunctive mode 12 ? ? ?
Pronominal verbs 12 ? ? ?
Total 236 78 30 3
Table 5: Number and Types of Exercises Produced from the 28 input semantics
90 exercises are generated targeting 4 different ped-
agogical goals (i.e. 4 distinct linguistic phenomena).
6 Conclusion
We presented a framework (called GramEx) for gen-
erating grammar exercises which are similar to those
often used in textbooks for second language learn-
ing. These exercises target a specific learning goal;
and, they involve short sentences that make it eas-
ier for the learner to concentrate on the grammatical
point to be learned.
One distinguishing feature of the approach is the
rich linguistic information associated by the gen-
erator with the source sentences used to construct
grammar exercises. Although space restriction pre-
vented us from showing it here, this information
includes, in addition to the morphosyntactic infor-
mation and the grammatical properties illustrated in
Figure 2 and Table 1 respectively, a semantic rep-
resentation, a derivation tree showing how the parse
tree of each sentence was obtained and optionally,
an underspecified semantics capturing the core pred-
icate/argument and modifier/modifiee relationships
expressed by each sentence. We are currently ex-
ploring how this information could be used to ex-
tend the approach to transformation exercises (e.g.,
passive/active) where the relation between exercise
question and exercise solution is more complex than
in FIB exercises.
Another interesting question which needs further
investigation is how to deal with exercise items that
have multiple solutions such as example (3) above.
Here we plan to use the fact that underspecified se-
mantics in GraDe permits associating many variants
with a given semantics.
Acknowledgments
We would like to thank the language teacher, Tex?s
French Grammar developers, and the anonymous re-
viewers for their useful comments. The research
presented in this paper was partially supported
by the European Fund for Regional Development
within the framework of the INTERREG IV A Alle-
gro Project5.
5http://www.allegro-project.eu/ and http:
//talc.loria.fr/-ALLEGRO-Nancy-.html
155
References
Itziar Aldabe, Maddalen Lopez de Lacalle, Montse Mar-
itxalar, Edurne Martinez, and Larraitz Uria. 2006.
Arikiturri: an automatic question generator based on
corpora and nlp techniques. In Proceedings of the
8th international conference on Intelligent Tutoring
Systems, ITS?06, pages 584?594, Berlin, Heidelberg.
Springer-Verlag.
Liu Chao-Lin, Wang Chun-Hung, Gao Zhao-Ming, and
Huang Shang-Ming. 2005. Applications of lexical
information for algorithmically composing multiple-
choice cloze items. In Proceedings of the second
workshop on Building Educational Applications Us-
ing NLP, EdAppsNLP 05, pages 1?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chia-Yin Chen, Hsien-Chin Liou, and Jason S. Chang.
2006. Fast: an automatic generation system for gram-
mar tests. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL ?06,
pages 1?4, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Coniam. 1997. A preliminary inquiry into using
corpus word frequency data in the automatic genera-
tion of english language cloze tests. CALICO Journal,
14:15?33.
Claire Gardent and German Kruszewski. 2012. Gener-
ation for grammar engineering. In 11th International
Conference on Natural Language Generation (ENLG).
Michael Heilman and Maxine Eskenazi. 2007. Ap-
plication of automatic thesaurus extraction for com-
puter generation of vocabulary questions. In Proceed-
ings of Speech and Language Technology in Education
(SLaTE2007), pages 65?68.
Nikiforos Karamanis, Le An Ha, and Ruslan Mitkov.
2006. Generating multiple-choice test items from
medical text: A pilot study. In Proceedings of the
Fourth International Natural Language Generation
Conference, pages 111?113, Sydney, Australia.
John Lee and Stephanie Seneff. 2007. Automatic gener-
ation of cloze items for prepositions. Proceedings of
Interspeech, pages 2173?2176.
Yi-Chien Lin, Li-Chun Sung, and Meng Chang Chen.
2007. An Automatic Multiple-Choice Question Gen-
eration Scheme for English Adjective Understandings.
In Workshop on Modeling, Management and Gener-
ation of Problems/Questions in eLearning, the 15th
International Conference on Computers in Education
(ICCE 2007), pages pages 137?142.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A computer-aided environment for generating
multiple-choice test items. Natural Language Engi-
neering, 12(2):177?194.
Adam Kilgarriff Simon Smith, P.V.S Avinesh. 2010.
Gap-fill Tests for Language Learners: Corpus-Driven
Item Generation. In Proceedings of ICON-2010: 8th
International Conference on Natural Language Pro-
cessing.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring non-native speakers? pro-
ficiency of english by using a test with automatically-
generated fill-in-the-blank questions. In Proceedings
of the second workshop on Building Educational Ap-
plications Using NLP, EdAppsNLP 05, pages 61?68,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
156
