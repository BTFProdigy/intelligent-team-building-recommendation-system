Proceedings of the ACL 2007 Demo and Poster Sessions, pages 85?88,
Prague, June 2007. c?2007 Association for Computational Linguistics
Poliqarp
An open source corpus indexer and search engine with syntactic extensions
Daniel Janus
Sentivision Polska Sp. z o.o.
Marynarska 19a, 02-674 Warsaw, Poland
nathell@korpus.pl
Adam Przepi?rkowski
Insitute of Computer Science
Polish Academy of Sciences
Ordona 21, 01-237 Warsaw, Poland
adamp@ipipan.waw.pl
Abstract
This paper presents recent extensions to
Poliqarp, an open source tool for index-
ing and searching morphosyntactically an-
notated corpora, which turn it into a tool for
indexing and searching certain kinds of tree-
banks, complementary to existing treebank
search engines. In particular, the paper dis-
cusses the motivation for such a new tool,
the extended query syntax of Poliqarp and
implementation and efficiency issues.
1 Introduction
The aim of this paper is to present extensions
to Poliqarp,1 an efficient open source indexer
and search tool for morphosyntactically annotated
XCES-encoded (Ide et al, 2000) corpora, with
query syntax based on that of CQP (Christ, 1994),
but extending it in interesting ways. Poliqarp
has been in constant development since 2003
(Przepi?rkowski et al, 2004) and it is currently em-
ployed as the search engine of the IPI PAN Cor-
pus of Polish (Przepi?rkowski, 2004) and the Lis-
bon corpus of Portuguese (Barreto et al, 2006),
as well as in other projects. Poliqarp has a typi-
cal server-client architecture, with various Poliqarp
clients developed so far, including GUI clients for
a variaty of operating systems (Linux, Windows,
MacOS, Solaris) and architectures (big-endian and
little-endian), as well as a PHP client. Since March
2006, the 1st stable version of Poliqarp (Janus and
1Polyinterpretation Indexing Query And Retrieval
Processor
Przepi?rkowski, 2006) is available under GPL.2 A
version of Poliqarp that implements various statisti-
cal extensions is at the beta-testing stage.
Although Poliqarp was designed as a tool for cor-
pora linguistically annotated at word-level only, the
extensions described in this paper turn it into an in-
dexing and search tool for certain kinds of treebanks,
complementary to existing treebank search engines.
Section 2 briefly introduces the basic query syn-
tax of Poliqarp, section 3 presents extensions of
Poliqarp aimed at the processing of treebanks, sec-
tion 4 discusses implementation and efficiency is-
sues, and section 5 concludes the paper.
2 Query Syntax
In the Poliqarp query language, just as in CQP, reg-
ular expressions may be formulated over corpus po-
sitions, e.g.: [pos="adj"]+, where any non-empty
sequence of adjectives is sought, or within values
of attributes, e.g.: [pos="a.*"], concerning forms
(henceforth: segments) tagged with POSs whose
names start with an a, e.g., adj and adv.
Parts of speech and morphosyntactic cate-
gories may be queried separately, e.g., the query
[gend=masc] could be used to search for masculine
segments, regardless of the POS or other categories,
while the query [pos="subst|ger"&gend!=masc]
can be used to find nominal and gerundive segments
which are not masculine.
A unique feature of Poliqarp is that it may be
used for searching corpora containing, in addition to
disambiguated interpretations, information about all
2Cf. http://poliqarp.sourceforge.net/.
85
possible morphosyntactic interpretations given by
the morphological analyser. For example, the query
[case~acc] finds all segments with an accusative
interpretation (even if this is not the interpretation
selected in a given context), while [case=acc] finds
segments which were disambiguated to accusative in
a given context.
Moreover, Poliqarp does not make the assump-
tion that only one interpretation must be correct for
any given segment; some examples of sentences
containing an ambiguous segment which cannot be
uniquely disambiguated even given unlimited con-
text and all the linguistic and encyclopaedic knowl-
edge are cited in (Przepi?rkowski et al, 2004). In
such cases, the = operator has the existential mean-
ing, i.e., [case=acc] finds segments with at least
one accusative interpretation marked as correct in
the context (?disambiguated?). On the other hand,
the operator == is universal, i.e., [case==acc] finds
segments whose all disambiguated interpretations
are accusative: segments which were truly uniquely
disambiguated to one (accusative) interpretation, or
segments which have many interpretations correct in
the context, but all of them are accusative.3 For com-
pleteness, the operator ~~ is added, which univer-
sally applies to all morphosyntactic interpretations,
i.e., [case~~acc] finds segments whose all interpre-
tations as given by a morphological analyser (before
disambiguation) are accusative.
The most detailed presentation of the orig-
inal query syntax of Poliqarp is available in
(Przepi?rkowski, 2004), downloadable from
http://korpus.pl/index.php?page=
publications.
3 Syntactic Extensions
(Przepi?rkowski, 2007) argues for the explicit rep-
resentation of both a syntactic head and a seman-
tic head for each syntactic group identified in a
(partially parsed) constituency-based (as opposed to
dependency-based) treebank. For example, for the
Polish syntactic group tuzin bia?ych koni, ?a dozen
of white horses?, lit. ?dozen-NOM/ACC white-GEN
horses-GEN?, the syntactic head is tuzin ?dozen?,
3In Polish this may happen, for example, in case of some
gerund forms which are homographs of true nouns, where
meaning does not make it possible to decide on the nominal /
gerundive interpretation of the form.
while the semantic head is koni ?horses?. The seg-
ment koni is also both the syntactic head and the
semantic head of the embedded nominal group bi-
a?ych koni ?white horses?. In general, following
(Przepi?rkowski, 2007), a given segment is a syn-
tactic head of at most one group (e.g., tuzin and koni
in the example above), but it may be a semantic head
of a number of groups (e.g., koni above is a semantic
head of bia?ych koni and of tuzin bia?ych koni).
This kind of representation is problematic for gen-
eral search tools for constituency-based treebanks,4
such as TIGERSearch (Lezius, 2002),5 which usu-
ally assume that the set of edges within a syntactic
representation of a sentence is a tree, in particular,
that it has a single root node and that each leaf has
(at most) one incoming edge.6 While the former as-
sumption is not a serious problem (an artificial sin-
gle root may always be added), the latter is fatal for
representations alluded to above, as a single segment
may be a semantic head of a number of syntactic
groups, i.e., it may have several incoming edges.
The extension of Poliqarp presented here makes
it possible to index and search for such (partial)
syntactic-semantic treebanks. Specifications of syn-
tactic constructions in the extended Poliqarp query
language syntax are similar to specifications of par-
ticular segments, but they use a different repertoire
of attributes, non-overlapping with the attributes
used to specify single segments. Two main at-
tributes to be used for querying for syntactic groups
are: type and head. The attribute type spec-
ifies the general syntactic type of the group, so
[type=Coordination] will find coordinated con-
structions, while [type="[PN]G"] will find prepo-
sitional and nominal groups.
The syntax of values of the attribute head differs
from that of the other attributes; its values must be
enclosed in a double or a single set of square brack-
ets, as in: [head=[...][...]] or [head=[...]].
In the first case, the first brackets specify the syntac-
tic head and second brackets specify the semantic
4It seems that it would also be problematic for depen-
dency tools such as Netgraph, cf. (Hajic? et al, 2006) and
http://quest.ms.mff.cuni.cz/netgraph/doc/
netgraph_manual.html.
5Cf. http://www.ims.uni-stuttgart.de/
projekte/TIGER/.
6In TIGER tools, there is a special mechanism for adding a
second edge, e.g., in order to represent control.
86
head, as in the following query which may be used
to find elective constructions of the type najstarszy
z koni ?(the) oldest of horses?, which are syntacti-
cally headed by the adjective and semantically by
the semantic head of the dependent of that adjective:
[head=[pos=adj][pos=noun]].
In the second case, the content of the single brack-
ets specifies both the syntactic head and the se-
mantic head and, additionally, makes the require-
ment that they be the same segment. This means
that the queries [head=[case=gen][case=gen]]
and [head=[case=gen]] have a slightly different
semantics: the first will find syntactic groups where
the two heads may be different or the same, but they
must be genitive; the second will find groups with
the two heads being necessarily the same genitive
segment.
The usefulness of such queries may be illus-
trated with a query for verbs which co-occur with
dative dependents denoting students; the first
approximation of such a query may look like this:
[pos=verb][head=[case=dat][base=student]].
This query will find not only dative nominal groups
headed by a form of STUDENT, but also dative
numeral groups whose main noun is a form of
STUDENT, appropriate dative adjectival elective
groups, etc.
As syntactic sugar, the constructs synh=[...]
and semh=[...] can be used to enforce a con-
straint only on, respectively, syntactic or semantic
head of a group.
It may seem that, given the possibility to specify
the syntactic head of the construction, the attribute
type is redundant; in fact, we are not currently
aware of cases where the specification type="PG"
or type="NG" could not be replaced by an ap-
propriate reference to the grammatical class (part of
speech) of the syntactic head. However, the type
attribute is useful for finding constructions which are
not defined by their heads, for example, oratio recta
constructions, and it is also useful for dealing with
coordinate structures.
4 Implementation Issues
To allow for fast searching, the original Poliqarp
uses its own compact binary format for corpora,
described in detail in (Janus, 2006) and briefly in
(Janus and Przepi?rkowski, 2006). Because the
number of syntactic groups can easily grow very
large and be on par with total number of words in a
fully-tagged corpus, the representation of syntactic
groups should be space-efficient, yet alow for fast
decoding and random access.
The key observation to achieving this goal is that,
due to the tree nature of the group set, any two
groups can be either mutually disjoint or completely
contained in each other. Thus, it is possible to seri-
alize the tree into a list, sorted by the lower bound of
a group,7 such that each group is immediately fol-
lowed by its direct subgroups.
More precisely, the on-disk representation of a
treebank is a bit vector that contains the following
data for each group: 1) synchronization bit (see be-
low), usually 0; 2) the difference between the lower
bound of the previous group and the lower bound
of the one in question, encoded in ?-code;8 3) ?-
encoded length of current group in segments; 4)
?-encoded number of type of this group (the map-
ping of numbers to type names is stored in a sepa-
rate on-disk dictionary in which two type numbers
are reserved: 0 for coordinated groups and 1 for
conjunctions); 5) if this is a coordinated construct
(i.e., type = 0) ? ?-encoded number of subsequent
groups (excluding the current one but including in-
direct subgroups) that are part of the coordination;9
or 6) if this is not a coordinated construct (i.e., it is
an ordinary group) ? offset of syntactic and seman-
tic head of this group, in that order, each represented
by a binary number of log l bits, where l stands for
the length of the group.
One drawback of this representation is that it does
not allow for random access: the ?-code and head
offsets have variable length, thus it is not possible to
determine which bit one should start with to decode
the group sequence for a certain segment. To miti-
gate this, a synchronization mechanism is employed.
7The corpus proper is represented by one large vector of
fixed-size structures denoting segments; here, the bounds of a
group mean offsets into that vector.
8The ?-code is a prefix-free variable-length code that en-
codes arbitrary integers so that the representation of small num-
bers takes few bits; see (Witten et al, 1999) for details.
9Special treatment of coordination is caused by the fact that,
as argued in (Przepi?rkowski, 2007), coordinate structures are
best treated as multi-headed constructions, with each conjunct
bringing its own syntactic and semantic head.
87
For every k-th segment (k is a constant defined for
the corpus, usually 1024), the bit offset of start of
the description of the earliest group that intersects
this segment is stored as an unsigned little-endian
32-bit integer in a separate file. In the description of
this group, the synchronization bit is set to 1, and the
lower bound is spelled in full (as an unsigned 32-bit
binary integer) so that it is not necessary to know the
previous lower bound to start decoding.
This synchronization lines up with the sparse in-
verted indexing mechanism used by Poliqarp for ef-
ficient searching. Poliqarp artificially splits the cor-
pus into fixed-size chunks and remembers which
segments occur in which chunks; if the search en-
gine makes random access to the corpus, the ac-
cessed segments? offsets are multiplies of the chunk
size. It is best, thus, to ascertain that the constant k
is also equal to this chunk size.
In a typical scenario with many mostly small
groups occurring close to each other, this encoding
schema is capable of achieving the ratio of well un-
der two bytes per group and does not incur a signifi-
cant overhead in corpus size (which is usually in the
range of 10?12 bytes times the number of segments
for a morphosyntactically but not structurally tagged
corpus). This is important, since disk access is the
key factor in Poliqarp?s performance.
5 Conclusions
In this paper, we presented an extension of Poliqarp,
a tool for indexing and searching morphosyntacti-
cally annotated corpora, towards the management of
syntactically annotated corpora. An interesting fea-
ture of thus extended Poliqarp is its ability to deal
with treebanks which do not adopt the ?at most one
incoming edge? assumption and which distinguish
between syntactic heads and semantic heads. We
also sketched the original and efficient method of
indexing such treebanks. The implementation of
the extensions currently approaches the alpha stage.
By the time of ACL 2007, we expect to release the
sources of a relatively stable beta-stage version.
References
Florbela Barreto, Ant?nio Branco, Eduardo Ferreira,
Am?lia Mendes, Maria Fernanda Nascimento, Filipe
Nunes, and Jo?o Silva. 2006. Open resources and
tools for the shallow processing of Portuguese: The
TagShare project. In Proceedings of the Fifth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2006).
Oli Christ. 1994. A modular and flexible architecture for
an integrated corpus query system. In COMPLEX?94,
Budapest.
Jan Hajic?, Eva Hajic?ov?, Jaroslava Hlav?c?ov?, V?clav
Klime?, Jir?? Mirovsk?, Petr Pajas, Jan ?te?p?nek, Bar-
bara Vidov? Hladk?, and Zdene?k ?abokrtsk?, 2006.
PDT 2.0 ? Guide. Charles University, Prague. June
20, 2006.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based standard for linguistic
corpora. In Proceedings of the Linguistic Resources
and Evaluation Conference, pages 825?830, Athens,
Greece.
Daniel Janus and Adam Przepi?rkowski. 2006. Poliqarp
1.0: Some technical aspects of a linguistic search en-
gine for large corpora. In Jacek Walin?ski, Krzysztof
Kredens, and Stanis?aw Goz?dz?-Roszkowski, editors,
The proceedings of Practical Applications of Linguis-
tic Corpora 2005, Frankfurt am Main. Peter Lang.
Daniel Janus. 2006. Metody przeszukiwania i obrazowa-
nia jego wynik?w w duz?ych korpusach tekst?w. Mas-
ter?s thesis, Uniwersytet Warszawski, Wydzia? Matem-
atyki, Informatyki i Mechaniki, Warsaw.
Wolfgang Lezius. 2002. TIGERSearch ? ein Suchw-
erkzeug f?r Baumbanken. In Stephan Busemann, ed-
itor, Proceedings der 6. Konferenz zur Verarbeitung
nat?rlicher Sprache (KONVENS 2002), Saarbr?cken.
Adam Przepi?rkowski, Zygmunt Krynicki, ?ukasz
De?bowski, Marcin Wolin?ski, Daniel Janus, and Pi-
otr Ban?ski. 2004. A search tool for corpora with
positional tagsets and ambiguities. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, LREC 2004, pages 1235?
1238, Lisbon. ELRA.
Adam Przepi?rkowski. 2004. The IPI PAN Corpus: Pre-
liminary version. Institute of Computer Science, Pol-
ish Academy of Sciences, Warsaw.
Adam Przepi?rkowski. 2007. On heads and coordina-
tion in valence acquisition. In Alexander Gelbukh,
editor, Computational Linguistics and Intelligent Text
Processing (CICLing 2007), Lecture Notes in Com-
puter Science, pages 50?61, Berlin. Springer-Verlag.
Ian H. Witten, Alistair Moffat, and Timothy C. Bell.
1999. Managing Gigabytes: Compressing and Index-
ing Documents and Images. Morgan Kaufmann Pub-
lishers Inc., San Francisco, CA, 2nd edition.
88
A Flexemic Tagset for Polish
Adam Przepi?rkowski
Institute of Computer Science
Polish Academy of Sciences
adamp@ipipan.waw.pl
Marcin Wolin?ski
Institute of Computer Science
Polish Academy of Sciences
wolinski@ipipan.waw.pl
Abstract
The article notes certain weaknesses of
current efforts aiming at the standard-
ization of POS tagsets for morphologi-
cally rich languages and argues that, in
order to achieve clear mappings between
tagsets, it is necessary to have clear
and formal rules of delimiting POSs and
grammatical categories within any given
tagset. An attempt at constructing such
a tagset for Polish is presented.
1 Introduction
The aim of this article is to address one of the ob-
jectives of the EACL 2003 workshop on Morpho-
logical Processing of Slavic Languages, namely,
to ?try to reveal lexical structures necessary for
morphological analysis and. . . discuss standard-
ization efforts in the field that can, for instance,
enable transfer of applied methods from one lan-
guage to the other or inform the annotation of mor-
phological information in corpora.?
One admirable standardization effort in the field
of Slavic part of speech (POS) tagging has been
the Multext-East project (Erjavec, 2001), one of
whose aims was to construct mutually compati-
ble tagsets for 8 European languages, including 4
Slavic languages (originally Bulgarian, Czech and
Slovene, later extended to Croatian); additionally,
a Multext-East-style tagset for Russian was con-
structed at the University of T?bingen (http:
//www.sfb441.uni-tuebingen.de/c1/
tagset.html). Those tagsets are based on a
common repertoire of grammatical classes (POSs;
e.g., ?verb?, ?noun?, ?adjective?, etc.) and gram-
matical categories (e.g., ?case?, ?person?, ?gender?,
etc.), and each tagset uses just a subset of those
grammatical classes and categories.
Despite the considerable success of Multext-
East, and the apparent uniformity of the result-
ing tagsets, certain weaknesses of this approach
are clear. First of all, the relative uniformity of
the POS classes across the 8 languages was at-
tained at the cost of introducing the grammatical
category ?type? whose values reflect the consider-
able differences between POS systems of the lan-
guages involved. Second, it is not clear that vari-
ous grammatical categories and their values have
the same interpretation in each language; for ex-
ample, it is rather surprising that only the Roma-
nian tagset explicitly mentions strong and weak
pronominal forms, it is not clear whether negative
pronouns in Romanian, Slovene, Czech and Bul-
garian are negative in the same sense of participat-
ing in Negative Concord, it is not clear why Roma-
nian has negative adverbs while, say, Czech lacks
them, etc. Finally, and most importantly from our
point of view, the approach adopted by Multext-
East does not clearly reflect cross-linguistic cor-
respondences, such as the one mentioned in (Er-
javec, 2001), that ?in the Romanian case system
the value ?direct? conflates ?nominative? and ?ac-
cusative?, while the value ?oblique? conflates ?gen-
itive? and ?dative?.? Such correspondences are not
exceptional, e.g., the at least three masculine gen-
ders of Polish (Man?czak, 1956; Saloni, 1976) are
mapped into the single masculine gender of many
other languages, the dual and the plural numbers
of some languages (Slovene, Czech) are mapped
to plural of other languages, etc.
In more general terms, we have identified the
following features of currently used tagsets for
Slavic in general and Polish in particular which
seem problematic from the point of view of their
reusability and cross-linguistic applicability:
  uncritical adoption of traditional and some-
times ill-defined POS classes, such as ?pro-
noun? or vaguely delimited classes such as
?verb? or ?noun? (it is often not clear whether
gerunds are ?verbs? or ?nouns? in such classi-
fications);
  POS classes and categories are often chosen
on the basis of a mix of morphological, syn-
tactic and semantic criteria, e.g., ?gender? in
Slavic is sometimes defined on the basis of
mixed morphosyntactic and semantic proper-
ties, and so are ?pronoun? and ?numeral?;
  mixing morphosyntactic annotation with
what might be called dictionary annotation;
e.g., tagsets often include tags for proper
names or morphosyntactically transparent
collocations, which ? in our opinion ? do
not belong to the realm of POS annotation;
  sometimes the priorities of such mixed cri-
teria are unclear, e.g., should the preposition
of in District of Columbia be tagged as an
ordinary preposition, or should it have the
?proper? tag as it is a part of a proper name?
  ignoring the finer points of the morphosyn-
tactic system of a given language, e.g.,
the multitude of genders in languages such
as Polish, or categories such as ?post-
prepositionality? and ?accommodability? (see
below);
  unclear segmentation rules (should so-called
analytic tenses or reflexive verbs be treated as
single units for the purpose of annotation?).
The main thesis of this paper is that, in order for
a tagset to be reusable and comparable with simi-
lar tagsets for related languages, it must be based
on a homogeneous set of clear formal (morpho-
logical and morphosyntactic) criteria. Only once
such criteria for delimiting grammatical classes
and categories are presented in detail, can those
classes and categories be mapped to grammati-
cal classes and categories of other similarly con-
structed tagsets.
The remainder of the paper presents such a
tagset for Polish, developed within a Polish cor-
pus project1 and deployed by a stochastic tagger
of Polish (De?bowski, 2003).
2 A Flexemic Tagset for Polish
The tagset presented in this section is based on the
following design assumptions:
  what is being tagged is a single orthographic
word or, in some well-defined cases, a part
thereof; multi-word constructions, even those
sometimes considered to be morphological
formations (so-called analytic forms) or dic-
tionary entries (proper names), should be
considered by a different level of process-
ing;2 cf. 2.1;
  grammatical categories reflect various oppo-
sitions in the morphological system, even
those oppositions which pertain to single
grammatical classes and are not recognized
by traditional grammars; cf. 2.2;
  the main criteria for delimiting grammati-
cal classes are morphological (how a given
form inflects; e.g., nouns inflect for case,
but not for gender) and morphosyntactic (in
which categories it agrees with other forms;
e.g., Polish nouns do not inflect for gen-
der but they agree in gender with adjectives
and verbs); semantic criteria are eschewed;
cf. 2.3.
2.1 Segmentation
By segmentation, or tokenization, we mean the
task of splitting the input text into tokens, i.e., seg-
1An Annotated Internet-Accessible Corpus of Written Pol-
ish (with Emphasis on NLP Applications), a 3-year project
financed by the State Committee for Scientific Research.
2In case of proper names, there exist many dedicated algo-
rithms and systems for finding them in texts, often developed
within the Message Understanding Conference series.
ments of texts which are subject to morphosyntac-
tic tagging. We propose the following guidelines
for segmentation (for a more complete discussion
see our other article in this volume):
  tokens do not contain white space;
  tokens either are punctuation marks or do not
contain any punctuation marks;
  an exception to the previous guideline are
certain words containing the hyphen (e.g.,
mass-media, s-ka = an abbreviation of sp?ka
?company?, etc.) and apostrophe used in Pol-
ish when inflecting foreign names (e.g. La-
grange?a); they are given by a list.
Those guidelines do not preculde the situation
where an orthographic word is split into several
POS tokens. For example, in the case of Polish
past tense finite verbs, the morpheme bearing in-
formation on person and number can be attached
to the verb itself (1a) or to some other word within
the sentence (1b). For that reason we always con-
sider such a ?floating inflection? morpheme as a
separate segment.3
(1) a. Dlaczego
Why
mi
I-dat
nie
not
powiedzia?as??
told be-you
?Why haven?t you told me??
b. Dlaczegos?
Why be-you
mi
I-dat
nie
not
powiedzia?a?
told
2.2 Morphological Categories
Although we proposed ignoring some information
often present in tagsets, e.g., the ?proper noun? vs.
?common noun? distinction, we argue that mor-
phological categories should be taken seriously
and should be as detailed as possible.
What follows is the complete list of morpholog-
ical categories assumed in the proposed tagset:
  number: sg , pl ;
  case: nom , acc , gen , dat , inst , loc , voc;
  gender: masculine personal m1 (facet), mas-
culine animate m2 (ko?n), masculine inani-
mate m3 (st?), feminine f (kobieta, zyrafa,
3Segmentation, as understood in the present context, is
discussed at length in (Przepi?rkowski and Wolin?ski, 2003).
ksi azka), two neuter genders n1 (dziecko), n2
(okno), and three plurale tantum genders p1
(wujostwo), p2 (drzwi), p3 (okulary);
  person: pri , sec , ter;
  degree: pos , comp , sup;
  aspect : imperf , perf ;
  negation: aff , neg;
  accentability (Pol.: akcentowo?s?c): akc ,
nakc;
  post-prepositionality (Pol.: poprzyimko-
wo?s?c): praep , npraep;
  accommodability (Pol.: akomodacyjno?s?c):
congr , rec;
  agglutination (Pol.: aglutynacyjno?s?c): nagl ,
agl ;
  vocability (Pol.: wokaliczno?s?c): wok , nwok .
It may seem surprising, at first, to see 9 gender
values in an Indo-European language (as opposed
to, say, a Bantu language), but this position is well
argued for by (Saloni, 1976), who distinguishes
those genders on the basis of agreement with ad-
jectives and numerals;4 we will not attempt to fur-
ther justify this position here.
Negation is a category of various de-verbal
classes, e.g., participles. Since we assume that the
words pisz acy ?writing? and niepisz acy ?not writ-
ing? have the same lemma pisa?c ?to write?, these
words have to be distinguished with this morpho-
logical category.
The category of accentability is used to dif-
ferentiate accented forms of nominal pronouns
(e.g. jego, mnie) from weak forms (go, mi). It
roughly corresponds to the category of clitic used
in Multext-East.
Post-prepositionality is another category of
nominal pronouns. It differentiates special forms
4Elsewhere, we propose reducing the number of gen-
ders, essentially, by factoring out the number information
(Wolin?ski, 2001) or the information about agreement with nu-
merals (Przepi?rkowski et al, 2002), but for the purposes of
this tagset we assume the original repertoire of genders pro-
posed by Saloni.
used only directly after a preposition (e.g., niego,
-?n) from forms that can be used in other contexts
(jego, go).
The category of accomodability is important for
the description of Polish numeral-nominal phrase.
Some Polish numerals have forms that agree in
case with noun (marked congr), as well as forms
that require a noun in genitive case (marked rec):
(2) Przyszli
came
dwaj
two-nom.congr
ch?opcy.
boys-nom
?Two boys came.?
(3) Przysz?o
came
dw?ch/dwu
two-nom.rec
ch?opc?w
boys-gen
?Two boys came.?
The need for the category of agglutination is a
result of the way past tense verb forms are seg-
mented (cf. (1) in sec. 2.1). For the majority of
Polish verbs the form used for the first and the sec-
ond person is the same as the third person form:
(4) a. Ty
you
przyszed?es?.
came
b. On
he
przyszed?.
came
But for some verbs these forms differ:
(5) a. Ty
you
nios?(nagl )es?.
carried
b. On
he
ni?s?(agl ).
carried
Vocability distinguishes those ?floating? forms
of the verb by?c ?to be? which attach to consonant-
final forms (wok , e.g., -em) from the forms which
attach to vowel-final forms (nwok , e.g., -m).
Various non-standard categories used above,
such as post-prepositionality, accomodability and
agglutination, are based on important work by
Zygmunt Saloni and his colleagues (Saloni, 1976;
Saloni, 1977; Gruszczyn?ski and Saloni, 1978;
Bien? and Saloni, 1982).
2.3 Morphological Classes
Morphological classes, or parts of speech, as-
sumed within various tagsets are usually taken
over more-or-less verbatim from traditional gram-
mars. For example, the Multext-East tagset for
Czech assumes the following parts of speech:
noun, verb, adjective, pronoun, adverb, adposi-
tion, conjunction, numeral, interjection, resid-
ual, abbreviation and particle.
While tagsets based on such POSs are well-
grounded in linguistic tradition, they do not repre-
sent a logically valid classification of wordforms
in the sense that the criteria which seem to under-
lie these classes do not always allow to uniquely
classify a given word. We will support this criti-
cism with two examples.
Let us first of all consider the classes pronoun
and adjective. The former is morphosyntactically
very heterogeneous:
  some pronouns inflect for gender (e.g., the
demonstrative pronoun ten, the possessive
pronoun m?j, but not the interrogative pro-
noun kto or the negative pronoun nikt);
  some pronouns, but not all, inflect for per-
son;
  some pronouns, but not all, inflect for num-
ber;
  the short reflexive pronoun sie does not
overtly inflect at all, although it may be con-
strued as a weak form of the anaphoric pro-
noun siebie.
It seems that the class of pronouns is defined
mainly, if not solely, on the basis of semantic in-
tuition. On the other hand, adjectives are well-
defined morphosyntactically, as the forms inflect-
ing for gender , number and case , but not, say,
person or voice .
Now, according to these definitions, it is not
clear, whether so-called possessive pronouns, such
as m?j ?my? should be classified as pronouns or
adjectives: semantically they belong to the for-
mer class, while morphosyntactically ? to the lat-
ter. (Traditionally, it is classified as a pronoun, of
course.)
Another, and perhaps more serious example
concerns so-called -nie/-cie gerunds, i.e., substan-
tiva verbalia (Puzynina, 1969) such as pi?c::picie
?to drink::drinking?, browsowa?c::browsowanie ?to
browse::browsing?.5 These are nominal forms in
the sense that they have gender (always n2 ) and
inflect for case and, potentially, for number , but
they are also productively related to verbs, have
the category of aspect and inflect for negation .
As such, they do not comfortably fit into the tradi-
tional class noun, whose members do not have as-
pect or negation , nor do they belong to the class
verb, whose members have no case . A similar
difficulty is encountered also in case of adjectival
participles, which ? apart from the adjectival in-
flectional categories of gender , number and case
? also inflect for negation and have aspect .
For this reason, and following the general ap-
proach of (Saloni, 1974) and (Bien?, 1991), we
propose to derive the notion of grammatical class
from the notion of exeme introduced by Bien?,
where flexeme is understood as a morphosyntac-
tically homogeneous set of forms belonging to the
same lexeme.
For example, a typical Polish verbal lexeme
contains a number of personal forms, a number
of impersonal forms, as well as, depending on
a particular understanding of the notion of lex-
eme, various deverbal forms, such as participles
and gerunds. These forms have very different
morphosyntactic properties: finite non-past tense
forms have the inflectional categories of person
and number, adjectival participles have the inflec-
tional properties of non-gradable adjectives and,
additionally, inflect for negation and have aspect,
gerunds inflect for case and, at least potentially, for
number, but not for person, etc. Ideally, flexemes
are subsets of such lexemes consisting of those
forms which have the same inflectional proper-
ties: all verbal forms of given lexeme with the
inflectional category of person and number are
grouped into one flexeme, other forms belong-
ing to this lexeme, but with adjectival inflectional
properties, are grouped into another flexeme, those
forms, which inflect for case but not for gender
are grouped into a gerundial flexeme, etc. Each of
such flexemes is characterized by a set of gram-
matical categories it inflects for and, perhaps, a set
of grammatical categories it has lexically set (e.g.,
5The second pair illustrates the productivity of the gerun-
dial derivational rule: browsowac? is, of course, a very recent
borrowing.
the gender of nouns).
Now, given the notion of flexeme, it is natural
to define grammatical classes as exemic classes,
i.e., classes of flexemes with the same inflec-
tional characteristics. For example, the grammat-
ical class non-past verb contains exactly those
flexemes which inflect for person and number,
and nothing else, and which also have the lexi-
cal category of aspect; the class noun contains ex-
actly those flexemes which inflect for number and
case, and have gender; the class gerund contains
exactly those flexemes which inflect for number,
case and negation, and have lexical gender (always
neuter, n2 , in case of gerunds) and aspect; etc.
It should be noted that, despite the way flex-
emes have been defined above, the notion of lex-
eme is of only secondary importance here: it is
invoked for the purpose of assigning a lemma to a
given form (e.g., a gerundial form such as przyj?s-
ciem ?coming-inst ? will be lemmatized to the in-
finitival form przyj?s?c ?to come?: even though the
form przyj?s?c does not belong to the exeme of
przyj?sciem, it does belong to the lexeme containing
przyj?sciem). Moreover, just as in case of decid-
ing whether two forms belong to the same lexeme,
also classification of two wordforms to the same
flexeme requires some semantic intuition: thus,
e.g., pies ?dog-nom? and psem ?dog-inst ? belong
to the same (f)lexeme, and so do rok ?year-sg? and
lata ?year-pl ?, but pies ?dog? and suka ?bitch? do
not.
The basic classification of flexemes into gram-
matical (?flexemic?) classes is given by the follow-
ing decision tree:
Inflects for case?
YES: Inflects for negation?
YES: Inflects for gender?
YES: 1. adjectival participle
NO: 2. gerund
NO: Inflects for gender?
YES: Has person?
YES: 3. nominal pronoun
NO: Inflects for number?
YES: 4. adjective
NO: 5. numeral
NO: 6. noun
NO: Inflects for gender?
YES: 7. l-participle
NO: Inflects for number?
YES: 8. (inflecting verbal forms)
NO: 9. (?non-inflecting? verbal
forms, adverbs, prepositions,
conjunctions)
Note that most of the classes in the ?inflects
for case? branch of the tree already are reason-
able POSs, i.e., they correspond to traditional
POSs (noun, adjective, numeral) or to their well-
defined subsets (nominal pronoun, gerund, ad-
jectival participle). It is important to realize,
however, that these classes are defined mainly on
the basis of the inflectional properties of their
members; e.g., the class numeral is much nar-
rower here than traditionally, as it does not include
so-called ordinal numerals (which, morphosyntac-
tically, are adjectives).
On the other hand, in the ?does not inflect for
case? branch, only the ?inflects for gender? class
corresponds to an intuitive set of forms, namely,
to so-called l-participles or past participles, i.e.,
verbal forms hosting ?floating inflections?; cf.
powiedziaa in (1) above.
The class 8. above can be further partitioned ac-
cording to the following criteria:
8. Has a ter (i.e., 3rd person) form?
YES: 8.1. non-past forms
NO: Has a pri sg form?
YES: 8.2. agglutinate
(-(e)m, -(e)?s, -?smy, -?scie)
NO: 8.3. imperative
Non-past verb forms correspond to present tense
for imperfective verbs (e.g., ide ?I am going?) and
future tense for perfective ones (e.g., p?jde ?I will
go?).
Further, we will remove from the class of nouns
the flexeme of the strong reflexive pronoun siebie,
which does not inflect for number and does not
have overt gender:
6. Inflects for number?
YES: 6.1. true noun
NO: 6.2. siebie
Moreover, inflectional class marked as 9. can
be further split according to non-inflectional mor-
phosyntactic properties of its members in the fol-
lowing way:
9. Has aspect?
YES: 9.1. non-inflecting verbal forms
NO: Inflects for degree or derived
from adjective?
YES: 9.2. adverb
NO: 9.3. preposition, conjunction,
etc.
In order to arrive at a class close to the traditional
class of adverbs, we had to define this class dis-
junctively; it should contain all adverbs inflecting
for degree, at least one of which does not seem
to be derived from an adjective (bardzo ?very?), as
well as all de-adjectival adverbs, some of which
do not (synthetically) inflect for degree (e.g., anty-
wirusowo ?anti-virus-like?, *antywirusowiej).
If our purpose were to define a purely flexemic
tagset for Polish, we would have to stop here (and
remove the ?derived from adjective? disjunct from
the subtree above). For example, it is impossible
to distinguish the impersonal -no/-to form, the in-
finitive, and adverbial participle of the same lex-
eme on the basis of their morphosyntactic prop-
erties alone: they all lack any inflectional cate-
gories and have the lexical category of aspect . For
this reason, we will further partition the class 9.1.
above on the basis of purely orthographic (or pho-
netic) information:
9.1. Ends in -no or -to?
YES: 9.1.1. impersonal -no/-to forms
(e.g., chodzono ?one used to
walk/go?, pito ?one used to
drink?)
NO: Ends in - ac or -szy?
YES: 9.1.2. adverbial participle
(e.g., czytaj ac ?reading?,
przeczytawszy ?having read?)
NO: 9.1.3. infinitive form (e.g.,
i?s?c ?to go?); should end
in -c or -?c
Finally, the class 9.3. consists of those word-
forms which do not inflect, and do not have as-
pect , i.e.:
9.3.1. conjunction
9.3.2. preposition
9.3.3. particle-adverb
The first two classes are closed classes, which can
be defined extensionally, by enumerating them.
All other non-inflecting, non-aspectual and non-
de-adjectival single-form flexemes fall into the
particle-adverb class.
The table on the next page presents the com-
plete repertoire of grammatical classes and their
respective inflectional (?  ?) and lexical (?  ?) cat-
egories. Some more ephemeral classes not men-
tioned in the decision tree are briefly described
below (a more complete description of a previous
version of this tagset is available in (Wolin?ski and
Przepi?rkowski, 2001)).
For Polish nouns of masculine personal (m1 )
gender a stylistically marked form is possible be-
sides a ?regular? form for nominative and vocative
nu
mb
er
cas
e
ge
nd
er
pe
rso
n
deg
ree
asp
ect
neg
ati
on
acc
en
t.
po
st-
pre
p.
acc
om
.
ag
gl.
vo
cab
.
noun   
depreciative noun   
adjective    
ad-adjectival adjective
post-prepositional adjective
adverb 
numeral    
pronoun (non-3rd person)     
pronoun (3rd person)      
pronoun siebie 
non-past verb   
future by?c   
agglut. by?c    
l-participle    
imperative   
-no/-to 
infinitive 
adv. pres. prtcp. 
adv. anter. prtcp. 
gerund     
adj. act. prtcp.     
adj. pass. prtcp.     
winien-like verb   
predicative
preposition 
conjunction
particle-adverb
alien (nominal)   
alien (other)
case in plural (e.g., profesory vs. profesorowie).
These special forms do not fit in the scheme of
regular nominal inflection, and so were moved to
a separate flexeme for depreciative noun.
Ad-adjectival adjectives are special forms of
adjectives used in compounds like angielsko-
polski ?English-Polish?. Moreover, some adjec-
tives (e.g., polski) have a special form that is re-
quired after some prepositions (e.g., po polsku ?in
Polish?). This form constitutes post-prepositional
adjective flexeme.
A few verbs (e.g., powinien ?should?) inflect in
an atypical way and lack some verbal flexemes
(e.g., imperative and l-participle). Winien-like
flexeme gathers present tense forms of these verbs
(which accept ?floating inflection?).
The class of predicatives consists of verbs
which do not inflect at all (e.g., warto ?be worth?,
mozna ?can/may?, trzeba ?must?).
3 Conclusions
Two tagsets can be compared and respective cor-
respondences between their grammatical classes
and categories can be found more easily when
the definitions of those classes and categories are
stated explicitly and formulated in terms of eas-
ily verifiable formal properties of particular word-
forms, such as their inflectional, morphosyntactic
and derivational characteristics, and their phono-
logical or orthographic makeup.
We presented a tagset for Polish constructed
with such criteria in mind. In particular, gram-
matical classes are understood as classes of flex-
emes, i.e., they are defined on the basis of, first
of all, inflectional and, secondly, morphosyntac-
tic properties of wordforms. Further distinctions,
such as those between non-inflecting forms of ver-
bal lexemes, are also made with the avoidance of
any recourse to the semantic or pragmatic prop-
erties of such forms. This allowed us to evade
the controversial issues of the exact extent of such
semantically-defined traditional POSs as numeral
and pronoun.
Despite the evasion of semantic criteria, the re-
sulting set of grammatical classes bears surpris-
ing affinity to traditional POSs, with classes such
as noun and adjective corresponding directly to
traditional POSs, and other classes, such as non-
past verb, l-participle or gerund being proper
subclasses of such traditional POSs as verb. Be-
cause of this fine-grainedness of the current tagset
we were able to evade the controversial issues of
whether to classify gerunds as nouns or as verbs,
and whether to classify adjectival participles as
adjectives or as verbs.
Acknowledgments
The tagset described here was highly influenced
by many discussions with ?ukasz De?bowski, by
the insightful comments we received from Zyg-
munt Saloni, and by the various remarks from Elz?-
bieta Hajnicz, Monika Korczakowska and Beata
Wierzcho?owska. The research reported here was
partly supported by the KBN (State Committee for
Scientific Research) grant 7 T11C 043 20.
References
Janusz S. Bien? and Zygmunt Saloni. 1982. Poje?-
cie wyrazu morfologicznego i jego zastosowanie do
opisu fleksji polskiej (wersja wste?pna). Prace Filo-
logiczne, XXXI:31?45.
Janusz S. Bien?. 1991. Koncepcja s?ownikowej infor-
macji morfologicznej i jej komputerowej weryfikacji,
volume 383 of Rozprawy Uniwersytetu Warsza-
wskiego. Wydawnictwa Uniwersytetu Warsza-
wskiego, Warsaw.
?ukasz De?bowski. 2003. Reconfigurable stochas-
tic tagger for languages with complex tag structure.
EACL 2003, Morphological Processing of Slavic
Languages.
Toma? Erjavec, editor. 2001. Specifications and Nota-
tion for MULTEXT-East Lexicon Encoding. Ljubl-
jana.
W?odzimierz Gruszczyn?ski and Zygmunt Saloni.
1978. Sk?adnia grup liczebnikowych we wsp??czes-
nym je?zyku polskim. Studia Gramatyczne, II:17?
42.
Witold Man?czak. 1956. Ile jest rodzaj?w w polskim?
Je?zyk Polski, XXXVI(2):116?121.
Adam Przepi?rkowski and Marcin Wolin?ski. 2003.
The unbearable lightness of tagging: A case study
in morphosyntactic tagging of Polish. EACL 2003,
4th International Workshop on Linguistically Inter-
preted Corpora (LINC-03).
Adam Przepi?rkowski, Anna Kups?c?, Ma?gorzata
Marciniak, and Agnieszka Mykowiecka. 2002. For-
malny opis je?zyka polskiego: Teoria i implemen-
tacja. Akademicka Oficyna Wydawnicza EXIT,
Warsaw.
Jadwiga Puzynina. 1969. Nazwy czynnos?ci we
wsp??czesnym je?zyku polskim. Wydawnictwo
Naukowe PWN, Warsaw.
Zygmunt Saloni. 1974. Klasyfikacja gramatyczna lek-
sem?w polskich. Je?zyk Polski, LIV(1):3?13.
Zygmunt Saloni. 1976. Kategoria rodzaju we
wsp??czesnym je?zyku polskim. In Kategorie gra-
matyczne grup imiennych we wsp??czesnym je?zyku
polskim, pages 41?75. Ossolineum, Wroc?aw.
Zygmunt Saloni. 1977. Kategorie gramatyczne liczeb-
nik?w we wsp??czesnym je?zyku polskim. Studia
Gramatyczne, I:145?173.
Marcin Wolin?ski and Adam Przepi?rkowski. 2001.
Projekt anotacji morfosynktaktycznej korpusu
je?zyka polskiego. IPI PAN Research Report 938,
Institute of Computer Science, Polish Academy of
Sciences.
Marcin Wolin?ski. 2001. Rodzaj?w w polszczyz?nie jest
osiem. In W?odzimierz Gruszczyn?ski, Urszula An-
drejewicz, Miros?aw Ban?ko, and Dorota Kopcin?ska,
editors, Nie bez znaczenia... Prace ofiarowane
Profesorowi Zygmuntowi Saloniemu z okazji ju-
bileuszu 15000 dni pracy naukowej, pages 303?305.
Wydawnictwo Uniwersytetu Bia?ostockiego, Bia?ys-
tok.
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 1?10,
Prague, June 2007. c?2007 Association for Computational Linguistics
Slavonic Information Extraction and Partial Parsing
Adam Przepi?rkowski
Insitute of Computer Science
Polish Academy of Sciences
Ordona 21, 01-237 Warsaw, Poland
adamp@ipipan.waw.pl
Abstract
Information Extraction (IE) often involves
some amount of partial syntactic processing.
This is clear in cases of interesting high-
level IE tasks, such as finding information
about who did what to whom (when, where,
how and why), but it is also true in case
of simpler IE tasks, such as finding com-
pany names in texts. The aim of this paper is
to give an overview of Slavonic phenomena
which pose particular problems for IE and
partial parsing, and some phenomena which
seem easier to treat in Slavonic than in Ger-
manic or Romance; I also mention various
tools which have been used for the partial
processing of Slavonic.
1 Introduction
The aim of this paper is to give a general but rather
biased overview of the problems of Information Ex-
traction (IE) in Slavonic. In particular, I discuss
linguistic phenomena which make IE in Slavonic
harder than in Germanic or Romance, ?2, but also
those which seem to make it easier, ?3. We will also
look at various general tools which have been used
in IE tasks in the context of Slavonic languages, es-
pecially at tools for partial (or shallow) parsing, ?4.
I deal mainly with Polish, as a good representa-
tive of the Slavonic family: although Polish is a rela-
tively large language, with about 44 million native
speakers world-wide (over 38 million in Poland),
the availability of linguistic resources and tools for
this language does not reflect this fact: it compares
unfavourably with Czech, and probably favourably
with, say, Ukrainian.
2 Slavonic is Hard
There are various characteristics of Slavonic lan-
guages1 that make them more difficult for automatic
processing, whether shallow or deep, than Germanic
and Romance languages.2 The two of them which
are most conspicuous, and identified as most prob-
lematic, e.g., in (Collins et al, 1999), are rich nomi-
nal inflection (?2.1) and free word order (?2.6). Oth-
ers, causing problems to varying extents, include:
idiosyncratic inflection of Slavonic proper names
(?2.2); unstable inflection of some foreign names
(?2.3); high degree of trans- and, especially, intra-
paradigmatic syncretisms (?2.4); and, on the more
syntactic level, the infamous quirkiness of numeral
phrases (NumPs; ?2.5).
2.1 Rich Nominal Inflection
The rich nominal inflection of Slavonic makes al-
ready the most basic IE task, namely Named En-
tity Recognition (NER), more difficult than in Ger-
manic or Romance. Slavonic nouns, apart from in-
1Many of the typological features discussed below dis-
tinguish between, on the one hand, East Slavonic (Russian,
Ukrainian, Belorussian, Rusyn), West Slavonic (Czech, Slovak,
Upper and Lower Sorbian, Polish, Kashubian) and the Western
subgroup of South Slavonic (Croatian, Bosnian, Serbian, Slove-
nian), and, on the other hand, the Eastern subgroup of South
Slavonic (Bulgarian and Macedonian). In this and the next sec-
tion I concentrate on the former group of Slavonic languages.
2By shallow or, equivalently, partial processing, I mean the
task of finding some syntactic structure without using lexical
resources such as valence dictionaries; by contrast, deep pro-
cessing involves finding the complete sentence structures with
the use of such lexical resources.
1
flecting for number (singular and plural; in Slove-
nian and Sorbian also dual), famously inflect for
about 6 (e.g., Russian, Slovenian) or 7 (e.g., Czech,
Croatian, Polish, Ukrainian) cases: the exact number
of cases cited in the literature for any particular lan-
guage often depends on the granularity of descrip-
tion, so Belorussian and Slovak have either 6 or 7
cases, depending on the inclusion in the description
the rare vocative forms, among the 7 Serbian cases,
dative and locative are sometimes conflated because
they ?only? differ in accent, the Polish case system
may be extended to 8 cases by postulating the dis-
tributive case (Gruszczyn?ski, 1989, p. 89), while the
number of Russian cases may also be reasonably in-
creased to 8 by adding a second genitive and a sec-
ond locative case (Jakobson, 1958).
While for many European languages a dictionary
of lemmata of proper names is sufficient for the task
of NER, (Steinberger and Pouliquen, 2007, ?3.3)
note that ?a minimum of morphological treatment?
is required for languages with rich nominal inflec-
tion, such as Balto-Slavonic or Finno-Ugric lan-
guages. Unfortunately, for the majority of Slavonic
languages, there are no (freely) publicly available re-
sources that could provide such ?minimum morpho-
logical treatment? of proper names. For example, the
only large free (but not open source) morphologi-
cal analyser for Polish, Morfeusz (Wolin?ski, 2006),
contains very few proper names.3 Moreover, the NE
content of commercial analysers is often rather low,
so that simple resource-light heuristics sometimes
give better results (Urban?ska and Mykowiecka,
2005, p. 214). Such heuristics usually involve the
creation of inflected forms by adding typical suffixes
(Popov et al, 2004; Urban?ska and Mykowiecka,
2005; Steinberger and Pouliquen, 2007), where the
suffix addition/substitution rules are either hand-
generated (Urban?ska and Mykowiecka, 2005) or
automatically acquired (Steinberger and Pouliquen,
2007).
3A new version of Morfeusz, containing a large dictionary
of proper names, is being prepared, but it is currently not clear
if it is going to be freely available for non-commercial research
purposes (M. Wolin?ski, p.c.).
2.2 Different Inflection of Homonymous
Common and Proper Nouns
As mentioned in (Piskorski, 2005) and discussed
at length in (Piskorski et al, 2007b), many Pol-
ish surnames have the same base forms as com-
mon names, for example, GRZYB (lit. ?mushroom?),
GO?A?B (lit. ?pigeon?) or KOWALSKI (lit. an adjec-
tive from ?smith?). This is a problem in itself in
recognising proper names, but it is further exacer-
bated by the fact that such proper nouns may have
different gender values, and different inflectional
paradigms, than the corresponding common nouns.
For example, while the common nouns GRZYB and
GO?A?B are, respectively, inanimate masculine and
animate masculine (cf. fn. 5), the corresponding sur-
names are virile or feminine, depending on the de-
notation; in case of singular feminine names, they
would not overtly inflect at all, while in case of sin-
gular masculine or plural uses, the forms are often
different than corresponding common forms, e.g.,
the accusative singular and plural forms of GO?A?B
would be go?e?bia and go?e?bie, when used as a com-
mon noun, and Go?a?ba and Go?a?b?w, when used as
a surname, etc. Obviously, once properly described,
such inflectional differences may actually help in
NER.
2.3 Difficult Inflection of Foreign Names
A problem relatively minor in comparison to other
problems discussed here is the inflection of foreign
names: although it is governed by strict prescrip-
tive rules, native speakers are often unaware of them
and different variants of the same form may be en-
countered in text; for example, while in Polish the
correct spelling of the singular instrumental form of
LINUX is Linuksem, the variant Linuxem is at least
as common, and the starkly wrong Linux?em and
Linux-em are also quite frequent. Similarly, proba-
bly few Poles realise that the correct locative forms
of BRANDT and PEIRCE are Brandcie and Peirsie,
and not, say, Brandtcie and Peirce?ie, and that al-
though the locative of REMARQUE is Remarque?u,
the instrumental is Remarkiem.4 A comprehensive
NER should be able to deal with various incorrect
forms of foreign NE occurring in Slavonic texts.
On the other hand, the inflection of proper names
4See http://so.pwn.pl/.
2
depends on their pronunciation, i.e., on their ori-
gin. For example, the genitive of CHARLES is ei-
ther Charlesa or Charles?a, depending on whether
it is an English name or a French name. Another
example, from (Piskorski et al, 2007b), is WILDE,
whose genitive form is either Wilde?a (English) or
Wildego (German). This feature, when properly en-
coded, may actually help distinguish between enti-
ties in NER.
2.4 Tagset Size and Syncretisms
A rich inflection system also implies that the size
of the tagset is very large. For example, given that
a Polish nominal form may have one of 2 numbers,
one of 7 cases and one of 5 genders,5 there are 70
possible nominal tags, not counting gerundial and
pronominal forms. In fact, there are 4179 possible
tags in the IPI PAN Tagset of Polish (Przepi?rkowski
and Wolin?ski, 2003a; Przepi?rkowski and Wolin?ski,
2003b), of which around 1150 occur in nature
(Przepi?rkowski, 2006b). Similarly, sizes of Czech
tagsets range from 1171 (Hajic? and Hladk?, 1997),
through 1631 (Pala et al, 1998), to theoreti-
cally 4257, but ?only? about 1100 actually used
(Mirovsk? et al, 2002). Such detailed tagsets make
it difficult to reach high accuracy, which ? on the
assumption that syntactic parsing is preceded by full
morphosyntactic disambiguation ? has negative in-
fluence on syntactic processing.
Another problem connected to the rich inflection
system of Slavonic languages is the large number
of syncretisms. For example, a typical Polish adjec-
tive may have 11 textually different forms (e.g., for
BIA?Y ?white?: biali, bia?a, bia?a?, bia?e, bia?ego, bi-
a?ej, bia?emu, bia?y, bia?ych, bia?ym, bia?ymi), but as
many as 70 different tags (2 numbers ? 7 cases ? 5
genders). There are also various systematic nominal
syncretisms which to some extent annul the advan-
tages that rich case system presents for the identi-
fication of grammatical roles. For example, in plu-
5Traditionally, 3 genders were assumed for Polish, as for
many other European languages, but (Man?czak, 1956) conclu-
sively shows that at least 5 gender values must be adopted
in Polish: virile (called also m1, personal masculine and hu-
man masculine), animate masculine (m2), inanimate mascu-
line (m3), neuter and feminine. Although this repertoire of gen-
ders was only recently adopted in general dictionaries (Ban?ko,
2000), it is still rather conservative; e.g., (Saloni, 1976) pro-
poses 9 genders.
ral, Polish non-virile (non-human-masculine) nouns
have the same form in the nominative and in the
accusative, while in the singular, inanimate mascu-
line and neuter forms do. Similarly, virile and an-
imate masculine nouns have the same singular ac-
cusative and singular genitive forms. So, for exam-
ple, in the rather artificial sentence Samochody dwie
minuty wyprzedzaja? autobusy ?(The) cars (for) two
minutes are overtaking (the) buses?, each of samo-
chody, dwie minuty and autobusy may be interpreted
as either nominative or accusative, i.e., as the subject
(nominative), the object (accusative) or a temporal
adjunct (accusative).
2.5 Numeral Phrases
An area of Slavonic syntax very well-known in
theoretical linguistics is the syntactic behaviour of
NumPs (Corbett, 1978; Franks, 1995); numerals
also turn out to be awkward for automatic process-
ing in various ways.6
First, the case of the noun (phrase) within an
NumP depends on the numeral7 and on the position
of the whole NumP in the sentence. For example,
for NumPs in the subject position, the noun is in the
nominative case, roughly, if the numeral is or ends in
2, 3 or 4 (with the exception of 12, 13 and 14), and it
is genitive otherwise.8 This means that the shallow
processor should recognise as a possible currency
quantity the sequence 152 dolary and 155 dolar?w,
but not *152 dolar?w or *155 dolary.9
Second, in case of ?typical? numerals (not ending
in 2, 3 or 4), the Polish NumP in subject position
does not agree with the verb; instead, the verb oc-
curs in the default 3rd person singular neuter form,10
6One of the largest formal grammars of Polish, (S?widz-
in?ski, 1992), implemented as a wide coverage deep parser in
(Wolin?ski, 2004), does not deal with NumPs at all. Later mod-
ifications of the parser in (Ogrodniczuk, 2006) include some
limited treatment of numerals.
7This property turned out to be problematic for adapting the
GF Parallel Resource Grammar to Russian (Khegai, 2006).
8Another exception is JEDEN ?1?, which is actually an adjec-
tive, rather than a numeral (Przepi?rkowski, 2006a). Also, the
description above holds for non-virile genders, but is even more
complicated for virile.
9The latter may occur in contexts like: . . . wed?ug paragrafu
155 dolary nie sa? s?rodkiem p?atniczym w Polsce ?. . . according
to paragraph 155 dollars are not a valid currency in Poland?.
10I argue elsewhere (Przepi?rkowski, 1996; Przepi?rkowski,
2004b) that such NumPs in subject position actually bear the
accusative case; hence, the lack of agreement.
3
which may make discovering the subject-verb rela-
tion more difficult.
Finally, and rather marginally, ?typical? NumPs
in copular constructions trigger very atypical agree-
ment with the predicative adjective, e.g.: 40
g?os?w by?o niewaz?nych/niewaz?ne ?40 votes be-
3RD.SG.NEUT invalid-PL.GEN/ACC?. It is easy to
overlook such constructions when developing a shal-
low grammar, and ? since they are rare ? it is dif-
ficult to learn them automatically from corpora.
2.6 Free Word Order
Last but certainly not least, the relatively free word
order11 makes the discovering of who did what to
whom (when, where, how and why) much more
difficult than finding the relative order of NPs and
PPs in the sentence. It may seem that the rich case
system may help here, as ? with active forms of
verbs ? subjects are usually nominative and ob-
jects are often accusative, but matters are much more
complicated because of the widespread syncretisms
mentioned in ?2.4, esp. the systematic nominative-
accusative and accusative-genitive syncretisms, and
because both complements and adjuncts may be ex-
pressed by the same cases (e.g., accusative temporal
adjuncts may look like objects of transitive verbs).
While the relatively free word order is seriously
felt in deep parsers and leads to the multiplication of
analyses, to the best of our knowledge most IE work
in Slavonic to date has concentrated on lower-level
tasks such as NER and, hence, has not yet tried to
systematically deal with this problem.
3 Slavonic is Easy
On a more positive note, the rich Slavonic inflec-
tional system may help at the higher levels of pro-
cessing. There are various linguistic phenomena
where overt case, gender and number agreement
allows to differentiate between interpretations and,
hence, to extract the information about who did what
to whom. To give two trivial constructed examples:
the English sentence I saw him drunk is ambigu-
ous in ways that are necessarily disambiguated by
11Of course, the term free word order as applied to Slavonic
means that the word order is conditioned largely by information
structure (i.e., not really free); modelling the constraints of in-
formation structure on word order is particularly important in
text generation (Kruijff-Korbayov? and Kruijff, 1999).
the two Polish translations of that sentence: Widzi-
a?em go pijany ?(I) saw him drunk-NOM? and Widzi-
a?em go pijanego ?(I) saw him drunk-ACC?. Perhaps
more interestingly, the lexical aspect of Slavonic
verbs may make conspicuous the meanings which
are only implicit in other languages, as in the Polish
Skoczy? na st?? ?(He) jumped-PERF on (the) table-
ACC? versus Skaka? na stole ?(He) jumped-IMPERF
on (the) table-LOC?, both translated into the English
He jumped on the table.
One phenomenon important for high level IE
where the rich inflectional system plays a positive
role, however, is coordination.
Coordination is infamous both in theoretical lin-
guistics and in Natural Language Processing (NLP);
in fact, while recent years witnessed an increase of
theoretical linguistic works on various aspects of co-
ordination, it seems that NLP lags behind in address-
ing this phenomenon head on. One of the exceptions
is (Dale and Mazur, 2007), which deals with the
problem of identifying the number of Named En-
tities (NEs) in expressions of the form ?X and Y?,
where X and Y are sequences of capitalised words,
e.g.: ?Victorian Casino and Gaming Authority? (sin-
gle entity) or ?American Express and Visa Interna-
tional? (two entities). (Dale and Mazur, 2007) note
that the problem is statistically non-negligible, as
around 5.7% of sequences of capitalised words with
an optional conjunction (i.e., candidates for NEs)
actually contain a conjunction. Similarly, (Rus et
al., 2007, p. 229) discuss the bracketing problem in
phrases such as ?[soccer and tennis] player? and
?navy and [marine corps]?, noting that ?[p]arsing
base Noun Phrases ... is not handled by current state-
of-the-art syntactic parsers?. Another kind of coor-
dination ambiguity is considered in (Steiner, 2006),
namely, the ?NP and NP? sequence as either an
NP-coordination, or a part of sentential coordination
(where the first NP is an object of the preceding verb
and the second NP is the subject of the following
verb).
Slavonic rich inflection makes the processing of
such potentially coordinate structures easier. For ex-
ample, case disagreement between two apparently
coordinated NPs is a strong clue that they in fact
belong to separate coordinated clauses, while agree-
ment is a (perhaps weaker) clue that they form an ac-
4
tually coordinated NP.12 Similarly, (dis)agreement
in case, number and gender may help decide whether
two apparently coordinated adjectival forms actually
form a coordinate structure.
4 Slavonic is Processable
After discussing ways in which Slavonic languages
seem to be hard or easy for Information Extraction,
let us look at practical attempts at Slavonic IE, espe-
cially those involving partial parsing.
It seems that there have been relatively few at-
tempts at applying shallow (or partial; cf. fn. 2)
grammars to particular practical tasks. In some
of these attempts no particular dedicated language
processing system was used to implement shallow
grammars: apparently they were coded directly in
the host programming language.
One example is (Sharoff, 2004), where shallow
parsing is used for the identification of preposi-
tional Multi Word Expressions in Russian, with
the following explanation of reasons for performing
some language-dependent processing: ?Given that
the word order in Russian (and other Slavonic lan-
guages) is relatively free and a typical word (i.e.
lemma) has many forms (typically from 9 for nouns
to 50 for verbs), the sequences of exact N-grams are
much less frequent than in English, thus rendering
purely statistical approaches useless.?
For Polish, simple shallow grammars were im-
plemented for the tasks of question answering
(Piechocin?ski and Mykowiecka, 2005) and auto-
matic valence acquisition (Fast and Przepi?rkowski,
2005; Przepi?rkowski and Fast, 2005); in the lat-
ter case a grammar was implemented as a cas-
cade of Perl regular expressions. Similarly, (Zeman,
2001) describes a Perl regular expression implemen-
tation of a shallow preprocessor for a deep statistical
parser. Much earlier, (Nenadic? and Vitas, 1998; Ne-
nadic?, 2000) developed shallow grammars of Serbo-
Croatian for the recognition of noun phrases (NPs)
and certain kinds of coordinate structures. See also
(Bekavac and Tadic?, 2007) on the recognition of
Croatian NEs with regular grammars.
Moreover, for Bulgarian a more general integrated
system was developed, called LINGUA (Tanev and
Mitkov, 2002), which ? apart from modules for
12Again, this test may fail due to case syncretisms; cf. ?2.4.
tokenisation, morphosyntactic analysis and disam-
biguation, and anaphora resolution ? includes an
NP extractor and a bottom-up grammar of Bul-
garian. This system, together with a set of shal-
low patterns for identifying definition patterns, has
been employed in a Question Answering prototype
system (Tanev, 2004). Bulgarian pattern-matching
grammars are also employed in (Koeva, 2007).
Apart from these language-specific implementa-
tions, there exist tools and toolboxes which facilitate
various IE tasks, including shallow parsing. Proba-
bly the best known such a general system is GATE
(Cunningham et al, 1995; Cunningham et al, 2002),
which contains some NE resources for Bulgarian
and Russian (Humphreys et al, 2002; Popov et al,
2004) and allows to write shallow (regular) gram-
mars in the JAPE subsystem (Cunningham et al,
2000).
A system similar in scope is SProUT (Becker et
al., 2002), whose shallow parsing language allows
to write regular grammars over HPSG-style (Pol-
lard and Sag, 1994) typed feature structures and
which includes the operation of unification. Prelim-
inary work on adapting SProUT to the processing
of Baltic and Slavonic languages is presented in
(Droz?dz?yn?ski et al, 2003), with much subsequent
work devoted to the processing of Polish, especially,
in the area of Information Extraction from medical
texts (Piskorski et al, 2004; Piskorski, 2004a; Pisko-
rski, 2004b; Marciniak et al, 2005; Mykowiecka et
al., 2005a; Mykowiecka et al, 2005b; Marciniak and
Mykowiecka, 2007).
Although GATE and SProUT may be adapted to
the processing of XML documents, they are perhaps
not the most natural choice for the further processing
of morphosyntactically annotated documents in, for
example, the XCES (XML Corpus Encoding Stan-
dard; (Ide et al, 2000)) format, as assumed, e.g.,
in the IPI PAN Corpus of Polish (Przepi?rkowski,
2004a), in the Slovak National Corpus (Garab?k
and Gianitsov?-Olo?tiakov?, 2005), or in the LT4eL
project (http://www.lt4el.eu/). Specialised
XML-aware tools exist for such tasks.
One of the earliest collections of XML process-
ing tools is the LT XML library (Brew et al, 2000),
whose second edition, LT-XML2 is currently under
preparation. One of the tools in that new edition,
lxtransduce (Tobin, 2005), is an efficient pro-
5
gram to add mark-up to XML files via regular gram-
mars over XML elements; this tool is currently used
for implementing definition-extraction grammars
for Bulgarian, Czech and Polish (Przepi?rkowski et
al., 2007).
A system well-known in Slavonic NLP is CLaRK
(Simov et al, 2001; Simov et al, 2002); it imple-
ments various XML mechanism and proposes a lan-
guage for developing shallow grammars over XML
documents; such grammars have been implemented
for Bulgarian, as reported in (Simov et al, 2004;
Simov and Osenova, 2004).
Finally, a new system, SPADE (Shallow Parsing
and Disambiguation Engine), abbreviated to ???
(Unicode character 0x2660), has recently been de-
veloped at the Institute of Computer Science, Pol-
ish Academy of Sciences (Przepi?rkowski, 2007b;
Buczyn?ski, 2007). This tool, unlike many other
shallow parsing tools,13 accepts a possibly mor-
phosyntactically ambiguous (XCES-encoded) input
and performs simultaneous morphosyntactic disam-
biguation and shallow parsing. For example, the rule
below, called P + co/kto, will match a possible
preposition followed by a possible form of one of
the pronouns CO ?what? or KTO ?who?,14 it will try
to unify the selected case of the preposition with
the case of the pronoun and, if that succeeds, it will
mark any non-unified interpretations as rejected and
it will mark the two words as a prepositional group
with the preposition (cf. 1 below) as its syntactic
head and the pronoun (cf. 2) as its semantic head.15
Moreover, any non-prepositional interpretations of
the first segment of the match and any non-nominal
interpretations of the second segment will be marked
as incorrect. The language for specification of seg-
ments is based on the query syntax of the Poliqarp
corpus search engine (Przepi?rkowski et al, 2004;
Janus and Przepi?rkowski, 2007), in turn based on
CQP (Christ, 1994).
RULE P + co/kto
Match: [pos~"prep"][base~"co|kto"]
13But the shallow grammars for Serbo-Croatian described in
(Nenadic? and Vitas, 1998; Nenadic?, 2000) were developed with
similar goals in mind.
14Left and right context of a match may be specified; here
they are empty.
15A rationale for distinguishing these two kinds of heads is
given in (Przepi?rkowski, 2007a).
Cond: unify(case,1,2)
Synt: group(PrepNG,1,2)
Morph: leave(pos~~"prep",1)
Morph: leave(pos~~"subst",2)
SPADE is currently employed for the shallow pro-
cessing of the IPI PAN Corpus of Polish.
5 Conclusion
While the relatively free word order of Slavonic lan-
guages makes the processing of Slavonic unambigu-
ously harder, I claim that the effects of the rich nom-
inal inflection are mixed: rich inflection dramatically
increases the complexity of low-level IE tasks such
as NER, but it is beneficial for high-level IE tasks
which involve filling scenario templates, as it facil-
itates identifying grammatical roles, parsing coordi-
nation, etc. Moreover, as becomes clear on the basis
of the overview of practical work on Slavonic IE in
the last decade, recent years have witnessed substan-
tially increased interest and activity in the area. I am
convinced that the Balto-Slavonic Natural Language
Processing workshop at ACL 2007 will further catal-
yse the development of this field.
Acknowledgments
I am grateful to Jakub Piskorski, Bruno Pouliquen,
Ralf Steinberger and Hristo Tanev for their generous
invitation to the BSNLP workshop, and to the Euro-
pean Commission?s Joint Research Centre in Ispra,
Italy, for financial support. This paper was written
during my stay at the University of Regensburg, Ger-
many, within the DAAD-MNiSW cooperation pro-
gramme Automatic methods of extracting linguistic
knowledge from corpora (2006?2007). Many thanks
to Qba Piskorski and Roland Meyer for comments
on the (as it turned out, pre-) final versions of the
paper.
References
Miros?aw Ban?ko, editor. 2000. Inny s?ownik je?zyka pol-
skiego. Wydawnictwo Naukowe PWN, Warsaw.
Markus Becker, Witold Droz?dz?yn?ski, Hans-Ulrich
Krieger, Jakub Piskorski, Ulrich Sch?fer, and Feiyu
Xu. 2002. SProUT ? shallow processing with typed
feature structures and unification. In Proceedings of
the International Conference on NLP (ICON 2002),
Mumbai, India.
6
Bo?o Bekavac and Marko Tadic?. 2007. Implementation
of Croatian NERC system. In Piskorski et al (Pisko-
rski et al, 2007a).
Leonard Bolc, Zbigniew Michalewicz, and Toyoaki
Nishida, editors. 2005. Intelligent Media Technology
for Communicative Intelligence, Second International
Workshop, IMTCI 2004, Warsaw, Poland, September
13-14, 2004, Revised Selected Papers, volume 3490 of
Lecture Notes in Computer Science. Springer-Verlag.
Chris Brew, David McKelvie, Richard Tobin, Henry
Thompson, and Andrei Mikheev, 2000. The XML Li-
brary LT XML version 1.2: User documentation and
reference guide. Language Technology Group, Uni-
versity of Edinburgh. http://www.ltg.ed.ac.
uk/software/xml/xmldoc/xmldoc.html.
Aleksander Buczyn?ski. 2007. An implementation of
combined partial parser and morphosyntactic disam-
biguator. In Proceedings of ACL 2007 Student Re-
search Workshop.
Oli Christ. 1994. A modular and flexible architecture for
an integrated corpus query system. In COMPLEX?94,
Budapest.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of ACL 1999, pages 505?518,
University of Maryland.
Greville G. Corbett. 1978. Numerous squishes and
squishy numerals in Slavonic. In Bernard Comrie, ed-
itor, Classification of Grammatical Categories, pages
43?73. Linguistic Research, Inc., Edmonton.
Hamish Cunningham, Robert Gaizauskas, and Yorick
Wilks. 1995. A general architecture for text en-
gineering (GATE) ? a new approach to language
engineering R&D. Technical report, Department of
Computer Science, University of Sheffield. http:
//xxx.lanl.gov/abs/cs.CL/9601009.
Hamish Cunningham, Diana Maynard, and Valentin
Tablan. 2000. JAPE: a Java Annotation Patterns
Engine (second edition). Technical Report CS?00?
10, Department of Computer Science, University of
Sheffield.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Robert Dale and Pawe? Mazur. 2007. Handling conjunc-
tions in named entities. In Gelbukh (Gelbukh, 2007),
pages 131?142.
Witold Droz?dz?yn?ski, Petr Homola, Jakub Piskorski, and
Vytautas Zinkevic?ius. 2003. Adapting SProUT
to processing Baltic and Slavonic languages. In
Hamish Cunningham, E. Paskaleva, Kalina Bontcheva,
and G. Angelova, editors, Information Extraction for
Slavonic and Other Central and Eastern European
Languages, pages 18?25, Borovets, Bulgaria.
ELRA. 2004. Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
LREC2004, Lisbon.
Jakub Fast and Adam Przepi?rkowski. 2005. Automatic
extraction of Polish verb subcategorization: An eval-
uation of common statistics. In Vetulani (Vetulani,
2005), pages 191?195.
Steven Franks. 1995. Parameters of Slavic Morphosyn-
tax. Oxford University Press, New York.
Radovan Garab?k, editor. 2005. Computer Treatment of
Slavic and East European Languages: Proceedings of
the Third International Seminar, Bratislava, Slovakia,
10?12 November 2005, Bratislava. VEDA: Vydava-
tel?stvo Slovenskej akad?me vied.
Radovan Garab?k and Lucia Gianitsov?-Olo?tiakov?.
2005. Manual morphological annotation of Slovak
translation of Orwell?s novel 1984 ? methods and
findings. In Garab?k (Garab?k, 2005), pages 59?66.
Alexander Gelbukh, editor. 2007. Computational
Linguistics and Intelligent Text Processing (CICLing
2007), Lecture Notes in Computer Science, Berlin.
Springer-Verlag.
W?odzimierz Gruszczyn?ski. 1989. Fleksja Rzec-
zownik?w Pospolitych we Wsp??czesnej Polszczyz?nie
Pisanej (na materiale ?S?ownika je?zyka polskiego?
PAN pod redakcja? W. Doroszewskiego), volume 122 of
Prace Je?zykoznawcze. Ossolineum, Wroc?aw.
Jan Hajic? and Barbara Hladk?. 1997. Probabilistic and
rule-based tagger of an inflective language - a compar-
ison. In Proceedings of the ANLP?97, pages 111?118,
Washington, DC.
K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks. 2002.
Slavonic named entities in GATE. Research Memo-
randum CS-02-01, University of Sheffield.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based standard for linguistic
corpora. In Proceedings of the Linguistic Resources
and Evaluation Conference, pages 825?830, Athens,
Greece.
Roman O. Jakobson. 1958. Morfologic?eskie nablju-
denija nad slavjanskim skloneniem. In Selected Writ-
ings II, pages 154?183. Mouton, The Hague.
7
Daniel Janus and Adam Przepi?rkowski. 2007. Poliqarp:
An open source corpus indexer and search engine with
syntactic extensions. In Proceedings of ACL 2007
Demo Session.
Janna Khegai. 2006. GF parallel resource grammars
and Russian. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 475?
482, Sydney, Australia. Association for Computational
Linguistics.
Mieczys?aw A. K?opotek, S?awomir T. Wierzchon?, and
Krzysztof Trojanowski, editors. 2005. Intelligent In-
formation Processing and Web Mining. Advances in
Soft Computing. Springer-Verlag, Berlin.
Svetla Koeva. 2007. Multi-word term extraction for Bul-
garian. In Piskorski et al (Piskorski et al, 2007a).
Ivana Kruijff-Korbayov? and Geert-JanM. Kruijff. 1999.
Handling word order in a multilingual system for gen-
eration of instructions. In V?clav Matousek, Pavel
Mautner, Jana Ocel?kov?, and Petr Sojka, editors,
Text, Speech and Dialogue - Second International
Workshop, TSD?99, Plzen, Czech Republic, September
1999, pages 83?88, Berlin. Springer-Verlag.
Witold Man?czak. 1956. Ile jest rodzaj?w w polskim?
Je?zyk Polski, XXXVI(2):116?121.
Ma?gorzata Marciniak and Agnieszka Mykowiecka.
2007. Automatic processing of diabetic patients? hos-
pital documentation. In Piskorski et al (Piskorski et
al., 2007a).
Ma?gorzata Marciniak, Agnieszka Mykowiecka, Anna
Kups?c?, and Jakub Piskorski. 2005. Intelligent content
extraction from Polish medical texts. In Bolc et al
(Bolc et al, 2005), pages 68?78.
Jir?? Mirovsk?, Roman Ondru?ka, and Daniel Pru??a.
2002. Searching through Prague Dependency Tree-
bank: Conception and architecture. In Proceedings of
the First Workshop on Treebanks and Linguistic Theo-
ries (TLT2002), pages 114?122, Sozopol, Bulgaria.
Agnieszka Mykowiecka, Anna Kups?c?, and Ma?gorzata
Marciniak. 2005a. Rule-based medical content
extraction and classification. In K?opotek et al
(K?opotek et al, 2005), pages 237?246.
Agnieszka Mykowiecka, Ma?gorzata Marciniak, and
Anna Kups?c?. 2005b. Making shallow look deeper:
Anaphora and comparisons in medical information ex-
traction. In Vetulani (Vetulani, 2005).
Goran Nenadic?. 2000. Local grammars and parsing co-
ordination of nouns in Serbo-Croatian. In Proceedings
of Text, Dialogue and Speech (TSD) 2000, pages 57?
62. Springer-Verlag.
Goran Nenadic? and Du?ko Vitas. 1998. Using local
grammars for agreement modeling in highly inflec-
tive languages. In Proceedings of Text, Dialogue and
Speech (TSD) 1998, pages 91?96.
Maciej Ogrodniczuk. 2006. Weryfikacja korpusu
wypowiednik?w polskich (z wykorzystaniem gramatyki
formalnej S?widzin?skiego). Ph. D. dissertation, Warsaw
University, Warsaw.
Karel Pala, Pavel Rychl?, and Pavel Smr?. 1998. Corpus
annotation in inflectional languages: Czech. In A Min
Tjoa and Roland R. Wagner, editors, Ninth Interna-
tional Workshop on Database and Expert Systems Ap-
plications, pages 149?153, Los Alamitos, California.
Dariusz Piechocin?ski and Agnieszka Mykowiecka. 2005.
Question answering in Polish using shallow parsing.
In Garab?k (Garab?k, 2005), pages 167?173.
Jakub Piskorski. 2004a. Extraction of Polish named-
entities. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
LREC2004 (ELR, 2004), pages 313?316.
Jakub Piskorski. 2004b. Rule-based named-entity recog-
nition for Polish. In Proceedings of the Workshop on
Named-Entity Recognition for NLP Applications held
in conjunction with the 1st International Joint Con-
ference on NLP, March 2004, Sanya, Hainan Island,
China.
Jakub Piskorski. 2005. Named-entity recognition for
Polish with SProUT. In Bolc et al (Bolc et al, 2005).
Jakub Piskorski, Peter Homola, Ma?gorzata Marciniak,
Agnieszka Mykowiecka, Adam Przepi?rkowski, and
Marcin Wolin?ski. 2004. Information extraction for
Polish using the SProUT platform. In Mieczys?aw A.
K?opotek, S?awomir T. Wierzchon?, and Krzysztof Tro-
janowski, editors, Intelligent Information Processing
and Web Mining, Advances in Soft Computing, pages
227?236. Springer-Verlag, Berlin.
Jakub Piskorski, Bruno Pouliquen, Ralf Steinberger, and
Hristo Tanev, editors. 2007a. Proceedings of the
BSNLP workshop at ACL 2007, Prague.
Jakub Piskorski, Marcin Sydow, and Anna Kups?c?. 2007b.
Lemmatization of Polish person names. In Piskorski
et al (Piskorski et al, 2007a).
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase
Structure Grammar. Chicago University Press / CSLI
Publications, Chicago, IL.
Borislav Popov, Angel Kirilov, DianaMaynard, and Dim-
itar Manov. 2004. Creation of reusable components
and language resources for Named Entity Recognition
in Russian. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
LREC2004 (ELR, 2004), pages 309?312.
8
Adam Przepi?rkowski. 1996. Case assignment in Pol-
ish: Towards an HPSG analysis. In Claire Grover and
Enric Vallduv?, editors, Studies in HPSG, volume 12
of Edinburgh Working Papers in Cognitive Science,
pages 191?228. Centre for Cognitive Science, Univer-
sity of Edinburgh.
Adam Przepi?rkowski. 2004a. The IPI PAN Corpus:
Preliminary version. Institute of Computer Science,
Polish Academy of Sciences, Warsaw.
Adam Przepi?rkowski. 2004b. O wartos?ci przypadka
podmiot?w liczebnikowych. Biuletyn Polskiego To-
warzystwa Je?zykoznawczego, LX:133?143.
Adam Przepi?rkowski. 2006a. O dystrybutywnym
PO i liczebnikach jedynkowych. Polonica, XXVI?
XXVII:171?178.
Adam Przepi?rkowski. 2006b. The potential of the IPI
PAN Corpus. Poznan? Studies in Contemporary Lin-
guistics, 41:31?48.
Adam Przepi?rkowski. 2007a. On heads and coordi-
nation in valence acquisition. In Gelbukh (Gelbukh,
2007), pages 50?61.
Adam Przepi?rkowski. 2007b. A preliminary formal-
ism for simultaneous rule-based tagging and partial
parsing. In Georg Rehm, Andreas Witt, and Lothar
Lemnitzer, editors, Data Structures for Linguistic Re-
sources and Applications: Proceedings of the Biennial
GLDV Conference 2007, pages 81?90. Gunter Narr
Verlag, T?bingen.
Adam Przepi?rkowski and Jakub Fast. 2005. Baseline
experiments in the extraction of Polish valence frames.
In K?opotek et al (K?opotek et al, 2005), pages 511?
520.
Adam Przepi?rkowski and Marcin Wolin?ski. 2003a.
A flexemic tagset for Polish. In Proceedings
of Morphological Processing of Slavic Languages,
EACL 2003, pages 33?40, Budapest.
Adam Przepi?rkowski and Marcin Wolin?ski. 2003b. The
unbearable lightness of tagging: A case study in mor-
phosyntactic tagging of Polish. In Proceedings of
the 4th International Workshop on Linguistically In-
terpreted Corpora (LINC-03), EACL 2003, pages 109?
116.
Adam Przepi?rkowski, Zygmunt Krynicki, ?ukasz
De?bowski, Marcin Wolin?ski, Daniel Janus, and Pi-
otr Ban?ski. 2004. A search tool for corpora with
positional tagsets and ambiguities. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, LREC2004 (ELR, 2004),
pages 1235?1238.
Adam Przepi?rkowski, ?ukasz Deg?rski, Miroslav
Spousta, Kiril Simov, Petya Osenova, Lothar Lem-
nitzer, Vladislav Kubon?, and Beata W?jtowicz. 2007.
Towards the automatic extraction of definitions in
Slavic. In Piskorski et al (Piskorski et al, 2007a).
Vasile Rus, Sireesha Ravi, Mihai C. Lintean, and
Philip M. McCarthy. 2007. Unsupervised method for
parsing coordinated base noun phrases. In Gelbukh
(Gelbukh, 2007), pages 229?240.
Zygmunt Saloni. 1976. Kategoria rodzaju we wsp??czes-
nym je?zyku polskim. In Roman Laskowski, editor,
Kategorie gramatyczne grup imiennych we wsp??czes-
nym je?zyku polskim, volume 14 of Prace Instytutu
Je?zyka Polskiego, pages 43?78. Ossolineum, Wroc?aw.
Serge Sharoff. 2004. What is at stake: a case study of
Russian expressions starting with a preposition. In
Takaaki Tanaka, Aline Villavicencio, Francis Bond,
and Anna Korhonen, editors, Second ACL Workshop
on Multiword Expressions: Integrating Processing,
pages 17?23, Barcelona, Spain. Association for Com-
putational Linguistics.
Kiril Simov and Petya Osenova. 2004. A hybrid strat-
egy for regular grammar parsing. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, LREC2004 (ELR, 2004),
pages 431?434.
Kiril Simov, Z. Peev, Milen Kouylekov, Alexander
Simov, M. Dimitrov, and A. Kiryakov. 2001. CLaRK
? an XML-based system for corpora development. In
Paul Rayson, Andrew Wilson, Tony McEnery, Andrew
Hardie, and Shereen Khoja, editors, Proceedings of the
Corpus Linguistics 2001 Conference, pages 558?560,
Lancaster.
Kiril Simov, Petya Osenova, Milena Slavcheva, Sia
Kolkovska, Elisaveta Balabanova, Dimitar Doikoff,
Krassimira Ivanova, Alexander Simov, and Milen
Kouylekov. 2002. Building a linguistically interpreted
corpus of Bulgarian. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation, LREC2002, pages 1729?1736, Las Pal-
mas, Canary Islands, Spain. ELRA.
Kiril Simov, Petya Osenova, Sia Kolkovska, Elisaveta
Balabanova, and Dimitar Doikoff. 2004. A language
resources infrastructure for Bulgarian. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, LREC2004 (ELR, 2004),
pages 1685?1688.
Ralf Steinberger and Bruno Pouliquen. 2007. Cross-
lingual Named Entity Recognition. Linguisticae In-
vestigationes. Special Issue on Named Entity Recog-
nition and Categorisation, Satoshi Sekine and Elisa-
bete Ranchhod (eds.), Forthcoming.
9
Ilona Steiner. 2006. Coordinate structures: On the rela-
tionship between parsing preferences and corpus fre-
quencies. In Pre-Proceedings of the International
Conference on Linguistic Evidence: Empirical, Theo-
retical and Computational Perspectives, T?bingen, 2?
4 February 2006, pages 88?92, T?bingen. SFB 441
?Linguistic Data Structures?, University of T?bingen,
Germany.
Marek S?widzin?ski. 1992. Gramatyka formalna je?zyka
polskiego, volume 349 of Rozprawy Uniwersytetu
Warszawskiego. Wydawnictwa Uniwersytetu Warsza-
wskiego, Warsaw.
Hristo Tanev. 2004. Socrates: A question answering
prototype for Bulgarian. In Recent Advances in Nat-
ural Language Processing III, Selected Papers from
RANLP 2003, pages 377?386. John Benjamins.
Hristo Tanev and Ruslan Mitkov. 2002. Shallow
language processing architecture for Bulgarian. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING2002), Taipei.
Richard Tobin, 2005. Lxtransduce, a replace-
ment for fsgmatch. University of Edinburgh.
http://www.cogsci.ed.ac.uk/~richard/
ltxml2/lxtransduce-manual.html.
Dominika Urban?ska and Agnieszka Mykowiecka. 2005.
Multi-words Named Entity Recognition in Polish
texts. In Garab?k (Garab?k, 2005), pages 208?215.
Zygmunt Vetulani, editor. 2005. Proceedings of the 2nd
Language & Technology Conference, Poznan?, Poland.
Marcin Wolin?ski. 2004. Komputerowa weryfikacja gra-
matyki S?widzin?skiego. Ph. D. dissertation, Institute of
Computer Science, Polish Academy of Sciences, War-
saw.
Marcin Wolin?ski. 2006. Morfeusz ? a practical
tool for the morphological analysis of Polish. In
Mieczys?aw A. K?opotek, S?awomir T. Wierzchon?, and
Krzysztof Trojanowski, editors, Intelligent Informa-
tion Processing and Web Mining. Proceedings of the
International IIS: IIPWM?06 Conference held in Us-
tron, Poland, June 19-22, 2006, Advances in Soft
Computing. Springer-Verlag, Berlin.
Daniel Zeman. 2001. How much will a RE-based pre-
processor help a statistical parser? In Proceedings of
the Seventh International Workshop on Parsing Tech-
nologies (IWPT-2001), 17-19 October 2001, Beijing,
China. Tsinghua University Press.
10
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43?50,
Prague, June 2007. c?2007 Association for Computational Linguistics
Towards the Automatic Extraction of Definitions in Slavic
1Adam Przepio?rkowski
2?ukasz Dego?rski
8Beata Wo?jtowicz
Institute of Computer Science PAS
Ordona 21, Warsaw, Poland
adamp@ipipan.waw.pl
ldegorski@bach.ipipan.waw.pl
beataw@bach.ipipan.waw.pl
4Kiril Simov
5Petya Osenova
Institute for Parallel Processing BAS
Bonchev St. 25A, Sofia, Bulgaria
kivs@bultreebank.org
petya@bultreebank.org
3Miroslav Spousta
7Vladislav Kubon?
Charles University
Malostranske? na?me?st?? 25
Prague, Czech Republic
spousta@ufal.ms.mff.cuni.cz
vk@ufal.ms.mff.cuni.cz
6Lothar Lemnitzer
University of Tu?bingen
Wilhelmstr. 19, Tu?bingen, Germany
lothar@sfs.uni-tuebingen.de
Abstract
This paper presents the results of the prelim-
inary experiments in the automatic extrac-
tion of definitions (for semi-automatic glos-
sary construction) from usually unstructured
or only weakly structured e-learning texts
in Bulgarian, Czech and Polish. The ex-
traction is performed by regular grammars
over XML-encoded morphosyntactically-
annotated documents. The results are less
than satisfying and we claim that the rea-
son for that is the intrinsic difficulty of the
task, as measured by the low interannota-
tor agreement, which calls for more sophis-
ticated deeper linguistic processing, as well
as for the use of machine learning classifica-
tion techniques.
1 Introduction
The aim of this paper is to report on the preliminary
results of a subtask of the European Project Lan-
guage Technology for eLearning (http://www.
lt4el.eu/) consisting in the identification of
term definitions in eLearning materials (Learning
Objects; henceforth: LOs), where definitions are
understood pragmatically, as those text fragments
which may, after perhaps some minor editing, be
put into a glossary. Such automatically extracted
term definitions are to be presented to the author or
the maintainer of the LO and, thus, significantly fa-
cilitate and accelerate the creation of a glossary for
a given LO. From this specification of the task it fol-
lows that good recall is much more important than
good precision, as it is easier to reject wrong glos-
sary candidates than to browse the LO for term def-
initions which were not automatically spotted.
The project involves 9 European languages in-
cluding 3 Slavic (and, regrettably, no Baltic) lan-
guages: one South Slavic, i.e., Bulgarian, and two
West Slavic, i.e., Czech and Polish. For all lan-
guages, shallow grammars identifying definitions
have been constructed; after mentioning some previ-
ous work on Information Extraction (IE) for Slavic
languages and on extraction of definitions in sec-
tion 2, we briefly describe the three Slavic grammars
developed within this project in section 3. Section 4
presents the results of the application of these gram-
mars to LOs in respective languages. These results
are evaluated in section 5, where main problems, as
well as some possible solutions, are discussed. Fi-
nally, section 6 concludes the paper.
43
2 Related Work
Definition extraction is an important NLP task,
most frequently a subtask of terminology extraction
(Pearson, 1996), the automatic creation of glossaries
(Klavans and Muresan, 2000; Klavans and Muresan,
2001), question answering (Miliaraki and Androut-
sopoulos, 2004; Fahmi and Bouma, 2006), learning
lexical semantic relations (Malaise? et al, 2004; Stor-
rer and Wellinghoff, 2006) and automatic construc-
tion of ontologies (Walter and Pinkal, 2006). Tools
for definition extraction are invariably language-
specific and involve shallow or deep processing,
with most work done for English (Pearson, 1996;
Klavans and Muresan, 2000; Klavans and Muresan,
2001) and other Germanic languages (Fahmi and
Bouma, 2006; Storrer and Wellinghoff, 2006; Wal-
ter and Pinkal, 2006), as well as French (Malaise? et
al., 2004). To the best of our knowledge, no previ-
ous attempts at definition extraction have been made
for Slavic, with the exception of some work on Bul-
garian (Tanev, 2004; Simov and Osenova, 2005).
Other work on Slavic information extraction has
been carried out mainly for the last 5 years. Prob-
ably the first forum where such work was compre-
hensively presented was the International Workshop
on Information Extraction for Slavonic and Other
Central and Eastern European Languages (IESL),
RANLP, Borovets, 2003, Bulgaria. One of the pa-
pers presented there, (Droz?dz?yn?ski et al, 2003), dis-
cusses shallow SProUT (Becker et al, 2002) gram-
mars for Czech, Polish and Lithuanian. SProUT has
subsequently been extensively used for the informa-
tion extraction from Polish medical texts (Piskorski
et al, 2004; Marciniak et al, 2005).1
3 Shallow Grammars for Definition
Extraction
The input to the task of definition extraction is
XML-encoded morphosyntactically-annotated text,
possibly with some keywords already marked by an
1SProUT has not been seriously considered for the task at
hand for two reasons: first, it was decided that only open source
tools will be used in the current project, if only available, sec-
ond, the input format to the current task is morphosyntactically-
annotated XML-encoded text, rather than raw text, as normally
expected by SProUT. The second obstacle could be removed by
converting input texts to the SProUT-internal XML representa-
tion.
independent process. For example, the representa-
tion of a Polish sentence starting as Konstruktywizm
k?adzie nacisk na (Eng. ?Constructivism puts em-
phasis on?) may be as follows:2
<s id="s9">
<markedTerm id="mt7" kw="y">
<tok base="konstruktywizm" ctag="subst"
id="t253"
msd="sg:nom:m3">Konstruktywizm</tok>
</markedTerm>
<tok base="klasc" ctag="fin" id="t254"
msd="sg:ter:imperf">kladzie</tok>
<tok base="nacisk" ctag="subst" id="t255"
msd="sg:acc:m3">nacisk</tok>
<tok base="na" ctag="prep" id="t256"
msd="acc">na</tok>
[...]
<tok base="." ctag="interp" id="t273">.
</tok>
</s>
For each language, definitions were manually
marked in two batches of texts: the first batch, con-
sulted during the process of grammar development,
contained at least 300 definitions, and the second
batch, held out for evaluation, contained about 150
definitions. All grammars are regular grammars im-
plemented with the use of the lxtransduce tool
(Tobin, 2005), a component of the LTXML2 toolset
developed at the University of Edinburgh.3 An ex-
ample of a simple rule for prepositional phrases is
given below:
<rule name="PP">
<seq>
<query match="tok[@ctag = ?prep?]"/>
<ref name="NP1">
<with-param name="case" value="??"/>
</ref>
</seq>
</rule>
This rule identifies a sequence whose first element
is a token tagged as a preposition and whose subse-
quent elements are identified by a rule called NP1.
This latter rule (not shown here for brevity) is a pa-
rameterised rule which finds a nominal phrase of a
given case, but the way it is called above ensures that
it will find an NP of any case.
2Part of the representation has been replaced by ?[...]?.
3Among the tools considered here were also CLaRK (Simov
et al, 2001), ultimately rejected because it currently does not
work in batch mode, and GATE / JAPE (Cunningham et al,
2002), not used here because we found GATE?s handling of
previously XML-annotated texts rather cumbersome and ill-
documented. Cf. also fn. 1.
44
Currently the grammars show varying degrees of
sophistication, with a small Bulgarian grammar (8
rules in a 2.5-kilobyte file), a larger Polish grammar
(34 rules in a 11KiB file) and a sophisticated Czech
grammar most developed (147 rules in a 28KiB
file). The patterns defined by these three grammars
are similar, but sufficiently different to defy an at-
tempt to write a single parameterised grammar.4 The
remainder of this section briefly describes the gram-
mars.
3.1 Bulgarian
The Bulgarian grammar is manually constructed af-
ter examination of the manually annotated defini-
tions. Here is a list of the rule schemata, together
with the number and percentage of matching defini-
tions:
Pattern # %
NP is NP 140 34.2
NP verb NP 18 29.8
NP - NP 21 5.0
This is NP 15 3.7
It represents NP 4 1.0
other patterns 107 26.2
Table 1: Bulgarian definition types
In the second schema above, ?verb? is a verb or
a verb phrase (not necessarily a constituent) which
is one of the following: ?????????????? (to repre-
sent), ????????? (to show), ?????????? (to mean),
???????? (to describe), ??? ????????? (to be used),
??????????? (to allow), ????? ?????????? ???
(to give opportunity), ??? ??????? (is called),
??????????? (to improve), ??????????? (to ensure),
?????? ??? (to serve as), ??? ???????? (to be under-
stood as), ???????????? (to denote), ????????? (to
contain), ?????????? (to determine), ?????????
(to include), ??? ???????? ????? (is defined as),
??? ???????? ??? (is based on).
We classify the rules in five types: copula defi-
nitions, copula definitions with anaphoric relation,
copula definitions with ellipsis of the copula, defi-
nitions with a verb phrase, definitions with a verb
4Because of this relative language-dependence of definition
patters, which includes, e.g., idiosyncratic case information,
we have not seriously considered re-using rules for other, non-
Slavic, languages.
phrase and anaphoric relation. Each of these types of
definitions defines an NP (sometimes via anaphoric
relation) by another one. There are some variations
of the models where some parenthetical expressions
are presented in the definition.
The grammar contains several most important
rules for each type. The different verb patterns are
encoded as a lexicon. For some of the rules, variants
with parenthetical phrases are also encoded. The rest
of the grammar is devoted to the recognition of noun
phrases and parenthetical phrases. For parentheti-
cal phrases, we have encoded a list of such possible
phrases, extracted on the basis of a bigger corpus.
The NP grammar in our view is the crucial grammar
for recognition of the definitions. Most work now
has to be invested into developing the more complex
and recursive NPs.
3.2 Czech
The Czech grammar for definition context extraction
is constructed to follow both linguistic intuition and
observation of common patterns in manually anno-
tated data.
We adapted a grammar5 based mainly on the ob-
servation of Czech Wikipedia entries. Encyclopedia
definitions are usually clear and very well structured,
but it is quite difficult to find such well-formed defi-
nitions in common texts, including learning objects.
The rules were extended using part of our manually
annotated texts, evaluated and adjusted in several it-
erations, based on the observation of the annotated
data.
Pattern # %
NP is/are NP 52 21.2
NP verb NP 45 18.4
structural 39 15.9
NP (NP) 30 12.2
NP -/:/= NP 20 8.2
other patterns 59 24.1
Table 2: Czech definition types
There are 21 top level rules, divided into five cate-
gories. Most of the correctly marked definitions fall
into the copula verb (?is/are?) category. The sec-
5The grammar was originally developed by Nguyen Thu
Trang.
45
ond most successful rule is the one using selected
verbs like ?definuje? (defines), ?znamen?? (means),
?vymezuje? (delimits), ?pr?edstavuje? (presents) and
several others. The remaining categories make use
of the typical patterns of characters (dash, colon,
equal sign and brackets) or additional structural in-
formation (e.g., HTML tags).
3.3 Polish
The Polish grammar rules are divided into three lay-
ers. Similarly to the Czech grammar, each layer only
refers to itself or lower layers. This allows for ex-
pressing top level rules in a clear and easily man-
ageable way.
The top level layer consists of rules representing
typical patterns found in Polish documents:
Pattern # %
NP (...) are/is NP-INS 40 15.6
NP -/: NP 39 15.2
NP (are/is) to NP-NOM 27 10.6
NP VP-3PERS 25 9.8
NP - i.e./or WH-question 11 4.3
N ADJ - PPAS 8 3.1
NP, i.e./or NP 7 2.7
NP-ACC one may
describe/define as NP-ACC 5 2.0
other patterns
(not in the grammar) 94 36.7
Table 3: Polish definition types
The middle layer consists of rules catching pat-
terns such as ?simple NP in given case, followed by
a sequence of non-punctuation elements? or ?cop-
ula?.
The bottom layer rules basically only refer to
POS markup in the input files (or other bottom layer
rules).
4 Results
As mentioned above, the testing corpus for each lan-
guage consists of about 150 definitions, unseen dur-
ing the construction of the grammar.6
6Obviously, three different corpora had to be used to eval-
uate the grammars for the three languages, but the corpora are
similar in size and character, so any differences in results stem
mostly from the differences in the three grammars.
The Bulgarian test corpus, containing around
76,800 tokens, consists of the third part of the
Calimera guidelines (http://www.calimera.
org/). We view this document as appropriate for
testing because it reflects the chosen domain and it
combines definitions from otherwise different sub-
domains, such as XML language, Internet usage,
etc. There are 203 manually annotated definitions
in this corpus: 129 definitions contained in one sen-
tence, 69 definitions split across 2 sentences, 4 def-
initions in 3 sentences and one definition in 4 sen-
tences. Note that the real test part is the set of the
129 definitions in one sentence, since the Bulgar-
ian grammar does not consider cross-sentence def-
initions in any way.
Czech data used for evaluation consist of several
chapters of the Calimera guidelines and Microsoft
Excel tutorial. The tutorial is a typical text used
in e-learning, consisting of five chapters describing
sheets, tables, formating, graphs and lists. The cor-
pus consists of over 90,000 tokens and contains 162
definitions, out of which 153 are contained in a sin-
gle sentence, 6 span 2 sentences, and 3 definitions
span 3 sentences.
Polish test corpus consists of over 83,200 tokens
containing 157 definitions: 148 definitions are con-
tained within one sentence, while 9 span 2 sen-
tences. The corpus is made up of 10 chapters of a
popular introduction to and history of computer sci-
ence and computer hardware.
Each grammar was quantitatively evaluated by
comparing manually annotated files with the same
files annotated automatically by the grammar. After
considering various ways of quantitative evaluation,
we decided to do the comparison at token level: pre-
cision was calculated as the ratio of the number of
those tokens which were parts of both a manually
marked definition and an automatically discovered
definition to the number of all tokens in automati-
cally discovered definitions, while recall was taken
to be the ratio of the number of tokens simultane-
ously in both kinds of definitions to the number of
tokens in all manually annotated definitions. Since,
for this task, recall is more important than precision,
we used the F2-measure for the combined result.7
7In general, F? = (1 + ?) ? (precision ? recall)/(? ?
precision+recall). Perhaps? larger than 2 could be used, but it
is currently not clear to us what criteria should be assumed when
46
The results for the three grammars are given in
Table 4. Note that the processing model for Czech
precision recall F2
Bulgarian 20.5% 2.2% 3.1
Czech 18.3% 40.7% 28.9
Polish 14.8% 22.2% 19.0
Table 4: Token-based evaluation of shallow gram-
mars
differs from the other two languages, as the input
text is converted to a flat format, as described in sec-
tion 5.3, and grammar rules are sensitive to sentence
boundaries (and may operate over them).
5 Evaluation and Possible Improvements
5.1 Interannotator Agreement
We calculated Cohen?s kappa statistic (1) for the cur-
rent task, where both the relative observed agree-
ment among raters Pr(a) and the probability that
agreement is due to chance Pr(e) where calculated
at token level.
? =
Pr(a) ? Pr(e)
1 ? Pr(e)
(1)
More specifically, we assumed that two annotators
agree on a token if the token belongs to a definition
either according to both annotations or according to
neither. In order to estimate the probability of agree-
ment due to chance Pr(e), we measured, separately
for each annotator, the proportion of tokens found in
definitions to all tokens in text, which resulted in two
probability estimates p1 and p2, and treated Pr(e) as
the probability that the two annotators agree if they
randomly, with their own probability, classify a to-
ken as belonging to a definition, i.e.:
Pr(e) = p1 ? p2 + (1 ? p1) ? (1 ? p2) (2)
The interannotator agreement (IAA) was mea-
sured this way for Czech and Polish, where ? for
each language ? the respective test corpus was an-
notated by two annotators. The results are 0.44 for
Czech and 0.31 for Polish. Such results are very low
for any classification task, and especially low for a
deciding on the exact value of ?. Note that it would not make
sense to use recall alone, as it is trivial to write all-accepting
grammars with 100% recall.
binary classification task. They show that the task of
identifying definitions in running texts and agreeing
on which parts of text count as a definition is intrin-
sically very difficult. They also call for the recon-
sideration of the evaluation and IAA measurement
methodology based on token classification.8
5.2 Evaluation Methodology
To the best of our knowledge, there is no estab-
lished evaluation methodology for the task of def-
inition extraction, where definitions may span sev-
eral sentences.9 For this reason we evaluated the re-
sults again, in a different way: we treated an auto-
matically discovered definition as correct, if it over-
lapped with a manually annotated definition. We
calculated precision as the number of automatic defi-
nitions overlapping with manual definitions, divided
by the number of automatic definitions, while re-
call ? as the number of manual definitions overlap-
ping automatic definitions, divided by the number of
manual definitions.10
The results for the three grammars, given in Ta-
ble 5, are much higher than those in Table 4 above,
although still less than satisfactory.
precision recall F2
Bulgarian 22.5% 8.9% 11.1
Czech 22.3% 46% 33.9
Polish 23.3% 32% 28.4
Table 5: Definition-based evaluation of shallow
grammars
5.3 Definitions and Sentence Boundaries
Regardless of the inherent difficulties of the task and
difficulties with the evaluation of the results, there
is clear room for improvement; one possible path
8A better approximation would be to measure IAA on the
basis of sentence or (as suggested by an anonymous reviewer)
NP classification; we intend to pursue this idea in future work.
9With the assumption that definitions are no longer than
a sentence, usually the task is treated as a classification task,
where sentences are classified as definitional or not, and ap-
propriate precision and recall measures are applied at sentence
level.
10At this stage definition fragments distributed across a num-
ber of different sentences were treated as different definitions,
which negatively affects the evaluation of the Bulgarian gram-
mar, as the Bulgarian test corpus contains a large number of
multi-sentence definitions.
47
to explore concerns multi-sentence definitions. As
noted above, for all languages considered here, there
were definitions which were spanning 2 or more sen-
tences; this turned out to be a problem especially for
Bulgarian, were 36% of definitions crossed a sen-
tence boundary.11
Such multi-sentence definitions are a problem be-
cause in the DTD adopted in this project definitions
are subelements of sentences rather than the other
way round. In case of a multi-sentence definition,
for each sentence there is a separate element en-
capsulating the part of the definition contained in
this sentence. Although these are linked via spe-
cial attributes and the information that they are part
of the same definition can subsequently be recov-
ered, it is difficult to construct an lxtransduce
grammar which would be able to automatically mark
such multi-sentence definitions: an lxtransduce
grammar expects to find a sequence of elements and
wrap them in a single larger element.
A solution to this technical problem has been im-
plemented in the Czech grammar, where first the in-
put text is flattened (via an XSLT script), so that,
e.g.:
<par id="d1p2">
<s id="d1p2s1">
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
</s>
</par>
becomes:
<par id="Sd1p2"/>
<s id="Sd1p2s1"/>
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
<s id="Ed1p2s1"/>
<par id="Ed1p2"/>
11An example of a Polish manually annotated multi-sentence
definition is: . . . opracowano techniki antyspamowe. Tech-
niki te drastycznie zaniz?aja? wartos?c? strony albo ja? banuja?. . .
(Eng. ?. . . anti-spam techniques were developed. Such tech-
niques drastically lower the value of the page or they ban it. . . ?).
The definition is split into two fragments fully contained in re-
spective sentences: techniki antyspamowe and Techniki te. . . .
No attempt at anaphora resolution is made.
This flattened representation is an input to a gram-
mar which is sensitive to the empty s and par ele-
ments and may discover definitions containing such
elements; in such a case, the postprocessing script,
which restores the hierarchical paragraph and sen-
tence structure, splits such definitions into smaller
elements, fully contained in respective sentences.
5.4 Problems Specific to Slavic
At least in case of the two West Slavic languages
considered here, the task of writing a definition
grammar is intrinsically more difficult than for Ger-
manic or Romance languages, mainly for the follow-
ing two reasons.
First, Czech and Polish have very rich nominal
inflection with a large number of paradigm-internal
syncretisms. These syncretisms are a common cause
of tagger errors, which percolate to further stages of
processing. Moreover, the number of cases makes it
more difficult to encode patterns like ?NP verb NP?,
as different verbs may combine with NPs of different
case. In fact, even two different copulas in Polish
take different cases!
Second, the relatively free word order increases
the number of rules that must be encoded, and makes
the grammar writing task more labour-intensive and
error-prone. The current version of the Polish gram-
mar, with 34 rules, is rather basic, and even the 147
rules of the Czech grammar do not take into consid-
eration all possible patterns of grammar definitions.
As Tables 4 and 5 show, there is a positive corre-
lation between the grammar size and the value of
F2, and the Bulgarian and Polish grammars certainly
have room to grow. Moreover, a path that is well
worth exploring is to drastically increase the num-
ber of rules and, hence, the recall, and then deal with
precision via Machine Learning methods (cf. sec-
tion 5.6).
5.5 Levels of Linguistic Processing
The work reported here has been an excercise in
definition extraction using shallow parsing methods.
However, the poor results suggest that this is one
of the tasks that require a much more sophisticated
and deeper approach to language analysis. In fact,
in turns out that virtually all successful attempts at
definition extraction that we are aware of build on
worked-out deep linguistic approaches (Klavans and
48
Muresan, 2000; Fahmi and Bouma, 2006; Walter
and Pinkal, 2006), some of them combining syn-
tactic and semantic information (Miliaraki and An-
droutsopoulos, 2004; Walter and Pinkal, 2006).
Unfortunately, for most Baltic and Slavic lan-
guages, such deep parsers are unavailable or have
not yet been extensively tested on real texts. One
exception is Czech, where a number of parsers were
already described and evaluated (on the Prague De-
pendency Treebank) in (Zeman, 2004, ? 14.2); the
best of these parsers reach 80?85% accuracy.
For Polish, apart from a number of linguistically
motivated toy parsers, there is a possibly wide cov-
erage deep parser (Wolin?ski, 2004), but it has not yet
been evaluated on naturally occurring texts. The sit-
uation is probably most dire for Bulgarian, although
there have been attempts at the induction of a depen-
dency parser from the BulTreeBank (Marinov and
Nivre, 2005; Chanev et al, 2006).
Nevertheless, if other possible paths of improve-
ment suggested in this section do not bring satisfac-
tory results, we plan to make an attempt at adapting
these parsers to the task at hand.
5.6 Postprocessing: Machine Learning and
Keyword Identification
Various approaches to the machine learning treat-
ment of the task of classifying sentences or snippets
as definitions or non-definitions can be found, e.g.,
in (Miliaraki and Androutsopoulos, 2004; Fahmi
and Bouma, 2006) and references therein. In the
context of the present work, such methods may be
used to postprocess apparent definitions found at
earlier processing stages and decide which of them
are genuine definitions. For example, (Fahmi and
Bouma, 2006) report that a system trained on 2299
sentences, including 1366 definition sentences, may
increase the accuracy of a definition extraction tool
from 59% to around 90%.12
Another possible improvement may consist in,
again, aiming at very high recall and then using
an independent keyword detector to mark keywords
(and key phrases) in text and classifying as genuine
definitions those definitions, whose defined term has
been marked as a keyword.
12The numbers are so high ?probably due to the fact that the
current corpus consists of encyclopedic material only? (Fahmi
and Bouma, 2006, fn. 4).
Whatever postprocessing technique or combina-
tion of techniques proves most efficient, it seems that
the linguistic processing should aim at high recall
rather than high precision, which further justifies the
use of the F2 measure for evaluation.13
6 Conclusion
To the best of our knowledge, this paper is the first
report on the task of definition extraction for a num-
ber of Slavic languages. It shows that the task is
intrinsically very difficult, which partially explains
the relatively low results obtained. It also calls atten-
tion to the fact that there is no established evaluation
methodology where possibly multi-sentence defini-
tions are involved and suggests what such method-
ology could amount to. Finally, the paper suggests
ways of improving the results, which we hope to fol-
low and report in the future.
References
Markus Becker et al 2002. SProUT ? shallow process-
ing with typed feature structures and unification. In
Proceedings of the International Conference on NLP
(ICON 2002), Mumbai, India.
Sharon A. Caraballo. 2001. Automatic Construction of a
Hypernym-Labeled Noun Hierarchy from Text. Ph. D.
dissertation, Brown University.
Atanas Chanev, Kiril Simov, Petya Osenova, and Sve-
toslav Marinov. 2006. Dependency conversion and
parsing of the BulTreeBank. In proceedings of the
LREC workshop Merging and Layering Linguistic In-
formation, Genoa, Italy.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Witold Droz?dz?yn?ski, Petr Homola, Jakub Piskorski, and
Vytautas Zinkevic?ius. 2003. Adapting SProUT to
processing Baltic and Slavonic languages. In Infor-
mation Extraction for Slavonic and Other Central and
Eastern European Languages, pp. 18?25.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications.
13Note that the situation here is different than in the task of
acquiring hyponymic relations from texts, where high-precision
manual rules (Hearst, 1992) must be augmented with statistical
clustering methods to increase recall (Caraballo, 2001).
49
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, Nantes, France.
Judith L. Klavans and Smaranda Muresan. 2000.
DEFINDER: Rule-based methods for the extraction of
medical terminology and their associated definitions
from on-line text. In Proceedings of the Annual Fall
Symposium of the American Medical Informatics As-
sociation.
Judith L. Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of AMIA Sym-
posium 2001.
V?ronique Malais?, Pierre Zweigenbaum, and Bruno
Bachimont. 2004. Detecting semantic relations be-
tween terms in definitions. In S. Ananadiou and
P. Zweigenbaum, editors, COLING 2004 CompuTerm
2004: 3rd International Workshop on Computational
Terminology, pp. 55?62, Geneva, Switzerland. COL-
ING.
Ma?gorzata Marciniak, Agnieszka Mykowiecka, Anna
Kups?c?, and Jakub Piskorski. 2005. Intelligent con-
tent extraction from Polish medical texts. In L. Bolc
et al, editors, Intelligent Media Technology for Com-
municative Intelligence, Second International Work-
shop, IMTCI 2004, Warsaw, Poland, September 13-14,
2004, Revised Selected Papers, volume 3490 of Lec-
ture Notes in Computer Science, pp. 68?78. Springer-
Verlag.
Svetoslav Marinov and Joakim Nivre. 2005. A data-
driven parser for Bulgarian. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, pp. 89?100, Barcelona.
Spyridoula Miliaraki and Ion Androutsopoulos. 2004.
Learning to identify single-snippet answers to defi-
nition questions. In Proceedings of COLING 2004,
pp. 1360?1366, Geneva, Switzerland. COLING.
Jennifer Pearson. 1996. The expression of defini-
tions in specialised texts: a corpus-based analysis.
In M. Gellerstam et al, editors, Proceedings of the
Seventh Euralex International Congress, pp. 817?824,
G?teborg.
Jakub Piskorski et al 2004. Information extraction for
Polish using the SProUT platform. In M. A. K?opotek
et al, editors, Intelligent Information Processing and
Web Mining, pp. 227?236. Springer-Verlag, Berlin.
Kiril Simov and Petya Osenova. 2005. BulQA:
Bulgarian-Bulgarian Question Answering at CLEF
2005. In CLEF, pp. 517?526.
Kiril Simov et al 2001. CLaRK ? an XML-based sys-
tem for corpora development. In P. Rayson et al, edi-
tors, Proceedings of the Corpus Linguistics 2001 Con-
ference, pp. 558?560, Lancaster.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
German text corpora. In Proceedings of LREC 2006.
Hristo Tanev. 2004. Socrates: A question answering
prototype for Bulgarian. In Recent Advances in Nat-
ural Language Processing III, Selected Papers from
RANLP 2003, pages 377?386. John Benjamins.
Richard Tobin, 2005. Lxtransduce, a replace-
ment for fsgmatch. University of Edinburgh.
http://www.cogsci.ed.ac.uk/~richard/
ltxml2/lxtransduce-manual.html.
Stephan Walter and Manfred Pinkal. 2006. Automatic
extraction of definitions from German court decisions.
In Proceedings of the Workshop on Information Ex-
traction Beyond The Document, pp. 20?28, Sydney,
Australia. Association for Computational Linguistics.
Marcin Wolin?ski. 2004. Komputerowa weryfikacja gra-
matyki S?widzin?skiego. Ph. D. dissertation, ICS PAS,
Warsaw.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph. D. dissertation, Charles University,
Prague.
50
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 64?67,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Stand-off TEI Annotation: the Case of the National Corpus of Polish 
Piotr Ba?ski
Institute of English Studies
University of Warsaw
Nowy ?wiat 4, 00-497 Warszawa, Poland
pkbanski@uw.edu.pl
Adam Przepi?rkowski
Institute of Computer Science
Polish Academy of Sciences
Ordona 21, 01-237 Warszawa, Poland
adamp@ipipan.waw.pl
Abstract
We present the annotation architecture of 
the National Corpus of Polish and discuss 
problems identified in the TEI stand-off 
annotation system,  which,  in  its  current 
version, is still very much unfinished and 
untested,  due  to  both  technical  reasons 
(lack  of  tools  implementing  the  TEI-
defined  XPointer  schemes)  and  certain 
problems concerning data representation. 
We  concentrate  on  two  features  that  a 
stand-off system should possess and that 
are conspicuously missing in the current 
TEI Guidelines.
1 Introduction
The present paper presents the National Corpus 
of  Polish  (NCP).1 The  project  is  a  joint 
undertaking  of  a  consortium  consisting  of 
institutions that created their own large corpora 
of Polish in the past (see (Przepi?rkowski et al, 
2008)  for  details);  these  corpora  formed  the 
initial data bank of the corpus. The intended size 
of the corpus is  one billion (109) tokens and as 
such, at the time of completion in 2010, the NCP 
is  going  to  be  one  of  the  largest  corpora 
available,  possibly  the  largest  corpus  featuring 
multiple levels of linguistic annotation of various 
kinds.  Currently,  a  hand-verified  one-million-
token subcorpus is being completed, and a basic, 
automatically created 430-million-token demo is 
available online at http://nkjp.pl/.
The project uses an extended morphosyntactic 
tagset with several years of practical use behind 
it  in  one  of  the  source  corpora  (cf. 
http://korpus.pl/)  and an open-source query en-
gine with a powerful, regex-based language and 
a graphical front-end.
Section  2  of  this  paper  talks  about  the 
encoding format adopted for the corpus, section 
1 The  Polish  name  of  the  corpus  is  Narodowy  Korpus 
J?zyka  Polskiego,  hence  the  abbreviation  NKJP,  used  in 
web addresses and namespace identifiers.
3 presents its general architecture, and section 4 
discusses  the  reasons  for,  and  our  implemen-
tation  of,  the  suggested  NCP  enhancements  to 
the TEI Guidelines.
2 The encoding format: stand-off TEI
The Text Encoding Initiative (TEI Consortium, 
2007) has been at the forefront of text annotation 
and resource interchange for many years. It has 
influenced corpus linguistic practices in at least 
three related ways. Firstly,  the formalism itself, 
in the mature  form,  has been used to  mark  up 
linguistic  corpora,  e.g.  the  British  National 
Corpus.  An  early  application  of  the  TEI,  the 
Corpus  Encoding  Standard  (CES;  see 
http://www.cs.vassar.edu/CES/), together with its 
XML  version,  XCES  (http://www.xces.org/), 
have served as de facto standards for corpus en-
coding  in  numerous  projects.  Finally,  the  ex-
perience gained in creating and using XCES (to-
gether with e.g. the feature-structure markup of 
the TEI) has served as a foundation for the Lin-
guistic  Annotation  Format  (LAF,  Ide  and  Ro-
mary, 2007), within ISO TC37 SC4.  LAF pro-
mises to provide a standard interchange format 
for  linguistic  resources  of  many  diverse  kinds 
and origins.
The relationship between the TEI (especially 
in its stand-off version) and the LAF is straight-
forward.  Both are implemented in XML, which 
makes transduction between a rigorous TEI for-
mat and the LAF ?dump? (pivot) format mostly a 
matter of fleshing out some data structures.
3 NCP ? general architecture 
Stand-off annotation is by now a well-grounded 
data representation technique,  pioneered by the 
CES and continuing to be the foundation of the 
LAF. In short, it assumes that the source text in 
the corpus, ideally kept in an unannotated form 
and in read-only files,  is the root of a possibly 
multi-file  system  of  data  descriptions  (each 
description focusing on a distinct aspect of the 
64
source data). The source text is typically accom-
panied by a level of primary segmentation, which 
may  be  the  lowest-level  XML  layer  of  anno-
tation.  The  other  files  form  a  possibly  multi-
leaved  and  multi-leveled  hierarchy  referencing 
either  the  level  of  primary  segmentation,  or 
higher  order  levels  of  description.  The  NCP 
follows these guidelines to the extent allowed by 
the TEI schema.
Each corpus text is kept in a separate directory 
together with the annotation files that reference it 
directly or indirectly, and with the header that is 
included  by  all  these  files.  Contents  of  an 
example directory are shown below.
(1) text.xml
header.xml
ann_morphosyntax.xml
ann_segmentation.xml
ann_structure.xml
All of these files contain TEI documents (or, in 
the case of header.xml, proper subsets thereof). 
They form a  hierarchy of  annotation levels,  as 
presented  in  Figure  1.  The  text.xml  file  is  the 
root,  referenced  by  the  layer  of  text  structure 
(providing  markup  from  the  paragraph  level 
upwards)  and  the  layer  of  segmentation.  The 
segmentation layer  is  further referenced by the 
layer of morphosyntactic information and word-
sense annotation. The morphosyntactic level, in 
turn,  is  the  basis  for  the  level  identifying  syn-
tactic  words,  which  constitutes  the  foundation 
upon  which  the  levels  identifying  syntactic 
chunks and named entities are built.
In  text.xml,  the  normalized  source  text  is 
divided in paragraph-sized chunks (enclosed in 
anonymous blocks, <ab>, to be further refined in 
the  text-structure  level  of  annotation).2 It  also 
2 Ideally,  as mentioned above,  the primary text should be 
stored without markup, and the segmentation layer should 
constitute the lowest-level XML document. This is exactly 
includes  two  headers:  the  main  corpus  header, 
which encodes information relevant to all  parts 
of  the  corpus,  and  the  local  header,  which 
records the information on the particular text and 
its annotations.
The segmentation file provides what the LAF 
calls the base segmentation level that is further 
used as the basis for other kinds of annotation. It 
is implemented as a TEI document with <seg> 
elements that contain XInclude instructions (see 
example (4) in the next section). As such, it may 
serve both as a separate annotation layer or as a 
merged  structure,  after  the  inclusion  directives 
are resolved. Crucially, in the latter case, which 
is  the  default  with  many parsers,  the  XPointer 
indexing information is lost. We shall come back 
to this issue in section 4.1.
The text-structure layer is defined similarly to 
the segmentation layer.  Other annotation layers 
replace the mechanism of XInclude with XLink, 
in the way advocated by the XCES.
The morphosyntactic layer of annotation con-
sists of a series of <seg> elements that contain 
TEI feature structures (i) providing basic infor-
mation on the segment,  (ii)  specifying the pos-
sible interpretations as identified by the morpho-
logical analyser, and (iii) pointing at the morpho-
what  the  LAF-encoded  American  National  Corpus  does, 
requiring dedicated tools for merging plain text corpus files 
with  the  segmentation  documents.  Unfortunately,  this  is 
where we reach the technological boundary of the XInclude 
system: it is unable to reference substrings in a plain text 
file,  due  to  a  weakly  motivated  ban  on  the  concurrent 
presence  of  @parse=?text?  attribute  and  the  @xpointer 
attribute.  We  therefore  enclose  the  source  text  in  ano-
nymous  blocks  (<ab>)  that  we  can  easily  address  with 
XPointers. An anonymous reviewer agrees that the lack of a 
single,  immutable  text  file  is  a  serious  weakness  of  this 
system and notes that being able to derive plain text from 
markup is no remedy. This may constitute either a case for 
XLink, or an argument for lifting the @parse/@pointer ban.
Figure 1: The logical data structure of the NCP
65
syntactic  description  selected  by  the  disambi-
guating agent.
The  higher-order  annotation  layers  also 
contain feature structures, which usually point at 
the  selected segments  of  annotation layers  that 
are one level  lower, and identify their  function 
within the given data structure.
4 Enhancements  to  the  TEI  stand-off 
recommendations
In this section, we first illustrate a case where the 
stand-off annotation system as advocated by the 
TEI  loses  information  on  the  boundedness  of 
segments,  and  then  move  on  to  illustrate  a 
different  issue  stemming  from  the  lack  of  a 
neutral bracket-like element in the TEI markup. 
4.1 Identification of bound segments
Segmentation  of  Polish  texts  is  not  a  trivial 
matter,  partially  because  of  the  person-number 
enclitics ? elements that can attach to almost any 
part  of  the  clause,  while  being  functionally 
related to  the  main  verb.  Segmenting them to-
gether  with  their  hosts,  apart  from  being  a 
methodologically  bad  move,  would  greatly  in-
crease the complexity of  the linguistic analysis 
built on top of such segmentations. The diamond 
in  (2)  below marks  alternative  positions  where 
the  2nd Person  Plural  clitic  (separated  by  a 
vertical  bar)  may  appear.  All  of  the  resulting 
sentences have the same interpretation.
(2) Czemu|?cie znowu? wczoraj? Piotra? gonili??
why|2pl again yesterday Piotr chased.prt
?Why did you chase Piotr yesterday again??
Yet  another  group  of  segmentation  problems 
concerns  compounds,  right-headed  (3a)  or  co-
ordinative (3b).
(3) a. ???to|czerwony materia?
yellow|red fabric
?yellowish red fabric?
b. ???to-czerwony materia?
?yellow and red fabric?
Inline markup of the above examples preserves 
information on which segment is bound (attached 
to  the  preceding  one)  or  free-standing.  This  is 
due  to  the  whitespace  intervening  between the 
<seg> elements in this kind of markup.
When,  however,  stand-off  markup  using  the 
XInclude  mechanism  is  applied  here,  com-
plications  arise.  The  segmental  level  of  anno-
tation with unresolved inclusions provides clear 
hints about the status of segments. This is due to 
XPointer  offsets,  as  can be seen  in  (4)  below, 
which is an example assuming that the adjective 
???to-czerwony is the first word in an <ab> ele-
ment bearing the @xml:id attribute set to ?t1?.3
(4)
<seg xml:id="segm_1.1-seg">
 <xi:include href="text.xml"
 xpointer="string-range(t1,0,5)"/></seg>
<seg xml:id="segm_1.2-seg">
 <xi:include href="text.xml"  
 xpointer="string-range(t1,5,1)"/></seg>
<seg xml:id="segm_1.3-seg">
 <xi:include href="text.xml" 
 xpointer="string-range(t1,6,8)"/></seg>
However, after inclusions are resolved, all of the 
offset  information  is  lost,  because  all  the 
@xpointer attributes (indeed, all the <xi:include> 
elements)  are  gone  and  all  that  remains  is  a 
sequence  of  <seg>  elements  such  as 
<seg>???to</seg><seg>-</seg><seg>cze
rwony</seg>.
While, in many cases, information on bound-
edness  could  be  recovered  from  the  morpho-
syntactic description of the given segment,  this 
does not resolve the issue because, firstly, a re-
course  to  morphosyntactic  annotation  layer  in 
order to recover information lost in the segmen-
tation layer is methodologically flawed (in some 
cases, it is perfectly imaginable that a text is only 
accompanied by the segmentation layer of anno-
tation and nothing else), and, secondly, morpho-
syntactic identity will not resolve all such cases. 
Consider the example of ???to-czerwony ?yellow 
and red?: the segment  czerwony here is bound, 
but  both  graphically  and  morphosyntactically 
identical  to  the  frequent  free-standing  segment 
czerwony ?red?.
In order to accommodate such cases, we have 
defined  an  additional  attribute  of  the  <seg> 
element,  @nkjp:nps,  where  ?nkjp:?  is  the  non-
TEI  namespace  prefix,  while  ?nps?  stands  for 
?no  preceding  space?  and  its  default  value  is 
?false?.  Naturally,  this  attribute  solves  issues 
specific to Polish and similar languages. It can be 
generalized  and  become  something  like 
@bound={?right?,  ?left?,  ?both?},  and  in  this 
shape, get incorporated into the TEI Guidelines. 
4.2 Structural  disjunction  between  alter-
native segmentations
One strategy to handle alternative segmentations, 
where the choice is between a single segment of 
3Note  that  here,  string-range()  is  an  XPointer  scheme 
defined by the TEI. It is not to be confused with the string-
range() function of the XPointer xpointer() scheme, defined 
by the W3C permanent working draft at http://www.w3.org/
TR/xptr-xpointer/.
66
the form <seg>New York</seg> and a sequence 
of two separate segments, <seg>New</seg> and 
<seg>York</seg>, is to perform radical segmen-
tation (always segment New and York separately) 
and provide an extra layer of alternative segmen-
tation that  may link the two parts  of  the name 
into  a  single  unit.  This  is  what  we  do  in  the 
creation  of  the  annotation  level  of  syntactic 
words that may, e.g., need to link the three seg-
ments of ???to-czerwony above into a single unit, 
because this is how they function in the syntactic 
representation.
In some cases, however, radical segmentation 
may create  false  or  misleading  representations, 
and  Polish  again  provides  numerous  relevant 
examples.  Sometimes  bound segments,  such as 
the person-number clitics illustrated in (2) above, 
are homophonous with parts of words.
(5) a. mia?|em vs. mia?em
had.prt|1sg fines.instr.sg
b. czy|m vs. czym
whether|1sg what.instr
c. gar|?cie vs. gar?cie
pot.acc|2pl fistful.nom.pl
One may attempt to defend radical segmentation 
for  case  (a)  on  the  not-so-innocent  assumption 
that segmenting tools might sometimes reach in-
side  morphological  complexes  and separate  af-
fixes  from stems,  rather  than clitics  from their 
hosts. However, examples (b) and (c) show that 
this is not a feasible approach here: the Instru-
mental  czym in (b) is monomorphemic, and the 
segmentation of  gar?cie ?fistfuls? into  gar- and 
-?cie is  likewise  false,  because  the  putative 
segment division would fall inside the root gar??.
Thus, radical segmentation is not an available 
strategy  in  the  case  at  hand.  What  we  need 
instead  is  a  way  to  express  the  disjunction 
between  a  sequence  such  as  <seg>mia?</seg> 
<seg>em</seg> (cf.  (5a)) on the one hand, and 
the  single  segment  <seg>mia?em</seg>  on  the 
other. It turns out that the TEI has no way of ex-
pressing this kind of relationship structurally.
The  TEI  Guidelines  offer  the  element 
<choice>, but it can only express disjunction bet-
ween  competing  segments,  and  never  between 
sequences thereof. The Guidelines also offer two 
non-structural methods of encoding disjunction. 
The first  uses the element  <join> (which is  an 
ID-based equivalent of a bracket ? it points to the 
segments that are to be virtually joined) and the 
element  <alt>  (which  points  at  encoding  alter-
natives).  The  other  utilizes  the  @exclude  at-
tribute, which, placed in one segment, points at 
elements that are to be ignored if the segment at 
hand  is  valid  (the  excluded  elements,  in  turn, 
point back at the excluding segment).
 Recall that the intended size of the corpus is 
one billion segments. Tools that process corpora 
of this size should not be forced to backtrack or 
look forward to see what forms a sequence and 
what the alternative to this sequence is. Instead, 
we  need  a  simple  structural  statement  of  dis-
junction between sequences.  The solution  used 
by  the  NCP  consists  in  (i)  adding  an  element 
meant to provide a semantically neutral bracket 
(<nkjp:paren>)  and  (ii)  including  <nkjp:paren> 
in the content model of <choice>. Note that this 
representation can be readily converted into the 
pivot format of the LAF:
(6)   <choice>
<seg>mia?em</seg>
<nkjp:paren>
<seg>mia?</seg>
<seg nkjp:nps=?true?>em</seg>
</nkjp:paren>
    </choice>
5 Conclusion
We have presented the TEI-P5-XML architecture 
of the National Corpus of Polish and identified 
some  weak  points  of  the  TEI-based  stand-off 
approach:  the  impossibility  of  keeping  the 
primary text unannotated in the XInclude system, 
the loss of information on segment-boundedness, 
and  the  absence  of  a  structural  statement  of 
disjunction between sequences of segments (this 
last  issue  is  also  due  to  the  lack,  among  the 
numerous detailed markup options provided by 
the  TEI,  of  a  semantically  neutral  bracket-like 
element  whose only role  would be to  embrace 
sequences of elements). 
We are grateful to the two anonymous LAW-
09 reviewers for their helpful comments.
References
Ide, N. and L. Romary. (2007). Towards International 
Standards  for  Language  Resources.  In  Dybkjaer, 
L.,  Hemsen, H., Minker, W. (eds.), Evaluation of 
Text and Speech Systems, Springer, 263-84. 
Przepi?rkowski, A., R. L. G?rski, B. Lewandowska-
Tomaszczyk and M. ?azi?ski. (2008). Towards the 
National  Corpus of Polish.  In  the proceedings of 
the  6th  Language  Resources  and  Evaluation 
Conference (LREC 2008), Marrakesh, Morocco.
TEI Consortium, eds.  2007.  TEI P5:  Guidelines  for 
Electronic Text Encoding and Interchange. Version 
1.2.0.  Last  updated  on  February  1st  2009.  TEI 
Consortium.
67
Coling 2010: Poster Volume, pages 1?8,
Beijing, August 2010
Towards the Adequate Evaluation of Morphosyntactic Taggers
Szymon Acedan?ski
Institute of Computer Science,
Polish Academy of Sciences
Institute of Informatics,
University of Warsaw
accek@mimuw.edu.pl
Adam Przepi?rkowski
Institute of Computer Science,
Polish Academy of Sciences
Institute of Informatics,
University of Warsaw
adamp@ipipan.waw.pl
Abstract
There exists a well-established and almost
unanimously adopted measure of tagger
performance, namely, accuracy. Although
it is perfectly adequate for small tagsets
and typical approaches to disambiguation,
we show that it is deficient when applied
to rich morphological tagsets and propose
various extensions designed to better cor-
relate with the real usefulness of the tag-
ger.
1 Introduction
Part-of-Speech (PoS) tagging is probably the
most common and best researched NLP task, the
first step in many higher level processing solu-
tions such as parsing, but also information re-
trieval, speech recognition and machine transla-
tion. There are also well established evaluation
measures, the foremost of which is accuracy, i.e.,
the percent of words for which the tagger assigns
the correct ? in the sense of some gold standard
? interpretation.
Accuracy works well for the original PoS tag-
ging task, where each word is assumed to have ex-
actly one correct tag, and where the information
carried by a tag is limited roughly to the PoS of
the word and only very little morphosyntactic in-
formation, as in typical tagsets for English. How-
ever, there are two cases where accuracy becomes
less than adequate: the situation where the gold
standard and / or the tagging results contain mul-
tiple tags marked as correct for a single word, and
the use of a rich morphosyntactic (or morphologi-
cal) tagset.
The first possibility is discussed in detail in
(Karwan?ska and Przepi?rkowski, 2009), but the
need for an evaluation measure for taggers which
do not necessarily fully disambiguate PoS was al-
ready noted in (van Halteren, 1999), where the use
of standard information retrieval measures preci-
sion and recall (as well as their harmonic mean,
the F-measure) is proposed. Other natural gen-
eralisations of the accuracy measure, able to deal
with non-unique tags either in the gold standard1
or in the tagging results, are proposed in (Kar-
wan?ska and Przepi?rkowski, 2009).
Standard accuracy is less than adequate also
in case of rich morphosyntactic tagsets, where
the full tag carries information not only about
PoS, but also about case, number, gender, etc.
Such tagsets are common for Slavic languages,
but also for Hungarian, Arabic and other lan-
guages. For example, according to one com-
monly used Polish tagset (Przepi?rkowski and
Wolin?ski, 2003), the form uda has the follow-
ing interpretations: fin:sg:ter:perf (a fi-
nite singular 3rd person perfective form of the
verb UDAC? ?pretend?), subst:pl:nom:n and
1There are cases were it makes sense to manually assign
a number of tags as correct to a given word, as any decision
would be fully arbitrary, regardless of the amount of con-
text and world knowledge available. For example, in some
Slavic languages, incl. Polish, there are verbs which option-
ally subcategorise for an accusative or a genitive comple-
ment, without any variation in meaning, and there are nouns
which are syncretic between these two cases, so for such
?verb + nounacc/gen? sequences it is impossible to fully dis-
ambiguate case; see also (Oliva, 2001).
1
subst:pl:acc:n (nominative or accusative
plural form of the neuter noun UDO ?thigh?).
Now, assuming that the right interpretation in a
given context is subst:pl:acc:n, accuracy
will equally harshly penalise the other nominal in-
terpretation (subst:pl:nom:n), which shares
with the correct interpretation not only PoS, but
also the values of gender and number, and the
completely irrelevant verbal interpretation. A
more accurate tagger evaluation measure should
distinguish these two non-optimal assignments
and treat subst:pl:nom:n as partially correct.
Similarly, the Polish tagset mentioned above
distinguishes between nouns and gerunds, with
some forms actually ambiguous between these
two interpretations. For example, zadanie may be
interpreted as a nominative or accusative form of
the noun ZADANIE ?task?, or a nominative or ac-
cusative form of the gerund derived from the verb
ZADAC? ?assign?. Since gerunds and nouns have
very similar distributions, any error in the assign-
ment of part of speech, noun vs. gerund, will most
probably not matter for a parser of Polish ? it
will still be able to construct the right tree, pro-
vided the case is correctly disambiguated. How-
ever, the ?all-or-nothing? nature of the accuracy
measure regards the tag differing from the correct
one only in part of speech or in case as harshly,
as it would regard an utterly wrong interpretation,
say, as an adverb.
In what follows we propose various evaluation
measures which differentiate between better and
worse incorrect interpretations, cf. ? 2. The im-
plementation of two such measures is described
in ? 3. Finally, ? 4 concludes the paper.
2 Proposed Measures
2.1 Full Interpretations and PoS
The first step towards a better accuracy mea-
sure might consist in calculating two accu-
racy measures: one for full tags, and the
other only for fragments of tags represent-
ing parts of speech. Two taggers wrongly
assigning either fin:sg:ter:perf (T1) or
subst:pl:nom:n (T2) instead of the correct
subst:pl:acc:n would fare equally well with
respect to the tag-level accuracy, but T2 would be
? rightly ? evaluated as better with respect to
the PoS-level accuracy.
The second example given in ? 1 shows, how-
ever, that the problem is more general and that a
tagger which gets the PoS wrong (say, gerund in-
stead of noun) but all the relevant categories (case,
number, gender) right may actually be more use-
ful in practice than the one that gets the PoS right
at the cost of confusing cases (say, accusative in-
stead of nominative).
2.2 Positional Accuracy
A generalisation of the idea of looking separately
at parts of speech is to split tags into their compo-
nents (or positions) and measure the correctness
of the tag by calculating the F-measure. For ex-
ample, if the (perfective, affirmative) gerundial in-
terpretation ger:sg:nom:n:perf:aff is as-
signed instead of the correct nominal interpreta-
tion subst:sg:nom:n, the tags agree on 3 po-
sitions (sg, nom, n), so the precision is 36 , the re-call ? 34 , which gives the F-measure of 0.6. Obvi-ously, the assignment of the correct interpretation
results in F-measure equal 1.0, and the completely
wrong interpretation gives F-measure 0.0. Taking
these values instead of the ?all-or-nothing? 0 or 1,
accuracy is reinterpreted as the average F-measure
over all tag assignments.
Note that while this measure, let us call it po-
sitional accuracy (PA), is more fine-grained than
the standard accuracy, it wrongly treats all com-
ponents of tags as of equal importance and dif-
ficulty. For example, there are many case syn-
cretisms in Polish, but practically no ambiguities
concerning the category of negation (see the value
aff above), so case is inherently much more diffi-
cult than negation, and also much more important
for syntactic parsing, and as such it should carry
more weight when evaluating tagging results.
2.3 Weighted Positional Accuracy
In the current section we make a simplifying as-
sumption that weights of positions are absolute,
rather than conditional, i.e., that the weight of, say,
case does not depend on part of speech, word or
context. Once the weights are attained, weighted
precision and recall may be used as in the follow-
ing example.
2
Assume that PoS, case, number and gender
have the same weight, say 2.0, which is 4 times
larger than that of any other category. Then, in
case ger:sg:nom:n:perf:aff is assigned
instead of the correct subst:sg:nom:n, pre-
cision and recall are given by:
P = 3? 2.04? 2.0 + 2? 0.5 =
2
3 ,
R = 3? 2.04? 2.0 =
3
4 .
This results in a higher F-measure than in case of
non-weighted positional accuracy.
The following subsections propose various
ways in which the importance of particular gram-
matical categories and of the part of speech may
be estimated.
2.3.1 Average Ambiguity
The average number of morphosyntactic inter-
pretations per word is sometimes given as a rough
measure of the difficulty of tagging. For exam-
ple, tagging English texts with the Penn Treebank
tagset is easier than tagging Czech or Polish, as
the average number of possible tags per word is
2.32 in English (Hajic?, 2004, p. 171), while it is
3.65 (Hajic? and Hladk?, 1997, p. 113) and 3.32
(Przepi?rkowski, 2008, p. 44) for common tagsets
for Czech and Polish, respectively.
By analogy, one measure of the difficulty of as-
signing the right value of a given category or part
of speech is the average number of different val-
ues of the category per word.
2.3.2 Importance for Parsing
All measures mentioned so far are intrinsic (in
vitro) evaluation measures, independent ? but
hopefully correlated with ? the usefulness of the
results in particular applications. On the other
hand, extrinsic (in vivo) evaluation estimates the
usefulness of tagging in larger systems, e.g., in
parsers. Full-scale extrinsic evaluation is rarely
used, as it is much more costly and often requires
user evaluation of the end system.
In this and the next subsections we propose
evaluation measures which combine the advan-
tages of both approaches. They are variants of
the weighted positional accuracy (WPA) measure,
where weights correspond to the usefulness of a
given category (or PoS) for a particular task.
Probably the most common task taking advan-
tage of morphosyntactic tagging is syntactic pars-
ing. Here, weights should indicate to what extent
the parser relies on PoS and particular categories
to arrive at the correct parse. Such weights may
be estimated from an automatically parsed corpus
in the following way:
for each category (including PoS) c do
count(c) = 0 {Initialise counts.}
end for
for each sentence s do
for each rule r used in s do
for each terminal symbol (word) t in the
RHS of r do
for each category c referred to by r in t
do
increase count(c)
end for
end for
end for
end for
{Use count(c)?s as weights.}
In prose: whenever a syntactic rule is used, in-
crease counts of all morphosyntactic categories
(incl. PoS) mentioned in the terminal symbols oc-
curring in this rule. These counts may be nor-
malised or used directly as weights.
We assume here that either the parser produces
a single parse for any sentence (assumption realis-
tic only in case of shallow parsers), or that the best
or at least most probable parse may be selected au-
tomatically, as in case of probabilistic grammars,
or that parses are disambiguated manually. In case
only a non-probabilistic deep parser is available,
and parses are not disambiguated manually, the
Expectation-Maximisation method may be used
to select a probable parse (De?bowski, 2009) or all
parses might be taken into account.
Note that, once a parser is available, such
weights may be calculated automatically and used
repeatedly for tagger evaluation, so the cost of us-
ing this measure is not significantly higher than
the cost of intrinsic measures, while at the same
time the correlation of the evaluation results with
the extrinsic application is much higher.
3
2.3.3 Importance for Corpus Search
The final variant (many more are imagin-
able) of WPA that we would like to de-
scribe here concerns another application of tag-
ging, namely, for the annotation of corpora.
Various corpus search engines, including the
IMS Open Corpus Workbench (http://cwb.
sourceforge.net/) and Poliqarp (http://
poliqarp.sourceforge.net/) allow the
user to search for particular parts of speech and
grammatical categories. Obviously, the tagger
should maximise the quality of the disambigua-
tion of those categories which occur frequently
in corpus queries, i.e., the weights should corre-
spond to the frequencies of particular categories
(and PoS) in user queries. Note that the only re-
source needed to calculate weights are the logs of
a corpus search engine.
An experiment involving an implementation of
this measure is described in detail in ? 3.
2.4 Conditional Weighted Positional
Accuracy
The importance and difficulty of a category may
depend on the part of speech. For example, af-
ter case syncretisms, gender ambiguity is one of
the main problems for the current taggers of Pol-
ish. But this problem concerns mainly pronouns
and adjectives, where the systematic gender syn-
cretism is high. On the other hand, nouns do not
inflect for gender, so only some nominal forms
are ambiguous with respect to gender. Moreover,
gerunds, which also bear gender, are uniformly
neuter, so here part of speech alone uniquely de-
termines the value of this category.
A straightforward extension of WPA capitalis-
ing on these observations is what we call con-
ditional weighted positional accuracy (CWPA),
where weights of morphosyntactic categories are
conditioned on PoS.
Note that not all variants of WPA may be easily
generalised to CWPA; although such an extension
is obvious for the average ambiguity (? 2.3.1), it is
less clear for the other two variants. For parsing-
related WPA, we assume that, even if a given rule
does not mention the PoS of a terminal symbol,2
2For example, in unification grammars and constraint-
based grammars a terminal may be identified only by the
that PoS may be read off the parse tree, so the con-
ditional weights may still be calculated. On the
other hand, logs of a corpus search engine are typ-
ically not sufficient to calculate such conditional
weights; e.g., a query for a sequence of 5 genitive
words occurring in logs would have to be rerun
on the corpus again in order to find out parts of
speech of the returned 5-word sequences. For a
large number of queries on a large corpus, this is
a potentially costly operation.
It is also not immediately clear how to gener-
alise precision and recall from WPA to CWPA.
Returning to the example above, where t1 =
ger:sg:nom:n:perf:aff is assigned in-
stead of the correct t2 = subst:sg:nom:n, we
note that the weights of number, case and gender
may now (and should, at least in case of gender!)
be different for the two parts of speech involved.
Hence, precision needs to be calculated with re-
spect to the weights for the automatically assigned
part of speech, and recall ? taking into account
weights for the gold standard part of speech:
P =
?t?1t?2w(t
?
1) +
?
c?C(t1,t2) ?tc1tc2w(c|t?1)
w(t?1) +
?
c?C(t1) w(c|t?1)
,
R =
?t?1t?2w(t
?
2) +
?
c?C(t1,t2) ?tc1tc2w(c|t?2)
w(t?2) +
?
c?C(t2) w(c|t?2)
,
where t? is the PoS of tag t, w(p) is the weight
of the part of speech p, w(c|p) is the conditional
weight of the category c for PoS p, C(t) is the set
of morphosyntactic categories of tag t, C(t1, t2)
is the set of morphosyntactic categories common
to tags t1 and t2, tc is the value of category c in
tag t, and ?ij is the Kronecker delta (equal to 1 if
i = j, and to 0 otherwise). Hence, for the example
above, these formulas may be simplified to:
P =
?
c?{n,c,g}w(c|ger)
w(ger) +?c?{n,c,g,a,neg}w(c|ger)
,
R =
?
c?{n,c,g}w(c|subst)
w(subst) +?c?{n,c,g}w(c|subst)
,
where n, c, g, a and neg stand for number, case,
gender, aspect and negation.
values of some of its categories, as in the following simple
rule, specifying prepositional phrases as a preposition gov-
erning a specific case and a non-empty sequence of words
bearing that case: PPcase=C ? Pcase=C X+case=C.
4
3 Experiment
To evaluate behaviour of the proposed metrics, a
number of experiments were performed using the
manually disambiguated part of the IPI PAN Cor-
pus of Polish (Przepi?rkowski, 2005). This sub-
corpus consists of 880 000 segments. Two tag-
gers of Polish were tested. TaKIPI (Piasecki and
Godlewski, 2006) is a tagger which was used for
automatic disambiguation of the remaining part of
the aforementioned corpus. It is a statistical clas-
sifier based on decision trees combined with some
automatically extracted, hand-crafted rules. This
tagger by default sometimes assigns more than
one tag to a segment, what is consistent with the
golden standard. There is a setting which allows
this behaviour to be switched off. This tagger was
tested with both settings. The other tagger is a
prototype version of this Brill tagger, presented by
Acedan?ski and Go?uchowski in (Acedan?ski and
Go?uchowski, 2009).
For comparison, four metrics were used: stan-
dard metrics for full tags and only parts of speech,
as well as Positional Accuracy and Weighted Posi-
tional Accuracy. For the last measure, the weights
were obtained by analysing logs of user queries of
the Poliqarp corpus search engine. Occurrences
of queries involving particular grammatical cat-
egories were counted and used as weights. Ob-
tained results are presented in Table 1.
Table 1: Occurrences of particular grammatical
categories in query logs of the Poliqarp corpus
search engine.
Category # occurrences
POS 37771
CASE 14055
NUMBER 2074
GENDER 552
ASPECT 222
PERSON 186
DEGREE 81
ACCOMMODABILITY 25
POST-PREP. 8
NEGATION 7
ACCENTABILITY 5
AGGLUTINATION 4
3.1 Scored information retrieval metrics
In ? 2 a number of methods of assigning a score
to a pair of tags were presented. From now on,
let name them scoring functions. One could use
them directly for evaluation, given that both the
tagger and the golden standard always assign a
single interpretation to each segment. This is not
the case for the corpus we use, hence we pro-
pose generalisation of standard information re-
trieval metrics (precision, recall and F-measure)
as well as strong and weak correctness (Kar-
wan?ska and Przepi?rkowski, 2009) to account for
scoring functions.
Denote by Ti and Gi the sets of tags assigned by
the tagger and the golden standard, accordingly,
to the i-th segment of the tagged text. The set of
all tags in the tagset is denoted by T. The scoring
function used is score:T ? T ? [0, 1]. Also, to
save up on notation, we define
score(t, A) := max
t??A
score(t, t?)
Now, given the text has n segments, we take
P =
?n
i=1
?
t?Ti score(t, Gi)?n
i=1 |Ti|
R =
?n
i=1
?
g?Gi score(g, Ti)?n
i=1 |Gi|
F = 2 ? P ?RP +R
WC =
?n
i=1 maxt?Ti score(t, Gi)
n
SC =
?n
i=1 min({score(t, Gi): t ? Ti}
? {score(g, Ti): g ? Gi})
n
Intuitions for scored precision and recall are that
precision specifies the percent of tags assigned by
the tagger which have a high score with some cor-
responding golden tag. Analogously recall esti-
mates the percent of golden tags which have high
scores with some corresponding tag assigned by
the tagger. The definition of recall is slightly dif-
ferent than proposed by Zi??ko et al (Zi??ko et
al., 2007) so that recall is never greater than one.3
3For example if the golden standard specifies a single tag
and the tagger determines two tags which all score 0.6 when
compared with the golden, then if we used equations from
Zi??ko et al, we would get the recall of 1.2.
5
3.2 Evaluation results
Now the taggers were trained on the same data
consisting of 90% segments of the corpus and then
tested on the remaining 10%. Results were 10-
fold cross-validated. They are presented in Ta-
bles 2, 3, 4 and 5.
As expected, the values obtained with PA and
WPA fall between the numbers for standard met-
rics calculated with full tags and only the part of
speech. What is worth observing is that the use
of WPA makes values for scored precision and re-
call much closer together. This can be justified
by the fact that the golden standard relatively fre-
quently contains more than one interpretation for
some tags, which differ only in values of less im-
portant grammatical categories. WPA is resilient
to such situations.
One may argue that such scoring functions may
hide a large number of tagging mistakes occurring
in low-weighted categories. But this is not the
case as the clearly most common tagging errors
reported in both (Piasecki and Godlewski, 2006)
and (Acedan?ski and Go?uchowski, 2009) are for
CASE, GENDER and NUMBER. Also, the moti-
vation for weighting grammatical categories is to
actually ignore errors in not important ones. To
be fair, though, one should make sure that the
weights used for evaluation match the actual ap-
plication domain of the analysed tagger, and if no
specific domain is known, using a number of mea-
sures is recommended.
It should also be noted that for classic infor-
mation retrieval metrics, the result of weak cor-
rectness for TaKIPI is more similar to 92.55% re-
ported by the authors (Piasecki and Godlewski,
2006) than 91.30% shown in (Karwan?ska and
Przepi?rkowski, 2009) despite using the same test
corpus and very similar methodology4 as the sec-
ond paper presents.
4 Conclusion
This paper stems from the observation that the
commonly used measure for tagger evaluation,
i.e., accuracy, does not distinguish between com-
pletely incorrect and partially correct interpreta-
4The only difference was not contracting the grammati-
cal category of ACCOMMODABILITY present for masculine
numerals in the golden standard.
tions, even though the latter may be sufficient for
some applications. We proposed a way of grad-
ing tag assignments, by weighting the importance
of particular categories (case, number, etc.) and
the part of speech. Three variants of the weighted
positional accuracy were presented: one intrin-
sic and two application-oriented, and an extension
of WPA to conditional WPA was discussed. The
variant of WPA related to the needs of the users
of a corpus search engine for the National Corpus
of Polish was implemented and its usefulness was
demonstrated. We plan to implement the parsing-
oriented WPA in the future.
We conclude that tagger evaluation is far from
being a closed chapter and the time has come to
adopt more subtle approaches than sheer accuracy,
approaches able to cope with morphological rich-
ness and oriented towards real applications.
References
Acedan?ski, Szymon and Konrad Go?uchowski. 2009.
A morphosyntactic rule-based Brill tagger for
Polish. In K?opotek, Mieczys?aw A., Adam
Przepi?rkowski, S?awomir T. Wierzchon?, and
Krzysztof Trojanowski, editors, Advances in In-
telligent Information Systems ? Design and Ap-
plications, pages 67?76. Akademicka Oficyna
Wydawnicza EXIT, Warsaw.
De?bowski, ?ukasz. 2009. Valence extraction using
the EM selection and co-occurrence matrices. Lan-
guage Resources and Evaluation, 43:301?327.
Hajic?, Jan and Barbora Hladk?. 1997. Probabilistic
and rule-based tagger of an inflective language - a
comparison. In Proceedings of the 5th Applied Nat-
ural Language Processing Conference, pages 111?
118, Washington, DC. ACL.
Hajic?, Jan. 2004. Disambiguation of Rich Inflection.
Karolinum Press, Prague.
Janus, Daniel and Adam Przepi?rkowski. 2007.
Poliqarp: An open source corpus indexer and search
engine with syntactic extensions. In Proceedings of
the ACL 2007 Demo and Poster Sessions, pages 85?
88, Prague.
Karwan?ska, Danuta and Adam Przepi?rkowski. 2009.
On the evaluation of two Polish taggers. In Goz?dz?-
Roszkowski, Stanis?aw, editor, The proceedings of
Practical Applications in Language and Computers
PALC 2009, Frankfurt am Main. Peter Lang. Forth-
coming.
6
Oliva, Karel. 2001. On retaining ambiguity in dis-
ambiguated corpora: Programmatic reflections on
why?s and how?s. TAL (Traitement Automatique des
Langues), 42(2):487?500.
Piasecki, Maciej and Grzegorz Godlewski. 2006. Ef-
fective Architecture of the Polish Tagger. In Sojka,
Petr, Ivan Kopecek, and Karel Pala, editors, TSD,
volume 4188 of Lecture Notes in Computer Science,
pages 213?220. Springer.
Przepi?rkowski, Adam and Marcin Wolin?ski. 2003.
The unbearable lightness of tagging: A case study in
morphosyntactic tagging of Polish. In Proceedings
of the 4th International Workshop on Linguistically
Interpreted Corpora (LINC-03), EACL 2003, pages
109?116.
Przepi?rkowski, Adam. 2005. The IPI PAN Corpus in
Numbers. In Proceedings of the 2nd Language &
Technology Conference, Poznan?, Poland.
Przepi?rkowski, Adam. 2008. Powierzchniowe
przetwarzanie je?zyka polskiego. Akademicka Ofi-
cyna Wydawnicza EXIT, Warsaw.
van Halteren, Hans. 1999. Performance of taggers.
In van Halteren, Hans, editor, Syntactic Wordclass
Tagging, volume 9 of Text, Speech and Language
Technology, pages 81?94. Kluwer, Dordrecht.
Zi??ko, Bartosz, Suresh Manandhar, and Richard Wil-
son. 2007. Fuzzy Recall and Precision for
Speech Segmentation Evaluation. In Proceedings
of 3rd Language & Technology Conference, Poznan,
Poland, October.
7
Table 2: Evaluation results ? standard information retrieval metrics, full tags
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 87.67% 92.10% 89.93% 84.72% 87.25%
TaKIPI ? one tag per seg. 88.68% 91.06% 90.94% 83.78% 87.21%
Brill 90.01% 92.44% 92.26% 85.00% 88.49%
Table 3: Evaluation results ? standard information retrieval metrics, PoS only
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.56% 97.52% 95.71% 97.61% 96.65%
TaKIPI ? one tag per seg. 96.53% 96.54% 96.58% 96.71% 96.65%
Brill 98.17% 98.18% 98.20% 98.26% 98.23%
Table 4: Evaluation results ? scored metrics, Positional Accuracy
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.23% 96.58% 95.69% 95.44% 95.57%
TaKIPI ? one tag per seg. 95.69% 96.10% 96.12% 95.00% 95.56%
Brill 97.02% 97.43% 97.42% 96.27% 96.84%
Table 5: Evaluation results ? scored metrics, Weighted PA, Poliqarp weights
Tagger C (%) WC (%) P (%) R (%) F (%)
TaKIPI ? defaults 95.20% 96.62% 95.34% 96.56% 95.95%
TaKIPI ? one tag per seg. 95.88% 95.93% 95.97% 95.94% 95.95%
Brill 97.34% 97.40% 97.41% 97.34% 97.38%
8
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 6?10,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
 
 
Harnessing NLP Techniques in the Processes of  
Multilingual Content Management 
 
 
Anelia Belogay Diman Karagyozov 
Tetracom IS Ltd. Tetracom IS Ltd. 
anelia@tetracom.com diman@tetracom.com 
Svetla Koeva Cristina Vertan 
Institute for Bulgarian Language Universitaet Hamburg 
svetla@dcl.bass.bg cristina.vertan@uni-hamburg.de 
Adam Przepi?rkowski Polivios Raxis 
Instytut Podstaw Informatyki Polskiej 
Akademii Nauk 
Atlantis Consulting SA 
adamp@ipipan.waw.pl raxis@atlantisresearch.gr 
Dan Cristea  
Universitatea Alexandru Ioan Cuza  
dcristea@info.uaic.ro  
 
 
Abstract 
The emergence of the WWW as the main 
source of distributing content opened the 
floodgates of information. The sheer 
volume and diversity of this content 
necessitate an approach that will reinvent 
the way it is analysed. The quantitative 
route to processing information which 
relies on content management tools 
provides structural analysis. The 
challenge we address is to evolve from 
the process of streamlining data to a level 
of understanding that assigns value to 
content. 
We present an open-source multilingual 
platform ATALS that incorporates 
human language technologies in the 
process of multilingual web content 
management. It complements a content 
management software-as-a-service 
component i-Publisher, used for creating, 
running and managing dynamic content-
driven websites with a linguistic 
platform. The platform enriches the 
content of these websites with revealing 
details and reduces the manual work of 
classification editors by automatically 
categorising content. The platform 
ASSET supports six European languages. 
We expect ASSET to serve as a basis for 
future development of deep analysis tools 
capable of generating abstractive 
summaries and training models for 
decision making systems. 
Introduction 
The advent of the Web revolutionized the way in 
which content is manipulated and delivered. As a 
result, digital content in various languages has 
become widely available on the Internet and its 
sheer volume and language diversity have 
presented an opportunity for embracing new 
methods and tools for content creation and 
distribution. Although significant improvements 
have been made in the field of web content 
management lately, there is still a growing 
demand for online content services that 
incorporate language-based technology. 
Existing software solutions and services such 
as Google Docs, Slingshot and Amazon 
implement some of the linguistic mechanisms 
addressed in the platform. The most used open-
source multilingual web content management 
6
  
systems (Joomla, Joom!Fish, TYPO3, Drupal)1 
offer low level of multilingual content 
management,   providing abilities for building 
multilingual sites. However, the available 
services are narrowly focused on meeting the 
needs of very specific target groups, thus leaving 
unmet the rising demand for a comprehensive 
solution for multilingual content management 
addressing the issues posed by the growing 
family of languages spoken within the EU. 
We are going to demonstrate the open-source 
content management platform ATLAS and as 
proof of concept, a multilingual library i-
librarian, driven by the platform. The 
demonstration aims to prove that people reading 
websites powered by ATLAS can easily find 
documents, kept in order via the automatic 
classification, find context-sensitive content, find 
similar documents in a massive multilingual data 
collection, and get short summaries in different 
languages that help the users to discern essential 
information with unparalleled clarity. 
The ?Technologies behind the system? chapter 
describes the implementation and the integration 
approach of the core linguistic processing 
framework and its key sub-components ? the 
categorisation, summarisation and machine-
translation engines. The chapter ?i-Librarian ? a 
case study? outlines the functionalities of an 
intelligent web application built with our system 
and the benefits of using it. The chapter 
?Evaluation? briefly discusses the user 
evaluation of the new system. The last chapter 
?Conclusion and Future Work? summarises the 
main achievements of the system and suggests 
improvements and extensions. 
Technologies behind the system 
The linguistic framework ASSET employs 
diverse natural language processing (NLP) tools 
technologically and linguistically in a platform, 
based on UIMA 2 . The UIMA pluggable 
component architecture and software framework 
are designed to analyse content and to structure 
it. The ATLAS core annotation schema, as a 
uniform representation model, normalizes and 
harmonizes the heterogeneous nature of the NLP 
tools3. 
                                                          
1 http://www.joomla.org/, http://www.joomfish.net/, 
http://typo3.org/, http://drupal.org/ 
2 http://uima.apache.org/ 
3 The system exploits heterogeneous NLP tools, for 
the supported natural languages, implemented in Java, 
C++ and Perl. Examples are: 
The processing of text in the system is split 
into three sequentially executed tasks. 
Firstly, the text is extracted from the input 
source (text or binary documents) in the ?pre-
processing? phase.  
Secondly, the text is annotated by several NLP 
tools, chained in a sequence in the ?processing? 
phase. The language processing tools are 
integrated in a language processing chain (LPC), 
so that the output of a given NLP tool is used as 
an input for the next tool in the chain. The 
baseline LPC for each of the supported languages 
includes a sentence and paragraph splitter, 
tokenizer, part of speech tagger, lemmatizer, 
word sense disambiguation, noun phrase chunker 
and named entity extractor (Cristea and Pistiol, 
2008). The annotations produced by each LPC 
along with additional statistical methods are 
subsequently used for detection of keywords and 
concepts, generation of summary of text, multi-
label text categorisation and machine translation.  
Finally, the annotations are stored in a fusion 
data store, comprising of relational database and 
high-performance Lucene4 indexes. 
The architecture of the language processing 
framework is depicted in Figure 1. 
 
 
 
Figure 1. Architecture and communication channels in 
our language processing framework. 
 
The system architecture, shown in Figure 2, is 
based on asynchronous message processing 
                                                                                        
OpenNLP (http://incubator.apache.org/opennlp/), 
RASP (http://ilexir.co.uk/applications/rasp/), 
Morfeusz (http://sgjp.pl/morfeusz/),  Panterra 
(http://code.google.com/p/pantera-tagger/), ParsEst 
(http://dcl.bas.bg/), TnT Tagger (http://www.coli.uni-
saarland.de/~thorsten/tnt/). 
4 http://lucene.apache.org/ 
7
  
patterns (Hohpe and Woolf, 2004) and thus 
allows the processing framework to be easily 
scaled horizontally. 
 
 
 
Figure 2. Top-level architecture of our CMS and its 
major components. 
Text Categorisation 
We implemented a language independent text 
categorisation tool, which works for user-defined 
and controlled classification hierarchies. The 
NLP framework converts the texts to a series of 
natural numbers, prior sending the texts to the 
categorisation engine. This conversion allows 
high level compression of the feature space. The 
categorisation engine employs different 
algorithms, such as Na?ve Bayesian, relative 
entropy, Class-Feature Centroid (CFC) (Guan et. 
al., 2009), and SVM. New algorithms can be 
easily integrated because of the chosen OSGi-
based architecture (OSGi Alliance, 2009). A 
tailored voting system for multi-label multi-class 
tasks consolidates the results of each of the 
categorisation algorithms. 
Summarisation (prototype phase) 
The chosen implementation approach for 
coherent text summarisation combines the well-
known LexRank algorithm (Erkan and Radev, 
2004) and semantic graphs and word-sense 
disambiguation techniques (Plaza and Diaz, 
2011). Furthermore, we have automatically built 
thesauri for the top-level domains in order to 
produce domain-focused extractive summaries. 
Finally, we apply clause-boundaries splitting in 
order to truncate the irrelevant or subordinating 
clauses in the sentences in the summary.  
Machine Translation (prototype phase) 
The machine translation (MT) sub-component 
implements the hybrid MT paradigm, combining 
an example-based (EBMT) component and a 
Moses-based statistical approach (SMT). Firstly, 
the input is processed by the example-based MT 
engine and if the whole or important chunks of it 
are found in the translation database, then the 
translation equivalents are used and if necessary 
combined (Gavrila, 2011). In all other cases the 
input is processed by the categorisation sub-
component in order to select the top-level 
domain and respectively, the most appropriate 
SMT domain- and POS-translation model 
(Niehues and Waibel, 2010). 
The translation engine in the system, based on 
MT Server Land (Federmann and Eisele, 2010),  
is able to accommodate and use different third 
party translation engines, such as the Google, 
Bing, Lusy or Yahoo translators. 
Case Study: Multilingual Library  
i-Librarian5  is a free online library that assists 
authors, students, young researchers, scholars, 
librarians and executives to easily create, 
organise and publish various types of documents 
in English, Bulgarian, German, Greek, Polish 
and Romanian. Currently, a sample of the 
publicly available library contains over 20 000 
books in English. 
On uploading a new document to i-Librarian, 
the system automatically provides the user with 
an extraction of the most relevant information 
(concepts and named entities, keywords). Later 
on, the retrieved information is used to generate 
suggestions for classification in the library 
catalogue, containing 86 categories, as well as a 
list of similar documents. Finally, the system 
compiles a summary and translates it in all 
supported languages. Among the supported 
formats are Microsoft Office documents, PDF, 
OpenOffice documents, books in various 
electronic formats, HTML pages and XML 
documents. Users have exclusive rights to 
manage content in the library at their discretion.   
The current version of the system supports 
English and Bulgarian. In early 2012 the Polish, 
Greek, German and Romanian languages will be 
in use. 
                                                          
5 i-Librarian web site is available at http://www.i-
librarian.eu/. One can access the i-Librarian demo content 
using ?demo@i-librarian.eu? for username and ?sandbox? 
for password. 
8
  
Evaluation 
The technical quality and performance of the 
system is being evaluated as well as its appraisal 
by prospective users. The technical evaluation 
uses indicators that assess the following key 
technical elements: 
? overall quality and performance 
attributes (MTBF6, uptime, response 
time); 
? performance of specific functional 
elements (content management, machine 
translation, cross-lingual content 
retrieval, summarisation, text 
categorisation).  
The user evaluation assesses the level of 
satisfaction with the system. We measure non 
functional elements such as: 
? User friendliness and satisfaction, clarity 
in responses and ease of use; 
? Adequacy and completeness of the 
provided data and functionality; 
? Impact on certain user activities and the 
degree of fulfilment of common tasks. 
We have planned for three rounds of user 
evaluation; all users are encouraged to try online 
the system, freely, or by following the provided 
base-line scenarios and accompanying exercises. 
The main instrument for collecting user feedback 
is an online interactive electronic questionnaire7. 
The second round of user evaluation is 
scheduled for Feb-March 2012, while the first 
round took place in Q1 2011, with the 
participation of 33 users. The overall user 
impression was positive and the Mean value of 
each indicator (in a 5-point Likert scale) was 
measured on AVERAGE or ABOVE 
AVERAGE.  
 
 
Figure 3. User evaluation ? UI friendliness and ease 
of use. 
                                                          
6 Mean Time Between Failures 
7 The electronic questionnaire is available at 
http://ue.atlasproject.eu 
 
Figure 4. User evaluation ? user satisfaction with the 
available functionalities in the system. 
 
 
Figure 5. User evaluation ? users productivity 
incensement. 
Acknowledgments 
ATLAS (Applied Technology for Language-
Aided CMS) is a European project funded under 
the CIP ICT Policy Support Programme, Grant 
Agreement 250467. 
Conclusion and Future Work 
The abundance of knowledge allows us to widen 
the application of NLP tools, developed in a 
research environment. The tailor made voting 
system maximizes the use of the different 
categorisation algorithms. The novel summary 
approach adopts state of the art techniques and 
the automatic translation is provided by a cutting 
edge hybrid machine translation system. 
The content management platform and the 
linguistic framework will be released as open-
source software. The language processing chains 
for Greek, Romanian, Polish and German will be 
fully implemented by the end of 2011. The 
summarisation engine and machine translation 
tools will be fully integrated in mid 2012. 
We expect this platform to serve as a basis for 
future development of tools that directly support 
decision making and situation awareness. We 
will use categorical and statistical analysis in 
order to recognise events and patterns, to detect 
opinions and predictions while processing 
The user interface is friendly and 
easy to use 
Excellent
28%
Good 
35%
Average
28%
Below 
Average
9%
Poor
Below Average
Average
Good 
Excellent
I am satisfied with the functionalities 
Below 
Average
3%
Average
38%
Excellent
31%
Good 
28%
Poor
Below
Average
Average
Good 
Excellent
The system increases y ur 
productivity 
Excellent
13%
Below 
Averag
9%
Average
31%
Good 
47%
Poor
Below
Average
Average
Good 
Excellent
9
  
extremely large volumes of disparate data 
resources. 
Demonstration websites 
The multilingual content management platform is 
available for testing at http://i-
publisher.atlasproject.eu/atlas/i-publisher/demo . 
One can access the CMS demo content using 
?demo? for username and ?sandbox2? for 
password. 
The multilingual library web site is available 
at http://www.i-librarian.eu/. One can access the 
i-Librarian demo content using ?demo@i-
librarian.eu? for username and ?sandbox? for 
password. 
References  
Dan Cristea and Ionut C. Pistol, 2008. Managing 
Language Resources and Tools using a Hierarchy 
of Annotation Schemas. In the proceedings of 
workshop 'Sustainability of Language Resources 
and Tools for Natural Language Processing', 
LREC, 2008 
Gregor Hohpe and Bobby Woolf. 2004. Enterprise 
Integration Patterns: Designing, Building, and 
Deploying Messaging Solutions. Addison-Wesley 
Professional. 
Hu Guan, Jingyu Zhou and Minyi Guo. A Class-
Feature-Centroid Classifier for Text 
Categorization. 2009. WWW 2009 Madrid, Track: 
Data Mining / Session: Learning, p201-210. 
OSGi Alliance. 2009. OSGi Service Platform, Core 
Specification, Release 4, Version 4.2. 
Gunes Erkan and Dragomir R. Radev. 2004. 
LexRank: Graph-based Centrality as Salience in 
Text Summarization. Journal of Artificial 
Intelligence Research 22 (2004), p457?479. 
Laura Plaza and Alberto Diaz. 2011. Using Semantic 
Graphs and Word Sense Disambiguation 
Techniques to Improve Text Summarization. 
Procesamiento del Lenguaje Natural, Revista n? 47 
septiembre de 2011 (SEPLN 2011), pp 97-105. 
Monica Gavrila. 2011. Constrained Recombination in 
an Example-based Machine Translation System, In 
the Proceedings of the EAMT-2011: the 15th 
Annual Conference of the European Association 
for Machine Translation, 30-31 May 2011, Leuven, 
Belgium, p. 193-200 
 Jan Niehues and Alex Waibel. 2010. Domain 
adaptation in statistical machine translation using 
factored translation models. EAMT 2010: 
Proceedings of the 14th Annual conference of the 
European Association for Machine Translation, 27-
28 May 2010, Saint-Rapha?l, France. 
Christian Federmann and Andreas Eisele. 2010. MT 
Server Land: An Open-Source MT Architecture. 
The Prague Bulletin of Mathematical Linguistics. 
NUMBER 94, 2010, p57?66 
10
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 42?47,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Machine Learning of Syntactic Attachment
from Morphosyntactic and Semantic Co-occurrence Statistics
Adam Slaski and Szymon Acedan?ski and Adam Przepi?rkowski
University of Warsaw
and
Institute of Computer Science
Polish Academy of Sciences
Abstract
The paper presents a novel approach to ex-
tracting dependency information in morpho-
logically rich languages using co-occurrence
statistics based not only on lexical forms
(as in previously described collocation-based
methods), but also on morphosyntactic and
wordnet-derived semantic properties of words.
Statistics generated from a corpus annotated
only at the morphosyntactic level are used
as features in a Machine Learning classifier
which is able to detect which heads of groups
found by a shallow parser are likely to be con-
nected by an edge in the complete parse tree.
The approach reaches the precision of 89%
and the recall of 65%, with an extra 6% re-
call, if only words present in the wordnet are
considered.
1 Introduction
The practical issue handled in this paper is how to
connect syntactic groups found by a shallow parser
into a possibly complete syntactic tree, i.e., how to
solve the attachment problem. To give a well-known
example from English, the task is to decide whether
in I shot an elephant in my pajamas1, the group in
my pajamas should be attached to an elephant or to
shot (or perhaps to I).
The standard approach to this problem relies on
finding collocation strengths between syntactic ob-
jects, usually between lexical items which are heads
of these objects, and resolve attachment ambigui-
ties on the basis of such collocation information.
1http://www.youtube.com/watch?v=NfN_
gcjGoJo
The current work extends this approach in two main
ways. First, we consider a very broad range of
features: not only lexical, but also lexico-semantic,
lexico-grammatical, and grammatical. Second, and
more importantly, we train classifiers based not on
these features directly, but rather on various associ-
ation measures calculated for each of the considered
features. This way the classifier selects which types
of features are important and which association mea-
sures are most informative for any feature type.
The proposed method is evaluated on Polish,
a language with rich inflection (and relatively free
word order), which exacerbates the usual data
sparseness problem in NLP.
In this work we assume that input texts are al-
ready part-of-speech tagged and chunked, the lat-
ter process resulting in the recognition of basic syn-
tactic groups. A syntactic group may, e.g., con-
sist of a verb with surrounding adverbs and particles
or a noun with its premodifiers. We assume that all
groups have a syntactic head and a semantic head. In
verbal and nominal groups both heads are the same
word, but in prepositional and numeral groups they
usually differ: the preposition and the numeral are
syntactic heads of the respective constituents, while
the semantic head is the head noun within the nomi-
nal group contained in these constituents.
To simplify some of the descriptions below, by
syntactic object we will understand either a shallow
group or a word. We will also uniformly talk about
syntactic and semantic heads of all syntactic objects;
in case of words, the word itself is its own syntac-
tic and semantic head. In effect, any syntactic ob-
ject may be represented by a pair of words (the two
42
heads), and each word is characterised by its base
form and its morphosyntactic tag.
2 Algorithm
The standard method of solving the PP-attachment
problem is based on collocation extraction (cf., e.g.,
(Hindle and Rooth, 1993)) and consists of three
main steps: first a training corpus is scanned and
frequencies of co-occurrences of pairs of words
(or more general: syntactic objects) are gathered;
then the collected data are normalised to obtain, for
each pair, the strength of their connection; finally,
information about such collocation strengths is em-
ployed to solve PP-attachment in new texts. An in-
stance of the PP-attachment problem is the choice
between two possible edges in a parse tree: (n1, pp)
and (n2, pp), where pp is the prepositional phrase,
and n1 and n2 are nodes in the tree (possible attach-
ment sites). This is solved by choosing the edge with
the node that has a stronger connection to the pp.
On this approach, collocations (defined as a rela-
tion between lexemes that co-occur more often than
would be expected by chance) are detected by taking
pairs of syntactic objects and only considering the
lemmata of their semantic heads. The natural ques-
tion is whether this could be generalised to other
properties of syntactic objects. In the following, the
term feature will refer to any properties of linguis-
tic objects taken into consideration in the process
of finding collocation strengths between pairs of ob-
jects.
2.1 Lexical and Morphosyntactic Features
To start with an example of a generalised colloca-
tion, let us consider morphosyntactic valence. In
order to extract valence links between two objects,
we should consider the lemma of one object (po-
tential predicate) and the morphosyntactic tag, in-
cluding the value of case, etc., of the other (potential
argument). This differs from standard (lexical) col-
location, where the same properties of both objects
are considered, namely, their lemmata.
Formally, we define a feature f to be a pair
of functions lf : so ? Lf and rf : so ? Rf , where
so stands for the set of syntactic objects and Lf , Rf
are the investigated properties. For example, to learn
dependencies between verbs and case values of their
objects, we can take lf (w) = base(semhead(w))
(the lemma of the semantic head of w) and rf (w) =
case(synhead(w)) (the case value of the syntactic
head of w). On the other hand, in order to obtain the
usual collocations, it is sufficient to take both func-
tions as mapping a syntactic object to a base form
of its semantic head.
What features should be considered in the task
of finding dependencies between syntactic objects?
The two features mentioned above, aimed at finding
lexical collocations and valence relations, are obvi-
ously useful. However, in a morphologically rich
language, like Polish, taking the full morphosyntac-
tic tag as the value of a feature function leads to
the data sparsity problem. Clearly, the most im-
portant valence information a tag may contribute is
part of speech and grammatical case. Hence, we
define the second function in the ?valence? feature
more precisely to be the base form and grammati-
cal case (if any), if the syntactic object is a preposi-
tion, or part of speech and grammatical case (if any),
otherwise. For example, consider the sentence Who
cares for the carers? and assume that it has already
been split into basic syntactic objects in the follow-
ing way: [Who] [cares] [for the carers] [?]. The syn-
tactic head of the third object is for and the lemma of
the semantic head is CARER. So, the valence feature
for the pair care and for the carers (both defined be-
low via their syntactic and semantic heads) will give:
lval (?CARE:verb, 3s; CARE:verb, 3s?) = CARE
rval (?FOR:prep, obj; CARER:noun, pl?) = ?FOR, obj?,
where 3s stands for the ?3rd person singular? prop-
erty of verbs and obj stands for the objective case in
English.
Additionally, 7 morphosyntactic features are de-
fined by projecting both syntactic objects onto any
(but the same of the two objects) combination
of grammatical case, gender and number. For exam-
ple one of those features is defined in the following
way:
lf (w) = rf (w) =
= ?case(synhead(w)), gender(synhead(w))?.
Another feature relates the two objects? syntactic
heads, by looking at the part of speech of the first
one and the case of the other one. The final feature
43
records information about syntactic (number, gen-
der, case) agreement between the objects.
2.2 Lexico-Semantic Features
Obviously, the semantics of syntactic objects is im-
portant in deciding which two objects are directly
connected in a syntactic tree. To this end, we utilise
a wordnet.
Ideally, we would like to represent a syntactic ob-
ject via its semantic class. In wordnets, semantic
classes are approximated by synsets (synonym sets)
which are ordered by the hyperonymy relation. We
could represent a syntactic object by its directly cor-
responding synset, but in terms of generalisation this
would hardly be an improvement over representing
such an object by its semantic head. In most cases
we need to represent the object by a hypernym of
its synset. But how far up should we go along the
hypernymy path to find a synset of the right granu-
larity? This is a difficult problem, so we leave it to
the classifier. Instead, lexico-semantic features are
defined in such a way that, for a given lexeme, all its
hypernyms are counted as observations.
After some experimentation, three features based
on this idea are defined:
1. lf (w) = base(semhead(w))
rf (w) = sset(w)
(for all sset(w) ? hypernyms(w)),
2. lf (w) = base(semhead(w))
rf (w) = ?sset(w), case(synhead(w))?
(for all sset(w) ? hypernyms(w)),
3. lf (w) = sset(w)
rf (w) = sset(w)
In the last feature, where both objects are repre-
sented by synsets, only those minimally general hy-
pernyms of the two objects are considered that co-
occur in the training corpus more than T (thresh-
old) times. In the experiments described below,
performed on a 1-million-word training corpus, the
threshold was set to 30.
2.3 Association Measures
For any two syntactic objects in the same sentence
the strength of association is computed between
them using each of the 14 features (standard col-
locations, 10 morphosyntactic features, 3 lexico-
semantic features) defined above. In fact, we use
not 1 but 6 association measures most suitable for
language analysis according to (Seretan, 2011): log
likehood ratio, chi-squared, t-score, z-score, point-
wise mutual information and raw frequency. The
last choice may seem disputable, but as was shown
in (Krenn and Evert, 2001) (and reported in vari-
ous works on valence acquisition), in some cases
raw frequency behaves better than more sophisti-
cated measures.
We are well aware that some of the employed
measures require the distribution of frequencies to
meet certain conditions that are not necessarily ful-
filled in the present case. However, as explained in
the following subsection, the decision which mea-
sures should ultimately be taken into account is left
to a classifier.
2.4 Classifiers
Let us first note that no treebank is needed for
computing the features and measures presented in
the previous section. These measures represent co-
occurrence strengths of syntactic objects based on
different grouping strategies (by lemma, by part
of speech, by case, gender, number, by wordnet
synsets, etc.). Any large, morphosyntactically an-
notated (and perhaps chunked) corpus is suitable for
computing such features. A treebank is only needed
to train a classifier which uses such measures as in-
put signals.2
In order to apply Machine Learning classifiers,
one must formally define what counts as an instance
of the classification problem. In the current case, for
each pair of syntactic objects in a sentence, a single
instance is generated with the following signals:
? absolute distance (in terms of the number of
sytnactic objects in between),
? ordering (the sign of the distance),
? 6 measures (see ? 2.3) of lexical collocation,
? 10 ? 6 = 60 values of morphosyntactic co-
occurrence measures,
? 3? 6 = 18 values of lexico-semantic (wordnet-
based) co-occurrence measures,
? a single binary signal based on 14 high-
precision low-recall handwritten syntactic de-
2We use the term signal instead of the more usual feature in
order to avoid confusion with features defined in ? 2.1 and in
? 2.2.
44
cision rules which define common grammati-
cal patterns like verb-subject agreement, geni-
tive construction, etc.; the rules look only at the
morphosyntactic tags of the heads of syntactic
objects,
? the classification target from the treebank: a bi-
nary signal describing whether the given pair of
syntactic objects form an edge in the parse tree.
The last signal is used for training the classifier and
then for evaluation. Note that lexical forms of the
compared syntactic objects or their heads are not
passed to the classifier, so the size of the training
treebank can be kept relatively small.
An inherent problem that needs to be addressed
is the imbalance between the sizes of two classifi-
cation categories. Of course, most of the pairs of
the syntactic objects do not form an edge in the
parse tree, so a relatively high classification accu-
racy may be achieved by the trivial classifier which
finds no edges at all. We experimented with various
well-known classifiers, such as decision trees, Sup-
port Vector Machines and clustering algorithms, and
also tried subsampling3 of the imbalanced data. Fi-
nally, satisfactory results were achieved by employ-
ing a Balanced Random Forest classifier.
Random Forest (Breiman, 2001) is a set of un-
pruned C4.5 (Quinlan, 1993) decision trees. When
building a single tree in the set, only a random subset
of all attributes is considered at each node and the
best is selected for splitting the data set. Balanced
Random Forest (BRF, (Chen et al, 2004)) is a mod-
ified version of the Random Forest. A single tree
of BRF is built by first randomly subsampling the
more frequent instances in the training set to match
the number of less frequent ones and then creating
a decision tree from this reduced data set.
3 Experiments and Evaluation
The approach presented above has been evaluated on
Polish.
First, a manually annotated 1-million-word
subcorpus of the National Corpus of Polish
(Przepi?rkowski et al, 2010), specifically, its mor-
phosyntactic and shallow syntactic annotation, was
3Removing enough negative instances in the training set to
balance the numbers of instances representing both classes.
used to compute the co-occurrence statistics. The
wordnet used for lexico-semantic measures was
S?owosiec? (Piasecki et al, 2009; Maziarz et al,
2012), the largest Polish wordnet.
Then a random subset of sentences from this cor-
pus was shallow-parsed by Spejd (Buczyn?ski and
Przepi?rkowski, 2009) and given to linguists, who
added annotation for the dependency links between
syntactic objects. Each sentence was processed by
two linguists, and in case of any discrepancy, the
sentence was simply rejected. The final corpus con-
tains 963 sentences comprising over 8000 tokens.
From this data we obtained over 23 500 classi-
fication problem instances. Then we performed
the classification using a BRF classifier written for
Weka (Witten and Frank, 2005) as part of the re-
search work on definition extraction with BRFs
(Kobylin?ski and Przepi?rkowski, 2008). The re-
sults were 10-fold cross-validated. A similar exper-
iment was performed taking into account only those
instances which describe syntactic objects with se-
mantic heads present in the wordnet. The results
were measured in terms of precision and recall over
edges in the syntactic tree: what percentage of found
edges are correct (precision) and what percentage of
correct edges were found by the algorithm (recall).
The obtained measures are presented in Table 1.
Expected
YES NO Classified
2674 319 YES
1781 21250 NO
Precision: 0.89
Recall: 0.60
F-measure: 0.72
Expected
YES NO Classified
1933 241 YES
1008 13041 NO
Precision: 0.89
Recall: 0.66
F-measure: 0.76
Table 1: Confusion matrix (# of instances) and measures
for the full data set and for data present in wordnet.
45
We also looked at the actual decision trees that
were generated during the training. We note that
the signal most frequently observed near the tops of
decision trees was the one from handwritten rules.
The second one was the distance. By looking at
the trees, we could not see any clear preferences for
other types of signals. This suggests that both mor-
phosyntactic and lexico-semantic signals contribute
to the accuracy of the classification.
Based on this inspection of decision trees, we per-
formed another experiment to learn how much im-
provement we get from generalised collocation sig-
nals. We evaluated ? on the same data ? a not so
trivial baseline algorithm which, for each syntactic
object, creates an edge to its nearest neighbour ac-
cepted by the handwritten rules, if any. Note that
this baseline builds on the fact that a node in a parse
tree has at most one parent, whereas the algorithm
described above does not encode this property, yet;
clearly, there is still some room for improvement.
The baseline reaches 0.78 precision and 0.47 recall
(F-measure is 0.59). Therefore, the improvement
from co-occurrence signals over this strong baseline
is 0.13, which is rather high. Also, given the high
precision, our algorithm may be suitable for using
in a cascade of classifiers.
4 Related Work
There is a plethora of relevant work on resolving PP-
attachment ambiguities in particular and finding de-
pendency links in general, and we cannot hope to do
it sufficient justice here.
One line of work, exemplified by the early influ-
ential paper (Hindle and Rooth, 1993), posits the
problem of PP-attachment as the problem of choos-
ing between a verb v and a noun n1 when attaching
a prepositional phrase defined by the syntactic head
p and the semantic head n2. Early work, including
(Hindle and Rooth, 1993), concentrated on lexical
associations, later also using wordnet information,
e.g., (Clark and Weir, 2000), in a way similar to
that described above. Let us note that this scenario
was criticised as unrealistic by (Atterer and Sch?tze,
2007), who argue that ?PP attachment should not
be evaluated in isolation, but instead as an integral
component of a parsing system, without using in-
formation from the gold-standard oracle?, as in the
approach proposed here.
Another rich thread of relevant research is con-
cerned with valence acquisition, where shallow
parsing and association measures based on mor-
phosyntactic features are often used at the stage
of collecting evidence, (Manning, 1993; Korhonen,
2002), also in work on Polish, (Przepi?rkowski,
2009). However, the aim in this task is the construc-
tion of a valence dictionary, rather than disambigua-
tion of attachment possibilities in a corpus.
A task more related to the current one is presented
in (Van Asch and Daelemans, 2009), where a PP-
attacher operates on top of a shallow parser. How-
ever, this memory-based module is fully trained on
a treebank (Penn Treebank, in this case) and is con-
cerned only with finding anchors for PPs, rather than
with linking any dependents to their heads.
Finally, much work has been devoted during the
last decade to probabilistic dependency parsing (see
(K?bler et al, 2009) for a good overview). Clas-
sifiers deciding whether ? at any stage of depen-
dency parsing ? to perform shift or reduce typically
rely on lexical and morphosyntactic, but not lexico-
semantic information (Nivre, 2006). Again, such
classifiers are fully trained on a treebank (converted
to parser configurations).
5 Conclusion
Treebanks are very expensive, morphosyntactically
annotated corpora are relatively cheap. The main
contribution of the current paper is a novel approach
to factoring out syntactic training in the process
of learning of syntactic attachment. All the fine-
grained lexical training data were collected from
a relatively large morphosyntactically annotated and
chunked corpus, and only less than 100 signals (al-
though many of them continuous) were used for
training the final classifier on a treebank. The ad-
vantage of this approach is that reasonable results
can be achieved on the basis of tiny treebanks (here,
less than 1000 sentences).
We are not aware of work fully analogous to ours,
either for Polish or for other languages, so we cannot
fully compare our results to the state of the art. The
comparison with a strong baseline algorithm which
uses handwritten rules shows a significant improve-
ment ? over 0.13 in terms of F-measure.
46
Acknowledgments
This research is supported by the POIG.01.01.02-
14-013/09 project which is co-financed by the Eu-
ropean Union under the European Regional Devel-
opment Fund.
References
Michaela Atterer and Hinrich Sch?tze. 2007. Preposi-
tional phrase attachment without oracles. Computa-
tional Linguistics, 33(4):469?476.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45:5?32.
Aleksander Buczyn?ski and Adam Przepi?rkowski. 2009.
Spejd: A shallow processing and morphological dis-
ambiguation tool. In Zygmunt Vetulani and Hans
Uszkoreit, editors, Human Language Technology:
Challenges of the Information Society, volume 5603
of Lecture Notes in Artificial Intelligence, pages 131?
141. Springer-Verlag, Berlin.
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Us-
ing random forest to learn imbalanced data. Technical
Report 666, University of California, Berkeley.
Stephen Clark and David Weir. 2000. A class-based
probabilistic approach to structural disambiguation. In
In Proceedings of the 18th International Conference
on Computational Linguistics, pages 194?200.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
?ukasz Kobylin?ski and Adam Przepi?rkowski. 2008.
Definition extraction with balanced random forests.
In Bengt Nordstr?m and Aarne Ranta, editors, Ad-
vances in Natural Language Processing: GoTAL 2008,
Gothenburg, Sweden, volume 5221 of Lecture Notes
in Artificial Intelligence, pages 237?247, Berlin.
Springer-Verlag.
Anna Korhonen. 2002. Subcategorization Acquisition.
PhD Thesis, University of Cambridge.
Brigitte Krenn and Stefan Evert. 2001. Can we do better
than frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Workshop on
Collocations, Toulouse, France.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool.
Christopher D. Manning. 1993. Automatic acquisition of
a large subcategorization dictionary from corpora. In
Proceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 235?242,
Columbus, OH.
Marek Maziarz, Maciej Piasecki, and Stanis?aw Sz-
pakowicz. 2012. Approaching plWordNet 2.0. In
Proceedings of the 6th Global Wordnet Conference,
Matsue, Japan.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer-Verlag, Berlin.
Maciej Piasecki, Stanis?aw Szpakowicz, and Bartosz
Broda. 2009. A Wordnet from the Ground
Up. Oficyna Wydawnicza Politechniki Wroclawskiej,
Wroc?aw.
Adam Przepi?rkowski, Rafa? L. G?rski, Marek ?azin?ski,
and Piotr Pe?zik. 2010. Recent developments in the
National Corpus of Polish. In Proceedings of the Sev-
enth International Conference on Language Resources
and Evaluation, LREC 2010, Valletta, Malta. ELRA.
Adam Przepi?rkowski. 2009. Towards the automatic ac-
quisition of a valence dictionary for Polish. In Ma?-
gorzata Marciniak and Agnieszka Mykowiecka, edi-
tors, Aspects of Natural Language Processing, volume
5070 of Lecture Notes in Computer Science, pages
191?210. Springer-Verlag, Berlin.
John Ross Quinlan. 1993. C4.5 Programs for Machine
Learning. Morgan Kaufmann.
Violeta Seretan. 2011. Syntax-Based Collocation Ex-
traction. Text, Speech and Language Technology.
Springer, Dordrecht.
Vincent Van Asch and Walter Daelemans. 2009. Prepo-
sitional phrase attachment in shallow parsing. In
Proceedings of the International Conference RANLP-
2009, pages 12?17, Borovets, Bulgaria, September.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, CA, 2nd edition.
47
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
