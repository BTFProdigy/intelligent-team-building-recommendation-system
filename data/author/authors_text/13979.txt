An Algorithm for Anaphora Resolution in 
Spanish Texts 
Manuel Palomar* 
University of Alicante 
Lidia Moreno t
Valencia University of Technology 
Jesfis Peral* 
University of Alicante 
Rafael Mufioz* 
University of Alicante 
Antonio Ferr~indez* 
University of Alicante 
Patricio Martinez-Barco* 
University of Alicante 
Maximiliano Saiz-Noeda* 
University of Alicante 
This paper presents an algorithm for identifying noun phrase antecedents ofthird person personal 
pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) 
in unrestricted Spanish texts. We define a list of constraints and preferences for different ypes 
of pronominal expressions, and we document in detail the importance of each kind of knowledge 
(lexical, morphological, syntactic, and statistical) in anaphora resolution for Spanish. The paper 
also provides a definition for syntactic onditions on Spanish NP-pronoun oncoreference using 
partial parsing. The algorithm has been evaluated on a corpus of 1,677 pronouns and achieved 
a success rate of 76.8%. We have also implemented four competitive algorithms and tested their 
performance in a blind evaluation on the same test corpus. This new approach could easily be 
extended to other languages uch as English, Portuguese, Italian, or Japanese. 
1. Introduction 
We present an algorithm for identifying noun phrase antecedents of personal pro- 
nouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pro- 
nouns) in Spanish. The algorithm identifies both intrasentential and intersentential 
antecedents and is applied to the syntactic analysis generated by the slot unifica- 
t ion parser (SUP) (Ferr~ndez, Palomar, and Moreno 1998b). It also combines different 
forms of knowledge by distinguishing between constraints and preferences. Whereas 
constraints are used as combinations of several kinds of knowledge (lexical, mor- 
phological, and syntactic), preferences are defined as a combination of heuristic rules 
extracted from a study of different corpora. 
We present he following main contributions in this paper: 
? an algorithm for anaphora resolution in Spanish texts that uses different 
kinds of knowledge 
* Department of Software and Computing Systems, Alicante, Spain. E-mail: (Palomar) 
mpalomar@dlsi.ua.es, (F rr~ndez) antonio@dlsi.ua.es, (Martfnez-Barco) patricio@dlsi.ua.es, (Peral) 
jperal@dlsi.ua.es, (Saiz-Noeda) max@dlsi.ua.es, (Mufioz) rafael@dlsi.ua.es 
t Department of Information Systems and Computation, Valencia, Spain. E-mail: hnoreno@dsic.upv.es 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
? an exhaustive study of the importance of each kind of knowledge in 
Spanish anaphora resolution 
? a proposal concerning syntactic onditions on NP-pronoun 
noncoreference in Spanish that can be evaluated on a partial parse tree 
? a proposal regarding preferences that are appropriate for resolving 
anaphora in Spanish and that could easily be extended to other 
languages 
? a blind test of the algorithm 
? a comparison with other approaches to anaphora resolution that we have 
applied to Spanish texts using the same blind test 
In Section 2, we show the classification scheme we used to identify the different ypes 
of anaphora that we would be resolving. In Section 3, we present he algorithm and 
discuss its main properties. In Section 4, we evaluate the algorithm. In Section 5, we 
compare our algorithm with several other approaches to anaphora resolution. Finally, 
we present our conclusions. 
2. Our Classification Scheme for Pronominal Expressions in Spanish 
In this section, we present our classification scheme for identifying the different ypes 
of anaphora that we will be resolving. Personal pronouns (PPR), demonstrative pro- 
nouns (DPR), reflexive pronouns (RPR), and omitted pronouns (OPa) are some of the 
most frequent ypes of anaphoric expressions found in Spanish and are the main 
subject of this study. Personal and demonstrative pronouns are further classified ac- 
cording to whether they appear within a prepositional phrase (PP) or whether they 
are complement personal pronouns (clitic pronouns1). We present examples for each 
of the four types of common anaphora. Each example is presented in three forms: as a 
Spanish sentence, as a word-to-word translation into English, and correctly translated 
into English. 2
2.1 Clitic Personal Pronouns (CPPR) 
In the case of clitic personal pronouns, I0, la, le 'him, her, it' and los, las, les 'them', we 
consider that the third person personal pronoun plays the role of the complement. 
(1) Ana abre \[la verja\]i y lai cierra tras de si. 
Ana opens \[the gate\]/ and it/ closes after herself 
'Ana opens the gate and closes it after herself.' 
2.2 Personal Pronouns Not Included in a PP (PPanotPP) 
We include in this class the personal pronouns ~l, ella, ello 'he, she, it' and ellas, ellos 
'they'. 
(2) Andr6si es mi vecino, t~li vive en el segundo piso. 
Andr6si is my neighbor Hei lives on the second floor 
'Andr6s is my neighbor. He lives on the second floor.' 
1 According to Mathews (1997), aclitic pronoun is a pronoun that is treated as an independent word in 
syntax but that forms a phonological unit with the verb that precedes or follows it. 
2 Coindexing indicates coreference b tween anaphor and antecedent. 
546 
Palomar et al Anaphora Resolution in Spanish Texts 
2.3 Personal Pronouns Included in a PP (PPRinPP) 
We include in this class the personal pronouns dl, ella, ello 'him, her, it' and ellas, ellos 
'them'. 
(3) Juan/ debe asistir pero Pedro lo har~i por 61i. 
Juani must attend but Pedro it will do for himi 
'Juan must attend but Pedro will do it for him.' 
2.4 Demonstrative Pronouns Not Included in a PP (DPRnotPP) 
We include in this class the demonstrative pronouns ~ste, dsta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqu~l, aqu~lla 'that'; and dsos, dsas, aqu~llos, aqudllas 'those'. 
(4) E1 Ferrarii gan6 al Ford. t~stei es el mejor. 
the Ferrarii beat the Ford This/ is the best 
'The Ferrari beat the Ford. This is the best.' 
2.5 Demonstrative Pronouns Included in a PP (DPRinPP) 
We include in this class the demonstrative pronouns ~ste, ~sta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqudl, aqudlla 'that'; and dsos, ~sas, aqu~llos, aqudllas 'those'. 
(5) Ana vive con Pacoi y cocina para 6stei diariamente. 
Ana lives with Pacoi and cooks for this/ every day 
'Ana lives with Paco and cooks for him every day.' 
2.6 Reflexive Pronouns (RPR) 
We include in this class the reflexive pronouns e, sL si mismo 'himself, herself, itself' 
and consigo, consigo mismo 'themselves'. 
(6) Anai abre la verja y la cierra tras de sfi. 
Anai opens the gate and it closes after herself/ 
'Ana opens the gate and closes it after herself.' 
2.7 Omitted Pronouns (Zero Pronouns OPa) 
The omitted pronoun is the most frequent ype of anaphoric expression in Spanish, as 
we will show in Section 4.2. Omitted pronouns occur when the pronominal subject is 
omitted. This kind of pronoun also occurs in other languages, such as Portuguese or 
Japanese; in these languages, it can also appear in object position, whereas in Spanish 
or Italian, it can appear only in subject position. In the following example, the omission 
is represented by the symbol 13 (the symbol does not appear in the correct ranslation 
into English). 
(7) Anai abre la verja y (~i la cierra tras de sf. 
Anai opens the gate and Oi it closes after herself 
'Ana opens the gate and she closes it after herself.' 
3. Anaphora Resolution Algorithm 
In the algorithm, all the types of anaphora are identified from left to right as they 
appear in the sentence. The most important proposals for anaphora resolution--such 
as those of Baldwin (1997), Lappin and Leass (1994), Hobbs (1978), or Kennedy and 
Boguraev (1996)--are based on a separation between constraints and preferences. 
547 
Computational Linguistics Volume 27, Number 4 
Constraints discard some of the candidates, whereas preferences simply sort the re- 
maining candidates. A constraint defines a property that must be satisfied in order 
for any candidate to be considered as a possible solution of the anaphor. For example, 
pronominal anaphors and antecedents must agree in person, gender, and number. 3 
Otherwise, the candidate is discarded as a possible solution. A preference is a charac- 
teristic that is not always satisfied by the solution of an anaphor. The application of 
preferences usually involves the use of heuristic rules in order to obtain a ranked list 
of candidates. 
Each type of anaphora has its own set of constraints and preferences, although 
they all follow the same general algorithm: constraints are applied first, followed by 
preferences. 
Based on the preceding description, our algorithm contains the following main 
components: 
? identification of the type of pronoun 
? constraints 
- -  morphological greement (person, gender, and number) 
- -  syntactic onditions on NP-pronoun oncoreference 
? preferences 
In order to apply this algorithm to unrestricted texts, it has been necessary to use 
partial parsing. In our partial-parsing scheme, as presented in Ferr~ndez, Palomar, and 
Moreno (1999), we only parse coordinated NPs and PPs, verbal chunks, pronouns, and 
what we have called free conjunctions (i.e., conjunctions that do not join coordinated 
NPs or PPs). Words that do not appear within these constituents are simply ignored. 
The NP constituents include coordinated adjectives, relative clauses, coordinated PPs, 
and appositives as modifiers. 
With this partial-parsing scheme, we divide a sentence into clauses by parsing first 
the free conjunction and then the verbs, as in the following example: 
(8) Pedro compr6 un regalo y se lo dio a Ana. 
Pedro bought a gift and her it gave to Ana 
'Pedro bought a gift and gave it to Ana.' 
In this example, we have parsed the following constituents: np(Pedro), v(comprO), np(un 
regalo),freeconj(y), pron(se), pron(lo), v(dio), pp(a Ana). We are able to divide this sentence 
into two clauses because it contains the free conjunction y 'and' and the two verbs 
compr6 'bought' and clio 'gave'. 
3.1 Identification of the Kind of Pronoun 
The algorithm uses partial-parse trees to automatically identify omitted pronouns by 
employing the following steps: 
? The sentence is divided into clauses (by parsing the free conjunction 
followed by the verbs). 
3 In our implementation, thismorphological information is extracted from the part-of-speech tagger. 
548 
Palomar et al Anaphora Resolution in Spanish Texts 
An NP or pronoun is sought for each clause by analyzing the clause 
constituents on the left-hand side of the verb, unless the verb is 
imperative or impersonal. The chosen NP or pronoun must agree in 
person and number with the clausal verb. (In evaluating this algorithm, 
Ferr~ndez and Peral \[2000\] achieved a success rate of 88% for detecting 
omitted pronouns.) 
The remaining pronouns are identified based on part-of-speech (POS) tagger out- 
puts. 
3.2 Morphological Agreement 
Person, gender, and number agreement are checked in order to discard potential an- 
tecedents. For example, in the sentence 
(9) Juanj vio a Rosa/. Ella/ estaba muy feliz. 
Juanj saw to Rosa/ Shei was very happy 
'Juan saw Rosa. She was very happy.' 
there are two possible antecedents for ella 'she', whose slot structures 4 are 
np (conc (sing, masc), X, Juan) 
np (conc (sing, fem), Y, Rosa) 
whereas the slot structure of the pronoun is 
pron (conc (sing, fem), Z, ella). 
In order to decide between the two antecedents, he unification of both slot struc- 
tures (pronoun and candidate) is carried out by the slot unification parser (Ferr~ndez, 
Palomar, and Moreno 1999). In this example, the candidate Juan is rejected by this 
morphological greement constraint. 
3.3 Syntactic Conditions on NP-Pronoun Noncoreference 
These conditions are based on c-command and minimal-governing-category constraints 
as formulated by Reinhart (1983) and on the noncoreference conditions of Lappin and 
Leass (1994). They are of great importance in any anaphora resolution system that 
does not use semantic information, as is the case with our proposal. In such systems, 
recency is important in selecting the antecedent of an anaphor. That is to say, the 
closest NP to the anaphor has a better chance of being selected as the solution. One 
problem, however, is that such constraints are formulated using full parsing, whereas 
if we want to work with unrestricted texts we should be using partial parsing, as 
previously defined. 
We have therefore proposed a set of noncoreference onditions for Spanish, using 
partial parsing, although they could easily be extended to other languages such as En- 
glish. In our system, the following types of pronouns are noncoreferential with a noun 
phrase (NP) under the conditions noted (noncoindexing indicates that a candidate is
rejected by these conditions). 
4 The term slot structure is defined in Ferr~ndez, Palomar, and Moreno (1998b). The slot structure stores 
morphological and syntactic information related to the different constituents of a sentence. 
549 
Computational Linguistics Volume 27, Number 4 
. 
(a) 
(b) 
(c) 
. 
(a) 
Reflexive pronouns are noncoreferential when: 
(b) 
(10) 
the NP is included in another constituent (e.g., the NP is 
included in a PP) 
Ante Luisj sei frot6 con la toalla. 
in front of Luisj himself/ rubbed with the towel 
'He rubbed himself with the towel in front of Luis.' 
In this sentence, we would have obtained the following sequence 
of constituents after our partial-parsing scheme: pp(prep(ante), 
np(Luis )) , pron(se) , v(frot6 ) , pp(prep( con) , np(la toalla) . Following 
the above-stated condition, the NP Luis cannot corefer with the 
reflexive pronoun se since Luis is included in a PP (ante Luis). 
the NP is in a different clause or sentence 
(11) Anaj trajo un cuchillo y Eva/ sei cort6. 
Anaj brought a knife and Eva/ herself/ cut 
'Ana brought a knife and Eva cut herself.' 
the NP appears after the verb and there is another NP in the 
same clause before the verb 
(12) 
(13) 
Juan/ sei cort6 con el cuchilloj. 
Juan/ himself/ cut with the knifej 
'Juan cut himself with the knife.' 
Under these conditions, coreference is allowed between the NP 
and the reflexive pronoun, since both are in the same clause. For 
example: 
Juan/ queria verlo por s~ mismoi. 
Juan/ wanted see it for himself/ 
'Juan wanted to see it for himself.' 
In this example, Juan and the reflexive pronoun si mismo 
'himself' corefer since Juan is in the same clause as the anaphor, 
it is not included in another constituent, and it appears before 
the verb. 
Clitic pronouns are noncoreferential when: 
the NP is included in a PP (except hose headed by the 
preposition a 'to') 
(14) Con Juan/ loj compr6. 
with Juan/ itj bought 
'I bought it with Juan.' 
the NP is located more than three constituents before the clitic 
pronoun in the same clause 
(15) En casai \[el martillo\]j no se loj di. 
at home/ \[the hammer\]j not him itj gave 
'I didn't give him the hammer at home.' 
550 
Palomar et al Anaphora Resolution in Spanish Texts 
. 
(a) 
(17) 
(b) 
In this example, the direct object el martillo 'the hammer '  has 
been moved from its common position after the verb, and it is 
necessary to fill the resulting gap with the pronoun lo 'it' even 
though it does not appear in the English translation. This 
phenomenon 5 can be considered an exception to the c-command 
constraints as formulated by Reinhart when applied to Spanish 
clitic pronouns. 
Moreover, if the last two conditions are not fulfilled by the NP and the 
verb is in the first or second person, then this NP will necessarily be the 
solution of the pronoun: 
(16) \[El boligrafo\]i 1Oi comprar~s en esa tienda. 
\[The pen\]/ iti will buy in that shop 
'You will buy the pen in that shop.' 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is in the same clause as the anaphor, and: 
the pronoun comes before the verb (in full parsing, this would 
mean that it is the subject of its clause) 
Ante Luisi 61j salud6 a Pedrok. 
in front of Luisi hey greeted to Pedrok 
'He greeted Pedro in front of Luis.' 
the pronoun comes after the verb (in full parsing, this would 
mean that it is the object of the verb) and the NP is not included 
in another NP 
(18) \[El padre de Juanj\]i le venci6 a 41j. 
\[Juanj's father\]/ him beat to himj 
'Juan's father beat him.' 
In this example, the pronoun ~I 'him' cannot corefer with the NP 
el padre de Juan 'Juan's father', but it can corefer with Juan since it 
is a modifier of the NP el padre de Juan. 
It should be mentioned that the clitic pronoun le is another 
form of the pronoun dl 'him'. This is a typical phenomenon i
Spanish, where clitic pronouns occupy the object position. 
Sometimes both the clitic pronoun and the object appear in the 
same clause, as occurs in the previous example and in the 
following one: 
(19) A Pedro/ yo lei vi ayer. 
to Pedroj I himi saw yesterday 
'I saw Pedro yesterday.' 
This example also illustrates the previously mentioned exception 
of c-command constraints for Spanish clitic pronouns. In this 
case, the direct object a Pedro 'to Pedro' has been moved before 
the verb, and the clitic pronoun le 'him' has been added. It 
should also be remarked that, as noted earlier, the clitic pronoun 
does not appear in the English translation. 
5 Mathews (1997) calls this phenomenon "clitic doubling" and defines it as the use of a clitic pronoun 
with the same referent and in the same syntactic function as another element in the same clause. 
551 
Computational Linguistics Volume 27, Number 4 
(c) the pronoun is included in a PP that is not included in another 
constituent and the NP is not included in another constituent 
(NP or PP) 
(20) \[El padre de Luisj\]i juega con 61j. 
\[Luisj's father\]/ plays with himj 
'Luis's father plays with him.' 
In this example, the pronoun ~I 'him' is included in a PP (which 
is not included in another constituent) and the NP el padre de 
Luis is not included in another NP or PP. Therefore, the NP 
cannot corefer with the pronoun. However, the NP Luis can 
corefer because it is included in the NP el padre de Luis. 
(d) the pronoun is included in an NP, so that the NP in which the 
pronoun is included cannot corefer with the pronoun 
(21) Pedro/ vio \[al hermano de ~li\] j. 
Pedro/ saw \[the brother of himi\]j 
'Pedro saw his brother.' 
(e) the pronoun is coordinated with other NPs, so that the other 
coordinated NPs cannot corefer with the pronoun 
(22) Juan/, \[el tio de Ana\]j, y 61k fueron de pesca. 
Juan/, \[Ana's uncle\]j, and hek went fishing 
'He, Juan, and Ana's uncle went fishing.' 
(f) the pronoun is included in a relative clause, and the following 
condition is met: 
. 
(24) 
i. the NP in which the relative clause is included does not 
corefer with the pronoun 
(23) Pedroj vio a \[un amigo que juega con 41j\]i. 
Pedroj saw to \[a friend that plays with himj\]i 
'Pedro saw a friend that he plays with.' 
ii. the NPs that are included in the relative clause follow 
the previous conditions 
iii. the remaining NPs outside the relative clause could 
corefer with the pronoun 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is not in the same clause as the pronoun. (In this case, the 
NP can corefer with the pronoun, except when this NP also appears in 
the same sentence and clause as the pronoun, in which case it will have 
been discarded by the previous noncoreference onditions.) 
Anaj y Evai son amigas. Evai lej ayuda mucho. 
Anay and Evai are friends Evai herj helps a lot 
'Ana and Eva are friends. Eva helps her a lot.' 
It is important o note that the above-mentioned conditions refer to those coor- 
dinated NPs and PPs that have been partially parsed. Moreover, as previously men- 
tioned, NPs can include relative clauses, appositives, coordinated PPs, and adjectives. 
552 
Palomar et al Anaphora Resolution in Spanish Texts 
We should also remark that we consider aconstituent A to be included in a constituent 
B if A modifies the head of B. Let us consider the following NP: 
(25) \[el hombre que ama a \[una mujer que lei ama\]j\]i 
\[the man who loves to \[a woman who him/ loveslj\]i 
'the man who loves a woman who loves him.' 
We consider that the pronoun le 'him' is included in the relative clause that mod- 
ifies the NP una mujer que le ama 'a woman who loves him', which then cannot corefer 
with it due to noncoreference ondition 3(f)i. Under condition 3(f)iii, however, the 
pronoun le 'him' could corefer with the entire NP el hombre que area a una mujer que le 
area 'the man who loves a woman who loves him'. 
Another example might be the following: 
(26) Eva/ tiene \[un tio que lei toma el pelo\]j. 
Evai has \[an uncle that heri teases\]j 
'Eva has an uncle who teases her.' 
In this example, the pronoun is included within the relative clause that modifies un 
tio 'an uncle', and therefore cannot corefer with it. But, following condition 3(f)iii, it 
can corefer with Eva. 
3.4 Preferences 
To obtain the different sets of preferences, we utilized the training corpus to identify 
the importance of each kind of knowledge that is used by humans when tracking 
down the NP antecedent of a pronoun. Our results are shown in Table 1. For our 
analysis, the antecedents for each pronoun in the text were identified, along with their 
configurational characteristics with reference to the pronoun. Thus, the table shows 
how often each configurational characteristic is valid for the solution of a particular 
pronoun. For example, the solution of a reflexive pronoun is a proper noun 53% of the 
time. The total number of pronoun occurrences in the study was 575. Thus, we were 
able to define the different patterns of Spanish pronoun resolution and apply them in 
order to obtain the evaluation results that are presented in this paper. The order of 
importance was determined by first sorting the preferences according to the percentage 
of each configurational characteristic; that is, preferences with higher percentages were 
applied before those with lower percentages. After several experiments on the training 
corpus, an optimal order--the one that produced the best performance--was obtained. 
Since in this evaluation phase we processed texts from different genres and by different 
authors, we can state that the final set of preferences obtained and their order of 
application can be used with confidence on any Spanish text. 
Based on the results presented in Table 1, we have extracted a set of preferences for 
each type of anaphora (listed below). We have distinguished between those pronouns 
that are included within PPs and those that are not. That is because when a pronoun 
is included in a PP, the preposition of this PP sets a preference. 
Preferences of omitted pronouns (OPR): 
1. NPs that are not of time, direction, quantity, or abstract type; that is to 
say, inanimate candidates are rejected (e.g., hal~past en, Market Street, 
three pounds, or a thing) 
2. NPs in the same sentence as the omitted pronotm 
553 
Computat ional  Linguistics Volume 27, Number  4 
Table 1 
Percentage validity of types of pronouns for different configuration characteristics of the 
training corpus (n = 575). 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Intrasentential 66 97 57 70 100 60 75 
Intersentential 34 3 43 30 0 40 25 
NPSentAnt ~ 9 3 4 16 50 9 38 
AntPPin b 7 9 14 27 50 20 25 
AntProper c 57 53 63 35 0 43 0 
AntIndef a 13 0 7 0 0 6 13 
AntRepeaff 72 66 79 65 50 71 50 
AntWithVerb f 14 94 20 24 0 26 25 
EqualPP g 100 100 100 78 100 97 100 
EqualPosVerb h 79 84 89 46 0 86 38 
BeforeVerb i 83 91 89 65 50 86 13 
NoTime d 100 100 100 100 100 100 100 
NoQuant i ty  k 100 100 100 100 100 97 100 
NoDirect ion I 100 100 100 97 100 100 100 
NoAbstract m 100 100 100 100 100 100 100 
NoCompany n 100 100 100 100 100 100 100 
a If the NP 
b If the NP 
c If the NP 
d If the NP 
e If the NP 
f If the NP 
g If the NP 
h If the NP 
i If the NP 
j If the NP 
k If the NP 
1 If the NP 
m If the NP 
n If the NP 
is included in another NP 
is included in a PP with the preposition en 'in' 
is a proper noun 
is an indefinite NP 
has been repeated more than once in the text 
has appeared with the verb of the anaphor more than once in the text 
has appeared in a PP more than once in the text 
occupies the same position with reference to the verb as the anaphor (before or after) 
appears before its verb 
is not a time-type 
is not a quantity-type 
is not a direction-type 
is not an abstract-type 
is not a company-type 
3. NPs  that  are in the same sentence  as the  anaphor  and  are also the  
so lu t ion  for  another  omi t ted  pronotm 
4. NPs  that  are in the prev ious  sentence  
5. NPs  that  are not  inc luded  in another  NP  (e.g., when they  appear  ins ide  
a re la t ive  c lause  or  appos i t i ve )  
6. NPs  that  are not  inc luded  in a PP or  are  inc luded  in a PP  when its 
p repos i t ion  is a ' to '  or  de ' o f '  
7. NPs  that  appear  be fore  the  verb  
8. NPs  that  have  been  repeated  more  than  once  in the  text  
Preferences of clitic personal pronouns (CPPR): 
1. NPs  that  are not  of  t ime,  d i rect ion ,  quant i ty ,  or  abst ract  type  
2. NPs  that  are in  the  same sentence  as the  anaphor  
554 
Palomar et al Anaphora Resolution in Spanish Texts 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a clause or appositive) 
5. NPs that are not included in a PP or are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. NPs that have appeared with the verb of the anaphor more than once 
Preferences of personal and demonstrative pronouns that are included in a PP 
(PPRinPP and DPRinPP): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that have been repeated more than once in the text 
6. NPs that are included in a PP 
7. NPs that occupy the same position (before or after) with respect o the 
verb as the anaphor 
Preferences of personal and demonstrative pronouns that are not included in a PP 
and of reflexive pronouns (PPRnotPP, DPRnotPP, and RPR): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that are not included in a PP or that are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. For the case of personal pronouns (PPRnotPP), NPs that are not 
included in a PP with the preposit ion en ' in' 
7. NPs that appear before their verbs (i.e., the verb of the sentence in 
which the NP appears) 
3.5 Resolution Procedure 
The resolution procedure consists of the following steps: 
1. Identify the type of anaphora: pronominal  (PPRinPP or PPRnotPP), 
demonstrat ive (DPRinPP or DPRnotPP), reflexive (RPR), or omitted 
(oPR). 
555 
Computational Linguistics Volume 27, Number 4 
2. Identify the NP candidate antecedents of a pronoun in order to create a 
list L. The list created will depend on the type of anaphor and the 
anaphoric accessibility space (empirically obtained from a deep study 
of the training corpus) and will be developed according to the 
following criteria: 
? For pronominal anaphora, demonstrative anaphora, and 
omitted pronouns, NP candidates will appear in the same 
sentence as the anaphor and in the four previous sentences. 
? For reflexive anaphora, NP candidates will appear in the same 
sentence as the anaphor. 
3. Apply constraints to L to obtain LI: 
(a) morphological agreement 
(b) syntactic onditions on NP-pronoun noncoreference 
4. If the number of elements of L1 - 1, then the solution is that element. 
5. If the number of elements of L1 = 0, then the solution is an exophor. 
6. If the number of elements of L1 > 1, then apply preferences to L1 to 
obtain L2. Depending on the type of anaphora, a different set and order 
of preferences will be applied (see Section 3.4). 
7. If the number of elements of L2 = 1, then the solution is that element. 
8. If the number of elements of L2 > 1, then apply the following three 
basic preferences in the order shown until only one candidate remains 
(these three preferences are common to all the pronouns): 
? NPs most repeated in the text 
? NPs that have appeared most with the verb of the anaphor 
? the first candidate of the remaining list (the closest one to the 
anaphor) 
After applying these basic preferences, the antecedent is obtained. 
4. Empirical Evaluation 
4.1 Description of Corpora 
We have tested the algorithm on both technical manuals and literary texts. In the first 
instance, we used a portion of the Spanish edition of the Blue Book corpus. 6 This 
corpus contains the handbook of the International Telecommunications Union CCITT, 
published in English, French, and Spanish; it is one of the most important collections of 
telecommunications texts available and contains 5,000,000 words automatically tagged 
by the Xerox tagger. In the second instance, the algorithm was tested on Lexesp, a 
corpus 7 that contains Spanish literary texts from different genres and by different 
6 CRATER (Proyecto CRATER 1994-1995) Corpus Resources and Terminology Extraction Project. Project 
supported by the European Community Commission (DG-XIII). Computational Linguistics Laboratory, 
Faculty of Philosophy and Fine Arts, Autonomous University of Madrid, Spain. 
7 The Lexesp corpus belongs to the project of the same name carried out by the Psychology Department 
of the University of Oviedo and developed by the Computational Linguistics Group of the University 
of Barcelona, with the collaboration fthe Language Processing Group of the Catalonia University of 
Technology, Spain. 
556 
Palomar et al Anaphora Resolution in Spanish Texts 
Table 2 
Pronoun occurrences in two types of texts. 
Total BB Corpus Lexesp Corpus 
Number of pronoun occurrences 
in the training corpus 575 123 
Number of pronoun occurrences 
in the test corpus 1,677 375 
452 
1,302 
authors. These texts were mainly obtained from newspapers and were automatically 
tagged by a different agger than the one used to tag the Blue Book. The portion of 
the Lexesp corpus that we processed contained various stories, related by a narrator, 
and written by different authors. As was the case for the Blue Book corpus, this 
corpus also contained 5,000,000 words. Since we worked on texts from different genres 
and by different authors, the applicability of our proposal to other kinds of texts is 
assured. 
We selected a subset of the Blue Book corpus and another subset of the Lex- 
esp corpus, and both were annotated with respect o coreference. One portion of the 
coreferentially tagged corpus (training corpus) was used for improving the rules for 
anaphora resolution (constraints and preferences), and another portion was reserved 
for test data (Table 2). 
The annotation phase was accomplished in the following manner: (1) two annota- 
tors were selected, (2) an agreement was reached between the annotators with regard 
to the annotation scheme, (3) each annotator annotated the corpus, and, finally, (4) a 
reliability test (Carletta et al 1997) was done on the annotation in order to guaran- 
tee the results. The reliability test used the kappa statistic that measures agreement 
between the annotations of two annotators in making judgments about categories. In 
this way, the annotation is considered a classification task consisting of defining an ad- 
equate solution among the candidate list. According to Vieira (1998), the classification 
task when tagging anaphora resolution can be reduced to a decision about whether 
each candidate is the solution or not. Thus, two different categories are considered 
for each anaphor: one for the correct antecedent and another for nonantecedents. Our 
experimentation showed one correct antecedent among an average of 14.5 possible 
candidates per anaphor after applying constraints. For computing the kappa statistic 
(k), see Siegel and Castellan (1988). 
According to Carletta et al (1997), a k measurement such as 0.68 < k < 0.8 allows 
us to draw encouraging conclusions, and a measurement k > 0.8 means there is to- 
tal reliability between the results of the two annotators. In our tests, we obtained a 
kappa measurement of k = 0.81. We therefore consider the annotation obtained for the 
evaluation to be totally reliable. 
4.2 Experimental Work 
We conducted a blind test over the entire test corpus of unrestricted Spanish texts by 
applying the algorithm to the partial syntactic structure generated by the slot unifica- 
tion parser. 
Over these corpora, our algorithm attained a success rate for anaphora resolution 
of 76.8%. We define "success rate" as the number of pronouns successfully resolved, 
divided by the total number of resolved pronouns. The total number of resolved pro- 
nouns was 1,677, including personal, demonstrative, reflexive, and omitted pronouns. 
557 
Computational Linguistics Volume 27, Number 4 
Table 3 
Results of blind test. 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP Total 
Num. of 
pronoun 
occurrences 228 80 1,099 107 20 94 49 1,677 
Num. of 
cases 
correctly 162 74 868 70 17 64 34 1,289 
resolved 
Success 
rate 71.0% 92.5% 78.9% 65.4% 85.0% 68.0% 69.3% 76.8% 
All of them were in the third person, with a noun phrase that appeared before the 
anaphor as their antecedent. Our algorithm's "recall percentage," defined as the num- 
ber of pronouns correctly resolved, divided by the total number of pronouns in the 
text, was therefore 76.8%. A breakdown of success rate results for each kind of pro- 
noun is also shown in Table 3. The pronouns were classified so as to provide the 
option of applying different kinds of knowledge to resolve each category of pronoun. 
One of the factors that affected the results was the complexity of the Lexesp corpus, 
due mainly to its complex narratives. On average, 16 words per sentence and 27 
candidates per anaphor were found in this corpus. 
In our experiment, a "successful resolution" occurred if the head of the solution 
offered by our algorithm was the same as that offered by two human experts. We 
adopted this definition of "success" because it allowed the system to be totally auto- 
matic: solutions given by the annotators were stored in a file and were later automat- 
ically compared with the solutions given by our system. Since semantic information 
was not used at all, PP attachments were not always correctly disambiguated. Hence, 
at times the differences imply corresponded to different subconstituents. 
After the evaluation process, we tested the results in order to identify the lim- 
itations of the algorithm with respect to the resolution process. We identified the 
following: 
? There were some mistakes in the POS tagging (causing an error rate of 
around 3%). 
? There were some mistakes in the partial parsing with respect o the 
identification of complex noun phrases (causing an error rate of around 
7%) (Palomar et al 1999). 
? Semantic information was not considered (causing an error rate of 
around 32%). An example of this type of error can be seen in the 
following text extracted from the Lexesp corpus: 
(27) Recuerdo, pot ejemplo, \[un pequefio claro en un bosque en 
medio de las montafias canadienses\]i, con tres lagunas diminutas 
que, a causa de los sedimentos del agua. tenfan distintos y chocantes 
colores. Esta rareza habia hecho del sitioi un espacio sagrado al que 
peregrinaron los indios durante siglos y seguramente antes los 
pobladores paleolfticos. Y eso se notaba. 
558 
Palomar et al Anaphora Resolution in Spanish Texts 
(28) 
Canad~i es un pals muy hermoso, y aqu41i no era, ni mucho 
rnenos, el lugar m~s bello: pero guardaba tranquilamente d ntro de sf 
toda su arrnonfa, como los melocotones guardan dentro de sf el duro 
hueso. 
'1 remember, for example, \[a small clearing in the woods in the 
middle of the Canadian mountains\]/, with three tiny lagoons that, 
due to the water sediments, had different and astonishing colors. 
This peculiarity had made the place/into a sacred site, to which the 
Indians made pilgrimages over the centuries, and surely even the 
Paleolithic Indians before them. And you could feel it. 
'Canada is a very beautiful country and that one/was by no 
means the most beautiful place: but it calmly kept within itself all of 
its harmony, like peaches that keep the hard seeds within.' 
In this text, the demonstrative pronoun aqudl 'that one' corefers with the 
antecedent un peque~o claro en un bosque n medio de las monta~as canadienses 
'a small clearing in the woods in the middle of the Canadian mountains', 
which is also linked to the definite noun phrase el sitio 'the place'. Our 
algorithm identified the proper noun Canadd, which is in the same 
sentence, as the anaphor, since the proper noun could only have been 
discarded by means of semantic information. 
As an example of an anaphor that was correctly resolved by the 
algorithm, we present he following sentence xtracted from the Blue 
Book corpus. In this case, the antecedent los sistemas de transmisidn 
analdgica 'the systems of analogue transmission' was correctly chosen for 
the personal pronoun ellos 'them': ' 
En las conexiones largas o de Iongitud media, es probable que la 
fuente principal de ruido de circuito estribe en \[los sistemas de 
transmisi6n anal6gica\]i, ya queen ellosi la potencia de ruido suele 
set proporcional  la Iongitud del circuito. 
'In long or medium connections, it is probable that the main source of 
circuit noise comes from \[the systems of analogue transmission\]/, 
since in them/the noise capacity is usually proportional to the length 
of the circuit.' 
The remainder of the errors were due to split antecedents (10%), 
cataphora (2%), exophora (3%), or exceptions in the application of 
preferences (43%). 
5. Comparison with Other Approaches to Anaphora Resolution 
5.1 Anaphora Resolution Approaches 
Common among all languages i the fact that the anaphora phenomenon requires im- 
ilar strategies for its resolution (e.g., pronouns or definite descriptions). All languages 
employ different kinds of knowledge, but their strategies differ only in the manner by 
which this knowledge is coordinated. For example, in some strategies just one kind 
of knowledge becomes the main selector for identifying the antecedent, with other 
kinds of knowledge being used merely to confirm or reject the proposed antecedent. 
In such cases, the typical kind of knowledge used as the selector is that of discourse 
structure. Centering theory, as employed by Strube and Hahn (1999) or Okumura and 
Tamura (1996), uses this type of approach. Other approaches, however, give equal 
559 
Computational Linguistics Volume 27, Number 4 
importance to each kind of knowledge and generally distinguish between constraints 
and preferences (Baldwin 1997; Lappin and Leass 1994; Carbonell and Brown 1988). 
Whereas constraints tend to be absolute and therefore discard possible antecedents, 
preferences tend to be relative and require the use of additional criteria (e.g., the use of 
heuristics that are not always satisfied by all antecedents). Nakaiwa and Shirai (1996) 
use this sort of resolution model, which involves the use of semantic and pragmatic 
constraints, such as constraints based on modal expressions, or constraints based on 
verbal semantic attributes or conjunctions. 
Our approach to anaphora resolution belongs in the latter category, since it com- 
bines different kinds of knowledge and no knowledge based on discourse structure 
is included. We choose to ignore discourse structure because obtaining this kind of 
knowledge requires not only an understanding of semantics but also knowledge about 
world affairs and the ability to almost perfectly parse any text under discussion (Az- 
zam, Humphreys, and Gaizauskas 1998). 
Still other approaches to anaphora resolution are based either on machine learn- 
ing techniques (Connolly, Burger, and Day 1994; Yamamoto and Sumita 1998; Paul, 
Yamamato, and Sumita 1999) or on the principles of uncertainty reasoning (Mitkov 
1995). 
Computational processing of semantic and domain information is relatively expen- 
sive when compared with other kinds of knowledge. Consequently, current anaphora 
resolution methods rely mainly on constraint and preference heuristics, which employ 
morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov 
\[1998\]). Such approaches have performed notably well. Lappin and Leass (1994) de- 
scribe an algorithm for pronominal anaphora resolution that achieves a high rate of 
correct analyses (85%). Their approach, however, operates almost exclusively on syn- 
tactic information. More recently, Kennedy and Boguraev (1996) proposed an algorithm 
for anaphora resolution that is actually a modified and extended version of the one 
developed by Lappin and Leass (1994). It works from the output of a POS tagger and 
achieves an accuracy rate of 75%. 
There are other approaches based on POS tagger outputs as well. For example, 
Mitkov and Stys (1997) propose a knowledge-poor approach to resolving pronouns 
in technical manuals in both English and Polish. The knowledge mployed in these 
approaches i limited to a small noun phrase grammar, a list of terms, and a set of 
antecedent indicators (definiteness, term preference, lexical reiteration, etc.). 
Still other approaches are based on statistical information, including the work of 
Dagan and Itai (1990, 1991) and Ge, Hale, and Charniak (1998), all of whom present a 
probabilistic model for pronoun resolution. 
We have adopted their ideas and adapted their algorithms to partial parsing and 
to Spanish texts in order to compare our results with their approaches. 
With reference to the differences between English and Spanish anaphora resolu- 
tion, we have made the following observations: 
Syntactic parallelism has played a more important role in English texts 
than in Spanish texts, since Spanish sentence structure is more flexible 
than English sentence structure. Spanish is a free-word-order language 
and has different syntactic onditions, which increases the difficulty of 
resolving Spanish pronouns (hence, the greater accuracy rate for English 
texts). 
? A greater number of possible antecedents was observed for Spanish 
pronouns than for English pronouns, due mainly to the greater average 
560 
Palomar et al Anaphora Resolution in Spanish Texts 
length of Spanish sentences (which also makes the resolution of Spanish 
pronouns more difficult). 
Spanish pronouns usually bear more morphological information. One 
result is that this constraint tends to discard more candidates in Spanish 
than in English. 
For comparison purposes, we implemented the following approaches on the same 
Spanish texts that were tested and described in Section 4.1. 
5.2 Hobbs's Algorithm 
Hobbs's algorithm (Hobbs 1978) is applied to the surface parse trees of sentences in 
a text. A surface parse tree represents the grammatical structure of a sentence. By 
reading the leaves of the parse tree from left to right, the original English sentence is
formed. The algorithm parses the tree in a predefined order and searches for a noun 
phrase of the correct gender and number. Hobbs tested his algorithm for the pronouns 
he, she, it, and they, using 100 examples taken from three different sources. Although 
the algorithm is very simple, it was successful 81.8% of the time. 
We implemented a version of Hobbs's algorithm for slot unification grammar for 
Spanish texts. Since full parsing was not done, our specifications for the algorithm 
were adjusted, as follows: 
? NPs were tested from left to right, as they were parsed in the sentence. 
? Afterward, the NPs that were included in an NP (breadth-first) were 
tested. 
? This test was interrupted when an NP agreed in gender and number 
with the anaphor. 
The problems we encountered in implementing Hobbs's algorithm are similar to 
those found in implementing other approaches: the adaptation to partial parsing, and 
the inherent difficulty of the Spanish language (i.e., its free-word-order characteristics). 
The results of our test of this version of Hobbs's algorithm on the test corpus 
appear in Table 4. 
5.3 Approaches Based on Constraints and Proximity Preference 
Our approach as also been compared with the typical baseline approach consisting of 
constraints and proximity preference; that is, the antecedent that appears closest o the 
anaphor is chosen from among those that satisfy the constraints. For this comparison, 
the same constraints that were used previously (i.e., morphological greement and 
syntactic onditions) were applied here. Then the antecedent a the head of the list of 
antecedents was proposed as the solution of the anaphor. These results are also listed 
in Table 4. As can be seen from the table, success rates were lower than those obtained 
through the joint application of all the preferences. 
5.4 Lappin and Leass's Algorithm 
An algorithm for identifying the noun phrase antecedents of third person pronouns 
and lexical anaphors (reflexive and reciprocal) is presented in Lappin and Leass (1994); 
this algorithm has exhibited a high rate (85%) of correct analyses in English texts. It 
relies on measures of salience that are derived from syntactic structures and on simple 
dynamic models of attentional state to select he antecedent oun phrase of a pronoun 
from a list of candidates. 
561 
Computational Linguistics Volume 27, Number 4 
We have implemented a version of Lappin and Leass's algorithm for Spanish texts. 
The original formulation of the algorithm proposes a syntactic filter on NP-pronoun 
coreference. This filter consists of six conditions for NP-pronoun oncoreference within 
any sentence (Lappin and Leass 1994, page 537). In applying this algorithm to Span- 
ish texts, we changed these conditions o as to capture the appropriate context. As 
mentioned previously, our algorithm does not have access to full syntactic knowledge. 
Accordingly, we employed partial parsing over the text in our application of Lappin 
and Leass's algorithm. The salience parameters were weighted (weight appears in 
parentheses) and applied in the following way: 
? Sentence recency (100): Applied when the NP appeared in the same 
sentence as the anaphor. 
? Subject emphasis (80): Applied when the NP was located before the 
verb of the clause in which it appeared. This heuristic was necessary 
because of our algorithm's lack of syntactic knowledge. It should be 
noted, however, that since Spanish is a nearly free-word-order language 
and the exchange of subject and object positions within Spanish 
sentences i common, the heuristic is often invalid. For example, the two 
Spanish sentences Pedro compr6 un regalo 'Pedro bought a present' and Un 
regalo compr6 Pedro 'A present bought Pedro' are equivalent to one 
another and to the English sentence Pedro bought a present. 
? Existential emphasis (70): In this instance, we applied the parameter in 
the same way as Lappin and Leass, since the entire NP was fully parsed, 
which allowed us to tell when it was a definite or an indefinite NP. 
? Accusative emphasis (50): Applied when the NP appeared after the verb 
of the clause in which it appeared and the NP did not appear inside 
another NP or PP. For example, in the sentence Pedro encontr6 el libro de 
Juana 'Pedro found Juana's book', a value was assigned to el libro de Juana 
'Juana's book' but not to Juana. Once again, it should be noted that this 
heuristic was necessary because of our algorithm's lack of syntactic 
knowledge. 
? Indirect object and oblique complement emphasis (40): Applied when 
the NP appeared in a PP with the Spanish preposition a 'to', which 
usually preceded the indirect object of its sentence. 
? Head noun emphasis (80): Applied when the NP was not contained in 
another NP. 
? Nonadverbial emphasis (50): Applied when the NP was not contained 
in an adverbial PP. In this case, its application depended on the kind of 
preposition in which the NP was included. 
? Parallelism reward (35): Applied when the NP occupied the same 
position as the anaphor with reference to the verb of the sentence (before 
or after the verb). 
Finally, we followed Lappin and Leass in assigning the additional salience value 
to NPs in the current sentence and in degrading the salience of NPs in preceding 
sentences. 
Our results exhibited some similarities with Lappin and Leass's experiments. 
For example, anaphora was strongly preferred over cataphora, and both approaches 
562 
Palomar et al Anaphora Resolution in Spanish Texts 
preferred intrasentential NPs to intersentential ones. These results can be seen in 
Table 4. 
5.5 Centering Approach 
The centering model proposed by Grosz, Joshi, and Weinstein (1983, 1995) provides 
a framework for modeling the local coherence of discourse. The model has two con- 
structs, a list of forward-looking centers and a backward-looking center, that can be 
assigned to each utterance Ui. The list of forward-looking centers Cf(Ui) ranks dis- 
course entities within the utterance Ui. The backward-looking center Cb(Ui+l) con- 
stitutes the most highly ranked element of Cf(Ui) that is finally realized in the next 
utterance Ui+l. In this way, the ranking imposed over Cf(Ui) must reflect he fact that 
the preferred center Cp(U/) (i.e., the most highly ranked element of Cf(Ui)) is most 
likely to be Cb(Ui+l). 
The ranking criteria used by Grosz, Joshi, and Weinstein (1995) order items in 
the Cf list using grammatical roles. Thus, entities with a subject role are preferred to 
entities with an object role, and objects are preferred to others (adjuncts, etc.). 
Grosz, Joshi, and Weinstein (1995) state that if any element of Cf(Ui) is realized 
by a pronoun in Ui+l, then Cb(Ui+l) must also be realized by a pronoun. 
Brennan, Friedman, and Pollard (1987) applied the centering model to pronoun 
resolution. They based their algorithm on the fact that centering transition relations 
will hold across adjacent utterances. 
Moreover, one crucial point in centering is the ranking of the forward-looking 
centers. Grosz, Joshi, and Weinstein (1995) state that Cf may be ordered using different 
factors, but they only use information about grammatical roles. However, both Strube 
(1998) and Strube and Hahn (1999) point out that it is difficult to define grammatical 
roles in free-word-order languages like German or Spanish. For languages like these, 
they propose other anking criteria dependent upon the information status of discourse 
entities. They claim that information about familiarity is crucial for the ranking of 
discourse ntities, at least in free-word-order languages. 
According to Strube's ranking criteria, two different sets of expressions, hearer- 
old discourse ntities (OLD) and hearer-new discourse ntities (NEW), can be distin- 
guished. OLD discourse ntities consist of evoked entities---coreferring resolved ex- 
pressions (pronominal and nominal anaphora, previously mentioned proper names, 
relative pronouns, appositives)--and unused entities (proper names and titles). The re- 
maining entities are assigned to the NEW set. The basic ranking criteria for pronominal 
anaphora resolution prefer OLD entities over NEW entities. 8 
Strube (1998) thus proposes the following adaptation to the centering model: 
The Cf list is replaced by the list of salient discourse ntities (S-list) 
containing discourse ntities that are realized in the current and previous 
utterance. 
? The elements of the S-list are ranked according to the basic ranking 
criteria and position information: 
If X E OLD and y C NEW, then x precedes y. 
If x, y ~ OLD or x, y E NEW, 
8 To resolve functional naphora,  third set, MED, which includes inferable information, must be added 
between the OLD and the NEW sets. However, this set is not needed to resolve pronominal naphora 
(Strube and Hahn 1999). 
563 
Computational Linguistics Volume 27, Number 4 
Table 4 
Comparative r sults of blind test. 
Total CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Num. of 
pronoun 1,677 228 80 1,099 107 20 94 49 
occurrences 
Hobbs's 
algorithm 62.7% 61% 85% 62% 62% 50% 66% 52% 
Lappin & 
Leass's 67.4% 66% 86% 67% 65% 60% 67% 60% 
algorithm 
Proximity 52.9% 55% 86% 47% 65% 85% 61% 65% 
Centering 
approach 62.6% 60% 85% 62% 61% 60% 62% 58% 
Our 
algorithm 76.8% 71% 92% 79% 65% 85% 68% 69% 
then if utterance(y) precedes utterance(x), then x precedes y, 
if utterance(y) = utterance(x) and pos(x) < pos(y), then x precedes y. 
Since there is not a clear definition of what an utterance is, the following 
criteria are assumed: tensed clauses are defined as utterances on their 
own and untensed clauses are processed with the main clause in order to 
constitute only one utterance. 
Incorporating these adaptations, Strube (1998) then proposes the following algo- 
rithm: 
1. If a referring expression is encountered, 
(a) if it is a pronoun, test the elements of the S-list in order until the 
test succeeds; 
(b) update the S-list using information about this referring 
expression. 
2. If the analysis of utterance U is finished, remove all discourse ntities 
from the S-list that are not realized in U. 
The evaluation of this algorithm was performed in Strube (1998) and obtained a 
precision of 85.4% for English, improving upon the results of the centering algorithm 
by Brennan, Friedman, and Pollard (1987), which achieved only 72.9% precision when 
it was applied to the same corpus. 
Consequently, in adapting the centering model to Spanish anaphora resolution, we 
followed Strube's indications. The success rate of the algorithm was not satisfactory, 
as can be seen in Table 4. 
6. Conclus ions 
In this paper, we have presented an algorithm for identifying noun phrase antecedents 
of third person personal pronouns, demonstrative pronouns, reflexive pronouns, and 
564 
Palomar et al Anaphora Resolution in Spanish Texts 
omitted pronouns in Spanish. The algorithm is applied to the syntactic structure gen- 
erated by the slot unification parser--see Ferrdndez, Palomar, and Moreno (1998a, 
1998b, 1999)--and coordinates different kinds of knowledge (lexical, morphological, 
and syntactic) by distinguishing between constraints and preferences. 
The main contribution ofthis paper is the introduction ofan algorithm for anaphora 
resolution for Spanish. In our work, we have undertaken an exhaustive study of the 
importance of each kind of knowledge in anaphora resolution for Spanish. Moreover, 
we have developed a definition of syntactic onditions of NP-pronoun noncorefer- 
ence in Spanish with partial parsing. We have also adapted our anaphora resolution 
algorithm to the problem of partial syntactic knowledge, that is to say, when partial 
parsing of the text is accomplished. 
For unrestricted texts, our approach is somewhat less accurate, since semantic 
information is not taken into account. For such texts, we are dealing with the output 
of a POS tagger, which does not provide this sort of knowledge. In order to test our 
approach with texts of different genres by different authors, we have worked with 
two different Spanish corpora, literary texts (the Lexesp corpus) and technical texts 
(the Blue Book), containing a total of 1,677 pronoun occurrences. 
The algorithm successfully identified the antecedent of the pronoun for 76.8% 
of these pronoun occurrences. Other algorithms usually work with different kinds 
of knowledge, different texts, and different languages. In order to make a more valid 
comparison of our algorithm with others, we adapted the other algorithms so that they 
would operate using only partial-parsing knowledge. In this evaluation, our algorithm 
has always obtained better esults. 
Moreover, based on the results on our study of the importance of each kind 
of knowledge, we can emphasize that constraints are very important for resolving 
anaphora successfully, since they considerably reduce the number of possible candi- 
dates. 
In future studies, we will attempt to evaluate the importance of semantic informa- 
tion in unrestricted texts for anaphora resolution in Spanish texts (Saiz-Noeda, Su~rez, 
and Peral 1999). This information will be obtained from a lexical tool (e.g., Spanish 
WordNet), which can be automatically consulted (since the tagger does not provide 
this information). 
Acknowledgments 
The authors wish to thank Ferran Pla, 
Natividad Prieto, and Antonio Molina for 
contributing their tagger (Pla 2000); and 
Richard Evans, Mikel Forcada, and Rafael 
Carrasco for their helpful revisions of the 
ideas presented in this paper. We are also 
grateful to several anonymous reviewers of 
Computational Linguistics for helpful 
comments on earlier drafts of this paper. 
Our work has been supported by the 
Spanish government (CICYT) with Grant 
TIC97-0671-C02-01/02. 
References 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Evaluating a
focus-based approach to anaphora 
resolution. In Proceedings ofthe 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference 
on Computational Linguistics 
(COLING-ACL'98), pages 74-78, Montreal 
(Canada). 
Baldwin, Breck. 1997. CogNIAC: High 
precision coreference with limited 
knowledge and linguistic resources. In
Proceedings of the ACL/EACL Workshop on 
Operational Factors in Practical, Robust 
Anaphora Resolution for Unrestricted Texts, 
pages 38--45, Madrid (Spain). 
Brennan, Susan E., Marilyn W. Friedman, 
and Carl J. Pollard. 1987. A centering 
approach to pronouns. In Proceedings ofthe 
25th Annual Meeting of the Association for 
Computational Linguistics (ACL'87), pages 
155-162, Stanford, CA (USA). 
Carbonell, Jaime G. and Ralf D. Brown. 
1988. Anaphora resolution: A
multi-strategy approach. In Proceedings of
the 12th International Conference on 
565 
Computational Linguistics Volume 27, Number 4 
Computational Linguistics (COLING'88), 
pages 96-101, Budapest (Hungary). 
Carletta, Jean, Amy Isard, Stephen Isard, 
Jacqueline C. Kowtko, Gwyneth 
Doherty-Sneddon, and Anne H. 
Anderson. 1997. The reliability of a 
dialogue structure coding scheme. 
Computational Linguistics, 23(1):13-32. 
Connolly, Dennis, John D. Burger, and 
David S. Day. 1994. A machine learning 
approach to anaphoric reference. In
Proceedings ofthe International Conference on 
New Methods in Language Processing 
(NEMLAP'94), pages 255-261, Manchester 
(UK). 
Dagan, Ido and Alon Itai. 1990. Automatic 
processing of large corpora for the 
resolution of anaphora references. In
Proceedings ofthe 13th International 
Conference on Computational Linguistics 
(COLING'90), pages 330-332, Helsinki 
(Finland). 
Dagan, Ido and Alon Itai. 1991. A statistical 
filter for resolving pronoun references. In
Yishai A. Feldman and Alfred Bruckstein, 
editors, Artificial Intelligence and Computer 
Vision. Elsevier Science Publishers B. V. 
(North-Holland), Amsterdam, pages 
125-135. 
Ferrlindez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998a. A computational 
approach to pronominal anaphora, 
one-anaphora and surface count 
anaphora. In Proceedings ofthe Second 
Colloquium on Discourse Anaphora nd 
Anaphora Resolution (DAARC'98), pages 
117-128, Lancaster (UK). 
Ferr~ndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998b. Anaphora 
resolution in unrestricted texts with 
partial parsing. In Proceedings ofthe 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
385-391, Montreal (Canada). 
Ferr~fndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1999. An empirical 
approach to Spanish anaphora resolution. 
Machine Translation, 14(3/4):191-216. 
Ferr~indez, Antonio and Jestis Peral. 2000. A 
computational pproach to zero-pronouns 
in Spanish. In Proceedings ofthe 38th 
Annual Meeting of the Association for 
Computational Linguistics (ACL'O0), pages 
166-172, Hong Kong (China). 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical approach to anaphora 
resolution. In Proceedings ofthe Sixth 
Workshop on Ven d Large Corpora, pages 
161-170, Montreal (Canada). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1983. Providing a unified 
account of definite noun phrases in 
discourse. In Proceedings ofthe 21st Annual 
Meeting of the Association for Computational 
Linguistics (ACL'83), pages 44-50, 
Cambridge, MA (USA). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1995. Centering: A framework 
for modeling the local coherence of 
discourse. Computational Linguistics, 
21(2):203-225. 
Hobbs, Jerry R. 1978. Resolving pronoun 
references. Lingua, 44:311-338. 
Kennedy, Christopher and Branimir 
Boguraev. 1996. Anaphora for everyone: 
Pronominal anaphora resolution without 
a parser. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics (COLING'96), pages 113-118, 
Copenhagen (Denmark). 
Lappin, Shalom and Herbert Leass. 1994. 
An algorithm for pronominal anaphora 
resolution. Computational Linguistics, 
20(4):535-561. 
Mathews, Peter H. 1997. The Concise Oxford 
Dictionary of Linguistics. Oxford University 
Press, Oxford (UK). 
Mitkov, Ruslan. 1995. An uncertainty 
reasoning approach to anaphora 
resolution. In Proceedings ofthe Natural 
Language Pacific Rim Symposium (NLPRS 
"95), pages 149-154, Seoul (Korea). 
Mitkov, Ruslan. 1998. Robust pronoun 
resolution with limited knowledge. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17 th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 869-875, 
Montreal (Canada). 
Mitkov, Ruslan and Malgorzata Stys. 1997. 
Robust reference resolution with limited 
knowledge: High precision genre-specific 
approach for English and Polish. In 
Proceedings ofthe International Conference on 
Recent Advances in Natural Language 
Processing (RANLP'97), pages 74-81, 
Tzigov Chark (Bulgaria). 
Nakaiwa, Hiromi and Satoshi Shirai. 1996. 
Anaphora resolution of Japanese zero 
pronouns with deictic reference. In
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
(COLING'96), pages 812-817, Copenhagen 
(Denmark). 
Okumura, Manabu and Kouji Tamura. 1996. 
Zero pronoun resolution in Japanese 
discourse based on centering theory. In 
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
566 
Palomar et al Anaphora Resolution in Spanish Texts 
(COLING'96), pages 871-876, Copenhagen 
(Denmark). 
Palomar, Manuel, Antonio Ferra'ndez, Lidia 
Moreno, Maximiliano Saiz-Noeda, Rafael 
Mu~oz, Patricio Martfnez-Barco, Jestis 
Peral, and Borja Navarro. 1999. A robust 
partial parsing strategy based on the slot 
unification grammars. In Proceedings ofthe 
6th Conference on Natural Language 
Processing (TALN'99), pages 263-272, 
Corsica (France). 
Paul, Michael, Kazuhide Yamamoto, and 
Eiichiro Sumita. 1999. Corpus-based 
anaphora resolution towards antecedent 
preference. In Proceedings ofthe ACL 
Workshop on Coreference and Its Applications, 
pages 47-52, College Park, MD (USA). 
Pla, Ferran. 2000. Etiquetado Ldxico y Andlisis 
Sintdctico Super~'cial Basado en Modelos 
Estadfsticos. Ph.D. thesis, Valencia 
University of Technology, Valencia 
(Spain). 
Proyecto CRATER. 1994-1995. Corpus 
Resources And Terminology ExtRaction. 
MLAP-93/20. http: //www.lllf.uam.es / 
proyectos/crater.html (page visited on 
04/17/01). 
Reinhart, Tanya. 1983. Anaphora nd Semantic 
Interpretation. Croom Hehn Linguistics 
series. Croom Helm Ltd., Beckenham, 
Kent (UK). 
Saiz-Noeda, Maximiliano, Armando Sudrez, 
and Jestis Peral. 1999. Propuesta de 
incorporacidn de informaci6n sem~ntica 
desde Wordnet alandlisis sintdctico 
parcial orientado a la resoluci6n de la 
an~ffora. Procesamiento del Lenguaje Natural, 
25:167-173. 
Siegel, Sidney and John N. Castellan. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill, New York, NY 
(USA), 2nd edition. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of
the 36th Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
1251-1257, Montreal (Canada). 
Strube, Michael and Udo Hahn. 1999. 
Functional centering: Grounding 
referential coherence in information 
structure. Computational Linguistics, 
25(3):309-344. 
Vieira, Renata. 1998. Processing of Definite 
Descriptions in Unrestricted Texts. Ph.D. 
thesis, University of Edinburgh, 
Edinburgh (UK). 
Yamamoto, Kazuhide and Eiichiro Sumita. 
1998. Feasibility study for ellipsis 
resolution in dialogues by 
machine-learning technique. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 385-391, 
Montreal (Canada). 
567 

Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Splitting Complex Temporal Questions for Question Answering systems  
E. Saquete, P. Mart??nez-Barco, R. Mun?oz, J.L. Vicedo
Grupo de investigacio?n del Procesamiento del Lenguaje y Sistemas de Informacio?n.
Departamento de Lenguajes y Sistemas Informa?ticos. Universidad de Alicante.
Alicante, Spain

stela,patricio,rafael,vicedo  @dlsi.ua.es
Abstract
This paper presents a multi-layered Question An-
swering (Q.A.) architecture suitable for enhanc-
ing current Q.A. capabilities with the possibility of
processing complex questions. That is, questions
whose answer needs to be gathered from pieces
of factual information scattered in different docu-
ments. Specifically, we have designed a layer ori-
ented to process the different types of temporal
questions. Complex temporal questions are first de-
composed into simpler ones, according to the tem-
poral relationships expressed in the original ques-
tion.
In the same way, the answers of each simple ques-
tion are re-composed, fulfilling the temporal restric-
tions of the original complex question.
Using this architecture, a Temporal Q.A. system
has been developed.
In this paper, we focus on explaining the first part
of the process: the decomposition of the complex
questions. Furthermore, it has been evaluated with
the TERQAS question corpus of 112 temporal ques-
tions. For the task of question splitting our system
has performed, in terms of precision and recall, 85%
and 71%, respectively.
1 Introduction
Question Answering could be defined as the pro-
cess of computer-answering to precise or arbitrary
questions formulated by users. Q.A. systems are es-
pecially useful to obtain a specific piece of informa-
tion without the need of manually going through all
the available documentation related to the topic.
Research in Question Answering mainly focuses
on the treatment of factual questions. These require
as an answer very specific items of data, such as
dates, names of entities or quantities, e.g., ?What is
the capital of Brazil??.

This paper has been supported by the Spanish government,
projects FIT-150500-2002-244, FIT-150500-2002-416, TIC-
2003-07158-C04-01 and TIC2000-0664-C02-02.
Temporal Q.A. is not a trivial task due to the com-
plexity temporal questions may reach. Current op-
erational Q.A. systems can deal with simple factual
temporal questions. That is, questions requiring to
be answered with a date, e.g. ?When did Bob Mar-
ley die??. or questions that include simple temporal
expressions in their formulation, e.g., ?Who won the
U.S. Open in 1999??. Processing this sort of ques-
tions is usually performed by identifying explicit
temporal expressions in questions and relevant doc-
uments, in order to gather the necessary information
to answer the queries.
Even though, it seems necessary to emphasize
that the system described in (Breck et al, 2000) is
the only one also using implicit temporal expression
recognition for Q.A. purposes. It does so by apply-
ing the temporal tagger developed by Mani and Wil-
son (2000).
However, issues like addressing the temporal
properties or the ordering of events in questions, re-
main beyond the scope of current Q.A. systems:

?Who was spokesman of the Soviet Embassy
in Baghdad during the invasion of Kuwait??

?Is Bill Clinton currently the President of the
United States??
This work presents a Question Answering system
capable of answering complex temporal questions.
This approach tries to imitate human behavior when
responding this type of questions. For example, a
human that wants to answer the question: ?Who
was spokesman of the Soviet Embassy in Baghdad
during the invasion of Kuwait?? would follow this
process:
1. First, he would decompose this question into
two simpler ones: ?Who was spokesman of the
Soviet Embassy in Baghdad?? and ?When did
the invasion of Kuwait occur??.
2. He would look for all the possible answers
to the first simple question: ?Who was
spokesman of the Soviet Embassy in Bagh-
dad??.
3. After that, he would look for the answer to the
second simple question: ?When did the inva-
sion of Kuwait occur??
4. Finally, he would give as a final answer one
of the answers to the first question (if there is
any), whose associated date stays within the
period of dates implied by the answer to the
second question. That is, he would obtain
the final answer by discarding all answers to
the simple questions which do not accomplish
the restrictions imposed by the temporal signal
provided by the original question (during).
Therefore, the treatment of complex question is
based on the decomposition of these questions into
simpler ones, to be resolved using conventional
Question Answering systems. Answers to simple
questions are used to build the answer to the origi-
nal question.
This paper has been structured in the following
fashion: first of all, section 2 presents our proposal
of a taxonomy for temporal questions. Section 3
describes the general architecture of our temporal
Q.A. system. Section 4 deepens into the first part
of the system: the decomposition unit. Finally, the
evaluation of the decomposition unit and some con-
clusions are shown.
2 Proposal of a Temporal Questions
Taxonomy
Before explaining how to answer temporal ques-
tions, it is necessary to classify them, since the
way to solve them will be different in each case.
Our classification distinguishes first between simple
questions and complex questions. We will consider
as simple those questions that can be solved directly
by a current General Purpose Question Answering
system, since they are formed by a single event. On
the other hand, we will consider as complex those
questions that are formed by more than one event
related by a temporal signal which establishes an
order relation between these events.
Simple Temporal Questions:
Type 1: Single event temporal questions without
temporal expression (TE). This kind of questions
are formed by a single event and can be directly
resolved by a Q.A. System, without pre- or post-
processing them. There are not temporal expres-
sions in the question. Example: ?When did Jordan
close the port of Aqaba to Kuwait??
Type 2: Single event temporal questions with tem-
poral expression. There is a single event in the ques-
tion, but there are one or more temporal expressions
that need to be recognized, resolved and annotated.
Each piece of temporal information could help to
search for an answer. Example: ?Who won the 1988
New Hampshire republican primary??. TE: 1988
Complex Temporal Questions:
Type 3: Multiple events temporal questions with
temporal expression. Questions that contain two or
more events, related by a temporal signal. This sig-
nal establishes the order between the events in the
question. Moreover, there are one or more tempo-
ral expressions in the question. These temporal ex-
pressions need to be recognized, resolved and an-
notated, and they introduce temporal constraints to
the answers of the question. Example: ?What did
George Bush do after the U.N. Security Council or-
dered a global embargo on trade with Iraq in August
90?? In this example, the temporal signal is after
and the temporal constraint is ?between 8/1/1990
and 8/31/1990?. This question can be divided into
the following ones:
 Q1: What did George Bush do?
 Q2: When the U.N. Security Council ordered
a global embargo on trade with Iraq?
Type 4: Multiple events temporal questions with-
out temporal expression. Questions that consist
of two or more events, related by a temporal sig-
nal. This signal establishes the order between the
events in the question. Example: ?What happened
to world oil prices after the Iraqi annexation of
Kuwait??. In this example, the temporal signal is
after and the question would be decomposed into:
 Q1: What happened to world oil prices?
 Q2: When did the Iraqi ?annexation? of
Kuwait occur?
How to process each type will be explained in de-
tail in the following sections.
3 Multi-layered Question-Answering
System Architecture
Current Question Answering system architectures
do not allow to process complex questions. That is,
questions whose answer needs to be gathered from
pieces of factual information that is scattered in a
document or through different documents. In or-
der to be able to process these complex questions,
we propose a multi-layered architecture. This ar-
chitecture increases the functionality of the current
Question-Answering systems, allowing us to solve
any type of temporal questions. Moreover, this sys-
tem could be easily augmented with new layers to
cope with questions that need complex processing
and are not temporal oriented.
Some examples of complex questions are:
 Temporal questions like ?Where did Michael
Milken study before going to the University of
Pennsylvania??. This kind of questions needs
to use temporal information and event ordering
to obtain the right answer.
 Script questions like ?How do I assemble a bi-
cycle??. In these questions, the final answer is
a set of ordered answers.
 Template-based questions like ?Which are the
main biographical data of Nelson Mandela??.
This question should be divided in a number of
factual questions asking for different aspects of
Nelson Mandela?s biography. Gathering their
respective answers will make it possible to an-
swer the original question.
These three types of question have in common
the necessity of an additional processing in order
to be solved. Our proposal to deal with them is
to superpose an additional processing layer, one by
each type, to a current General Purpose Question
Answering system, as it is shown in Figure 1. This
layer will perform the following steps:
 Decomposition of the question into simple
events to generate simple questions (sub-
questions) and the ordering of the sub-
questions.
 Sending simple questions to a current General
Purpose Question Answering system.
 Receiving the answers to the simple questions
from the current General Purpose Question
Answering system.
 Filtering and comparison between sub-answers
to build the final complex answer.
	
	













 
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 30?37,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Evaluating Knowledge-based Approaches to the Multilingual Extension of
a Temporal Expression Normalizer
Matteo Negri
ITC-irst
Povo - Trento, Italy
negri@itc.it
Estela Saquete, Patricio Mart??nez-Barco, Rafael Mun?oz
DLSI, University of Alicante
Alicante, Spain
{stela,patricio,rafael}@dlsi.ua.es
Abstract
The extension to new languages is a well
known bottleneck for rule-based systems.
Considerable human effort, which typi-
cally consists in re-writing from scratch
huge amounts of rules, is in fact required
to transfer the knowledge available to the
system from one language to a new one.
Provided sufficient annotated data, ma-
chine learning algorithms allow to mini-
mize the costs of such knowledge trans-
fer but, up to date, proved to be ineffec-
tive for some specific tasks. Among these,
the recognition and normalization of tem-
poral expressions still remains out of their
reach. Focusing on this task, and still ad-
hering to the rule-based framework, this
paper presents a bunch of experiments on
the automatic porting to Italian of a system
originally developed for Spanish. Differ-
ent automatic rule translation strategies are
evaluated and discussed, providing a com-
prehensive overview of the challenge.
1 Introduction
In recent years, inspired by the success of MUC
evaluations, a growing number of initiatives (e.g.
TREC1, CLEF2, CoNLL3, Senseval4) have been
developed to boost research towards the automatic
understanding of textual data. Since 1999, the Au-
tomatic Content Extraction (ACE) program5 has
been contributing to broaden the varied scenario
of evaluation campaigns by proposing three main
1http://trec.nist.gov
2http://clef-campaign.org
3http://www.cnts.ua.ac.be/conll
4http://www.senseval.org
5http://www.nist.gov/speech/tests/ace
tasks, namely the recognition of entities, rela-
tions, and events. In 2004, the Timex2 Detec-
tion and Recognition task6 (also known as TERN,
for Time Expression Recognition and Normaliza-
tion) has been added to the ACE program, making
the whole evaluation exercise more complete. The
main goal of the task was to foster research on sys-
tems capable of automatically detecting temporal
expressions (TEs) present in an English text, and
normalizing them with respect to a specifically de-
fined annotation standard.
Within the above mentioned evaluation exer-
cises, the research activity on monolingual tasks
has gradually been complemented by a consid-
erable interest towards multilingual and cross-
language capabilities of NLP systems. This trend
confirms how portability across languages has
now become one of the key challenges for Natu-
ral Language Processing research, in the effort of
breaking the language barrier hampering systems?
application in many real use scenarios. In this di-
rection, machine learning techniques have become
the standard approach in many NLP areas. This
is motivated by several reasons, including i) the
fact that considerable amounts of annotated data,
indispensable to train ML-based algorithms, are
now available for many tasks, and ii) the difficulty,
inherent to rule-based approaches, of porting lan-
guage models from one language to new ones. In
fact, while supervised ML algorithms can be eas-
ily extended to new languages given an annotated
training corpus, rule-based approaches require to
redefine the set of rules, adapting them to each new
language. This is a time consuming and costly
work, as it usually consists in manually rewriting
from scratch huge amounts of rules.
6http://timex2.mitre.org
30
In spite of their effectiveness for some tasks,
ML techniques still fall short from providing ef-
fective solutions for others. This is confirmed by
the outcomes of the TERN 2004 evaluation, which
provide a clear picture of the situation. In spite
of the good results obtained in the TE recognition
task (Hacioglu et al, 2005), the normalization by
means of ML techniques has not been tackled yet,
and still remains an unresolved problem.
Considering the inadequacy of ML techniques
to deal with the normalization problem, and fo-
cusing on portability across languages, this pa-
per extends and completes the previous work pre-
sented in (Saquete et al, 2006b) and (Saquete et
al., 2006a). More specifically, we address the fol-
lowing crucial issue: how to minimize the costs
of building a rule-based TE recognition system
for a new language, given an already existing sys-
tem for another language. Our goal is to experi-
ment with different automatic porting procedures
to build temporal models for new languages, start-
ing from previously defined ones. Still adhering
to the rule-based paradigm, we analyse different
porting methodologies that automatically learn the
TE recognition model used by the system in one
language, adjusting the set of normalization rules
for the new target language.
In order to provide a clear and comprehen-
sive overview of the challenge, an incremental ap-
proach is proposed. Starting from the architecture
of an existing system developed for Spanish (Sa-
quete et al, 2005), we present a bunch of exper-
iments which take advantage of different knowl-
edge sources to build an homologous system for
Italian. Building on top of each other, such exper-
iments aim at incrementally analyzing the contri-
bution of additional information to attack the TE
normalization task. More specifically, the follow-
ing information will be considered:
? The output of online translators;
? The information mined from a manually an-
notated corpus;
? A combination of the two.
2 The task: TE recognition and
normalization
The TERN task consists in automatically detect-
ing, bracketing, and normalizing all the time ex-
pressions mentioned within an English text. The
recognized TEs are then annotated according to
the TIMEX2 annotation standard described in
(Ferro et al, 2005). Markable TEs include both
absolute (or explicit) expressions (e.g. ?April 15,
2006?), and relative (or anaphoric) expressions
(e.g. ?three years ago?). Also markable are du-
rations (e.g. ?two weeks?), event-anchored ex-
pressions (e.g. ?two days before departure?), and
sets of times (e.g. ?every week?). Detection and
bracketing concern systems? capability to recog-
nize TEs within an input text, and correctly deter-
mine their extension. Normalization concerns the
ability of the system to correctly assign, for each
detected TE, the correct values to the TIMEX2
normalization attributes. The meaning of these at-
tributes can be summarized as follows:
? VAL: contains the normalized value of a TE
(e.g. ?2004-05-06? for ?May 6th, 2004?)
? ANCHOR VAL: contains a normalized form
of an anchoring date-time.
? ANCHOR DIR: captures the relative
direction-orientation between VAL and
ANCHOR VAL.
? MOD: captures temporal modifiers (pos-
sible values include: ?approximately?,
?more than?, ?less than?)
? SET: identifies expressions denoting sets of
times (e.g. ?every year?).
2.1 The evaluation benchmark
Moving to a new language, an evaluation bench-
mark is necessary to test systems performances.
For this purpose, the temporal annotations of the
Italian Content Annotation Bank (I-CAB-temp7)
have been selected.
I-CAB consists of 525 news documents
taken from the Italian newspaper L?Adige
(http://www.adige.it), and contains around
182,500 words. Its 3,830 temporal expressions
(2,393 in the training part of the corpus, and 1,437
in the test part) have been manually annotated
following the TIMEX2 standard with some adap-
tations to the specific morpho-syntactic features
of Italian, which has a far richer morphology than
English (see (Magnini et al, 2006) for further
details).
7I-CAB is being developed as part of the three-year
project ONTOTEXT funded by the Provincia Autonoma di
Trento, Italy. See http://tcc.itc.it/projects/ontotext
31
3 The starting point: TERSEO
As a starting point for our experiments we used
TERSEO, a system originally developed for the
automatic annotation of TEs appearing in a Span-
ish written text in compliance with the TIMEX2
standard (see (Saquete, 2005) for a thorough de-
scription of TERSEO?s main features and func-
tionalities).
TEXT
POS 
TAGGER
RECOGNITION: 
PARSER
Lexical and
morphological
information
Temporal 
expression
recognition
DATE
ESTIMATION
Dictionary
Temporal
Expression
Grammar
TEMPORAL
EXPRESSION
NORMALIZATION
EVENT 
ORDERING
ORDERED
TEXT
Documental 
DataBase
Figure 1: System?s architecture.
Basically (see Figure 1), the TE recognition
and normalization process is carried out in two
phases. The first phase (recognition) includes a
pre-processing of the input text, which is tagged
with lexical and morphological information that
will be used as input to a temporal parser. The
temporal parser is implemented using an as-
cending technique (chart parser) and relies on a
language-specific temporal grammar. As TEs can
be divided into absolute and relative ones, such
grammar is tuned for discriminating between the
two groups. On the one hand, absolute TEs di-
rectly provide and fully describe a date. On the
other hand, relative TEs require some degree of
reasoning (as in the case of anaphora resolution).
In the second phase of the process, in order to
translate these expressions into their normalized
form, the lexical context in which they occur is
considered. At this stage, a normalization unit
is in charge of determining the appropriate refer-
ence date (anchor) associated to each anaphoric
TE, calculating its value, and finally generating the
corresponding TIMEX2 tag.
?From a multilingual perspective, an impor-
tant feature of TERSEO is the distinction between
recognition rules, which are language-specific,
and normalization rules, which are language-
independent and potentially reusable for any other
language. Taking the most from the modular ar-
chitecture of the system, a first multilingual exten-
sion has been evaluated over the English TERN
2004 test set. In that extension, the English
temporal model was automatically obtained from
the Spanish one, through the automatic transla-
tion into English8 of the Spanish TEs recognized
by the system (Saquete et al, 2004). The re-
sulting English TEs were then mapped onto the
corresponding language-independent normaliza-
tion rules, with good results (compared with other
participants to the competition) both in terms of
precision and recall. These results are shown in
Table 1.
Prec Rec F
timex2 0.673 0.728 0.699
anchor dir 0.658 0.877 0.752
anchor val 0.684 0.912 0.782
set 0.800 0.667 0.727
text 0.770 0.620 0.690
val 0.757 0.735 0.746
Table 1: Evaluation of English-TERSEO over the
TERN 2004 test set
The positive results of this experience demon-
strated the viability of the adopted solutions, and
motivate our further investigation with Italian as a
new target language.
4 Porting TERSEO to Italian
Due to the separation between language-specific
recognition rules and language-independent nor-
malization rules, the bulk of the porting process
relies on the adaptation of the recognition rules
to the new target language. Taking advantage of
different knowledge sources (either alone or in
combination), an incremental approach has been
adopted, in order to determine the contribution of
additional information on the performance of the
resulting system for Italian.
8Altavista Babel Fish Translation has been used for this
purpose (http://world.altavista.com).
32
4.1 Using online translators
As a first experiment, the same procedure adopted
for the extension to English has been followed.
This represents the simplest approach for porting
TERSEO to other languages, and will be consid-
ered as a baseline for comparison with the results
achieved in further experiments. The only minor
difference with respect to the original procedure
is that now, since two aligned sets of recognition
rules (i.e. for Spanish and for English) are avail-
able, both models have been used. The reason for
considering both models is the fact that they com-
plement each other: on the one hand, the Span-
ish model was obtained manually and showed high
precision values in detection (88%); on the other
hand, although the English model showed lower
precision results in detection (77%), the on-line
translators from English to Italian perform better
than translators from Spanish to Italian.
The process is carried out in the following four
steps.
1. Eng-Ita translation. All the English TEs
known by the system are translated into Ital-
ian9. Starting English, the probability of ob-
taining higher quality translations is maxi-
mized.
2. Spa-Ita translation. For each English TE
without an Italian translation, the correspond-
ing Spanish expression is translated into Ital-
ian. Also the Spanish TEs that do not have an
English equivalent are translated from Span-
ish10 into Italian. This way, the coverage
of the resulting model is maximized, becom-
ing comparable to the hand-crafted Spanish
model.
3. TE Filtering. A filtering module is used to
guarantee the correctness of the translations.
For this purpose, the translated expressions
are searched in the Web with Google. If an
expression is not found by Google it is given
up; otherwise it is considered as a valid Ital-
ian TE. The inconvenience of adopting this
simple filtering strategy occurs in case of am-
biguous expressions, i.e. when a correct ex-
pression is obtained through translation, and
9Also for English to Italian translation, Altavista Babel
Fish Translation has been used
10Using the Spanish-Italian translator available at
http://www.tranexp.com:2000/Translate/result.shtml
Google returns at least on document contain-
ing it, but the expression is not a tempo-
ral one. In these cases the system will er-
roneously store in its database non-temporal
expressions. In this experiment the results
returned by Google have not been analyzed
(only the number of hits has been taken into
account), nor the impact of these errors has
been estimated. A more precise analysis of
the output of the web search has been left as
a future improvement direction.
4. Normalization rules assignment. Finally,
the resulting Italian translations are mapped
onto the language-independent normalization
rules associated with the original English and
Spanish TEs.
The development of this first automatic porting
procedure required one person/week for software
implementation, and less than an hour to obtain
the new model for Italian. The performance of the
resulting system, evaluated over the test set of I-
CAB, is shown in table 2.
Prec Rec F
timex2 0.725 0.833 0.775
anchor dir 0.211 0.593 0.311
anchor val 0.203 0.571 0.300
set 0.152 1.000 0.263
text 0.217 0.249 0.232
val 0.364 0.351 0.357
Table 2: Porting to Italian based on translations
The results achieved by the translation-based
approach are controversial. On the one hand, we
observe a detection performance in line with the
English version of the system. The timex2 at-
tribute, which indicates the proportion of detected
TEs11, has even higher scores, both in terms of
precision (+5%) and recall (+11%), with respect
to the English system. On the other hand, both
bracketing (see the text attribute, which indicates
the quality of extent recognition) and normaliza-
tion (described by the other attributes) show a per-
formance drop. Unfortunately, the reasons of this
drop are still unclear. One possible explanation
is that, due to the intrinsic difficulties presented
by the Italian language, the translation-based ap-
proach falls short from providing an adequate cov-
erage of the many possible TE variants. While
11At least one overlapping character in the extent of the
reference and the system output is required for tag aligment.
33
the presence of lexical triggers denoting a TE ap-
pearing in a text (e.g. the Italian translations of
?years?, ?Monday?, ?afternoon?, ?yesterday?) can
be easily captured by this approach, the complex-
ity of many language-specific constructs is out of
its reach.
4.2 Using an annotated corpus
In a second experiment, the annotations of the
training portion of I-CAB have been used as a pri-
mary knowledge source. The main purpose of this
approach is to maximize the coverage of the Ital-
ian TEs, starting from language-specific knowl-
edge mined from the corpus. The basic hypothe-
sis is that a bottom-up porting methodology, led by
knowledge in the target language, is more effective
than the top-down approach based on knowledge
derived from models built for other languages.
The former, in fact, is in principle more suitable to
capture language-specific TE variations. In order
to test the validity of ths hypothesis, the following
two-step process has been set up:
1. TE Collection and translation. The Italian ex-
pressions are collected from the I-CAB train-
ing portion, and translated both into Spanish
and English.
2. Normalization rules assignment. Italian TEs
are assigned to the appropriate normalization
rules. For each Italian TE mined from the
corpus, the selection is done considering the
normalization rules assigned to its transla-
tions. If both the Spanish and English ex-
pressions are found in their respective mod-
els, and are associated with the same normal-
ization rule, then this rule is assigned also to
the Italian expression. Also, when only one
of the translated expressions is found in the
existing models, the normalization rule is as-
signed. In case of discrepancies, i.e. if both
expressions are found, but are not associated
to the same normalization rule, then one of
the languages must be prioritized. Since the
manually obtained Spanish model has shown
a higher precision, Spanish rules are pre-
ferred.
As the corpus-based approach is mostly built on
the same software used for the translation-based
porting procedure, it did not require additional
time for implementation. Also in this case, the
new model for Italian has been obtained in less
than one hour. Performance results calculated over
the I-CAB test set are reported in Table 3.
Prec Rec F
timex2 0.730 0.839 0.781
anchor dir 0.412 0.414 0.413
anchor val 0.339 0.340 0.339
set 0.030 1.000 0.059
text 0.222 0.255 0.238
val 0.285 0.274 0.279
Table 3: Porting based on corpus annotations
These results partially confirm our working hy-
pothesis, showing a performance increase in terms
of the Italian TEs correctly recognized by the sys-
tem. In fact, both the timex2 attribute, which
indicates the coverage of the system (detection),
and the text attribute, which refers to the TEs
extent determination (bracketing), are slightly in-
creased. This may lead to the conclusion that auto-
matic porting procedures can actually benefit from
language-specific knowledge derived from a cor-
pus.
However, looking at the other TIMEX2 at-
tributes, the situation is not so clear due to the less
coherent behaviour of the system on normaliza-
tion. While for two attributes (anchor dir and an-
chor val) the system performs better, for the other
two (set and val) a performance drop is observed.
A possible reason for that could be related to the
limited number of TE examples that can be ex-
tracted from the Italian corpus (whose dimensions
are relatively small compared to the annotated cor-
pora available for English). In fact, compared to
the sum of English and Spanish examples used for
the translation-based porting procedure, the Ital-
ian expressions present in the corpus are fewer and
repetitive. For instance, with 131, 140, and 30 oc-
currences, the expressions ?oggi? (?today?), ?ieri?
(?yesterday?), and ?domani? (?tomorrow?) repre-
sent around 12.5% of the 2,393 Italian TEs con-
tained in the I-CAB training set.
4.3 Combining online translators and an
annotated corpus
In light of the previous considerations, a third ex-
periment has been conducted combining the top-
down approach proposed in Section 4.1 and the
bottom-up approach proposed in Section 4.2. The
underlying hypothesis is that the induction of an
effective temporal model for Italian can bene-
fit from the combination of the large amount of
examples coming from translations on the one
34
side, and from the more precise language-specific
knowledge derived from the corpus on the other.
To check the validity of this hypothesis, the pro-
cess described in Section 4.2 has been modified
adding an additional phase. In this phase, the set
of TEs derived from I-CAB is augmented with the
expressions already available in the Spanish and
English TE sets. The new porting process is car-
ried out in the following steps:
1. TE Collection and translation. The Italian ex-
pressions are collected from the I-CAB train-
ing portion, and translated both into Spanish
and English.
2. Normalization rules assignment. With the
same methodology described in Section 4.2
(step 2), the Italian TEs mined from the cor-
pus are mapped onto the appropriate normal-
ization rules assigned to their translations.
3. TE set augmentation. The set of Italian TEs
is automatically augmented with new expres-
sions derived from the Spanish and English
TE sets. As described in Section 4.1, these
expressions are first translated into Italian us-
ing on-line translators, then filtered through
Web searches. The remaining TEs are in-
cluded in the Italian model, and related to the
same normalization rules assigned to the cor-
responding Spanish or English TEs.
Also this porting experiment was carried out
with minimal modifications of the existing code.
The automatic acquisition of the new model for
Italian required around one hour. Evaluation re-
sults, calculated over the I-CAB test set are pre-
sented in Table 4.
Prec Rec F
timex2 0.726 0.834 0.776
anchor dir 0.578 0.475 0.521
anchor val 0.516 0.424 0.465
set 0.182 1.000 0.308
text 0.258 0.296 0.276
val 0.564 0.545 0.555
Table 4: Porting based on corpus annotations and
online translators
As can be seen from the table, the combina-
tion of the two approaches leads to an overall per-
formance improvement with respect to the previ-
ous experiments. Apart from a slight decrease in
terms of detection (timex2 attribute), both brack-
eting and normalization performance benefit from
such combination. The improvement on bracket-
ing (text attribute) is around 4% with respect to
both the previous experiments. On average, the
improvement for the normalization attributes is
around 15% with respect to the translation-based
method (ranging from +4,5% for the set attribute,
to +20% for the val attribute), and 20% with re-
spect to the corpus-based method (ranging from
+11% for the anchor dir attribute, to +30% for
the set attribute). These performance improve-
ments are summarized in Table 5, which reports
the F-Measure scores achieved by the three port-
ing approaches.
F-Tran. F-Corpus F-Comb.
timex2 0.775 0.781 0.776
anchor dir 0.311 0.413 0.521
anchor val 0.300 0.339 0.465
set 0.152 0.059 0.308
text 0.263 0.238 0.276
val 0.232 0.279 0.555
Table 5: F-Measure scores comparison
These results confirm the validity of our work-
ing hypothesis, showing that:
? taken in isolation, both the knowledge de-
rived from models built for other languages,
and the language-specific knowledge derived
from an annotated corpus, have a limited im-
pact on the system?s performance;
? taken in combination, the top-down and the
bottom-up approaches can complement each
other, allowing to cope with the complexity
of the porting task.
5 Comparing TERSEO with a
language-specific system
For the sake of completeness, the results achieved
by our combined porting procedure have been
compared with those achieved, over the I-CAB
test set, by a system specifically designed for
Italian. The ITA-Chronos system (Negri and
Marseglia, 2004), a multilingual system for the
recognition and normalization of TEs in Italian
and English, has been used for this purpose. Up to
date, being among the two top performing systems
at TERN 2004, Chronos represents the state-of-
the-art with respect to the TERN task. In addition,
to the best of our knowledge, this is the only sys-
tem effectively dealing with the Italian language.
35
Like all the other state-of-the-art systems ad-
dressing the recognition/normalization task, ITA-
Chronos is a rule-based system. From a design
point of view, it shares with TERSEO a rather
similar architecture which relies on different sets
of rules. These are regular expressions that check
for specific features of the input text, such as the
presence of particular word senses, lemmas, parts
of speech, symbols, or strings satisfying specific
predicates12 . Each set of rules is in charge of
dealing with different aspects of the problem. In
particular, a set of around 350 rules is designed
for TE recognition and is capable of recognizing
with high Precision/Recall rates a broad variety of
TEs. Other sets of regular expressions, for a total
of around 700 rules, are used in the normalization
phase, and are in charge of handling each specific
TIMEX2 normalization attribute. The results ob-
tained by the Italian version of Chronos over the
I-CAB test set are shown in Table 6.
Prec Rec F F-Comb
timex2 0.925 0.908 0.917 0.776 (-14%)
anchor dir 0.733 0.636 0.681 0.521 (-16%)
anchor val 0.495 0.462 0.478 0.465 (-1.3%)
set 0.616 0.500 0.552 0.308 (-24%)
text 0.859 0.843 0.851 0.276 (-57%)
val 0.636 0.673 0.654 0.555 (-10%)
Table 6: Evaluation of ITA-Chronos over the I-
CAB test set
As expected, the distance between the results
obtained by ITA-Chronos and the best Italian sys-
tem automatically obtained from TERSEO (F-
Comb) is considerable. On average, in terms of
F-Measure, the scores obtained by ITA-TERSEO
are 20% lower, ranging from -1.3% for the an-
chor val attribute, to -57% for the text attribute.
However, going beyond the raw numbers, a com-
prehensive evaluation must also take into account
the great difference, in terms of the required time,
effort, and resources deployed in the development
of the two systems. While the implementation of
the manual one took several months, the automatic
porting procedure of TERSEO to Italian (in all the
three modalities described in this paper) is a very
fast process that can be accomplished in less than
an hour. Considering the trade-off between per-
formance and effort required for system?s devel-
12For instance, the predicates ?Weekday-p? and
?Time Unit-p? are respectively satisfied by strings such
as ?Monday?, ?Tuesday?, ..., ?Sunday?, and ?second?,
?minute?, ?hour?, ?day?, ..., ?century?. Of course, this also
holds for the Italian equivalents of these expressions
opment, the proposed methodology represents a
viable solution to attack the porting problem.
6 Conclusions
In this paper, the problem of automatically extend-
ing to new languages a rule-based system for TE
recognition and normalization has been addressed.
Adopting an incremental approach, different port-
ing strategies, for the creation of an Italian system
starting from an already available Spanish system,
have been evaluated and discussed. Each exper-
iment has been carried out considering the con-
tribution of different knowledge sources for rules
translation. Firstly, the contribution given by the
output of online translators has been evaluated,
showing detection performances in line with a pre-
viously developed English extension of the sys-
tem, but a performance drop in terms of normal-
ization performance. Then, the contribution of
knowledge mined from an annotated corpus has
been considered. Results show a performance in-
crease in terms of detection and bracketing, but
a less coherent behaviour in terms of normaliza-
tion. Finally, a combined approach has been ex-
perimented, resulting in an overall performance
increase. System?s performance is still far from
the results obtained by a state-of-the-art system for
Italian but, considering the trade-off between per-
formance and effort required for system?s devel-
opment, results are encouraging.
References
L. Ferro, L. Gerber, I. Mani, B. Sundheim, and G. Wil-
son. 2005. Tides.2005 standard for the annotation
of temporal expressions. Technical report, MITRE.
K. Hacioglu, Y. Chen, and B. Douglas. 2005. Time
Expression Labeling for English and Chinese Text.
In Proceedings of CICLing 2005, pages 548?559.
B. Magnini, E. Pianta, C. Girardi, M. Negri, L. Ro-
mano, M. Speranza, and R. Sprugnoli. 2006. I-
CAB: the Italian Content Annotation Bank. In Pro-
ceedings of LREC 2006. To appear.
M. Negri and L. Marseglia. 2004. Recognition and
normalization of time expressions: Itc-irst at tern
2004. Technical report, ITC-irst, Trento.
E. Saquete, P. Martnez-Barco, and R. Muoz. 2004.
Evaluation of the automatic multilinguality for time
expression resolution. In DEXA Workshops, pages
25?30. IEEE Computer Society.
E. Saquete, R. Muoz, and P. Martnez-Barco. 2005.
Event ordering using TERSEO system. Data and
36
Knowledge Engineering Journal, page (To be pub-
lished).
E. Saquete, P. Martinez-Barco, R. Munoz R., M. Ne-
gri, M. Speranza, and R. Sprugnoli R. 2006a. Au-
tomatic resolution rule assignment to multilingual
temporal expressions using annotated corpora. In
Proceedings of the TIME 2006 International Sym-
posium on Temporal Representation and Reasoning.
To Appear.
E. Saquete, P. Martinez-Barco, R. Munoz R., M. Negri,
M. Speranza, and R. Sprugnoli R. 2006b. Multilin-
gual Extension of a Temporal Expression Normal-
izer using Annotated Corpora. In Proceedings of the
EACL Workshop on Cross-Language Knowledge In-
duction.
E. Saquete. 2005. Temporal information Resolution
and its application to Temporal Question Answer-
ing. Phd, Departamento de Lenguages y Sistemas
Informa?ticos. Universidad de Alicante, June.
37
A proposal to automatically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia
Antonio Toral
University of Alicante
Carretera San Vicente S/N
Alicante 03690, Spain
atoral@dlsi.ua.es
Rafael Mun?oz
University of Alicante
Carretera San Vicente S/N
Alicante 03690, Spain
rafael@dlsi.ua.es
Abstract
This paper describes a method to automat-
ically create and maintain gazetteers for
Named Entity Recognition (NER). This
method extracts the necessary information
from linguistic resources. Our approach is
based on the analysis of an on-line ency-
clopedia entries by using a noun hierarchy
and optionally a PoS tagger. An impor-
tant motivation is to reach a high level of
language independence. This restricts the
techniques that can be used but makes the
method useful for languages with few re-
sources. The evaluation carried out proves
that this approach can be successfully used
to build NER gazetteers for location (F
78%) and person (F 68%) categories.
1 Introduction
Named Entity Recognition (NER) was defined at
the MUC conferences (Chinchor, 1998) as the task
consisting of detecting and classifying strings of
text which are considered to belong to different
classes (e.g. person, location, organization, date,
time). Named Entities are theoretically identified
and classified by using evidence. Two kinds of
evidence have been defined (McDonald, 1996).
These are internal and external evidence. Internal
evidence is the one provided from within the se-
quence of words that constitute the entity. In con-
trast, external evidence is the criteria that can be
obtained by the context in which entities appear.
Since the time NER was introduced, mainly two
approaches have been adopted to deal with this
task. One is referred as knowledge-based and uses
explicit resources like rules and gazetteers, which
commonly are hand-crafted. The other follows the
learning paradigm and usually uses as a resource a
tagged corpus which is used to train a supervised
learning algorithm.
In the knowledge-based approach two kind of
gazetteers can be distinguished. On one hand there
are trigger gazetteers, which contain key words
that indicate the possible presence of an entity of
a given type. These words usually are common
nouns. E.g. ms. indicates that the entity after it
is a person entity. On the other hand there are en-
tity gazetteers which contain entities themselves,
which usually are proper nouns. E.g. Portugal
could be an instance in a location gazetteer.
Initially, and specially for the MUC confer-
ences, most of the NER systems developed did
belong to the knowledge-based approach. This ap-
proach proved to be able to obtain high scores. In
fact, the highest score obtained by a knowledge-
based system in MUC-7 reached F 93.39 %
(Mikheev et al, 1998). However, this approach
has an important problem: gazetteers and rules are
difficult and tedious to develop and to maintain. If
the system is to be used for an open domain, lin-
guistic experts are needed to build the rules, and
besides, it takes too much time to tune these re-
sources in order to obtain satisfactory results. Be-
cause of this, lately most of the research falls into
the learning-based paradigm.
Regarding the creation and maintenance of
gazetteers, several problems have been identified,
these are mainly:
? Creation and maintenance effort
? Overlaps between gazetteers
The first problem identified assumes that the
gazetteers are manually created and maintained.
However, this is not always the case. Gazetteers
56
could be automatically created and maintained by
extracting the necessary information from avail-
able linguistic resources, which we think is a
promising line of future research.
Several research works have been carried out in
this direction. An example of this is a NER sys-
tem which uses trigger gazetteers automatically
extracted from WordNet (Magnini et al, 2002)
by using wordnet predicates. The advantage in
this case is that the resource used is multilingual
and thus, porting it to another language is almost
straightforward (Negri and Magnini, 2004).
There is also a work that deals with automat-
ically building location gazetteers from internet
texts by applying text mining procedures (Ouri-
oupina, 2002), (Uryupina, 2003). However, this
work uses linguistic patterns, and thus is language
dependent. The author claims that the approach
may successfully be used to create gazetteers for
NER.
We agree with (Magnini et al, 2002) that in or-
der to automatically create and maintain trigger
gazetteers, using a hierarchy of common nouns is
a good approach. Therefore, we want to focus on
the automatically creation and maintenance of en-
tity gazetteers. Another reason for this is that the
class of common nouns (the ones being triggers) is
much more stable than the class of proper names
(the ones in entity gazetteers). Because of this,
the maintenance of the latter is important as new
entities to be taken into account appear. For exam-
ple, if we refer to presidents, the trigger word used
might be ?president? and it is uncommon that the
trigger used to refer to them changes over time.
On the other hand, the entities being presidents
change as new presidents appear and current pres-
idents will disappear.
Our aim is to find a method which allow us to
automatically create and maintain entity gazetteers
by extracting the necessary information from lin-
guistic resources. An important restriction though,
is that we want our method to be as independent of
language as possible.
The rest of this paper is structured as follows.
In the next section we discuss about our proposal.
Section three presents the results we have obtained
and some comments about them. Finally, in sec-
tion four we outline our conclusions and future
work.
2 Approach
In this section we present our approach to auto-
matically build and maintain dictionaries of proper
nouns. In a nutshell, we analyse the entries of an
encyclopedia with the aid of a noun hierarchy. Our
motivation is that proper nouns that form entities
can be obtained from the entries in an encyclo-
pedia and that some features of their definitions
in the encyclopedia can help to classify them into
their correct entity category.
The encyclopedia used has been Wikipedia1 .
According to the English version of Wikipedia
2
, Wikipedia is a multi-lingual web-based, free-
content encyclopedia which is updated continu-
ously in a collaborative way. The reasons why we
have chosen this encyclopedia are the following:
? It is a big source of information. By De-
cember 2005, it has over 2,500,000 defini-
tions. The English version alone has more
than 850,000 entries.
? Its content has a free license, meaning that it
will always be available for research without
restrictions and without needing to acquire
any license.
? It is a general knowledge resource. Thus, it
can be used to extract information for open
domain systems.
? Its data has some degree of formality and
structure (e.g. categories) which helps to pro-
cess it.
? It is a multilingual resource. Thus, if we are
able to develop a language independent sys-
tem, it can be used to create gazetteers for any
language for which Wikipedia is available.
? It is continuously updated. This is a very
important fact for the maintenance of the
gazetteers.
The noun hierarchy used has been the noun hi-
erarchy from WordNet (Miller, 1995). This is a
widely used resource for NLP tasks. Although
initially being a monolingual resource for the En-
glish language, a later project called EuroWordNet
(Vossen, 1998), provided wordnet-like hierarchies
1http://www.wikipedia.org
2http://en.wikipedia.org/wiki/Main Page
57
for a set of languages of the European Union. Be-
sides, EuroWordNet defines a language indepen-
dent index called Inter-Lingual-Index (ILI) which
allows to establish relations between words in
wordnets of different languages. The ILI facili-
tates also the development of wordnets for other
languages.
From this noun hierarchy we consider the nodes
(called synsets in WordNet) which in our opinion
represent more accurately the different kind of en-
tities we are working with (location, organization
and person). For example, we consider the synset
6026 as the corresponding to the entity class Per-
son. This is the information contained in synset
number 6026:
person, individual, someone,
somebody, mortal,
human, soul -- (a human being;
"there was too much for one person
to do")
Given an entry from Wikipedia, a PoS-tagger
(Carreras et al, 2004) is applied to the first sen-
tence of its definition. As an example, the first
sentence of the entry Portugal in the Simple En-
glish Wikipedia 3 is presented here:
Portugal portugal NN
is be VBZ
a a DT
country country NN
in in IN
the the DT
south-west south-west NN
of of IN
Europe Europe NP
. . Fp
For every noun in a definition we obtain the
synset of WordNet that contains its first sense4.
We follow the hyperonymy branch of this synset
until we arrive to a synset we have considered be-
longing to an entity class or we arrive to the root of
the hierarchy. If we arrive to a considered synset,
then we consider that noun as belonging to the en-
tity class of the considered synset. The following
example may clarify this explanation:
portugal --> LOCATION
3http://simple.wikipedia.org/wiki/Portugal
4We have also carried out experiments taking into account
all the senses provided by WordNet. However, the perfor-
mance obtained is not substantially better while the process-
ing time increases notably.
country --> LOCATION
south-west --> NONE
europe --> LOCATION
As it has been said in the abstract, the appli-
cation of a PoS tagger is optional. The algorithm
will perform considerably faster with it as with the
PoS data we only need to process the nouns. If a
PoS tagger is not available for a language, the al-
gorithm can still be applied. The only drawback
is that it will perform slower as it needs to pro-
cess all the words. However, through our experi-
mentation we can conclude that the results do not
significantly change.
Finally, we apply a weighting algorithm which
takes into account the amount of nouns in the defi-
nition identified as belonging to the different entity
types considered and decides to which entity type
the entry belongs. This algorithm has a constant
Kappa which allows to increase or decrease the
distance required within categories in order to as-
sign an entry to a given class. The value of Kappa
is the minimum difference of number of occur-
rences between the first and second most frequent
categories in an entry in order to assign the entry
to the first category. In our example, for any value
of Kappa lower than 4, the algorithm would say
that the entry Portugal belongs to the location en-
tity type.
Once we have this basic approach we apply dif-
ferent heuristics which we think may improve the
results obtained and which effect will be analysed
in the section about results.
The first heuristic, called is instance, tries to de-
termine whether the entries from Wikipedia are in-
stances (e.g. Portugal) or word classes (e.g. coun-
try). This is done because of the fact that named
entities only consider instances. Therefore, we are
not interested in word classes. We consider that an
entry from Wikipedia is an instance when it has an
associated entry in WordNet and it is an instance.
The procedure to determine if an entry from Word-
Net is an instance or a word class is similar to the
one used in (Magnini et al, 2002).
The second heuristic is called is in wordnet. It
simply determines if the entries from Wikipedia
have an associated entry in WordNet. If so, we
may use the information from WordNet to deter-
mine its category.
58
3 Experiments and results
We have tested our approach by applying it to
3517 entries of the Simple English Wikipedia
which were randomly selected. Thus, these en-
tries have been manually tagged with the expected
entity category5. The distribution by entity classes
can be seen in table 1:
As it can be seen in table 1, the amount of enti-
ties of the categories Person and Location are bal-
anced but this is not the case for the type Organi-
zation. There are very few instances of this type.
This is understandable as in an encyclopedia lo-
cations and people are defined but this is not the
usual case for organizations.
According to what was said in section 2, we
considered the heuristics explained there by car-
rying out two experiments. In the first one we
applied the is instance heuristic. The second ex-
periment considers the two heuristics explained in
section 2 (is instance and is in wordnet). We do
not present results without the first heuristic as
through our experimentation it proved to increase
both recall and precision for every entity category.
For each experiment we considered two values
of a constant Kappa which is used in our algo-
rithm. The values are 0 and 2 as through exper-
imentation we found these are the values which
provide the highest recall and the highest preci-
sion, respectively. Results for the first experiment
can be seen in table 2 and results for the second
experiment in table 3.
As it can be seen in these tables, the best re-
call for all classes is obtained in experiment 2 with
Kappa 0 (table 3) while the best precision is ob-
tained in experiment 1 with Kappa 2 (table 2).
The results both for location and person cat-
egories are in our opinion good enough to the
purpose of building and maintaining good quality
gazetteers after a manual supervision. However,
the results obtained for the organization class are
very low. This is mainly due to the fact of the
high interaction between this category and loca-
tion combined with the practically absence of tra-
ditional entities of the organization type such as
companies. This interaction can be seen in the in-
depth results which presentation follows.
In order to clarify these results, we present more
in-depth data in tables 4 and 5. These tables
present an error analysis, showing the false posi-
5This data is available for research at http://www.
dlsi.ua.es/?atoral/index.html\#resources
tives, false negatives, true positives and true nega-
tives among all the categories for the configuration
that provides the highest recall (experiment 2 with
Kappa 0) and for the one that provides the highest
precision (experiment 1 with Kappa 2).
In tables 4 and 5 we can see that the interactions
within classes (occurrences tagged as belonging to
one class but NONE and guessed as belonging to
other different class but NONE) is low. The only
case in which it is significant is between location
and organization. In table 5 we can see that 12 en-
tities tagged as organization are classified as LOC
while 20 tagged as organization are guessed with
the correct type. Following with these, 5 entities
tagged as location where classified as organiza-
tion. This is due to the fact that countries and
related entities such as ?European Union? can be
considered both as organizations or locations de-
pending on their role in a text.
4 Conclusions
We have presented a method to automatically cre-
ate and maintain entity gazetteers using as re-
sources an encyclopedia, a noun hierarchy and,
optionally, a PoS tagger. The method proves to be
helpful for these tasks as it facilitates the creation
and maintenance of this kind of resources.
In our opinion, the principal drawback of our
system is that it has a low precision for the con-
figuration for which it obtains an acceptable value
of recall. Therefore, the automatically created
gazetteers need to pass a step of manual supervi-
sion in order to have a good quality.
On the positive side, we can conclude that our
method is helpful as it takes less time to automat-
ically create gazetteers with our method and after
that to supervise them than to create that dictio-
naries from scratch. Moreover, the updating of the
gazetteers is straightforward; just by executing the
procedure, the new entries in Wikipedia (the en-
tries that did not exist at the time the procedure
was performed the last time) would be analysed
and from these set, the ones detected as entities
would be added to the corresponding gazetteers.
Another important fact is that the method has
a high degree of language independence; in or-
der to apply this approach to a new language, we
need a version of Wikipedia and WordNet for that
language, but the algorithm and the process does
not change. Therefore, we think that our method
can be useful for the creation of gazetteers for lan-
59
Entity type Number of instances Percentage
NONE 2822
LOC 404 58
ORG 55 8
PER 236 34
Table 1: Distribution by entity classes
k LOC ORG PER
prec rec F?=1 prec rec F?=1 prec rec F?=1
0 66.90 94.55 78.35 28.57 18.18 22.22 61.07 77.11 68.16
2 86.74 56.68 68.56 66.66 3.63 6.89 86.74 30.50 45.14
Table 2: Experiment 1. Results applying is instance heuristic
k LOC ORG PER
prec rec F?=1 prec rec F?=1 prec rec F?=1
0 62.88 96.03 76.00 16.17 20.00 17.88 43.19 84.74 57.22
2 77.68 89.60 83.21 13.95 10.90 12.24 46.10 62.71 53.14
Table 3: Experiment 2. Results applying is instance and is in wordnet heuristics
Tagged Guessed
NONE LOC ORG PER
NONE 2777 33 1 11
LOC 175 229 0 0
ORG 52 1 2 0
PER 163 1 0 72
Table 4: Results fn-fp (results 1 k=2)
Tagged Guessed
NONE LOC ORG PER
NONE 2220 196 163 243
LOC 8 387 5 4
ORG 20 12 20 3
PER 30 9 2 195
Table 5: Results fn-fp (results 2 k=0)
60
guages in which NER gazetteers are not available
but have Wikipedia and WordNet resources.
During the development of this research, several
future works possibilities have appeared. Regard-
ing the task we have developed, we consider to
carry out new experiments incorporating features
that Wikipedia provides such as links between
pairs of entries. Following with this, we consider
to test more complex weighting techniques for our
algorithm.
Besides, we think that the resulting gazetteers
for the configurations that provide high precision
and low recall, although not being appropriate for
building gazetteers for NER systems, can be in-
teresting for other tasks. As an example, we con-
sider to use them to extract verb frequencies for
the entity categories considered which can be later
used as features for a learning based Named Entity
Recogniser.
Acknowledgements
This research has been partially funded by the
Spanish Government under project CICyT num-
ber TIC2003-07158-C04-01 and by the Valencia
Government under project number GV04B-268.
We also would like to specially thank Borja
Navarro for his valuable help on WordNet.
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An Open-Source Suite of Language Ana-
lyzers. In Proceedings of the 4th LREC Conference.
N. Chinchor. 1998. Overview of MUC-7. In Proceed-
ings of the Seventh Message Understanding Confer-
ence (MUC-7).
B. Magnini, M. Negri, R. Preete, and H. Tanev. 2002.
A wordnet-based approach to named entities recog-
nition. In Proceedings of SemaNet ?02: Building
and Using Semantic Networks, pages 38?44.
D. McDonald. 1996. Internal and external evidence
in the identification and semantic categorization of
proper names. Corpus Processing for Lexical Aqui-
sition, pages 21?39, chapter 2.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG system used for MUC-7. In Seventh
Message Understanding Conference (MUC-7): Pro-
ceedings of a Conference held in Fairfax, Virginia,
29 April-1 May.
G. A. Miller. 1995. Wordnet: A lexical database for
english. Communications of ACM, (11):39?41.
M. Negri and B. Magnini. 2004. Using wordnet pred-
icates for multilingual named entity recognition. In
Proceedings of The Second Global Wordnet Confer-
ence, pages 169?174.
O. Ourioupina. 2002. Extracting geographical knowl-
edge from the internet. In Proceedings of the ICDM-
AM International Workshop on Active Mining.
O. Uryupina. 2003. Semi-supervised learning of geo-
graphical gazetteers from the internet. In Proceed-
ings of the HLT-NAACL 2003 Workshop on Analysis
of Geographic References, pages 18?25.
P. Vossen. 1998. Introduction to eurowordnet. Com-
puters and the Humanities, 32:73?89.
61
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 73?80,
Rochester, April 2007 c?2007 Association for Computational Linguistics
DLSITE-2: Semantic Similarity Based on Syntactic
Dependency Trees Applied to Textual Entailment
Daniel Micol, O?scar Ferra?ndez, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{dmicol, ofe, rafael, mpalomar}@dlsi.ua.es
Abstract
In this paper we attempt to deduce tex-
tual entailment based on syntactic depen-
dency trees of a given text-hypothesis pair.
The goals of this project are to provide an
accurate and fast system, which we have
called DLSITE-2, that can be applied in
software systems that require a near-real-
time interaction with the user. To accom-
plish this we use MINIPAR to parse the
phrases and construct their correspond-
ing trees. Later on we apply syntactic-
based techniques to calculate the seman-
tic similarity between text and hypothe-
sis. To measure our method?s precision we
used the test text corpus set from Second
PASCAL Recognising Textual Entailment
Challenge (RTE-2), obtaining an accuracy
rate of 60.75%.
1 Introduction
There are several methods used to determine tex-
tual entailment for a given text-hypothesis pair. The
one described in this paper uses the information
contained in the syntactic dependency trees of such
phrases to deduce whether there is entailment or
not. In addition, semantic knowledge extracted from
WordNet (Miller et al, 1990) has been added to
achieve higher accuracy rates.
It has been proven in several competitions and
other workshops that textual entailment is a complex
task. One of these competitions is PASCAL Recog-
nising Textual Entailment Challenge (Bar-Haim et
al., 2006), where each participating group develops a
textual entailment recognizing system attempting to
accomplish the best accuracy rate of all competitors.
Such complexity is the reason why we use a combi-
nation of various techniques to deduce whether en-
tailment is produced.
Currently there are few research projects related
to the topic discussed in this paper. Some systems
use syntactic tree matching as the textual entailment
decision core module, such as (Katrenko and Adri-
aans, 2006). It is based on maximal embedded syn-
tactic subtrees to analyze the semantic relation be-
tween text and hypothesis. Other systems use syn-
tactic trees as a collaborative module, not being the
core, such as (Herrera et al, 2006). The application
discussed in this paper belongs to the first set of sys-
tems, since syntactic matching is its main module.
The remainder of this paper is structured as fol-
lows. In the second section we will describe the
methods implemented in our system. The third one
contains the experimental results, and the fourth and
last discusses such results and proposes future work
based on our actual research.
2 Methods
The system we have built aims to provide a good
accuracy rate in a short lapse of time, making it
feasible to be included in applications that require
near-real-time responses due to their interaction with
the user. Such a system is composed of few mod-
ules that behave collaboratively. These include tree
construction, filtering, embedded subtree search and
graph node matching. A schematic representation of
the system architecture is shown in Figure 1.
73
Figure 1: DLSITE-2 system architecture.
Each of the steps or modules of DLSITE-2 is de-
scribed in the following subsections, that are num-
bered sequentially according to their execution or-
der.
2.1 Tree generation
The first module constructs the corresponding syn-
tactic dependency trees. For this purpose, MINI-
PAR (Lin, 1998) output is generated and afterwards
parsed for each text and hypothesis of our corpus.
Phrase tokens, along with their grammatical infor-
mation, are stored in an on-memory data structure
that represents a tree, which is equivalent to the men-
tioned syntactic dependency tree.
2.2 Tree filtering
Once the tree has been constructed, we may want
to discard irrelevant data in order to reduce our sys-
tem?s response time and noise. For this purpose we
have generated a database of relevant grammatical
categories, represented in Table 1, that will allow
us to remove from the tree all those tokens whose
category does not belong to such list. The result-
ing tree will have the same structure as the original,
but will not contain any stop words nor irrelevant to-
kens, such as determinants or auxiliary verbs. The
whole list of ignored grammatical categories is rep-
resented in Table 2.
We have performed tests taking into account and
discarding each grammatical category, which has al-
lowed us to generate both lists of relevant and ig-
nored grammatical categories.
Verbs, verbs with one argument, verbs with two ar-
guments, verbs taking clause as complement, verb
Have, verb Be
Nouns
Numbers
Adjectives
Adverbs
Noun-noun modifiers
Table 1: Relevant grammatical categories.
2.3 Graph embedding detection
The next step of our system consists in determining
whether the hypothesis? tree is embedded into the
text?s. Let us first define the concept of embedded
tree (Katrenko and Adriaans, 2006).
Definition 1: Embedded tree A tree
T1 = (V1, E1) is embedded into another
one T2 = (V2, E2) iff
1. V1 ? V2, and
2. E1 ? E2
where V1 and V2 represent the vertices,
and E1 and E2 the edges.
In other words, a tree, T1, is embedded into an-
other one, T2, if all nodes and branches of T1 are
present in T2.
We believe that it makes sense to reduce the strict-
ness of such a definition to allow the appearance
of intermediate nodes in the text?s branches that are
74
Determiners
Pre-determiners
Post-determiners
Clauses
Inflectional phrases
Preposition and preposition phrases
Specifiers of preposition phrases
Auxiliary verbs
Complementizers
Table 2: Ignored grammatical categories.
not present in the corresponding hypothesis? branch,
which means that we allow partial matching. There-
fore, a match between two branches will be pro-
duced if all nodes of the first one, namely ?1 ? E1,
are present in the second, namely ?2 ? E2, and their
respective order is the same, allowing the possibil-
ity of appearance of intermediate nodes that are not
present in both branches. This is also described in
(Katrenko and Adriaans, 2006).
To determine whether the hypothesis? tree is em-
bedded into the text?s, we perform a top-down
matching process. For this purpose we first compare
the roots of both trees. If they coincide, we then pro-
ceed to compare their respective child nodes, which
are the tokens that have some sort of dependency
with their respective root token.
In order to add more flexibility to our system,
we do not require the pair of tokens to be ex-
actly the same, but rather set a threshold that rep-
resents the minimum similarity value between them.
This is a difference between our approach and the
one described in (Katrenko and Adriaans, 2006).
Such a similarity is calculated by using the Word-
Net::Similarity tool (Pedersen et al, 2004), and,
concretely, the Wu-Palmer measure, as defined in
Equation 1 (Wu and Palmer, 1994).
Sim(C1, C2) =
2N3
N1 +N2 + 2N3
(1)
where C1 and C2 are the synsets whose similarity
we want to calculate, C3 is their least common su-
perconcept, N1 is the number of nodes on the path
from C1 to C3, N2 is the number of nodes on the
path from C2 to C3, and N3 is the number of nodes
on the path from C3 to the root. All these synsets
and distances can be observed in Figure 2.
Figure 2: Distance between two synsets.
If the similarity rate is greater or equal than the
established threshold, which we have set empirically
to 80%, we will consider the corresponding hypoth-
esis? token as suitable to have the same meaning
as the text?s token, and will proceed to compare its
child nodes in the hypothesis? tree. On the other
hand, if such similarity value is less than the cor-
responding threshold, we will proceed to compare
the children of such text?s tree node with the actual
hypothesis? node that was being analyzed.
The comparison between the syntactic depen-
dency trees of both text and hypothesis will be com-
pleted when all nodes of either tree have been pro-
cessed. If we have been able to find a match for all
the tokens within the hypothesis, the corresponding
tree will be embedded into the text?s and we will be-
lieve that there is entailment. If not, we will not be
able to assure that such an implication is produced
and will proceed to execute the next module of our
system.
Next, we will present a text-hypothesis pair sam-
ple where the syntactic dependency tree of the hy-
pothesis (Figure 3(b)) is embedded into the text?s
(Figure 3(a)). The mentioned text-hypothesis pair
is the following:
Text: Mossad is one of the world?s most
well-known intelligence agencies, and is
often viewed in the same regard as the CIA
and MI6.
Hypothesis: Mossad is an intelligence
agency.
75
(a) Mossad is one of the world?s most well-known intelligence agencies, and is often viewed
in the same regard as the CIA and MI6.
(b) Mossad is an intelligence
agency.
Figure 3: Representation of a hypothesis? syntactic dependency tree that is embedded into the text?s.
As one can see in Figure 3, the hypothesis? syn-
tactic dependency tree represented is embedded into
the text?s because all of its nodes are present in
the text in the same order. There is one exception
though, that is the word an. However, since it is a
determinant, the filtering module will have deleted
it before the graph embedding test is performed.
Therefore, in this example the entailment would be
recognized.
2.4 Graph node matching
Once the embedded subtree comparison has fin-
ished, and if its result is negative, we proceed to per-
form a graph node matching process, termed align-
ment, between both the text and the hypothesis. This
operation consists in finding pairs of tokens in both
trees whose lemmas are identical, no matter whether
they are in the same position within the tree. We
would like to point out that in this step we do not
use the WordNet::Similarity tool.
Some authors have already designed similar
matching techniques, such as the ones described in
(MacCartney et al, 2006) and (Snow et al, 2006).
However, these include semantic constraints that we
have decided not to consider. The reason of this
decision is that we desired to overcome the textual
entailment recognition from an exclusively syntactic
perspective. Therefore, we did not want this module
to include any kind of semantic knowledge.
The weight given to a token that has been found
in both trees will depend on the depth in the hypoth-
esis? tree and the token?s grammatical relevance.
The first of these factors depends on an empirically-
calculated weight that assigns less importance to a
node the deeper it is located in the tree. This weight
is defined in Equation 2. The second factor gives
different relevance depending on the grammatical
category and relationship. For instance, a verb will
have the highest weight, while an adverb or an ad-
jective will have less relevance. The values assigned
to each grammatical category and relationship are
also empirically-calculated and are shown in Tables
3 and 4, respectively.
Grammatical category Weight
Verbs, verbs with one argument, verbs
with two arguments, verbs taking
clause as complement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun mod-
ifiers
0.5
Verbs Have and Be 0.3
Table 3: Weights assigned to the grammatical cate-
gories.
76
Grammatical relationship Weight
Subject of verbs, surface subject, ob-
ject of verbs, second object of ditran-
sitive verbs
1.0
The rest 0.5
Table 4: Weights assigned to the grammatical rela-
tionships.
Let ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively. We as-
sume we have found members of a synset, namely ?,
present in both ? and ?. Now let ? be the weight as-
signed to ??s grammatical category (defined in Table
3), ? the weight of ??s grammatical relationship (de-
fined in Table 4), ? an empirically-calculated value
that represents the weight difference between tree
levels, and ?? the depth of the node that contains
the synset ? in ?. We define the function ?(?) as
represented in Equation 2.
?(?) = ? ? ? ? ???? (2)
The value obtained by calculating the expression
of Equation 2 would represent the relevance of a
synset in our system. The experiments performed
reveal that the optimal value for ? is 1.1.
For a given pair (? , ?), we define the set ? as the
one that contains the synsets present in both trees:
? = ? ? ? ?? ? ?, ? ? ? (3)
Therefore, the similarity rate between ? and ?, de-
noted by the symbol ?, would be defined as:
?(?, ?) =
?
???
?(?) (4)
One should note that a requirement of our sys-
tem?s similarity measure would be to be independent
of the hypothesis length. Thus, we must define the
normalized similarity rate, as shown in Equation 5.
?(?, ?) =
?(?, ?)
?
???
?(?)
=
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value, ?(?, ?), has been cal-
culated, it will be provided to the user together with
the corresponding text-hypothesis pair identifier. It
will be his responsibility to choose an appropriate
threshold that will represent the minimum similarity
rate to be considered as entailment between text and
hypothesis. All values that are under such a thresh-
old will be marked as not entailed. For this purpose,
we suggest using a development corpus in order to
obtain the optimal threshold value, as it is done in
the RTE challenges.
3 Experimental results
The experimental results shown in this paper were
obtained processing a set of text-hypothesis pairs
from RTE-2. The organizers of this challenge pro-
vide development and test corpora to the partic-
ipants, both of them containing 800 pairs manu-
ally annotated for logical entailment. It is com-
posed of four subsets, each of them correspond-
ing to typical true and false entailments in different
tasks, such as Information Extraction (IE), Informa-
tion Retrieval (IR), Question Answering (QA), and
Multi-document Summarization (SUM). For each
task, the annotators selected the same amount of true
entailments as negative ones (50%-50% split).
The organizers have also defined two measures to
evaluate the participating systems. All judgments
returned by the systems will be compared to those
manually assigned by the human annotators. The
percentage of matching judgments will provide the
accuracy of the system, i.e. the percentage of cor-
rect responses. As a second measure, the average
precision will be computed. This measure evaluates
the ability of the systems to rank all the pairs in the
corpus according to their entailment confidence, in
decreasing order from the most certain entailment to
the least. Average precision is a common evaluation
measure for system rankings that is defined as shown
in Equation 6.
AP =
1
R
n?
i=1
E(i)
#correct up to pair i
i
(6)
where n is the amount of the pairs in the test corpus,
R is the total number of positive pairs in it, i ranges
over the pairs, ordered by their ranking, and E(i) is
defined as follows:
77
E(i) =
?
?
?
1 if the i? th pair is positive,
0 otherwise.
(7)
As we previously mentioned, we tested our sys-
tem against RTE-2 development corpus, and used
the test one to evaluate it.
First, Table 5 shows the accuracy (ACC) and av-
erage precision (AP), both as a percentage, obtained
processing the development corpus from RTE-2 for
a threshold value of 68.9%, which corresponds to
the highest accuracy that can be obtained using our
system for the mentioned corpus. It also provides
the rate of correctly predicted true and false entail-
ments.
Task ACC AP TRUE FALSE
IE 52.00 51.49 54.00 50.00
IR 55.50 58.99 32.00 79.00
QA 57.50 54.72 53.00 62.00
SUM 65.00 81.35 39.00 91.00
Overall 57.50 58.96 44.50 70.50
Table 5: Results obtained for the development cor-
pus.
Next, let us show in Table 6 the results obtained
processing the test corpus, which is the one used
to compare the different systems that participated in
RTE-2, with the same threshold as before.
Task ACC AP TRUE FALSE
IE 50.50 47.33 75.00 26.00
IR 64.50 67.67 59.00 70.00
QA 59.50 58.16 80.00 39.00
SUM 68.50 75.86 49.00 88.00
Overall 60.75 57.91 65.75 55.75
Table 6: Results obtained for the test corpus.
As one can observe in the previous table, our
system provides a high accuracy rate by using
mainly syntactical measures. The number of text-
hypothesis pairs that succeeded the graph embed-
ding evaluation was three for the development cor-
pus and one for the test set, which reflects the strict-
ness of such module. However, we would like to
point out that the amount of pairs affected by the
mentioned module will depend on the corpus na-
ture, so it can vary significantly between different
corpora.
Let us now compare our results with the ones that
were achieved by the systems that participated in
RTE-2. One should note that the criteria for such
ranking is based exclusively on the accuracy, ignor-
ing the average precision value. In addition, each
participating group was allowed to submit two dif-
ferent systems to RTE-2. We will consider here the
best result of both systems for each group. The men-
tioned comparison is shown in Table 7, and contains
only the systems that had higher accuracy rates than
our approach.
Participant Accuracy
(Hickl et al, 2006) 75.38
(Tatu et al, 2006) 73.75
(Zanzotto et al, 2006) 63.88
(Adams, 2006) 62.62
(Bos and Markert, 2006) 61.62
DLSITE-2 60.75
Table 7: Comparison of some of the teams that par-
ticipated in RTE-2.
As it is reflected in Table 7, our system would
have obtained the sixth position out of twenty-four
participants, which is an accomplishment consider-
ing the limited number of resources that it has built-
in.
Since one of our system?s modules is based on
(Katrenko and Adriaans, 2006), we will compare
their results with ours to analyze whether the modi-
fications we introduced perform correctly. In RTE-
2, they obtained an accuracy rate of 59.00% for the
test corpus. The reason why we believe we have
achieved better results than their system is due to
the fact that we added semantic knowledge to our
graph embedding module. In addition, the syntactic
dependency trees to which we have applied such a
module have been previously filtered to ensure that
they do not contain irrelevant words. This reduces
the system?s noise and allows us to achieve higher
accuracy rates.
In the introduction of this paper we mentioned
that one of the goals of our system was to provide
78
a high accuracy rate in a short lapse of time. This is
one of the reasons why we chose to construct a light
system where one of the aspects to minimize was its
response time. Table 8 shows the execution times1
of our system for both development and test text cor-
pora from RTE-2. These include total and average2
response times.
Development Test
Total 1045 1023
Average 1.30625 1.27875
Table 8: DLSITE-2 response times (in seconds).
As we can see, accurate results can be obtained
using syntactic dependency trees in a short lapse of
time. However, there are some limitations that our
system does not avoid. For instance, the tree em-
bedding test is not applicable when there is no verb
entailment. This is reflected in the following pair:
Text: Tony Blair, the British Prime Minis-
ter, met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The root node of the hypothesis? tree would be
the one corresponding to the verb is. Since the en-
tailment here is implicit, there is no need for such a
verb to appear in the text. However, this is not com-
patible with our system, since is would not match
any node of the text?s tree, and thus the hypothesis?
tree would not be found embedded into the text?s.
The graph matching process would not behave
correctly either. This is due to the fact that the main
verb, which has the maximum weight because it is
the root of the hypothesis? tree and its grammatical
category has the maximum relevance, is not present
in the text, so the overall similarity score would have
a considerable handicap.
The example of limitation of our system that we
have presented is an apposition. To avoid this spe-
cific kind of situations that produce an undesired be-
havior in our system, we could add a preprocess-
ing module that transforms the phrases that have the
1The machine we used to measure the response times had an
Intel Core 2 Duo processor at 2GHz.
2Average response times are calculated diving the totals by
the number of pairs in the corpus.
structureX , Y , Z intoX is Y , andZ. For the shown
example, the resulting text and hypothesis would be
as follows:
Text: Tony Blair is the British Prime Min-
ister, and met Jacques Chirac in London.
Hypothesis: Tony Blair is the British
Prime Minister.
The transformed text would still be syntactically
correct, and the entailment would be detected since
the hypothesis? syntactic dependency tree is embed-
ded into the text?s.
4 Conclusions and future work
The experimental results obtained from this research
demonstrate that it is possible to apply a syntactic-
based approach to deduce textual entailment from a
text-hypothesis pair. We can obtain good accuracy
rates using the discussed techniques with very short
response times, which is very useful for assisting
different kinds of tasks that demand near-real-time
responses to user interaction.
The baseline we set for our system was to achieve
better results than the ones we obtained with our last
participation in RTE-2. As it is stated in (Ferra?ndez
et al, 2006), the maximum accuracy value obtained
by then was 55.63% for the test corpus. Therefore,
our system is 9.20% more accurate compared to the
one that participated in RTE-2, which represents a
considerable improvement.
The authors of this paper believe that if higher ac-
curacy rates are desired, a step-based systemmust be
constructed. This would have several preprocessing
units, such as negation detectors, multi-word associ-
ators and so on. The addition of these units would
definitely increase the response time preventing the
system from being used in real-time tasks.
Future work can be related to the cases where no
verb entailment is produced. For this purpose we
propose to extract a higher amount of semantic in-
formation that would allow us to construct a charac-
terized representation based on the input text, so that
we can deduce entailment even if there is no appar-
ent structure similarity between text and hypothesis.
This would mean to create an abstract conceptual-
ization of the information contained in the analyzed
phrases, allowing us to deduce ideas that are not
79
explicitly mentioned in the parsed text-hypothesis
pairs.
In addition, the weights and thresholds defined
in our system have been established empirically. It
would be interesting to calculate those values by
means of a machine learning algorithm and com-
pare them to the ones we have obtained empirically.
Some authors have already performed this compari-
son, being one example the work described in (Mac-
Cartney et al, 2006).
Acknowledgments
The authors of this paper would like to thank pro-
fessors Borja Navarro and Rafael M. Terol for their
help and critical comments.
This research has been supported by the under-
graduate research fellowships financed by the Span-
ish Ministry of Education and Science, the project
TIN2006-15265-C06-01 financed by such ministry,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Rod Adams. 2006. Textual Entailment Through Ex-
tended Lexical Overlap. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Johan Bos, and Katja Markert. 2006. When logical infer-
ence helps determining textual entailment (and when it
doesnt). In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
O?scar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic Forms andWordNet relation-
ships to Textual Entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Jesu?s Herrera, Anselmo Pen?as, A?lvaro Rodrigo, and Fe-
lisa Verdejo. 2006. UNED at PASCAL RTE-2 Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCC?s GROUNDHOG
System. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Venice, Italy.
Sophia Katrenko, and Pieter Adriaans. 2006. Using
Maximal Embedded Syntactic Subtrees for Textual En-
tailment Recognition. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06), NewYork City, NewYork, United States of Amer-
ica.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography 1990 3(4):235-
244.
Ted Pedersen, Siddhart Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-04), Boston, Massachus-
sets, United States of America.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American As-
sociation of Computational Linguistics (NAACL-06),
New York City, New York, United States of America.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi,
and DanMoldovan. 2006. COGEX at the Second Rec-
ognizing Textual Entailment Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
Zhibiao Wu, and Martha Palmer. 1994. Verb Semantics
and Lexical Selection. In Proceedings of the 32nd An-
nual Meeting of the Associations for Computational
Linguistics, pages 133-138, Las Cruces, New Mexico,
United States of America.
Fabio M. Zanzotto, Alessandro Moschitti, Marco Pen-
nacchiotti, and Maria T. Pazienza. 2006. Learning
textual entailment from examples. In Proceedings of
the Second PASCAL Challenges Workshop on Recog-
nising Textual Entailment, Venice, Italy.
80
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 66?71,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Perspective-Based Approach for Solving Textual Entailment Recognition
O?scar Ferra?ndez, Daniel Micol, Rafael Mun?oz, and Manuel Palomar
Natural Language Processing and Information Systems Group
Department of Computing Languages and Systems
University of Alicante
San Vicente del Raspeig, Alicante 03690, Spain
{ofe, dmicol, rafael, mpalomar}@dlsi.ua.es
Abstract
The textual entailment recognition system
that we discuss in this paper represents
a perspective-based approach composed of
two modules that analyze text-hypothesis
pairs from a strictly lexical and syntactic
perspectives, respectively. We attempt to
prove that the textual entailment recognition
task can be overcome by performing indi-
vidual analysis that acknowledges us of the
maximum amount of information that each
single perspective can provide. We compare
this approach with the system we presented
in the previous edition of PASCAL Recognis-
ing Textual Entailment Challenge, obtaining
an accuracy rate 17.98% higher.
1 Introduction
Textual entailment recognition has become a popu-
lar Natural Language Processing task within the last
few years. It consists in determining whether one
text snippet (hypothesis) entails another one (text)
(Glickman, 2005). To overcome this problem sev-
eral approaches have been studied, being the Recog-
nising Textual Entailment Challenge (RTE) (Bar-
Haim et al, 2006; Dagan et al, 2006) the most re-
ferred source for determining which one is the most
accurate.
Many of the participating groups in previous edi-
tions of RTE, including ourselves (Ferra?ndez et al,
2006), designed systems that combined a variety of
lexical, syntactic and semantic techniques. In our
contribution to RTE-3 we attempt to solve the tex-
tual entailment recognition task by analyzing two
different perspectives separately, in order to ac-
knowledge the amount of information that an indi-
vidual perspective can provide. Later on, we com-
bine both modules to obtain the highest possible ac-
curacy rate. For this purpose, we analyze the pro-
vided corpora by using a lexical module, namely
DLSITE-1, and a syntactic one, namely DLSITE-2.
Once all results have been obtained we perform a
voting process in order to take into account all sys-
tem?s judgments.
The remainder of this paper is structured as fol-
lows. Section two describes the system we have
built, providing details of the lexical and syntactic
perspectives, and explains the difference with the
one we presented in RTE-2. Third section presents
the experimental results, and the fourth one provides
our conclusions and describes possible future work.
2 System Specification
This section describes the systemwe have developed
in order to participate in RTE-3. It is based on sur-
face techniques of lexical and syntactic analysis. As
the starting point we have used our previous system
presented in the second edition of the RTE Chal-
lenge (Ferra?ndez et al, 2006). We have enriched
it with two independent modules that are intended
to detect some misinterpretations performed by this
system. Moreover, these new modules can also rec-
ognize entailment relations by themselves. The per-
formance of each separate module and their combi-
nation with our previous system will be detailed in
section three.
Next, Figure 1 represents a schematic view of the
system we have developed.
66
Figure 1: System architecture.
As we can see in the previous Figure, our sys-
tem is composed of three modules that are coordi-
nated by an input scheduler. Its commitment is to
provide the text-hypothesis pairs to each module in
order to extract their corresponding similarity rates.
Once all rates for a given text-hypothesis pair have
been calculated, they will be processed by an output
gatherer that will provide the final judgment. The
method used to calculate the final entailment deci-
sion consists in combining the outputs of both lex-
ical and syntactic modules, and these outputs with
our RTE-2 system?s judgment. The output gatherer
will be detailed later in this paper when we describe
the experimental results.
2.1 RTE-2 System
The approach we presented in the previous edition of
RTE attempts to recognize textual entailment by de-
termining whether the text and the hypothesis are re-
lated using their respective derived logic forms, and
by finding relations between their predicates using
WordNet (Miller et al, 1990). These relations have
a specific weight that provide us a score represent-
ing the similarity of the derived logic forms and de-
termining whether they are related or not.
For our participation in RTE-3 we decided to ap-
ply our previous system because it allows us to han-
dle some kinds of information that are not correctly
managed by the new approaches developed for the
current RTE edition.
2.2 Lexical Module
This method relies on the computation of a wide va-
riety of lexical measures, which basically consists of
overlap metrics. Although in other related work this
kind of metrics have already been used (Nicholson
et al, 2006), the main contribution of this module is
the fact that it only deals with lexical features with-
out taking into account any syntactic nor semantic
information. The following paragraphs list the con-
sidered lexical measures.
Simple matching: initialized to zero. A boolean
value is set to one if the hypothesis word appears in
the text. The final weight is calculated as the sum of
all boolean values and normalized dividing it by the
length of the hypothesis.
Levenshtein distance: it is similar to simple match-
ing. However, in this case we use the mentioned
distance as the similarity measure between words.
When the distance is zero, the increment value is
one. On the other hand, if such value is equal to one,
the increment is 0.9. Otherwise, it will be the inverse
of the obtained distance.
Consecutive subsequence matching: this measure
assigns the highest relevance to the appearance of
consecutive subsequences. In order to perform this,
we have generated all possible sets of consecutive
subsequences, from length two until the length in
words, from the text and the hypothesis. If we pro-
ceed as mentioned, the sets of length two extracted
from the hypothesis will be compared to the sets of
the same length from the text. If the same element is
present in both the text and the hypothesis set, then
a unit is added to the accumulated weight. This pro-
cedure is applied for all sets of different length ex-
tracted from the hypothesis. Finally, the sum of the
weight obtained from each set of a specific length is
normalized by the number of sets corresponding to
67
this length, and the final accumulated weight is also
normalized by the length of the hypothesis in words
minus one. This measure is defined as follows:
CSmatch =
|H|?
i=2
f(SHi)
|H| ? 1
(1)
where SHi contains the hypothesis? subsequences
of length i, and f(SHi) is defined as follows:
f(SHi) =
?
j?SHi
match(j)
|H| ? i+ 1
(2)
being match(j) equal to one if there exists an ele-
ment k that belongs to the set that contains the text?s
subsequences of length i, such that k = j.
One should note that this measure does not con-
sider non-consecutive subsequences. In addition, it
assigns the same relevance to all consecutive sub-
sequences with the same length. Furthermore, the
longer the subsequence is, the more relevant it will
be considered.
Tri-grams: two sets containing tri-grams of letters
belonging to the text and the hypothesis were cre-
ated. All the occurrences in the hypothesis? tri-
grams set that also appear in the text?s will increase
the accumulated weight in a factor of one unit. The
weight is normalized by the size of the hypothesis?
tri-grams set.
ROUGE measures: considering the impact of n-
gram overlap metrics in textual entailment, we be-
lieve that the idea of integrating these measures1 into
our system is very appealing. We have implemented
them as defined in (Lin, 2004).
Each measure is applied to the words, lemmas and
stems belonging to the text-hypothesis pair. Within
the entire set of measures, each one of them is con-
sidered as a feature for the training and test stages
of a machine learning algorithm. The selected one
was a Support Vector Machine due to the fact that its
properties are suitable for recognizing entailment.
2.3 Syntactic Module
The syntactic module we have built is composed of
few submodules that operate collaboratively in order
1The considered measures were ROUGE-N with n=2 and
n=3, ROUGE-L, ROUGE-W and ROUGE-S with s=2 and s=3.
to obtain the highest possible accuracy by using only
syntactic information.
The commitment of the first two submodules is
to generate an internal representation of the syntac-
tic dependency trees generated by MINIPAR (Lin,
1998). For this purpose we obtain the output of such
parser for the text-hypothesis pairs, and then process
it to generate an on-memory internal representation
of the mentioned trees. In order to reduce our sys-
tem?s noise and increase its accuracy rate, we only
keep the relevant words and discard the ones that we
believe do not provide useful information, such as
determinants and auxiliary verbs. After this step has
been performed we can proceed to compare the gen-
erated syntactic dependency trees of the text and the
hypothesis.
The graph node matching, termed alignment, be-
tween both the text and the hypothesis consists in
finding pairs of words in both trees whose lemmas
are identical, no matter whether they are in the same
position within the tree. Some authors have already
designed similar matching techniques, such as the
one described in (Snow et al, 2006). However, these
include semantic constraints that we have decided
not to consider. The reason of this decision is that we
desired to overcome the textual entailment recogni-
tion from an exclusively syntactic perspective. The
formula that provides the similarity rate between the
dependency trees of the text and the hypothesis in
our system, denoted by the symbol ?, is shown in
Equation 3:
?(?, ?) =
?
???
?(?) (3)
where ? and ? represent the text?s and hypothesis?
syntactic dependency trees, respectively, and ? is the
set that contains all synsets present in both trees, be-
ing ? = ? ? ? ?? ? ?, ? ? ?. As we can observe in
Equation 3, ? depends on another function, denoted
by the symbol ?, which provides the relevance of
a synset. Such a weight factor will depend on the
grammatical category and relation of the synset. In
addition, we believe that the most relevant words of
a phrase occupy the highest positions in the depen-
dency tree, so we desired to assign different weights
depending on the depth of the synset. With all these
factors we define the relevance of a word as shown
68
in Equation 4:
?(?) = ? ? ? ? ???? (4)
where ? is a synset present in both ? and ?, ? rep-
resents the weight assigned to ??s grammatical cat-
egory (Table 1), ? the weight of ??s grammatical
relationship (Table 2), ? an empirically calculated
value that represents the weight difference between
tree levels, and ?? the depth of the node that contains
the synset ? in ?. The performed experiments reveal
that the optimal value for ? is 1.1.
Grammatical category Weight
Verbs, verbs with one argument, verbs with
two arguments, verbs taking clause as com-
plement
1.0
Nouns, numbers 0.75
Be used as a linking verb 0.7
Adjectives, adverbs, noun-noun modifiers 0.5
Verbs Have and Be 0.3
Table 1: Weights assigned to the relevant grammati-
cal categories.
Grammatical relationship Weight
Subject of verbs, surface subject, object of
verbs, second object of ditransitive verbs
1.0
The rest 0.5
Table 2: Weights assigned to the grammatical rela-
tionships.
We would like to point out that a requirement of
our system?s similarity measure is to be independent
of the hypothesis length. Therefore, we must de-
fine the normalized similarity rate, as represented in
Equation 5:
?(?, ?) =
?
???
?(?)
?
???
?(?)
(5)
Once the similarity value has been calculated, it
will be provided to the user together with the cor-
responding text-hypothesis pair identifier. It will be
his responsibility to choose an appropriate threshold
that will represent the minimum similarity rate to be
considered as entailment between text and hypothe-
sis. All values that are under such a threshold will
be marked as not entailed.
3 System Evaluation
In order to evaluate our system we have generated
several results using different combinations of all
three mentioned modules. Since the lexical one uses
a machine learning algorithm, it has to be run within
a training environment. For this purpose we have
trained our system with the corpora provided in the
previous editions of RTE, and also with the develop-
ment corpus from the current RTE-3 challenge. On
the other hand, for the remainder modules the devel-
opment corpora was used to set the thresholds that
determine if the entailment holds.
The performed tests have been obtained by per-
forming different combinations of the described
modules. First, we have calculated the accuracy
rates using only each single module separately.
Later on we have combined those developed by our
research group for this year?s RTE challenge, which
are DLSITE-1 (the lexical one) and DLSITE-2 (the
syntactic one). Finally we have performed a voting
process between these two systems and the one we
presented in RTE-2.
The combination of DLSITE-1 and DLSITE-2 is
described as follows. If both modules agree, then the
judgement is straightforward, but if they do not, we
then decide the judgment depending on the accuracy
of each one for true and false entailment situations.
In our case, DLSITE-1 performs better while dealing
with negative examples, so its decision will prevail
over the rest. Regarding the combination of the three
approaches, we have developed a voting strategy.
The results obtained by our system are represented
in Table 3. As it is reflected in such table, the high-
est accuracy rate obtained using the RTE-3 test cor-
pus was achieved applying only the lexical module,
namely DLSITE-1. On the other hand, the syntac-
tic one had a significantly lower rate, and the same
happened with the system we presented in RTE-2.
Therefore, a combination of them will most likely
produce less accurate results than the lexical mod-
ule, as it is shown in Table 3. However, we would
like to point out that these results depend heavily on
the corpus idiosyncrasy. This can be proven with the
results obtained for the RTE-2 test corpus, where the
grouping of the three modules provided the highest
accuracy rates of all possible combinations.
69
RTE-2 test RTE-3 dev RTE-3 test
Overall Overall Overall IE IR QA SUM
RTE-2 system 0.5563 0.5523 0.5400 0.4900 0.6050 0.5100 0.5550
DLSITE-1 0.6188 0.7012 0.6563 0.5150 0.7350 0.7950 0.5800
DLSITE-2 0.6075 0.6450 0.5925 0.5050 0.6350 0.6300 0.6000
DLSITE-1&2 0.6212 0.6900 0.6375 0.5150 0.7150 0.7400 0.5800
Voting 0.6300 0.6900 0.6375 0.5250 0.7050 0.7200 0.6000
Table 3: Results obtained with the corpora from RTE-2 and RTE-3.
3.1 Results Analysis
We will now perform an analysis of the results
shown in the previous section. First, we would like
to mention the fact that our system does not be-
have correctly when it has to deal with long texts.
Roughly 11% and 13% of the false positives of
DLSITE-1 and DLSITE-2, respectively, are caused
by misinterpretations of long texts. The underlying
reason of these failures is the fact that it is easier to
find a lexical and syntactic match when a long text
is present in the pair, even if there is not entailment.
In addition, we consider very appealing to show
the accuracy rates corresponding to true and false
entailment pairs individually. Figure 2 represents the
mentioned rates for all system combinations that we
displayed in Table 3.
Figure 2: Accuracy rates obtained for true and false
entailments using the RTE-3 test corpus.
As we can see in Figure 2, the accuracy rates
for true and false entailment pairs vary significantly.
The modules we built for our participation in RTE-3
obtained high accuracy rates for true entailment text-
hypothesis pairs, but in contrast they behaved worse
in detecting false entailment pairs. This is the oppo-
site to the system we presented in RTE-2, since it has
a much higher accuracy rate for false cases than true
ones. When we combinedDLSITE-1 andDLSITE-2,
their accuracy rate for true entailments diminished,
although, on the other hand, the rate for false ones
raised. The voting between all three modules pro-
vided a higher accuracy rate for false entailments be-
cause the system we presented at RTE-2 performed
well in these cases.
Finally, we would like to discuss some examples
that lead to failures and correct forecasts by our two
new approaches.
Pair 246 entailment=YES task=IR
T: Overall the accident rate worldwide for commercial aviation
has been falling fairly dramatically especially during the period
between 1950 and 1970, largely due to the introduction of new
technology during this period.
H: Airplane accidents are decreasing.
Pair 246 is incorrectly classified by DLSITE-1
due to the fact that some words of the hypothesis do
not appear in the same manner in the text, although
they have similar meaning (e.g. airplane and
aviation). However, DLSITE-2 is able to establish a
true entailment for this pair, since the hypothesis?
syntactic dependency tree can be matched within the
text?s, and the similarity measure applied between
lemmas obtains a high score. This fact produces
that, in this case, the voting also achieves a correct
prediction for pair 246.
Pair 736 entailment=YES task=SUM
T: In a security fraud case, Michael Milken was sentenced to 10
years in prison.
H: Milken was imprisoned for security fraud.
Pair 736 is correctly classified by DLSITE-1 since
there are matches for all hypothesis? words (except
imprisoned) and some subsequences. In contrast,
DLSITE-2 does not behave correctly with this exam-
ple because the main verbs do not match, being this
fact a considerable handicap for the overall score.
70
4 Conclusions and Future Work
This research provides independent approaches con-
sidering mainly lexical and syntactic information. In
order to achieve this, we expose and analyze a wide
variety of lexical measures as well as syntactic struc-
ture comparisons that attempt to solve the textual en-
tailment recognition task. In addition, we propose
several combinations between these two approaches
and integrate them with our previous RTE-2 system
by using a voting strategy.
The results obtained reveal that, although the
combined approach provided the highest accuracy
rates for the RTE-2 corpora, it has not accom-
plished the expected reliability in the RTE-3 chal-
lenge. Nevertheless, in both cases the lexical-based
module achieved better results than the rest of the in-
dividual approaches, being the optimal for our par-
ticipation in RTE-3, and obtaining an accuracy rate
of about 70% and 65% for the development and test
corpus, respectively. One should note that these re-
sults depend on the idiosyncrasies of the RTE cor-
pora. However, these corpora are the most reliable
ones for evaluating textual entailment recognizers.
Future work can be related to the development
of a semantic module. Our system achieves good
lexical and syntactic comparisons between texts, but
we believe that we should take advantage of the se-
mantic resources in order to achieve higher accuracy
rates. For this purpose we plan to build a module
that constructs characterized representations based
on the text using named entities and role labeling in
order to extract semantic information from a text-
hypothesis pair. Another future research line could
consist in applying different recognition techniques
depending on the type of entailment task. We have
noticed that the accuracy of our approach differs
when the entailment is produced mainly by lexical
or syntactic implications. We intend to establish an
entailment typology and tackle each type by means
of different points of view or approaches.
Acknowledgments
This research has been partially funded by the
QALL-ME consortium, which is a 6th Framework
Research Programme of the European Union (EU),
contract number FP6-IST-033860 and by the Span-
ish Government under the project CICyT number
TIN2006-1526-C06-01. It has also been supported
by the undergraduate research fellowships financed
by the Spanish Ministry of Education and Science,
and the project ACOM06/90 financed by the Span-
ish Generalitat Valenciana.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual En-
tailment Challenge. Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, pages 1?9.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In Quin?onero-Candela et al, edi-
tors, MLCW 2005, LNAI Volume 3944, pages 177?190.
Springer-Verlag.
Oscar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-
cio Mart??nez-Barco, and Manuel Palomar. 2006. An
approach based on Logic forms and wordnet relation-
ships to textual entailment performance. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 22?26, Venice,
Italy.
Oren Glickman. 2005. Applied Textual Entailment Chal-
lenge. Ph.D. thesis, Bar Ilan University.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Stan Szpakow-
icz Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990. In-
troduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235?244.
Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.
2006. Detecting Entailment Using an Extended Imple-
mentation of the Basic Elements Overlap Metrics. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment, pages 122?
127, Venice, Italy.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. In Proceedings of the North American
Association of Computational Linguistics, New York
City, New York, United States of America.
71
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 608?616,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UMCC_DLSI: Multidimensional Lexical-Semantic Textual Similarity 
 
Antonio Fern?ndez, Yoan Guti?rrez, 
Alexander Ch?vez, H?ctor D?vila, Andy 
Gonz?lez, Rainel Estrada , Yenier Casta?eda 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba 
 
Sonia V?zquez 
Andr?s Montoyo, Rafael Mu?oz,  
 
DLSI, University of Alicante 
Carretera de San Vicente S/N 
Alicante, Spain 
 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI system, which 
participated in the first Semantic Textual 
Similarity task (STS) of SemEval-2012. Our 
supervised system uses different kinds of 
semantic and lexical features to train classifiers 
and it uses a voting process to select the correct 
option. Related to the different features we can 
highlight the resource ISR-WN1 used to extract 
semantic relations among words and the use of 
different algorithms to establish semantic and 
lexical similarities. In order to establish which 
features are the most appropriate to improve 
STS results we participated with three runs 
using different set of features. Our best 
approach reached the position 18 of 89 runs, 
obtaining a general correlation coefficient up to 
0.72. 
1. Introduction 
SemEval 2012 competition for evaluating Natural 
Language Processing (NLP) systems presents a 
new task called Semantic Textual Similarity (STS) 
(Agirre et al, 2012). In STS the participating 
systems must examine the degree of semantic 
equivalence between two sentences. The goal of 
this task is to create a unified framework for the 
evaluation of semantic textual similarity modules 
and to characterize their impact on NLP 
applications. 
STS is related to Textual Entailment (TE) and 
Paraphrase tasks. The main difference is that STS 
                                                   
1 Integration of Semantic Resource based on WordNet. 
assumes bidirectional graded equivalence between 
the pair of textual snippets. In the case of TE the 
equivalence is directional (e.g. a student is a 
person, but a person is not necessarily a student). 
In addition, STS differs from TE and Paraphrase in 
that, rather than being a binary yes/no decision, 
STS is a similarity-graded notion (e.g. a student 
and a person are more similar than a dog and a 
person). This bidirectional gradation is useful for 
NLP tasks such as Machine Translation, 
Information Extraction, Question Answering, and 
Summarization. Several semantic tasks could be 
added as modules in the STS framework, ?such as 
Word Sense Disambiguation and Induction, 
Lexical Substitution, Semantic Role Labeling, 
Multiword Expression detection and handling, 
Anaphora and Co-reference resolution, Time and 
Date resolution and Named Entity Recognition, 
among others?2  
1.1. Description of 2012 pilot task 
In STS, all systems were provided with a set of 
sentence pairs obtained from a segmented corpus. 
For each sentence pair, s1 and s2, all participants 
had to quantify how similar s1 and s2 were, 
providing a similarity score. The output of 
different systems was compared to the manual 
scores provided by SemEval-2012 gold standard 
file, which range from 5 to 0 according to the next 
criterions3:  
? (5) ?The two sentences are equivalent, as they 
mean the same thing?. 
                                                   
2
 http://www.cs.york.ac.uk/semeval-2012/task6/ 
3
 http://www.cs.york.ac.uk/semeval-
2012/task6/data/uploads/datasets/train-readme.txt 
608
? (4) ?The two sentences are mostly equivalent, 
but some unimportant details differ?. 
? (3) ?The two sentences are roughly equivalent, 
but some important information 
differs/missing?. 
? (2) ?The two sentences are not equivalent, but 
share some details?. 
? (1) ?The two sentences are not equivalent, but 
are on the same topic?. 
? (0) ?The two sentences are on different topics?. 
After this introduction, the rest of the paper is 
organized as follows. Section 2 shows the 
architecture of our system and a description of the 
different runs. In section 3 we describe the 
algorithms and methods used to obtain the features 
for our system, and Section 4 describe the training 
phase. The obtained results and a discussion are 
provided in Section 5, and finally the conclusions 
and future works in Section 6. 
2. System architecture and description of 
the runs 
As we can see in Figure 1 our three runs begin 
with the pre-processing of SemEval 2012?s 
training set. Every sentence pair is tokenized, 
lemmatized and POS tagged using Freeling tool 
(Atserias et al, 2006). Afterwards, several 
methods and algorithms are applied in order to 
extract all features for our Machine Learning 
System (MLS). Each run uses a particular group of 
features. 
The Run 1 (MultiSemLex) is our main run. 
This takes into account all extracted features and 
trains a model with a Voting classifier composed 
by the following techniques: Bagging (using M5P), 
Bagging (using REPTree), Random SubSpace 
(using REPTree) and MP5. The training corpus has 
been provided by SemEval-2012 competition, in 
concrete by the Semantic Textual Similarity task.  
The Runs 2 and 3 use the same classifier, but 
including different features. Run 2 (MultiLex) uses 
(see Figure 1) features extracted from Lexical-
Semantic Metrics (LS-M) described in section 3.1, 
Lexical-Semantic Alignment (LS-A) described in 
section 3.2 and Sentiment Polarity (SP) described 
in section 3.3.  
On the other hand, the Run 3 (MultiSem) uses 
features extracted only from Semantic Alignment 
(SA) described in section 3.4 and the textual edit 
distances named QGram-Distances. 
 
Figure 1. System Architecture. 
As a result, we obtain three trained models 
capable to estimate the similarity value between 
two sentences. 
Finally, we test our system with the SemEval 
2012 test set (see Table 7 with the results of our 
three runs). The following section describes the 
features extraction process. 
      Run 1 
      Voting  
      Classifier 
Training set from 
SemEval 2012 
Pre-Processing (using Freeling) 
 
Run 3 
Voting classifier 
Run 2 
Voting classifier 
Similarity Scores 
Feature extraction 
Lexical-Semantic Metrics 
 
Lexical-semantic 
alignment 
Semantic 
alignment 
Sentiment 
Polarity 
Jaro QGram Rel. Concept . . . 
Tokenizing Lemmatizing POS tagging 
SemEval 2012 
Test set 
     Training Process (using Weka) 
609
3. Description of the features used in the 
Machine Learning System 
Sometimes, when two sentences are very similar, 
one sentence is in a high degree lexically 
overlapped by the other. Inspired by this fact we 
developed various algorithms, which measure the 
level of overlapping by computing a quantity of 
matching words (the quantity of lemmas that 
correspond exactly by its morphology) in a pair of 
sentences. In our system, we used lexical and 
semantic similarity measures as features for a 
MLS. Other features were extracted from a lexical-
semantic sentences alignment and a variant using 
only a semantic alignment.  
3.1. Similarity measures 
We have used well-known string based similarity 
measures like: Needleman-Wunch (NW) (sequence 
alignment), Smith-Waterman (SW) (sequence 
alignment), Jaro, Jaro-Winkler (JaroW), Chapman-
Mean-Length (CMLength), QGram-Distance 
(QGramD), Block-Distance (BD), Jaccard 
Similarity (JaccardS), Monge-Elkan (ME) and 
Overlap-Coefficient (OC). These algorithms have 
been obtained from an API (Application Program 
Interface) SimMetrics library v1.54 for .NET 2.0. 
Copyright (c) 2006 by Chris Parkinson. We 
obtained 10 features for our MLS from these 
similarity measures. 
Using Levenshtein?s edit distance (LED), we 
computed also two different algorithms in order to 
obtain the alignment of the phrases. In the first 
one, we considered a value of the alignment as the 
LED between two sentences and the normalized 
variant named NomLED. Contrary to (Tatu et al, 
2006), we do not remove the punctuation or stop 
words from the sentences, neither consider 
different cost for transformation operation, and we 
used all the operations (deletion, insertion and 
substitution). The second one is a variant that we 
named Double Levenshtein?s Edit Distance 
(DLED). For this algorithm, we used LED to 
measure the distance between the sentences, but to 
compare the similarity between the words, we used 
LED again. Another feature is the normalized 
variant of DLED named NomDLED. 
The unique difference between classic LED 
algorithm and DLED is the comparison of 
                                                   
4
 http://sourceforge.net/projects/simmetrics/ 
similitude between two words. With LED should 
be: ?[?] = ?[?], whereas for our DLED we 
calculate words similarity also with LED (e.g. ????(?[?], ?[?]) <= 2). Values above a decision 
threshold (experimentally 2) mean unequal words. 
We obtain as result two new different features 
from these algorithms. 
Another distance we used is an extension of 
LED named Extended Distance (EDx) (see 
(Fern?ndez Orqu?n et al, 2009) for details). This 
algorithm is an extension of the Levenshtein?s 
algorithm, with which penalties are applied by 
considering what kind of operation or 
transformation is carried out (insertion, deletion, 
substitution, or non-operation) in what position, 
along with the character involved in the operation. 
In addition to the cost matrixes used by 
Levenshtein?s algorithm, EDx also obtains the 
Longest Common Subsequence (LCS) 
(Hirschberg, 1977) and other helpful attributes for 
determining similarity between strings in a single 
iteration. It is worth noting that the inclusion of all 
these penalizations makes the EDx algorithm a 
good candidate for our approach. In our previous 
work (Fern?ndez Orqu?n et al, 2009), EDx 
demonstrated excellent results when it was 
compared with other distances as (Levenshtein, 
1965), (Needleman and Wunsch, 1970), (Winkler, 
1999). How to calculate EDx is briefly described 
as follows (we refer reader to (Fern?ndez Orqu?n et 
al., 2009) for a further description): 
EDx = ??  ?????????????,???????(???????)????????? ?? ; (1) 
 
 
Where: ? - Transformations accomplished on the words (?, ?, ?, ?). ? - Not operations at all, ? - Insertion, ? - Deletion, ? - Substitution.  
We formalize ? as a vector: 
? =
???
??(0,0)(1,0) :: ??(0,1)(1,1) :: ?????
??
 
?1 and ?2 - The examined words ?1j - The j-th character of the word ?1 
610
?2k - The k-th character of the word ?2 ? - The weight of each character 
We can vary all this weights in order to make a 
flexible penalization to the interchangeable 
characters.  ??1j - The weight of characters at ?1j ??2k - The weight of characters at ?2k ? = ?? + 1 ?? ?i ? ?? ?? ?i = ? ? ; ? = ?? + 1 ?? ?i ? ?? ?? ?i = ? ? ? - The biggest word length of the language ? - Edit operations length ?i - Operation at (?) position ???? - Greatest value of ? ranking 
? = ? 2????(2???? + 1)???????  (2) 
As we can see in the equation (1), the term ?(??) ? ???????, ?(???)? is the Cartesian product that 
analyzes the importance of doing i-th operation 
between the characters at j-th and k-th position 
The term (2R??? + 1)??? in equation (1) penalizes 
the position of the operations. The most to the left 
hand the operation is the highest the penalization 
is. The term ? (see equation (2) normalizes the 
EDx into [0,1] interval. This measure is also used 
as a feature for the system. 
We also used as a feature the Minimal 
Semantic Distances (Breadth First Search (BFS)) 
obtained between the most relevant concepts of 
both sentences. The relevant concepts pertain to 
semantic resources ISR-WN (Guti?rrez et al, 
2011a; 2010b), as WordNet (Miller et al, 1990), 
WordNet Affect (Strapparava and Valitutti, 2004), 
SUMO (Niles and Pease, 2001) and Semantic 
Classes (Izquierdo et al, 2007). Those concepts 
were obtained after having applied the Association 
Ratio (AR) measure between concepts and words 
over each sentence. The obtained distances for 
each resource SUMO, WordNet Affect, WordNet 
and Semantic Classes are named SDist, AffDist, 
WNDist and SCDist respectively. 
ISR-WN, takes into account different kind of 
labels linked to WN: Level Upper Concepts 
(SUMO), Domains and Emotion labels. In this 
work, our purpose is to use a semantic network, 
which links different semantic resources aligned to 
WN. After several tests, we decided to apply ISR-
WN. Although others resources provide different 
semantic relations, ISR-WN has the highest 
quantity of semantic dimensions aligned, so it is a 
suitable resource to run our algorithm.  
Using ISR-WN we are able to extract 
important information from the interrelations of 
four ontological resources: WN, WND, WNA and 
SUMO. ISR-WN resource is based on WN1.6 or 
WN2.0 versions. In the last updated version, 
Semantic Classes and SentiWordNet were also 
included. Furthermore, ISR-WN provides a tool 
that allows the navigation across internal links. At 
this point, we can discover the multidimensionality 
of concepts that exists in each sentence. In order to 
establish the concepts associated to each sentence 
we apply Relevant Semantic Trees (Guti?rrez et 
al., 2010a; Guti?rrez et al, 2011b) approach using 
the provided links of ISR-WN. We refer reader to 
(Guti?rrez et al, 2010a) for a further description. 
3.2. Lexical-Semantic alignment 
Another algorithm that we created is the Lexical-
Semantic Alignment. In this algorithm, we tried to 
align the sentences by its lemmas. If the lemmas 
coincide we look for coincidences among parts of 
speech, and then the phrase is realigned using both. 
If the words do not share the same part of speech, 
they will not be aligned. Until here, we only have 
taken into account a lexical alignment. From now 
on, we are going to apply a semantic variant. After 
all the process, the non-aligned words will be 
analyzed taking into account its WorldNet?s 
relations (synonymy, hyponymy, hyperonymy, 
derivationally ? related ? form, similar-to, verbal 
group, entailment and cause-to relation); and a set 
of equivalencies like abbreviations of months, 
countries, capitals, days and coins. In the case of 
the relation of hyperonymy and hyponymy, the 
words will be aligned if there is a word in the first 
sentence that is in the same relation (hyperonymy 
or hyponymy) of another one in the second 
sentence. For the relations of ?cause-to? and 
?implication? the words will be aligned if there is a 
word in the first sentence that causes or implicates 
another one of the second sentence. All the other 
types of relations will be carried out in 
bidirectional way, that is, there is an alignment if a 
word of the first sentence is a synonymous of 
another one belonging to the second one or vice 
versa. Finally, we obtain a value we called 
alignment relation. This value is calculated as ??? =  ??? / ????. Where ??? is the final 
611
alignment value, ??? is the number of aligned 
word and ???? is the number of words of the 
shorter phrase. This value is also another feature 
for our system. 
3.3. Sentiment Polarity Feature 
Another feature is obtained calculating 
SentiWordNet Polarities matches of the analyzed 
sentences (see (Guti?rrez et al, 2011c) for detail). 
This analysis has been applied from several 
dimensions (WordNet, WordNet Domains, 
WordNet Affect, SUMO, and Semantic Classes) 
where the words with sentimental polarity offer to 
the relevant concepts (for each conceptual resource 
from ISR-WN (e.g. WordNet, WordNet Domains, 
WordNet Affect, SUMO, and Semantic Classes)) 
its polarity values. Other analysis were the 
integration of all results of polarity in a measure 
and further a voting process where all polarities 
output are involved (for more details see 
(Fern?ndez et al, 2012)). 
The final measure corresponds to ?? =????? + ?????, where ????1 is a polarity value of 
the sentence ?1 and ????? is a polarity value of the 
sentence ?2. The negative, neutral, and positive 
values of polarities are represented as -1, 0 and 1 
respectively. 
3.4. Semantic Alignment 
This alignment method depends on calculating the 
semantic similarity between sentences based on an 
analysis of the relations, in ISR-WN, of the words 
that fix them. 
First, the two sentences are pre-processed with 
Freeling and the words are classified according to 
their parts of speech (noun, verb, adjective, and 
adverbs.).  
We take 30% of the most probable senses of 
every word and we treat them as a group. The 
distance between two groups will be the minimal 
distance between senses of any pair of words 
belonging to the group. For example: 
 
Figure 2. Minimal Distance between ?Run? and 
?Chase?. 
In the example of Figure 2 the ???? = 2 is 
selected for the pair ?Run-Chase?, because this 
pair has the minimal cost=2.  
For nouns and the words that are not found in 
WordNet like common nouns or Christian names, 
the distance is calculated in a different way. In this 
case, we used LED. 
Let's see the following example: 
We could take the pair 99 of corpus MSRvid 
(from training set) with a litter of transformation in 
order to a better explanation of our method. 
Original pair 
A: A polar bear is running towards a group of 
walruses. 
B: A polar bear is chasing a group of walruses. 
Transformed pair: 
A1: A polar bear runs towards a group of cats. 
B1: A wale chases a group of dogs. 
Later on, using the algorithm showed in the 
example of Figure 2, a matrix with the distances 
between all groups of both sentences is created 
(see Table 1). 
GROUPS polar bear runs towards group cats 
wale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2 
chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3 
group     Dist:=0  
dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1 
Table 1. Distances between the groups. 
Using the Hungarian Algorithm (Kuhn, 1955) 
for Minimum Cost Assignment, each group of the 
smaller sentence is checked with an element of the 
biggest sentence and the rest is marked as words 
that were not aligned. 
In the previous example the words ?toward? 
and ?polar? are the words that were not aligned, so 
the number of non-aligned words is 2. There is 
only one perfect match: ?group-group? (match 
with ???? = 0). The length of the shortest sentence 
is 4. The Table 2 shows the results of this analysis. 
Number of 
exact 
coincidences 
(Same) 
Total Distances 
of optimal 
Matching 
(Cost) 
Number of 
non-
aligned 
Words 
(Dif) 
Number of 
lemmas of 
shorter 
sentence 
(Min) 
1 5 2 4 
Table 2. Features extracted from the analyzed sentences. 
This process has to be repeated for the verbs, 
nouns (see Table 3), adjectives, and adverbs. On 
the contrary, the tables have to be created only 
with the similar groups of the sentences. Table 3 
Lemma: Chase 
 
 
 
 
Lemma: Run 
 
 
 
 
Dist=2 
2 
3 
5 
Sense 1 
Sense 2 
Sense 1 
Sense 2 
4 
612
shows features extracted from the analysis of 
nouns. 
GROUPS bear group cats 
wale Dist := 2  Dist := 2 
group  Dist := 0  
dogs Dist := 1  Dist := 1 
Table 3. Distances between the groups of nouns. 
Number of 
exact 
coincidences 
(SameN) 
Total 
Distances of 
optimal 
Matching 
(CostN) 
Number of 
non-aligned 
Words 
(DifN) 
Number of 
lemmas of 
shorter 
sentence 
(MinN) 
1 3 0 3 
Table 4. Feature extracted the analysis of nouns. 
Several attributes are extracted from the pair of 
sentences. Four attributes from the entire 
sentences, four attributes considering only verbs, 
only nouns, only adjectives, and only adverbs. 
These attributes are:  
? Number of exact coincidences (Same) 
? Total distance of optimal matching (Cost). 
? Number of words that do not match (Dif). 
? Number of lemmas of the shortest sentence 
(Min). 
As a result, we finally obtain 20 attributes from 
this alignment method. For each part-of-speech, 
the attributes are represented adding to its names 
the characters N, V, A and R to represent features 
for nouns, verbs, adjectives, and adverbs 
respectively. 
It is important to remark that this alignment 
process searches to solve, for each word from the 
rows (see Table 3) its respectively word from the 
columns. 
4. Description of the training phase 
For the training process, we used a supervised 
learning framework, including all the training set 
(MSRpar, MSRvid and SMTeuroparl) as a training 
corpus. Using 10 fold cross validation with the 
classifier mentioned in section 2 (experimentally 
selected). 
As we can see in Table 5, the features: FAV, 
EDx, CMLength, QGramD, BD, Same, SameN, 
obtain values over 0.50 of correlation. The more 
relevant are EDx and QGramD, which were 
selected as a lexical base for the experiment in Run 
3. It is important to remark that feature SameN and 
Same only using number of exact coincidences 
obtain an encourage value of correlation. 
 
Feature Correlation Feature Correlation Feature Correlation 
Correlation using all 
features 
(correspond to Run 1) 
FAV 0.5064 ME 0.4971 CostV 0.1517 
0.8519 
LED 0.4572 OC 0.4983 SameN 0.5307 
DLED 0.4782 SDist 0.4037 MinN 0.4149 
NormLED 0.4349 AffDist 0.4043 DifN 0.1132 
NormDLED 0.4457 WNDist 0.2098 CostN 0.1984 
EDx 0.596 SCDist  0.1532 SameA 0.4182 
NW 0.2431 PV 0.0342 MinA 0.4261 
SW 0.2803 Same 0.5753 DifA 0.3818 
Jaro 0.3611 Min 0.5398 CostA 0.3794 
JaroW 0.2366 Dif 0.2588 SameR 0.3586 
CMLength 0.5588 Cost 0.2568 MinR 0.362 
QGramD 0.5749 SameV 0.3004 DifR 0.3678 
BD 0.5259 MinV 0.4227 CostR 0.3461 
JaccardS 0.4849 DifV 0.2634 
  
 
Table 5. Correlation of individual features over all training sets. 
 
We decide to include the Sentiment Polarity as 
a feature, because our previous results on Textual 
Entailment task in (Fern?ndez et al, 2012). But, 
contrary to what we obtain in this paper, the 
influence of the polarity (PV) for this task is very 
low, its contribution working together with other 
features is not remarkable, but neither negative 
(Table 6), So we decide remaining in our system. 
In oder to select the lexical base for Run 3 
(MultiSem, features marked in bold) we compared 
the individual influences of the best lexical 
features (EDx, QGramD, CMLength), obtaining 
613
the 0.82, 0.83, 0.81 respectively (Table 6). Finally, 
we decided to use QGramD. 
The conceptual features SDist, AffDist, 
WNDist, SCDist do not increase the similarity 
score, this is due to the generality of the obtained 
concept, losing the essential characteristic between 
both sentences. Just like with PV we decide to 
keep them in our system. 
As we can see in Table 5, when all features are 
taking into account the system obtain the best 
score. 
Feature Pearson (MSRpar, MSRvid and SMTeuroparl) 
SDist 
 
       
0.8509 
AffDist 
 
       
WNDist 
 
       
SCDist 
 
       
EDx 
 
      
0.8507 
PV 
 
   
 
  QGramD 
 
    
0.8491 
CMLength
 
 
0.8075 
   
Same 
0.7043 
0.795 0.829 0.8302 0.8228 
Min 
Dif 
Cost 
SameV 
0.576 MinV DifV 
CostV 
SameN 
0.5975 MinN DifN 
CostN 
SameA 
0.4285 MinA DifA 
CostA 
SameR 
0.3778 MinR DifR 
CostR 
Table 6. Features influence.  
Note: Gray cells mean features that are not taking into 
account. 
5. Result and discussion 
Semantic Textual Similarity task of SemEval-2012 
offered three official measures to rank the 
systems5: 
1. ALL: Pearson correlation with the gold 
standard for the five datasets, and 
corresponding rank. 
2. ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
                                                   
5
 http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=results-update 
standard using least squares, and 
corresponding rank. 
3. Mean: Weighted mean across the five datasets, 
where the weight depends on the number of 
pairs in the dataset. 
4. Pearson for individual datasets. 
Using these measures, our main run (Run 1) 
obtained the best results (see Table 7). This 
demonstrates the importance of tackling this 
problem from a multidimensional lexical-semantic 
point of view. 
Run MSRpar MSRvid SMT-eur On-WN SMT-
news 
1 0.6205 0.8104 0.4325 0.6256 0.4340 
2 0.6022 0.7709 0.4435 0.4327 0.4264 
3 0.5269 0.7756 0.4688 0.6539 0.5470 
Table 7. Official SemEval 2012 results. 
Run ALL Rank ALLnrm RankNrm Mean RankMean 
1 0.7213 18 0.8239 14 0.6158 15 
2 0.6630 26 0.7922 46 0.5560 49 
3 0.6529 29 0.8115 23 0.6116 16 
Table 8. Ranking position of our runs in SemEval 2012. 
The Run 2 uses a lot of lexical analysis and not 
much of semantic analysis. For this reason, the 
results for Run 2 is poorer (in comparison to the 
Run 3) (see Table 7) for the test sets: SMT-eur, 
On-WN and SMT-news. Of course, these tests 
have more complex semantic structures than the 
others. However, for test MSRpar it function better 
and for test MSRvid it functions very similar to 
Run 3. 
Otherwise, the Run 3 uses more semantic 
analysis that Run 2 (it uses all features mentioned 
except feature marked in bold on Table 6) and only 
one lexical similarity measure (QGram-Distance). 
This makes it to work better for test sets SMT-eur, 
On-WN and SMT-news (see Table 7). It is 
important to remark that this run obtains important 
results for the test SMT-news, positioning this 
variant in the fifth place of 89 runs. Moreover, it is 
interesting to notice (Table 7) that when mixing the 
semantic features with the lexical one (creating 
Run 1) it makes the system to improve its general 
results, except for the test: SMT-eur, On-WN and 
SMT-news in comparison with Run 3. For these 
test sets seem to be necessary more semantic 
analysis than lexical in order to improve similarity 
estimation. We assume that Run 1 is non-balance 
according to the quantity of lexical and semantic 
features, because this run has a high quantity of 
614
lexical and a few of semantic analysis. For that 
reason, Run 3 has a better performance than Run 1 
for these test sets. 
Even when the semantic measures demonstrate 
significant results, we do not discard the lexical 
help on Run 3. After doing experimental 
evaluations on the training phase, when lexical 
feature from QGram-Distance is not taken into 
account, the Run 3 scores decrease. This 
demonstrates that at least a lexical base is 
necessary for the Semantic Textual Similarity 
systems. 
6. Conclusion and future works 
This paper introduced a new framework for 
recognizing Semantic Textual Similarity, which 
depends on the extraction of several features that 
can be inferred from a conventional interpretation 
of a text. 
As mentioned in section 2 we have conducted 
three different runs, these runs only differ in the 
type of attributes used. We can see in Table 7 that 
all runs obtained encouraging results. Our best run 
was placed between the first 18th positions of the 
ranking of Semeval 2012 (from 89 Runs) in all 
cases. Table 8 shows the reached positions for the 
three different runs and the ranking according to 
the rest of the teams.  
In our participation, we used a MLS that works 
with features extracted from five different 
strategies: String Based Similarity Measures, 
Semantic Similarity Measures, Lexical-Semantic 
Alignment, Semantic Alignment, and Sentiment 
Polarity Cross-checking. 
We have conducted the semantic features 
extraction in a multidimensional context using the 
resource ISR-WN, the one that allowed us to 
navigate across several semantic resources 
(WordNet, WordNet Domains, WordNet Affect, 
SUMO, SentiWorNet and Semantic Classes). 
Finally, we can conclude that our system 
performs quite well. In our current work, we show 
that this approach can be used to correctly classify 
several examples from the STS task of SemEval-
2012. Comparing with the best run (UKP_Run2 
(see Table 9)) of the ranking our main run has very 
closed results. In two times we increased the best 
UKP?s run (UKP_Run 2), for MSRvid test set in 
0.2824 points and for On-WN test set in 0.1319 
points (see Table 10).  
Run ALL Rank ALLnrm RankNrm Mean RankMean 
(UKP) 
Run 2 0.8239 1 0.8579 2 0.6773 1 
Table 9. The best run of SemEval 2012. 
It is important to remark that we do not expand 
any corpus to train the classifier of our system. 
This fact locates us at disadvantage according to 
other teams that do it. 
Run ALL MSRpar MSRvid SMT-
eur 
On-
WN 
SMT-
news 
(UKP) 
Run 2 0.8239 0.8739 0.528 0.6641 0.4937 0.4937 
(Our) 
Run 1 0.721 0.6205 0.8104 0.4325 0.6256 0.434 
Table 10. Comparison of our distance with the best. 
As future work we are planning to enrich our 
semantic alignment method with Extended 
WordNet (Moldovan and Rus, 2001), we think that 
with this improvement we can increase the results 
obtained with texts like those in On-WN test set. 
Acknowledgments 
This paper has been supported partially by 
Ministerio de Ciencia e Innovaci?n - Spanish 
Government (grant no. TIN2009-13391-C04-01), 
and Conselleria d'Educaci?n - Generalitat 
Valenciana (grant no. PROMETEO/2009/119 and 
ACOMP/2010/288). 
Reference 
Antonio Fern?ndez, Yoan Guti?rrez, Rafael Mu?oz and 
Andr?s Montoyo. 2012. Approaching Textual 
Entailment with Sentiment Polarity. In  ICAI'12 - The 
2012 International Conference on Artificial 
Intelligence, Las Vegas, Nevada, USA.  
Antonio Celso Fern?ndez Orqu?n, D?az Blanco Josval, 
Alfredo Fundora Rolo and Rafael Mu?oz Guillena. 
2009. Un algoritmo para la extracci?n de 
caracter?sticas lexicogr?ficas en la comparaci?n de 
palabras. In  IV Convenci?n Cient?fica Internacional 
CIUM, Matanzas, Cuba.  
Carlo Strapparava and Alessandro Valitutti. 2004. 
WordNet-Affect: an affective extension of WordNet. 
In Proceedings of the 4th International Conference on 
Language Resources and Evaluation (LREC 2004), 
Lisbon,  1083-1086.  
Daniel S. Hirschberg. 1977. Algorithms for the longest 
common subsequence problem Journal of the ACM, 
24: 664?675. 
615
Dan I. Moldovan and Vasile Rus. 2001. Explaining 
Answers with Extended WordNet ACL. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor 
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A 
Pilot on Semantic Textual Similarity. In Proceedings 
of the 6th International Workshop on Semantic 
Evaluation (SemEval 2012), in conjunction with the 
First Joint Conference on Lexical and Computational 
Semantics (*SEM 2012), Montreal, Canada, ACL.  
George A. Miller, Richard Beckwith, Christiane 
Fellbaum, Derek Gross and Katherine Miller. 1990. 
Introduction to WordNet: An On-line Lexical 
Database International Journal of Lexicography, 
3(4):235-244. 
Harold W. Kuhn. 1955. The Hungarian Method for the 
assignment problem Naval Research Logistics 
Quarterly,  2: 83?97. 
Ian Niles and Adam Pease. 2001. Origins of the IEEE 
Standard Upper Ontology. In  Working Notes of the 
IJCAI-2001 Workshop on the IEEE Standard Upper 
Ontology, Seattle, Washington, USA.  
Jordi Atserias, Bernardino Casas, Elisabet Comelles, 
Meritxell Gonz?lez, Llu?s Padr? and Muntsa Padr?. 
2006. FreeLing 1.3: Syntactic and semantic services 
in an open source NLP library. In  Proceedings of the 
fifth international conference on Language Resources 
and Evaluation (LREC 2006), Genoa, Italy.  
Marta Tatu, Brandon Iles, John Slavick, Novischi 
Adrian and Dan Moldovan. 2006. COGEX at the 
Second Recognizing Textual Entailment Challenge. 
In Proceedings of the Second PASCAL Recognising 
Textual Entailment Challenge Workshop, Venice, 
Italy,  104-109. 
Rub?n Izquierdo, Armando Su?rez and German Rigau. 
2007. A Proposal of Automatic Selection of Coarse-
grained Semantic Classes for WSD Procesamiento 
del Lenguaje Natural,  39: 189-196. 
Saul B. Needleman and Christian D. Wunsch. 1970. A 
general method applicable to the search for 
similarities in the amino acid sequence of two 
proteins Journal of Molecular Biology,  48(3): 443-
453. 
Vladimir Losifovich Levenshtein. 1965. Binary codes 
capable of correcting spurious insertions and 
deletions of ones. Problems of information 
Transmission.  pp. 8-17.  
William E. Winkler. 1999. The state of record linkage 
and current research problems. Technical Report. 
U.S. Census Bureau, Statistical Research Division. 
Yoan Guti?rrez, Antonio Fern?ndez, And?s Montoyo 
and Sonia V?zquez. 2010a. UMCC-DLSI: Integrative 
resource for disambiguation task. In  Proceedings of 
the 5th International Workshop on Semantic 
Evaluation, Uppsala, Sweden, Association for 
Computational Linguistics,  427-432.  
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2010b. Integration of semantic 
resources based on WordNet XXVI Congreso de la 
Sociedad Espa?ola para el Procesamiento del 
Lenguaje Natural,  45: 161-168. 
Yoan Guti?rrez, Antonio Fern?ndez, Andr?s Montoyo 
and Sonia V?zquez. 2011a. Enriching the Integration 
of Semantic Resources based on WordNet 
Procesamiento del Lenguaje Natural,  47: 249-257. 
Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo. 
2011b. Improving WSD using ISR-WN with Relevant 
Semantic Trees and SemCor Senses Frequency. In  
Proceedings of the International Conference Recent 
Advances in Natural Language Processing 2011, 
Hissar, Bulgaria, RANLP 2011 Organising 
Committee,  233--239.  
Yoan Guti?rrez, Sonia V?zquez and Andr?s Montoyo. 
2011c. Sentiment Classification Using Semantic 
Features Extracted from WordNet-based Resources. 
In  Proceedings of the 2nd Workshop on 
Computational Approaches to Subjectivity and 
Sentiment Analysis (WASSA 2.011), Portland, 
Oregon., Association for Computational Linguistics,  
139--145.  
616
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 109?118, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Textual Similarity based on Lexical-Semantic features 
 
 
Alexander Ch?vez, Antonio Fern?ndez Orqu?n, 
H?ctor D?vila, Yoan Guti?rrez, Armando 
Collazo, Jos? I. Abreu 
DI, University of Matanzas 
Autopista a Varadero km 3 ?  
Matanzas, Cuba.  
{alexander.chavez, tony, 
hector.davila, yoan.gutierrez, 
armando.collazo, jose.abreu}@umcc.cu 
Andr?s Montoyo, Rafael Mu?oz 
DLSI, University of Alicante Carretera de 
San Vicente S/N Alicante, Spain. 
{montoyo,rafael}@dlsi.ua.es 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI system, which 
participated in the Semantic Textual 
Similarity task (STS) of SemEval-2013. Our 
supervised system uses different types of 
lexical and semantic features to train a 
Bagging classifier used to decide the correct 
option. Related to the different features we 
can highlight the resource ISR-WN used to 
extract semantic relations among words and 
the use of different algorithms to establish 
semantic and lexical similarities. In order to 
establish which features are the most 
appropriate to improve STS results we 
participated with three runs using different 
set of features. Our best run reached the 
position 44 in the official ranking, obtaining 
a general correlation coefficient of 0.61. 
1 Introduction 
SemEval-2013 (Agirre et al, 2013) presents the 
task Semantic Textual Similarity (STS) again. In 
STS, the participating systems must examine the 
degree of semantic equivalence between two 
sentences. The goal of this task is to create a 
unified framework for the evaluation of semantic 
textual similarity modules and to characterize 
their impact on NLP applications. 
STS is related to Textual Entailment (TE) and 
Paraphrase tasks. The main difference is that 
STS assumes bidirectional graded equivalence 
between the pair of textual snippets. 
In case of TE, the equivalence is directional 
(e.g. a student is a person, but a person is not 
necessarily a student). In addition, STS differs 
from TE and Paraphrase in that, rather than 
being a binary yes/no decision, STS is a 
similarity-graded notion (e.g. a student is more 
similar to a person than a dog to a person).  
This graded bidirectional is useful for NLP 
tasks such as Machine Translation (MT), 
Information Extraction (IE), Question 
Answering (QA), and Summarization. Several 
semantic tasks could be added as modules in the 
STS framework, ?such as Word Sense 
Disambiguation and Induction, Lexical 
Substitution, Semantic Role Labeling, Multiword 
Expression detection and handling, Anaphora 
and Co-reference resolution, Time and Date 
resolution and Named Entity, among others?1  
1.1 Description of 2013 pilot task 
This edition of SemEval-2013 remain with the 
same classification approaches that in their first 
version in 2012. The output of different systems 
was compared to the reference scores provided 
by SemEval-2013 gold standard file, which 
range from five to zero according to the next 
criterions2: (5) ?The two sentences are 
equivalent, as they mean the same thing?. (4) 
?The two sentences are mostly equivalent, but 
some unimportant details differ?. (3) ?The two 
sentences are roughly equivalent, but some 
important information differs/missing?. (2) ?The 
two sentences are not equivalent, but share some 
details?. (1) ?The two sentences are not 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
2 http://www.cs.york.ac.uk/semeval-
2012/task6/data/uploads/datasets/train-readme.txt 
109
equivalent, but are on the same topic?. (0) ?The 
two sentences are on different topics?. 
After this introduction, the rest of the paper is 
organized as follows. Section 3 shows the 
Related Works. Section 4 presents our system 
architecture and description of the different runs. 
In section 4 we describe the different features 
used in our system. Results and a discussion are 
provided in Section 5 and finally we conclude in 
Section 6. 
2 Related Works 
There are more extensive literature on measuring 
the similarity between documents than to 
between sentences. Perhaps the most recently 
scenario is constituted by the competition of 
SemEval-2012 task 6: A Pilot on Semantic 
Textual Similarity (Aguirre and Cerd, 2012). In 
SemEval-2012, there were used different tools 
and resources like stop word list, multilingual 
corpora, dictionaries, acronyms, and tables of 
paraphrases, ?but WordNet was the most used 
resource, followed by monolingual corpora and 
Wikipedia? (Aguirre and Cerd, 2012). 
According to Aguirre, Generic NLP tools were 
widely used. Among those that stand out were 
tools for lemmatization and POS-tagging 
(Aguirre and Cerd, 2012). On a smaller scale 
word sense disambiguation, semantic role 
labeling and time and date resolution. In 
addition, Knowledge-based and distributional 
methods were highly used. Aguirre and Cerd 
remarked on (Aguirre and Cerd, 2012) that 
alignment and/or statistical machine translation 
software, lexical substitution, string similarity, 
textual entailment and machine translation 
evaluation software were used to a lesser extent. 
It can be noted that machine learning was widely 
used to combine and tune components. 
Most of the knowledge-based methods ?obtain 
a measure of relatedness by utilizing lexical 
resources and ontologies such as WordNet 
(Miller et al, 1990b) to measure definitional 
overlap, term distance within a graphical 
taxonomy, or term depth in the taxonomy as a 
measure of specificity? (Banea et al, 2012). 
Some scholars as in (Corley and Mihalcea, 
June 2005) have argue ?the fact that a 
comprehensive metric of text semantic similarity 
should take into account the relations between 
words, as well as the role played by the various 
entities involved in the interactions described by 
each of the two sentences?. This idea is resumed 
in the Principle of Compositionality, this 
principle posits that the meaning of a complex 
expression is determined by the meanings of its 
constituent expressions and the rules used to 
combine them (Werning et al, 2005). Corley 
and Mihalcea in this article combined metrics of 
word-to-word similarity, and language models 
into a formula and they pose that this is a 
potentially good indicator of the semantic 
similarity of the two input texts sentences. They 
modeled the semantic similarity of a sentence as 
a function of the semantic similarity of the 
component words (Corley and Mihalcea, June 
2005). 
One of the top scoring systems at SemEval-
2012 (?ari? et al, 2012) tended to use most of 
the aforementioned resources and tools. They 
predict the human ratings of sentence similarity 
using a support-vector regression model with 
multiple features measuring word-overlap 
similarity and syntax similarity. They also 
compute the similarity between sentences using 
the semantic alignment of lemmas. First, they 
compute the word similarity between all pairs of 
lemmas from first to second sentence, using 
either the knowledge-based or the corpus-based 
semantic similarity. They named this method 
Greedy Lemma Aligning Overlap. 
Daniel B?r presented the UKP system, which 
performed best in the Semantic Textual 
Similarity (STS) task at SemEval-2012 in two 
out of three metrics. It uses a simple log-linear 
regression model, trained on the training data, to 
combine multiple text similarity measures of 
varying complexity. 
3 System architecture and description 
of the runs 
As we can see in Figure 1, our three runs begin 
with the pre-processing of SemEval-2013?s 
training set. Every sentence pair is tokenized, 
lemmatized and POS-tagged using Freeling 2.2 
tool (Atserias et al, 2006). Afterwards, several 
methods and algorithms are applied in order to 
extract all features for our Machine Learning 
System (MLS). Each run uses a particular group 
of features. 
110
Figure 1. System Architecture. 
The Run 1 (named MultiSemLex) is our main 
run. This takes into account all extracted features 
and trains a model with a Bagging classifier 
(Breiman, 1996) (using REPTree). The training 
corpus has been provided by SemEval-2013 
competition, in concrete by the Semantic Textual 
Similarity task.  
The Run 2 (named MultiLex) and Run 3 
(named MultiSem) use the same classifier, but 
including different features. Run 2 uses (see 
Figure 1) features extracted from Lexical-
Semantic Metrics (LS-M) described in section 
4.1, and Lexical-Semantic Alignment (LS-A) 
described in section 4.2. 
On the other hand, Run 3 uses features 
extracted only from Semantic Alignment (SA) 
described in section 4.3. 
As a result, we obtain three trained models 
capable to estimate the similarity value between 
two phrases. 
Finally, we test our system with the SemEval-
2013 test set (see Table 14 with the results of our 
three runs). The following section describes the 
features extraction process. 
4 Description of the features used in the 
Machine Learning System 
Many times when two phrases are very similar, 
one sentence is in a high degree lexically 
overlapped by the other. Inspired in this fact we 
developed various algorithms, which measure 
the level of overlapping by computing a quantity 
of matching words in a pair of phrases. In our 
system, we used as features for a MLS lexical 
and semantic similarity measures. Other features 
were extracted from a lexical-semantic sentences 
alignment and a variant using only a semantic 
alignment. 
4.1 Similarity measures 
We have used well-known string based 
similarity measures like: Needleman-Wunch 
(sequence alignment), Smith-Waterman 
(sequence alignment), Smith-Waterman-Gotoh, 
Smith-Waterman-Gotoh-Windowed-Affine, 
Jaro, Jaro-Winkler, Chapman-Length-Deviation, 
Chapman-Mean-Length, QGram-Distance, 
Block-Distance, Cosine Similarity, Dice 
Similarity, Euclidean Distance, Jaccard 
Similarity, Matching Coefficient, Monge-Elkan 
and Overlap-Coefficient. These algorithms have 
been obtained from an API (Application 
Program Interface) SimMetrics library v1.5 for 
.NET 2.03. We obtained 17 features for our MLS 
from these similarity measures. 
Using Levenshtein?s edit distance (LED), we 
computed also two different algorithms in order 
to obtain the alignment of the phrases. In the first 
one, we considered a value of the alignment as 
the LED between two sentences. Contrary to 
(Tatu et al, 2006), we do not remove the 
punctuation or stop words from the sentences, 
                                                          
3 Copyright (c) 2006 by Chris Parkinson, available in 
http://sourceforge.net/projects/simmetrics/ 
Run1.Bagging Classifier 
Training set from 
SemEval 2013 
Pre-Processing (using Freeling) 
 
Run 3 Bagging  
classifier 
Run 2 Bagging 
classifier 
Similarity Scores 
Feature extraction 
Lexical-Semantic Metrics 
 
Lexical-semantic 
alignment 
Semantic 
alignment 
 
Jaro QGra
m 
Rel. 
Concept 
. . . 
Tokenizing Lemmatizing POS tagging 
SemEval 
2013 Test 
set 
     Training Process (using Weka) 
111
neither consider different cost for transformation 
operation, and we used all the operations 
(deletion, insertion and substitution).  
The second one is a variant that we named 
Double Levenshtein?s Edit Distance (DLED) 
(see Table 9 for detail). For this algorithm, we 
used LED to measure the distance between the 
phrases, but in order to compare the words, we 
used LED again (Fern?ndez et al, 2012; 
Fern?ndez Orqu?n et al, 2009). 
Another distance we used is an extension of 
LED named Extended Distance (in spanish 
distancia extendida (DEx)) (see (Fern?ndez et 
al., 2012; Fern?ndez Orqu?n et al, 2009) for 
details). This algorithm is an extension of the 
Levenshtein?s algorithm, with which penalties 
are applied by considering what kind of 
transformation (insertion, deletion, substitution, 
or non-operation) and the position it was carried 
out, along with the character involved in the 
operation. In addition to the cost matrixes used 
by Levenshtein?s algorithm, DEx also obtains 
the Longest Common Subsequence (LCS) 
(Hirschberg, 1977) and other helpful attributes 
for determining similarity between strings in a 
single iteration. It is worth noting that the 
inclusion of all these penalizations makes the 
DEx algorithm a good candidate for our 
approach.  
In our previous work (Fern?ndez Orqu?n et al, 
2009), DEx demonstrated excellent results when 
it was compared with other distances as 
(Levenshtein, 1965), (Neeedleman and Wunsch, 
1970), (Winkler, 1999). We also used as a 
feature the Minimal Semantic Distances 
(Breadth First Search (BFS)) obtained between 
the most relevant concepts of both sentences. 
The relevant concepts pertain to semantic 
resources ISR-WN (Guti?rrez et al, 2011; 
2010a), as WordNet (Miller et al, 1990a), 
WordNet Affect (Strapparava and Valitutti, 
2004), SUMO (Niles and Pease, 2001) and 
Semantic Classes (Izquierdo et al, 2007). Those 
concepts were obtained after having applied the 
Association Ratio (AR) measure between 
concepts and words over each sentence. (We 
refer reader to (Guti?rrez et al, 2010b) for a 
further description). 
Another attribute obtained by the system was a 
value corresponding with the sum of the smaller 
distances (using QGram-Distance) between the 
words or the lemmas of the phrase one with each 
words of the phrase two. 
As part of the attributes extracted by the 
system, was also the value of the sum of the 
smaller distances (using Levenshtein) among 
stems, chunks and entities of both phrases. 
4.2 Lexical-Semantic alignment 
Another algorithm that we created is the Lexical-
Semantic Alignment. In this algorithm, we tried 
to align the phrases by its lemmas. If the lemmas 
coincide we look for coincidences among parts-
of-speech4 (POS), and then the phrase is 
realigned using both. If the words do not share 
the same POS, they will not be aligned. To this 
point, we only have taken into account a lexical 
alignment. From now on, we are going to apply 
a semantic variant. After all the process, the non-
aligned words will be analyzed taking into 
account its WordNet?s relations (synonymy, 
hyponymy, hyperonymy, derivationally-related-
form, similar-to, verbal group, entailment and 
cause-to relation); and a set of equivalences like 
abbreviations of months, countries, capitals, days 
and currency. In case of hyperonymy and 
hyponymy relation, words are going to be 
aligned if there is a word in the first sentence 
that is in the same relation (hyperonymy or 
hyponymy) with another one in the second 
sentence. For the relations ?cause-to? and 
?implication? the words will be aligned if there 
is a word in the first sentence that causes or 
implicates another one in the second sentence. 
All the other types of relations will be carried 
out in bidirectional way, that is, there is an 
alignment if a word of the first sentence is a 
synonymous of another one belonging to the 
second one or vice versa. 
Finally, we obtain a value we called alignment 
relation. This value is calculated as ??? =
 ??? / ????. Where ??? is the final 
alignment value, ??? is the number of aligned 
words, and ???? is the number of words of the 
shorter phrase. The  ??? value is also another 
feature for our system. Other extracted attributes 
they are the quantity of aligned words and the 
quantity of not aligned words. The core of the 
alignment is carried out in different ways, which 
                                                          
4 (noun, verb, adjective, adverbs, prepositions, 
conjunctions, pronouns, determinants, modifiers, etc.) 
112
are obtained from several attributes.  Each way 
can be compared by: 
? the part-of-speech. 
? the morphology and the part-of-speech. 
? the lemma and the part-of-speech. 
? the morphology, part-of-speech, and 
relationships of WordNet. 
? the lemma, part-of-speech, and 
relationships of WordNet. 
4.3 Semantic Alignment 
This alignment method depends on calculating 
the semantic similarity between sentences based 
on an analysis of the relations, in ISR-WN, of 
the words that fix them. 
First, the two sentences are pre-processed with 
Freeling and the words are classified according 
to their POS, creating different groups. 
The distance between two words will be the 
distance, based on WordNet, of the most 
probable sense of each word in the pair, on the 
contrary of our previously system in SemEval 
2012. In that version, we assumed the selected 
sense after apply a double Hungarian Algorithm 
(Kuhn, 1955), for more details  please refer to 
(Fern?ndez et al, 2012). The distance is 
computed according to the equation (1): 
?(?, ?) = ? ? ? ?(?[?], ?[? + 1])?=??=0 ; (1) 
Where ? is the collection of synsets 
corresponding to the minimum path between 
nodes ? and ?, ? is the length of ? subtracting 
one, ? is a function that search the relation 
connecting ? and ? nodes, ? is a weight 
associated to the relation searched by ? (see 
Table 1). 
Relation Weight 
Hyponym, Hypernym 2 
Member_Holonym, Member_Meronym, 
Cause, Entailment 
5 
Similar_To 10 
Antonym 200 
Other relation different to Synonymy 60 
Table 1. Weights applied to WordNet relations. 
Table 1 shows the weights associated to 
WordNet relations between two synsets. 
Let us see the following example: 
? We could take the pair 99 of corpus 
MSRvid (from training set of SemEval-
2013) with a littler transformation in 
order to a better explanation of our 
method. 
Original pair 
A: A polar bear is running towards a group of 
walruses. 
B: A polar bear is chasing a group of walruses. 
Transformed pair: 
A1: A polar bear runs towards a group of cats. 
B1: A wale chases a group of dogs. 
Later on, using equation (1), a matrix with the 
distances between all groups of both phrases is 
created (see Table 2). 
GROUPS polar bear runs towards group cats 
wale Dist:=3 Dist:=2 Dist:=3 Dist:=5  Dist:=2 
chases Dist:=4 Dist:=3 Dist:=2 Dist:=4  Dist:=3 
group     Dist:=0  
dogs Dist:=3 Dist:=1 Dist:=4 Dist:=4  Dist:=1 
Table 2. Distances between groups. 
Using the Hungarian Algorithm (Kuhn, 1955) 
for Minimum Cost Assignment, each group of 
the first sentence is checked with each element 
of the second sentence, and the rest is marked as 
words that were not aligned. 
In the previous example the words ?toward? 
and ?polar? are the words that were not aligned, 
so the number of non-aligned words is two. 
There is only one perfect match: ?group-group? 
(match with cost=0). The length of the shortest 
sentence is four. The Table 3 shows the results 
of this analysis. 
Number of exact 
coincidence 
Total Distances of 
optimal Matching 
Number of 
non-aligned 
Words 
1 5 2 
Table 3. Features from the analyzed sentences. 
This process has to be repeated for nouns (see 
Table 4), verbs, adjective, adverbs, prepositions, 
conjunctions, pronouns, determinants, modifiers, 
digits and date times. On the contrary, the tables 
have to be created only with the similar groups 
of the sentences. Table 4 shows features 
extracted from the analysis of nouns. 
GROUPS bear group cats 
wale Dist := 2  Dist := 2 
group  Dist := 0  
dogs Dist := 1  Dist := 1 
Table 4. Distances between groups of nouns. 
113
Number of 
exact 
coincidence 
Total Distances 
of optimal 
Matching 
Number of non-aligned 
Words 
1 3 0 
Table 5. Feature extracted from analysis of nouns. 
Several attributes are extracted from the pair of 
sentences (see Table 3 and Table 5). Three 
attributes considering only verbs, only nouns, 
only adjectives, only adverbs, only prepositions, 
only conjunctions, only pronouns, only 
determinants, only modifiers, only digits, and 
only date times. These attributes are:  
? Number of exact coincidences 
? Total distance of matching 
? Number of words that do not match 
Many groups have particular features 
according to their parts-of-speech. The group of 
the nouns has one more feature that indicates if 
the two phrases have the same number (plural or 
singular). For this feature, we take the average of 
the number of each noun in the phrase like a 
number of the phrase.  
For the group of adjectives we added a feature 
indicating the distance between the nouns that 
modify it from the aligned adjectives, 
respectively.  
For the verbs, we search the nouns that precede 
it, and the nouns that are next of the verb, and 
we define two groups. We calculated the 
distance to align each group with every pair of 
aligned verbs. The verbs have other feature that 
specifies if all verbs are in the same verbal time.  
With the adverbs, we search the verb that is 
modified by it, and we calculate their distance 
from all alignment pairs.  
With the determinants and the adverbs we 
detect if any of the alignment pairs are 
expressing negations (like don?t, or do not) in 
both cases or not. Finally, we determine if the 
two phrases have the same principal action. For 
all this new features, we aid with Freeling tool. 
As a result, we finally obtain 42 attributes from 
this alignment method. It is important to remark 
that this alignment process searches to solve, for 
each word from the rows (see Table 4) it has a 
respectively word from the columns. 
4.4 Description of the alignment feature 
From the alignment process, we extract different 
features that help us a better result of our MLS. 
Table 6 shows the group of features with lexical 
and semantic support, based on WordNet 
relation (named F1). Each of they were named 
with a prefix, a hyphen and a suffix. Table 7 
describes the meaning of every prefix, and Table 
8 shows the meaning of the suffixes. 
Features 
CPA_FCG, CPNA_FCG, SIM_FCG, CPA_LCG, 
CPNA_LCG, SIM_LCG, CPA_FCGR, 
CPNA_FCGR, SIM_FCGR, CPA_LCGR, 
CPNA_LCGR, SIM_LCGR 
Table 6. F1. Semantic feature group. 
Prefixes Descriptions 
CPA Number of aligned words. 
CPNA Number of non-aligned words. 
SIM Similarity 
Table 7. Meaning of each prefixes. 
Prefixes Compared words for? 
FCG Morphology and POS 
LCG Lemma and POS 
FCGR Morphology, POS and WordNet relation. 
LCGR Lemma, POS and WordNet relation. 
Table 8. Suffixes for describe each type of alignment. 
Features Descriptions 
LevForma Levenshtein Distance between two 
phrases comparing words by 
morphology 
LevLema The same as above, but now 
comparing by lemma. 
LevDoble Idem, but comparing again by 
Levenshtein and accepting words 
match if the distance is ? 2. 
DEx Extended Distance 
NormLevF, 
NormLevL 
Normalized forms of LevForma and 
LevLema. 
Table 9. F2. Lexical alignment measures. 
Features 
NWunch, SWaterman, SWGotoh, SWGAffine, Jaro, 
JaroW, CLDeviation, CMLength, QGramD, BlockD, 
CosineS, DiceS, EuclideanD, JaccardS, MaCoef, 
MongeElkan, OverlapCoef. 
Table 10. Lexical Measure from SimMetrics library. 
Features Descriptions 
AxAQGD_L All against all applying QGramD 
and comparing by lemmas of the 
words. 
AxAQGD_F Same as above, but applying 
QGramD and comparing by 
morphology. 
AxAQGD_LF Idem, not only comparing by lemma 
but also by morphology. 
AxALev_LF All against all applying Levenhstein 
114
comparing by morphology and 
lemmas. 
AxA_Stems Idem, but applying Levenhstein 
comparing by the stems of the 
words. 
Table 11. Aligning all against all. 
Other features we extracted were obtained 
from the following similarity measures (named 
F2) (see Table 9 for detail). 
We used another group named F3, with lexical 
measure extracted from SimMetric library (see 
Table 10 for detail). 
Finally we used a group of five feature (named 
F4), extracted from all against all alignment (see 
Table 11 for detail). 
4.5 Description of the training phase 
For the training process, we used a supervised 
learning framework, including all the training set 
as a training corpus. Using ten-fold cross 
validation with the classifier mentioned in 
section 3 (experimentally selected). 
As we can see in Table 12, the attributes 
corresponding with the Test 1 (only lexical 
attributes) obtain 0.7534 of correlation. On the 
other side, the attributes of the Test 2 (lexical 
features with semantic support) obtain 0.7549 of 
correlation, and all features obtain 0.7987. Being 
demonstrated the necessity to tackle the problem 
of the similarity from a multidimensional point 
of view (see Test 3 in the Table 12). 
Features 
Correlation on the training data of SemEval-
2013 
Test 1 Test 2 Test 3 
F1 
 0.7549 
0.7987 
F2 
F3 0.7534  
F4   
Table 12. Features influence. Gray cells mean 
features are not taking into account. 
5 Result and discussion 
Semantic Textual Similarity task of SemEval-
2013 offered two official measures to rank the 
systems5: Mean- the main evaluation value, 
Rank- gives the rank of the submission as 
ordered by the "mean" result. 
                                                          
5http://ixa2.si.ehu.es/sts/index.php?option=com_content&vi
ew=article&id=53&Itemid=61 
Test data for the core test datasets, coming 
from the following: 
Corpus Description 
Headlineas: news headlines mined from several news 
sources by European Media Monitor 
using the RSS feed. 
OnWN: mapping of lexical resources OnWN. The 
sentences are sense definitions from 
WordNet and OntoNotes. 
FNWN: the sentences are sense definitions from 
WordNet and FrameNet. 
SMT: SMT dataset comes from DARPA GALE 
HTER and HyTER. One sentence is a 
MT output and the other is a reference 
translation where a reference is generated 
based on human post editing. 
Table 13. Test Core Datasets. 
Using these measures, our second run (Run 2) 
obtained the best results (see Table 14). As we 
can see in Table 14, our lexical run has obtained 
our best result, given at the same time worth 
result in our other runs. This demonstrates that 
tackling this problem with combining multiple 
lexical similarity measure produce better results 
in concordance to this specific test corpora. 
To explain Table 14 we present following 
descriptions: caption in top row mean: 1- 
Headlines, 2- OnWN, 3- FNWN, 4- SMT and 5- 
mean. 
Run 1 R 2 R 3 R 4 R 5 R 
1 0.5841 60 0.4847 54 0.2917 52 0.2855 66 0.4352 58 
2 0.6168 55 0.5557 39 0.3045 50 0.3407 28 0.4833 44 
3 0.3846 85 0.1342 88 -0.0065 85 0.2736 72 0.2523 87 
Table 14. Official SemEval-2013 results over test 
datasets. Ranking (R). 
The Run 1 is our main run, which contains the 
junction of all attributes (lexical and semantic 
attributes).  Table 14 shows the results of all the 
runs for a different corpus from test phase. As 
we can see, Run 1 did not obtain the best results 
among our runs. 
Otherwise, Run 3 uses more semantic analysis 
than Run 2, from this; Run 3 should get better 
results than reached over the corpus of FNWN, 
because this corpus is extracted from FrameNet 
corpus (Baker et al, 1998) (a semantic network). 
FNWN provides examples with high semantic 
content than lexical. 
Run 3 obtained a correlation coefficient of 
0.8137 for all training corpus of SemEval 2013, 
115
while Run 2 and Run 1 obtained 0.7976 and 
0.8345 respectively with the same classifier 
(Bagging using REPTree, and cross validation 
with ten-folds). These results present a 
contradiction between test and train evaluation. 
We think it is consequence of some obstacles 
present in test corpora, for example:  
In headlines corpus there are great quantity of 
entities, acronyms and gentilics that we not take 
into account in our system. 
The corpus FNWN presents a non-balance 
according to the length of the phrases. 
In OnWN -test corpus-, we believe that some 
evaluations are not adequate in correspondence 
with the training corpus. For example, in line 7 
the goal proposed was 0.6, however both phrases 
are semantically similar. The phrases are: 
? the act of lifting something 
? the act of climbing something. 
We think that 0.6 are not a correct evaluation 
for this example. Our system result, for this 
particular case, was 4.794 for Run 3, and 3.814 
for Run 2, finally 3.695 for Run 1. 
6 Conclusion and future works 
This paper have introduced a new framework for 
recognizing Semantic Textual Similarity, which 
depends on the extraction of several features that 
can be inferred from a conventional 
interpretation of a text. 
As mentioned in section 3 we have conducted 
three different runs, these runs only differ in the 
type of attributes used. We can see in Table 14 
that all runs obtained encouraging results. Our 
best run was situated at 44th position of 90 runs 
of the ranking of SemEval-2013.  Table 12 and 
Table 14 show the reached positions for the three 
different runs and the ranking according to the 
rest of the teams.  
In our participation, we used a MLS that works 
with features extracted from five different 
strategies: String Based Similarity Measures, 
Semantic Similarity Measures, Lexical-Semantic 
Alignment and Semantic Alignment. 
We have conducted the semantic features 
extraction in a multidimensional context using 
the resource ISR-WN, the one that allowed us to 
navigate across several semantic resources 
(WordNet, WordNet Domains, WordNet Affect, 
SUMO, SentiWordNet and Semantic Classes). 
Finally, we can conclude that our system 
performs quite well. In our current work, we 
show that this approach can be used to correctly 
classify several examples from the STS task of 
SemEval-2013. Compared with the best run of 
the ranking (UMBC_EBIQUITY- ParingWords) 
(see Table 15) our main run has very close 
results in headlines (1), and SMT (4) core test 
datasets. 
Run 1 2 3 4 5 6 
(First) 0.7642 0.7529 0.5818 0.3804 0.6181 1 
(Our) 
RUN 2 
0.6168 0.5557 0.3045 0.3407 0.4833 44 
Table 15. Comparison with best run (SemEval 2013). 
As future work we are planning to enrich our 
semantic alignment method with Extended 
WordNet (Moldovan and Rus, 2001), we think 
that with this improvement we can increase the 
results obtained with texts like those in OnWN 
test set. 
6.1 Team Collaboration 
Is important to remark that our team has been 
working up in collaboration with INAOE 
(Instituto Nacional de Astrof?sica, ?ptica y 
Electr?nica) and LIPN (Laboratoire 
d'Informatique de Paris-Nord), Universit? Paris 
13 universities, in order to encourage the 
knowledge interchange and open shared 
technology. Supporting this collaboration, 
INAOE-UPV (Instituto Nacional de Astrof?sica, 
?ptica y Electr?nica and Universitat Polit?cnica 
de Val?ncia) team, in concrete in INAOE-UPV-
run 3 has used our semantic distances for nouns, 
adjectives, verbs and adverbs, as well as lexical 
attributes like LevDoble, NormLevF, NormLevL 
and Ext (see influence of these attributes in 
Table 12). 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
116
Reference 
Agirre, E.; D. Cer; M. Diab and W. Guo. *SEM 2013 
Shared Task: Semantic Textual Similarity 
including a Pilot on Typed-Similarity. *SEM 
2013: The Second Joint Conference on Lexical and 
Computational Semantics, Association for 
Computational Linguistics, 2013.  
Aguirre, E. and D. Cerd. SemEval 2012 Task 6:A 
Pilot on Semantic Textual Similarity. First Join 
Conference on Lexical and Computational 
Semantic (*SEM), Montr?al, Canada, Association 
for Computational Linguistics., 2012. 385-393 p.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006. 
Baker, C. F.; C. J. Fillmore and J. B. Lowe. The 
berkeley framenet project. Proceedings of the 17th 
international conference on Computational 
linguistics-Volume 1, Association for 
Computational Linguistics, 1998. 86-90 p.  
Banea, C.; S. Hassan; M. Mohler and R. Mihalcea. 
UNT:A Supervised Synergistic Approach to 
SemanticText Similarity. First Joint Conference on 
Lexical and Computational Semantics (*SEM), 
Montr?al. Canada, Association for Computational 
Linguistics, 2012. 635?642 p.  
Breiman, L. Bagging predictors Machine learning, 
1996, 24(2): 123-140. 
Corley, C. and R. Mihalcea. Measuring the Semantic 
Similarity of Texts, Association for Computational 
Linguistic. Proceedings of the ACL Work shop on 
Empirical Modeling of Semantic Equivalence and 
Entailment, pages 13?18, June 2005. 
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; 
A. Gonz?lez; R. Estrada; Y. Casta?eda; S. 
V?zquez; A. Montoyo and R. Mu?oz. 
UMCC_DLSI: Multidimensional Lexical-
Semantic Textual Similarity. {*SEM 2012}: The 
First Joint Conference on Lexical and 
Computational Semantics -- Volume 1: 
Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Fern?ndez Orqu?n, A. C.; J. D?az Blanco; A. Fundora 
Rolo and R. Mu?oz Guillena. Un algoritmo para la 
extracci?n de caracter?sticas lexicogr?ficas en la 
comparaci?n de palabras. IV Convenci?n 
Cient?fica Internacional CIUM, Matanzas, Cuba, 
2009.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based 
on WordNet. XXVI Congreso de la Sociedad 
Espa?ola para el Procesamiento del Lenguaje 
Natural, Universidad Polit?cnica de Valencia, 
Valencia, SEPLN 2010, 2010a. 161-168 p. 1135-
5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011, 47: 249-257. 
Hirschberg, D. S. Algorithms for the longest common 
subsequence problem J. ACM, 1977, 24: 664?675. 
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Kuhn, H. W. The Hungarian Method for the 
assignment problem Naval Research Logistics 
Quarterly, 1955, 2: 83?97. 
Levenshtein, V. I. Binary codes capable of correcting 
spurious insertions and deletions of ones. Problems 
of information Transmission. 1965. pp. 8-17 p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller. Five papers on WordNet. 
Princenton University, Cognositive Science 
Laboratory, 1990a. 
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller Introduction to WordNet: An On-
line Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990b. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Neeedleman, S. and C. Wunsch A general method 
applicable to the search for similarities in the 
amino acid sequence of two proteins Mol. Biol, 
1970, 48(443): 453. 
Niles, I. and A. Pease. Origins of the IEEE Standard 
Upper Ontology. Working Notes of the IJCAI-
2001 Workshop on the IEEE Standard Upper 
Ontology, Seattle, Washington, USA., 2001. 
117
?ari?, F.; G. Glava?; Mladenkaran; J. ?najder and B. 
D. Basi?. TakeLab: Systems for Measuring 
Semantic Text Similarity.  Montr?al, Canada, First 
Join Conference on Lexical and Computational 
Semantic (*SEM), pages 385-393. Association for 
Computational Linguistics., 2012.  
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of 
the 4th International Conference on Language 
Resources and Evaluation (LREC 2004), Lisbon, 
2004. 1083-1086 p. 
Tatu, M.; B. Iles; J. Slavick; N. Adrian and D. 
Moldovan. COGEX at the Second Recognizing 
Textual Entailment Challenge. Proceedings of the 
Second PASCAL Recognising Textual Entailment 
Challenge Workshop, Venice, Italy, 2006. 104-109 
p.  
Werning, M.; E. Machery and G. Schurz. The 
Compositionality of Meaning and Content, 
Volume 1: Foundational issues. ontos verlag 
[Distributed in] North and South America by 
Transaction Books, 2005. p. Linguistics & 
philosophy, Bd. 1. 3-937202-52-8. 
Winkler, W. The state of record linkage and current 
research problems. Technical Report, Statistical 
Research Division, U.S, Census Bureau, 1999. 
 
 
118
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 93?97, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(EPS): Paraphrases Detection Based on Semantic 
Distance 
 
H?ctor D?vila, Antonio Fern?ndez Orqu?n, 
Alexander Ch?vez, Yoan Guti?rrez, Armando 
Collazo, Jos? I. Abreu 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba. 
{hector.davila, tony, 
alexander.chavez, yoan.gutierrez, 
armando.collazo, 
jose.abreu}@umcc.cu 
Andr?s Montoyo, Rafael Mu?oz 
DLSI, University of Alicante Carretera 
de San Vicente S/N Alicante, Spain. 
{montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
This paper describes the specifications and 
results of UMCC_DLSI-(EPS) system, which 
participated in the first Evaluating Phrasal 
Semantics of SemEval-2013. Our supervised 
system uses different kinds of semantic 
features to train a bagging classifier used to 
select the correct similarity option. Related to 
the different features we can highlight the 
resource WordNet used to extract semantic 
relations among words and the use of different 
algorithms to establish semantic similarities. 
Our system obtains promising results with a 
precision value around 78% for the English 
corpus and 71.84% for the Italian corpus. 
1 Introduction 
It is well known finding words similarity, even 
when it is lexical or semantic can improve 
entailment recognition and paraphrase 
identification; and ultimately lead to improvements 
in a wide range of applications in Natural 
Language Processing (NLP). Several areas like 
question answering, query expansion, information 
retrieval, and many others, depend on phrasal 
semantics (PS). PS, is concerned with how the 
meaning of a sentence is composed both from the 
meaning of the constituent words, and from extra 
meaning contained within the structural 
organization of the sentence itself (Dominey, 
2005). 
The aim of SemEval 2013 competition is also 
discovering similarity, specifically in Evaluating 
Phrasal Semantics (EPS). The goal of this task is to 
evaluate how well systems can judge the semantic 
similarity of a word and a short sequence of words. 
That is, given a set of pairs of this type; classify it 
on negative (if the meaning of the word is 
semantically different to the meaning of the 
sequence) or positive (if the meaning of the 
sequence, as a whole, is semantically close to the 
meaning of the word).  
Based on this, we developed a system capable to 
detect if two phrases are semantically close. 
The rest of this paper, specifically section 2 is a 
brief Related Work. Section 3 describes the system 
architecture and our run. Continuing with section 4 
we describe the training phase. Following that, 
section 5 presents the results and discussion for our 
Machine Learning System. Finally we conclude 
and propose our future works (Section 6). 
2 Related Work 
There have been many WordNet-based similarity 
measures, among other highlights the work of 
researchers like (Budanitsky and Hirst, 2006; 
Leacock and Chodorow, 1998; Mihalcea et al, 
2006; Richardson et al, 1994). 
On the other hand, WordNet::Similarity1 
(Pedersen et al, 2004) has been used by other 
researchers in an interesting array of domains. 
WordNet::Similarity implements measures of 
similarity and relatedness between a pair of 
concepts (or synsets2) based on the structure and 
content of WordNet. According to (Pedersen et al, 
2004), three of the six measures of similarity are 
based on the information content of the least 
                                                     
1http://sourceforge.net/projects/wn-similarity/ 
2 A group of English words into sets of synonyms. 
93
common subsumer (LCS). These measures include 
res (Resnik, 1995), lin (Lin, 1998), and jcn (Jiang 
and Conrath, 1997). 
Pursuant to Pedersen, there are three other 
similarity measures based on path lengths between 
a pair of concepts: lch (Leacock and Chodorow, 
1998), wup (Wu and Palmer, 1994), and path. 
Our proposal differs from those of 
WordNet::Similarity and other measures of 
similarity in the way we selected the relevant 
WordNet relations (see section 3.2 for detail). 
Unlike others, our measure assign weight to 
WordNet relations (any we consider relevant) 
depending to the place they occupy in the 
minimum path and the previously visited relations. 
Besides these, the novelty of our approach is 
using the weights as a function of semantic 
relations in a minimal distance path and also the 
method we used to arrive to those weight functions 
or rules. 
3 System Architecture and description of 
the run 
As we can see in Figure 1 our run begin with the 
pre-processing of SemEval 2013?s training set. 
Every sentence pair is tokenized, lemmatized and 
POS-tagged using Freeling 2.2 tool (Atserias et al, 
2006). Afterwards, several methods and algorithms 
are applied in order to extract all features for our 
Machine Learning System (MLS). The system 
trains the classifier using a model based on 
bagging (using JRip3). The training corpus has 
been provided by SemEval-2013 competition, in 
concrete by the EPS task. As a result, we obtain a 
trained model capable to detect if one phrase 
implies other. Finally, we test our system with the 
SemEval 2013 test set (see Table 2 with the results 
of our run). The following section describes the 
features extraction process. 
3.1 Description of the features used in the 
Machine Learning System 
In order to detect entailment between a pair of 
phrases, we developed an algorithm that searches a 
semantic distance, according to WordNet (Miller et 
al., 1990), between each word in the first phrase 
with each one in the second phrase. 
We used four features which intend to measure 
the level of proximity between both sentences: 
                                                     
3 JRip is an inference and rules-based learner. 
? The minimum distance to align the first 
phrase with the second (MinDist). See section 
3.2 for details. 
? The maximal distance to align the first phrase 
with the second (MaxDist). 
? The average of all distances results to align 
the first phrase with the second one. 
(AverageDistance). 
? The absolute relative error of all distances 
results to align the first phrase with the 
second respect to the average of them. 
 
Figure 1. System Architecture. 
Other features included are the most frequent 
relations contained in the shorted path of the 
minimum distance; result to align the first phrase 
with the second one. Following table shows the 
relations selected as most frequent. 
A weight was added to each of them, according 
to the place it occupy in the shortest path between 
two synsets. The shortest path was calculated using 
Breadth -First-Search algorithm (BFS) (Cormen et 
al., 2001). 
In addition, there is one feature that takes into 
account any other relationship that is not 
previously considered. 
Finally, as a result we obtain 22 features from 
this alignment method. 
Semeval 2013 test 
set
?
Pre-Processing (using Freeling 2.2)
Tokenizing Lemmatizing POS Tagging
Run 1
Bagging Classifier (JRip)
Feature Extraction
MinDistance MaxDistance error ?
Training set from 
Semeval 2013
Pre-Processing (using Freeling 2.2)
Tokenizing Lemmatizing POS Tagging
Feature Extraction
MinDistance MaxDistance error
Supervised Model
Training process (using Weka)
Bagging Classifier (JRip)
Paraphrases Detection
94
Relation Weight (? function) 
Antonym 1000 
Synonym 0 
Hyponym/ Hypernym 
100 if exist an antonym 
before, 30 if exist other 
relation before (except 
synonym, hyponym, 
hypernym), 5 otherwise. 
Meber_Holonym/ 
PartHolonym 
100 if exist an antonym 
before, 20 if exist a 
hyponym or a hypernym,10 
otherwise. 
Cause/ Entailment 
100 if exist an antonym 
before, 2 otherwise. 
Similar_To 
100 if exist an antonym 
before, 3 otherwise. 
Attribute 
100 if exist an antonym 
before, 8 otherwise. 
Also_See 
100 if exist an antonym 
before, 10 otherwise. 
Derivationaly_Related_Form 
100 if exist an antonym 
before, 5 otherwise. 
Domain_Of_Synset_Topic 
100 if exist an antonym 
before, 13 otherwise. 
Domain_Of_Synset_Usage 
100 if exist an antonym 
before, 60 otherwise. 
Member_Of_Domain_Topic 
100 if exist an antonym 
before, 13 otherwise. 
Member_Of_Domain_Usage 
100 if exist an antonym 
before, 60 otherwise. 
Other 100 
Table 1. Most frequents relations with their weight. 
3.2 Semantic Distance 
As aforementioned, our distance depends on 
calculating the similarity between sentences, based 
on the analysis of WordNet relations, and we only 
took into account the most frequent ones. When 
searching the shortest path between two WordNet 
synsets, frequents relations were considered the 
ones extracted according to the analysis made in 
the training corpus, provided by SemEval-2013. 
The distance between two synsets is calculated 
with the relations found; and simply it is the sum 
of the weights assigned to each connection. 
????????(?, ?) =  ????????(??, ??), ? (?, ?) (1) 
????????(?, ?) = ???(?? , ??), ?(?, ?) (2) 
???(??; ??) = ? ?(???(?[?], ?[? + 1]))
?=?
?=0
 (3) 
? = ???(??; ??) (4) 
Where ? and ? represents the i-th and j-th sense of 
the word; P and Q represents words collections; ?? 
is the X-th word of ?; ?? is the Y-th word of ?; 
???????? obtains a value that represents a 
minimal semantic distance across WordNet (Miller 
et al, 2006) resource (this resource is involved into 
the integrator resource, ISR-WN (Guti?rrez et al, 
2011a; 2010a); ????????  the minimal semantic 
distance between two words; ??? represents the 
minimal semantic distance between two senses 
collections; ? is a collection of synsets that 
represents the minimal path between two synsets 
using BFS; ??? obtains semantic relation types 
between two synsets; W is a functions that apply 
the rules described in Table 1. The maximum and 
average distance is calculated in a similar fashion 
but using the maximum and average instead of the 
minimum. 
3.3 Semantic Alignment 
First, the two sentences are pre-processed with 
Freeling 2.2 and the words are classified according 
to their parts-of-speech. Then, all senses of every 
word are taken and treated as a group. Distance 
between two groups will be the minimal distance 
(described in 3.1) between senses of any pair of 
words belonging to the group. 
In the example of Figure 2, Dist=280 is selected 
for the pair ?Balance-Culture? (minimal cost).  
Following the explanation on section 3.1 we 
extract the features guided to measure the level of 
proximity between both sentences. 
 
Figure 2. Distance between ?Balance? and ?Culture?. 
A maximum and average distance is calculated in a 
similar fashion, but using the maximum and 
average instead of the minimum. 
4 Description of the training phase 
For the training process, we used a supervised 
learning framework (based on Weka4), including 
all the training set (positive and negative instances) 
as a training corpus. We conduct several 
experiments in order to select the correct classifier, 
the best result being obtained with a model based 
on bagging (using JRip algorithm). Finally, we 
used 10-fold cross validation technique with the 
selected classifier, obtaining a classification value 
of 73.21%. 
                                                     
4 http://prdownloads.sourceforge.net/weka/ 
L mma: Balance
Sense 1
Sense 2
Lemma: Culture
Sense 1
Sense 2
3350
1030 280
880
Dist=280
95
5 Results and discussion 
EPS task of SemEval-2013 offered many official 
measures to rank the systems. Some of them are 
the following: 
o F-Measure (FM): Correct Response (CR), 
Instances correctly classified, True positives 
(TP), Instances correctly classified as 
positive. False Positives (FP), Instances 
incorrectly classified as positive, True 
Negatives (TN), Instances correctly 
classified as negative, False Negatives (FN), 
Instances incorrectly classified as negative. 
Corpus FM CR TP FP TN FN 
English 0.6892 2826 1198 325 1628 755 
Italian 0.6396 574 245 96 329 180 
Table 2. Official SemEval 2013 results. 
The behavior of our system, for English and 
Italian corpus is shown in Table 2. 
The only thing that changes to process the 
Italian corpus is that Freeling is used as input to 
identify Italian words and it returns the English 
WN synsets. The process continues in the same 
way as English. 
Figure 3: Semantic Distance distribution between 
negative and positive instances.  
As shown in Table 2, our main drawback is to 
classify positive instances. Sometimes, the distance 
between positive phrases is very far. This is due to 
the relations found in the minimum path are very 
similar to the one found in other pairs of negatives 
instances; this can be the cause of our MLS 
classifies them as negatives (see Figure 3). 
Figure 3 shows a distributional graphics that 
take a sample of 200 negative and positive 
instances. The graphics illustrate how close to zero 
value the positive instances are, while the 
negatives are far away from this value. However, 
in the approximate range between 80 and 200, we 
can see values of positive and negative instances 
positioning together. This can be the cause that our 
MLS misclassified some positive instances as 
negative. 
6 Conclusion and future work 
This paper introduced a new framework for EPS, 
which depends on the extraction of several features 
from WordNet relations. We have conducted the 
semantic features extraction in a multidimensional 
context using the resource ISR-WN(Guti?rrez et 
al., 2010a). 
Our semantic distance provides an appealing 
approach for dealing with phrasal detection based 
on WordNet relation. Our team reached the sixth 
position of ten runs for English corpus, with a 
small difference of 0.07 points compared to the 
best results with respect to accuracy parameter. 
Despite the problems caused by poorly selected 
positive instances, our distance (labeled as Our) 
obtained very similar results to those obtained by 
the best team (labeled as First5), which indicates 
that our work is well underway (see Table 3 for 
details). 
Team accuracy recall precision 
First 0.802611 0.751664 0.836944128 
Our 0.723502 0.613415 0.786605384 
Table 3. Comparative results (English corpus). 
It is important to remark that our system has 
been the only competitor to evaluate Italian texts. 
It has been possible due to our system include 
Freeling in the preprocessing stage. 
Our future work will aim to resolve instances 
misclassified by our algorithm. In addition, we will 
introduce lexical substitutions (synonyms) to 
expand the corpus, we will also apply conceptual 
semantic similarity using relevant semantic trees 
(Guti?rrez et al, 2010b; Guti?rrez et al, 2011b). 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
                                                     
5 christian_wartena. Team HsH. 
0
500
1000
Semantic Distance Distribution
Positive Instances Negative Instances
96
semantic services in an open-source NLP library. 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation (LREC?06), 
2006. 48-55 p.  
Budanitsky, A. and G. Hirst Evaluating wordnet-based 
measures of lexical semantic relatedness 
Computational Linguistics, 2006, 32(1): 13-47. 
Cormen, T. H.; C. E. Leiserson; R. L. Rivest and C. 
Stein. Introduction to algorithms. MIT press, 2001. 
0262032937. 
Dominey, P. F. Aspects of descriptive, referential, and 
information structure in phrasal semantics: A 
construction-based model Interaction Studies, 2005, 
6(2): 287-310. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, 
RANLP 2011 Organising Committee, 2011b. 233--
239 p.  
Jiang, J. J. and D. W. Conrath Semantic similarity based 
on corpus statistics and lexical taxonomy arXiv 
preprint cmp-lg/9709008, 1997. 
Leacock, C. and M. Chodorow Combining local context 
and WordNet similarity for word sense identification 
WordNet: An electronic lexical database, 1998, 
49(2): 265-283. 
Lin, D. An information-theoretic definition of 
similarity. Proceedings of the 15th international 
conference on Machine Learning, San Francisco, 
1998. 296-304 p.  
Mihalcea, R.; C. Corley and C. Strapparava. Corpus-
based and knowledge-based measures of text 
semantic similarity. Proceedings of the national 
conference on artificial intelligence, Menlo Park, 
CA; Cambridge, MA; London; AAAI Press; MIT 
Press; 1999, 2006. 775 p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller Introduction to WordNet: An On-line 
Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990. 
Miller, G. A.; C. Fellbaum; R. Tengi; P. Wakefield; H. 
Langone and B. R. Haskell. WordNet a lexical 
database for the English language. Cognitive Science 
Laboratory Princeton University 2006. 
Pedersen, T.; S. Patwardhan and J. Michelizzi. 
WordNet:: Similarity: measuring the relatedness of 
concepts. Demonstration Papers at HLT-NAACL 
2004, Association for Computational Linguistics, 
2004. 38-41 p.  
Resnik, P. Using information content to evaluate 
semantic similarity in a taxonomy arXiv preprint 
cmp-lg/9511007, 1995. 
Richardson, R.; A. F. Smeaton and J. Murphy. Using 
WordNet as a knowledge base for measuring 
semantic similarity between words, Technical Report 
Working Paper CA-1294, School of Computer 
Applications, Dublin City University, 1994. 
Wu, Z. and M. Palmer. Verbs semantics and lexical 
selection. Proceedings of the 32nd annual meeting on 
Association for Computational Linguistics, 
Association for Computational Linguistics, 1994. 
133-138 p.  
 
 
97
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 241?249, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Reinforcing a Ranking Algorithm with Sense 
Frequencies and Multidimensional Semantic Resources to solve 
Multilingual Word Sense Disambiguation 
 
Yoan Guti?rrez, Yenier 
Casta?eda, Andy Gonz?lez, 
Rainel Estrada, Dennys D. Piug, 
Jose I. Abreu, Roger P?rez 
Antonio Fern?ndez Orqu?n, 
Andr?s Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas DLSI, University of Alicante Independent Consultant 
Matanzas, Cuba Alicante, Spain USA 
{yoan.gutierrez, 
yenier.castaneda, 
rainel.estrada, 
dennys.puig, jose.abreu, 
roger.perez}@umcc.cu, 
andy.gonzalez@infonet.umcc
.cu 
antonybr@yahoo.com, 
{montoyo,rafael}@dlsi.ua.
es 
info@franccamara.c
om 
 
Abstract 
This work introduces a new unsupervised 
approach to multilingual word sense 
disambiguation. Its main purpose is to 
automatically choose the intended sense 
(meaning) of a word in a particular context for 
different languages. It does so by selecting the 
correct Babel synset for the word and the 
various Wiki Page titles that mention the 
word. BabelNet contains all the output 
information that our system needs, in its Babel 
synset. Through Babel synset, we find all the 
possible Synsets for the word in WordNet. 
Using these Synsets, we apply the 
disambiguation method Ppr+Freq to find what 
we need. To facilitate the work with WordNet, 
we use the ISR-WN which offers the 
integration of different resources to WordNet. 
Our system, recognized as the best in the 
competition, obtains results around 69% of 
Recall. 
1 Introduction 
Word Sense Disambiguation (WSD) focuses on 
resolving the semantic ambiguity of a given word.  
This is an important task in Natural Language 
Processing (NLP) because in many applications, 
such as Automatic Translation, it is essential to 
know the exact meaning of a word in a given 
context. In order to solve semantic ambiguity, 
different systems have been developed. However, 
we can categorize them in two main groups: 
supervised and unsupervised systems. The 
supervised ones need large quantity of hand-tagged 
data in order to gather enough information to build 
rules, train systems, and so on. Unsupervised 
systems, on the other hand, do not need such a 
large amount of hand-tagged datasets. This means 
that, when there aren?t enough corpora to train the 
systems, an unsupervised system is a good option. 
A sub-task of WSD is Multilingual Word Sense 
Disambiguation (MWSD) (Navigli et al, 2013) 
that aims at resolving ambiguities in different 
languages. 
In a language, there are words that have only one 
sense (or meaning), but in other languages, the 
same words can have different senses. For 
example, ?patient? is a word that in English can be 
either a noun or an adjective, but in German, it 
only has one sense - ?viz? (a person that needs 
treatment). This shows that the information 
obtained by combining two languages can be more 
useful for WSD because the word senses in each 
language can complement each other. For it to be 
useful, MWSD needs a multilingual resource that 
contains different languages, such as BabelNet 
(Navigli and Ponzetto, 2010; 2012) and 
EuroWordNet (Vossen, 1998). 
241
As the preferred disambiguation method, we 
decided to use the Ppr+Freq (Personalized Page 
Rank combined with Frequencies of senses)  
(Guti?rrez, 2012) method because, among 
unsupervised systems, graph-based methods have 
obtained more promising results.  
It is worth mentioning the relevant approaches 
used by the scientific community to achieve 
promising results. One approach used is structural 
interconnections, such as Structural Semantic 
Interconnections (SSI), which create structural 
specifications of the possible senses for each word 
in a context (Navigli and Velardi, 2005). The other 
approaches used are ?Exploring the integration of 
WordNet? (Miller et al, 1990), FrameNet (Laparra 
et al, 2010) and those using Page-Rank such as 
(Sinha and Mihalcea, 2007) and (Agirre and Soroa, 
2009). 
The aforementioned types of graph based 
approaches have achieved relevant results in both 
the SensEval-2 and SensEval-3 competitions (see 
Table 1). 
Algorithm Recall 
TexRank (Mihalcea, 2005)  54.2% 
(Sinha and Mihalcea, 2007) 56.4% 
(Tsatsaronis et al, 2007) 49.2% 
Ppr (Agirre and Soroa, 2009) 58.6% 
Table 1. Relevant WSD approaches. Recall measure is 
calculated recalls using SensEval-2 (English All Word 
task) guidelines over. 
Experiments using SensEval-2 and SensEval-3 
corpora suggest that Ppr+Freq (Guti?rrez, 2012) 
can lead to better results by obtaining over 64% of 
Recall. Therefore we selected Ppr+Freq as the 
WSD method for our system. 
The key proposal for this work is an 
unsupervised algorithm for MWSD, which uses an 
unsupervised method, Ppr+Freq, for semantic 
disambiguation with resources like BabelNet (as 
sense inventory only) (Navigli and Ponzetto, 2010) 
and ISR-WN (as knowledge base) (Guti?rrez et al, 
2011a; 2010a). 
ISR-WN was selected as the default knowledge 
base because of previous NLP research, which 
included: (Fern?ndez et al, 2012; Guti?rrez et al, 
2010b; Guti?rrez et al, 2012; 2011b; 2011c; 
2011d), which achieved relevant results using ISR-
WN as their knowledge base. 
2 System architecture  
By using one of BabelNet (BN) features, our 
technique begins by looking for all the Babel 
synsets (Bs) linked to the lemma of each word in 
the sentence that we need to disambiguate.  
Through the Bs offsets, we can get its 
corresponding WordNet Synset (WNS), which 
would be retrieved from WordNet (WN) using the 
ISR-WN resource. As a result, for each lemma, we 
have a WordNet Synset List (WNSL) from which 
our Word Sense Disambiguation method obtains 
one WNS as the correct meaning. 
Our WSD method consists of applying a 
modification of the Personalizing PageRank (Ppr) 
algorithm (Agirre and Soroa, 2009), which 
involves the senses frequency. More specifically, 
the key proposal is known as Ppr+Freq (see 
Section 2.3).  
Given a set of WNSLs of WNSL, as words 
window, we applied the Synsets ranking method, 
Ppr+Freq, which ranks in a descending order, the 
Synsets of each lemma according to a calculated 
factor of relevance. The first Synset (WNS) of 
each WNSL (the most relevant) is established as 
the correct one and its associated Babel synset (Bs) 
is also tagged as correct. To determine the Wiki 
Page Titles (WK), we examine the WIKI 
(Wikipedia pages) and WIKIRED (Wikipedia 
pages redirections) in the correct Babel synset 
obtained. 
Figure 1 shows a general description of our 
system that is made up of the following steps: 
I. Obtaining lemmas  
II. Obtaing WN Synset of selected lemmas  
III. Applying Ppr+Freq method  
IV. Assigning Synset, Babel synset and Wiki 
page title 
Note that ISR-WN contains WN as its nucleus. 
This allows linking both resources, BabelNet and 
ISR-WN.
242
 
Figure 1. General process description taking as instance a sentence provided by the trial dataset. 
 
2.1 Obtaining lemmas  
For each input sentence, we extract the labeled 
lemmas. As an example, for the sentence, ?The 
struggle against the drug lords in Colombia will be 
a near thing,? the selected lemmas are: ?struggle,? 
?drug_lord,? ?Colombia?, and ?near_thing.? 
 
Figure 2. Obtaining synset of lemmas. 
 
2.2 Obtaing WN Synset of selected lemmas  
For each lemma obtained in the previous section, 
we look through BabelNet to recover the Bs that 
contains the lemma among its labels. When BSs 
are mapped to WN, we use the ISR-WN resource 
to find the corresponding Synset. Since a lemma 
can appear in a different Bs, it can be mapped with 
several WNS. Thus, we get a Synset list for each 
lemma in the sentence. In case the lemma does not 
have an associated Bs, its list would be empty. An 
example of this step is shown on Figure 2. 
2.3 Applying Ppr+Freq method 
In the above case, Ppr+Freq modifies the ?classic? 
Page Rank approach instead of assigning the same 
weight for each sense of WN in the disambiguation 
graph (??). 
The PageRank (Brin and Page, 1998) 
adaptation, Ppr , which was popularized by (Agirre 
IV . Assigning Synset, Babel Synset and Wiki page title
? The struggle against the drug lords in Colombia will be a near thing .?
struggle drug_lord Colombia near_thing
Wikipedia WordNet BabelNet
ISR-WN
WordNet
(WN)
SUMO
WN-Domain
WN-Affect
SemanticClass eXtended WN3.0
eXtended WN1.7
struggle%1:04:01:: drug_lord%1:18:00:: colombia%1:15:00:: near_thing%1:04:00::
bn:00009079n bn:00028876n bn:00020697n bn:00057109n
-- Drug_Lord Colombia --
I. Obtaing lemmas
II. Obtaining Synset of selected lemmas
III. Applying Ppr+Freq method
WN key
BS
WK
struggle
drug_lord Colombia
near_thing
struggle
bn:00074762n wn:00587514n
bn:00009079n wn:00739796n
bn:00009080n wn:00901980n
drug_lord bn:00028876n wn:09394468n
colombia
bn:00020697n wn:08196765n
bn:02051949n
bn:02530766n
near_thing bn:00057109n wn:00193543n
Sentence lemmas 
Babel synset 
WordNet synset 
243
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main idea 
behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ? to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the more 
strength the voter would have. Thus, PageRank is 
generated by applying a random walkthrough from 
the internal interconnection of ?, where the final 
relevance of ??  represents the random walkthrough 
probability over ?, and ending on ??. 
Ppr+Freq includes the existent semantic and 
frequency patterns of each sense of the word to 
disambiguate while finding a way to connect each 
one of these words in a knowledge base. 
The new graph-based approach of WSD 
generates a graph of disambiguated words for each 
input sentence. For that reason, it is necessary to 
classify the word senses according to the other 
words that compose the context. The general 
method is shown in Figure 3. This method is 
divided into three steps: 
I. Creation of a disambiguation graph 
II. Application of Ppr+Freq in the generated 
graph 
III. Selection of the correct answer 
Creation of a disambiguation graph: In the first 
step, a disambiguation graph is built by means of a 
Breath First Search (BFS) over the ?super? graph 
composed by all the resources integrated into ISR-
WN. The components involved in this process are: 
WordNet, SUMO (Zouaq et al, 2009) WordNet 
Domains (Magnini and Cavaglia, 2000) WordNet 
Affects (Strapparava and Valitutti, 2004) Semantic 
Classes (Izquierdo et al, 2007) and eXtended 
WordNet (XWN) relations (Moldovan and Rus, 
2001). This search aims to recover all senses 
(nodes), domain labels (from WordNet Domain 
and WordNet Affects), SUMO categories, and 
Semantic Classes labels through the shortest path 
between every pair of senses in the WNSL set 
associated with the input sentence. Using ISR-WN 
as the KB, through experimentation, we obtained 
the shortest paths with a length of five edges. For a 
better understanding of this process, see (Guti?rrez, 
2012). 
Application of Ppr+Freq in the generated 
graph: In the second step, we use the weighted 
Personalized PageRank. Here, all the vertices from 
vector ? in ?? are initialized with the value  
1
?
 ; 
where ? is the number of nodes in ??. On the 
other hand, the vertices that represent word senses 
in the analyzed sentence are not initialized with 
this value. Instead, they are initialized with values 
in the range [0?1], which are associated to their 
occurrence frequency in SemCor1 (Corpus and 
sense frequencies knowledge). In the last step, 
after applying the Ppr+Freq algorithm over ??, we 
get a representative vector which contains ISR-WN 
nodes in ?? sorted in a descending order by a 
ranking score computed by this algorithm. For a 
better description, see (Guti?rrez, 2012). 
Selection of the correct answer: As the correct 
sense, we take the highest ranked sense of each 
target word involved in this vector. Note that 
domain labels, SUMO categories, semantic class 
labels, and affect labels are ranked too. They could 
be used in the future to determine relevant 
conceptualizations that would be useful for text 
classification and more. 
In our system, we assume the following 
configuration: dumping factor ? = 0.85 and like in 
(Agirre and Soroa, 2009) we used 30 iterations. A 
detailed explanation about PageRank algorithm 
can be found in (Agirre and Soroa, 2009). 
Table 2 shows an example that analyzes the 
Synset for each word in the sentence and also 
shows how the higher ranked Synsets of the target 
words are selected as the correct ones. For a 
detailed explanation of Ppr+Freq, see (Guti?rrez, 
2012). 
2.4 Assigning Synset, Babel synset and Wiki 
Pages 
In this step, English is handled differently from 
other languages because WordNet Synsets are 
available only for English. The following sections 
explain how we proceed in each case. Once the 
Synsets list is obtained for each lemma in section 
2.3, selecting the correct answer for the lemma is 
all that?s left to do. 
                                                     
1 http://www.cse.unt.edu/~rada/downloads.html 
244
 
Figure 3. General process of WSD with Ppr+Freq. 
2.4.1 English 
Given a lemma, we go through its Synset list from 
beginning to end looking for the first Synset that 
contains a key2 for the lemma. If such Synset 
exists, it is designated as the Synset for the lemma. 
Otherwise, no Synset is assigned. 
As already explained, each Synset in the list is 
connected to a Bs. Therefore, the lemma linked 
with the correct WNS selected in the previous step, 
is chosen as the correct lemma. In case no Synsets 
were designated as the correct ones, we take the 
first Bs in BN, which contains the lemma among 
its labels.  
To determine the Wiki pages titles (WK) we 
examine the WIKIRED and WIKI labels in the 
correct Bs selected in the preceding step. This 
search is restricted only to labels corresponding to 
the analyzed language and discriminating upper 
and lower case letters. Table 2 shows some sample 
results of the WSD process. 
Lemma struggle drug_lord 
WNS 00739796n 09394468n 
WN key struggle%1:04:01:: drug_lord%1:18:00:: 
Bs bn:00009079n bn:00028876n 
WK - Drug_Lord 
Lemma colombia near_thing 
WNS 08196765n 00193543n 
WN key colombia%1:15:00:: near_thing%1:04:00:: 
Bs bn:00020697n bn:00057109n 
WK Colombia - 
Table 2 : Example of English Language. 
                                                     
2A sense_key is the best way to represent a sense in 
semantic tagging or other systems that refer to WordNet 
senses. sense_key?s are independent of WordNet sense 
numbers and synset_offset?s, which vary between versions of 
the database. 
2.4.2 Other languages  
For this scenario, we introduce a change in the first 
step discussed in the previous section. The reason 
is that the Synsets do not contain any keys in any 
other language than English. Thus, the correct 
Synset for the lemma is the first in the Synset list 
for the lemma obtained, as described, in section 
2.3. 
3 Results 
We tested three versions (runs) of the proposed 
approach and evaluated them through a trial 
dataset provided by Task123 of Semeval-2013 
using babelnet-1.0.1. Table 3 shows the result for 
each run. Note that the table results were 
calculated with the traditional WSD recall 
measure, being this measure which has ranked 
WSD systems on mostly Semeval competitions. 
On the other hand, note that our precision and 
recall results are different because the coverage is 
not 100%. See Table 5. 
 English French 
Runs WNS Bs WK Bs WK 
Run1 0.70 0.71 0.77 0.59 0.85 
Run2 0.70 0.71 0.78 0.60 0.85 
Run3 0.69 0.70 0.77 - - 
Table 3 : Results of runs with trial recall values. 
As can be noticed on Table 3, results of different 
versions do not have big differences, but in 
general, Run2 achieves the best results; it?s better 
                                                     
3 http://www.cs.york.ac.uk/semeval-2013/task12 
ISR-WN
footballer#1 | cried#9 | winning#3
footballer | cry | winning
Lemmas
?The footballer cried when winning?
Disambiguation
Graph
(0,9)
Footballer#1
(0,3)
cry#7
(0,4)
cry#9
(0,2)
cry#10
(0,2)
cry#11
(0,2)
cry#12
(0,2)
winning#1
(0,3)
winning#3
Creating GD
Ppr+Freq
Selecting senses
245
than Run1 in the WK with a 78% in English and 
Bs with 60% in French. The best results are in the 
WK in French with a value of 85%. 
Since we can choose to include different 
resources into ISR-WN, it is important to analyze 
how doing so would affect the results. Table 4 
shows comparative results for Run 2 of a trial 
dataset with BabelNet version 1.1.1. 
As can be observed in Table 4, the result does not 
have a significant change even though we used the 
ISR-WN with all resources.  
A better analysis of Ppr+Freq in, as it relates to 
the influence of each resource involved in ISR-WN 
(similar to Table 4 description) assessing 
SensEval-2 and SensEval-3 dataset, is shown in 
(Guti?rrez, 2012). There are different resource 
combinations showing that only XWN1.7 and all 
ISR-WN resources obtain the highest performance. 
Other analysis found in (Guti?rrez, 2012) evaluates 
the influence of adding the sense frequency for 
Ppr+Freq.  
By excluding the Factotum Domain, we obtain 
the best result in Bs 54% for French (only 1% 
more than the version used in the competition). 
The other results are equal, with a 69% in WNS, 
66% in Bs, 64% in WK for English, and 69% in 
WK for French. 
        English French 
WN Domains Sumo Affect Factotum 
Domain 
SemanticClass XWN3.0 XWN1.7 WNS Bs WK Bs WK 
X X X X X X X X 0.69 0.66 0.64 0.53 0.69 
X X  X X X X X 0.69 0.66 0.64 0.53 0.69 
X    X X X X 0.68 0.65 0.64 0.52 0.69 
X X X X  X X X 0.69 0.66 0.64 0.54 0.69 
X X X X  X  X 0.68 0.65 0.65 0.53 0.69 
Table 4. Influence of different resources that integrate ISR-WN in our technique. 
    Wikipedia BabelNet WordNet 
System Language Precision Recall F-score Precision Recall F-score Precision Recall F-score 
MFS DE 0.836 0.827 0.831 0.676 0.673 0.686 - - - 
  EN 0.86 0.753 0.803 0.665 0.665 0.656 0.63 0.63 0.63 
  ES 0.83 0.819 0.824 0.645 0.645 0.644 - - - 
  FR 0.698 0.691 0.694 0.455 0.452 0.501 - - - 
  IT 0.833 0.813 0.823 0.576 0.574 0.572 - - - 
Run1 DE 0.758 0.46 0.572 0.619 0.617 0.618 - - - 
  EN 0.619 0.484 0.543 0.677 0.677 0.677 0.639 0.635 0.637 
  ES 0.773 0.493 0.602 0.708 0.703 0.705 - - - 
  FR 0.817 0.48 0.605 0.608 0.603 0.605 - - - 
  IT 0.785 0.458 0.578 0.659 0.656 0.657 - - - 
Run2 DE 0.769 0.467 0.581 0.622 0.62 0.621 - - - 
  EN 0.62 0.487 0.546 0.685 0.685 0.685 0.649 0.645 0.647 
  ES 0.778 0.502 0.61 0.713 0.708 0.71 - - - 
  FR 0.815 0.478 0.603 0.608 0.603 0.605 - - - 
  IT 0.787 0.463 0.583 0.659 0.657 0.658 - - - 
Run3 EN 0.622 0.489 0.548 0.68 0.68 0.68 0.642 0.639 0.64 
Table 5. Results of Runs for Task12 of semeval-2013 using the test dataset. 
 
246
3.1 Run1 
In this Run, WNSLs consist of all the target words 
involved in each sentence. This run is applied at 
the sentence level. The results for the competition 
are shown in Table 5. For this Run, the best result 
was obtained for Spanish with a 70.3% in Bs and 
49.3% in WK of Recall. As we can see, for Run1 
the precision is high for Wikipedia disambiguation, 
obtaining for French the best result of the ranking. The 
low Recall in Wikipedia is due to the exact mismatching 
of labels between our system output and the gold 
standard. This fact, affects the rest of our runs. 
3.2 Run2 
In this Run, WNSLs consist of all the target words 
involved in each domain. We can obtain the target 
words because the training and test dataset contain 
the sentences grouped by topics.  For instance, for 
English, 13 WNSLs are established. This Run is 
applied at the corpora level. The results for the 
competition are shown in Table 5. It is important to 
emphasize that our best results ranked our 
algorithm as first place among all proposed 
approaches for the MWSD task. 
For this run, the best Recall was obtained for 
Spanish with a 70.8% in Bs and 50.2% in WK. 
This Run also has the best result of the three runs. 
For the English competition, it ended up with a 
64.5% in WNS, 68.5% in Bs, and 48.7% in WK. 
This Run obtained promising results, which took 
first place in the competition. It also had better 
results than that of the First Sense (Most Frequent 
Sense) baseline in Bs results for all languages, 
except for German. In Bs, it only obtained lower 
results in German with a 62% of Recall for our 
system and 67.3% for the First Sense baseline. 
3.3 Run3 
In this run, WNSLs consist of all the words 
included in each sentence. This run uses target 
words and non-target words of each sentence, as 
they are applied to the sentence level. The results 
for the competition are shown in Table 5.  
As we can see, the behavior of this run is similar 
to the previous runs. 
4 Conclusions and Future work  
The above results suggest that our proposal is a 
promising approach. It is also important to notice 
that a richer knowledgebase can be built by 
combining different resources such as BabelNet 
and ISR-WN, which can lead to an improvement 
of the results. Notwithstanding, our system has 
been recognized as the best in the competition, 
obtaining results around 70% of Recall. 
According to the Task12 results4, only the 
baseline Most Frequent Sense (MFS) could 
improve our scores in order to achieve better WK 
and German (DE) disambiguation. Therefore, we 
plan to review this point to figure out why we 
obtained better results in other categories, but not 
for this one. At the same time, further work will 
use the internal Babel network to run the Ppr+Freq 
method in an attempt to find a way to enrich the 
semantic network obtained for each target sentence 
to disambiguate. On top of that, we plan to 
compare Ppr (Agirre and Soroa, 2009) with 
Ppr+Freq using the Task12 dataset. 
Availability of our Resource 
In case researchers would like to use our resource, 
it is available at the GPLSI5 home page or by 
contacting us via email. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 12th 
conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. 
                                                     
4 http://www.cs.york.ac.uk/semeval-
2013/task12/index.php?id=results 
5 http://gplsi.dlsi.ua.es/ 
247
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; A. 
Gonz?lez; R. Estrada; Y. Casta?eda; S. V?zquez; A. 
Montoyo and R. Mu?oz. UMCC_DLSI: 
Multidimensional Lexical-Semantic Textual 
Similarity. {*SEM 2012}: The First Joint Conference 
on Lexical and Computational Semantics -- Volume 
1: Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Guti?rrez, Y. An?lisis Sem?ntico Multidimensional 
aplicado a la Desambiguaci?n del Lenguaje Natural. 
Departamento de Lenguajes y Sistemas Inform?ticos. 
Alicante, Alicante, 2012. 189. p. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, RANLP 
2011 Organising Committee, 2011b. 233--239 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Sentiment 
Classification Using Semantic Features Extracted 
from WordNet-based Resources. Proceedings of the 
2nd Workshop on Computational Approaches to 
Subjectivity and Sentiment Analysis (WASSA 
2.011), Portland, Oregon., Association for 
Computational Linguistics, 2011c. 139--145 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Word Sense 
Disambiguation: A Graph-Based Approach Using N-
Cliques Partitioning Technique. en:  Natural 
Language Processing and Information Systems. 
MU?OZ, R.;MONTOYO, A.et al Springer Berlin / 
Heidelberg, 2011d. 6716: 112-124.p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. A graph-
Based Approach to WSD Using Relevant Semantic 
Trees and N-Cliques Model. CICLing 2012, New 
Delhi, India, 2012. 225-237 p.  
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Laparra, E.; G. Rigau and M. Cuadros. Exploring the 
integration of WordNet and FrameNet. Proceedings 
of the 5th Global WordNet Conference (GWC'10), 
Mumbai, India, 2010.  
Magnini, B. and G. Cavaglia. Integrating Subject Field 
Codes into WordNet. Proceedings of Third 
International Conference on Language Resources and 
Evaluation (LREC-2000), 2000. 1413--1418 p.  
Mihalcea, R. Unsupervised large-vocabulary word sense 
disambiguation with graph-based algorithms for 
sequence data labeling. Proceedings of HLT05, 
Morristown, NJ, USA., 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Navigli, R.; D. Jurgens and D. Vannella. SemEval-2013 
Task 12: Multilingual Word Sense Disambiguation. . 
Proceedings of the 7th International Workshop on 
Semantic Evaluation (SemEval 2013), in conjunction 
with the Second Joint Conference on Lexical and 
Computational Semantics (*SEM 2013), Atlanta, 
Georgia, 2013.  
Navigli, R. and S. P. Ponzetto. BabelNet: Building a 
Very Large Multilingual Semantic Network. 
Proceedings of the 48th Annual Meeting of the 
Association for Computational Linguistics, Uppsala, 
Sweden, Association for Computational Linguistics, 
2010. 216--225 p.  
Navigli, R. and S. P. Ponzetto BabelNet: The automatic 
construction, evaluation and application of a wide-
coverage multilingual semantic network Artif. Intell., 
2012, 193: 217-250. 
Navigli, R. and P. Velardi Structural Semantic 
Interconnections: A Knowledge-Based Approach to 
Word Sense Disambiguation IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 2005, 
27(7): 1075-1086. 
Sinha, R. and R. Mihalcea. Unsupervised Graph-based 
Word Sense Disambiguation Using Measures of 
Word Semantic Similarity. Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC 2007), Irvine, CA, 2007. 
248
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Tsatsaronis, G.; M. Vazirgiannis and I. 
Androutsopoulos. Word sense disambiguation with 
spreading activation networks generated from 
thesauri. IJCAI, 2007.  
Vossen, P. EuroWordNet: A Multilingual Database with 
Lexical Semantic Networks.  Dordrecht, Kluwer 
Academic Publishers, 1998.  
Zouaq, A.; M. Gagnon and B. Ozell. A SUMO-based 
Semantic Analysis for Knowledge Extraction. 
Proceedings of the 4th Language & Technology 
Conference, Pozna?, Poland, 2009.  
 
 
249
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 443?449, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(SA): Using a ranking algorithm and informal features 
to solve Sentiment Analysis in Twitter 
Yoan Guti?rrez, Andy Gonz?lez, 
Roger P?rez, Jos? I. Abreu 
University of Matanzas, Cuba 
{yoan.gutierrez, roger.perez 
,jose.abreu}@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, 
Alejandro Mosquera, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, 
{amosquera, montoyo, 
rafael}@dlsi.ua.es 
Franc Camara 
Independent Consultant 
USA 
info@franccamara
.com 
 
Abstract 
In this paper, we describe the development 
and performance of the supervised system 
UMCC_DLSI-(SA). This system uses corpora 
where phrases are annotated as Positive, 
Negative, Objective, and Neutral, to achieve 
new sentiment resources involving word 
dictionaries with their associated polarity. As 
a result, new sentiment inventories are 
obtained and applied in conjunction with 
detected informal patterns, to tackle the 
challenges posted in Task 2b of the Semeval-
2013 competition. Assessing the effectiveness 
of our application in sentiment classification, 
we obtained a 69% F-Measure for neutral and 
an average of 43% F-Measure for positive 
and negative using Tweets and SMS 
messages. 
1 Introduction 
Textual information has become one of the most 
important sources of data to extract useful and 
heterogeneous knowledge from. Texts can provide 
factual information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions, or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention.  
Many researchers, such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working on this and related areas. 
Related to assessment Sentiment Analysis (SA) 
systems, some international competitions have 
taken place. Some of those include: Semeval-2010 
(Task 18: Disambiguating Sentiment Ambiguous 
Adjectives 1 ) NTCIR (Multilingual Opinion 
Analysis Task (MOAT 2)) TASS 3  (Workshop on 
Sentiment Analysis at SEPLN workshop) and 
Semeval-2013 (Task 2 4  Sentiment Analysis in 
Twitter) (Kozareva et al, 2013). 
In this paper, we introduce a system for Task 2 
b) of the Semeval-2013 competition. 
1.1 Task 2 Description 
In participating in ?Task 2: Sentiment Analysis in 
Twitter? of Semeval-2013, the goal was to take a 
given message and its topic and classify whether it 
had a positive, negative, or neutral sentiment 
towards the topic. For messages conveying, both a 
positive and negative sentiment toward the topic, 
the stronger sentiment of the two would end up as 
the classification. Task 2 included two sub-tasks. 
Our team focused on Task 2 b), which provides 
two training corpora as described in Table 3, and 
two test corpora: 1) sms-test-input-B.tsv (with 
2094 SMS) and 2) twitter-test-input-B.tsv (with 
3813 Twit messages). 
The following section shows some background 
approaches. Subsequently, in section 3, we 
describe the UMCC_DLSI-(SA) system that was 
used in Task 2 b). Section 4 describes the 
assessment of the obtained resource from the 
Sentiment Classification task. Finally, the 
conclusion and future works are presented in 
section 5. 
2 Background 
The use of sentiment resources has proven to be a 
necessary step for training and evaluating  systems 
that implement sentiment analysis, which also 
                                                 
1 http://semeval2.fbk.eu/semeval2.php 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
3 http://www.daedalus.es/TASS/ 
4http://www.cs.york.ac.uk/semeval-2013/task2/ 
443
include fine-grained opinion mining (Balahur, 
2011). 
In order to build sentiment resources, several 
studies have been conducted. One of the first is the 
relevant work by (Hu and Liu, 2004) using lexicon 
expansion techniques by adding synonymy and 
antonym relations provided by WordNet 
(Fellbaum, 1998; Miller et al, 1990) Another one 
is the research described by (Hu and Liu, 2004; 
Liu et al, 2005) which obtained an Opinion 
Lexicon compounded by a list of positive and 
negative opinion words or sentiment words for 
English (around 6800 words). 
A similar approach has been used for building 
WordNet-Affect (Strapparava and Valitutti, 2004) 
which expands six basic categories of emotion; 
thus, increasing the lexicon paths in WordNet. 
Nowadays, many sentiment and opinion 
messages are provided by Social Media. To deal 
with the informalities presented in these sources, it 
is necessary to have intermediary systems that 
improve the level of understanding of the 
messages. The following section offers a 
description of this phenomenon and a tool to track 
it. 
2.1 Text normalization 
Several informal features are present in opinions 
extracted from Social Media texts. Some research 
has been conducted in the field of lexical 
normalization for this kind of text. TENOR 
(Mosquera and Moreda, 2012) is a multilingual 
text normalization tool for Web 2.0 texts with an 
aim to transform noisy and informal words into 
their canonical form. That way, they can be easily 
processed by NLP tools and applications. TENOR 
works by identifying out-of-vocabulary (OOV) 
words such as slang, informal lexical variants, 
expressive lengthening, or contractions using a 
dictionary lookup and replacing them by matching 
formal candidates in a word lattice using phonetic 
and lexical edit distances. 
2.2 Construction of our own Sentiment 
Resource  
Having analyzed the examples of SA described in 
section 2, we proposed building our own sentiment 
resource (Guti?rrez et al, 2013) by adding lexical 
and informal patterns to obtain classifiers that can 
deal with Task 2b of Semeval-2013. We proposed 
the use of a method named RA-SR (using Ranking 
Algorithms to build Sentiment Resources) 
(Guti?rrez et al, 2013) to build sentiment word 
inventories based on senti-semantic evidence 
obtained after exploring text with annotated 
sentiment polarity information. Through this 
process, a graph-based algorithm is used to obtain 
auto-balanced values that characterize sentiment 
polarities, a well-known technique in Sentiment 
Analysis. This method consists of three key stages: 
(I) Building contextual word graphs; (II) Applying 
a ranking algorithm; and (III) Adjusting the 
sentiment polarity values. 
These stages are shown in the diagram in Figure 1, 
which the development of sentimental resources 
starts off by giving four corpora of annotated 
sentences (the first with neutral sentences, the 
second with objective sentences, the third with 
positive sentences, and the last with negative 
sentences). 
 
 
Figure 1. Resource walkthrough development 
process. 
2.3 Building contextual word graphs 
Initially, text preprocessing is performed by 
applying a Post-Tagging tool (using Freeling 
(Atserias et al, 2006) tool version 2.2 in this case) 
to convert all words to lemmas 5 . After that, all 
obtained lists of lemmas are sent to RA-SR, then 
divided into four groups: neutral, objective, 
positive, and negative candidates. As the first set 
                                                 
5 Lemma denotes canonic form of the words. 
Phr se 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
W ight =1
W ight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
W1 W2 W3
W4W5
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W1 W2 W4
W1
W5
Phrase 1 Neutral 
Phrases
W1 W6 W7 W8
W8 W7 W3
W6 W8 W7 W5
W5
W5 W2
Objective 
Phrases
Phrase 3
Phrase 2
Phrase 1
(II) 
W1 W2 W3 W4 W5 W6 W7 W8
(II) 
W1 W2 W3
W5
W6 W7
W8
Default Weight = 1/N
(I)(I)
Default Weight = 1/N
444
of results, four contextual graphs are 
obtained:  ????,   ???? , ????,  and ???? , where 
each graph includes the words/lemmas from the 
neutral, objective, positive and negative sentences 
respectively. These graphs are generated after 
connecting all words for each sentence into 
individual sets of annotated sentences in 
concordance with their annotations (??? , ??? , 
???, ??? ). 
Once the four graphs representing neutral, 
objective, positive and negative contexts are 
created, we proceed to assign weights to apply 
graph-based ranking techniques in order to auto-
balance the particular importance of each vertex ?? 
into ????, ????, ???? and ????. 
As the primary output of the graph-based ranking 
process, the positive, negative, neutral, and 
objective values are calculated using the PageRank 
algorithm and normalized with equation (1). For a 
better understanding of how the contextual graph 
was built see (Guti?rrez et al, 2013). 
2.4 Applying a ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ????, ????, ???? 
and ???? take the default of 1/N as their weight 
to define the weight of ? vector, which is used in 
our proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 4) as positive or negative, in relation to 
their respective graph, a weight value of 1 (in a 
range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
After that, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking algorithm 
is able to increase the significance of the words 
related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre and 
Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main 
idea behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
? i to ? j. As a result, the relevance of ? j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the 
more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
? , where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??.  
In our system, we apply the following 
configuration: dumping factor ? = 0.85 and, like 
in (Agirre and Soroa, 2009) we used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009)  
After applying PageRank, in order to obtain 
standardized values for both graphs, we normalize 
the rank values by applying the equation (1), 
where ???(??) obtains the maximum rank value 
of ?? vector (rankings? vector). 
??? = ???/???(??) (1) 
2.5 Adjusting the sentiment polarity values 
After applying the PageRank algorithm on????, 
???? , ????  and ???? , having normalized their 
ranks, we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. 
?? is represented by ???  lemmas, which would 
have, at that time, four assigned values: Neutral, 
Objective, Positive, and Negative, all of which 
correspond to a calculated rank obtained by the 
PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (2) 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (3) 
Where ???  is the Positive value and ???  the 
Negative value related to each lemma in ??. 
In order to standardize again the ???  and ??? 
values and making them more representative in a 
[0?1] scale, we proceed to apply a normalization 
process over the ??? and ??? values. 
From there, based on the objective features 
commented by (Baccianella et al, 2010), we 
assume the same premise to establish an 
alternative objective value of the lemmas. 
Equation (4) is used for that: 
?????? = 1 ? |??? ? ???| (4) 
Where ??????  represents the alternative 
objective value. 
445
As a result, each word obtained in the sentiment 
resource has an associated value of: positivity 
(??? , see equation (2)), negativity (??? , see 
equation (3)), objectivity(????_???,  obtained by 
PageRank over ????  and normalized with 
equation (1)), calculated-objectivity (??????, now 
cited as ???_???????? ) and neutrality (??? , 
obtained by PageRank over ???? and normalized 
with equation (1)). 
3  System Description 
The system takes annotated corpora as input from 
which two models are created. One model is 
created by using only the data provided at 
Semeval-2013 (Restricted Corpora, see Table 3), 
and the other by using extra data from other 
annotated corpora (Unrestricted Corpora, see 
Table 3). In all cases, the phrases are pre-
processed using Freeling 2.2 pos-tagger (Atserias 
et al, 2006) while a dataset copy is normalized 
using TENOR (described in section 2.1). 
The system starts by extracting two sets of 
features. The Core Features (see section 3.1) are 
the Sentiment Measures and are calculated for a 
standard and normalized phrase. The Support 
Features (see section 3.2) are based on regularities, 
observed in the training dataset, such as 
emoticons, uppercase words, and so on. 
The supervised models are created using Weka6 
and a Logistic classifier, both of which the system 
uses to predict the values of the test dataset. The 
selection of the classifier was made after analyzing 
several classifiers such as: Support Vector 
Machine, J48 and REPTree. Finally, the Logistic 
classifier proved to be the best by increasing the 
results around three perceptual points. 
The test data is preprocessed in the same way 
the previous corpora were. The same process of 
feature extraction is also applied. With the 
aforementioned features and the generated models, 
the system proceeds to classify the final values of 
Positivity, Negativity, and Neutrality.  
3.1 The Core Features 
The Core Features is a group of measures based on 
the resource created early (see section 2.2). The 
system takes a sentence preprocessed by Freeling 
2.2 and TENOR. For each lemma of the analyzed 
sentence, ??? , ??? , ???_???????? ,  ????_???, 
                                                 
6 http://www.cs.waikato.ac.nz/ 
and ???  are calculated by using the respective 
word values assigned in RA-SR. The obtained 
values correspond to the sum of the corresponding 
values for each intersecting word between the 
analyzed sentence (lemmas list) and the obtained 
resource by RA-SR. Lastly, the aforementioned 
attributes are normalized by dividing them by the 
number of words involved in this process. 
Other calculated attributes are: ???_????? , 
???_????? , ???_????????_????? , 
???_????_????? and ???_?????. These attributes 
count each involved iteration for each feature type 
( ??? , ??? , ????_??? , ??????  and ??? 
respectively, where the respective value may be 
greater than zero. 
Attributes ???  and cnn are calculated by 
counting the amount of lemmas in the phrases 
contained in the Sentiment Lexicons (Positive and 
Negative respectively).  
All of the 12 attributes described previously are 
computed for both, the original, and the 
normalized (using TENOR) phrase, totaling 24 
attributes. The Core features are described next.  
Feature Name Description 
??? 
Sum of respective value of each word. 
??? 
???_???????? 
????_??? 
??? 
???_????? 
Counts the words where its respective value 
is greater than zero 
???_????? 
???_????????_????? 
????_???_????? 
???_????? 
??? (to positive) Counts the words contained in the 
Sentiment Lexicons for their respective 
polarities. 
??? (to negative) 
Table 1. Core Features 
3.2 The Support Features 
The Support Features is a group of measures based 
on characteristics of the phrases, which may help 
with the definition on extreme cases. The emotPos 
and emotNeg values are the amount of Positive 
and Negative Emoticons found in the phrase. The 
exc and itr are the amount of exclamation and 
interrogation signs in the phrase. The following 
table shows the attributes that represent the 
support features: 
Feature Name Description 
??????? 
Counts the respective Emoticons 
??????? 
??? (exclamation marks (?!?)) 
Counts the respective marks 
??? (question marks (???)) 
?????_????? Counts the uppercase words 
?????_??? Sums the respective values of the 
Uppercase words ?????_??? 
?????_???_?????_??? (to Counts the Uppercase words 
446
positivity) contained in their respective 
Graph ?????_???_?????_???(to 
negativity) 
?????_???_?????_???? (to 
positivity) 
Counts the Uppercase words 
contained in the Sentiment 
Lexicons 7 for their respective 
polarity  
?????_???_?????_???? (to 
negativity) 
???????_????? Counts the words with repeated 
chars  
???????_??? Sums the respective values of the 
words with repeated chars ???????_??? 
???????_???_?????_???? (in 
negative lexical resource ) 
Counts the words with repeated 
chars contained in the respective 
lexical resource ???????_???_?????_???? (in 
positive lexical resource ) 
???????_???_?????_??? (in 
positive graph ) 
Counts the words with repeated 
chars contained in the respective 
graph ???????_???_?????_???  (in 
negative graph ) 
Table 2. The Support Features 
4 Evaluation 
In the construction of the sentiment resource, we 
used the annotated sentences provided by the 
corpora described in Table 3. The resources listed 
in Table 3 were selected to test the functionality of 
the words annotation proposal with subjectivity 
and objectivity. Note that the shadowed rows 
correspond to constrained runs corpora: tweeti-b-
sub.dist_out.tsv 8  (dist), b1_tweeti-objorneu-
b.dist_out.tsv 9  (objorneu), twitter-dev-input-
B.tsv10 (dev). 
The resources from Table 3 that include 
unconstrained runs corpora are: all the previously 
mentioned ones, Computational-intelligence11 (CI) 
and stno12 corpora. 
The used sentiment lexicons are from the 
WordNetAffect_Categories13 and opinion-words14 
files as shown in detail in Table 4. 
Some issues were taken into account throughout 
this process. For instance, after obtaining a 
contextual graph ?, factotum words are present in 
most of the involved sentences (i.e., verb ?to be?). 
This issue becomes very dangerous after applying 
the PageRank algorithm because the algorithm 
                                                 
7 Resources described in Table 4. 
8Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
9Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
10 http://www.cs.york.ac.uk/semeval-2013/task2/ 
11A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
12NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
13 http://wndomains.fbk.eu/wnaffect.html 
14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
strengthens the nodes possessing many linked 
elements. For that reason, the subtractions ??? ?
??? and ??? ? ??? are applied, where the most 
frequent words in all contexts obtain high values. 
The subtraction becomes a dumping factor.  
As an example, when we take the verb ?to be?, 
before applying equation (1), the verb achieves the 
highest values in each subjective context graph 
(????  and ????)  namely, 9.94 and 18.67 rank 
values respectively. These values, once equation 
(1) is applied, are normalized obtaining both 
??? =  1 and ??? =  1 in a range [0...1]. At the 
end, when the following steps are executed 
(Equations (2) and (3)), the verb ?to be? 
achieves ??? = 0 , ??? = 0  and 
therefore  ?????? = 1 . Through this example, it 
seems as though we subjectively discarded words 
that appear frequently in both contexts (Positive 
and Negative). 
Corpus N P O Neu 
Obj 
or Neu 
Unk T 
C UC 
dist 176 368 110 34 - - 688 X X 
objorneu 828 1972 788 1114 1045 - 5747 X X 
dev 340 575 - 739 - - 1654 X X 
CI 6982 6172 - - - - 13154  X 
stno15 1286 660 - 384 - 10000 12330  X 
T 9272 9172 898 1532 1045 10000 31919   
Table 3. Corpora used to apply RA-SR. Positive (P), 
Negative (N), Objective (Obj/O), Unknow (Unk), Total 
(T), Constrained (C), Unconstrained (UC). 
Sources P N T 
WordNet-Affects_Categories 
 (Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words  
(Hu and Liu, 2004; Liu et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 4. Sentiment Lexicons. Positive (P), Negative 
(N) and Total (T). 
   Precision (%) Recall (%) Total (%) 
 C Inc P  N  Neu P N Neu Prec Rec F1 
Run1 8032 1631 80,7 83,8 89,9 90,9 69,5 86,4 84,8 82,3 82,9 
Run2 19101 4671 82,2 77,3 89,4 80,7 81,9 82,3 83,0 81,6 80,4 
Table 5. Training dataset evaluation using cross-
validation (Logistic classifier (using 10 folds)). 
Constrained (Run1), Unconstrained (Run2), Correct(C), 
Incorrect (Inc). 
4.1 The training evaluation 
In order to assess the effectiveness of our trained 
classifiers, we performed some evaluation tests.  
Table 5 shows relevant results obtained after 
applying our system to an environment (specific 
domain). The best results were obtained with the 
                                                 
15 NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
447
restricted corpus. The information used to increase 
the knowledge was not balanced or perhaps is of 
poor quality. 
4.2 The test evaluation 
The test dataset evaluation is shown in Table 6, 
where system results are compared with the best 
results in each case. We notice that the constrained 
run is better in almost every aspect. In the few 
cases where it was lower, there was a minimal 
difference. This suggests that the information used 
to increase our Sentiment Resource was 
unbalanced (high difference between quantity of 
tagged types of annotated phrases), or was of poor 
quality. By comparing these results with the ones 
obtained by our system on the test dataset, we 
notice that on the test dataset, the results fell in the 
middle of the effectiveness scores. After seeing 
these results (Table 5 and Table 6), we assumed 
that our system performance is better in a 
controlled environment (or specific domain). To 
make it more realistic, the system must be trained 
with a bigger and more balanced dataset. 
Table 6 shows the results obtained by our 
system while comparing them to the best results of 
Task 2b of Semeval-2013. In Table 5, we can see 
the difference between the best systems. They are 
the ones in bold and underlined as target results.  
These results have a difference of around 20 
percentage points. The grayed out ones correspond 
to our runs. 
      Precision (%) Recall (%) Total 
Runs C Inc P N Neu P N Neu Prec Rec F 1 
1_tw 2082 1731 60,9 46,5 52,8 49,8 41,4 64,1 53,4 51,8 49,3 
1_tw_cnd 2767 1046 81,4 69,7 67,7 66,7 60,4 82,6 72,9 69,9 69,0 
2_tw 2026 1787 58,0 42,2 42,2 52,2 43,9 57,4 47,4 51,2 49,0 
2_tw_ter 2565 1248 71,1 54,6 68,6 74,7 59,4 63,1 64,8 65,7 64,9 
1_sms 1232 862 43,9 46,1 69,5 55,9 31,7 68,9 53,2 52,2 43,4 
1_sms_cnd 1565 529 73,1 55,4 85,2 73,0 75,4 75,3 71,2 74,5 68,5 
2_sms 1023 1071 38,4 31,4 68,3 60,0 38,3 47,8 46,0 48,7 40,7 
2_sms_ava 1433 661 60,9 49,4 81,4 65,9 63,7 71,0 63,9 66,9 59,5 
Table 6. Test dataset evaluation using official scores. 
Corrects(C), Incorrect (Inc). 
Table 6 run descriptions are as follows:  
? UMCC_DLSI_(SA)-B-twitter-constrained 
(1_tw), 
? NRC-Canada-B-twitter-constrained 
(1_tw_cnd),  
? UMCC_DLSI_(SA)-B-twitter-unconstrained 
(2_tw), 
? teragram-B-twitter-unconstrained (2_tw_ter), 
? UMCC_DLSI_(SA)-B-SMS-constrained 
(1_sms), 
? NRC-Canada-B-SMS-constrained 
(1_sms_cnd), UMCC_DLSI_(SA)-B-SMS-
unconstrained (2_sms), 
? AVAYA-B-sms-unconstrained (2_sms_ava). 
As we can see in the training and testing 
evaluation tables, our training stage offered more 
relevant scores than the best scores in Task2b 
(Semaval-2013). This means that we need to 
identify the missed features between both datasets 
(training and testing). 
For that reason, we decided to check how many 
words our system (more concretely, our Sentiment 
Resource) missed. Table 7 shows that our system 
missed around 20% of the words present in the test 
dataset. 
 hits miss miss (%) 
twitter 23807 1591 6,26% 
sms 12416 2564 17,12% 
twitter nonrepeat   2426 863 26,24% 
sms norepeat 1269 322 20,24% 
Table 7. Quantity of words used by our system over 
the test dataset. 
5 Conclusion and further work 
Based on what we have presented, we can say that 
we could develop a system that would be able to 
solve the SA challenge with promising results. The 
presented system has demonstrated election 
performance on a specific domain (see Table 5) 
with results over 80%. Also, note that our system, 
through the SA process, automatically builds 
sentiment resources from annotated corpora.  
For future research, we plan to evaluate RA-SR 
on different corpora. On top of that, we also plan 
to deal with the number of neutral instances and 
finding more words to evaluate the obtained 
sentiment resource. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government through 
the project PROMETEO 
(PROMETEO/2009/199). 
448
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and Computing 
Systems. Alacant, Univeristy of Alacant, 2011. 299. 
p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. Martinez-
Barco. The OpAL System at NTCIR 8 MOAT. 
Proceedings of NTCIR-8 Workshop Meeting, 
Tokyo, Japan., 2010. 241-245 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer Networks 
and ISDN Systems, 1998, 30(1-7): 107-117. 
Fellbaum, C. WordNet. An Electronic Lexical 
Database.  University of Cambridge, 1998. p. The 
MIT Press.  
Guti?rrez, Y.; A. Gonz?lez; A. F. Orqu?n; A. Montoyo 
and R. Mu?oz. RA-SR: Using a ranking algorithm to 
automatically building resources for subjectivity 
analysis over annotated corpora. 4th Workshop on 
Computational Approaches to Subjectivity, 
Sentiment & Social Media Analysis (WASSA 2013), 
Atlanta, Georgia, 2013.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000.  
Hu, M. and B. Liu. Mining and Summarizing Customer 
Reviews. Proceedings of the ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004), USA, 2004.  
Kim, S.-M. and E. Hovy. Extracting Opinions, Opinion 
Holders, and Topics Expressed in Online News 
Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the association 
for computational linguistics (COLING/ACL 2006), 
Sydney, Australia, 2006. 1-8 p.  
Kozareva, Z.; P. Nakov; A. Ritter; S. Rosenthal; V. 
Stoyonov and T. Wilson. Sentiment Analysis in 
Twitter. in:  Proceedings of the 7th International 
Workshop on Semantic Evaluation. Association for 
Computation Linguistics, 2013. 
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Mosquera, A. and P. Moreda. TENOR: A Lexical 
Normalisation Tool for Spanish Web 2.0 Texts. in:  
Text, Speech and Dialogue - 15th International 
Conference (TSD 2012). Springer, 2012. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in Language. 
Kluwer Academic Publishers, Netherlands, 2005.  
 
  
 
449
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 636?643, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Semantic and Lexical features for detection and 
classification Drugs in biomedical texts 
 
 
Armando Collazo, Alberto 
Ceballo, Dennys D. Puig, Yoan 
Guti?rrez, Jos? I. Abreu, Roger 
P?rez 
Antonio Fern?ndez 
Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas 
Autopista a Varadero km 3 ? 
Matanzas, Cuba 
{armando.collazo, dennys.puig, 
yoan.gutierrez, jose.abreu, 
roger.perez}@umcc.cu, 
alberto.ceballo@infonet.umcc.cu 
DLSI, University of Alicante 
Carretera de San Vicente 
S/N Alicante, Spain 
antonybr@yahoo.com, 
{montoyo, 
rafael}@dlsi.ua.es 
Independent Consultant 
USA 
info@franccamara.com 
 
Abstract 
In this paper we describe UMCC_DLSI-
(DDI) system which attempts to detect and 
classify drug entities in biomedical texts. 
We discuss the use of semantic class and 
words relevant domain, extracted with ISR-
WN (Integration of Semantic Resources 
based on WordNet) resource to obtain our 
goal. Following this approach our system 
obtained an F-Measure of 27.5% in the 
DDIExtraction 2013 (SemEval 2013 task 
9). 
1. Introduction 
To understand biological processes, we must 
clarify how some substances interact with our 
body and one to each other. One of these 
important relations is the drug-drug interactions 
(DDIs). They occur when one drug interacts 
with another or when it affects the level, or 
activity of another drug. DDIs can change the 
way medications act in the body, they can cause 
powerful, dangerous and unexpected side 
effects, and also they can make the medications 
less effective. 
As suggested by (Segura-Bedmar et al, 2011), 
?...the detection of DDI is an important research 
area in patient safety since these interactions 
can become very dangerous and increase health 
care costs?. More recent studies (Percha and 
Altman, 2013) reports that ??Recent estimates 
indicate that DDIs cause nearly 74000 
emergency room visits and 195000 
hospitalizations each year in the USA?.  
But, on the other hand, there is an expansion in 
the volume of published biomedical research, 
and therefore the underlying biomedical 
knowledge base (Cohen and Hersh, 2005). 
Unfortunately, as often happens, this 
information is unstructured or in the best case 
scenario semi-structured. 
As we can see in (Tari et al, 2010), ?Clinical 
support tools often provide comprehensive lists 
of DDIs, but they usually lack the supporting 
scientific evidences and different tools can 
return inconsistent results?.  
Although, as mentioned (Segura-Bedmar et al, 
2011) ?there are different databases supporting 
healthcare professionals in the detection of DDI, 
these databases are rarely complete, since their 
update periods can reach up to three years?. In 
addition to these and other difficulties, the great 
amount of drug interactions are frequently 
reported in journals of clinical pharmacology 
and technical reports, due to this fact, medical 
literature becomes most effective source for 
detection of DDI. Thereby, the management of 
DDI is a critical issue due to the overwhelming 
amount of information available on them 
(Segura-Bedmar et al, 2011). 
636
1.1. Task Description 
With the aim of reducing the time the health care 
professionals invest on reviewing the literature, 
we present a feature-based system for drug 
detection and classification in biomedical texts. 
The DDIExtraction2013 task was divided into 
two subtasks: Recognition and classification of 
drug names (Task 9.1) and Extraction of drug-
drug interactions (Task 9.2). Our system was 
developed to be presented in the Task 9.1. In this 
case, participants were to detect and classify the 
drugs that were present in the test data set which 
was a set of sentences related to the biomedical 
domain obtained from a segmented corpus. The 
output consisted of a list mentioning all the 
detected drugs with information concerning the 
sentence it was detected from as well as its 
offset in that sentence (the position of the first 
and the last character of the drug in the sentence, 
0 being the first character of a sentence). Also 
the type of the drug should have been provided. 
As to the type, participants had to classify 
entities in one of these four groups1: 
? Drug: any chemical agent used for 
treatment, cure, prevention or diagnose of 
diseases, which have been approved for 
human usage. 
? Brand: any drug which firstly have been 
developed by a pharmaceutical company. 
? Group: any term in the text designating a 
relation among pharmaceutical substances. 
? No-Human: any chemical agent which 
affects the human organism. An active 
substance non-approved for human usage 
as medication. 
In the next section of the paper, we present 
related works (Section 2). In Section 3, we 
discuss the feature-based system we propose. 
Evaluation results are discussed in Section 4. 
Finally, we conclude and propose future work 
(Section 5). 
2. Related Work 
One of the most important workshops on the 
domain of Bioinformatics has been BioCreAtIve 
(Critical Assessment of Information Extraction 
                                                     
1 http://www.cs.york.ac.uk/semeval-2013/task9 
in Biology) (Hirschman et al, 2005). This 
workshop has improved greatly the Information 
Extraction techniques applied to the biological 
domain. The goal of the first BioCreAtIvE 
challenge was to provide a set of common 
evaluation tasks to assess the state-of-the-art for 
text mining applied to biological problems. The 
workshop was held in Granada, Spain on March 
28-31, 2004. 
According to Hirschman, the first 
BioCreAtIvE assessment achieved a high level 
of international participation (27 groups from 10 
countries). The best system results for a basic 
task (gene name finding and normalization), 
where a balanced 80% precision/recall or better, 
which potentially makes them suitable for real 
applications in biology. The results for the 
advanced task (functional annotation from free 
text) were significantly lower, demonstrating the 
current limitations of text-mining approaches. 
The greatest contribution of BioCreAtIve was 
the creation and release of training and test data 
sets for both tasks (Hirschman et al, 2005). 
One of the seminal works where the issue of 
drug detection was mentioned was (Gr?nroos et 
al., 1995). Authors argue the problem can be 
solved by using a computerized information 
system, which includes medication data of 
individual patients as well as information about 
non-therapeutic drug-effects. Also, they suggest 
a computerized information system to build 
decision support modules that, automatically 
give alarms or alerts of important drug effects 
other than therapeutic effects. If these warnings 
concern laboratory tests, they would be checked 
by a laboratory physician and only those with 
clinical significance would be sent to clinicians. 
Here, it is important to note the appearance of 
the knowledgebase DrugBank 2 . Since its first 
release in 2006 (Wishart et al, 2008) it has been 
widely used to facilitate in silico drug target 
discovery, drug design, drug docking or 
screening, drug metabolism prediction, drug 
interaction prediction and general 
pharmaceutical education. DrugBank has also 
significantly improved the power and simplicity 
of its structure query and text query searches. 
                                                     
2 http://redpoll.pharmacy.ualberta.ca/drugbank/ 
637
Later on, in 2010 Tari propose an approach 
that integrates text mining and automated 
reasoning to derive DDIs (Tari et al, 2010). 
Through the extraction of various facts of drug 
metabolism, they extract, not only the explicitly 
DDIs mentioned in text, but also the potential 
interactions that can be inferred by reasoning. 
This approach was able to find several potential 
DDIs that are not present in DrugBank. This 
analysis revealed that 81.3% of these 
interactions are determined to be correct. 
On the DDIExtraction 2011 (Segura-Bedmar et 
al., 2011) workshop (First Challenge Task on 
Drug-Drug Interaction Extraction) the best 
performance was achieved by the team WBI 
from Humboldt-Universitat, Berlin. This team 
combined several kernels and a case-based 
reasoning (CBR) system, using a voting 
approach. 
In this workshop relation extraction was 
frequently and successfully addressed by 
machine learning methods. Some of the more 
common used features were co-occurrences, 
character n-grams, Maximal Frequent 
Sequences, bag-of-words, keywords, etc. 
Another used technique is distant supervision. 
The first system evaluating distant supervision 
for drug-drug interaction was presented in 
(Bobi? et al, 2012), they have proposed a 
constraint to increase the quality of data used for 
training based on the assumption that no self-
interaction of real-world objects are described in 
sentences. In addition, they merge information 
from IntAct and the University of Kansas 
Proteomics Service (KUPS) database in order to 
detect frequent exceptions from the distant 
supervision assumption and make use of more 
data sources. 
Another important work related to Biomedical 
Natural Language Processing was BioNLP 
(Bj?rne et al, 2011) it is an application of 
natural language processing methods to analyze 
textual data on biology and medicine, often 
research articles. They argue that information 
extraction techniques can be used to mine large 
text datasets for relevant information, such as 
relations between specific types of entities. 
Inspired in the previews works the system we 
propose makes use of machine learning methods 
too, using some of the common features 
described above, such as the n-grams and 
keywords and co-occurrences, but we also add 
some semantic information to enrich those 
features. 
3. System Description  
As it has been mentioned before, the system was 
developed to detect and classify drugs in 
biomedical texts, so the process is performed in 
two main phases:  
? drug detection. 
? drug classification. 
Both phases are determined by the following 
stages, described in Figure 1: 
I. Preprocessing 
II. Feature extraction 
III. Classification 
 
 
 
 
 
 
 
Figure 1. Walkthrough system process. 
Given a biomedical sentence, the system 
obtains the lemmas and POS-tag of every token 
Classification 
 
Pre-Processing (using Freeling 2.2) 
Training set from Semeval 2013 
DDIExtraction2013 task 
Run(3) 
MFSC?s 
CSC MF 2-grams, 3-grams 
UC 
UCA 
MWord 
N 
 
   
I 
II 
III 
Tokenizing 
Run(1) Run(2) 
CSC - All 
EMFS
C 
DRD 
CD 
GC 
InRe 
WNum 
Feature Extraction 
Lemmatizing 
 
POS tagging 
 
 
   
 
   
638
of the sentence, by means of Freeling tool 3 . 
After that, it is able to generate candidates 
according to certain parameters (see section 3.3). 
Then, all the generated candidates are 
processed to extract the features needed for the 
learning methods, in order to determine which 
candidates are drugs. 
After the drugs are detected, the system 
generates a tagged corpus, following the 
provided training corpus structure, containing 
the detected entities, and then it proceeds to 
classify each one of them. To do so, another 
supervised learning algorithm was used (see 
section 3.3). 
3.1. Candidates generation 
Drugs and drug groups, as every entity in 
Natural Language, follow certain grammatical 
patterns. For instance, a drug is usually a noun 
or a set of nouns, or even a combination of verbs 
and nouns, especially verbs in the past participle 
tense and gerunds. But, one thing we noticed is 
that both drugs and drug groups end with a noun 
and as to drug groups that noun is often in the 
plural. 
Based on that idea, we decided to generate 
candidates starting from the end of each 
sentence and going forward. 
Generation starts with the search of a pivot 
word, which in this case is a noun. When the 
pivot is found, it is added to the candidates list, 
and then the algorithm takes the word before the 
pivot to see if it complies with one of the 
patterns i.e. if the word is a noun, an adjective, a 
gerund or past participle verb. If it does, then it 
and the pivot form another candidate.  
After that, the algorithm continues until it finds 
a word that does not comply with a pattern. In 
this case, it goes to the next pivot and stops 
when all the nouns in the sentence have been 
processed, or the first word of the sentence is 
reached. 
3.2. Feature Description 
For the DDIExtraction20134 task 9 three runs of 
the same system were performed with different 
                                                     
3 http://nlp.lsi.upc.edu/freeling/ 
features each time. The next sections describes 
the features we used. 
3.2.1. Most Frequent Semantic Classes 
(MFSC) 
Given a word, its semantic class label (Izquierdo 
et al, 2007) is obtained from WordNet using the 
ISR-WN resource (Guti?rrez et al, 2011; 2010). 
The semantic class is that associated to the most 
probable sense of the word. For each entity in 
the training set we take the words in the same 
sentence and for each word its semantic class is 
determined. This way, we identify the 4005 most 
frequent semantic classes associated to words 
surrounding the entities in the training set.  
For a candidate entity we use 400 features to 
encode information with regard to whether or 
not in its same sentence a word can be found 
belonging to one of the most frequent semantic 
classes. 
Each one of these features takes a value 
representing the distance (measured in words) a 
candidate is from the nearest word with same 
semantic class which represents the attribute.  
If the word is to the left of the candidate, the 
attribute takes a negative value, if it is to the 
right, the value is positive, and zero if no word 
with that semantic class is present in the 
sentence the candidate belongs to. 
To better understand that, consider A1 is the 
attribute which indicates if in the sentence of the 
candidate a word can be found belonging to the 
semantic class 1. Thus, the value of A1 is the 
distance the candidate is from the closest word 
with semantic class 1 in the sentence that is 
being analyzed. 
3.2.2. Candidate Semantic Class (CSC) 
The semantic class of candidates is also included 
in the feature set, if the candidate is a multi-
word, then the semantic class of the last word 
(the pivot word) is taken.  
 
                                                                               
4 http://www.cs.york.ac.uk/semeval-2013/task9/ 
5 This value was extracted from our previous experiment. 
639
3.2.3. Most Frequent Semantic Classes 
from Entities (EMFSC) 
In order to add more semantic information, we 
decided to find the most frequent semantic 
classes among all the entities that were tagged in 
the training data set. We included, in the feature 
set, all the semantic classes with a frequency of 
eight or more, because all the classes we wanted 
to identify were represented in that threshold. In 
total, they make 29 more features. The values of 
every one of them, is the sum of the number of 
times it appears in the candidate.  
3.2.4. Candidate Semantic Class All 
Words (CSC-All) 
This feature is similar to CSC, but in this case 
the candidate is a multi-word, we not only look 
for the semantic class of the pivot, but also the 
whole candidate as one. 
3.2.5. Drug-related domains (DRD) 
Another group of eight attributes describes how 
many times each one of the candidates belongs 
to one of the following drug-related domains 
(DRD) (medicine, anatomy, biology, chemistry, 
physiology, pharmacy, biochemistry, genetics).  
These domains where extracted from WordNet 
Domains. In order to determine the domain that 
a word belongs to, the proposal of DRelevant 
(V?zquez et al, 2007; V?zquez et al, 2004) was 
used. 
To illustrate how the DRD features take their 
values, consider the following sentence: 
??until the lipid response to Accutane is 
established.? 
One of the candidates the system generates 
would be ?lipid response?. It is a two-word 
candidate, so we take the first word and see if it 
belongs to one of the above domains. If it does, 
then we add one to that feature. If the word does 
not belong to any of the domains, then its value 
will be zero. We do the same with the other 
word. In the end, we have a collection where 
every value corresponds to each one of the 
domains. For the example in question the 
collection would be:  
 
 
medicine 1 
anatomy 0 
biology 0 
chemistry 0 
physiology 1 
pharmacy 0 
biochemistry 0 
genetics 0 
Table 1. DRD value assignment example. 
3.2.6. Candidate word number (WNum) 
Because there are candidates that are a multi-
word and others that are not, it may be the case 
that a candidate, which is a multi-word, has an 
EMFSC bigger than others which are not a 
multi-word, just because more than one of the 
words that conform it, have a frequent semantic 
class.  
We decided to add a feature, called WNum, 
which would help us normalize the values of the 
EMFSC. The value of the feature would be the 
number of words the candidate has. Same thing 
happens with DRD. 
3.2.7. Candidate Domain (CD) 
The value of this nominal feature is the domain 
associated to the candidate. If the candidate is a 
multi-word; we get the domain of all the words 
as a whole. In both cases the domain for a single 
word as well as for a multi-word is determined 
using the relevant domains obtained by 
(V?zquez et al, 2007; V?zquez et al, 2004). 
3.2.8. Maximum Frequent 2-grams, 3-
grams 
Drugs usually contain sequences of characters 
that are very frequent in biomedical domain 
texts. These character sequences are called n-
grams, where n is the number of characters in 
the sequence. Because of that, we decided to add 
the ten most frequent n-grams with n between 
two and three. The selected n-grams are the 
following: ?in? (frequency: 8170), ?ne? (4789), 
?ine? (3485), ?ti? (3234), ?id? (2768), ?an? 
(2704), ?ro? (2688), ?nt? (2593), ?et? (2423), 
?en? (2414). 
These features take a value of one if the 
candidate has the corresponding character 
sequence and zero if it does not. For instance: if 
640
we had the candidate ?panobinostat? it will 
generate the following collection:  
?in? 1 
?ne? 0 
?ine? 0 
?ti? 0 
?id? 0 
?an? 1 
?ro? 0 
?nt? 0 
?et? 0 
?en? 0 
Table 2. MF 2-gram, 3-gram. 
3.2.9. Uppercase (UC), Uppercase All 
(UCA). Multi-word (MWord) and 
Number (N) 
Other features say if the first letter of the 
candidate is an uppercase; if all of the letters are 
uppercase (UCA); if it is a multi-word (MWord) 
and also if it is in the singular or in the plural 
(N).  
3.2.10. L1, L2, L3 and R1, R2, R3 
The Part-of-Speech tags of the closest three 
surrounding words of the candidates are also 
included. We named those features L1, L2, and 
L3 for POS tags to the left of the candidate, and 
R1, R2, and R3 for those to the right. 
3.2.11. POS-tagging combination (GC) 
Different values are assigned to candidates, in 
order to identify its POS-tagging combination. 
For instance: to the following entity ?combined 
oral contraceptives? taken from DDI13-train-
TEES-analyses-130304.xml6 training file, which 
was provided for task 9.1, corresponds 5120. 
This number is the result of combining the four 
grammatical categories that really matter to us: 
R for adverb, V for verb, J for adjective, N for 
noun.  
A unique number was given to each 
combination of those four letters. We named this 
feature  GC. 
 
                                                     
6 http://www.cs.york.ac.uk/semeval-2013/task9 
3.2.12. In resource feature (InRe) 
A resource was created which contains all the 
drug entities that were annotated in the training 
corpus, so another attribute tells the system if the 
candidate is in the resource.  
Since all of the entities in the training data set 
were in the resource this attribute could take a 
value of one for all instances. Thus the classifier 
could classify correctly all instances in the 
training data set just looking to this attribute, 
which is not desirable. To avoid that problem, 
we randomly set its value to zero every 9/10 of 
the training instances. 
3.3. Classification 
All the features extracted in the previous stages 
are used in this stage to obtain the two models, 
one for drug detection phase, and the other for 
drug classification phase.  
We accomplished an extensive set of 
experiments in order to select the best classifier. 
All algorithms implemented in WEKA, except 
those that were designed specifically for a 
regression task, were tried. In each case we 
perform a 10-fold cross-validation. In all 
experiments the classifiers were settled with the 
default configuration. From those tests we select 
a decision tree, the C4.5 algorithm (Guti?rrez et 
al., 2011; 2010) implemented as the J48 
classifier in WEKA.  This classifier yields the 
better results for both drug detection and drug 
classification. 
The classifier was trained using a set of 463 
features, extracted from the corpus provided by 
SemEval 2013, the task 9 in question. 
As it was mentioned before, three runs were 
performed for the competition. Run (1) used the 
following features for drug detection: MFSC 
(only 200 frequent semantic classes), MF 2-
grams, 3-grams, UC, UCA, MWord, N, L1, L2, 
L3, R1, R2, R3, CSC, CD, WNum, GC and 
InRe.  
Drug classification in this run used the same 
features except for CD, WNum, and GC. Run 
(2) has all the above features, but we added the 
remaining 200 sematic classes that we left out in 
Run (1) to the detection and the classification 
models. In Run (3), we added EMFSC feature to 
the detection and the classification models. 
641
4. Results 
In the task, the results of the participants were 
compared to a gold-standard and evaluated 
according to various evaluation criteria:  
? Exact evaluation, which demands not only 
boundary match, but also the type of the 
detected drug has to be the same as that of 
the gold-standard. 
? Exact boundary matching (regardless of 
the type). 
? Partial boundary matching (regardless of 
the type) 
? Type matching. 
Precision and recall were calculated using the 
scoring categories proposed by MUC 7: 
? COR: the output of the system and the 
gold-standard annotation agree. 
? INC: the output of the system and the 
gold-standard annotation disagree. 
? PAR: the output of the system and the 
gold-standard annotation are not identical 
but has some overlapping text. 
? MIS: the number of gold-standard entities 
that were not identify by the system. 
? SPU: the number of entities labeled by the 
system that are not in the gold-standard. 
Table 3 , Table 4 and Table 5 show the system 
results in the DDIExtraction2013 competition 
for Run (1).  
Run (2) and Run (3) results are almost the 
same as Run (1). It is an interesting result since 
in those runs 200 additional features were 
supplied to the classifier.  In feature evaluation, 
using CfsSubsetEval and GeneticSearch with 
WEKA we found that all these new features 
were ranked as worthless for the classification. 
On the other hand, the following features were 
the ones that really influenced the classifiers: 
MFSC (215 features only), MF 2-grams, 3-
grams (?ne?, ?ine?, ?ti?, ?ro?, ?et?, ?en?), 
WNum, UC, UCA, L1, R1, CSC, CSC-All, CD, 
DRD (anatomy, physiology, pharmacy, 
biochemistry), InRe, GC and EMFS, specifically 
music.n.01, substance.n.01, herb.n.01, 
artifact.n.01, nutriment.n.01, nonsteroidal_anti-
inflammatory.n.01, causal_agent.n.01 have a 
                                                     
7http://www.itl.nist.gov/iaui/894.02/related_projects/muc/m
uc_sw/muc_sw_manual.html 
frequency of 8, 19, 35, 575, 52, 80, 63 
respectively.  
Measure Strict 
Exact 
Matching 
Partial 
Matching 
Type 
COR 319 354 354 388 
INC 180 145 0 111 
PAR 0 0 145 0 
MIS 187 187 187 187 
SPU 1137 1137 1137 1137 
Precision 0.19 0.22 0.22 0.24 
Recall 0.47 0.52 0.62 0.57 
Table 3. Run (1), all scores. 
Measure Drug Brand Group Drug_n 
COR 197 20 93 9 
INC 23 2 43 1 
PAR 0 0 0 0 
MIS 131 37 19 111 
SPU 754 47 433 14 
Precision 0.2 0.29 016 0.38 
Recall 0.56 0.34 0.6 0.07 
F1 0.3 0.31 0.26 0.12 
Table 4. Scores for entity types, exact matching in 
Run (1). 
 Precision Recall F1 
Macro average 0.26 0.39 0.31 
Strict matching 0.19 0.46 0.27 
Table 5. Macro average and Strict matching measures 
in Run (1). 
5. Conclusion and future works 
In this paper we show the description of 
UMCC_DLSI-(DDI) system, which is able to 
detect and classify drugs in biomedical texts 
with acceptable efficacy. It introduces in this 
thematic the use of semantic information such as 
semantic classes and the relevant domain of the 
words, extracted with ISR-WN resource. With 
this approach we obtained an F-Measure of 
27.5% in the Semeval DDI Extraction2013 task 
9.  
As further work we propose to eliminate some 
detected bugs (i.e. repeated instances, 
multiwords missed) and enrich our knowledge 
base (ISR-WN), using biomedical sources as 
UMLS8, SNOMED9 and OntoFis10. 
                                                     
8 http://www.nlm.nih.gov/research/umls 
9 http://www.ihtsdo.org/snomed-ct/ 
10 http://rua.ua.es/dspace/handle/10045/14216 
642
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
References 
Bj?rne, J.; A. Airola; T. Pahikkala and T. Salakoski 
Drug-Drug Interaction Extraction from 
Biomedical Texts with SVM and RLS Classifiers 
Proceedings of the 1st Challenge Task on Drug-
Drug Interaction Extraction, 2011, 761: 35-42. 
Bobi?, T.; R. Klinger; P. Thomas and M. Hofmann-
Apitius Improving Distantly Supervised Extraction 
of Drug-Drug and Protein-Protein Interactions 
EACL 2012, 2012: 35. 
Cohen, A. M. and W. R. Hersh A survey of current 
work in biomedical text mining Briefings in 
bioinformatics, 2005, 6(1): 57-71. 
Gr?nroos, P.; K. Irjala; J. Heiskanen; K. Torniainen 
and J. Forsstr?m Using computerized individual 
medication data to detect drug effects on clinical 
laboratory tests Scandinavian Journal of Clinical 
& Laboratory Investigation, 1995, 55(S222): 31-
36. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based 
on WordNet. XXVI Congreso de la Sociedad 
Espa?ola para el Procesamiento del Lenguaje 
Natural, Universidad Polit?cnica de Valencia, 
Valencia, SEPLN 2010, 2010. 161-168 p. 1135-
5948 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011, 47: 249-257. 
Hirschman, L.; A. Yeh; C. Blaschke and A. Valencia 
Overview of BioCreAtIvE: critical assessment of 
information extraction for biology BMC 
bioinformatics, 2005, 6(Suppl 1): S1. 
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Percha, B. and R. B. Altman Informatics confronts 
drug?drug interactions Trends in pharmacological 
sciences, 2013. 
Segura-Bedmar, I.; P. Mart?nez and D. S?nchez-
Cisneros The 1st DDIExtraction-2011 challenge 
task: Extraction of Drug-Drug Interactions from 
biomedical texts Challenge Task on Drug-Drug 
Interaction Extraction, 2011, 2011: 1-9. 
Tari, L.; S. Anwar; S. Liang; J. Cai and C. Baral 
Discovering drug?drug interactions: a text-mining 
and reasoning approach based on properties of 
drug metabolism Bioinformatics, 2010, 26(18): 
i547-i553. 
V?zquez, S.; A. Montoyo and Z. Kozareva. 
Extending Relevant Domains for Word Sense 
Disambiguation. IC-AI?07. Proceedings of the 
International Conference on Artificial Intelligence 
USA, 2007.  
V?zquez, S.; A. Montoyo and G. Rigau. Using 
Relevant Domains Resource for Word Sense 
Disambiguation. IC-AI?04. Proceedings of the 
International Conference on Artificial Intelligence, 
Ed: CSREA Press. Las Vegas, E.E.U.U., 2004. 
Wishart, D. S.; C. Knox; A. C. Guo; D. Cheng; S. 
Shrivastava; D. Tzur; B. Gautam and M. Hassanali 
DrugBank: a knowledgebase for drugs, drug 
actions and drug targets Nucleic acids research, 
2008, 36(suppl 1): D901-D906. 
643
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 94?99,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
 
RA-SR: Using a ranking algorithm to automatically building resources 
for subjectivity analysis over annotated corpora 
Yoan Guti?rrez, Andy Gonz?lez 
University of Matanzas, Cuba 
yoan.gutierrez@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, {montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
In this paper we propose a method that 
uses corpora where phrases are annotated 
as Positive, Negative, Objective and 
Neutral, to achieve new sentiment 
resources involving words dictionaries 
with their associated polarity. Our 
method was created to build sentiment 
words inventories based on senti-
semantic evidences obtained after 
exploring text with annotated sentiment 
polarity information. Through this 
process a graph-based algorithm is used 
to obtain auto-balanced values that 
characterize sentiment polarities well 
used on Sentiment Analysis tasks. To 
assessment effectiveness of the obtained 
resource, sentiment classification was 
made, achieving objective instances over 
80%. 
1 Introduction 
In recent years, textual information has become 
one of the most important sources of knowledge 
to extract useful data. Texts can provide factual 
information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention. Among most widely used terms 
in Natural Language Processing, in concrete in 
Sentiment Analysis (SA) and Opinion Mining, is 
the subjectivity term proposed by (Wiebe, 1994). 
This author defines it as ?linguistic expression of 
somebody?s opinions, sentiments, emotions, 
evaluations, beliefs and speculations?. Another 
important aspect opposed to subjectivity is the 
objectivity, which constitute a fact expression 
(Balahur, 2011). Other interesting terms also 
proposed by (Wiebe et al, 2005) considers, 
private state, theses terms involve opinions, 
beliefs, thoughts, feelings, emotions, goals, 
evaluations and judgments.  
Many researchers such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working in this way and related areas. To 
build systems able to lead SA challenges it is 
necessary to achieve sentiment resources 
previously developed. These resources could be 
annotated corpora, affective semantic structures, 
and sentiment dictionaries.  
In this paper we propose a method that uses 
annotated corpora where phrases are annotated as 
Positive, Negative, Objective and Neutral, to 
achieve new resources for subjectivity analysis 
involving words dictionaries with their 
associated polarity.  
The next section shows different sentiment and 
affective resources and their main characteristics. 
After that, our proposal is developed in section 3. 
Section 4, present a new sentiment resource 
obtained after evaluating RA-SR over many 
corpora. Section 5 described the evaluation and 
analysis of the obtained resource, and also an 
assessment of the obtained resource in Sentiment 
Classification task. Finally, conclusion and 
further works are presented in section 6. 
2 Related work 
It is known that the use of sentiment resources 
has proven to be a necessary step for training and 
evaluation for systems implementing sentiment 
analysis, including also fine-grained opinion 
mining (Balahur, 2011). 
Different techniques have been used into 
product reviews to obtain lexicons of subjective 
words with their associated polarity. We can 
study the relevant research promoted by (Hu and 
Liu, 2004) which start with a set of seed 
adjectives (?good? and ?bad?) and reinforce the 
semantic knowledge applying a expanding the 
lexicon with synonymy and antonymy relations 
provided by WordNet (Miller et al, 1990). As 
result of Hu and Liu researches an Opinion 
Lexicon is obtained with around 6800 positive 
94
 and negative English words (Hu and Liu, 2004; 
Liu et al, 2005). 
A similar approach has been used in building 
WordNet-Affect (Strapparava and Valitutti, 
2004). In this case the building method starting 
from a larger of seed affective words set. These 
words are classified according to the six basic 
categories of emotion (joy, sadness, fear, 
surprise, anger and disgust), are also expanded 
increase the lexicon using paths in WordNet. 
Other widely used in SA has been 
SentiWordNet resource (Esuli and Sebastiani, 
2006)). The main idea that encouraged its 
construction has been that ?terms with similar 
glosses in WordNet tend to have similar 
polarity?. 
Another popular lexicon is MicroWNOp 
(Cerini et al, 2007). It contains opinion words 
with their associated polarity. It has been built on 
the basis of a set of terms extracted from the 
General Inquirer1 (Stone et al, 1996).  
The problem is that these resources do not 
consider the context in which the words appear. 
Some methods tried to overcome this critique 
and built sentiment lexicons using the local 
context of words. 
We can mentioned to (Pang et al, 2002) whom 
built a lexicon with associated polarity value, 
starting with a set of classified seed adjectives 
and using conjunctions (?and?) disjunctions 
(?or?, ?but?) to deduce orientation of new words 
in a corpus. 
(Turney, 2002) classifies words according to 
their polarity based on the idea that terms with 
similar orientation tend to co-occur in 
documents.  
On the contrary in (Balahur and Montoyo, 
2008b), is computed the polarity of new words 
using ?polarity anchors? (words whose polarity 
is known beforehand) and Normalized Google 
Distance (Cilibrasi and Vit?nyi, 2007) scores 
using as training examples opinion words 
extracted from ?pros and cons reviews? from the 
same domain. This research achieved the lexical 
resource Emotion Triggers (Balahur and 
Montoyo, 2008a). 
Another approach that uses the polarity of the 
local context for computing word polarity is the 
one presented by (Popescu and Etzioni, 2005), 
who use a weighting function of the words 
around the context to be classified. 
All described resources have been obtained 
manually or semi-automatically. Therefore, we 
                                                 
1
 http://www.wjh.harvard.edu/~inquirer/ 
focus our target in archiving automatically new 
sentiment resources supported over some of 
aforementioned resources. In particular, we will 
offer contributions related with methods to build 
sentiment lexicons using the local context of 
words. 
3 Our method 
We propose a method named RA-SR (using 
Ranking Algorithms to build Sentiment 
Resources) to build sentiment words inventories 
based on senti-semantic evidences obtained after 
exploring text with annotated sentiment polarity 
information. Through this process a graph-based 
algorithm is used to obtain auto-balanced values 
that characterize sentiment polarities widely used 
on Sentiment Analysis tasks. This method 
consists of three main stages: (I) Building 
contextual words graphs; (II) Applying ranking 
algorithm; and (III) Adjusting sentiment polarity 
values. 
 
Figure 1. Resource walkthrough development process. 
These stages are represented in the diagram of 
Figure 1, where the development process begins 
introducing two corpuses of annotated sentences 
with positive and negative sentences 
respectively. Initially, a preprocessing of the text 
is made applying Freeling pos-tagger (Atserias et 
al., 2006) version 2.2 to convert all words to 
lemmas2. After that, all lemmas lists obtained are 
introduced in RA-SR, divided in two groups (i.e. 
positive and negative candidates, ????  and ????).  
3.1 Building contextual words graphs 
Giving two sets of sentences (???? and ????) 
annotated as positive and negative respectively, 
where ????	 = [?????, ? , ?????]  and ????	 =[?????, ? , ?????]	  contains list ?  involving 
words lemmatized by Freeling 2.2 Pos-Tagger 
                                                 
2
 Lemma denotes canonic form of the words. 
Corpora
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
Weight =1
Weight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
95
 (Atserias et al, 2006), a process to build two 
lexical contextual graphs, ????  and ????  is 
applied. Those sentences are manually annotated 
as positive and negative respectively. These 
graphs involve lemmas from the positive and 
negative sentences respectively. 
A contextual graph ?  is defined as an 
undirected graph ? =	 (?, ?) , where ?  denotes 
the set of vertices and ? the set of edges. Given 
the list ?	 = [?1 	? ??]  a lemma graph is created 
establishing links among all lemmas of each 
sentence, where words involved allow to 
interconnect sentences ??  in ? . As a result 
word/lemma networks ????  and ????  are 
obtained, where ?	 = 	?	 = [?? 	? ??]	  and for 
every edge (?? , ??)?	? being ??, ???	?. Therefore, ?? and ??	 are the same. 
Then, having two graphs, we proceed to 
initialize weight to apply graph-based ranking 
techniques in order to auto-balance the particular 
importance of each ?? into ???? and ????. 
3.2 Applying ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ???? and ???? take 
the default value 1/N as their weight to define 
the weight of ?  vector, which is used in our 
proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 2) as positive or negative, in relation 
to their respective graph, a weight value of 1 (in 
a range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
Thereafter, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking 
algorithm is able to increase the significance of 
the words related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre 
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and the one that has obtained relevant 
results, was an inspiration to us in this work. The 
main idea behind this algorithm is that, for each 
edge between ?i and ?j in graph ?, a vote is made 
from ?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ????  relevance. The philosophy 
behind it is that, the more important the vertex is, 
the more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
?, where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??. 
In our system, we apply the following equation 
and configuration:  
 
??	 = 	????	 +	(1 ? ?)? (1) 
Where: ?	 is a probabilistic transition matrix 
?	?	? , being ??,?  = ???	  if a link from ? i to ? j 
exist, in other case zero is assigned; ? is a vector ?	?	1	with values previously described in this 
section; ?? is the probabilistic structural vector 
obtained after a random walkthrough to arrive to 
any vertex; ?	  is a dumping factor with value 
0.85, and like in (Agirre and Soroa, 2009) we 
used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009). 
After applying PageRank, in order to obtain 
standardized values for both graphs, we 
normalize the rank values by applying the 
following equation: 
 
??? = ???/???(??) (2) 
Where ???(??)  obtains the maximum rank 
value of ?? vector. 
3.3 Adjusting sentiment polarity values 
After applying the PageRank algorithm on ???? 
and ???? , and having normalized their ranks, 
we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. ??	is represented by ???  lemmas, which would 
have, at that time, two assigned values: Positive, 
and Negative, which correspond to a calculated 
rank obtained by the PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (3) 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (4) 
Where ??? is the Positive value and ??? the 
Negative value related to each lemma in ??. 
In order to standardize the ??? and ??? values 
again and making them more representative in a 
[0?1] scale, we proceed to apply a 
normalization process over the ???  and ??? 
values. 
Following and based on the objective features 
commented by (Baccianella et al, 2010), we 
assume their same premise to establish objective 
values of the lemmas. Equation (5) is used to this 
96
 proceeding, where ???  represent the objective 
value. ??? = 1 ? |??? ? ???| (5) 
4 Sentiment Resource obtained 
At the same time we have obtained a ?? where 
each word is represented by ???, ??? and ??? 
values, acquired automatically from annotated 
sentiment corpora. With our proposal we have 
been able to discover new sentiment words in 
concordance of contexts in which the words 
appear. Note that the new obtained resource 
involves all lemmas identified into the annotated 
corpora. ???, ???, and ??? are nominal values 
between range [0? 	1]. 
5 Evaluation 
In the construction of the sentiment resource we 
used the annotated sentences provided from 
corpora described on Table 1. Note that we only 
used the sentences annotated positively and 
negatively. The resources involved into this table 
were a selection made to prove the functionality 
of the words annotation proposal of subjectivity 
and objectivity. 
The sentiment lexicons used were provided 
from WordNetAffect_Categories 3 and opinion-
words4 files and shown in detail in Table 2. 
Corpus Neg Pos Obj Neu Obj 
or Neu Unknow Total 
computational-
intelligence5 6982 6172 - - - - 13154 
tweeti-b-
sub.dist_out.tsv6 176 368 110 34 - - 688 
b1_tweeti-
objorneu-
b.dist_out.tsv6 
828 1972 788 1114 1045 - 5747 
stno7 1286 660 
 
384 - 10000 12330 
Total 9272 9172 898 1532 1045 10000 31919 
Table 1. Corpora used to apply RA-SR. 
Sources Pos Neg Total 
WordNet-Affects_Categories 
(Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words (Hu and Liu, 2004; Liu 
et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 2. Sentiment Lexicons. 
Some issues were taking into account through 
this process. For example, after obtaining a 
                                                 
3
 http://wndomains.fbk.eu/wnaffect.html 
4
 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
5
 A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
6
 Train dataset of Semeval-2013 (Task 2. Sentiment 
Analysis in Twitter, subtask b.) 
7
 Test dataset of NTCIR Multilingual Opinion Analysis 
Task (MOAT) http://research.nii.ac.jp/ntcir/ntcir-
ws8/meeting/ 
contextual graph ? factotum words are present in 
mostly of the involved sentences (i.e. verb ?to 
be?). This aspect is very dangerous after 
applying PageRank algorithm, because this 
algorithm because this algorithm strengthens the 
nodes possessing many linked elements. For that 
reason, the subtractions ??? ? ???  and ??? ????  are applied, where the most frequently 
words in all contexts obtains high values and 
being the subtraction a damping factor.  
Following an example; when we take the verb 
?to be?, before applying equation (2), verb ?to 
be? archives the highest values into each context 
graph (????  and ???? ), 9.94 and 18.67 rank 
values respectively. These values, applying 
equation (2), are normalized obtaining both ???	 = 	1  and ???	 = 	1  in a range [0...1]. 
Finally, when the next steps are executed 
(Equations (3) and (4)) verb ?to be? 
achieves ???	 = 0 , ??? = 0  and 
therefore 	???	 = 1 . Through this example it 
seems as we subjectively discarded words that 
appear frequently in both contexts (Positive and 
Negative contexts). 
Using the corpora from Table 1 we obtain 
25792 sentimentally annotated lemmas with ???, ??? and ???  features. Of them 12420 positive 
and 11999 negative lemmas were discovered, , 
and 1373 words already derived from existing 
lexical resources. 
Another contribution has been the ??? , ??? 
and ???  scores assigned to words of lexical 
inventory, which were used to reinforce the 
contextual graphs in the building process. Those 
words in concordance to our scenario count 842 
Positives and 383 Negatives.  
5.1 Sentiment Resource Applied on 
Sentiment Analysis 
To know if our method offers resources that 
improve the SA state of the art, we propose a 
baseline supported on the sentiment dictionaries, 
and other method (Ranking Sentiment Resource 
(RSR)) supported over our obtained resource. 
The baseline consists on analyzing sentences 
applying Equation (6) and Equation (7). 
?????????? = ?????????????????	 (6) 
?????????? = ?????????????????	 (7) 
Where: ???????? is the total of positive words 
(aligned with the sentiment dictionaries) in the 
sentence; ????????  is the total of negative 
words (aligned with the sentiment dictionaries) 
97
 in the sentence; ?????????  is the total of 
words in the sentence.  
Using these measures over the analyzed 
sentences, for each sentence, we obtain two 
attributes, ?????????? and	??????????; and 
a third attribute (named Classification) 
corresponding to its classification. 
On the other hand, we propose RSR. This SA 
method uses in a different way the Equation (6) 
and Equation (7), and introduces Equation (8).  
?????????? = ?????????????????	 (8) 
Being ???????? the sum of Positive ranking 
values of the sentence words, aligned with the 
obtained resource (??); ????????  the sum of 
Negative ranking values of the sentence words, 
aligned with the obtained resource (?? ); and ???????  the sum of Objective ranking values 
of the sentence words, aligned with the obtained 
resource (??). 
In RSR method we proved with two approach, 
RSR (1/di) and RSR (1-(1/di)). The first approach 
is based on a resource developed using 
PageRank with  ??,? = 1/??   and the other 
approach is using ??,? = 1 ? (1/??) . Table 3 
shows experimentation results. 
The evaluation has been applied over a corpus 
provided by ?Task 2. Sentiment Analysis in 
Twitter, subtask b?, in particular tweeti-b-
sub.dist_out.tsv file. This corpus contains 597 
annotated phrases, of them Positives (314), 
Negatives (155), Objectives (98) or Neutrals 
(30). For our understanding this quantity of 
instances offers a representative perception of 
RA-SR contribution; however we will think to 
evaluate RA-SR over other corpora in further 
researches. 
 
C I R. Pos (%) 
R. Neg 
(%) 
R. Obj 
(%) 
R. 
Neu 
(%) 
Total 
P. 
(%) 
Total 
R. 
(%) 
Baseline 366 231 91.1 51.6 0.0 0.0 48.2 61.3 
RSR(1/di) 416 181 87.3 39.4 80.6 6.7% 67.8 69.7 
RSR(1-(1/di) 469 128 88.5 70.3 81.6 6.7% 76.8 78.6 
Table 3. Logistic function (Cross-validation 10 folds) 
over tweeti-b-sub.dist_out.tsv8 corpus (597 instances). 
Recall (R), Precision (P), Correct (C), Incorrect (I). 
As we can see the baseline only is able to 
dealing with negative and positive instances. Is 
important to remark that our proposal starting up 
knowing only the words used in baseline and is 
able to growing sentiment information to other 
words related to them. We can see this fact on 
                                                 
8
 Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b.) 
Table 3, RSR is able to classify objective 
instances over 80% of Recall and the baseline 
does not.  
Other relevant element is the recall difference 
between RSR (1/di) and RSR (1 ? (1/??) . 
Traditionally (1/??) result value has been 
assigned to ? in PageRank algorithm. We have 
demonstrated that in lexical contexts RSR (1-
(1/di)) approach offers a better performance of 
PageRank algorithm, showing recall differences 
around 10 perceptual points. 
6 Conclusion and further works 
As a conclusion we can say that our proposal is 
able to automatically increase sentiment 
information, obtaining 25792 sentimentally 
annotated lemmas with ??? , ???  and ??? 
features. Of them 12420 positive and 11999 
negative lemmas were discovered. 
In other hand, The RSR is capable to classify 
objective instances over 80% and negatives over 
70%. We cannot tackle efficiently neutral 
instances, perhaps it is due to the lack of neutral 
information in the sentiment resource we used. 
Also, it could be due to the low quantity of 
neutral instances in the evaluated corpus. 
In further research we will evaluate RA-SR 
over different corpora, and we are also going to 
deal with the number of neutral instances. 
The variant RSR (1 ? (1/??)  performs better 
than RSR(1/??) one. This demonstrates that in 
lexical contexts using PageRank with ??,? = 1 ?(1/??) offers a better performance. Other further 
work consists in exploring Social Medias to 
expand our retrieved sentiment resource 
obtaining real time evidences that occur in Web 
2.0. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
98
 Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. p.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006. p.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 
p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and 
Computing Systems. Alacant, Univeristy of 
Alacant, 2011. 299. p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. 
Martinez-Barco. The OpAL System at NTCIR 8 
MOAT. Proceedings of NTCIR-8 Workshop 
Meeting, Tokyo, Japan., 2010. 241-245 p.  
Balahur, A. and A. Montoyo. Applying a culture 
dependent emotion trigger database for text 
valence and emotion classification. Procesamiento 
del Lenguaje Natural, 2008a. p.  
Balahur, A. and A. Montoyo. Building a 
recommender system using community level social 
filtering. 5th International Workshop on Natural 
Language and Cognitive Science (NLPCS), 2008b. 
32-41 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer 
Networks and ISDN Systems, 1998, 30(1-7): 107-
117. 
Cerini, S.; V. Compagnoni; A. Demontis; M. 
Formentelli and G. Gandini Language resources 
and linguistic theory: Typology, second language 
acquisition, English linguistics (Forthcoming), 
chapter Micro-WNOp: A gold standard for the 
evaluation of automatically compiled lexical 
resources for opinion mining., 2007. 
Cilibrasi, R. L. and P. M. B. Vit?nyi The Google 
Similarity Distance IEEE TRANSACTIONS ON 
KNOWLEDGE AND DATA ENGINEERING, 
2007, VOL. 19, NO 3. 
Esuli, A. and F. Sebastiani. SentiWordNet: A Publicly 
Available Lexical Resource for Opinion Mining. 
Fifth international conference on Languaje 
Resources and Evaluation Genoa - ITaly., 2006. 
417-422 p.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000. 
p.  
Hu, M. and B. Liu. Mining and Summarizing 
Customer Reviews. Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (KDD-2004), USA, 
2004. p.  
Kim, S.-M. and E. Hovy. Extracting Opinions, 
Opinion Holders, and Topics Expressed in Online 
News Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the 
association for computational linguistics 
(COLING/ACL 2006), Sydney, Australia, 2006. 1-
8 p.  
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005. p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller Introduction to WordNet: An On-line 
Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990. 
Pang, B.; L. Lee and S. Vaithyanathan. Thumbs up? 
Sentiment Classification using machine learning 
techniquies. EMNLP -02, the Conference on 
Empirical Methods in Natural Language 
Processing, USA, 2002. 79-86 p.  
Popescu, A. M. and O. Etzioni. Extracting product 
features and opinions from reviews. Proccedings of 
HLT-EMNLP, Canada, 2005. p.  
Stone, P.; D. C.Dumphy; M. S. Smith and D. M. 
Ogilvie The General Inquirer: A Computer 
Approach to Content Analysis The MIT Press, 
1996. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language 
Resources and Evaluation (LREC 2004), Lisbon, 
2004. 1083-1086 p.  
Turney, P. D. Thumbs up or thumbs down? Semantic 
orientation applied to unsupervised classification 
of reviews. Proceeding 40th Annual Meeting of the 
Association for Computational Linguistic. ACL 
2002, USA, 2002. 417-424 p.  
Wiebe, J. Tracking point of view in narrative 
Computational Linguistic, 1994, 20(2): 233-287. 
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in 
Language. Kluwer Academic Publishers, 
Netherlands, 2005. p.  
99
