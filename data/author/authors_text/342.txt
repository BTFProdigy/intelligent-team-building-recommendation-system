Extracting the Lowest-Frequency Words: 
Pitfalls and Possibilities 
Marc  Weeber*  
University of Groningen 
R. Hara ld  Baayen*  
Max Planck Institute for 
Psycholinguistics 
Re in  Vos t 
University of Groningen, University of 
Maastricht 
In a medical information extraction system, we use common word association techniques to 
extract side-effect-related t rms. Many of these terms have a frequency of less than five. Standard 
word-association-based applications disregard the lowest-frequency words, and hence disregard 
useful information. We therefore devised an extraction system for the full word frequency range. 
This system computes the significance of association by the log-likelihood ratio and Fisher's exact 
test. The output of the system shows a recurrent, corpus-independent pa tern in both recall and 
the number of significant words. We will explain these patterns by the statistical behavior of the 
lowest-frequency words. We used Dutch verb-particle combinations a a second and independent 
collocation extraction application to illustrate the generality of the observed phenomena. We will 
conclude that a) word-association-based extraction systems can be enhanced by also considering 
the lowest-frequency words, b) significance levels should not be fixed but adjusted for the optimal 
window size, c) hapax legomena, words occurring only once, should be disregarded a priori in 
the statistical analysis, and d) the distribution of the targets to extract should be considered in 
combination with the extraction method. 
1. Introduction 
The research reported here arose from an attempt o determine the conditions under 
which optimal recall and precision are obtained for the extraction of terms related to 
side effects of drugs in medical abstracts. We used the standard technique of defining a 
window around a seed term, side-effect in our case, and selected as potentially relevant 
terms those words that appeared more often in these windows than expected under 
chance conditions. 
Our original question concerned the extent o which recall and precision are in- 
fluenced by the size of the window. It turns out, however, that a preliminary question 
needs to be answered first, namely, how to gauge the significance of the large effect of 
the lowest-frequency words on recall, precision, and the number of words extracted 
as potentially relevant erms. 
* Groningen University Institute for Drug Exploration, Department ofSocial Pharmacy and 
Pharmacoepidemiology, Ant. Deusinglaan 1,9713 AV Groningen, The Netherlands. E-mail: 
marc@farm.rug.nl 
t Faculty of Health Sciences, Department ofHealth Ethics and Philosophy, P.O. Box 616, 6200 MD 
Maastricht, The Netherlands. E-mail: rein.vos@zw.unimaas.nl 
:~ Max Planck Institute for Psycholinguistics, P.O. Box 310, 6500 AH Nijmegen. E-mail: baayen@mpi.nl 
(~) 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 3 
o.i 
Z 
150 - 
100 
50  
0 I \ ] l r l l , , , . , ,  . . . . . . .  
5 10 15 20 
0.15 
Z 0.10 
Z 0.05 
5 10 15 20 
Frequency Class Frequency Class 
(a) (b) 
Figure 1 
Frequency distribution of medical expert word types. Panel (a) shows the number of 
side-effect-related word types as judged by a medical expert (Nexpert) as a function of the 
first 23 frequency classes. Panel (b) shows the proportion of expert ypes/total corpus types 
(Ntotal) for the first 23 frequency classes. The horizontal dashed line indicates the mean 
proportion of 0.0619. 
It is common practice in information retrieval to discard the lowest-frequency 
words a priori as nonsignificant (Rijsbergen 1979). In Smadja's collocation algorithm 
Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993). 
Church and Hanks (1990) use mutual information to identify collocations, a method 
they claim is reasonably effective for words with a frequency of not less than five. 
A frequency threshold of five seems quite low. Unfortunately, even this lower 
frequency threshold of five is too high for the extraction of side-effect-related rms 
from our medical abstracts. To see this, consider the left panel of Figure 1, which 
plots the number of side-effect-related words in our corpus of abstracts as judged 
by a medical expert, as a function of word-frequency lass. The side-effect-related 
words with a frequency of less than five account for 295 of a total of 432 expert 
words (68.3%). The right panel of Figure 1 shows that the first 23 word-frequency 
classes are characterized by, on average, the same proportion of side-effect-related 
words. The a priori assumption of Rijsbergen (1979) that the lowest-frequency words 
are nonsignificant is not warranted for our data, and, we suspect not for many other 
data sets as well. 
The recent literature has seen some discussion of the appropriate statistical meth- 
ods for analyzing the contingency tables that contain the counts of how a word is 
distributed inside and outside the windows around a seed term. Dunning (1993) has 
called attention to the log-likelihood ratio, G 2, as appropriate for the analysis of such 
contingency tables, especially when such contingency tables concern very low fre- 
quency words. Pedersen (1996) and Pedersen, Kayaalp, and Bruce (1996) follow up 
Dunning's uggestion that Fisher's exact est might be even more appropriate for such 
contingency tables. 
We have therefore investigated for the full range of word frequencies whether 
there is an optimal window size with respect o recall and the number of significant 
words extracted using both the log-likelihood ratio and Fisher's exact test. In Sec- 
tion 2, we will show that indeed there seems to be an optimal window size for both 
statistical tests. However, a recurrent pattern of local optima calls this conclusion into 
question. Upon closer inspection, this recurrent pattern appears at fixed ratios of the 
number of words inside the window to the number of words outside the window 
(complement). 
302 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
In Section 3, we will relate the recurrent patterns of local optima at fixed window- 
complement ratios (henceforth W/C-ratios) to the distributions ofthe lowest-frequency 
words over window and complement. We will call attention to the critical effect of the 
choice of W/C-ratios on the significance of the lowest-frequency words. 
As the improvement in the extraction of side-effect terms from medical abstracts, 
as gauged by the F-measure, which combines recall and precision (Rijsbergen 1979), 
is small, we also applied the same approach to the extraction of Dutch verb-particle 
combinations from a newspaper corpus. In Section 4, we report substantially better 
results for this more lexical extraction task, which is subject o the same statistical 
behavior of the lowest-frequency words. 
In the last section, we will discuss the consequences of our findings for the op- 
timization of word-based extraction systems and collocation research with respect o 
the lowest-frequency words. 
2. An Optimal Window Size for Medical Abstracts? 
The MEDLINE bibliographic database contains a large number of abstracts of scien- 
tific journal papers discussing medical and drug-related research. Typically, abstracts 
discussing medical drugs mention the side effects of these drugs briefly. Information 
on side effects is potentially relevant for finding new applications for existing drugs 
(Rikken and Vos 1995). We are therefore interested in any terms related to the side 
effects of drugs. 
Before proceeding, it may be useful to clarify the way in which the present re- 
search differs from standard research on collocations. In the latter kind of research, 
there is no a priori knowledge of which combinations of words are true collocations. 
Moreover, the most salient collocations generally are found at the top of a list ranked 
according to measures for surprise or association, such as G 2 or mutual information 
(Manning and Sch~itze 1999). The large numbers of word combinations with signifi- 
cant but low values for these measures are often of less interest. Low-frequency words 
are predominant among these kinds of collocations. In our research, we likewise find 
many low-frequency terms for side effects with low ranks in medical abstracts. The 
relatively well-known side effects that are mentioned frequently can be captured by 
examining the top ranks in the lists of extracted words. At the same time, the rarely 
mentioned side-effect terms are no less important, and in post marketing surveillance 
the extraction of such side-effect terms may be crucial for the acceptance or rejection 
of new medicines. 
Is reliable automatic extraction of both low- and high-frequency side-effect terms 
from MEDLINE abstracts feasible? To answer this question, we explored the efficacy 
of a standard collocation-based term extraction method that extracts those words that 
appear more frequently in the immediate neighborhood of a given seed term than 
might be expected under chance conditions. 
We compiled two corpora on the side effects of the cardiovascular d ugs captopril 
and enalapril from MEDLINE abstracts. The first corpus contains all abstracts mention- 
ing captopril and the word side. The second corpus contains all abstracts mentioning 
captopril and at least one of the compounds side-effect, side effect, side-effects, and side 
effects. Thus, the second corpus is a subset of the first. The first corpus is comprised 
of 118,675 tokens and 7,678 types; the second corpus 103,603 tokens and 6,582 types. 
A medical expert marked 432 of the latter word types as side-effect-related rms. The 
left panel of Figure 1 summarizes the head of the frequency distribution of these terms 
in the larger corpus. Note that most side-effect-related rms have a frequency lower 
303 
Computational Linguistics Volume 26, Number 3 
Table 1 
General 2x2 contingency table. A = frequency of the target in the window 
corpus, B = frequency of the target in the complement corpus, W = total 
number of words in the window, C = total number of words in the 
complement. Corpus size N = W + C. 
window complement 
frequency of target A B 
sum frequency of other words W - A C - B 
W C 
A+B 
W+C-A-B  
W+C 
than five. What we need, then, is an extraction method that is sensitive enough to 
select such very low frequency terms. 
In the collocation-based method studied here, the neighborhood of a given seed 
term is defined in terms of a window around the seed term. We constructed windows 
around all seed terms in the corpus, leading to a window corpus and a complement 
corpus. The window corpus contains all words that appear within a given window 
size of the seed term. For instance, with a window size of 10, any word appearing 
from five words before the seed to five words after the seed as well as the seed itself is 
included in the window corpus. The word tokens not in the window corpus comprise 
the complement corpus. Any type in the window corpus is a potential side-effect- 
related term. For any such target type, we tabulate its distribution in window and 
complement corpora in a contingency table like Table 1. 
Given W and C, we need to know whether the frequency of the target in the 
window corpus, A, is high enough to warrant extraction. Typically, given the marginal 
B and distribution of the contingency table, a target is extracted for which wA--~A > ~-2-~, 
for which the tabulated istribution is nonhomogeneous according to tests such as G 2 
and Fisher's exact test for a given cMevel. 
In this approach, the window size is a crucial variable. At small window sizes, 
many potentially relevant erms fail to appear in the window corpus. However, at 
large window sizes, many irrelevant words are found in the window corpus and may 
be extracted spuriously. 
To see to what extent window size may affect the results of the extraction proce- 
dure, consider the solid lines in panels (a) and (b) of Figure 2. The left panel shows the 
results for recall when we use the log-likelihood ratio, G 2, the right panel the results 
for Fisher's exact test. We define recall as the proport ion of the number of side-effect 
words extracted and the total number of side-effect words available in the window. 
For both statistical tests, recall seems to be optimal at window size 2. However, 
at this window size, the number of words extracted is very small. This can be seen in 
panels (c) and (d). Considered jointly, panels (a) and (c) suggest an optimal window 
size of 24 for our larger corpus (corpus 1), as recall is still high, and the number of 
significant words is maximal. When Fisher's exact est is used instead of G 2, panels (b) 
and (d) suggest 42 as the optimal size. 
The dashed lines in panels (a) to (d) show the corresponding results for our smaller 
corpus (corpus 2). Unsurprisingly, the general pattern for this subcorpus is quite sim- 
ilar, although the drops in recall and the number of significant words, Nsig, occur at 
somewhat smaller window sizes. 
Interestingly, we can synchronize the curves for both corpora by plotting recall and 
the number of significant items, Nsig, against the window-complement ratio (W/C). 
This is shown in panels (e) and (f). These panels suggest not an optimal window size 
304 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
04 f 0.3 i 
~ 0.2 ~ i 
0 20 40 6(J 80 100 120 
24 86 
Window Size 
(a) 
6??r A 
'~ 4??t / ',il _ . ~ ~  ~ \[ 
. , /  i . . . . . .  
0 20 40 60 80 100 120 
24 86 
Window Size 
(c) 
 4oo\[ , 
200I, 
03 i } 
0.2 
o. h 
0 20 40 60 80 100 120 
6 42 82 
Window Size 
(b) 
3oo r , 
~o 
0 20 40 60 80 100 120 
6 42 82 
Window Size 
(d) 
300 I 
 iilI  ..... "1 
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 
0.17 0.62 0.05 0.29 0.58 
w/c w/c 
(e) (t) 
Figure 2 
Results of the word extraction procedure (a = 0.05). Solid line = corpus 1, dashed line = 
corpus 2. Panel (a) shows the log-likelihood, G2, recall results as a function of the window 
size. Panel (b) shows recall values for Fisher's exact est. Panel (c) shows the total number of 
significant words (Nsig) as a function of the window size for G 2. Panel (d) shows the same as 
(c) but for Fisher's exact test. Panel (e), G 2, and (f), Fisher's exact est, also show the total 
number of significant words, but as a function of the W/C-ratio; the ratio of the number of 
words in the window corpus to the number of words in the complement corpus. 
but an optimal W/C-ratio (0.17 for G 2 and 0.29 for Fisher's exact test). Although we 
now seem to have shown that recall and Nsig depend on the choice of w indow size, 
the sudden drops in recall and Nsig and the reoccurrence of such drops at various 
W/C-ratios is a source of worry, not only for G 2 results, but also for the results based 
on Fisher's exact test. A further source of worry is the fact that the two tests diverge 
considerably with respect o the optimal W/C-ratio. 
3. Contingency Tables and the Lowest-Frequency Words 
Before we can have any confidence in the optimality of a given W/C-ratio, we should 
understand why the saw-tooth-shaped patterns of Nsig arise. Both the log-likelihood 
ratio (G 2) and Fisher's exact test compute the significance of contingency tables similar 
to Table 1. So why is it that the left panels in Figure 2 differ from the right panels? 
G 2 has a 2-distribution as N --* cx~. This convergence is not guaranteed for low 
expected frequencies and sparse tables, which renders use of G 2 problematic for our 
lowest-frequency words in that it may  suggest words to be more remarkable than they 
305 
Computational Linguistics Volume 26, Number 3 
Table 2 
Contingency tables for hapax legomena, dis legomena, nd tris legomena. 
W = number of words in window corpus; C = number of words in 
complement corpus. Total corpus size: N = W + C. 
(a): 1 0 (b): 2 0 (c): 1 1 
W-1 C W-2  C W-1  C-1  
(d): 3 0 (e): 2 1 (f): 1 2 
W-3  C W-2  C-1  W-1  C-2  
really are. Fisher's exact test, on the other hand, does not use an approximation to a 
probability distribution but computes the exact hypergeometric distribution given the 
marginal totals of the contingency table. While Fisher's exact test is suitable for the 
analysis of sparse tables, it is inherently conservative because it regards the marginal 
totals not as stochastic variables but as fixed boundary conditions. Consequently, this 
test is likely to reject words that are in fact remarkably distributed in the contingency 
table. The difference in behavior of the two tests is clearly visible in panels (c) and (d) 
of Figure 2: the number of significant words (Nsig) according to G 2 is roughly twice 
as large as that according to Fisher's exact test. 
When a hapax legomenon 1, a word with frequency 1, occurs in the window corpus, 
we use contingency table (a) as shown in Table 2. For dis legomena, words with a 
frequency of 2, that appear at least once in the window corpus, we obtain the two 
contingency tables (b) and (c). The interesting contingency tables for tris legomena re 
tables (d) to (f). These six tables are relevant for 63.8% of the side-effect-related terms 
as judged by our medical expert. 
How do changes in the W/C-ratio affect G 2 and Fisher's exact test, when applied 
to contingency tables (a) to (f)? In other words, how does the choice of the window 
size affect whether a low-frequency word is judged to be a significant erm, for fixed 
A and B (e.g., A = 1 and B = 0 for a hapax legomenon)? 
First, consider contingency tables with B = 0, for instance tables (a), (b), and (d). 
For small A, (A ~ W, C), it is easily seen (see the appendix) that the critical W/C-ratio 
based on the log-likelihood ratio is: 
W 1 
C ~/eX/2 - 1' 
(1) 
with X the X 2 value corresponding to a given s-level with 1 degree of freedom. For 
A = 1 and c~ -- 0.05, X = 3.84, the critical W/C-ratio equals 0.1718. This is exactly 
the W/C-ratio in panel (e) in Figure 2 at which the first and largest drop in the num- 
ber of significant words occurs. Up to this ratio, any hapax legomenon appearing in 
the window corpus is judged to be a significant erm. For W/C > 0.1718, no hapax 
legomenon will be extracted. 
Fisher's exact test is far more conservative. For this test, the critical W/C-ratio is 
1 The term hapax legomenon (literally 'read once') goes back to classical studies and was originally used 
to refer to the words used once only in the works of a given author, e.g., Homer. By analogy, dis 
legomenon and tris legomenon have come into use to refer to words occurring only twice or three 
times. 
306 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
Table 3 
Critical W/C-ratios where sparse and skewed contingency tables lose 
significance. Equations 1 and 2 provide the ratios for the B = 0 cases. The 
other ratios are obtained by simulations. 
distribution G 2 Fisher 
A-B cz = 0.05 c~ = 0.01 c~ -- 0.05 c~ -- 0.01 
1 - 0 0.1718 0.0375 0.0526 0.0101 
1 - 1 0.0400 0.0092 0.0260 0.0050 
2 - 0 0.6204 0.2348 0.2880 0.1111 
1 - 2 0.0232 0.0053 0.0172 0.0033 
2 - 1 0.1917 0.0824 0.1565 0.0626 
3 - 0 1.1155 0.4938 0.5833 0.2746 
hapax legomena 
dis legomena 
tris legomena 
(see the appendix for details): 
w 
C 1-  ?/-P' (2) 
where P is the s-level. For A -- 1 and P = 0.05, the critical W/C-ratio for a hapax 
legomenon equals 0.0526. In panel (f) of Figure 2, we observe the first drop in the 
number of significant words at precisely this W/C-ratio. For very small W/C-ratios, 
any hapax legomenon in the window corpus is also judged to be significant according 
to Fisher's exact test. Compared to G 2, Fisher's exact test rejects hapax legomena s 
significant at much smaller W/C-ratios. Note that when W/C -- 0.05/0.95 = 0.0526, 
i.e., when the window corpus is exactly 1/20 of the total corpus, the probability that a 
hapax legomenon appears in the window corpus equals 0.05. Our conclusion is that, 
with the W/C-ratio as the only determinant of significance, the windowing method is 
not powerful enough to distinguish between relevant and irrelevant hapax legomena. 
In other words, hapax legomena should be removed from consideration a priori. 
For dis legomena that appear exclusively in the window corpus, the critical ratios 
are 0.6204 for G 2, corresponding to the second major drop in panel (e) of Figure 2, 
and 0.2880 for Fisher's exact test, corresponding to the severe drop following the 
maximum of Nsig in panel (f). The third major drop in this panel corresponds to the 
critical W/C-ratio for tris legomena occurring three times in the window corpus. 
For contingency tables with B > 0; A > B; A, B <~ W, C, critical W/C-ratios are not 
easy to capture analytically. We therefore carried out a simulation study for W + C = 
100,000. For fixed A and B and a given s-level, we calculated the critical W/C-ratio 
by iterative approximation. Results are summarized in Table 3. 
When we highlight these critical ratios in Figure 2 by means of vertical dashed 
lines, we obtain Figure 3. Panels (a) to (d) correspond to the curves for corpus 2 in the 
first four panels of Figure 2. For the log-likelihood ratio, we observe that both the major 
and minor drops in recall and the number of significant words (Nsig) occur at the W/C- 
ratios where different distributions of the lowest-frequency words lose significance. For 
Fisher's exact test, we observe exactly the same pattern. Panels (e) and (f) show the 
number of significant words for a pseudorandomized version of corpus 2 where we 
used the same tokens but randomized the order of their appearance. Although the 
number of significant words is lower, the saw-tooth-shaped pattern with the sudden 
drops at fixed ratios reemerges. 
We conclude that W and C are the prime determinants of both recall and the 
number of significant words. At first sight, Fisher's test is clearly preferable to the 
307 
Computational Linguistics Volume 26, Number 3 
1-1 1-0 2-1 3-1 2-0 0.41- ~ ~ 
= Fi i 
~03 ~ ii i 
~ 0.2 , 
0.0 0.2 0.4 0.6 0.8 
W/C 
(a) 
1-1 1-0 2-1 3-1 2-0 
600 I 
 42?o?o1 
0.0 0.2 0.4 0.6 0.8 
W/C 
(c) 
1-1 1-0 2-1 3-1 2-0 
400 f
300\[ 
Z 200\[ 
100\[ 
1-0 2-1 2-0 
~ 0.3\[ 
0.2 
0.1 
3-1 3-0 
0.0 0.2 0.4 0.6 0.8 
W/C 
(b) 
1-0 2-1 2:0 3-1 3-0 
0.0 0.2 0.4 0.6 0.8 
W/C 
(d) 
1-0 
150\[ 
Z "~ 100\[ 
50\[, 
0.0 
2-1 2-0 3-1 3-0 
0.0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 
W/C W/C 
(e) (0 
F igure  3 
Results of word extraction procedure (a = 0.05) with A-B distributions. Panels (a), 
log-likelihood ratio, G 2, and (b), Fisher's exact est, show the recall results of the extraction 
procedure for corpus 2. Panels (c) and (d) show the total number of significant words (Nsig), 
again for G 2 and Fisher's exact est, respectively (see also Figure 2). Panels (e) and (f) show the 
results for a randomized corpus for G 2 and Fisher's exact est. The numbers above the panels 
indicate the A-B distribution of the contingency tables in Table 2. 
log-likelihood ratio because the extreme saw-tooth-shaped pattern is substantially re- 
duced. However, the use of Fisher's exact est does not eliminate the effect of the choice 
of window and complement size on the number of significant words and recall. At 
specific W/C-ratios, nonnegligible numbers of words with the lowest frequency of oc- 
currence suddenly lose significance. Moreover, in our discussion thus far, we have not 
taken extraction precision into account nor the trade-off between precision and recall. 
For the assessment of overall extraction results, we turn to the F-measure (Rijsbergen 
1979), a measure that assigns equal weights to precision (P) and recall (R): 
2PR 
F= P+R"  (3) 
Figure 4 plots precision, recall, and F as a function of the W/C-ratio. The common 
trade-off between recall and precision is clearly present for the smaller window sizes, 
with the F-measure providing a kind of average. 
Thus far, we have applied a common collocation extraction technique to a semantic 
association task. Actual extraction performance is low: F is maximally 0.17. To gauge 
308 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
0.4 I . . . . . . . .  
(}.3 
k 
o.2 ~'i' 
{7. , i : ,  " " /  . . . .  ~- "  q , _  1 i 
(}.(7 0.2 {}.4 (}.6 - 0.8 I .{} 
{7.17 {7,62 
u2 
(}.3 
0.2 
{7.1 
{}.(7 0.2 0.4 0.6 
0.{}5 0 .29 0 .58 
i t 
,i i k 
O.8 I.O 
w/c  w/c  
Figure 4 
F, recall, and precision as a function of the W/C-ratio. Recall (R, dashed line), F (solid line), 
and precision (P, dotted line) using G 2 (left panel) and Fisher's exact est (right panel) for our 
second corpus plotted as a function of the W/C-ratio. 
whether better results can be obtained with the present echniques, we examined the 
extraction of Dutch verb-particle combinations. 
4. Extracting Verb-Particle Combinations 
In English, the particle of verb-particle combinations always follows the verb, as in 
she rang him up. In Dutch, the particle can occur either before or after the verb. When 
it occurs before the verb, it is separated from the verb by te ('to') and/or  one or more 
auxiliary verbs. Extracting such particle-verb combinations i relatively straightfor- 
ward. However, when the particle follows the verb, it may be separated from the verb 
by many constituents of arbitrary complexity: Hij zegt de belangrijke afspraak met de pro- 
grammeur voor vanmiddag af ('he says the important meeting with the programmer for 
this afternoon off'; i.e., he cancels the meeting). How well does our present approach 
lend itself to the extraction of verb-particle combinations with the particle af ('off') 
when the particle follows the verb? 
We investigated this question by studying verb-particle combinations with af from 
a Dutch newspaper corpus of about 4.5 million word tokens. We extracted by hand all 
sentences from the corpus that contain af (3,802 sentences, 97,903 tokens) and singled 
out those sentences in which af belongs to a verb-particle combination in which the 
verb occurs to the left of the particle (2,202 sentences with 42,825 tokens). The targets 
to extract from the 2,202 sentences are 436 different verb inflections, of which 276 have 
a frequency of less than five. Just as the judgments of a medical expert were used in 
the preceding extraction task to provide a frame of reference for the evaluation of 
precision and recall, the present lexical extraction task has as its frame of reference the 
2,202 sentences that we judged to contain a verb followed at some point to the right 
by a particle. How many of the 436 different verb inflections can we extract with our 
windowing technique, and what is the trade-off between recall and precision? 
To answer this question, we defined windows to the left of the seed term af in the 
range of positions \[-12, -1\]. We calculated the W/C-ratio for each window size. For 
each word in all windows, we calculated its significance according to G 2 and Fisher's 
exact test. Using the 436 target verb inflections as a frame of reference, we computed 
precision, recall, and F. Panel (a) of Figure 5 plots F as a function of the W/C-ratio. 
F reaches a maximum F of 0.31 at W/C = 0.59 for G 2 (the solid line in the figure) 
and a maximum of 0.27 at W/C = 0.50 for Fisher's exact test (the dashed line). These 
309 
Computational Linguistics Volume 26, Number 3 
03\[ / - 
{}2 / / /  
O. _ 
~}2 -{}14 {}.6 {}.8 I.{} 
Z 
400 
200 
/ / J  J J\ 
0.2 0.4 0.6 {}T8 I.{} 
W/C  
(a} 
W/{." 
(b) 
{}.3 
0.2 
{}. 1 
f .~ , .  J 
/ 
7 
/ 
__  L t . . . .  J .  
0,2 0.4 0.6 
4{10 
~0 
Z 
200 
1}.8 1,11 
L . . . . .  ? _ _ _  ? L _ ? 
{}.2 0.4 0,6 0.8 I .{} 
W/C  W/C  
(c) (d) 
Figure 5 
Extraction results for the af corpus. Panel (a) shows F for G 2 (solid line) and Fisher's exact est 
(dashed line) as a function of the W/C-ratio. Panel (b) displays the number of significant 
words (Nsig) according to both tests. Panel (c) shows F for G 2 at c~ = 0.05 (solid line) and 
Fisher's exact est at c~ = 0.1 (dotted line). Panel (d) shows Nsig for G 2 at c~ = 0.05 and for 
Fisher's exact est at c~ -- 0.1. 
results compare favorably with the maximum F of 0.17 obtained for the extraction of 
side-effect terms from medical abstracts. 
Panel (b) of Figure 5 shows the by-this-time familiar saw-tooth-shaped pattern of 
the number of significant word types as function of the W/C-ratio. We observe again 
that Fisher's exact test is more conservative, and in the extraction task, less successful, 
than G 2. However, by opting for a more liberal c~-level we can compensate for the 
conservatism of Fisher's exact test and obtain an F profile that is indistinguishable 
from that of G 2 as shown in panel (c) for ~ -- 0.1. Panel (d) returns to the number of 
significant erms (Nsig) when Fisher's exact test is used with c~ = 0.1. Note that the 
optimal W/C-ratio according to F for G 2 (0.59) still leads to a higher Nsig than the 
optimal W/C-ratio (0.83) for Fisher's exact test with c~ -- 0.1. However, in the case of 
Fisher's exact test, the precision is much higher than when G 2 is used. These results 
suggest hat the choice of G 2 or Fisher's exact test should be guided by the desired 
trade-off between precision and recall. 
5. D iscuss ion  
The question that originally motivated the present research concerned the determina- 
tion of the optimal window size for the extraction of side-effect-related words. Most 
310 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
words that are judged by a medical expert o be related to side effects have frequencies 
of use that are so low that they fall below the frequency thresholds generally used 
in standard information extraction techniques. Is it nevertheless possible to single out 
such low-frequency terms through optimal window size estimation, especially since 
the log-likelihood ratio and Fisher's exact est have recently been advanced as suitable 
techniques even for the analysis of the lowest-frequency ranges? 
Manipulation of the window size revealed a saw-tooth-shaped pattern in the num- 
ber of significant words (Nsig) that depends not on the window size itself but on 
the W/C-ratio. This saw-tooth-shaped pattern arises most prominently when the log- 
likelihood ratio is used to extract significant words, but it is also clearly visible when 
Fisher's exact test is used. This pattern is due to the way in which these tests eval- 
uate surprise as a function of the window size for the lowest-frequency words. We 
argue that hapax legomena should be disregarded a priori, while for low-frequency 
words with frequency greater than 1, only the most extreme distributions over win- 
dow and complement are reliable in that we are confident that these terms are really 
related to the seed. For dis and tris legomena, for instance, all occurrences should in 
effect be concentrated in the window. Only then are we confident that there is truly a 
relationship between the seed and the target. 
With these restrictions, the optimum W/C-ratio for our side-effect data is just 
smaller than 0.2880, using Fisher's exact test, which amounts to an optimal win- 
dow size of 36. Of the 295 terms with a frequency of 4 or less that a medical expert 
judged to be side-effect-related rms, we capture 14, which amounts to 4.8%. When 
we exclude the hapax legomena s impossible to extract reliably a priori, we capture 
14/122 = 11.5%. Although the gain in number of significant low-frequency items is 
small, the success for the low-frequency items is still reasonable when compared to the 
corresponding success rate of 26/137 = 19.0% for the items with a frequency of 5 or 
more. These results uggest that the windowing technique is far from optimal for the 
extraction of side-effect terms from medical abstracts, irrespective of the frequencies 
of these terms. 
The windowing technique applied to the extraction of Dutch verb-particle com- 
binations led to more encouraging results. Choosing 0.4625 as the optimal W/C-ratio 
for the af data, which amounts to accepting dis legomena with a 2-0 distribution, and 
using a = 0.1 with Fisher's exact est, we obtain an optimal window size of 5. With 
this window, we extract 42 of the 139 lowest-frequency words in the 2 to 4 range, i.e., 
30.2%. This compares favorably to the success rate of 60/170 = 35.2% for verbs with 
a frequency greater than 4. When we use G 2 instead of Fisher's exact test to obtain 
improved recall at the cost of lesser precision, we extract 58/139 = 41.7% of the lowest- 
frequency words in the 2 to 4 range and 64/170 = 37.6% of the higher-frequency words 
(optimum W/C-ratio 0.6204, corresponding window size of 7). For this more lexical 
extraction task, extraction success rates are comparable for the lower-frequency and 
the higher-frequency words. Neglecting the extraction of the lower-frequency words 
a priori would have led to the loss of nearly half of the words currently extracted. 
The difference in the results between the two extraction tasks, side effects in medi- 
cal abstracts and verb-af combinations in a newspaper corpus, is due to the difference 
in the distributions of the targets around the seed terms. Concentrating on the lowest- 
frequency word tokens, the left panel of Figure 6 shows their distribution for the 
side-effect corpus. The right panel shows the corresponding distribution for the af 
corpus. The side-effect terms reveal a wide scatter around the seed at position 0. By 
contrast, verbs predominantly cluster close to the left of af. Apparently, the distance 
between the verb and the particle is more constrained than the distance between side- 
effect terms and the seed term. The optimal window size of 7 (position -7) for G 2 
311 
Computational Linguistics Volume 26, Number 3 
10 
i k 
-300 -200 -100 0 100 200 
40 
20 
-40 -20 
Position Position 
(a) (b) 
, ll,,,,,it,, .... ,, 
20 
Figure 6 
Frequency distribution of words occurring two to four times. Panel (a) shows for the side 
effect corpus how the expert words with a frequency of 2, 3, and 4 are distributed around the 
seed term. Panel (b) shows this distribution for the af corpus. 
obtained above ties in with the distribution of the lowest-frequency words: 68% of all 
lowest-frequency tokens are in this window. For the side-effect corpus, only 31% of 
all low-frequency tokens are in the optimal window of 36 for Fisher's exact test. This 
suggests that the optimal window size must be ascertained on the basis of the distri- 
bution of targets around the seed, on the one hand, and by optimizing the statistics, 
on the other hand. 
As an illustration of how the statistics can be optimized, we return to the af data. 
When we look at the distribution of the lowest-frequency words in Figure 6, an optimal 
window size of 8 to the left suggests itself. This translates into a W/C-ratio of 0.6689. 
Given that we want to retain dis legomena with a 2-0 distribution, we proceed to 
compute the corresponding significance levels for both G 2 and Fisher's exact test by 
Equations 1 and 2. The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's 
exact test is 0.161. The extraction results for both tests as measured by F are 0.31 and 
0.33, respectively. This procedure allows us to extract 64/139 = 46.0% of the low- 
frequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 
64/139 = 46.0% and 79/170 = 46.7%, respectively, using Fisher's exact test. Note that 
this technique is optimal for the extraction of the lowest-frequency words, leading to 
identical performance for G 2 and Fisher's exact test for these words. For the higher- 
frequency words, Fisher's exact test leads to a slightly better recall with the same 
precision scores (0.31 for both tests). 
While we have observed reasonable results with both G 2 and Fisher's exact est, we 
have not yet discussed how these results compare to the results that can be obtained 
with a technique commonly used in corpus linguistics based on the mutual information 
(MI) measure (Church and Hanks 1990): 
I(x,y) --- log 2 P(x,y) (4) P(x)P(y) 
In (4), y is the seed term and x a potential target word. A high MI score for a given 
target word suggests an association between this target and the seed term. Or perhaps 
more precisely, a low MI score suggests a dissociation between target and seed word 
(Manning and Sch/itze 1999). To compute recall, precision, and F, we require a cut-off 
value. As there is no theoretically motivated cut-off value, we vary it systematically. 
Panel (a) of Figure 7 plots the results for the af corpus. The x-axis represents the MI 
312 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
f -~k  
g o 
(a) 
/ - -  < ~ 
0.I 0 .2~-"7~__7  _ .q,-" " 
,~', ... ., 04. Slgl~l/l~ nc (Z3 e/eLe ~9- ~?~\C 
(b) 
Figure 7 
Extraction results (F) for the af corpus for mutual information and Fisher's exact est. Panel (a) 
shows the F score as a function of both W/C-ratio and mutual information cut-off value. 
Panel (b) shows F as a function of W/C-ratio and the significance l vel c~ used with Fisher's 
exact est. 
cut-off value, the y-axis the W/C-ratio, and the z-axis the F value. Note that F is rather 
indifferent to variation in window size and MI cut-off value. It varies between 0 (at 
the right-hand edge) to 0.17, with most values around 0.15 (the plateau in the figure). 
Interestingly, the highest possible MI cut-off point equals 4.27: the right-hand edge of 
the plateau. In fact, 4.27 is the maximum MI score for this corpus size (42,825) and 
the frequency of the seed term af (2,206), irrespective of the frequency of the target 
word, reached when all occurrences of the target word are concentrated in the window 
(see the appendix for details). Consequently, any hapax legomenon appearing in the 
window will automatically be assigned the maximum value of MI, along with target 
words with the most extreme W/C distributions (Window-Complement: 2-0, 3-0, 4-0, 
etc.). This has the unfortunate consequence that, with regard to their MI score, truly 
remarkably distributed target words become indistinguishable from the statistically 
unremarkable hapax legomena. 
Panel (b) of Figure 7 displays the corresponding results when we use Fisher's exact 
test rather than MI. Instead of varying the MI cut-off value, we vary the significance 
level a. Note that the resulting F scores tend to be roughly twice as high as those 
obtained with MI-based extraction. As there are a number of very similar local maxima, 
the choice of window size and significance l vel should be based on the desired trade- 
off between precision and recall given the general distribution of the target words 
around the seed term. 2We conclude that, at least for the present word extraction task, 
Fisher's exact est compares favorably to mutual information (as does G2). 
All the analyses presented thus far are conditional analyses, in the sense that we 
compiled new corpora from the database of abstracts and from the newspaper corpus 
containing only relevant abstracts (containing the drug names captopril and enalapril 
as well as the term side-effect) and relevant sentences (containing the particle af and 
its verb to its left), respectively. The size of the complement was always determined 
with respect o these new conditional corpora, and not with respect o all MEDLINE 
2 Note that we manipulate the a-levels in the same way as the MI cut-off values. In the present 
technique, the a-level is a parameter that we vary to optimize extraction results for a training data set. 
Our use of a should be carefully distinguished from the function of preset a-levels when testing the 
significance of observed ifferences in experimentally obtained data. 
313 
Computational Linguistics Volume 26, Number 3 
Table 4 
General and specific 2 x 2 contingency tables for low-frequency words. 
Table (a) provides the general notation of the counts in a 2 x 2 
contingency table. In table (b), A = frequency of rare words (1, 2, 3 . . . .  ), 
W = number of words in window, C = number of words in complement. 
Corpus size N = W + C. 
(a): 1/11 /'/12 /'/1+ 
/'/21 /'/22 /'/2+ 
//+1 //+2 //+q- 
(b): A 
W-A  
W 
0 A 
C W+C-A 
C W+C 
abstracts or to the complete newspaper corpus. This raises the question of whether 
better results might have been obtained if the complete data sets had been used. In 
principle, more data might imply more power. At the same time, more data also entails 
the risk of more noise. At least for our af data, enlarging the complement leads to worse 
performance. When we allow any sentence that contains af in our analyses, F decreases 
from 0.31 to 0.23 for G 2. When we base the analyses on the complete newspaper corpus, 
F reduces further to 0.19. The reason for this decrease in performance is probably due to 
the W/C-ratio being very low for all practical window sizes, i.e., at the very left part of 
the saw-tooth-shaped pattern characterizing Nsig as a function of W/C. Consequently, 
any low-frequency word is singled out as a significant item whenever it occurs at least 
once in the window. Given the Zipfian structure of word-frequency distributions, a
great many spurious low-frequency words are extracted. 
As mentioned in the introduction, the received wisdom is that the windowing 
method is unreliable for events with a frequency of less than 5. By means of an 
analysis of the behavior of statistical tests for 2 x 2 contingency tables with sparse 
data, a method for optimizing the use of these tests has been developed. We hope 
that this technique will prove to be useful for domains in which the extraction of 
low-probability events is crucial. 
Appendix 
Log-Likelihood Ratio 
For the general contingency table, table (a) in Table 4, the log-likelihood ratio is defined 
by (Agresti 1990): 
G 2 = 2 ~_, ~ nijin(nij/mq), 
i j 
where rhq = ni+n+j/n++. When we use the specific contingency table for hapax legom- 
ena, table (b) in Table 4, we obtain for a specific G 2 of X the formula: 
W+C (W-A) (W+C)  W+C 
X/2 = A ln~+(W-A) ln  W(W+C-A)  +CInw+C-A"  
= In (W-  A) w-A - InW w + In(W q- C) w+C - In(W q- C - A) w+C-A, 
= In (W-  A)W-A(w q- C) w+C 
wW(w q- C - A) w+C-A ' 
(W - A)W-A(w q- C) w+C eX/2 
wW(w + C - A) w+c-A " 
314 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
We rewrite the latter equation to: 
eX/2w w 
(W-A) w-A 
eX/2wW(w - -  A) A 
(W-A)  w 
(W + C) w+c 
(W + C - A) w+c-A' 
(w + c)W(w + c)c(w + c - A?  
(W + C - A)W(W + C - A) c 
Because W >> A and therefore W + C >> A, we rewrite the formula above as follows: 
eX/2 W w W A (w + c)W(w + c)C(w + c) A 
w w (w + c)W(w + c)  c 
eX/2w A = (W ? C) A, 
~ W  = W+ C. 
So that the ratio is: 
W 1 
C ~ - 1 '  
When N > 10,000, the error of this equation is smaller than 0.001. 
Fisher's Exact Test 
With Fisher's exact est, the observed marginal totals are used to compute the hyper- 
geometric distribution, which is defined for the general 2 x 2 table, table (a) of Table 4, 
as (Agresti 1990): 
?/2+ ) (n '+) (n+ , rill - -  F / l l  
n+l 
The probability of every possible table with given marginal totals adds to 1. We use 
Fisher's exact est that sums the hypergeometric probabilities of all tables with prob- 
abilities less than or equal to the observed table. With B -- 0, table (b) in Table 4 is the 
only table we are interested in so that the probability P for this contingency table is: 
P 
A C-A  
W-A  ) 
(w + c - A)! 
(w + c)~ ' 
W!C! 
W~(W + C - A)~ 
(w - A)~(W + C)~" 
W(W- 1)... (W-A  + 1)(W-A)! (W + C - A)! 
(w-a)~ (w+ c ) (w+ c -  1 ) . . . (w+ c -A  + 1)(w+ c -A) !  
315 
Computational Linguistics Volume 26, Number 3 
Because A = 1,2 ,3 , . . . ,  W >> A and therefore W + C >> A, we allow ourselves to 
formulate W! = wA(w - A)! and (W + C)! = (W + c)A(w + C - A)!. We therefore 
rewrite Fisher's exact test as follows: 
The W/C-ratio is then: 
wAw (w + 
P = 
(W + c )Aw!(w + C)!' 
WA 
(W + C) 
W - 
W+C" 
w 
C 1-  ~Y-P" 
When N > 20,000, the error of this equation is smaller than 0.001. 
Practical Issues Using Fisher's Exact Test. We used a network algorithm to compute 
Fisher's exact est (Mehta and Patel 1986; Clarkson, Fan, and Joe 1993). This algorithm 
is computationally intensive, but since many words have the same table, only a few 
tables have to be computed and their results can be cached. It takes an average of 50 
seconds to compute one window size in a 100,000 word corpus on a Pentium 133MHz, 
48MB Linux machine. 
Source code for the algorithm can be found at: http://www, acm. org/pubs/citations/ 
j ournals/toms/1986-12-2/p154-meht a/ 
Mutual Information 
Given the definition of Mutual Information (Church and Hanks 1990), 
I(x,y) = log 2 P(x,y) 
P(x)P(y)" 
we consider the distribution of a window word according to the contingency table (a) 
in Table 4. P(x) is the relative frequency of the target word, P(y) is the relative frequency 
of the seed term, and P(x,y) is the frequency of the target word in the window. In 
terms of the contingency table, we have: 
/'/11 
I(x,y) = log 2 n++ 
//1+ S ' 
f /++ 7"/++ 
where S is the frequency of the seed. Substituting nn = nl+ - n12, we find that 
/11+ - -  F/12 
I(x,y) = log 2 n++ nl+ S ' 
/ /++ / /++ 
1 
= log 2 n++ 
//1+ S ' 
n++(nl+ -- nu) " n++ 
316 
Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 
= log2(n++) - log2(S) - log2(nl+) + log2(nl+ - n12). 
For a given corpus and extraction task, corpus size (n++) and the frequency of the 
seed term S are fixed, so that we can write 
I(x,y) = C - log2(nl+) + log2(nl+ - n12). 
As n12 K nl+, I(x,y) reaches its maximum value (C) when n12 = 0, i.e., when all 
instances of the target word are in the window, irrespective of the frequency of the 
target. 
Acknowledgments 
We are indebted to three anonymous 
reviewers whose criticisms have led to 
substantial improvements. This study was 
financially supported by the Dutch National 
Research Council NWO (PIONIER grant to 
the third author). 
References 
Agresti, Alan. 1990. Categorical Data Analysis. 
John Wiley & Sons, New York. 
Church, Kenneth W. and Patrick Hanks. 
1990. Word association norms, mutual 
information, and lexicography. 
Computational Linguistics, 16(1):22-29. 
Clarkson, Douglas B., Yuan-An Fan, and 
Harry Joe. 1993. A remark on algorithm 
643: FEXACT: An algorithm for 
performing Fisher's exact est in r x c 
contingency tables. ACM Transactions on 
Mathematical Software, 19(4):484-488. 
Dunning, Ted. 1993. Accurate methods for 
the statistics of surprise and coincidence. 
Computational Linguistics, 19(1):61-74. 
Manning, Christopher D. and Hinrich 
Schiitze, 1999. Foundations of Statistical 
Natural Language Processing, chapter 5, 
Collocations. The MIT Press, Cambridge, 
MA. 
Mehta, Cyrus R. and Nitin R. Patel. 1986. 
Algorithm 643. FEXACT: A fortran 
subroutine for Fisher's exact est on 
unordered r x c contingency tables. ACM 
Transactions on Mathematical Software, 
12(2):154-161. 
Pedersen, Ted. 1996. Fishing for exactness. 
In Proceedings ofthe South-Central SAS 
Users Group Conference, pages 188-200, 
Austin, TX. 
Pedersen, Ted, Mehmet Kayaalp, and 
Rebecca Bruce. 1996. Significant lexical 
relationships. In Proceedings ofthe 13th 
National Conference on Artificial Intelligence. 
AAAI Press/The MIT Press, Menlo Park, 
CA, pages 455-460. 
Rijsbergen, Cornelis J. van. 1979. Information 
Retrieval. Second edition. Butterworths, 
London. 
Rikken, Floor and Rein Vos. 1995. How 
adverse drug reactions can play a role in 
innovative drug research. Pharmacy World 
and Science, 17(6):195-200. 
Smadja, Frank. 1993. Retrieving collocations 
from text: Xtract. Computational Linguistics, 
19(1):143-177. 
317 

Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 41?52,
Dublin, Ireland, August 24 2014.
Electrophysiological correlates of noun-noun compound processing by
non-native speakers of English
Cecile De Cat
1
, Ekaterini Klepousniotou
2
, Harald Baayen
3
1
Linguistics & Phonetics, University of Leeds, UK
c.decat@leeds.ac.uk
2
Institute for Psychological Sciences, University of Leeds, UK
e.klepousniotou@leeds.ac.uk
3
Quantitative Linguistics, University of T?bingen, Germany
harald.baayen@uni-tuebingen.de
Abstract
We report on an experimental study of the processing of noun-noun compounds by native and
non-native speakers of English, based on Event-Related Potentials recorded during a mask-
primed lexical decision task. Analysis was by generalised linear mixed-effect modelling and
generalised additive mixed modelling. Non-native processing is found to display headedness
effects induced by the mothertongue. The frequency of the constituent nouns and of the in-
tended compounds are also shown to have an effect on processing.
1 Introduction
This study examines the processing of noun-noun compounds by native and non-native speakers of
English. Compounds have been extensively studied in the past 40 years from a myriad of viewpoints
(Libben and Jarema, 2006; Lieber and ?tekauer, 2009). A key concern has been whether the pro-
cessing of compounds consists in retrieving entities listed in the mind (Butterworth, 1983) or requires
decomposition into constituents listed separately (Semenza et al., 1997; Libben, 1998). Dual-routes
theories contend that the two processes exist side by side (Sandra, 1990). It is now widely accepted
that both constituents are activated during processing, at least in non-lexicalised compounds (Jarema,
2006; Zhang et al., 2012). Noun-noun compounds have also been shown to be processed differently
to non-compounds of similar morphological complexity and length, with compounds yielding longer
reaction times and different electrophysiological correlates (El Yagoubi et al., 2008).
Endocentric compounds contain a head element (dust in (1)) whose lexical category and interpretive
features are inherited by the compound and contribute the core of its meaning (e.g. a kind of dust).
The other element acts as a modifier of that head.
(1) moon dust (?dust from the moon /dust made of moon /dust with moon-like properties?)
Here we focus on endocentric noun-noun compounds (henceforth NNCs), which have been argued to
embody an underlying structure (Libben, 2006) that is hierarchical, involving the (possibly recursive)
subordination of a modifier to a grammatical head (or a modifier-head compound, as in (2)), with head-
directionality that mirrors that of other noun-complement structures in the same language (Zipser,
2013).
(2) [child [amateur [puppet theatre]]]
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
41
Headedness plays a specific role in the processing of NNCs, as demonstrated by research on Italian
(which crucially features the two word orders in NNCs). Based on a lexical decision task, El Yagoubi
et al. (2008) found priming effects induced by the head, independently of its position in the NNC.
Headedeness effects are not distinguishable from position-in-the-string effects in languages such as
English. For instance, Jarema et al. (1999) observed no difference in the priming of NNCs by the head
or the modifier. Here we take this line of research further, by investigating whether headedness in the
mothertongue affects the processing of transparent, irreversible NNCs in highly advanced second
language learners of English.
Event-related potentials (ERPs) can provide insight into the neural activity associated with the pro-
cessing of compounds. Functional interpretations can be inferred from the temporal and spatial char-
acteristics of electromagnetic activity, and ERP components can sometimes reveal the engagement of
the cognitive processes involved. Our approach is this paper is exploratory (Otten and Rugg, 2005)
and will focus on identifying differences in the amplitude of the EEG signal that can be traced back
to properties of the participants (such as their language background) and properties of the compounds
(such as their frequency of occurrence, and the frequencies of occurrence of their constituents). Infer-
ences based on previously identified ERP components will be drawn in the discussion as appropriate.
Our research questions are: (i) Does non-native processing of NNCs result in different ERP signatures
to native processing? (ii) Is non-native processing of NNCs affected by headedness effects from the
mothertongue?
2 Materials and methods
We registered the electrophysiological response of the brain to visual stimuli presented in the context
of a (masked) primed lexical decision task. Stimuli were irreversible NNCs presented in licit (3-a)
and reversed order (3-b).
(3) a. coal dust
b. #dust coal
The participant groups differed in mothertongue: English (control group), Spanish or German (exper-
imental groups). Like English, German features productive compounding, with a head-last structure.
Whereas in Spanish, compounds are essentially head-first, and not productive.
2.1 Participants
Ten native British English speakers (4 female, mean age 22;11 years; STD 3;3 years), ten native
German learners of English (7 female, mean age 26;5 years; STD 5;7 years) and ten native Spanish
learners of English (3 female, mean age 26;11 years; STD 5;3 years) took part in the study. Partic-
ipants all had initial second-language exposure after 8 years of age, and all scored above 60% on a
cloze test from the Cambridge Certificate in Advanced English. All were right-handed based on the
Briggs and Nebes inventory (Briggs and Nebes, 1975), had no speech or language difficulties and had
normal or corrected-to normal vision.
2.2 Stimuli
Experimental stimuli consisted of prime-target pairs, presented in 4 experimental conditions in a 3
(Group) x 2 (Prime Condition) x 2 (Word Order) design. The prime was either the head (e.g. dust in
(3)) or the modifier (e.g. coal in (3)) of the intended compound.
The Word Order factor had 2 levels: licit (modifier - head, as in (3-a)) or reversed (head - modifier,
as in (3-b)). All the NNCs were endocentric and featured a transparent, modification relationship. All
42
items were tested for irreversibility on an independent group of 30 native speakers. The frequency
of the licit compounds and their constituent nouns was estimated from the post-1990 data in Google
N-grams. To avoid lexicalisation effects, only compounds with very low frequencies were included
(i.e. below 3,300 ? mean = 359.5, compared with a mean of 279,300 for the constituent nouns).
There was a total of 480 test items (based on 120 compounds), of which 240 are included in the
present study (as we focus on the Head Prime condition only). The items were pseudo-randomised
into 8 different orders (assigned randomly to participants) and presented in 4 blocks, with a rest in
between.
2.3 Procedure
Participants were tested individually in a single session lasting approximately 1.5 hours. Stimuli
were presented visually in light grey text on a black background. Each trial began with the visual
presentation of a series of exclamation marks (!!!) for 1000 ms, which was a signal for the participant
to rest their eyes and blink After a delay of 100 ms a fixation point (+) was presented for 250 ms to
signal that the trial was about to begin. After a 100 ms mask (#######), the prime was presented for
100 ms followed by a second mask (for 50 ms) and the target (for 1000 ms). After a delay of 500 ms a
question mark (?) appeared for 2000 ms during which time participants had to make a lexical decision
about the target (as acceptable or not) by pressing (with their right hand) one of two buttons on a
hand-held button box (counterbalanced across participants). Participants were instructed to respond
as accurately as possible; accuracy and reaction times (in ms from the onset of the ??") were recorded.
After the response (or at the end of 2000 ms if the participant did not respond), there was a delay
of 100 ms before the next trial started. The experimental session was preceded by a practice session
comprising 20 trials, which was repeated until participants could perform the task and procedure with
no errors (usually one or two practice sessions sufficed).
The EEG was recorded (Neuroscan Synamps2) from 60 Ag/AgCl electrodes embedded in a cap based
on the extended version of the International 10-20 positioning system (Sharbrough et al., 1991). Ad-
ditional electrodes were placed on the left and right mastoids. Data were recorded using a central
reference electrode placed between Cz and CPz. The ground electrode was positioned between Fz
and Fpz. To capture noise articfacts in the EEG signal due to eye movements, electro-oculograms
(EOGs) were recorded using electrodes positioned at either side of the eyes, and above and below the
left eye. At the beginning of the experiment electrode impedances were below 10 k?. The analogue
EEG and EOG recordings were amplified (band pass filter 0.1 to 100Hz), and continuously digitised
(32-bit) at a sampling frequency of 500 Hz. Data were processed offline using Neuroscan Edit 4.3
software (Compumedics Neuroscan) and filtered (0.1-40Hz, 96 dB/Oct, Butterworth zero phase filter).
The effect of eye-blink artifacts was minimised by estimating and correcting their contribution to the
EEG using a regression procedure which involves calculating an average blink from 32 blinks for each
participant, and removing the contribution of the blink from all other channels on a point-by-point ba-
sis. Data were epoched between -100 and 1100 ms relative to the onset of the experimental targets
and baseline-corrected by subtracting the mean amplitude over the pre-stimulus interval. Epochs were
rejected if participants did not make a response within the allocated time (during presentation of the
??"), or if they made an incorrect response. Subsequently the data was downsampled to 125 Hz. Trial
rejection was not done a priori but based on the residuals of the modelling, resulting in only 0.7% of
discarded data.
43
3 Results
3.1 Accuracy analysis
The responses on the lexical decision task were analysed with a generalised linear mixed-effect model
with a binomial link function, using the lme4 package, version 1.0-4 (Bates et al., 2013) with the
?bobyqa? optimizer. Only those predictors that contributed to the model fit were retained, as shown in
Table 1. The covariate ?Compound Frequency? did not reach significance. The model provided a sub-
stantially improved fit compared to the null-hypothesis model with random intercepts for participant
and item only.
Coefficient Std. Error Z p
Intercept -0.7565 1.7174 -0.4405 0.6596
Word Order: Licit -0.0828 0.1644 -0.5035 0.6146
L1: German -0.6339 0.3123 -2.0299 0.0424
L1: Spanish -0.7670 0.4135 -1.8549 0.0636
Proficiency 3.7191 1.7052 2.1811 0.0292
Word Order: Licit by L1: German 0.8710 0.1474 5.9074 0.0000
Word Order: Licit by L1: Spanish 0.9322 0.1410 6.6101 0.0000
Table 1: Coefficients of a logistic mixed-effects regression model fitted to the accuracy data. The
reference level for Word Order is Reversed, and for L1: English
Table 1 indicates that for English speakers, accuracy did not differ for the licit and reversed word order
conditions. For non-native speakers, accuracy was higher in the Licit Word Order condition, compared
with the ReversedWord Order condition. Across groups, greater proficiency afforded higher accuracy.
Figure 1 visualizes this pattern of results.
?
?
?
0.8
8
0.9
0
0.9
2
0.9
4
Mothertongue
Res
pon
se A
ccu
rac
y
English German Spanish
Reversed
?
? ?
Wo
rd.O
rde
r
Licit
0.70 0.75 0.80 0.85 0.90 0.95 1.00
0.8
6
0.9
0
0.9
4
Proficiency
Res
pon
se A
ccu
rac
y
Reversed
Wo
rd.O
rde
r
Licit
Figure 1: Partial effects of the predictors in the logistic model for response accuracy.
3.2 ERP analysis
We analysed the electrophysiological response elicited by the presentation of compound words with
the generalized additive mixed model (GAMM, (Wood, 2006; Tremblay and Baayen, 2010; Baayen, to
appear; Baayen et al., in preparation; Kryuchkova et al., 2012)). Generalized additive mixed models
extend the generalized linear mixed model with tools (thin plate regression splines, tensor product
smooths) for modeling non-linear functional relations between one or more predictors and a response
variable. GAMMs, as implemented in the mgcv package 1.7-28, offer three important advantages for
the analysis of EEG data compared to standard linear models and analysis of variance. First, GAMMs
are optimized for dealing with non-linear functional relations between a response (here, the amplitude)
44
and one or more numerical predictors (resulting in wiggly curves, wiggly surfaces, or, in the case of
more than two predictors, wiggly hypersurfaces). Second, GAMMs decompose the EEG amplitude
into a sequence of additive components, thereby affording the analyst a toolkit for separating out
partial effects due to different kinds of predictors (e.g., language group, time, compound frequency,
constituent frequency). Third, GAMMs can capture AR1 autocorrelative processes in the signal, and
therefore protect against anti-conservative p-values and mistakingly taking noise for complex EPR
signatures (as has been shown to occur by Tanner et al., 2013).
We include for analysis only trials that elicited a correct response. The time window analysed was
limited to 0?800 ms, time-locked to the onset of stimulus presentation. Autocorrelations in the resid-
ual error were removed by including in the GAMM an autocorrelation parameter ? = 0.9 for AR1
error for each basic time series in the data (the time series amplitudes for each unique combination of
subject and item). Inclusion of ? was essential for removing most of the autocorrelational structure
from the model?s residuals.
A. parametric coefficients Estimate Std. Error t-value p-value
Intercept (English Reversed) -0.6815 1.8695 -0.3645 0.7155
Compound Frequency (English Reversed) 0.0659 0.0756 0.8711 0.3837
English:Licit 1.0720 0.1845 5.8103 < 0.0001
German:Reversed 0.6172 2.5967 0.2377 0.8121
German:Licit 0.8199 2.5977 0.3156 0.7523
Spanish:Reversed 0.0311 2.5986 0.0120 0.9905
Spanish:Licit -3.6624 2.6002 -1.4085 0.1590
Comp. Frequency:English Licit -0.2747 0.0392 -7.0097 < 0.0001
Comp. Frequency:German Reversed -0.0577 0.0405 -1.4254 0.1540
Comp. Frequency:German Licit -0.0826 0.0397 -2.0837 0.0372
Comp. Frequency:Spanish Reversed -0.1139 0.0414 -2.7536 0.0059
Comp. Frequency:Spanish Licit 0.2361 0.0404 5.8473 < 0.0001
B. smooth terms edf Ref.df F-value p-value
smooth in Time English:Licit 8.5809 8.7860 11.5648 < 0.0001
diff. curve Time: German:Licit 1.0111 1.0212 0.1285 0.7255
diff. curve Time: Spanish:Licit 6.7504 7.8925 4.4964 < 0.0001
diff. curve Time: English:Reversed 1.9025 2.3906 1.0696 0.3436
diff. curve Time: German:Reversed 1.0074 1.0141 0.4174 0.5210
diff. curve Time: Spanish:Reversed 1.0069 1.0095 1.6952 0.1925
tensor product surface F1 and F2 (English, Licit) 3.0189 3.0349 2.1154 0.0951
diff. surface German:Licit 11.2569 12.3579 7.2697 < 0.0001
diff. surface Spanish:Licit 12.9312 13.6137 60.1585 < 0.0001
diff. surface English:Reversed 3.9839 4.0083 17.6082 < 0.0001
diff. surface German:Reversed 9.0655 10.4566 5.5875 < 0.0001
diff. surface Spanish:Reversed 14.7736 14.9639 28.2189 < 0.0001
random intercepts Compound 107.6142 111.0000 34.7869 < 0.0001
by-subject random wiggly curves Trial 163.4484 267.0000 43.8796 < 0.0001
by-subject random wiggly curves Time 170.5793 267.0000 2.4442 < 0.0001
Table 2: Generalized additive mixed model fitted to the amplitude of the electrophysiological response
of the brain to English compounds at channel C1.
In what follows, we focus on channel C1, which revealed a pattern of results typical for surrounding
channels. The amplitude of the EEG signal was modeled (without any prior averaging) as an additive
function of Word order (Licit vs. Reversed), Compound Frequency, the Constituent Frequency of
Modifier and of Head, and Participant Group (English, German, Spanish). Proficiency did not reach
significance and did not improve the model fit significantly, so we did not include this predictor in the
final model.
GAMMs currently can only accomodate interactions of smooths with a single factor. In order to
study the interaction of speaker group and word order, we therefore created a new factor GO with
45
as levels English:Licit, English:Reversed, German:Licit, German:Reversed, Spanish:Licit, and Span-
ish:Reversed, using treatment contrasts with as reference level English:Reversed. In the parametric
part of the model (the upper half of Table 2), the coefficients for the main effect of GO and its inter-
action with compound frequency are to be interpreted in the familiar way, with the interaction terms
specifying differences in the slope of compound frequency for the non-reference levels of GO.
GO also interacted with the constituent frequencies. For this three-way interaction, we recoded GO
as an ordered factor, which is how the bam function of the MGCV package is instructed to construct
a reference surface (in our implementation, for English:Licit) and difference surfaces for the other
factor levels with respect to the standard compound forms as read by English native speakers.
Table 2 summarizes the GAMM fitted to the amplitude of the EEG signal at channel C1. First consider
the parametric part of the model, presented in the upper half of the table, which concerns the main
effect of GO and its interaction with log-transformed compound frequency. This interaction is sum-
marized in Figure 2. Black lines denote the Licit Word Order condition, grey lines the Reversed Word
Order condition. Compound frequency did not have much of an effect in the Reversed conditions.
0 2 4 6 8
?5
?4
?3
?2
?1
0
1
Log Compound Frequency
Pa
rtia
l E
ffe
ct 
(Am
plit
ud
e)
Eng, Reversed
Eng, Licit
Germ, Reversed
Germ, Licit
Span, Reversed
Span, Licit
Figure 2: The three-way interaction of Participant Group, Grammaticality, and Compound Frequency.
For English (solid lines), a compound frequency is present in the licit condition, with greater com-
pound frequencies inducing more negative amplitudes. For German (dashed lines), the slope was
close to zero in both conditions, indicating the absence of a frequency effect. The Spanish speakers
(dotted lines) revealed a regression line with an opposite slope to that for the English speakers in the
Licit condition, and with a much lower intercept. This reversal of the slope, as compared to English,
may be a consequence of the fact that in Spanish, translation equivalents would be expressed with the
opposite constituent order.
The non-parametric part of the model, reported in the lower half of Table 2, handles non-linear effects
in the model, using thin plate regression splines for wiggly curves and tensor product smooths for wig-
gly surfaces. The first row of the non-parametric subtable summarizes a smooth in time for English
46
licit compounds. This smooth is visualized in the left panel of Figure 3, together with its 95% confi-
dence interval. The model required 8.78 effective degrees of freedom (edf) to capture a (significant)
positive inflection around 300 ms post stimulus onset. (Higher edfs indicate greater wiggliness.) The
next 5 rows in Table 2 describe the difference curves for the remaining levels of GO. The only level
for which this difference curve is significant is Spanish:Licit. The second panel of Figure 3 presents
this difference curve, which required 7.89 effective degrees of freedom. As the difference curve is
significantly above the X-axis around 300 ms post stimulus onset, and significantly below the X-axis
after 600 ms, we conclude that the Spanish speakers reading licit compounds had a higher positivity
around 300 ms compared to the English speakers reading the same compound, combined with more
negative amplitudes after 600 ms post stimulus.
0 200 400 600 800
?3
?2
?1
0
1
2
Time
am
pli
tud
e
English, Licit
0 200 400 600 800
?3
?2
?1
0
1
2
Time
dif
fer
en
ce
 in
 am
pli
tud
e
Spanish, Licit
Figure 3: The interaction of participant Group, Grammaticality, and Time. The left panel shows the
smooth for English in the Licit Word Order condition; the right panel shows the difference curve with
respect to the left panel for the Spanish participants.
EEG amplitudes were also modulated by an interaction of the constituent frequencies by GO, which
we modeled with a tensor surface for English:Licit and difference tensor surfaces for the other levels
of GO. The second set of 6 rows in Table 2 present the summary statistics, and Figure 3 the smoothed
surfaces. The upper left panel presents the reference smooth for English native speakers reading com-
pounds in their licit order. For channel C1, this surface is not well-supported statistically (p = 0.095),
but at neighboring channels (e.g., Cz, FC1) higher-frequency constituents elicited significantly higher
amplitudes. Interestingly, when the constituents are reversed, significantly more negative amplitudes
for compounds with high constituent frequencies are observed for native English speakers, as shown
in the lower left panel. German speakers show a similar pattern with more negative amplitudes for
both licit and reversed compounds (center panels). The strongest negativities are present for Spanish
speakers in the licit condition (upper right). In the reversed condition, Spanish speakers show a pat-
tern of somewhat increased negativity (lower right) that, however, does not vary much with constituent
frequency.
47
Figure 4: The interaction of left and right constituent frequency by grammaticality and language
group. The upper left panel presents the smooth surface for English:Licit, the remaining panels present
difference surfaces with respect to the English:Licit condition. Darker shades of gray indicate more
negative amplitudes. Contour lines are 0.5 units apart in panels 1, 2, 5, and 6; they are 2 units apart in
panel 3, and 1 unit apart in panel 4.
The final three rows of Table 2 specify the random-effects structure of the model. Random intercepts
for compound were included in order to allow for differences in baseline amplitude across compounds.
For subjects, two random wiggly curves were included. The first models changes in amplitude as
subjects go through the experiment. The second models subject-specific changes over within-trial
time. The random wiggly curves are the nonlinear equivalent of what in the context of a linear mixed-
effects model would be ?random straight lines? obtained by combining random intercepts with random
slopes. For EEG data, where amplitude changes non-linearly with time, the flexibility of penalized
and shrunk regression splines is essential.
4 Discussion
The non-native participants performed the lexical decision task with a high level of accuracy. For
the licit compounds, accuracy was comparable to that of native speakers. For reversed compounds,
accuracy dropped slightly, from around 94% to around 88%. From this, we conclude, first, that all
subjects have acquired NNC structures in English, and second, that non-native speakers are more
likely to accept novel noun combinations as English compounds.
Knowledge of whether a two-word combination is in fact licit in English can arise from two sources.
On the one hand, speakers may be familiar with the compound, as evidenced by an effect of compound
frequency. For the native English speakers resonding to licit compounds, an effect of compound fre-
48
quency was indeed present in the EEG amplitudes. On the other hand, speakers may infer the intended
meaning from the constituents (e.g., English beach ball indexing German Wasserball, ?water ball?).
Constituent effects were well attested in the EEG amplitudes. Interestingly, for English speakers, con-
stituent frequency effects gave rise to more positive amplitudes in the licit condition (significantly at
neighboring channels) whereas in the reversed condition, amplitudes were more negative for higher-
frequency constituents. In other words, when English speakers are confronted with reversed com-
pounds, which for them are actually novel compounds, the compound frequency effect disappears,
and a constituent frequency effect emerges that is opposite in sign to that for normal compounds.
Of the non-native speakers of English, only the Spanish speakers revealed a compound frequency
effect, with a slope opposite in sign to that for the English speakers. If higher amplitude in the signal
indicates increased processing effort, the effect of the frequency of the intended compound could be
interpreted as facilitating in the Licit Word Order condition in the native speakers but inhibiting in the
Spanish group (and without much effect in the German group). We hypothesize that Spanish speakers
find licit English compounds more difficult precisely because in their native language, the order of
the constituents would have been reversed. It is only these speakers that have a word order conflict to
resolve.
All speakers (non-native as well as English) responding to reversed (i.e., for them, novel) compounds,
show more negative amplitudes for compounds with higher constituent frequencies. We interpret this
as evidence for constituent-driven, decompositional processing. The especially pronounced negativi-
ties for Spanish speakers in the Licit Word Order context (which go hand in hand with a positive slope
for compound frequency) suggest that for these speakers increased processing resources are called
upon to resolve the conflict between English and Spanish constituent order, in spite of native-like
performance in the evaluation of compounds in that condition.
A positive peak around 300 ms post-stimulus was found in all groups in both conditions, and exac-
erbated in the Spanish group in the Licit Word Order condition. This peak could be interpreted as a
P300, indexing attentional resources. El Yagoubi et al. (2008) found that right-headed NNCs in Ital-
ian yielded a greater P300 and interpreted this as evidence that processing this marked (but in Italian
equally grammatical) word order required increased attentional resources. If the P300 observed here
reflects a peak of attentional engagement, we expect its amplitude to predict scores on an Attention
Network Task (Fan et al., 2005) ? something we will investigate in the next phase of this study.
With respect to the absence of a significant N400 effect between the Word Order conditions, we first
note that the N400 may vanish due to familiarization, and also to masked priming (Coulson et al.,
2005; Brown and Hagoort, 1993). However, and perhaps more importantly, reversed compounds are
not semantically anomalous. To the contrary, they invite interpretation and, as we have documented,
give rise to constituent-driven processes of interpretation. From this perspective, an N400 would then
characterize the processing of semantic anomalies that cannot be resolved through morphological
processing.
5 Concluding remarks
This study set out to investigate (i) whether non-native processing of NNCs results in different ERP
signatures compared to native processing, and (ii) whether non-native processing of NNCs is affected
by constituent order in the mothertongue. Analysis of the EEG amplitudes revealed that English
native speakers read licit compounds using both whole-word information (as indexed by compound
frequency) in congruence with constituent information (as indexed by constituent frequency with
a positive effect) whereas non-native speakers and English speakers reading novel (reversed) com-
pounds resort to decompositional interpretation indexed by a negative effect on amplitudes. Further-
49
more, Spanish readers undergo interference from the different constituent order possibilities in their
own language, leading to a reversed compound frequency effect and strongly enhanced constituent
frequency effects (with a negative sign) when reading English licit compounds.
This pattern of results is, for native speakers, consistent with the early effects of compound frequency
observed using eye-tracking by, e.g., Kuperman et al. (2008, 2009) and Miwa et al. (2014) for English,
Finnish, and Japanese respectively. The importance of constituent-driven processing for non-native
speakers is reminiscent of the decompositional eye-movement patterns of less-proficient readers re-
ported by Kuperman & Van Dyke (2011).
We conclude with noting that the insights gleaned from the EEG amplitudes would not have been
possible without generalized additive mixed models. At the same time, we believe we are only seeing
the tip of the iceberg. For instance, the model can be improved by allowing the interaction of the con-
stituent frequencies by group and constituent order to vary with time, using a five-way tensor product
smooth. Two considerations have withheld us from following up on this considerably more com-
plex model. First, without specific hypotheses as a guide, interpretation becomes extremely difficult.
Second, we are concerned that with a relative small number of compounds (120), overfitting might
become an issue. For future research specifically addressing the development over time of constituent
(and whole-word) frequency effects, we recommend designs with larger numbers of compounds.
6 Acknowledgments
This project was financed by pump-priming funds from the University of Leeds? Faculty of Arts and
by a British Academy Quantitative Skills Acquisition award (SQ120066) to the first author. Many
thanks to Antoine Tremblay for help with the initial data preparation, to Cyrus Shaoul for friendly
technical and coding advice, to Jacolien van Rij for helpful suggestions for the gamm analysis and
to Raphael Morschett, Chris Norton, Kremena Koleva and Natasha Rust for the data collection and
pre-processing.
References
R.Harald Baayen, Jacolien van Rij, C?cile De Cat, and Simon Wood. in preparation. Autocorrelated errors in
experimental data in the language sciences: Some solutions offered by generalized additive mixed models.
R. Harald Baayen. to appear. Analyzing Linguistic Data. A Practical Introduction to Statistics Using R (second,
augmented edition). CUP, Cambridge.
Douglas Bates, Martin Maechler, Ben Bolker, and Steven Walker, 2013. lme4: Linear mixed-effects models
using Eigen and S4. R package version 1.0-4.
G.G. Briggs and R.D. Nebes. 1975. Patterns of hand preference in a student population. Cortex, 11:230?238.
Colin Brown and Peter Hagoort. 1993. The processing nature of the n400: Evidence from masked priming.
Journal of Cognitive Neuroscience, 5(1):34?44.
B. Butterworth. 1983. Lexical representation. In B. Butterworth, editor, Language Production, pages 257?294.
Academic Press, San Diego, CA.
S. Coulson, Kara D. Federmeier, C. Van Petten, and Marta Kutas. 2005. Right hemisphere sensitivity to word-
and sentence-level context: Evidence from event-related brain potentials. Journal of Experimental Psychology:
Learning, Memory, & Cognition, 31:129?147.
Radouane El Yagoubi, Valentina Chiarelli, Sara Mondini, Gelsomina Perrone, Morena Danieli, and Carlo Se-
menza. 2008. Neural correlates of Italian nominal compounds and potential impacts of headedness effect: An
ERP study. Cognitive Neuropsychology, 25(4):559?581.
50
Jin Fan, Bruce D. McCandliss, John Fossella, Jonathan I. Flombaum, and Michael Posner. 2005. The activation
of attentional networks. NeuroImage, 26(2):471?479.
Gonia Jarema, C. Busson, R. Nikolova, K. Tsapkini, and Gary Libben. 1999. Processing compounds: A
cross-linguistic study. Brain and Language, 68:362?369.
Gonia Jarema. 2006. Compound representation and processing: A cross-language perspective. In Gary Libben
and Gonia Jarema, editors, The Representation and Processing of CompoundWords, pages 45?70. OUP, Oxford.
T. Kryuchkova, B. V. Tucker, L. Wurm, and R. H. Baayen. 2012. Danger and usefulness in auditory lexical
processing: evidence from electroencephalography. Brain and Language, 122:81?91.
V. Kuperman and J.A. Van Dyke. 2011. Effects of individual differences in verbal skills on eye-movement
patterns duing sentence reading. Journal of memory and language, 65(1):42?73.
V. Kuperman, R. Bertram, and R. H. Baayen. 2008. Morphological dynamics in compound processing. Lan-
guage and Cognitive Processes, 23:1089?1132.
V. Kuperman, R. Schreuder, R. Bertram, and R. H. Baayen. 2009. Reading of multimorphemic Dutch com-
pounds: Towards a multiple route model of lexical processing. Journal of Experimental Psychology: HPP,
35:876?895.
Gary Libben and Gonia Jarema, editors. 2006. The Representation and Processing of Compound Words. OUP,
Oxford.
Gary Libben. 1998. Semantic transparency in the processing of compounds. Brain and Language, 61:30?44.
Gary Libben. 2006. Why study compound processing? An overview of the issues. In Gary Libben and Gonia
Jarema, editors, The Representation and Processing of Compound Words, pages 1?22. OUP, Oxford.
Rochelle Lieber and Pavol ?tekauer, editors. 2009. The Oxford Handbook of Compounding. Oxford University
Press, Oxford.
Koji Miwa, Gary Libben, Ton Dijkstra, and Harald Baayen. 2014. The time-course of lexical activation in
japanese morphographic word recognitin: Evidence for a character-driven processing model. The Quarterly
Journal of Experimental Psychology, 67(1):79?113.
Leun Otten and Michael Rugg. 2005. Interpreting event-related brain potentials. In Todd Handy, editor,
Event-related potentials: A methods handbook, pages 3?17. MIT Press, Cambridge, MA.
D. Sandra. 1990. On the representation and processing of compound words: Automatic access to constituent
morphemes does not occur. The Quarterly Journal of Experimental Psychology, 42A:529?567.
Carlo Semenza, C. Luzzatti, and S. Carabelli. 1997. Morphological represntation of compound nouns: A study
on Italian aphasic patients. Journal of Neurolinguistics, 10:33?43.
F. Sharbrough, G.E. Chatrian, R.P. Lesser, H. Luders, M. Nuwer, and T.W. Picton. 1991. American electroen-
cephalographic society guidelines for standard electrode position nomenclature. Journal of Clinical Neuro-
physiology, 8:200?202.
Darren Tanner, Kayo Inoue, and Lee Osterhout. 2013. Brain-based individual differences in online L2 gram-
matical comprehension. Bilingualism: Language and Cognition, 17(2):277?293.
Antoine Tremblay and R. Harald Baayen. 2010. Holistic processing of regular four-word sequences: A behav-
ioral and ERP study of the effects of structure, frequency, and probability on immediate free recall. In D. Wood,
editor, Perspectives on formulaic language: Acquisition and communication, pages 151?173. The Continuum
International Publishing Group., London.
Simon Wood. 2006. Generalised additive models: An introduction with R. Chapman and Hall/CRC, Boca
Raton, FL.
51
J. I. E. Zhang, Richard C. Anderson, Qiuying Wang, Jerome Packard, Xinchun Wu, Shan Tang, and Xiaoling
Ke. 2012. Insight into the structure of compound words among speakers of chinese and english. Applied
Psycholinguistics, 33(4):753?779.
Katharina Zipser. 2013. Proto-language, phrase structure and nominal compounds. Which of them fit together?
In Poster presented at ICL 2013, Geneva.
52
