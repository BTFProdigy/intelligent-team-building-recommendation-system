Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 123?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 17: All-words Word Sense Disambiguation
on a Specific Domain
Eneko Agirre
IXA NLP group
UBC
Donostia, Basque Country
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
UBC
Donostia, Basque Country
oier.lopezdelacalle@ehu.es
Christiane Fellbaum
Department of Computer Science
Princeton University
Princeton, USA
fellbaum@princeton.edu
Andrea Marchetti
IIT
CNR
Pisa, Italy
andrea.marchetti@iit.cnr.it
Antonio Toral
ILC
CNR
Pisa, Italy
antonio.toral@ilc.cnr.it
Piek Vossen
Faculteit der Letteren
Vrije Universiteit Amsterdam
Amsterdam, Netherlands
p.vossen@let.vu.nl
Abstract
Domain portability and adaptation of NLP
components and Word Sense Disambiguation
systems present new challenges. The diffi-
culties found by supervised systems to adapt
might change the way we assess the strengths
and weaknesses of supervised and knowledge-
based WSD systems. Unfortunately, all ex-
isting evaluation datasets for specific domains
are lexical-sample corpora. With this paper
we want to motivate the creation of an all-
words test dataset for WSD on the environ-
ment domain in several languages, and present
the overall design of this SemEval task.
1 Introduction
Word Sense Disambiguation (WSD) competitions
have focused on general domain texts, as attested
in the last Senseval and Semeval competitions (Kil-
garriff, 2001; Mihalcea et al, 2004; Pradhan et al,
2007). Specific domains pose fresh challenges to
WSD systems: the context in which the senses occur
might change, distributions and predominant senses
vary, some words tend to occur in fewer senses in
specific domains, and new senses and terms might
be involved. Both supervised and knowledge-based
systems are affected by these issues: while the first
suffer from different context and sense priors, the
later suffer from lack of coverage of domain-related
words and information.
Domain adaptation of supervised techniques is a
hot issue in Natural Language Processing, includ-
ing Word Sense Disambiguation. Supervised Word
Sense Disambiguation systems trained on general
corpora are known to perform worse when applied
to specific domains (Escudero et al, 2000; Mart??nez
and Agirre, 2000), and domain adaptation tech-
niques have been proposed as a solution to this prob-
lem with mixed results.
Current research on applying WSD to specific do-
mains has been evaluated on three available lexical-
sample datasets (Ng and Lee, 1996; Weeber et al,
2001; Koeling et al, 2005). This kind of dataset
contains hand-labeled examples for a handful of se-
lected target words. As the systems are evaluated on
a few words, the actual performance of the systems
over complete texts can not be measured. Differ-
ences in behavior of WSD systems when applied to
lexical-sample and all-words datasets have been ob-
served on previous Senseval and Semeval competi-
tions (Kilgarriff, 2001; Mihalcea et al, 2004; Prad-
han et al, 2007): supervised systems attain results
on the high 80?s and beat the most frequent base-
line by a large margin for lexical-sample datasets,
but results on the all-words datasets were much more
modest, on the low 70?s, and a few points above the
most frequent baseline.
Thus, the behaviour of WSD systems on domain-
specific texts is largely unknown. While some words
could be supposed to behave in similar ways, and
thus be amenable to be properly treated by a generic
123
WSD algorithm, other words have senses closely
linked to the domain, and might be disambiguated
using purpose-built domain adaptation strategies (cf.
Section 4). While it seems that domain-specific
WSD might be a tougher problem than generic
WSD, it might well be that domain-related words
are easier to disambiguate.
The main goal of this task is to provide a mul-
tilingual testbed to evaluate WSD systems when
faced with full-texts from a specific domain, that of
environment-related texts. The paper is structured
as follows. The next section presents current lexi-
cal sample datasets for domain-specific WSD. Sec-
tion 3 presents some possible settings for domain
adaptation. Section 4 reviews the state-of-the art in
domain-specific WSD. Section 5 presents the design
of our task, and finally, Section 6 draws some con-
clusions.
2 Specific domain datasets available
We will briefly present the three existing datasets
for domain-related studies in WSD, which are all
lexical-sample.
The most commonly used dataset is the Defense
Science Organization (DSO) corpus (Ng and Lee,
1996), which comprises sentences from two differ-
ent corpora. The first is the Wall Street Journal
(WSJ), which belongs to the financial domain, and
the second is the Brown Corpus (BC) which is a bal-
anced corpora of English usage. 191 polysemous
words (nouns and verbs) of high frequency in WSJ
and BC were selected and a total of 192,800 occur-
rences of these words were tagged with WordNet 1.5
senses, more than 1,000 instances per word in aver-
age. The examples from BC comprise 78,080 oc-
currences of word senses, and examples from WSJ
consist on 114,794 occurrences. In domain adapta-
tion experiments, the Brown Corpus examples play
the role of general corpora, and the examples from
the WSJ play the role of domain-specific examples.
Koeling et al (2005) present a corpus were the
examples are drawn from the balanced BNC cor-
pus (Leech, 1992) and the SPORTS and FINANCES
sections of the newswire Reuters corpus (Rose et al,
2002), comprising around 300 examples (roughly
100 from each of those corpora) for each of the 41
nouns. The nouns were selected because they were
salient in either the SPORTS or FINANCES domains,
or because they had senses linked to those domains.
The occurrences were hand-tagged with the senses
from WordNet version 1.7.1 (Fellbaum, 1998). In
domain adaptation experiments the BNC examples
play the role of general corpora, and the FINANCES
and SPORTS examples the role of two specific do-
main corpora.
Finally, a dataset for biomedicine was developed
by Weeber et al (2001), and has been used as
a benchmark by many independent groups. The
UMLS Metathesaurus was used to provide a set of
possible meanings for terms in biomedical text. 50
ambiguous terms which occur frequently in MED-
LINE were chosen for inclusion in the test set. 100
instances of each term were selected from citations
added to the MEDLINE database in 1998 and man-
ually disambiguated by 11 annotators. Twelve terms
were flagged as ?problematic? due to substantial dis-
agreement between the annotators. In addition to the
meanings defined in UMLS, annotators had the op-
tion of assigning a special tag (?none?) when none
of the UMLS meanings seemed appropriate.
Although these three corpora are useful for WSD
research, it is difficult to infer which would be the
performance of a WSD system on full texts. The
corpus of Koeling et al, for instance, only includes
words which where salient for the target domains,
but the behavior of WSD systems on other words
cannot be explored. We would also like to note that
while the biomedicine corpus tackles scholarly text
of a very specific domain, the WSJ part of the DSO
includes texts from a financially oriented newspaper,
but also includes news of general interest which have
no strict relation to the finance domain.
3 Possible settings for domain adaptation
When performing supervised WSD on specific do-
mains the first setting is to train on a general domain
data set and to test on the specific domain (source
setting). If performance would be optimal, this
would be the ideal solution, as it would show that a
generic WSD system is robust enough to tackle texts
from new domains, and domain adaptation would
not be necessary.
The second setting (target setting) would be to
train the WSD systems only using examples from
124
the target domain. If this would be the optimal set-
ting, it would show that there is no cost-effective
method for domain adaptation. WSD systems would
need fresh examples every time they were deployed
in new domains, and examples from general do-
mains could be discarded.
In the third setting, the WSD system is trained
with examples coming from both the general domain
and the specific domain. Good results in this setting
would show that supervised domain adaptation is
working, and that generic WSD systems can be sup-
plemented with hand-tagged examples from the tar-
get domain.
There is an additional setting, where a generic
WSD system is supplemented with untagged exam-
ples from the domain. Good results in this setting
would show that semi-supervised domain adapta-
tion works, and that generic WSD systems can be
supplemented with untagged examples from the tar-
get domain in order to improve their results.
Most of current all-words generic supervised
WSD systems take SemCor (Miller et al, 1993) as
their source corpus, i.e. they are trained on SemCor
examples and then applied to new examples. Sem-
Cor is the largest publicly available annotated cor-
pus. It?s mainly a subset of the Brown Corpus, plus
the novel The Red Badge of Courage. The Brown
corpus is balanced, yet not from the general domain,
as it comprises 500 documents drawn from differ-
ent domains, each approximately 2000 words long.
Although the Brown corpus is balanced, SemCor is
not, as the documents were not chosen at random.
4 State-of-the-art in WSD for specific
domains
Initial work on domain adaptation for WSD sys-
tems showed that WSD systems were not able to
obtain better results on the source or adaptation set-
tings compared to the target settings (Escudero et
al., 2000), showing that a generic WSD system (i.e.
based on hand-annotated examples from a generic
corpus) would not be useful when moved to new do-
mains.
Escudero et al (2000) tested the supervised adap-
tation scenario on the DSO corpus, which had exam-
ples from the Brown Corpus and Wall Street Journal
corpus. They found that the source corpus did not
help when tagging the target corpus, showing that
tagged corpora from each domain would suffice, and
concluding that hand tagging a large general corpus
would not guarantee robust broad-coverage WSD.
Agirre and Mart??nez (2000) used the same DSO cor-
pus and showed that training on the subset of the
source corpus that is topically related to the target
corpus does allow for domain adaptation, obtaining
better results than training on the target data alone.
In (Agirre and Lopez de Lacalle, 2008), the au-
thors also show that state-of-the-art WSD systems
are not able to adapt to the domains in the context
of the Koeling et al (2005) dataset. While WSD
systems trained on the target domain obtained 85.1
and 87.0 of precision on the sports and finances do-
mains, respectively, the same systems trained on the
BNC corpus (considered as a general domain cor-
pus) obtained 53.9 and 62.9 of precision on sports
and finances, respectively. Training on both source
and target was inferior that using the target examples
alone.
Supervised adaptation
Supervised adaptation for other NLP tasks has been
widely reported. For instance, (Daume? III, 2007)
shows that a simple feature augmentation method
for SVM is able to effectively use both labeled tar-
get and source data to provide the best domain-
adaptation results in a number of NLP tasks. His
method improves or equals over previously explored
more sophisticated methods (Daume? III and Marcu,
2006; Chelba and Acero, 2004). In contrast, (Agirre
and Lopez de Lacalle, 2009) reimplemented this
method and showed that the improvement on WSD
in the (Koeling et al, 2005) data was marginal.
Better results have been obtained using purpose-
built adaptation methods. Chan and Ng (2007) per-
formed supervised domain adaptation on a manu-
ally selected subset of 21 nouns from the DSO cor-
pus. They used active learning, count-merging, and
predominant sense estimation in order to save tar-
get annotation effort. They showed that adding just
30% of the target data to the source examples the
same precision as the full combination of target and
source data could be achieved. They also showed
that using the source corpus significantly improved
results when only 10%-30% of the target corpus
was used for training. In followup work (Zhong et
125
Projections for 2100 suggest that temperature in Europe will have risen by between 2 to 6.3 C above 1990
levels. The sea level is projected to rise, and a greater frequency and intensity of extreme weather events are
expected. Even if emissions of greenhouse gases stop today, these changes would continue for many decades
and in the case of sea level for centuries. This is due to the historical build up of the gases in the atmosphere
and time lags in the response of climatic and oceanic systems to changes in the atmospheric concentration
of the gases.
Figure 1: Sample text from the environment domain.
al., 2008), the feature augmentation approach was
combined with active learning and tested on the
OntoNotes corpus, on a large domain-adaptation ex-
periment. They significantly reduced the effort of
hand-tagging, but only obtained positive domain-
adaptation results for smaller fractions of the target
corpus.
In (Agirre and Lopez de Lacalle, 2009) the au-
thors report successful adaptation on the (Koeling
et al, 2005) dataset on supervised setting. Their
method is based on the use of unlabeled data, re-
ducing the feature space with SVD, and combina-
tion of features using an ensemble of kernel meth-
ods. They report 22% error reduction when using
both source and target data compared to a classifier
trained on target the target data alone, even when the
full dataset is used.
Semi-supervised adaptation
There are less works on semi-supervised domain
adaptation in NLP tasks, and fewer in WSD task.
Blitzer et al (2006) used Structural Correspondence
Learning and unlabeled data to adapt a Part-of-
Speech tagger. They carefully select so-called pivot
features to learn linear predictors, perform SVD on
the weights learned by the predictor, and thus learn
correspondences among features in both source and
target domains. Agirre and Lopez de Lacalle (2008)
show that methods based on SVD with unlabeled
data and combination of distinct feature spaces pro-
duce positive semi-supervised domain adaptation re-
sults for WSD.
Unsupervised adaptation
In this context, we take unsupervised to mean
Knowledge-Based methods which do not require
hand-tagged corpora. The predominant sense acqui-
sition method was succesfully applied to specific do-
mains in (Koeling et al, 2005). The methos has two
steps: In the first, a corpus of untagged text from the
target domain is used to construct a thesaurus of sim-
ilar words. In the second, each target word is disam-
biguated using pairwise WordNet-based similarity
measures, taking as pairs the target word and each of
the most related words according to the thesaurus up
to a certain threshold. This method aims to obtain,
for each target word, the sense which is the most
predominant for the target corpus. When a general
corpus is used, the most predominant sense in gen-
eral is obtained, and when a domain-specific corpus
is used, the most predominant sense for that corpus
is obtained (Koeling et al, 2005). The main motiva-
tion of the authors is that the most frequent sense is a
very powerful baseline, but it is one which requires
hand-tagging text, while their method yields simi-
lar information automatically. The results show that
they are able to obtain good results. In related work,
(Agirre et al, 2009) report improved results using
the same strategy but applying a graph-based WSD
method, and highlight the domain-adaptation poten-
tial of unsupervised knowledge-based WSD systems
compared to supervised WSD.
5 Design of the WSD-domain task
This task was designed in the context of Ky-
oto (Piek Vossen and VanGent, 2008)1, an Asian-
European project that develops a community plat-
form for modeling knowledge and finding facts
across languages and cultures. The platform op-
erates as a Wiki system with an ontological sup-
port that social communities can use to agree on the
meaning of terms in specific domains of their inter-
est. Kyoto will focus on the environmental domain
because it poses interesting challenges for informa-
tion sharing, but the techniques and platforms will
be independent of the application domain. Kyoto
1http://www.kyoto-project.eu/
126
will make use of semantic technologies based on
ontologies and WSD in order to extract and repre-
sent relevant information for the domain, and is thus
interested on measuring the performance of WSD
techniques on this domain.
The WSD-domain task will comprise comparable
all-words test corpora on the environment domain.
Texts from the European Center for Nature Con-
servation2 and Worldwide Wildlife Forum3 will be
used in order to build domain specific test corpora.
We will select documents that are written for a gen-
eral but interested public and that involve specific
terms from the domain. The document content will
be comparable across languages. Figure 1 shows an
example in English related to global warming.
The data will be available in a number of lan-
guages: English, Dutch, Italian and Chinese. The
sense inventories will be based on wordnets of the
respective languages, which will be updated to in-
clude new vocabulary and senses. The test data will
comprise three documents of around 2000 words
each for each language. The annotation procedure
will involve double-blind annotation plus adjudica-
tion, and inter-tagger agreement data will be pro-
vided. The formats and scoring software will fol-
low those of Senseval-34 and SemEval-20075 En-
glish all-words tasks.
There will not be training data available, but par-
ticipants are free to use existing hand-tagged cor-
pora and lexical resources (e.g. SemCor and pre-
vious Senseval and SemEval data). We plan to make
available a corpus of documents from the same do-
main as the selected documents, as well as wordnets
updated to include the terms and senses in the se-
lected documents.
6 Conclusions
Domain portability and adaptation of NLP com-
ponents and Word Sense Disambiguation systems
present new challenges. The difficulties found by
supervised systems to adapt might change the way
we assess the strengths and weaknesses of super-
vised and knowledge-based WSD systems. Unfor-
tunately, all existing evaluation datasets for specific
2http://www.ecnc.org
3http://www.wwf.org
4http://www.senseval.org/senseval3
5http://nlp.cs.swarthmore.edu/semeval/
domains are lexical-sample corpora. With this paper
we have motivated the creation of an all-words test
dataset for WSD on the environment domain in sev-
eral languages, and presented the overall design of
this SemEval task.
Further details can be obtained from the Semeval-
20106 website, our task website7, and in our distri-
bution list8
7 Acknowledgments
The organization of the task is partially funded
by the European Commission (KYOTO FP7 ICT-
2007-211423) and the Spanish Research Depart-
ment (KNOW TIN2006-15049-C03-01).
References
Eneko Agirre and Oier Lopez de Lacalle. 2008. On ro-
bustness and domain adaptation using SVD for word
sense disambiguation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 17?24, Manchester, UK, August.
Coling 2008 Organizing Committee.
Eneko Agirre and Oier Lopez de Lacalle. 2009. Super-
vised domain adaptation for wsd. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-09).
E. Agirre, O. Lopez de Lacalle, and A. Soroa. 2009.
Knowledge-based WSD and specific domains: Per-
forming over supervised WSD. In Proceedings of IJ-
CAI, Pasadena, USA.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain adap-
tation with active learning for word sense disambigua-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
49?56, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Barcelona, Spain.
6http://semeval2.fbk.eu/
7http://xmlgroup.iit.cnr.it/SemEval2010/
8http://groups.google.com/groups/wsd-domain
127
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
Gerard Escudero, Lluiz Ma?rquez, and German Rigau.
2000. An Empirical Study of the Domain Dependence
of Supervised Word Sense Disambiguation Systems.
Proceedings of the joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, EMNLP/VLC.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
A. Kilgarriff. 2001. English Lexical Sample Task De-
scription. In Proceedings of the Second International
Workshop on evaluating Word Sense Disambiguation
Systems, Toulouse, France.
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense
acquisition. In Proceedings of the Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing.
HLT/EMNLP, pages 419?426, Ann Arbor, Michigan.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
David Mart??nez and Eneko Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. Conference
on Empirical Method in Natural Language.
R. Mihalcea, T. Chklovski, and Adam Killgariff. 2004.
The Senseval-3 English lexical sample task. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), Barcelona, Spain.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Human Language Technology Workshop. Dis-
tributed as Human Language Technology by San Ma-
teo, CA: Morgan Kaufmann Publishers., pages 303?
308, Princeton, NJ.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computationla Linguistics (ACL), pages 40?47.
Nicoletta Calzolari Christiane Fellbaum Shu-kai Hsieh
Chu-Ren Huang Hitoshi Isahara Kyoko Kanzaki An-
drea Marchetti Monica Monachini Federico Neri
Remo Raffaelli German Rigau Maurizio Tescon
Piek Vossen, Eneko Agirre and Joop VanGent. 2008.
Kyoto: a system for mining, structuring and distribut-
ing knowledge across languages and cultures. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of
the Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pages 87?92, Prague, Czech
Republic.
Tony G. Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volumen 1: from Yes-
terday?s News to Tomorrow?s Language Resources.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC-2002),
pages 827?832, Las Palmas, Canary Islands.
Marc Weeber, James G. Mork, and Alan R. Aronson.
2001. Developing a test collection for biomedical
word sense disambiguation. In Proceedings of the
AMAI Symposium, pages 746?750, Washington, DC.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An em-
pirical study. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1002?1010, Honolulu, Hawaii, October.
Association for Computational Linguistics.
128
A proposal to automatically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia
Antonio Toral
University of Alicante
Carretera San Vicente S/N
Alicante 03690, Spain
atoral@dlsi.ua.es
Rafael Mun?oz
University of Alicante
Carretera San Vicente S/N
Alicante 03690, Spain
rafael@dlsi.ua.es
Abstract
This paper describes a method to automat-
ically create and maintain gazetteers for
Named Entity Recognition (NER). This
method extracts the necessary information
from linguistic resources. Our approach is
based on the analysis of an on-line ency-
clopedia entries by using a noun hierarchy
and optionally a PoS tagger. An impor-
tant motivation is to reach a high level of
language independence. This restricts the
techniques that can be used but makes the
method useful for languages with few re-
sources. The evaluation carried out proves
that this approach can be successfully used
to build NER gazetteers for location (F
78%) and person (F 68%) categories.
1 Introduction
Named Entity Recognition (NER) was defined at
the MUC conferences (Chinchor, 1998) as the task
consisting of detecting and classifying strings of
text which are considered to belong to different
classes (e.g. person, location, organization, date,
time). Named Entities are theoretically identified
and classified by using evidence. Two kinds of
evidence have been defined (McDonald, 1996).
These are internal and external evidence. Internal
evidence is the one provided from within the se-
quence of words that constitute the entity. In con-
trast, external evidence is the criteria that can be
obtained by the context in which entities appear.
Since the time NER was introduced, mainly two
approaches have been adopted to deal with this
task. One is referred as knowledge-based and uses
explicit resources like rules and gazetteers, which
commonly are hand-crafted. The other follows the
learning paradigm and usually uses as a resource a
tagged corpus which is used to train a supervised
learning algorithm.
In the knowledge-based approach two kind of
gazetteers can be distinguished. On one hand there
are trigger gazetteers, which contain key words
that indicate the possible presence of an entity of
a given type. These words usually are common
nouns. E.g. ms. indicates that the entity after it
is a person entity. On the other hand there are en-
tity gazetteers which contain entities themselves,
which usually are proper nouns. E.g. Portugal
could be an instance in a location gazetteer.
Initially, and specially for the MUC confer-
ences, most of the NER systems developed did
belong to the knowledge-based approach. This ap-
proach proved to be able to obtain high scores. In
fact, the highest score obtained by a knowledge-
based system in MUC-7 reached F 93.39 %
(Mikheev et al, 1998). However, this approach
has an important problem: gazetteers and rules are
difficult and tedious to develop and to maintain. If
the system is to be used for an open domain, lin-
guistic experts are needed to build the rules, and
besides, it takes too much time to tune these re-
sources in order to obtain satisfactory results. Be-
cause of this, lately most of the research falls into
the learning-based paradigm.
Regarding the creation and maintenance of
gazetteers, several problems have been identified,
these are mainly:
? Creation and maintenance effort
? Overlaps between gazetteers
The first problem identified assumes that the
gazetteers are manually created and maintained.
However, this is not always the case. Gazetteers
56
could be automatically created and maintained by
extracting the necessary information from avail-
able linguistic resources, which we think is a
promising line of future research.
Several research works have been carried out in
this direction. An example of this is a NER sys-
tem which uses trigger gazetteers automatically
extracted from WordNet (Magnini et al, 2002)
by using wordnet predicates. The advantage in
this case is that the resource used is multilingual
and thus, porting it to another language is almost
straightforward (Negri and Magnini, 2004).
There is also a work that deals with automat-
ically building location gazetteers from internet
texts by applying text mining procedures (Ouri-
oupina, 2002), (Uryupina, 2003). However, this
work uses linguistic patterns, and thus is language
dependent. The author claims that the approach
may successfully be used to create gazetteers for
NER.
We agree with (Magnini et al, 2002) that in or-
der to automatically create and maintain trigger
gazetteers, using a hierarchy of common nouns is
a good approach. Therefore, we want to focus on
the automatically creation and maintenance of en-
tity gazetteers. Another reason for this is that the
class of common nouns (the ones being triggers) is
much more stable than the class of proper names
(the ones in entity gazetteers). Because of this,
the maintenance of the latter is important as new
entities to be taken into account appear. For exam-
ple, if we refer to presidents, the trigger word used
might be ?president? and it is uncommon that the
trigger used to refer to them changes over time.
On the other hand, the entities being presidents
change as new presidents appear and current pres-
idents will disappear.
Our aim is to find a method which allow us to
automatically create and maintain entity gazetteers
by extracting the necessary information from lin-
guistic resources. An important restriction though,
is that we want our method to be as independent of
language as possible.
The rest of this paper is structured as follows.
In the next section we discuss about our proposal.
Section three presents the results we have obtained
and some comments about them. Finally, in sec-
tion four we outline our conclusions and future
work.
2 Approach
In this section we present our approach to auto-
matically build and maintain dictionaries of proper
nouns. In a nutshell, we analyse the entries of an
encyclopedia with the aid of a noun hierarchy. Our
motivation is that proper nouns that form entities
can be obtained from the entries in an encyclo-
pedia and that some features of their definitions
in the encyclopedia can help to classify them into
their correct entity category.
The encyclopedia used has been Wikipedia1 .
According to the English version of Wikipedia
2
, Wikipedia is a multi-lingual web-based, free-
content encyclopedia which is updated continu-
ously in a collaborative way. The reasons why we
have chosen this encyclopedia are the following:
? It is a big source of information. By De-
cember 2005, it has over 2,500,000 defini-
tions. The English version alone has more
than 850,000 entries.
? Its content has a free license, meaning that it
will always be available for research without
restrictions and without needing to acquire
any license.
? It is a general knowledge resource. Thus, it
can be used to extract information for open
domain systems.
? Its data has some degree of formality and
structure (e.g. categories) which helps to pro-
cess it.
? It is a multilingual resource. Thus, if we are
able to develop a language independent sys-
tem, it can be used to create gazetteers for any
language for which Wikipedia is available.
? It is continuously updated. This is a very
important fact for the maintenance of the
gazetteers.
The noun hierarchy used has been the noun hi-
erarchy from WordNet (Miller, 1995). This is a
widely used resource for NLP tasks. Although
initially being a monolingual resource for the En-
glish language, a later project called EuroWordNet
(Vossen, 1998), provided wordnet-like hierarchies
1http://www.wikipedia.org
2http://en.wikipedia.org/wiki/Main Page
57
for a set of languages of the European Union. Be-
sides, EuroWordNet defines a language indepen-
dent index called Inter-Lingual-Index (ILI) which
allows to establish relations between words in
wordnets of different languages. The ILI facili-
tates also the development of wordnets for other
languages.
From this noun hierarchy we consider the nodes
(called synsets in WordNet) which in our opinion
represent more accurately the different kind of en-
tities we are working with (location, organization
and person). For example, we consider the synset
6026 as the corresponding to the entity class Per-
son. This is the information contained in synset
number 6026:
person, individual, someone,
somebody, mortal,
human, soul -- (a human being;
"there was too much for one person
to do")
Given an entry from Wikipedia, a PoS-tagger
(Carreras et al, 2004) is applied to the first sen-
tence of its definition. As an example, the first
sentence of the entry Portugal in the Simple En-
glish Wikipedia 3 is presented here:
Portugal portugal NN
is be VBZ
a a DT
country country NN
in in IN
the the DT
south-west south-west NN
of of IN
Europe Europe NP
. . Fp
For every noun in a definition we obtain the
synset of WordNet that contains its first sense4.
We follow the hyperonymy branch of this synset
until we arrive to a synset we have considered be-
longing to an entity class or we arrive to the root of
the hierarchy. If we arrive to a considered synset,
then we consider that noun as belonging to the en-
tity class of the considered synset. The following
example may clarify this explanation:
portugal --> LOCATION
3http://simple.wikipedia.org/wiki/Portugal
4We have also carried out experiments taking into account
all the senses provided by WordNet. However, the perfor-
mance obtained is not substantially better while the process-
ing time increases notably.
country --> LOCATION
south-west --> NONE
europe --> LOCATION
As it has been said in the abstract, the appli-
cation of a PoS tagger is optional. The algorithm
will perform considerably faster with it as with the
PoS data we only need to process the nouns. If a
PoS tagger is not available for a language, the al-
gorithm can still be applied. The only drawback
is that it will perform slower as it needs to pro-
cess all the words. However, through our experi-
mentation we can conclude that the results do not
significantly change.
Finally, we apply a weighting algorithm which
takes into account the amount of nouns in the defi-
nition identified as belonging to the different entity
types considered and decides to which entity type
the entry belongs. This algorithm has a constant
Kappa which allows to increase or decrease the
distance required within categories in order to as-
sign an entry to a given class. The value of Kappa
is the minimum difference of number of occur-
rences between the first and second most frequent
categories in an entry in order to assign the entry
to the first category. In our example, for any value
of Kappa lower than 4, the algorithm would say
that the entry Portugal belongs to the location en-
tity type.
Once we have this basic approach we apply dif-
ferent heuristics which we think may improve the
results obtained and which effect will be analysed
in the section about results.
The first heuristic, called is instance, tries to de-
termine whether the entries from Wikipedia are in-
stances (e.g. Portugal) or word classes (e.g. coun-
try). This is done because of the fact that named
entities only consider instances. Therefore, we are
not interested in word classes. We consider that an
entry from Wikipedia is an instance when it has an
associated entry in WordNet and it is an instance.
The procedure to determine if an entry from Word-
Net is an instance or a word class is similar to the
one used in (Magnini et al, 2002).
The second heuristic is called is in wordnet. It
simply determines if the entries from Wikipedia
have an associated entry in WordNet. If so, we
may use the information from WordNet to deter-
mine its category.
58
3 Experiments and results
We have tested our approach by applying it to
3517 entries of the Simple English Wikipedia
which were randomly selected. Thus, these en-
tries have been manually tagged with the expected
entity category5. The distribution by entity classes
can be seen in table 1:
As it can be seen in table 1, the amount of enti-
ties of the categories Person and Location are bal-
anced but this is not the case for the type Organi-
zation. There are very few instances of this type.
This is understandable as in an encyclopedia lo-
cations and people are defined but this is not the
usual case for organizations.
According to what was said in section 2, we
considered the heuristics explained there by car-
rying out two experiments. In the first one we
applied the is instance heuristic. The second ex-
periment considers the two heuristics explained in
section 2 (is instance and is in wordnet). We do
not present results without the first heuristic as
through our experimentation it proved to increase
both recall and precision for every entity category.
For each experiment we considered two values
of a constant Kappa which is used in our algo-
rithm. The values are 0 and 2 as through exper-
imentation we found these are the values which
provide the highest recall and the highest preci-
sion, respectively. Results for the first experiment
can be seen in table 2 and results for the second
experiment in table 3.
As it can be seen in these tables, the best re-
call for all classes is obtained in experiment 2 with
Kappa 0 (table 3) while the best precision is ob-
tained in experiment 1 with Kappa 2 (table 2).
The results both for location and person cat-
egories are in our opinion good enough to the
purpose of building and maintaining good quality
gazetteers after a manual supervision. However,
the results obtained for the organization class are
very low. This is mainly due to the fact of the
high interaction between this category and loca-
tion combined with the practically absence of tra-
ditional entities of the organization type such as
companies. This interaction can be seen in the in-
depth results which presentation follows.
In order to clarify these results, we present more
in-depth data in tables 4 and 5. These tables
present an error analysis, showing the false posi-
5This data is available for research at http://www.
dlsi.ua.es/?atoral/index.html\#resources
tives, false negatives, true positives and true nega-
tives among all the categories for the configuration
that provides the highest recall (experiment 2 with
Kappa 0) and for the one that provides the highest
precision (experiment 1 with Kappa 2).
In tables 4 and 5 we can see that the interactions
within classes (occurrences tagged as belonging to
one class but NONE and guessed as belonging to
other different class but NONE) is low. The only
case in which it is significant is between location
and organization. In table 5 we can see that 12 en-
tities tagged as organization are classified as LOC
while 20 tagged as organization are guessed with
the correct type. Following with these, 5 entities
tagged as location where classified as organiza-
tion. This is due to the fact that countries and
related entities such as ?European Union? can be
considered both as organizations or locations de-
pending on their role in a text.
4 Conclusions
We have presented a method to automatically cre-
ate and maintain entity gazetteers using as re-
sources an encyclopedia, a noun hierarchy and,
optionally, a PoS tagger. The method proves to be
helpful for these tasks as it facilitates the creation
and maintenance of this kind of resources.
In our opinion, the principal drawback of our
system is that it has a low precision for the con-
figuration for which it obtains an acceptable value
of recall. Therefore, the automatically created
gazetteers need to pass a step of manual supervi-
sion in order to have a good quality.
On the positive side, we can conclude that our
method is helpful as it takes less time to automat-
ically create gazetteers with our method and after
that to supervise them than to create that dictio-
naries from scratch. Moreover, the updating of the
gazetteers is straightforward; just by executing the
procedure, the new entries in Wikipedia (the en-
tries that did not exist at the time the procedure
was performed the last time) would be analysed
and from these set, the ones detected as entities
would be added to the corresponding gazetteers.
Another important fact is that the method has
a high degree of language independence; in or-
der to apply this approach to a new language, we
need a version of Wikipedia and WordNet for that
language, but the algorithm and the process does
not change. Therefore, we think that our method
can be useful for the creation of gazetteers for lan-
59
Entity type Number of instances Percentage
NONE 2822
LOC 404 58
ORG 55 8
PER 236 34
Table 1: Distribution by entity classes
k LOC ORG PER
prec rec F?=1 prec rec F?=1 prec rec F?=1
0 66.90 94.55 78.35 28.57 18.18 22.22 61.07 77.11 68.16
2 86.74 56.68 68.56 66.66 3.63 6.89 86.74 30.50 45.14
Table 2: Experiment 1. Results applying is instance heuristic
k LOC ORG PER
prec rec F?=1 prec rec F?=1 prec rec F?=1
0 62.88 96.03 76.00 16.17 20.00 17.88 43.19 84.74 57.22
2 77.68 89.60 83.21 13.95 10.90 12.24 46.10 62.71 53.14
Table 3: Experiment 2. Results applying is instance and is in wordnet heuristics
Tagged Guessed
NONE LOC ORG PER
NONE 2777 33 1 11
LOC 175 229 0 0
ORG 52 1 2 0
PER 163 1 0 72
Table 4: Results fn-fp (results 1 k=2)
Tagged Guessed
NONE LOC ORG PER
NONE 2220 196 163 243
LOC 8 387 5 4
ORG 20 12 20 3
PER 30 9 2 195
Table 5: Results fn-fp (results 2 k=0)
60
guages in which NER gazetteers are not available
but have Wikipedia and WordNet resources.
During the development of this research, several
future works possibilities have appeared. Regard-
ing the task we have developed, we consider to
carry out new experiments incorporating features
that Wikipedia provides such as links between
pairs of entries. Following with this, we consider
to test more complex weighting techniques for our
algorithm.
Besides, we think that the resulting gazetteers
for the configurations that provide high precision
and low recall, although not being appropriate for
building gazetteers for NER systems, can be in-
teresting for other tasks. As an example, we con-
sider to use them to extract verb frequencies for
the entity categories considered which can be later
used as features for a learning based Named Entity
Recogniser.
Acknowledgements
This research has been partially funded by the
Spanish Government under project CICyT num-
ber TIC2003-07158-C04-01 and by the Valencia
Government under project number GV04B-268.
We also would like to specially thank Borja
Navarro for his valuable help on WordNet.
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An Open-Source Suite of Language Ana-
lyzers. In Proceedings of the 4th LREC Conference.
N. Chinchor. 1998. Overview of MUC-7. In Proceed-
ings of the Seventh Message Understanding Confer-
ence (MUC-7).
B. Magnini, M. Negri, R. Preete, and H. Tanev. 2002.
A wordnet-based approach to named entities recog-
nition. In Proceedings of SemaNet ?02: Building
and Using Semantic Networks, pages 38?44.
D. McDonald. 1996. Internal and external evidence
in the identification and semantic categorization of
proper names. Corpus Processing for Lexical Aqui-
sition, pages 21?39, chapter 2.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG system used for MUC-7. In Seventh
Message Understanding Conference (MUC-7): Pro-
ceedings of a Conference held in Fairfax, Virginia,
29 April-1 May.
G. A. Miller. 1995. Wordnet: A lexical database for
english. Communications of ACM, (11):39?41.
M. Negri and B. Magnini. 2004. Using wordnet pred-
icates for multilingual named entity recognition. In
Proceedings of The Second Global Wordnet Confer-
ence, pages 169?174.
O. Ourioupina. 2002. Extracting geographical knowl-
edge from the internet. In Proceedings of the ICDM-
AM International Workshop on Active Mining.
O. Uryupina. 2003. Semi-supervised learning of geo-
graphical gazetteers from the internet. In Proceed-
ings of the HLT-NAACL 2003 Workshop on Analysis
of Geographic References, pages 18?25.
P. Vossen. 1998. Introduction to eurowordnet. Com-
puters and the Humanities, 32:73?89.
61
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?5,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Language Resources Factory: case study on the acquisition of
Translation Memories?
Marc Poch
UPF Barcelona, Spain
marc.pochriera@upf.edu
Antonio Toral
DCU Dublin, Ireland
atoral@computing.dcu.ie
Nu?ria Bel
UPF Barcelona, Spain
nuria.bel@upf.edu
Abstract
This paper demonstrates a novel distributed
architecture to facilitate the acquisition of
Language Resources. We build a factory
that automates the stages involved in the ac-
quisition, production, updating and mainte-
nance of these resources. The factory is de-
signed as a platform where functionalities
are deployed as web services, which can
be combined in complex acquisition chains
using workflows. We show a case study,
which acquires a Translation Memory for a
given pair of languages and a domain using
web services for crawling, sentence align-
ment and conversion to TMX.
1 Introduction
A fundamental issue for many tasks in the field of
Computational Linguistics and Language Tech-
nologies in general is the lack of Language Re-
sources (LRs) to tackle them successfully, espe-
cially for some languages and domains. It is the
so-called LRs bottleneck.
Our objective is to build a factory of LRs that
automates the stages involved in the acquisition,
production, updating and maintenance of LRs
required by Machine Translation (MT), and by
other applications based on Language Technolo-
gies. This automation will significantly cut down
the required cost, time and human effort. These
reductions are the only way to guarantee the con-
tinuous supply of LRs that Language Technolo-
gies demand in a multilingual world.
? We would like to thank the developers of Soaplab, Tav-
erna, myExperiment and Biocatalogue for solving our ques-
tions and attending our requests. This research has been
partially funded by the EU project PANACEA (7FP-ICT-
248064).
2 Web Services and Workflows
The factory is designed as a platform of web ser-
vices (WSs) where the users can create and use
these services directly or combine them in more
complex chains. These chains are called work-
flows and can represent different combinations of
tasks, e.g. ?extract the text from a PDF docu-
ment and obtain the Part of Speech (PoS) tagging?
or ?crawl this bilingual website and align its sen-
tence pairs?. Each task is carried out using NLP
tools deployed as WSs in the factory.
Web Service Providers (WSPs) are institutions
(universities, companies, etc.) who are willing
to offer services for some tasks. WSs are ser-
vices made available from a web server to re-
mote users or to other connected programs. WSs
are built upon protocols, server and program-
ming languages. Their massive adoption has con-
tributed to make this technology rather interoper-
able and open. In fact, WSs allow computer pro-
grams distributed in different locations to interact
with each other.
WSs introduce a completely new paradigm in
the way we use software tools. Before, every
researcher or laboratory had to install and main-
tain all the different tools that they needed for
their work, which has a considerable cost in both
human and computing resources. In addition, it
makes it more difficult to carry out experiments
that involve other tools because the researcher
might hesitate to spend time resources on in-
stalling new tools when there are other alterna-
tives already installed.
The paradigm changes considerably with WSs,
as in this case only the WSP needs to have a deep
knowledge of the installation and maintenance of
the tool, thus allowing all the other users to benefit
1
from this work. Consequently, researchers think
about tools from a high level and solely regard-
ing their functionalities, thus they can focus on
their work and be more productive as the time re-
sources that would have been spent to install soft-
ware are freed. The only tool that the users need
to install in order to design and run experiments is
a WS client or a Workflow editor.
3 Choosing the tools for the platform
During the design phase several technologies
were analyzed to study their features, ease of use,
installation, maintenance needs as well as the es-
timated learning curve required to use them. In-
teroperability between components and with other
technologies was also taken into account since
one of our goals is to reach as many providers and
users as possible. After some deliberation, a set of
technologies that have proved to be successful in
the Bioinformatics field were adopted to build the
platform. These tools are developed by the my-
Grid1 team. This group aims to develop a suite
of tools for researchers that work with e-Science.
These tools have been used in numerous projects
as well as in different research fields as diverse as
astronomy, biology and social science.
3.1 Web Services: Soaplab
Soaplab (Senger et al 2003)2 allows a WSP to
deploy a command line tool as a WS just by writ-
ing a metadata file that describes the parameters
of the tool. Soaplab takes care of the typical is-
sues regarding WSs automatically, including tem-
porary files, protocols, the WSDL file and its pa-
rameters, etc. Moreover, it creates a Web interface
(called Spinet) where WSs can be tested and used
with input forms. All these features make Soaplab
a suitable tool for our project. Moreover, its nu-
merous successful stories make it a safe choise;
e.g., it has been used by the European Bioinfor-
matics Institute3 to deploy their tools as WSs.
3.2 Registry: Biocatalogue
Once the WSs are deployed by WSPs, some
means to find them becomes necessary. Biocat-
alogue (Belhajjame et al 2008)4 is a registry
1http://www.mygrid.org.uk
2http://soaplab.sourceforge.net/
soaplab2/
3http://www.ebi.ac.uk
4http://www.biocatalogue.org/
where WSs can be shared, searched for, annotated
with tags, etc. It is used as the main registration
point for WSPs to share and annotate their WSs
and for users to find the tools they need. Bio-
catalogue is a user-friendly portal that monitors
the status of the WSs deployed and offers multi-
ple metadata fields to annotate WSs.
3.3 Workflows: Taverna
Now that users can find WSs and use them, the
next step is to combine them to create complex
chains. Taverna (Missier et al 2010)5 is an open
source application that allows the user to create
high-level workflows that integrate different re-
sources (mainly WSs in our case) into a single
experiment. Such experiments can be seen as
simulations which can be reproduced, tuned and
shared with other researchers.
An advantage of using workflows is that the
researcher does not need to have background
knowledge of the technical aspects involved in
the experiment. The researcher creates the work-
flow based on functionalities (each WS provides a
function) instead of dealing with technical aspects
of the software that provides the functionality.
3.4 Sharing workflows: myExperiment
MyExperiment (De Roure et al 2008)6 is a so-
cial network used by workflow designers to share
workflows. Users can create groups and share
their workflows within the group or make them
publically available. Workflows can be annotated
with several types of information such as descrip-
tion, attribution, license, etc. Users can easily find
examples that will help them during the design
phase, being able to reuse workflows (or parts of
them) and thus avoiding reinveinting the wheel.
4 Using the tools to work with NLP
All the aforementioned tools were installed, used
and adapted to work with NLP. In addition, sev-
eral tutorials and videos have been prepared7 to
help partners and other users to deploy and use
WSs and to create workflows.
Soaplab has been modified (a patch has been
developed and distributed)8 to limit the amount of
data being transfered inside the SOAP message in
5http://www.taverna.org.uk/
6http://www.myexperiment.org/
7http://panacea-lr.eu/en/tutorials/
8http://myexperiment.elda.org/files/5
2
order to optimize the network usage. Guidelines
that describe how to limit the amount of concur-
rent users of WSs as well as to limit the maximum
size of the input data have been prepared.9
Regarding Taverna, guidelines and workflow
examples have been shared among partners show-
ing the best way to create workflows for the
project. The examples show how to benefit from
useful features provided by this tool, such as
?retries? (to execute up to a certain number of
times a WS when it fails) and ?parallelisation? (to
run WSs in parallel, thus increasing trhoughput).
Users can view intermediate results and parame-
ters using the provenance capture option, a useful
feature while designing a workflow. In case of any
WS error in one of the inputs, Taverna will report
the error message produced by the WS or proces-
sor component that causes it. However, Taverna
will be able to continue processing the rest of the
input data if the workflow is robust (i.e. makes
use of retry and parallelisation) and the error is
confined to a WS (i.e. it does not affect the rest of
the workflow).
An instance of Biocatalogue and one of my-
Experiment have been deployed to be the Reg-
istry and the portal to share workflows and other
experiment-related data. Both have been adapted
by modifying relevant aspects of the interface
(layout, colours, names, logos, etc.). The cate-
gories that make up the classification system used
in the Registry have been adapted to the NLP
field. At the time of writing there are more than
100 WSs and 30 workflows registered.
5 Interoperability
Interoperability plays a crucial role in a platform
of distributed WSs. Soaplab deploys SOAP10
WSs and handles automatically most of the issues
involved in this process, while Taverna can com-
bine SOAP and REST11 WSs. Hence, we can say
that communication protocols are being handled
by the tools. However, parameters and data inter-
operability need to be addressed.
5.1 Common Interface
To facilitate interoperability between WSs and to
easily exchange WSs, a Common Interface (CI)
9http://myexperiment.elda.org/files/4
10http://www.w3.org/TR/soap/
11http://www.ics.uci.edu/?fielding/
pubs/dissertation/rest_arch_style.htm
has been designed for each type of tool (e.g. PoS-
taggers, aligners, etc.). The CI establishes that all
WSs that perform a given task must have the same
mandatory parameters. That said, each tool can
have different optional parameters. This system
eases the design of workflows as well as the ex-
change of tools that perform the same task inside
a workflow. The CI has been developed using an
XML schema.12
5.2 Travelling Object
A goal of the project is to facilitate the deploy-
ment of as many tools as possible in the form of
WSs. In many cases, tools performing the same
task use in-house formats. We have designed a
container, called ?Travelling Object? (TO), as the
data object that is being transfered between WSs.
Any tool that is deployed needs to be adapted to
the TO, this way we can interconnect the different
tools in the platform regardless of their original
input/output formats.
We have adopted for TO the XML Corpus En-
coding Standard (XCES) format (Ide et al 2000)
because it was the already existing format that re-
quired the minimum transduction effort from the
in-house formats. The XCES format has been
used successfully to build workflows for PoS tag-
ging and alignment.
Some WSs, e.g. dependency parsers, require a
more complex representation that cannot be han-
dled by the TO. Therefore, a more expressive for-
mat has been adopted for these. The Graph Anno-
tation Format (GrAF) (Ide and Suderman, 2007)
is a XML representation of a graph that allows
different levels of annotation using a ?feature?
value? paradigm. This system allows different
in-house formats to be easily encapsulated in this
container-based format. On the other hand, GrAF
can be used as a pivot format between other for-
mats (Ide and Bunt, 2010), e.g. there is software
to convert GrAF to UIMA and GATE formats (Ide
and Suderman, 2009) and it can be used to merge
data represented in a graph.
Both TO and GrAF address syntactic interop-
erability while semantic interoperability is still an
open topic.
12http://panacea-lr.eu/en/
info-for-professionals/documents/
3
6 Evaluation
The evaluation of the factory is based on its
features and usability requirements. A binary
scheme (yes/no) is used to check whether each re-
quirement is fulfilled or not. The quality of the
tools is not altered as they are deployed as WSs
without any modification. According to the eval-
uation of the current version of the platform, most
requirements are fulfilled (Aleksic? et al 2012).
Another aspect of the factory that is being eval-
uated is its performance and scalabilty. They do
not depend on the factory itself but on the design
of the workflows and WSs. WSPs with robust
WSs and powerful servers will provide a better
and faster service to users (considering that the
service is based on the same tool). This is analo-
gous to the user installing tools on a computer; if
the user develops a fragile script to chain the tools
the execution may fail, while if the computer does
not provide the required computational resources
the performance will be poor.
Following the example of the Bioinformatics
field where users can benefit of powerful WSPs,
the factory is used as a proof of concept that these
technologies can grow and scale to benefit many
users.
7 Case study
We introduce a case study in order to demonstrate
the capabilities of the platform. It regards the ac-
quisition of a Translation Memory (TM) for a lan-
guage pair and a specific domain. This is deemed
to be very useful for translators when they start
translating documents for a new domain. As at
that early stage they still do not have any content
in their TM, having the automatically acquired
TM can be helpful in order to get familiar with
the characteristic bilingual terminology and other
aspects of the domain. Another obvious potential
use of this data would be to use it to train a Statis-
tical MT system.
Three functionalities are needed to carry out
this process: acquisition of the data, its alignment
and its conversion into the desired format. These
are provided by WSs available in the registry.
First, we use a domain-focused bilingual
crawler13 in order to acquire the data. Given a pair
of languages, a set of web domains and a set of
seed terms that define the target domain for these
13http://registry.elda.org/services/127
languages, this tool will crawl the webpages in
the domains and gather pairs of web documents
in the target languages that belong to the target
domain. Second, we apply a sentence aligner.14
It takes as input the pairs of documents obtained
by the crawler and outputs pairs of equivalent sen-
tences.Finally, convert the aligned data into a TM
format. We have picked TMX15 as it is the most
common format for TMs. The export is done by
a service that receives as input sentence-aligned
text and converts it to TMX.16
The ?Bilingual Process, Sentence Alignment of
bilingual crawled data with Hunalign and export
into TMX?17 is a workflow built using Taverna
that combines the three WSs in order to provide
the functionality needed. The crawling part is
ommitted because data only needs to be crawled
once; crawled data can be processed with differ-
ent workflows but it would be very inefficient to
crawl the same data each time. A set of screen-
shots showing the WSs and the workflow, together
with sample input and output data is available.18
8 Demo and Requirements
The demo aims to show the web portals and tools
used during the development of the case study.
First, the Registry19 to find WSs, the Spinet Web
client to easily test them and Taverna to finally
build a workflow combining the different WSs.
For the live demo, the workflows will be already
designed because of the time constraints. How-
ever, there are videos on the web that illustrate
the whole process. It will be also interesting to
show the myExperiment portal,20 where all pub-
lic workflows can be found. Videos of workflow
executions will also be available.
Regarding the requirements, a decent internet
connection is critical for an acceptable perfor-
mance of the whole platform, specially for remote
WSs and workflows. We will use a laptop with
Taverna installed to run the workflow presented
in Section 7.
14http://registry.elda.org/services/92
15http://www.gala-global.org/
oscarStandards/tmx/tmx14b.html
16http://registry.elda.org/services/219
17http://myexperiment.elda.org/
workflows/37
18http://www.computing.dcu.ie/?atoral/
panacea/eacl12_demo/
19http://registry.elda.org
20http://myexperiment.elda.org
4
References
Vera Aleksic?, Olivier Hamon, Vassilis Papavassiliou,
Pavel Pecina, Marc Poch, Prokopis Prokopidis, Va-
leria Quochi, Christoph Schwarz, and Gregor Thur-
mair. 2012. Second evaluation report. Evalu-
ation of PANACEA v2 and produced resources
(PANACEA project Deliverable 7.3). Technical re-
port.
Khalid Belhajjame, Carole Goble, Franck Tanoh, Jiten
Bhagat, Katherine Wolstencroft, Robert Stevens,
Eric Nzuobontane, Hamish McWilliam, Thomas
Laurent, and Rodrigo Lopez. 2008. Biocatalogue:
A curated web service registry for the life science
community. In Microsoft eScience conference.
David De Roure, Carole Goble, and Robert Stevens.
2008. The design and realisation of the myexperi-
ment virtual research environment for social sharing
of workflows. Future Generation Computer Sys-
tems, 25:561?567, May.
Nancy Ide and Harry Bunt. 2010. Anatomy of anno-
tation schemes: mapping to graf. In Proceedings of
the Fourth Linguistic Annotation Workshop, LAW
IV ?10, pages 247?255, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Nancy Ide and Keith Suderman. 2007. GrAF: A
Graph-based Format for Linguistic Annotations. In
Proceedings of the Linguistic Annotation Workshop,
pages 1?8, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Nancy Ide and Keith Suderman. 2009. Bridging
the Gaps: Interoperability for GrAF, GATE, and
UIMA. In Proceedings of the Third Linguistic An-
notation Workshop, pages 27?34, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based encoding standard
for linguistic corpora. In Proceedings of the Second
International Language Resources and Evaluation
Conference. Paris: European Language Resources
Association.
Paolo Missier, Stian Soiland-Reyes, Stuart Owen,
Wei Tan, Aleksandra Nenadic, Ian Dunlop, Alan
Williams, Thomas Oinn, and Carole Goble. 2010.
Taverna, reloaded. In M. Gertz, T. Hey, and B. Lu-
daescher, editors, SSDBM 2010, Heidelberg, Ger-
many, June.
Martin Senger, Peter Rice, and Thomas Oinn. 2003.
Soaplab - a unified sesame door to analysis tools.
In All Hands Meeting, September.
5
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?189,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Active Learning for Post-Editing Based Incrementally Retrained MT
Aswarth Dara Josef van Genabith Qun Liu John Judge Antonio Toral
School of Computing
Dublin City University
Dublin, Ireland
{adara,josef,qliu,jjudge,atoral}@computing.dcu.ie
Abstract
Machine translation, in particular statis-
tical machine translation (SMT), is mak-
ing big inroads into the localisation and
translation industry. In typical work-
flows (S)MT output is checked and (where
required) manually post-edited by hu-
man translators. Recently, a significant
amount of research has concentrated on
capturing human post-editing outputs as
early as possible to incrementally up-
date/modify SMT models to avoid repeat
mistakes. Typically in these approaches,
MT and post-edits happen sequentially
and chronologically, following the way
unseen data (the translation job) is pre-
sented. In this paper, we add to the ex-
isting literature addressing the question
whether and if so, to what extent, this
process can be improved upon by Active
Learning, where input is not presented
chronologically but dynamically selected
according to criteria that maximise perfor-
mance with respect to (whatever is) the re-
maining data. We explore novel (source
side-only) selection criteria and show per-
formance increases of 0.67-2.65 points
TER absolute on average on typical indus-
try data sets compared to sequential PE-
based incrementally retrained SMT.
1 Introduction and Related Research
Machine Translation (MT) has evolved dramati-
cally over the last two decades, especially since
the appearance of statistical approaches (Brown et
al., 1993). In fact, MT is nowadays succesfully
used in the localisation and translation industry,
as for many relevant domains such as technical
documentation, post-editing (PE) of MT output by
human translators (compared to human translation
from scratch) results in notable productivity gains,
as a number of industry studies have shown con-
vincingly, e.g. (Plitt and Masselot, 2010). Fur-
thermore, incremental retraining and update tech-
niques (Bertoldi et al., 2013; Levenberg et al.,
2010; Mathur et al., 2013; Simard and Foster,
2013) allow these PEs to be fed back into the MT
model, resulting in an MT system that is contin-
uously updated to perform better on forthcoming
sentences, which should lead to a further increase
in productivity.
Typically, post-editors are presented with MT
output units (sentences) in the order in which input
sentences appear one after the other in the trans-
lation job. Because of this, incremental MT re-
training and update models based on PE outputs
also proceed in the same chronological order de-
termined by the input data. This may be sub-
optimal. In this paper we study the application of
Active Learning (AL) to the scenario of PE MT
and subsequent PE-based incremental retraining.
AL selects data (here translation inputs and their
MT outputs for PE) according to criteria that max-
imise performance with respect to the remaining
data and may diverge from processing data items
in chronological order. This may allow incremen-
tally PE-based retrained MT to (i) improve more
rapidly than chronologically PE-based retrained
MT and (ii) result in overall productivity gains.
The main contributions of this paper include:
? Previous work (Haffari et al., 2009; Blood-
good and Callison-Burch, 2010) shows that,
given a (static) training set, AL can im-
prove the quality of MT. By contrast, here
we show that AL-based data selection for hu-
man PE improves incrementally and dynami-
cally retrained MT, reducing overall PE time
of translation jobs in the localisation industry
application scenarios.
? We propose novel selection criteria for AL-
based PE: we adapt cross-entropy difference
(Moore and Lewis, 2010; Axelrod et al.,
2011), originally used for domain adaptation,
and propose an extension to cross entropy
difference with a vocabulary saturation filter
(Lewis and Eetemadi, 2013).
? While much of previous work concentrates
on research datasets (e.g. Europarl, News
Commentary), we use industry data (techni-
185
cal manuals). (Bertoldi et al., 2013) shows
that the repetition rate of news is consider-
ably lower than that of technical documenta-
tion, which impacts on the results obtained
with incremental retraining.
? Unlike in previous research, our AL-based
selection criteria take into account only the
source side of the data. This supports se-
lection before translation, keeping costs to a
minimum, a priority in commercial PE MT
applications.
? Our experiments show that AL-based selec-
tion works for PE-based incrementally re-
trained MT with overall performance gains
around 0.67 to 2.65 TER absolute on average.
AL has been successfully applied to many tasks
in natural language processing, including pars-
ing (Tang et al., 2002), named entity recogni-
tion (Miller et al., 2004), to mention just a few. See
(Olsson, 2009) for a comprehensie overview of
the application of AL to natural language process-
ing. (Haffari et al., 2009; Bloodgood and Callison-
Burch, 2010) apply AL to MT where the aim is to
build an optimal MT model from a given, static
dataset. To the best of our knowledge, the most
relevant previous research is (Gonz?alez-Rubio et
al., 2012), which applies AL to interactive MT. In
addition to differences in the AL selection criteria
and data sets, our goals are fundamentally differ-
ent: while the previous work aimed at reducing
human effort in interactive MT, we aim at reduc-
ing the overall PE time in PE-based incremental
MT update applications in the localisation indus-
try.
In our experiments reported in Section 3 below
we want to explore a space consisting of a con-
siderable number of selection strategies and incre-
mental retraining batch sizes. In order to be able to
do this, we use the target side of our industry trans-
lation memory data to approximate human PE out-
put and automatic TER (Snover et al., 2006) scores
as a proxy for human PE times (O?Brien, 2011).
2 Methodology
Given a translation job, our goal is to reduce the
overall PE time. At each stage, we select sen-
tences that are given to the post editor in such a
way that uncertain sentences (with respect to the
MT system at hand)
1
are post-edited first. We then
translate the n top-ranked sentences using the MT
system and use the human PEs of the MT outputs
to retrain the system. Algorithm 1 describes our
1
The uncertainty of a sentence with respect to the model
can be measured according to different criteria, e.g. percent-
age of unknown n-grams, perplexity etc.
method, where s and t stand for source and target,
respectively.
Algorithm 1 Sentence Selection Algorithm
Input:
L?? Initial training data
M?? Initial MT model
for C ? (Random,Sequential,Ngram,CED,CEDN) do
U?? Translation job
while size(U) > 0 do
U1.s?? SelectTopSentences(C, U.s)
U1
1
.t?? Translate(M, U1.s)
U1.t?? PostEdit(U1
1
.t)
U?? U - U1
L?? L ? U1
M?? TrainModel (L)
end while
end for
We use two baselines, i.e. random and sequen-
tial. In the random baseline, the batch of sentences
at each iteration are selected randomly. In the se-
quential baseline, the batches of sentences follow
the same order as the data.
Aside from the Random and Sequential base-
lines we use the following selection criteria:
? N-gram Overlap. An SMT system will en-
counter problems translating sentences con-
taining n-grams not seen in the training data.
Thus, PEs of sentences with high number of
unseen n-grams are considered to be more in-
formative for updating the current MT sys-
tem. However, for the MT system to trans-
late unseen n-grams accurately, they need to
be seen a minimum number V times.
2
We
use an n-gram overlap function similar to
the one described in (Gonz?alez-Rubio et al.,
2012) given in Equation 1 where N(T
(i)
) and
N(S
(i)
) return i-grams in training data and
the sentence S, respectively.
unseen(S) =
n
?
i=1
{|N(T
(i)
) ?N(S
(i)
)|>V }
n
?
i=1
N(S
(i)
)
(1)
? Cross Entropy Difference (CED). This met-
ric is originally used in data selection (Moore
and Lewis, 2010; Axelrod et al., 2011).
Given an in-domain corpus I and a general
corpus O, language models are built from
both,
3
and each sentence in O is scored ac-
cording to the entropy H difference (Equation
2
Following (Gonz?alez-Rubio et al., 2012) we use V =
10.
3
In order to make the LMs comparable they have the same
size. As commonly the size of O is considerable bigger than
I, this means that the LM for O is built from a subset of the
same size as I.
186
2). The lower the score given to a sentence,
the more useful it is to train a system for the
specific domain I .
score(s) = H
I
(s)?H
O
(s) (2)
In our AL scenario, we have the current train-
ing corpus L and an untranslated corpus U.
CED is applied to select sentences from U
that are (i) different from L (as we would like
to add sentences that add new information to
the model) and (ii) similar to the overall cor-
pus U (as we would like to add sentences that
are common in the untranslated data). Hence
we apply CED and select sentences from U
that have high entropy with respect to L and
low entropy with respect to U (Equation 3).
score(s) = H
U
(s)?H
L
(s) (3)
? CED + n-gram (CEDN). This is an exten-
sion of the CED criterion inspired by the con-
cept of the vocabulary saturation filter (Lewis
and Eetemadi, 2013). CED may select many
very similar sentences, and thus it may be the
case that some of them are redundant. By
post-processing the selection made by CED
with vocabulary saturation we aim to spot
and remove redudant sentences. This works
in two steps. In the first step, all the sentences
from U are scored using the CED metric. In
the second step, we down-rank sentences that
are considered redundant. The top sentence is
selected, and its n-grams are stored in local-
ngrams. For the remaining sentences, one by
one, their n-grams are matched against local-
ngrams. If the intersection between them is
lower than a predefined threshold, the current
sentence is added and localngrams is updated
with the n-grams from the current sentence.
Otherwise the sentence is down-ranked to the
bottom. In our experiments, the value n = 1
produces best results.
3 Experiments and Results
We use technical documentation data taken from
Symantec translation memories for the English?
French (EN?FR) and English?German (EN?DE)
language pairs (both directions) for our experi-
ments. The statistics of the data (training and in-
cremental splits) are shown in Table 1.
All the systems are trained using the
Moses (Koehn et al., 2007) phrase-based sta-
tistical MT system, with IRSTLM (Federico et
al., 2008) for language modelling (n-grams up
to order five) and with the alignment heuristic
grow-diag-final-and.
For the experiments, we considered two settings
for each language pair in each direction. In the
first setting, the initial MT system is trained using
the training set (39,679 and 54,907 sentence pairs
for EN?FR and EN?DE, respectively). Then, a
batch of 500 source sentences is selected from the
incremental dataset according to each of the se-
lection criteria, and translations are obtained with
the initial MT system. These translations are post-
edited and the corrected translations are added to
the training data.
4
We then train a new MT sys-
tem using the updated training data (initial training
data plus PEs of the first batch of sentences). The
updated model will be used to translate the next
batch. The same process is repeated until the in-
cremental dataset is finished (16 and 20 iterations
for English?French and English?German, respec-
tively). For each batch we compute the TER score
between the MT output and the refererence trans-
lations for the sentences of that batch. We then
compute the average TER score for all the batches.
These average scores, for each selection criterion,
are reported in Table 2.
In the second setting, instead of using the whole
training data, we used a subset of (randomly se-
lected) 5,000 sentence pairs for training the initial
MT system and a subset of 20,000 sentences from
the remaining data as the incremental dataset.
Here we take batches of 1,000 sentences (thus 20
batches). The results are shown in Table 3.
The first setting aims to reflect the situation
where a translation job is to be completed for a do-
main for which we have a considerable amount of
data available. Conversely, the second setting re-
flects the situation where a translation job is to be
carried out for a domain with little (if any) avail-
able data.
Dir Random Seq. Ngram CED CEDN
EN?FR 29.64 29.81 28.97 29.25 29.05
FR?EN 27.08 27.04 26.15 26.63 26.39
EN?DE 24.00 24.08 22.34 22.60 22.32
DE?EN 19.36 19.34 17.70 17.97 17.48
Table 2: TER average scores for Setting 1
Dir Random Seq. Ngram CED CEDN
EN?FR 36.23 36.26 35.20 35.48 35.17
FR?EN 33.26 33.34 32.26 32.69 32.17
EN?DE 32.23 32.19 30.58 31.96 29.98
DE?EN 27.24 27.29 26.10 26.73 24.94
Table 3: TER average scores for Setting 2
For Setting 1 (Table 2), the best result is ob-
tained by the CEDN criterion for two out of the
four directions. For EN?FR, n-gram overlap
4
As this study simulates the post-editing, we use the ref-
erences of the translated segments instead of the PEs.
187
Type
EN?FR EN?DE
Sentences Avg. EN SL Avg. FR SL Sentences Avg. EN SL Avg. DE SL
Training 39,679 13.55 15.28 54,907 12.66 12.90
Incremental 8,000 13.74 15.50 10,000 12.38 12.61
Table 1: Data Statistics for English?French and English?German Symantec Translation Memory Data.
SL stands for sentence length, EN stands for English, FR stands for French and DE stands for German
performs slightly better than CEDN (0.08 points
lower) with a decrease of 0.67 and 0.84 points
when compared to the baselines (random and se-
quential, respectively). For FR?EN, n-gram
overlap results in a decrease of 0.93 and 0.89
points compared to the baselines. The decrease in
average TER score is higher for the EN?DE and
for DE?EN directions, i.e. 1.68 and 1.88 points
respectively for CEDN compared to the random
baseline.
In the scenario with limited data available be-
forehand (Table 3), CEDN is the best performing
criterion for all the language directions. For the
EN?FR and FR?EN language pairs, CEDN results
in a decrease of 1.06 and 1.09 points compared to
the random baseline. Again, the decrease is higher
for the EN?DE and DE?EN language pairs, i.e.
2.25 and 2.30 absolute points on average.
Figure 1 shows the TER scores per iteration for
each of the criteria, for the scenario DE?EN Set-
ting 2 (the trends are similar for the other scenar-
ios). The two baselines exhibit slight improve-
ment over the iterations, both starting at around
.35 TER points and finishing at around .25 points.
Conversely, all the three criteria start at very high
scores (in the range [.5,.6]) and then improve con-
siderably to arrive at scores below .1 for the last
iterations. Compared to Ngram and CED, CEDN
reaches better scores earlier on, being the criterion
with the lowest score up to iteration 13.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 RandomSeqNgramCEDCEDN
Iteration
TER
 sco
re
Figure 1: Results per iteration, DE?EN Setting 2
Figure 1 together with Tables 2 and 3 show
that AL for PE-based incremental MT retrain-
ing really works: all AL based methods (Ngram,
CED, CEDN) show strong improvements over
both baselines after the initial 8-9 iterations (Fig-
ure 1) and best performance on the complete incre-
mental data sets, resulting in a noticeable decrease
of the overall TER score (Tables 2 and 3). In six
out of eight scenarios, our novel metric CEDN ob-
tains the best result.
4 Conclusions and Future Work
This paper has presented an application of AL to
MT for dynamically selecting automatic transla-
tions of sentences for human PE, with the aim of
reducing overall PE time in a PE-based incremen-
tal MT retraining scenario in a typical industrial
localisation workflow that aims to capitalise on
human PE as early as possible to avoid repeat mis-
takes.
Our approach makes use of source side informa-
tion only, uses two novel selection criteria based
on cross entropy difference and is tested on indus-
trial data for two language pairs. Our best per-
forming criteria allow the incrementally retrained
MT systems to improve their performance earlier
and reduce the overall TER score by around one
and two absolute points for English?French and
English?German, respectively.
In order to be able to explore a space of selec-
tion criteria and batch sizes, our experiments sim-
ulate PE, in the sense that we use the target ref-
erence (instead of PEs) and approximate PE time
with TER. Given that TER correlates well with PE
time (O?Brien, 2011), we expect AL-based selec-
tion of sentences for human PE to lead to overall
reduction of PE time. In the future work, we plan
to do the experiments using PEs to retrain the sys-
tem and measuring PE time.
In this work, we have taken batches of sentences
(size 500 to 1,000) and do full retraining. As fu-
ture work, we plan to use fully incremental retrain-
ing and perform the selection on a sentence-by-
sentence basis (instead of taking batches).
Finally and importantly, a potential drawback of
our approach is that by dynamically selecting in-
dividual sentences for PE, the human post-editor
looses context, which they may use if processing
sentences sequentially. We will explore the trade
off between the context lost and the productivity
gain achieved, and ways of supplying context (e.g.
previous and following sentence) for real PE.
188
Acknowledgements
This work is supported by Science Foundation
Ireland (Grants 12/TIDA/I2438, 07/CE/I1142 and
12/CE/I2267) as part of the Centre for Next Gen-
eration Localisation (www.cngl.ie) at Dublin City
University. We would like to thank Symantec for
the provision of data sets used in our experiments.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Proceedings of the XIV Machine Transla-
tion Summit, pages 35?42, Nice, France.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Jan
Hajic, Sandra Carberry, and Stephen Clark, editors,
ACL, pages 854?864. The Association for Computer
Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2012. Active learning for
interactive machine translation. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?12, pages 245?254, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In HLT-NAACL, pages 415?
423. The Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL 2007, pages 177?180, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 394?
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
William Lewis and Sauleh Eetemadi. 2013. Dramati-
cally reducing training data size through vocabulary
saturation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 281?291,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online learning approaches in com-
puter assisted translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, ACL, pages 301?308, Sofia, Bulgaria.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT, pages
337?342.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Sharon O?Brien. 2011. Towards predicting
post-editing productivity. Machine Translation,
25(3):197?215, September.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical Report T2009:06.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. Prague Bull. Math.
Linguistics, 93:7?16.
Michel Simard and George Foster. 2013. Pepr: Post-
edit propagation using phrase-based statistical ma-
chine translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 191?198, Nice,
France.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231, Cambridge,
MA.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 120?127, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
189
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 20?23,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
A Web Application for the Diagnostic Evaluation of Machine Translation
over Specific Linguistic Phenomena
Antonio Toral Sudip Kumar Naskar Joris Vreeke Federico Gaspari Declan Groves
School of Computing
Dublin City University
Ireland
{atoral, snaskar, fgaspari, dgroves}@computing.dcu.ie joris.vreeke@dcu.ie
Abstract
This paper presents a web application and a
web service for the diagnostic evaluation of
Machine Translation (MT). These web-based
tools are built on top of DELiC4MT, an open-
source software package that assesses the per-
formance of MT systems over user-defined
linguistic phenomena (lexical, morphological,
syntactic and semantic). The advantage of the
web-based scenario is clear; compared to the
standalone tool, the user does not need to carry
out any installation, configuration or mainte-
nance of the tool.
1 Automatic Evaluation of Machine
Translation beyond Overall Scores
Machine translation (MT) output can be evaluated
using different approaches, which can essentially be
divided into human and automatic, both of which,
however, present a number of shortcomings. Hu-
man evaluation tends to be more reliable in a num-
ber of ways and can be tailored to a variety of situ-
ations, but is rather expensive (both in terms of re-
sources and time) and is difficult to replicate. On
the other hand, standard automatic MT evaluation
metrics such as BLEU (Papineni et al, 2002) and
METEOR (Banerjee and Lavie, 2005) are consid-
erably cheaper and provide faster results, but return
rather crude scores that are difficult to interpret for
MT users and developers alike. Crucially, current
standard automatic MT evaluation metrics also lack
any diagnostic value, i.e. they cannot identify spe-
cific weaknesses in the MT output. Diagnostic in-
formation can be extremely valuable for MT devel-
opers and users, e.g. to improve the performance of
the system or to decide which output is more suited
for particular scenarios.
An interesting alternative to the traditional MT
evaluation metrics is to evaluate the performance
of MT systems over specific linguistic phenomena.
While retaining the main advantage of automatic
metrics (low cost), this approach provides more fine-
grained linguistically-motivated evaluation. The lin-
guistic phenomena, also referred to as linguistic
checkpoints, can be defined in terms of linguistic in-
formation at different levels (lexical, morphological,
syntactic, semantic, etc.) that appear in the source
language. Examples of such linguistic checkpoints,
what translation information they can represent, and
their relevance for MT are provided in Table 1.
Checkpoint Relevance for MT
Lexical Words that can have multiple translations in
the target. For example, the preposition ?de?
in Spanish can be translated into English as
?of? or ?from? depending on the context.
Syntactic Syntactic constructs that are difficult to trans-
late. E.g., a checkpoint containing the se-
quence a noun (noun1) followed by the
preposition ?de?, followed by another noun
(noun2) when translating from Spanish to
English. The equivalent English construct
would be noun2?s noun1, the translation thus
involving some reordering.
Semantic Words with multiple meanings, which possi-
bly correspond to different translations in the
target language. Polysemous words can be
collected from electronic dictionaries such as
WordNet (Miller, 1995).
Table 1: Linguistic Checkpoints
Checkpoints can also be built by combining el-
20
ements from different categories. For example, by
combining lexical and syntantic elements, we could
define a checkpoint for prepositional phrases (syn-
tactic element) which start with the preposition ?de?
(lexical element).
Woodpecker (Zhou et al, 2008) is a tool that per-
forms diagnostic evaluation of MT systems over lin-
guistic checkpoints for English?Chinese. Probably
due to its limitation to one language pair, its pro-
prietary nature as well as rather restrictive licensing
conditions, Woodpecker does not seem to have been
widely used in the community, in spite of its ability
to support diagnostic evaluation.
DELiC4MT1 is an open-source software that fol-
lows the same approach as Woodpecker. However,
DELiC4MT is easily portable to any language pair2
and provides additional functionality such as filter-
ing of noisy checkpoint instances and support for
statistical significance tests. This paper focuses on
the usage of this tool through a web application and
a web service from the user?s perspective. Details
regarding its implementation, evaluation, etc. can
be found in (Toral et al, 2012; Naskar et al, 2011).
2 Web Services for Language Technology
Tools
There exist many freely available language pro-
cessing tools, some of which are distributed under
open-source licenses. In order to use these tools,
they need to be downloaded, installed, configured
and maintained, which results in high cost both in
terms of manual effort and computing resources.
The requirement for in-depth technical knowledge
severely limits the usability of these tools amongst
non-technical users, particularly in our case amongst
translators and post-editors.
Web services introduce a new paradigm in the
way we use software tools where only providers
of the tools are required to have knowledge re-
garding their installation, configuration and mainte-
nance. This enables wider adoption of the tools and
reduces the learning curve for users as the only infor-
mation needed is basic knowledge of the functional-
1http://www.computing.dcu.ie/?atoral/
delic4mt/
2It has already been tested on language pairs involving
the following languages: Arabic, Bulgarian, Dutch, English,
French, German, Hindi, Italian, Turkish and Welsh.
ity and input/output parameters (which can be easily
included, e.g. as part of an online tutorial). While
this paradigm is rather new in the field of compu-
tational linguistics, it is quite mature and successful
in other fields such as bioinformatics (Oinn et al,
2004; Labarga et al, 2007).
Related work includes two web applications in the
area of MT evaluation. iBLEU (Madnani, 2011) or-
ganises BLEU scoring information in a visual man-
ner. Berka et al (2012) perform automatic error de-
tection and classification of MT output.
Figure 1: Web interface for the web service.
3 Demo
The demo presented in this paper consists of a
web service and a web application built on top of
DELiC4MT that allow to assess the performance of
MT systems on different linguistic phenomena de-
21
Figure 2: Screenshot of the web application (visualisation of results).
fined by the user. The following subsections detail
both parts of the demo.
3.1 Web Service
A SOAP-compliant web service3 has been built on
top of DELiC4MT. It receives the following input
parameters (see Figure 1):
1. Word alignment between the source and target
sides of the testset, in the GIZA++ (Och and
Ney, 2003) output format.
2. Linguistic checkpoint defined as a Ky-
bot4 (Vossen et al, 2010) profile.
3. Output of the MT system to be evaluated, in
plain text, tokenised and one sentence per line.
4. Source and target sides of the testset (or
gold standard), in KAF format (Bosma et al,
2009).5
The tool then evaluates the performance of the
MT system (input parameter 3) on the linguistic phe-
nomenon (parameter 2) by following this procedure:
3http://registry.elda.org/services/301
4Kybot profiles can be understood as regular expressions
over KAF documents, http://kyoto.let.vu.nl/svn/
kyoto/trunk/modules/mining_module/
5An XML format for text analysis based on representation
standards from ISO TC37/SC4.
? Occurrences of the linguistic phenomenon (pa-
rameter 2) are identified in the source side of
the testset (parameter 4).
? The equivalent tokens of these occurrences in
the target side (parameter 5) are found by using
word alignment information (parameter 1).
? For each checkpoint instance, the tool checks
how many of the n-grams present in the refer-
ence of the checkpoint instance are contained
in the output produced by the MT system (pa-
rameter 3).
3.2 Web Application
The web application builds a graphical interface on
top of the web service. It allows the user to visualise
the results in a fine-grained manner, the user can see
the performance of the MT system for each single
occurrence of the linguistic phenomenon.
Sample MT output for the ?noun? checkpoint for
the English to French language direction is shown
in Figure 2. Two occurrences of the checkpoint are
shown. The first one regards the source noun ?mr.?
and its translation in the reference ?monsieur?, iden-
tified through word alignments. The alignment (4-
4) indicates that both the source and target tokens
appear at the fifth position (0-based index) in the
sentence. The reference token (?monsieur?) is not
found in the MT output and thus a score of 0/1
22
(0 n-gram matches out of a total of 1 possible n-
gram) is assigned to the MT system for this noun in-
stance. Conversely, the score for the second occur-
rence (?speaker?) is 1/1 since the MT output con-
tains the 1-gram of the reference translation (?ora-
teur?).
The recall-based overall score is shown at the bot-
tom of the figure (0.5025). This is calculated by
summing up the scores (matching n-grams) for all
the occurrences (803) and dividing the result by the
total number of possible n-grams (1598).
4 Conclusions
In this paper we have presented a web applica-
tion and a web service for the diagnostic evalua-
tion of MT output over linguistic phenomena using
DELiC4MT. The tool allows users and developers
of MT systems to easily receive fine-grained feed-
back on the performance of their MT systems over
linguistic checkpoints of their interest. The applica-
tion is open-source, freely available and adaptable to
any language pair.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 under
grant agreements FP7-ICT-4-248531 and PIAP-GA-
2012-324414 and through Science Foundation Ire-
land as part of the CNGL (grant 07/CE/I1142)
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Intrin-
sic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, Proceedings of the
ACL-05 Workshop, pages 65?72, University of Michi-
gan, Ann Arbor, Michigan, USA.
Jan Berka, Ondej Bojar, Mark Fishel, Maja Popovi, and
Daniel Zeman. 2012. Automatic MT Error Anal-
ysis: Hjerson Helping Addicter. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), Istanbul, Turkey.
European Language Resources Association (ELRA).
W. E. Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic
semantic annotation format. In Proceedings of the
GL2009 Workshop on Semantic Annotation, Septem-
ber.
Alberto Labarga, Franck Valentin, Mikael Andersson,
and Rodrigo Lopez. 2007. Web services at the euro-
pean bioinformatics institute. Nucleic Acids Research,
35(Web-Server-Issue):6?11.
Nitin Madnani. 2011. iBLEU: Interactively Debugging
and Scoring Statistical Machine Translation Systems.
In Proceedings of the 2011 IEEE Fifth International
Conference on Semantic Computing, ICSC ?11, pages
213?214, Washington, DC, USA. IEEE Computer So-
ciety.
George A. Miller. 1995. WordNet: a lexical database for
English. Commun. ACM, 38(11):39?41, November.
Sudip Kumar Naskar, Antonio Toral, Federico Gaspari,
and Andy Way. 2011. A Framework for Diagnostic
Evaluation of MT based on Linguistic Checkpoints. In
Proceedings of the 13th Machine Translation Summit,
pages 529?536, Xiamen, China, September.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51, March.
Tom Oinn, Matthew Addis, Justin Ferris, Darren Marvin,
Martin Senger, Mark Greenwood, Tim Carver, Kevin
Glover, Matthew R. Pocock, Anil Wipat, and Peter Li.
2004. Taverna: a tool for the composition and en-
actment of bioinformatics workflows. Bioinformatics,
20(17):3045?3054, November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Antonio Toral, Sudip Kumar Naskar, Federico Gaspari,
and Declan Groves. 2012. DELiC4MT: A Tool for
Diagnostic MT Evaluation over User-defined Linguis-
tic Phenomena. The Prague Bulletin of Mathematical
Linguistics, pages 121?132.
Piek Vossen, German Rigau, Eneko Agirre, Aitor Soroa,
Monica Monachini, and Roberto Bartolini. 2010. KY-
OTO: an open platform for mining facts. In Proceed-
ings of the 6th Workshop on Ontologies and Lexical
Resources, pages 1?10, Beijing, China.
Ming Zhou, Bo Wang, Shujie Liu, Mu Li, Dongdong
Zhang, and Tiejun Zhao. 2008. Diagnostic evalu-
ation of machine translation systems using automati-
cally constructed linguistic check-points. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics - Volume 1, COLING ?08, pages
1121?1128, Stroudsburg, PA, USA. Association for
Computational Linguistics.
23
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 19?27,
Beijing, August 2010
Automatic Extraction of Arabic Multiword Expressions
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith
School of Computing, Dublin City University
{mattia,atoral,ltounsi,ppecina,josef}@computing.dcu.ie
Abstract
In this paper we investigate the automatic
acquisition of Arabic Multiword Expres-
sions (MWE). We propose three com-
plementary approaches to extract MWEs
from available data resources. The first
approach relies on the correspondence
asymmetries between Arabic Wikipedia
titles and titles in 21 different languages.
The second approach collects English
MWEs from Princeton WordNet 3.0,
translates the collection into Arabic us-
ing Google Translate, and utilizes differ-
ent search engines to validate the output.
The third uses lexical association mea-
sures to extract MWEs from a large unan-
notated corpus. We experimentally ex-
plore the feasibility of each approach and
measure the quality and coverage of the
output against gold standards.
1 Introduction
A lexicon of multiword expressions (MWEs) has
a significant importance as a linguistic resource
because MWEs cannot usually be analyzed lit-
erally, or word-for-word. In this paper we ap-
ply three approaches to the extraction of Arabic
MWEs from multilingual, bilingual, and monolin-
gual data sources. We rely on linguistic informa-
tion, frequency counts, and statistical measures to
create a refined list of candidates. We validate the
results with manual and automatic testing.
The paper is organized as follows: in this intro-
duction we describe MWEs and provide a sum-
mary of previous related research. Section 2 gives
a brief description of the data sources used. Sec-
tion 3 presents the three approaches used in our
experiments, and each approach is tested and eval-
uated in its relevant sub-section. In Section 4 we
discuss the results of the experiments. Finally, we
conclude in Section 5.
1.1 What Are Multiword Expressions?
Multiword expressions (MWEs) are defined
as idiosyncratic interpretations that cross word
boundaries or spaces (Sag et al, 2002). The exact
meaning of an MWE is not directly obtained from
its component parts. Accommodating MWEs in
NLP applications has been reported to improve
tasks, such as text mining (SanJuan and Ibekwe-
SanJuan, 2006), syntactic parsing (Nivre and Nils-
son, 2004; Attia, 2006), and Machine Translation
(Deksne, 2008).
There are two basic criteria for identifying
MWEs: first, component words exhibit statisti-
cally significant co-occurrence, and second, they
show a certain level of semantic opaqueness or
non-compositionality. Statistically significant co-
occurrence can give a good indication of how
likely a sequence of words is to form an MWE.
This is particularly interesting for statistical tech-
niques which utilize the fact that a large number
of MWEs are composed of words that co-occur to-
gether more often than can be expected by chance.
The compositionality, or decomposabil-
ity (Villavicencio et al 2004), of MWEs is also
a core issue that presents a challenge for NLP ap-
plications because the meaning of the expression
is not directly predicted from the meaning of the
component words. In this respect, composition-
alily varies between phrases that are highly com-
19
positional, such as,       	  
 	   qfla-
?idatun ?askariyyatun, ?military base?, and those
that show a degree of idiomaticity, such as,       

   Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 213?218,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CNGL-DCU-Prompsit Translation Systems for WMT13
Raphael Rubino?, Antonio Toral?, Santiago Cort?s Va?llo?,
Jun Xie?, Xiaofeng Wu?, Stephen Doherty[, Qun Liu?
?NCLT, Dublin City University, Ireland
?Prompsit Language Engineering, Spain
?ICT, Chinese Academy of Sciences, China
?,[CNGL, Dublin City University, Ireland
?,?{rrubino, atoral, xfwu, qliu}@computing.dcu.ie
?santiago@prompsit.com
?junxie@ict.ac.cn
[stephen.doherty@dcu.ie
Abstract
This paper presents the experiments con-
ducted by the Machine Translation group
at DCU and Prompsit Language Engineer-
ing for the WMT13 translation task. Three
language pairs are considered: Spanish-
English and French-English in both direc-
tions and German-English in that direc-
tion. For the Spanish-English pair, the use
of linguistic information to select paral-
lel data is investigated. For the French-
English pair, the usefulness of the small in-
domain parallel corpus is evaluated, com-
pared to an out-of-domain parallel data
sub-sampling method. Finally, for the
German-English system, we describe our
work in addressing the long distance re-
ordering problem and a system combina-
tion strategy.
1 Introduction
This paper presents the experiments conducted
by the Machine Translation group at DCU1 and
Prompsit Language Engineering2 for the WMT13
translation task on three language pairs: Spanish-
English, French-English and German-English.
For these language pairs, the language and trans-
lation models are built using different approaches
and datasets, thus presented in this paper in sepa-
rate sections.
In Section 2, the systems built for the Spanish-
English pair in both directions are described. We
investigate the use of linguistic information to se-
lect parallel data. In Section 3, we present the sys-
tems built for the French-English pair in both di-
1http://www.nclt.dcu.ie/mt/
2http://www.prompsit.com/
rections. The usefulness of the small in-domain
parallel corpus is evaluated, compared to an out-
of-domain parallel data sub-sampling method. In
Section 4, for the German-English system, aiming
at exploring the long distance reordering problem,
we first describe our efforts in a dependency tree-
to-string approach, before combining different hi-
erarchical systems with a phrase-based system and
show a significant improvement over three base-
line systems.
2 Spanish-English
This section describes the experimental setup for
the Spanish-English language pair.
2.1 Setting
Our setup uses the MOSES toolkit, version
1.0 (Koehn et al, 2007). We use a pipeline
with the phrase-based decoder with standard pa-
rameters, unless noted otherwise. The decoder
uses cube pruning (-cube-pruning-pop-limit 2000
-s 2000), MBR (-mbr-size 800 -mbr-scale 1) and
monotone at punctuation reordering.
Individual language models (LMs), 5-gram and
smoothed using a simplified version of the im-
proved Kneser-Ney method (Chen and Goodman,
1996), are built for each monolingual corpus using
IRSTLM 5.80.01 (Federico et al, 2008). These
LMs are then interpolated with IRSTLM using
the test set of WMT11 as the development set. Fi-
nally, the interpolated LMs are merged into one
LM preserving the weights using SRILM (Stol-
cke, 2002).
We use all the parallel corpora available for
this language pair: Europarl (EU), News Com-
mentary (NC), United Nations (UN) and Common
Crawl (CC). Regarding monolingual corpora, we
use the freely available monolingual corpora (Eu-
213
roparl, News Commentary, News 2007?2012) as
well as the target side of several parallel corpora:
Common Crawl, United Nations and 109 French?
English corpus (only for English as target lan-
guage). Both the parallel and monolingual data
are tokenised and truecased using scripts from the
MOSES toolkit.
2.2 Data selection
The main contribution in our participation regards
the selection of parallel data. We follow the
perplexity-based approach to filter monolingual
data (Moore and Lewis, 2010) extended to filter
parallel data (Axelrod et al, 2011). In our case, we
do not measure perplexity only on word forms but
also using different types of linguistic information
(lemmas and named entities) (Toral, 2013).
We build LMs for the source and target sides
of the domain-specific corpus (in our case NC)
and for a random subset of the non-domain-
specific corpus (EU, UN and CC) of the same size
(number of sentences) of the domain-specific cor-
pus. Each parallel sentence s in the non-domain-
specific corpus is then scored according to equa-
tion 1 where PPIsl(s) is the perplexity of s in
the source side according to the domain-specific
LM and PPOsl(s) is the perplexity of s in the
source side according to the non-domain-specific
LM. PPItl(s) and PPOtl(s) contain the corre-
sponding values for the target side.
score(s) = 12 ? (PPIsl(s)? PPOsl(s))
+(PPItl(s)? PPOtl(s)) (1)
Table 1 shows the results obtained using four
models: word forms (forms), forms and named en-
tities (forms+nes), lemmas (lem) and lemmas and
named entities (lem+nes). Details on these meth-
ods can be found in Toral (2013).
For each corpus we selected two subsets (see in
bold in Table 1), the one for which one method
obtained the best perplexity (top 5% of EU us-
ing forms, 2% of UN using lemmas and 50% of
CC using forms and named entities) and a big-
ger one used to compare the performance in SMT
(top 14% of EU using lemmas and named entities
(lem+nes), top 12% of UN using forms and named
entities and the whole CC). These subsets are used
as training data in our systems.
As we can see in the table, the use of lin-
guistic information allows to obtain subsets with
lower perplexity than using solely word forms, e.g.
1057.7 (lem+nes) versus 1104.8 (forms) for 14%
of EU. The only exception to this is the subset that
comprises the top 5% of EU, where perplexity us-
ing word forms (957.9) is the lowest one.
corpus size forms forms+nes lem lem+nes
EU 5% 957.9 987.2 974.3 1005.514% 1104.8 1058.7 1111.6 1057.7
UN 2% 877.1 969.6 866.6 962.212% 1203.2 1130.9 1183.8 1131.6
CC 50% 573.0 547.2 574.5 546.4100% 560.1 560.1 560.1 560.1
Table 1: Perplexities in data selection
2.3 Results
Table 2 presents the results obtained. Note that
these were obtained during development and thus
the systems are tuned on WMT?s 2011 test set and
tested on WMT?s 2012 test set.
All the systems share the same LM. The first
system (no selection) is trained with the whole NC
and EU. The second (small) and third (big) sys-
tems use as training data the whole NC and sub-
sets of EU (5% and 14%, respectively), UN (2%
and 12%, respectively) and CC (50% and 100%,
respectively), as shown in Table 1.
System #sent. BLEU BLEUcased
no selection 2.1M 31.99 30.96
small 1.4M 33.12 32.05
big 3.8M 33.49 32.43
Table 2: Number of sentences and BLEU scores
obtained on the WMT12 test set for the different
systems on the EN?ES translation task.
The advantage of data selection is clear. The
second system, although smaller in size compared
to the first (1.4M sentence pairs versus 2.1M),
takes its training from a more varied set of data,
and its performance is over one absolute BLEU
point higher.
When comparing the two systems that rely on
data selection, one might expect the one that uses
data with lower perplexity (small) to perform bet-
ter. However, this is not the case, the third system
(big) performing around half an absolute BLEU
point higher than the second (small). This hints
at the fact that perplexity alone is not an optimal
metric for data selection, but size should also be
considered. Note that the size of system 3?s phrase
table is more than double that of system 2.
214
3 French-English
This section describe the particularities of the MT
systems built for the French-English language pair
in both directions. The goal of the experimen-
tal setup presented here is to evaluate the gain of
adding small in-domain parallel data into a trans-
lation system built on a sub-sample of the out-of-
domain parallel data.
3.1 Data Pre-processing
All the available parallel and monolingual data for
the French-English language pair, including the
last versions of LDC Gigaword corpora, are nor-
malised and special characters are escaped using
the scripts provided by the shared task organisers.
Then, the corpora are tokenised and for each lan-
guage a true-case model is built on the concatena-
tion of all the data after removing duplicated sen-
tences, using the scripts included in MOSES dis-
tribution. The corpora are then true-cased before
being used to build the language and the transla-
tion models.
3.2 Language Model
To build our final language models, we first build
LMs on each corpus individually. All the monolin-
gual corpora are considered, as well as the source
or target side of the parallel corpora if the data
are not already in the monolingual data. We build
modified Kneser-Ney discounted 5-gram LMs us-
ing the SRILM toolkit for each corpus and sepa-
rate the LMs in three groups: one in-domain (con-
taining news-commentary and news crawl cor-
pora), another out-of-domain (containing Com-
mon Crawl, Europarl, UN and 109 corpora), and
the last one with LDC Gigaword LMs (the data
are kept separated by news source, as distributed
by LDC). The LMs in each group are linearly in-
terpolated based on their perplexities obtained on
the concatenation of all the development sets from
previous WMT translation tasks. The same devel-
opment corpus is used to linearly interpolate the
in-domain and LDC LMs. We finally obtain two
LMs, one containing out-of-domain data which is
only used to filter parallel data, and another one
containing in-domain data which is used to filter
parallel data, tuning the translation model weights
and at decoding time. Details about the number of
n-grams in each language model are presented in
Table 3.
French English
out in out in
1-gram 4.0 3.3 4.2 10.7
2-gram 43.0 44.0 48.2 161.9
3-gram 54.2 61.8 63.4 256.8
4-gram 99.7 119.2 103.2 502.7
5-gram 136.4 165.0 125.4 680.7
Table 3: Number of n-grams (in millions) for the
in-domain and out-of-domain LMs in French and
English.
3.3 Translation Model
Two phrase-based translation models are built
using MGIZA++ (Gao and Vogel, 2008) and
MOSES3, with the default alignment heuris-
tic (grow-diag-final) and bidirectional reordering
models. The first translation model is in-domain,
built with the news-commentary corpus. The sec-
ond one is built on a sample of all the other paral-
lel corpora available for the French-English lan-
guage pair. Both corpora are cleaned using the
script provided with Moses, keeping the sentences
with a length below 80 words. For the second
translation model, we used the modified Moore-
Lewis method based on the four LMs (two per
language) presented in section 3.2. The sum of
the source and target perplexity difference is com-
puted for each sentence pair of the corpus. We set
an acceptance threshold to keep a limited amount
of sentence pairs. The kept sample finally con-
tains ? 3.7M sentence pairs to train the translation
model. Statistics about this data sample and the
news-commentary corpus are presented in Table 4.
The test set of WMT12 translation task is used to
optimise the weights for the two translation mod-
els with the MERT algorithm. For this tuning step,
the limit of target phrases loaded per source phrase
is set to 50. We also use a reordering constraint
around punctuation marks. The same parameters
are used during the decoding of the test set.
news-commentary sample
tokens FR 4.7M 98.6M
tokens EN 4.0M 88.0M
sentences 156.5k 3.7M
Table 4: Statistics about the two parallel corpora,
after pre-processing, used to train the translation
models.
3Moses version 1.0
215
3.4 Results
The two translation models presented in Sec-
tion 3.3 allow us to design three translation sys-
tems: one using only the in-domain model, one
using only the model built on the sub-sample of
the out-of-domain data, and one using both mod-
els by giving two decoding paths to Moses. For
this latter system, the MERT algorithm is also used
to optimise the translation model weights. Results
obtained on the WMT13 test set, measured with
the official automatic metrics, are presented in Ta-
ble 5. The submitted system is the one built on
the sub-sample of the out-of-domain parallel data.
This system was chosen during the tuning step be-
cause it reached the highest BLEU scores on the
development corpus, slightly above the combina-
tion of the two translation models.
News-Com. Sample Comb.
FR-EN
BLEUdev 26.9 30.0 29.9
BLEU 27.0 30.8 30.4
BLEUcased 26.1 29.8 29.3
TER 62.9 58.9 59.3
EN-FR
BLEUdev 27.1 29.7 29.6
BLEU 26.6 29.6 29.4
BLEUcased 25.8 28.7 28.5
TER 65.1 61.8 62.0
Table 5: BLEU and TER scores obtained by our
systems. BLEUdev is the score obtained on the
development set given by MERT, while BLEU,
BLEUcased and TER are obtained on the test set
given by the submission website.
For both FR-EN and EN-FR tasks, the best re-
sults are reached by the system built on the sub-
sample taken from the out-of-domain parallel data.
Using only News-Commentary to build a trans-
lation model leads to acceptable BLEU scores,
with regards to the size of the training corpus.
When the sub-sample of the out-of-domain par-
allel data is used to build the translation model,
adding a model built on News-Commentary does
not improve the results. The difference between
these two systems in terms of BLEU score (both
cased sensitive and insensitive) indicates that sim-
ilar results can be achieved, however it appears
that the amount of sentence pairs in the sample
is large enough to limit the impact of the small
in-domain corpus parallel. Further experiments
are still required to determine the minimum sam-
ple size needed to outperform both the in-domain
system and the combination of the two translation
models.
4 German-English
In this section we describe our work on German
to English subtask. Firstly we describe the De-
pendency tree to string method which we tried but
unfortunately failed due to short of time. Secondly
we discuss the baseline system and the preprocess-
ing we performed. Thirdly a system combination
method is described.
4.1 Dependency Tree to String Method
Our original plan was to address the long distance
reordering problem in German-English transla-
tion. We use Xie?s Dependency tree to string
method(Xie et al, 2011) which obtains good re-
sults on Chinese to English translation and ex-
hibits good performance at long distance reorder-
ing as our decoder.
We use Stanford dependency parser4 to parse
the English side of the data and Mate-Tool5 for
the German side. The first set of experiments did
not lead to encouraging results and due to insuffi-
cient time, we decide to switch to other decoders,
based on statistical phrase-based and hierarchical
approaches.
4.2 Baseline System
In this section we describe the three baseline sys-
tem we used as well as the preprocessing technolo-
gies and the experiments set up.
4.2.1 Preprocessing and Corpus
We first use the normalisation scripts provided by
WMT2013 to normalise both English and Ger-
man side. Then we escape special characters on
both sides. We use Stanford tokeniser for English
and OpenNLP tokeniser6 for German. Then we
train a true-case model using with Europarl and
News-Commentary corpora, and true-case all the
corpus we used. The parallel corpus is filtered
with the standard cleaning scripts provided with
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://code.google.com/p/mate-tools/
6http://opennlp.sourceforge.net/
models-1.5/
216
MOSES. We split the German compound words
with jWordSplitter7.
All the corpus provided for the shared task are
used for training our translation models, while
WMT2011 and WMT2012 test sets are used to
tune the models parameters. For the LM, we
use all the monolingual data provided, including
LDC Gigaword. Each LM is trained with the
SRILM toolkit, before interpolating all the LMs
according to their weights obtained by minimiz-
ing the perplexity on the tuning set (WMT2011
and WMT2012 test sets). As SRILM can only
interpolate 10 LMs, we first interpolate a LM with
Europarl, News Commentary, News Crawl (2007-
2012, each year individually, 6 separate parts),
then we interpolate a new LM with this interpo-
lated LM and LDC Gigawords (we kept the Gi-
gaword subsets separated according to the news
sources as distributed by LDC, which leads to 7
corpus).
4.2.2 Three baseline systems
We use the data set up described by the former
subsection and build up three baseline systems,
namely PB MOSES (phrase-based), Hiero MOSES
(hierarchical) and CDEC (Dyer et al, 2010). The
motivation of choosing Hierarchical Models is to
address the German-English?s long reorder prob-
lem. We want to test the performance of CDEC and
Hiero MOSES and choose the best. PB MOSES is
used as our benchmark. The three results obtained
on the development and test sets for the three base-
line system and the system combination are shown
in the Table 6.
Development Test
PB MOSES 22.0 24.0
Hiero MOSES 22.1 24.4
CDEC 22.5 24.4
Combination 23.0 24.8
Table 6: BLEU scores obtained by our systems on
the development and test sets for the German to
English translation task.
From the Table 6 we can see that on develop-
ment set, CDEC performs the best, and its much
better than MOSES?s two decoder, but on test
set, Hiero MOSES and CDEC performs as well as
each other, and they both performs better than PB
Model.
7http://www.danielnaber.de/
jwordsplitter/
4.3 System Combination
We also use a word-level combination strat-
egy (Rosti et al, 2007) to combine the three trans-
lation hypotheses. To combine these systems, we
first use the Minimum Bayes-Risk (MBR) (Kumar
and Byrne, 2004) decoder to obtain the 5 best hy-
pothesis as the alignment reference for the Con-
fusion Network (CN) (Mangu et al, 2000). We
then use IHMM (He et al, 2008) to choose the
backbone build the CN and finally search for and
generate the best translation.
We tune the system parameters on development
set with Simple-Simplex algorithm. The param-
eters for system weights are set equal. Other pa-
rameters like language model, length penalty and
combination coefficient are chosen when we see a
good improvement on development set.
5 Conclusion
This paper presented a set of experiments con-
ducted on Spanish-English, French-English and
German-English language pairs. For the Spanish-
English pair, we have explored the use of linguistic
information to select parallel data and use this as
the training for SMT. However, the comparison of
the performance obtained using this method and
the purely statistical one (i.e. perplexity on word
forms) remains to be carried out. Another open
question regards the optimal size of the selected
data. As we have seen, minimum perplexity alone
cannot be considered an optimal metric since us-
ing a larger set, even if it has higher perplexity,
allowed us to obtain notably higher BLEU scores.
The question is then how to decide the optimal size
of parallel data to select.
For the French-English language pair, we inves-
tigated the usefulness of the small in-domain par-
allel data compared to out-of-domain parallel data
sub-sampling. We show that with a sample con-
taining ? 3.7M sentence pairs extracted from the
out-of-domain parallel data, it is not necessary to
use the small domain-specific parallel data. Fur-
ther experiments are still required to determine the
minimum sample size needed to outperform both
the in-domain system and the combination of the
two translation models.
Finally, for the German-English language pair,
we presents our exploitation of long ordering
problem. We compared two hierarchical models
with one phrase-based model, and we also use a
system combination strategy to further improve
217
the translation systems performance.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran) and through Science Foundation Ireland
as part of the CNGL (grant 07/CE/I1142).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12. Association for Computational
Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57. Association for
Computational Linguistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 98?107. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word er-
ror minimization and other applications of confu-
sion networks. Computer Speech & Language,
14(4):373?400.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 228?235.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In John H. L. Hansen and
Bryan L. Pellom, editors, INTERSPEECH. ISCA.
Antonio Toral. 2013. Hybrid Selection of Language
Model Training Data Using Linguistic Information
and Perplexity. In Proceedings of the Second Work-
shop on Hybrid Approaches to Machine Translation
(HyTra), ACL 2013.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226. Association for Computational
Linguistics.
218
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 8?12,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Hybrid Selection of Language Model Training Data Using Linguistic
Information and Perplexity
Antonio Toral
School of Computing
Dublin City University
Dublin, Ireland
atoral@computing.dcu.ie
Abstract
We explore the selection of training data
for language models using perplexity. We
introduce three novel models that make
use of linguistic information and evaluate
them on three different corpora and two
languages. In four out of the six scenar-
ios a linguistically motivated method out-
performs the purely statistical state-of-the-
art approach. Finally, a method which
combines surface forms and the linguisti-
cally motivated methods outperforms the
baseline in all the scenarios, selecting data
whose perplexity is between 3.49% and
8.17% (depending on the corpus and lan-
guage) lower than that of the baseline.
1 Introduction
Language models (LMs) are a fundamental piece
in statistical applications that produce natural lan-
guage text, such as machine translation and speech
recognition. In order to perform optimally, a LM
should be trained on data from the same domain
as the data that it will be applied to. This poses a
problem, because in the majority of applications,
the amount of domain-specific data is limited.
A popular strand of research in recent years to
tackle this problem is that of training data selec-
tion. Given a limited domain-specific corpus and
a larger non-domain-specific corpus, the task con-
sists on finding suitable data for the specific do-
main in the non-domain-specific corpus. The un-
derlying assumption is that a non-domain-specific
corpus, if broad enough, contains sentences sim-
ilar to a domain-specific corpus, which therefore,
would be useful for training models for that do-
main.
This paper focuses on the approach that uses
perplexity for the selection of training data. The
first works in this regard (Gao et al, 2002; Lin
et al, 1997) use the perplexity according to a
domain-specific LM to rank the text segments (e.g.
sentences) of non-domain-specific corpora. The
text segments with perplexity less than a given
threshold are selected.
A more recent method, which can be consid-
ered the state-of-the-art, is Moore-Lewis (Moore
and Lewis, 2010). It considers not only the cross-
entropy1 according to the domain-specific LM but
also the cross-entropy according to a LM built
on a random subset (equal in size to the domain-
specific corpus) of the non-domain-specific cor-
pus. The additional use of a LM from the non-
domain-specific corpus allows to select a subset
of the non-domain-specific corpus which is bet-
ter (the perplexity of a test set of the specific do-
main has lower perplexity on a LM trained on
this subset) and smaller compared to the previ-
ous approaches. The experiment was carried out
for English, using Europarl (Koehn, 2005) as the
domain-specific corpus and LDC Gigaword2 as
the non-domain-specific one.
In this paper we study whether the use of two
types of linguistic knowledge (lemmas and named
entities) can contribute to obtain better results
within the perplexity-based approach.
2 Methodology
We explore the use of linguistic information for
the selection of data to train domain-specific LMs
from non-domain-specific corpora. Our hypothe-
sis is that ranking by perplexity on n-grams that
represent linguistic patterns (rather than n-grams
that represent surface forms) captures additional
information, and thus may select valuable data that
is not selected according solely to surface forms.
We use two types of linguistic information at
1note that using cross-entropy is equivalent to using per-
plexity since they are monotonically related.
2http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2007T07
8
word level: lemmas and named entity categories.
We experiment with the following models:
? Forms (hereafter f), uses surface forms. This
model replicates the Moore-Lewis approach
and is to be considered the baseline.
? Forms and named entities (hereafter fn), uses
surface forms, with the exception of any word
detected as a named entity, which is substi-
tuted by its type (e.g. person, organisation).
? Lemmas (hereafter l), uses lemmas.
? Lemmas and named entities (hereafter ln),
uses lemmas, with the exception of any word
detected as a named entity, which is substi-
tuted by its type.
A sample sentence, according to each of these
models, follows:
f: I declare resumed the session of the
European Parliament
fn: I declare resumed the session of the
NP00O00
l: i declare resume the session of the
european_parliament
ln: i declare resume the session of the
NP00O00
Table 1 shows the number of n-grams on LMs
built on the English side of News Commentary v8
(hereafter NC) for each of the models. Regarding
1-grams, compared to f, the substitution of named
entities by their categories (fn) results in smaller
vocabulary size (-24.79%). Similarly, the vocabu-
lary is reduced for the models l (-8.39%) and ln (-
44.18%). Although not a result in itself, this might
be an indication that using linguistically motivated
models could be useful to deal with data sparsity.
n f fn l ln
1 65076 48945 59619 36326
2 981077 847720 835825 702118
3 2624800 2382629 2447759 2212709
4 3633724 3412719 3523888 3325311
5 3929751 3780064 3856917 3749813
Table 1: Number of n-grams in LMs built using
the different models
Our procedure follows that of the Moore-Lewis
method. We build LMs for the domain-specific
corpus and for a random subset of the non-
domain-specific corpus of the same size (number
of sentences) of the domain-specific corpus. Each
sentence s in the non-domain-specific corpus is
then scored according to equation 1 where PPI(s)
is the perplexity of s according to the domain-
specific LM and PPO(s) is the perplexity of s ac-
cording to the non-domain-specific LM.
score(s) = PPI(s)? PPO(s) (1)
We build LMs for the domain-specific and non-
domain-specific corpora using the four models
previously introduced. Then we rank the sen-
tences of the non-domain-specific corpus for each
of these models and keep the highest ranked sen-
tences according to a threshold. Finally, we build a
LM on the set of sentences selected3 and compute
the perplexity of the test set on this LM.
We also investigate the combination of the four
models. The procedure is fairly straightforward:
given the sentences selected by all the models for
a given threshold, we iterate through these sen-
tences following the ranking order and keeping all
the distinct sentences selected until we obtain a set
of sentences whose size is the one indicated by the
threshold. I.e. we add to our distinct set of sen-
tences first the top ranked sentence by each of the
methods, then the sentence ranked second by each
method, and so on.
3 Experiments
3.1 Setting
We use corpora from the translation task at
WMT13.4 Our domain-specific corpus is NC, and
we carry out experiments with three non-domain-
specific corpora: a subset of Common Crawl5
(hereafter CC), Europarl version 7 (hereafter EU),
and United Nations (Eisele and Chen, 2010) (here-
after UN). We use the test data from WMT12
(newstest2012) as our test set. We carry out ex-
periments on two languages for which these cor-
pora are available: English (referred to as ?en? in
tables) and Spanish (?es? in tables).
We test the methods on three very different non-
domain-specific corpora, both in terms of the top-
ics that they cover (text crawled from web in CC,
parliamentary speeches in EU and official docu-
ments from United Nations in UN) and their size
3For the linguistic methods we replace the sentences se-
lected (which contain lemmas and/or named entities) with the
corresponding sentences in the original corpus (containing
only word forms).
4http://www.statmt.org/wmt13/
translation-task.html
5http://commoncrawl.org/
9
(around 2 million sentences both for CC and EU,
and around 11 million for UN). This can be con-
sidered as a contribution of this paper since pre-
vious works such as Moore and Lewis (2010)
and, more recently, Axelrod et al (2011) test the
Moore-Lewis method on only one non-domain-
specific corpus: LDC Gigaword and an unpub-
lished general-domain corpus, respectively.
All the LMs are built with IRSTLM
5.80.01 (Federico et al, 2008), use up to 5-grams
and are smoothed using a simplified version of
the improved Kneser-Ney method (Chen and
Goodman, 1996). For lemmatisation and named
entity recognition we use Freeling 3.0 (Padro? and
Stanilovsky, 2012). The corpora are tokenised
and truecased using scripts from the Moses
toolkit (Koehn et al, 2007).
3.2 Experiments with Different Models
Figures 1, 2 and 3 show the perplexities obtained
by each method on different subsets selected from
the English corpora CC, EU and UN, respectively.
We obtain these subsets according to different
thresholds, i.e. percentages of sentences selected
from the non-domain-specific corpus. These are
the first 164 ranked sentences,
1
32 ,
1
16 ,
1
8 ,
1
4 ,
1
2 and
1.6 Corresponding figures for Spanish are omitted
due to the limited space available and also because
the trends in those figures are very similar.
64 32 16 8 4 2 1600
650
700
750
800
850
900
950
1000
1050
1100
ffnlln
Size 1/x
Perp
lexity
Figure 1: Results of the different methods on CC
In all the figures, the results are very similar re-
gardless of the use of lemmas. The use of named
entities, however, produces substantially different
results. The models that do not use named entity
categories obtain the best results for lower thresh-
olds (up to 1/32 for CC, and up to 1/16 both for
6An additional threshold, 1128 , is used for the United Na-
tions corpus
64 32 16 8 4 2 11000
1100
1200
1300
1400
1500
1600
779ff9
nlSiz1e 
/ixP
fi lrp
Figure 2: Results of the different methods on EU
643 21 84 62 3 1 4 6055
6655
6855
6755
6955
6055
ffnlln
Size 6/x
Perp
lexity
Figure 3: Results of the different methods on UN
EU and UN). If the best perplexity is obtained
with a lower threshold than this (the case of EU,
1/32, and UN, 1/64), then methods that do not
use named entities obtain the best result. How-
ever, if the optimal perplexity is obtained with a
higher threshold (the case of CC, 1/2), then using
named entities yields the best result.
Table 2 presents the results for each model. For
each scenario (corpus and language combination),
we show the threshold for which the best result is
obtained (column best). The perplexity obtained
on data selected by each model is shown in the
subsequent columns. For the linguistic methods,
we also show the comparison of their performance
to the baseline (as percentages, columns diff). The
perplexity when using the full corpus is shown
(column full) together with the comparison of this
result to the best method (last column diff).
The results, as previously seen in Figures 1, 2
and 3, differ with respect to the corpus but follow
similar trends across languages. For CC we obtain
the best results using named entities. The model
ln obtains the best result for English (5.54% lower
10
corpus best f fn diff l diff ln diff full diff
cc en 1/2 660.77 625.62 -5.32 660.58 -0.03 624.19 -5.54 638.24 -2.20
eu en 1/32 1072.98 1151.13 7.28 1085.66 1.18 1170.00 9.04 1462.61 -26.64
un en 1/64 984.08 1127.55 14.58 979.06 -0.51 1121.45 13.96 1939.44 -49.52
cc es 1/2 499.22 480.17 -3.82 498.93 -0.06 480.45 -3.76 481.96 -0.37
eu es 1/16 788.62 813.32 3.13 801.50 1.63 825.13 4.63 960.06 -17.86
un es 1/32 725.93 773.89 6.61 723.37 -0.35 771.25 6.24 1339.78 -46.01
Table 2: Results for the different models
perplexity than the baseline), while the model fn
obtains the best result for Spanish (3.82%), al-
though in both cases the difference between these
two models is rather small.
For the other corpora, the best results are ob-
tained without named entities. In the case of EU,
the baseline obtains the best result, although the
model l is not very far (1.18% higher perplexity
for English and 1.63% for Spanish). This trend
is reversed for UN, the model l obtaining the best
scores but close to the baseline (-0.51%, -0.35%).
3.3 Experiments with the Combination of
Models
Table 3 shows the perplexities obtained by the
method that combines the four models (column
comb) for the threshold that yielded the best re-
sult in each scenario (see Table 2), compares these
results (column diff) to those obtained by the base-
line (column f) and shows the percentage of sen-
tences that this method inspected from the sen-
tences selected by the individual methods (column
perc).
corpus f comb diff perc
cc en 660.77 613.83 -7.10 76.90
eu en 1072.98 1035.51 -3.49 70.51
un en 984.08 908.47 -7.68 74.58
cc es 499.22 478.87 -4.08 74.61
eu es 788.62 748.22 -5.12 68.05
un es 725.93 666.62 -8.17 74.32
Table 3: Results of the combination method
The combination method outperforms the base-
line and any of the individual linguistic models
in all the scenarios. The perplexity obtained by
combining the models is substantially lower than
that obtained by the baseline (ranging from 3.49%
to 8.17%). In all the scenarios, the combination
method takes its sentences from roughly the top
70% sentences ranked by the individual methods.
4 Conclusions and Future Work
This paper has explored the use of linguistic infor-
mation (lemmas and named entities) for the task
of training data selection for LMs. We have intro-
duced three linguistically motivated models, and
compared them to the state-of-the-art method for
perplexity-based data selection across three dif-
ferent corpora and two languages. In four out
of these six scenarios a linguistically motivated
method outperforms the state-of-the-art approach.
We have also presented a method which com-
bines surface forms and the three linguistically
motivated methods. This combination outper-
forms the baseline in all the scenarios, select-
ing data whose perplexity is between 3.49% and
8.17% (depending on the corpus and language)
lower than that of the baseline.
Regarding future work, we have several plans.
One interesting experiment would be to apply
these models to a morphologically-rich language,
to check if, as hypothesised, these models deal bet-
ter with sparse data.
Another strand regards the application of these
models to filter parallel corpora, e.g. following the
extension of the Moore-Lewis method (Axelrod et
al., 2011) or in combination with other methods
which are deemed to be more suitable for parallel
data, e.g. (Mansour et al, 2011).
We have used one type of linguistic informa-
tion in each LM, but another possibility is to com-
bine different pieces of linguistic information in
a single LM, e.g. following a hybrid LM that
uses words and tags, depending of the frequency
of each type (Ruiz et al, 2012).
Given the fact that the best result is obtained
with different models depending on the corpus, it
would be worth to investigate whether given a new
corpus, one could predict the best method to be ap-
plied and the threshold for which one could expect
to obtain the minimum perplexity.
11
Acknowledgments
We would like to thank Raphae?l Rubino for in-
sightful conversations. The research leading to
these results has received funding from the Eu-
ropean Union Seventh Framework Programme
FP7/2007-2013 under grant agreements PIAP-
GA-2012-324414 and FP7-ICT-2011-296347.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andreas Eisele and Yu Chen. 2010. Multiun: A
multilingual corpus from united nation documents.
In Nicoletta Calzolari, Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, LREC. Eu-
ropean Language Resources Association.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to sta-
tistical language modeling for chinese. 1(1):3?33,
March.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Keh-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document clas-
sification and multiple domain-specific language
models. In George Kokkinakis, Nikos Fakotakis,
and Evangelos Dermatas, editors, EUROSPEECH.
ISCA.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining translation and language model
scoring for domain-specific data filtering. In In-
ternational Workshop on Spoken Language Trans-
lation, pages 222?229, San Francisco, California,
USA, December.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Nick Ruiz, Arianna Bisazza, Roldano Cattoni, and
Marcello Federico. 2012. FBK?s Machine Trans-
lation Systems for IWSLT 2012?s TED Lectures. In
Proceedings of the 9th International Workshop on
Spoken Language Translation (IWSLT).
12
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171?177,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Abu-MaTran at WMT 2014 Translation Task:
Two-step Data Selection and RBMT-Style Synthetic Rules
Raphael Rubino
?
, Antonio Toral
?
, Victor M. S
?
anchez-Cartagena
??
,
Jorge Ferr
?
andez-Tordera
?
, Sergio Ortiz-Rojas
?
, Gema Ram??rez-S
?
anchez
?
,
Felipe S
?
anchez-Mart??nez
?
, Andy Way
?
?
Prompsit Language Engineering, S.L., Elche, Spain
{rrubino,vmsanchez,jferrandez,sortiz,gramirez}@prompsit.com
?
NCLT, School of Computing, Dublin City University, Ireland
{atoral,away}@computing.dcu.ie
?
Dep. Llenguatges i Sistemes Inform`atics, Universitat d?Alacant, Spain
fsanchez@dlsi.ua.es
Abstract
This paper presents the machine trans-
lation systems submitted by the Abu-
MaTran project to the WMT 2014 trans-
lation task. The language pair concerned
is English?French with a focus on French
as the target language. The French to En-
glish translation direction is also consid-
ered, based on the word alignment com-
puted in the other direction. Large lan-
guage and translation models are built us-
ing all the datasets provided by the shared
task organisers, as well as the monolin-
gual data from LDC. To build the trans-
lation models, we apply a two-step data
selection method based on bilingual cross-
entropy difference and vocabulary satura-
tion, considering each parallel corpus in-
dividually. Synthetic translation rules are
extracted from the development sets and
used to train another translation model.
We then interpolate the translation mod-
els, minimising the perplexity on the de-
velopment sets, to obtain our final SMT
system. Our submission for the English to
French translation task was ranked second
amongst nine teams and a total of twenty
submissions.
1 Introduction
This paper presents the systems submitted by the
Abu-MaTran project (runs named DCU-Prompsit-
UA) to the WMT 2014 translation task for the
English?French language pair. Phrase-based sta-
tistical machine translation (SMT) systems were
submitted, considering the two translation direc-
tions, with the focus on the English to French di-
rection. Language models (LMs) and translation
models (TMs) are trained using all the data pro-
vided by the shared task organisers, as well as
the Gigaword monolingual corpora distributed by
LDC.
To train the LMs, monolingual corpora and the
target side of the parallel corpora are first used
individually to train models. Then the individ-
ual models are interpolated according to perplex-
ity minimisation on the development sets.
To train the TMs, first a baseline is built us-
ing the News Commentary parallel corpus. Sec-
ond, each remaining parallel corpus is processed
individually using bilingual cross-entropy differ-
ence (Axelrod et al., 2011) in order to sepa-
rate pseudo in-domain and out-of-domain sen-
tence pairs, and filtering the pseudo out-of-
domain instances with the vocabulary saturation
approach (Lewis and Eetemadi, 2013). Third,
synthetic translation rules are automatically ex-
tracted from the development set and used to train
another translation model following a novel ap-
proach (S?anchez-Cartagena et al., 2014). Finally,
we interpolate the four translation models (base-
line, in-domain, filtered out-of-domain and rules)
by minimising the perplexity obtained on the de-
velopment sets and investigate the best tuning and
decoding parameters.
The reminder of this paper is organised as fol-
lows: the datasets and tools used in our experi-
ments are described in Section 2. Then, details
about the LMs and TMs are given in Section 3 and
Section 4 respectively. Finally, we evaluate the
performance of the final SMT system according to
different tuning and decoding parameters in Sec-
tion 5 before presenting conclusions in Section 6.
171
2 Datasets and Tools
We use all the monolingual and parallel datasets
in English and French provided by the shared task
organisers, as well as the LDC Gigaword for the
same languages
1
. For each language, a true-case
model is trained using all the data, using the train-
truecaser.perl script included in the MOSES tool-
kit (Koehn et al., 2007).
Punctuation marks of all the monolingual and
parallel corpora are then normalised using the
script normalize-punctuation.perl provided by the
organisers, before being tokenised and true-cased
using the scripts distributed with the MOSES tool-
kit. The same pre-processing steps are applied to
the development and test sets. As development
sets, we used all the test sets from previous years
of WMT, from 2008 to 2013 (newstest2008-2013).
Finally, the training parallel corpora are cleaned
using the script clean-corpus-n.perl, keeping the
sentences longer than 1 word, shorter than 80
words, and with a length ratio between sentence
pairs lower than 4.
2
The statistics about the cor-
pora used in our experiments after pre-processing
are presented in Table 1.
For training LMs we use KENLM (Heafield et
al., 2013) and the SRILM tool-kit (Stolcke et al.,
2011). For training TMs, we use MOSES (Koehn
et al., 2007) version 2.1 with MGIZA++ (Och and
Ney, 2003; Gao and Vogel, 2008). These tools are
used with default parameters for our experiments
except when explicitly said.
The decoder used to generate translations is
MOSES using features weights optimised with
MERT (Och, 2003). As our approach relies on
training individual TMs, one for each parallel cor-
pus, our final TM is obtained by linearly interpo-
lating the individual ones. The interpolation of
TMs is performed using the script tmcombine.py,
minimising the cross-entropy between the TM
and the concatenated development sets from 2008
to 2012 (noted newstest2008-2012), as described
in Sennrich (2012). Finally, we make use of the
findings from WMT 2013 brought by the win-
ning team (Durrani et al., 2013) and decide to use
the Operation Sequence Model (OSM), based on
minimal translation units and Markov chains over
sequences of operations, implemented in MOSES
1
LDC2011T07 English Gigaword Fifth Edition,
LDC2011T10 French Gigaword Third Edition
2
This ratio was empirically chosen based on words fertil-
ity between English and French.
Corpus Sentences (k) Words (M)
Monolingual Data ? English
Europarl v7 2,218.2 59.9
News Commentary v8 304.2 7.4
News Shuffled 2007 3,782.5 90.2
News Shuffled 2008 12,954.5 308.1
News Shuffled 2009 14,680.0 347.0
News Shuffled 2010 6,797.2 157.8
News Shuffled 2011 15,437.7 358.1
News Shuffled 2012 14,869.7 345.5
News Shuffled 2013 21,688.4 495.2
LDC afp 7,184.9 869.5
LDC apw 8,829.4 1,426.7
LDC cna 618.4 45.7
LDC ltw 986.9 321.1
LDC nyt 5,327.7 1,723.9
LDC wpb 108.8 20.8
LDC xin 5,121.9 423.7
Monolingual Data ? French
Europarl v7 2,190.6 63.5
News Commentary v8 227.0 6.5
News Shuffled 2007 119.0 2.7
News Shuffled 2008 4,718.8 110.3
News Shuffled 2009 4,366.7 105.3
News Shuffled 2010 1,846.5 44.8
News Shuffled 2011 6,030.1 146.1
News Shuffled 2012 4,114.4 100.8
News Shuffled 2013 9,256.3 220.2
LDC afp 6,793.5 784.5
LDC apw 2,525.1 271.3
Parallel Data
10
9
Corpus
21,327.1
549.0 (EN)
642.5 (FR)
Common Crawl 3,168.5
76.0 (EN)
82.7 (FR)
Europarl v7 1,965.5
52.5 (EN)
56.7 (FR)
News Commentary v9 181.3
4.5 (EN)
5.3 (FR)
UN 12,354.7
313.4 (EN)
356.5 (FR)
Table 1: Data statistics after pre-processing of the
monolingual and parallel corpora used in our ex-
periments.
and introduced by Durrani et al. (2011).
3 Language Models
The LMs are trained in the same way for both
languages. First, each monolingual and parallel
corpus is considered individually (except the par-
allel version of Europarl and News Commentary)
and used to train a 5-gram LM with the modified
Kneser-Ney smoothing method. We then interpo-
late the individual LMs using the script compute-
best-mix available with the SRILM tool-kit (Stol-
cke et al., 2011), based on their perplexity scores
on the concatenation of the development sets from
2008 to 2012 (the 2013 version is held-out for the
tuning of the TMs).
172
The final LM for French contains all the word
sequences from 1 to 5-grams contained in the
training corpora without any pruning. However,
with the computing resources at our disposal, the
English LMs could not be interpolated without
pruning non-frequent n-grams. Thus, n-grams
with n ? [3; 5] with a frequency lower than 2 were
removed. Details about the final LMs are given in
Table 2.
1-gram 2-gram 3-gram 4-gram 5-gram
English 13.4 198.6 381.2 776.3 1,068.7
French 6.0 75.5 353.2 850.8 1,354.0
Table 2: Statistics, in millions of n-grams, of the
interpolated LMs.
4 Translation Models
In this Section, we describe the TMs trained for
the shared task. First, we present the two-step data
selection process which aims to (i) separate in and
out-of-domain parallel sentences and (ii) reduce
the total amount of out-of-domain data. Second,
a novel approach for the automatic extraction of
translation rules and their use to enrich the phrase
table is detailed.
4.1 Parallel Data Filtering and Vocabulary
Saturation
Amongst the parallel corpora provided by the
shared task organisers, only News Commentary
can be considered as in-domain regarding the de-
velopment and test sets. We use this training
corpus to build our baseline SMT system. The
other parallel corpora are individually filtered us-
ing bilingual cross-entropy difference (Moore and
Lewis, 2010; Axelrod et al., 2011). This data
filtering method relies on four LMs, two in the
source and two in the target language, which
aim to model particular features of in and out-of-
domain sentences.
We build the in-domain LMs using the source
and target sides of the News Commentary paral-
lel corpus. Out-of-domain LMs are trained on a
vocabulary-constrained subset of each remaining
parallel corpus individually using the SRILM tool-
kit, which leads to eight models (four in the source
language and four in the target language).
3
3
The subsets contain the same number of sentences and
the same vocabulary as News Commentary.
Then, for each out-of-domain parallel corpus,
we compute the bilingual cross-entropy difference
of each sentence pair as:
[H
in
(S
src
)?H
out
(S
src
)] + [H
in
(S
trg
)?H
out
(S
trg
)] (1)
where S
src
and S
trg
are the source and the tar-
get sides of a sentence pair, H
in
and H
out
are
the cross-entropies of the in and out-of-domain
LMs given a sentence pair. The sentence pairs are
then ranked and the lowest-scoring ones are taken
to train the pseudo in-domain TMs. However,
the cross-entropy difference threshold required to
split a corpus in two parts (pseudo in and out-of-
domain) is usually set empirically by testing sev-
eral subset sizes of the top-ranked sentence pairs.
This method is costly in our setup as it would lead
to training and evaluating multiple SMT systems
for each of the pseudo in-domain parallel corpora.
In order to save time and computing power,
we consider only pseudo in-domain sentence pairs
those with a bilingual cross-entropy difference be-
low 0, i.e. those deemed more similar to the
in-domain LMs than to the out-of-domain LMs
(H
in
< H
out
). A sample of the distribution of
scores for the out-of-domain corpora is shown in
Figure 1. The resulting pseudo in-domain corpora
are used to train individual TMs, as detailed in Ta-
ble 3.
-4
-2
 0
 2
 4
 6
 8
 10
0 2k 4k 6k 8k 10k
Bilin
gual
 Cro
ss-E
ntro
py D
iffer
ence
Sentence Pairs
Common CrawlEuroparl10^9UN
Figure 1: Sample of ranked sentence-pairs (10k)
of each of the out-of-domain parallel corpora with
bilingual cross-entropy difference
The results obtained using the pseudo in-
domain data show BLEU (Papineni et al., 2002)
scores superior or equal to the baseline score.
Only the Europarl subset is slightly lower than
the baseline, while the subset taken from the 10
9
corpus reaches the highest BLEU compared to the
other systems (30.29). This is mainly due to the
173
size of this subset which is ten times larger than
the one taken from Europarl. The last row of Ta-
ble 3 shows the BLEU score obtained after interpo-
lating the four pseudo in-domain translation mod-
els. This system outperforms the best pseudo in-
domain one by 0.5 absolute points.
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 208.3 27.73
Europarl 142.0 27.63
10
9
Corpus 1,442.4 30.29
UN 642.4 28.91
Interpolation - 30.78
Table 3: Number of sentence pairs and BLEU
scores reported by MERT on English?French new-
stest2013 for the pseudo in-domain corpora ob-
tained by filtering the out-of-domain corpora with
bilingual cross-entropy difference. The interpola-
tion of pseudo in-domain models is evaluated in
the last row.
After evaluating the pseudo in-domain parallel
data, the remaining sentence pairs for each cor-
pora are considered out-of-domain according to
our filtering approach. However, they may still
contain useful information, thus we make use of
these corpora by building individual TMs for each
corpus (in a similar way we built the pseudo in-
domain models). The total amount of remaining
data (more than 33 million sentence pairs) makes
the training process costly in terms of time and
computing power. In order to reduce these costs,
sentence pairs with a bilingual cross-entropy dif-
ference higher than 10 were filtered out, as we no-
ticed that most of the sentences above this thresh-
old contain noise (non-alphanumeric characters,
foreign languages, etc.).
We also limit the size of the remaining data by
applying the vocabulary saturation method (Lewis
and Eetemadi, 2013). For the out-of-domain sub-
set of each corpus, we traverse the sentence pairs
in the order they are ranked by perplexity differ-
ence and filter out those sentence pairs for which
we have seen already each 1-gram at least 10
times. Each out-of-domain subset from each par-
allel corpus is then used to train a TM before inter-
polating them to create the pseudo out-of-domain
TM. The results reported by MERT obtained on
the newstest2013 development set are detailed in
Table 4.
Mainly due to the sizes of the pseudo out-of-
Corpus Sentences (k) BLEU
dev
Baseline 181.3 27.76
Common Crawl 1,598.7 29.84
Europarl 461.9 28.87
10
9
Corpus 5,153.0 30.50
UN 1,707.3 29.03
Interpolation - 31.37
Table 4: Number of sentence pairs and BLEU
scores reported by MERT on English?French
newstest2013 for the pseudo out-of-domain cor-
pora obtained by filtering the out-of-domain cor-
pora with bilingual cross-entropy difference, keep-
ing sentence pairs below an entropy score of 10
and applying vocabulary saturation. The interpo-
lation of pseudo out-of-domain models is evalu-
ated in the last row.
domain subsets, the reported BLEU scores are
higher than the baseline for the four individual
SMT systems and the interpolated one. This latter
system outperforms the baseline by 3.61 absolute
points. Compared to the results obtained with the
pseudo in-domain data, we observe a slight im-
provement of the BLEU scores using the pseudo
out-of-domain data. However, despite the com-
paratively larger sizes of the latter datasets, the
BLEU scores reached are not that higher. For in-
stance with the 10
9
corpus, the pseudo in and out-
of-domain subsets contain 1.4 and 5.1 million sen-
tence pairs respectively, and the two systems reach
30.3 and 30.5 BLEU. These scores indicate that
the pseudo in-domain SMT systems are more ef-
ficient on the English?French newstest2013 devel-
opment set.
4.2 Extraction of Translation Rules
A synthetic phrase-table based on shallow-transfer
MT rules and dictionaries is built as follows. First,
a set of shallow-transfer rules is inferred from the
concatenation of the newstest2008-2012 develop-
ment corpora exactly in the same way as in the
UA-Prompsit submission to this translation shared
task (S?anchez-Cartagena et al., 2014). In sum-
mary, rules are obtained from a set of bilingual
phrases extracted from the parallel corpus after
its morphological analysis and part-of-speech dis-
ambiguation with the tools in the Apertium rule-
based MT platform (Forcada et al., 2011).
The extraction algorithm commonly used in
phrase-based SMT is followed with some added
heuristics which ensure that the bilingual phrases
174
extracted are compatible with the bilingual dic-
tionary. Then, many different rules are generated
from each bilingual phrase; each of them encodes
a different degree of generalisation over the partic-
ular example it has been extracted from. Finally,
the minimum set of rules which correctly repro-
duces all the bilingual phrases is found based on
integer linear programming search (Garfinkel and
Nemhauser, 1972).
Once the rules have been inferred, the phrase
table is built from them and the original rule-
based MT dictionaries, following the method
by S?anchez-Cartagena et al. (2011), which was
one of winning systems
4
(together with two on-
line SMT systems) in the pairwise manual evalu-
ation of the WMT11 English?Spanish translation
task (Callison-Burch et al., 2011). This phrase-
table is then interpolated with the baseline TM and
the results are presented in Table 5. A slight im-
provement over the baseline is observed, which
motivates the use of synthetic rules in our final MT
system. This small improvement may be related
to the small coverage of the Apertium dictionar-
ies: the English?French bilingual dictionary has a
low number of entries compared to more mature
language pairs in Apertium which have around 20
times more bilingual entries.
System BLEU
dev
Baseline 27.76
Baseline+Rules 28.06
Table 5: BLEU scores reported by MERT on
English?French newstest2013 for the baseline
SMT system standalone and with automatically
extracted translation rules.
5 Tuning and Decoding
We present in this Section a short selection of our
experiments, amongst 15+ different configura-
tions, conducted on the interpolation of TMs, tun-
ing and decoding parameters. We first interpolate
the four TMs: the baseline, the pseudo in and out-
of-domain, and the translation rules, minimising
the perplexity obtained on the concatenated de-
velopment sets from 2008 to 2012 (newstest2008-
2012). We investigate the use of OSM trained on
pseudo in-domain data only or using all the paral-
lel data available. Finally, we make variations of
4
No other system was found statistically significantly bet-
ter using the sign test at p ? 0.1.
the number of n-bests used by MERT.
Results obtained on the development set new-
stest2013 are reported in Table 6. These scores
show that adding OSM to the interpolated trans-
lation models slightly degrades BLEU. However,
by increasing the number of n-bests considered by
MERT to 200-bests, the SMT system with OSM
outperforms the systems evaluated previously in
our experiments. Adding the synthetic translation
rules degrades BLEU (as indicated by the last row
in the Table), thus we decide to submit two sys-
tems to the shared task: one without and one with
synthetic rules. By submitting a system without
synthetic rules, we also ensure that our SMT sys-
tem is constrained according to the shared task
guidelines.
System BLEU
dev
Baseline 27.76
+ pseudo in + pseudo out 31.93
+ OSM 31.90
+ MERT 200-best 32.21
+ Rules 32.10
Table 6: BLEU scores reported by MERT on
English?French newstest2013 development set.
As MERT is not suitable when a large number
of features are used (our system uses 19 fetures),
we switch to the Margin Infused Relaxed Algo-
rithm (MIRA) for our submitted systems (Watan-
abe et al., 2007). The development set used is
newstest2012, as we aim to select the best decod-
ing parameters according to the scores obtained
when decoding the newstest2013 corpus, after de-
truecasing and de-tokenising using the scripts dis-
tributed with MOSES. This setup allowed us to
compare our results with the participants of the
translation shared task last year. We pick the de-
coding parameters leading to the best results in
terms of BLEU and decode the official test set of
WMT14 newstest2014. The results are reported in
Table 7. Results on newstest2013 show that the de-
coding parameters investigation leads to an over-
all improvement of 0.1 BLEU absolute. The re-
sults on newstest2014 show that adding synthetic
rules did not help improving BLEU and degraded
slightly TER (Snover et al., 2006) scores.
In addition to our English?French submission,
we submitted a French?English translation. Our
French?English MT system is built on the align-
ments obtained from the English?French direc-
tion. The training processes between the two sys-
175
System BLEU13A TER
newstest2013
Best tuning 31.02 60.77
cube-pruning (pop-limit 10000) 31.04 60.71
increased table-limit (100) 31.06 60.77
monotonic reordering 31.07 60.69
Best decoding 31.14 60.66
newstest2014
Best decoding 34.90 54.70
Best decoding + Rules 34.90 54.80
Table 7: Case sensitive results obtained with
our final English?French SMT system on new-
stest2013 when experimenting with different de-
coding parameters. The best parameters are kept
to translate the WMT14 test set (newstest2014)
and official results are reported in the last two
rows.
tems are identical, except for the synthetic rules
which are not extracted for the French?English
direction. Tuning and decoding parameters for
this latter translation direction are the best ones
obtained in our previous experiments on this
shared task. The case-sensitive scores obtained
for French?English on newstest2014 are 35.0
BLEU13A and 53.1 TER, which ranks us at the
fifth position for this translation direction.
6 Conclusion
We have presented the MT systems developed by
the Abu-MaTran project for the WMT14 trans-
lation shared task. We focused on the French?
English language pair and particularly on the
English?French direction. We have used a two-
step data selection process based on bilingual
cross-entropy difference and vocabulary satura-
tion, as well as a novel approach for the extraction
of synthetic translation rules and their use to en-
rich the phrase table. For the LMs and the TMs,
we rely on training individual models per corpus
before interpolating them by minimising perplex-
ity according to the development set. Finally, we
made use of the findings of WMT13 by including
an OSM model.
Our English?French translation system was
ranked second amongst nine teams and a total of
twenty submissions, while our French?English
submission was ranked fifth. As future work,
we plan to investigate the effect of adding to the
phrase table synthetic translation rules based on
larger dictionaries. We also would like to study the
link between OSM and the different decoding pa-
rameters implemented in MOSES, as we observed
inconsistent results in our experiments.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain Adaptation Via Pseudo In-domain
Data Selection. In Proceedings of EMNLP, pages
355?362.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceed-
ings of WMT, pages 22?64.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL/HLT,
pages 1045?1054.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of WMT, pages 112?119.
Mikel L Forcada, Mireia Ginest??-Rosell, Jacob Nord-
falk, Jim O?Regan, Sergio Ortiz-Rojas, Juan An-
tonio P?erez-Ortiz, Felipe S?anchez-Mart??nez, Gema
Ram??rez-S?anchez, and Francis M Tyers. 2011.
Apertium: A Free/Open-source Platform for Rule-
based Machine Translation. Machine Translation,
25(2):127?144.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Robert S Garfinkel and George L Nemhauser. 1972.
Integer Programming, volume 4. Wiley New York.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings of ACL, pages 690?696.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of ACL, Interactive Poster and Demonstra-
tion Sessions, pages 177?180.
176
William D. Lewis and Sauleh Eetemadi. 2013. Dra-
matically Reducing Training Data Size Through Vo-
cabulary Saturation. In Proceedings of WMT, pages
281?291.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of ACL, pages 220?224.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL, volume 1, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, pages 311?318.
V??ctor M. S?anchez-Cartagena, Felipe S?anchez-
Mart??nez, and Juan Antonio P?erez-Ortiz. 2011. In-
tegrating Shallow-transfer Rules into Phrase-based
Statistical Machine Translation. In Proceedings of
MT Summit XIII, pages 562?569.
V??ctor M. S?anchez-Cartagena, Juan Antonio P?erez-
Ortiz, and Felipe S?anchez-Mart??nez. 2014. The
UA-Prompsit Hybrid Machine Translation System
for the 2014 Workshop on Statistical Machine
Translation. In Proceedings of WMT.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statisti-
cal Machine Translation. In Proceedings of EACL,
pages 539?549.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of AMTA, pages 223?231.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and Out-
look. In Proceedings of ASRU.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online Large-margin Train-
ing for Statistical Machine Translation. In Proceed-
ings of EMNLP.
177
