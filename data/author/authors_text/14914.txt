Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 940?949, Dublin, Ireland, August 23-29 2014.
The Impact of Deep Hierarchical Discourse Structures
in the Evaluation of Text Coherence
Vanessa Wei Feng
1
, Ziheng Lin
2
, and Graeme Hirst
1
1
Department of Computer Science
University of Toronto
{weifeng, gh}@cs.toronto.edu
2
Singapore Press Holdings
linziheng@gmail.com
Abstract
Previous work by Lin et al. (2011) demonstrated the effectiveness of using discourse relations
for evaluating text coherence. However, their work was based on discourse relations annotated
in accordance with the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), which encodes
only very shallow discourse structures; therefore, they cannot capture long-distance discourse
dependencies. In this paper, we study the impact of deep discourse structures for the task of co-
herence evaluation, using two approaches: (1) We compare a model with features derived from
discourse relations in the style of Rhetorical Structure Theory (RST) (Mann and Thompson,
1988), which annotate the full hierarchical discourse structure, against our re-implementation of
Lin et al.?s model; (2) We compare a model encoded using only shallow RST-style discourse
relations, against the one encoded using the complete set of RST-style discourse relations. With
an evaluation on two tasks, we show that deep discourse structures are truly useful for better dif-
ferentiation of text coherence, and in general, RST-style encoding is more powerful than PDTB-
style encoding in these settings.
1 Introduction
In a well-written text, utterances are not simply presented in an arbitrary order; rather, they are presented
in a logical and coherent form, so that the readers can easily interpret the meaning that the writer wishes
to present. Therefore, coherence is one of the most essential aspects of text quality. Given its importance,
the automatic evaluation of text coherence is one of the crucial components of many NLP applications.
A particularly popular model for the evaluation of text coherence is the entity-based local coherence
model of Barzilay and Lapata (B&L) (2005; 2008), which extracts mentions of entities in the text, and
models local coherence by the transitions, from one sentence to the next, in the grammatical role of each
mention. Since the initial publication of this model, a number of extensions have been proposed, the
majority of which are focused on enriching the original feature set. However, these enriched feature
sets are usually application-specific, i.e., it requires a certain expertise and intuition to conceive good
features.
In contrast, we seek insights of better feature encoding from a more general problem: discourse parsing
(to be introduced in Section 2). Discourse parsing aims to identify the discourse relations held among
various discourse units in the text. Therefore, one can expect that discourse parsing provides useful
information to the evaluation of text coherence, because, essentially, the existence and the distribution of
discourse relations are the basis of the coherence in a text.
In fact, there is already evidence showing that discourse relations can help better capture text coher-
ence. Lin et al. (2011) use a PDTB-style discourse parser (to be introduced in Section 2.1) to identify
discourse relations in the text, and they represent a text by entities and their associated discourse roles
in each sentence. In their experiments, using discourse roles alone, their model performs very simi-
lar or even better than B&L?s model. Combining their discourse role features with B&L?s entity-based
transition features further improves the performance.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
940
S1
: The dollar finished lower yesterday, after tracking another rollercoaster session on Wall Street.
S
2
: [Concern about the volatile U.S. stock market had faded in recent sessions]C
2.1
, [and traders
appeared content to let the dollar languish in a narrow range until tomorrow, when the preliminary
report on third-quarter U.S. gross national product is released.]C
2.2
S
3
: But seesaw gyrations in the Dow Jones Industrial Average yesterday put Wall Street back in the
spotlight and inspired market participants to bid the U.S. unit lower.
Three discourse relations are presented in the text above:
1. Implicit EntRel between S
1
as Arg1, and S
2
as Arg2.
2. Explicit Conjunction within S
2
: C
2.1
as Arg1, C
2.2
as Arg2, with and as the connective.
3. Explicit Contrast between S
2
as Arg1 and S
3
as Arg2, with but as the connective.
Figure 1: An example text fragment composed of three sentences, and its PDTB-style discourse relations.
However, PDTB-style discourse relations encode only very shallow discourse structures, i.e., the re-
lations are mostly local, e.g., within a single sentence or between two adjacent sentences. Therefore,
in general, features derived from PDTB-style discourse relations cannot capture long discourse depen-
dency, and thus the resulting model is still limited to being a local model. Nonetheless, long-distance
discourse dependency could be quite useful for capturing text coherence from a global point of view.
Therefore, in this paper, we study the effect of deep hierarchical discourse structure in the evalua-
tion of text coherence, by adopting two approaches to perform a direct comparison between models that
incorporate deep hierarchical discourse structures and models with shallow structures. To evaluate our
models, we conduct experiments on two datasets, each of which resembles a real sub-task in the evalu-
ation of text coherence: sentence ordering and essay scoring. On both tasks, the model derived from
deep discourse structures is shown to be more powerful than the model derived from shallow discourse
structures. Moreover, for sentence ordering, combining our model with entity-based transition features
achieves the best performance. However, for essay scoring, the combination is detrimental.
2 Discourse parsing
Discourse parsing is the problem of identifying the discourse structure within a text, by recognizing the
specific type of its discourse relations, such as Contrast, Explanation, and Causal relations. Although
discourse parsing is still relatively less well-studied, a number of theories have been proposed to capture
different rhetorical characteristics or to serve different applications.
Currently, the two main directions in the study of discourse parsing are PDTB-style and RST-style
parsing. These two directions are based on distinct theoretical frameworks, and each can be potentially
useful for particular kinds of downstream applications. As will be discussed shortly, the major difference
between PDTB- and RST-style discourse parsing is the notion of deep hierarchical discourse structure,
which, according to our hypothesis, can be very useful for recognizing text coherence.
2.1 PDTB-style Discourse Parsing
The Penn Discourse Treebank (PDTB), developed by Prasad et al. (2008), is currently the largest
discourse-annotated corpus, consisting of 2159 Wall Street Journal articles. The annotation in PDTB
adopts the predicate-argument view of discourse relations, where a discourse connective (e.g., because)
is treated as a predicate that takes two text spans as its arguments. The argument that the discourse con-
nective structurally attaches to is called Arg2, and the other argument is called Arg1. In PDTB, relations
are further categorized into explicit and implicit relations: a relation is explicit if there is an explicit dis-
course connective presented in the text; otherwise, it is implicit. PDTB relations focus more on locality
and adjacency: explicit relations seldom connect text units beyond local context; for implicit relations,
941
S1
: [The dollar finished lower yesterday,]e
1
[after tracking another rollercoaster session on Wall
Street.]e
2
S
2
: [Concern about the volatile U.S. stock market had faded in recent sessions,]e
3
[and traders
appeared content to let the dollar languish in a narrow range until tomorrow,]e
4
[when the preliminary
report on third-quarter U.S. gross national product is released.]e
5
S
3
: [But seesaw gyrations in the Dow Jones Industrial Average yesterday put Wall Street back in the
spotlight]e
6
[and inspired market participants to bid the U.S. unit lower.]e
7
Condition
(e1-e7)
(e1) (e2)
(e1-e2) (e3-e7)
(e4-e5)
(e4) (e5)
Background
Temporal
List Cause
(e6) (e7)
(e6-e7)(e3-e5)
(e3)
Contrast
Figure 2: An example text fragment composed of seven EDUs, and its RST discourse tree representation.
only adjacent sentences within paragraphs are examined for the existence of implicit relations.
The PDTB-style discourse parsing is thus the type of framework in accordance with the PDTB, which
extracts the discourse relations in a text, by identifying the presence of discourse connectives, the asso-
ciated discourse arguments, and the specific types of the relations. An example text fragment is shown
in Figure 1, consisting of three sentences, S
1
, S
2
, and S
3
. A sentence may further contain clauses, e.g.,
C
2.1
and C
2.2
in S
2
. The three PDTB-style discourse relations in this text are explained below the text.
2.2 RST-style Discourse Parsing
RST-style discourse parsing follows the theoretical framework of Rhetorical Structure Theory (RST)
(Mann and Thompson, 1988). In the framework of RST, a coherent text can be represented as a discourse
tree whose leaves are non-overlapping text spans called elementary discourse units (EDUs); these are the
minimal text units of discourse trees. Adjacent nodes can be related through particular discourse relations
to form a discourse subtree, which can then be related to other adjacent nodes in the tree structure.
RST-style discourse relations can be categorized into two types: mononuclear and multi-nuclear. In
mononuclear relations, one of the text spans, the nucleus, is more salient than the other, the satellite,
while in multi-nuclear relations, all text spans are equally important for interpretation.
Consider Figure 2, in which the same example as in Figure 1 is chunked into seven EDUs (e
1
-e
7
),
segmented by square brackets. Its discourse tree representation is shown below in the figure, following
the notational convention of RST. The two EDUs e
1
and e
2
are related by a mononuclear relation Tem-
poral, where e
1
is the more salient span; e
4
and e
5
are related by Condition, with e
4
as the nucleus; and
e
6
and e
7
are related by Cause, with e
7
as the nucleus. Then, the spans (e
3
-e
5
) and (e
6
-e
7
) are related by
Contrast to form a higher-level discourse structure, and so on. Finally, a Background relation merges the
span (e
1
-e
2
) and (e
3
-e
7
) on the top level of the tree.
As can be seen, thanks to the tree-structured representation of RST, compared to PDTB-style repre-
sentation, we have a full hierarchy of discourse relations in the text: discourse relations exist not only in
a local context, but also on higher text levels, such as between S
1
and the concatenation of S
2
and S
3
.
3 Entity-based Local Coherence Model
The entity-based local coherence model was initially developed by Barzilay and Lapata (B&L) (2005;
2008). The fundamental assumption of this model is that a document makes repeated reference to ele-
ments of a set of entities that are central to its topic.
For a document d, an entity grid is constructed, in which the columns represent the entities referred
942
S1
: [The dollar]
S
finished lower [yesterday]
X
, after tracking [another rollercoaster session]
O
on
[Wall Street]
X
.
S
2
: [Concern]
S
about [the volatile U.S. stock market]
X
had faded in [recent sessions]
X
, and
[traders]
S
appeared content to let [the dollar]
S
languish in [a narrow range]
X
until [tomorrow]
X
,
when [the preliminary report]
S
on [third-quarter U.S. gross national product]
X
is released.
S
3
: But [seesaw gyrations]
S
in [the Dow Jones Industrial Average]
X
[yesterday]
X
put [Wall
Street]
O
back in [the spotlight]
X
and inspired [market participants]
O
to bid [the U.S. unit]
S
lower.
d
o
l
l
a
r
y
e
s
t
e
r
d
a
y
s
e
s
s
i
o
n
W
a
l
l
S
t
r
e
e
t
c
o
n
c
e
r
n
m
a
r
k
e
t
s
e
s
s
i
o
n
s
t
r
a
d
e
r
s
r
a
n
g
e
t
o
m
o
r
r
o
w
r
e
p
o
r
t
G
N
P
g
y
r
a
t
i
o
n
s
D
J
I
A
s
p
o
t
l
i
g
h
t
p
a
r
t
i
c
i
p
a
n
t
s
S
1
S X O X - - - - - - - - - - - -
S
2
S - - - S X S X X X S X - - - -
S
3
S X - O - - - - - - - - S X X O
Table 1: The entity grid for the example text with three sentences and eighteen entities. Grid cells
correspond to grammatical roles: subjects (S), objects (O), or neither (X).
to in d, and rows represent the sentences. Each cell corresponds to the grammatical role of an entity in
the corresponding sentence: subject (S), object (O), neither (X), or nothing (?), and an entity is defined
as a class of coreferent noun phrases. If the entity serves in multiple roles in a single sentence, then
we resolve its grammatical role following the priority order: S  O  X  ?. Consider the text in our
previous examples; its entity grid is shown in Table 1, and the entities are highlighted in boldface in the
text above
1
. A local transition is defined as a sequence {S,O,X,?}
n
, representing the occurrence and
grammatical roles of an entity in n adjacent sentences. Such transition sequences can be extracted from
the entity grid as continuous subsequences in each column. For example, the entity dollar in Table 1
has a bigram transition {S,S} from sentence 1 to 2. The entity grid is then encoded as a feature vector
?(d) = (p
1
(d), p
2
(d), . . . , p
m
(d)), where p
t
(d) is the normalized frequency of the transition t in the
entity grid, and m is the number of transitions with length no more than a predefined length k. p
t
(d) is
computed as the number of occurrences of t in the entity grid of document d, divided by the total number
of transitions of the same length. Moreover, entities are differentiated by their salience ? an entity is
deemed to be salient if it occurs at least l times in the text, and non-salient otherwise ? and transitions
are computed separately for salient and non-salient entities.
3.1 Extension: Lin et al.?s Discourse Role Matrix
As mentioned previously, most extensions to B&L?s entity-based local coherence model focus on enrich-
ing the feature set, including the work of Filippova and Strube (2007), Cheung and Penn (2010), Elsner
and Charniak (2011), and Lin et al. (2011). To the best of our knowledge, the only exception is Feng and
Hirst (2012a)?s extension from the perspective of improving the learning procedure.
Among various extensions to B&L?s entity-based local coherence model, the one most related to ours
is Lin et al. (2011)?s work on encoding a text as a set of entities with their associated discourse roles. Lin
et al. observed that coherent texts preferentially follow certain relation patterns. However, simply using
such patterns to measure the coherence of a text can result in feature sparseness. To solve this problem,
they expand the relation sequence into a discourse role matrix, as shown in Table 2. Columns correspond
to the entities in the text and rows represent the contiguous sentences. Each cell
?
E
i
,S
j
?
corresponds to
the set of discourse roles that the entity E
i
serves as in sentence S
j
. For example, the entity yesterday
from S
3
takes part in Arg2 of the last relation, so the cell ?yesterday,S
3
? contains the role Contrast.Arg2.
1
Text elements are considered to be a single entity with multiple mentions if they refer to the same object or concept in the
world, even if they have different textual realizations; e.g., dollar in S
1
and U.S. unit in S
3
refer to the same entity.
943
dollar yesterday session Wall Street concern market
S
1
EntRel.Arg1 EntRel.Arg1 EntRel.Arg1 EntRel.Arg1 nil nil
S
2
EntRel.Arg2
nil nil nil
EntRel.Arg2 EntRel.Arg2
Conj.Arg2 Conj.Arg1 Conj.Arg1
Contrast.Arg1 Contrast.Arg1 Contrast.Arg1
S
3
Contrast.Arg2 Contrast.Arg2 nil Contrast.Arg2 nil nil
Table 2: A fragment of Lin et al.?s PDTB-style discourse role matrix for the example text with the first
six entities across three sentences.
An entry may be empty (with a symbol nil, as in ?yesterday,S
2
?) or contain multiple discourse roles (as in
?dollar,S
2
?). Next, the frequencies of the discourse role transitions of lengths 2 and 3, e.g., EntRel.Arg1
? Conjunction.Arg2 and EntRel.Arg1? nil? Contrast.Arg2, are calculated with respect to the matrix.
For example, the frequency of EntRel.Arg1? Conjunction.Arg2 is 1/24 = 0.042 in Table 2.
4 Methodology
As discussed in Section 1, the main objective of our work is to study the impact of deep hierarchical dis-
course structures in the evaluation of text coherence. In order to conduct a direct comparison between a
model with features derived from deep hierarchical discourse relations and a model with features derived
from shallow discourse relations only, we adopt two separate approaches: (1) We implement a model
with features derived from RST-style discourse relations, and compare it against a model with features
derived from PDTB-style relations. (2) In the framework of RST-style discourse parsing, we deprive the
model of any information from higher-level discourse relations and compare its performance against the
model that uses the complete set of discourse relations. Moreover, as a baseline, we also re-implemented
B&L?s entity-based local coherence model, and we will study the effect of incorporating one of our dis-
course feature sets into this baseline model. Therefore, we have four ways to encode discourse relation
features, namely, entity-based, PDTB-style, full RST-style, and shallow RST-style.
4.1 Entity-based Feature Encoding
In entity-based feature encoding, our goal is to formulate a text into an entity grid, such as the one shown
in Table 1, from which we extract entity-based local transitions. In our re-implementation of B&L, we
use the same parameter settings as B&L?s original model, i.e., the optimal transition length k = 3 and the
salience threshold l = 2. However, when extracting entities in each sentence, e.g., dollar, yesterday, etc.,
we do not perform coreference resolution; rather, for better coverage, we follow the suggestion of Elsner
and Charniak (2011) and extract all nouns (including non-head nouns) as entities. We use the Stanford
dependency parser (de Marneffe et al., 2006) to extract nouns and their grammatical roles. This strategy
of entity extraction also applies to the other three feature encoding methods to be described below.
4.2 PDTB-style Feature Encoding
To encode PDTB-style discourse relations into the model, we parse the texts using an end-to-end PDTB-
style discourse parser
2
developed by Lin et al. (2014). The F
1
score of this parser is around 85% for rec-
ognizing explicit relations and around 40% for recognizing implicit relations. A text is thus represented
by a discourse role matrix in the same way as shown in Table 2. Most parameters in our PDTB-style fea-
ture encoding follow those of Lin et al. (2011): each entity is associated with the fully-fledged discourse
roles, i.e., with type and argument information included; the maximum length of discourse role transi-
tions is 3; and transitions are generated separately for salient and non-salient entities with a threshold set
at 2. However, compared to Lin et al.?s model, there are two differences in our re-implementation, and
evaluated on a held-out development set, these modifications are shown to be effective in improving the
performance.
2
http://wing.comp.nus.edu.sg/
?
linzihen/parser/
944
dollar yesterday session Wall Street concern market
S
1
Background.N Background.N Temporal.S Temporal.S nil nil
Temporal.N Temporal.N
List.N List.N List.N
S
2
Condition.N nil nil nil Contrast.S Contrast.S
Contrast.S
Contrast.N
S
3
Background.N Cause.S nil Cause.S nil nil
Cause.N
Table 3: A fragment of the full RST-style discourse role matrix for the example text with the first six
entities across three sentences.
First, we differentiate between intra- and multi-sentential discourse relations, which is motivated by a
finding in the field of RST-style discourse parsing ? distributions of various discourse relation types are
quite distinct between intra-sentential and multi-sentential instances (Feng and Hirst, 2012b; Joty et al.,
2012) ? and we assume that a similar phenomenon exists for PDTB-style discourse relations. Therefore,
we assign two sets of discourse roles to each entity: intra-sentential and multi-sentential roles, which are
the roles that the entity plays in the corresponding intra- and multi-sentential relations.
Second, instead of Level-1 PDTB discourse relations (6 in total), we use Level-2 relations (18 in total)
in feature encoding, so that richer information can be captured in the model, resulting in 18? 2 = 36
different discourse roles with argument attached. We then generate four separate set of features for the
combination of intra-/multi-sentential discourse relation roles, and salient/non-salient entities, among
which transitions consisting of only nil symbols are excluded. Therefore, the total number of features in
PDTB-style encoding is 4? (36
2
+36
3
?2)? 192K.
4.3 Full RST-style Feature Encoding
For RST-style feature encoding, we parse the texts using an end-to-end RST-style discourse parser de-
veloped by Feng and Hirst (2014), which produces a discourse tree representation for each text, such as
the one shown in Figure 2. For relation labeling, the overall accuracy of this discourse parser is 58%,
evaluated on the RST-DT.
We encode the RST-style discourse relations in a similar fashion to PDTB-style encoding. However,
since the definition of discourse roles depends on the particular discourse framework, here, we adapt Lin
et al.?s PDTB-style encoding by replacing the PDTB-style discourse relations with RST-style discourse
relations, and the argument information (Arg1 or Arg2) by the nuclearity information (nucleus or the
satellite) in an RST-style discourse relation. More importantly, in order to reflect the hierarchical struc-
ture in an RST-style discourse parse tree, when extracting the set of discourse relations that an entity
participates in, we find all those discourse relations that the entity appears in the main EDUs of each
relation
3
and represent the role of the entity in each of these discourse relations. In this way, we can
encode long-distance discourse relations for the most relevant entities. For example, considering the
RST-style discourse tree representation in Figure 2, we encode the Background relation for the entities
dollar and yesterday in S
1
, as well as the entity dollar in S
3
, but not for the remaining entities in the text,
even though the Background relation covers the whole text. The corresponding full RST-style discourse
role matrix for the example text is shown in Table 3.
As in PDTB-style feature encoding, we differentiate between intra- and multi-sentential discourse
relations; we use 17 coarse-grained classes of RST-style relations in feature encoding; the optimal transi-
3
The main EDUs of a discourse relation are the EDUs obtained by traversing the discourse subtree in which the relation of
interest constitutes the root node, following the nucleus branches down to the leaves. For instance, for the RST discourse tree
in Figure 2, the main EDUs of the Background relation on the top level are {e
1
,e
7
}, and the main EDUs of the List relation
among (e
3
-e
5
) are {e
3
,e
4
}.
945
tion length k is 3; and the salience threshold l is 2. The total number of features in RST-style encoding is
therefore 4?(34
2
+34
3
?2)? 162K, which is roughly the same as that in PDTB-style feature encoding.
4.4 Shallow RST-style Feature Encoding
Shallow RST-style encoding is almost identical to full RST-style encoding, as introduced in Section
4.3, except that, when we derive discourse roles, we consider shallow discourse relations only. To be
consistent with the majority of PDTB-style discourse relations, we define shallow discourse relations as
those relations which hold between text spans of the same sentence, or between two adjacent sentences.
For example, in Figure 2, the Background relation between (e
1
-e
2
) and (e
3
-e
7
) is not a shallow discourse
relation (it holds between a single sentence and the concatenation of two sentences), and thus will be
excluded from shallow RST-style feature encoding.
5 Experiments
To evaluate our proposed model with deep discourse structures encoded, we conduct two series of exper-
iments on two different datasets, each of which simulates a sub-task in the evaluation of text coherence,
i.e., sentence ordering and essay scoring. Since text coherence is a matter of degree rather than a bi-
nary classification, in both evaluation tasks we formulate the problem as a pairwise preference ranking
problem. Specifically, given a set of texts with different degrees of coherence, we train a ranker which
learns to prefer a more coherent text over a less coherent counterpart. Accuracy is therefore measured
as the fraction of correct pairwise rankings as recognized by the ranker. In our experiments, we use the
SVM
light
package
4
(Joachims, 1999) with the ranking configuration, and all parameters are set to their
default values.
5.1 Sentence Ordering
The task of sentence ordering, which has been extensively studied in previous work, attempts to simulate
the situation where, given a predefined set of information-bearing items, we need to determine the best
order in which the items should be presented. As argued by Barzilay and Lapata (2005), sentence order-
ing is an essential step in many content-generation components, such as multi-document summarization.
In this task, we use a dataset consisting of a subset of the Wall Street Journal (WSJ) corpus, in which
the minimum length of a text is 20 sentences, and the average length is 41 sentences. For each text, we
create 20 random permutations by shuffling the original order of the sentences. In total, we have 735
source documents and 735?20 = 14,700 permutations. Because the RST-style discourse parser we use
is trained on a fraction of the WSJ corpus, we remove the training texts from our dataset, to guarantee
that the discourse parser will not perform exceptionally well on some particular texts. However, since
the PDTB-style discourse parser we use is trained on almost the entire WSJ corpus, we cannot do the
same for the PDTB-style parser.
In this experiment, our learning instances are pairwise ranking preferences between a source text and
one of its permutations, where the source text is always considered more coherent than its permutations.
Therefore, we have 735?20 = 14,700 total pairwise rankings, and we conduct 5-fold cross-validation on
five disjoint subsets. In each fold, one-fifth of the rankings are used for testing, and the rest for training.
5.2 Essay Scoring
The second task is essay scoring, and we use a subset of International Corpus of Learner English (ICLE)
(Granger et al., 2009). The dataset consists of 1,003 essays about 34 distinct topics, written by university
undergraduates speaking 14 native languages who are learners of English as a Foreign Language. Each
essay has been annotated with an organization score from 1 to 4 at half-point increments by Persing et
al. (2010). We use these organization scores to approximate the degrees of coherence in the essays. The
average length of the essays is 32 sentences, and the average organization score is 3.05, with a standard
deviation of 0.59.
4
http://svmlight.joachis.org/
946
Model sentence ordering essay scoring
No discourse structure Entity 95.1 66.4
Shallow discourse structures
PDTB 97.2 82.2
PDTB&Entity 97.3 83.3
Shallow RST 98.5 87.2
Shallow RST&Entity 98.8 87.2
Deep discourse structures
Full RST 99.1 88.3
Full RST&Entity 99.3 87.7
Table 4: Accuracy (%) of various models on the two evaluation tasks: sentence ordering and essay
scoring. For sentence ordering, accuracy difference is significant with p < .01 for all pairs of models
except between PDTB and PDTB&Entity. For essay scoring, accuracy difference is significant with
p < .01 for all pairs of models except between shallow RST and shallow RST&Entity. Significance is
determined with the Wilcoxon signed-rank test.
In this experiment, our learning instances are pairwise ranking preferences between a pair of essays
on the same topic written by students speaking the same native language, excluding pairs with the same
organization score. In total, we have 22,362 pairwise rankings. Similarly, we conduct 5-fold cross-
validations on these rankings.
In fact, the two datasets used in the two evaluation tasks reflect different characteristics by themselves.
The WSJ dataset, although somewhat artificial due to the permuting procedure, is representative of texts
with well-formed syntax. By contrast, the ICLE dataset, although not artificial, contains occasional
syntactic errors, because the texts are written by non-native English speakers. Therefore, using these two
distinct datasets allows us to evaluate our models in tasks where different challenges may be expected.
6 Results
In this section, we demonstrate the performance of our models with discourse roles encoded in one of
the three ways: PDTB-style, full RST-style or shallow RST-style, and compare against their combination
with our re-implemented B&L?s entity-based local transition features. The evaluation is conducted on
the two tasks, sentence ordering and essay scoring, and the accuracy is reported as the fraction of correct
pairwise rankings averaged over 5-fold cross-validation.
The performance of various models is shown in Table 4. The first section of the table shows the
results of our re-implementation of B&L?s entity-based local coherence model, representing the effect
with no discourse structure encoded. The second section shows the results of four models with shallow
discourse structures encoded, including the two basic models, PDTB-style and shallow RST-style feature
encoding, and their combination with the entity-based feature encoding. The last section shows the
results of our models with deep discourse structures encoded, including the RST-style feature encoding
and its combination with the entity-base feature encoding. With respect to the performance, we observe
a number of consistent patterns across both evaluation tasks.
First, with no discourse structure encoded, the entity-based model (the first row) performs the worst
among all models, suggesting that discourse structures are truly important and can capture coherence in
a more sophisticated way than pure grammatical roles. Moreover, the performance gap is particularly
large for essay scoring, which is probably due to the fact that, as argued by Persing et al. (2010), the
organization score, which we use to approximate the degrees of coherence, is not equivalent to text
coherence. Organization relates more to the logical development in the texts, while coherence is about
lexical and semantic continuity; but discourse relations can capture the logical relations at least to some
extent.
Secondly, with deep discourse structures encoded, the RST-style model in the third section signif-
icantly outperforms (p < .01) the models with shallow discourse structures, i.e., the PDTB-style and
947
shallow RST-style models in the middle section, confirming our intuition that deep discourse structures
are more powerful than shallow structures. This is also the case when entity-based features are included.
Finally, considering the models in the middle section of the table, we can gain more insight into the
difference between PDTB-style and RST-style encoding. As can be seen, even without information from
the more powerful deep hierarchical discourse structures, shallow RST-style encoding still significantly
outperforms PDTB-style encoding on both tasks (p < .01). This is primarily due to the fact that the
discourse relations discovered by RST-style parsing have wider coverage of the text
5
, and thus induce
richer information about the text. Therefore, because of its ability to annotate deep discourse structures
and its better coverage of discourse relations, RST-style discourse parsing is generally more powerful
than PDTB-style parsing, as far as coherence evaluation is concerned.
However, with respect to combining full RST-style features with entity features, we have contradictory
results on the two tasks: for sentence ordering, the combination is significantly better than each single
model, while for essay scoring, the combination is worse than using RST-style features alone. This is
probably related to the previously discussed issue of using entity-based features for essay scoring, due to
the subtle difference between coherence and organization.
7 Conclusion and Future Work
In this paper, we have studied the impact of deep discourse structures in the evaluation of text coher-
ence by two approaches. In the first approach, we implemented a model with discourse role features
derived from RST-style discourse parsing, which represents deep discourse structures, and compared it
against our re-implemented Lin et al. (2011)?s model derived from PDTB-style parsing, with no deep
discourse structures annotated. In the second approach, we compared our complete RST-style model
against a model with shallow RST-style encoding. Evaluated on the two tasks, sentence ordering and
essay scoring, deep discourse structures are shown to be effective for better differentiation of text coher-
ence. Moreover, we showed that, even without deep discourse structures, shallow RST-style encoding is
more powerful than PDTB-style encoding, because it has better coverage of discourse relations in texts.
Finally, combining discourse relations with entity-based features is shown to have an inconsistent effect
on the two evaluation tasks, which is probably due to the different nature of the two tasks.
In our future work, we wish to explore the effect of automatic discourse parsers in our methodology.
As discussed previously, the PDTB- and RST-style discourse parsers used in our experiments are far from
perfect. Therefore, it is possible that using automatically extracted discourse relations creates some bias
to the training procedure; it is also possible that what our model actually learns is the distribution over
those discourse relations which automatic discourse parsers are mostly confident with, and thus errors (if
any) made on other relations do not matter. One potential way to verify these two possibilities is to study
the effect of each particular type of discourse relation to the resulting model, and we leave it for future
exploration.
Acknowledgements
We thank the reviewers for their valuable advice and comments. This work was financially supported by
the Natural Sciences and Engineering Research Council of Canada and by the University of Toronto.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings
of the 42rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 141?148.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-based local coherence modelling using topological fields.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages
186?195.
5
The entire text is covered by the annotation produced by RST-style discourse parsing, while this is generally not true for
PDTB-style discourse parsing.
948
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources
and Evaluation (LREC 2006).
Micha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 125?129.
Vanessa Wei Feng and Graeme Hirst. 2012a. Extending the entity-based coherence model with multiple ranks. In
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics
(EACL 2012), pages 315?324, Avignon, France.
Vanessa Wei Feng and Graeme Hirst. 2012b. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 60?68,
Jeju, Korea.
Vanessa Wei Feng and Graeme Hirst. 2014. A linear-time bottom-up discourse parser with constraints and post-
editing. In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics (ACL
2014), Baltimore, USA, June.
Katja Filippova and Michael Strube. 2007. Extending the entity-grid coherence model to semantically related
entities. In Proceedings of the Eleventh European Workshop on Natural Language Generation (ENLG 2007),
pages 139?142.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. International Corpus of Learner
English (Version 2). Presses universitaires de Louvain.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch?olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods ? Support Vector Learning, chapter 11, pages 169?184. MIT Press, Cam-
bridge, MA.
Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012, pages 904?915.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies (ACL 2011), Portland, Oregon, USA, June.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 2:151?184.
William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Isaac Persing, Alan Davis, and Vincent Ng. 2010. Modeling organization in student essays. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229?239, Cambridge, MA,
October. Association for Computational Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation (LREC 2008).
949
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12?23,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Discourse Analysis for Article-Wide Temporal Classification
Jun-Ping Ng1, Min-Yen Kan1,2, Ziheng Lin3, Wei Feng4, Bin Chen5, Jian Su5, Chew-Lim Tan1
1School of Computing, National University of Singapore, Singapore
2Interactive and Digital Media Institute, National University of Singapore, Singapore
3Research & Innovation, SAP Asia Pte Ltd, Singapore
4Department of Computer Science, University of Toronto, Canada
5Institute for Infocomm Research, Singapore
junping@comp.nus.edu.sg
Abstract
In this paper we classify the temporal relations
between pairs of events on an article-wide ba-
sis. This is in contrast to much of the exist-
ing literature which focuses on just event pairs
which are found within the same or adjacent
sentences. To achieve this, we leverage on dis-
course analysis as we believe that it provides
more useful semantic information than typical
lexico-syntactic features. We propose the use
of several discourse analysis frameworks, in-
cluding 1) Rhetorical Structure Theory (RST),
2) PDTB-styled discourse relations, and 3)
topical text segmentation. We explain how
features derived from these frameworks can be
effectively used with support vector machines
(SVM) paired with convolution kernels. Ex-
periments show that our proposal is effective
in improving on the state-of-the-art signifi-
cantly by as much as 16% in terms of F1, even
if we only adopt less-than-perfect automatic
discourse analyzers and parsers. Making use
of more accurate discourse analysis can fur-
ther boost gains to 35%.
1 Introduction
A good amount of research had been invested in un-
derstanding temporal relationships within text. Par-
ticular areas of interest include determining the re-
lationship between an event mention and a time ex-
pression (timex), as well as determining the relation-
ship between two event mentions. The latter, which
we refer to as event-event (E-E) temporal classifica-
tion is the focus of this work.
For a given event pair which consists of two
events e1 and e2 found anywhere within an article,
we want to be able to determine if e1 happens be-
fore e2 (BEFORE), after e2 (AFTER), or within the
same time span as e2 (OVERLAP).
Consider this sentence1:
At least 19 people were killed and 114 people were
wounded in Tuesday?s southern Philippines airport blast,
officials said, but reports said the death toll could climb
to 30.
(1)
Three event mentions found within the sentence are
bolded. We say that there is an OVERLAP rela-
tionship between the ?killed ? wounded? event pair
as these two events happened together after the air-
port blast. Similarly there is a BEFORE relationship
between both the ?killed ? said?, and ?wounded ?
said? event pairs, as the death and injuries happened
before reports from the officials.
Being able to infer these temporal relationships
allows us to build up a better understanding of the
text in question, and can aid several natural lan-
guage understanding tasks such as information ex-
traction and text summarization. For example, we
can build up a temporal characterization of an article
by constructing a temporal graph denoting the rela-
tionships between all events within an article (Ver-
hagen et al, 2009). This can then be used to help
construct an event timeline which layouts sequen-
tially event mentions in the order they take place (Do
et al, 2012). The temporal graph can also be used
in text summarization, where temporal order can be
used to improve sentence ordering and thereby the
eventual generated summary (Barzilay et al, 2002).
Given the importance and value of temporal re-
lations, the community has organized shared tasks
1From article AFP ENG 20030304.0250 of the ACE 2005
corpus (ACE, 2005).
12
to spur research efforts in this area, including the
TempEval-1, -2 and -3 evaluation workshops (Ver-
hagen et al, 2009; Verhagen et al, 2010; Uzzaman
et al, 2012). Most related work in this area have
focused primarily on the task defintitions of these
evaluation workshops. In the task definitions, E-
E temporal classification involves determining the
relationship between events found within the same
sentence, or in adjacent sentences. For brevity we
will refer to this loosely as intra-sentence E-E tem-
poral classification in the rest of this paper.
This definition however is limiting and insuffi-
cient. It was adopted as a trade-off between com-
pleteness, and the need to simplify the evaluation
process (Verhagen et al, 2009). In particular, one
deficiency is that it does not allow us to construct the
complete temporal graph we seek. As illustrated in
Figure 1, being able to perform only intra-sentence
E-E temporal classification may result in a forest of
disconnected temporal graphs. A sentence s3 sepa-
rates events C and D, as such an intra-sentence E-E
classification system will not be able to determine
the temporal relationship between them. While we
can determine the relationship between A and C in
the figure with the use of temporal transitivity rules
(Setzer et al, 2003; Verhagen, 2005), we cannot re-
liably determine the relationship between say A and
D.
A
B C
D E
s1
s2
s3
s4
Figure 1: A disconnected temporal graph of events within
an article. Horizontal lines depict sentences s1 to s4, and
the circles identify events of interest.
In this work, we seek to overcome this limitation,
and study what can enable effective article-wide E-E
temporal classification. That is, we want to be able
to determine the temporal relationship between two
events located anywhere within an article.
The main contribution of our work is going
beyond the surface lexical and syntactic features
commonly adopted by existing state-of-the-art ap-
proaches. We suggest making use of semantically
motivated features derived from discourse analysis
instead, and show that these discourse features are
superior.
While we are just focusing on E-E temporal
classification, our work can complement other ap-
proaches such as the joint inference approach pro-
posed by Do et al (2012) and Yoshikawa et al
(2009) which builds on top of event-timex (E-T) and
E-E temporal classification systems. We believe that
improvements to the underlying E-T and E-E classi-
fication systems will help with global inference.
2 Related Work
Many researchers have worked on the E-E temporal
classification problem, especially as part of the Tem-
pEval series of evaluation workshops. Bethard and
Martin (2007) presented one of the earliest super-
vised machine learning systems, making use of sup-
port vector machines (SVM) with a variety of lexical
and syntactic features. Kolya et al (2010) described
a conditional random field (CRF) based learner mak-
ing use of similar features. Other researchers includ-
ing Uzzaman and Allen (2010) and Ha et al (2010)
made use of Markov Logic Networks (MLN). By
leveraging on the transitivity properties of temporal
relationships (Setzer et al, 2003), they found that
MLNs are useful in inferring new temporal relation-
ships from known ones.
Recognizing that the temporal relationships be-
tween event pairs and time expressions are related,
Yoshikawa et al (2009) proposed the use of a joint
inference model and showed that improvements in
performance are obtained. However this gain is at-
tributed to the joint inference model they had devel-
oped, making use of similar surface features.
To the best of our knowledge, the only piece
of work to have gone beyond sentence boundaries
and tackle the problem of article-wide E-E temporal
classification is by Do et al (2012). Making use of
integer linear programming (ILP), they built a joint
inference model which is capable of classifying tem-
poral relationships between any event pair within
a given document. They also showed that event
co-reference information can be useful in determin-
ing these temporal relationships. However they did
not make use of features directed specifically at de-
termining the temporal relationships of event pairs
13
across different sentences. Other than event co-
reference information, they adopted the same mix
of lexico-syntactic features.
Underlying these disparate data-driven methods
for similar temporal processing tasks, the reviewed
works all adopted a similar set of surface fea-
tures including vocabulary features, part-of-speech
tags, constituent grammar parses, governing gram-
mar nodes and verb tenses, among others. We ar-
gue that these features are not sufficiently discrimi-
native of temporal relationships because they do not
explain how sentences are combined together, and
thus are unable to properly differentiate between the
different temporal classifications. Supporting our
argument is the work of Smith (2010), where she
argued that syntax cannot fully account for the un-
derlying semantics beneath surface text. D?Souza
and Ng (2013) found out as much, and showed that
adopting richer linguistic features such as lexical re-
lations from curated dictionaries (e.g. Webster and
WordNet) as well as discourse relations help tempo-
ral classification. They had shown that the Penn Dis-
course TreeBank (PDTB) style (Prasad et al, 2008)
discourse relations are useful. We expand on their
study to assess the utility of adopting additional dis-
course frameworks as alternative and complemen-
tary views.
3 Making Use of Discourse
To highlight the deficiencies of surface features, we
quote here an example from Lascarides and Asher
(1993):
[A] Max opened the door. The room was pitch dark.
[B] Max switched off the light. The room was pitch dark.
(2)
The two lines of text A and B in Example 2 have
similar syntactic structure. Given only syntactic fea-
tures, we may be drawn to conclude that they share
similar temporal relationships. However in the first
line of text, the events temporally OVERLAP, while
in the second line they do not. Clearly, syntax alone
is not going to be useful to help us arrive at the cor-
rect temporal relations.
If existing surface features are insufficient, what is
sufficient? Given a E-E pair which crosses sentence
boundaries, how can we determine the temporal re-
lationship between them? We take our cue from the
work of Lascarides and Asher (1993). They sug-
gested instead that discourse relations hold the key
to interpreting such temporal relationships.
Building on their observations, we believe that
discourse analysis is integral to any solution for the
problem of article-wide E-E temporal classification.
We thus seek to exploit a series of different discourse
analysis studies, including 1) the Rhetorical Struc-
ture Theory (RST) discourse framework, 2) Penn
Discourse Treebank (PDTB)-styled discourse rela-
tions based on the lexicalized Tree Adjoining Gram-
mar for Discourse (D-LTAG), and 3) topical text seg-
mentation, and validate their effectiveness for tem-
poral classification.
RST Discourse Framework. RST (Mann and
Thompson, 1988) is a well-studied discourse anal-
ysis framework. In RST, a piece of text is split into a
sequence of non-overlapping text fragments known
as elementary discourse units (EDUs). Neighboring
EDUs are related to each other by a typed relation.
Most RST relations are hypotactic, where one of the
two EDUs participating in the relationship is demar-
cated as a nucleus, and the other a satellite. The nu-
cleus holds more importance, from the point of view
of the writer, while the satellite?s purpose is to pro-
vide more information to help with the understand-
ing of the nucleus. Some RST relations are however
paratactic, where the two participating EDUs are
both marked as nuclei. A discourse tree can be com-
posed by viewing each EDU as a leaf node. Nodes
in the discourse tree are linked to one another via the
discourse relations that hold between the EDUs.
RST discourse relations capture the semantic re-
lation between two EDUs, and these often offer a
clue to the temporal relationship between events in
the two EDUs too. As an example, let us refer once
again to Example 2. Recall that in the second line of
text ?switched off? happens BEFORE ?dark?. The
RST discourse structure for the second line of text
is shown on the left of Figure 2. We see that the
two sentences are related via a ?Result? discourse
relation. This fits our intuition that when there is
causation, there should be a BEFORE/AFTER rela-
tionship. The RST discourse relation in this case is
very useful in helping us determine the relationship
between the two events.
PDTB-styled Discourse Relations. Another widely
adopted discourse relation annotation is the PDTB
framework (Prasad et al, 2008). Unlike the RST
14
Max switched off the light. The room was pitch dark.
RESULT
The room was pitch dark.
CONTINGENCY :: CAUSE
arg1 arg2
Max switched off the light.
Figure 2: RST and PDTB discourse structures for the second line of text in Example 2. The structure on the left is the
RST discourse structure, while the structure on the right is for PDTB.
framework, the discourse relations in PDTB build on
the work on D-LTAG by Webber (2004), a lexicon-
grounded approach to discourse analysis. Practi-
cally, this means that instead of starting from a pre-
identified set of discourse relations, PDTB-styled
annotations are more focused on detecting possible
connectives (can be either explicit or implicit) within
the text, before identifying the text fragments which
they connect and how they are related to one another.
Applied again to the second line of text we have in
Example 2, we get a structure as shown on the right
side of Figure 2. From the figure we can see that
the two sentences are related via a ?Cause? relation-
ship. Similar to what we have explained earlier for
the case of RST, the presence of a causal effect here
strongly hints to us that events in the two sentences
share a BEFORE/AFTER relationship.
At this point we want to note the differences be-
tween the use of the RST framework and PDTB-
styled discourse relations in the context of our work.
The theoretical underpinnings behind these two dis-
course analysis are very different, and we believe
that they can be complementary to each other. First,
the RST framework breaks up text within an article
linearly into non-overlapping EDUs. Relations can
only be defined between neighboring EDUs. How-
ever this constraint is not found in PDTB-styled re-
lations, where a text fragment can participate in one
discourse relation, and a subsequence of it partic-
ipate in another. PDTB relations are also not re-
stricted only to adjacent text fragments. In this as-
pect, the flexibility of the PDTB relations can com-
plement the seemingly more rigid RST framework.
Second, with PDTB-styled relations not every
sentence needs to be in a relation with another as
the PDTB framework does not aim to build a global
discourse tree that covers all sentence pairs. This is
a problem when we need to do an article-wide anal-
ysis. The RST framework does not suffer from this
limitation however as we can build up a discourse
tree connecting all the text within a given article.
Topical Text Segmentation. A third complemen-
tary type of inter-sentential analysis is topical text
segmentation. This form of segmentation separates
a piece of text into non-overlapping segments, each
of which can span several sentences. Each segment
represents passages or topics, and provides a coarse-
grained study of the linear structure of the text (Sko-
rochod?Ko, 1972; Hearst, 1994). The transition be-
tween segments can represent possible topic shifts
which can provide useful information about tempo-
ral relationships.
Referring to Example 32, we have delimited the
different lines of text into segments with parenthe-
ses along with a subscript. Segment (1) talks about
the casualty numbers seen at a medical centre, while
Segment (2) provides background information that
informs us a bomb explosion had taken place. The
segment boundary signals to us a possible temporal
shift and can help us to infer that the bombing event
took place BEFORE the deaths and injuries had oc-
curred.
(The Davao Medical Center, a regional government hos-
pital, recorded 19 deaths with 50 wounded. Medical
evacuation workers however said the injured list was
around 114, spread out at various hospitals.)1
(A powerful bomb tore through a waiting shed at the
Davao City international airport at about 5.15 pm (0915
GMT) while another explosion hit a bus terminal at the
city.)2
(3)
4 Methodology
Having motivated the use of discourse analysis for
our problem, we now proceed to explain how we can
make use of them for temporal classification. The
different facets of discourse analysis that we are ex-
ploring in this work are structural in nature. RST
2From article AFP ENG 20030304.0250 of the ACE 2005
corpus.
15
EDU2 EDU3
r2
r1
EDU1
A
B
Figure 3: A possible RST discourse tree. The two circles
denote two events A and B which we are interested in.
t1 t2
t3
t4
r1 r2
r3
B
A
Figure 4: A possible PDTB-styled discourse annotation
where the circles represent events we are interested in.
and PDTB discourse relations are commonly repre-
sented as graphs, and we can also view the output
of text segmentation as a graph with individual text
segments forming vertices, and the transitions be-
tween them forming edges.
Considering this, we propose the use of support
vector machines (SVM), adopting a convolution ker-
nel (Collins and Duffy, 2001) for its kernel function
(Vapnik, 1999; Moschitti, 2006). The use of convo-
lution kernels allows us to do away with the exten-
sive feature engineering typically required to gener-
ate flat vectorized representations of features. This
process is time consuming and demands specialized
knowledge to achieve representations that are dis-
criminative, yet are sufficiently generalized. Con-
volution kernels had also previously been shown to
work well for the related problem of E-T temporal
classification (Ng and Kan, 2012), where the fea-
tures adopted are similarly structural in nature.
We now describe our use of the discourse analysis
frameworks to generate appropriate representations
for input to the convolution kernel.
RST Discourse Framework. Recall that the RST
framework provides us with a discourse tree for an
entire input article. In recent years several automatic
RST discourse parsers have been made available. In
our work, we first make use of the parser by Feng
and Hirst (2012) to obtain a discourse tree represen-
tation of our input. To represent the meaningful por-
tion of the resultant tree, we encode path information
between the two sentences of interest.
We illustrate this procedure using the example
discourse tree illustrated in Figure 3. EDUs includ-
ing EDU1 to EDU3 form the vertices while dis-
course relations r1 and r2 between the EDUs form
the edges. For a E-E pair, {A,B}, we can obtain a
feature structure by first locating the EDUs within
which A and B are found. A is found inside EDU1
and B is found within EDU3. We trace the short-
est path between EDU1 and EDU3, and use this
path as the feature structure for the E-E pair, i.e.
{r1 ? r2}.
PDTB-styled Discourse Relations. We make use of
the automatic PDTB discourse parser from Lin et al
(2013) to obtain the discourse relations over an input
article. Similar to how we work with the RST dis-
course framework, for a given E-E pair, we retrieve
the relevant text fragments and use the shortest path
linking the two events as a feature structure for our
convolution kernel classifier.
An example of a possible PDTB-styled discourse
annotation is shown in Figure 4. The horizontal
lines represent different sentences in an article. The
parentheses delimit text fragments, t1 to t4, which
have been identified as arguments participating in
discourse relations, r1 to r3. For a given E-E pair
{A,B}, we use the trace of the shortest path be-
tween them i.e. {r1 ? r2} as a feature structure.
We take special care to regularize the input (as,
unlike EDUs in RST, arguments to different PDTB
relations may overlap, as in r2 and r3). We model
each PDTB discourse annotation as a graph and em-
ploy Dijkstra?s shortest path algorithm. The graph
resulting from the annotation in Figure 4 is given in
Figure 5. Each text fragment ti maps to a vertex
ni in the graph. PDTB relations between text frag-
ments form edges between corresponding vertices.
As r2 relates t2 to both t3 and t4, two edges link
up n2 to the corresponding vertices n3 and n4 re-
spectively. By doing this, Dijkstra?s algorithm will
always allow us to find the desired shortest path.
n1 n2 n3 n4
r1
r2 r3
r2
Figure 5: Graph derived from discourse annotation in
Figure 4.
16
Topical Text Segmentation. Taking as input a com-
plete text article, we make use of the state-of-the-art
text segmentation system from Kazantseva and Sz-
pakowicz (2011). The output of the system is a se-
ries of non-overlapping, linear text segments, which
we can number sequentially.
In Figure 6 the horizontal lines represent sen-
tences. Parentheses with subscripts mark out the
segment boundaries. We can see two segments s1
and s2 here. Given a target E-E pair {A,B} (repre-
sented as circles inside the figure), we identify the
segment number of the corresponding segment in
which each of A and B is found. We build a fea-
ture structure with the identified segment numbers,
i.e. {s1 ? s2} to capture the segmentation.
A
B
s1
s2
Figure 6: A possible segmentation of three sentences into
two segments.
5 Results
We conduct a series of experiments to validate the
utility of our proposed features.
Data Set. We make use of the same data set built
by Do et al (2012). The data set consists of 20
newswire articles which originate from the ACE
2005 corpus (ACE, 2005). Initially, the data set
consist of 324 event mentions, and a total of 375
annotated E-E pairs. We perform the same temporal
saturation step as described in Do et al (2012), and
obtained a total of 7,994 E-E pairs3.
A breakdown of the number of instances by each
temporal classes is shown in Table 1. Unlike earlier
data sets such as that for TempEval-2 where more
than half (about 55%) of test instances belong to the
3Though we have obtained the data set from the original au-
thors, there was a discrepancy in the number of E-E pairs. The
original paper reported a total of 376 annotated E-E pairs. Be-
sides this, we also repeated the saturation steps iteratively until
no new relationship pairs are generated. We believe this to be
an enhancement as it ensures that all inferred temporal relation-
ships are generated.
OVERLAP class, OVERLAP instances make up just
10% of the data set.
This difference is due mainly to the fact that our
data set consists not only of intra-sentence E-E pairs,
but also of article-wide E-E pairs. Figure 7 shows
the number of instances for each temporal class bro-
ken down by the number of sentences (i.e. sentence
gap) that separate the events within each E-E pair.
We see that as the sentence gap increases, the pro-
portion of OVERLAP instances decreases. The in-
tuitive explanation for this is that when event men-
tions are very far apart in an article, it becomes more
unlikely that they happen within the same time span.
Class AFTER BEFORE OVERLAP
# E-E pairs 3,588 (45%) 3,589 (45%) 815 (10%)
Table 1: Number of E-E pairs in data set attributable to
each temporal class. Percentages shown in parentheses.
Figure 7: Breakdown of number of E-E pairs for each
temporal class based on sentence gap.
Experiments. The work done in Do et al (2012) is
highly related to our experiments, and so we have
reported the relevant results for local E-E classifi-
cation in Row 1 of Table 2 as a reference. While
largely comparable, note that a direct comparison is
not possible because 1) the number of E-E instances
we have is slightly different from what was reported,
and 2) we do not have access to the exact partitions
they have created for 5-fold cross-validation.
As such, we have implemented a baseline adopt-
ing similar surface lexico-syntactic features used in
previous work (Mani et al, 2006; Bethard and Mar-
tin, 2007; Ng and Kan, 2012; Do et al, 2012), in-
cluding 1) part-of-speech tags, 2) tenses, 3) depen-
dency parses, 4) relative position of events in article,
17
System Precision Recall F1
(1) DO2012 43.86 52.65 47.46
(2) BASE 59.55 38.14 46.50
(3) BASE + RST + PDTB + TOPICSEG 71.89 41.99 53.01
(4) BASE + RST + PDTB + TOPICSEG + COREF 75.23 43.58 55.19
(5) BASE + O-RST + PDTB + O-TOPICSEG + O-COREF 78.35 54.24 64.10
Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive
row is statistically significant, but a comparison is not possible between rows (1) and (2).
5) the number of sentences between the target events
and 6) VerbOcean (Chklovski and Pantel, 2004) re-
lations between events. This baseline system, and
the subsequent systems we will describe, comprises
of three separate one-vs-all classifiers for each of the
temporal classes. The result obtained by our base-
line is shown in Row 2 (i.e. BASE) in Table 2. We
note that our baseline is competitive and performs
similarly to the results obtained by Do et al (2012).
However as we do not have the raw judgements from
Do?s system, we cannot test for statistical signifi-
cance.
We also implemented our proposed features and
show the results obtained in the remaining rows of
Table 2. In Row 3, RST denotes the RST discourse
feature, PDTB denotes the PDTB-styled discourse
features, and TOPICSEG denotes the text segmen-
tation feature. Compared to our own baseline, there
is a relative increase of 14% in F1, which is statis-
tically significant when verified with the one-tailed
Student?s paired t-test (p < 0.01).
In addition, Do et al (2012) have shown the value
of event co-reference. Therefore we have also in-
cluded this feature by making use of an automatic
event co-reference system by Chen et al (2011).
The result obtained after adding this feature (de-
noted by COREF) is shown in Row 4. The relative in-
crease in F1 of about 4% from Row 3 is statistically
significant (p < 0.01) and affirms that event co-
reference is a useful feature to have, together with
our proposed features. We note that our complete
system in Row 4 gives a 16% improvement in F1,
relative to the reference system DO2012 in Row 1.
To get a better idea of the performance we can ob-
tain if oracular versions of our features are available,
we also show the results obtained if hand-annotated
RST discourse structures, text segments, as well as
event co-reference information were used. Annota-
tions for the RST discourse structures and text seg-
ments were performed by the first author (RST an-
notations were made following the annotation guide-
lines given by Carlson and Marcu (2001)). Oracular
event co-reference information was included in the
dataset that we have used.
In Row 5 the prefix O denotes oracular versions
of the features we had proposed. From the results
we see that there is a marked increase of over 15%
in F1 relative to Row 4. Compared to Do?s state-of-
the-art system, there is also a relative gain of at least
35%. These oracular results further confirm the im-
portance of non-local discourse analysis for tempo-
ral processing.
6 Discussion
Ablation tests. We performed ablation tests to as-
sess the efficacy of the discourse features used in
our earlier experiments. Starting from the full sys-
tem, we dropped each discourse feature in turn to see
the effect this has on overall system performance.
Our test is performed over the same data set, again
with 5-fold cross-validation. The results in Table 3
show a statistically significant (based on the one-
tailed Student?s paired t-test) drop in F1 in each case,
which proves that each of our proposed features is
useful and required.
From the ablation tests, we also observe that the
RST discourse feature contributes the most to over-
all system performance while the PDTB discourse
feature contributes the least. However we should not
conclude prematurely that the former is more use-
ful than the latter; as the results are obtained using
parses from automatic systems, and are not reflec-
tive of the full utility of ground truth discourse an-
notations.
Useful Relations. The ablation test results showed
us that discourse relations (in particular RST dis-
18
Figure 8: Proportion of occurence in temporal classes for every RST and PDTB relation.
Ablated Feature Change in F1 Sig
?RST -9.03 **
?TOPICSEG -2.98 **
?COREF -2.18 **
?PDTB -1.42 *
Table 3: Ablation test results. ?**? and ?*? denote statis-
tically significance against the full system with p < 0.01
and p < 0.05, respectively.
course relations) are the most important in our sys-
tem. We have also motivated our work earlier with
the intuition that certain relations such as the RST
?Result? and the PDTB ?Cause? relations provide
very useful temporal cues. We now offer an intro-
spection into the use of these discourse relations.
Figure 8 illustrates the relative proportion of tem-
poral classes in which each RST and PDTB re-
lation appear. If the relations are randomly dis-
tributed, we should expect their distribution to fol-
low that of the temporal classes as shown in Table 1.
However we see that many of the relations do not
follow this distribution. For example, we observe
that several relations such as the RST ?Condition?
and PDTB ?Cause? relations are almost exclusively
found within AFTER and BEFORE event pairs only,
while the RST ?Manner-means? and PDTB ?Syn-
chrony? relations occur in a disproportionately large
number of OVERLAP event pairs. These relations
are likely useful in disambiguating between the dif-
ferent temporal classes.
To verify this, we examine the convolution tree
fragments that lie on the support vector of our SVM
classifier. The work of Pighin and Moschitti (2010)
in linearizing kernel functions allows us to take a
look at these tree fragments. Applying the lineariza-
tion process leads to a different classifier from the
one we have used. The identified tree fragments are
therefore just an approximation to those actually em-
ployed by our classifier. However, this analysis still
offers an introspection as to what relations are most
influential for classification.
BEFORE OVERLAP
B1 (Temporal ... O1 (Manner-means ...
B2 (Temporal (Elaboration ...
B3 (Condition (Explanation ...
B4 (Condition (Attribution ...
B5 (Elaboration (Bckgrnd ...
Table 4: Subset of top RST discourse fragments on sup-
port vectors identified by linearizing kernel function.
Table 4 shows a subset of the top RST discourse
fragments identified for the BEFORE and OVER-
LAP one-vs-all classifiers. The list is in line with
what we expect from Figure 8. The former consists
of fragments containing relations such as ?Tempo-
ral? and ?Condition?, while the latter has a sole frag-
ment containing ?Manner-Means?.
To illustrate what these fragments may mean, we
show several example sentences from our data set
in Example 4. Sentence A consists of the tree frag-
ment B1, i.e. ?(Temporal...?. Its corresponding dis-
course structure is illustrated in the top half of Fig-
ure 9. This fragment indicates to us (correctly) that
the event ?wielded? happened BEFORE Milosevic
was ?swept out? of power. Sentence B is made
up of tree fragment O1, i.e. ?(Manner-means...?,
19
and its discourse structure is shown in the bottom
half of Figure 9. As with the previous example, the
fragment suggests (correctly) that there should be a
OVERLAP relationship for the ?requested ? said?
event pair.
[A] Milosevic and his wife wielded enormous power in
Yugoslavia for more than a decade before he was swept
out of power after a popular revolt in October 2000.
[B] The court order was requested by Jack Welch?s at-
torney, Daniel K. Webb, who said Welch would likely be
asked about his business dealings, his health and entries
in his personal diary.
(4)
Milosevic ? wielded? 
a decade 
before.. swept out.. 
power
after a?  October
2000.
temporal
temporal
The court? requested
by Jack .. Webb,
elaboration
who said Welch would ?
diary.
attribution
manner-means
Figure 9: RST discourse structures for sentences A (top
half) and B (bottom half) in Example 4.
Segment Numbers. From the ablation test results,
text segmentation is the next most important feature
after the RST discourse feature. This is interesting
given that the defined feature structure for topical
text segmentation is not the most intuitive. By us-
ing actual segment numbers, the structure may not
generalize well for articles of different lengths for
example, as each article may have vastly different
number of segments. The transition across segments
may also not carry the same semantic significance
for different articles.
Our experiments have however shown that this
feature design is useful in improving performance.
This is likely because:
1. The default settings of the text segmentation
system we had used are such that precision is
favoured over recall (Kazantseva and Szpakow-
icz, 2011, p. 292). As such there is just an aver-
age of between two to three identified segments
per article. This makes the feature more gener-
alizable despite making use of actual segment
numbers.
2. The style of writing in newswire articles which
we are experimenting on generally follows
common journalistic guidelines. The semantics
behind the transitions across the coarse-grained
segments that were identified are thus likely to
be of a similar nature across many different ar-
ticles.
We leave for future work an investigation into
whether more fine-grained topic segments can lead
to further performance gains. In particular, it will be
interesting to study if work on argumentative zoning
(Teufel and Kan, 2011) can be applied to newswire
articles, and whether the subsequent learnt docu-
ment structures can be used to delineate topic seg-
ments more accurately.
Error Analysis. Besides examining the features we
had used, we also want to get a better idea of the er-
rors made by our classifier. Recall that we are using
separate one-vs-all classifiers for each of the tempo-
ral classes, so each of the three classifiers generates
a column in the aggregate confusion matrix shown
in Table 5. In cases where none of the SVM clas-
sifiers return a positive confidence value, we do not
assign a temporal class (captured as column N). The
high number of event pairs which are not assigned to
any temporal class explains the lower recall scores
obtained by our system, as observed in Table 2.
Predicted
O B A N
O 119 (14.7%) 114 (14.1%) 104 (12.8%) 474 (58.5%)
B 19 (0.5%) 2067 (57.9%) 554 (15.5%) 928 (26.0%)
A 16 (0.5%) 559 (15.7%) 2046 (57.3%) 947 (26.5%)
Table 5: Confusion matrix obtained for the full system,
classifying into (O)VERLAP, (B)EFORE, (A)FTER, and
(N)o result.
Additionally, an interesting observation is the low
percentage of OVERLAP instances that our classi-
fier managed to predict correctly. About 57% of
BEFORE and AFTER instances are classified cor-
20
rectly, however only about 15% of OVERLAP in-
stances are correct.
Figure 10 offers more evidence to suggest that
our classifier works better for the BEFORE and AF-
TER classes than the OVERLAP class. We see that
as sentence gap increases, we achieve a fairly con-
sistent performance for both BEFORE and AFTER
instances. OVERLAP instances are concentrated
where the sentence gap is less than 7, with the best
accuracy figure coming in below 30%.
Although not definitive, this may be because our
data set consists of much fewer OVERLAP in-
stances than the other two classes. This bias may
have led to insufficient training data for accurate
OVERLAP classification. It will be useful to inves-
tigate if using a more balanced data set for training
can help overcome this problem.
Figure 10: Accuracy of the classifer for each temporal
class, plotted against the sentence gap of each E-E pair.
7 Conclusion
We believe that discourse features play an important
role in the temporal ordering of events in text. We
have proposed the use of different discourse anal-
ysis frameworks and shown that they are effective
for classifying the temporal relationships of article-
wide E-E pairs. Our proposed discourse-based fea-
tures are robust and work well even though auto-
matic discourse analysis is noisy. Experiments fur-
ther show that improvements to these underlying
discourse analysis systems will benefit system per-
formance.
In future work, we will like to explore how to
better exploit the various discourse analysis frame-
works for temporal classification. For instance, RST
relations are either hypotactic or paratactic. Marcu
(1997) made use of this to generate automatic sum-
maries by considering EDUs which are nuclei to be
more salient. We believe it is interesting to examine
how such information can help. We are also inter-
ested to apply discourse features in the context of a
global inferencing system (Yoshikawa et al, 2009;
Do et al, 2012), as we think such analyses will also
benefit these systems as well.
Acknowledgments
We like to express our gratitude to Quang Xuan Do,
Wei Lu, and Dan Roth for generously making avail-
able the data set they have used for their work in
EMNLP 2012. We would also like to thank the
anonymous reviewers who reviewed this paper for
their valuable feedback.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
ACE. 2005. The ACE 2005 (ACE05) Evaluation Plan.
October.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring Strategies for Sentence Order-
ing in Multidocument News Summarization. Journal
of Artificial Intelligence Research (JAIR), 17:35?55.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 129?132, June.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, Informa-
tion Sciences Institute, University of Southern Califor-
nia, July.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A Unified Event Coreference Resolution by In-
tegrating Multiple Resolvers. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 102?110, Novem-
ber.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, July.
21
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP), pages
677?689, July.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
Temporal Relations with Rich Linguistic Knowledge.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), pages 918?927, June.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistics Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL), pages 60?68, July.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling Temporal
Relations with Markov Logic and Lexical Ontology.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval), pages 341?344, July.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation
of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 9?16, June.
Anna Kazantseva and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 284?293, July.
Anup Kumar Kolya, Asif Ekbal, and Sivaji Bandyopad-
hyay. 2010. JU CSE TEMP: A First Step Towards
Evaluating Events, Time Expressions and Temporal
Relations. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
345?350, July.
Alex Lascarides and Nicholas Asher. 1993. Temporal
Interpretation, Discourse Relations and Commonsense
Entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013. A
PDTB-styled End-to-End Discourse Parser. Natural
Language Engineering, FirstView:1?34, February.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 753?760, July.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From Discourse Structures to Text
Summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, volume 97,
pages 82?88, July.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), September.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved Tem-
poral Relation Classification using Dependency Parses
and Selective Crowdsourced Annotations. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 2109?2124, December.
Daniele Pighin and Alessandro Moschitti. 2010. On Re-
verse Feature Engineering of Syntactic Tree Kernels.
In Proceedings of the 14th Conference on Natural Lan-
guage Learning (CoNLL), August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC), May.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple.
2003. Using Semantic Inferences for Temporal An-
notation Comparison. In Proceedings of the 4th In-
ternational Workshop on Inference in Computational
Semantics (ICoS), September.
Eduard F. Skorochod?Ko. 1972. Adaptive Method of
Automatic Abstracting and Indexing. In Proceedings
of the IFIP Congress, pages 1179?1182.
Carlota S. Smith. 2010. Temporal Structures in Dis-
course. Text, Time, and Context, 87:285?302.
Simone Teufel and Min-Yen Kan. 2011. Robust Argu-
mentative Zoning for Sensemaking in Scholarly Doc-
uments. In Advanced Language Technologies for Dig-
ital Libraries, pages 154?170. Springer.
Naushad Uzzaman and James F. Allen. 2010. TRIPS and
TRIOS System for TempEval-2: Extracting Temporal
Information. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
276?283, July.
Naushad Uzzaman, Hector Llorens, James F. Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2012. TempEval-3: Evaluating Events, Time Expres-
sions, and Temporal Relations. Computing Research
Repository (CoRR), abs/1206.5333.
Vladimir N. Vapnik, 1999. The Nature of Statistical
Learning Theory, chapter 5. Springer.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval Challenge: Identifying
22
Temporal Relations in Text. Language Resources and
Evaluation, 43(2):161?179.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
57?62, July.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
Bonnie Webber. 2004. D-LTAG: Extending Lexicalized
TAG to Discourse. Cognitive Science, 28(5):751?779.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly Identifying
Temporal Relations with Markov Logic. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL) and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language Pro-
cessing (AFNLP), pages 405?413, August.
23
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315?324,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Extending the Entity-based Coherence Model with Multiple Ranks
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
We extend the original entity-based coher-
ence model (Barzilay and Lapata, 2008)
by learning from more fine-grained coher-
ence preferences in training data. We asso-
ciate multiple ranks with the set of permuta-
tions originating from the same source doc-
ument, as opposed to the original pairwise
rankings. We also study the effect of the
permutations used in training, and the effect
of the coreference component used in en-
tity extraction. With no additional manual
annotations required, our extended model
is able to outperform the original model on
two tasks: sentence ordering and summary
coherence rating.
1 Introduction
Coherence is important in a well-written docu-
ment; it helps make the text semantically mean-
ingful and interpretable. Automatic evaluation
of coherence is an essential component of vari-
ous natural language applications. Therefore, the
study of coherence models has recently become
an active research area. A particularly popular
coherence model is the entity-based local coher-
ence model of Barzilay and Lapata (B&L) (2005;
2008). This model represents local coherence
by transitions, from one sentence to the next, in
the grammatical role of references to entities. It
learns a pairwise ranking preference between al-
ternative renderings of a document based on the
probability distribution of those transitions. In
particular, B&L associated a lower rank with au-
tomatically created permutations of a source doc-
ument, and learned a model to discriminate an
original text from its permutations (see Section
3.1 below). However, coherence is matter of de-
gree rather than a binary distinction, so a model
based only on such pairwise rankings is insuffi-
ciently fine-grained and cannot capture the sub-
tle differences in coherence between the permuted
documents.
Since the first appearance of B&L?s model,
several extensions have been proposed (see Sec-
tion 2.3 below), primarily focusing on modify-
ing or enriching the original feature set by incor-
porating other document information. By con-
trast, we wish to refine the learning procedure
in a way such that the resulting model will be
able to evaluate coherence on a more fine-grained
level. Specifically, we propose a concise exten-
sion to the standard entity-based coherence model
by learning not only from the original docu-
ment and its corresponding permutations but also
from ranking preferences among the permutations
themselves.
We show that this can be done by assigning a
suitable objective score for each permutation indi-
cating its dissimilarity from the original one. We
call this a multiple-rank model since we train our
model on a multiple-rank basis, rather than tak-
ing the original pairwise ranking approach. This
extension can also be easily combined with other
extensions by incorporating their enriched feature
sets. We show that our multiple-rank model out-
performs B&L?s basic model on two tasks, sen-
tence ordering and summary coherence rating,
evaluated on the same datasets as in Barzilay and
Lapata (2008).
In sentence ordering, we experiment with
different approaches to assigning dissimilarity
scores and ranks (Section 5.1.1). We also exper-
iment with different entity extraction approaches
315
Manila Miles Island Quake Baco
1 ? ? X X ?
2 S ? O ? ?
3 X X X X X
Table 1: A fragment of an entity grid for five entities
across three sentences.
(Section 5.1.2) and different distributions of per-
mutations used in training (Section 5.1.3). We
show that these two aspects are crucial, depend-
ing on the characteristics of the dataset.
2 Entity-based Coherence Model
2.1 Document Representation
The original entity-based coherence model is
based on the assumption that a document makes
repeated reference to elements of a set of entities
that are central to its topic. For a document d, an
entity grid is constructed, in which the columns
represent the entities referred to in d, and rows
represent the sentences. Each cell corresponds
to the grammatical role of an entity in the corre-
sponding sentence: subject (S), object (O), nei-
ther (X), or nothing (?). An example fragment
of an entity grid is shown in Table 1; it shows
the representation of three sentences from a text
on a Philippine earthquake. B&L define a lo-
cal transition as a sequence {S ,O, X,?}n, repre-
senting the occurrence and grammatical roles of
an entity in n adjacent sentences. Such transi-
tion sequences can be extracted from the entity
grid as continuous subsequences in each column.
For example, the entity ?Manila? in Table 1 has
a bigram transition {S , X} from sentence 2 to 3.
The entity grid is then encoded as a feature vector
?(d) = (p1(d), p2(d), . . . , pm(d)), where pt(d) is
the probability of the transition t in the entity grid,
and m is the number of transitions with length no
more than a predefined optimal transition length
k. pt(d) is computed as the number of occurrences
of t in the entity grid of document d, divided by
the total number of transitions of the same length
in the entity grid.
For entity extraction, Barzilay and Lapata
(2008) had two conditions: Coreference+ and
Coreference?. In Coreference+, entity corefer-
ence relations in the document were resolved by
an automatic coreference resolution tool (Ng and
Cardie, 2002), whereas in Coreference?, nouns
are simply clustered by string matching.
2.2 Evaluation Tasks
Two evaluation tasks for Barzilay and Lapata
(2008)?s entity-based model are sentence order-
ing and summary coherence rating.
In sentence ordering, a set of random permu-
tations is created for each source document, and
the learning procedure is conducted on this syn-
thetic mixture of coherent and incoherent docu-
ments. Barzilay and Lapata (2008) experimented
on two datasets: news articles on the topic of
earthquakes (Earthquakes) and narratives on the
topic of aviation accidents (Accidents). A train-
ing data instance is constructed as a pair con-
sisting of a source document and one of its ran-
dom permutations, and the permuted document
is always considered to be less coherent than the
source document. The entity transition features
are then used to train a support vector machine
ranker (Joachims, 2002) to rank the source docu-
ments higher than the permutations. The model is
tested on a different set of source documents and
their permutations, and the performance is evalu-
ated as the fraction of correct pairwise rankings in
the test set.
In summary coherence rating, a similar exper-
imental framework is adopted. However, in this
task, rather than training and evaluating on a set
of synthetic data, system-generated summaries
and human-composed reference summaries from
the Document Understanding Conference (DUC
2003) were used. Human annotators were asked
to give a coherence score on a seven-point scale
for each item. The pairwise ranking preferences
between summaries generated from the same in-
put document cluster (excluding the pairs consist-
ing of two human-written summaries) are used by
a support vector machine ranker to learn a dis-
criminant function to rank each pair according to
their coherence scores.
2.3 Extended Models
Filippova and Strube (2007) applied Barzilay and
Lapata?s model on a German corpus of newspa-
per articles with manual syntactic, morphological,
and NP coreference annotations provided. They
further clustered entities by semantic relatedness
as computed by the WikiRelated! API (Strube and
Ponzetto, 2006). Though the improvement was
not significant, interestingly, a short subsection in
316
their paper described their approach to extending
pairwise rankings to longer rankings, by supply-
ing the learner with rankings of all renderings as
computed by Kendall?s ?, which is one of our
extensions considered in this paper. Although
Filippova and Strube simply discarded this idea
because it hurt accuracies when tested on their
data, we found it a promising direction for further
exploration. Cheung and Penn (2010) adapted
the standard entity-based coherence model to the
same German corpus, but replaced the original
linguistic dimension used by Barzilay and Lap-
ata (2008) ? grammatical role ? with topologi-
cal field information, and showed that for German
text, such a modification improves accuracy.
For English text, two extensions have been pro-
posed recently. Elsner and Charniak (2011) aug-
mented the original features used in the standard
entity-based coherence model with a large num-
ber of entity-specific features, and their extension
significantly outperformed the standard model
on two tasks: document discrimination (another
name for sentence ordering), and sentence inser-
tion. Lin et al(2011) adapted the entity grid rep-
resentation in the standard model into a discourse
role matrix, where additional discourse informa-
tion about the document was encoded. Their ex-
tended model significantly improved ranking ac-
curacies on the same two datasets used by Barzi-
lay and Lapata (2008) as well as on the Wall Street
Journal corpus.
However, while enriching or modifying the
original features used in the standard model is cer-
tainly a direction for refinement of the model, it
usually requires more training data or a more so-
phisticated feature representation. In this paper,
we instead modify the learning approach and pro-
pose a concise and highly adaptive extension that
can be easily combined with other extended fea-
tures or applied to different languages.
3 Experimental Design
Following Barzilay and Lapata (2008), we wish
to train a discriminative model to give the cor-
rect ranking preference between two documents
in terms of their degree of coherence. We experi-
ment on the same two tasks as in their work: sen-
tence ordering and summary coherence rating.
3.1 Sentence Ordering
In the standard entity-based model, a discrimina-
tive system is trained on the pairwise rankings be-
tween source documents and their permutations
(see Section 2.2). However, a model learned from
these pairwise rankings is not sufficiently fine-
grained, since the subtle differences between the
permutations are not learned. Our major contribu-
tion is to further differentiate among the permuta-
tions generated from the same source documents,
rather than simply treating them all as being of the
same degree of coherence.
Our fundamental assumption is that there exists
a canonical ordering for the sentences of a doc-
ument; therefore we can approximate the degree
of coherence of a document by the similarity be-
tween its actual sentence ordering and that canon-
ical sentence ordering. Practically, we automati-
cally assign an objective score for each permuta-
tion to estimate its dissimilarity from the source
document (see Section 4). By learning from all
the pairs across a source document and its per-
mutations, the effective size of the training data
is increased while no further manual annotation
is required, which is favorable in real applica-
tions when available samples with manually an-
notated coherence scores are usually limited. For
r source documents each with m random permuta-
tions, the number of training instances in the stan-
dard entity-based model is therefore r ? m, while
in our multiple-rank model learning process, it is
r ?
(
m+1
2
)
? 12 r ? m
2 > r ? m, when m > 2.
3.2 Summary Coherence Rating
Compared to the standard entity-based coherence
model, our major contribution in this task is to
show that by automatically assigning an objective
score for each machine-generated summary to es-
timate its dissimilarity from the human-generated
summary from the same input document cluster,
we are able to achieve performance competitive
with, or even superior to, that of B&L?s model
without knowing the true coherence score given
by human judges.
Evaluating our multiple-rank model in this task
is crucial, since in summary coherence rating,
the coherence violations that the reader might en-
counter in real machine-generated texts can be
more precisely approximated, while the sentence
ordering task is only partially capable of doing so.
317
4 Dissimilarity Metrics
As mentioned previously, the subtle differences
among the permutations of the same source docu-
ment can be used to refine the model learning pro-
cess. Considering an original document d and one
of its permutations, we call ? = (1, 2, . . . ,N) the
reference ordering, which is the sentence order-
ing in d, and pi = (o1, o2, . . . , oN) the test order-
ing, which is the sentence ordering in that permu-
tation, where N is the number of sentences being
rendered in both documents.
In order to approximate different degrees of co-
herence among the set of permutations which bear
the same content, we need a suitable metric to
quantify the dissimilarity between the test order-
ing pi and the reference ordering ?. Such a metric
needs to satisfy the following criteria: (1) It can be
automatically computed while being highly corre-
lated with human judgments of coherence, since
additional manual annotation is certainly undesir-
able. (2) It depends on the particular sentence
ordering in a permutation while remaining inde-
pendent of the entities within the sentences; oth-
erwise our multiple-rank model might be trained
to fit particular probability distributions of entity
transitions rather than true coherence preferences.
In our work we use three different metrics:
Kendall?s ? distance, average continuity, and edit
distance.
Kendall?s ? distance: This metric has been
widely used in evaluation of sentence ordering
(Lapata, 2003; Lapata, 2006; Bollegala et al
2006; Madnani et al 2007)1. It measures the
disagreement between two orderings ? and pi in
terms of the number of inversions of adjacent sen-
tences necessary to convert one ordering into an-
other. Kendall?s ? distance is defined as
? =
2m
N(N ? 1)
,
where m is the number of sentence inversions nec-
essary to convert ? to pi.
Average continuity (AC): Following Zhang
(2011), we use average continuity as the sec-
ond dissimilarity metric. It was first proposed
1Filippova and Strube (2007) found that their perfor-
mance dropped when using this metric for longer rankings;
but they were using data in a different language and with
manual annotations, so its effect on our datasets is worth try-
ing nonetheless.
by Bollegala et al(2006). This metric esti-
mates the quality of a particular sentence order-
ing by the number of correctly arranged contin-
uous sentences, compared to the reference order-
ing. For example, if pi = (. . . , 3, 4, 5, 7, . . . , oN),
then {3, 4, 5} is considered as continuous while
{3, 4, 5, 7} is not. Average continuity is calculated
as
AC = exp
?
??????
1
n ? 1
n?
i=2
log (Pi + ?)
?
?????? ,
where n = min(4,N) is the maximum number
of continuous sentences to be considered, and
? = 0.01. Pi is the proportion of continuous sen-
tences of length i in pi that are also continuous in
the reference ordering ?. To represent the dis-
similarity between the two orderings pi and ?, we
use its complement AC? = 1 ? AC, such that the
larger AC? is, the more dissimilar two orderings
are2.
Edit distance (ED): Edit distance is a com-
monly used metric in information theory to mea-
sure the difference between two sequences. Given
a test ordering pi, its edit distance is defined as the
minimum number of edits (i.e., insertions, dele-
tions, and substitutions) needed to transform it
into the reference ordering ?. For permutations,
the edits are essentially movements, which can
be considered as equal numbers of insertions and
deletions.
5 Experiments
5.1 Sentence Ordering
Our first set of experiments is on sentence order-
ing. Following Barzilay and Lapata (2008), we
use all transitions of length ? 3 for feature extrac-
tion. In addition, we explore three specific aspects
in our experiments: rank assignment, entity ex-
traction, and permutation generation.
5.1.1 Rank Assignment
In our multiple-rank model, pairwise rankings
between a source document and its permutations
are extended into a longer ranking with multiple
ranks. We assign a rank to a particular permuta-
tion, based on the result of applying a chosen dis-
similarity metric from Section 4 (?, AC, or ED) to
the sentence ordering in that permutation.
We experiment with two different approaches
to assigning ranks to permutations, while each
2We will refer to AC? as AC from now on.
318
source document is always assigned a zero (the
highest) rank.
In the raw option, we rank the permutations di-
rectly by their dissimilarity scores to form a full
ranking for the set of permutations generated from
the same source document.
Since a full ranking might be too sensitive to
noise in training, we also experiment with the
stratified option, in which C ranks are assigned to
the permutations generated from the same source
document. The permutation with the smallest dis-
similarity score is assigned the same (zero, the
highest) rank as the source document, and the one
with the largest score is assigned the lowest (C?1)
rank; then ranks of other permutations are uni-
formly distributed in this range according to their
raw dissimilarity scores. We experiment with 3
to 6 ranks (the case where C = 2 reduces to the
standard entity-based model).
5.1.2 Entity Extraction
Barzilay and Lapata (2008)?s best results were
achieved by employing an automatic coreference
resolution tool (Ng and Cardie, 2002) for ex-
tracting entities from a source document, and the
permutations were generated only afterwards ?
entity extraction from a permuted document de-
pends on knowing the correct sentence order and
the oracular entity information from the source
document ? since resolving coreference relations
in permuted documents is too unreliable for an au-
tomatic tool.
We implement our multiple-rank model with
full coreference resolution using Ng and Cardie?s
coreference resolution system, and entity extrac-
tion approach as described above ? the Coref-
erence+ condition. However, as argued by El-
sner and Charniak (2011), to better simulate
the real situations that human readers might en-
counter in machine-generated documents, such
oracular information should not be taken into ac-
count. Therefore we also employ two alterna-
tive approaches for entity extraction: (1) use the
same automatic coreference resolution tool on
permuted documents ? we call it the Corefer-
ence? condition; (2) use no coreference reso-
lution, i.e., group head noun clusters by simple
string matching ? B&L?s Coreference? condi-
tion.
5.1.3 Permutation Generation
The quality of the model learned depends on
the set of permutations used in training. We are
not aware of how B&L?s permutations were gen-
erated, but we assume they are generated in a per-
fectly random fashion.
However, in reality, the probabilities of seeing
documents with different degrees of coherence are
not equal. For example, in an essay scoring task,
if the target group is (near-) native speakers with
sufficient education, we should expect their essays
to be less incoherent ? most of the essays will
be coherent in most parts, with only a few minor
problems regarding discourse coherence. In such
a setting, the performance of a model trained from
permutations generated from a uniform distribu-
tion may suffer some accuracy loss.
Therefore, in addition to the set of permutations
used by Barzilay and Lapata (2008) (PSBL), we
create another set of permutations for each source
document (PSM) by assigning most of the proba-
bility mass to permutations which are mostly sim-
ilar to the original source document. Besides its
capability of better approximating real-life situ-
ations, training our model on permutations gen-
erated in this way has another benefit: in the
standard entity-based model, all permuted doc-
uments are treated as incoherent; thus there are
many more incoherent training instances than co-
herent ones (typically the proportion is 20:1). In
contrast, in our multiple-rank model, permuted
documents are assigned different ranks to fur-
ther differentiate the different degrees of coher-
ence within them. By doing so, our model will
be able to learn the characteristics of a coherent
document from those near-coherent documents as
well, and therefore the problem of lacking coher-
ent instances can be mitigated.
Our permutation generation algorithm is shown
in Algorithm 1, where ? = 0.05, ? = 5.0,
MAX NUM = 50, and K and K? are two normal-
ization factors to make p(swap num) and p(i, j)
proper probability distributions. For each source
document, we create the same number of permu-
tations as PSBL.
5.2 Summary Coherence Rating
In the summary coherence rating task, we are
dealing with a mixture of multi-document sum-
maries generated by systems and written by hu-
mans. Barzilay and Lapata (2008) did not assume
319
Algorithm 1 Permutation Generation.
Input: S 1, S 2, . . . , S N ; ? = (1, 2, . . . ,N)
Choose a number of sentence swaps
swap num with probability e???swap num/K
for i = 1? swap num do
Swap a pair of sentence (S i, S j)
with probability p(i, j) = e???|i? j|/K?
end for
Output: pi = (o1, o2, . . . , oN)
a simple binary distinction among the summaries
generated from the same input document clus-
ter; rather, they had human judges give scores for
each summary based on its degree of coherence
(see Section 3.2). Therefore, it seems that the
subtle differences among incoherent documents
(system-generated summaries in this case) have
already been learned by their model.
But we wish to see if we can replace hu-
man judgments by our computed dissimilarity
scores so that the original supervised learning is
converted into unsupervised learning and yet re-
tain competitive performance. However, given
a summary, computing its dissimilarity score is
a bit involved, due to the fact that we do not
know its correct sentence order. To tackle this
problem, we employ a simple sentence align-
ment between a system-generated summary and
a human-written summary originating from the
same input document cluster. Given a system-
generated summary Ds = (S s1, S s2, . . . , S sn) and
its corresponding human-written summary Dh =
(S h1, S h2, . . . , S hN) (here it is possible that n ,
N), we treat the sentence ordering (1, 2, . . . ,N)
in Dh as ? (the original sentence ordering), and
compute pi = (o1, o2, . . . , on) based on Ds. To
compute each oi in pi, we find the most similar
sentence S h j, j ? [1,N] in Dh by computing their
cosine similarity over all tokens in S h j and S si;
if all sentences in Dh have zero cosine similarity
with S si, we assign ?1 to oi.
Once pi is known, we can compute its ?dissimi-
larity? from ? using a chosen metric. But because
now pi is not guaranteed to be a permutation of ?
(there may be repetition or missing values, i.e.,
?1, in pi), Kendall?s ? cannot be used, and we use
only average continuity and edit distance as dis-
similarity metrics in this experiment.
The remaining experimental configuration is
the same as that of Barzilay and Lapata (2008),
with the optimal transition length set to ? 2.
6 Results
6.1 Sentence Ordering
In this task, we use the same two sets of source
documents (Earthquakes and Accidents, see Sec-
tion 3.1) as Barzilay and Lapata (2008). Each
contains 200 source documents, equally divided
between training and test sets, with up to 20 per-
mutations per document. We conduct experi-
ments on these two domains separately. For each
domain, we accompany each source document
with two different sets of permutations: the one
used by B&L (PSBL), and the one generated from
our model described in Section 5.1.3 (PSM). We
train our multiple-rank model and B&L?s standard
two-rank model on each set of permutations using
the SVMrank package (Joachims, 2006), and eval-
uate both systems on their test sets. Accuracy is
measured as the fraction of correct pairwise rank-
ings for the test set.
6.1.1 Full Coreference Resolution with
Oracular Information
In this experiment, we implement B&L?s fully-
fledged standard entity-based coherence model,
and extract entities from permuted documents us-
ing oracular information from the source docu-
ments (see Section 5.1.2).
Results are shown in Table 2. For each test sit-
uation, we list the best accuracy (in Acc columns)
for each chosen dissimilarity metric, with the cor-
responding rank assignment approach. C repre-
sents the number of ranks used in stratifying raw
scores (?N? if using raw configuration, see Sec-
tion 5.1.1 for details). Baselines are accuracies
trained using the standard entity-based coherence
model3.
Our model outperforms the standard entity-
based model on both permutation sets for both
datasets. The improvement is not significant
when trained on the permutation set PSBL, and
is achieved only with one of the three metrics;
3There are discrepancies between our reported accuracies
and those of Barzilay and Lapata (2008). The differences are
due to the fact that we use a different parser: the Stanford de-
pendency parser (de Marneffe et al 2006), and might have
extracted entities in a slightly different way than theirs, al-
though we keep other experimental configurations as close
as possible to theirs. But when comparing our model with
theirs, we always use the exact same set of features, so the
absolute accuracies do not matter.
320
Condition: Coreference+
Perms Metric
Earthquakes Accidents
C Acc C Acc
PSBL
? 3 79.5 3 82.0
AC 4 85.2 3 83.3
ED 3 86.8 6 82.2
Baseline 85.3 83.2
PSM
? 3 86.8 3 85.2*
AC 3 85.6 1 85.4*
ED N 87.9* 4 86.3*
Baseline 85.3 81.7
Table 2: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference+ op-
tion. Accuracies which are significantly better than the
baseline (p < .05) are indicated by *.
but when trained on PSM (the set of permutations
generated from our biased model), our model?s
performance significantly exceeds B&L?s4 for all
three metrics, especially as their model?s perfor-
mance drops for dataset Accidents.
From these results, we see that in the ideal sit-
uation where we extract entities and resolve their
coreference relations based on the oracular infor-
mation from the source document, our model is
effective in terms of improving ranking accura-
cies, especially when trained on our more realistic
permutation sets PSM.
6.1.2 Full Coreference Resolution without
Oracular Information
In this experiment, we apply the same auto-
matic coreference resolution tool (Ng and Cardie,
2002) on not only the source documents but also
their permutations. We want to see how removing
the oracular component in the original model af-
fects the performance of our multiple-rank model
and the standard model. Results are shown in Ta-
ble 3.
First we can see when trained on PSM, run-
ning full coreference resolution significantly hurts
performance for both models. This suggests that,
in real-life applications, where the distribution of
training instances with different degrees of co-
herence is skewed (as in the set of permutations
4Following Elsner and Charniak (2011), we use the
Wilcoxon Sign-rank test for significance.
Condition: Coreference?
Perms Metric
Earthquakes Accidents
C Acc C Acc
PSBL
? 3 71.0 3 73.3
AC 3 *76.8 3 74.5
ED 4 *77.4 6 74.4
Baseline 71.7 73.8
PSM
? 3 55.9 3 51.5
AC 4 53.9 6 49.0
ED 4 53.9 5 52.3
Baseline 49.2 53.2
Table 3: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference? op-
tion. Accuracies which are significantly better than the
baseline (p < .05) are indicated by *.
generated from our model), running full corefer-
ence resolution is not a good option, since it al-
most makes the accuracies no better than random
guessing (50%).
Moreover, considering training using PSBL,
running full coreference resolution has a different
influence for the two datasets. For Earthquakes,
our model significantly outperforms B&L?s while
the improvement is insignificant for Accidents.
This is most probably due to the different way that
entities are realized in these two datasets. As an-
alyzed by Barzilay and Lapata (2008), in dataset
Earthquakes, entities tend to be referred to by pro-
nouns in subsequent mentions, while in dataset
Accidents, literal string repetition is more com-
mon.
Given a balanced permutation distribution as
we assumed in PSBL, switching distant sentence
pairs in Accidents may result in very similar en-
tity distribution with the situation of switching
closer sentence pairs, as recognized by the auto-
matic tool. Therefore, compared to Earthquakes,
our multiple-rank model may be less powerful in
indicating the dissimilarity between the sentence
orderings in a permutation and its source docu-
ment, and therefore can improve on the baseline
only by a small margin.
6.1.3 No Coreference Resolution
In this experiment, we do not employ any coref-
erence resolution tool, and simply cluster head
321
Condition: Coreference?
Perms Metric
Earthquakes Accidents
C Acc C Acc
PSBL
? 4 82.8 N 82.0
AC 3 78.0 3 **84.2
ED N 78.2 3 *82.7
Baseline 83.7 80.1
PSM
? 3 **86.4 N **85.7
AC 4 *84.4 N **86.6
ED 5 **86.7 N **84.6
Baseline 82.6 77.5
Table 4: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in sentence ordering using Coreference? op-
tion. Accuracies which are significantly better than the
baseline are indicated by * (p < .05) and ** (p < .01).
nouns by string matching. Results are shown in
Table 4.
Even with such a coarse approximation of
coreference resolution, our model is able to
achieve around 85% accuracy in most test cases,
except for dataset Earthquakes, training on PSBL
gives poorer performance than the standard model
by a small margin. But such inferior perfor-
mance should be expected, because as explained
above, coreference resolution is crucial to this
dataset, since entities tend to be realized through
pronouns; simple string matching introduces too
much noise into training, especially when our
model wants to train a more fine-grained discrim-
inative system than B&L?s. However, we can see
from the result of training on PSM, if the per-
mutations used in training do not involve swap-
ping sentences which are too far away, the result-
ing noise is reduced, and our model outperforms
theirs. And for dataset Accidents, our model
consistently outperforms the baseline model by a
large margin (with significance test at p < .01).
6.1.4 Conclusions for Sentence Ordering
Considering the particular dissimilarity metric
used in training, we find that edit distance usually
stands out from the other two metrics. Kendall?s ?
distance proves to be a fairly weak metric, which
is consistent with the findings of Filippova and
Strube (2007) (see Section 2.3). Figure 1 plots
the testing accuracies as a function of different
68.0 
73.0 
78.0 
83.0 
88.0 
3 4 5 6 N 
Acc
urac
y (%
) 
C 
Earthquake ED Coref+ 
Earthquake ED Coref? 
Accidents ED Coref+ 
Accidents  ED Coref? 
Accidents ? Coref- 
Figure 1: Effect of C on testing accuracies in selected
sentence ordering experimental configurations.
choices of C?s with the configurations where our
model outperforms the baseline model. In each
configuration, we choose the dissimilarity metric
which achieves the best accuracy reported in Ta-
bles 2 to 4 and the PS BL permutation set. We
can see that the dependency of accuracies on the
particular choice of C is not consistent across all
experimental configurations, which suggests that
this free parameter C needs careful tuning in dif-
ferent experimental setups.
Combining our multiple-rank model with sim-
ple string matching for entity extraction is a ro-
bust option for coherence evaluation, regardless
of the particular distribution of permutations used
in training, and it significantly outperforms the
baseline in most conditions.
6.2 Summary Coherence Rating
As explained in Section 3.2, we employ a simple
sentence alignment between a system-generated
summary and its corresponding human-written
summary to construct a test ordering pi and calcu-
late its dissimilarity between the reference order-
ing ? from the human-written summary. In this
way, we convert B&L?s supervised learning model
into a fully unsupervised model, since human an-
notations for coherence scores are not required.
We use the same dataset as Barzilay and Lap-
ata (2008), which includes multi-document sum-
maries from 16 input document clusters generated
by five systems, along with reference summaries
composed by humans.
In this experiment, we consider only average
continuity (AC) and edit distance (ED) as dissimi-
larity metrics, with raw configuration for rank as-
signment, and compare our multiple-rank model
with the standard entity-based model using ei-
ther full coreference resolution5 or no resolution
5We run the coreference resolution tool on all documents.
322
Entities Metric Same Full
Coreference+
AC 82.5 *72.6
ED 81.3 **73.0
Baseline 78.8 70.9
Coreference?
AC 76.3 72.0
ED 78.8 71.7
Baseline 80.0 72.3
Table 5: Accuracies (%) of extending the stan-
dard entity-based coherence model with multiple-rank
learning in summary rating. Baselines are results of
standard entity-based coherence model. Accuracies
which are significantly better than the corresponding
baseline are indicated by * (p < .05) and ** (p < .01).
for entity extraction. We train both models on
the ranking preferences (144 in all) among sum-
maries originating from the same input document
cluster using the SVMrank package (Joachims,
2006), and test on two different test sets: same-
cluster test and full test. Same-cluster test is the
one used by Barzilay and Lapata (2008), in which
only the pairwise rankings (80 in all) between
summaries originating from the same input doc-
ument cluster are tested; we also experiment with
full test, in which pairwise rankings (1520 in all)
between all summary pairs excluding two human-
written summaries are tested.
Results are shown in Table 5. Coreference+
and Coreference? denote the configuration of
using full coreference resolution or no resolu-
tion separately. First, clearly for both models,
performance on full test is inferior to that on
same-cluster test, but our model is still able to
achieve performance competitive with the stan-
dard model, even if our fundamental assumption
about the existence of canonical sentence order-
ing in documents with same content may break
down on those test pairs not originating from the
same input document cluster. Secondly, for the
baseline model, using the Coreference? configu-
ration yields better accuracy in this task (80.0%
vs. 78.8% on same-cluster test, and 72.3% vs.
70.9% on full test), which is consistent with the
findings of Barzilay and Lapata (2008). But our
multiple-rank model seems to favor the Corefer-
ence+ configuration, and our best accuracy even
exceeds B&L?s best when tested on the same set:
82.5% vs. 80.0% on same-cluster test, and 73.0%
vs. 72.3% on full test.
When our model performs poorer than the
baseline (using Coreference? configuration), the
difference is not significant, which suggests that
our multiple-rank model with unsupervised score
assignment via simple cosine matching can re-
main competitive with the standard model, which
requires human annotations to obtain a more fine-
grained coherence spectrum. This observation is
consistent with Banko and Vanderwende (2004)?s
discovery that human-generated summaries look
quite extractive.
7 Conclusions
In this paper, we have extended the popular co-
herence model of Barzilay and Lapata (2008) by
adopting a multiple-rank learning approach. This
is inherently different from other extensions to
this model, in which the focus is on enriching
the set of features for entity-grid construction,
whereas we simply keep their original feature set
intact, and manipulate only their learning method-
ology. We show that this concise extension is
effective and able to outperform B&L?s standard
model in various experimental setups, especially
when experimental configurations are most suit-
able considering certain dataset properties (see
discussion in Section 6.1.4).
We experimented with two tasks: sentence or-
dering and summary coherence rating, following
B&L?s original framework. In sentence ordering,
we also explored the influence of removing the
oracular component in their original model and
dealing with permutations generated from differ-
ent distributions, showing that our model is robust
for different experimental situations. In summary
coherence rating, we further extended their model
such that their original supervised learning is con-
verted into unsupervised learning with competi-
tive or even superior performance.
Our multiple-rank learning model can be easily
adapted into other extended entity-based coher-
ence models with their enriched feature sets, and
further improvement in ranking accuracies should
be expected.
Acknowledgments
This work was financially supported by the Nat-
ural Sciences and Engineering Research Council
of Canada and by the University of Toronto.
323
References
Michele Banko and Lucy Vanderwende. 2004. Us-
ing n-grams to understand the nature of summaries.
In Proceedings of Human Language Technologies
and North American Association for Computational
Linguistics 2004: Short Papers, pages 1?4.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 42rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005),
pages 141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2006. A bottom-up approach to sen-
tence ordering for multi-document summarization.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 385?392.
Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-
based local coherence modelling using topological
fields. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2010), pages 186?195.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Micha Elsner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2011),
pages 125?129.
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation
(ENLG 2007), pages 139?142.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the 8th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2002), pages 133?142.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2006), pages
217?226.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL 2003), pages
545?552.
Mirella Lapata. 2006. Automatic evaluation of in-
formation ordering: Kendall?s tau. Computational
Linguistics, 32(4):471?484.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2011), pages 997?1006.
Nitin Madnani, Rebecca Passonneau, Necip Fazil
Ayan, John M. Conroy, Bonnie J. Dorr, Ju-
dith L. Klavans, Dianne P. O?Leary, and Judith D.
Schlesinger. 2007. Measuring variability in sen-
tence ordering for news summarization. In Pro-
ceedings of the Eleventh European Workshop on
Natural Language Generation (ENLG 2007), pages
81?88.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics (ACL 2002),
pages 104?111.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! Computing semantic relatedness using
Wikipedia. In Proceedings of the 21st National
Conference on Artificial Intelligence, pages 1219?
1224.
Renxian Zhang. 2011. Sentence ordering driven by
local and global coherence for summary generation.
In Proceedings of the ACL 2011 Student Session,
pages 6?11.
324
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 987?996,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Classifying Arguments by Scheme
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
Argumentation schemes are structures or tem-
plates for various kinds of arguments. Given
the text of an argument with premises and con-
clusion identified, we classify it as an instance
of one of five common schemes, using features
specific to each scheme. We achieve accura-
cies of 63?91% in one-against-others classifi-
cation and 80?94% in pairwise classification
(baseline = 50% in both cases).
1 Introduction
We investigate a new task in the computational anal-
ysis of arguments: the classification of arguments
by the argumentation schemes that they use. An ar-
gumentation scheme, informally, is a framework or
structure for a (possibly defeasible) argument; we
will give a more-formal definition and examples in
Section 3. Our work is motivated by the need to de-
termine the unstated (or implicitly stated) premises
that arguments written in natural language normally
draw on. Such premises are called enthymemes.
For instance, the argument in Example 1 consists
of one explicit premise (the first sentence) and a con-
clusion (the second sentence):
Example 1 [Premise:] The survival of the entire
world is at stake.
[Conclusion:] The treaties and covenants aiming
for a world free of nuclear arsenals and other con-
ventional and biological weapons of mass destruc-
tion should be adhered to scrupulously by all na-
tions.
Another premise is left implicit ? ?Adhering to
those treaties and covenants is a means of realizing
survival of the entire world?. This proposition is an
enthymeme of this argument.
Our ultimate goal is to reconstruct the en-
thymemes in an argument, because determining
these unstated assumptions is an integral part of un-
derstanding, supporting, or attacking an entire argu-
ment. Hence reconstructing enthymemes is an im-
portant problem in argument understanding. We be-
lieve that first identifying the particular argumenta-
tion scheme that an argument is using will help to
bridge the gap between stated and unstated proposi-
tions in the argument, because each argumentation
scheme is a relatively fixed ?template? for arguing.
That is, given an argument, we first classify its ar-
gumentation scheme; then we fit the stated proposi-
tions into the corresponding template; and from this
we infer the enthymemes.
In this paper, we present an argument scheme
classification system as a stage following argument
detection and proposition classification. First in Sec-
tion 2 and Section 3, we introduce the background
to our work, including related work in this field,
the two core concepts of argumentation schemes and
scheme-sets, and the Araucaria dataset. In Section 4
and Section 5 we present our classification system,
including the overall framework, data preprocessing,
feature selection, and the experimental setups. In
the remaining section, we present the essential ap-
proaches to solve the leftover problems of this paper
which we will study in our future work, and discuss
the experimental results, and potential directions for
future work.
987
2 Related work
Argumentation has not received a great deal of at-
tention in computational linguistics, although it has
been a topic of interest for many years. Cohen
(1987) presented a computational model of argu-
mentative discourse. Dick (1987; 1991a; 1991b) de-
veloped a representation for retrieval of judicial de-
cisions by the structure of their legal argument ? a
necessity for finding legal precedents independent of
their domain. However, at that time no corpus of ar-
guments was available, so Dick?s system was purely
theoretical. Recently, the Araucaria project at Uni-
versity of Dundee has developed a software tool for
manual argument analysis, with a point-and-click in-
terface for users to reconstruct and diagram an ar-
gument (Reed and Rowe, 2004; Rowe and Reed,
2008). The project also maintains an online repos-
itory, called AraucariaDB, of marked-up naturally
occurring arguments collected by annotators world-
wide, which can be used as an experimental corpus
for automatic argumentation analysis (for details see
Section 3.2).
Recent work on argument interpretation includes
that of George, Zukerman, and Nieman (2007), who
interpret constructed-example arguments (not natu-
rally occurring text) as Bayesian networks. Other
contemporary research has looked at the automatic
detection of arguments in text and the classification
of premises and conclusions. The work closest to
ours is perhaps that of Mochales and Moens (2007;
2008; 2009a; 2009b). In their early work, they fo-
cused on automatic detection of arguments in legal
texts. With each sentence represented as a vector of
shallow features, they trained a multinomial na??ve
Bayes classifier and a maximum entropy model on
the Araucaria corpus, and obtained a best average
accuracy of 73.75%. In their follow-up work, they
trained a support vector machine to further classify
each argumentative clause into a premise or a con-
clusion, with an F1 measure of 68.12% and 74.07%
respectively. In addition, their context-free grammar
for argumentation structure parsing obtained around
60% accuracy.
Our work is ?downstream? from that of Mochales
and Moens. Assuming the eventual success of their,
or others?, research program on detecting and clas-
sifying the components of an argument, we seek to
determine how the pieces fit together as an instance
of an argumentation scheme.
3 Argumentation schemes, scheme-sets,
and annotation
3.1 Definition and examples
Argumentation schemes are structures or templates
for forms of arguments. The arguments need not be
deductive or inductive; on the contrary, most argu-
mentation schemes are for presumptive or defeasible
arguments (Walton and Reed, 2002). For example,
argument from cause to effect is a commonly used
scheme in everyday arguments. A list of such argu-
mentation schemes is called a scheme-set.
It has been shown that argumentation schemes
are useful in evaluating common arguments as falla-
cious or not (van Eemeren and Grootendorst, 1992).
In order to judge the weakness of an argument, a set
of critical questions are asked according to the par-
ticular scheme that the argument is using, and the
argument is regarded as valid if it matches all the
requirements imposed by the scheme.
Walton?s set of 65 argumentation schemes (Wal-
ton et al, 2008) is one of the best-developed scheme-
sets in argumentation theory. The five schemes de-
fined in Table 1 are the most commonly used ones,
and they are the focus of the scheme classification
system that we will describe in this paper.
3.2 Araucaria dataset
One of the challenges for automatic argumentation
analysis is that suitable annotated corpora are still
very rare, in spite of work by many researchers.
In the work described here, we use the Araucaria
database1, an online repository of arguments, as our
experimental dataset. Araucaria includes approxi-
mately 660 manually annotated arguments from var-
ious sources, such as newspapers and court cases,
and keeps growing. Although Araucaria has sev-
eral limitations, such as rather small size and low
agreement among annotators2, it is nonetheless one
of the best argumentative corpora available to date.
1http://araucaria.computing.dundee.ac.uk/doku.php#
araucaria argumentation corpus
2The developers of Araucaria did not report on inter-
annotator agreement, probably because some arguments are an-
notated by only one commentator.
988
Argument from example
Premise: In this particular case, the individual a
has property F and also property G.
Conclusion: Therefore, generally, if x has prop-
erty F, then it also has property G.
Argument from cause to effect
Major premise: Generally, if A occurs, then B will
(might) occur.
Minor premise: In this case, A occurs (might oc-
cur).
Conclusion: Therefore, in this case, B will
(might) occur.
Practical reasoning
Major premise: I have a goal G.
Minor premise: Carrying out action A is a means
to realize G.
Conclusion: Therefore, I ought (practically
speaking) to carry out this action A.
Argument from consequences
Premise: If A is (is not) brought about, good (bad)
consequences will (will not) plausibly occur.
Conclusion: Therefore, A should (should not) be
brought about.
Argument from verbal classification
Individual premise: a has a particular property F.
Classification premise: For all x, if x has property
F, then x can be classified as having property
G.
Conclusion: Therefore, a has property G.
Table 1: The five most frequent schemes and their defini-
tions in Walton?s scheme-set.
Arguments in Araucaria are annotated in a XML-
based format called ?AML? (Argument Markup
Language). A typical argument (see Example 2)
consists of several AU nodes. Each AU node is a
complete argument unit, composed of a conclusion
proposition followed by optional premise proposi-
tion(s) in a linked or convergent structure. Each of
these propositions can be further defined as a hier-
archical collection of smaller AUs. INSCHEME is
the particular scheme (e.g., ?Argument from Con-
sequences?) of which the current proposition is a
member; enthymemes that have been made explicit
are annotated as ?missing = yes?.
Example 2 Example of argument markup from
Araucaria
<TEXT>If we stop the free creation of art, we will stop
the free viewing of art.</TEXT>
<AU>
<PROP identifier="C" missing="yes">
<PROPTEXT offset="-1">
The prohibition of the free creation of art should
not be brought about.</PROPTEXT>
<INSCHEME scheme="Argument from Consequences"
schid="0" />
</PROP>
<LA>
<AU>
<PROP identifier="A" missing="no">
<PROPTEXT offset="0">
If we stop the free creation of art, we will
stop the free viewing of art.</PROPTEXT>
<INSCHEME scheme="Argument from Consequences"
schid="0" />
</PROP>
</AU>
<AU>
<PROP identifier="B" missing="yes">
<PROPTEXT offset="-1">
The prohibition of free viewing of art is not
acceptable.</PROPTEXT>
<INSCHEME scheme="Argument from Consequences"
schid="0" />
</PROP>
</AU>
</LA>
</AU>
There are three scheme-sets used in the anno-
tations in Araucaria: Walton?s scheme-set, Katzav
and Reed?s (2004) scheme-set, and Pollock?s (1995)
scheme-set. Each of these has a different set of
schemes; and most arguments in Araucaria are
marked up according to only one of them. Our
experimental dataset is composed of only those
arguments annotated in accordance with Walton?s
scheme-set, within which the five schemes shown in
Table 1 constitute 61% of the total occurrences.
4 Methods
4.1 Overall framework
As we noted above, our ultimate goal is to recon-
struct enthymemes, the unstated premises, in an ar-
gument by taking advantage of the stated proposi-
tions; and in order to achieve this goal we need to
first determine the particular argumentation scheme
that the argument is using. This problem is de-
picted in Figure 1. Our scheme classifier is the
dashed round-cornered rectangle portion of this
989
Detecting 
argumentative text
ARGUMENTATIVE 
SEGMENT
Premise / 
conclusion 
classifier
CONCLUSION
PREMISE #1
PREMISE #2
Scheme classifier
TEXT
ARGUMENTATION 
SCHEME
Argument 
template fitter
CONSTRUCTED 
ENTHYMEME
Figure 1: Overall framework of this research.
overall framework: its input is the extracted con-
clusion and premise(s) determined by an argument
detector, followed by a premise / conclusion classi-
fier, given an unknown text as the input to the entire
system. And the portion below the dashed round-
rectangle represents our long-term goal ? to recon-
struct the implicit premise(s) in an argument, given
its argumentation scheme and its explicit conclusion
and premise(s) as input. Since argument detection
and classification are not the topic of this paper, we
assume here that the input conclusion and premise(s)
have already been retrieved, segmented, and classi-
fied, as for example by the methods of Mochales and
Moens (see Section 2 above). And the scheme tem-
plate fitter is the topic of our on-going work.
4.2 Data preprocessing
From all arguments in Araucaria, we first ex-
tract those annotated in accordance with Walton?s
scheme-set. Then we break each complex AU
node into several simple AUs where no conclusion
or premise proposition nodes have embedded AU
nodes. From these generated simple arguments, we
extract those whose scheme falls into one of the five
most frequent schemes as described in Table 1. Fur-
thermore, we remove all enthymemes that have been
inserted by the annotator and ignore any argument
with a missing conclusion, since the input to our pro-
posed classifier, as depicted in Figure 1, cannot have
any access to unstated argumentative propositions.
The resulting preprocessed dataset is composed of
393 arguments, of which 149, 106, 53, 44, and 41
respectively belong to the five schemes in the order
shown in Table 1.
4.3 Feature selection
The features used in our work fall into two cat-
egories: general features and scheme-specific fea-
tures.
4.3.1 General features
General features are applicable to arguments belong-
ing to any of the five schemes (shown in Table 2).
For the features conLoc, premLoc, gap, and
lenRat, we have two versions, differing in terms
of their basic measurement unit: sentence-based
and token-based. The final feature, type, indicates
whether the premises contribute to the conclusion
in a linked or convergent order. A linked argument
(LA) is one that has two or more inter-dependent
premise propositions, all of which are necessary to
make the conclusion valid, whereas in a conver-
gent argument (CA) exactly one premise proposi-
tion is sufficient to do so. Since it is observed that
there exists a strong correlation between type and
the particular scheme employed while arguing, we
believe type can be a good indicator of argumenta-
tion scheme. However, although this feature is avail-
able to us because it is included in the Araucaria an-
notations, its value cannot be obtained from raw text
as easily as other features mentioned above; but it is
possible that we will in the future be able to deter-
mine it automatically by taking advantage of some
scheme-independent cues such as the discourse re-
lation between the conclusion and the premises.
4.3.2 Scheme-specific features
Scheme-specific features are different for each
scheme, since each scheme has its own cue phrases
or patterns. The features for each scheme are shown
in Table 3 (for complete lists of features see Feng
(2010)). In our experiments in Section 5 below, all
these features are computed for all arguments; but
990
conLoc: the location (in token or sentence) of the
conclusion in the text.
premLoc: the location (in token or sentence) of
the first premise proposition.
conFirst: whether the conclusion appears before
the first premise proposition.
gap: the interval (in token or sentence) between
the conclusion and the first premise proposi-
tion.
lenRat: the ratio of the length (in token or sen-
tence) of the premise(s) to that of the conclu-
sion.
numPrem: the number of explicit premise propo-
sitions (PROP nodes) in the argument.
type: type of argumentation structure, i.e., linked
or convergent.
Table 2: List of general features.
the features for any particular scheme are used only
when it is the subject of a particular task. For ex-
ample, when we classify argument from example
in a one-against-others setup, we use the scheme-
specific features of that scheme for all arguments;
when we classify argument from example against
argument from cause to effect, we use the scheme-
specific features of those two schemes.
For the first three schemes (argument from ex-
ample, argument from cause to effect, and practi-
cal reasoning), the scheme-specific features are se-
lected cue phrases or patterns that are believed to be
indicative of each scheme. Since these cue phrases
and patterns have differing qualities in terms of their
precision and recall, we do not treat them all equally.
For each cue phrase or pattern, we compute ?confi-
dence?, the degree of belief that the argument of in-
terest belongs to a particular scheme, using the dis-
tribution characteristics of the cue phrase or pattern
in the corpus, as described below.
For each argument A, a vector CV = {c1, c2, c3}
is added to its feature set, where each ci indicates
the ?confidence? of the existence of the specific fea-
tures associated with each of the first three schemes,
schemei. This is defined in Equation 1:
ci =
1
N
mi?
k=1
(P (schemei|cpk) ? dik) (1)
Argument from example
8 keywords and phrases including for example,
such as, for instance, etc.; 3 punctuation cues: ?:?,
?;?, and ???.
Argument from cause to effect
22 keywords and simple cue phrases including re-
sult, related to, lead to, etc.; 10 causal and non-
causal relation patterns extracted from WordNet
(Girju, 2003).
Practical reasoning
28 keywords and phrases including want, aim, ob-
jective, etc.; 4 modal verbs: should, could, must,
and need; 4 patterns including imperatives and in-
finitives indicating the goal of the speaker.
Argument from consequences
The counts of positive and negative propositions
in the conclusion and premises, calculated from
the General Inquirer2.
Argument from verbal classification
The maximal similarity between the central word
pairs extracted from the conclusion and the
premise; the counts of copula, expletive, and neg-
ative modifier dependency relations returned by
the Stanford parser3 in the conclusion and the
premise.
2 http://www.wjh.harvard.edu/?inquirer/
3 http://nlp.stanford.edu/software/lex-parser.shtml
Table 3: List of scheme-specific features.
Here mi is the number of scheme-specific cue
phrases designed for schemei; P (schemei|cpk) is the
prior probability that the argument A actually be-
longs to schemei, given that some particular cue
phrase cpk is found in A; dik is a value indicat-
ing whether cpk is found in A; and the normaliza-
tion factor N is the number of scheme-specific cue
phrase patterns designed for schemei with at least
one support (at least one of the arguments belonging
to schemei contains that cue phrase). There are two
ways to calculate dik, Boolean and count: in Boolean
mode, dik is treated as 1 if A matches cpk; in count
mode, dik equals to the number of times A matches
cpk; and in both modes, dik is treated as 0 if cpk is
not found inA.
991
For argument from consequences, since the arguer
has an obvious preference for some particular con-
sequence, sentiment orientation can be a good in-
dicator for this scheme, which is quantified by the
counts of positive and negative propositions in the
conclusion and premise.
For argument from verbal classification, there ex-
ists a hypernymy-like relation between some pair of
propositions (entities, concepts, or actions) located
in the conclusion and the premise respectively. The
existence of such a relation is quantified by the max-
imal Jiang-Conrath Similarity (Jiang and Conrath,
1997) between the ?central word? pairs extracted
from the conclusion and the premise. We parse each
sentence of the argument with the Stanford depen-
dency parser, and a word or phrase is considered to
be a central word if it is the dependent or governor of
several particular dependency relations, which basi-
cally represents the attribute or the action of an en-
tity in a sentence, or the entity itself. For example,
if a word or phrase is the dependent of the depen-
dency relation agent, it is therefore considered as a
?central word?. In addition, an arguer tends to use
several particular syntactic structures (copula, exple-
tive, and negative modifier) when using this scheme,
which can be quantified by the counts of those spe-
cial relations in the conclusion and the premise(s).
5 Experiments
5.1 Training
We experiment with two kinds of classification: one-
against-others and pairwise. We build a pruned
C4.5 decision tree (Quinlan, 1993) for each different
classification setup, implemented by Weka Toolkit
3.65 (Hall et al, 2009).
One-against-others classification A one-against-
others classifier is constructed for each of the five
most frequent schemes, using the general features
and the scheme-specific features for the scheme of
interest. For each classifier, there are two possi-
ble outcomes: target scheme and other; 50% of the
training dataset is arguments associated with tar-
get scheme, while the rest is arguments of all the
other schemes, which are treated as other. One-
against-other classification thus tests the effective-
5http://cs.waikato.ac.nz/ml/weka
ness of each scheme?s specific features.
Pairwise classification A pairwise classifier is
constructed for each of the ten possible pairings
of the five schemes, using the general features and
the scheme-specific features of the two schemes in
the pair. For each of the ten classifiers, the train-
ing dataset is divided equally into arguments be-
longing to scheme1 and arguments belonging to
scheme2, where scheme1 and scheme2 are two dif-
ferent schemes among the five. Only features asso-
ciated with scheme1 and scheme2 are used.
5.2 Evaluation
We experiment with different combinations of gen-
eral features and scheme-specific features (discussed
in Section 4.3). To evaluate each experiment, we
use the average accuracy over 10 pools of randomly
sampled data (each with baseline at 50%6) with 10-
fold cross-validation.
6 Results
We first present the best average accuracy (BAA) of
each classification setup. Then we demonstrate the
impact of the feature type (convergent or linked ar-
gument) on BAAs for different classification setups,
since we believe type is strongly correlated with
the particular argumentation scheme and its value is
the only one directly retrieved from the annotations
of the training corpus. For more details, see Feng
(2010).
6.1 BAAs of each classification setup
target scheme BAA dik base type
example 90.6 count token yes
cause 70.4 Boolean
/ count
token no
reasoning 90.8 count sentence yes
consequences 62.9 ? sentence yes
classification 63.2 ? token yes
Table 4: Best average accuracies (BAAs) (%) of one-
against-others classification.
6We also experiment with using general features only, but
the results are consistently below or around the sampling base-
line of 50%; therefore, we do not use them as a baseline here.
992
example cause reason-
ing
conse-
quences
cause 80.6
reasoning 93.1 94.2
consequences 86.9 86.7 97.9
classification 86.0 85.6 98.3 64.2
Table 5: Best average accuracies (BAAs) (%) of pairwise
classification.
Table 4 presents the best average accuracies of
one-against-others classification for each of the five
schemes. The subsequent three columns list the
particular strategies of features incorporation under
which those BAAs are achieved (the complete set of
possible choices is given in Section 4.3.):
? dik: Boolean or count ? the strategy of com-
bining scheme-specific cue phrases or patterns
using either Boolean or count for dik.
? base: sentence or token ? the basic unit of ap-
plying location- or length-related general fea-
tures.
? type: yes or no ? whether type (convergent or
linked argument) is incorporated into the fea-
ture set.
As Table 4 shows, one-against-others classifica-
tion achieves high accuracy for argument from ex-
ample and practical reasoning: 90.6% and 90.8%.
The BAA of argument from cause to effect is only
just over 70%. However, with the last two schemes
(argument from consequences and argument from
verbal classification), accuracy is only in the low
60s; there is little improvement of our system over
the majority baseline of 50%. This is probably due
at least partly to the fact that these schemes do not
have such obvious cue phrases or patterns as the
other three schemes which therefore may require
more world knowledge encoded, and also because
the available training data for each is relatively small
(44 and 41 instances, respectively). The BAA for
each scheme is achieved with inconsistent choices
of base and dik, but the accuracies that resulted from
different choices vary only by very little.
Table 5 shows that our system is able to correctly
differentiate between most of the different scheme
pairs, with accuracies as high as 98%. It has poor
performance (64.0%) only for the pair argument
from consequences and argument from verbal clas-
sification; perhaps not coincidentally, these are the
two schemes for which performance was poorest in
the one-against-others task.
6.2 Impact of type on classification accuracy
As we can see from Table 6, for one-against-others
classifications, incorporating type into the feature
vectors improves classification accuracy in most
cases: the only exception is that the best average ac-
curacy of one-against-others classification between
argument from cause to effect and others is obtained
without involving type into the feature vector ?
but the difference is negligible, i.e., 0.5 percent-
age points with respect to the average difference.
Type also has a relatively small impact on argument
from verbal classification (2.6 points), compared to
its impact on argument from example (22.3 points),
practical reasoning (8.1 points), and argument from
consequences (7.5 points), in terms of the maximal
differences.
Similarly, for pairwise classifications, as shown
in Table 7, type has significant impact on BAAs, es-
pecially on the pairs of practical reasoning versus
argument from cause to effect (17.4 points), prac-
tical reasoning versus argument from example (22.6
points), and argument from verbal classification ver-
sus argument from example (20.2 points), in terms
of the maximal differences; but it has a relatively
small impact on argument from consequences ver-
sus argument from cause to effect (0.8 point), and
argument from verbal classification versus argument
from consequences (1.1 points), in terms of average
differences.
7 Future Work
In future work, we will look at automatically clas-
sifying type (i.e., whether an argument is linked or
convergent), as type is the only feature directly re-
trieved from annotations in the training corpus that
has a strong impact on improving classification ac-
curacies.
Automatically classifying type will not be easy,
because sometimes it is subjective to say whether a
premise is sufficient by itself to support the conclu-
sion or not, especially when the argument is about
993
target scheme BAA-t BAA-no t max diff min diff avg diff
example 90.6 71.6 22.3 10.6 14.7
cause 70.4 70.9 ?0.5 ?0.6 ?0.5
reasoning 90.8 83.2 8.1 7.5 7.7
consequences 62.9 61.9 7.5 ?0.6 4.2
classification 63.2 60.7 2.6 0.4 2.0
Table 6: Accuracy (%) with and without type in one-against-others classification. BAA-t is best average accuracy with
type, and BAA-no t is best average accuracy without type. max diff, min diff, and avg diff are maximal, minimal, and
average differences between each experimental setup with type and without type while the remaining conditions are
the same.
scheme1 scheme2 BAA-t BAA-no t max diff min diff avg diff
cause example 80.6 69.7 10.9 7.1 8.7
reasoning example 93.1 73.1 22.8 19.1 20.1
reasoning cause 94.2 80.5 17.4 8.7 13.9
consequences example 86.9 76.0 13.8 6.9 10.1
consequences cause 87.7 86.7 3.8 ?1.5 ?0.1
consequences reasoning 97.9 97.9 10.6 0.0 0.8
classification example 86.0 74.6 20.2 3.7 7.1
classification cause 85.6 76.8 9.0 3.7 7.1
classification reasoning 98.3 89.3 8.9 4.2 8.3
classification consequences 64.0 60.0 6.5 ?1.3 1.1
Table 7: Accuracy (%) with and without type in pairwise classification. Column headings have the same meanings as
in Table 6.
personal opinions or judgments. So for this task,
we will initially focus on arguments that are (or at
least seem to be) empirical or objective rather than
value-based. It will also be non-trivial to deter-
mine whether an argument is convergent or linked
? whether the premises are independent of one an-
other or not. Cue words and discourse relations be-
tween the premises and the conclusion will be one
helpful factor; for example, besides generally flags
an independent premise. And one premise may be
regarded as linked to another if either would become
an enthymeme if deleted; but determining this in the
general case, without circularity, will be difficult.
We will also work on the argument template fitter,
which is the final component in our overall frame-
work. The task of the argument template fitter is to
map each explicitly stated conclusion and premise
into the corresponding position in its scheme tem-
plate and to extract the information necessary for en-
thymeme reconstruction. Here we propose a syntax-
based approach for this stage, which is similar to
tasks in information retrieval. This can be best ex-
plained by the argument in Example 1, which uses
the particular argumentation scheme practical rea-
soning.
We want to fit the Premise and the Conclusion of
this argument into the Major premise and the Con-
clusion slots of the definition of practical reasoning
(see Table 1), and construct the following conceptual
mapping relations:
1. Survival of the entire world ?? a goal G
2. Adhering to the treaties and covenants aiming
for a world free of nuclear arsenals and other
conventional and biological weapons of mass
destruction ?? action A
Thereby we will be able to reconstruct the missing
Minor premise ? the enthymeme in this argument:
Carrying out adhering to the treaties and
covenants aiming for a world free of nuclear
arsenals and other conventional and biological
994
weapons of mass destruction is a means of real-
izing survival of the entire world.
8 Conclusion
The argumentation scheme classification system that
we have presented in this paper introduces a new
task in research on argumentation. To the best of
our knowledge, this is the first attempt to classify
argumentation schemes.
In our experiments, we have focused on the five
most frequently used schemes in Walton?s scheme-
set, and conducted two kinds of classification: in
one-against-others classification, we achieved over
90% best average accuracies for two schemes, with
other three schemes in the 60s to 70s; and in pair-
wise classification, we obtained 80% to 90% best
average accuracies for most scheme pairs. The poor
performance of our classification system on other
experimental setups is partly due to the lack of train-
ing examples or to insufficient world knowledge.
Completion of our scheme classification system
will be a step towards our ultimate goal of recon-
structing the enthymemes in an argument by the pro-
cedure depicted in Figure 1. Because of the signifi-
cance of enthymemes in reasoning and arguing, this
is crucial to the goal of understanding arguments.
But given the still-premature state of research of ar-
gumentation in computational linguistics, there are
many practical issues to deal with first, such as the
construction of richer training corpora and improve-
ment of the performance of each step in the proce-
dure.
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada and by the University of Toronto. We are
grateful to Suzanne Stevenson for helpful comments
and suggestions.
References
Robin Cohen. 1987. Analyzing the structure of ar-
gumentative discourse. Computational Linguistics,
13(1?2):11?24.
Judith Dick. 1987. Conceptual retrieval and case law.
In Proceedings, First International Conference on Ar-
tificial Intelligence and Law, pages 106?115, Boston,
May.
Judith Dick. 1991a. A Conceptual, Case-relation Repre-
sentation of Text for Intelligent Retrieval. Ph.D. thesis,
Faculty of Library and Information Science, Univer-
sity of Toronto, April.
Judith Dick. 1991b. Representation of legal text for con-
ceptual retrieval. In Proceedings, Third International
Conference on Artificial Intelligence and Law, pages
244?252, Oxford, June.
Vanessa Wei Feng. 2010. Classifying argu-
ments by scheme. Technical report, Depart-
ment of Computer Science, University of Toronto,
November. http://ftp.cs.toronto.edu/pub/
gh/Feng-MSc-2010.pdf.
Sarah George, Ingrid Zukerman, and Michael Niemann.
2007. Inferences, suppositions and explanatory exten-
sions in argument interpretation. User Modeling and
User-Adapted Interaction, 17(5):439?474.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In Proceedings of the
ACL 2003 Workshop on Multilingual Summarization
and Question Answering, pages 76?83, Morristown,
NJ, USA. Association for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations Newsletter, 11(1):10?18.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical taxon-
omy. In International Conference Research on Com-
putational Linguistics (ROCLING X), pages 19?33.
Joel Katzav and Chris Reed. 2004. On argumentation
schemes and the natural classification of arguments.
Argumentation, 18(2):239?259.
Raquel Mochales and Marie-Francine Moens. 2008.
Study on the structure of argumentation in case law. In
Proceedings of the 2008 Conference on Legal Knowl-
edge and Information Systems, pages 11?20, Amster-
dam, The Netherlands. IOS Press.
Raquel Mochales and Marie-Francine Moens. 2009a.
Argumentation mining: the detection, classification
and structure of arguments in text. In ICAIL ?09: Pro-
ceedings of the 12th International Conference on Arti-
ficial Intelligence and Law, pages 98?107, New York,
NY, USA. ACM.
Raquel Mochales and Marie-Francine Moens. 2009b.
Automatic argumentation detection and its role in law
and the semantic web. In Proceedings of the 2009
Conference on Law, Ontologies and the Semantic Web,
pages 115?129, Amsterdam, The Netherlands. IOS
Press.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
995
of arguments in legal texts. In ICAIL ?07: Proceed-
ings of the 11th International Conference on Artificial
Intelligence and Law, pages 225?230, New York, NY,
USA. ACM.
John L. Pollock. 1995. Cognitive Carpentry: A
Blueprint for How to Build a Person. Bradford Books.
The MIT Press, May.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Machine Learning, 16(3):235?240.
Chris Reed and Glenn Rowe. 2004. Araucaria: Software
for argument analysis, diagramming and representa-
tion. International Journal of Artificial Intelligence
Tools, 14:961?980.
Glenn Rowe and Chris Reed. 2008. Argument diagram-
ming: The Araucaria project. In Knowledge Cartog-
raphy, pages 163?181. Springer London.
Frans H. van Eemeren and Rob Grootendorst. 1992.
Argumentation, Communication, and Fallacies: A
Pragma-Dialectical Perspective. Routledge.
Douglas Walton and Chris Reed. 2002. Argumenta-
tion schemes and defeasible inferences. In Workshop
on Computational Models of Natural Argument, 15th
European Conference on Artificial Intelligence, pages
11?20, Amsterdam, The Netherlands. IOS Press.
Douglas Walton, Chris Reed, and Fabrizio Macagno.
2008. Argumentation Schemes. Cambridge University
Press.
996
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 60?68,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Text-level Discourse Parsing with Rich Linguistic Features
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gh@cs.toronto.edu
Abstract
In this paper, we develop an RST-style text-
level discourse parser, based on the HILDA
discourse parser (Hernault et al, 2010b). We
significantly improve its tree-building step by
incorporating our own rich linguistic features.
We also analyze the difficulty of extending
traditional sentence-level discourse parsing to
text-level parsing by comparing discourse-
parsing performance under different discourse
conditions.
1 Introduction
In a well-written text, no unit of the text is com-
pletely isolated; interpretation requires understand-
ing the unit?s relation with the context. Research in
discourse parsing aims to unmask such relations in
text, which is helpful for many downstream applica-
tions such as summarization, information retrieval,
and question answering.
However, most existing discourse parsers oper-
ate on individual sentences alone, whereas discourse
parsing is more powerful for text-level analysis.
Therefore, in this work, we aim to develop a text-
level discourse parser. We follow the framework of
Rhetorical Structure Theory (Mann and Thompson,
1988) and we take the HILDA discourse parser (Her-
nault et al, 2010b) as the basis of our work, because
it is the first fully implemented text-level discourse
parser with state-of-the-art performance. We signif-
icantly improve the performance of HILDA?s tree-
building step (introduced in Section 5.1 below) by
incorporating rich linguistic features (Section 5.3).
In our experiments (Section 6), we also analyze the
difficulty with extending traditional sentence-level
discourse parsing to text-level parsing, by compar-
ing discourse parsing performance under different
discourse conditions.
2 Discourse-annotated corpora
2.1 The RST Discourse Treebank
Rhetorical Structure Theory (Mann and Thompson,
1988) is one of the most widely accepted frame-
works for discourse analysis. In the framework of
RST, a coherent text can be represented as a dis-
course tree whose leaves are non-overlapping text
spans called elementary discourse units (EDUs);
these are the minimal text units of discourse trees.
Adjacent nodes can be related through particular dis-
course relations to form a discourse subtree, which
can then be related to other adjacent nodes in the tree
structure. According to RST, there are two types of
discourse relations, hypotactic (?mononuclear?) and
paratactic (?multi-nuclear?). In mononuclear rela-
tions, one of the text spans, the nucleus, is more
salient than the other, the satellite, while in multi-
nuclear relations, all text spans are equally important
for interpretation.
The example text fragment shown in Figure 1
consists of four EDUs (e1-e4), segmented by square
brackets. Its discourse tree representation is shown
below in the figure, following the notational conven-
tion of RST. The two EDUs e1 and e2 are related by a
mononuclear relation ATTRIBUTION, where e1 is the
more salient span; the span (e1-e2) and the EDU e3
are related by a multi-nuclear relation SAME-UNIT,
where they are equally salient.
60
[Catching up with commercial competitors in retail banking
and financial services,]e1 [they argue,]e2 [will be difficult,]e3
[particularly if market conditions turn sour.]e4
(e1) (e2)
attribution
(e1-e3)
same-unit
(e3)
(e4)
condition
(e1-e4)
(e1-e2)
Figure 1: An example text fragment (wsj 0616) com-
posed of four EDUs, and its RST discourse tree repre-
sentation.
The RST Discourse Treebank (RST-DT) (Carlson
et al, 2001), is a corpus annotated in the framework
of RST. It consists of 385 documents (347 for train-
ing and 38 for testing) from the Wall Street Jour-
nal. In RST-DT, the original 24 discourse relations
defined by Mann and Thompson (1988) are further
divided into a set of 18 relation classes with 78 finer-
grained rhetorical relations in total, which provides
a high level of expressivity.
2.2 The Penn Discourse Treebank
The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008) is another annotated discourse corpus. Its
text is a superset of that of RST-DT (2159 Wall
Street Journal articles). Unlike RST-DT, PDTB does
not follow the framework of RST; rather, it follows
a lexically grounded, predicate-argument approach
with a different set of predefined discourse relations,
as proposed by Webber (2004). In this framework, a
discourse connective (e.g., because) is considered to
be a predicate that takes two text spans as its argu-
ments. The argument that the discourse connective
structurally attaches to is called Arg2, and the other
argument is called Arg1 ? unlike in RST, the two
arguments are not distinguished by their saliency
for interpretation. Another important difference be-
tween PDTB and RST-DT is that in PDTB, there
does not necessarily exist a tree structure covering
the full text, i.e., PDTB-styled discourse relations
exist only in a very local contextual window. In
PDTB, relation types are organized hierarchically:
there are 4 classes, which can be further divided into
16 types and 23 subtypes.
3 Related work
Discourse parsing was first brought to prominence
by Marcu (1997). Since then, many different algo-
rithms and systems (Soricut and Marcu, 2003; Reit-
ter, 2003; LeThanh et al, 2004; Baldridge and Las-
carides, 2005; Subba and Di Eugenio, 2009; Sagae,
2009; Hernault et al, 2010b) have been proposed,
which extracted different textual information and
adopted various approaches for discourse tree build-
ing. Here we briefly review two fully implemented
text-level discourse parsers with the state-of-the-art
performance.
The HILDA discourse parser of Hernault and his
colleagues (duVerle and Prendinger, 2009; Hernault
et al, 2010b) is the first fully-implemented feature-
based discourse parser that works at the full text
level. Hernault et al extracted a variety of lexi-
cal and syntactic features from the input text, and
trained their system on RST-DT. While some of their
features were inspired by the previous work of oth-
ers, e.g., lexico-syntactic features borrowed from
Soricut and Marcu (2003), Hernault et al also pro-
posed the novel idea of discourse tree building by
using two classifiers in cascade ? a binary struc-
ture classifier to determine whether two adjacent text
units should be merged to form a new subtree, and
a multi-class classifier to determine which discourse
relation label should be assigned to the new subtree
? instead of the more-usual single multi-class clas-
sifier with the additional label NO-REL. Hernault
et al obtained 93.8% F-score for EDU segmenta-
tion, 85.0% accuracy for structure classification, and
66.8% accuracy for 18-class relation classification.
Lin et al (2009) attempted to recognize implicit
discourse relations (discourse relations which are
not signaled by explicit connectives) in PDTB by us-
ing four classes of features ? contextual features,
constituent parse features, dependency parse fea-
tures, and lexical features ? and explored their indi-
vidual influence on performance. They showed that
the production rules extracted from constituent parse
trees are the most effective features, while contex-
tual features are the weakest. Subsequently, they
fully implemented an end-to-end PDTB-style dis-
course parser (Lin et al, 2010).
Recently, Hernault et al (2010a) argued that more
effort should be focused on improving performance
61
on certain infrequent relations presented in the dis-
course corpora, since due to the imbalanced distribu-
tion of different discourse relations in both RST-DT
and PDTB, the overall accuracy score can be over-
whelmed by good performance on the small sub-
set of frequent relations, even though the algorithms
perform poorly on all other relations. However, be-
cause of infrequent relations for which we do not
have sufficient instances for training, many unseen
features occur in the test data, resulting in poor test
performance. Therefore, Hernault et al proposed
a semi-supervised method that exploits abundant,
freely-available unlabeled data as a basis for feature
vector extension to alleviate such issues.
4 Text-level discourse parsing
Not until recently has discourse parsing for full texts
been a research focus ? previously, discourse pars-
ing was only performed on the sentence level1. In
this section, we explain why we believe text-level
discourse parsing is crucial.
Unlike syntactic parsing, where we are almost
never interested in parsing above sentence level,
sentence-level parsing is not sufficient for discourse
parsing. While a sequence of local (sentence-level)
grammaticality can be considered to be global gram-
maticality, a sequence of local discourse coherence
does not necessarily form a globally coherent text.
For example, the text shown in Figure 2 contains
two sentences, each of which is coherent and sen-
sible itself. However, there is no reasonable content
transition between these two sentences, so the com-
bination of the two sentences does not make much
sense. If we attempt to represent the text as an RST
discourse tree like the one shown in Figure 1, we
find that no discourse relation can be assigned to re-
late the spans (e1-e2) and (e3-e4) and the text cannot
be represented by a valid discourse tree structure.
In order to rule out such unreasonable transitions
between sentences, we have to expand the text units
upon which discourse parsing is performed: from
sentences to paragraphs, and finally paragraphs to
1Strictly speaking, for PDTB-style discourse parsing (e.g.,
Lin et al (2009; 2010)), there is no absolute distinction between
sentence-level and text-level parsing, since in PDTB, discourse
relations are annotated at a level no higher than that of adjacent
sentences. Here we are concerned with RST-style discourse
parsing.
[No wonder he got an A for his English class,]e1 [he was
studying so hard.]e2 [He avoids eating chocolates,]e3 [since he
is really worried about gaining weight.]e4
(e1) (e2)
cause
(e1-e2)
(e3) (e4)
cause
(e3-e4)
?
Figure 2: An example of incoherent text fragment com-
posed of two sentences. The two EDUs associated with
each sentence are coherent themselves, whereas the com-
bination of the two sentences is not coherent at the sen-
tence boundary. No discourse relation can be associated
with the spans (e1-e2) and (e3-e4).
the full text.
Text-level discourse parsing imposes more con-
straints on the global coherence than sentence-level
discourse parsing. However, if, technically speak-
ing, text-level discourse parsing were no more diffi-
cult than sentence-level parsing, any sentence-level
discourse parser could be easily upgraded to a text-
level discourse parser just by applying it to full
texts. In our experiments (Section 6), we show
that when applied above the sentence level, the per-
formance of discourse parsing is consistently infe-
rior to that within individual sentences, and we will
briefly discuss what the key difficulties with extend-
ing sentence-level to text-level discourse parsing are.
5 Method
We use the HILDA discourse parser of Hernault et
al. (2010b) as the basis of our work. We refine Her-
nault et al?s original feature set by incorporating our
own features as well as some adapted from Lin et al
(2009). We choose HILDA because it is a fully im-
plemented text-level discourse parser with the best
reported performance up to now. On the other hand,
we also follow the work of Lin et al (2009), because
their features can be good supplements to those used
by HILDA, even though Lin et al?s work was based
on PDTB. More importantly, Lin et al?s strategy of
performing feature selection prior to classification
proves to be effective in reducing the total feature
dimensions, which is favorable since we wish to in-
corporate rich linguistic features into our discourse
parser.
62
5.1 Bottom-up approach and two-stage
labeling step
Following the methodology of HILDA, an input text
is first segmented into EDUs. Then, from the EDUs,
a bottom-up approach is applied to build a discourse
tree for the full text. Initially, a binary Structure clas-
sifier evaluates whether a discourse relation is likely
to hold between consecutive EDUs. The two EDUs
which are most probably connected by a discourse
relation are merged into a discourse subtree of two
EDUs. A multi-class Relation classifier evaluates
which discourse relation label should be assigned to
this new subtree. Next, the Structure classifier and
the Relation classifier are employed in cascade to re-
evaluate which relations are the most likely to hold
between adjacent spans (discourse subtrees of any
size, including atomic EDUs). This procedure is re-
peated until all spans are merged, and a discourse
tree covering the full text is therefore produced.
Since EDU boundaries are highly correlated with
the syntactic structures embedded in the sentences,
EDU segmentation is a relatively trivial step ? us-
ing machine-generated syntactic parse trees, HILDA
achieves an F-score of 93.8% for EDU segmenta-
tion. Therefore, our work is focused on the tree-
building step, i.e., the Structure and the Relation
classifiers. In our experiments, we improve the over-
all performance of these two classifiers by incorpo-
rating rich linguistic features, together with appro-
priate feature selection. We also explore how these
two classifiers perform differently under different
discourse conditions.
5.2 Instance extraction
Because HILDA adopts a bottom-up approach for
discourse tree building, errors produced on lower
levels will certainly propagate to upper levels, usu-
ally causing the final discourse tree to be very dis-
similar to the gold standard. While appropriate post-
processing may be employed to fix these errors and
help global discourse tree recovery, we feel that it
might be more effective to directly improve the raw
instance performance of the Structure and Relation
classifiers. Therefore, in our experiments, all classi-
fications are conducted and evaluated on the basis of
individual instances.
Each instance is of the form (SL,SR), which is a
pair of adjacent text spans SL (left span) and SR (right
span), extracted from the discourse tree representa-
tion in RST-DT. From each discourse tree, we ex-
tract positive instances as those pairs of text spans
that are siblings of the same parent node, and neg-
ative examples as those pairs of adjacent text spans
that are not siblings in the tree structure. In all in-
stances, both SL and SR must correspond to a con-
stituent in the discourse tree, which can be either an
atomic EDU or a concatenation of multiple consec-
utive EDUs.
5.3 Feature extraction
Given a pair of text spans (SL,SR), we extract the
following seven types of features.
HILDA?s features: We incorporate the origi-
nal features used in the HILDA discourse parser
with slight modification, which include the follow-
ing four types of features occurring in SL, SR, or
both: (1) N-gram prefixes and suffixes; (2) syntac-
tic tag prefixes and suffixes; (3) lexical heads in the
constituent parse tree; and (4) POS tag of the domi-
nating nodes.
Lin et al?s features: Following Lin et al (2009),
we extract the following three types of features: (1)
pairs of words, one from SL and one from SR, as
originally proposed by Marcu and Echihabi (2002);
(2) dependency parse features in SL, SR, or both; and
(3) syntactic production rules in SL, SR, or both.
Contextual features: For a globally coherent
text, there exist particular sequential patterns in the
local usage of different discourse relations. Given
(SL,SR), the pair of text spans of interest, contextual
features attempt to encode the discourse relations as-
signed to the preceding and the following text span
pairs. Lin et al (2009) also incorporated contextual
features in their feature set. However, their work
was based on PDTB, which has a very different an-
notation framework from RST-DT (see Section 2):
in PDTB, annotated discourse relations can form a
chain-like structure such that contextual features can
be more readily extracted. However, in RST-DT, a
full text is represented as a discourse tree structure,
so the previous and the next discourse relations are
not well-defined.
We resolve this problem as follows. Suppose SL =
(ei-e j) and SR = (e j+1-ek), where i? j < k. To find
the previous discourse relation RELprev that immedi-
63
ately precedes (SL,SR), we look for the largest span
Sprev = (eh-ei?1),h < i, such that it ends right before
SL and all its leaves belong to a single subtree which
neither SL nor SR is a part of. If SL and SR belong
to the same sentence, Sprev must also be a within-
sentence span, and it must be a cross-sentence span
if SL and SR are a cross-sentence span pair. RELprev
is then the discourse relation which covers Sprev. The
next discourse relation RELnext that immediately fol-
lows (SL,SR) is found in the analogous way.
However, when building a discourse tree using
a greedy bottom-up approach, as adopted by the
HILDA discourse parser, RELprev and RELnext are
not always available; therefore these contextual fea-
tures represent an idealized situation. In our ex-
periments we wish to explore whether incorporating
perfect contextual features can help better recognize
discourse relations, and if so, set an upper bound of
performance in more realistic situations.
Discourse production rules: Inspired by Lin et
al. (2009)?s syntactic production rules as features,
we develop another set of production rules, namely
discourse production rules, derived directly from the
tree structure representation in RST-DT.
For example, with respect to the RST discourse
tree shown in Figure 1, we extract the following
discourse production rules: ATTRIBUTION ? NO-
REL NO-REL, SAME-UNIT ? ATTRIBUTION NO-
REL, CONDITION ? SAME-UNIT NO-REL, where
NO-REL denotes a leaf node in the discourse subtree.
The intuition behind using discourse production
rules is that the discourse tree structure is able to re-
flect the relatedness of different discourse relations
? discourse relations on the lower level of the tree
can determine the relation of their direct parent to
some degree. Hernault et al (2010b) attempt to
capture such relatedness by traversing a discourse
subtree and encoding its traversal path as features,
but since they used a depth-first traversal order, the
information encoded in a node?s direct children is
too distant; whereas most useful information can be
gained from the relations covering these direct chil-
dren.
Semantic similarities: Semantic similarities are
useful for recognizing relations such as COMPARI-
SON, when there are no explicit syntactic structures
or lexical features signaling such relations.
We use two subsets of similarity features for verbs
and nouns separately. For each verb in either SL or
SR, we look up its most frequent verb class ID in
VerbNet2, and specify whether that verb class ID ap-
pears in SL, SR, or both. For nouns, we extract all
pairs of nouns from (SL,SR), and compute the aver-
age similarity among these pairs. In particular, we
use path similarity, lch similarity, wup similarity,
res similarity, jcn similarity, and lin similarity pro-
vided in the nltk.wordnet.similarity package (Bird et
al., 2009) for computing WordNet-based similarity,
and always choose the most frequent sense for each
noun.
Cue phrases: We compile a list of cue phrases,
the majority of which are connectives collected by
Knott and Dale (1994). For each cue phrase in this
list, we determine whether it appears in SL or SR. If
a cue phrase appears in a span, we also determine
whether its appearance is in the beginning, the end,
or the middle of that span.
5.4 Feature selection
If we consider all possible combinations of the fea-
tures listed in Section 5.3, the resulting data space
can be horribly high dimensional and extremely
sparse. Therefore, prior to training, we first conduct
feature selection to effectively reduce the dimension
of the data space.
We employ the same feature selection method as
Lin et al (2009). Feature selection is done for each
feature type separately. Among all features belong-
ing to the feature type to be selected, we first ex-
tract all possible features that have been seen in the
training data, e.g., when applying feature selection
for word pairs, we find all word pairs that appear
in some text span pair that have a discourse relation
between them. Then for each extracted feature, we
compute its mutual information with all 18 discourse
relation classes defined in RST-DT, and use the high-
est mutual information to evaluate the effectiveness
of that feature. All extracted features are sorted to
form a ranked list by effectiveness. After that, we
use a threshold to select the top features from that
ranked list. The total number of selected features
used in our experiments is 21,410.
2http://verbs.colorado.edu/?mpalmer/
projects/verbnet
64
6 Experiments
As discussed in Section 5.1, our research focus in
this paper is the tree-building step of the HILDA
discourse parser, which consists of two classifica-
tions: Structure and Relation classification. The bi-
nary Structure classifier decides whether a discourse
relation is likely to hold between consecutive text
spans, and the multi-class Relation classifier decides
which discourse relation label holds between these
two text spans if the Structure classifier predicts the
existence of such a relation.
Although HILDA?s bottom-up approach is aimed
at building a discourse tree for the full text, it does
not explicitly employ different strategies for within-
sentence text spans and cross-sentence text spans.
However, we believe that discourse parsing is signif-
icantly more difficult for text spans at higher levels
of the discourse tree structure. Therefore, we con-
duct the following three sub-experiments to explore
whether the two classifiers behave differently under
different discourse conditions.
Within-sentence: Trained and tested on text span
pairs belonging to the same sentence.
Cross-sentence: Trained and tested on text span
pairs belonging to different sentences.
Hybrid: Trained and tested on all text span pairs.
In particular, we split the training set and the test-
ing set following the convention of RST-DT, and
conduct Structure and Relation classification by in-
corporating our rich linguistic features, as listed in
Section 5.3 above. To rule out all confounding fac-
tors, all classifiers are trained and tested on the basis
of individual text span pairs, by assuming the dis-
course subtree structure (if any) covering each indi-
vidual text span has been already correctly identified
(no error propagation).
6.1 Structure classification
The number of training and testing instances used in
this experiment for different discourse conditions is
listed in Table 1. Instances are extracted in the man-
ner described in Section 5.2. We observe that the
distribution of positive and negative instances is ex-
tremely skewed for cross-sentence instances, while
for all conditions, the distribution is similar in the
training and the testing set.
In this experiment, classifiers are trained using
Dataset Pos # Neg # Total #
Within
Training 11,087 10,188 21,275
Testing 1,340 1,181 2,521
Cross
Training 6,646 49,467 56,113
Testing 882 6,357 7,239
Hybrid
Training 17,733 59,655 77,388
Testing 2,222 7,539 9,761
Table 1: Number of training and testing instances used in
Structure classification.
the SVMperf classifier (Joachims, 2005) with a lin-
ear kernel.
Structure classification performance for all three
discourse conditions is shown in Table 2. The
columns Full and NC (No Context) denote the per-
formance of using all features listed in Section 5.3
and all features except for contextual features re-
spectively. As discussed in Section 5.3, contex-
tual features represent an ideal situation which is
not always available in real applications; therefore,
we wish to see how they affect the overall per-
formance by comparing the performance obtained
with them and without them as features. The col-
umn HILDA lists the performance of using Hernault
et al (2010b)?s original features, and Baseline de-
notes the performance obtained by always picking
the more frequent class. Performance is measured
by four metrics: accuracy, precision, recall, and F1
score on the test set, shown in the first section in
each sub-table.
Under the within-sentence condition, we observe
that, surprisingly, incorporating contextual features
boosts the overall performance by a large margin,
even though it requires only 38 additional features.
Under the cross-sentence condition, our features re-
sult in lower accuracy and precision than HILDA?s
features. However, under this discourse condition,
the distribution of positive and negative instances
in both training and test sets is extremely skewed,
which makes it more sensible to compare the recall
and F1 scores for evaluation. In fact, our features
achieve much higher recall and F1 score despite a
much lower precision and a slightly lower accuracy.
In the second section of each sub-table, we also
list the F1 score on the training data. This allows
65
us to compare the model-fitting capacity of differ-
ent feature sets from another perspective, especially
when the training data is not sufficiently well fitted
by the model. For example, looking at the training
F1 score under the cross-sentence condition, we can
see that classification using full features and clas-
sification without contextual features both perform
significantly better on the training data than HILDA
does. At the same time, such superior performance
is not due to possible over-fitting on the training
data, because we are using significantly fewer fea-
tures (21,410 for Full and 21,372 for NC) than Her-
nault et al (2010b)?s 136,987; rather, it suggests
that using carefully selected rich linguistic features
is able to better model the problem itself.
Comparing the results obtained under the first
two conditions, we see that the binary classification
problem of whether a discourse relation is likely to
hold between two adjacent text spans is much more
difficult under the cross-sentence condition. One
major reason is that many features that are predictive
for within-sentence instances are no longer applica-
ble (e.g., Dependency parse features). In addition,
given the extremely imbalanced nature of the dataset
under this discourse condition, we might need to
employ special approaches to deal with this needle-
in-a-haystack problem. This difficulty can also be
perceived from the training performance. Compared
to the within-sentence condition, all features fit the
training data much more poorly under the cross-
sentence condition. This suggests that sophisticated
features or models in addition to our rich linguis-
tic features must be incorporated in order to fit the
problem sufficiently well. Unfortunately, this under-
fitting issue cannot be resolved by exploiting any
abundant linguistic resources for feature vector ex-
tension (e.g., Hernault et al (2010a)), because the
poor training performance is no longer caused by the
unknown features found in test vectors.
Turning to the hybrid condition, the performance
of Full features is surprisingly good, probably be-
cause we have more available training data than the
other two conditions. However, with contextual fea-
tures removed, our features perform quite similarly
to those of Hernault et al (2010b), but still with
a marginal, but nonetheless statistically significant,
improvement on recall and F1 score.
Full NC HILDA Baseline
Within-sentence
Accuracy 91.04* 85.17* 83.74 53.15
Precision 92.71* 85.36* 84.81 53.15
Recall 90.22* 87.01* 84.55 100.00
F1 91.45* 86.18* 84.68 69.41
Train F1 97.87* 96.23* 95.42 68.52
Cross-sentence
Accuracy 87.69 86.68 89.13 87.82
Precision 49.60 44.73 61.90 ?
Recall 63.95* 39.46* 28.00 0.00
F1 55.87* 41.93* 38.56 ?
Train F1 87.25* 71.93* 49.03 ?
Hybrid
Accuracy 95.64* 87.03 87.04 77.24
Precision 94.77* 74.19 79.41 ?
Recall 85.92* 65.98* 58.15 0.00
F1 89.51* 69.84* 67.13 ?
Train F1 93.15* 80.79* 72.09 ?
Table 2: Structure classification performance (in percent-
age) on text spans of within-sentence, cross-sentence, and
all level. Performance that is significantly superior to that
of HILDA (p < .01, using the Wilcoxon sign-rank test for
significance) is denoted by *.
6.2 Relation classification
The Relation classifier has 18 possible output la-
bels, which are the coarse-grained relation classes
defined in RST-DT. We do not consider nuclearity
when classifying different discourse relations, i.e.,
ATTRIBUTION[N][S] and ATTRIBUTION[S][N] are
treated as the same label. The training and test in-
stances in this experiment are from the positive sub-
set used in Structure classification.
In this experiment, classifiers are trained using
LibSVM classifier (Chang and Lin, 2011) with a lin-
ear kernel and probability estimation.
Relation classification performance under three
discourse conditions is shown in Table 3. We list
the performance achieved by Full, NC, and HILDA
features, as well as the majority baseline, which is
obtained by always picking the most frequent class
label (ELABORATION in all cases).
66
Full NC HILDA Baseline
Within-sentence
MAFS 0.490 0.485 0.446 ?
WAFS 0.763 0.762 0.740 ?
Acc (%) 78.06 78.13 76.42 31.42
TAcc (%) 99.90 99.93 99.26 33.38
Cross-sentence
MAFS 0.194 0.184 0.127 ?
WAFS 0.334 0.329 0.316 ?
Acc (%) 46.83 46.71 45.69 42.52
TAcc (%) 78.30 67.30 57.70 47.79
Hybrid
MAFS 0.440 0.428 0.379 ?
WAFS 0.607 0.604 0.588 ?
Acc (%) 65.30 65.12 64.18 35.82
TAcc (%) 99.96 99.95 90.11 38.78
Table 3: Relation classification performance on text
spans of within-sentence, cross-sentence, and all levels.
Following Hernault et al (2010a), we use Macro-
averaged F-scores (MAFS) to evaluate the perfor-
mance of each classifier. Macro-averaged F-score
is not influenced by the number of instances that
exist in each relation class, by equally weighting
the performance of each relation class3. Therefore,
the evaluation is not biased by the performance on
those prevalent classes such as ATTRIBUTION and
ELABORATION. For reasons of space, we do not
show the class-wise F-scores, but in our results,
we find that using our features consistently provides
superior performance for most class relations over
HILDA?s features, and therefore results in higher
overall MAFS under all conditions. We also list two
other metrics for performance on the test data ?
Weight-averaged F-score (WAFS), which weights
the performance of each relation class by the num-
ber of its existing instances, and the testing accuracy
(Acc) ? but these metrics are relatively more bi-
3No significance test is reported for relation classification,
because we are comparing MAFS, which equally weights the
performance of each relation. Therefore, traditional signifi-
cance tests which operate on individual instances rather than
individual relation classes are not applicable.
ased evaluation metrics in this task. Similar to Struc-
ture classification, the accuracy on the training data
(TAcc)4 is listed in the second section of each sub-
table. It demonstrates that our carefully selected rich
linguistic features are able to better fit the classifi-
cation problem, especially under the cross-sentence
condition.
Similar to our observation in Structure classifica-
tion, the performance of Relation classification for
cross-sentence instances is also much poorer than
that on within-sentence instances, which again re-
veals the difficulty of text-level discourse parsing.
7 Conclusions
In this paper, we aimed to develop an RST-style
text-level discourse parser. We chose the HILDA
discourse parser (Hernault et al, 2010b) as the ba-
sis of our work, and significantly improved its tree-
building step by incorporating our own rich linguis-
tic features, together with features suggested by Lin
et al (2009). We analyzed the difficulty of extending
traditional sentence-level discourse parsing to text-
level parsing by showing that using exactly the same
set of features, the performance of Structure and Re-
lation classification on cross-sentence instances is
consistently inferior to that on within-sentence in-
stances. We also explored the effect of contextual
features on the overall performance. We showed
that contextual features are highly effective for both
Structure and Relation classification under all dis-
course conditions. Although perfect contextual fea-
tures are available only in idealized situations, when
they are correct, together with other features, they
can almost correctly predict the tree structure and
better predict the relation labels. Therefore, an it-
erative updating approach, which progressively up-
dates the tree structure and the labeling based on the
current estimation, may push the final results toward
this idealized end.
Our future work will be to fully implement an
end-to-end discourse parser using our rich linguis-
tic features, and focus on improving performance on
cross-sentence instances.
4We use accuracy instead of MAFS as the evaluation metric
on the training data because it is the metric that the training
procedure is optimized toward.
67
Acknowledgments
This work was financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada and by the University of Toronto.
References
Jason Baldridge and Alex Lascarides. 2005. Probabilis-
tic head-driven parsing for discourse structure. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning, pages 96?103.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python ? Analyzing
Text with the Natural Language Toolkit. O?Reilly.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2001. Building a discourse-tagged corpus in the
framework of Rhetorical Structure Theory. In Pro-
ceedings of Second SIGdial Workshop on Discourse
and Dialogue, pages 1?10.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:1?27.
David A. duVerle and Helmut Prendinger. 2009. A
novel discourse parser based on Support Vector Ma-
chine classification. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, Volume 2, ACL ?09,
pages 665?673, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010a. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 399?409, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010b. HILDA: A discourse
parser using support vector machine classification. Di-
alogue and Discourse, 1(3):1?33.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In International
Conference on Machine Learning (ICML), pages 377?
384.
Alistair Knott and Robert Dale. 1994. Using linguistic
phenomena to motivate a set of coherence relations.
Discourse Processes, 18(1).
Huong LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Generating discourse structures for
written texts. In Proceedings of the 20th International
Conference on Computational Linguistics, pages 329?
335.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, Volume 1, EMNLP ?09, pages 343?351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
report, School of Computing, National University of
Singapore.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
368?375, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
Daniel Marcu. 1997. The rhetorical parsing of natu-
ral language texts. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics, pages 96?103.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
David Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature sup-
port vector models. LDV Forum, 18(1/2):38?52.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies, pages 81?84.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical informa-
tion. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, Volume 1, pages 149?156.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic informa-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 566?574.
Bonnie Webber. 2004. D-LTAG: Extending lexicalized
TAG to discourse. Cognitive Science, 28(5):751?779.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 511?521,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Linear-Time Bottom-Up Discourse Parser
with Constraints and Post-Editing
Vanessa Wei Feng
Department of Computer Science
University of Toronto
Toronto, ON, Canada
weifeng@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
Toronto, ON, Canada
gh@cs.toronto.edu
Abstract
Text-level discourse parsing remains a
challenge. The current state-of-the-art
overall accuracy in relation assignment is
55.73%, achieved by Joty et al (2013).
However, their model has a high order of
time complexity, and thus cannot be ap-
plied in practice. In this work, we develop
a much faster model whose time complex-
ity is linear in the number of sentences.
Our model adopts a greedy bottom-up ap-
proach, with two linear-chain CRFs ap-
plied in cascade as local classifiers. To en-
hance the accuracy of the pipeline, we add
additional constraints in the Viterbi decod-
ing of the first CRF. In addition to effi-
ciency, our parser also significantly out-
performs the state of the art. Moreover,
our novel approach of post-editing, which
modifies a fully-built tree by considering
information from constituents on upper
levels, can further improve the accuracy.
1 Introduction
Discourse parsing is the task of identifying the
presence and the type of the discourse relations
between discourse units. While research in dis-
course parsing can be partitioned into several di-
rections according to different theories and frame-
works, Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is probably the most am-
bitious one, because it aims to identify not only
the discourse relations in a small local context, but
also the hierarchical tree structure for the full text:
from the relations relating the smallest discourse
units (called elementary discourse units, EDUs),
to the ones connecting paragraphs.
For example, Figure 1 shows a text fragment
consisting of two sentences with four EDUs in
total (e
1
-e
4
). Its discourse tree representation is
shown below the text, following the notation con-
vention of RST: the two EDUs e
1
and e
2
are re-
lated by a mononuclear relation CONSEQUENCE,
where e
2
is the more salient span (called nucleus,
and e
1
is called satellite); e
3
and e
4
are related by
another mononuclear relation CIRCUMSTANCE,
with e
4
as the nucleus; the two spans e
1:2
and e
3:4
are further related by a multi-nuclear relation SE-
QUENCE, with both spans as the nucleus.
Conventionally, there are two major sub-tasks
related to text-level discourse parsing: (1) EDU
segmentation: to segment the raw text into EDUs,
and (2) tree-building: to build a discourse tree
from EDUs, representing the discourse relations in
the text. Since the first sub-task is considered rela-
tively easy, with the state-of-art accuracy at above
90% (Joty et al, 2012), the recent research focus
is on the second sub-task, and often uses manual
EDU segmentation.
The current state-of-the-art overall accuracy of
the tree-building sub-task, evaluated on the RST
Discourse Treebank (RST-DT, to be introduced in
Section 8), is 55.73% by Joty et al (2013). How-
ever, as an optimal discourse parser, Joty et al?s
model is highly inefficient in practice, with re-
spect to both their DCRF-based local classifiers,
and their CKY-like bottom-up parsing algorithm.
DCRF (Dynamic Conditional Random Fields) is
a generalization of linear-chain CRFs, in which
each time slice contains a set of state variables
and edges (Sutton et al, 2007). CKY parsing is
a bottom-up parsing algorithm which searches all
possible parsing paths by dynamic programming.
Therefore, despite its superior performance, their
model is infeasible in most realistic situations.
The main objective of this work is to develop
a more efficient discourse parser, with similar or
even better performance with respect to Joty et
al.?s optimal parser, but able to produce parsing re-
sults in real time.
Our contribution is three-fold. First, with a
511
[On Aug. 1, the state tore up its controls,]e
1
[and food prices leaped]e
2
[Without buffer
stocks,]e
3
[inflation exploded.]e
4
wsj 1146
e1 e2
consequence
e1:4
e3 e4
circumstance
sequence
e1:2 e3:4
Figure 1: An example text fragment composed of
two sentences and four EDUs, with its RST dis-
course tree representation shown below.
greedy bottom-up strategy, we develop a discourse
parser with a time complexity linear in the total
number of sentences in the document. As a re-
sult of successfully avoiding the expensive non-
greedy parsing algorithms, our discourse parser is
very efficient in practice. Second, by using two
linear-chain CRFs to label a sequence of discourse
constituents, we can incorporate contextual infor-
mation in a more natural way, compared to us-
ing traditional discriminative classifiers, such as
SVMs. Specifically, in the Viterbi decoding of
the first CRF, we include additional constraints
elicited from common sense, to make more ef-
fective local decisions. Third, after a discourse
(sub)tree is fully built from bottom up, we perform
a novel post-editing process by considering infor-
mation from the constituents on upper levels. We
show that this post-editing can further improve the
overall parsing performance.
2 Related work
2.1 HILDA discourse parser
The HILDA discourse parser by Hernault et al
(2010) is the first attempt at RST-style text-level
discourse parsing. It adopts a pipeline framework,
and greedily builds the discourse tree from the bot-
tom up. In particular, starting from EDUs, at each
step of the tree-building, a binary SVM classifier
is first applied to determine which pair of adjacent
discourse constituents should be merged to form a
larger span, and another multi-class SVM classi-
fier is then applied to assign the type of discourse
relation that holds between the chosen pair.
The strength of HILDA?s greedy tree-building
strategy is its efficiency in practice. Also, the em-
ployment of SVM classifiers allows the incorpora-
tion of rich features for better data representation
(Feng and Hirst, 2012). However, HILDA?s ap-
proach also has obvious weakness: the greedy al-
gorithm may lead to poor performance due to local
optima, and more importantly, the SVM classifiers
are not well-suited for solving structural problems
due to the difficulty of taking context into account.
2.2 Joty et al?s joint model
Joty et al (2013) approach the problem of text-
level discourse parsing using a model trained by
Conditional Random Fields (CRF). Their model
has two distinct features.
First, they decomposed the problem of text-
level discourse parsing into two stages: intra-
sentential parsing to produce a discourse tree for
each sentence, followed by multi-sentential pars-
ing to combine the sentence-level discourse trees
and produce the text-level discourse tree. Specif-
ically, they employed two separate models for
intra- and multi-sentential parsing. Their choice
of two-stage parsing is well motivated for two rea-
sons: (1) it has been shown that sentence bound-
aries correlate very well with discourse bound-
aries, and (2) the scalability issue of their CRF-
based models can be overcome by this decompo-
sition.
Second, they jointly modeled the structure and
the relation for a given pair of discourse units.
For example, Figure 2 shows their intra-sentential
model, in which they use the bottom layer to rep-
resent discourse units; the middle layer of binary
nodes to predict the connection of adjacent dis-
course units; and the top layer of multi-class nodes
to predict the type of the relation between two
units. Their model assigns a probability to each
possible constituent, and a CKY-like parsing al-
gorithm finds the globally optimal discourse tree,
given the computed probabilities.
The strength of Joty et al?s model is their joint
modeling of the structure and the relation, such
that information from each aspect can interact with
the other. However, their model has a major defect
in its inefficiency, or even infeasibility, for appli-
cation in practice. The inefficiency lies in both
their DCRF-based joint model, on which infer-
ence is usually slow, and their CKY-like parsing
algorithm, whose issue is more prominent. Due to
the O(n
3
) time complexity, where n is the number
512
R2
S2
U2U1
R3
S3
U3
Rj
Sj
Uj
Rt-1
St-1
Ut-1
Relation sequence
Structure sequence
Unit sequence at level i
Figure 2: Joty et al (2013)?s intra-sentential Con-
dition Random Fields.
of input discourse units, for large documents, the
parsing simply takes too long
1
.
3 Overall work flow
Figure 3 demonstrates the overall work flow of our
discourse parser. The general idea is that, similar
to Joty et al (2013), we perform a sentence-level
parsing for each sentence first, followed by a text-
level parsing to generate a full discourse tree for
the whole document. However, in addition to effi-
ciency (to be shown in Section 6), our discourse
parser has a distinct feature, which is the post-
editing component (to be introduced in Section 5),
as outlined in dashes.
Our discourse parser works as follows. A doc-
ument D is first segmented into a list of sen-
tences. Each sentence S
i
, after being segmented
into EDUs (not shown in the figure), goes through
an intra-sentential bottom-up tree-building model
M
intra
, to form a sentence-level discourse tree T
S
i
,
with the EDUs as leaf nodes. After that, we ap-
ply the intra-sentential post-editing model P
intra
to
modify the generated tree T
S
i
to T
p
S
i
, by considering
upper-level information.
We then combine all sentence-level discourse
tree T
p
S
i
?s using our multi-sentential bottom-up
tree-building model M
multi
to generate the text-
level discourse tree T
D
. Similar to sentence-level
parsing, we also post-edit T
D
using P
multi
to pro-
duce the final discourse tree T
p
D
.
1
The largest document in the RST-DT contains over 180
sentences, i.e., n > 180 for their multi-sentential CKY pars-
ing. Intuitively, suppose the average time to compute the
probability of each constituent is 0.01 second, then in total,
the CKY-like parsing takes over 16 hours. It is possible to op-
timize Joty et al?s CKY-like parsing by replacing their CRF-
based computation for upper-level constituents with some lo-
cal computation based on the probabilities of lower-level con-
stituents. However, such optimization is beyond the scope of
this paper.
4 Bottom-up tree-building
For both intra- and multi-sentential parsing, our
bottom-up tree-building process adopts a similar
greedy pipeline framework like the HILDA dis-
course parser (discussed in Section 2.1), to guar-
antee efficiency for large documents. In partic-
ular, starting from the constituents on the bot-
tom level (EDUs for intra-sentential parsing and
sentence-level discourse trees for multi-sentential
parsing), at each step of the tree-building, we
greedily merge a pair of adjacent discourse con-
stituents such that the merged constituent has the
highest probability as predicted by our structure
model. The relation model is then applied to as-
sign the relation to the new constituent.
4.1 Linear-chain CRFs as Local models
Now we describe the local models we use to make
decisions for a given pair of adjacent discourse
constituents in the bottom-up tree-building. There
are two dimensions for our local models: (1) scope
of the model: intra- or multi-sentential, and (2)
purpose of the model: for determining structures
or relations. So we have four local models, M
struct
intra
,
M
rel
intra
, M
struct
multi
, and M
rel
multi
.
While our bottom-up tree-building shares the
greedy framework with HILDA, unlike HILDA,
our local models are implemented using CRFs.
In this way, we are able to take into account the
sequential information from contextual discourse
constituents, which cannot be naturally repre-
sented in HILDA with SVMs as local classifiers.
Therefore, our model incorporates the strengths
of both HILDA and Joty et al?s model, i.e., the
efficiency of a greedy parsing algorithm, and the
ability to incorporate sequential information with
CRFs.
As shown by Feng and Hirst (2012), for a pair
of discourse constituents of interest, the sequential
information from contextual constituents is cru-
cial for determining structures. Therefore, it is
well motivated to use Conditional Random Fields
(CRFs) (Lafferty et al, 2001), which is a discrimi-
native probabilistic graphical model, to make pre-
dictions for a sequence of constituents surround-
ing the pair of interest.
In this sense, our local models appear similar
to Joty et al?s non-greedy parsing models. How-
ever, the major distinction between our models
and theirs is that we do not jointly model the struc-
ture and the relation; rather, we use two linear-
513
DS1
Si
Sn
...
...
Mintra
Mintrai
Mintra
Pintra...
... ...
Pintra
Pintra
Pmulti
Mmulti
...
...
1ST iST nST pSnT pS iT
pST 1 DT pDT
Figure 3: The work flow of our proposed discourse parser. In the figure, M
intra
and M
multi
stand for the
intra- and multi-sentential bottom-up tree-building models, and P
intra
and P
multi
stand for the intra- and
multi-sentential post-editing models.
chain CRFs to model the structure and the relation
separately. Although joint modeling has shown to
be effective in various NLP and computer vision
applications (Sutton et al, 2007; Yang et al, 2009;
Wojek and Schiele, 2008), our choice of using two
separate models is for the following reasons:
First, it is not entirely appropriate to model the
structure and the relation at the same time. For
example, with respect to Figure 2, it is unclear
how the relation node R
j
is represented for a train-
ing instance whose structure node S
j
= 0, i.e., the
units U
j?1
and U
j
are disjoint. Assume a special
relation NO-REL is assigned for R
j
. Then, in the
tree-building process, we will have to deal with the
situations where the joint model yields conflicting
predictions: it is possible that the model predicts
S
j
= 1 and R
j
= NO-REL, or vice versa, and we
will have to decide which node to trust (and thus
in some sense, the structure and the relation is no
longer jointly modeled).
Secondly, as a joint model, it is mandatory to
use a dynamic CRF, for which exact inference is
usually intractable or slow. In contrast, for linear-
chain CRFs, efficient algorithms and implementa-
tions for exact inference exist.
4.2 Structure models
4.2.1 Intra-sentential structure model
Figure 4a shows our intra-sentential structure
model M
struct
intra
in the form of a linear-chain CRF.
Similar to Joty et al?s intra-sentential model, the
first layer of the chain is composed of discourse
constituents U
j
?s, and the second layer is com-
posed of binary nodes S
j
?s to indicate the proba-
bility of merging adjacent discourse constituents.
S2
U2U1
S3
U3
Sj
Uj
St
Ut
Structure 
sequence
A ll units in 
sentence 
at level i
(a) Intra-sentential structure model M
struct
intra
.
Sj-1
Uj-1Uj-2
Sj
Uj
Structure 
sequence
A d jacent 
units  at 
level i
Uj+ 1
Sj+ 1Sj-1
Uj-3
Sj+ 2
Uj+ 2
C1 C
2
C3
(b) Multi-sentential structure model M
struct
multi
. C
1
, C
2
, and C
3
denote the three chains for predicting U
j
and U
j+1
.
Figure 4: Local structure models.
At each step in the bottom-up tree-building pro-
cess, we generate a single sequence E, consisting
of U
1
,U
2
, . . . ,U
j
, . . . ,U
t
, which are all the current
discourse constituents in the sentence that need
to be processed. For instance, initially, we have
the sequence E
1
= {e
1
,e
2
, . . . ,e
m
}, which are the
EDUs of the sentence; after merging e
1
and e
2
on
the second level, we have E
2
= {e
1:2
,e
3
, . . . ,e
m
};
after merging e
4
and e
5
on the third level, we have
E
3
= {e
1:2
,e
3
,e
4:5
, . . . ,e
m
}, and so on.
Because the structure model is the first com-
ponent in our pipeline of local models, its accu-
racy is crucial. Therefore, to improve its accuracy,
we enforce additional commonsense constraints in
its Viterbi decoding. In particular, we disallow 1-
1 transitions between adjacent labels (a discourse
unit can be merged with at most one adjacent unit),
and we disallow all-zero sequences (at least one
514
pair must be merged).
Since the computation of E
i
does not depend
on a particular pair of constituents, we can use the
same sequence E
i
to compute structural probabili-
ties for all adjacent constituents. In contrast, Joty
et al?s computation of intra-sentential sequences
depends on the particular pair of constituents: the
sequence is composed of the pair in question, with
other EDUs in the sentence, even if those EDUs
have already been merged. Thus, different CRF
chains have to be formed for different pairs of con-
stituents. In addition to efficiency, our use of a
single CRF chain for all constituents can better
capture the sequential dependencies among con-
text, by taking into account the information from
partially built discourse constituents, rather than
bottom-level EDUs only.
4.2.2 Multi-sentential structure model
For multi-sentential parsing, where the smallest
discourse units are single sentences, as argued by
Joty et al (2013), it is not feasible to use a long
chain to represent all constituents, due to the fact
that it takes O(TM
2
) time to perform the forward-
backward exact inference on a chain with T units
and an output vocabulary size of M, thus the over-
all complexity for all possible sequences in their
model is O(M
2
n
3
)
2
.
Instead, we choose to take a sliding-window
approach to form CRF chains for a particular pair
of constituents, as shown in Figure 4b. For exam-
ple, suppose we wish to compute the structural
probability for the pairU
j?1
andU
j
, we form three
chains, each of which contains two contextual
constituents: C
1
= {U
j?3
,U
j?2
,U
j?1
,U
j
},
C
2
= {U
j?2
,U
j?1
,U
j
,U
j+1
}, and C
3
=
{U
j?1
,U
j
,U
j+1
,U
j+2
}. We then find the chain
C
t
,1 ? t ? 3, with the highest joint probability
over the entire sequence, and assign its marginal
probability P(S
t
j
= 1) to P(S
j
= 1).
Similar to M
struct
intra
, for M
struct
multi
, we also include
additional constraints in the Viterbi decoding, by
disallowing transitions between two ones, and dis-
allowing the sequence to be all zeros if it contains
all the remaining constituents in the document.
4.3 Relation models
4.3.1 Intra-sentential relation model
The intra-sentential relation model M
rel
intra
, shown
in Figure 5a, works in a similar way to M
struct
intra
, as
2
The time complexity will be reduced to O(M
2
n
2
), if we
use the same chain for all constituents as in our M
struct
intra
.
described in Section 4.2.1. The linear-chain CRF
contains a first layer of all discourse constituents
U
j
?s in the sentence on level i, and a second layer
of relation nodes R
j
?s to represent the relation be-
tween a pair of discourse constituents.
However, unlike the structure model, adjacent
relation nodes do not share discourse constituents
on the first layer. Rather, each relation node R
j
attempts to model the relation of one single con-
stituent U
j
, by taking U
j
?s left and right subtrees
U
j,L
and U
j,R
as its first-layer nodes; if U
j
is a sin-
gle EDU, then the first-layer node of R
j
is simply
U
j
, and R
j
is a special relation symbol LEAF
3
.
Since we know, a priori, that the constituents in the
chains are either leaf nodes or the ones that have
been merged by our structure model, we never
need to worry about the NO-REL issue as out-
lined in Section 4.1.
In the bottom-up tree-building process, after
merging a pair of adjacent constituents using
M
struct
intra
into a new constituent, say U
j
, we form a
chain consisting of all current constituents in the
sentence to decide the relation label for U
j
, i.e.,
the R
j
node in the chain. In fact, by perform-
ing inference on this chain, we produce predic-
tions not only for R
j
, but also for all other R nodes
in the chain, which correspond to all other con-
stituents in the sentence. Since those non-leaf con-
stituents are already labeled in previous steps in
the tree-building, we can now re-assign their rela-
tions if the model predicts differently in this step.
Therefore, this re-labeling procedure can compen-
sate for the loss of accuracy caused by our greedy
bottom-up strategy to some extent.
4.3.2 Multi-sentential relation model
Figure 5b shows our multi-sentential relation
model. Like M
rel
intra
, the first layer consists of adja-
cent discourse units, and the relation nodes on the
second layer model the relation of each constituent
separately.
Similar to M
struct
multi
introduced in Section 4.2.2,
M
rel
multi
also takes a sliding-window approach to
predict labels for constituents in a local context.
For a constituent U
j
to be predicted, we form three
chains, and use the chain with the highest joint
probability to assign or re-assign relations to con-
stituents in that chain.
3
These leaf constituents are represented using a special
feature vector is leaf = True; thus the CRF never labels
them with relations other than LEAF.
515
Relation sequence
A ll units in sentence at level i
R1
U1, RU1, L
R2
U2
Rj
Uj, RUj, L
Rt
Ut, RUt, L
(a) Intra-sentential relation model M
rel
intra
.
Relation sequence
A d jacent units  at level i
R1
Uj-2, RUj-2, L
Rj-1
Uj-1
Rj
Uj, RUj, L
Rj+ 1
Uj+ 1, RUj+ 1, L
Rj+ 2
Uj+ 2
C1 C
2
C3
(b) Multi-sentential relation model M
rel
multi
. C
1
, C
2
, and C
3
denote the three sliding windows for predictingU
j,L
andU
j,R
.
Figure 5: Local relation models.
5 Post-editing
After an intra- or multi-sentential discourse tree
is fully built, we perform a post-editing to con-
sider possible modifications to the current tree, by
considering useful information from the discourse
constituents on upper levels, which is unavailable
in the bottom-up tree-building process.
The motivation for post-editing is that, some
particular discourse relations, such as TEXTUAL-
ORGANIZATION, tend to occur on the top levels
of the discourse tree; thus, information such as the
depth of the discourse constituent can be quite in-
dicative. However, the exact depth of a discourse
constituent is usually unknown in the bottom-up
tree-building process; therefore, it might be ben-
eficial to modify the tree by including top-down
information after the tree is fully built.
The process of post-editing is shown in Algo-
rithm 1. For each input discourse tree T , which
is already fully built by bottom-up tree-building
models, we do the following:
Lines 3 ? 9: Identify the lowest level of T on
which the constituents can be modified according
to the post-editing structure component, P
struct
. To
do so, we maintain a list L to store the discourse
constituents that need to be examined. Initially, L
consists of all the bottom-level constituents in T .
At each step of the loop, we consider merging the
pair of adjacent units in L with the highest proba-
bility predicted by P
struct
. If the predicted pair is
not merged in the original tree T , then a possible
modification is located; otherwise, we merge the
pair, and proceed to the next iteration.
Lines 10 ? 12: If modifications have been pro-
posed in the previous step, we build a new tree
Algorithm 1 Post-editing algorithm.
Input: A fully built discourse tree T .
1: if |T |= 1 then
2: return T . Do nothing if it is a single
EDU.
3: L? [U
1
,U
2
, . . . ,U
t
] . The bottom-level
constituents in T .
4: while |L|> 2 do
5: i? PREDICTMERGING(L,P
struct
)
6: p? PARENT(L[i],L[i+1],T )
7: if p = NULL then
8: break
9: Replace L[i] and L[i+1] with p
10: if |L|= 2 then
11: L? [U
1
,U
2
, . . . ,U
t
]
12: T
p
? BUILDTREE(L,P
struct
,P
rel
,T )
Output: T
p
T
p
using P
struct
as the structure model, and P
rel
as the relation model, from the constituents on
which modifications are proposed. Otherwise, T
p
is built from the bottom-level constituents of T .
The upper-level information, such as the depth of
a discourse constituent, is derived from the initial
tree T .
5.1 Local models
The local models, P
{struct|rel}
{intra|multi}
, for post-editing
is almost identical to their counterparts of the
bottom-up tree-building, except that the linear-
chain CRFs in post-editing includes additional
features to represent information from constituents
on higher levels (to be introduced in Section 7).
6 Linear time complexity
Here we analyze the time complexity of each com-
ponent in our discourse parser, to quantitatively
demonstrate the time efficiency of our model. The
following analysis is focused on the bottom-up
tree-building process, but a similar analysis can be
carried out for the post-editing process. Since the
number of operations in the post-editing process is
roughly the same (1.5 times in the worst case) as
in the bottom-up tree-building, post-editing shares
the same complexity as the tree-building.
6.1 Intra-sentential parsing
Suppose the input document is segmented into
n sentences, and each sentence S
k
contains m
k
EDUs. For each sentence S
k
with m
k
EDUs, the
516
overall time complexity to perform intra-sentential
parsing is O(m
2
k
). The reason is the following. On
level i of the bottom-up tree-building, we generate
a single chain to represent the structure or relation
for all the m
k
? i constituents that are currently in
the sentence. The time complexity for performing
forward-backward inference on the single chain is
O((m
k
? i)?M
2
) =O(m
k
? i), where the constant
M is the size of the output vocabulary. Starting
from the EDUs on the bottom level, we need to
perform inference for one chain on each level dur-
ing the bottom-up tree-building, and thus the total
time complexity is ?
m
k
i=1
O(m
k
? i) = O(m
2
k
).
The total time to generate sentence-level dis-
course trees for n sentences is ?
n
k=1
O(m
2
k
). It is
fairly safe to assume that each m
k
is a constant,
in the sense that m
k
is independent of the total
number of sentences in the document. There-
fore, the total time complexity ?
n
k=1
O(m
2
k
) ? n?
O(max
1? j?n
(m
2
j
)) = n?O(1) = O(n), i.e., linear
in the total number of sentences.
6.2 Multi-sentential parsing
For multi-sentential models, M
struct
multi
and M
rel
multi
, as
shown in Figures 4b and 5b, for a pair of con-
stituents of interest, we generate multiple chains
to predict the structure or the relation.
By including a constant number k of discourse
units in each chain, and considering a constant
number l of such chains for computing each ad-
jacent pair of discourse constituents (k = 4 for
M
struct
multi
and k = 3 for M
rel
multi
; l = 3), we have an
overall time complexity of O(n). The reason is
that it takes l?O(kM
2
) =O(1) time, where l,k,M
are all constants, to perform exact inference for a
given pair of adjacent constituents, and we need
to perform such computation for all n?1 pairs of
adjacent sentences on the first level of the tree-
building. Adopting a greedy approach, on an ar-
bitrary level during the tree-building, once we de-
cide to merge a certain pair of constituents, say
U
j
and U
j+1
, we only need to recompute a small
number of chains, i.e., the chains which originally
include U
j
or U
j+1
, and inference on each chain
takes O(1). Therefore, the total time complexity
is (n?1)?O(1)+(n?1)?O(1) = O(n), where
the first term in the summation is the complexity
of computing all chains on the bottom level, and
the second term is the complexity of computing
the constant number of chains on higher levels.
We have thus showed that the time complexity
is linear in n, which is the number of sentences in
the document. In fact, under the assumption that
the number of EDUs in each sentence is indepen-
dent of n, it can be shown that the time complexity
is also linear in the total number of EDUs
4
.
7 Features
In our local models, to encode two adjacent units,
U
j
and U
j+1
, within a CRF chain, we use the fol-
lowing 10 sets of features, some of which are mod-
ified from Joty et al?s model.
Organization features: WhetherU
j
(orU
j+1
) is
the first (or last) constituent in the sentence (for
intra-sentential models) or in the document (for
multi-sentential models); whether U
j
(or U
j+1
) is
a bottom-level constituent.
Textual structure features: Whether U
j
con-
tains more sentences (or paragraphs) than U
j+1
.
N-gram features: The beginning (or end) lexi-
cal n-grams in each unit; the beginning (or end)
POS n-grams in each unit, where n ? {1,2,3}.
Dominance features: The PoS tags of the head
node and the attachment node; the lexical heads of
the head node and the attachment node; the domi-
nance relationship between the two units.
Contextual features: The feature vector of the
previous and the next constituent in the chain.
Substructure features: The root node of the left
and right discourse subtrees of each unit.
Syntactic features: whether each unit corre-
sponds to a single syntactic subtree, and if so, the
top PoS tag of the subtree; the distance of each
unit to their lowest common ancestor in the syntax
tree (intra-sentential only).
Entity transition features: The type and the
number of entity transitions across the two units.
We adopt Barzilay and Lapata (2008)?s entity-
based local coherence model to represent a doc-
ument by an entity grid, and extract local transi-
tions among entities in continuous discourse con-
stituents. We use bigram and trigram transitions
with syntactic roles attached to each entity.
4
We implicitly made an assumption that the parsing time
is dominated by the time to perform inference on CRF chains.
However, for complex features, the time required for fea-
ture computation might be dominant. Nevertheless, a care-
ful caching strategy can accelerate feature computation, since
a large number of multi-sentential chains overlap with each
other.
517
Cue phrase features: Whether a cue phrase oc-
curs in the first or last EDU of each unit. The cue
phrase list is based on the connectives collected by
Knott and Dale (1994)
Post-editing features: The depth of each unit in
the initial tree.
8 Experiments
For pre-processing, we use the Stanford CoreNLP
(Klein and Manning, 2003; de Marneffe et al,
2006; Recasens et al, 2013) to syntactically parse
the texts and extract coreference relations, and we
use Penn2Malt
5
to lexicalize syntactic trees to ex-
tract dominance features.
For local models, our structure models are
trained using MALLET (McCallum, 2002) to in-
clude constraints over transitions between adja-
cent labels, and our relation models are trained
using CRFSuite (Okazaki, 2007), which is a fast
implementation of linear-chain CRFs.
The data that we use to develop and evaluate
our discourse parser is the RST Discourse Tree-
bank (RST-DT) (Carlson et al, 2001), which is a
large corpus annotated in the framework of RST.
The RST-DT consists of 385 documents (347 for
training and 38 for testing) from the Wall Street
Journal. Following previous work on the RST-DT
(Hernault et al, 2010; Feng and Hirst, 2012; Joty
et al, 2012; Joty et al, 2013), we use 18 coarse-
grained relation classes, and with nuclearity at-
tached, we have a total set of 41 distinct relations.
Non-binary relations are converted into a cascade
of right-branching binary relations.
9 Results and Discussion
9.1 Parsing accuracy
We compare four different models using manual
EDU segmentation. In Table 1, the jCRF model
in the first row is the optimal CRF model proposed
by Joty et al (2013). gSVM
FH
in the second row
is our implementation of HILDA?s greedy parsing
algorithm using Feng and Hirst (2012)?s enhanced
feature set. The third model, gCRF, represents our
greedy CRF-based discourse parser, and the last
row, gCRF
PE
, represents our parser with the post-
editing component included.
In order to conduct a direct comparison with
Joty et al?s model, we use the same set of eval-
5
http://stp.lingfil.uu.se/
?
nivre/
research/Penn2Malt.html.
Model Span Nuc Relation
Acc MAFS
jCRF 82.5 68.4 55.7 N/A
gSVM
FH
82.8 67.1 52.0 27.4/23.3
gCRF 84.9
?
69.9
?
57.2
?
35.3/31.3
gCRF
PE
85.7
??
71.0
??
58.2
??
36.2/32.3
Human 88.7 77.7 65.8 N/A
?: significantly better than gSVM
FH
(p < .01)
?: significantly better than gCRF (p < .01)
Table 1: Performance of different models using
gold-standard EDU segmentation, evaluated us-
ing the constituent accuracy (%) for span, nucle-
arity, and relation. For relation, we also report the
macro-averaged F1-score (MAFS) for correctly
retrieved constituents (before the slash) and for
all constituents (after the slash). Statistical sig-
nificance is verified using Wilcoxon?s signed-rank
test.
uation metrics, i.e., the unlabeled and labeled pre-
cision, recall, and F-score
6
as defined by Marcu
(2000). For evaluating relations, since there is a
skewed distribution of different relation types in
the corpus, we also include the macro-averaged
F1-score (MAFS)
7
as another metric, to empha-
size the performance of infrequent relation types.
We report the MAFS separately for the correctly
retrieved constituents (i.e., the span boundary is
correct) and all constituents in the reference tree.
As demonstrated by Table 1, our greedy CRF
models perform significantly better than the other
two models. Since we do not have the actual out-
put of Joty et al?s model, we are unable to con-
duct significance testing between our models and
theirs. But in terms of overall accuracy, our gCRF
model outperforms their model by 1.5%. More-
over, with post-editing enabled, gCRF
PE
signif-
icantly (p < .01) outperforms our initial model
gCRF by another 1% in relation assignment, and
this overall accuracy of 58.2% is close to 90% of
human performance. With respect to the macro-
averaged F1-scores, adding the post-editing com-
ponent also obtains about 1% improvement.
However, the overall MAFS is still at the lower
6
For manual segmentation, precision, recall, and F-score
are the same.
7
MAFS is the F1-score averaged among all relation
classes by equally weighting each class. Therefore, we can-
not conduct significance test between different MAFS.
518
Avg Min Max
# of EDUs 61.74 4 304
# of Sentences 26.11 2 187
# of EDUs per sentence 2.36 1 10
Table 2: Characteristics of the 38 documents in the
test data.
end of 30% for all constituents. Our error anal-
ysis shows that, for two relation classes, TOPIC-
CHANGE and TEXTUAL-ORGANIZATION, our
model fails to retrieve any instance, and for
TOPIC-COMMENT and EVALUATION, our model
scores a class-wise F
1
score lower than 5%. These
four relation classes, apart from their infrequency
in the corpus, are more abstractly defined, and thus
are particularly challenging.
9.2 Parsing efficiency
We further illustrate the efficiency of our parser by
demonstrating the time consumption of different
models.
First, as shown in Table 2, the average number
of sentences in a document is 26.11, which is al-
ready too large for optimal parsing models, e.g.,
the CKY-like parsing algorithm in jCRF, let alne
the fact that the largest document contains sev-
eral hundred of EDUs and sentences. Therefore,
it should be seen that non-optimal models are re-
quired in most cases.
In Table 3, we report the parsing time
8
for the
last three models, since we do not know the time of
jCRF. Note that the parsing time excludes the time
cost for any necessary pre-processing. As can be
seen, our gCRF model is considerably faster than
gSVM
FH
, because, on one hand, feature compu-
tation is expensive in gSVM
FH
, since gSVM
FH
utilizes a rich set of features; on the other hand,
in gCRF, we are able to accelerate decoding by
multi-threading MALLET (we use four threads).
Even for the largest document with 187 sentences,
gCRF is able to produce the final tree after about
40 seconds, while jCRF would take over 16 hours
assuming each DCRF decoding takes only 0.01
second. Although enabling post-editing doubles
the time consumption, the overall time is still ac-
ceptable in practice, and the loss of efficiency can
be compensated by the improvement in accuracy.
8
Tested on a Linux system with four duo-core 3.0GHz
processors and 16G memory.
Model Parsing Time (seconds)
Avg Min Max
gSVM
FH
11.19 0.42 124.86
gCRF 5.52 0.05 40.57
gCRF
PE
10.71 0.12 84.72
Table 3: The parsing time (in seconds) for the 38
documents in the test set of RST-DT. Time cost of
any pre-processing is excluded from the analysis.
10 Conclusions
In this paper, we presented an efficient text-level
discourse parser with time complexity linear in
the total number of sentences in the document.
Our approach was to adopt a greedy bottom-
up tree-building, with two linear-chain CRFs as
local probabilistic models, and enforce reason-
able constraints in the first CRF?s Viterbi decod-
ing. While significantly outperforming the state-
of-the-art model by Joty et al (2013), our parser
is much faster in practice. In addition, we pro-
pose a novel idea of post-editing, which modifies a
fully-built discourse tree by considering informa-
tion from upper-level constituents. We show that,
although doubling the time consumption, post-
editing can further boost the parsing performance
to close to 90% of human performance.
In future work, we wish to further explore the
idea of post-editing, since currently we use only
the depth of the subtrees as upper-level informa-
tion. Moreover, we wish to study whether we can
incorporate constraints into the relation models, as
we do to the structure models. For example, it
might be helpful to train the relation models us-
ing additional criteria, such as Generalized Ex-
pectation (Mann and McCallum, 2008), to better
take into account some prior knowledge about the
relations. Last but not least, as reflected by the
low MAFS in our experiments, some particularly
difficult relation types might need specifically de-
signed features for better recognition.
Acknowledgments
We thank Professor Gerald Penn and the review-
ers for their valuable advice and comments. This
work was financially supported by the Natural
Sciences and Engineering Research Council of
Canada and by the University of Toronto.
519
References
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning (CoNLL-2005),
pages 96?103, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure The-
ory. In Proceedings of Second SIGDial Workshop
on Discourse and Dialogue (SIGDial 2001), pages
1?10.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL 2012), pages 60?68, Jeju,
Korea.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification.
Dialogue and Discourse, 1(3):1?33.
Shafiq Joty, Giuseppe Carenini, and Raymond T.
Ng. 2012. A novel discriminative framework
for sentence-level discourse analysis. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL 2012, pages 904?915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 486?496, Sofia, Bul-
garia, August.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), ACL 2003, pages
423?430, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alistair Knott and Robert Dale. 1994. Using linguistic
phenomena to motivate a set of coherence relations.
Discourse Processes, 18(1):35?64.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
2001, pages 282?289, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Gideon S. Mann and Andrew McCallum. 2008. Gen-
eralized Expectation Criteria for semi-supervised
learning of Conditional Random Fields. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies (ACL 2008), pages 870?878, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
William Mann and Sandra Thompson. 1988. Rhetor-
ical structure theory: Toward a functional theory of
text organization. Text, 8(3):243?281.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Philippe Muller, Stergos Afantenos, Pascal Denis, and
Nicholas Asher. 2012. Constrained decoding for
text-level discourse parsing. In Proceedings of
COLING 2012, pages 1883?1900, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 627?633, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 566?574, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. The Journal of Ma-
chine Learning Research, 8:693?723, May.
Christian Wojek and Bernt Schiele. 2008. A dynamic
conditional random field model for joint labeling of
object and scene classes. In European Conference
520
on Computer Vision (ECCV 2008), pages 733?747,
Marseille, France.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura, and Sadaoki Furui.
2009. Combining a two-step conditional random
field model and a joint source channel model for
machine transliteration. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 72?75, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
521
