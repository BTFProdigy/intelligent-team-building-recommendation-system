Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Joshua: An Open Source Toolkit for Parsing-based Machine Translation
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,+ Sanjeev Khudanpur,
Lane Schwartz,? Wren N. G. Thornton, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe Joshua, an open source
toolkit for statistical machine transla-
tion. Joshua implements all of the algo-
rithms required for synchronous context
free grammars (SCFGs): chart-parsing, n-
gram language model integration, beam-
and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array
grammar extraction and minimum error
rate training. It uses parallel and dis-
tributed computing techniques for scala-
bility. We demonstrate that the toolkit
achieves state of the art translation per-
formance on the WMT09 French-English
translation task.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high bar-
rier to entry for other researchers, and makes ex-
periments difficult to duplicate and compare. In
this paper, we describe Joshua, a general-purpose
open source toolkit for parsing-based machine
translation, serving the same role as Moses (Koehn
et al, 2007) does for regular phrase-based ma-
chine translation.
Our toolkit is written in Java and implements
all the essential algorithms described in Chiang
(2007): chart-parsing, n-gram language model in-
tegration, beam- and cube-pruning, and k-best ex-
traction. The toolkit also implements suffix-array
grammar extraction (Lopez, 2007) and minimum
error rate training (Och, 2003). Additionally, par-
allel and distributed computing techniques are ex-
ploited to make it scalable (Li and Khudanpur,
2008b). We have also made great effort to ensure
that our toolkit is easy to use and to extend.
The toolkit has been used to translate roughly
a million sentences in a parallel corpus for large-
scale discriminative training experiments (Li and
Khudanpur, 2008a). We hope the release of the
toolkit will greatly contribute the progress of the
syntax-based machine translation research.1
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: The Joshua code is organized
into separate packages for each major aspect of
functionality. In this way it is clear which files
contribute to a given functionality and researchers
can focus on a single package without worrying
about the rest of the system. Moreover, to mini-
mize the problems of unintended interactions and
unseen dependencies, which is common hinder-
ance to extensibility in large projects, all exten-
sible components are defined by Java interfaces.
Where there is a clear point of departure for re-
search, a basic implementation of each interface is
provided as an abstract class to minimize the work
necessary for new extensions.
End-to-end Cohesion: There are many compo-
nents to a machine translation pipeline. One of the
great difficulties with current MT pipelines is that
these diverse components are often designed by
separate groups and have different file format and
interaction requirements. This leads to a large in-
vestment in scripts to convert formats and connect
the different components, and often leads to unten-
able and non-portable projects as well as hinder-
1The toolkit can be downloaded at http://www.
sourceforge.net/projects/joshua, and the in-
structions in using the toolkit are at http://cs.jhu.
edu/?ccb/joshua.
135
ing repeatability of experiments. To combat these
issues, the Joshua toolkit integrates most critical
components of the machine translation pipeline.
Moreover, each component can be treated as a
stand-alone tool and does not rely on the rest of
the toolkit we provide.
Scalability: Our third design goal was to en-
sure that the decoder is scalable to large models
and data sets. The parsing and pruning algorithms
are carefully implemented with dynamic program-
ming strategies, and efficient data structures are
used to minimize overhead. Other techniques con-
tributing to scalability includes suffix-array gram-
mar extraction, parallel and distributed decoding,
and bloom filter language models.
Below we give a short description about the
main functions implemented in our Joshua toolkit.
2.1 Training Corpus Sub-sampling
Rather than inducing a grammar from the full par-
allel training data, we made use of a method pro-
posed by Kishore Papineni (personal communica-
tion) to select the subset of the training data con-
sisting of sentences useful for inducing a gram-
mar to translate a particular test set. This method
works as follows: for the development and test
sets that will be translated, every n-gram (up to
length 10) is gathered into a map W and asso-
ciated with an initial count of zero. Proceeding
in order through the training data, for each sen-
tence pair whose source-to-target length ratio is
within one standard deviation of the average, if
any n-gram found in the source sentence is also
found in W with a count of less than k, the sen-
tence is selected. When a sentence is selected, the
count of every n-gram in W that is found in the
source sentence is incremented by the number of
its occurrences in the source sentence. For our
submission, we used k = 20, which resulted in
1.5 million (out of 23 million) sentence pairs be-
ing selected for use as training data. There were
30,037,600 English words and 30,083,927 French
words in the subsampled training corpus.
2.2 Suffix-array Grammar Extraction
Hierarchical phrase-based translation requires a
translation grammar extracted from a parallel cor-
pus, where grammar rules include associated fea-
ture values. In real translation tasks, the grammars
extracted from large training corpora are often far
too large to fit into available memory.
In such tasks, feature calculation is also very ex-
pensive in terms of time required; huge sets of
extracted rules must be sorted in two directions
for relative frequency calculation of such features
as the translation probability p(f |e) and reverse
translation probability p(e|f) (Koehn et al, 2003).
Since the extraction steps must be re-run if any
change is made to the input training data, the time
required can be a major hindrance to researchers,
especially those investigating the effects of tok-
enization or word segmentation.
To alleviate these issues, we extract only a sub-
set of all available rules. Specifically, we follow
Callison-Burch et al (2005; Lopez (2007) and use
a source language suffix array to extract only those
rules which will actually be used in translating a
particular set of test sentences. This results in a
vastly smaller rule set than techniques which ex-
tract all rules from the training set.
The current code requires suffix array rule ex-
traction to be run as a pre-processing step to ex-
tract the rules needed to translate a particular test
set. However, we are currently extending the de-
coder to directly access the suffix array. This will
allow the decoder at runtime to efficiently extract
exactly those rules needed to translate a particu-
lar sentence, without the need for a rule extraction
pre-processing step.
2.3 Decoding Algorithms2
Grammar formalism: Our decoder assumes a
probabilistic synchronous context-free grammar
(SCFG). Currently, it only handles SCFGs of the
kind extracted by Heiro (Chiang, 2007), but is eas-
ily extensible to more general SCFGs (e.g., (Gal-
ley et al, 2006)) and closely related formalisms
like synchronous tree substitution grammars (Eis-
ner, 2003).
Chart parsing: Given a source sentence to de-
code, the decoder generates a one-best or k-best
translations using a CKY algorithm. Specifically,
the decoding algorithm maintains a chart, which
contains an array of cells. Each cell in turn main-
tains a list of proven items. The parsing process
starts with the axioms, and proceeds by applying
the inference rules repeatedly to prove new items
until proving a goal item. Whenever the parser
proves a new item, it adds the item to the appro-
priate chart cell. The item also maintains back-
2More details on the decoding algorithms are provided in
(Li et al, 2009a).
136
pointers to antecedent items, which are used for
k-best extraction.
Pruning: Severe pruning is needed in order to
make the decoding computationally feasible for
SCFGs with large target-language vocabularies.
In our decoder, we incorporate two pruning tech-
niques: beam and cube pruning (Chiang, 2007).
Hypergraphs and k-best extraction: For each
source-language sentence, the chart-parsing algo-
rithm produces a hypergraph, which represents
an exponential set of likely derivation hypotheses.
Using the k-best extraction algorithm (Huang and
Chiang, 2005), we extract the k most likely deriva-
tions from the hypergraph.
Parallel and distributed decoding: We also
implement parallel decoding and a distributed
language model by exploiting multi-core and
multi-processor architectures and distributed com-
puting techniques. More details on these two fea-
tures are provided by Li and Khudanpur (2008b).
2.4 Language Models
In addition to the distributed LM mentioned
above, we implement three local n-gram language
models. Specifically, we first provide a straightfor-
ward implementation of the n-gram scoring func-
tion in Java. This Java implementation is able to
read the standard ARPA backoff n-gram models,
and thus the decoder can be used independently
from the SRILM toolkit.3 We also provide a na-
tive code bridge that allows the decoder to use the
SRILM toolkit to read and score n-grams. This
native implementation is more scalable than the
basic Java LM implementation. We have also im-
plemented a Bloom Filter LM in Joshua, following
Talbot and Osborne (2007).
2.5 Minimum Error Rate Training
Johsua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu. The optimization
consists of a series of line-optimizations along
the dimensions corresponding to the parameters.
The search across a dimension uses the efficient
method of Och (2003). Each iteration of our
MERT implementation consists of multiple weight
3This feature allows users to easily try the Joshua toolkit
without installing the SRILM toolkit and compiling the native
bridge code. However, users should note that the basic Java
LM implementation is not as scalable as the native bridge
code.
updates, each reflecting a greedy selection of the
dimension giving the most gain. Each iteration
also optimizes several random ?intermediate ini-
tial? points in addition to the one surviving from
the previous iteration, as an approximation to per-
forming multiple random restarts. More details on
the MERT method and the implementation can be
found in Zaidan (2009).4
3 WMT-09 Translation Task Results
3.1 Training and Development Data
We assembled a very large French-English train-
ing corpus (Callison-Burch, 2009) by conducting
a web crawl that targted bilingual web sites from
the Canadian government, the European Union,
and various international organizations like the
Amnesty International and the Olympic Commit-
tee. The crawl gathered approximately 40 million
files, consisting of over 1TB of data. We converted
pdf, doc, html, asp, php, etc. files into text, and
preserved the directory structure of the web crawl.
We wrote set of simple heuristics to transform
French URLs onto English URLs, and considered
matching documents to be translations of each
other. This yielded 2 million French documents
paired with their English equivalents. We split the
sentences and paragraphs in these documents, per-
formed sentence-aligned them using software that
IBM Model 1 probabilities into account (Moore,
2002). We filtered and de-duplcated the result-
ing parallel corpus. After discarding 630 thousand
sentence pairs which had more than 100 words,
our final corpus had 21.9 million sentence pairs
with 587,867,024 English words and 714,137,609
French words.
We distributed the corpus to the other WMT09
participants to use in addition to the Europarl
v4 French-English parallel corpus (Koehn, 2005),
which consists of approximately 1.4 million sen-
tence pairs with 39 million English words and 44
million French words. Our translation model was
trained on these corpora using the subsampling de-
scried in Section 2.1.
For language model training, we used the
monolingual news and blog data that was as-
sembled by the University of Edinburgh and dis-
tributed as part of WMT09. This data consisted
4The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
137
of 21.2 million English sentences with half a bil-
lion words. We used SRILM to train a 5-gram
language model using a vocabulary containing the
500,000 most frequent words in this corpus. Note
that we did not use the English side of the parallel
corpus as language model training data.
To tune the system parameters we used News
Test Set from WMT08 (Callison-Burch et al,
2008), which consists of 2,051 sentence pairs
with 43 thousand English words and 46 thou-
sand French words. This is in-domain data that
was gathered from the same news sources as the
WMT09 test set.
3.2 Translation Scores
The translation scores for four different systems
are reported in Table 1.5
Baseline: In this system, we use the GIZA++
toolkit (Och and Ney, 2003), a suffix-array archi-
tecture (Lopez, 2007), the SRILM toolkit (Stol-
cke, 2002), and minimum error rate training (Och,
2003) to obtain word-alignments, a translation
model, language models, and the optimal weights
for combining these models, respectively.
Minimum Bayes Risk Rescoring: In this sys-
tem, we re-ranked the n-best output of our base-
line system using Minimum Bayes Risk (Kumar
and Byrne, 2004). We re-score the top 300 trans-
lations to minimize expected loss under the Bleu
metric.
Deterministic Annealing: In this system, in-
stead of using the regular MERT (Och, 2003)
whose training objective is to minimize the one-
best error, we use the deterministic annealing
training procedure described in Smith and Eisner
(2006), whose objective is to minimize the ex-
pected error (together with the entropy regulariza-
tion technique).
Variational Decoding: Statistical models in
machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then computationally in-
tractable. Therefore, most systems use a simple
Viterbi approximation that measures the goodness
5Note that the implementation of the novel techniques
used to produce the non-baseline results is not part of the cur-
rent Joshua release, though we plan to incorporate it in the
next release.
System BLEU-4
Joshua Baseline 25.92
Minimum Bayes Risk Rescoring 26.16
Deterministic Annealing 25.98
Variational Decoding 26.52
Table 1: The uncased BLEU scores on WMT-09
French-English Task. The test set consists of 2525
segments, each with one reference translation.
of a string using only its most probable deriva-
tion. Instead, we develop a variational approxima-
tion, which considers all the derivations but still
allows tractable decoding. More details will be
provided in Li et al (2009b). In this system, we
have used both deterministic annealing (for train-
ing) and variational decoding (for decoding).
4 Conclusions
We have described a scalable toolkit for parsing-
based machine translation. It is written in Java
and implements all the essential algorithms de-
scribed in Chiang (2007) and Li and Khudanpur
(2008b): chart-parsing, n-gram language model
integration, beam- and cube-pruning, and k-best
extraction. The toolkit also implements suffix-
array grammar extraction (Callison-Burch et al,
2005; Lopez, 2007) and minimum error rate train-
ing (Och, 2003). Additionally, parallel and dis-
tributed computing techniques are exploited to
make it scalable. The decoder achieves state of
the art translation performance.
Acknowledgments
This research was supported in part by the Defense
Advanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001 and
the National Science Foundation under grants
No. 0713448 and 0840112. The views and find-
ings are the authors? alone.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
138
Chris Callison-Burch. 2009. A 109 word parallel cor-
pus. In preparation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, , and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In Proceedings of AMTA.
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009a. Decoding in joshua:
Open source, parsing-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In preparation.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the ACL/Coling.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
139
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192?201,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
PARADIGM: Paraphrase Diagnostics through Grammar Matching
Jonathan Weese and Juri Ganitkevitch
Johns Hopkins University
Chris Callison-Burch
University of Pennsylvania
Abstract
Paraphrase evaluation is typically done ei-
ther manually or through indirect, task-
based evaluation. We introduce an in-
trinsic evaluation PARADIGM which mea-
sures the goodness of paraphrase col-
lections that are represented using syn-
chronous grammars. We formulate two
measures that evaluate these paraphrase
grammars using gold standard sentential
paraphrases drawn from a monolingual
parallel corpus. The first measure calcu-
lates how often a paraphrase grammar is
able to synchronously parse the sentence
pairs in the corpus. The second mea-
sure enumerates paraphrase rules from the
monolingual parallel corpus and calculates
the overlap between this reference para-
phrase collection and the paraphrase re-
source being evaluated. We demonstrate
the use of these evaluation metrics on para-
phrase collections derived from three dif-
ferent data types: multiple translations
of classic French novels, comparable sen-
tence pairs drawn from different newspa-
pers, and bilingual parallel corpora. We
show that PARADIGM correlates with hu-
man judgments more strongly than BLEU
on a task-based evaluation of paraphrase
quality.
1 Introduction
Paraphrases are useful in a wide range of natu-
ral language processing applications. A variety
of data-driven approaches have been proposed to
generate paraphrase resources (see Madnani and
Dorr (2010) for a survey of these methods). Few
objective metrics have been established to evalu-
ate these resources. Instead, paraphrases are typi-
cally evaluated using subjective manual evaluation
or through task-based evaluations.
Different researchers have used different crite-
ria for manual evaluations. For example, Barzilay
and McKeown (2001) evaluated their paraphrases
by asking judges whether paraphrases were ?ap-
proximately conceptually equivalent.? Ibrahim
et al. (2003) asked judges whether their para-
phrases were ?roughly interchangeable given the
genre.? Bannard and Callison-Burch (2005) re-
placed phrases with paraphrases in a number of
sentences and asked judges whether the substitu-
tions ?preserved meaning and remained grammat-
ical.? The results of these subjective evaluations
are not easily reusable.
Other researchers have evaluated their para-
phrases through task-based evaluations. Lin and
Pantel (2001) measured their potential impact on
question-answering. Cohn and Lapata (2007)
evaluate their applicability in the text-to-text gen-
eration task of sentence compression. Zhao et al.
(2009) use them to perform sentence compression
and simplification and to compute sentence simi-
larity. Several researchers have demonstrated that
paraphrases can improve machine translation eval-
uation (c.f. Kauchak and Barzilay (2006), Zhou
et al. (2006), Madnani (2010) and Snover et al.
(2010)).
We introduce an automatic evaluation met-
ric called PARADIGM, PARAphrase DIagnostics
through Grammar Matching. This metric eval-
uates paraphrase collections that are represented
using synchronous grammars. Synchronous tree-
adjoining grammars (STAGs), synchronous tree
substitution grammars (STSGs), and synchronous
context free grammars (SCFGs) are popular for-
malisms for representing paraphrase rules (Dras,
1997; Cohn and Lapata, 2007; Madnani, 2010;
Ganitkevitch et al., 2011). We present two mea-
sures that evaluate these paraphrase grammars us-
ing gold standard sentential paraphrases drawn
from a monolingual parallel corpus, which have
been previously proposed as a good resource
192
for paraphrase evaluation (Callison-Burch et al.,
2008; Cohn et al., 2008).
The first of our two proposed metrics calculates
how often a paraphrase grammar is able to syn-
chronously parse the sentence pairs in a test set.
The second measure enumerates paraphrase rules
from a monolingual parallel corpus and calculates
the overlap between this reference paraphrase col-
lection, and the paraphrase resource being evalu-
ated.
2 Related work and background
The most closely related work is ParaMetric
(Callison-Burch et al., 2008), which is a set of
objective measures for evaluating the quality of
phrase-based paraphrases. ParaMetric extracts a
set of gold-standard phrasal paraphrases from sen-
tential paraphrases that have been manually word-
aligned. The sentential paraphrases used in Para-
Metric were drawn from a data set originally cre-
ated to evaluate machine translation output using
the BLEU metric. Cohn et al. (2008) argue that
these sorts of monolingual parallel corpora are ap-
propriate for evaluating paraphrase systems, be-
cause they are naturally occurring sources of para-
phrases.
Callison-Burch et al. (2008) calculated three
types of metrics in ParaMetric. The manual word
alignments were used to calculate how well an
automatic paraphrasing technique is able to align
the paraphrases in a sentence pair. This measure
is limited to a class of paraphrasing techniques
that perform alignment (like MacCartney et al.
(2008)). Most methods produce a list of para-
phrases for a given input phrase. So Callison-
Burch et al. (2008) calculate two more gener-
ally applicable measures by comparing the para-
phrases in an automatically extracted resource to
gold standard paraphrases extracted via the align-
ments. These allow a lower-bound on precision
and relative recall to be calculated.
Liu et al. (2010) introduce the PEM metric as an
alternative to BLEU, since BLEU prefers iden-
tical paraphrases. PEM uses a second language
as a pivot to judge semantic equivalence. This re-
quires use of some bilingual data. Chen and Dolan
(2011) suggest using BLEU together with their
metric PINC, which uses n-grams to measure lex-
ical difference between paraphrases.
PARADIGM extends the ideas in ParaMetric
from lexical and phrasal paraphrasing techniques
to paraphrasing techniques that also generate syn-
tactic templates, such as Zhao et al. (2008), Cohn
and Lapata (2009), Madnani (2010) and Ganitke-
vitch et al. (2011). Instead of extracting gold stan-
dard paraphrases using techniques from phrase-
based machine translation, we use grammar ex-
traction techniques (Weese et al., 2011) to ex-
tract gold standard paraphrase grammar rules from
ParaMetric?s word-aligned sentential paraphrases.
Using these rules, we calculate the overlap be-
tween a gold standard paraphrase grammar and an
automatically generated paraphrase grammar.
Moreover, like ParaMetric, PARADIGM is able
to do further analysis on a restricted class of para-
phrasing models. In this case, PARADIGM evalu-
ates how well certain models are able to produce
synchronous parses of sentence pairs drawn from
monolingual parallel corpora. PARADIGM?s dif-
ferent metrics are explained in Section 4, but first
we give background on synchronous parsing and
synchronous grammars.
2.1 Synchronous parsing with SCFGs
Synchronous context-free grammars
An SCFG (Lewis and Stearns, 1968; Aho and
Ullman, 1972) is similar to a context-free gram-
mar, except that it generates pairs of strings
in correspondence. Each production rule in an
SCFG rewrites a non-terminal symbol as a pair of
phrases, which may have contain a mix of words
and non-terminals symbols. The grammar is syn-
chronous because both phrases in the pair must
have an identical set of non-terminals (though they
can come in different orders), and corresponding
non-terminals must be rewritten using the same
rule.
Much recent work in MT (and, by extension,
paraphrasing approaches that use MT machinery)
has been focused on choosing an appropriate set of
non-terminal symbols. The Hiero model (Chiang,
2007) used a single non-terminal symbolX . Other
approaches have read symbols from constituent
parses of the training data (Galley et al., 2004;
Galley et al., 2006; Zollmann and Venugopal,
2006). Labels based combinatory categorial gram-
mar (Steedman and Baldridge, 2011) have also
been used (Almaghout et al., 2010; Weese et al.,
2012).
Synchronous parsing
Wu (1997) introduced a parsing algorithm using
a variant of CKY. Dyer recently showed (2010)
193
an
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resign
to
him
want
others
while
,
him
impeach
to
propose
people
some
D
T
NP
V
B
P
V
B
P
R
P
C
C
N
N
S
V
B
P
P
R
P
V
B
P
R
T
VP
VP
S
VP
S
NP
VP
VP
NP
S
VP
S
S
.
Figure 1: PARADIGM extracts lexical, phrasal and
syntactic paraphrases from parsed, word-aligned
sentence pairs.
that the average parse time can be significantly im-
proved by using a two-pass algorithm.
The question of whether a source-reference pair
is reachable under a model must be addressed in
end-to-end discriminative training in MT (Liang
et al., 2006a; Gimpel and Smith, 2012). Auli et
al. (2009) showed that only approximately 30% of
training pairs are reachable under a phrase-based
model. This result is confirmed by our results in
paraphrasing.
3 Paraphrase grammar extraction
Like ParaMetric, PARADIGM extracts gold stan-
dard paraphrases from word-aligned sentential
paraphrases. PARADIGM goes further by parsing
one of the two input sentences, and uses the parse
tree to extract syntactic paraphrase rules, follow-
ing recent advances in syntactic approaches to ma-
chine translation (like Galley et al. (2004), Zoll-
mann and Venugopal (2006), and others). Figure 1
shows an example of a parsed sentence pair. From
that pair it is possible to extract a wide variety
of non-identical paraphrases, which include lexi-
cal paraphrases (single word synonyms), phrasal
paraphrases, and syntactic paraphrases that in-
clude a mix of words and syntactic non-terminal
CC? and while
VBP? want propose
VBP? expect want
DT? some some people
S? him to step down him to resign
VP? step down resign
VP? to step down to resign
VP? want to impeach him propose to impeach him
VP? want VP propose VP
VP? want to impeach PRP propose to impeach PRP
VP? VBP him to step down VBP him to resign
S? PRP to step down PRP to resign
Figure 2: Four examples each of lexical, phrasal,
and syntactic paraphrases that can be extracted
from the sentence pair in Figure 1.
symbols. Figure 2 shows a set of four examples
for each type that can be extracted from Figure 1.
These rules are formulated as SCFG rules,
with a syntactic left-hand nonterminal symbol
and two English right-hand sides representing the
paraphrase. The examples above include non-
terminal symbols that represent whole syntac-
tic constituents. It is also possible to create
more complex non-terminal symbols that describe
CCG-like non-constituent phrases. For example,
we could extract a rule like
S/VP? <NNS want him to, NNS expect him to>
Using constituents only, we are able to ex-
tract 45 paraphrase rules from Figure 1. Adding
CCG-style slashed constituents yields 66 addi-
tional rules.
4 PARADIGM: Evaluating paraphrase
grammars
By considering a paraphrase model as a syn-
chronous context-free grammar, we propose to
measure the model?s goodness using the following
criteria:
1. What percentage of sentential paraphrases
are reachable under the model? That is, given
a collection of sentence pairs (a
i
, b
i
) and an
SCFG G, where each pair of a and b are sen-
tential paraphrases, how many of the pairs are
in the language of G? We evaluate this by
producing a synchronous parse for the pairs,
as shown in Figure 3.
2. Given a collection of gold-standard para-
phrase rules, how many of those paraphrases
exist as rules in G? To calculate this, we
look at the overlap of grammars (described in
194
of the 
twelve cartoons
insulting
mohammad
CD
NNS
JJ
NP
NP
VP
NP
12 the islamic prophet
CD
NNS
JJ
NP
NP
VP
NP
cartoons offensivethat were to sparked riots
violent unrest was caused by
NP
NPVBD
VBD
VP
VP
S
S
Figure 3: We measure the goodness of paraphrase
grammars by determine how often they can be
used to synchronously parse gold-standard sen-
tential paraphrases. Note we do not require the
synchronous derivation to match a gold-standard
parse tree.
Section 4.2 below), examining different cate-
gories of rules and thresholding based on how
frequently the rule was used in the gold stan-
dard data.
These criteria correspond to properties that we
think are desirable in paraphrase models. They
also have the advantage that they do not depend
on human judgments and so can be calculated au-
tomatically.
4.1 Synchronous parse coverage
Paraphrase grammars should be able to explain
sentential paraphrases. For example, Figure
3 shows a sentence pair that is synchronously
parseable by one paraphrase grammar. In general,
we say that the more such sentence pairs that a
paraphrase grammar can synchronously parse, the
better it is.
The synchronous derivation allows us to draw
inferences about parts of the sentence pair that are
in correspondence; for instance, in Figure 3, vi-
olent unrest corresponds to riots and mohammad
corresponds to the islamic prophet.
4.2 Grammar overlap defined
We measure grammar overlap by comparing the
sets of production rules for two different gram-
mars. If the grammars contain rules that are equiv-
alent, the equivalent rules are in the grammars?
overlap.
We consider two types of overlapping, which
we will call strict and non-strict overlap. For strict
overlap, we say that two rules are equivalent if
they are identical, that is, if they have the same
left-hand side non-terminal symbol, their source
sides are identical strings, and their target sides are
identical strings. (This includes identical indexing
on non-terminal symbols on the right hand sides
of the rule.)
To calculate non-strict overlap, we ignore the
identities of non-terminal symbols in the left-hand
and right-hand sides of the rules. That is, two rules
are considered equivalent if they are identical after
all the non-terminal symbols have been replaced
by one equivalent symbol.
For example, in non-strict overlap, the syntactic
rule
NP ? ?N
1
?s N
2
; the N
2
of N
1
?
would match the Hiero rule
X ? ?X
1
?s X
2
; the X
2
of X
1
?
If we are considering two Hiero grammars,
strict and non-strict intersection are the same op-
eration since they only have on non-terminal X .
4.3 Precision lower bound and relative recall
Callison-Burch et al. (2008) use the notion of over-
lap between two paraphrase sets to define two met-
rics, precision lower bound and relative recall.
These are calculated the same way as standard
precision and recall. Relative recall is qualified
as ?relative? because it is calculated on a poten-
tially incomplete set of gold standard paraphrases.
There may exist valid paraphrases that do not oc-
cur in that set. Similarly, only a lower bound on
precision can be calculated because the candidate
set may contain valid paraphrases that do not oc-
cur in the gold standard set.
5 Experiments
5.1 Data
We extracted paraphrase grammars from a vari-
ety of different data sources, including four collec-
tions of sentential paraphrases. These included:
? Multiple translation corpora that were
compiled by the Linguistics Data Consortium
(LDC) for the purposes of evaluating ma-
chine translation quality with the BLEU met-
ric. We collected eight LDC corpora that all
have multiple English translations.
1
1
LDC Catalog numbers LDC2002T01, LDC2005T05,
LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14,
LDC2010T17, and LDC2010T23.
195
sentence total
Corpus pairs words
LDC Multiple Translations 83,284 2,254,707
Classic French Literature 75,106 682,978
MSR Paraphrase Corpus 5,801 219,492
ParaMetric 970 21,944
Table 1: Amount of English?English parallel data.
LDC data has 4 parallel translations per sentence.
Literature data is from Barzilay and McKeown
(2001). MSR data is from Quirk et al. (2004)
and Dolan et al. (2004). ParaMertic data is from
Callison-Burch et al. (2008).
? Classic French Literature that were trans-
lated by different translators, and which were
compiled by Barzilay and McKeown (2001).
? The MSR Paraphrase corpus which con-
sists of sentence pairs drawn from compara-
ble news articles drawn from different web
sites in the same date rate. The sentence pairs
were aligned heuristically aligned and then
manually judged to be paraphrases.
? The ParaMetric data which consists of 900
manually word-aligned sentence pairs col-
lected by Cohn et al. (2008). 300 sentence
pairs were drawn from each of the 3 above
sources. We use this to extract the gold stan-
dard paraphrase grammar.
The size of the data from each source is summa-
rized in Table 1.
For each dataset, after tokenizing and normaliz-
ing, we parsed one sentence in each English pair
using the Berkeley constituency parser (Liang et
al., 2006b). We then obtained word-level align-
ments, either using GIZA++ (Och and Ney, 2000)
or, in the case of ParaMetric, using human annota-
tions.
We used the Thrax grammar extractor (Weese
et al., 2011) to extract Hiero-style and syntactic
SCFGs from the paraphrase data. In the syntac-
tic setting we allowed labeling of rules with ei-
ther constituent labels or CCG-style slashed cat-
egories. The size of the extracted grammars is
shown in Table 2.
We also used version 0.2 of the SCFG-based
paraphrase collection known as the ParaPhrase
DataBase or PPDB (Ganitkevitch et al., 2013).
The PPDB paraphrases were extracted using the
pivoting technique (Bannard and Callison-Burch,
Grammar Rules
LDC Hiero 52,784,462
Lit. Hiero 3,288,546
MSR Hiero 2,456,513
ParaMetric Hiero 584,944
LDC Syntax 23,978,477
Lit. Syntax 715,154
MSR Syntax 406,115
ParaMetric Syntax 317,772
PPDB-v0.2-small 1,292,224
PPDB-v0.2-large 9,456,356
PPDB-v0.2-xl 46,592,161
Table 2: Size of various paraphrase grammars.
Grammar freq. ? 1 freq. ? 2
ParaMetric Syntax 317,772 21,709
LDC Hiero 5,840 (1.8%) 416 (1.9%)
Lit. Hiero 6,152 (1.9%) 359 (1.7%)
MSR Hiero 10,012 (3.2%) 315 (1.5%)
LDC Syntax 48,833 (15.3%) 7,748 (35.6%)
Lit. Syntax 14,431 (4.5%) 1,960 (9.0%)
MSR Syntax 21,197 (6.7%) 2,053 (9.5%)
PPDB-v0.2-small 15,831 (5.0%) 5,673 (26.1%)
PPDB-v0.2-large 31,277 (9.8%) 8,245 (37.9%)
PPDB-v0.2-xl 47,720 (15.0%) 10,049 (46.2%)
Table 3: Size of strict overlap (number of rules and
% of the gold standard) of each grammar with a
syntactic grammar derived from ParaMetric. freq.
? 2 means we first removed all rules that ap-
peared only once from the ParaMetric grammar.
The number in parentheses shows the percentage
of ParaMetric rules that are present in the overlap.
2005) on bilingual parallel corpora containing
over 42 million sentence pairs.
The PPDB release includes a tool for pruning
the grammar to a smaller size by retaining only
high-precision paraphrases. We include PPDB
grammars for several different pruning settings in
our analysis.
5.2 Experimental setup
We calculated our two metrics for each of the
grammars listed in Table 2.
To perform synchronous parsing, we used the
Joshua decoder (Post et al., 2013), which includes
an implementation of Dyer?s two-pass parsing al-
gorithm (2010). After splitting the LDC data into
10 equal pieces, we trained paraphrase models on
nine-tenths of the data and parsed the other tenth.
Grammars trained from other sources (the MSR
corpus, French literature domain, and PPDB) were
also evaluated on the held-out tenth of LDC data.
196
Grammar freq. ? 1 freq. ? 2
ParaMetric Syntax 200,385 20,699
LDC Hiero 41,346 (20.6%) 5,323 (25.8%)
Lit. Hiero 36,873 (18.4%) 4,606 (22.3%)
MSR Hiero 58,970 (29.4%) 6,741 (32.6%)
LDC Syntax 37,231 (11.7%) 5,055 (24.5%)
Lit. Syntax 19,530 (9.7%) 3,121 (15.1%)
MSR Syntax 28,016 (14.0%) 3,564 (17.2%)
PPDB-v0.2-small 13,003 (6.5%) 3,661 (17.7%)
PPDB-v0.2-large 22,431 (11.2%) 4,837 (23.4%)
PPDB-v0.2-xl 31,294 (15.6%) 5,590 (27.0%)
Table 4: Size of non-strict overlap of each gram-
mar with the syntactic grammar derived from
ParaMetric. The number in parentheses shows the
percentage of ParaMetric rules that are present in
the overlap.
Grammar syntactic phrasal lexical
ParaMetric 238,646 73,320 5,806
LDC
Syn
36,375 (15%) 8,806 (12%) 3,652 (62%)
MSR
Syn
7,734 (3%) 11,254 (15%) 2,209 (38%)
PPDB-xl 40,822 (17%) 3,765 (5%) 3,142 (54%)
Table 5: Number of paraphrases of each type
in each grammar?s strict overlap with the syntac-
tic ParaMetric grammar. Numbers in parentheses
show the percentage of ParaMetric rules of each
type.
Note that the LDC data contains 4 independent
translations of each foreign sentence, giving 6 pos-
sible (unordered) paraphrase pairs. We evaluated
coverage in two ways (corresponding to the two
columns in Table 6): first, considering all possible
sentence pairs from the test data, how many were
able to be parsed?
Secondly, if we consider all the English sen-
tences that correspond to one foreign sentence,
how many foreign sentences had at least one pair
of English translations that could be parsed syn-
chronously?
For grammar overlap, we perform both strict
and non-strict calculations (see Section 4.2)
against a syntactic grammar derived from hand-
aligned ParaMetric data.
5.3 Grammar overlap results
In Table 5 we see a breakdown of the types of para-
phrases in the overlap for three of the models. Al-
though the PPDB-xl overlap is much larger than
the other two, about 80% of its rules are syntac-
tic transformations. The LDC and MSR models
have a much larger proportion of phrasal and lexi-
cal rules.
Next we will look at the grammar overlap num-
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 10 100 1k 10k 100k  0 0.04 0.08 0.12 0.16Precision Lower Bound Relative RecallNumber of rules RecallPrec.
Figure 4: Precision lower bound and relative recall
when overlapping different sizes of PPDB with the
syntactic ParaMetric grammar.
bers presented in Table 3 and Table 4.
Note the non-intuitive result that for some
grammars (notably PPDB), the non-strict overlap
is smaller than the strict overlap. This is because
rules with different non-terminals only count once
in the non-strict overlap; for example, in PPDB-
small,
NN?? answer ; reply ?
VB?? answer ; reply ?
count as separate entries when calculating strictly,
but when ignoring non-terminals, they count as
only one type of rule.
The fact that the non-strict overlaps are smaller
means that there must be many rules in PPDB that
are identical except for non-terminal labels.
5.4 Precision and recall results
Figure 4 shows relative recall and precision lower
bound calculated for various sizes of PPDB rela-
tive to the ParaMetric grammar. The x-axis rep-
resents the size of the grammar as we vary from
keeping only the most probable rules to including
less probable ones. Restricting to high probability
rules makes the grammar much smaller, resulting
in higher precision.
5.5 Synchronous parsing results
Table 6 shows the percentage of sentence pairs that
were reachable in a held-out portion of the LDC
multiple-translation data.
We find that a grammar trained on LDC data
vastly outperforms data from any other domain.
This is not surprising ? we shouldn?t expect a
model trained on French literature to be able to
197
Grammar % (all) % (any)
LDC Hiero 9.5 33.0
Lit. Hiero 1.8 9.6
MSR Hiero 1.7 9.2
LDC Syntax 9.1 30.2
Lit. Syntax 2.0 10.7
MSR Syntax 1.9 10.4
PM Syntax 1.7 9.8
PPDB-v0.2-small 1.8 3.3
PPDB-v0.2-large 2.5 4.5
PPDB-v0.2-xl 3.5 6.2
Table 6: Parse coverage on held-out LDC data.
The all column considers every possible sentential
paraphrase in the test set. The any column consid-
ers a sentence parsed if any of its paraphrases was
able to parsed.
handle some of the vocabulary found in news sto-
ries that were originally in Arabic or Chinese.
The PPDB data outperforms both French litera-
ture and MSR models if we look all possible sen-
tence pairs from test data (the column labeled ?all?
in the table). However, when we consider whether
any pair from a set of 4 translations can be trans-
lated, the PPDB models do not do as well. This
implies that PPDB tends to be able to reach many
pairs from the same set of translations, but there
are many translations that it cannot handle at all.
By contrast, the literature- and MSR-trained mod-
els can reach at least one pair from 10% of the
test examples, even though the absolute number
of pairs they can reach is lower.
5.6 Effects of grammar size and choice of
syntactic labels
Table 2 shows that the PPDB-derived grammars
are much larger than the syntactic models derived
from other domains. It may seem surprising that
they should perform worse, but adding more rules
to the grammar just by varying non-terminal labels
isn?t likely to help overall parse coverage. This
suggests a new pruning method: keep only the top
k label variations for each rule type.
If we compare the syntactic models to the Hi-
ero models trained from the same data, we see
that their overall reachability performance is not
very different. This implies that paraphrases can
be annotated with linguistic information without
necessarily hurting their ability to explain partic-
ular sentence pairs. Contrast this result, with, for
example, those of Koehn et al. (2003), showing
that restricting translation models to only syntac-
tic phrases hurts overall translation performance.
The comparable performance between Hiero and
syntactic models seems to hold regardless of do-
main.
6 Correlation with human judgments
To validate PARADIGM, we calculated its correla-
tion with human judgments of paraphrase quality
on the sentence compression text-to-text genera-
tion task, which has been used to evaluate para-
phrase grammars in previous research (Cohn and
Lapata, 2007; Zhao et al., 2009; Ganitkevitch et
al., 2011; Napoles et al., 2011). We created sen-
tence compression systems for five of the para-
phrase grammars described in Section 5.1. We fol-
lowed the methodology outlined by Ganitkevitch
et al. (2011) and did the following:
? Each paraphrase grammar was augmented
with an appropriate set of rule-level features
that capture information pertinent to the task.
In this case, the paraphrase rules were given
two additional features that shows how the
number of words and characters changed af-
ter applying the rule.
? Similarly to how the weights of the mod-
els are set using minimum error rate training
in statistical machine translation, the weights
for each of the paraphrase grammars using
the PRO tuning method (Hopkins and May,
2011).
? Instead of optimizing to the BLEU metric, as
is done in machine translation, we optimized
to PR
?
ECIS, a metric developed for sentence
compression that adapts BLEU so that it in-
cludes a ?verbosity penalty? (Ganitkevitch et
al., 2011) to encourage the compression sys-
tems to produce shorter output.
? We created a development set with sentence
compressions by selecting 1000 pairs of sen-
tences from the multiple translation corpus
where two English translations of the same
foreign sentences differed in each other by a
length ratio of 0.67?0.75.
? We decoded a test set of 1000 sentences us-
ing each of the grammars and its optimized
198
weights with the Joshua decoder (Ganitke-
vitch et al., 2012). The selected in the same
fashion as the dev sentences, so each one had
a human-created reference compression.
We conducted a human evaluation to judge the
meaning and grammaticality of the sentence com-
pressions derived from each paraphrase grammar.
We presented workers on Mechanical Turk with
the input sentence to the compression sentence
(the long sentence), along with 5 shortened out-
puts from our compression systems. To ensure
that workers were producing reliable judgments
we also presented them with a positive control (a
reference compression written by a person) and a
negative controls (a compressed output that was
generated by randomly deleted words). We ex-
cluded judgments from workers who did not per-
form well on the positive and negative controls.
Meaning and grammaticality were scored on
5-point scales where 5 is best. These human
scores were averaged over 2000 judgments (1000
sentences x 2 annotators) for each system. The
systems? outputs were then scored with BLEU,
PR
?
ECIS, and their paraphrase grammars were
scored PARADIGM?s relative recall and precision
lower-bound estimates. For each grammar, we
also calculated the average length of parseable
sentences.
We calculated the correlation between the hu-
man judgements and the automatic scores, using
Spearman?s rank correlation coefficient ?. This
is methodology is the same that is used to quan-
tify the goodness of automatic evaluation metrics
in the machine translation literature (Przybocki et
al., 2008; Callison-Burch et al., 2010). The pos-
sible values of ? range between 1 (where all sys-
tems are ranked in the same order) and ?1 (where
the systems are ranked in the reverse order). Thus
an automatic evaluation metric with a higher abso-
lute value for ? is making predictions that are more
similar to the human judgments than an automatic
evaluation metric with a lower absolute ?.
Table 7 shows that our PARADIGM scores cor-
relate more highly with human judgments than ei-
ther BLEU or PR
?
ECIS for the 5 systems in our eval-
uation. This suggests that it may be a better predic-
tor of the goodness of paraphrase grammars than
MT metrics, when the paraphrase grammars are
used for text-to-text generation tasks.
MEANING GRAMMAR
BLEU -0.7 -0.1
PR
?
ECIS -0.6 +0.2
PINC +0.1 +0.4
PARADIGM
precision
+0.6 +0.1
PARADIGM
recall
+0.1 +0.4
PARADIGM
avg?len
-0.3 +0.4
Table 7: The correlation (Spearman?s ?) of dif-
ferent automatic evaluation metrics with human
judgments of paraphrase quality for the text-to-
text generation task of sentence compression.
7 Summary
We have introduced two new metrics for evaluat-
ing paraphrase grammars, and looked at several
models from a variety of domains. Using these
metrics we can perform a variety of analyses about
SCFG-based paraphrase models:
? Automatically-extracted grammars can parse
a small fraction of held-out data (?30%).
This is comparable to results in MT (Auli et
al., 2009).
? In-domain training data is necessary in or-
der to parse held-out data. A model trained
on newswire data parsed 30% of held-out
newswire sentence pairs, versus to <10% for
literature or parliamentary data.
? SCFGs with syntactic labels perform just as
well as simpler models with a single non-
terminal label.
? Automatically-extracted syntactic grammars
tend to have a reasonable overlap with gram-
mars derived from human-aligned data, in-
cluding more 45% of the gold-standard gram-
mar?s paraphrase rules that occurred at least
twice.
? We showed that PARADIGM more strongly
correlates with human judgments of the
meaning and grammaticality of paraphrases
produced by sentence compression systems
than standard automatic evaluation measures
like BLEU.
PARADIGM will help researchers developing
paraphrase resources to perform similar diagnos-
tics on their models, and quickly evaluate their
systems.
199
Acknowledgements
This material is based on research sponsored by
the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling. Pren-
tice Hall.
Hala Almaghout, Jie Jiang, and Andy Way. 2010.
CCG augmented hierarchical phrase-based machine
translation. In Proc. of IWSLT.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proc. of ACL.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. ParaMetric: An automatic evaluation
metric for paraphrasing. In Proc. of COLING.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation (WMT10).
David L. Chen and William Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proc. of ACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of EMNLP-
CoLing.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research (JAIR), 34:637?674.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4).
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrases cor-
pora: Exploiting massively parallel news sources. In
Proc. of COLING.
Mark Dras. 1997. Representing paraphrases using
synchronous tree adjoining grammars. In Proceed-
ings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 516?518,
Madrid, Spain, July. Association for Computational
Linguistics.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL 2004: Main Proceedings, pages
273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961?968.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283?291, Montr?eal, Canada, June. As-
sociation for Computational Linguistics.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proc. NAACL.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. of NAACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proc. of the Second International
Workshop on Paraphrasing.
200
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philip M. Lewis and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the ACM,
15(3):465?488.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proc. of
ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 104?111, New
York City, USA, June. Association for Computa-
tional Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules from text. Natural Language Engi-
neering, 7(3):343?360.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. PEM: a paraphrase evaluation metric exploit-
ing parallel texts. In Proc. of EMNLP.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 802?811, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97, Portland, Oregon,
June. Association for Computational Linguistics.
Franz Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440?447, Hong Kong,
China, October.
Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan
Weese, Yuan Cao, and Chris Callison-Burch. 2013.
Joshua 5.0: Sparser, better, faster, server. In Proc. of
WMT.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monlingual machine translation for paraphrase gen-
eration. In Proc. of EMNLP.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Robert Borsley and
Kersti B?orjars, editors, Non-Transformational Syn-
tax. Wiley-Blackwell.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of ACL.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
478?484, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proc. of WMT.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using para-
phrases to evaluate summaries automatically. In
Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June. Association for Computational Linguistics.
201
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu?, and Xuchen Yao
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland USA
?University of Maryland, College Park, Maryland USA
Abstract
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
1 Introduction
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as ?elect? or nominaliza-
tions such as ?election?. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al, 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the ?lemma match? heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
2 PARMA
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
1https://github.com/hltcoe/parma
63
RF
? Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
? Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
LDC MTC
? As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
? I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
[meeting]5 .
Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data set
created from the LDC?s Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
?item? with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T . Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
item i in S and item j in T . A full alignment is an
assignment ~a = {aij : i ? NS , j ? NT }, where
NS and NT are the set of item indices for S and T
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T )
with an L1 regularizer (with parameter ?). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold ? on alignment probabilities to get a
classifier. We perform line search on ? and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
2.1 Features
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
2Note that type is not the same thing as part of speech: we
allow nominal predicates like ?death?.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume?, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like ?planet? and ?earth?. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
3While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
4in tokens, not counting some words like determiners and
auxiliary verbs
5like its part of speech tag and whether the it was tagged
as a named entity
6mentions that appear earlier in the document and earlier
in a given sentence are given preference
64
treat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmore?s Frame Semantics (Fill-
more, 1976; Baker et al, 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations ?buy? and ?sell?) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization ?transfer?).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the node?s parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent ?edit? and ?no edit?
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
3 Evaluation
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al, 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N ? 1 pairs for a cluster
of size N ). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
7https://github.com/cnap/anno-pipeline
65
annotated documents from the English Gigaword
Fifth Edition corpus (Parker et al, 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
? on the Roth and Frank dev set, but choose the
regularizer ? based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
8LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
Figure 2: We plotted the PARMA?s performance on
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Frank?s data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
?relatedness? of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs andBp re-
spectively, precision and recall are:
P = |A ?Bp||A| R =
|A ?Bs|
|Bs|
(1)
66
F1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
Table 1: PARMA outperforms the baseline lemma
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Frank?s data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Frank?s reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
4 Results
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
5 Conclusion
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
9We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
Acknowledgements
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1?8. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
67
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montre?al, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
68
Transactions of the Association for Computational Linguistics, 1 (2013) 165?178. Action Editor: David Chiang.
Submitted 11/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Learning to translate with products of novices: a suite of open-ended
challenge problems for teaching MT
Adam Lopez1, Matt Post1, Chris Callison-Burch1,2, Jonathan Weese, Juri Ganitkevitch,
Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,
Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,
Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao?
Department of Computer Science, Johns Hopkins University
1Human Language Technology Center of Excellence, Johns Hopkins University
2Computer and Information Science Department, University of Pennsylvania
Abstract
Machine translation (MT) draws from several
different disciplines, making it a complex sub-
ject to teach. There are excellent pedagogical
texts, but problems in MT and current algo-
rithms for solving them are best learned by
doing. As a centerpiece of our MT course,
we devised a series of open-ended challenges
for students in which the goal was to im-
prove performance on carefully constrained
instances of four key MT tasks: alignment,
decoding, evaluation, and reranking. Students
brought a diverse set of techniques to the prob-
lems, including some novel solutions which
performed remarkably well. A surprising and
exciting outcome was that student solutions
or their combinations fared competitively on
some tasks, demonstrating that even newcom-
ers to the field can help improve the state-of-
the-art on hard NLP problems while simulta-
neously learning a great deal. The problems,
baseline code, and results are freely available.
1 Introduction
A decade ago, students interested in natural lan-
guage processing arrived at universities having been
exposed to the idea of machine translation (MT)
primarily through science fiction. Today, incoming
students have been exposed to services like Google
Translate since they were in secondary school or ear-
lier. For them, MT is science fact. So it makes sense
to teach statistical MT, either on its own or as a unit
? The first five authors were instructors and the remaining au-
thors were students in the worked described here. This research
was conducted while Chris Callison-Burch was at Johns Hop-
kins University.
in a class on natural language processing (NLP), ma-
chine learning (ML), or artificial intelligence (AI). A
course that promises to show students how Google
Translate works and teach them how to build some-
thing like it is especially appealing, and several uni-
versities and summer schools now offer such classes.
There are excellent introductory texts?depending
on the level of detail required, instructors can choose
from a comprehensive MT textbook (Koehn, 2010),
a chapter of a popular NLP textbook (Jurafsky and
Martin, 2009), a tutorial survey (Lopez, 2008), or
an intuitive tutorial on the IBM Models (Knight,
1999b), among many others.
But MT is not just an object of academic study.
It?s a real application that isn?t fully perfected, and
the best way to learn about it is to build an MT sys-
tem. This can be done with open-source toolkits
such as Moses (Koehn et al, 2007), cdec (Dyer et
al., 2010), or Joshua (Ganitkevitch et al, 2012), but
these systems are not designed for pedagogy. They
are mature codebases featuring tens of thousands of
source code lines, making it difficult to focus on
their core algorithms. Most tutorials present them
as black boxes. But our goal is for students to learn
the key techniques in MT, and ideally to learn by
doing. Black boxes are incompatible with this goal.
We solve this dilemma by presenting students
with concise, fully-functioning, self-contained com-
ponents of a statistical MT system: word alignment,
decoding, evaluation, and reranking. Each imple-
mentation consists of a na??ve baseline algorithm in
less than 150 lines of Python code. We assign them
to students as open-ended challenges in which the
goal is to improve performance on objective eval-
uation metrics as much as possible. This setting
mirrors evaluations conducted by the NLP research
165
community and by the engineering teams behind
high-profile NLP projects such as Google Translate
and IBM?s Watson. While we designate specific al-
gorithms as benchmarks for each task, we encour-
age creativity by awarding more points for the best
systems. As additional incentive, we provide a web-
based leaderboard to display standings in real time.
In our graduate class on MT, students took a va-
riety of different approaches to the tasks, in some
cases devising novel algorithms. A more exciting re-
sult is that some student systems or combinations of
systems rivaled the state of the art on some datasets.
2 Designing MT Challenge Problems
Our goal was for students to freely experiment with
different ways of solving MT problems on real data,
and our approach consisted of two separable com-
ponents. First, we provided a framework that strips
key MT problems down to their essence so students
could focus on understanding classic algorithms or
invent new ones. Second, we designed incentives
that motivated them to improve their solutions as
much as possible, encouraging experimentation with
approaches beyond what we taught in class.
2.1 Decoding, Reranking, Evaluation, and
Alignment for MT (DREAMT)
We designed four assignments, each corresponding
to a real subproblem in MT: alignment, decoding,
evaluation, and reranking.1 From the more general
perspective of AI, they emphasize the key problems
of unsupervised learning, search, evaluation design,
and supervised learning, respectively. In real MT
systems, these problems are highly interdependent,
a point we emphasized in class and at the end of each
assignment?for example, that alignment is an exer-
cise in parameter estimation for translation models,
that model choice is a tradeoff between expressivity
and efficient inference, and that optimal search does
not guarantee optimal accuracy. However, present-
ing each problem independently and holding all else
constant enables more focused exploration.
For each problem we provided data, a na??ve solu-
tion, and an evaluation program. Following Bird et
al. (2008) and Madnani and Dorr (2008), we imple-
mented the challenges in Python, a high-level pro-
1http://alopez.github.io/dreamt
gramming language that can be used to write very
concise programs resembling pseudocode.2,3 By de-
fault, each baseline system reads the test data and
generates output in the evaluation format, so setup
required zero configuration, and students could be-
gin experimenting immediately. For example, on re-
ceipt of the alignment code, aligning data and eval-
uating results required only typing:
> align | grade
Students could then run experiments within minutes
of beginning the assignment.
Three of the four challenges also included unla-
beled test data (except the decoding assignment, as
explained in ?4). We evaluated test results against a
hidden key when assignments were submitted.
2.2 Incentive Design
We wanted to balance several pedagogical goals: un-
derstanding of classic algorithms, free exploration
of alternatives, experience with typical experimental
design, and unhindered collaboration.
Machine translation is far from solved, so we ex-
pected more than reimplementation of prescribed al-
gorithms; we wanted students to really explore the
problems. To motivate exploration, we made the as-
signments competitive. Competition is a powerful
force, but must be applied with care in an educa-
tional setting.4 We did not want the consequences
of ambitious but failed experiments to be too dire,
and we did not want to discourage collaboration.
For each assignment, we guaranteed a passing
grade for matching the performance of a specific tar-
get alorithm. Typically, the target was important
but not state-of-the-art: we left substantial room for
improvement, and thus competition. We told stu-
dents the exact algorithm that produced the target ac-
curacy (though we expected them to derive it them-
selves based on lectures, notes, or literature). We
did not specifically require them to implement it, but
the guarantee of a passing grade provided a power-
ful incentive for this to be the first step of each as-
signment. Submissions that beat this target received
additional credit. The top five submissions received
full credit, while the top three received extra credit.
2http://python.org
3Some well-known MT systems have been implemented in
Python (Chiang, 2007; Huang and Chiang, 2007).
4Thanks to an anonymous reviewer for this turn of phrase.
166
This scheme provided strong incentive to continue
experimentation beyond the target alorithm.5
For each assignment, students could form teams
of any size, under three rules: each team had to pub-
licize its formation to the class, all team members
agreed to receive the same grade, and teams could
not drop members. Our hope was that these require-
ments would balance the perceived competitive ad-
vantage of collaboration against a reluctance to take
(and thus support) teammates who did not contribute
to the competitive effort.6 This strategy worked: out
of sixteen students, ten opted to work collaboratively
on at least one assignment, always in pairs.
We provided a web-based leaderboard that dis-
played standings on the test data in real time, iden-
tifying each submission by a pseudonymous han-
dle known only to the team and instructors. Teams
could upload solutions as often as they liked before
the assignment deadline. The leaderboard displayed
scores of the default and target alorithms. This in-
centivized an early start, since teams could verify
for themselves when they met the threshold for a
passing grade. Though effective, it also detracted
from realism in one important way: it enabled hill-
climbing on the evaluation metric. In early assign-
ments, we observed a few cases of this behavior,
so for the remaining assignments, we modified the
leaderboard so that changes in score would only be
reflected once every twelve hours. This strategy
trades some amount of scientific realism for some
measure of incentive, a strategy that has proven
effective in other pedagogical tools with real-time
feedback (Spacco et al, 2006).
To obtain a grade, teams were required to sub-
mit their results, share their code privately with the
instructors, and publicly describe their experimen-
tal process to the class so that everyone could learn
from their collective effort. Teams were free (but not
required) to share their code publicly at any time.
5Grades depend on institutional norms. In our case, high grades
in the rest of class combined with matching all assignment tar-
get alorithms would earn a B+; beating two target alorithms
would earn an A-; top five placement on any assignment would
earn an A; and top three placement compensated for weaker
grades in other course criteria. Everyone who completed all
four assignments placed in the top five at least once.
6The equilibrium point is a single team, though this team would
still need to decide on a division of labor. One student contem-
plated organizing this team, but decided against it.
Some did so after the assignment deadline.
3 The Alignment Challenge
The first challenge was word alignment: given a par-
allel text, students were challenged to produce word-
to-word alignments with low alignment error rate
(AER; Och and Ney, 2000). This is a variant of a
classic assignment not just in MT, but in NLP gen-
erally. Klein (2005) describes a version of it, and we
know several other instructors who use it.7 In most
of these, the object is to implement IBM Model 1
or 2, or a hidden Markov model. Our version makes
it open-ended by asking students to match or beat an
IBM Model 1 baseline.
3.1 Data
We provided 100,000 sentences of parallel data from
the Canadian Hansards, totaling around two million
words.8 This dataset is small enough to align in
a few minutes with our implementation?enabling
rapid experimentation?yet large enough to obtain
reasonable results. In fact, Liang et al (2006) report
alignment accuracy on data of this size that is within
a fraction of a point of their accuracy on the com-
plete Hansards data. To evaluate, we used manual
alignments of a small fraction of sentences, devel-
oped by Och and Ney (2000), which we obtained
from the shared task resources organized by Mihal-
cea and Pedersen (2003). The first 37 sentences
of the corpus were development data, with manual
alignments provided in a separate file. Test data con-
sisted of an additional 447 sentences, for which we
did not provide alignments.9
3.2 Implementation
We distributed three Python programs with the
data. The first, align, computes Dice?s coefficient
(1945) for every pair of French and English words,
then aligns every pair for which its value is above an
adjustable threshold. Our implementation (most of
7Among them, Jordan Boyd-Graber, John DeNero, Philipp
Koehn, and Slav Petrov (personal communication).
8http://www.isi.edu/natural-language/download/hansard/
9This invited the possibility of cheating, since alignments of the
test data are publicly available on the web. We did not adver-
tise this, but as an added safeguard we obfuscated the data by
distributing the test sentences randomly throughout the file.
167
Listing 1 The default aligner in DREAMT: thresh-
olding Dice?s coefficient.
for (f, e) in bitext:
for f_i in set(f):
f_count[f_i] += 1
for e_j in set(e):
fe_count[(f_i,e_j)] += 1
for e_j in set(e):
e_count[e_j] += 1
for (f_i, e_j) in fe_count.keys():
dice[(f_i,e_j)] = \
2.0 * fe_count[(f_i, e_j)] / \
(f_count[f_i] + e_count[e_j])
for (f, e) in bitext:
for (i, f_i) in enumerate(f):
for (j, e_j) in enumerate(e):
if dice[(f_i,e_j)] >= cutoff:
print "%i-%i " % (i,j)
which is shown in Listing 1) is quite close to pseu-
docode, making it easy to focus on the algorithm,
one of our pedagogical goals. The grade program
computes AER and optionally prints an alignment
grid for sentences in the development data, showing
both human and automatic alignments. Finally the
check program verifies that the results represent
a valid solution, reporting an error if not?enabling
students to diagnose bugs in their submissions.
The default implementation enabled immediate
experimentation. On receipt of the code, students
were instructed to align the first 1,000 sentences and
compute AER using a simple command.
> align -n 1000 | grade By varying the
number of input sentences and the threshold for an
alignment, students could immediately see the effect
of various parameters on alignment quality.
We privately implemented IBM Model 1 (Brown
et al, 1993) as the target alorithm for a passing
grade. We ran it for five iterations with English
as the target language and French as the source.
Our implementation did not use null alignment
or symmetrization?leaving out these common im-
provements offered students the possibility of dis-
covering them independently, and thereby rewarded.
A
E
R
?
10
0
20
30
40
50
60
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 1: Submission history for the alignment challenge.
Dashed lines represent the default and baseline system
performance. Each colored line represents a student, and
each dot represents a submission. For clarity, we show
only submissions that improved the student?s AER.
3.3 Challenge Results
We received 209 submissions from 11 teams over a
period of two weeks (Figure 1). Everyone eventually
matched or exceeded IBM Model 1 AER of 31.26.
Most students implemented IBM Model 1, but we
saw many other solutions, indicating that many truly
experimented with the problem:
? Implementing heuristic constraints to require
alignment of proper names and punctuation.
? Running the algorithm on stems rather than sur-
face words.
? Initializing the first iteration of Model 1 with
parameters estimated on the observed align-
ments in the development data.
? Running Model 1 for many iterations. Most re-
searchers typically run Model 1 for five itera-
tions or fewer, and there are few experiments
in the literature on its behavior over many iter-
ations, as there are for hidden Markov model
taggers (Johnson, 2007). Our students carried
out these experiments, reporting runs of 5, 20,
100, and even 2000 iterations. No improve-
ment was observed after 20 iterations.
168
? Implementing various alternative approaches
from the literature, including IBM Model 2
(Brown et al, 1993), competitive linking
(Melamed, 2000), and smoothing (Moore,
2004).
One of the best solutions was competitive linking
with Dice?s coefficient, modified to incorporate the
observation that alignments tend to be monotonic by
restricting possible alignment points to a window of
eight words around the diagonal. Although simple,
it acheived an AER of 18.41, an error reduction over
Model 1 of more than 40%.
The best score compares unfavorably against a
state-of-the-art AER of 3.6 (Liu et al, 2010). But
under a different view, it still represents a significant
amount of progress for an effort taking just over two
weeks: on the original challenge from which we ob-
tained the data (Mihalcea and Pedersen, 2003) the
best student system would have placed fifth out of
fifteen systems. Consider also the combined effort of
all the students: when we trained a perceptron clas-
sifier on the development data, taking each student?s
prediction as a feature, we obtained an AER of 15.4,
which would have placed fourth on the original chal-
lenge. This is notable since none of the systems
incorporated first-order dependencies on the align-
ments of adjacent words, long noted as an impor-
tant feature of the best alignment models (Och and
Ney, 2003). Yet a simple system combination of stu-
dent assignments is as effective as a hidden Markov
Model trained on a comparable amount of data (Och
and Ney, 2003).
It is important to note that AER does not neces-
sarily correlate with downstream performance, par-
ticularly on the Hansards dataset (Fraser and Marcu,
2007). We used the conclusion of the assignment as
an opportunity to emphasize this point.
4 The Decoding Challenge
The second challenge was decoding: given a fixed
translation model and a set of input sentences, stu-
dents were challenged to produce translations with
the highest model score. This challenge introduced
the difficulties of combinatorial optimization under
a deceptively simple setup: the model we provided
was a simple phrase-based translation model (Koehn
et al, 2003) consisting only of a phrase table and tri-
gram language model. Under this simple model, for
a French sentence f of length I , English sentence
e of length J , and alignment a where each element
consists of a span in both e and f such that every
word in both e and f is aligned exactly once, the
conditional probability of e and a given f is as fol-
lows.10
p(e, a|f) =
?
?i,i?,j,j???a
p(f i?i |ej
?
j )
J+1?
j=1
p(ej |ej?1, ej?2)
(1)
To evaluate output, we compute the conditional
probability of e as follows.
p(e|f) =
?
a
p(e, a|f) (2)
Note that this formulation is different from the typ-
ical Viterbi objective of standard beam search de-
coders, which do not sum over all alignments, but
approximate p(e|f) by maxa p(e, a|f). Though the
computation in Equation 2 is intractable (DeNero
and Klein, 2008), it can be computed in a few min-
utes via dynamic programming on reasonably short
sentences. We ensured that our data met this crite-
rion. The corpus-level probability is then the prod-
uct of all sentence-level probabilities in the data.
The model includes no distortion limit or distor-
tion model, for two reasons. First, leaving out the
distortion model slightly simplifies the implementa-
tion, since it is not necessary to keep track of the last
word translated in a beam decoder; we felt that this
detail was secondary to understanding the difficulty
of search over phrase permutations. Second, it actu-
ally makes the problem more difficult, since a simple
distance-based distortion model prefers translations
with fewer permutations; without it, the model may
easily prefer any permutation of the target phrases,
making even the Viterbi search problem exhibit its
true NP-hardness (Knight, 1999a; Zaslavskiy et al,
2009).
Since the goal was to find the translation with the
highest probability, we did not provide a held-out
test set; with access to both the input sentences and
10For simplicity, this formula assumes that e is padded with two
sentence-initial symbols and one sentence-final symbol, and
ignores the probability of sentence segmentation, which we
take to be uniform.
169
the model, students had enough information to com-
pute the evaluation score on any dataset themselves.
The difficulty of the challenge lies simply in finding
the translation that maximizes the evaluation. In-
deed, since the problem is intractable, even the in-
structors did not know the true solution.11
4.1 Data
We chose 48 French sentences totaling 716 words
from the Canadian Hansards to serve as test data.
To create a simple translation model, we used the
Berkeley aligner to align the parallel text from the
first assignment, and extracted a phrase table using
the method of Lopez (2007), as implemented in cdec
(Dyer et al, 2010). To create a simple language
model, we used SRILM (Stolcke, 2002).
4.2 Implementation
We distributed two Python programs. The first,
decode, decodes the test data monotonically?
using both the language model and translation
model, but without permuting phrases. The imple-
mentation is completely self-contained with no ex-
ternal dependencies: it implements both models and
a simple stack decoding algorithm for monotonic
translation. It contains only 122 lines of Python?
orders of magnitude fewer than most full-featured
decoders. To see its similarity to pseudocode, com-
pare the decoding algorithm (Listing 2) with the
pseudocode in Koehn?s (2010) popular textbook (re-
produced here as Algorithm 1). The second pro-
gram, grade, computes the log-probability of a set
of translations, as outline above.
We privately implemented a simple stack decoder
that searched over permutations of phrases, similar
to Koehn (2004). Our implementation increased the
codebase by 44 lines of code and included param-
eters for beam size, distortion limit, and the maxi-
mum number of translations considered for each in-
put phrase. We posted a baseline to the leaderboard
using values of 50, 3, and 20 for these, respectively.
11We implemented a version of the Lagrangian relaxation algo-
rithm of Chang and Collins (2011), but found it difficult to
obtain tight (optimal) solutions without iteratively reintroduc-
ing all of the original constraints. We suspect this is due to
the lack of a distortion penalty, which enforces a strong pref-
erence towards translations with little reordering. However,
the solution found by this algorithm is only approximates the
objective implied by Equation 2, which sums over alignments.
We also posted an oracle containing the most prob-
able output for each sentence, selected from among
all submissions received so far. The intent of this
oracle was to provide a lower bound on the best pos-
sible output, giving students additional incentive to
continue improving their systems.
4.3 Challenge Results
We received 71 submissions from 10 teams (Fig-
ure 2), again exhibiting variety of solutions.
? Implementation of greedy decoder which at
each step chooses the most probable translation
from among those reachable by a single swap
or retranslation (Germann et al, 2001; Langlais
et al, 2007).
? Inclusion of heuristic estimates of future cost.
? Implementation of a private oracle. Some stu-
dents observed that the ideal beam setting was
not uniform across the corpus. They ran their
decoder under different settings, and then se-
lected the most probable translation of each
sentence.
Many teams who implemented the standard stack
decoding algorithm experimented heavily with its
pruning parameters. The best submission used ex-
tremely wide beam settings in conjunction with a
reimplementation of the future cost estimate used in
Moses (Koehn et al, 2007). Five of the submissions
beat Moses using its standard beam settings after it
had been configured to decode with our model.
We used this assignment to emphasize the im-
portance of good models: the model score of the
submissions was generally inversely correlated with
BLEU, possibly because our simple model had no
distortion limits. We used this to illustrate the differ-
ence between model error and search error, includ-
ing fortuitous search error (Germann et al, 2001)
made by decoders with less accurate search.
5 The Evaluation Challenge
The third challenge was evaluation: given a test cor-
pus with reference translations and the output of sev-
eral MT systems, students were challenged to pro-
duce a ranking of the systems that closely correlated
with a human ranking.
170
Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.
stacks = [{} for _ in f] + [{}]
stacks[0][lm.begin()] = initial_hypothesis
for i, stack in enumerate(stacks[:-1]):
for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:
for j in xrange(i+1,len(f)+1):
if f[i:j] in tm:
for phrase in tm[f[i:j]]:
logprob = h.logprob + phrase.logprob
lm_state = h.lm_state
for word in phrase.english.split():
(lm_state, word_logprob) = lm.score(lm_state, word)
logprob += word_logprob
logprob += lm.end(lm_state) if j == len(f) else 0.0
new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
if lm_state not in stacks[j] or \
stacks[j][lm_state].logprob < logprob:
stacks[j][lm_state] = new_hypothesis
winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
def extract_english(h):
return "" if h.predecessor is None else "%s%s " %
(extract_english(h.predecessor), h.phrase.english)
print extract_english(winner)
Algorithm 1 Basic stack decoding algorithm,
adapted from Koehn (2010), p. 165.
place empty hypothesis into stack 0
for all stacks 0...n? 1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hypothesis
place in stack
recombine with existing hypothesis
prune stack if too big
5.1 Data
We chose the English-to-German translation sys-
tems from the 2009 and 2011 shared task at the an-
nual Workshop for Machine Translation (Callison-
Burch et al, 2009; Callison-Burch et al, 2011), pro-
viding the first as development data and the second
as test data. We chose these sets because BLEU
(Papineni et al, 2002), our baseline metric, per-
formed particularly poorly on them; this left room
for improvement in addition to highlighting some
lo
g 1
0
p(
e|f
)?
C
-1200
-1250
-1300
-1350
-1400
-20
days
-18
days
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 2: Submission history for the decoding challenge.
The dotted green line represents the oracle over submis-
sions.
deficiencies of BLEU. For each dataset we pro-
vided the source and reference sentences along with
anonymized system outputs. For the development
data we also provided the human ranking of the sys-
171
tems, computed from pairwise human judgements
according to a formula recommended by Bojar et al
(2011).12
5.2 Implementation
We provided three simple Python programs:
evaluate implements a simple ranking of the sys-
tems based on position-independent word error rate
(PER; Tillmann et al, 1997), which computes a bag-
of-words overlap between the system translations
and the reference. The grade program computes
Spearman?s ? between the human ranking and an
output ranking. The check program simply ensures
that a submission contains a valid ranking.
We were concerned about hill-climbing on the test
data, so we modified the leaderboard to report new
results only twice a day. This encouraged students to
experiment on the development data before posting
new submissions, while still providing intermittent
feedback.
We privately implemented a version of BLEU,
which obtained a correlation of 38.6 with the human
rankings, a modest improvement over the baseline
of 34.0. Our implementation underperforms the one
reported in Callison-Burch et al (2011) since it per-
forms no tokenization or normalization of the data.
This also left room for improvement.
5.3 Evaluation Challenge Results
We received 212 submissions from 12 teams (Fig-
ure 3), again demonstrating a wide range of tech-
niques.
? Experimentation with the maximum n-gram
length and weights in BLEU.
? Implementation of smoothed versions of BLEU
(Lin and Och, 2004).
? Implementation of weighted F-measure to bal-
ance both precision and recall.
? Careful normalization of the reference and ma-
chine translations, including lowercasing and
punctuation-stripping.
12This ranking has been disputed over a series of papers (Lopez,
2012; Callison-Burch et al, 2012; Koehn, 2012). The paper
which initiated the dispute, written by the first author, was di-
rectly inspired by the experience of designing this assignment.
Sp
ea
rm
an
?s
?
0.8
0.6
0.4
-7
days
-6
days
-5
days
-4
days
-3
days
-2
days
-1
days
due
Figure 3: Submission history for the evaluation chal-
lenge.
? Implementation of several techniques used in
AMBER (Chen and Kuhn, 2005).
The best submission, obtaining a correlation of
83.5, relied on the idea that the reference and ma-
chine translation should be good paraphrases of each
other (Owczarzak et al, 2006; Kauchak and Barzi-
lay, 2006). It employed a simple paraphrase sys-
tem trained on the alignment challenge data, us-
ing the pivot technique of Bannard and Callison-
Burch (2005), and computing the optimal alignment
between machine translation and reference under a
simple model in which words could align if they
were paraphrases. When compared with the 20
systems submitted to the original task from which
the data was obtained (Callison-Burch et al, 2011),
this system would have ranked fifth, quite near the
top-scoring competitors, whose correlations ranged
from 88 to 94.
6 The Reranking Challenge
The fourth challenge was reranking: given a test cor-
pus and a large N -best list of candidate translations
for each sentence, students were challenged to select
a candidate translation for each sentence to produce
a high corpus-level BLEU score. Due to an error
our data preparation, this assignment had a simple
solution that was very difficult to improve on. Nev-
ertheless, it featured several elements that may be
useful for future courses.
172
6.1 Data
We obtained 300-best lists from a Spanish-English
translation system built with the Joshua toolkit
(Ganitkevitch et al, 2012) using data and resources
from the 2011 Workshop on Machine Translation
(Callison-Burch et al, 2011). We provided 1989
training sentences, consisting of source and refer-
ence sentences along with the candidate translations.
We also included a test set of 250 sentences, for
which we provided only the source and candidate
translations. Each candidate translation included six
features from the underlying translation system, out
of an original 21; our hope was that students might
rediscover some features through experimentation.
6.2 Implementation
We conceived of the assignment as one in which stu-
dents could apply machine learning or feature engi-
neering to the task of reranking the systems, so we
provided several tools. The first of these, learn,
was a simple program that produced a vector of
feature weights using pairwise ranking optimization
(PRO; Hopkins and May, 2011), with a perceptron
as the underlying learning algorithm. A second,
rerank, takes a weight vector as input and reranks
the sentences; both programs were designed to work
with arbitrary numbers of features. The grade pro-
gram computed the BLEU score on development
data, while check ensured that a test submission
is valid. Finally, we provided an oracle program,
which computed a lower bound on the achievable
BLEU score on the development data using a greedy
approximation (Och et al, 2004). The leaderboard
likewise displayed an oracle on test data. We did
not assign a target alorithm, but left the assignment
fully open-ended.
6.3 Reranking Challenge Outcome
For each assignment, we made an effort to create
room for competition above the target alorithm.
However, we did not accomplish this in the rerank-
ing challenge: we had removed most of the features
from the candidate translations, in hopes that stu-
dents might reinvent some of them, but we left one
highly predictive implicit feature in the data: the
rank order of the underlying translation system. Stu-
dents discovered that simply returning the first can-
didate earned a very high score, and most of them
quickly converged to this solution. Unfortunately,
the high accuracy of this baseline left little room for
additional competition. Nevertheless, we were en-
couraged that most students discovered this by acci-
dent while attempting other strategies to rerank the
translations.
? Experimentation with parameters of the PRO
algorithm.
? Substitution of alternative learning algorithms.
? Implementation of a simplified minimum
Bayes risk reranker (Kumar and Byrne, 2004).
Over a baseline of 24.02, the latter approach ob-
tained a BLEU of 27.08, nearly matching the score
of 27.39 from the underlying system despite an im-
poverished feature set.
7 Pedagogical Outcomes
Could our students have obtained similar results by
running standard toolkits? Undoubtedly. However,
our goal was for students to learn by doing: they
obtained these results by implementing key MT al-
gorithms, observing their behavior on real data, and
improving them. This left them with much more in-
sight into how MT systems actually work, and in
this sense, DREAMT was a success. At the end of
class, we requested written feedback on the design
of the assignments. Many commented positively on
the motivation provided by the challenge problems:
? The immediate feedback of the automatic grad-
ing was really nice.
? Fast feedback on my submissions and my rela-
tive position on the leaderboard kept me both
motivated to start the assignments early and to
constantly improve them. Also knowing how
well others were doing was a good way to
gauge whether I was completely off track or not
when I got bad results.
? The homework assignments were very engag-
ing thanks to the clear yet open-ended setup
and their competitive aspects.
Students also commented that they learned a lot
about MT and even research in general:
173
Question 1 2 3 4 5 N/A
Feedback on my work for this course is useful - - - 4 9 3
This course enhanced my ability to work effectively in a team 1 - 5 8 2 -
Compared to other courses at this level, the workload for this course is high - 1 7 6 1 1
Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
? I learned the most from the assignments.
? The assignments always pushed me one step
more towards thinking out loud how the par-
ticular task can be completed.
? I appreciated the setup of the homework prob-
lems. I think it has helped me learn how to
set up and attack research questions in an or-
ganized way. I have a much better sense for
what goes into an MT system and what prob-
lems aren?t solved.
We also received feedback through an anonymous
survey conducted at the end of the course before
posting final grades. Each student rated aspects
of the course on a five point Likert scale, from 1
(strongly disagree) to 5 (strongly agree). Several
questions pertained to assignments (Table 1), and al-
lay two possible concerns about competition: most
students felt that the assignments enhanced their col-
laborative skills, and that their open-endedness did
not result in an overload of work. For all survey
questions, student satisfaction was higher than av-
erage for courses in our department.
8 Discussion
DREAMT is inspired by several different ap-
proaches to teaching NLP, AI, and computer sci-
ence. Eisner and Smith (2008) teach NLP using
a competitive game in which students aim to write
fragments of English grammar. Charniak et al
(2000) improve the state-of-the-art in a reading com-
prehension task as part of a group project. Christo-
pher et al (1993) use NACHOS, a classic tool for
teaching operating systems by providing a rudimen-
tary system that students then augment. DeNero and
Klein (2010) devise a series of assignments based
on Pac-Man, for which students implement several
classic AI techniques. A crucial element in such ap-
proaches is a highly functional but simple scaffold-
ing. The DREAMT codebase, including grading and
validation scripts, consists of only 656 lines of code
(LOC) over four assignments: 141 LOC for align-
ment, 237 LOC for decoding, 86 LOC for evalua-
tion, and 192 LOC for reranking. To simplify imple-
mentation further, the optional leaderboard could be
delegated to Kaggle.com, a company that organizes
machine learning competitions using a model sim-
ilar to the Netflix Challenge (Bennet and Lanning,
2007), and offers pro bono use of its services for
educational challenge problems. A recent machine
learning class at Oxford hosted its assignments on
Kaggle (Phil Blunsom, personal communication).
We imagine other uses of DREAMT. It could be
used in an inverted classroom, where students view
lecture material outside of class and work on prac-
tical problems in class. It might also be useful in
massive open online courses (MOOCs). In this for-
mat, course material (primarily lectures and quizzes)
is distributed over the internet to an arbitrarily large
number of interested students through sites such as
coursera.org, udacity.com, and khanacademy.org. In
many cases, material and problem sets focus on spe-
cific techniques. Although this is important, there is
also a place for open-ended problems on which stu-
dents apply a full range of problem-solving skills.
Automatic grading enables them to scale easily to
large numbers of students.
On the scientific side, the scale of MOOCs might
make it possible to empirically measure the effec-
tiveness of hands-on or competitive assignments,
by comparing course performance of students who
work on them against that of those who do not.
Though there is some empirical work on competi-
tive assignments in the computer science education
literature (Lawrence, 2004; Garlick and Akl, 2006;
Regueras et al, 2008; Ribeiro et al, 2009), they
generally measure student satisfaction and retention
rather than the more difficult question of whether
such assignments actually improve student learning.
However, it might be feasible to answer such ques-
174
tions in large, data-rich virtual classrooms offered
by MOOCs. This is an interesting potential avenue
for future work.
Because our class came within reach of state-of-
the-art on each problem within a matter of weeks,
we wonder what might happen with a very large
body of competitors. Could real innovation oc-
cur? Could we solve large-scale problems? It may
be interesting to adopt a different incentive struc-
ture, such as one posed by Abernethy and Frongillo
(2011) for crowdsourcing machine learning prob-
lems: rather than competing, everyone works to-
gether to solve a shared task, with credit awarded
proportional to the contribution that each individual
makes. In this setting, everyone stands to gain: stu-
dents learn to solve problems as they are found in
the real world, instructors learn new insights into the
problems they pose, and, in the long run, users of
AI technology benefit from overall improvements.
Hence it is possible that posing open-ended, real-
world problems to students might be a small piece
of the puzzle of providing high-quality NLP tech-
nologies.
Acknowledgments
We are grateful to Colin Cherry and Chris Dyer
for testing the assignments in different settings and
providing valuable feedback, and to Jessie Young
for implementing a dual decomposition solution to
the decoding assignment. We thank Jason Eis-
ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,
Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-
ney Napoles, Michael Rushanan, Joanne Selinski,
Svitlana Volkova, and the anonymous reviewers for
lively discussion and helpful comments on previous
drafts of this paper. Any errors are our own.
References
J. Abernethy and R. M. Frongillo. 2011. A collaborative
mechanism for crowdsourcing prediction problems. In
Proc. of NIPS.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
J. Bennet and S. Lanning. 2007. The netflix prize. In
Proc. of the KDD Cup and Workshop.
S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008.
Multidisciplinary instruction with the natural language
toolkit. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
O. Bojar, M. Ercegovc?evic?, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2).
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 work-
shop on statistical machine translation. In Proc. of
WMT.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. In Proc. of EMNLP.
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,
M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,
W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Read-
ing comprehension programs in a statistical-language-
processing class. In Proc. of Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
B. Chen and R. Kuhn. 2005. AMBER: A modified
BLEU, enhanced ranking metric. In Proc. of WMT.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
W. A. Christopher, S. J. Procter, and T. E. Anderson.
1993. The nachos instructional operating system. In
Proc. of USENIX.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL.
J. DeNero and D. Klein. 2010. Teaching introductory
articial intelligence with Pac-Man. In Proc. of Sym-
posium on Educational Advances in Artificial Intelli-
gence.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
J. Eisner and N. A. Smith. 2008. Competitive grammar
writing. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
175
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3).
J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and
C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and paraphrases. In Proc. of WMT.
R. Garlick and R. Akl. 2006. Intra-class competitive
assignments in CS2: A one-year study. In Proc. of
International Conference on Engineering Education.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. of ACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing. Prentice Hall, 2nd edition.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
D. Klein. 2005. A core-tools statistical NLP course. In
Proc. of Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL.
K. Knight. 1999a. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4).
K. Knight. 1999b. A statistical MT tutorial workbook.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. of AMTA.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn. 2012. Simulating human judgment in machine
translation evaluation campaigns. In Proc. of IWSLT.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
P. Langlais, A. Patry, and F. Gotti. 2007. A greedy de-
coder for phrase-based statistical machine translation.
In Proc. of TMI.
R. Lawrence. 2004. Teaching data structures using
competitive games. IEEE Transactions on Education,
47(4).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3).
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3).
A. Lopez. 2012. Putting human assessments of machine
translation systems in order. In Proc. of WMT.
N. Madnani and B. Dorr. 2008. Combining open-source
with research to re-engineer a hands-on introductory
NLP course. In Proc. of Workshop on Issues in Teach-
ing Computational Linguistics.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2).
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proc. on Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2004. Improving IBM word alignment
model 1. In Proc. of ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
NAACL.
K. Owczarzak, D. Groves, J. V. Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
L. Regueras, E. Verdu?, M. Verdu?, M. Pe?rez, J. de Castro,
and M. Mun?oz. 2008. Motivating students through
on-line competition: An analysis of satisfaction and
learning styles.
P. Ribeiro, M. Ferreira, and H. Simo?es. 2009. Teach-
ing artificial intelligence and logic programming in a
competitive environment. Informatics in Education,
(Vol 8 1):85.
J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,
N. Padua-Perez, and F. Emad. 2006. Experiences with
marmoset: Designing and using an advanced submis-
sion and testing system for programming courses. In
Proc. of Innovation and technology in computer sci-
ence education.
176
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In Proc. of European Conf. on Speech
Communication and Technology.
M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009.
Phrase-based statistical machine translation as a trav-
eling salesman problem. In Proc. of ACL.
177
178
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 44?52, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems
Lushan Han, Abhay Kashyap, Tim Finin
Computer Science and
Electrical Engineering
University of Maryland, Baltimore County
Baltimore MD 21250
{lushan1,abhay1,finin}@umbc.edu
James Mayfield and Jonathan Weese
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore MD 21211
mayfield@jhu.edu, jonny@cs.jhu.edu
Abstract
We describe three semantic text similarity
systems developed for the *SEM 2013 STS
shared task and the results of the correspond-
ing three runs. All of them shared a word sim-
ilarity feature that combined LSA word sim-
ilarity and WordNet knowledge. The first,
which achieved the best mean score of the 89
submitted runs, used a simple term alignment
algorithm augmented with penalty terms. The
other two runs, ranked second and fourth, used
support vector regression models to combine
larger sets of features.
1 Introduction
Measuring semantic text similarity has been a re-
search subject in natural language processing, infor-
mation retrieval and artificial intelligence for many
years. Previous efforts have focused on compar-
ing two long texts (e.g., for document classification)
or a short text with a long text (e.g., Web search),
but there are a growing number of tasks requiring
computing the semantic similarity between two sen-
tences or other short text sequences. They include
paraphrase recognition (Dolan et al, 2004), Twitter
tweets search (Sriram et al, 2010), image retrieval
by captions (Coelho et al, 2004), query reformula-
tion (Metzler et al, 2007), automatic machine trans-
lation evaluation (Kauchak and Barzilay, 2006) and
schema matching (Han et al, 2012).
There are three predominant approaches to com-
puting short text similarity. The first uses informa-
tion retrieval?s vector space model (Meadow, 1992)
in which each text is modeled as a ?bag of words?
and represented using a vector. The similarity be-
tween two texts is then computed as the cosine
similarity of the vectors. A variation on this ap-
proach leverages web search results (e.g., snip-
pets) to provide context for the short texts and en-
rich their vectors using the words in the snippets
(Sahami and Heilman, 2006). The second approach
is based on the assumption that if two sentences or
other short text sequences are semantically equiva-
lent, we should be able to align their words or ex-
pressions. The alignment quality can serve as a
similarity measure. This technique typically pairs
words from the two texts by maximizing the sum-
mation of the word similarity of the resulting pairs
(Mihalcea et al, 2006). The third approach com-
bines different measures and features using machine
learning models. Lexical, semantic and syntactic
features are computed for the texts using a variety
of resources and supplied to a classifier, which then
assigns weights to the features by fitting the model
to training data (Saric et al, 2012).
For evaluating different approaches, the 2013 Se-
mantic Textual Similarity (STS) task asked auto-
matic systems to compute sentence similarity ac-
cording to a scale definition ranging from 0 to 5,
with 0 meaning unrelated and 5 semantically equiv-
alent (Agirre et al, 2012; Agirre et al, 2013). The
example sentence pair ?The woman is playing the
violin? and ?The young lady enjoys listening to the
guitar? is scored as only 1 and the pair ?The bird is
bathing in the sink? and ?Birdie is washing itself in
the water basin? is given a score of 5.
The vector-space approach tends to be too shallow
for the task, since solving it well requires discrimi-
nating word-level semantic differences and goes be-
44
yond simply comparing sentence topics or contexts.
Our first run uses an align-and-penalize algorithm,
which extends the second approach by giving penal-
ties to the words that are poorly aligned. Our other
two runs use a support vector regression model to
combine a large number of general and domain spe-
cific features. An important and fundamental feature
used by all three runs is a powerful semantic word
similarity model based on a combination of Latent
Semantic Analysis (LSA) (Deerwester et al, 1990;
Landauer and Dumais, 1997) and knowledge from
WordNet (Miller, 1995).
The remainder of the paper proceeds as follows.
Section 2 presents the hybrid word similarity model.
Section 3 describes the align-and-penalize approach
used for the PairingWords run. In Section 4 we de-
scribe the SVM approach used for the Galactus and
Saiyan runs. Section 5 discusses the results and is
followed by a short conclusion.
2 Semantic Word Similarity Model
Our word similarity model was originally developed
for the Graph of Relations project (UMBC, 2013a)
which maps informal queries with English words
and phrases for an RDF linked data collection into
a SPARQL query. For this, we wanted a metric
in which only the semantics of a word is consid-
ered and not its lexical category. For example, the
verb ?marry? should be semantically similar to the
noun ?wife?. Another desiderata was that the met-
ric should give highest scores and lowest scores in
its range to similar and non-similar words, respec-
tively. In this section, we describe how we con-
structed the model by combining LSA word simi-
larity and WordNet knowledge.
2.1 LSA Word Similarity
LSA Word Similarity relies on the distributional hy-
pothesis that words occurring in the same contexts
tend to have similar meanings (Harris, 1968).
2.1.1 Corpus Selection and Processing
In order to produce a reliable word co-occurrence
statistics, a very large and balanced text corpus is
required. After experimenting with several cor-
pus choices including Wikipedia, Project Gutenberg
e-Books (Hart, 1997), ukWaC (Baroni et al, 2009),
Reuters News stories (Rose et al, 2002) and LDC
gigawords, we selected the Web corpus from the
Stanford WebBase project (Stanford, 2001). We
used the February 2007 crawl, which is one of the
largest collections and contains 100 million web
pages from more than 50,000 websites. The Web-
Base project did an excellent job in extracting tex-
tual content from HTML tags but still has abun-
dant text duplications, truncated text, non-English
text and strange characters. We processed the collec-
tion to remove undesired sections and produce high
quality English paragraphs. We detected paragraphs
using heuristic rules and only retrained those whose
length was at least two hundred characters. We elim-
inated non-English text by checking the first twenty
words of a paragraph to see if they were valid En-
glish words. We used the percentage of punctuation
characters in a paragraph as a simple check for typi-
cal text. We removed duplicated paragraphs using a
hash table. Finally, we obtained a three billion words
corpus of good quality English, which is available at
(Han and Finin, 2013).
2.1.2 Word Co-Occurrence Generation
We performed POS tagging and lemmatization on
the WebBase corpus using the Stanford POS tagger
(Toutanova et al, 2000). Word/term co-occurrences
are counted in a moving window of a fixed size
that scans the entire corpus1. We generated two co-
occurrence models using window sizes ?1 and ?4
because we observed different natures of the models.
?1 window produces a context similar to the depen-
dency context used in (Lin, 1998a). It provides a
more precise context but only works for comparing
words within the same POS. In contrast, a context
window of ?4 words allows us to compute semantic
similarity between words with different POS.
Our word co-occurrence models were based on
a predefined vocabulary of more than 22,000 com-
mon English words and noun phrases. We also
added to it more than 2,000 verb phrases extracted
from WordNet. The final dimensions of our word
co-occurrence matrices are 29,000 ? 29,000 when
words are POS tagged. Our vocabulary includes
only open-class words (i.e. nouns, verbs, adjectives
and adverbs). There are no proper nouns in the vo-
cabulary with the only exception of country names.
1We used a stop-word list consisting of only the three arti-
cles ?a?, ?an? and ?the?.
45
Word Pair ?4 model ?1 model
1. doctor NN, physician NN 0.775 0.726
2. car NN, vehicle NN 0.748 0.802
3. person NN, car NN 0.038 0.024
4. car NN, country NN 0.000 0.016
5. person NN, country NN 0.031 0.069
6. child NN, marry VB 0.098 0.000
7. wife NN, marry VB 0.548 0.274
8. author NN, write VB 0.364 0.128
9. doctor NN, hospital NN 0.473 0.347
10. car NN, driver NN 0.497 0.281
Table 1: Ten examples from the LSA similarity model
2.1.3 SVD Transformation
Singular Value Decomposition (SVD) has been
found to be effective in improving word similar-
ity measures (Landauer and Dumais, 1997). SVD
is typically applied to a word by document ma-
trix, yielding the familiar LSA technique. In
our case we apply it to our word by word ma-
trix. In literature, this variation of LSA is some-
times called HAL (Hyperspace Analog to Lan-
guage) (Burgess et al, 1998).
Before performing SVD, we transform the raw
word co-occurrence count fij to its log frequency
log(fij + 1). We select the 300 largest singular val-
ues and reduce the 29K word vectors to 300 dimen-
sions. The LSA similarity between two words is de-
fined as the cosine similarity of their corresponding
word vectors after the SVD transformation.
2.1.4 LSA Similarity Examples
Ten examples obtained using LSA similarity are
given in Table 1. Examples 1 to 6 illustrate that the
metric has a good property of differentiating simi-
lar words from non-similar words. Examples 7 and
8 show that the ?4 model can detect semantically
similar words even with different POS while the ?1
model yields much worse performance. Example 9
and 10 show that highly related but not substitutable
words can also have a strong similarity but the ?1
model has a better performance in discriminating
them. We call the ?1 model and the ?4 model
as concept similarity and relation similarity respec-
tively since the ?1 model has a good performance
on nouns and the ?4 model is good at computing
similarity between relations regardless of POS of
words, such as ?marry to? and ?is the wife of?.
2.2 Combining with WordNet Knowledge
Statistical word similarity measures have limita-
tions. Related words can have similarity scores as
high as what similar words get, as illustrated by
?doctor? and ?hospital? in Table 1. Word similar-
ity is typically low for synonyms having many word
senses since information about different senses are
mashed together (Han et al, 2013). By using Word-
Net, we can reduce the above issues.
2.2.1 Boosting LSA similarity using WordNet
We increase the similarity between two words if any
of the following relations hold.
? They are in the same WordNet synset.
? One word is the direct hypernym of the other.
? One word is the two-link indirect hypernym of
the other.
? One adjective has a direct similar to relation
with the other.
? One adjective has a two-link indirect similar to
relation with the other.
? One word is a derivationally related form of the
other.
? One word is the head of the gloss of the other
or its direct hypernym or one of its direct hy-
ponyms.
? One word appears frequently in the glosses of
the other and its direct hypernym and its direct
hyponyms.
We use the algorithm described in (Collins, 1999)
to find a word gloss header. We require a minimum
LSA similarity of 0.1 between the two words to filter
out noisy data when extracting WordNet relations.
We define a word?s ?significant senses? to deal
with the problem of WordNet trivial senses. The
word ?year?, for example, has a sense ?a body of
students who graduate together? which makes it a
synonym of the word ?class?. This causes problems
because ?year? and ?class? are not similar, in gen-
eral. A sense is significant, if any of the following
conditions are met: (i) it is the first sense; (ii) its
WordNet frequency count is not less than five; or
(iii) its word form appears first in its synset?s word
46
form list and it has a WordNet sense number less
than eight.
We assign path distance of zero to the category
1, path distance of one to the category 2, 4 and 6,
and path distance of two to the other categories. The
new similarity between word x and y by combining
LSA similarity and WordNet relations is shown in
the following equation
sim?(x, y) = simLSA(x, y) + 0.5e??D(x,y) (1)
where D(x, y) is the minimal path distance between
x and y. Using the e??D(x,y) to transform simple
shortest path length has been demonstrated to be
very effective according to (Li et al, 2003). The pa-
rameter ? is set to be 0.25, following their experi-
mental results. The ceiling of sim?(x, y) remains
1.0 and we simply cut the excess.
2.2.2 Dealing with words of many senses
For a word w with many WordNet senses (currently
ten or more), we use its synonyms with fewer senses
(at most one third of that of w) as its substitutions in
computing similarity with another word. Let Sx and
Sy be the sets of all such substitutions of the words
x and y respectively. The new similarity is obtained
using Equation 2.
sim(x, y) = max( max
sx?Sx?{x}
sim?(sx, y),
max
sy?Sy?{y}
sim?(x, sy)) (2)
An online demonstration of a similar model
developed for the GOR project is available
(UMBC, 2013b), but it lacks some of this version?s
features.
3 Align-and-Penalize Approach
First we hypothesize that STS similarity between
two sentences can be computed using
STS = T ? P ? ? P ?? (3)
where T is the term alignments score, P ? is the
penalty for bad term alignments and P ?? is the
penalty for syntactic contradictions led by the align-
ments. However P ?? had not been fully implemented
and was not used in our STS submissions. We show
it here just for completeness.
3.1 Aligning terms in two sentences
We start by applying the Stanford POS tagger to tag
and lemmatize the input sentences. We use our pre-
defined vocabulary, POS tagging data and simple
regular expressions to recognize multi-word terms
including noun and verb phrases, proper nouns,
numbers and time. We ignore adverbs with fre-
quency count larger than 500, 000 in our corpus and
stop words with general meaning.
Equation 4 shows our aligning function g which
finds the counterpart of term t ? S in sentence S?.
g(t) = argmax
t??S?
sim?(t, t?) (4)
sim?(t, t?) is a wrapper function over sim(x, y) in
Equation 2 that uses the relation similarity model.
It compares numerical and time terms by their val-
ues. If they are equal, 1 is returned; otherwise 0.
sim?(t, t?) provides limited comparison over pro-
nouns. It returns 1 between subject pronouns I, we,
they, he, she and their corresponding object pro-
nouns. sim?(t, t?) also outputs 1 if one term is the
acronym of the other term, or if one term is the head
of the other term, or if two consecutive terms in a
sentence match a single term in the other sentence
(e.g. ?long term? and ?long-term?). sim?(t, t?) fur-
ther adds support for matching words2 not presented
in our vocabulary using a simple string similarity al-
gorithm. It computes character bigram sets for each
of the two words without using padding characters.
Dice coefficient is then applied to get the degree of
overlap between the two sets. If it is larger than two
thirds, sim?(t, t?) returns a score of 1; otherwise 0.
g(t) is direction-dependent and does not achieve
one-to-one mapping. This property is useful in mea-
suring STS similarity because two sentences are of-
ten not exact paraphrases of one another. Moreover,
it is often necessary to align multiple terms in one
sentence to a single term in the other sentence, such
as when dealing with repetitions and anaphora or,
e.g., mapping ?people writing books? to ?writers?.
Let S1 and S2 be the sets of terms in two input
sentences. We define term alignments score T as the
following equation shows.
?
t?S1 sim
?(t, g(t))
2 ? |S1|
+
?
t?S2 sim
?(t, g(t))
2 ? |S2|
(5)
2We use the regular expression ?[A-Za-z][A-Za-z]*? to
identify them.
47
3.2 Penalizing bad term alignments
We currently treat two kinds of alignments as ?bad?,
as described in Equation 6. For the set Bi, we have
an additional restriction that neither of the sentences
has the form of a negation. In defining Bi, we used
a collection of antonyms extracted from WordNet
(Mohammad et al, 2008). Antonym pairs are a spe-
cial case of disjoint sets. The terms ?piano? and ?vi-
olin? are also disjoint but they are not antonyms. In
order to broaden the set Bi we will need to develop
a model that can determine when two terms belong
to disjoint sets.
Ai =
{
?t, g(t)? |t ? Si ? sim?(t, g(t)) < 0.05
}
Bi = {?t, g(t)? |t ? Si ? t is an antonymof g(t)}
i ? {1, 2} (6)
We show how we compute P ? in Equation 7.
PAi =
?
?t,g(t)??Ai (sim
?(t, g(t)) +wf (t) ? wp(t))
2 ? |Si|
PBi =
?
?t,g(t)??Bi (sim
?(t, g(t)) + 0.5)
2 ? |Si|
P ? = PA1 + PB1 + PA2 + PB2 (7)
The wf (t) and wp(t) terms are two weighting func-
tions on the term t. wf (t) inversely weights the log
frequency of term t and wp(t) weights t by its part of
speech tag, assigning 1.0 to verbs, nouns, pronouns
and numbers, and 0.5 to terms with other POS tags.
4 SVM approach
We used the scores from the align-and-penalize ap-
proach along with several other features to learn a
support vector regression model. We started by ap-
plying the following preprocessing steps.
? The sentences were tokenized and POS-tagged
using NLTK?s (Bird, 2006) default Penn Tree-
bank based tagger.
? Punctuation characters were removed from the
tokens except for the decimal point in numbers.
? All numbers written as words were converted
into numerals, e.g., ?2.2 million? was replaced
by ?2200000? and ?fifty six? by ?56?.
? All mentions of time were converted into mil-
itary time, e.g., ?5:40pm? was replaced by
?1740? and ?1h30am? by ?0130?.
? Abbreviations were expanded using a compiled
list of commonly used abbreviations.
? About 80 stopwords were removed.
4.1 Ngram Matching
The sentence similarities are derived as a function of
the similarity scores of their corresponding paired
word ngrams. These features closely resemble the
ones used in (Saric et al, 2012). For our system, we
used unigrams, bigrams, trigrams and skip-bigrams,
a special form of bigrams which allow for arbitrary
distance between two tokens.
An ngram from the first sentence is exclusively
paired with an ngram from the second which has the
highest similarity score. Several similarity metrics
are used to generate different features. For bigrams,
trigrams and skip-bigrams, the similarity score for
two ngrams is computed as the arithmetic mean of
the similarity scores of the individual words they
contain. For example, for the bigrams ?he ate? and
?she spoke?, the similarity score is the average of the
similarity scores between the words ?he? and ?she?
and the words ?ate? and ?spoke?.
The ngram overlap of two sentences is defined
as ?the harmonic mean of the degree to which
the second sentence covers the first and the de-
gree to which the first sentence covers the second?
(Saric et al, 2012). Given sets S1 and S2 containing
ngrams from sentences 1 and 2, and sets P1 and P2
containing their paired ngrams along with their sim-
ilarity scores, the ngram overlap score for a given
ngram type is computed using the following equa-
tion.
HM
(
?
n?P1 w(n).sim(n)
?
n?S1 w(n)
,
?
n?P2 w(n).sim(n)
?
n?S2 w(n)
)
(8)
In this formula, HM is the harmonic mean, w(n) is
the weight assigned for the given ngram and sim(n)
is the similarity score of the paired word.
By default, all the ngrams are assigned a uniform
weight of 1. But since different words carry differ-
ent amount of information, e.g. ?acclimatize? vs.
?take?, ?cardiologist? vs. ?person?, we also use in-
formation content as weights. The information con-
tent of a word is as defined in (Saric et al, 2012).
ic(w) = ln
(
?
w??C freq(w
?
)
freq(w)
)
(9)
48
Here C is the set of words in the corpus and freq(w)
is the frequency of a word in the corpus. The
weight of an ngram is the sum of its constituent word
weights. We use refined versions of Google ngram
frequencies (Michel et al, 2011) from (Mem, 2008)
and (Saric et al, 2012) to get the information con-
tent of the words. Words not in this list are assigned
the average weight.
We used several word similarity metrics for
ngram matching apart from the similarity metric de-
scribed in section 2. Our baseline similarity metric
was an exact string match which assigned a score
of 1 if two tokens contained the same sequence of
characters and 0 otherwise. We also used NLTK?s
library to compute WordNet based similarity mea-
sures such as Path Distance Similarity, Wu-Palmer
Similarity (Wu and Palmer, 1994) and Lin Similar-
ity (Lin, 1998b). For Lin Similarity, the Semcor cor-
pus was used for the information content of words.
4.2 Contrast Scores
We computed contrast scores between two sen-
tences using three different lists of antonym pairs
(Mohammad et al, 2008). We used a large list con-
taining 3.5 million antonym pairs, a list of about
22,000 antonym pairs from Wordnet and a list of
50,000 pairs of words with their degree of contrast.
Contrast scores between two sentences were derived
as a function of the number of antonym pairs be-
tween them similar to equation 8 but with negative
values to indicate contrast scores.
4.3 Features
We constructed 52 features from different combina-
tions of similarity metrics, their parameters, ngram
types (unigram, bigram, trigram and skip-bigram)
and ngram weights (equal weight vs. information
content) for all sentence pairs in the training data.
? We used scores from the align-and-penalize ap-
proach directly as a feature.
? Using exact string match over different ngram
types and ngram weights, we extracted eight
features (4 ? 4). We also developed four addi-
tional features (2 ? 2) by includin stopwords in
bigrams and trigrams, motivated by the nature
of MSRvid dataset.
? We used the LSA boosted similarity metric in
three modes: concept similarity, relation simi-
larity and mixed mode, which used the concept
model for nouns and relation model for verbs,
adverbs and adjectives. A total of 24 features
were extracted (4 ? 2 ? 3).
? For Wordnet-based similarity measures, we
used uniform weights for Path and Wu-Palmer
similarity and used the information content of
words (derived from the Semcor corpus) for
Lin similarity. Skip bigrams were ignored and
a total of nine features were produced (3 ? 3).
? Contrast scores used three different lists of
antonym pairs. A total of six features were ex-
tracted using different weight values (3 ? 2).
4.4 Support Vector Regression
The features described in 4.3 were used in dif-
ferent combinations to train several support vec-
tor regression (SVR) models. We used LIBSVM
(Chang and Lin, 2011) to learn the SVR models and
ran a grid search provided by (Saric et al, 2012) to
find the optimal values for the parameters C , g and
p. These models were then used to predict the scores
for the test sets.
The Galactus system was trained on all of STS
2012 data and used the full set of 52 features. The
FnWN dataset was handled slightly differently from
the others. We observed that terms like ?frame? and
?entity? were used frequently in the five sample sen-
tence pairs and treated them as stopwords. To ac-
commodate the vast difference in sentence lengths,
equation 8 was modified to compute the arithmetic
mean instead of the harmonic mean.
The Saiyan system employed data-specific train-
ing and features. The training sets were subsets of
the supplied STS 2012 dataset. More specifically,
the model for headlines was trained on 3000 sen-
tence pairs from MSRvid and MSRpar, SMT used
1500 sentence pairs from SMT europarl and SMT
news, while OnWN used only the 750 OnWN sen-
tence pairs from last year. The FnWN scores were
directly used from the Align-and-Penalize approach.
None of the models for Saiyan used contrast fea-
tures and the model for SMT also ignored similarity
scores from exact string match metric.
49
5 Results and discussion
Table 2 presents the official results of our three runs
in the 2013 STS task. Each entry gives a run?s Pear-
son correlation on a dataset as well as the rank of the
run among all 89 runs submitted by the 35 teams.
The last row shows the mean of the correlations and
the overall ranks of our three runs.
We tested performance of the align-and-penalize
approach on all of the 2012 STS datasets. It ob-
tained correlation values of 0.819 on MSRvid, 0.669
on MSRpar, 0.553 on SMTeuroparl, 0.567 on SMT-
news and 0.722 on OnWN for the test datasets, and
correlation values of 0.814 on MSRvid, 0.707 on
MSRpar and 0.646 on SMTeuroparl for the training
datasets. The performance of the approach without
using the antonym penalty is also tested, producing
correlation scores of 0.795 on MSRvid, 0.667 on
MSRpar, 0.554 on SMTeuroparl, 0.566 on SMTnew
and 0.727 on OnWN, for the test datasets, and 0.794
on MSRvid, 0.707 on MSRpar and 0.651 on SM-
Teuroparl for the training datasets. The average of
the correlation scores on all eight datasets with and
without the antonym penalty is 0.6871 and 0.6826,
respectively. Since the approach?s performance was
only slightly improved when the antonym penalty
was used, we decided to not include this penalty in
our PairingWords run in the hope that its simplicity
would make it more robust.
During development, our SVM approach
achieved correlations of 0.875 for MSRvid, 0.699
for MSRpar, 0.559 for SMTeuroparl, 0.625 for
SMTnews and 0.729 for OnWN on the 2012 STS
test data. Models were trained on their respective
training sets while SMTnews used SMTeuroparl and
OnWN used all the training sets. We experimented
with different features and training data to study
their influence on the performance of the models.
We found that the unigram overlap feature, based on
boosted LSA similarity and weighted by informa-
tion content, could independently achieve very high
correlations. Including more features improved the
accuracy slightly and in some cases added noise.
The difficulty in selecting data specific features and
training for novel datasets is indicated by Saiyan?s
contrasting performance on headlines and OnWN
datasets. The model used for Headlines was trained
on data from seemingly different domains (MSRvid,
Dataset Pairing Galactus Saiyan
Headlines (750 pairs) 0.7642 (3) 0.7428 (7) 0.7838 (1)
OnWN (561 pairs) 0.7529 (5) 0.7053 (12) 0.5593 (36)
FNWN (189 pairs) 0.5818 (1) 0.5444 (3) 0.5815 (2)
SMT (750 pairs) 0.3804 (8) 0.3705 (11) 0.3563 (16)
Weighted mean 0.6181 (1) 0.5927 (2) 0.5683 (4)
Table 2: Performance of our three systems on the four
test sets.
MSRpar) while OnWN was trained only on OnWN
from STS 2012. When the model for headlines
dataset was used to predict the scores for OnWN,
the correlation jumped from 0.55 to 0.71 indicating
that the earlier model suffered from overfitting.
Overfitting is not evident in the performance of
PairingWords and Galactus, which have more con-
sistent performance over all datasets. The relatively
simple PairingWords system has two advantages: it
is faster, since the current Galactus requires comput-
ing a large number of features; and its performance
is more predictable, since training is not needed thus
eliminating noise induced from diverse training sets.
6 Conclusion
We described three semantic text similarity systems
developed for the *SEM 2013 STS shared task and
the results of the corresponding three runs we sub-
mitted. All of the systems used a lexical similarity
feature that combined POS tagging, LSA word sim-
ilarity and WordNet knowledge.
The first run, which achieved the best mean score
out of all 89 submissions, used a simple term align-
ment algorithm augmented with two penalty met-
rics. The other two runs, ranked second and fourth
out of all submissions, used support vector regres-
sion models based on a set of more than 50 addi-
tional features. The runs differed in their feature
sets, training data and procedures, and parameter
settings.
Acknowledgments
This research was supported by AFOSR award
FA9550-08-1-0265 and a gift from Microsoft.
50
References
[Agirre et al2012] Eneko Agirre, Mona Diab, Daniel Cer,
and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task
6: a pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and Com-
putational Semantics, pages 385?393. Association for
Computational Linguistics.
[Agirre et al2013] Eneko Agirre, Daniel Cer, Mona Diab,
Aitor Gonzalez-Agirre, and Weiwei Guo. 2013. *sem
2013 shared task: Semantic textual similarity, includ-
ing a pilot on typed-similarity. In *SEM 2013: The
Second Joint Conference on Lexical and Computa-
tional Semantics. Association for Computational Lin-
guistics.
[Baroni et al2009] M. Baroni, S. Bernardini, A. Fer-
raresi, and E. Zanchetta. 2009. The wacky wide
web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
[Bird2006] Steven Bird. 2006. Nltk: the natural lan-
guage toolkit. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL ?06,
pages 69?72, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Burgess et al1998] C. Burgess, K. Livesay, and K. Lund.
1998. Explorations in context space: Words, sen-
tences, discourse. Discourse Processes, 25:211?257.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Systems and
Technology, 2:27:1?27:27.
[Coelho et al2004] T.A.S. Coelho, Pa?vel Pereira Calado,
Lamarque Vieira Souza, Berthier Ribeiro-Neto, and
Richard Muntz. 2004. Image retrieval using multiple
evidence ranking. IEEE Trans. on Knowl. and Data
Eng., 16(4):408?417.
[Collins1999] Michael John Collins. 1999. Head-driven
statistical models for natural language parsing. Ph.D.
thesis, University of Pennsylvania.
[Deerwester et al1990] Scott Deerwester, Susan T. Du-
mais, George W. Furnas, Thomas K. Landauer, and
Richard Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(6):391?407.
[Dolan et al2004] Bill Dolan, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction of
large paraphrase corpora: exploiting massively paral-
lel news sources. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, COL-
ING ?04. Association for Computational Linguistics.
[Ganitkevitch et al2013] Juri Ganitkevitch, Ben-
jamin Van Durme, and Chris Callison-Burch. 2013.
PPDB: The paraphrase database. In HLT-NAACL
2013.
[Han and Finin2013] Lushan Han and Tim Finin. 2013.
UMBC webbase corpus. http://ebiq.org/r/351.
[Han et al2012] Lushan Han, Tim Finin, and Anupam
Joshi. 2012. Schema-free structured querying of db-
pedia data. In Proceedings of the 21st ACM interna-
tional conference on Information and knowledge man-
agement, pages 2090?2093. ACM.
[Han et al2013] Lushan Han, Tim Finin, Paul McNamee,
Anupam Joshi, and Yelena Yesha. 2013. Improving
Word Similarity by Augmenting PMI with Estimates
of Word Polysemy. IEEE Transactions on Knowledge
and Data Engineering, 25(6):1307?1322.
[Harris1968] Zellig Harris. 1968. Mathematical Struc-
tures of Language. Wiley, New York, USA.
[Hart1997] M. Hart. 1997. Project gutenberg electronic
books. http://www.gutenberg.org/wiki/Main Page.
[Kauchak and Barzilay2006] David Kauchak and Regina
Barzilay. 2006. Paraphrasing for automatic evalua-
tion. In HLT-NAACL ?06, pages 455?462.
[Landauer and Dumais1997] T. Landauer and S. Dumais.
1997. A solution to plato?s problem: The latent se-
mantic analysis theory of the acquisition, induction,
and representation of knowledge. In Psychological
Review, 104, pages 211?240.
[Li et al2003] Y. Li, Z.A. Bandar, and D. McLean.
2003. An approach for measuring semantic similar-
ity between words using multiple information sources.
IEEE Transactions on Knowledge and Data Engineer-
ing, 15(4):871?882.
[Lin1998a] Dekang Lin. 1998a. Automatic retrieval and
clustering of similar words. In Proc. 17th Int. Conf. on
Computational Linguistics, pages 768?774, Montreal,
CN.
[Lin1998b] Dekang Lin. 1998b. An information-
theoretic definition of similarity. In Proceedings of the
Fifteenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
[Meadow1992] Charles T. Meadow. 1992. Text Informa-
tion Retrieval Systems. Academic Press, Inc.
[Mem2008] 2008. Google word frequency counts.
http://bit.ly/10JdTRz.
[Metzler et al2007] Donald Metzler, Susan Dumais, and
Christopher Meek. 2007. Similarity measures for
short segments of text. In Proceedings of the 29th
European conference on IR research, pages 16?27.
Springer-Verlag.
[Michel et al2011] Jean-Baptiste Michel, Yuan K. Shen,
Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
51
Martin A. Nowak, and Erez L. Aiden. 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176?182, January 14.
[Mihalcea et al2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence, pages 775?780. AAAI Press.
[Miller1995] G.A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):41.
[Mohammad et al2008] Saif Mohammad, Bonnie Dorr,
and Graeme Hirst. 2008. Computing word-pair
antonymy. In Proc. Conf. on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-2008), October.
[Rose et al2002] Tony Rose, Mark Stevenson, and Miles
Whitehead. 2002. The reuters corpus volume 1 - from
yesterdays news to tomorrows language resources. In
In Proceedings of the Third International Conference
on Language Resources and Evaluation, pages 29?31.
[Sahami and Heilman2006] Mehran Sahami and Timo-
thy D. Heilman. 2006. A web-based kernel function
for measuring the similarity of short text snippets. In
Proceedings of the 15th international conference on
World Wide Web, WWW ?06, pages 377?386. ACM.
[Saric et al2012] Frane Saric, Goran Glavas, Mladen
Karan, Jan Snajder, and Bojana Dalbelo Basic. 2012.
Takelab: systems for measuring semantic text simi-
larity. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, pages 441?
448. Association for Computational Linguistics.
[Sriram et al2010] Bharath Sriram, Dave Fuhry, Engin
Demir, Hakan Ferhatosmanoglu, and Murat Demirbas.
2010. Short text classification in twitter to improve
information filtering. In Proceedings of the 33rd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 841?842.
ACM.
[Stanford2001] Stanford. 2001. Stanford WebBase
project. http://bit.ly/WebBase.
[Toutanova et al2000] Kristina Toutanova, Dan
Klein, Christopher Manning, William Mor-
gan, Anna Rafferty, and Michel Galley. 2000.
Stanford log-linear part-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
[UMBC2013a] UMBC. 2013a. Graph of relations
project. http://ebiq.org/j/95.
[UMBC2013b] UMBC. 2013b. Semantic similarity
demonstration. http://swoogle.umbc.edu/SimService/.
[Wu and Palmer1994] Z. Wu and M. Palmer. 1994. Verb
semantic and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-1994), pages 133?138, Las
Cruces (Mexico).
52
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478?484,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joshua 3.0: Syntax-based Machine Translation
with the Thrax Grammar Extractor
Jonathan Weese1, Juri Ganitkevitch1, Chris Callison-Burch1, Matt Post2 and Adam Lopez1,2
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present progress on Joshua, an open-
source decoder for hierarchical and syntax-
based machine translation. The main fo-
cus is describing Thrax, a flexible, open
source synchronous context-free grammar ex-
tractor. Thrax extracts both hierarchical (Chi-
ang, 2007) and syntax-augmented machine
translation (Zollmann and Venugopal, 2006)
grammars. It is built on Apache Hadoop for
efficient distributed performance, and can eas-
ily be extended with support for new gram-
mars, feature functions, and output formats.
1 Introduction
Joshua is an open-source1 toolkit for hierarchical
machine translation of human languages. The origi-
nal version of Joshua (Li et al, 2009) was a reim-
plementation of the Python-based Hiero machine-
translation system (Chiang, 2007); it was later ex-
tended (Li et al, 2010) to support richer formalisms,
such as SAMT (Zollmann and Venugopal, 2006).
The main focus of this paper is to describe this
past year?s work in developing Thrax (Weese, 2011),
an open-source grammar extractor for Hiero and
SAMT grammars. Grammar extraction has shown
itself to be something of a black art, with decod-
ing performance depending crucially on a variety
of features and options that are not always clearly
described in papers. This hindered direct com-
parison both between and within grammatical for-
malisms. Thrax standardizes Joshua?s grammar ex-
1http://github.com/joshua-decoder/joshua
traction procedures by providing a flexible and con-
figurable means of specifying these settings. Sec-
tion 3 presents a systematic comparison of the two
grammars using identical feature sets.
In addition, Joshua now includes a single pa-
rameterized script that implements the entire MT
pipeline, from data preparation to evaluation. This
script is built on top of a module called CachePipe.
CachePipe is a simple wrapper around shell com-
mands that uses SHA-1 hashes and explicitly-
provided lists of dependencies to determine whether
a command needs to be run, saving time both in run-
ning and debugging machine translation pipelines.
2 Thrax: grammar extraction
In modern machine translation systems such as
Joshua (Li et al, 2009) and cdec (Dyer et al, 2010),
a translation model is represented as a synchronous
context-free grammar (SCFG). Formally, an SCFG
may be considered as a tuple
(N,S, T?, T? , G)
where N is a set of nonterminal symbols of the
grammar, S ? N is the goal symbol, T? and T?
are the source- and target-side terminal symbol vo-
cabularies, respectively, and G is a set of production
rules of the grammar.
Each rule in G is of the form
X ? ??, ?,??
where X ? N is a nonterminal symbol, ? is a se-
quence of symbols from N ? T?, ? is a sequence of
478
symbols from N ? T? , and ? is a one-to-one cor-
respondence between the nonterminal symbols of ?
and ?.
The language of an SCFG is a set of ordered pairs
of strings. During decoding, the set of candidate
translations of an input sentence f is the set of all
e such that the pair (f, e) is licensed by the transla-
tion model SCFG. Each candidate e is generated by
applying a sequence of production rules (r1 . . . rn).
The cost of applying each rule is:
w(X ? ??, ??) =
?
i
?i(X ? ??, ??)
?i (1)
where each ?i is a feature function and ?i is the
weight for ?i. The total translation model score of
a candidate e is the product of the rules used in its
derivation. This translation model score is then com-
bined with other features (such as a language model
score) to produce an overall score for each candidate
translation.
2.1 Hiero and SAMT
Throughout this work, we will reference two par-
ticular SCFG types known as Hiero and Syntax-
Augmented Machine Translation (SAMT).
A Hiero grammar (Chiang, 2007) is an SCFG
with only one type of nonterminal symbol, tradi-
tionally labeled X . A Hiero grammar can be ex-
tracted from a parallel corpus of word-aligned sen-
tence pairs as follows: If (f ji , e
l
k) is a sub-phrase
of the sentence pair, we say it is consistent with
the pair?s alignment if none of the words in f ji are
aligned to words outside of elk, and vice-versa. The
consistent sub-phrase may be extracted as an SCFG
rule. Furthermore, if a consistent phrase is contained
within another one, a hierarchical rule may be ex-
tracted by replacing the smaller piece with a nonter-
minal.
An SAMT grammar (Zollmann and Venugopal,
2006) is similar to a Hiero grammar, except that the
nonterminal symbol set is much larger, and its la-
bels are derived from a parse tree over either the
source or target side in the following manner. For
each rule, if the target side is spanned by one con-
stituent of the parse tree, we assign that constituent?s
label as the nonterminal symbol for the rule. Other-
wise, we assign an extended category of the form
C1 + C2, C1/C2, or C2 \C1 ? indicating that the
das begr??e ich sehr .
i
very
much
welcome this
.
PRP
NP
S
RB RB
ADVP
VP
VBP DT
NP
.
Figure 1: An aligned sentence pair.
target side spans two adjacent constituents, is a C1
missing a C2 to the right, or is a C1 missing a C2
on the left, respectively. Table 1 contains a list of
Hiero and SAMT rules extracted from the training
sentence pair in Figure 1.
2.2 System overview
The following were goals in the design of Thrax:
? the ability to extract different SCFGs (such as
Hiero and SAMT), and to adjust various extrac-
tion parameters for the grammars;
? the ability to easily change and extend the fea-
ture sets for each rule
? scalability to arbitrarily large training corpora.
Thrax treats the grammar extraction and scoring
as a series of dependent Hadoop jobs. Hadoop
(Venugopal and Zollmann, 2009) is an implementa-
tion of Google?s MapReduce (Dean and Ghemawat,
2004), a framework for distributed processing of
large data sets. Hadoop jobs have two parts. In the
map step, a set of key/value pairs is mapped to a set
of intermediate key/value pairs. In the reduce step,
all intermediate values associated with an interme-
diate key are merged.
The first step in the Thrax pipeline is to extract all
the grammar rules. The map step in this job takes as
input word-aligned sentence pairs and produces a set
of ordered pairs (r, c) where r is a rule and c is the
number of times it was extracted. During the reduce
step, these rule counts are summed, so the result is
a set of rules, along with the total number of times
each rule was extracted from the entire corpus.
479
Span Hiero SAMT
[1, 3] X ? ?sehr, very much? ADV P ? ?sehr, very much?
[0, 3] X ? ?X sehr, X very much? PRP +ADV P ? ?PRP sehr, PRP very much?
[3, 4] X ? ?begru??e,welcome? V BP ? ?begru??e,welcome?
[0, 6] X ? ?X ich sehr ., i very much X .? S ? ?V P ich sehr ., i very much V P .?
[0, 6] X ? ?X ., X .? S ? ?S/. ., S/. .?
Table 1: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.
Given the rules and their counts, a separate
Hadoop job is run for each feature. These jobs can
all be submitted at once and run in parallel, avoid-
ing the linear sort-and-score workflow. The output
from each feature job is the same set of pairs (r, c)
as the input, except each rule r has been annotated
with some feature score f .
After the feature jobs have been completed, we
have several copies of the grammar, each of which
has been scored with one feature. A final Hadoop
job combines all these scores to produce the final
grammar.
Some users may not have access to a Hadoop
cluster. Thrax can be run in standalone or pseudo-
distributed mode on a single machine. It can also
be used with Amazon Elastic MapReduce,2 a web
service that provides computation time on a Hadoop
cluster on-demand.
2.3 Extraction
The first step in the Thrax workflow is the extraction
of grammar rules from an input corpus. As men-
tioned above, Hiero and SAMT grammars both re-
quire a parallel corpus with word-level alignments.
SAMT additionally requires that the target side of
the corpus be parsed.
There are several parameters that can make a sig-
nificant difference in a grammar?s overall translation
performance. Each of these parameters is easily ad-
justable in Thrax by changing its value in a configu-
ration file.
? maximum rule span
? maximum span of consistent phrase pairs
? maximum number of nonterminals
? minimum number of aligned terminals in rule
2http://aws.amazon.com/elasticmapreduce/
? whether to allow adjacent nonterminals on
source side
? whether to allow unaligned words at the edges
of consistent phrase pairs
Chiang (2007) gives reasonable heuristic choices
for these parameters when extracting a Hiero gram-
mar, and Lopez (2008) confirms some of them (max-
imum rule span of 10, maximum number of source-
side symbols at 5, and maximum number of non-
terminals at 2 per rule). ?) provided comparisons
among phrase-based, hierarchical, and syntax-based
models, but did not report extensive experimentation
with the model parameterizations.
When extracting Hiero- or SAMT-style gram-
mars, the first Hadoop job in the Thrax workflow
takes in a parallel corpus and produces a set of rules.
But in fact Thrax?s extraction mechanism is more
general than that; all it requires is a function that
maps a string to a set of rules. This makes it easy
to implement new grammars and extract them using
Thrax.
2.4 Feature functions
Thrax considers feature functions of two types: first,
there are features that can be calculated by looking
at each rule in isolation. Such features do not re-
quire a Hadoop job to calculate their scores, since
we may inspect the rules in any order. (In practice,
we calculate the scores at the very last moment be-
fore outputting the final grammar.) We call these
features simple features. Thrax implements the fol-
lowing simple features:
? a binary indicator functions denoting:
? whether the rule is purely abstract (i.e.,
has no terminal symbols)
480
? the rule is purely lexical (i.e., has no non-
terminals)
? the rule is monotonic or has reordering
? the rule has adjacent nonterminals on the
source side
? counters for
? the number of unaligned words in the rule
? the number of terminals on the target side
of the rule
? a constant phrase penalty
In addition to simple features, Thrax also imple-
ments map-reduce features. These are features that
require comparing rules in a certain order. Thrax
uses Hadoop to sort the rules efficiently and calcu-
late these feature functions. Thrax implements the
following map-reduce features:
? Phrasal translation probabilities p(?|?) and
p(?|?), calculated with relative frequency:
p(?|?) =
C(?, ?)
C(?)
(2)
(and vice versa), where C(?) is the number of
times a given event was extracted.
? Lexical weighting plex(?|?,A) and
plex(?|?,A). We calculate these weights
as given in (Koehn et al, 2003): let A be the
alignment between ? and ?, so (i, j) ? A if
and only if the ith word of ? is aligned to the
jth word of ?. Then we can define plex(?|?) as
n?
i=1
1
|{j : (i, j) ? A}|
?
(i,j)?A
w(?j |?i) (3)
where ?i is the ith word of ?, ?j is the jth word
of ?, and w(y|x) is the relative frequency of
seeing word y given x.
? Rarity penalty, given by
exp(1? C(X ? ??, ??)) (4)
where again C(?) is a count of the number of
times the rule was extracted.
The above features are all implemented and can
be turned on or off with a keyword in the Thrax con-
figuration file.
It is easy to extend Thrax with new feature func-
tions. For simple features, all that is needed is to im-
plement Thrax?s SIMPLEFEATURE interface defin-
ing a method that takes in a rule and calculates a
feature score. Map-reduce features are slightly more
complex: to subclass MAPREDUCEFEATURE, one
must define a mapper and reducer, but also a sort
comparator to determine in what order the rules are
compared during the reduce step.
2.5 Related work
Joshua includes a simple Hiero extractor (Schwartz
and Callison-Burch, 2010). The extractor runs as a
single Java process, which makes it difficult to ex-
tract larger grammars, since the host machine must
have enough memory to hold all of the rules at once.
Joshua?s extractor scores each rule with three feature
functions ? lexical probabilities in two directions,
and one phrasal probability score p(?|?).
The SAMT implementation of Zollmann and
Venugopal (2006) includes a several-thousand-line
Perl script to extract their rules. In addition to
phrasal and lexical probabilities, this extractor im-
plements several other features that are also de-
scribed in section 2.4.
Finally, the cdec decoder (Dyer et al, 2010) in-
cludes a grammar extractor that performs well only
when all rules can be held in memory.
Memory usage is a limitation of both the Joshua
and cdec extractors. Translation models can be very
large, and many feature scores require accumulation
of statistical data from the entire set of extracted
rules. Since it is impractical to keep the entire gram-
mar in memory, rules are usually sorted on disk and
then read sequentially. Different feature calcula-
tions may require different sort orders, leading to a
linear workflow that alternates between sorting the
grammar and calculating a feature score. To cal-
culate more feature scores, more sorts have to be
performed. This discourages the implementation of
new features. For example, Joshua?s built-in rule ex-
tractor calculates the phrasal probability p(?|?) for
each rule but, to save time, does not calculate its ob-
vious counterpart p(?|?), which would require an-
other sort.
481
Language pair sentences (K) words (M)
cs?en 332 4.7
de?en 279 5.5
en?cs 487 6.9
en?de 359 7.2
en?fr 682 12.5
fr?en 792 14.4
Table 2: Training data size after subsampling.
The SAMT extractor does not have a problem
with large data sets; SAMT can run on Hadoop, as
Thrax does.
The Joshua and cdec extractors only extract Hiero
grammars, and Zollmann and Venugopal?s extractor
can only extract SAMT-style grammars. They are
not designed to score arbitrary feature sets, either.
Since variation in translation models and feature sets
can have a significant effect on translation perfor-
mance, we have developed Thrax in order to make it
easy to build and test new models.
3 Experiments
We built systems for six language pairs for the WMT
2011 shared task: cz-en, en-cz, de-en, en-de, fr-en,
and en-fr.3 For each language pair, we built both
SAMT and hiero grammars.4 Table 3 contains the
results on the complete WMT 2011 test set.
To train the translation models, we used the pro-
vided Europarl and news commentary data. For cz-
en and en-cz, we also used sections of the CzEng
parallel corpus (Bojar and Z?abokrtsky?, 2009). The
parallel data was subsampled using Joshua?s built-
in subsampler to select sentences with n-grams rel-
evant to the tuning and test set. We used SRILM
to train a 5-gram language model with Kneser-Ney
smoothing using the appropriate side of the paral-
lel data. For the English LM, we also used English
Gigaword Fourth Edition.5
Before extracting an SCFG with Thrax, we used
the provided Perl scripts to tokenize and normalize
3fr=French, cz=Czech, de=German, en=English.
4Except for fr-en and en-fr. We were unable to decode with
SAMT grammars for these language pairs due to their large size.
We have since resolved this issue and will have scores for the
final version of the paper.
5LDC2009T13
pair hiero SAMT improvement
cz-en 21.1 21.7 +0.6
en-cz 16.8 16.9 +0.1
de-en 18.9 19.5 +0.6
en-de 14.3 14.9 +0.6
fr-en 28.0 - -
en-fr 30.4 - -
Table 3: Single-reference BLEU-4 scores.
the data. We also removed any sentences longer than
50 tokens (after tokenization). For SAMT grammar
extraction, we parsed the English training data us-
ing the Berkeley Parser (Petrov et al, 2006) with the
provided Treebank-trained grammar.
We tuned the model weights against the
WMT08 test set (news-test2008) using Z-
MERT (Zaidan, 2009), an implementation of mini-
mum error-rate training included with Joshua. We
decoded the test set to produce a 300-best list of
unique translations, then chose the best candidate for
each sentence using Minimum Bayes Risk reranking
(Kumar and Byrne, 2004). Figure 2 shows an exam-
ple derivation with an SAMT grammar. To re-case
the 1-best test set output, we trained a true-case 5-
gram language model using the same LM training
data as before, and used an SCFG translation model
to translate from the lowercased to true-case output.
The translation model used rules limited to five to-
kens in length, and contained no hierarchical rules.
4 CachePipe: Cached pipeline runs
Machine translation pipelines involve the specifica-
tion and execution of many different datasets, train-
ing procedures, and pre- and post-processing tech-
niques that can have large effects on translation out-
come, and which make direct comparisons between
systems difficult. The complexity of managing these
pipelines and experimental environments has led to a
number of different experimental management sys-
tems, such as Experiment.perl,6 Joshua 2.0?s Make-
file system (Li et al, 2010), and LoonyBin (Clark
and Lavie, 2010). In addition to managing the
pipeline, these scripts employ different techniques
to avoid expensive recomputation by caching steps.
6http://www.statmt.org/moses/?n=
FactoredTraining.EMS
482
the
reactor type will be operated with uranium
VBN
DT+NP
GLUE
VP
PP
der reaktortyp , das nicht
angereichert
wird zwar mit uran betrieben
, which is
not
enriched
ist .
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
.
S
VBN
DT+NP
GLUE
VP
PP
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
S
Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals
and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.
However, these approaches are based on simple but
unreliable heuristics (such as timestamps or file ex-
istence) to make the caching determination.
Our solution to the caching dependency problem
is CachePipe. CachePipe is designed with the fol-
lowing goals: (1) robust content-based dependency
checking and (2) ease of use, including minimal
editing of existing scripts. CachePipe is essentially
a wrapper around command invocations. Presented
with a command to run and a list of file dependen-
cies, it computes SHA-1 hashes of the dependencies
and of the command invocation and stores them; the
command is executed only if any of those hashes are
different from previous runs. A basic invocation in-
volves specifying (1) a name or identifier associated
with the command or step, (2) the command to run,
and (3) a list of file dependencies. For example, to
copy file a to b from a shell prompt, the following
command could be used:
cachecmd copy "cp a b" a b
The first time the command is run, the file would be
copied; afterwards, the command would be skipped
after CachePipe verified that the contents of the de-
pendencies a and b had not changed.
CachePipe is open-source software, distributed
with Joshua or available separately.7 It currently
provides both a shell script interface and a program-
matic API for Perl. It accepts a number of other
arguments and dependency types. It also serves as
the foundation of a new script in Joshua 3.0 that im-
plements the complete Joshua pipeline, from data
preparation to evaluation.
5 Future work
Thrax is currently limited to SCFG-based translation
models. A natural development would be to extract
GHKM grammars (Galley et al, 2004) or more re-
cent tree-to-tree models (Zhang et al, 2008; Liu et
al., 2009; Chiang, 2010). We also hope that Thrax
will continue to be extended with more feature func-
tions as researchers develop and contribute them.
Acknowledgements
This research was supported by in part by the Eu-
roMatrixPlus project funded by the European Com-
mission (7th Framework Programme), and by the
NSF under grant IIS-0713448. Opinions, interpre-
tations, and conclusions are the authors? alone.
7https://github.com/joshua-decoder/
cachepipe
483
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, Uppsala, Sweden,
July.
Jonathan H. Clark and Alon Lavie. 2010. Loony-
bin: Keeping language technologists sane through au-
tomated management of experimental (hyper) work-
flows. In Proc. LREC.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In OSDI.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc.
ACL 2010 System Demonstrations, pages 7?12.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
ACL, Suntec, Singapore, August.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. COLING, Manchester, UK,
August.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL, Sydney, Aus-
tralia, July.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua:
Suffix arrays and prefix trees. The Prague Bulletin of
Mathematical Linguistics, 93:157?166, January.
Mark Steedman. 1999. Alternating quantifier scope in
ccg. In Proc. ACL, Stroudsburg, PA, USA.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67?78.
Jonathan Weese. 2011. A systematic comparison of syn-
chronous context-free grammars for machine transla-
tion. Master?s thesis, Johns Hopkins University, May.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. NAACL Workshop on Statistcal Machine Trans-
lation, New York, New York.
484
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 75?78,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Processing Informal, Romanized Pakistani Text Messages
Ann Irvine and Jonathan Weese and Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Abstract
Regardless of language, the standard character
set for text messages (SMS) and many other
social media platforms is the Roman alphabet.
There are romanization conventions for some
character sets, but they are used inconsistently
in informal text, such as SMS. In this work, we
convert informal, romanized Urdu messages
into the native Arabic script and normalize
non-standard SMS language. Doing so pre-
pares the messages for existing downstream
processing tools, such as machine translation,
which are typically trained on well-formed,
native script text. Our model combines infor-
mation at the word and character levels, al-
lowing it to handle out-of-vocabulary items.
Compared with a baseline deterministic ap-
proach, our system reduces both word and
character error rate by over 50%.
1 Introduction
There are many reasons why systematically process-
ing informal text, such as Twitter posts or text mes-
sages, could be useful. For example, during the Jan-
uary 2010 earthquake in Haiti, volunteers translated
Creole text messages that survivors sent to English
speaking relief workers. Machine translation (MT)
could supplement or replace such crowdsourcing ef-
forts in the future. However, working with SMS data
presents several challenges. First, messages may
have non-standard spellings and abbreviations (?text
speak?), which we need to normalize into standard
language. Second, many languages that are typically
written in a non-Roman script use a romanized ver-
sion for SMS, which we need to deromanize. Nor-
malizing and deromanizing SMS messages would
allow us to use existing MT engines, which are typ-
ically trained on well-formed sentences written in
their native-script, in order to translate the messages.
With this work, we use and release a corpus of
1 million (4, 195 annotated) anonymized text mes-
sages sent in Pakistan1. We deromanize and normal-
ize messages written in Urdu, although the general
approach is language-independent. Using Mechan-
ical Turk (MTurk), we collect normalized Arabic
script annotations of romanized messages in order to
both train and evaluate a Hidden Markov Model that
automates the conversion. Our model drastically
outperforms our baseline deterministic approach and
its performance is comparable to the agreement be-
tween annotators.
2 Related Work
There is a strong thread of research dedicated to nor-
malizing Twitter and SMS informal English (Sproat
et al, 2001). Choudhury et al (2007) use a super-
vised English SMS dataset and build a character-
level HMM to normalize individual tokens. Aw et
al. (2006) model the same task using a statistical MT
system, making the output context-sensitive at the
cost of including a character-level analysis. More
recently, Han and Baldwin (2011) use unsupervised
methods to build a pipeline that identifies ill-formed
English SMS word tokens and builds a dictionary
of their most likely normalized forms. Beaufort et
al. (2010) use a large amount of training data to su-
pervise an FST-based French SMS normalizer. Li
and Yarowsky (2008) present methods that take ad-
vantage of monolingual distributional similarities to
identify the full form of abbreviated Chinese words.
One challenge in working with SMS data is that pub-
lic data is sparse (Chen and Kan, 2011). Translit-
eration is well-studied (Knight and Graehl, 1997;
Haizhou et al, 2004; Li et al, 2010) and is usually
viewed as a subproblem of MT.
With this work, we release a corpus of SMS mes-
sages and attempt to normalize Urdu SMS texts. Do-
ing so involves the same challenges as normalizing
English SMS texts and has the added complexity
that we must also deromanize, a process similar to
the transliteration task.
1See http://www.cs.jhu.edu/?anni/papers/
urduSMS/ for details about obtaining the corpus.
75
!"#$#%&'()*++&$*! ,#-./(0&1&%($&#2(13(4&5&5('3$6(7$4&(1*(8&"1&#(13("1#(1*9(:1&'3(+1&2&+1(8&"1('39();<=>?@=!
"#$%&#%'! ()*&!
+',-./#$01#2.$! !"#$!!"#$!%&!'#(!)%*!+!#,-*!%&!'(")*+!%&!'&,!%&!+!%./! 0#1#2!-*+!%*!
3$%4056!7)#$54#2.$! 86')'!#)'!9.&!:'.:4';!5''/5!'<')9.$'!05!5=&*90$%>!.?!5=&*9!0=5!%..*!
Figure 1: Example of SMS with MTurk annotations
3 Data and Annotation
Our Pakistani SMS dataset was provided by the
Transnational Crisis Project, and it includes 1 mil-
lion (724,999 unique) text messages that were sent
in Pakistan just prior to the devastating July 2010
floods. The messages have been stripped of all
metadata including sender, receiver, and timestamp.
Messages are written in several languages, though
most are in Urdu, English, or a combination of the
two. Regardless of language, all messages are com-
posed in the Roman alphabet. The dataset contains
348,701 word types, 49.5% of which are singletons.
We posted subsets of the SMS data to MTurk to
perform language identification, followed by dero-
manization and normalization on Urdu messages.
In the deromanization and normalization task, we
asked MTurk workers to convert all romanized
words into script Urdu and use full, non-abbreviated
word forms. We applied standard techniques for
eliminating noise in the annotation set (Callison-
Burch and Dredze, 2010) and limited annotators to
those in Pakistan. We also asked annotators to in-
dicate if a message contained private, sensitive, or
offensive material, and we removed such messages
from our dataset.
We gathered deromanization and normalization
MTurk annotations for 4,195 messages. In all ex-
periments, we use 3,695 of our annotated SMS texts
for training and 500 for testing. We found that 18%
of word tokens and 44% of word types in the test
data do not appear in the training data. An example
of a fully annotated SMS is shown in Figure 1.
Figure 2 shows that, in general, productive MTurk
annotators also tend to produce high quality annota-
tions, as measured by an additional round of MTurk
annotations which asked workers to choose the best
annotation among the three we gathered. The raw
average annotator agreements as measured by char-
acter and word level edit distance are 40.5 and 66.9,
respectively. However, the average edit distances
0 100 200 300 400
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of MTurk HITs completed
P
e
r
c
e
n
t
 
o
f
 
a
n
n
o
t
a
t
i
o
n
s
 
v
o
t
e
d
 
b
e
s
t
Good Performance
Mediocre Performance
Poor Performance
Figure 2: Productivity vs. percent of annotations voted
best among three deromanizations gathered on MTurk.
!"#$%&' ()*+,$-+.),/'01#2342,"56' 7,89$/:'
'!"#' ;#+*'0<6';+#+*'0<6';+#*'0=6';+#2*'0>6' 8#+"2'
'$%&'#'' ;),/+'0?6';,'/+'0@6';,/+'0<6';),'/+'0=6' A:$":'
'()"*)+'' B4/#+'0>6'!"#$0>6'B)/+#)'0>6'B4/#),'0>6' )&:2#'%2)%92'
','-'' ;:4/:'0<=6';$'0>6';:4/'0>6' :+%%5'
''./0'' C48,)'0>6'8+,C)'0>6' D#2E5'
'123$4'' F+&2$,'0G6'F+&2,'0G6'F++&2$,'0@6'F+&+$,'0@6'F&2$,'0<6' ":$&H":+&'
Figure 3: Urdu words romanized in multiple ways. The
Urdu word for ?2? is pronounced approximately ?du.?
between ?good? MTurk workers (at least 50% of a
worker?s messages are voted best) and the deroman-
ization which was voted best (when the two are dif-
ferent) are 25.1 (character) and 53.7 (word).
We used an automatic aligner to align the words
in each Arabic script annotation to words in the orig-
inal romanized message. The alignments show an
average fertility of 1.04 script words per romanized
word. Almost all alignments are one-to-one and
monotonic. Since there is no reordering, the align-
ment is a simplified case of word alignment in MT.
Using the aligned dataset, we examine how Urdu
words are romanized. The average entropy for non-
singleton script word tokens is 1.49 bits. This means
it is common for script words to be romanized in
multiple ways (4.2 romanizations per script word on
average). Figure 3 shows some examples.
4 Deromanization and Normalization
In order to deromanize and normalize Urdu SMS
texts in a single step, we use a Hidden Markov
Model (HMM), shown in Figure 4. To estimate the
probability that one native-script word follows an-
76
???? ??? ???????
walo kia soratehal
Figure 4: Illustration of HMM with an example from
SMS data. English translation: ?What?s the situation??
other, we use a bigram language model (LM) with
add-1 smoothing (Lidstone, 1920) and compare two
sources of LM training data.
We use two sources of data to estimate the prob-
ability of a romanized word given a script word:
(1) a dictionary of candidates generated from auto-
matically aligned training data, (2) a character-based
transliteration model (Irvine et al, 2010).
If r is a romanized word and u is a script Urdu
word, the dictionary-based distribution, pDICT(r|u),
is given by relative frequency estimations over the
aligned training data, and the transliteration-based
distribution, pTRANS(r|u), is defined by the transliter-
ation model scores. We define the model?s emission
probability distribution as the linear interpolation of
these two distributions:
pe(r|u) = (1? ?)pDICT(r|u) + ?pTRANS(r|u)
When ? = 0, the model uses only the dictionary,
and when ? = 1 only the transliterations.
Intuitively, we want the dictionary-based model to
memorize patterns like abbreviations in the training
data and then let the transliterator take over when a
romanized word is out-of-vocabulary (OOV).
5 Results and discussion
In the eight experiments summarized in Table 1, we
vary the following: (1) whether we estimate HMM
emissions from the dictionary, the transliterator, or
both (i.e., we vary ?), (2) language model training
data, and (3) transliteration model training data.
Our baseline uses an Urdu-extension of the Buck-
walter Arabic deterministic transliteration map.
Even our worst-performing configuration outper-
forms this baseline by a large margin, and the best
configuration has a performance comparable to the
agreement among good MTurk workers.
LM Translit ? CER WER
1 News ? Dict 41.5 63.3
2 SMS ? Dict 38.2 57.1
3 SMS Eng Translit 33.4 76.2
4 SMS SMS Translit 33.3 74.1
5 News SMS Both 29.0 58.1
6 News Eng Both 28.4 57.2
7 SMS SMS Both 25.0 50.1
8 SMS Eng Both 24.4 49.5
Baseline: Buckwalter Determ. 64.6 99.9
Good MTurk Annotator Agreement 25.1 53.7
Table 1: Deromanization and normalization results on
500 SMS test set. Evaluation is by character (CER) and
word error rate (WER); lower scores are better. ?LM?
indicates the data used to estimate the language model
probabilities: News refers to Urdu news corpus and SMS
to deromanized side of our SMS training data. ?Translit?
column refers to the training data that was used to train
the transliterator: SMS; SMS training data; Eng; English-
Urdu transliterations. ? refers to the data used to estimate
emissions: transliterations, dictionary entries, or both.
Unsurprisingly, using the dictionary only (Exper-
iments 1-2) performs better than using translitera-
tions only (Experiments 3-4) in terms of word error
rate, and the opposite is true in terms of character
error rate. Using both the dictionary derived from
the SMS training data and the transliterator (Experi-
ments 5?8) outperforms using only one or the other
(1?4). This confirms our intuition that using translit-
eration to account for OOVs in combination with
word-level learning from the training data is a good
strategy2.
We compare results using two language model
training corpora: (1) the Urdu script side of our
SMS MTurk data, and (2) the Urdu side of an Urdu-
English parallel corpus,3 which contains news-
domain text. We see that using the SMS MTurk data
(7?8) outperforms the news text (5?6). This is due to
the fact that the news text is out of domain with re-
spect to the content of SMS texts. In future work, we
plan to mine Urdu script blog and chat data, which
may be closer in domain to the SMS texts, providing
better language modeling probabilities.
2We experimented with different ? values on held out data
and found its value did not impact system performance signifi-
cantly unless it was set to 0 or 1, ignoring the transliterations or
dictionary. We set ? = 0.5 for the rest of the experiments.
3LDC2006E110
77
Training Freq. bins Length Diff. bins
Bin CER WER Bin CER WER
100+ 9.8 14.8 0 23.5 43.3
10?99 15.2 22.1 1, 2 29.1 48.7
1?9 27.5 37.2 -1, -2 42.3 70.1
0 73.5 96.6 ?3 100.3 100.0
?-3 66.4 87.3
Table 2: Results on tokens in the test set, binned by train-
ing frequency or difference in character length with their
reference. Length differences are number of characters
in romanized token minus the number of characters in its
deromanization. ? = 0.5 for all.
We compare using a transliterator trained on ro-
manized/deromanized word pairs extracted from the
SMS text training data with a transliterator trained
on English words paired with their Urdu translitera-
tions and find that performance is nearly equivalent.
The former dataset is noisy, small, and in-domain
while the latter is clean, large, and out-of-domain.
We expect that the SMS word pairs based translit-
erator would outperform the English-Urdu trained
transliterator given more, cleaner data.
To understand in more detail when our system
does well and when it does not, we performed ad-
ditional experiments on the token level. That is, in-
stead of deromanizing and normalizing entire SMS
messages, we take a close look at the kinds of ro-
manized word tokens that the system gets right and
wrong. We bin test set word tokens by their frequen-
cies in the training data and by the difference be-
tween their length (in characters) and the length of
their reference deromanization. Results are given in
Table 2. Not surprisingly, the system performs better
on tokens that it has seen many times in the training
data than on tokens it has never seen. It does not
perform perfectly on high frequency items because
the entropy of many romanized word types is high.
The system also performs best on romanized word
types that have a similar length to their deromanized
forms. This suggests that the system is more suc-
cessful at the deromanization task than the normal-
ization task, where lengths are more likely to vary
substantially due to SMS abbreviations.
6 Summary
We have defined a new task: deromanizing and nor-
malizing SMS messages written in non-native Ro-
man script. We have introduced a unique new anno-
tated dataset that allows exploration of informal text
for a low resource language.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING/ACL.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normaliz-
ing SMS messages. In Proceedings of ACL.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In NAACL-NLT Workshop on Creating Speech
and Language Data With Mechanical Turk.
Tao Chen and Min-Yen Kan. 2011. Creating a live, pub-
lic short message service corpus: The NUS SMS cor-
pus. Computation and Language, abs/1112.2468.
Monojit Choudhury, Vijit Jain Rahul Saraf, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. In International Journal on Docu-
ment Analysis and Recognition.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of ACL.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
Proceedings of ACL.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
Proceedings of the Association for Machine Transla-
tion in the America, AMTA ?10.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of ACL.
Zhifei Li and David Yarowsky. 2008. Unsupervised
translation induction for Chinese abbreviations using
monolingual corpora. In Proceedings of ACL/HLT.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Per-
vouchine. 2010. Report of NEWS 2010 translitera-
tion generation shared task. In Proceedings of the ACL
Named Entities WorkShop.
George James Lidstone. 1920. Note on the general case
of the Bayes-Laplace formula for inductive or a poste-
riori probabilities. Transactions of the Faculty of Ac-
tuaries, 8:182?192.
Richard Sproat, Alan W. Black, Stanley F. Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech & Language, pages 287?333.
78
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 222?231,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Using Categorial Grammar to Label Translation Rules
Jonathan Weese and Chris Callison-Burch and Adam Lopez
Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
Adding syntactic labels to synchronous
context-free translation rules can improve
performance, but labeling with phrase struc-
ture constituents, as in GHKM (Galley et al,
2004), excludes potentially useful translation
rules. SAMT (Zollmann and Venugopal,
2006) introduces heuristics to create new
non-constituent labels, but these heuristics
introduce many complex labels and tend to
add rarely-applicable rules to the translation
grammar. We introduce a labeling scheme
based on categorial grammar, which allows
syntactic labeling of many rules with a mini-
mal, well-motivated label set. We show that
our labeling scheme performs comparably to
SAMT on an Urdu?English translation task,
yet the label set is an order of magnitude
smaller, and translation is twice as fast.
1 Introduction
The Hiero model of Chiang (2007) popularized
the usage of synchronous context-free grammars
(SCFGs) for machine translation. SCFGs model
translation as a process of isomorphic syntactic
derivation in the source and target language. But the
Hiero model is formally, not linguistically syntactic.
Its derivation trees use only a single non-terminal la-
bel X , carrying no linguistic information. Consider
Rule 1.
X ? ? maison ; house ? (1)
We can add syntactic information to the SCFG
rules by parsing the parallel training data and pro-
jecting parse tree labels onto the spans they yield and
their translations. For example, if house was parsed
as a noun, we could rewrite Rule 1 as
N ? ? maison ; house ?
But we quickly run into trouble: how should we
label a rule that translates pour l?e?tablissement de
into for the establishment of? There is no phrase
structure constituent that corresponds to this English
fragment. This raises a model design question: what
label do we assign to spans that are natural trans-
lations of each other, but have no natural labeling
under a syntactic parse? One possibility would be
to discard such translations from our model as im-
plausible. However, such non-compositional trans-
lations are important in translation (Fox, 2002), and
they have been repeatedly shown to improve trans-
lation performance (Koehn et al, 2003; DeNeefe et
al., 2007).
Syntax-Augmented Machine Translation (SAMT;
Zollmann and Venugopal, 2006) solves this prob-
lem with heuristics that create new labels from the
phrase structure parse: it labels for the establish-
ment of as IN+NP+IN to show that it is the con-
catenation of a noun phrase with a preposition on
either side. While descriptive, this label is unsatis-
fying as a concise description of linguistic function,
fitting uneasily alongside more natural labels in the
phrase structure formalism. SAMT introduces many
thousands of such labels, most of which are seen
very few times. While these heuristics are effective
(Zollmann et al, 2008), they inflate grammar size,
hamper effective parameter estimation due to feature
sparsity, and slow translation speed.
Our objective is to find a syntactic formalism that
222
enables us to label most translation rules without re-
lying on heuristics. Ideally, the label should be small
in order to improve feature estimation and reduce
translation time. Furthering an insight that informs
SAMT, we show that combinatory categorial gram-
mar (CCG) satisfies these requirements.
Under CCG, for the establishment of is labeled
with ((S\NP)\(S\NP))/NP. This seems complex, but
it describes exactly how the fragment should com-
bine with other English words to create a complete
sentence in a linguistically meaningful way. We
show that CCG is a viable formalism to add syntax
to SCFG-based translation.
? We introduce two models for labeling SCFG
rules. One uses labels from a 1-best CCG parse
tree of training data; the second uses the top la-
bels in each cell of a CCG parse chart.
? We show that using 1-best parses performs as
well as a syntactic model using phrase structure
derivations.
? We show that using chart cell labels per-
forms almost as well than SAMT, but the non-
terminal label set is an order of magnitude
smaller and translation is twice as fast.
2 Categorial grammar
Categorial grammar (CG) (Adjukiewicz, 1935; Bar-
Hillel et al, 1964) is a grammar formalism in
which words are assigned grammatical types, or cat-
egories. Once categories are assigned to each word
of a sentence, a small set of universal combinatory
rules uses them to derive a sentence-spanning syn-
tactic structure.
Categories may be either atomic, like N, VP, S,
and other familiar types, or they may be complex
function types. A function type looks like A/B and
takes an argument of type B and returns a type A.
The categories A and B may themselves be either
primitives or functions. A lexical item is assigned a
function category when it takes an argument ? for
example, a verb may be function that needs to be
combined with its subject and object, or an a adjec-
tive may be a function that takes the noun it modifies
as an argument.
Lexical item Category
and conj
cities NP
in (NP\NP)/NP
own (S\NP)/NP
properties NP
they NP
various NP/NP
villages NP
Table 1: An example lexicon, mapping words to cat-
egories.
We can combine two categories with function ap-
plication. Formally, we write
X/Y Y ? X (2)
to show that a function type may be combined with
its argument type to produce the result type. Back-
ward function application also exists, where the ar-
gument occurs to the left of the function.
Combinatory categorial grammar (CCG) is an ex-
tension of CG that includes more combinators (op-
erations that can combine categories). Steedman
and Baldridge (2011) give an excellent overview of
CCG.
As an example, suppose we want to analyze the
sentence ?They own properties in various cities and
villages? using the lexicon shown in Table 1. We as-
sign categories according to the lexicon, then com-
bine the categories using function application and
other combinators to get an analysis of S for the
complete sentence. Figure 1 shows the derivation.
As a practical matter, very efficient CCG parsers
are available (Clark and Curran, 2007). As shown
by Fowler and Penn (2010), in many cases CCG is
context-free, making it an ideal fit for our problem.
2.1 Labels for phrases
Consider the German?English phrase pair der gro?e
Mann ? the tall man. It is easily labeled as an NP
and included in the translation table. By contrast,
der gro?e? the tall, doesn?t typically correspond to
a complete subtree in a phrase structure parse. Yet
translating the tall is likely to be more useful than
translating the tall man, since it is more general?it
can be combined with any other noun translation.
223
They own properties in various cities and villages
NP (S\NP )/NP NP (NP\NP )/NP NP/NP NP conj NP
> <?>
NP NP\NP
<
NP
>
NP\NP
<
NP
>
S\NP
<
S
Figure 1: An example CCG derivation for the sentence ?They own properties in various cities and villages?
using the lexicon from Table 1. ? indicates a conjunction operation; > and < are forward and backward
function application, respectively.
Using CG-style labels with function types, we can
assign the type (for example) NP/N to the tall to
show that it can be combined with a noun on its right
to create a complete noun phrase.1 In general, CG
can produce linguistically meaningful labels of most
spans in a sentence simply as a matter of course.
2.2 Minimal, well-motivated label set
By allowing slashed categories with CG, we in-
crease the number of labels allowed. Despite the in-
crease in the number of labels, CG is advantageous
for two reasons:
1. Our labels are derived from CCG derivations,
so phrases with slashed labels represent well-
motivated, linguistically-informed derivations,
and the categories can be naturally combined.
2. The set of labels is small, relative to SAMT ?
it?s restricted to the labels seen in CCG parses
of the training data.
In short, using CG labels allows us to keep more
linguistically-informed syntactic rules without mak-
ing the set of syntactic labels too big.
3 Translation models
3.1 Extraction from parallel text
To extract SCFG rules, we start with a heuristic to
extract phrases from a word-aligned sentence pair
1We could assign NP/N to the determiner the and N/N to the
adjective tall, then combine those two categories using function
composition to get a category NP/N for the two words together.
For
most
people
,
P
o
u
r
l
a
m
a
j
o
r
i
t
?
d
e
s
g
e
n
s
,
Figure 2: A word-aligned sentence pair fragment,
with a box indicating a consistent phrase pair.
(Tillmann, 2003). Figure 2 shows a such a pair, with
a consistent phrase pair inside the box. A phrase
pair (f, e) is said to be consistent with the alignment
if none of the words of f are aligned outside the
phrase e, and vice versa ? that is, there are no align-
ment points directly above, below, or to the sides of
the box defined by f and e.
Given a consistent phrase pair, we can immedi-
ately extract the rule
X ? ?f, e? (3)
as we would in a phrase-based MT system. How-
ever, whenever we find a consistent phrase pair that
is a sub-phrase of another, we may extract a hierar-
chical rule by treating the inner phrase as a gap in
the larger phrase. For example, we may extract the
rule
X ? ? Pour X ; For X ? (4)
from Figure 3.
224
For
most
people
,
P
o
u
r
l
a
m
a
j
o
r
i
t
?
d
e
s
g
e
n
s
,
Figure 3: A consistent phrase pair with a sub-phrase
that is also consistent. We may extract a hierarchical
SCFG rule from this training example.
The focus of this paper is how to assign labels
to the left-hand non-terminal X and to the non-
terminal gaps on the right-hand side. We discuss five
models below, of which two are novel CG-based la-
beling schemes.
3.2 Baseline: Hiero
Hiero (Chiang, 2007) uses the simplest labeling pos-
sible: there is only one non-terminal symbol, X , for
all rules. Its advantage over phrase-based translation
in its ability to model phrases with gaps in them,
enabling phrases to reorder subphrases. However,
since there?s only one label, there?s no way to in-
clude syntactic information in its translation rules.
3.3 Phrase structure parse tree labeling
One first step for adding syntactic information is to
get syntactic labels from a phrase structure parse
tree. For each word-aligned sentence pair in our
training data, we also include a parse tree of the tar-
get side.
Then we can assign syntactic labels like this: for
each consistent phrase pair (representing either the
left-hand non-terminal or a gap in the right hand
side) we see if the target-language phrase is the exact
span of some subtree of the parse tree.
If a subtree exactly spans the phrase pair, we can
use the root label of that subtree to label the non-
terminal symbol. If there is no such subtree, we
throw away any rules derived from the phrase pair.
As an example, suppose the English side of the
phrase pair in Figure 3 is analyzed as
PP
IN
For
NP
JJ
most
NN
people
Then we can assign syntactic labels to Rule 4 to pro-
duce
PP ? ? Pour NP ; For NP ? (5)
The rules extracted by this scheme are very sim-
ilar to those produced by GHKM (Galley et al,
2004), in particular resulting in the ?composed
rules? of Galley et al (2006), though we use sim-
pler heuristics for handling of unaligned words and
scoring in order to bring the model in line with both
Hiero and SAMT baselines. Under this scheme we
throw away a lot of useful translation rules that don?t
translate exact syntactic constituents. For example,
we can?t label
X ? ? Pour la majorite? des ; For most ? (6)
because no single node exactly spans For most: the
PP node includes people, and the NP node doesn?t
include For.
We can alleviate this problem by changing the
way we get syntactic labels from parse trees.
3.4 SAMT
The Syntax-Augmented Machine Translation
(SAMT) model (Zollmann and Venugopal, 2006)
extracts more rules than the other syntactic model
by allowing different labels for the rules. In SAMT,
we try several different ways to get a label for a
span, stopping the first time we can assign a label:
? As in simple phrase structure labeling, if a sub-
tree of the parse tree exactly spans a phrase, we
assign that phrase the subtree?s root label.
? If a phrase can be covered by two adjacent sub-
trees with labels A and B, we assign their con-
catenation A+B.
? If a phrase spans part of a subtree labeled A that
could be completed with a subtree B to its right,
we assign A/B.
225
? If a phrase spans part of a subtree A but is miss-
ing a B to its left, we assign A\B.
? Finally, if a phrase spans three adjacent sub-
trees with labels A, B, and C, we assign
A+B+C.
Only if all of these assignments fail do we throw
away the potential translation rule.
Under SAMT, we can now label Rule 6. For is
spanned by an IN node, and most is spanned by a JJ
node, so we concatenate the two and label the rule
as
IN+JJ? ? Pour la majorite? des ; For most ? (7)
3.5 CCG 1-best derivation labeling
Our first CG model is similar to the first phrase struc-
ture parse tree model. We start with a word-aligned
sentence pair, but we parse the target sentence using
a CCG parser instead of a phrase structure parser.
When we extract a rule, we see if the consistent
phrase pair is exactly spanned by a category gener-
ated in the 1-best CCG derivation of the target sen-
tence. If there is such a category, we assign that cat-
egory label to the non-terminal. If not, we throw
away the rule.
To continue our extended example, suppose the
English side of Figure 3 was analyzed by a CCG
parser to produce
For most people
(S/S)/N N/N N
>
N
>
S/S
Then just as in the phrase structure model, we
project the syntactic labels down onto the extractable
rule yielding
S/S ? ? Pour N ; For N ? (8)
This does not take advantage of CCG?s ability to
label almost any fragment of language: the frag-
ments with labels in any particular sentence depend
on the order that categories were combined in the
sentence?s 1-best derivation. We can?t label Rule 6,
because no single category spanned For most in the
derivation. In the next model, we increase the num-
ber of spans we can label.
S/S
S/S N
(S/S)/N N/N N
For peoplemost
Figure 4: A portion of the parse chart for a sentence
starting with ?For most people . . . .? Note that the
gray chart cell is not included in the 1-best derivation
of this fragment in Section 3.5.
3.6 CCG parse chart labeling
For this model, we do not use the 1-best CCG deriva-
tion. Instead, when parsing the target sentence, for
each cell in the parse chart, we read the most likely
label according to the parsing model. This lets us as-
sign a label for almost any span of the sentence just
by reading the label from the parse chart.
For example, Figure 4 represents part of a CCG
parse chart for our example fragment of ?For most
people.? Each cell in the chart shows the most prob-
able label for its span. The white cells of the chart
are in fact present in the 1-best derivation, which
means we could extract Rule 8 just as in the previous
model.
But the 1-best derivation model cannot label Rule
6, and this model can. The shaded chart cell in Fig-
ure 4 holds the most likely category for the span For
most. So we assign that label to the X:
S/S ? ? Pour la majorite? des ; For most ? (9)
By including labels from cells that weren?t used
in the 1-best derivation, we can greatly increase the
number of rules we can label.
4 Comparison of resulting grammars
4.1 Effect of grammar size and label set on
parsing efficiency
There are sound theoretical reasons for reducing the
number of non-terminal labels in a grammar. Trans-
lation with a synchronous context-free grammar re-
quires first parsing with the source-language projec-
tion of the grammar, followed by intersection of the
226
target-language projection of the resulting grammar
with a language model. While there are many possi-
ble algorithms for these operations, they all depend
on the size of the grammar.
Consider for example the popular cube pruning
algorithm of Chiang (2007), which is a simple ex-
tension of CKY. It works by first constructing a set
of items of the form ?A, i, j?, where each item corre-
sponds to (possibly many) partial analyses by which
nonterminal A generates the sequence of words from
positions i through j of the source sentence. It then
produces an augmented set of items ?A, i, j, u, v?, in
which items of the first type are augmented with left
and right language model states u and v. In each
pass, the number of items is linear in the number of
nonterminal symbols of the grammar. This observa-
tion has motivated work in grammar transformations
that reduce the size of the nonterminal set, often re-
sulting in substantial gains in parsing or translation
speed (Song et al, 2008; DeNero et al, 2009; Xiao
et al, 2009).
More formally, the upper bound on parsing com-
plexity is always at least linear in the size of the
grammar constant G, where G is often loosely de-
fined as a grammar constant; Iglesias et al (2011)
give a nice analysis of the most common translation
algorithms and their dependence on G. Dunlop et
al. (2010) provide a more fine-grained analysis of G,
showing that for a variety of implementation choices
that it depends on either or both the number of rules
in the grammar and the number of nonterminals in
the grammar. Though these are worst-case analyses,
it should be clear that grammars with fewer rules or
nonterminals can generally be processed more effi-
ciently.
4.2 Number of rules and non-terminals
Table 2 shows the number of rules we can extract
under various labeling schemes. The rules were ex-
tracted from an Urdu?English parallel corpus with
202,019 translations, or almost 2 million words in
each language.
As we described before, moving from the phrase-
structure syntactic model to the extended SAMT
model vastly increases the number of translation
rules ? from about 7 million to 40 million rules.
But the increased rule coverage comes at a cost: the
non-terminal set has increased in size from 70 (the
Model Rules NTs
Hiero 4,171,473 1
Syntax 7,034,393 70
SAMT 40,744,439 18,368
CG derivations 8,042,683 505
CG parse chart 28,961,161 517
Table 2: Number of translation rules and non-
terminal labels in an Urdu?English grammar under
various models.
size of the set of Penn Treebank tags) to over 18,000.
Comparing the phrase structure syntax model to
the 1-best CCG derivation model, we see that the
number of extracted rules increases slightly, and the
grammar uses a set of about 500 non-terminal labels.
This does not seem like a good trade-off; since we
are extracting from the 1-best CCG derivation there
really aren?t many more rules we can label than with
a 1-best phrase structure derivation.
But when we move to the full CCG parse chart
model, we see a significant difference: when read-
ing labels off of the entire parse chart, instead of
the 1-best derivation, we don?t see a significant in-
crease in the non-terminal label set. That is, most
of the labels we see in parse charts of the train-
ing data already show up in the top derivations: the
complete chart doesn?t contain many new labels that
have never been seen before.
But by using the chart cells, we are able to as-
sign syntactic information to many more translation
rules: over 28 million rules, for a grammar about 34
the size of SAMT?s. The parse chart lets us extract
many more rules without significantly increasing the
size of the syntactic label set.
4.3 Sparseness of nonterminals
Examining the histograms in Figure 5 gives us a
different view of the non-terminal label sets in our
models. In each histogram, the horizontal axis mea-
sures label frequency in the corpus. The height of
each bar shows the number of non-terminals with
that frequency.
For the phrase structure syntax model, we see
there are maybe 20 labels out of 70 that show up
on rules less than 1000 times. All the other labels
show up on very many rules.
227
More sparse Less sparse
1 10 102 103 104 105 106
Label Frequency (logscale)
1
10
1
10
1
10
1
10
102
103
N
um
be
r
of
L
ab
el
s
(l
og
sc
al
e)
Phrase structure
CCG 1-best
CCG chart
SAMT
Figure 5: Histograms of label frequency for each model, illustrating the sparsity of each model.
Moving to SAMT, with its heuristically-defined
labels, shows a very different story. Not only does
the model have over 18,000 non-terminal labels, but
thousands of them show up on fewer than 10 rules
apiece. If we look at the rare label types, we see that
a lot of them are improbable three way concatena-
tions A+B+C.
The two CCG models have similar sparseness
profiles. We do see some rare labels occurring only
a few times in the grammars, but the number of
singleton labels is an order of magnitude smaller
than SAMT. Most of the CCG labels show up in
the long tail of very common occurrences. Interest-
ingly, when we move to extracting labels from parse
charts rather than derivations, the number of labels
increases only slightly. However, we also obtain a
great deal more evidence for each observed label,
making estimates more reliable.
5 Experiments
5.1 Data
We tested our models on an Urdu?English transla-
tion task, in which syntax-based systems have been
quite effective (Baker et al, 2009; Zollmann et al,
2008). The training corpus was the National Insti-
tute of Standards and Technology Open Machine
Translation 2009 Evaluation (NIST Open MT09).
According to the MT09 Constrained Training Con-
ditions Resources list2 this data includes NIST Open
MT08 Urdu Resources3 and the NIST Open MT08
Current Test Set Urdu?English4. This gives us
202,019 parallel translations, for approximately 2
million words of training data.
5.2 Experimental design
We used the scripts included with the Moses MT
toolkit (Koehn et al, 2007) to tokenize and nor-
malize the English data. We used a tokenizer and
normalizer developed at the SCALE 2009 workshop
(Baker et al, 2009) to preprocess the Urdu data. We
used GIZA++ (Och and Ney, 2000) to perform word
alignments.
For phrase structure parses of the English data, we
used the Berkeley parser (Petrov and Klein, 2007).
For CCG parses, and for reading labels out of a parse
chart, we used the C&C parser (Clark and Curran,
2007).
After aligning and parsing the training data, we
used the Thrax grammar extractor (Weese et al,
2011) to extract all of the translation grammars.
We used the same feature set in all the transla-
tion grammars. This includes, for each rule C ?
?f ; e?, relative-frequency estimates of the probabil-
2http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf
3LDC2009E12
4LDC2009E11
228
Model BLEU sec./sent.
Hiero 25.67 (0.9781) 0.05
Syntax 27.06 (0.9703) 3.04
SAMT 28.06 (0.9714) 63.48
CCG derivations 27.3 (0.9770) 5.24
CCG parse chart 27.64 (0.9673) 33.6
Table 3: Results of translation experiments on
Urdu?English. Higher BLEU scores are better.
BLEU?s brevity penalty is reported in parentheses.
ities p(f |A), p(f |e), p(f |e,A), p(e|A), p(e|f), and
p(e|f,A).
The feature set alo includes lexical weighting for
rules as defined by Koehn et al (2003) and various
binary features as well as counters for the number of
unaligned words in each rule.
To train the feature weights we used the Z-MERT
implementation (Zaidan, 2009) of the Minimum
Error-Rate Training algorithm (Och, 2003).
To decode the test sets, we used the Joshua ma-
chine translation decoder (Weese et al, 2011). The
language model is a 5-gram LM trained on English
GigaWord Fourth Edition.5
5.3 Evaluation criteria
We measure machine translation performance using
the BLEU metric (Papineni et al, 2002). We also
report the translation time for the test set in seconds
per sentence. These results are shown in Table 3.
All of the syntactic labeling schemes show an im-
provement over the Hiero model. Indeed, they all
fall in the range of approximately 27?28 BLEU. We
can see that the 1-best derivation CCG model per-
forms slightly better than the phrase structure model,
and the CCG parse chart model performs a little bet-
ter than that. SAMT has the highest BLEU score.
The models with a larger number of rules perform
better; this supports our assertion that we shouldn?t
throw away too many rules.
When it comes to translation time, the three
smaller models (Hiero, phrase structure syntax, and
CCG 1-best derivations) are significantly faster than
the two larger ones. However, even though the CCG
parse chart model is almost 34 the size of SAMT in
terms of number of rules, it doesn?t take 34 of the
5LDC2009T13
time. In fact, it takes only half the time of the SAMT
model, thanks to the smaller rule label set.
6 Discussion and Future Work
Finding an appropriate mechanism to inform phrase-
based translation models and their hierarchical vari-
ants with linguistic syntax is a difficult problem
that has attracted intense interest, with a variety
of promising approaches including unsupervised
clustering (Zollmann and Vogel, 2011), merging
(Hanneman et al, 2011), and selection (Mylonakis
and Sima?an, 2011) of labels derived from phrase-
structure parse trees very much like those used by
our baseline systems. What we find particularly
attractive about CCG is that it naturally assigns
linguistically-motivated labels to most spans of a
sentence using a reasonably concise label set, possi-
bility obviating the need for further refinement. In-
deed, the analytical flexibility of CCG has motivated
its increasing use in MT, from applications in lan-
guage modeling (Birch et al, 2007; Hassan et al,
2007) to more recent proposals to incorporate it into
phrase-based (Mehay, 2010) and hierarchical trans-
lation systems (Auli, 2009).
Our new model builds on these past efforts, rep-
resenting a more fully instantiated model of CCG-
based translation. We have shown that the label
scheme allows us to keep many more translation
rules than labels based on phrase structure syntax,
extracting almost as many rules as the SAMT model,
but keeping the label set an order of magnitude
smaller, which leads to more efficient translation.
This simply scratches the surface of possible uses of
CCG in translation. In future work, we plan to move
from a formally context-free to a formally CCG-
based model of translation, implementing combina-
torial rules such as application, composition, and
type-raising.
Acknowledgements
Thank you to Michael Auli for providing code to
inspect the full chart from the C&C parser. This
research was supported in part by the NSF under
grant IIS-0713448 and in part by the EuroMatrix-
Plus project funded by the European Commission
(7th Framework Programme). Opinions, interpreta-
tions, and conclusions are the authors? alone.
229
References
Kazimierz Adjukiewicz. 1935. Die syntaktische kon-
nexita?t. In Storrs McCall, editor, Polish Logic 1920?
1939, pages 207?231. Oxford University Press.
Michael Auli. 2009. CCG-based models for statisti-
cal machine translation. Ph.D. Proposal, University
of Edinburgh.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,
Mike Kayser, Lori Levin, Justin Marinteau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation: Fi-
nal report of the 2009 summer camp for advanced lan-
guage exploration (scale). Technical report, Human
Language Technology Center of Excellence.
Yehoshua Bar-Hillel, Chaim Gaifman, and Eliyahu
Shamir. 1964. On categorial and phrase-structure
grammars. In Yehoshua Bar-Hillel, editor, Language
and Information, pages 99?115. Addison-Wesley.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proc. of WMT.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics, 33(4).
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proc. EMNLP.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In
Proc. NAACL, pages 227?235.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2010. Reducing the grammar constant: an analysis
of CYK parsing efficiency. Technical report CSLU-
2010-02, OHSU.
Timothy A. D. Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory categorial
grammar. In Proc. ACL, pages 335?344.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich
syntactic translation models. In In ACL, pages 961?
968.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proc. of WMT.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proc. of ACL.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchi-
cal phrase-based translation representations. In Proc.
EMNLP, pages 1373?1383.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Frederico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL Demonstration Session.
Dennis Mehay. 2010. Linguistically motivated syntax
for machine translation. Ph.D. Proposal, Ohio State
University.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proc. of ACL-HLT.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. HLT-NAACL.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In Proc.
EMNLP, pages 167?176.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Kersti
Bo?rjars, editors, Non-Transformational Syntax. Wiley-
Blackwell.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proc. of
EMNLP.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. In Proc. of WMT.
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu, and
Ming Zhou. 2009. Better synchronous binarization
for machine translation. In Proc. EMNLP, pages 362?
370.
230
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91(1):79?88.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proc. of ACL-HLT.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proc. of COLING.
231
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 283?291,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joshua 4.0: Packing, PRO, and Paraphrases
Juri Ganitkevitch1, Yuan Cao1, Jonathan Weese1, Matt Post2, and Chris Callison-Burch1
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present Joshua 4.0, the newest version
of our open-source decoder for parsing-based
statistical machine translation. The main con-
tributions in this release are the introduction
of a compact grammar representation based
on packed tries, and the integration of our
implementation of pairwise ranking optimiza-
tion, J-PRO. We further present the exten-
sion of the Thrax SCFG grammar extractor
to pivot-based extraction of syntactically in-
formed sentential paraphrases.
1 Introduction
Joshua is an open-source toolkit1 for parsing-based
statistical machine translation of human languages.
The original version of Joshua (Li et al, 2009) was
a reimplementation of the Python-based Hiero ma-
chine translation system (Chiang, 2007). It was later
extended to support grammars with rich syntactic
labels (Li et al, 2010a). More recent efforts in-
troduced the Thrax module, an extensible Hadoop-
based extraction toolkit for synchronous context-
free grammars (Weese et al, 2011).
In this paper we describe a set of recent exten-
sions to the Joshua system. We present a new com-
pact grammar representation format that leverages
sparse features, quantization, and data redundancies
to store grammars in a dense binary format. This al-
lows for both near-instantaneous start-up times and
decoding with extremely large grammars. In Sec-
tion 2 we outline our packed grammar format and
1joshua-decoder.org
present experimental results regarding its impact on
decoding speed, memory use and translation quality.
Additionally, we present Joshua?s implementation
of the pairwise ranking optimization (Hopkins and
May, 2011) approach to translation model tuning.
J-PRO, like Z-MERT, makes it easy to implement
new metrics and comes with both a built-in percep-
tron classifier and out-of-the-box support for widely
used binary classifiers such as MegaM and Max-
Ent (Daume? III and Marcu, 2006; Manning and
Klein, 2003). We describe our implementation in
Section 3, presenting experimental results on perfor-
mance, classifier convergence, and tuning speed.
Finally, we introduce the inclusion of bilingual
pivoting-based paraphrase extraction into Thrax,
Joshua?s grammar extractor. Thrax?s paraphrase ex-
traction mode is simple to use, and yields state-of-
the-art syntactically informed sentential paraphrases
(Ganitkevitch et al, 2011). The full feature set of
Thrax (Weese et al, 2011) is supported for para-
phrase grammars. An easily configured feature-level
pruning mechanism allows to keep the paraphrase
grammar size manageable. Section 4 presents de-
tails on our paraphrase extraction module.
2 Compact Grammar Representation
Statistical machine translation systems tend to per-
form better when trained on larger amounts of bilin-
gual parallel data. Using tools such as Thrax, trans-
lation models and their parameters are extracted
and estimated from the data. In Joshua, translation
models are represented as synchronous context-free
grammars (SCFGs). An SCFG is a collection of
283
rules {ri} that take the form:
ri = Ci ? ??i, ?i,?i, ~?i?, (1)
where left-hand side Ci is a nonterminal symbol, the
source side ?i and the target side ?i are sequences
of both nonterminal and terminal symbols. Further,
?i is a one-to-one correspondence between the non-
terminal symbols of ?i and ?i, and ~?i is a vector of
features quantifying the probability of ?i translat-
ing to ?i, as well as other characteristics of the rule
(Weese et al, 2011). At decoding time, Joshua loads
the grammar rules into memory in their entirety, and
stores them in a trie data structure indexed by the
rules? source side. This allows the decoder to effi-
ciently look up rules that are applicable to a particu-
lar span of the (partially translated) input.
As the size of the training corpus grows, so does
the resulting translation grammar. Using more di-
verse sets of nonterminal labels ? which can signifi-
cantly improve translation performance ? further ag-
gravates this problem. As a consequence, the space
requirements for storing the grammar in memory
during decoding quickly grow impractical. In some
cases grammars may become too large to fit into the
memory on a single machine.
As an alternative to the commonly used trie struc-
tures based on hash maps, we propose a packed trie
representation for SCFGs. The approach we take is
similar to work on efficiently storing large phrase
tables by Zens and Ney (2007) and language mod-
els by Heafield (2011) and Pauls and Klein (2011) ?
both language model implementations are now inte-
grated with Joshua.
2.1 Packed Synchronous Tries
For our grammar representation, we break the SCFG
up into three distinct structures. As Figure 1 in-
dicates, we store the grammar rules? source sides
{?i}, target sides {?i}, and feature data {~?i} in sep-
arate formats of their own. Each of the structures
is packed into a flat array, and can thus be quickly
read into memory. All terminal and nonterminal
symbols in the grammar are mapped to integer sym-
bol id?s using a globally accessible vocabulary map.
We will now describe the implementation details for
each representation and their interactions in turn.
2.1.1 Source-Side Trie
The source-side trie (or source trie) is designed
to facilitate efficient lookup of grammar rules by
source side, and to allow us to completely specify a
matching set of rule with a single integer index into
the trie. We store the source sides {?i} of a grammar
in a downward-linking trie, i.e. each trie node main-
tains a record of its children. The trie is packed into
an array of 32-bit integers. Figure 1 illustrates the
composition of a node in the source-side trie. All
information regarding the node is stored in a con-
tiguous block of integers, and decomposes into two
parts: a linking block and a rule block.
The linking block stores the links to the child trie
nodes. It consists of an integer n, the number of chil-
dren, and n blocks of two integers each, containing
the symbol id aj leading to the child and the child
node?s address sj (as an index into the source-side
array). The children in the link block are sorted by
symbol id, allowing for a lookup via binary or inter-
polation search.
The rule block stores all information necessary to
reconstruct the rules that share the source side that
led to the current source trie node. It stores the num-
ber of rules, m, and then a tuple of three integers
for each of the m rules: we store the symbol id of
the left-hand side, an index into the target-side trie
and a data block id. The rules in the data block are
initially in an arbitrary order, but are sorted by ap-
plication cost upon loading.
2.1.2 Target-Side Trie
The target-side trie (or target trie) is designed to
enable us to uniquely identify a target side ?i with a
single pointer into the trie, as well as to exploit re-
dundancies in the target side string. Like the source
trie, it is stored as an array of integers. However,
the target trie is a reversed, or upward-linking trie:
a trie node retains a link to its parent, as well as the
symbol id labeling said link.
As illustrated in Figure 1, the target trie is ac-
cessed by reading an array index from the source
trie, pointing to a trie node at depth d. We then fol-
low the parent links to the trie root, accumulating
target side symbols gj into a target side string gd1 as
we go along. In order to match this traversal, the tar-
get strings are entered into the trie in reverse order,
i.e. last word first. In order to determine d from a
284
# children
# rules
child symbol
child address
rule left-hand side
target address
data block id
n ?
m ?
a
j
s
j
+
1
C
j
t
j
b
j
.
.
.
.
.
.
n
m
.
.
.
.
.
.
parent symbol
parent address
g
j
t
j-1
.
.
.
.
.
.
# features
feature id
feature value
n ?
f
j
v
j
.
.
.
n
.
.
.
.
.
Feature block 
index
Feature byte
buffer
Target trie
array
Source trie
array
f
j
.
.
.
.
.
.
Quantization
b
j
.
.
.
.
.
.
f
j
q
j
Figure 1: An illustration of our packed grammar data structures. The source sides of the grammar rules are
stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries
for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full
target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain
a data block id, which identifies feature data attached to the rule. The features are encoded according to a
type/quantization specification and stored as variable-length blocks of data in a byte buffer.
pointer into the target trie, we maintain an offset ta-
ble in which we keep track of where each new trie
level begins in the array. By first searching the offset
table, we can determine d, and thus know how much
space to allocate for the complete target side string.
To further benefit from the overlap there may be
among the target sides in the grammar, we drop the
nonterminal labels from the target string prior to in-
serting them into the trie. For richly labeled gram-
mars, this collapses all lexically identical target sides
that share the same nonterminal reordering behavior,
but vary in nonterminal labels into a single path in
the trie. Since the nonterminal labels are retained in
the rules? source sides, we do not lose any informa-
tion by doing this.
2.1.3 Features and Other Data
We designed the data format for the grammar
rules? feature values to be easily extended to include
other information that we may want to attach to a
rule, such as word alignments, or locations of occur-
rences in the training data. In order to that, each rule
ri has a unique block id bi associated with it. This
block id identifies the information associated with
the rule in every attached data store. All data stores
are implemented as memory-mapped byte buffers
that are only loaded into memory when actually re-
quested by the decoder. The format for the feature
data is detailed in the following.
The rules? feature values are stored as sparse fea-
tures in contiguous blocks of variable length in a
byte buffer. As shown in Figure 1, a lookup table
is used to map the bi to the index of the block in the
buffer. Each block is structured as follows: a sin-
gle integer, n, for the number of features, followed
by n feature entries. Each feature entry is led by an
integer for the feature id fj , and followed by a field
of variable length for the feature value vj . The size
of the value is determined by the type of the feature.
Joshua maintains a quantization configuration which
maps each feature id to a type handler or quantizer.
After reading a feature id from the byte buffer, we
retrieve the responsible quantizer and use it to read
the value from the byte buffer.
Joshua?s packed grammar format supports Java?s
standard primitive types, as well as an 8-bit quan-
tizer. We chose 8 bit as a compromise between
compression, value decoding speed and transla-
285
Grammar Format Memory
Hiero (43M rules)
Baseline 13.6G
Packed 1.8G
Syntax (200M rules)
Baseline 99.5G
Packed 9.8G
Packed 8-bit 5.8G
Table 1: Decoding-time memory use for the packed
grammar versus the standard grammar format. Even
without lossy quantization the packed grammar rep-
resentation yields significant savings in memory
consumption. Adding 8-bit quantization for the real-
valued features in the grammar reduces even large
syntactic grammars to a manageable size.
tion performance (Federico and Bertoldi, 2006).
Our quantization approach follows Federico and
Bertoldi (2006) and Heafield (2011) in partitioning
the value histogram into 256 equal-sized buckets.
We quantize by mapping each feature value onto the
weighted average of its bucket. Joshua allows for an
easily per-feature specification of type. Quantizers
can be share statistics across multiple features with
similar value distributions.
2.2 Experiments
We assess the packed grammar representation?s
memory efficiency and impact on the decoding
speed on the WMT12 French-English task. Ta-
ble 1 shows a comparison of the memory needed
to store our WMT12 French-English grammars at
runtime. We can observe a substantial decrease in
memory consumption for both Hiero-style gram-
mars and the much larger syntactically annotated
grammars. Even without any feature value quantiza-
tion, the packed format achieves an 80% reduction
in space requirements. Adding 8-bit quantization
for the log-probability features yields even smaller
grammar sizes, in this case a reduction of over 94%.
In order to avoid costly repeated retrievals of indi-
vidual feature values of rules, we compute and cache
the stateless application cost for each grammar rule
at grammar loading time. This, alongside with a lazy
approach to rule lookup allows us to largely avoid
losses in decoding speed.
Figure shows a translation progress graph for the
WMT12 French-English development set. Both sys-
 0 500 1000 1500 2000 2500
 0  500  1000  1500  2000  2500Sentences Translated Seconds PassedStandardPacked
Figure 2: A visualization of the loading and decod-
ing speed on the WMT12 French-English develop-
ment set contrasting the packed grammar represen-
tation with the standard format. Grammar loading
for the packed grammar representation is substan-
tially faster than that for the baseline setup. Even
with a slightly slower decoding speed (note the dif-
ference in the slopes) the packed grammar finishes
in less than half the time, compared to the standard
format.
tems load a Hiero-style grammar with 43 million
rules, and use 16 threads for parallel decoding. The
initial loading time for the packed grammar repre-
sentation is dramatically shorter than that for the
baseline setup (a total of 176 seconds for loading and
sorting the grammar, versus 1897 for the standard
format). Even though decoding speed is slightly
slower with the packed grammars (an average of 5.3
seconds per sentence versus 4.2 for the baseline), the
effective translation speed is more than twice that of
the baseline (1004 seconds to complete decoding the
2489 sentences, versus 2551 seconds with the stan-
dard setup).
3 J-PRO: Pairwise Ranking Optimization
in Joshua
Pairwise ranking optimization (PRO) proposed by
(Hopkins and May, 2011) is a new method for dis-
criminative parameter tuning in statistical machine
translation. It is reported to be more stable than the
popular MERT algorithm (Och, 2003) and is more
scalable with regard to the number of features. PRO
treats parameter tuning as an n-best list reranking
problem, and the idea is similar to other pairwise
ranking techniques like ranking SVM and IR SVMs
286
(Li, 2011). The algorithm can be described thusly:
Let h(c) = ?w,?(c)? be the linear model score
of a candidate translation c, in which ?(c) is the
feature vector of c and w is the parameter vector.
Also let g(c) be the metric score of c (without loss
of generality, we assume a higher score indicates a
better translation). We aim to find a parameter vector
w such that for a pair of candidates {ci, cj} in an n-
best list,
(h(ci)? h(cj))(g(ci)? g(cj)) =
?w,?(ci)??(cj)?(g(ci)? g(cj)) > 0,
namely the order of the model score is consistent
with that of the metric score. This can be turned into
a binary classification problem, by adding instance
??ij = ?(ci)??(cj)
with class label sign(g(ci) ? g(cj)) to the training
data (and symmetrically add instance
??ji = ?(cj)??(ci)
with class label sign(g(cj) ? g(ci)) at the same
time), then using any binary classifier to find the w
which determines a hyperplane separating the two
classes (therefore the performance of PRO depends
on the choice of classifier to a large extent). Given
a training set with T sentences, there are O(Tn2)
pairs of candidates that can be added to the training
set, this number is usually much too large for effi-
cient training. To make the task more tractable, PRO
samples a subset of the candidate pairs so that only
those pairs whose metric score difference is large
enough are qualified as training instances. This fol-
lows the intuition that high score differential makes
it easier to separate good translations from bad ones.
3.1 Implementation
PRO is implemented in Joshua 4.0 named J-PRO.
In order to ensure compatibility with the decoder
and the parameter tuning module Z-MERT (Zaidan,
2009) included in all versions of Joshua, J-PRO is
built upon the architecture of Z-MERT with sim-
ilar usage and configuration files(with a few extra
lines specifying PRO-related parameters). J-PRO in-
herits Z-MERT?s ability to easily plug in new met-
rics. Since PRO allows using any off-the-shelf bi-
nary classifiers, J-PRO provides a Java interface that
enables easy plug-in of any classifier. Currently, J-
PRO supports three classifiers:
? Perceptron (Rosenblatt, 1958): the percep-
tron is self-contained in J-PRO, no external re-
sources required.
? MegaM (Daume? III and Marcu, 2006): the clas-
sifier used by Hopkins and May (2011).2
? Maximum entropy classifier (Manning and
Klein, 2003): the Stanford toolkit for maxi-
mum entropy classification.3
The user may specify which classifier he wants to
use and the classifier-specific parameters in the J-
PRO configuration file.
The PRO approach is capable of handling a large
number of features, allowing the use of sparse dis-
criminative features for machine translation. How-
ever, Hollingshead and Roark (2008) demonstrated
that naively tuning weights for a heterogeneous fea-
ture set composed of both dense and sparse features
can yield subpar results. Thus, to better handle the
relation between dense and sparse features and pro-
vide a flexible selection of training schemes, J-PRO
supports the following four training modes. We as-
sume M dense features and N sparse features are
used:
1. Tune the dense feature parameters only, just
like Z-MERT (M parameters to tune).
2. Tune the dense + sparse feature parameters to-
gether (M +N parameters to tune).
3. Tune the sparse feature parameters only with
the dense feature parameters fixed, and sparse
feature parameters scaled by a manually speci-
fied constant (N parameters to tune).
4. Tune the dense feature parameters and the scal-
ing factor for sparse features, with the sparse
feature parameters fixed (M+1 parameters to
tune).
J-PRO supports n-best list input with a sparse fea-
ture format which enumerates only the firing fea-
tures together with their values. This enables a more
compact feature representation when numerous fea-
tures are involved in training.
2hal3.name/megam
3nlp.stanford.edu/software
287
0 10 20 300
10
20
30
40
Iteration
BLE
U
Dev set MT03 (10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT04(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT05(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Dev set MT03 (1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT04(1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT05(1026 features)
 
 
PercepMegaMMax?Ent
Figure 3: Experimental results on the development and test sets. The x-axis is the number of iterations (up to
30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper
row: results trained using only dense features (10 features); Lower row: results trained using dense+sparse
features (1026 features). Left column: development set (MT03); Middle column: test set (MT04); Right
column: test set (MT05).
Datasets Z-MERT
J-PRO
Percep MegaM Max-Ent
Dev (MT03) 32.2 31.9 32.0 32.0
Test (MT04) 32.6 32.7 32.7 32.6
Test (MT05) 30.7 30.9 31.0 30.9
Table 2: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).
3.2 Experiments
We did our experiments using J-PRO on the NIST
Chinese-English data, and BLEU score was used as
the quality metric for experiments reported in this
section.4 The experimental settings are as the fol-
lowing:
Datasets: MT03 dataset (998 sentences) as devel-
opment set for parameter tuning, MT04 (1788 sen-
tences) and MT05 (1082 sentences) as test sets.
Features: Dense feature set include the 10 regular
features used in the Hiero system; Sparse feature set
4We also experimented with other metrics including TER,
METEOR and TER-BLEU. Similar trends as reported in this
section were observed. These results are omitted here due to
limited space.
includes 1016 target-side rule POS bi-gram features
as used in (Li et al, 2010b).
Classifiers: Perceptron, MegaM and Maximum
entropy.
PRO parameters: ? = 8000 (number of candidate
pairs sampled uniformly from the n-best list), ? = 1
(sample acceptance probability), ? = 50 (number of
top candidates to be added to the training set).
Figure 3 shows the BLEU score curves on the
development and test sets as a function of itera-
tions. The upper and lower rows correspond to
the results trained with 10 dense features and 1026
dense+sparse features respectively. We intentionally
selected very bad initial parameter vectors to verify
the robustness of the algorithm. It can be seen that
288
with each iteration, the BLEU score increases mono-
tonically on both development and test sets, and be-
gins to converge after a few iterations. When only 10
features are involved, all classifiers give almost the
same performance. However, when scaled to over a
thousand features, the maximum entropy classifier
becomes unstable and the curve fluctuates signifi-
cantly. In this situation MegaM behaves well, but
the J-PRO built-in perceptron gives the most robust
performance.
Table 2 compares the results of running Z-MERT
and J-PRO. Since MERT is not able to handle nu-
merous sparse features, we only report results for
the 10-feature setup. The scores for both setups
are quite close to each other, with Z-MERT doing
slightly better on the development set but J-PRO
yielding slightly better performance on the test set.
4 Thrax: Grammar Extraction at Scale
4.1 Translation Grammars
In previous years, our grammar extraction methods
were limited by either memory-bounded extractors.
Moving towards a parallelized grammar extraction
process, we switched from Joshua?s formerly built-
in extraction module to Thrax for WMT11. How-
ever, we were limited to a simple pseudo-distributed
Hadoop setup. In a pseudo-distributed cluster, all
tasks run on separate cores on the same machine
and access the local file system simultaneously, in-
stead of being distributed over different physical ma-
chines and harddrives. This setup proved unreliable
for larger extractions, and we were forced to reduce
the amount of data that we used to train our transla-
tion models.
For this year, however, we had a permanent clus-
ter at our disposal, which made it easy to extract
grammars from all of the available WMT12 data.
We found that on a properly distributed Hadoop
setup Thrax was able to extract both Hiero gram-
mars and the much larger SAMT grammars on the
complete WMT12 training data for all tested lan-
guage pairs. The runtimes and resulting (unfiltered)
grammar sizes for each language pair are shown in
Table 3 (for Hiero) and Table 4 (for SAMT).
Language Pair Time Rules
Cs ? En 4h41m 133M
De ? En 5h20m 219M
Fr ? En 16h47m 374M
Es ? En 16h22m 413M
Table 3: Extraction times and grammar sizes for Hi-
ero grammars using the Europarl and News Com-
mentary training data for each listed language pair.
Language Pair Time Rules
Cs ? En 7h59m 223M
De ? En 9h18m 328M
Fr ? En 25h46m 654M
Es ? En 28h10m 716M
Table 4: Extraction times and grammar sizes for
the SAMT grammars using the Europarl and News
Commentary training data for each listed language
pair.
4.2 Paraphrase Extraction
Recently English-to-English text generation tasks
have seen renewed interest in the NLP commu-
nity. Paraphrases are a key component in large-
scale state-of-the-art text-to-text generation systems.
We present an extended version of Thrax that im-
plements distributed, Hadoop-based paraphrase ex-
traction via the pivoting approach (Bannard and
Callison-Burch, 2005). Our toolkit is capable of
extracting syntactically informed paraphrase gram-
mars at scale. The paraphrase grammars obtained
with Thrax have been shown to achieve state-of-the-
art results on text-to-text generation tasks (Ganitke-
vitch et al, 2011).
For every supported translation feature, Thrax im-
plements a corresponding pivoted feature for para-
phrases. The pivoted features are set up to be aware
of the prerequisite translation features they are de-
rived from. This allows Thrax to automatically de-
tect the needed translation features and spawn the
corresponding map-reduce passes before the pivot-
ing stage takes place. In addition to features use-
ful for translation, Thrax also offers a number of
features geared towards text-to-text generation tasks
such as sentence compression or text simplification.
Due to the long tail of translations in unpruned
289
Source Bitext Sentences Words Pruning Rules
Fr ? En 1.6M 45M p(e1|e2), p(e2|e1) > 0.001 49M
{Da + Sv + Cs + De + Es + Fr} ? En 9.5M 100M
p(e1|e2), p(e2|e1) > 0.02 31M
p(e1|e2), p(e2|e1) > 0.001 91M
Table 5: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word
counts refer to the English side of the bitexts used.
translation grammars and the combinatorial effect
of pivoting, paraphrase grammars can easily grow
very large. We implement a simple feature-level
pruning approach that allows the user to specify up-
per or lower bounds for any pivoted feature. If a
paraphrase rule is not within these bounds, it is dis-
carded. Additionally, pivoted features are aware of
the bounding relationship between their value and
the value of their prerequisite translation features
(i.e. whether the pivoted feature?s value can be guar-
anteed to never be larger than the value of the trans-
lation feature). Thrax uses this knowledge to dis-
card overly weak translation rules before the pivot-
ing stage, leading to a substantial speedup in the ex-
traction process.
Table 5 gives a few examples of large paraphrase
grammars extracted from WMT training data. With
appropriate pruning settings, we are able to obtain
paraphrase grammars estimated over bitexts with
more than 100 million words.
5 Additional New Features
? With the help of the respective original au-
thors, the language model implementations by
Heafield (2011) and Pauls and Klein (2011)
have been integrated with Joshua, dropping
support for the slower and more difficult to
compile SRILM toolkit (Stolcke, 2002).
? We modified Joshua so that it can be used as
a parser to analyze pairs of sentences using a
synchronous context-free grammar. We imple-
mented the two-pass parsing algorithm of Dyer
(2010).
6 Conclusion
We present a new iteration of the Joshua machine
translation toolkit. Our system has been extended to-
wards efficiently supporting large-scale experiments
in parsing-based machine translation and text-to-text
generation: Joshua 4.0 supports compactly repre-
sented large grammars with its packed grammars,
as well as large language models via KenLM and
BerkeleyLM.We include an implementation of PRO,
allowing for stable and fast tuning of large feature
sets, and extend our toolkit beyond pure translation
applications by extending Thrax with a large-scale
paraphrase extraction module.
Acknowledgements This research was supported
by in part by the EuroMatrixPlus project funded
by the European Commission (7th Framework Pro-
gramme), and by the NSF under grant IIS-0713448.
Opinions, interpretations, and conclusions are the
authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Compu-
tational Linguistics.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of WMT06, pages
94?101. Association for Computational Linguistics.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguistics.
290
Kristy Hollingshead and Brian Roark. 2008. Rerank-
ing with baseline system scores and ranks as features.
Technical report, Center for Spoken Language Under-
standing, Oregon Health & Science University.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010a. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Zhifei Li, Ziyuan Wang, and Sanjeev Khudanpur. 2010b.
Unsupervised discriminative language model training
for machine translation using simulated confusion sets.
In Proceedings of COLING, Beijing, China, August.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Morgan &
Claypool Publishers.
Chris Manning and Dan Klein. 2003. Optimization,
maxent models, and conditional estimation without
magic. In Proceedings of HLT/NAACL, pages 8?8. As-
sociation for Computational Linguistics.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41rd
Annual Meeting of the Association for Computational
Linguistics (ACL-2003), Sapporo, Japan.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of WMT11.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Richard Zens and Hermann Ney. 2007. Efficient phrase-
table representation for machine translation with appli-
cations to online MT and speech translation. In Pro-
ceedings of HLT/NAACL, pages 492?499, Rochester,
New York, April. Association for Computational Lin-
guistics.
291
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 206?212,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joshua 5.0: Sparser, better, faster, server
Matt Post1 and Juri Ganitkevitch2 and Luke Orland1 and Jonathan Weese2 and Yuan Cao2
1Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Sciences Department
University of Pennsylvania
Abstract
We describe improvements made over the
past year to Joshua, an open-source trans-
lation system for parsing-based machine
translation. The main contributions this
past year are significant improvements in
both speed and usability of the grammar
extraction and decoding steps. We have
also rewritten the decoder to use a sparse
feature representation, enabling training of
large numbers of features with discrimina-
tive training methods.
1 Introduction
Joshua is an open-source toolkit1 for hierarchical
and syntax-based statistical machine translation
of human languages with synchronous context-
free grammars (SCFGs). The original version of
Joshua (Li et al, 2009) was a port (from Python to
Java) of the Hiero machine translation system in-
troduced by Chiang (2007). It was later extended
to support grammars with rich syntactic labels (Li
et al, 2010). Subsequent efforts produced Thrax,
the extensible Hadoop-based extraction tool for
synchronous context-free grammars (Weese et al,
2011), later extended to support pivoting-based
paraphrase extraction (Ganitkevitch et al, 2012).
Joshua 5.0 continues our yearly update cycle.
The major components of Joshua 5.0 are:
?3.1 Sparse features. Joshua now supports an
easily-extensible sparse feature implementa-
tion, along with tuning methods (PRO and
kbMIRA) for efficiently setting the weights
on large feature vectors.
1joshua-decoder.org
?3.2 Significant speed increases. Joshua 5.0 is up
to six times faster than Joshua 4.0, and also
does well against hierarchical Moses, where
end-to-end decoding (including model load-
ing) of WMT test sets is as much as three
times faster.
?3.3 Thrax 2.0. Our reengineered Hadoop-based
grammar extractor, Thrax, is up to 300%
faster while using significantly less interme-
diate disk space.
?3.4 Many other features. Joshua now includes a
server mode with fair round-robin scheduling
among and within requests, a bundler for dis-
tributing trained models, improvements to the
Joshua pipeline (for managing end-to-end ex-
periments), and better documentation.
2 Overview
Joshua is an end-to-end statistical machine trans-
lation toolkit. In addition to the decoder com-
ponent (which performs the actual translation), it
includes the infrastructure needed to prepare and
align training data, build translation and language
models, and tune and evaluate them.
This section provides a brief overview of the
contents and abilities of this toolkit. More infor-
mation can be found in the online documentation
(joshua-decoder.org/5.0/).
2.1 The Pipeline: Gluing it all together
The Joshua pipeline ties together all the infrastruc-
ture needed to train and evaluate machine transla-
tion systems for research or industrial purposes.
Once data has been segmented into parallel train-
ing, development, and test sets, a single invocation
of the pipeline script is enough to invoke this entire
infrastructure from beginning to end. Each step is
206
broken down into smaller steps (e.g., tokenizing a
file) whose dependencies are cached with SHA1
sums. This allows a reinvoked pipeline to reliably
skip earlier steps that do not need to be recom-
puted, solving a common headache in the research
and development cycle.
The Joshua pipeline is similar to other ?ex-
periment management systems? such as Moses?
Experiment Management System (EMS), a much
more general, highly-customizable tool that al-
lows the specification and parallel execution of
steps in arbitrary acyclic dependency graphs
(much like the UNIX make tool, but written with
machine translation in mind). Joshua?s pipeline
is more limited in that the basic pipeline skeleton
is hard-coded, but reduced versatility covers many
standard use cases and is arguably easier to use.
The pipeline is parameterized in many ways,
and all the options below are selectable with
command-line switches. Pipeline documentation
is available online.
2.2 Data preparation, alignment, and model
building
Data preparation involves data normalization (e.g.,
collapsing certain punctuation symbols) and tok-
enization (with the Penn treebank or user-specified
tokenizer). Alignment with GIZA++ (Och and
Ney, 2000) and the Berkeley aligner (Liang et al,
2006b) are supported.
Joshua?s builtin grammar extractor, Thrax, is
a Hadoop-based extraction implementation that
scales easily to large datasets (Ganitkevitch et al,
2013). It supports extraction of both Hiero (Chi-
ang, 2005) and SAMT grammars (Zollmann and
Venugopal, 2006) with extraction heuristics eas-
ily specified via a flexible configuration file. The
pipeline also supports GHKM grammar extraction
(Galley et al, 2006) using the extractors available
from Michel Galley2 or Moses.
SAMT and GHKM grammar extraction require
a parse tree, which are produced using the Berke-
ley parser (Petrov et al, 2006), or can be done out-
side the pipeline and supplied as an argument.
2.3 Decoding
The Joshua decoder is an implementation of the
CKY+ algorithm (Chappelier et al, 1998), which
generalizes CKY by removing the requirement
2nlp.stanford.edu/?mgalley/software/
stanford-ghkm-latest.tar.gz
that the grammar first be converted to Chom-
sky Normal Form, thereby avoiding the complex-
ities of explicit binarization schemes (Zhang et
al., 2006; DeNero et al, 2009). CKY+ main-
tains cubic-time parsing complexity (in the sen-
tence length) with Earley-style implicit binariza-
tion of rules. Joshua permits arbitrary SCFGs, im-
posing no limitation on the rank or form of gram-
mar rules.
Parsing complexity is still exponential in the
scope of the grammar,3 so grammar filtering re-
mains important. The default Thrax settings ex-
tract only grammars with rank 2, and the pipeline
implements scope-3 filtering (Hopkins and Lang-
mead, 2010) when filtering grammars to test sets
(for GHKM).
Joshua uses cube pruning (Chiang, 2007) with
a default pop limit of 100 to efficiently explore the
search space. Other decoder options are too nu-
merous to mention here, but are documented on-
line.
2.4 Tuning and testing
The pipeline allows the specification (and optional
linear interpolation) of an arbitrary number of lan-
guage models. In addition, it builds an interpo-
lated Kneser-Ney language model on the target
side of the training data using KenLM (Heafield,
2011; Heafield et al, 2013), BerkeleyLM (Pauls
and Klein, 2011) or SRILM (Stolcke, 2002).
Joshua ships with MERT (Och, 2003) and PRO
implementations. Tuning with k-best batch MIRA
(Cherry and Foster, 2012) is also supported via
callouts to Moses.
3 What?s New in Joshua 5.0
3.1 Sparse features
Until a few years ago, machine translation systems
were for the most part limited in the number of fea-
tures they could employ, since the line-based op-
timization method, MERT (Och, 2003), was not
able to efficiently search over more than tens of
feature weights. The introduction of discrimina-
tive tuning methods for machine translation (Liang
et al, 2006a; Tillmann and Zhang, 2006; Chiang
et al, 2008; Hopkins and May, 2011) has made
it possible to tune large numbers of features in
statistical machine translation systems, and open-
3Roughly, the number of consecutive nonterminals in a
rule (Hopkins and Langmead, 2010).
207
source implementations such as Cherry and Foster
(2012) have made it easy.
Joshua 5.0 has moved to a sparse feature rep-
resentation internally. First, to clarify terminol-
ogy, a feature as implemented in the decoder is
actually a template that can introduce any number
of actual features (in the standard machine learn-
ing sense). We will use the term feature function
for these templates and feature for the individual,
traditional features that are induced by these tem-
plates. For example, the (typically dense) features
stored with the grammar on disk are each separate
features contributed by the PHRASEMODEL fea-
ture function template. The LANGUAGEMODEL
template contributes a single feature value for each
language model that was loaded.
For efficiency, Joshua does not store the en-
tire feature vector during decoding. Instead, hy-
pergraph nodes maintain only the best cumulative
score of each incoming hyperedge, and the edges
themselves retain only the hyperedge delta (the in-
ner product of the weight vector and features in-
curred by that edge). After decoding, the feature
vector for each edge can be recomputed and ex-
plicitly represented if that information is required
by the decoder (for example, during tuning).
This functionality is implemented via the fol-
lowing feature function interface, presented here
in simplified pseudocode:
interface FeatureFunction:
apply(context, accumulator)
The context comprises fixed pieces of the input
sentence and hypergraph:
? the hypergraph edge (which represents the
SCFG rule and sequence of tail nodes)
? the complete source sentence
? the input span
The accumulator object?s job is to accumulate
feature (name,value) pairs fired by a feature func-
tion during the application of a rule, via another
interface:
interface Accumulator:
add(feature_name, value)
The accumulator generalization4 permits the use
of a single feature-gathering function for two ac-
cumulator objects: the first, used during decoding,
maintains only a weighted sum, and the second,
4Due to Kenneth Heafield.
used (if needed) during k-best extraction, holds
onto the entire sparse feature vector.
For tuning large sets of features, Joshua sup-
ports both PRO (Hopkins and May, 2011), an in-
house version introduced with Joshua 4.0, and k-
best batch MIRA (Cherry and Foster, 2012), im-
plemented via calls to code provided by Moses.
3.2 Performance improvements
We introduced many performance improvements,
replacing code designed to get the job done under
research timeline constraints with more efficient
alternatives, including smarter handling of locking
among threads, more efficient (non string-based)
computation of dynamic programming state, and
replacement of fixed class-based array structures
with fixed-size literals.
We used the following experimental setup to
compare Joshua 4.0 and 5.0: We extracted a large
German-English grammar from all sentences with
no more than 50 words per side from Europarl v.7
(Koehn, 2005), News Commentary, and the Com-
mon Crawl corpora using Thrax default settings.
After filtering against our test set (newstest2012),
this grammar contained 70 million rules. We then
trained three language models on (1) the target
side of our grammar training data, (2) English
Gigaword, and (3) the monolingual English data
released for WMT13. We tuned a system using
kbMIRA and decoded using KenLM (Heafield,
2011). Decoding was performed on 64-core 2.1
GHz AMD Opteron processors with 256 GB of
available memory.
Figure 1 plots the end-to-end runtime5 as a
function of the number of threads. Each point in
the graph is the minimum of at least fifteen runs
computed at different times over a period of a few
days. The main point of comparison, between
Joshua 4.0 and 5.0, shows that the current version
is up to 500% faster than it was last year, espe-
cially in multithreaded situations.
For further comparison, we took these models,
converted them to hierarchical Moses format, and
then decoded with the latest version.6 We com-
piled Moses with the recommended optimization
settings7 and used the in-memory (SCFG) gram-
5i.e., including model loading time and grammar sorting
6The latest version available on Github as of June 7, 2013
7With tcmalloc and the following compile flags:
--max-factors=1 --kenlm-max-order=5
debug-symbols=off
208
 500 1000 2000 3000 4000
 5000 10000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 4.0 (in-memory)Moses (in-memory)Joshua 4.0 (packed)Joshua 5.0 (packed)
Figure 1: End-to-end runtime as a function of the
number of threads. Each data point is the mini-
mum of at least fifteen different runs.
 200 300 400 500 1000 2000
 3000 4000 5000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 5.0Moses
Figure 2: Decoding time alone.
mar format. BLEU scores were similar.8 In this
end-to-end setting, Joshua is about 200% faster
than Moses at high thread counts (Figure 1).
Figure 2 furthers the Moses and Joshua com-
parison by plotting only decoding time (subtract-
ing out model loading and sorting times). Moses?
decoding speed is 2?3 times faster than Joshua?s,
suggesting that the end-to-end gains in Figure 1
are due to more efficient grammar loading.
3.3 Thrax 2.0
The Thrax module of our toolkit has undergone
a similar overhaul. The rule extraction code was
822.88 (Moses), 22.99 (Joshua 4), and 23.23 (Joshua 5).
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 3: Here, position-aware lexical and part-of-
speech n-gram features, labeled dependency links,
and features reflecting the phrase?s CCG-style la-
bel NP/NN are included in the context vector.
rewritten to be easier to understand and extend, al-
lowing, for instance, for easy inclusion of alterna-
tive nonterminal labeling strategies.
We optimized the data representation used for
the underlying map-reduce framework towards
greater compactness and speed, resulting in a
300% increase in extraction speed and an equiv-
alent reduction in disk I/O (Table 1). These
gains enable us to extract a syntactically labeled
German-English SAMT-style translation grammar
from a bitext of over 4 million sentence pairs in
just over three hours. Furthermore, Thrax 2.0 is
capable of scaling to very large data sets, like
the composite bitext used in the extraction of the
paraphrase collection PPDB (Ganitkevitch et al,
2013), which counted 100 million sentence pairs
and over 2 billion words on the English side.
Furthermore, Thrax 2.0 contains a module fo-
cused on the extraction of compact distributional
signatures over large datasets. This distribu-
tional mode collects contextual features for n-
gram phrases, such as words occurring in a win-
dow around the phrase, as well as dependency-
based and syntactic features. Figure 3 illustrates
the feature space. We then compute a bit signature
from the resulting feature vector via a randomized
locality-sensitive hashing projection. This yields a
compact representation of a phrase?s typical con-
text. To perform this projection Thrax relies on
the Jerboa toolkit (Van Durme, 2012). As part of
the PPDB effort, Thrax has been used to extract
rich distributional signatures for 175 million 1-
to-4-gram phrases from the Annotated Gigaword
corpus (Napoles et al, 2012), a parsed and pro-
209
Cs-En Fr-En De-En Es-En
Rules 112M 357M 202M 380M
Space Time Space Time Space Time Space Time
Joshua 4.0 120GB 112 min 364GB 369 min 211GB 203 min 413GB 397 min
Joshua 5.0 31GB 25 min 101GB 81 min 56GB 44 min 108GB 84 min
Difference -74.1% -77.7% -72.3% -78.0% -73.5% -78.3% -73.8% -78.8%
Table 1: Comparing Hadoop?s intermediate disk space use and extraction time on a selection of Europarl
v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax?s final
grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers
and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.
cessed version of the English Gigaword (Graff et
al., 2003).
Thrax is distributed with Joshua and is also
available as a separate download.9
3.4 Other features
Joshua 5.0 also includes many features designed
to increase its usability. These include:
? A TCP/IP server architecture, designed to
handle multiple sets of translation requests
while ensuring fairness in thread assignment
both across and within these connections.
? Intelligent selection of translation and lan-
guage model training data using cross-
entropy difference to rank training candidates
(Moore and Lewis, 2010; Axelrod et al,
2011) (described in detail in Orland (2013)).
? A bundler for easy packaging of trained mod-
els with all of its dependencies.
? A year?s worth of improvements to the
Joshua pipeline, including many new features
and supported options, and increased robust-
ness to error.
? Extended documentation.
4 WMT Submissions
We submitted a constrained entry for all tracks ex-
cept English-Czech (nine in total). Our systems
were constructed in a straightforward fashion and
without any language-specific adaptations using
the Joshua pipeline. For each language pair, we
trained a Hiero system on all sentences with no
more than fifty words per side in the Europarl,
News Commentary, and Common Crawl corpora.
9github.com/joshua-decoder/thrax
We built two interpolated Kneser-Ney language
models: one from the monolingual News Crawl
corpora (2007?2012), and another from the tar-
get side of the training data. For systems translat-
ing into English, we added a third language model
built on Gigaword. Language models were com-
bined linearly into a single language model using
interpolation weights from the tuning data (new-
stest2011). We tuned our systems with kbMIRA.
For truecasing, we used a monolingual translation
system built on the training data, and finally deto-
kenized with simple heuristics.
5 Summary
The 5.0 release of Joshua is the result of a signif-
icant year-long research, engineering, and usabil-
ity effort that we hope will be of service to the
research community. User-friendly packages of
Joshua are available from joshua-decoder.
org, while developers are encouraged to partic-
ipate via github.com/joshua-decoder/
joshua. Mailing lists, linked from the main
Joshua page, are available for both.
Acknowledgments Joshua?s sparse feature rep-
resentation owes much to discussions with Colin
Cherry, Barry Haddow, Chris Dyer, and Kenneth
Heafield at MT Marathon 2012 in Edinburgh.
This material is based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
210
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362, Edinburgh, Scotland, UK., July.
J.C. Chappelier, M. Rajman, et al 1998. A generalized
CYK algorithm for parsing stochastic CFG. In First
Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133?137.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL-HLT, pages 427?436, Montre?al,
Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, Waikiki, Hawaii, USA, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, Adam Pauls, and Dan Klein. 2009.
Asynchronous binarization for synchronous gram-
mars. In Proceedings of ACL, Suntec, Singapore,
August.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL/COLING, Sydney, Australia, July.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, PRO, and paraphrases. In Proceedings of
the Workshop on Statistical Machine Translation.
Juri Ganitkevitch, Chris Callison-Burch, and Benjamin
Van Durme. 2013. Ppdb: The paraphrase database.
In Proceedings of HLT/NAACL.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2003.
English gigaword. Linguistic Data Consortium,
Philadelphia.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of ACL, Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguis-
tics.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proceedings of
EMNLP, pages 646?655.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation, Athens,
Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proceedings of the Workshop on
Statistical Machine Translation.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of ACL/COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of
NAACL, pages 104?111, New York City, USA, June.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of ACL (short papers), pages 220?224.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL, pages 440?
447, Hong Kong, China, October.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
Sapporo, Japan.
Luke Orland. 2013. Intelligent selection of trans-
lation model training data for machine translation
with TAUS domain data: A summary. Master?s the-
sis, Johns Hopkins University, Baltimore, Maryland,
June.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
pages 258?267, Portland, Oregon, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL,
Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Seventh International
Conference on Spoken Language Processing.
211
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721?728, Syd-
ney, Australia, July.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Ex-
cellence, Johns Hopkins University.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the
Thrax grammar extractor. In Proceedings of the
Workshop on Statistical Machine Translation.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, New York, New York.
212
