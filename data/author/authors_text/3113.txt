Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 37?44,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Coverage and Inheritance in The Preposition Project
                      Ken Litkowski                        
                       CL Research                          
                     9208 Gue Road                        
      Damascus, MD 20872       
ken@clres.com 
                       Orin Hargraves                      
                 5130 Band Hall Hill Road            
        Westminster, MD 21158     
orinkh@carr.org  
Abstract
In The Preposition Project (TPP), 13
prepositions have now been analyzed and
considerable data made available. These
prepositions, among the most common words
in English, contain 211 senses. By analyzing
the coverage of these senses, it is shown that
TPP provides potentially greater breadth and
depth than other inventories of the range of
semantic roles. Specific inheritance
mechanisms are developed within the
preposition sense inventory and shown to be
viable and provide a basis for the
rationalization of the range of preposition
meaning. In addition, this rationalization can
be used for developing a data-driven mapping
of a semantic role hierarchy. Based on these
findings and methodology, the broad
structure of a WordNet-like representation of
preposition meaning, with self-contained
disambiguation tests, is outlined.
1 Introduction
The Preposition Project (TPP, Litkowski &
Hargraves, 2005)
1
 provides a large amount of data
for a small number of prepositions. To date, 13 out
of 373 prepositions (among the most frequent in
English) have been analyzed. We examined the data
for these prepositions to determine (1) their
coverage of the semantic space of semantic
relations, (2) the extent to which these data could be
extrapolated to prepositions not yet covered, and (3)
what types of analyses might be useful to fill
shortcomings in the data. Examining these issues
seems important to determining the extent to which
the data in the project can be used in NLP
applications.
TPP is designed to provide a comprehensive
database of preposition senses, so it is useful to
provide a mechanism for assessing the extent of
coverage, not only in comparison with the range of
meanings described in traditional grammar, but also
in comparison with analyses within the
computational linguistics community. Similarly, it
seems important to determine how, if at all, the data
developed thus far can be leveraged for use with
other preposition meanings not yet analyzed, e.g.,
through mechanisms of inheritance. Finally, through
these analyses, it is useful to identify any
shortcomings in data being developed in TPP and
what further should be undertaken.
In the following sections, we first provide an
overview of TPP and extensions to its available
data that have occurred since its inception. Next, we
examine issues of coverage in relation to the range
of preposition meaning contained in Quirk et al
(1985), alongside the ranges in other resources such
as the Penn Treebank, FrameNet, and Lexical
Conceptual Structures. This analysis also considers
accounts of semantic relations that have been
presented in literature that has used these other
resources. Next, we critically examine claims of the
inheritance of preposition meaning as described in
Litkowski (2002), including consideration of
inheritance mechanisms in FrameNet. This analysis
suggests some mechanisms for a data-driven or
corpus-based approach to the identification of a
semantic relation inventory. Finally, based on these
analyses of coverage and inheritance, we identify
some next steps TPP needs to take.
2 The Preposition Project
The primary objective of TPP is to characterize
each of 847 preposition senses for 373 prepositions
(including 220 phrasal prepositions with 309
senses) with a semantic role name and the syntactic
and semantic properties of its complement and
attachment point. The preposition sense inventory
is taken from the Oxford Dictionary of English
1
http://www.clres.com/prepositions.html.
37
(2004).
2
 Starting from the senses for a particular
preposition, a set of instances of that preposition
are extracted from the FrameNet database. A
lexicographer then assigns a sense from the
inventory to each instance. While engaged in this
sense assignment, the lexicographer accumulates an
understanding of the behavior of the preposition,
assigns a name to each sense (characterizing its
semantic type), and characterizes the syntactic and
semantic properties of the preposition complement
and its point of attachment or head. Each sense is
also characterized by its syntactic function and its
meaning, identifying the relevant paragraph(s)
where it is discussed in Quirk et al
TPP then makes available the sense analysis
(including the lexicographer?s overview) and the set
of instances for each preposition that is analyzed. In
addition, the disambiguated instances are then
analyzed to provide the set of FrameNet frames and
frame elements associated with each sense. The set
of sentences is provided in Senseval format, along
with an answer key, for use in development of
preposition disambiguation routines (ranging from
300 to over 4000 sentences for ?of?). Finally, using
the FrameNet frame and frame element of the
tagged instances, syntactic alternation patterns
(other syntactic forms in which the semantic role
may be realized) are provided for each FrameNet
target word; this data constitutes a suitable corpus
for use in studying, for example, English verb
classes (see Levin, 1993).
An important next step for TPP is the use of
these disambiguated instances to refine the
characterization of the syntactic and semantic
properties of the complement and the point of
attachment. As the lexicographer has analyzed the
sense inventory for a preposition, the question of its
use in relation to other words is continually raised.
In particular, the question is whether a sense stands
alone or is selected for by a verb or other word
(most frequently, an adjective).
3
 The lexicographer
has observed that selection might be occurring. The
extent to which this occurs will be examined when
an attempt is made, for example, to develop
decision lists for disambiguating among a
preposition?s senses.
4
 We hope, as a result, that the
number of instances available for disambiguation
will permit a more definitive characterization of
selection.
Since Litkowski & Hargraves (2005), several
additions have been made to the data and analyses
available under TPP. First, Oxford University Press
has granted permission to provide the definitions
and examples of the senses for each definition from
the Oxford Dictionary of English (ODE, 2003)
(and its predecessor, the New Oxford Dictionary of
English (NODE, 1997)). Second, a summary file of
all senses has been prepared from the individual
preposition sense analyses, facilitating overview
analysis of the full sense inventory (e.g., sorting the
table on different columns). Third, the
lexicographer has disambiguated the ending
preposition of definitions as those prepositions are
analyzed (e.g., in sense 1 of about, on the subject
of, identifying the applicable sense of of); 451
prepositions have been so tagged.
At present, the following 13 prepositions have
been analyzed (with the initial number of senses in
parentheses): about (6), against (10), at (12), by
(22), for (14), from (14), in (11), of (18), on (23),
over (16), through (13), to (17), and with (16).
The number of senses has changed based on
changes from NODE to ODE and based on
evidence developed in the project (adding 19 senses
that are attested with the FrameNet data). These
prepositions include the most frequent in English
(see Boonthum et al, 2006 for the top 10 based on
the Brown corpus). In summary, the 13 prepositions
(out of 373 identified in Litkowski, 2002) have 210
senses (19 have been added during the course of
TPP) out of the original 847 senses.
It is noteworthy also that in moving from
NODE to ODE, 60 prepositions have been
removed. Some of these prepositions are variant
spellings (e.g. abaht for about). Most are phrasal
prepositions, e.g., to the accompaniment of. In
2
TPP does not include particle senses of such words
as in or over (or any other particles) used with verbs
to make phrasal verbs. In this context, phrasal verbs
are to be distinguished from verbs that select a
preposition (such as on in rely on), which may be
characterized as a collocation. We are grateful to an
anonymous reviewer for raising this issue.
3
We are grateful to an anonymous reviewer for this
characterization.
4
The anonymous reviewer asked whether TPP
excludes senses that are selected for. This prompted
an examination of whether this might be the case.
Although it is the intent that such senses be included,
an examination of how FrameNet instances are
generated raises the possibility that such instances
may have excluded. Procedures are currently being
developed to ensure that such instances are not
excluded.
38
NODE, the definitions constitute a lexicographic
statement that the meaning of the phrase has an
idiomatic status, i.e., is not solely recoverable based
on an understanding of the meanings of its
constituents. In ODE, such phrases are identified as
having collocative status and thereby rendered in
example usages with italics, but not given a
definition. Such phrases will be retained in TPP.
Litkowski & Hargraves (2005) provides more
details on the methodology used in TPP and the
databases that are available.
3 Semantic Coverage of TPP
Although only a small percentage of the
prepositions have as yet been analyzed,
approximately 25 percent of the total number of
senses are included in the 13 prepositions. This
percentage is sufficient to assess their coverage of
the semantic space of prepositional meaning.
3.1 Assessing the Broad Spectrum of Semantic
Space
To assess the coverage, the first question is what
inventory should be used. The linguistics and
computational linguistics literatures are replete with
introspective lists of semantic roles. Gildea &
Jurafsky (2002) present a list of 18 that may be
viewed as reasonably well-accepted. O?Hara (2005)
provides several compilations based on Penn
Treebank annotations, FrameNet, OpenCyc, and
Factotum. Boonthum et al (2006) includes an
assessment of semantic roles in Jackendoff, Dorr?s
Lexical Conceptual Structures preposition
database, and Barker?s analysis of preposition
meaning; she posits a list of 7 overarching semantic
roles (although specifically intended for use in
paraphrase analysis). Without going into a detailed
analysis of each of these lists, all of which are
relatively small in number, the semantic relations
included in TPP clearly cover each of the lists.
However, since the semantic relations in these lists
are relatively coarse-grained, this assessment is not
sufficient.
Quirk et al (1985) is arguably the most
comprehensive introspective compilation of the
range of preposition meaning. As indicated above,
in analyzing the senses for a preposition, the
lexicographer includes a reference to a section in
Quirk et al(specifically in Chapter 9). Quirk et al
describe the meanings of prepositions in 50
sections, with the majority of discussion devoted to
spatial and temporal prepositions. By comparing
the references in the spreadsheets for each
preposition (i.e., a data-driven approach), we find
that only 4 sections are not yet mentioned. These
are 9.21 (between), 9.56 (concession), 9.58
(exception and addition), and 9.59 (negative
condition). In general, then, TPP broadly covers the
full range of meanings expressed by prepositions as
described in Quirk et al.
However, for almost half of the senses analyzed
in TPP (100 of 210), the lexicographer was unable
to assign a Quirk paragraph in Chapter 9 or
elsewhere. This raises the question of whether
Quirk et al can be viewed as comprehensive. A
preliminary examination of the semantic relations
assigned by the lexicographer and not assigned a
Quirk paragraph indicates that the range of
prepositional meaning is more extensive than what
is provided in Quirk et al
Two major categories of missing semantic
relations emerge from this analysis. Of the 100
senses without a Quirk paragraph, 28 involve
prepositional usages pertaining to quantities. These
include the semantic relations like Age (?at six he
contracted measles?, ScaleValue (?an increase of
5%?), RatioDenominator (?ten miles to the
gallon?), Exponent (?10 to the fourth power?),
ValueBasis (?a tax on tea?), Price (?copies are
available for $5"), and UnitSize (?billing is by the
minute?). Another 32 involve prepositions used to
establish a point of reference, similar to the
Standard in Quirk (section 9.62), except indicating
a much broader set. These include semantic
relations like FormerState (?wakened from a
dream?), KnowledgeSource (?information from
books?), NameUsed (?call him by his last name?),
ParentName (?a child by her first husband?),
Experiencer (?a terrible time for us?), and
Comparator (?that?s nothing compared to this?).
The remaining 40 semantic relations, such as
MusicalKey (?in F minor?), Drug (?on dope?), and
ProfessionAspect (?a job in publishing?), appear to
represent finer-grained points of prepositional
meaning.
This assessment of coverage suggests that TPP
currently not only covers the broad range of
semantic space, but also identifies gaps that have
not received adequate treatment in the linguistic
literature. Perhaps such gaps may be viewed as
?beneath the radar? and not warranting elaborate
treatment. However, it is highly likely that these
39
Semantic
Relation Frequency Definitions Examples
Location 0.404 expressing location or arrival in a
particular place or position
crouched at the edge of the track
Temporal 0.072 expressing the time when an event
takes place
avoid confusiong at this late stage
Level 0.039 denoting a particular point or segment
on a scale
charged at two percent
Skill 0.038 expressing a particular state or
condition, or a relationship between an
individual and a skill
brilliant at the job
ActionObject 0.276 expressing the object of a look,
gesture, thought, action, or plan
moaned at him
Stimulus 0.171 expressing the means by which
something is done or the cause of an
action or reaction
boiled at his lack of thought
Table 1. Frequency of ?at? FrameNet Instances in The Preposition Project
senses occur with considerable frequency and
should be treated.
It is somewhat premature to perform a
comprehensive analysis of coverage that provides a
full characterization of the semantic space of
preposition meaning based on the 25 percent of
senses that have been analyzed thus far. However,
the available data are sufficient to begin such an
effort; this issue is further discussed below.
3.2 Assessing Finer-Grained Spectra of
Prepositional Meaning
While examining the broad coverage of preposition
meaning, several issues affecting the treatment of
individual prepositions in the computational
linguistics literature emerged. These issues also
provide a perspective on the potential value of the
analyses being performed in TPP.
O?Hara (2005), in attempting to create a
framework for analysis and identification of
semantic relations, examined the utility of Penn
Treebank II annotations and FrameNet frame
elements. He examined sentences containing at in
both corpora. In Treebank, he noted that there were
four senses: locative (0.732), temporal (0.239),
manner (0.020), and direction (0.006). In
FrameNet, with some combination of frame
elements, he identified five major senses: addressee
(0.315), other (0.092), phenomenon (0.086), goal
(0.079), and content (0.051).
Table 1 provides a coarse-grained analysis of at
developed in TPP (6 additional subsenses are not
shown). Although frequencies are shown in the
table, they should not be taken seriously, since the
FrameNet instances on which they are based makes
no claim to be representative. In particular,
FrameNet seldom annotates temporal references
since they are usually viewed as peripheral frame
elements that may occur with virtually all frames.
Nonetheless, the frequencies in the FrameNet
instances does indicate that each of the at senses is
likely to occur at levels that should not be ignored
or glossed over.
In comparing TPP results with Penn Treebank
characterizations, it seems that, not only might the
corpus be unrepresentative, but that the linguistic
introspection does not capture the more natural
array of senses. Thus, by combining corpus
evidence (from FrameNet) with a lexicographic
perspective for carving out sense distinctions, an
improved balance results. It should also be noted
that in Table 1, the final sense for Stimulus
emerged from the FrameNet data and from Quirk
and was not identified in the ODE sense inventory.
Comparing TPP results with O?Hara?s
aggregation of FrameNet frame elements indicates
the difficulty of working directly with the large
number of frame elements (currently over 700). As
Gildea & Jurafsky noted, it is difficult to map these
frame elements into higher level semantic roles.
Some assistance is available from the FrameNet
inheritance hierarchy, but this is still not well-
developed. This issue is taken up further below in
describing how TPP?s data-driven approach may
facilitate this kind of mapping.
In summary, the methodology being followed in
TPP arguably provides a more natural and a more
assuredly complete coverage of the fine-grained
senses associated with an individual preposition.
40
4 Inheritance Within the Preposition
Sense Inventory
The preceding discussion provides some assurance
that TPP provides broad coverage of the range of
prepositional meaning and fine-grained analysis of
the behavior of individual prepositions. However,
the large number of preposition senses requires
some additional work to manage these broad and
fine-grained spectra. Litkowski (2002) provided a
graph-theoretical analysis that arranged
prepositions into a hierarchy. However, that
analysis treated individual prepositions as
aggregations, i.e., all senses were combined into
nodes in a digraph. With the finer-grained analysis
now available in TPP data, a more in-depth
examination of inheritance within the preposition
sense inventory is possible.
4.1 Initial Considerations for Mapping Out the
Inheritance Hierarchy
Of the 847 senses described in Litkowski (2002),
and used as the starting point for the analysis in
TPP, most follow the prototypical form of a
prepositional phrase followed by a terminal
(dangling) preposition, e.g., for sense 1 of about,
on the subject of. Litkowski viewed the terminal
preposition as a hypernym. However, 62 senses do
not have terminal prepositions (but rather usually
verbs) and an additional 164 senses are usage notes
describing behavior (such as the senses of at shown
in Table 1). These 226 senses were viewed as being
primitive, while the remaining 621 were viewed as
being derived in some way dependent on the
putative hypernym.
Among the 13 prepositions that have been
analyzed thus far, 11 senses having a non-
preposition hypernym and 100 senses with usage
notes have been characterized. Thus, only about
half of the so-called primitives have been assigned
a semantic relation type. Further analysis of the
range of meaning of these primitives should await
a more complete coverage of these senses. The kind
of analysis envisioned among these senses is
determining how they group together and what
range of semantic meaning they express. This will
be discussed further below.
Of the 621 senses with a preposition hypernym,
411 end in one of the 13 prepositions that have been
analyzed, with 175 ending in of and 74 in to. The
remaining 210 senses end in prepositions with at
most a few cases of the same preposition. Most of
these remaining senses, in fact, are the ones that
gave rise to the definitional cycles and hierarchical
analysis of the digraph described in Litkowski
(2002). As a result, senses with a preposition
hypernym form a set sufficient in size for a more
detailed analysis of inheritance within the
preposition inventory.
4.2 The Meaning of an Inheritance Hierarchy
for Prepositions
The assumption underlying an inheritance analysis
of preposition definitions with a terminal
preposition is that such definitions are substitutable
for the preposition that is defined. For example, in
a book about ancient Greece, about can be
replaced by its definition to obtain a book on the
subject of ancient Greece. This sense of about has
been labeled SubjectConsidered (or equivalently,
Topic or Subject) by the lexicographer. In the
inheritance analysis, this definition of about is said
to have of as its hypernym.
Clearly, the hypernymic ascription for
prepositions is by analogy only. To say that about
isa of makes little sense. In TPP, the lexicographer
develops three pieces of information about each
sense: a semantic relation name, the properties of
the prepositional object, and the properties of the
word to which the prepositional phrase is attached.
In analyzing the definition for about, of is attached
to the word subject. Thus, nothing about the
attachment properties of of can be inherited into
saying anything about the attachment properties of
about. At best, then, the semantic relation name and
complement properties of the applicable sense of of
can be inherited. Indeed, this can be put into the
form of a hypothesis: the semantic relation name
and the complement properties of an inherited
sense are more general than those of the inheriting
sense.
As mentioned above, the lexicographer has
disambiguated the terminal preposition in senses
that use one of the 13 prepositions that have been
analyzed. This has been done for 451 definitions in
the 411 senses. It is noteworthy that in only 29
cases did the lexicographer assign multiple senses
(i.e., viewing the applicable sense as ambiguous). In
other words, despite the fact that most of these
definitions contained only 4 or 5 words, sufficient
context enabled resolution to a specific sense of the
hypernym. In 8 cases, the multiple inheritance was
41
Semantic
Relation Preposition
Complement
Properties Definition
Hypernym
Semantic
Relation
Hypernym
Complement
Properties
Opposing
Force
against sth actively resisted in resistance to; as
protection from
Thing
Prevented
participle or noun
denoting thing
prevented
Thing
Surmounted
over a physical entity that
can have sth above it
extending directly
upwards from
Space
Origin
point in space or
abstraction
identified as origin
Thing Bored through permeable or breakable
physical object
so as to make a hole
or opening in (a
physical object)
Thing
Entered
sth capable of
being entered or of
incorporating or
enveloping input
Beneficiary for usually a person;
otherwise, sth capable of
benefitting
on behalf of or to the
benefit of (someone or
something)
Recipient noun representing
the obj. of action
denoted in the
POA
Feature
Backdrop
on background on which
the POA is located
forming a distinctive
or marked part of (the
surface of something)
Whole object of which the
POA is a part,
piece, or sample
Downside against downside; the con in a
pro/con situation
in conceptual contrast
to
Comparator second term of a
comparison
Table 2. Inheritance of Semantic Relations and Complement Properties
for all senses, as in the case of frae, a Scottish
dialectical form of from.
In making the sense assignments, 175 of which
(39 percent) involved of, the lexicographer noted
that a large number of cases (132 of 373) involved
phrasal prepositions that ended in of, e.g., into the
arms of and in the name of. In these cases, the
definition (as developed by Oxford lexicographers)
merely substituted one phrase ending in of for the
phrase being defined (into the possession or control
of and for the sake of for the two examples). This
observation was a major reason for requiring that
any hypernymic ascription within the preposition
inventory could not be based on the prototypical isa
hierarchy applicable to nouns.
Among the 411 senses for which the terminal
preposition had been disambiguated, 48 senses
occurred as definitions of the 13 prepositions that
have been analyzed in TPP. For these 48 senses,
each of which was fully characterized, the
characterization of the terminal prepositions was
also available, thus enabling us to test the
hypothesis about what could be inherited. Table 2
shows the results for 6 of these senses, giving first
the semantic relation assigned to the sense by the
lexicographer, the preposition, the characterization
of the complement properties for that sense, the
definition (with the hypernymic preposition in bold),
the semantic relation of the sense that the
lexicographer judged to be the appropriate sense of
the hypernymic preposition, and the complement
properties of that sense.
The examples in Table 2 support the hypothesis
about inheritance. The other 42 cases are similar,
although for some, the hypernymic semantic
relation or hypernymic complement properties are
not as close to the preposition sense being
examined. In a few cases, for example, the
complement properties are as general as ?any
noun.? In such cases, what gets inherited may not
provide much in the way of specificity to aid in
analyzing the behavior of the inheriting preposition.
However, viewed from the perspective of the
digraph analysis performed in Litkowski (2002),
this inheritance analysis provides confidence that
there is an ordering relationship within the
preposition sense inventory that can be exploited.
In the digraph analysis in Litkowski (2002),
where the prepositions were analyzed as aggregated
nodes, the inheritance mechanism provides the basis
for splitting nodes based on the specific sense
assignments that can now be made. In particular, in
Table 3, showing one node of the preposition
digraph that was characterized as a single strong
component (number 12) containing 33 prepositions,
the sense-specific assignments will permit the
disaggregation of these prepositions into smaller
groups that are closely related.
42
Table 3. Strong Components
Entries
12 in favour of, along with, with respect to,
in proportion to, in relation to, in
connection with, with reference to, in
respect of, as regards, concerning, about,
with, in place of, instead of, in support of,
except, other than, apart from, in addition
to, behind, beside, next to, following,
past, beyond, after, to, before, in front of,
ahead of, for, by, according to
In considering the type of analysis described by
Table 2, it is important to note that the results
followed from the reliance on a data-driven
approach. Characterizations of individual senses are
made locally with respect to observed behavior of a
single preposition. It is only after these analyses
that results from several tables and spreadsheets
can be conjoined to produce something like Table 2.
It is also important to note that the results in
Table 2 must be viewed as preliminary. Although it
is expected that the central hypothesis about
inheritance will remain valid, it is expected that the
characterizations of the complement properties will
undergo considerable refinement. One of the
primary goals of TPP is to develop a data-driven set
of disambiguation criteria for distinguishing among
preposition senses. Methods such as those
developed by O?Hara (2005) and Boonthum et al
(2006) suggest that refined characterizations will
emerge. The large instance sets (in Senseval format)
will provide an ample data set for this analysis.
Finally, it is expected that the semantic relation
names will also undergo some additional revisions.
Again, since these names are developed locally with
respect to single prepositions, they do not reflect
what may be a final set when they are analyzed
together. This is discussed in the next section.
5 Next Steps for The Preposition Project
The analyses of issues concerning coverage and
inheritance within the preposition sense inventory
suggest at least two major new goals for TPP. One
is the rationalization of the semantic relation types
and the other is the aggregation of characterizations
about the senses into a convenient and usable data
structure, perhaps following WordNet.
5.1 Rationalization of Semantic Relation Types
The semantic relation types that have been
developed thus far in TPP have been extremely
useful in assessing the current coverage of the
semantic space of prepositions and in examining the
possibilities of an inheritance structure for the
senses. However, the analyses have shown that
there are some gaps in broad coverage and some
that will affect fine-grained characterizations of the
semantic space.
In performing the analyses of the 13
prepositions and their 211 senses, the names for the
semantic relations for an individual preposition
have been developed without regard to those from
other prepositions or the linguistic literature, based
on the individual definitions in ODE and the
instances from FrameNet that have been tagged.
Although frame element names are available to the
lexicographer when examining FrameNet instances,
they are only in the background. As a result, these
names provide a data-driven basis for
characterizing the semantic space of prepositions.
Given the importance of these names for the
types of analyses described above, it is valuable to
complete the assignment of names, even without the
full-scale analysis of sentence instances.
Completion of this task would represent only a
preliminary assignment, modifiable when instances
are more fully analyzed.
With a relatively complete set of semantic
relation types, ?rationalization? of the set, i.e.,
reorganization in such a way to make it more
logical and consistent can be performed. At present,
among the 211 semantic relation types, there are 36
duplicate names, some appearing multiple times,
e.g., AgentName appears 5 times. Some names are
only slight variants of one another, such as
Timeframe and TimePeriod. Many names can be
grouped together for analysis. For example, in the
time space, such semantic relations as
ActivePeriod, ClockHour, CreationDate,
FutureTime, Hour, PeriodBisected, PointInTime,
TargetTime, TimeOrigin, and TimePeriod would
be examined together.
In pursuing this rationalization, outside
resources can be used more efficiently. In
particular, the FrameNet naming conventions and
inheritance hierarchy can be examined in more
detail (as well as critiqued). In addition, it will be
possible to take into account other treatments of
particular prepositions or fine-grained areas of
semantic space more easily.
Rationalization not only will ensure consistency
in naming, but provide a vehicle for appropriate
43
data-driven mapping. This will provide a basis for
or against conventional groupings that have been
posited in the linguistics and computational
linguistics literature. It is not expected that this
rationalization will produce anything unexpected,
but it will provide an underlying support for
characterizing the range of prepositional meaning.
5.2 Towards a WordNet Representation of
Prepositional Meaning
The amount of data generated in TPP has been
prodigious and is difficult to comprehend and
exploit. With a firmer basis established in section 4
above for inheritance mechanisms, combined with
the digraph analysis described in Litkowski (2002),
it seems possible to move toward a representation
that is similar to WordNet.
By following the inheritance structure, based on
the analyses described in section 4, combined with
a rationalization of semantic relation names, it
seems likely that there will be a relatively small
number of primitive concepts. The digraph analysis
yields synsets in the manner of WordNet, so we can
visualize that nodes in a WordNet preposition
network will consist of preposition names and
preposition glosses (i.e., definitions). In addition,
the objective will be to provide an improved
characterization of complement and attachment
properties that will accompany each node. Thus,
such a WordNet-like preposition network will
represent not only meanings, but also provide the
capability for disambiguation.
6 Conclusions
Although only a small number of prepositions have
been analyzed in The Preposition Project, the data
that has been generated has proved sufficient for a
broad assessment of the range of preposition
meaning. Not only has it been possible to
demonstrate that the project currently provides a
comparable broad coverage, but also that it reveals
potential gaps in previous analyses of coverage.
The data has also proved sufficient for the
articulation of appropriate inheritance mechanisms
within the preposition sense inventory. These results
have permitted the development of procedures that
can be used for mapping out the space of semantic
roles. In addition, with these results, it is possible to
lay out steps toward a WordNet-like representation
of prepositions and their behavior.
References
Bonnie Dorr. 1996. Lexical Conceptual Structures for
Prepositions
(http://www.umiacs.umd.edu/~bonnie/AZ-preps-
English.lcs)
Chutima Boonthum, Shunichi Toida, & Irwin
Levinstein. 2006. Preposition Senses:
Generalized Disambiguation Model. Conference
on Intelligent Text Processing and
Computational Linguistics (CICLING-2006).
Mexico City.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational
Linguistics, 28 (3), 245-288.
Kenneth C. Litkowski. 2002. Digraph Analysis of
Dictionary Preposition Definitions. Word Sense
Disambiguation: Recent Success and Future
Directions. Philadelphia, PA: Association for
Computational Linguistics.
Kenneth C. Litkowski & Orin Hargraves. 2005. The
Preposition Project. ACL-SIGSEM Workshop on
?The Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic
Formalisms and Applications?, University of
Essex - Colchester, United Kingdom. 171-179. 
The New Oxford Dictionary of English. 1998. (J.
Pearsall, Ed.). Oxford: Clarendon Press.
Thomas P. O?Hara. 2005. Empirical Acquisition of
Conceptual Distinctions via Dictionary
Definitions. Ph.D. Thesis. New Mexico State
University.
The Oxford Dictionary of English. 2003. (A.
Stevension and C. Soanes, Eds.). Oxford:
Clarendon Press.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
& Jan Svartik. (1985). A comprehensive
grammar of the English language. London:
Longman.
44
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 24?29,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 06: Word-Sense Disambiguation of Prepositions
                      Ken Litkowski                           
                        CL Research                            
                     9208 Gue Road                          
      Damascus, MD 20872          
ken@clres.com 
                       Orin Hargraves                         
              5130 Band Hall Hill Road                  
  Westminster, MD 21158     
orinhargraves@googlemail.com  
Abstract
The SemEval-2007 task to disambiguate
prepositions was designed as a lexical sample
task. A set of over 25,000 instances was
developed, covering 34 of the most frequent
English prepositions, with two-thirds of the
instances for training and one-third as the test
set. Each instance identified a preposition to be
tagged in a full sentence taken from the
FrameNet corpus (mostly from the British
National Corpus). Definitions from the Oxford
Dictionary of English formed the sense
inventories. Three teams participated, with all
achieving supervised results significantly
better than baselines, with a high fine-grained
precision of 0.693. This level is somewhat
similar to results on lexical sample tasks with
open class words, indicating that significant
progress has been made. The data generated in
the task provides ample opportunitites for
further investigations of preposition behavior.
1 Introduction
The SemEval-2007 task to disambiguate prepositions
was designed as a lexical sample task to investigate
the extent to which an important  closed class of
words could be disambiguated. In addition, because
they are a closed class, with stable senses, the
requisite datasets for this task are enduring and can
be used as long as the problem of preposition
disambiguation remains. The data used in this task
was developed in The Preposition Project (TPP,
Litkowski & Hargraves (2005) and Litkowski &
Hargraves (2006)),
1
 with further refinements to fit
the requirements of a SemEval task. 
In the following sections, we first describe the
motivations for a preposition disambiguation task.
Next, we describe the development of the datasets
used for the task, i.e., the instance sets and the sense
inventories. We describe how the task was performed
and how it was evaluated (essentially using the same
scoring methods as previous Senseval lexical sample
tasks). We present the results obtained from the
participating teams and provide an initial analysis of
these results. Finally, we identify several further
types of analyses that will provide further insights
into the characterization of preposition behavior.
2 Motivation
Prepositions are a closed class, meaning that the
number of prepositions remains relatively constant
and that their meanings are relatively stable. Despite
this, their treatment in computational linguistics has
been somewhat limited. In the Penn Treebank, only
two types of prepositions are recognized (IN
(locative, temporal, and manner) and TO (direction))
(O?Hara, 2005). Prepositions are viewed as function
words that occur with high frequency and therefore
carry little meaning. A task to disambiguate
prepositions would, in the first place, allow this
limited treatment to be confronted more fully.
Preposition behavior has been the subject of
much research, too voluminous to cite here. Three
recent workshops on prepositions have been
sponsored by the ACL-SIGSEM: Toulouse in 2003,
Colchester in 2005, and Trento in 2006. For the most
part, these workshops have focused on individual
prepositions, with various investigations of more
generalized behavior. The SemEval preposition
disambiguation task provides a vehicle to examine
whether these behaviors are substantiated with a
well-defined set of corpus instances.
Prepositions assume more importance when they1http://www.clres.com/prepositions.html.
24
are considered in relation to verbs. While linguistic
theory focuses on subjects and objects as important
verb arguments, quite frequently there is an
additional oblique argument realized in a
prepositional phrase. But with the focus on the verbs,
the prepositional phrases do not emerge as having
more than incidental importance. However, within
frame semantics (Fillmore, 1976), prepositions rise
to a greater prominence; frequently, two or three
prepositional phrases are identified as constituting
frame elements. In addition, frame semantic analyses
indicate the possibility of a greater number of
prepositional phrases acting as adjuncts (particularly
identifying time and location frame elements). While
linguistic theories may identify only one or two
prepositions associated with an argument of a verb,
frame semantic analyses bring in the possibility of a
greater variety of prepositions introducing the same
type of frame element. The preposition
disambiguation task provides an opportunity to
examine this type of variation.
The question of prepositional phrase attachment
is another important issue. Merlo & Esteve Ferrer
(2006) suggest that this problem is a four-way
disambiguation task, depending on the properties of
nouns and verbs and whether the prepositional
phrases are arguments or adjuncts. Their analysis
relied on Penn Treebank data. Further insights may
be available from the finer-grained data available in
the preposition disambiguation task.
Another important thread of investigation
concerning preposition behavior is the task of
semantic role (and perhaps semantic relation)
labeling (Gildea & Jurafsky, 2002). This task has
been the subject of a previous Senseval task
(Automatic Semantic Role Labeling, Litkowski
(2004)) and two shared tasks on semantic role
labeling in the Conference on Natural Language
Learning (Carreras & Marquez (2004) and Carreras
& Marquez (2005)). In addition, three other tasks in
SemEval-2007 (semantic relations between nominals,
task 4; temporal relation labeling, task 15; and frame
semantic structure extraction, task 19) address issues
of semantic role labeling. Since a great proportion of
these semantic roles are realized in prepositional
phrases, this gives greater urgency to understanding
preposition behavior.
Despite the predominant view of prepositions as
function words carrying little meaning, this view is
not borne out in dictionary treatment of their
definitions. To all appearances, prepositions exhibit
definitional behavior similar to that of open class
words. There is a reasonably large number of distinct
prepositions and they show a range of polysemous
senses. Thus, with a suitable set of instances, they
may be amenable to the same types of analyses as
open class words.
3 Preparation of Datasets
The development of the datasets for the preposition
disambiguation task grew directly out of TPP. This
project essentially articulates the corpus selection, the
lexicon choice, and the production of the gold
standard. The primary objective of TPP is to
characterize each of 847 preposition senses for 373
prepositions (including 220 phrasal prepositions with
309 senses)
2
 with a semantic role name and the
syntactic and semantic properties of its complement
and attachment point. The preposition sense
inventory is taken from the Oxford Dictionary of
English (ODE, 2004).
3
 
3.1 Corpus Development
For a particular preposition, a set of instances is
extracted from the FrameNet database.
4
 FrameNet
was chosen since it provides well-studied sentences
drawn from the British National Corpus (as well as
a limited set of sentences from other sources). Since
the sentences to be selected for frame analysis were
generally chosen for some open class verb or noun,
these sentences would be expected to provide no bias
with respect to prepositions. In addition, the use of
this resource makes available considerable
information for each sentence in its identification of
2
The number of prepositions and the number of senses
is not fixed, but has changed during the course of the
project, as will become clear.
3
TPP does not include particle senses of such words as
in or over (or any other particles) used with verbs to
make phrasal verbs. In this context, phrasal verbs are
to be distinguished from verbs that select a preposition
(such as on in rely on), which may be characterized as
a collocation.
4
http://framenet.icsi.berkeley.edu/ 
25
frame elements, their phrase type, and their
grammatical function. The FrameNet data was also
made accessible in a form (FrameNet Explorer)
5
 to
facilitate a lexicographer?s examination of
preposition instances.
Each sentence in the FrameNet data is labeled
with a subcorpus name. This name is generally
intended only to capture some property of a set of
instances. In particular, many of these subcorpus
names include a string ppprep and this identification
was used for the selection of instances. Thus,
searching the FrameNet corpus for subcorpora
labeled ppof or ppafter would yield sentences
containing a prepositional phrase with a desired
preposition. This technique was used for many
common prepositions, yielding 300 to 4500
instances. The technique was modified for
prepositions with fewer instances. Instead, all
sentences having a phrase beginning with a desired
preposition were selected. 
The number of sentences eventually used in the
SemEval task is shown in Table 1. More than 25,000
instances for 34 prepositions were tagged in TPP and
used for the SemEval-2007 task.
3.2 Lexicon Development
As mentioned above, ODE (and its predecessor, the
New Oxford Dictionary of English (NODE, 1997))
was used as the sense inventory for the prepositions.
ODE is a corpus-based, lexicographically-drawn
sense inventory, with a two-level hierarchy,
consisting of a set of core senses and a set of
subsenses (if any) that are semantically related to the
core sense. The full set of information, both printed
and in electronic form, containing additional
lexicographic information, was made publicly
available for TPP, and hence, the SemEval
disambiguation task.
The sense inventory was not used as absolute and
further information was added during TPP. The
lexicographer (Hargraves) was free to add senses,
particularly as the corpus evidence provided by the
FrameNet data suggested. The process of refining the
sense inventory was performed as the lexicographer
assigned a sense to each instance. While engaged in
this sense assignment, the lexicographer accumulated
an understanding of the behavior of the preposition,
assigning a name to each sense (characterizing its
semantic type), and characterizing the syntactic and
semantic properties of the preposition complement
and its point of attachment or head. Each sense was
also characterized by its syntactic function and its
meaning, identifying the relevant paragraph(s) where
it is discussed in Quirk et al(1985).
After sense assignments were completed, the set
of instances for each preposition was analyzed
against the FrameNet database. In particular, the
FrameNet frames and frame elements associated with
each sense was identified. The set of sentences was
provided in SemEval format in an XML file with the
preposition tagged as <head>, along with an answer
key (also identifying the FrameNet frame and frame
element). Finally, using the FrameNet frame and
frame element of the tagged instances, syntactic
alternation patterns (other syntactic forms in which
the semantic role may be realized) are provided for
each FrameNet target word for each sense.
All of the above information was combined into
a preposition database.
6
 For SemEval-2007, entries
for the target prepositions were combined into an
XML file as the ?Definitions? to be used as the sense
inventory, where each sense was given a unique
identifier. All prepositions for which a set of
instances had been analyzed in TPP were included.
These 34 prepositions are shown in Table 1 (below,
beyond, and near were used in the trial set).
3.3 Gold Standard Production
Unlike previous Senseval lexical sample tasks,
tagging was not performed as a separate step. Rather,
sense tagging was completed as an integral part of
TPP. Funding was unavailable to perform additional
tagging with other lexicographers and the appropriate
interannotator agreement studies have not yet been
completed. At this time, only qualitative assessments
of the tagging can be given.
As indicated, the sense inventory for each
preposition evolved as the lexicographer examined
5
Available for the Windows operating system at
http://www.clres.com for those with access to the
FrameNet data.
6
The full database is viewable in the Online TPP
(http://www.clres.com/cgi-bin/onlineTPP/find_prep.cgi
).
26
the set of FrameNet instances. Multiple sources (such
as Quirk et al) and lexicographic experience were
important components of the sense tagging. The
tagging was performed without any deadlines and
with full adherence to standard lexicographic
principles. Importantly, the availability of the
FrameNet corpora facilitated the sense assignment,
since many similar instances were frequently
contiguous in the instance set (e.g., associated with
the same target word and frame).
Another important factor suggesting higher
quality in the sense assignment is the quality of the
sense inventory. Unlike previous Senseval lexical
sample tasks, the sense inventory was developed
using lexicographic principles and was quite stable.
In arriving at the sense inventory, the lexicographer
was able to compare ODE with its predecessor
NODE, noting in most cases that the senses had not
changed or had changed in only minor ways. 
Finally, the lexicographer had little difficulty in
making sense assignments. The sense distinctions
were well enough drawn that there was relatively
little ambiguity given a sentence context. The
lexicographer was not constrained to selecting one
sense, but could tag a preposition with multiple
senses as deemed necessary. Out of 25,000 instances,
only 350 instances received multiple senses.
4 Task Organization and Evaluation
The organization followed standard SemEval
(Senseval) procedures. The data were prepared in
XML, using Senseval DTDs. That is, each instance
was labeled with an instance identifier as an XML
attribute. Within the <instance> tag, the FrameNet
sentence was labeled as the <context> and included
one item, the target preposition, in the <head> tag.
The FrameNet sentence identifier was used as the
instance identifier, enabling participants to make use
of other FrameNet data. Unlike lexical sample tasks
for open class words, only one sentence was provided
as the context. Although no examination of whether
this is sufficient context for prepositions, it seems
likely that all information necessary for preposition
disambiguation is contained in the local context.
A trial set of three prepositions was provided (the
three smallest instance sets that had been developed).
For each of the remaining 34 prepositions, the data
was split in a ratio of two to one between training and
test data. The training data included the sense
identifier. Table 1 shows the total number of
instances for each preposition, along with the number
in the training and the test sets.
Answers were submitted in the standard Senseval
format, consisting of the lexical item name, the
instance identifier, the system sense assignments, and
optional comments. Although participants were not
restricted to selecting only one sense, all did so and
did not provide either multiple senses or weighting of
different senses. Because of this, a simple Perl script
was used to score the results, giving precision, recall,
and F-score.
7
 The answers were also scored using the
standard Senseval scoring program, which records a
result for ?attempted? rather than F-score, with
precision interpreted as percent of attempted
instances that are correct and recall as percent of
total instances that are correct.
8
 Table 1 reports the
standard SemEval recall, while Tables 2 and 3 use
the standard notions of precision and recall.
5 Results
Tables 2 and 3 present the overall fine-grained and
coarse-grained results, respectively, for the three
participating teams (University of Melbourne, Ko?
University, and Instituto Trentino di Cultura, IRST).
The tables show the team designator, and the results
over all prepositions, giving the precision, the recall,
and the F-score. The table also shows the results for
two baselines. The FirstSense baseline selects the
first sense of each preposition as the answer (under
the assumption that the senses are organized
somewhat according to prominence). The FreqSense
baseline selects the most frequent sense from the
training set. Table 1 shows the fine-grained recall
scores for each team for each preposition. Table 1
also shows the entropy and perplexity for each
preposition, based on the data from the training sets.
7
Precision is the percent of total correct instances and
recall is the percent of instances attempted, so that an
F-score can be computed.
8
The standard SemEval (Senseval) scoring program,
scorer2, does not work to compute a coarse-grained
score for the preposition instances, since senses are
numbers such as ?4(2a)? and not alphabetic.
27
Table 2. Fine-Grained Scores
(All Prepositions - 8096 Instances)
Team Prec Rec F
MELB-YB 0.693 1.000 0.818
KU 0.547 1.000 0.707
IRST-BP 0.496 0.864 0.630
FirstSense 0.289 1.000 0.449
FreqSense 0.396 1.000 0.568
Table 3. Coarse-Grained Scores
(All Prepositions - 8096 Instances)
Team Prec Rec F
MELB-YB 0.755 1.000 0.861
KU 0.642 1.000 0.782
IRST-BP 0.610 0.864 0.715
FirstSense 0.441 1.000 0.612
FreqSense 0.480 1.000 0.649
As can be seen, all participating teams performed
significantly better than the baselines. Additional
improvements occurred at the coarse grain, although
the differences are not dramatically higher.
All participating teams used supervised systems,
using the training data for their submissions. The
University of Melbourne used a maximum entropy
system using a wide variety of syntactic and semantic
features. Ko? University used a statistical language
model (based on Google ngram data) to measure the
likelihood of various substitutes for various senses.
IRST-BP used Chain Clarifying Relationships, in
which contextual lexical and syntactic features of
representative contexts are used for learning sense
discriminative patterns. Further details on their
methods are available in their respective papers.
6 Discussion
Examination of the detailed results by preposition in
Table 1 shows that performance is inversely related
to polysemy. The greater number of senses leads to
reduced performance. The first sense heuristic has a
correlation of -0.64; the most frequent sense heuristic
has a correlation of -0.67. the correlations for
MELB, KU, and IRST are -0.40, -0.70, and -0.56,
respectively. The scores are also negatively
correlated with the number of test instances. The
correlations are -0.34 and -0.44 for the first sense
and the most frequent sense heuristics. For the
systems, the scores are -0.17, -0.48, and -0.39 for
Melb, KU, and IRST.
The scores for each preposition are strongly
negatively correlated with entropy and perplexity, as
frequently observed in lexical sample disambiguation.
For MELB-YB and IRST-BP, the correlation with
entropy is about -0.67, while for KU, the correlation
is -0.885. For perplexity, the correlation is -0.55 for
MELB-YB, -0.62 for IRST-ESP , and -0.82 for KU.
More detailed analysis is required to examine the
performance for each preposition, particularly for the
most frequent prepositions (of, in, from, with, to, for,
on, at, into, and by). Performance on these
prepositions ranged from fairly good to mediocre to
relatively poor. In addition, a comparison of the
various attributes of the TPP sense information with
the different performances might be fruitful. Little of
this information was used by the various systems.
7 Conclusions
The SemEval-2007 preposition disambiguation task
can be considered successful, with results that can be
exploited in general NLP tasks. In addition, the task
has generated considerable information for further
examination of preposition behavior.
References
Xavier Carreras and Lluis Marquez. 2004.
Introduction to the CoNLL-2004 Shared Task:
Semantic Role Labeling. In: Proceedings of
CoNLL-2004.
Xavier Carreras and Lluis Marquez. 2005.
Introduction to the CoNLL-2005 Shared Task:
Semantic Role Labeling. In: Proceedings of
CoNLL-2005.
Charles Fillmore. 1976. Frame Semantics and the
Nature of Language. Annals of the New York
Academy of Sciences, 280: 20-32.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational
Linguistics, 28 (3), 245-288.
Kenneth C. Litkowski. 2004. Senseval-3 Task:
Automatic Labeling of Semantic Roles. In
Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text. ACL. 9-12. 
Kenneth C. Litkowski & Orin Hargraves. 2005. The
Preposition Project. In: ACL-SIGSEM Workshop
on the Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic Formalisms
28
and Applications, University of Essex -
Colchester, United Kingdom. 171-179. 
Kenneth C. Litkowski.& Orin Hargraves.  2006.
Coverage and Inheritance in The Preposition
Project. In: Proceedings of the Third ACL-
SIGSEM Workshop on Prepositions. Trento, Italy.
ACL. 89-94.
Paola Merlo and Eva Esteve Ferrer. 2006. The Notion
of Argument in Prepositional Phrase Attachment.
Computational Linguistics, 32 (3), 341-377.
The New Oxford Dictionary of English. 1998. (J.
Pearsall, Ed.). Oxford: Clarendon Press.
Thomas P. O?Hara. 2005. Empirical Acquisition of
Conceptual Distinctions via Dictionary
Definitions. Ph.D. Thesis. New Mexico State .
The Oxford Dictionary of English. 2003. (A.
Stevension and C. Soanes, Eds.). Oxford:
Clarendon Press.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
& Jan Svartik. (1985). A comprehensive grammar
of the English language. London: Longman.
Table 1. SemEval-2007 Preposition Disambiguation
Prepostition Senses Ent Perp
Number of Instances
Fine-Grained Recall
Participating Teams Baselines
Total Training Test Melb KU IRST
First
Sense
Freq
Sense
about 6 0.63 1.54 1074 710 364 0.885 0.934 0.780 0.885 0.885
above 9 1.80 3.49 71 48 23 0.652 0.522 0.565 0.043 0.609
across 3 0.23 1.17 470 319 151 0.960 0.960 0.914 0.960 0.960
after 11 2.15 4.44 156 103 53 0.472 0.585 0.585 0.434 0.434
against 10 1.89 3.69 287 195 92 0.880 0.793 0.826 0.446 0.435
along 4 0.30 1.23 538 365 173 0.954 0.954 0.936 0.954 0.954
among 4 1.55 2.93 150 100 50 0.660 0.680 0.620 0.300 0.300
around 6 2.05 4.13 490 335 155 0.561 0.535 0.381 0.155 0.452
as 2 0.00 1.00 258 174 84 1.000 1.000 0.988 1.000 1.000
at 12 2.38 5.21 1082 715 367 0.790 0.662 0.646 0.425 0.425
before 4 1.33 2.51 67 47 20 0.600 0.850 0.800 0.450 0.450
behind 9 1.31 2.47 206 138 68 0.662 0.676 0.471 0.662 0.662
beneath 6 1.22 2.33 85 57 28 0.714 0.679 0.750 0.571 0.571
beside 3 0.00 1.00 91 62 29 1.000 1.000 1.000 1.000 1.000
between 9 2.11 4.31 313 211 102 0.814 0.765 0.892 0.422 0.422
by 22 2.53 5.77 758 510 248 0.730 0.556 0.391 0.000 0.371
down 5 1.18 2.26 485 332 153 0.654 0.647 0.680 0.438 0.438
during 2 1.00 2.00 120 81 39 0.769 0.564 0.667 0.615 0.385
for 15 2.84 7.17 1429 951 478 0.573 0.395 0.456 0.036 0.238
from 16 2.85 7.21 1784 1206 578 0.642 0.415 0.512 0.279 0.279
in 15 2.81 7.01 2085 1397 688 0.561 0.436 0.494 0.362 0.362
inside 5 1.63 3.10 105 67 38 0.579 0.579 0.605 0.368 0.526
into 10 2.14 4.41 901 604 297 0.616 0.539 0.586 0.290 0.451
like 7 1.26 2.40 391 266 125 0.856 0.808 0.592 0.120 0.768
of 20 3.14 8.80 4482 3004 1478 0.681 0.374 0.144 0.000 0.205
off 7 1.16 2.23 237 161 76 0.658 0.776 0.408 0.171 0.763
on 25 3.42 10.68 1313 872 441 0.624 0.469 0.351 0.218 0.206
onto 3 0.60 1.52 175 117 58 0.879 0.879 0.776 0.879 0.879
over 17 2.52 5.73 298 200 98 0.510 0.510 0.480 0.010 0.327
round 8 2.31 4.95 263 181 82 0.610 0.512 0.000 0.037 0.378
through 16 2.71 6.54 649 441 208 0.524 0.538 0.481 0.322 0.495
to 17 2.43 5.38 1755 1183 572 0.745 0.579 0.558 0.322 0.322
towards 6 0.71 1.63 316 214 102 0.931 0.873 0.833 0.873 0.873
with 18 3.05 8.27 1769 1191 578 0.699 0.455 0.635 0.149 0.249
Total 332 24653 16557 8096 0.693 0.547 0.496 0.289 0.396
29
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 30?35,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 07: Coarse-Grained English All-Words Task
Roberto Navigli
Universita` di Roma ?La Sapienza?
Dipartimento di Informatica
Via Salaria, 00198 - Roma Italy
navigli@di.uniroma1.it
Kenneth C. Litkowski
CL Research
9208 Gue Road
Damascus MD 20872
ken@clres.com
Orin Hargraves
Lexicographer
orinhargraves
@googlemail.com
Abstract
This paper presents the coarse-grained En-
glish all-words task at SemEval-2007. We
describe our experience in producing a
coarse version of the WordNet sense inven-
tory and preparing the sense-tagged corpus
for the task. We present the results of par-
ticipating systems and discuss future direc-
tions.
1 Introduction
It is commonly thought that one of the major obsta-
cles to high-performance Word Sense Disambigua-
tion (WSD) is the fine granularity of sense inven-
tories. State-of-the-art systems attained a disam-
biguation accuracy around 65% in the Senseval-3
all-words task (Snyder and Palmer, 2004), where
WordNet (Fellbaum, 1998) was adopted as a ref-
erence sense inventory. Unfortunately, WordNet is
a fine-grained resource, encoding sense distinctions
that are difficult to recognize even for human an-
notators (Edmonds and Kilgarriff, 2002). Making
WSD an enabling technique for end-to-end applica-
tions clearly depends on the ability to deal with rea-
sonable sense distinctions.
The aim of this task was to explicitly tackle the
granularity issue and study the performance of WSD
systems on an all-words basis when a coarser set
of senses is provided for the target words. Given
the need of the NLP community to work on freely
available resources, the solution of adopting a dif-
ferent computational lexicon is not viable. On the
other hand, the production of a coarse-grained sense
inventory is not a simple task. The main issue
is certainly the subjectivity of sense clusters. To
overcome this problem, different strategies can be
adopted. For instance, in the OntoNotes project
(Hovy et al, 2006) senses are grouped until a 90%
inter-annotator agreement is achieved. In contrast,
as we describe in this paper, our approach is based
on a mapping to a previously existing inventory
which encodes sense distinctions at different levels
of granularity, thus allowing to induce a sense clus-
tering for the mapped senses.
We would like to mention that another SemEval-
2007 task dealt with the issue of sense granularity
for WSD, namely Task 17 (subtask #1): Coarse-
grained English Lexical Sample WSD. In this paper,
we report our experience in organizing Task 07.
2 Task Setup
The task required participating systems to annotate
open-class words (i.e. nouns, verbs, adjectives, and
adverbs) in a test corpus with the most appropriate
sense from a coarse-grained version of the WordNet
sense inventory.
2.1 Test Corpus
The test data set consisted of 5,377 words of run-
ning text from five different articles: the first three
(in common with Task 17) were obtained from the
WSJ corpus, the fourth was the Wikipedia entry for
computer programming1, the fifth was an excerpt of
Amy Steedman?s Knights of the Art, biographies of
Italian painters2. We decided to add the last two
1http://en.wikipedia.org/wiki/Computer programming
2http://www.gutenberg.org/etext/529
30
article domain words annotated
d001 JOURNALISM 951 368
d002 BOOK REVIEW 987 379
d003 TRAVEL 1311 500
d004 COMPUTER SCIENCE 1326 677
d005 BIOGRAPHY 802 345
total 5377 2269
Table 1: Statistics about the five articles in the test
data set.
texts to the initial dataset as we wanted the corpus to
have a size comparable to that of previous editions
of all-words tasks.
In Table 1 we report the domain, number of run-
ning words, and number of annotated words for the
five articles. We observe that articles d003 and d004
are the largest in the corpus (they constitute 51.87%
of it).
2.2 Creation of a Coarse-Grained Sense
Inventory
To tackle the granularity issue, we produced a
coarser-grained version of the WordNet sense inven-
tory3 based on the procedure described by Navigli
(2006). The method consists of automatically map-
ping WordNet senses to top level, numbered entries
in the Oxford Dictionary of English (ODE, (Soanes
and Stevenson, 2003)). The semantic mapping be-
tween WordNet and ODE entries was obtained in
two steps: first, we disambiguated with the SSI algo-
rithm (Navigli and Velardi, 2005) the definitions of
the two dictionaries, together with additional infor-
mation (hypernyms and domain labels); second, for
each WordNet sense, we determined the best match-
ing ODE coarse entry. As a result, WordNet senses
mapped to the same ODE entry were assigned to the
same sense cluster. WordNet senses with no match
were associated with a singleton sense.
In contrast to the automatic method above, the
sense mappings for all the words in our test cor-
pus were manually produced by the third author, an
expert lexicographer, with the aid of a mapping in-
terface. Not all the words in the corpus could be
mapped directly for several reasons: lacking entries
in ODE (e.g. adjectives underlying and shivering),
3We adopted WordNet 2.1, available from:
http://wordnet.princeton.edu
different spellings (e.g. after-effect vs. aftereffect,
halfhearted vs. half-hearted, etc.), derivatives (e.g.
procedural, gambler, etc.). In most of the cases, we
asked the lexicographer to map senses of the orig-
inal word to senses of lexically-related words (e.g.
WordNet senses of procedural were mapped to ODE
senses of procedure, etc.). When this mapping was
not straightforward, we just adopted the WordNet
sense inventory for that word.
We released the entire sense groupings (those in-
duced from the manual mapping for words in the
test set plus those automatically derived on the other
words) and made them available to the participants.
2.3 Sense Annotation
All open-class words (i.e. nouns, verbs, adjectives,
and adverbs) with an existing sense in the WordNet
inventory were manually annotated by the third au-
thor. Multi-word expressions were explicitly iden-
tified in the test set and annotated as such (this was
made to allow a fair comparison among systems in-
dependent of their ability to identify multi-word ex-
pressions).
We excluded auxiliary verbs, uncovered phrasal
and idiomatic verbs, exclamatory uses, etc. The
annotator was allowed to tag words with multiple
coarse senses, but was asked to make a single sense
assignment whenever possible.
The lexicographer annotated an overall number
of 2,316 content words. 47 (2%) of them were ex-
cluded because no WordNet sense was deemed ap-
propriate. The remaining 2,269 content words thus
constituted the test data set. Only 8 of them were as-
signed more than one sense: specifically, two coarse
senses were assigned to a single word instance4 and
two distinct fine-grained senses were assigned to 7
word instances. This was a clear hint that the sense
clusters were not ambiguous for the vast majority of
words.
In Table 2 we report information about the pol-
ysemy of the word instances in the test set. Over-
all, 29.88% (678/2269) of the word instances were
monosemous (according to our coarse sense inven-
tory). The average polysemy of the test set with the
coarse-grained sense inventory was 3.06 compared
to an average polysemy with the WordNet inventory
4d005.s004.t015
31
polysemy N V A R all
monosemous 358 86 141 93 678
polysemous 750 505 221 115 1591
total 1108 591 362 208 2269
Table 2: Statistics about the test set polysemy (N =
nouns, V = verbs, A = adjectives, R = adverbs).
of 6.18.
2.4 Inter-Annotator Agreement
Recent estimations of the inter-annotator agreement
when using the WordNet inventory report figures of
72.5% agreement in the preparation of the English
all-words test set at Senseval-3 (Snyder and Palmer,
2004) and 67.3% on the Open Mind Word Expert an-
notation exercise (Chklovski and Mihalcea, 2002).
As the inter-annotator agreement is often consid-
ered an upper bound for WSD systems, it was de-
sirable to have a much higher number for our task,
given its coarse-grained nature. To this end, beside
the expert lexicographer, a second author indepen-
dently performed part of the manual sense mapping
(590 word senses) described in Section 2.2. The
pairwise agreement was 86.44%.
We repeated the same agreement evaluation on
the sense annotation task of the test corpus. A sec-
ond author independently annotated part of the test
set (710 word instances). The pairwise agreement
between the two authors was 93.80%. This figure,
compared to those in the literature for fine-grained
human annotations, gives us a clear indication that
the agreement of human annotators strictly depends
on the granularity of the adopted sense inventory.
3 Baselines
We calculated two baselines for the test corpus: a
random baseline, in which senses are chosen at
random, and the most frequent baseline (MFS), in
which we assign the first WordNet sense to each
word in the dataset.
Formally, the accuracy of the random baseline
was calculated as follows:
BLRand = 1|T |
|T |?
i=1
1
|CoarseSenses(wi)|
where T is our test corpus, wi is the i-th word
instance in T , and CoarseSenses(wi) is the set of
coarse senses for wi according to the sense cluster-
ing we produced as described in Section 2.2.
The accuracy of the MFS baseline was calculated
as:
BLMFS = 1|T |
|T |?
i=1
?(wi, 1)
where ?(wi, k) equals 1 when the k-th sense of
word wi belongs to the cluster(s) manually associ-
ated by the lexicographer to word wi (0 otherwise).
Notice that our calculation of the MFS is based on
the frequencies in the SemCor corpus (Miller et al,
1993), as we exploit WordNet sense rankings.
4 Results
12 teams submitted 14 systems overall (plus two
systems from a 13th withdrawn team that we will
not report). According to the SemEval policy for
task organizers, we remark that the system labelled
as UOR-SSI was submitted by the first author (the
system is based on the Structural Semantic Inter-
connections algorithm (Navigli and Velardi, 2005)
with a lexical knowledge base composed by Word-
Net and approximately 70,000 relatedness edges).
Even though we did not specifically enrich the al-
gorithm?s knowledge base on the task at hand, we
list the system separately from the overall ranking.
The results are shown in Table 3. We calcu-
lated a MFS baseline of 78.89% and a random base-
line of 52.43%. In Table 4 we report the F1 mea-
sures for all systems where we used the MFS as a
backoff strategy when no sense assignment was at-
tempted (this possibly reranked 6 systems - marked
in bold in the table - which did not assign a sense
to all word instances in the test set). Compared
to previous results on fine-grained evaluation exer-
cises (Edmonds and Kilgarriff, 2002; Snyder and
Palmer, 2004), the systems? results are much higher.
On the other hand, the difference in performance
between the MFS baseline and state-of-the-art sys-
tems (around 5%) on coarse-grained disambiguation
is comparable to that of the Senseval-3 all-words ex-
ercise. However, given the novelty of the task we
believe that systems can achieve even better perfor-
32
System A P R F1
NUS-PT 100.0 82.50 82.50 82.50
NUS-ML 100.0 81.58 81.58 81.58
LCC-WSD 100.0 81.45 81.45 81.45
GPLSI 100.0 79.55 79.55 79.55
BLMFS 100.0 78.89 78.89 78.89
UPV-WSD 100.0 78.63 78.63 78.63
TKB-UO 100.0 70.21 70.21 70.21
PU-BCD 90.1 69.72 62.80 66.08
RACAI-SYNWSD 100.0 65.71 65.71 65.71
SUSSX-FR 72.8 71.73 52.23 60.44
USYD 95.3 58.79 56.02 57.37
UOFL 92.7 52.59 48.74 50.60
SUSSX-C-WD 72.8 54.54 39.71 45.96
SUSSX-CR 72.8 54.30 39.53 45.75
UOR-SSI? 100.0 83.21 83.21 83.21
Table 3: System scores sorted by F1 measure (A =
attempted, P = precision, R = recall, F1 = F1 mea-
sure, ?: system from one of the task organizers).
mance by heavily exploiting the coarse nature of the
sense inventory.
In Table 5 we report the results for each of the
five articles. The interesting aspect of the table is
that documents from some domains seem to have
predominant senses different from those in Sem-
Cor. Specifically, the MFS baseline performs more
poorly on documents d004 and d005, from the
COMPUTER SCIENCE and BIOGRAPHY domains
respectively. We believe this is due to the fact that
these documents have specific predominant senses,
which correspond less often to the most frequent
sense in SemCor than for the other three documents.
It is also interesting to observe that different systems
perform differently on the five documents (we high-
light in bold the best performing systems on each
article).
Finally, we calculated the systems? performance
by part of speech. The results are shown in Table
6. Again, we note that different systems show dif-
ferent performance depending on the part-of-speech
tag. Another interesting aspect is that the perfor-
mance of the MFS baseline is very close to state-of-
the-art systems for adjectives and adverbs, whereas
it is more than 3 points below for verbs, and around
5 for nouns.
System F1
NUS-PT 82.50
NUS-ML 81.58
LCC-WSD 81.45
GPLSI 79.55
BLMFS 78.89
UPV-WSD 78.63
SUSSX-FR 77.04
TKB-UO 70.21
PU-BCD 69.72
RACAI-SYNWSD 65.71
SUSSX-C-WD 64.52
SUSSX-CR 64.35
USYD 58.79
UOFL 54.61
UOR-SSI? 83.21
Table 4: System scores sorted by F1 measure with
MFS adopted as a backoff strategy when no sense
assignment is attempted (?: system from one of the
task organizers). Systems affected are marked in
bold.
System N V A R
NUS-PT 82.31 78.51 85.64 89.42
NUS-ML 81.41 78.17 82.60 90.38
LCC-WSD 80.69 78.17 85.36 87.98
GPLSI 80.05 74.45 82.32 86.54
BLMFS 77.44 75.30 84.25 87.50
UPV-WSD 79.33 72.76 84.53 81.25
TKB-UO 70.76 62.61 78.73 74.04
PU-BCD 71.41 59.69 66.57 55.67
RACAI-SYNWSD 64.02 62.10 71.55 75.00
SUSSX-FR 68.09 51.02 57.38 49.38
USYD 56.06 60.43 58.00 54.31
UOFL 57.65 48.82 25.87 60.80
SUSSX-C-WD 52.18 35.64 42.95 46.30
SUSSX-CR 51.87 35.44 42.95 46.30
UOR-SSI? 84.12 78.34 85.36 88.46
Table 6: System scores by part-of-speech tag (N
= nouns, V = verbs, A = adjectives, R = adverbs)
sorted by overall F1 measure (best scores are marked
in bold, ?: system from one of the task organizers).
33
d001 d002 d003 d004 d005
System P R P R P R P R P R
NUS-PT 88.32 88.32 88.13 88.13 83.40 83.40 76.07 76.07 81.45 81.45
NUS-ML 86.14 86.14 88.39 88.39 81.40 81.40 76.66 76.66 79.13 79.13
LCC-WSD 87.50 87.50 87.60 87.60 81.40 81.40 75.48 75.48 80.00 80.00
GPLSI 83.42 83.42 86.54 86.54 80.40 80.40 73.71 73.71 77.97 77.97
BLMFS 85.60 85.60 84.70 84.70 77.80 77.80 75.19 75.19 74.20 74.20
UPV-WSD 84.24 84.24 80.74 80.74 76.00 76.00 77.11 77.11 77.10 77.10
TKB-UO 78.80 78.80 72.56 72.56 69.40 69.40 70.75 70.75 58.55 58.55
PU-BCD 77.16 67.94 75.52 67.55 64.96 58.20 68.86 61.74 64.42 60.87
RACAI-SYNWSD 71.47 71.47 72.82 72.82 66.80 66.80 60.86 60.86 59.71 59.71
SUSSX-FR 79.10 57.61 73.72 53.30 74.86 52.40 67.97 48.89 65.20 51.59
USYD 62.53 61.69 59.78 57.26 60.97 57.80 60.57 56.28 47.15 45.51
UOFL 61.41 59.24 55.93 52.24 48.00 45.60 53.42 47.27 44.38 41.16
SUSSX-C-WD 66.42 48.37 61.31 44.33 55.14 38.60 50.72 36.48 42.13 33.33
SUSSX-CR 66.05 48.10 60.58 43.80 59.14 41.40 48.67 35.01 40.29 31.88
UOR-SSI? 86.14 86.14 85.49 85.49 79.60 79.60 86.85 86.85 75.65 75.65
Table 5: System scores by article (best scores are marked in bold, ?: system from one of the task organizers).
5 Systems Description
In order to allow for a critical and comparative in-
spection of the system results, we asked the partici-
pants to answer some questions about their systems.
These included information about whether:
1. the system used semantically-annotated and
unannotated resources;
2. the system used the MFS as a backoff strategy;
3. the system used the coarse senses provided by
the organizers;
4. the system was trained on some corpus.
We believe that this gives interesting information
to provide a deeper understanding of the results. We
summarize the participants? answers to the question-
naires in Table 7. We report about the use of seman-
tic resources as well as semantically annotated cor-
pora (SC = SemCor, DSO = Defence Science Organ-
isation Corpus, SE = Senseval corpora, OMWE =
Open Mind Word Expert, XWN = eXtended Word-
Net, WN = WordNet glosses and/or relations, WND
= WordNet Domains), as well as information about
the use of unannotated corpora (UC), training (TR),
MFS (based on the SemCor sense frequencies), and
the coarse senses provided by the organizers (CS).
As expected, several systems used lexico-semantic
information from the WordNet semantic network
and/or were trained on the SemCor semantically-
annotated corpus.
Finally, we point out that all the systems perform-
ing better than the MFS baseline adopted it as a
backoff strategy when they were not able to output a
sense assignment.
6 Conclusions and Future Directions
It is commonly agreed that Word Sense Disambigua-
tion needs emerge and show its usefulness in end-
to-end applications: after decades of research in the
field it is still unclear whether WSD can provide
a relevant contribution to real-world applications,
such as Information Retrieval, Question Answering,
etc. In previous Senseval evaluation exercises, state-
of-the-art systems achieved performance far below
70% and even the agreement between human anno-
tators was discouraging. As a result of the discus-
sion at the Senseval-3 workshop in 2004, one of the
aims of SemEval-2007 was to tackle the problems
at the roots of WSD. In this task, we dealt with the
granularity issue which is a major obstacle to both
system and human annotators. In the hope of over-
coming the current performance upper bounds, we
34
System SC DSO SE OMWE XWN WN WND OTHER UC TR MFS CS
GPLSI
? ? ? ? ? ? ? ? ? ? ? ?
LCC-WSD
? ? ? ? ? ? ? ? ? ? ? ?
NUS-ML
? ? ? ? ? ? ? ? ? ? ? ?
NUS-PT
? ? ? ? ? ? ? Parallel corpus ? ? ? ?
PU-BCD
? ? ? ? ? ? ? ? ? ? ? ?
RACAI-SYNWSD ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-C-WD ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-CR ? ? ? ? ? ? ? ? ? ? ? ?
SUSSX-FR ? ? ? ? ? ? ? ? ? ? ? ?
TKB-UO ? ? ? ? ? ? ? ? ? ? ? ?
UOFL ? ? ? ? ? ? ? ? ? ? ? ?
UOR-SSI? ? ? ? ? ? ? ? SSI LKB ? ? ? ?
UPV-WSD ? ? ? ? ? ? ? ? ? ? ? ?
USYD
? ? ? ? ? ? ? ? ? ? ? ?
Table 7: Information about participating systems (SC = SemCor, DSO = Defence Science Organisation
Corpus, SE = Senseval corpora, OMWE = Open Mind Word Expert, XWN = eXtended WordNet, WN =
WordNet glosses and/or relations, WND = WordNet Domains, UC = use of unannotated corpora, TR = use
of training, MFS = most frequent sense backoff strategy, CS = use of coarse senses from the organizers, ?:
system from one of the task organizers).
proposed the adoption of a coarse-grained sense in-
ventory. We found the results of participating sys-
tems interesting and stimulating. However, some
questions arise. First, it is unclear whether, given
the novelty of the task, systems really achieved the
state of the art or can still improve their performance
based on a heavier exploitation of coarse- and fine-
grained information from the adopted sense inven-
tory. We observe that, on a technical domain such
as computer science, most supervised systems per-
formed worse due to the nature of their training set.
Second, we still need to show that coarse senses can
be useful in real applications. Third, a full coarse
sense inventory is not yet available: this is a major
obstacle to large-scale in vivo evaluations. We be-
lieve that these aspects deserve further investigation
in the years to come.
Acknowledgments
This work was partially funded by the Interop NoE (508011),
6th European Union FP. We would like to thank Martha Palmer
for providing us the first three texts of the test corpus.
References
Tim Chklovski and Rada Mihalcea. 2002. Building a sense
tagged corpus with open mind word expert. In Proc. of ACL
2002 Workshop on WSD: Recent Successes and Future Di-
rections. Philadelphia, PA.
Philip Edmonds and Adam Kilgarriff. 2002. Introduction to the
special issue on evaluating word sense disambiguation sys-
tems. Journal of Natural Language Engineering, 8(4):279?
291.
Christiane Fellbaum, editor. 1998. WordNet: an Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Comp. Volume, pages 57?
60, New York City, USA.
George A. Miller, Claudia Leacock, Randee Tengi, and Ross T.
Bunker. 1993. A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technology, pages
303?308, Princeton, NJ, USA.
Roberto Navigli and Paola Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach to word
sense disambiguation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence (PAMI), 27(7):1063?1074.
Roberto Navigli. 2006. Meaningful clustering of senses helps
boost word sense disambiguation performance. In Proc. of
the 44th Annual Meeting of the Association for Computa-
tional Linguistics joint with the 21st International Confer-
ence on Computational Linguistics (COLING-ACL 2006),
pages 105?112. Sydney, Australia.
Benjamin Snyder and Martha Palmer. 2004. The english all-
words task. In Proc. of ACL 2004 SENSEVAL-3 Workshop,
pages 41?43. Barcelona, Spain.
Catherine Soanes and Angus Stevenson, editors. 2003. Oxford
Dictionary of English. Oxford University Press.
35
