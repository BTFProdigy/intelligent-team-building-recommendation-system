Using Syntactic Information to Extract Relevant Terms for Multi-Document
Summarization
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/ Juan del Rosal, 16 - 28040 Madrid - Spain
http://nlp.uned.es
Abstract
The identification of the key concepts in a set of
documents is a useful source of information for
several information access applications. We are
interested in its application to multi-document
summarization, both for the automatic genera-
tion of summaries and for interactive summa-
rization systems.
In this paper, we study whether the syntactic po-
sition of terms in the texts can be used to predict
which terms are good candidates as key con-
cepts. Our experiments show that a) distance
to the verb is highly correlated with the proba-
bility of a term being part of a key concept; b)
subject modifiers are the best syntactic locations
to find relevant terms; and c) in the task of auto-
matically finding key terms, the combination of
statistical term weights with shallow syntactic
information gives better results than statistical
measures alone.
1 Introduction
The fundamental question addressed in this article
is: can syntactic information be used to find the
key concepts of a set of documents? We will pro-
vide empirical answers to this question in a multi-
document summarization environment.
The identification of key terms out of a set of doc-
uments is a common problem in information access
applications and, in particular, in text summariza-
tion: a fragment containing one or more key con-
cepts can be a good candidate to be part of a sum-
mary.
In single-document summarization, key terms are
usually obtained from the document title or head-
ing (Edmundson, 1969; Preston, 1994; Kupiec et
al., 1995). In multi-document summarization, how-
ever, some processing is needed to identify key con-
cepts (Lin and Hovy, 2002; Kraaij et al, 2002;
Schlesinger et al, 2002). Most approaches are
based on statistical criteria.
Criteria to elaborate a manual summary depend,
by and large, on the user interpretation of both the
information need and the content of documents.
This is why this task has also been attempted from
an interactive perspective (Boguraev et al, 1998;
Buyukkokten et al, 1999; Neff and Cooper, 1999;
Jones et al, 2002; Leuski et al, 2003). A standard
feature of such interactive summarization assistants
is that they offer a list of relevant terms (automati-
cally extracted from the documents) which the user
may select to decide or refine the focus of the sum-
mary.
Our hypothesis is that the key concepts of a doc-
ument set will tend to appear in certain syntactic
functions along the sentences and clauses of the
texts. To confirm this hypothesis, we have used
a test bed with manually produced summaries to
study:
? which are the most likely syntactic functions
for the key concepts manually identified in the
document sets.
? whether this information can be used to auto-
matically extract the relevant terms from a set
of documents, as compared to standard statis-
tical term weights.
Our reference corpus is a set of 72 lists of key
concepts, manually elaborated by 9 subjects on
8 different topics, with 100 documents per topic.
It was built to study Information Synthesis tasks
(Amigo et al, 2004) and it is, to the best of
our knowledge, the multi-document summarization
testbed with a largest number of documents per
topic. This feature enables us to obtain reliable
statistics on term occurrences and prominent syn-
tactic functions.
The paper is organized as follows: in Section 2
we review the main approaches to the evaluation
of automatically extracted key concepts for summa-
rization. In Section 3 we describe the creation of the
reference corpus. In Section 4 we study the correla-
tion between key concepts and syntactic function in
texts, and in Section 5 we discuss the experimental
results of syntactic function as a predictor to extract
key concepts. Finally, in Section 6 we draw some
conclusions.
2 Evaluation of automatically extracted
key concepts
It is necessary, in the context of an interactive sum-
marization system, to measure the quality of the
terms suggested by the system, i.e., to what extent
they are related to the key topics of the document
set.
(Lin and Hovy, 1997) compared different strate-
gies to generate lists of relevant terms for summa-
rization using Topic Signatures. The evaluation was
extrinsic, comparing the quality of the summaries
generated by a system using different term lists as
input. The results, however, cannot be directly ex-
trapolated to interactive summarization systems, be-
cause the evaluation does not consider how informa-
tive terms are for a user.
From an interactive point of view, the evaluation
of term extraction approaches can be done, at least,
in two ways:
? Evaluating the summaries produced in the in-
teractive summarization process. This option
is difficult to implement (how do we evaluate
a human produced summary? What is the ref-
erence gold standard?) and, in any case, it is
too costly: every alternative approach would
require at least a few additional subjects per-
forming the summarization task.
? Comparing automatically generated term lists
with manually generated lists of key concepts.
For instance, (Jones et al, 2002) describes a
process of supervised learning of key concepts
from a training corpus of manually generated
lists of phrases associated to a single docu-
ment.
We will, therefore, use the second approach,
evaluating the quality of automatically generated
term lists by comparing them to lists of key con-
cepts which are generated by human subjects after a
multi-document summarization process.
3 Test bed: the ISCORPUS
We have created a reference test bed, the ISCOR-
PUS1 (Amigo et al, 2004) which contains 72 man-
ually generated reports summarizing the relevant in-
formation for a given topic contained in a large doc-
ument set.
For the creation of the corpus, nine subjects per-
formed a complex multi-document summarization
1Available at http://nlp.uned.es/ISCORPUS.
task for eight different topics and one hundred rele-
vant documents per topic. After creating each topic-
oriented summary, subjects were asked to make a
list of relevant concepts for the topic, in two cate-
gories: relevant entities (people, organizations, etc.)
and relevant factors (such as ?ethnic conflicts? as
the origin of a civil war) which play a key role in
the topic being summarized.
These are the relevant details of the ISCORPUS
test bed:
3.1 Document collection and topic set
We have used the Spanish CLEF 2001-2003 news
collection testbed (Peters et al, 2002), and selected
the eight topics with the largest number of docu-
ments manually judged as relevant from the CLEF
assessment pools. All the selected CLEF topics
have more than one hundred documents judged as
relevant by the CLEF assessors; for homogeneity,
we have restricted the task to the first 100 docu-
ments for each topic (using a chronological order).
This set of eight CLEF topics was found to have
two differentiated subsets: in six topics, it is neces-
sary to study how a situation evolves in time: the
importance of every event related to the topic can
only be established in relation with the others. The
invasion of Haiti by UN and USA troops is an ex-
ample of such kind of topics. We refer to them as
?Topic Tracking? (TT) topics, because they are suit-
able for such a task. The other two questions, how-
ever, resemble ?Information Extraction? (IE) tasks:
essentially, the user has to detect and describe in-
stances of a generic event (for instance, cases of
hunger strikes and campaigns against racism in Eu-
rope in this case); hence we will refer to them as IE
summaries.
3.2 Generation of manual summaries
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of summaries. All
subjects were given an in-place detailed description
of the task, in order to minimize divergent interpre-
tations. They were told they had to generate sum-
maries with a maximum of information about ev-
ery topic within a 50 sentence space limit, using a
maximum of 30 minutes per topic. The 50 sentence
limit can be temporarily exceeded and, once the 30
minutes have expired, the user can still remove sen-
tences from the summary until the sentence limit is
reached back.
3.3 Manual identification of key concepts
After summarizing every topic, the following ques-
tionnaire was filled in by users:
? Who are the main people involved in the topic?
? What are the main organizations participating in the topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e., a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
a topic about the invasion of Haiti by UN and USA
troops:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is generated
for each topic, joining all the answers given by the
nine subjects. These lists of key concepts constitute
the gold standard for all the experiments described
below.
3.4 Shallow parsing of documents
Documents are processed with a robust shallow
parser based in finite automata. The parser splits
sentences in chunks and assigns a label to every
chunk. The set of labels is:
? [N]: noun phrases, which correspond to
names or adjectives preceded by a determiner,
punctuation sign, or beginning of a sentence.
? [V]: verb forms.
? [Mod]: adverbial and prepositional phrases,
made up of noun phrases introduced by an ad-
verb or preposition. Note that this is the mech-
anism to express NP modifiers in Spanish (as
compared to English, where noun compound-
ing is equally frequent).
? [Sub]: words introducing new subordinate
clauses within a sentence (que, cuando, mien-
tras, etc.).
? [P]: Punctuation marks.
This is an example output of the chunker:
Previamente [Mod] ,[P]el presidente Bill Clinton [N] hab??a di-
cho [V] que [Sub] tenemos [V] la obligacion [N] de cambiar la
pol??tica estadounidense [Mod] que [Sub] no ha funcionado [V] en
Hait?? [Mod].[P]
Although the precision of the parser is limited,
the results are good enough for the statistical mea-
sures used in our experiments.
4 Distribution of key concepts in syntactic
structures
We have extracted empirical data to answer these
questions:
? Is the probability of finding a key concept cor-
related with the distance to the verb in a sen-
tence or clause?
? Is the probability of finding a key concept in a
noun phrase correlated with the syntactic func-
tion of the phrase (subject, object, etc.)?
? Within a noun phrase, where is it more likely
to find key concepts: in the noun phrase head,
or in the modifiers?
We have used certain properties of Spanish syn-
tax (such as being an SVO language) to decide
which noun phrases play a subject function, which
are the head and modifiers of a noun phrase, etc. For
instance, NP modifiers usually appear after the NP
head in Spanish, and the specification of a concept
is usually made from left to right.
4.1 Distribution of key concepts with verb
distance
Figure 1 shows, for every topic, the probability of
finding a word from the manual list of key con-
cepts in fixed distances from the verb of a sen-
tence. Stop words are not considered for computing
word distance. The broader line represents the aver-
age across topics, and the horizontal dashed line is
the average probability across all positions, i.e., the
probability that a word chosen at random belongs to
the list of key concepts.
The plot shows some clear tendencies in the data:
the probability gets higher when we get close to the
verb, falls abruptly after the verb, and then grows
steadily again. For TT topics, the probability of
finding relevant concepts immediately before the
verb is 56% larger than the average (0.39 before the
verb, versus 0.25 in any position). This is true not
only as an average, but also for all individual TT
topics. This can be an extremely valuable result: it
shows a direct correlation between the position of a
term in a sentence and the importance of the term
in the topic. Of course, this direct distance to the
verb should be adapted for languages with different
syntactic properties, and should be validated for dif-
ferent domains.
The behavior of TT and IE topics is substantially
different. IE topics have smaller probabilities over-
all, because there are less key concepts common to
all documents. For instance, if the topic is ?cases of
hunger strikes?, there is little in common between
Figure 1: Probability of finding key concepts at fixed distances from verb
all cases of hunger strikes found in the collection;
each case has its own relevant people and organiza-
tions, for instance. Users try to make abstraction of
individual cases to write key concepts, and then the
number of key concepts is smaller. The tendency
to have larger probabilities just before the verb and
smaller probabilities just after the verb, however,
can also be observed for IE topics.
Figure 2: Probability of finding key concepts in sub-
ject NPs versus other NPs
4.2 Key Concepts and Noun Phrase Syntactic
Function
We wanted also to confirm that it is more likely to
find a key concept in a subject noun phrase than
in general NPs. For this, we have split compound
sentences in chunks, separating subordinate clauses
([Sub] type chunks). Then we have extracted se-
quences with the pattern [N][Mod]*. We assume
that the sentence subject is a sequence [N][Mod]*
occurring immediately before the verb. For in-
stance:
El presidente [N] en funciones [Mod] de
Hait?? [Mod] ha afirmado [V] que [Sub]...
The rest of [N] and [Mod] chunks are consid-
ered as part of the sentence verb phrase. In a ma-
jority of cases, these assumptions lead to a correct
identification of the sentence subject. We do not
capture, however, subjects of subordinate sentences
or subjects appearing after the verb.
Figure 2 shows how the probability of finding a
key concept is always larger in sentence subjects.
This result supports the assumption in (Boguraev
et al, 1998), where noun phrases receive a higher
weight, as representative terms, if they are syntactic
subjects.
4.3 Distribution of key concepts within noun
phrases
Figure 3: Probability of finding key concepts in NP
head versus NP modifiers
For this analysis, we assume that, in
[N][Mod]* sequences identified as subjects,
[N] is the head and [Mod]* are the modifiers.
Figure 3 shows that the probability of finding a
key concept in the NP modifiers is always higher
than in the head (except for topic TT3, where it is
equal). This is not intuitive a priori; an examination
of the data reveals that the most characteristic con-
cepts for a topic tend to be in the complements: for
instance, in ?the president of Haiti?, ?Haiti? carries
more domain information than ?president?. This
seems to be the most common case in our news
collection. Of course, it cannot be guaranteed that
these results will hold in other domains.
5 Automatic Selection of Key Terms
We have shown that there is indeed a correlation be-
tween syntactic information and the possibility of
finding a key concept. Now, we want to explore
whether this syntactic information can effectively
be used for the automatic extraction of key concepts.
The problem of extracting key concepts for sum-
marization involves two related issues: a) What
kinds of terms should be considered as candidates?
and b) What is the optimal weighting criteria for
them?
There are several possible answers to the first
question. Previous work includes using noun
phrases (Boguraev et al, 1998; Jones et al, 2002),
words (Buyukkokten et al, 1999), n-grams (Leuski
et al, 2003; Lin and Hovy, 1997) or proper
nouns, multi-word terms and abbreviations (Neff
and Cooper, 1999).
Here we will focus, however, in finding appro-
priate weighting schemes on the set of candidate
terms. The most common approach in interactive
single-document summarization is using tf.idf mea-
sures (Jones et al, 2002; Buyukkokten et al, 1999;
Neff and Cooper, 1999), which favour terms which
are frequent in a document and infrequent across
the collection. In the iNeast system (Leuski et al,
2003), the identification of relevant terms is ori-
ented towards multi-document summarization, and
they use a likelihood ratio (Dunning, 1993) which
favours terms which are representative of the set of
documents as opposed to the full collection.
Other sources of information that have been used
as complementary measures consider, for instance,
the number of references of a concept (Boguraev
et al, 1998), its localization (Jones et al, 2002)
or the distribution of the term along the document
(Buyukkokten et al, 1999; Boguraev et al, 1998).
5.1 Experimental setup
A technical difficulty is that the key concepts in-
troduced by the users are intellectual elaborations,
which result in complex expressions which might
even not be present (literally) in the documents.
Hence, we will concentrate on extracting lists of
terms, checking whether these terms are part of
some key concept. We will assume that, once key
terms are found, it is possible to generate full nomi-
nal expressions using, for instance, phrase browsing
strategies (Pen?as et al, 2002).
We will then compare different weighting criteria
to select key terms, using two evaluation measures:
a recall measure saying how well manually selected
key concepts are covered by the automatically gen-
erated term list; and a noise measure counting the
number of terms which do not belong to any key
concept. An optimal list will reach maximum recall
with a minimum of noise. Formally:
R =
|Cl|
|C|
Noise = |Ln|
where C is the set of key concepts manually se-
lected by users; L is a (ranked) list of terms gen-
erated by some weighting schema; Ln is the subset
of terms in L which do not belong to any key con-
cept; and Cl is the subset of key concepts which are
represented by at least one term in the ranked list L.
Here is a (fictitious) example of how R and
Noise are computed:
C = {Haiti, reinstatement of democracy, UN and USA troops}
L = {Haiti, soldiers, UN, USA, attempt}
?
Cl = {Haiti, UN and USA troops} R = 2/3
Ln = {soldiers,attempt} Noise = 2
We will compare the following weighting strate-
gies:
TF The frequency of a word in the set of documents
is taken as a baseline measure.
Likelihood ratio This is taken from (Leuski et al,
2003) and used as a reference measure. We
have implemented the procedure described in
(Rayson and Garside, 2000) using unigrams
only.
OKAPImod We have also considered a measure
derived from Okapi and used in (Robertson et
al., 1992). We have adapted the measure to
consider the set of 100 documents as one single
document.
TFSYNTAX Using our first experimental result,
TFSYNTAX computes the weight of a term
as the number of times it appears preceding a
verb.
Figure 4: Comparison of weighting schemes to ex-
tract relevant terms
5.2 Results
Figure 4 draws Recall/Noise curves for all weight-
ing criteria. They all give similar results except our
TFSYNTAX measure, which performs better than
the others for TT topics. Note that the TFSYN-
TAX measure only considers 10% of the vocabu-
lary, which are the words immediately preceding
verbs in the texts.
In order to check whether this result is consistent
across topics (and not only the effect on an average)
we have compared recall for term lists of size 50 for
individual topics. We have selected 50 as a number
which is large enough to reach a good coverage and
permit additional filtering in an interactive summa-
rization process, such as the iNeast terminological
clustering described in (Leuski et al, 2003).
Figure 5 shows these results by topic. TFSYN-
TAX performs consistently better for all topics ex-
cept one of the IE topics, where the maximum like-
lihood measure is slightly better.
Apart from the fact that TFSYNTAX performs
better than all other methods, it is worth noticing
that sophisticated weighting mechanisms, such as
Okapi and the likelihood ratio, do not behave bet-
ter than a simple frequency count (TF).
6 Conclusions
The automatic extraction of relevant concepts for
a set of related documents is a part of many mod-
els of automatic or interactive summarization. In
this paper, we have analyzed the distribution of rel-
evant concepts across different syntactic functions,
and we have measured the usefulness of detecting
key terms to extract relevant concepts.
Our results suggest that the distribution of key
concepts in sentences is not uniform, having a max-
imum in positions immediately preceding the sen-
tence main verb, in noun phrases acting as subjects
and, more specifically, in the complements (rather
than the head) of noun phrases acting as subjects.
This evidence has been collected using a Spanish
news collection, and should be corroborated outside
the news domain and also adapted to be used for non
SVO languages.
We have also obtained empirical evidence that
statistical weights to select key terms can be im-
proved if we restrict candidate words to those which
precede the verb in some sentence. The combi-
nation of statistical measures and syntactic criteria
overcomes pure statistical weights, at least for TT
topics, where there is certain consistency in the key
concepts across documents.
Acknowledgments
This research has been partially supported by a re-
search grant of the Spanish Government (project
Hermes) and a research grant from UNED. We are
indebted to J. Cigarra?n who calculated the Okapi
weights used in this work.
References
E. Amigo, J. Gonzalo, V. Peinado, A. Pen?as, and
F. Verdejo. 2004. Information synthesis: an em-
pirical study. In Proceedings of the 42th Annual
Meeting of the ACL, Barcelona, July.
B. Boguraev, C. Kennedy, R. Bellamy, S. Brawer,
Y. Wong, and J. Swartz. 1998. Dynamic Presen-
tation of Document Content for Rapid On-line
Skimming. In Proceedings of the AAAI Spring
Figure 5: Comparison of weighting schemes by topic
1998 Symposium on Intelligent Text Summariza-
tion, Stanford, CA.
O. Buyukkokten, H. Garc??a-Molina, and
A. Paepcke. 1999. Seeing the Whole in
Parts: Text Summarization for Web Browsing
on Handheld Devices. In Proceedings of 10th
International WWW Conference.
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1):61?74.
H. P. Edmundson. 1969. New Methods in Auto-
matic Extracting. Journal of the Association for
Computing Machinery, 16(2):264?285.
S. Jones, S. Lundy, and G. W. Paynter. 2002. In-
teractive Document Summarization Using Auto-
matically Extracted Keyphrases. In Proceedings
of the 35th Hawaii International Conference on
System Sciences, Big Island, Hawaii.
W. Kraaij, M. Spitters, and A. Hulth. 2002.
Headline Extraction based on a Combination of
Uni- and Multi-Document Summarization Tech-
niques. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalua-
tion, Philadelphia, PA, July.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of SI-
GIR?95.
A. Leuski, C. Y. Lin, and S. Stubblebine. 2003.
iNEATS: Interactive Multidocument Summariza-
tion. In Proceedings of the 4lst Annual Meeting
of the ACL (ACL 2003), Sapporo, Japan.
C.-Y. Lin and E.H. Hovy. 1997. Identifying Top-
ics by Position. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing
(ANLP), Washington, DC.
C. Lin and E. Hovy. 2002. NeATS in DUC
2002. In Proceedings of the DUC 2002 Work-
shop on Multi-Document Summarization Evalu-
ation, Philadelphia, PA, July.
M. S. Neff and J. W. Cooper. 1999. ASHRAM: Ac-
tive Summarization and Markup. In Proceedings
of HICSS-32: Understanding Digital Documents.
A. Pen?as, F. Verdejo, and J. Gonzalo. 2002. Ter-
minology Retrieval: Towards a Synergy be-
tween Thesaurus and Free Text Searching. In IB-
ERAMIA 2002, pages 684?693, Sevilla, Spain.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
S. Preston, K.and Williams. 1994. Managing the
Information Overload. Physics in Business, June.
P. Rayson and R. Garside. 2000. Comparing Cor-
pora Using Frequency Profiling. In Proceedings
of the workshop on Comparing Corpora, pages
1?6, Honk Kong.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conference, pages 21?30.
J. D. Schlesinger, M. E. Okurowski, J. M. Conroy,
D. P. O?Leary, A. Taylor, J. Hobbs, and H. Wil-
son. 2002. Understanding Machine Performance
in the Context of Human Performance for Multi-
Document Summarization. In Proceedings of the
DUC 2002 Workshop on Multi-Document Sum-
marization Evaluation, Philadelphia, PA, July.
An Empirical Study of Information Synthesis Tasks
Enrique Amigo? Julio Gonzalo V??ctor Peinado Anselmo Pen?as Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,victor,anselmo,felisa}@lsi.uned.es
Abstract
This paper describes an empirical study of the ?In-
formation Synthesis? task, defined as the process of
(given a complex information need) extracting, or-
ganizing and inter-relating the pieces of information
contained in a set of relevant documents, in order to
obtain a comprehensive, non redundant report that
satisfies the information need.
Two main results are presented: a) the creation
of an Information Synthesis testbed with 72 reports
manually generated by nine subjects for eight com-
plex topics with 100 relevant documents each; and
b) an empirical comparison of similarity metrics be-
tween reports, under the hypothesis that the best
metric is the one that best distinguishes between
manual and automatically generated reports. A met-
ric based on key concepts overlap gives better re-
sults than metrics based on n-gram overlap (such as
ROUGE) or sentence overlap.
1 Introduction
A classical Information Retrieval (IR) system helps
the user finding relevant documents in a given text
collection. In most occasions, however, this is only
the first step towards fulfilling an information need.
The next steps consist of extracting, organizing and
relating the relevant pieces of information, in or-
der to obtain a comprehensive, non redundant report
that satisfies the information need.
In this paper, we will refer to this process as In-
formation Synthesis. It is normally understood as
an (intellectually challenging) human task, and per-
haps the Google Answer Service1 is the best gen-
eral purpose illustration of how it works. In this ser-
vice, users send complex queries which cannot be
answered simply by inspecting the first two or three
documents returned by a search engine. These are a
couple of real, representative examples:
a) I?m looking for information concerning the history of text
compression both before and with computers.
1http://answers.google.com
b) Provide an analysis on the future of web browsers, if
any.
Answers to such complex information needs are
provided by experts which, commonly, search the
Internet, select the best sources, and assemble the
most relevant pieces of information into a report,
organizing the most important facts and providing
additional web hyperlinks for further reading. This
Information Synthesis task is understood, in Google
Answers, as a human task for which a search engine
only provides the initial starting point. Our mid-
term goal is to develop computer assistants that help
users to accomplish Information Synthesis tasks.
From a Computational Linguistics point of view,
Information Synthesis can be seen as a kind of
topic-oriented, informative multi-document sum-
marization, where the goal is to produce a single
text as a compressed version of a set of documents
with a minimum loss of relevant information. Un-
like indicative summaries (which help to determine
whether a document is relevant to a particular topic),
informative summaries must be helpful to answer,
for instance, factual questions about the topic. In
the remainder of the paper, we will use the term
?reports? to refer to the summaries produced in an
Information Synthesis task, in order to distinguish
them from other kinds of summaries.
Topic-oriented multi-document summarization
has already been studied in other evaluation ini-
tiatives which provide testbeds to compare alterna-
tive approaches (Over, 2003; Goldstein et al, 2000;
Radev et al, 2000). Unfortunately, those stud-
ies have been restricted to very small summaries
(around 100 words) and small document sets (10-
20 documents). These are relevant summarization
tasks, but hardly representative of the Information
Synthesis problem we are focusing on.
The first goal of our work has been, therefore,
to create a suitable testbed that permits qualitative
and quantitative studies on the information synthe-
sis task. Section 2 describes the creation of such a
testbed, which includes the manual generation of 72
reports by nine different subjects across 8 complex
topics with 100 relevant documents per topic.
Using this testbed, our second goal has been to
compare alternative similarity metrics for the Infor-
mation Synthesis task. A good similarity metric
provides a way of evaluating Information Synthe-
sis systems (comparing their output with manually
generated reports), and should also shed some light
on the common properties of manually generated re-
ports. Our working hypothesis is that the best metric
will best distinguish between manual and automati-
cally generated reports.
We have compared several similarity metrics, in-
cluding a few baseline measures (based on docu-
ment, sentence and vocabulary overlap) and a state-
of-the-art measure to evaluate summarization sys-
tems, ROUGE (Lin and Hovy, 2003). We also intro-
duce another proximity measure based on key con-
cept overlap, which turns out to be substantially bet-
ter than ROUGE for a relevant class of topics.
Section 3 describes these metrics and the experi-
mental design to compare them; in Section 4, we an-
alyze the outcome of the experiment, and Section 5
discusses related work. Finally, Section 6 draws the
main conclusions of this work.
2 Creation of an Information Synthesis
testbed
We refer to Information Synthesis as the process
of generating a topic-oriented report from a non-
trivial amount of relevant, possibly interrelated doc-
uments. The first goal of our work is the generation
of a testbed (ISCORPUS) with manually produced
reports that serve as a starting point for further em-
pirical studies and evaluation of information synthe-
sis systems. This section describes how this testbed
has been built.
2.1 Document collection and topic set
The testbed must have a certain number of features
which, altogether, differentiate the task from current
multi-document summarization evaluations:
Complex information needs. Being Informa-
tion Synthesis a step which immediately follows a
document retrieval process, it seems natural to start
with standard IR topics as used in evaluation con-
ferences such as TREC2, CLEF3 or NTCIR4. The
title/description/narrative topics commonly used in
such evaluation exercises are specially well suited
for an Information Synthesis task: they are complex
2http://trec.nist.gov
3http://www.clef-campaign.org
4http://research.nii.ac.jp/ntcir/
and well defined, unlike, for instance, typical web
queries.
We have selected the Spanish CLEF 2001-2003
news collection testbed (Peters et al, 2002), be-
cause Spanish is the native language of the subjects
recruited for the manual generation of reports. Out
of the CLEF topic set, we have chosen the eight
topics with the largest number of documents man-
ually judged as relevant from the assessment pools.
We have slightly reworded the topics to change the
document retrieval focus (?Find documents that...?)
into an information synthesis wording (?Generate a
report about...?). Table 1 shows the eight selected
topics.
C042: Generate a report about the invasion of Haiti by UN/US
soldiers.
C045: Generate a report about the main negotiators of the
Middle East peace treaty between Israel and Jordan, giving
detailed information on the treaty.
C047: What are the reasons for the military intervention of
Russia in Chechnya?
C048: Reasons for the withdrawal of United Nations (UN)
peace- keeping forces from Bosnia.
C050: Generate a report about the uprising of Indians in
Chiapas (Mexico).
C085: Generate a report about the operation ?Turquoise?, the
French humanitarian program in Rwanda.
C056: Generate a report about campaigns against racism in
Europe.
C080: Generate a report about hunger strikes attempted in
order to attract attention to a cause.
Table 1: Topic set
This set of eight CLEF topics has two differenti-
ated subsets: in a majority of cases (first six topics),
it is necessary to study how a situation evolves in
time; the importance of every event related to the
topic can only be established in relation with the
others. The invasion of Haiti by UN and USA troops
(C042) is an example of such a topic. We will refer
to them as ?Topic Tracking? (TT) reports, because
they resemble the kind of topics used in such task.
The last two questions (56 and 80), however, re-
semble Information Extraction tasks: essentially,
the user has to detect and describe instances of
a generic event (cases of hunger strikes and cam-
paigns against racism in Europe); hence we will re-
fer to them as ?IE? reports.
Topic tracking reports need a more elaborated
treatment of the information in the documents, and
therefore are more interesting from the point of view
of Information Synthesis. We have, however, de-
cided to keep the two IE topics; first, because they
also reflect a realistic synthesis task; and second, be-
cause they can provide contrastive information as
compared to TT reports.
Large document sets. All the selected CLEF
topics have more than one hundred documents
judged as relevant by the CLEF assessors. For ho-
mogeneity, we have restricted the task to the first
100 documents for each topic (using a chronologi-
cal order).
Complex reports. The elaboration of a com-
prehensive report requires more space than is al-
lowed in current multi-document summarization ex-
periences. We have established a maximum of fifty
sentences per summary, i.e., half a sentence per doc-
ument. This limit satisfies three conditions: a) it
is large enough to contain the essential information
about the topic, b) it requires a substantial compres-
sion effort from the user, and c) it avoids defaulting
to a ?first sentence? strategy by lazy (or tired) users,
because this strategy would double the maximum
size allowed.
We decided that the report generation would be
an extractive task, which consists of selecting sen-
tences from the documents. Obviously, a realistic
information synthesis process also involves rewrit-
ing and elaboration of the texts contained in the doc-
uments. Keeping the task extractive has, however,
two major advantages: first, it permits a direct com-
parison to automatic systems, which will typically
be extractive; and second, it is a simpler task which
produces less fatigue.
2.2 Generation of manual reports
Nine subjects between 25 and 35 years-old were re-
cruited for the manual generation of reports. All
of them self-reported university degrees and a large
experience using search engines and performing in-
formation searches.
All subjects were given an in-place detailed de-
scription of the task in order to minimize divergent
interpretations. They were told that, in a first step,
they had to generate reports with a maximum of in-
formation about every topic within the fifty sentence
space limit. In a second step, which would take
place six months afterwards, they would be exam-
ined from each of the eight topics. The only docu-
mentation allowed during the exam would be the re-
ports generated in the first phase of the experiment.
Subjects scoring best would be rewarded.
These instructions had two practical effects: first,
the competitive setup was an extra motivation for
achieving better results. And second, users tried to
take advantage of all available space, and thus most
reports were close to the fifty sentences limit. The
time limit per topic was set to 30 minutes, which is
tight for the information synthesis task, but prevents
the effects of fatigue.
We implemented an interface to facilitate the gen-
eration of extractive reports. The system displays a
list with the titles of relevant documents in chrono-
logical order. Clicking on a title displays the full
document, where the user can select any sentence(s)
and add them to the final report. A different frame
displays the selected sentences (also in chronolog-
ical order), together with one bar indicating the re-
maining time and another bar indicating the remain-
ing space. The 50 sentence limit can be temporarily
exceeded and, when the 30 minute limit has been
reached, the user can still remove sentences from
the report until the sentence limit is reached back.
2.3 Questionnaires
After summarizing every topic, the following ques-
tionnaire was filled in by every user:
? Who are the main people involved in the topic?
? What are the main organizations participating in the
topic?
? What are the key factors in the topic?
Users provided free-text answers to these ques-
tions, with their freshly generated summary at hand.
We did not provide any suggestions or constraints
at this point, except that a maximum of eight slots
were available per question (i.e. a maximum of
8X3 = 24 key concepts per topic, per user).
This is, for instance, the answer of one user for
the topic 42 about the invasion of Haiti by UN and
USA troops in 1994:
People Organizations
Jean Bertrand Aristide ONU (UN)
Clinton EEUU (USA)
Raoul Cedras OEA (OAS)
Philippe Biambi
Michel Josep Francois
Factors
militares golpistas (coup attempting soldiers)
golpe militar (coup attempt)
restaurar la democracia (reinstatement of democracy)
Finally, a single list of key concepts is gener-
ated for each topic, joining all the different answers.
Redundant concepts (e.g. ?war? and ?conflict?)
were inspected and collapsed by hand. These lists
of key concepts constitute the gold standard for the
similarity metric described in Section 3.2.5.
Besides identifying key concepts, users also filled
in the following questionnaire:
? Were you familiarized with the topic?
? Was it hard for you to elaborate the report?
? Did you miss the possibility of introducing annotations
or rewriting parts of the report by hand?
? Do you consider that you generated a good report?
? Are you tired?
Out of the answers provided by users, the most
remarkable facts are that:
? only in 6% of the cases the user missed ?a lot?
the possibility of rewriting/adding comments
to the topic. The fact that reports are made ex-
tractively did not seem to be a significant prob-
lem for our users.
? in 73% of the cases, the user was quite or very
satisfied about his summary.
These are indications that the practical con-
straints imposed on the task (time limit and extrac-
tive nature of the summaries) do not necessarily
compromise the representativeness of the testbed.
The time limit is very tight, but the temporal ar-
rangement of documents and their highly redundant
nature facilitates skipping repetitive material (some
pieces of news are discarded just by looking at the
title, without examining the content).
2.4 Generation of baseline reports
We have automatically generated baseline reports in
two steps:
? For every topic, we have produced 30 tentative
baseline reports using DUC style criteria:
? 18 summaries consist only of picking the
first sentence out of each document in 18
different document subsets. The subsets
are formed using different strategies, e.g.
the most relevant documents for the query
(according to the Inquery search engine),
one document per day, the first or last 50
documents in chronological order, etc.
? The other 12 summaries consist of a)
picking the first n sentences out of a set
of selected documents (with different val-
ues for n and different sets of documents)
and b) taking the full content of a few doc-
uments. In both cases, document sets are
formed with similar criteria as above.
? Out of these 30 baseline reports, we have se-
lected the 10 reports which have the highest
sentence overlap with the manual summaries.
The second step increases the quality of the base-
lines, making the task of differentiating manual and
baseline reports more challenging.
3 Comparison of similarity metrics
Formal aspects of a summary (or report), such
as legibility, grammatical correctness, informative-
ness, etc., can only be evaluated manually. How-
ever, automatic evaluation metrics can play a useful
role in the evaluation of how well the information
from the original sources is preserved (Mani, 2001).
Previous studies have shown that it is feasible to
evaluate the output of summarization systems au-
tomatically (Lin and Hovy, 2003). The process is
based in similarity metrics between texts. The first
step is to establish a (manual) reference summary,
and then the automatically generated summaries are
ranked according to their similarity to the reference
summary.
The challenge is, then, to define an appropriate
proximity metric for reports generated in the infor-
mation synthesis task.
3.1 How to compare similarity metrics without
human judgments? The QARLA
estimation
In tasks such as Machine Translation and Summa-
rization, the quality of a proximity metric is mea-
sured in terms of the correlation between the rank-
ing produced by the metric, and a reference ranking
produced by human judges. An optimal similarity
metric should produce the same ranking as human
judges.
In our case, acquiring human judgments about
the quality of the baseline reports is too costly, and
probably cannot be done reliably: a fine-grained
evaluation of 50-sentence reports summarizing sets
of 100 documents is a very complex task, which
would probably produce different rankings from
different judges.
We believe there is a cheaper and more robust
way of comparing similarity metrics without using
human assessments. We assume a simple hypothe-
sis: the best metric should be the one that best dis-
criminates between manual and automatically gen-
erated reports. In other words, a similarity metric
that cannot distinguish manual and automatic re-
ports cannot be a good metric. Then, all we need
is an estimation of how well a similarity metric sep-
arates manual and automatic reports. We propose
to use the probability that, given any manual report
Mref , any other manual report M is closer to Mref
than any other automatic report A:
QARLA(sim) = P (sim(M,Mref ) > sim(A,Mref ))
where M,Mref ?M, A ? A
where M is the set of manually generated re-
ports, A is the set of automatically generated re-
ports, and ?sim? is the similarity metric being eval-
uated.
We refer to this value as the QARLA5 estimation.
QARLA has two interesting features:
? No human assessments are needed to compute
QARLA. Only a set of manually produced
summaries and a set of automatic summaries,
for each topic considered. This reduces the
cost of creating the testbed and, in addition,
eliminates the possible bias introduced by hu-
man judges.
? It is easy to collect enough data to achieve sta-
tistically significant results. For instance, our
testbed provides 720 combinations per topic
to estimate QARLA probability (we have
nine manual plus ten automatic summaries per
topic).
A good QARLA value does not guarantee that
a similarity metric will produce the same rankings
as human judges, but a good similarity metric must
have a good QARLA value: it is unlikely that
a measure that cannot distinguish between manual
and automatic summaries can still produce high-
quality rankings of automatic summaries by com-
parison to manual reference summaries.
3.2 Similarity metrics
We have compared five different metrics using the
QARLA estimation. The first three are meant as
baselines; the fourth is the standard similarity met-
ric used to evaluate summaries (ROUGE); and the
last one, introduced in this paper, is based on the
overlapping of key concepts.
3.2.1 Baseline 1: Document co-selection metric
The following metric estimates the similarity of two
reports from the set of documents which are repre-
sented in both reports (i.e. at least one sentence in
each report belongs to the document).
DocSim(Mr,M) =
|Doc(Mr) ?Doc(M)|
|Doc(Mr)|
where Mr is the reference report, M a second re-
port and Doc(Mr), Doc(M) are the documents to
which the sentences in Mr,M belong to.
5Quality criterion for reports evaluation metrics
3.2.2 Baselines 2 and 3: Sentence co-selection
The more sentences in common between two re-
ports, the more similar their content will be. We can
measure Recall (how many sentences from the ref-
erence report are also in the contrastive report) and
Precision (how many sentences from the contrastive
report are also in the reference report):
SentenceSimR(Mr,M) =
|S(Mr) ? S(M)|
|S(Mr)|
SentenceSimP (Mr,M) =
|S(Mr) ? S(M)|
|S(M)|
where S(Mr), S(M) are the sets of sentences in
the reports Mr (reference) and M (contrastive).
3.2.3 Baseline 4: Perplexity
A language model is a probability distribution over
word sequences obtained from some training cor-
pora (see e.g. (Manning and Schutze, 1999)). Per-
plexity is a measure of the degree of surprise of a
text or corpus given a language model. In our case,
we build a language model LM(Mr) for the refer-
ence report Mr, and measure the perplexity of the
contrastive report M as compared to that language
model:
PerplexitySim(Mr,M) =
1
Perp(LM(Mr),M)
We have used the Good-Turing discount algo-
rithm to compute the language models (Clarkson
and Rosenfeld, 1997). Note that this is also a base-
line metric, because it only measures whether the
content of the contrastive report is compatible with
the reference report, but it does not consider the cov-
erage: a single sentence from the reference report
will have a low perplexity, even if it covers only a
small fraction of the whole report. This problem
is mitigated by the fact that we are comparing re-
ports of approximately the same size and without
repeated sentences.
3.2.4 ROUGE metric
The distance between two summaries can be estab-
lished as a function of their vocabulary (unigrams)
and how this vocabulary is used (n-grams). From
this point of view, some of the measures used in the
evaluation of Machine Translation systems, such as
BLEU (Papineni et al, 2002), have been imported
into the summarization task. BLEU is based in the
precision and n-gram co-ocurrence between an au-
tomatic translation and a reference manual transla-
tion.
(Lin and Hovy, 2003) tried to apply BLEU as
a measure to evaluate summaries, but the results
were not as good as in Machine Translation. In-
deed, some of the characteristics that define a good
translation are not related with the features of a good
summary; then Lin and Hovy proposed a recall-
based variation of BLEU, known as ROUGE. The
idea is the same: the quality of a proposed sum-
mary can be calculated as a function of the n-grams
in common between the units of a model summary.
The units can be sentences or discourse units:
ROUGEn =
?
C?{MU}
?
n-gram?C Countm
?
C?{MU}
?
n-gram?C Count
where MU is the set of model units, Countm is
the maximum number of n-grams co-ocurring in a
peer summary and a model unit, and Count is the
number of n-grams in the model unit. It has been
established that unigram and bigram based metrics
permit to create a ranking of automatic summaries
better (more similar to a human-produced ranking)
than n-grams with n > 2.
For our experiment, we have only considered un-
igrams (lemmatized words, excluding stop words),
which gives good results with standard summaries
(Lin and Hovy, 2003).
3.2.5 Key concepts metric
Two summaries generated by different subjects may
differ in the documents that contribute to the sum-
mary, in the sentences that are chosen, and even in
the information that they provide. In our Informa-
tion Synthesis settings, where topics are complex
and the number of documents to summarize is large,
it is likely to expect that similarity measures based
on document, sentence or n-gram overlap do not
give large similarity values between pairs of man-
ually generated summaries.
Our hypothesis is that two manual reports, even if
they differ in their information content, will have the
same (or very similar) key concepts; if this is true,
comparing the key concepts of two reports can be a
better similarity measure than the previous ones.
In order to measure the overlap of key concepts
between two reports, we create a vector ~kc for every
report, such that every element in the vector repre-
sents the frequency of a key concept in the report in
relation to the size of the report:
kc(M)i =
freq(Ci,M)
|words(M)|
being freq(Ci,M) the number of times the
key concept Ci appears in the report M , and
|words(M)| the number of words in the report.
The key concept similarity NICOS (Nuclear In-
formative Concept Similarity) between two reports
M and Mr can then be defined as the inverse of the
Euclidean distance between their associated concept
vectors:
NICOS(M,Mr) =
1
| ~kc(Mr)? ~kc(M)|
In our experiment, the dimensions of kc vectors
correspond to the list of key concepts provided by
our test subjects (see Section 2.3). This list is our
gold standard for every topic.
4 Experimental results
Figure 1 shows, for every topic (horizontal axis),
the QARLA estimation obtained for each similarity
metric, i.e., the probability of a manual report being
closer to other manual report than to an automatic
report. Table 2 shows the average QARLA measure
across all topics.
Metric TT topics IE topics
Perplexity 0.19 0.60
DocSim 0.20 0.34
SentenceSimR 0.29 0.52
SentenceSimP 0.38 0.57
ROUGE 0.54 0.53
NICOS 0.77 0.52
Table 2: Average QARLA
For the six TT topics, the key concept similarity
NICOS performs 43% better than ROUGE, and all
baselines give poor results (all their QARLA proba-
bilities are below chance, QARLA < 0.5). A non-
parametric Wilcoxon sign test confirms that the dif-
ference between NICOS and ROUGE is highly sig-
nificant (p < 0.005). This is an indication that the
Information Synthesis task, as we have defined it,
should not be studied as a standard summarization
problem. It also confirms our hypothesis that key
concepts tend to be stable across different users, and
may help to generate the reports.
The behavior of the two Information Extraction
(IE) topics is substantially different from TT topics.
While the ROUGE measure remains stable (0.53
versus 0.54), the key concept similarity is much
worse with IE topics (0.52 versus 0.77). On the
other hand, all baselines improve, and some of them
(SentenceSim precision and perplexity) give better
results than both ROUGE and NICOS.
Of course, no reliable conclusion can be obtained
from only two IE topics. But the observed differ-
ences suggest that TT and IE may need different
approaches, both to the automatic generation of re-
ports and to their evaluation.
Figure 1: Comparison of similarity metrics by topic
One possible reason for this different behavior is
that IE topics do not have a set of consistent key
concepts; every case of a hunger strike, for instance,
involves different people, organizations and places.
The average number of different key concepts is
18.7 for TT topics and 28.5 for IE topics, a differ-
ence that reveals less agreement between subjects,
supporting this argument.
5 Related work
Besides the measures included in our experiment,
there are other criteria to compare summaries which
could as well be tested for Information Synthesis:
Annotation of relevant sentences in a corpus.
(Khandelwal et al, 2001) propose a task, called
?Temporal Summarization?, that combines summa-
rization and topic tracking. The paper describes the
creation of an evaluation corpus in which the most
relevant sentences in a set of related news were an-
notated. Summaries are evaluated with a measure
called ?novel recall?, based in sentences selected by
a summarization system and sentences manually as-
sociated to events in the corpus. The agreement rate
between subjects in the identification of key events
and the sentence annotation does not correspond
with the agreement between reports that we have
obtained in our experiments. There are, at least, two
reasons to explain this:
? (Khandelwal et al, 2001) work on an average
of 43 documents, half the size of the topics in
our corpus.
? Although there are topics in both experiments,
the information needs in our testbed are more
complex (e.g. motivations for the invasion of
Chechnya)
Factoids. One of the problems in the evalua-
tion of summaries is the versatility of human lan-
guage. Two different summaries may contain the
same information. In (Halteren and Teufel, 2003),
the content of summaries is manually represented,
decomposing sentences in factoids or simple facts.
They also annotate the composition, generalization
and implication relations between extracted fac-
toids. The resulting measure is different from un-
igram based similarity. The main problem of fac-
toids, as compared to other metrics, is that they re-
quire a costly manual processing of the summaries
to be evaluated.
6 Conclusions
In this paper, we have reported an empirical study
of the ?Information Synthesis? task, defined as the
process of (given a complex information need) ex-
tracting, organizing and relating the pieces of infor-
mation contained in a set of relevant documents, in
order to obtain a comprehensive, non redundant re-
port that satisfies the information need.
We have obtained two main results:
? The creation of an Information Synthesis
testbed (ISCORPUS) with 72 reports manually
generated by 9 subjects for 8 complex topics
with 100 relevant documents each.
? The empirical comparison of candidate metrics
to estimate the similarity between reports.
Our empirical comparison uses a quantitative cri-
terion (the QARLA estimation) based on the hy-
pothesis that a good similarity metric will be able to
distinguish between manual and automatic reports.
According to this measure, we have found evidence
that the Information Synthesis task is not a standard
multi-document summarization problem: state-of-
the-art similarity metrics for summaries do not per-
form equally well with the reports in our testbed.
Our most interesting finding is that manually
generated reports tend to have the same key con-
cepts: a similarity metric based on overlapping key
concepts (NICOS) gives significantly better results
than metrics based on language models, n-gram co-
ocurrence and sentence overlapping. This is an in-
dication that detecting relevant key concepts is a
promising strategy in the process of generating re-
ports.
Our results, however, has also some intrinsic lim-
itations. Firstly, manually generated summaries are
extractive, which is good for comparison purposes,
but does not faithfully reflect a natural process of
human information synthesis. Another weakness is
the maximum time allowed per report: 30 minutes
seems too little to examine 100 documents and ex-
tract a decent report, but allowing more time would
have caused an excessive fatigue to users. Our vol-
unteers, however, reported a medium to high satis-
faction with the results of their work, and in some
occasions finished their task without reaching the
time limit.
ISCORPUS is available at:
http://nlp.uned.es/ISCORPUS
Acknowledgments
This research has been partially supported by a
grant of the Spanish Government, project HERMES
(TIC-2000-0335-C03-01). We are indebted to E.
Hovy for his comments on an earlier version of
this paper, and C. Y. Lin for his assistance with the
ROUGE measure. Thanks also to our volunteers for
their valuable cooperation.
References
P. Clarkson and R. Rosenfeld. 1997. Statistical
language modeling using the CMU-Cambridge
toolkit. In Proceeding of Eurospeech ?97,
Rhodes, Greece.
J. Goldstein, V. O. Mittal, J. G. Carbonell, and
J. P. Callan. 2000. Creating and Evaluating
Multi-Document Sentence Extract Summaries.
In Proceedings of Ninth International Confer-
ences on Information Knowledge Management
(CIKM?00), pages 165?172, McLean, VA.
H. V. Halteren and S. Teufel. 2003. Examin-
ing the Consensus between Human Summaries:
Initial Experiments with Factoids Analysis. In
HLT/NAACL-2003 Workshop on Automatic Sum-
marization, Edmonton, Canada.
V. Khandelwal, R. Gupta, and J. Allan. 2001. An
Evaluation Corpus for Temporal Summarization.
In Proceedings of the First International Confer-
ence on Human Language Technology Research
(HLT 2001), Tolouse, France.
C. Lin and E. H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-ocurrence
Statistics. In Proceeding of the 2003 Language
Technology Conference (HLT-NAACL 2003), Ed-
monton, Canada.
I. Mani. 2001. Automatic Summarization, vol-
ume 3 of Natural Language Processing. John
Benjamins Publishing Company, Amster-
dam/Philadelphia.
C. D. Manning and H. Schutze. 1999. Foundations
of statistical natural language processing. MIT
Press, Cambridge Mass.
P. Over. 2003. Introduction to DUC-2003: An In-
trinsic Evaluation of Generic News Text Summa-
rization Systems. In Proceedings of Workshop on
Automatic Summarization (DUC 2003).
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?
318, Philadelphia.
C. Peters, M. Braschler, J. Gonzalo, and M. Kluck,
editors. 2002. Evaluation of Cross-Language
Information Retrieval Systems, volume 2406 of
Lecture Notes in Computer Science. Springer-
Verlag, Berlin-Heidelberg-New York.
D. R. Radev, J. Hongyan, and M. Budzikowska.
2000. Centroid-Based Summarization of Mul-
tiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceed-
ings of the Workshop on Automatic Summariza-
tion at the 6th Applied Natural Language Pro-
cessing Conference and the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics, Seattle, WA, April.
Proceedings of the 43rd Annual Meeting of the ACL, pages 280?289,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
QARLA:A Framework for the Evaluation of Text Summarization Systems
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,anselmo,felisa}@lsi.uned.es
Abstract
This paper presents a probabilistic
framework, QARLA, for the evaluation
of text summarisation systems. The in-
put of the framework is a set of man-
ual (reference) summaries, a set of base-
line (automatic) summaries and a set of
similarity metrics between summaries.
It provides i) a measure to evaluate the
quality of any set of similarity metrics,
ii) a measure to evaluate the quality of
a summary using an optimal set of simi-
larity metrics, and iii) a measure to eval-
uate whether the set of baseline sum-
maries is reliable or may produce biased
results.
Compared to previous approaches, our
framework is able to combine different
metrics and evaluate the quality of a set
of metrics without any a-priori weight-
ing of their relative importance. We pro-
vide quantitative evidence about the ef-
fectiveness of the approach to improve
the automatic evaluation of text sum-
marisation systems by combining sev-
eral similarity metrics.
1 Introduction
The quality of an automatic summary can be es-
tablished mainly with two approaches:
Human assessments: The output of a number of
summarisation systems is compared by hu-
man judges, using some set of evaluation
guidelines.
Proximity to a gold standard: The best auto-
matic summary is the one that is closest to
some reference summary made by humans.
Using human assessments has some clear ad-
vantages: the results of the evaluation are inter-
pretable, and we can trace what a system is do-
ing well, and what is doing poorly. But it also
has a couple of serious drawbacks: i) different hu-
man assessors reach different conclusions, and ii)
the outcome of a comparative evaluation exercise
is not directly reusable for new techniques, i.e., a
summarisation strategy developed after the com-
parative exercise cannot be evaluated without ad-
ditional human assessments made from scratch.
Proximity to a gold standard, on the other hand,
is a criterion that can be automated (see Section 6),
with the advantages of i) being objective, and ii)
once gold standard summaries are built for a com-
parative evaluation of systems, the resulting test-
bed can iteratively be used to refine text summari-
sation techniques and re-evaluate them automati-
cally.
This second approach, however, requires solv-
ing a number of non-trivial issues. For instance,
(i) How can we know whether an evaluation met-
ric is good enough for automatic evaluation?, (ii)
different users produce different summaries, all of
them equally good as gold standards, (iii) if we
have several metrics which test different features
of a summary, how can we combine them into an
optimal test?, (iv) how do we know if our test bed
280
Figure 1: Illustration of some of the restrictions on Q,K
is reliable, or the evaluation outcome may change
by adding, for instance, additional gold standards?
In this paper, we introduce a probabilistic
framework, QARLA, that addresses such issues.
Given a set of manual summaries and another set
of baseline summaries per task, together with a set
of similarity metrics, QARLA provides quantita-
tive measures to (i) select and combine the best
(independent) metrics (KING measure), (ii) apply
the best set of metrics to evaluate automatic sum-
maries (QUEEN measure), and (iii) test whether
evaluating with that test-bed is reliable (JACK
measure).
2 Formal constraints on any evaluation
framework based on similarity metrics
We are looking for a framework to evaluate au-
tomatic summarisation systems objectively using
similarity metrics to compare summaries. The in-
put of the framework is:
? A summarisation task (e.g. topic oriented, in-
formative multi-document summarisation on
a given domain/corpus).
? A set T of test cases (e.g. topic/document set
pairs for the example above)
? A set of summaries M produced by humans
(models), and a set of automatic summaries
A (peers), for every test case.
? A set X of similarity metrics to compare
summaries.
An evaluation framework should include, at
least:
? A measure QM,X(a) ? [0, 1] that estimates
the quality of an automatic summary a, us-
ing the similarity metrics in X to compare
the summary with the models in M . With
Q, we can compare the quality of automatic
summaries.
? A measure KM,A(X) ? [0, 1] that estimates
the suitability of a set of similarity metrics X
for our evaluation purposes. With K, we can
choose the best similarity metrics.
Our main assumption is that all manual sum-
maries are equally optimal and, while they are
likely to be different, the best similarity metric is
the one that identifies and uses the features that are
common to all manual summaries, grouping and
separating them from the automatic summaries.
With these assumption in mind, it is useful to
think of some formal restrictions that any evalua-
tion framework Q,K must hold. We will consider
the following ones (see illustrations in Figure 1):
(1) Given two automatic summaries a, a? and a
similarity measure x, if a is more distant to all
manual summaries than a?, then a cannot be better
281
than a?. Formally: ?m ? M.x(a,m) < x(a?,m) ?
QM,x(a) ? QM,x(a?)
(2) A similarity metric x is better when it is able
to group manual summaries more closely, while
keeping them more distant from automatic sum-
maries: (?m,m? ? M.x(m,m?) > x?(m,m?) ? ?m ?
M,a ? Ax(a,m) < x?(a,m)) ? KM,A(x) > KM,A(x?)
(3) If x is a perfect similarity metric, the quality of
a manual summary cannot be zero: KM,A(x) = 1 ?
?m ?M.QM,x(m) > 0
(4) The quality of a similarity metric or a summary
should not be dependent on scale issues. In gen-
eral, if x? = f(x) with f being a growing mono-
tonic function, then KM,A(x) = KM,A(x?) and
QM,x(a) = QM,x?(a) .
(5) The quality of a similarity metric should
not be sensitive to repeated elements in A, i.e.
KM,A?{a}(x) = KM,A?{a,a}(x).
(6) A random metric x should have KM,A(x) = 0.
(7) A non-informative (constant) metric x should
have KM,A(x) = 0.
3 QARLA evaluation framework
3.1 QUEEN: Estimation of the quality of an
automatic summary
We are now looking for a function QM,x(a) that
estimates the quality of an automatic summary a ?
A, given a set of models M and a similarity metric
x.
An obvious first attempt would be to compute
the average similarity of a to all model summaries
in M in a test sample. But such a measure depends
on scale properties: metrics producing larger sim-
ilarity values will produce larger Q values; and,
depending on the scale properties of x, this cannot
be solved just by scaling the final Q value.
A probabilistic measure that solves this problem
and satisfies all the stated formal constraints is:
QUEENx,M (a) ? P (x(a,m) ? x(m?,m??))
which defines the quality of an automatic sum-
mary a as the probability over triples of manual
summaries m,m?,m?? that a is closer to a model
than the other two models to each other. This mea-
sure draws from the way in which some formal re-
strictions on Q are stated (by comparing similarity
values), and is inspired in the QARLA criterion
introduced in (Amigo et al, 2004).
Figure 2: Summaries quality in a similarity metric
space
Figure 2 illustrates some of the features of the
QUEEN estimation:
? Peers which are very far from the set of
models all receive QUEEN = 0. In other
words, QUEEN does not distinguish between
very poor automatic summarisation strate-
gies. While this feature reduces granularity
of the ranking produced by QUEEN, we find
it desirable, because in such situations, the
values returned by a similarity measure are
probably meaningless.
? The value of QUEEN is maximised for the
peers that ?merge? with the models. For
QUEEN values between 0.5 and 1, peers are
effectively merged with the models.
? An ideal metric (that puts all models to-
gether) would give QUEEN(m) = 1 for all
models, and QUEEN(a) = 0 for all peers
that are not put together with the models.
This is a reasonable boundary condition say-
ing that, if we can distinguish between mod-
els and peers perfectly, then all peers are
poor emulations of human summarising be-
haviour.
3.2 Generalisation of QUEEN to metric sets
It is desirable, however, to have the possibility of
evaluating summaries with respect to several met-
rics together. Let us imagine, for instance, that
the best metric turns out to be a ROUGE (Lin and
Hovy, 2003a) variant that only considers unigrams
to compute similarity. Now consider a summary
282
which has almost the same vocabulary as a hu-
man summary, but with a random scrambling of
the words which makes it unreadable. Even if the
unigram measure is the best hint of similarity to
human performance, in this case it would produce
a high similarity value, while any measure based
on 2-grams, 3-grams or on any simple syntactic
property would detect that the summary is useless.
The issue is, therefore, how to find informative
metrics, and then how to combine them into an op-
timal single quality estimation for automatic sum-
maries. The most immediate way of combining
metrics is via some weighted linear combination.
But our example suggests that this is not the op-
timal way: the unigram measure would take the
higher weight, and therefore it would assign a fair
amount of credit to a summary that can be strongly
rejected with other criteria.
Alternatively, we can assume that a summary is
better if it is closer to the model summaries ac-
cording to all metrics. We can formalise this idea
by introducing a universal quantifier on the vari-
able x in the QUEEN formula. In other words,
QUEENX,M (a) can be defined as the probability,
measured over M ?M ?M , that for every metric
in X the automatic summary a is closer to a model
than two models to each other.
QUEENX,M (a) ? P (?x ? X.x(a,m) ? x(m?,m??))
We can think of the generalised QUEEN mea-
sure as a way of using a set of tests (every simi-
larity metric in X) to falsify the hypothesis that a
given summary a is a model. If, for every compar-
ison of similarities between a,m,m?,m??, there is
at least one test that a does not pass, then a is re-
jected as a model.
This generalised measure is not affected by the
scale properties of every individual metric, i.e. it
does not require metric normalisation and it is not
affected by metric weighting. In addition, it still
satisfies the properties enumerated for its single-
metric counterpart.
Of course, the quality ranking provided by
QUEEN is meaningless if the similarity metric x
does not capture the essential features of the mod-
els. Therefore, we need to estimate the quality of
similarity metrics in order to use QUEEN effec-
tively.
3.3 KING: estimation of the quality of a
similarity metric
Now we need a measure KM,A(x) that estimates
the quality of a similarity metric x to evaluate
automatic summaries (peers) by comparison to
human-produced models.
In order to build a suitable K estimation, we
will again start from the hypothesis that the best
metric is the one that best characterises human
summaries as opposed to automatic summaries.
Such a metric should identify human summaries
as closer to each other, and more distant to peers
(second constraint in Section 2). By analogy with
QUEEN, we can try (for a single metric):
KM,A(x) ? P (x(a,m) < x(m
?,m??)) =
1? (QUEENx,M (a))
which is the probability that two models are
closer to each other than a third model to a peer,
and has smaller values when the average QUEEN
value of peers decreases. The generalisation of K
to metric sets would be simply:
KM,A(X) ? 1? (QUEENX,M (a)))
This measure, however, does not satisfy formal
conditions 3 and 5. Condition 3 is violated be-
cause, given a limited set of models, the K mea-
sure grows with a large number of metrics in X ,
eventually reaching K = 1 (perfect metric set).
But in this situation, QUEEN(m) becomes 0 for
all models, because there will always exist a met-
ric that breaks the universal quantifier condition
over x.
We have to look, then, for an alternative for-
mulation for K. The best K should minimise
QUEEN(a), but having the quality of the models
as a reference. A direct formulation can be:
KM,A(X) = P (QUEEN(m) > QUEEN(a))
According to this formula, the quality of a met-
ric set X is the probability that the quality of a
283
model is higher than the quality of a peer ac-
cording to this metric set. This formula satisfies
all formal conditions except 5 (KM,A?{a}(x) =
KM,A?{a,a}(x)), because it is sensitive to repeated
peers. If we add a large set of identical (or very
similar peers), K will be biased towards this set.
We can define a suitable K that satisfies condi-
tion 5 if we apply a universal quantifier on a. This
is what we call the KING measure:
KINGM,A(X) ?
P (?a ? A.QUEENM,X(m) > QUEENM,X(a))
KING is the probability that a model is better
than any peer in a test sample. In terms of a qual-
ity ranking, it is the probability that a model gets a
better ranking than all peers in a test sample. Note
that KING satisfies all restrictions because it uses
QUEEN as a quality estimation for summaries; if
QUEEN is substituted for a different quality mea-
sure, some of the properties might not hold any
longer.
Figure 3: Metrics quality representation
Figure 3 illustrates the behaviour of the KING
measure in boundary conditions. The left-
most figure represents a similarity metric which
mixes models and peers randomly. Therefore,
P (QUEEN(m) > QUEEN(a)) ? 0.5. As there
are seven automatic summaries, KING = P (?a ?
A,QUEEN(m) > QUEEN(a)) ? 0.57 ? 0
The rightmost figure represents a metric which
is able to group models and separate them from
peers. In this case, QUEEN(a) = 0 for all peers,
and then KING(x) = 1.
3.4 JACK:Reliability of the peers set
Once we detect a difference in quality between
two summarisation systems, the question is now
whether this result is reliable. Would we get the
same results using a different test set (different ex-
amples, different human summarisers (models) or
different baseline systems)?
The first step is obviously to apply statistical
significance tests to the results. But even if they
give a positive result, it might be insufficient. The
problem is that the estimation of the probabilities
in KING,QUEEN assumes that the sample sets
M,A are not biased. If M,A are biased, the re-
sults can be statistically significant and yet un-
reliable. The set of examples and the behaviour
of human summarisers (models) should be some-
how controlled either for homogeneity (if the in-
tended profile of examples and/or users is narrow)
or representativity (if it is wide). But how to know
whether the set of automatic summaries is repre-
sentative and therefore is not penalising certain au-
tomatic summarisation strategies?
Our goal is, therefore, to have some estimation
JACK(X,M,A) of the reliability of the test set to
compute reliable QUEEN,KING measures. We
can think of three reasonable criteria for this es-
timation:
1. All other things being equal, if the elements
of A are more heterogeneous, we are enhanc-
ing the representativeness of A (we have a
more diverse set of (independent) automatic
summarization strategies represented), and
therefore the reliability of the results should
be higher. Reversely, if all automatic sum-
marisers employ similar strategies, we may
end up with a biased set of peers.
2. All other things being equal, if the elements
of A are closer to the model summaries in M ,
the reliability of the results should be higher.
3. Adding items to A should not reduce its reli-
ability.
A possible formulation for JACK which satis-
fies that criteria is:
JACK(X,M,A) ? P (?a, a? ? A.QUEEN(a) >
0 ? QUEEN(a?) > 0 ? ?x ? X.x(a, a?) ? x(a,m))
i.e. the probability over all model summaries m
of finding a couple of automatic summaries a, a?
284
which are closer to each other than to m according
to all metrics.
This measure satisfies all three constraints: it
can be enlarged by increasing the similarity of the
peers to the models (the x(m,a) factor in the in-
equality) or decreasing the similarity between au-
tomatic summaries (the x(a, a?) factor in the in-
equality). Finally, adding elements to A can only
increase the chances of finding a pair of automatic
summaries satisfying the condition in JACK.
Figure 4: JACK values
Figure 4 illustrates how JACK works: in the
leftmost part of the figure, peers are grouped to-
gether and far from the models, giving a low JACK
value. In the rightmost part of the figure, peers are
distributed around the set of models, closely sur-
rounding them, receiving a high JACK value.
4 A Case of Study
In order to test the behaviour of our evaluation
framework, we have applied it to the ISCORPUS
described in (Amigo et al, 2004). The ISCOR-
PUS was built to study an Information Synthesis
task, where a (large) set of relevant documents has
to be studied to give a brief, well-organised answer
to a complex need for information. This corpus
comprises:
? Eight topics extracted from the CLEF Span-
ish Information Retrieval test set, slightly re-
worded to move from a document retrieval
task (find documents about hunger strikes
in...) into an Information Synthesis task
(make a report about major causes of hunger
strikes in...).
? One hundred relevant documents per topic
taken from the CLEF EFE 1994 Spanish
newswire collection.
? M : Manual extractive summaries for every
topic made by 9 different users, with a 50-
sentence upper limit (half the number of rel-
evant documents).
? A: 30 automatic reports for every topic made
with baseline strategies. The 10 reports with
highest sentence overlap with the manual
summaries were selected as a way to increase
the quality of the baseline set.
We have considered the following similarity
metrics:
ROUGESim: ROUGE is a standard measure
to evaluate summarisation systems based on
n-gram recall. We have used ROUGE-1
(only unigrams with lemmatization and stop
word removal), which gives good results with
standard summaries (Lin and Hovy, 2003a).
ROUGE can be turned into a similarity met-
ric ROUGESim simply by considering only
one model when computing its value.
SentencePrecision: Given a reference and a con-
trastive summary, the number of fragments of
the contrastive summary which are also in the
reference summary, in relation to the size of
the reference summary.
SentenceRecall: Given a reference and a con-
trastive summary, the number of fragments of
the reference summary which are also in the
contrastive summary, in relation to the size of
the contrastive summary.
DocSim: The number of documents used to select
fragments in both summaries, in relation to
the size of the contrastive summary.
VectModelSim: Derived from the Euclidean dis-
tance between vectors of relative word fre-
quencies representing both summaries.
NICOS (key concept overlap): Same as Vect-
ModelSim, but using key-concepts (manually
identified by the human summarisers after
producing the summary) instead of all non-
empty words.
285
TruncatedVectModeln: Same as VectModelSim,
but using only the n more frequent terms
in the reference summary. We have used
10 variants of this measure with n =
1, 8, 64, 512.
4.1 Quality of Similarity Metric Sets
Figure 5 shows the quality (KING values averaged
over the eight ISCORPUS topics) of every individ-
ual metric. The rightmost part of the figure also
shows the quality of two metric sets:
? The first one ({ROUGESim, VectModelSim,
TruncVectModel.1}) is the metric set that
maximises KING, using only similarity met-
rics that do not require manual annotation
(i.e. excluding NICOS) or can only be ap-
plied to extractive summaries (i.e. DocSim,
SentenceRecall and SentencePrecision).
? The second one ({ TruncVectModel.1, ROU-
GESim, DocSim, VectModelSim }) is the best
combination considering all metrics.
The best result of individual metrics is obtained
by ROUGESim (0.39). All other individual met-
rics give scores below 0.31. Both metric sets, on
the other, are better than ROUGESim alone, con-
firming that metric combination is feasible to im-
prove system evaluation. The quality of the best
metric set (0.47) is 21% better than ROUGESim.
4.2 Reliability of the test set
The 30 automatic summaries (baselines) per topic
were built with four different classes of strategies:
i) picking up the first sentence from assorted sub-
sets of documents, ii) picking up first and second
sentences from assorted documents, iii) picking
up first, second or third sentences from assorted
documents, and iv) picking up whole documents
with different algorithms to determine which are
the most representative documents.
Figure 6 shows the reliability (JACK) of every
subset, and the reliability of the whole set of au-
tomatic summaries, computed with the best met-
ric set. Note that the individual subsets are all
below 0.2, while the reliability of the full set of
peers goes up to 0.57. That means that the con-
dition in JACK is satisfied for more than half of
the models. This value would probably be higher
if state-of-the-art summarisation techniques were
represented in the set of peers.
5 Testing the predictive power of the
framework
The QARLA probabilistic framework is designed
to evaluate automatic summarisation systems and,
at the same time, similarity metrics conceived as
well to evaluate summarisation systems. There-
fore, testing the validity of the QARLA proposal
implies some kind of meta-meta-evaluation, some-
thing which seems difficult to design or even to
define.
It is relatively simple, however, to perform some
simple cross-checkings on the ISCORPUS data to
verify that the qualitative information described
above is reasonable. This is the test we have im-
plemented:
If we remove a model m from M , and pretend it
is the output of an automatic summariser, we can
evaluate the peers set A and the new peer m using
M ? = M\{m} as the new model set. If the evalu-
ation metric is good, the quality of the new peer m
should be superior to all other peers inA. What we
have to check, then, is whether the average quality
of a human summariser on all test cases (8 topics
in ISCORPUS) is superior to the average quality
of any automatic summariser. We have 9 human
subjects in the ISCORPUS test bed; therefore, we
can repeat this test nine times.
With this criterion, we can compare our quality
measure Q with state-of-the-art evaluation mea-
sures such as ROUGE variants. Table 1 shows
the results of applying this test on ROUGE-
1, ROUGE-2, ROUGE-3, ROUGE-4 (as state-
of-the-art references) and QUEEN(ROUGESim),
QUEEN(Best Metric Combination) as representa-
tives of the QARLA framework. Even if the test is
very limited by the number of topics, it confirms
the potential of the framework, with the highest
KING metric combination doubling the perfor-
mance of the best ROUGE measure (6/9 versus 3/9
correct detections).
286
Figure 5: Quality of similarity metrics
Figure 6: Reliability of ISCORPUS peer sets
Evaluation criterion human summarisers ranked first
ROUGE-1 3/9
ROUGE-2 2/9
ROUGE-3 1/9
ROUGE-4 1/9
QUEEN(ROUGESim) 4/9
QUEEN(Best Metric Combination) 6/9
Table 1: Results of the test of identifying the manual summariser
287
6 Related work and discussion
6.1 Application of similarity metrics to
evaluate summaries
Both in Text Summarisation and Machine Trans-
lation, the automatic evaluation of systems con-
sists of computing some similarity metric between
the system output and a human model summary.
Systems are then ranked in order of decreasing
similarity to the gold standard. When there are
more than one reference items, similarity is calcu-
lated over a pseudo-summary extracted from every
model. BLEU (Papineni et al, 2001) and ROUGE
(Lin and Hovy, 2003a) are the standard similar-
ity metrics used in Machine Translation and Text
Summarisation. Generating a pseudo-summary
from every model, the results of a evaluation met-
ric might depend on the scale properties of the
metric regarding different models; our QUEEN
measure, however, does not depend on scales.
Another problem of the direct application of a
single evaluation metric to rank systems is how to
combine different metrics. The only way to do
this is by designing an algebraic combination of
the individual metrics into a new combined met-
ric, i.e. by deciding the weight of each individual
metric beforehand. In our framework, however, it
is not necessary to prescribe how similarity met-
rics should be combined, not even to know which
ones are individually better indicators.
6.2 Meta-evaluation of similarity metrics
The question of how to know which similar-
ity metric is best to evaluate automatic sum-
maries/translations has been addressed by
? comparing the quality of automatic items
with the quality of manual references (Culy
and Riehemann, 2003; Lin and Hovy,
2003b). If the metric does not identify that
the manual references are better, then it is not
good enough for evaluation purposes.
? measuring the correlation between the values
given by different metrics (Coughlin, 2003).
? measuring the correlation between the rank-
ings generated by each metric and rank-
ings generated by human assessors. (Joseph
P. Turian and Melamed, 2003; Lin and Hovy,
2003a).
The methodology which is closest to our frame-
work is ORANGE (Lin, 2004), which evaluates a
similarity metric using the average ranks obtained
by reference items within a baseline set. As in
our framework, ORANGE performs an automatic
meta-evaluation, there is no need for human as-
sessments, and it does not depend on the scale
properties of the metric being evaluated (because
changes of scale preserve rankings). The OR-
ANGE approach is, indeed, closely related to the
original QARLA measure introduced in (Amigo et
al., 2004).
Our KING,QUEEN, JACK framework, how-
ever, has a number of advantages over ORANGE:
? It is able to combine different metrics, and
evaluate the quality of metric sets, without
any a-priori weighting of their relative impor-
tance.
? It is not sensitive to repeated (or very similar)
baseline elements.
? It provides a mechanism, JACK, to check
whether a set X,M,A of metrics, manual
and baseline items is reliable enough to pro-
duce a stable evaluation of automatic sum-
marisation systems.
Probably the most significant improvement over
ORANGE is the ability of KING,QUEEN, JACK
to combine automatically the information of dif-
ferent metrics. We believe that a comprehensive
automatic evaluation of a summary must neces-
sarily capture different aspects of the problem with
different metrics, and that the results of every indi-
vidual metric should not be combined in any pre-
scribed algebraic way (such as a linear weighted
combination). Our framework satisfies this con-
dition. An advantage of ORANGE, however, is
that it does not require a large number of gold stan-
dards to reach stability, as in the case of QARLA.
Finally, it is interesting to compare the rankings
produced by QARLA with the output of human
assessments, even if the philosophy of QARLA
is not considering human assessments as the gold
standard for evaluation. Our initial tests on DUC
288
Figure 7: KING vs Pearson correlation with manual rankings in DUC for 1024 metrics combinations
test beds are very promising, reaching Pearson
correlations of 0.9 and 0.95 between human as-
sessments and QUEEN values for DUC 2004 tasks
2 and 5 (Over and Yen, 2004), using metric sets
with highest KING values. The figure 7 shows
how Pearson correlation grows up with higher
KING values for 1024 metric combinations.
Acknowledgments
We are indebted to Ed Hovy, Donna Harman, Paul
Over, Hoa Dang and Chin-Yew Lin for their in-
spiring and generous feedback at different stages
in the development of QARLA. We are also in-
debted to NIST for hosting Enrique Amigo? as a
visitor and for providing the DUC test beds. This
work has been partially supported by the Spanish
government, project R2D2 (TIC-2003-7180).
References
E. Amigo, V. Peinado, J. Gonzalo, A. Pen?as, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL), Barcelona, July.
Deborah Coughlin. 2003. Correlating Automated and
Human Assessments of Machine Translation Qual-
ity. In In Proceedings of MT Summit IX, New Or-
leans,LA.
Christopher Culy and Susanne Riehemann. 2003. The
Limits of N-Gram Translation Evaluation Metrics.
In Proceedings of MT Summit IX, New Orleans,LA.
Luke Shen Joseph P. Turian and I. Dan Melamed.
2003. Evaluation of Machine Translation and its
Evaluation. In In Proceedings of MT Summit IX,
New Orleans,LA.
C. Lin and E. H. Hovy. 2003a. Automatic Evaluation
of Summaries Using N-gram Co-ocurrence Statis-
tics. In Proceeding of 2003 Language Technology
Conference (HLT-NAACL 2003).
Chin-Yew Lin and Eduard Hovy. 2003b. The Poten-
tial and Limitations of Automatic Sentence Extrac-
tion for Summarization. In Dragomir Radev and Si-
mone Teufel, editors, HLT-NAACL 2003 Workshop:
Text Summarization (DUC03), Edmonton, Alberta,
Canada, May 31 - June 1. Association for Computa-
tional Linguistics.
C. Lin. 2004. Orange: a Method for Evaluating Au-
tomatic Metrics for Machine Translation. In Pro-
ceedings of the 36th Annual Conference on Compu-
tational Linguisticsion for Computational Linguis-
tics (Coling?04), Geneva, August.
P. Over and J. Yen. 2004. An introduction to duc 2004
intrinsic evaluation of generic new text summariza-
tion systems. In Proceedings of DUC 2004 Docu-
ment Understanding Workshop, Boston.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, jul.
289
Word Sense Disambiguation based on
Term to Term Similarity in a Context Space
Javier Artiles
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
javart@bec.uned.es
Anselmo Pen?as
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
anselmo@lsi.uned.es
Felisa Verdejo
Dpto. Lenguajes y
Sistemas Informa?ticos
UNED, Spain
felisa@lsi.uned.es
Abstract
This paper describes the exemplar based ap-
proach presented by UNED at Senseval-3. In-
stead of representing contexts as bags of terms
and defining a similarity measure between con-
texts, we propose to represent terms as bags
of contexts and define a similarity measure be-
tween terms. Thus, words, lemmas and senses
are represented in the same space (the context
space), and similarity measures can be defined
between them. New contexts are transformed
into this representation in order to calculate
their similarity to the candidate senses. We
show how standard similarity measures obtain
better results in this framework. A new similar-
ity measure in the context space is proposed for
selecting the senses and performing disambigua-
tion. Results of this approach at Senseval-3 are
here reported.
1 Introduction
Word Sense Disambiguation (WSD) is the task
of deciding the appropriate sense for a partic-
ular use of a polysemous word, given its tex-
tual or discursive context. A previous non triv-
ial step is to determine the inventory of mean-
ings potentially attributable to that word. For
this reason, WSD in Senseval is reformulated as
a classification problem where a dictionary be-
comes the class inventory. The disambiguation
process, then, consists in assigning one or more
of these classes to the ambiguous word in the
given context. The Senseval evaluation forum
provides a controlled framework where different
WSD systems can be tested and compared.
Corpus-based methods have offered encour-
aging results in the last years. This kind of
methods profits from statistics on a training
corpus, and Machine Learning (ML) algorithms
to produce a classifier. Learning algorithms
can be divided in two main categories: Super-
vised (where the correct answer for each piece of
training is provided) and Unsupervised (where
the training data is given without any answer
indication). Tests at Senseval-3 are made in
various languages for which two main tasks are
proposed: an all-words task and a lexical sam-
ple task. Participants have available a training
corpus, a set of test examples and a sense inven-
tory in each language. The training corpora are
available in a labelled and a unlabelled format;
the former is mainly for supervised systems and
the latter mainly for the unsupervised ones.
Several supervised ML algorithms have been
applied to WSD (Ide and Ve?ronis, 1998), (Es-
cudero et al, 2000): Decision Lists, Neural Net-
works, Bayesian classifiers, Boosting, Exemplar-
based learning, etc. We report here the
exemplar-based approach developed by UNED
and tested at the Senseval-3 competition in the
lexical sample tasks for English, Spanish, Cata-
lan and Italian.
After this brief introduction, Sections 2 and
3 are devoted, respectively, to the training data
and the processing performed over these data.
Section 4 characterizes the UNED WSD system.
First, we describe the general approach based on
the representation of words, lemmas and senses
in a Context Space. Then, we show how results
are improved by applying standard similarity
measures as cosine in this Context Space. Once
the representation framework is established, we
define the criteria underlying the final similar-
ity measure used at Senseval-3, and we com-
pare it with the previous similarity measures.
Section 5 reports the official results obtained at
the Senseval-3 Lexical Sample tasks for English,
Spanish, Italian and Catalan. Finally, we con-
clude and point out some future work.
2 Data
Each Lexical Sample Task has a relatively large
training set with disambiguated examples. The
test examples set has approximately a half of
the number of the examples in the training data.
Each example offers an ambiguous word and its
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
surrounding context, where the average context
window varies from language to language. Each
training example gives one or more semantic la-
bels for the ambiguous word corresponding to
the correct sense in that context.
Senseval-3 provided the training data and the
test data in XML format. The XML tagging
conventions provides an excellent ground for the
corpora processing, allowing a simple way for
the data browsing and transformation. How-
ever, some of the XML well-formedness con-
straints are not completely satisfied. For exam-
ple, there is no XML declaration and no root
element in the English Lexical Sample docu-
ments. Once these shortcomings are fixed any
XML parser can normally read and process the
data.
Despite the similarity in the structure of the
different corpora at the lexical sample task
in different languages, we had found a het-
erogeneous vocabulary both in the XML tags
and the attributes, forcing to develop ?ad hoc?
parsers for each language. We missed a common
and public document type definition for all the
tasks.
Sense codification is another field where dif-
ferent solutions had been taken. In the English
corpus nouns and adjectives are annotated using
the WordNet 1.7.1. classification1 (Fellbaum,
1998), while the verbs are based on Wordsmyth2
(Scott, 1997). In the Catalan and Spanish tasks
the sense inventory gives a more coarse-grained
classification than WordNet. Both tasks have
provided a dictionary with additional informa-
tion as examples, typical collocations and the
equivalent synsets at WordNet 1.5. Finally, the
Italian sense inventory is based on the Multi-
Wordnet dictionary3 (Pianta et al, 2002). Un-
like the other mentioned languages , the Italian
task doesn?t provide a separate file with the dic-
tionary.
Besides the training data provided by Sen-
seval, we have used the SemCor (Miller et al,
1993) collection in which every word is already
tagged in its part of speech, sense and synset of
WordNet.
3 Preprocessing
A tokenized version of the Catalan, Spanish and
Italian corpora has been provided. In this ver-
sion every word is tagged with its lemma and
1http://www.cogsci.princeton.edu/ wn/
2http://www.wordsmyth.net
3http://multiwordnet.itc.it/
part of speech tag. This information has been
manually annotated by human assessors both in
the Catalan and the Spanish corpora. The Ital-
ian corpus has been processed automatically by
the TnT POStagger4 (Brants, 2000) including
similar tags.
The English data lacked of this information,
leading us to apply the TreeTagger5 (Schmid,
1994) tool to the training and test data as a
previous step to the disambiguation process.
Since the SemCor collection is already tagged,
the preprocessing consisted in the segmentation
of texts by the paragraph tag, obtaining 5382
different fragments. Each paragraph of Semcor
has been used as a separate training example
for the English lexical sample task. We applied
the mapping provided by Senseval to represent
verbs according to the verb inventory used in
Senseval-3.
4 Approach
The supervised UNED WSD system is an ex-
emplar based classifier that performs the disam-
biguation task measuring the similarity between
a new instance and the representation of some
labelled examples. However, instead of repre-
senting contexts as bags of terms and defining
a similarity measure between the new context
and the training contexts, we propose a rep-
resentation of terms as bags of contexts and
the definition of a similarity measure between
terms. Thus, words, lemmas and senses are
represented in the same space, where similar-
ity measures can be defined between them. We
call this space the Context Space. A new disam-
biguation context (bag of words) is transformed
into the Context Space by the inner product,
becoming a kind of abstract term suitable to be
compared with singular senses that are repre-
sented in the same Context Space.
4.1 Representation
The training corpus is represented in the usual
two-dimension matrix A as shown in Figure 1,
where
? c1, ..., cN is the set of examples or con-
texts in the training corpus. Contexts are
treated as bags of words or lemmas.
? lem1, ..., lemT is the set of different words
or lemmas in all the training contexts.
4http://www.coli.uni-sb.de/ thorsten/tnt/
5http://www.ims.uni-stuttgart.de/projekte/corplex/
TreeTagger/
? wi,j is the weight for lemi in the training
context cj .
A new instance q, represented with the vec-
tor of weights (w1q, ..., wiq, ..., wTq), is trans-
formed into a vector in the context space ~q =
(q1, ..., qj , ..., qN ), where ~q is given by the usual
inner product ~q = q ? A (Figure 1):
qj =
T?
i=1
wiqwij
Figure 1: Representation of terms in the Con-
text Space, and transformation of new in-
stances.
If vectors cj (columns of matrix A) and vector
q (original test context) are normalized to have
a length equal to 1, then qj become the cosine
between vectors q and cj . More formally,
~q = q.A = (cos(q, c1), ..., cos(q, cj), ..., cos(q, cN ))
where
cos(q, cj) =
T?
i=1
wiq
?q?
wij
?cj?
and
?x? =
?
?
i
x2i
At this point, both senses and the representa-
tion of the new instance ~q are represented in the
same context space (Figure 2) and a similarity
measure can be defined between them:
sim( ~senik, ~q)
where senik is the k candidate sense for the
ambiguous lemma lemi. Each component j of
~senik is set to 1 if lemma lemi is used with sense
senik in the training context j, and 0 otherwise.
Figure 2: Similarity in the Context Space.
For a new context of the ambiguous lemma
lemi, the candidate sense with higher similarity
is selected:
argmaxk sim( ~senik, ~q)
4.2 Bag of words versus bag of contexts
Table 1 shows experimental results over the En-
glish Lexical Sample test of Senseval-3. Sys-
tem has been trained with the Senseval-3 data
and the SemCor collection. The Senseval train-
ing data has been lemmatized and tagged with
TreeTagger. Only nouns and adjectives have
been considered in their canonical form.
Three different weights wij have been tested:
? Co-occurrence: wij and wiq are set to {0,1}
depending on whether lemi is present or
not in context cj and in the new instance q
respectively. After the inner product q ? A,
the components qj of ~q get the number of
co-occurrences of different lemmas in both
q and the training context cj .
? Term Frequency: wij is set to tfij , the num-
ber of occurrences of lemi in the context cj .
? tf.idf : wij = (1 + log(tfij)) ? (log( Ndfi )),
a standard tf.idf weight where dfi is the
number of contexts that contain lemi.
These weights have been normalized ( wij||cj ||)
and so, the inner product q?A generates a vector
~q of cosines as described above, where qj is the
cosine between q and context cj .
Two similarity measures have been compared.
The first one (maximum) is a similarity of q as
bag of words with the training contexts of sense
sen. The second one (cosine) is the similarity
of sense sen with ~q in the context space:
? Maximum: sim( ~sen, ~q) =
= MaxNj=1 (senj ? qj) =
= Max{j/sen?cj}qj =
= Max{j/sen?cj}cos(q, cj)
Weight Similarity Nouns Adjectives Verbs Total
Co-occurrences Maximum 60.76% 35.85% 60.75% 59.75%
(normalized) Cosine 59.99% 55.97% 63.88% 61.78%
Term frequency Maximum 56.83% 50.31% 56.85% 56.58%
(normalized) Cosine 60.76% 53.46% 63.83% 62.01%
tf.idf Maximum 59.82% 48.43% 59.94% 59.42%
(normalized) Cosine 60.27% 53.46% 64.29% 62.01%
Most frequent
(baseline) 54.01% 54.08% 56.45% 55.23%
Table 1: Bag of words versus bag of contexts, precision-recall
Similarity with sense sen is the high-
est similarity (cosine) between q (as bag of
words) and each of the training contexts
(as bag of words) for sense sen.
? Cosine: sim( ~sen, ~q) = cos( ~sen, ~q) =
=
?
{j/sen?cj}
senj
|| ~sen|| ?
cos(q,cj)
||~q||
Similarity with sense sen is the co-
sine in the Context Space between ~q and
~sen
Table 1 shows that almost all the results are
improved when the similarity measure (cosine)
is applied in the Context Space. The exception
is the consideration of co-ocurrences to disam-
biguate nouns. This exception led us to explore
an alternative similarity measure aimed to im-
prove results over nouns. The following sections
describe this new similarity measure and the cri-
teria underlying it.
4.3 Criteria for the similarity measure
Co-occurrences behave quite good to disam-
biguate nouns as it has been shown in the exper-
iment above. However, the consideration of co-
occurrences in the Context Space permits acu-
mulative measures: Instead of selecting the can-
didate sense associated to the training context
with the maximum number of co-occurrences,
we can consider the co-occurences of q with all
the contexts. The weights and the similarity
function has been set out satisfying the follow-
ing criteria:
1. Select the sense senk assigned to more
training contexts ci that have the maxi-
mum number of co-occurrences with the
test context q. For example, if sense sen1
has two training contexts with the highest
number of co-occurrences and sense sen2
has only one with the same number of co-
occurrences, sen1 must receive a higher
value than sen2.
2. Try to avoid label inconsistencies in the
training corpus. There are some training
examples where the same ambiguous word
is used with the same meaning but tagged
with different sense by human assessors.
Table 2 shows an example of this kind of
inconsistencies.
4.4 Similarity measure
We assign the weights wij and wiq to have ~q a
vector of co-occurrences, where qj is the number
of different nouns and adjectives that co-occurr
in q and the training context cj . In this way, wij
is set to 1 if lemi is present in the context cj .
Otherwise wij is set to 0. Analogously for the
new instance q, wiq is set to 1 if lemi is present
in q and it is set to 0 otherwise.
According to the second criterium, if there
is only one context c1 with the higher num-
ber of co-occurrences with q, then we reduce
the value of this context by reducing artifi-
cially its number of co-occurrences: Being c2
a context with the second higher number of co-
occurrences with q, then we assign to the first
context c1 the number of co-occurrences of con-
text c2.
After this slight modification of ~q we imple-
ment the similarity measure between ~q and a
sense senk according to the first criterium:
sim( ~sen, ~q) =
N?
j=1
senj ? N
qj
Finally, for a new context of lemi we select
the candidate sense that gives more value to the
similarity measure:
argmaxk sim( ~senk, ~q)
<answer instance=?grano.n.1? senseid=?grano.4?/>
<previous> La Federacin Nacional de Cafeteros de Colombia explic que el nuevo valor fue estable-
cido con base en el menor de los precios de reintegro mnimo de grano del pas de los ltimos tres das,
y que fue de 1,3220 dlares la libra, que fue el que alcanz hoy en Nueva York, y tambin en la tasa rep-
resentativa del mercado para esta misma fecha (1.873,77 pesos por dlar). </previous> <target>
El precio interno del caf colombiano permaneci sin modificacin hasta el 10 de noviembre de 1999,
cuando las autoridades cafetaleras retomaron el denominado ?sistema de ajuste automtico?, que
tiene como referencia la cotizacin del <head>grano</head> nacional en los mercados interna-
cionales. </target>
<answer instance=?grano.n.9? senseid=?grano.3?/>
<previous> La carga qued para maana en 376.875 pesos (193,41 dlares) frente a los 375.000 pesos
(192,44 dlares) que rigi hasta hoy. </previous> <target> El reajuste al alza fue adoptado por
el Comit de Precios de la Federacin que fijar el precio interno diariamente a partir de este lunes
tomando en cuenta la cotizacin del <head>grano</head> en el mercado de Nueva York y la tasa
de cambio del da, que para hoy fueron de 1,2613 dlares libra y1.948,60 pesos por dlar </target>
Table 2: Example of inconsistencies in human annotation
Weight Similarity Nouns Adjectives Verbs Total
Co-occurrences Without criterium 2 65.6% 45.9% 62.5% 63.3%
(not normalized) With criterium 2 66.5% 45.9% 63.4% 64.1%
Table 3: Precision-recall for the new similarity measure
Table 3 shows experimental results over the
English Lexical Sample test under the same con-
ditions than experiments in Table 1.
Comparing results in both tables we observe
that the new similarity measure only behaves
better for the disambiguation of nouns. How-
ever, the difference is big enough to improve
overall results. The application of the second
criterium (try to avoid label inconsistencies)
also improves the results as shown in Tables 3
and 4. Table 4 shows the effect of applying this
second criterium to all the languages we have
participated in. With the exception of Cata-
lan, all results are improved slightly (about 1%)
after the filtering of singular labelled contexts.
Although it is a regular behavior, this improve-
ment is not statistically significative.
With Without
Criterium 2 Criterium 2
Spanish 81.8% 80.9%
Catalan 81.8% 82.0%
English 64.1% 63.3%
Italian 49.8% 49.3%
Table 4: Incidence of Criterium 2, precision-
recall
5 Results at Senseval-3
The results submited to Senseval-3 were gener-
ated with the system described in Section 4.4.
Since one sense is assigned to every test con-
text, precison and recall have equal values. Ta-
ble 4 shows official results for the Lexical Sam-
ple Task at Senseval-3 in the four languages we
have participated in: Spanish, Catalan, English
and Italian.
Fine Coarse Baseline
grained grained (most frequent)
Spanish 81.8% - 67%
Catalan 81.8% - 66%
English 64.1% 72% 55%
Italian 49.8% - -
Table 5: Official results at Senseval-3, precision-
recall
Differences between languages are quite re-
markable and show the system dependence on
the training corpora and the sense inventory.
In the English task, 16 test instances have
a correct sense not present in the training cor-
pus. Since we don?t use the dictionary informa-
tion our system was unable to deal with none of
them. In the same way, 68 test instances have
been tagged as ?Unasignable? sense and again
the system was unable to detect none of them.
6 Conclusion and work in progress
We have shown the exemplar-based WSD sys-
tem developed by UNED for the Senseval-3 lexi-
cal sample tasks. The general approach is based
on the definition of a context space that be-
comes a flexible tool to prove quite different
similarity measures between training contexts
and new instances. We have shown that stan-
dard similarity measures improve their results
applied inside this context space. We have es-
tablished some criteria to instantiate this gen-
eral approach and the resulting system has been
evaluated at Senseval-3. The new similarity
measure improves the disambiguation of nouns
and obtains better overall results. The work in
progress includes:
? the study of new criteria to lead us to al-
ternative measures,
? the development of particular disambigua-
tion strategies for verbs, nouns and adjec-
tives,
? the inclusion of the dictionary information,
and
? the consideration of WordNet semantic re-
lationships to extend the training corpus.
Acknowledgements
Special thanks to Julio Gonzalo for the lending
of linguistic resources, and to V??ctor Peinado for
his demonstrated sensibility.
This work has been supported by the Spanish
Ministry of Science and Technology through the
following projects:
? Hermes (TIC-2000-0335-C03-01)
? Syembra (TIC-2003-07158-C04-02)
? R2D2 (TIC 2003-07158-104-01)
References
Thorsten Brants. 2000. Tnt - a statistical part-
of-speech tagger. In In Proceedings of the
Sixth Applied Natural Language Processing
Conference ANLP-2000.
G. Escudero, L. Ma`rquez, and G. Rigau. 2000.
A comparison between supervised learning al-
gorithms for word sense disambiguation. In
In Proceedings of the 4th Computational Nat-
ural Language Learning Workshop, CoNLL.
Christiane Fellbaum, editor. 1998. WordNet
An Electronic Lexical Database. The MIT
Press.
N. Ide and J. Ve?ronis. 1998. Introduction to the
special issue on word sense disambiguation:
The state of the art. Computational Linguis-
tics.
G. Miller, C. Leacock, T. Randee, and
R. Bunker. 1993. A semantic concordance.
In In Procedings of the 3rd DARPA Work-
shop on Human Language Technology.
E. Pianta, L. Bentivogli, and C. Girardi. 2002.
Multiwordnet: developing an aligned mul-
tilingual database. In In Proceedings of
the First International Conference on Global
WordNet.
Helmut Schmid. 1994. Probabilistic part-of-
speech tagging using decision trees. In Inter-
national Conference on New Methods in Lan-
guage Processing.
M. Scott. 1997. Wordsmith tools lexical analy-
sis software for data driven learning and re-
search. Technical report, The University of
Liverpool.
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 49?56, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating DUC 2004 Tasks with the QARLA Framework
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, Felisa Verdejo
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
c/Juan del Rosal, 16 - 28040 Madrid - Spain
{enrique,julio,anselmo,felisa}@lsi.uned.es
Abstract
This papers reports the application of
the QARLA evaluation framework to the
DUC 2004 testbed (tasks 2 and 5). Our
experiment addresses two issues: how
well QARLA evaluation measures corre-
late with human judgements, and what ad-
ditional insights can be provided by the
QARLA framework to the DUC evalua-
tion exercises.
1 Introduction
QARLA (Amigo? et al, 2005) is a framework that
uses similarity to models as a building block for
the evaluation of automatic summarisation systems.
The input of QARLA is a summarisation task, a set
of test cases, a set of similarity metrics, and sets of
models and automatic summaries (peers) for each
test case. With such a testbed, QARLA provides:
? A measure, QUEEN, which combines assorted
similarity metrics to estimate the quality of au-
tomatic summarisers.
? A measure, KING, to select the best combina-
tion of similarity metrics.
? An estimation, JACK, of the reliability of the
testbed for evaluation purposes.
The QARLA framework does not rely on human
judges. It is interesting, however, to find out how
well an evaluation using QARLA correlates with hu-
man judges, and whether QARLA can provide ad-
ditional insights into an evaluation based on human
assessments.
In this paper, we apply the QARLA framework
(QUEEN, KING and JACK measures) to the out-
put of two different evaluation exercises: DUC 2004
tasks 2 and 5 (Over and Yen, 2004). Task 2 re-
quires short (one-hundred word) summaries for as-
sorted document sets; Task 5 consists of generating
a short summary in response to a ?Who is? question.
In Section 2, we summarise the QARLA evalua-
tion framework; in Section 3, we describe the sim-
ilarity metrics used in the experiments. Section 4
discusses the results of the QARLA framework us-
ing such metrics on the DUC testbeds. Finally, Sec-
tion 5 draws some conclusions.
2 The QARLA evaluation framework
QARLA uses similarity to models for the evalua-
tion of automatic summarisation systems. Here we
summarise its main features; the reader may refer to
(Amigo? et al, 2005) for details.
The input of the framework is:
? A summarisation task (e.g. topic oriented, in-
formative multi-document summarisation on a
given domain/corpus).
? A set T of test cases (e.g. topic/document set
pairs for the example above)
? A set of summaries M produced by humans
(models), and a set of automatic summaries A
(peers), for every test case.
? A set X of similarity metrics to compare sum-
maries.
With this input, QARLA provides three main
measures that we describe below.
49
2.1 QUEEN : Estimating the quality of an
automatic summary
QUEEN operates under the assumption that a sum-
mary is better if it is closer to the model summaries
according to all metrics; it is defined as the probabil-
ity, measured onM ?M ?M , that for every metric
in X the automatic summary a is closer to a model
than two models to each other:
QUEENX,M (a) ? P (?x ? X.x(a,m) ? x(m
?,m??))
where a is the automatic summary being eval-
uated, ?m,m?,m??? are three models in M , and
x(a,m) stands for the similarity ofm to a. QUEEN
is stated as a probability, and therefore its range of
values is [0, 1].
We can think of the QUEEN measure as using a
set of tests (every similarity metric in X) to falsify
the hypothesis that a given summary a is a model.
Given ?a,m,m?,m???, we test x(a,m) ? x(m?,m??)
for each metric x. a is accepted as a model only if
it passes the test for every metric. QUEEN(a) is,
then, the probability of acceptance for a in the sam-
ple space M ?M ?M .
This measure has some interesting properties: (i)
it is able to combine different similarity metrics
into a single evaluation measure; (ii) it is not af-
fected by the scale properties of individual metrics,
i.e. it does not require metric normalisation and
it is not affected by metric weighting. (iii) Peers
which are very far from the set of models all receive
QUEEN=0. In other words, QUEEN does not distin-
guish between very poor summarisation strategies.
(iv) The value of QUEEN is maximised for peers
that ?merge? with the models under all metrics inX .
(v) The universal quantifier on the metric parameter
x implies that adding redundant metrics do not bias
the result of QUEEN.
Now the question is: which similarity metrics
are adequate to evaluate summaries? Imagine that
we use a similarity metric based on sentence co-
selection; it might happen that humans do not agree
on which sentences to select, and therefore emulat-
ing their sentence selection behaviour is both easy
(nobody agrees with each other) and useless. We
need to take into account which are the features that
human summaries do share, and evaluate according
to them. This is provided by the KING measure.
2.2 KING: estimating the quality of similarity
metrics
The measure KINGM,A(X) estimates the quality of
a set of similarity metrics X using a set of models
M and a set of peers A. KING is defined as the
probability that a model has higher QUEEN value
than any peer in a test sample. Formally:
KINGM,A(X) ?
P (?a ? A,QUEENM,X(m) > QUEENM,X(a))
For example, an ideal metric -that puts all models
together-would give QUEEN(m) = 1 for all mod-
els, and QUEEN(a) = 0 for all peers which are not
put together with the models, obtaining KING = 1.
KING satisfies several interesting properties: (i)
KING does not depend on the scale properties of the
metric; (ii) Adding repeated or very similar peers
do not alter the KING measure, which avoids one
way of biasing the measure. (iii) the KING value of
random and constant metrics is zero or close to zero.
2.3 JACK: reliability of the peer set
Once we detect a difference in quality between two
summarisation systems, the question is now whether
this result is reliable. Would we get the same results
using a different test set (different examples, differ-
ent human summarisers (models) or different base-
line systems)?
The first step is obviously to apply statistical sig-
nificance tests to the results. But even if they give a
positive result, it might be insufficient. The problem
is that the estimation of the probabilities in KING
assumes that the sample sets M,A are not biased.
If M,A are biased, the results can be statistically
significant and yet unreliable. The set of examples
and the behaviour of human summarisers (models)
should be somehow controlled either for homogene-
ity (if the intended profile of examples and/or users
is narrow) or representativity (if it is wide). But how
to know whether the set of automatic summaries is
representative and therefore is not penalising certain
automatic summarisation strategies?
This is addressed by the JACK measure:
50
JACK(X,M,A) ? P (?a, a? ? A|
?x ? X.x(a, a?) ? x(a,m) ? x(a?, a) ? x(a?,m)?
QUEEN(a) > 0 ? QUEEN(a?) > 0)
i.e. the probability over all model summariesm of
finding a couple of automatic summaries a, a? which
are closer to m than to each other according to all
metrics. This measure satisfies three desirable prop-
erties: (i) it can be enlarged by increasing the sim-
ilarity of the peers to the models (the x(m,a) fac-
tor in the inequalities), i.e. enhancing the quality of
the peer set; (ii) it can also be enlarged by decreas-
ing the similarity between automatic summaries (the
x(a, a?) factor in the inequality), i.e. augmenting the
diversity of (independent) automatic summarisation
strategies represented in the test bed; (iii) adding el-
ements to A cannot diminish the JACK value, be-
cause of the existential quantifier on a, a?.
3 Selection of similarity metrics
Each different similarity metric characterises differ-
ent features of a summary. Our first objective is
to select the best set of metrics, that is, the metrics
which best characterise the human summaries (mod-
els) as opposed to automatic summaries. The second
objective is to obtain as much information as possi-
ble about the behaviour of automatic summaries.
In this Section, we begin by describing a set of
59 metrics used as a starting point. Some of them
provide overlapping information; the second step is
then to select a subset of metrics that minimises re-
dundancy and, at the same time, maximises quality
(KING values). Finally, we analyse the characteris-
tics of the selected metrics.
3.1 Similarity metrics
For this work, we have considered the following
similarity metrics:
ROUGE based metrics (R): ROUGE (Lin and
Hovy, 2003) estimates the quality of an au-
tomatic summary on the basis of the n-gram
coverage related to a set of human summaries
(models). Although ROUGE is an evaluation
metric, we can adapt it to behave as a sim-
ilarity metric between pairs of summaries if
we consider only one model in the computa-
tion. There are different kinds of ROUGE met-
rics such as ROUGE-W, ROUGE-L, ROUGE-
1, ROUGE-2, ROUGE-3, ROUGE-4, etc. (Lin,
2004b). Each of these metrics has been ap-
plied over summaries with three preprocessing
options: with stemming and stopword removal
(type c); only with stopwords removal (type b);
or without any kind of preprocessing (type a).
All these combinations give 24 similarity met-
rics based on ROUGE.
Inverted ROUGE based metrics (Rpre): ROUGE
metrics are recall oriented. If we reverse the di-
rection of the similarity computation, we obtain
precision oriented metrics (i.e. Rpre(a, b) =
R(b, a)). In this way, we generate another 24
metrics based on inverted ROUGE.
TruncatedVectModel (TVMn): This family of met-
rics compares the distribution of the n most
relevant terms from original documents in the
summaries. The process is the following: (1)
obtaining the n most frequent lemmas ignoring
stopwords; (2) generating a vector with the rel-
ative frequency of each term in the summary;
(3) calculating the similarity between two vec-
tors as the inverse of the Euclidean distance.
We have used 9 variants of this measure with
n = 1, 4, 8, 16, 32, 64, 128, 256, 512.
AveragedSentencelengthSim (AVLS): This is a very
simple metric that compares the average length
of the sentences in two summaries. It can be
useful to compare the degree of abstraction of
the summaries.
GRAMSIM: This similarity metric compares the
distribution of the part-of-speech tags in the
two summaries. The processing is the follow-
ing: (1) part-of-speech tagging of summaries
using TreeTagger ; (2) generation of a vector
with the tags frequency for each summary; (3)
calculation of the similarity between two vec-
tors as the inverse of the Euclidean distance.
This similarity metric is not content oriented,
but syntax-oriented.
51
Figure 1: Similarity Metric Clusters
3.2 Clustering similarity metrics
From the set of metrics described above we have 57
(24+24+9) content oriented metrics, plus two met-
rics based on stylistic features (AVLS and GRAM-
SIM). However, the 57 metrics characterising sum-
mary contents are highly redundant. Thus, cluster-
ing similar metrics seems desirable.
We perform an automatic clustering process us-
ing the following notion of proximity between two
metric sets:
sim(X,X ?) ? Prob[H(X) ? H(X ?)]
where H(X) ? ?x ? X.x(a,m) ? x(m?,m??)
Two metrics sets are similar, according to the for-
mula, if they behave similarly with respect to the
QUEEN condition (H predicate in the formula),
i.e. the probability that the two sets of metrics dis-
criminate the same automatic summaries when they
are compared to the same pair of models.
Figure 1 shows the clustering of similarity met-
rics for the DUC 2004 Task 2. The number of clus-
ters was fixed in 10. After the clustering process, the
48 ROUGE metrics are grouped in 7 sets, and the 9
TVM metrics are grouped in 3 sets. In each clus-
ter, the metric with highest KING has been marked
in boldface. Note that the ROUGE-c metrics (with
stemming) with highest KING are those based on re-
call whereas the ROUGE-a/b metrics (without stem-
ming) are those based on precision. Regarding TVM
clusters, the metrics with highest KING in each clus-
ter are those based on a higher number of terms.
Finally, we select the metric with highest KING
in each group, obtaining the 10 most representative
metrics.
3.3 Best evaluation metric: KING values
Figure 2 shows the KING values for the selected
similarity metrics, which represent how every metric
characterises model summaries as opposed to auto-
matic summaries. These are the main results:
? The last column shows the best metric set,
considering all possible metric combinations.
In both DUC tasks, the best combination is
{Rpre-W-1.2.b,TVM.512. This metric set gets
better KING values than any individual metric
in isolation (17% better than the second best for
task 2, and 23% better for task 5). This is an in-
teresting result confirming that we can improve
our ability to characterise human summaries
just by combining standard similarity metrics
in the QARLA framework. Note also that both
metrics in the best set are content-oriented.
? Rpre-W.1.2.b (inverted ROUGE measure, us-
ing non-contiguous word sequences, remov-
ing stopwords, without stemming) obtains the
highest individual KING for task 2, and is one
of the best in task 5, confirming that ROUGE-
based metrics are a robust way of evaluating
summaries, and indicating that non-contiguous
word sequences can be more useful for evalua-
tion purposes than n-grams.
52
Figure 2: Similarity Metric quality
? TVM metrics get higher values when consid-
ering more terms (TVM.512), confirming that
comparing with just a few terms (e.g. TVM.4)
is not informative enough.
? Overall, KING values are higher for task
5, suggesting that there is more agreement
between human summaries in topic-oriented
tasks.
3.4 Reliability of the results
The JACK measure estimates the reliability of
QARLA results, and is correlated with the diversity
of automatic summarisation strategies included in
the testbed. In principle, the larger the number of au-
tomatic summaries, the higher the JACK values we
should obtain. The important point is to determine
when JACK values tend to stabilise; at this point, it
is not useful to add more automatic summaries with-
out introducing new summarisation strategies.
Figure 3 shows how JACKRpre-W,TVM.512 values
grow when adding automatic summaries. For more
than 10 systems, JACK values grow slower in both
tasks. Absolute JACK values are higher in Task 2
than in task 5, indicating that systems tend to pro-
duce more similar summaries in Task 5 (perhaps be-
cause it is a topic-oriented task). This result suggests
that we should incorporate more diverse summarisa-
tion strategies in Task 5 to enhance the reliability of
the testbed for evaluation purposes with QARLA.
4 Evaluation of automatic summarisers:
QUEEN values
The QUEEN measure provides two kinds of infor-
mation to compare automatic summarisation sys-
tems: which are the best systems -according to the
best metric set-, and which are the individual fea-
tures of every automatic summariser -according to
individual similarity metrics-.
4.1 System ranking
The best metric combination for both tasks was
{Rpre-W,TVM.512}; therefore, our global system
evaluation uses this combination of content-oriented
metrics. Figure 4 shows the QUEEN{Rpre-W,TVM.512}
values for each participating system in DUC 2004,
also including the model summaries. As expected,
model summaries obtain the highest QUEEN values
in both DUC tasks, with a significant distance with
respect to the automatic summaries.
4.2 Correlation with human judgements
The manual ranking generated in DUC is based on a
set of human-produced evaluation criteria, whereas
the QARLA framework gives more weight to the as-
pects that characterise model summaries as opposed
to automatic summaries. It is interesting, however,
to find out whether both evaluation methodologies
are correlated. Indeed, this is the case: the Pearson
correlation between manual and QUEEN rankings is
0.92 for the Task 2 and 0.96 for the Task 5.
Of course, QUEEN values depend on the chosen
metric set X; it is also interesting to check whether
53
Figure 3: JACK vs. Number of Automatic Summaries
Figure 4: QUEEN system ranking for the best metric set (A-H are models)
Figure 5: Correlation Between DUC and QARLA results
54
Figure 6: QUEEN values over GRAMSIM
metrics with higher KING values lead to QUEEN
rankings more similar to human judgements. Fig-
ure 5 shows the Pearson correlation between man-
ual and QUEEN rankings for 1024 metric combina-
tions with different KING values. The figure con-
firms that higher KING values are associated with
rankings closer to human judgements.
4.3 Stylistic features
The best metric combination leaves out similarity
metrics based on stylistic features. It is interesting,
however, to see how automatic summaries behave
with respect to this kind of features. Perhaps the
most remarkable fact about stylistic similarities is
that, in the case of the GRAMSIM metric, task 2
and task 5 exhibit a rather different behaviour (see
Figure 6). In task 2, systems merge with the models,
while in task 5 the QUEEN values of the systems
are inferior to the models. This suggests that there
is some stylistic component in models that systems
are not capturing in the topic-oriented task.
5 Related work
The methodology which is closest to our frame-
work is ORANGE (Lin, 2004a), which evaluates a
similarity metric using the average ranks obtained
by reference items within a baseline set. As in
our framework, ORANGE performs an automatic
meta-evaluation, there is no need for human assess-
ments, and it does not depend on the scale properties
of the metric being evaluated (because changes of
scale preserve rankings). The ORANGE approach
is, indeed, intimately related to the original QARLA
measure introduced in (Amigo et al, 2004).
There are several approaches to the automatic
evaluation of summarisation and Machine Transla-
tion systems (Culy and Riehemann, 2003; Coughlin,
2003). Probably the most significant improvement
over ORANGE is the ability to combine automati-
cally the information of different metrics. Our im-
pression is that a comprehensive automatic evalua-
tion of a summary must necessarily capture different
aspects of the problem with different metrics, and
that the results of every individual checking (metric)
should not be combined in any prescribed algebraic
way (such as a linear weighted combination). Our
framework satisfies this condition.
ORANGE, however, has also an advantage over
the QARLA framework, namely that it can be used
for evaluation metrics which are not based on sim-
ilarity between model/peer pairs. For instance,
ROUGE can be applied directly in the ORANGE
framework without any reformulation.
6 Conclusions
The application of the QARLA evaluation frame-
work to the DUC testbed provides some useful in-
sights into the problem of evaluating text summari-
sation systems:
? The results show that a combination of simi-
larity metrics behaves better than any metric in
isolation. The best metric set is {Rpre-W, TVM.512},
a combination of content-oriented metrics. Un-
55
surprisingly, stylistic similarity is less useful
for evaluation purposes.
? The evaluation provided by QARLA correlates
well with the rankings provided by DUC hu-
man judges. For both tasks, metric sets with
higher KING values slightly outperforms the
best ROUGE evaluation measure.
? QARLA measures show that DUC tasks 2 and
5 are quite different in nature. In Task 5, human
summaries are more similar, and the automatic
summarisation strategies evaluated are less di-
verse.
Acknowledgements
We are indebted to Ed Hovy, Donna Harman, Paul
Over, Hoa Dang and Chin-Yew Lin for their inspir-
ing and generous feedback at different stages in the
development of QARLA. We are also indebted to
NIST for hosting Enrique Amigo? as a visitor and
for providing the DUC test beds. This work has
been partially supported by the Spanish government,
project R2D2 (TIC-2003-7180).
References
E. Amigo?, J. Gonzalo, A. Pen?as, and F. Verdejo. 2005.
QARLA: a Framework for the Evaluation of Text
Summarization Systems. In Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL 2005).
E. Amigo, V. Peinado, J. Gonzalo, A. Pen?as, and
F. Verdejo. 2004. An Empirical Study of Information
Synthesis Tasks. In Proceedings of the 42th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Barcelona, July.
Deborah Coughlin. 2003. Correlating Automated and
Human Assessments of Machine Translation Quality.
In In Proceedings of MT Summit IX, New Orleans,LA.
Christopher Culy and Susanne Riehemann. 2003. The
Limits of N-Gram Translation Evaluation Metrics. In
Proceedings of MT Summit IX, New Orleans,LA.
C. Lin and E. H. Hovy. 2003. Automatic Evaluation of
Summaries Using N-gram Co-ocurrence Statistics. In
Proceeding of 2003 Language Technology Conference
(HLT-NAACL 2003).
C. Lin. 2004a. Orange: a Method for Evaluating Au-
tomatic Metrics for Machine Translation. In Pro-
ceedings of the 36th Annual Conference on Compu-
tational Linguisticsion for Computational Linguistics
(Coling?04), Geneva, August.
Chin-Yew Lin. 2004b. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
P. Over and J. Yen. 2004. An introduction to DUC 2004
Intrinsic Evaluation of Generic New Text Summariza-
tion Systems. In Proceedings of DUC 2004 Document
Understanding Workshop, Boston.
56
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 89?94,
Prague, June 2007. c?2007 Association for Computational Linguistics
Experiments of UNED at the Third
Recognising Textual Entailment Challenge
A?lvaro Rodrigo, Anselmo Pen?as, Jesu?s Herrera, Felisa Verdejo
Departmento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
Madrid, Spain
{alvarory, anselmo, jesus.herrera, felisa}@lsi.uned.es
Abstract
This paper describes the experiments devel-
oped and the results obtained in the partic-
ipation of UNED in the Third Recognising
Textual Entailment (RTE) Challenge. The
experiments are focused on the study of the
effect of named entities in the recognition
of textual entailment. While Named Entity
Recognition (NER) provides remarkable re-
sults (accuracy over 70%) for RTE on QA
task, IE task requires more sophisticated
treatment of named entities such as the iden-
tification of relations between them.
1 Introduction
The systems presented to the Third Recognizing
Textual Entailment Challenge are based on the one
presented to the Second RTE Challenge (Herrera
et al, 2006b) and the ones presented to the An-
swer Validation Exercise (AVE) 2006 (Rodrigo et
al., 2007).
Since a high quantity of pairs of RTE-3 collec-
tions contain named entities (82.6% of the hypothe-
ses in the test collection contain at least one named
entity), the objective of this work is to study the ef-
fect of named entity recognition on textual entail-
ment in the framework of the Third RTE Challenge.
In short, the techniques involved in the experi-
ments in order to reach these objectives are:
? Lexical overlapping between ngrams of text
and hypothesis.
? Entailment between named entities.
? Branch overlapping between dependency trees
of text and hypothesis.
In section 2, the main components of the systems
are described in detail. Section 3 describes the infor-
mation our systems use for the entailment decision.
The description of the two runs submitted are given
in Section 4. The results obtained and its analysis are
described in Section 5. Section 6 shows a discussion
of the results. Finally, some conclusions and future
work are given.
2 Systems Description
The proposed systems are based on surface tech-
niques of lexical and syntactic analysis considering
each task (Information Extraction, Information Re-
trieval, Question Answering and Text Summariza-
tion) of the RTE Challenge independently.
The systems accept pairs of text snippets (text and
hypothesis) at the input and give a boolean value at
the output: YES if the text entails the hypothesis and
NO otherwise. This value is obtained by the appli-
cation of the learned model by a SVM classifier.
The main components of the systems are the fol-
lowing:
2.1 Linguistic processing
Firstly, each text-hypothesis pair is preprocessed in
order to obtain the following information for the en-
tailment decision:
? POS: a Part of Speech Tagging is performed in
order to obtain lemmas for both text and hy-
pothesis using the Freeling POS tagger (Car-
reras et al, 2004).
89
<t>...Iraq invaded Kuwait on <TIMEX>August 2 1990</TIMEX>...</t>
<h>Iraq invaded Kuwait in <NUMEX>1990</NUMEX></h>
Figure 1: Example of an error when disambiguating the named entity type.
<t>...Chernobyl accident began on
<ENTITY>Saturday April 26 1986</ENTITY>...</t>
<h>The Chernobyl disaster was in <ENTITY>1986</ENTITY></h>
Figure 2: Example of a pair that justifies the process of entailment.
<pair id=??5?? entailment=??NO?? task=??IE?? length=??short??>
<t>The Communist Party USA was a small Maoist political party
which was founded in 1965 by members of the Communist Party around
Michael Laski who took the side of China in the Sino-Soviet split.
</t>
<h>Michael Laski was an opponent of China.</h>
</pair>
<pair id=??7?? entailment=??NO?? task=??IE?? length=??short??>
<t>Sandra Goudie was first elected to Parliament in the 2002
elections, narrowly winning the seat of Coromandel by defeating
Labour candidate Max Purnell and pushing incumbent Green MP
Jeanette Fitzsimons into third place.</t>
<h>Sandra Goudie was defeated by Max Purnell.</h>
</pair>
<pair id=??8?? entailment=??NO?? task=??IE?? length=??short??>
<t>Ms. Minton left Australia in 1961 to pursue her studies in
London.</t>
<h>Ms. Minton was born in Australia.</h>
</pair>
Figure 3: IE pairs with entailment between named entities but not between named entities relations.
90
? NER: the Freeling Named Entity Recogniser is
also applied to recover the information needed
by the named entity entailment module that is
described in the following section. Numeric ex-
pressions, proper nouns and temporal expres-
sions of each text and hypothesis are tagged.
? Dependency analysis: a dependency tree of
each text and hypothesis is obtained using Lin?s
Minipar (Lin, 1998).
2.2 Entailment between named entities
Once the named entities of the hypothesis and the
text are detected, the next step is to determine the
entailment relations between the named entities in
the text and the named entities in the hypothesis. In
(Rodrigo et al, 2007) the following entailment rela-
tions between named entities were defined:
1. A Proper Noun E1 entails a Proper Noun E2 if
the text string of E1 contains the text string of
E2.
2. A Time Expression T1 entails a Time Expres-
sion T2 if the time range of T1 is included in
the time range of T2.
3. A numeric expression N1 entails a numeric ex-
pression N2 if the range associated to N2 en-
closes the range of N1.
Some characters change in different expressions
of the same named entity as, for example, in a proper
noun with different wordings (e.g. Yasser, Yaser,
Yasir). To detect the entailment in these situations,
when the previous process fails, we implemented a
modified entailment decision process taking into ac-
count the edit distance of Levenshtein (Levensthein,
1966). Thus, if two named entities differ in less than
20%, then we assume that exists an entailment rela-
tion between these named entities.
However, this definition of named entities entail-
ment does not support errors due to wrong named
entities classification as we can see in Figure 1. The
expression 1990 represents a year but it is recog-
nised as a numeric expression in the hypothesis.
However the same expression is recognised as a tem-
poral expression in the text and, therefore, the ex-
pression in the hypothesis cannot be entailed by it
according to the named entities entailment definition
above.
We quantified the effect of these errors in recog-
nising textual entailment. For this purpose, we de-
veloped the following two settings:
1. A system based in dependency analysis and
WordNet (Herrera et al, 2006b) that uses the
categorization given by the NER tool, where
the entailment relations between named entities
are the previously ones defined.
2. The same system based on dependency analysis
and WordNet but not using the categorization
given by the NER tool. All named entities de-
tected receive the same tag and a named entity
E1 entails a named entity E2 if the text string
of E1 contains the text string of E2 (see Figure
2).
We checked the performance of these two settings
over the test corpus set of the Second RTE Chal-
lenge. The results obtained, using the accuracy mea-
sure that is the fraction of correct responses accord-
ing to (Dagan et al, 2006), are shown in table 1. The
table shows that with an easier and a more robust
processing (NER without classification) the perfor-
mance is not only maintained, but it is even slightly
higher.
This fact led us to ignore the named entity catego-
rization given by the tool and assume that text and
hypothesis are related and close texts where same
expressions must receive same categories, without
the need of classification. Thus, all detected named
entities receive the same tag and we consider that a
named entity E1 entails a named entity E2 if the text
string of E1 contains the text string of E2.
Table 1: Entailment between numeric expressions.
Accuracy
Setting 1 0.610
Setting 2 0.614
2.3 Sentence level matching
A tree matching module, which searches for match-
ing branches into the hypotheses? dependency trees,
is used. There is a potential matching branch per
leaf. A branch from the hypothesis is considered
91
a ?matching branch? only if all its nodes from the
root to the leaf are involved in a lexical entailment
(Herrera et al, 2006a). In this way, the subtree con-
formed by all the matching branches from a hypoth-
esis? dependency tree is included in the respective
text?s dependency tree, giving an idea of tree inclu-
sion.
We assumed that the larger is the included sub-
tree of the hypothesis? dependency tree, the more
semantically similar are the text and the hypothesis.
Thus, the existence or absence of an entailment rela-
tion from a text to its respective hypothesis considers
the portion of the hypothesis? tree that is included in
the text?s tree.
3 Entailment decision
A SVM classifier was applied in order to train a
model from the development corpus. The model was
trained with a set of features obtained from the pro-
cessing described above. The features we have used
and the training strategies were the following:
3.1 Features
We prepared the following features to feed the SVM
model:
1. Percentage of nodes of the hypothesis? de-
pendency tree pertaining to matching branches
according to section 2.3 considering, respec-
tively:
? Lexical entailment between the words of
the snippets involved.
? Lexical entailment between the lemmas of
the snippets involved.
2. Percentage of words of the hypothesis in the
text (treated as bags of words).
3. Percentage of unigrams (lemmas) of the hy-
pothesis in the text (treated as bags of lemmas).
4. Percentage of bigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
5. Percentage of trigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
6. A boolean value indicating if there is or not any
named entity in the hypothesis that is not en-
tailed by one or more named entities in the text
according to the named entity entailment deci-
sion described in section 2.2.
Table 2: Experiments with separate training over the
development corpus using cross validation.
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.64 0.67
Setting 2 0.62 0.66
Table 3: Experiments with separate training over the
test corpus.
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.59 0.62
Setting 2 0.60 0.64
Table 4: Results for run 1 and run 2.
Accuracy
run 1 run 2
IE 52.50% 53.50%
IR 67% 67%
QA 72% 72%
SUM 58% 60%
Overall 62.38% 63.12%
3.2 Training
About the decision of how to perform the training
in our SVM models, we wanted to study the effect
of training a unique model compared to training one
different model per task.
For this purpose we used the following two set-
tings:
1. A SVM model that uses features 2, 3, 4 and 5
from section 3.1.
2. A SVM model that uses features 2, 3, 4, 5 and
6 from section 3.1.
Each setting was training using cross validation
over the development set of the Third RTE Chal-
lenge in two different ways:
1. Training a unique model for all pairs.
92
2. Training one model for each task. Each model
is trained with only pairs from the same task
that the model will predict.
The results obtained in the experiments are shown
in table 2. As we can see in the table, with the train-
ing of one model for each task results are slightly
better, increasing performance of both settings. Tak-
ing into account these results, we took the decision
of using a different training for each task in the runs
submitted.
Our decision was confirmed after the runs submis-
sion to RTE-3 Challenge with new experiments over
the RTE-3 test corpus, using the RTE-3 development
corpus as training (see table 3 for results).
4 Runs Submitted
Two different runs were submitted to the Third RTE
Challenge. Each run was trained using the method
described in section 3.2 with the following subset of
the features described in section 3.1:
? Run 1 was obtained using the features 2, 3,
4 and 5 from section 3.1. These features ob-
tained good results for pairs from the QA task,
as we can see in (Rodrigo et al, 2007), and
we wanted to check their performance in other
tasks.
? Run 2 was obtained using the following fea-
tures for each task:
? IE: features 2, 3, 4, 5 and 6 from section
3.1. These ones were the features that ob-
tained the best results for IE pairs in our
experiments over the development set.
? IR: features 2, 3, 4 and 5 from section 3.1.
These ones were the features that obtained
best results for IR pairs in our experiments
over the development set.
? QA: feature 6 from section 3.1. We chose
this feature, which had obtained an ac-
curacy over 70% in previous experiments
over the development set in QA pairs, to
study the effect of named entities in QA
pairs.
? SUM: features 1, 2 and 3 from section 3.1.
We selected these features to show the im-
portance of dependency analysis in SUM
pairs as it is shown in section 6.
5 Results
Accuracy was applied as the main measure to the
participating systems.
The results obtained over the test corpus for the
two runs submitted are shown in table 4.
As we can see in both runs, different accuracy val-
ues are obtained depending on the task. The best re-
sult is obtained in pairs from QA with a 72% accu-
racy in the two runs, although two different systems
are applied. This result pushes us to use this system
for Answer Validation (Pen?as et al, 2007). Results
in run 2, which uses a different setting for each task,
are slightly better than results in run 1, but only in
IE and SUM. However, results are too close to ac-
cept a confirmation of our initial intuition that pairs
from different tasks could need not only a different
training, but also the use of different approaches for
the entailment decision.
6 Discussion
In run 2 we used NER for IE and QA, the two tasks
with the higher percentage of pairs with at least one
named entity in the hypothesis (98.5% in IE and
97% in QA).
Our previous work about the use of named enti-
ties in textual entailment (Rodrigo et al, 2007) sug-
gested that NER permitted to obtain good results.
However, after the RTE-3 experience, we found that
the use of NER does not improve results in all tasks,
but only in QA in a solid way with the previous
work.
We performed a qualitative study over the IE pairs
showing that, as it can be expected, in pairs from IE
the relations between named entities are more im-
portant that named entities themselves.
Figure 3 shows some examples where all named
entities are entailed but not the relation between
them. In pair 5 both Michael Laski and China are
entailed but the relation between them is took the
side of in the text, and was an opponent of in the
hypothesis. The same problem appears in the other
pairs with the relation left instead was born in (pair
8) or passive voice instead active voice (pair 7).
Comparing run 1 and run 2, dependency analysis
shows its usefulness in SUM pairs, where texts and
hypotheses have a higher syntactic parallelism than
in pairs from other tasks. This statement is shown
93
Table 5: Percentage of hypothesis nodes in matching
branches.
Percentage
SUM 75,505%
IE 7,353%
IR 6,422%
QA 8,496%
in table 5 where the percentage of hypothesis nodes
pertaining to matching branches in the dependency
tree is much higher in SUM pairs than in the rest of
tasks.
This syntactic parallelism seems to be the respon-
sible for the 2% increasing between the first and the
second run in SUM pairs.
7 Conclusions and future work
The experiments have been focused on the study of
the importance of considering entailment between
named entities in the recognition of textual entail-
ment, and the use of a separate training for each task.
As we have seen, both approaches increase slightly
the accuracy of the proposed systems. As we have
also shown, different approaches for each task could
also increase the system performance.
Future work is focused on improving the perfor-
mance in IE pairs taking into account relations be-
tween named entities.
Acknowledgments
This work has been partially supported by the Span-
ish Ministry of Science and Technology within the
project R2D2-SyEMBRA (TIC-2003-07158-C04-
02), the Regional Government of Madrid under the
Research Network MAVIR (S-0505/TIC-0267), the
Education Council of the Regional Government of
Madrid and the European Social Fund.
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers.. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC04). Lisbon, Portugal, 2004.
I. Dagan, O. Glickman and B. Magnini. 2006. The
PASCAL Recognising Textual Entailment Challenge.
In Quin?onero-Candela et al, editors, MLCW 2005,
LNAI Volume 3944, Jan 2006, Pages 177 - 190.
J. Herrera, A. Pen?as and F. Verdejo. 2006a. Textual En-
tailment Recognition Based on Dependency Analysis
and WordNet. In Quin?onero-Candela et al, editors,
MLCW 2005, LNAI Volume 3944, Jan 2006, Pages
231-239.
J. Herrera, A. Pen?as, A?. Rodrigo and F. Verdejo. 2006b.
UNED at PASCAL RTE-2 Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
V. I. Levensthein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. In Soviet
Physics - Doklady, volume 10, pages 707710, 1966.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Systems,
Granada, Spain, May, 1998.
A. Pen?as, A?. Rodrigo, V. Sama and F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2006. In
Lecture Notes in Computer Science. In press.
A?. Rodrigo, A. Pen?as, J. Herrera and F. Verdejo. 2007.
The Effect of Entity Recognition on Answer Validation.
In Lecture Notes in Computer Science. In press.
94
Coling 2010: Poster Volume, pages 979?987,
Beijing, August 2010
Filling Knowledge Gaps in Text for Machine Reading 
Anselmo Pe?as Eduard Hovy
UNED NLP & IR Group USC Information Sciences Institute 
anselmo@lsi.uned.es hovy@isi.edu 
Abstract
Texts are replete with gaps, information 
omitted since authors assume a certain 
amount of background knowledge. We de-
fine the process of enrichment that fills 
these gaps.  We describe how enrichment 
can be performed using a Background 
Knowledge Base built from a large corpus.  
We evaluate the effectiveness of various 
openly available background knowledge 
bases and we identify the kind of informa-
tion necessary for enrichment.   
1 Introduction: Knowledge Gaps 
Automated understanding of connected text re-
mains an unsolved challenge in NLP.  In contrast 
to systems that harvest information from large 
collections of text, or that extract only certain 
pre-specified kinds of information from single 
texts, the task of extracting and integrating all 
information from a single text, and building a 
coherent and relatively complete representation 
of its content, is still beyond current capabilities.
A significant obstacle is the fact that text al-
ways omits information that is important, but that 
people recover effortlessly. Authors leave out 
information that they assume is known to their 
readers, since its inclusion (under the Gricean 
maxim of minimality) would carry an additional, 
often pragmatic, import. The problem is that sys-
tems cannot perform the recovery since they lack 
the requisite background knowledge and inferen-
tial machinery to use it.   
In this research we address the problem of au-
tomatically recovering such omitted information 
to ?plug the gaps? in text.  To do so, we describe 
the background knowledge required as well as a 
procedure of enrichment, which recognizes 
where gaps exist and fills them out using appro-
priate background knowledge as needed. We de-
fine enrichment as:
Def: Enrichment is the process of adding ex-
plicitly to a text?s representation the information 
that is either implicit or missing in the text.   
Central to enrichment is the source of the new 
knowledge. The use of Proposition Stores as 
Background Knowledge Bases (BKB) have been 
argued to be useful for improving parsing, co-
reference resolution, and word sense disambigua-
tion (Clark and Harrison 2009). We argue here 
that Proposition Stores are also useful for 
Enrichment and show how in Section 4. Howev-
er, we show in Section 5 that current BKB re-
sources such as TextRunner (Banko et al 2007) 
and DART (Clark and Harrison 2009) are not 
ideal for enrichment purposes. In some cases 
there is a lack in normalization. But the most im-
portant shortcoming is the lack in answering 
about instances, their possible classes, how they 
relate to propositions, and how different proposi-
tions are related through them. We propose easy 
to achieve extensions in this direction. We test 
this hypothesis building our own Proposition 
Store with the proposed extensions, and compare 
it with them for enrichment in the US football 
domain. 
To perform enrichment, we begin with an ini-
tial simple text representation and a Proposition 
Stores as a background knowledge base. We ex-
ecute a simple formalized procedure to select and 
attach appropriate elements from the BKB to the 
entities and implicit relations present in the initial 
text representation. Surprisingly, we find that 
some quite simple processing can be effective if 
we are able to contextualize the text under inter-
pretation.   
We describe in Section 2 our textual represen-
tations and in Section 3 the process of building 
the Proposition Store. Enrichment is described in 
Section 4, and an evaluation and comparison is 
performed in Section 5.   
2 Text Representation 
The initial, shallow, text representation must cap-
ture the first impression of what is going on in 
the text, possibly (unfortunately) losing some 
fragments. After the first impression, in accord 
with the purpose of the reading, we ?contextual-
979
ize? each sentence, expanding its initial represen-
tation with the relevant related background 
knowledge in our base. 
During this process of making explicit the im-
plicit semantic relations it will become apparent 
whether we need to recover some of the missed 
elements, whether we need to expand some oth-
ers, etc. So the process is identified with the 
growing of the context until deeper interpretation 
is possible. This approach resembles some well-
known theories such as the Theory of Relevance 
(Sperber and Wilson, 1995). The particular me-
thod we envisage is related to Interpretation as 
Abduction (Hobbs et al 1993). 
How can the initial information be represented 
so as to enable the context to grow into an inter-
pretation? We hypothesize that: 
1. Behind certain syntactic dependencies there 
are semantic relations. 
2. In the case of dependencies between nouns, 
this semantic relation can be made more ex-
plicit using verbs and/or prepositions. The 
knowledge base (our Proposition Store) must 
help us find them. 
We look for a semantic representation close 
enough to the syntactic representation we can 
obtain from the dependency graph. The main 
syntactic dependencies we want to represent in 
order to enable enrichment are: 
1. Dependencies between nouns such as noun-
noun compounds (nn) or possessive (poss). 
2. Dependencies between nouns and verbs, 
such as subject and object relations. 
3. Prepositions having two nouns as arguments. 
Then the preposition becomes the label for 
the relation, being the object of the preposi-
tion the target of the relation. 
We collapse the syntactic dependencies be-
tween verb, subject, and object into a single se-
mantic relation. Since we are assuming that the 
verb is the more explicit expression of a semantic 
relation, we fix this in the initial representation. 
The subject will be the source of the relation and 
the object will be the target of the relation. When 
the verb has more arguments we consider its ex-
pansion as a new node as referred in Section 4.4. 
Figure 1 shows the initial minimal representa-
tion for the sentence we will use for our discus-
sion: ?San Francisco's Eric Davis intercepted a 
Steve Walsh pass on the next series to set up a 
seven-yard Young touchdown pass to Brent 
Jones?. Notice that some pieces of the text are 
missing in the initial representation of the text, as 
for example ?on the next series? or ?seven-
yard?.
3 Background Knowledge Base  
We will use a Proposition Stores as a Back-
ground Knowledge Base (BKB). We built it from 
a collection of 30,826 New York Times articles 
about US football, similar to the kind of texts we 
want to interpret.  We parsed the collection using 
a standard dependency parser (Marneffe and 
Manning, 2008; Klein and Maning, 2003) and, 
after collapsing some syntactic dependencies, 
obtained 3,022,305 raw elements in the BKB.  
3.1 Types of elements in the BKB 
We distinguish three kinds of elements in our 
Background Knowledge Base: Entities, Proposi-
tions, and Lexical relations. All three have asso-
ciated their frequency in the reference collection. 
Entities: We distinguish between entity classes 
and entity instances: 
1. Entity classes: Entity classes are denoted by 
nouns. We don?t restrict classes to any par-
ticular predefined set. In addition, we intro-
duce two special classes: Person and Group. 
These two classes are related to the use of 
pronouns in text. Pronouns ?I?, ?he? and 
?she? are linked to class Person. Pronouns 
?we? and ?they? are linked to class Group. 
For example, the occurrence of the pronoun 
?he? in ?He threw a pass? would produce an 
additional count of the proposition ?per-
son:throw:pass?. 
set up to 
Young 
Brent
Jones touchdown 
pass2 
Figure 1. Initial text representation.
nn 
Steve
Walsh 
Eric 
Davis
pass1 
intercept
nn nn 
San Francisco 
poss 
980
2. Entity Instances: Entity instances are indi-
cated by proper nouns. Proper nouns are 
identified by the part of speech tagging. 
Some of these instances will participate in 
the ?has-instance? relation (see below).   
When they participate in a proposition they 
produce proposition instances. 
Propositions: Following Clark and Harrison 
(2009) we call propositions the tuples of words 
that have some determined pattern of syntactic 
relations among them. We focus on NVN, 
NVNPN and NPN proposition types. For exam-
ple, a NVNPN proposition is a full instantiation 
of: Subject:Verb:Object:Prep:Complement.
The first three elements are the subject, the 
verb and the direct object. Fourth is the preposi-
tion that attaches the PP complement to the verb. 
For simplicity, indirect objects are considered as 
a Complement with the preposition ?to?. 
The following are the most frequent NVN 
propositions in the BKB ordered by frequency. 
NVN 2322 'NNP':'beat':'NNP' 
NVN 2231 'NNP':'catch':'pass' 
NVN 2093 'NNP':'throw':'pass' 
NVN 1799 'NNP':'score':'touchdown' 
NVN 1792 'NNP':'lead':'NNP' 
NVN 1571 'NNP':'play':'NNP' 
NVN 1534 'NNP':'win':'game' 
NVN 1355 'NNP':'coach':'NNP' 
NVN 1330 'NNP':'replace':'NNP' 
NVN 1322 'NNP':'kick':'goal' 
The ?NNP? tag replaces specific proper nouns 
(instances) found in the proposition.  
When a sentence has more than one comple-
ment, a new occurrence is counted for each com-
plement. For example, given the sentence 
?Steve_Walsh threw a pass to Brent_Jones in the 
first quarter?, we would add a count to each of 
the following propositions: 
Steve_Walsh:throw:pass 
Steve_Walsh:throw:pass:to:Brent_Jones 
Steve_Walsh:throw:pass:in:quarter 
Notice that we include only the heads of the 
noun phrases in the propositions. 
We call proposition classes the propositions 
that only involve instance classes (e.g., ?per-
son:throw:pass?), and proposition instances
those that involve at least one entity instance 
(e.g., ?Steve_Walsh:throw:pass?).
Proposition instances are useful for the track-
ing of a entity instance. For example, 
?'Steve_Walsh':'supplant':'John_Fourcade':'as':'
quarterback'?. When a proposition instance is 
found, it is stored also as a proposition class re-
placing the proper nouns by a special word 
(NNP) to indicate the presence of an entity in-
stance. The enrichment of the text is based on the 
use of most frequent proposition classes. 
Lexical Relations: We make use of very general 
patterns considering appositions and copula 
verbs (detected by the Stanford parser) in order 
to extract ?is?, and ?has-instance? relations: 
1. Is: between two entity classes. They denote a 
kind of identity between both entity classes, 
but not in any specific hierarchical relation 
such as hyponymy. Neither is a relation of 
synonymy. As a result, it is somehow a kind 
of underspecified relation that groups those 
more specific. For example, if we ask the 
BKB what a ?receiver? is, the most frequent 
relations are: 
290 'person':is:'receiver' 
29 'player':is:'receiver' 
16 'pick':is:'receiver' 
15 'one':is:'receiver' 
14 'receiver':is:'target' 
8 'end':is:'receiver' 
7 'back':is:'receiver' 
6 'position':is:'receiver' 
The number indicates the frequency of the 
relation in the collection. 
2. Has-instance: between an entity class and an 
entity instance. For example, if we ask for 
instances of team, the top instances with 
more support in the collection are: 
192 'team':has-instance:'Jets' 
189 'team':has-instance:'Giants' 
43 'team':has-instance:'Eagles' 
40 'team':has-instance:'Bills' 
36 'team':has-instance:'Colts' 
35 'team':has-instance:'Miami' 
But we can ask also for the possible classes of 
an instance. For example, all the entity classes 
for ?Eric_Davis? are: 
12 'cornerback':has-instance:'Eric_Davis' 
1 'hand':has-instance:'Eric_Davis' 
1 'back':has-instance:'Eric_Davis'  
We still work on other lexical relations such 
as ?part-of? and ?is-value-of?. For example, the 
most frequent ?is-value-of? relations are: 
5178 '[0-9]-[0-9]':is-value-of:'lead' 
3996 '[0-9]-[0-9]':is-value-of:'record' 
2824 '[0-9]-[0-9]':is-value-of:'loss' 
1225 '[0-9]-[0-9]':is-value-of:'season' 
981
4 Enrichment operations 
The goal of the following enrichment operations 
is to make explicit what kind of semantic rela-
tions and entity classes are involved in the text. 
4.1 Fusion of nodes 
Sometimes, the syntactic dependency ties two or 
more words that form a single concept. This is 
the case with multiword terms such as ?tight
end?, ?field goal?, ?running back?, etc. In these 
cases, the meaning of the compound is beyond 
the syntactic dependency. Thus, we shouldn?t 
look for its explicit meaning. Instead, we fuse the 
nodes into a single one. 
The question is whether the fusion of the 
words into a single expression allows or not the 
consideration of possible paraphrases. For exam-
ple, in the case of ?field:nn:goal?, we don?t find 
other ways to express the concept in the BKB. 
However, in the case of ?touchdown:nn:pass? we 
can find, for example, ?pass:for:touchdown? a 
significant amount of times, and we want to iden-
tify them as equivalent expressions. 
4.2 Building context for instances 
Suppose we wish to determine what kind of enti-
ty ?Steve Walsh? is in the context of the syntactic 
dependency ?Steve_Walsh:nn:pass?. First, we 
look into the BKB for the possible entity classes 
of Steve_Walsh previously found in the collec-
tion. In this particular case, the most frequent 
class is ?quarterback?: 
40 'quarterback':has-instance:'Steve_Walsh' 
2 'junior':has-instance:'Steve_Walsh' 
But what happens if we see ?Steve_Walsh? for 
the first time? Then we need to take into account 
the classes shared by other instances in the same 
syntactic context. The most frequent are ?Mari-
no?, ?Kelly?, ?Elway?, etc. From them we are 
able to infer the most plausible class for the new 
entity. In our example, quarterback:
20 'quarterback':has-instance:'Marino' 
6 'passer':has-instance:'Marino' 
?
17 'quarterback':has-instance:'Kelly' 
6 'passer':has-instance:'Kelly' 
?
16 'quarterback':has-instance:'Elway' 
9 'player':has-instance:'Elway' 
4.3 Building context for dependencies 
Now we want to determine the meaning behind 
such syntactic dependencies as: 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,
?Young:nn:pass? or ?pass:to:Brent_Jones?. 
We have two ways for adding more meaning 
to these syntactic dependencies: find the most 
appropriate prepositions to describe them, and 
find the most appropriate verbs. Whether one, the 
other, or both is useful has to be determined dur-
ing the reasoning system development. 
Finding the prepositions 
Several types of propositions in the BKB involve 
prepositions. The most relevant are NPN and 
NVNPN. In the case of ?touchdown:nn:pass?,
?for? is clearly the best interpretation: 
NPN 712 'pass':'for':'touchdown' 
NPN 24 'pass':'include':'touchdown' 
In the case of ?Steve_Walsh:nn:pass? and 
?Young:nn:pass?, since we know they are quar-
terbacks, we can ask for all the prepositions be-
tween ?pass? and ?quarterback?:
NPN 23 'pass':'from':'quarterback' 
NPN 14 'pass':'by':'quarterback' 
If we don?t have any evidence on the instance 
class, and we know only that they are instances, 
the pertinent query to the BKB obtains: 
NPN 1305 'pass':'to':'NNP' 
NPN 1085 'pass':'from':'NNP' 
NPN 147 'pass':'by':'NNP' 
In the case of ?Young:nn:pass? (in ?Young
pass to Brent Jones?), there exists already the 
preposition ?to? (?pass:to:Brent_Jones?), so the 
most promising choice becomes the second, 
?pass:from:Young?, which has one order of 
magnitude more occurrences than its successor. 
In the case of ?Steve_Walsh:nn:pass? (in ?Eric 
Davis intercepted a Steve Walsh pass?) we can 
use additional information: we know that ?Er-
ic_Davis:intercept:pass?. So, we can try to find 
the appropriate preposition using NVNPN propo-
sitions in the following way: 
?Eric_Davis:intercept:pass:P:Steve_Walsh? 
Asking the BKB about the propositions that 
involve two instances with ?intercept? and 
?pass?, we obtain: 
NVNPN 48 'NNP':'intercept':'pass':'by':'NNP' 
982
NVNPN 26 'NNP':'intercept':'pass':'at':'NNP' 
NVNPN 12 'NNP':'intercept':'pass':'from':'NNP' 
We could also query the BKB with the classes 
we have already found for ?Eric_Davis? (cor-
nerback, player, person):
NVNPN 11 'person':'intercept':'pass':'by':'NNP' 
NVNPN 4 'person':'intercept':'pass':'at':'NNP' 
NVNPN 2 'person':'intercept':'pass':'in':'NNP' 
NVNPN 2 'person':'intercept':'pass':'against':'NNP' 
NVNPN 1 'cornerback':'intercept':'pass':'by':'NNP' 
All these queries accumulate evidence over the 
preposition ?by? (?pass:by:Steve_Walsh?).
Finding the verbs 
The next exercise is to find a verb able to give 
meaning to syntactic dependencies such as 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,
?Young:nn:pass? or ?pass:to:Brent_Jones?. 
We can ask the BKB what instances (NNP) do 
with passes. The most frequent propositions are: 
NVN 2241 'NNP':'catch':'pass' 
NVN 2106 'NNP':'throw':'pass' 
NVN 844 'NNP':'complete':'pass' 
NVN 434 'NNP':'intercept':'pass' 
?
NVNPN 758 'NNP':'throw':'pass':'to':'NNP' 
NVNPN 562 'NNP':'catch':'pass':'for':'yard' 
NVNPN 338 'NNP':'complete':'pass':'to':'NNP' 
NVNPN 255 'NNP':'catch':'pass':'from':'NNP' 
Considering the evidence of ?Brent_Jones? being 
instance of ?end? (tight end), if we ask the BKB 
about the most frequent relations between ?end?
and ?pass? we find: 
NVN 28 'end':'catch':'pass' 
NVN 6 'end':'drop':'pass' 
So, in this case, the BKB suggests that the 
syntactic dependency ?pass:to:Brent_Jones?
means ?Brent Jones is an end catching a pass?.
Or in other words, that ?Brent_Jones? has a role 
of ?catch-ER? with respect to ?pass?.
If we want to accumulate more evidence on 
this we can consider NVNPN propositions in-
cluding ?touchdown?. We only find evidence for 
the most general classes (NNP and person):
NVNPN 189 NNP:'catch':'pass':'for':'touchdown' 
NVNPN 26 NNP:'complete':'pass':'for':'touchdown' 
NVNPN 84 person:catch:pass:for:touchdown 
NVNPN 18 person:complete:pass:for:touchdown 
This means that when we have ?touchdown?, we 
don?t have counts for the second option 
?Brent_Jones:drop:pass?, while ?catch? be-
comes stronger. 
In the case of ?Steve_Walsh:nn:pass? we hy-
pothesize that ?Steve_Walsh? is a ?quarterback?. 
Asking the BKB about the most plausible rela-
tion between a quarterback and a pass we find: 
NVN 98 'quarterback':'throw':'pass' 
NVN 27 'quarterback':'complete':'pass' 
Again, if we take into account that it is a 
?touchdown:nn:pass?, then only the second op-
tion ?Steve_Walsh:complete:pass? is consistent 
with the NVNPN propositions. So, in this case, 
the BKB suggests that the syntactic dependency 
?Steve_Walsh:nn:pass? means ?Steve_Walsh is a 
quarterback completing a pass?. 
Finally, with respect to ?touchdown:nn:pass?, 
we can ask about the verbs that relate them: 
NVN 14 'pass':'set_up':'touchdown' 
NVN 6 'pass':'score':'touchdown' 
NVN 5 'pass':'produce':'touchdown' 
Figure 2 shows the resulting enrichment after 
the process described. 
4.4 Expansion of relations 
Sometimes, the sentence shows a verb with more 
than two arguments. In our example, we have 
?Eric_David:intercept:pass:on:series?. In these 
cases, relations can be expanded into new nodes. 
Following our example, the new node is the 
eventuality of ?intercept? (?intercept-ION?), 
?Eric_Davis? is the ?intercept-ER? and ?pass? is 
the ?intercept-ED?. Then, the missing informa-
tion is attached to the new node (see Figure 3).  
In addition, we can proceed with the expan-
sion of the context considering this new node. 
For example, we are working with the hypothesis 
that ?Steve_Walsh? is an instance of ?quarter-
back? and thus, its most plausible relations with 
?pass? are ?throw? and ?complete?. However, 
now we can ask about the most frequent relation 
between ?quarterback? and a nominalization of 
catch (28) 
drop (6)
throw (98)
complete (27) 
by for 
has-instance (12) has-instance (33) 
to
quarterback end
Young Brent Jones touchdown 
pass
Figure 2. Enrichment of the noun phrase: ?Young 
touchdown pass to Brent Jones?
983
?intercept?. The most frequent proposition is 
?quarterback:throw:interception?, supported 35 
times in the collection. In this way, we have in-
ferred that the nominalization for the eventuality 
of intercept is interception (in our documents). 
Two further actions are possible: reinforce the 
hypothesis of ?throw:pass? instead of ?com-
plete:pass? and add the hypothesis that 
?Steve_Walsh:throw:interception?.
Finally, notice that since ?set_up? doesn?t 
need to accommodate more arguments, we can 
maintain the collapsed edge. 
4.5 Constraining the interpretations 
Some of the inferences being performed are local 
in the sense that they involve only an entity and a 
relation. However, these local inferences must be 
coherent both with the sentence and the complete 
document. To ensure this coherence we can use 
additional information as a way to constrain dif-
ferent hypotheses. In section 4.3 we showed the 
use of NVNPN propositions to constrain NVN 
ones. Another example is the case of ?Er-
ic_Davis:intercept:pass?. We can ask the BKB 
for the entity classes that participate in such kind 
of proposition: 
NVN 75 'person':'intercept':'pass' 
NVN 14 'cornerback':'intercept':'pass' 
NVN 11 'defense':'intercept':'pass' 
NVN 8 'safety':'intercept':'pass' 
NVN 7 'group':'intercept':'pass' 
So the local inference for the kind of entity 
?Eric_Davis? is (cornerback) must be coherent 
with the fact that it intercepted a pass. In this 
case ?cornerback? and ?person? are properly 
reinforced. In some sense, we are using these 
additional constrains as selectional preferences. 
5 Evaluation
Properly evaluating the enrichment process is 
very difficult.  Ideally, one would compare the 
output of an enrichment engine?a text graph 
fully fleshed out with additional knowledge?to 
a gold-standard graph containing all relevant in-
formation explicitly, and measure Recall and 
Precision of the links added by enrichment.  But 
since we have no gold standard examples, and it 
is unclear how much knowledge should be in-
cluded manually if one were to try to build some, 
two options remain: extrinsic evaluations and 
measuring the utility of the BKB in providing 
knowledge. We are in the process of performing 
an extrinsic evaluation, by measuring how much 
QA about the text read improves using the 
enriched representation.  We report here the re-
sults of comparing the utility, for enrichment 
purposes, of two other publicly available back-
ground knowledge bases: DART (Clark and Har-
rison, 2009) and TextRunner (Banko et al 2007). 
5.1 Ability to answer about instances 
As shown in our examples, BKBs need the abili-
ty to answer about instances and their classes. 
The BKBs don?t need to be completely popu-
lated, but at least have enough instance-class at-
tachments in order to allow analogy. 
Neither DART nor TextRunner allow asking 
about possible classes for a particular instance. 
This is out of the scope of TextRunner. In 
DART, instances are replaced by one of three 
basic categories (person, place, organization). 
Although storing the original proper nouns at-
tached to the assigned class would be 
straightforward, these three general classes are 
not enough to support inference. This leads us to 
the next ability. 
5.2 Ability to discover new classes and 
relations 
While quarterbacks throw passes, ends usually 
catch or drop them. As we have shown in our 
examples, classifying them as ?person? or even 
?player? is not specific enough for enrichment. 
Using a predefined set of entity classes doesn?t 
seem a good approach for midterm goals. First, 
human abstraction is not correlated with the ap-
propriate granularity level that enable recovering 
intercept-ER 
throw (98) 
complete (27)
on 
has-instance (17) poss 
intercept
quarterback San Francisco 
Steve
Walsh 
Eric 
Davis
intercept-ION 
pass
Figure 3. Expansion of ?intercept? relation 
intercept-ED 
throw (35) 
series 
nn 
984
of relevant background knowledge. Second, an-
notation will be needed for training.   
In our Proposition Stores, we count simply 
what is explicitly said in the texts about our in-
stances. This seems correlated to an appropriate 
level of granularity. Furthermore, an instance can 
be attached to several classes that can be compat-
ible (quarterback, player, person, leader, veteran, 
etc.). Frequencies tell us the classes we have to 
consider in the first place in order to find a cohe-
rent interpretation of the text. 
5.3 Ability to constrain interpretation 
and accumulate evidence 
Enrichment must be guided by the coherence of 
the ensuing interpretation. For this reason BKBs 
must allow different types of queries over the 
same elements. The aim is to constrain as much 
as possible the relations we recover to the ones 
that give a coherent interpretation of the text. 
As shown in our example, we require the abili-
ty to ask different syntactic contexts/structures 
(NN, NVNPN, etc.), not only NVN (subject-
verb-object). Achieving this is very difficult for 
approaches that don?t use parsing.   
5.4 Ability to digest enough knowledge 
adapted to the domain 
None of the abilities discussed above are relevant 
if the BKB doesn?t contain enough knowledge 
about the domain in which we want to enrich 
documents. To evaluate, we ran three simple 
queries related to the US football domain in or-
der to assess the suitability of the BKBs for 
enrichment: What do quarterbacks do with 
passes? What do persons do with passes? Who 
intercepts passes? Table 1 shows the results ob-
tained with DART, TextRunner and our BKB. 
Although DART is a general domain BKB 
built using parsing, its approach doesn?t allow 
one to process enough information to answer the 
first question (first row in Table 1). A web scale 
resource such as TextRunner is better suited for 
this purpose. However, results show its lack of 
normalization. On the other hand, our BKB is 
able to return a clean and relevant answer. 
The second question (second row) shows the 
ability of the three BKBs to deal with a basic 
abstraction needed for inference. Since TextRun-
ner doesn?t perform any kind of processing over 
entities or pronouns, it doesn?t recover relevant 
knowledge for this question in the football do-
main.  In addition, the table shows the need for 
domain adaptation: most of the TextRunner rela-
tions, such as ?person:gets:pass? or ?per-
son:bought:pass?, refer to different domains. 
DART shows the same effect: the first two en-
tries (?person:make:pass?, ?person:take:pass?) 
belong to different domains. 
DART1 TextRunner2 BKB (Football) 
(no results) (~200) threw  
(~100) completed  
(36) to throw  
(26) has thrown  
(19) makes  
(19) has  
(18) fires  
(99) throw 
(25) complete 
(7) have 
(5) attempt 
(5) not-throw 
(4) toss 
(3) release 
(47) make          
(45) take            
(36) complete    
(30) throw         
(25) let              
(23) catch   
(1) make            
(1) expect          
(22) gets  
(17) makes  
(10) has  
(10) receives  
(7) who has  
(7) must have  
(6) acting on  
(6) to catch  
(6) who buys  
(5) bought  
(5) admits  
(5) gives  
(824) catch 
(546) throw 
(256) complete 
(136) have 
(59) intercept 
(56) drop 
(39) not-catch 
(37) not-throw 
(36) snare 
(27) toss 
(23) pick off 
(20) run 
(13) person      
(6) person/ 
place/ organi-
zation 
(2) full-back 
(1) place     
(30) Early  
(26) Two plays  
(24) fumble  
(20) game  
(20) ball  
(17) Defensively  
(75) person 
(14) cornerback 
(11) defense 
(8) safety 
(7) group 
(5) linebacker 
Table 1. Comparison of DART, TextRunner and our 
BKB for the following queries (rows): (1) quarter-
back:X:pass, (2) person:X:pass, (3) X:intercept:pass.
Frequencies are in parentheses. 
Finally, the third question is aimed at recover-
ing possible agents (those that intercept passes in 
our case). Again, as shown in DART, the re-
duced set of classes given by the entity recogniz-
er is not enough for the football domain. But 
having no classes (TextRunner) is even worse, 
showing its orientation to discovering relations 
rather than to generalizing and answering about 
their possible arguments. Our approach is able to 
discover plausible agent-classes for the query. 
Other queries related to the football domain 
show the same behavior. 
                                                          
1 Available at http://userweb.cs.utexas.edu/users/pclark/dart/ 
2 After aggregating partial results for each cluster using 
http://www.cs.washington.edu/research/textrunner/
985
6 Related Work 
Our approach lies between macro-reading and 
Open Information Extraction (OIE). Macro-
reading (Mitchell et al 2009) is a different task 
from ours; it seeks to populate ontologies.  Here 
concepts and relations are predefined by the on-
tology.
OIE (Banko et al 2007) does not use a prede-
fined set of semantic classes and relations and is 
aimed at web scale. For this reason the frame-
work does not include a complete NLP pipeline. 
The resulting lack of term normalization and ab-
sence of domain adaptation (e.g., the query per-
son:X:pass return throw but also buy) makes the 
results less relevant to single-document reading.  
When, as with DART, the complete NLP pipe-
line is applied over a general corpus, the amount 
of information to be processed has to be limited 
due to computational cost. Ultimately, too little 
knowledge remains for working in a specific 
domain. For example, asking DART about 
?quarterback:X:pass? produces no results. 
Our approach takes advantage of both worlds, 
ensuring that enough amounts of documents re-
lated to the domain will be processed with a 
complete NLP pipeline. Doing so provides 
cleaner and canonical representations (our propo-
sitions) and even higher counts than TextRunner 
for our domain. This level of processing will be 
scalable in the midterm; various people including 
(Huang and Sagae, 2010) are working in linear 
time parsers with state-of-the-art performance. 
Another intermediate point between a collec-
tion of domain documents and the general web, 
reached by restricting processing to the results of 
a web query, is explored in IE-on-demand (Se-
kine 2006; Shinyama and Sekine 2006). Howev-
er, they use a predefined set of entity classes, 
preventing from discovering the appropriate gra-
nularity level that enables retrieval of relevant 
background knowledge. We do not predefine the 
concepts/classes and relations, but discover them 
from what it is explicitly said in the collection. 
The process of building the BKB described 
here is closely related to DART (Clark and Har-
rison, 2009) which in turn is related to KNEXT 
(Van Durme and Schubert, 2008). Perhaps the 
most important extension we performed is the 
inclusion of lexical relations (like ?has-
instance?) that activate more powerful uses of 
the Proposition Stores. 
7 Conclusions and Future Work 
In building a BKB, limiting oneself to a specific 
domain provides some powerful benefits. Ambi-
guity is reduced inside the domain, making 
counts in propositions more accurate. Also, fre-
quency distributions of propositions differ from 
one domain to another. For example, the list of 
the most frequent NVN propositions in our BKB 
(see Section 3.1) is, by itself, an indication of the 
most salient and important events specifically in 
the US football domain. Furthermore, the amount 
of text required to build the BKB is reduced sig-
nificantly allowing processing such as parsing. 
The task of inferring omitted but necessary in-
formation is a significant part of automated text 
interpretation. In this paper we show that even 
simple kinds of information, gleaned relatively 
straightforwardly from a parsed corpus, can be 
quite useful.  Though they are still lexical and 
not even starting to be semantic, propositions 
consisting of verbs as relations between nouns 
seem to provide a surprising amount of utility.  It 
remains a research problem to determine what 
kinds and levels of knowledge are most useful in 
the long run.   
In the paper, we discuss only the propositions 
that are grounded in instantial statements about 
players and events.  But for true learning by 
reading, a system has to be able to recognize 
when the input expresses general rules, and to 
formulate such input as axioms or inferences.  In 
addition is the significant challenge of generaliz-
ing certain kinds of instantial propositions to 
produce inferences.  At which point, for exam-
ple, should the system decide that ?all football 
players have teams?, and how should it do so? 
This remains a topic for future work.   
A further topic of investigation is the time at 
which expansion should occur. Doing so at ques-
tion time, in the manner of traditional task-
oriented back-chaining inference, is the obvious 
choice, but some limited amount of forward 
chaining at reading time seems appropriate too, 
especially if it can significantly assist with text 
processing tasks, in the manner of expectation-
driven understanding.    
Finally, as discussed above, the evaluation of 
intrinsic evaluation procedures remains to be de-
veloped.   
986
Acknowledgments 
We are grateful to Hans Chalupsky and David 
Farwell for their comments and input for this 
work. We acknowledge the builders of TextRun-
ner and DART for their willingness to make their 
resources openly available.   
This work has been partially supported by the 
Spanish Government through the "Programa Na-
cional de Movilidad de Recursos Humanos del 
Plan Nacional de I+D+i 2008-2011? (Grant 
PR2009-0020). Research supported in part by 
Air Force Contract FA8750-09-C-0172 under the 
DARPA Machine Reading Program. 
References  
Banko, M., Cafarella, M., Soderland, S., Broadhead, 
M., Etzioni, O. 2007. Open Information Extrac-
tion from the Web. IJCAI 2007. 
Barker, K. 2007. Building Models by Reading Texts. 
Invited talk at the AAAI 2007 Spring Symposium 
on Machine Reading, Stanford University. 
Clark, P. and Harrison, P. 2009. Large-scale extrac-
tion and use of knowledge from text. The Fifth 
International Conference on Knowledge Capture 
(K-CAP 2009). 
http://www.cs.utexas.edu/users/pclark/dart/ 
Hobbs, J.R., Stickel, M., Appelt, D. and Martin, P., 
1993. Interpretation as Abduction. Artificial In-
telligence, Vol. 63, Nos. 1-2, pp. 69-142.  
http://www.isi.edu/~hobbs/interp-abduct-ai.pdf 
Huang, L. and Sagae, K. 2010. Dynamic Program-
ming for Linear-Time Shift-Reduce Parsing. 
ACL 2010. 
Klein, D. and Manning, C.D. 2003. Accurate Unlexi-
calized Parsing. Proceedings of the 41st Meeting 
of the Association for Computational Linguistics, 
pp. 423-430 
Marneffe, M. and Manning, C.D. 2008. The Stanford 
typed dependencies representation. In COLING 
2008 Workshop on Cross-framework and Cross-
domain Parser Evaluation. 
Mitchell, T. M., Betteridge, J., Carlson, A., Hruschka, 
E., and Wang, R. Populating the Semantic Web 
by Macro-reading Internet Text. The Semantic 
Web - ISWC 2009. LNCS Volume 5823. Sprin-
ger-Verlag. 
Sekine, S. 2006. On Demand Information Extraction. 
COLING 2006. 
Shinyama, Y. and Sekine, S. 2006. Preemptive Infor-
mation Extraction using Unrestricted Relation 
Discovery. HLT-NAACL 2006. 
Sperber, D. and Wilson, D. 1995. Relevance: Com-
munication and cognition (2nd ed.) Oxford, 
Blackwell.
Van Durme, B., Schubert, L. 2008. Open Knowledge 
Extraction through Compositional Language 
Processing. Symposium on Semantics in Systems 
for Text Processing, STEP 2008. 
987
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1415?1424,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Simple Measure to Assess Non-response
Anselmo Pen?as and Alvaro Rodrigo
UNED NLP & IR Group
Juan del Rosal, 16
28040 Madrid, Spain
{anselmo,alvarory@lsi.uned.es}
Abstract
There are several tasks where is preferable not
responding than responding incorrectly. This
idea is not new, but despite several previous at-
tempts there isn?t a commonly accepted mea-
sure to assess non-response. We study here an
extension of accuracy measure with this fea-
ture and a very easy to understand interpreta-
tion. The measure proposed (c@1) has a good
balance of discrimination power, stability and
sensitivity properties. We show also how this
measure is able to reward systems that main-
tain the same number of correct answers and
at the same time decrease the number of in-
correct ones, by leaving some questions unan-
swered. This measure is well suited for tasks
such as Reading Comprehension tests, where
multiple choices per question are given, but
only one is correct.
1 Introduction
There is some tendency to consider that an incorrect
result is simply the absence of a correct one. This is
particularly true in the evaluation of Information Re-
trieval systems where, in fact, the absence of results
sometimes is the worse output.
However, there are scenarios where we should
consider the possibility of not responding, because
this behavior has more value than responding incor-
rectly. For example, during the process of introduc-
ing new features in a search engine it is important
to preserve users? confidence in the system. Thus,
a system must decide whether it should give or not
a result in the new fashion or keep on with the old
kind of output. A similar example is the decision
about showing or not ads related to the query. Show-
ing wrong ads harms the business model more than
showing nothing. A third example more related to
Natural Language Processing is the Machine Read-
ing evaluation through reading comprehension tests.
In this case, where multiple choices for a question
are offered, choosing a wrong option should be pun-
ished against leaving the question unanswered.
In the latter case, the use of utility functions is
a very common option. However, utility functions
give arbitrary value to not responding and ignore
the system?s behavior showed when it responds (see
Section 2). To avoid this, we present c@1 measure
(Section 2.2), as an extension of accuracy (the pro-
portion of correctly answered questions). In Sec-
tion 3 we show that no other extension produces a
sensible measure. In Section 4 we evaluate c@1 in
terms of stability, discrimination power and sensibil-
ity, and some real examples of its behavior are given
in the context of Question Answering. Related work
is discussed in Section 5.
2 Looking for the Value of Not Responding
Lets take the scenario of Reading Comprehension
tests to argue about the development of the measure.
Our scenario assumes the following:
? There are several questions.
? Each question has several options.
? One option is correct (and only one).
The first step is to consider the possibility of not
responding. If the system responds, then the assess-
ment will be one of two: correct or wrong. But if
1415
the system doesn?t respond there is no assessment.
Since every question has a correct answer, non re-
sponse is not correct but it is not incorrect either.
This is represented in contingency Table 1, where:
? nac: number of questions for which the answer
is correct
? naw: number of questions for which the answer
is incorrect
? nu: number of questions not answered
? n: number of questions (n = nac + naw + nu)
Correct (C) Incorrect (?C)
Answered (A) nac naw
Unanswered (?A) nu
Table 1: Contingency table for our scenario
Let?s start studying a simple utility function able
to establish the preference order we want:
? -1 if question receives an incorrect response
? 0 if question is left unanswered
? 1 if question receives a correct response
Let U(i) be the utility function that returns one of
the above values for a given question i. Thus, if we
want to consider n questions in the evaluation, the
measure would be:
UF = 1
n
n
?
i=1
U(i) = nac ? naw
n
(1)
The rationale of this utility function is intuitive:
not answering adds no value and wrong answers add
negative values. Positive values of UF indicate more
correct answers than incorrect ones, while negative
values indicate the opposite. However, the utility
function is giving an arbitrary value to the prefer-
ences (-1, 0, 1).
Now we want to interpret in some way the value
that Formula (1) assigns to unanswered questions.
For this purpose, we need to transform Formula (1)
into a more meaningful measure with a parameter
for the number of unanswered questions (nu). A
monotonic transformation of (1) permit us to pre-
serve the ranking produced by the measure. Let
f(x)=0.5x+0.5 be the monotonic function to be used
for the transformation. Applying this function to
Formula (1) results in Formula (2):
0.5nac ? naw
n
+ 0.5 = 0.5
n
[nac ? naw + n] =
= 0.5
n
[nac ? naw + nac + naw + nu]
= 0.5
n
[2nac + nu] =
nac
n
+ 0.5nu
n
(2)
Measure (2) provides the same ranking of sys-
tems than measure (1). The first summand of For-
mula (2) corresponds to accuracy, while the second
is adding an arbitrary constant weight of 0.5 to the
proportion of unanswered questions. In other words,
unanswered questions are receiving the same value
as if half of them had been answered correctly.
This does not seem correct given that not answer-
ing is being rewarded in the same proportion to all
the systems, without taking into account the per-
formance they have shown with the answered ques-
tions. We need to propose a more sensible estima-
tion for the weight of unanswered questions.
2.1 A rationale for the Value of Unanswered
Questions
According to the utility function suggested, unan-
swered questions would have value as if half of them
had been answered correctly. Why half and not other
value? Even more, Why a constant value? Let?s gen-
eralize this idea and estate more clearly our hypoth-
esis:
Unanswered questions have the same value as if a
proportion of them would have been answered cor-
rectly.
We can express this idea according to contingency
Table 1 in the following way:
P (C) = P (C ?A) + P (C ? ?A) =
= P (C ?A) + P (C/?A) ? P (?A)
(3)
P (C ? A) can be estimated by nac/n, P (?A)
can be estimated by nu/n, and we have to estimate
P (C/?A). Our hypothesis is saying that P (C/?A)
1416
is different from 0. The utility measure (2) corre-
sponds to P(C) in Formula (3) where P (C/?A) re-
ceives a constant value of 0.5. It is assuming arbi-
trarily that P (C/?A) = P (C/A).
Following this, our measure must consist of two
parts: The overall accuracy and a better estimation
of correctness over the unanswered questions.
2.2 The Measure Proposed: c@1
From the answered questions we have already ob-
served the proportion of questions that received a
correct answer (P (C ?A) = nac/n). We can use this
observation as our estimation for P (C/?A) instead
of the arbitrary value of 0.5.
Thus, the measure we propose is c@1 (correct-
ness at one) and is formally represented as follows:
c@1 = nac
n
+ nac
n
nu
n
= 1
n
(nac +
nac
n
nu) (4)
The most important features of c@1 are:
1. A system that answers all the questions will re-
ceive a score equal to the traditional accuracy
measure: nu=0 and therefore c@1=nac/n.
2. Unanswered questions will add value to c@1
as if they were answered with the accuracy al-
ready shown.
3. A system that does not return any answer would
receive a score equal to 0 due to nac=0 in both
summands.
According to the reasoning above, we can inter-
pret c@1 in terms of probability as P (C) where
P (C/?A) has been estimated with P (C ? A). In
the following section we will show that there is no
other estimation for P (C/?A) able to provide a rea-
sonable evaluation measure.
3 Other Estimations for P (C/?A)
In this section we study whether other estimations
of P (C/?A) can provide a sensible measure for QA
when unanswered questions are taken into account.
They are:
1. P (C/?A) ? 0
2. P (C/?A) ? 1
3. P (C/?A) ? P (?C/?A) ? 0.5
4. P (C/?A) ? P (C/A)
5. P (C/?A) ? P (?C/A)
3.1 P (C/?A) ? 0
This estimation considers the absence of response as
incorrect response and we have the traditional accu-
racy (nac/n).
Obviously, this is against our purposes.
3.2 P (C/?A) ? 1
This estimation considers all unanswered questions
as correctly answered. This option is not reasonable
and is given for completeness: systems giving no
answer would get maximum score.
3.3 P (C/?A) ? P (?C/?A) ? 0.5
It could be argued that since we cannot have obser-
vations of correctness for unanswered questions, we
should assume equiprobability between P (C/?A)
and P (?C/?A). In this case, P(C) corresponds
to the expression (2) already discussed. As previ-
ously explained, in this case we are giving an arbi-
trary constant value to unanswered questions inde-
pendently of the system?s performance shown with
answered ones. This seems unfair. We should be
aiming at rewarding those systems not responding
instead of giving wrong answers, not reward the sole
fact that the system is not responding.
3.4 P (C/?A) ? P (C/A)
An alternative is to estimate the probability of cor-
rectness for the unanswered questions as the pre-
cision observed over the answered ones: P(C/A)=
nac/(nac+ naw). In this case, our measure would be
like the one shown in Formula (5):
P (C) = P (C ?A) + P (C/?A) ? P (?A) =
= P (C/A) ? P (A) + P (C/A) ? P (?A) =
= P (C/A) = nac
nac + naw
(5)
The resulting measure is again the observed pre-
cision over the answered ones. This is not a sensible
measure, as it would reward a cheating system that
decides to leave all questions unanswered except one
for which it is sure to have a correct answer.
1417
Furthermore, from the idea that P (C/?A) is
equal to P (C/A) the underlying assumption is that
systems choose to answer or not to answer ran-
domly, whereas we want to reward the systems that
choose not responding because they are able to de-
cide that their candidate options are wrong or be-
cause they are unable to decide which candidate is
correct.
3.5 P (C/?A) ? P (?C/A)
The last option to be considered explores the idea
that systems fail not responding in the same propor-
tion that they fail when they give an answer (i.e. pro-
portion of incorrect answers).
Estimating P (C/?A) as naw / (nac+ naw), the
measure would be:
P (C) = P (C ?A) + P (C/?A) ? P (?A) =
= P (C ?A) ? P (?C/A) ? P (?A) =
= nac
n
+ naw
nac + naw
? nu
n
(6)
This measure is very easy to cheat. It is possible
to obtain almost a perfect score just by answering in-
correctly only one question and leaving unanswered
the rest of the questions.
4 Evaluation of c@1
When a new measure is proposed, it is important
to study the reliability of the results obtained us-
ing that measure. For this purpose, we have cho-
sen the method described by Buckley and Voorhees
(2000) for assessing the stability and discrimination
power, as well as the method described by Voorhees
and Buckley (2002) for examining the sensitivity of
our measure. These methods have been used for
studying IR metrics (showing similar results with
the methods based on statistics (Sakai, 2006)), as
well as for evaluating the reliability of other QA
measures different to the ones studied here (Sakai,
2007a; Voorhees, 2002; Voorhees, 2003).
We have compared the results over c@1 with the
ones obtained using both accuracy and the utility
function (UF) defined in Formula (1). This compari-
son is useful to show how confident can a researcher
be with the results obtained using each evaluation
measure.
In the following subsections we will first show the
data used for our study. Then, the experiments about
stability and sensitivity will be described.
4.1 Data sets
We used the test collections and runs from the Ques-
tion Answering track at the Cross Language Evalu-
ation Forum 2009 (CLEF) (Pen?as et al, 2010). The
collection has a set of 500 questions with their an-
swers. The 44 runs in different languages contain
the human assessments for the answers given by ac-
tual participants. Systems could chose not to answer
a question. In this case, they had the chance to sub-
mit their best candidate in order to assess the perfor-
mance of their validation module (the one that de-
cides whether to give or not the answer).
This data collection allows us to compare c@1
and accuracy over the same runs.
4.2 Stability vs. Discrimination Power
The more stable a measure is, the lower the probabil-
ity of errors associated with the conclusion ?system
A is better than system B? is. Measures with a high
error must be used more carefully performing more
experiments than in the case of using a measure with
lower error.
In order to study the stability of c@1 and to com-
pare it with accuracy we used the method described
by Buckley and Voorhees (2000). This method al-
lows also to study the number of times systems are
deemed to be equivalent with respect to a certain
measure, which reflects the discrimination power of
that measure. The less discriminative the measure
is, the more ties between systems there will be. This
means that longer difference in scores will be needed
for concluding which system is better (Buckley and
Voorhees, 2000).
The method works as follows: let S denote a set
of runs. Let x and y denote a pair of runs from S.
Let Q denote the entire evaluation collection. Let f
represents the fuzziness value, which is the percent
difference between scores such that if the difference
is smaller than f then the two scores are deemed to
be equivalent. We apply the algorithm of Figure 1
to obtain the information needed for computing the
error rate (Formula (7)). Stability is inverse to this
value, the lower the error rate is, the more stable
the measure is. The same algorithm gives us the
1418
proportion of ties (Formula (8)), which we use for
measuring discrimination power, that is the lower
the proportion of ties is, the more discriminative the
measure is.
for each pair of runs x,y ? S
for each trial from 1 to 100
Qi = select at random subcol of size c from Q;
margin = f * max (M(x,Qi),M(y,Qi));
if(|M(x,Qi) - M(y,Qi)| < |margin|)
EQM (x,y)++;
else if(|M(x,Qi) > M(y,Qi)|)
GTM (x,y)++;
else
GTM (y,x)++;
Figure 1: Algorithm for computing EQM (x,y),
GTM (x,y) and GTM (y,x) in the stability method
We assume that for each measure the correct de-
cision about whether run x is better than run y hap-
pens when there are more cases where the value of
x is better than the value of y. Then, the number of
times y is better than x is considered as the number
of times the test is misleading, while the number of
times the values of x and y are equivalent is consid-
ered the number of ties.
On the other hand, it is clear that larger fuzziness
values decrease the error rate but also decrease the
discrimination power of a measure. Since a fixed
fuzziness value might imply different trade-offs for
different metrics, we decided to vary the fuzziness
value from 0.01 to 0.10 (following the work by Sakai
(2007b)) and to draw for each measure a proportion-
of-ties / error-rate curve. Figure 2 shows these
curves for the c@1, accuracy and UF measures. In
the Figure we can see how there is a consistent de-
crease of the error rate of all measures when the
proportion of ties increases (this corresponds to the
increase in the fuzziness value). Figure 2 shows
that the curves of accuracy and c@1 are quite simi-
lar (slightly better behavior of c@1) , which means
that they have a similar stability and discrimination
power.
The results suggest that the three measures are
quite stable, having c@1 and accuracy a lower er-
ror rate than UF when the proportion of ties grows.
These curves are similar to the ones obtained for
Figure 2: Error-rate / Proportion of ties curves for accu-
racy, c@1 and UF with c = 250
other QA evaluation measures (Sakai, 2007a).
4.3 Sensitivity
The swap-rate (Voorhees and Buckley, 2002) repre-
sents the chance of obtaining a discrepancy between
two question sets (of the same size) as to whether
a system is better than another given a certain dif-
ference bin. Looking at the swap-rates of all the
difference performance bins, the performance dif-
ference required in order to conclude that a run is
better than another for a given confidence value can
be estimated. For example, if we want to know the
required difference for concluding that system A is
better than system B with a confidence of 95%, then
we select the difference that represents the first bin
where the swap-rate is lower or equal than 0.05.
The sensitivity of the measure is the number of
times among all the comparisons in the experi-
ment where this performance difference is obtained
(Sakai, 2007b). That is, the more comparisons ac-
complish the estimated performance difference, the
more sensitive is the measure. The more sensitive
the measure, the more useful it is for system dis-
crimination.
The swap method works as follows: let S denote
a set of runs, let x and y denote a pair of runs from S.
Let Q denote the entire evaluation collection. And
let d denote a performance difference between two
runs. Then, we first define 21 performance differ-
ence bins: the first bin represents performance dif-
ferences between systems such that 0 ? d < 0.01;
the second bin represents differences such that 0.01
? d < 0.02; and the limits for the remaining bins in-
crease by increments of 0.01, with the last bin con-
taining all the differences equal or higher than 0.2.
1419
Error rateM =
?
x,y?S min(GTM (x, y), GTM (y, x))
?
x,y?S(GTM (x, y) + GTM (y, x) + EQM (x, y))
(7)
Prop T iesM =
?
x,y?S EQM (x, y)
?
x,y?S(GTM (x, y) + GTM (y, x) + EQM (x, y))
(8)
Let BIN(d) denote a mapping from a difference d to
one of the 21 bins where it belongs. Thus, algorithm
in Figure 3 is applied for calculating the swap-rate
of each bin.
for each pair of runs x,y ? S
for each trial from 1 to 100
select Qi , Q
?
i ? Q, where
Qi ? Q
?
i == ? and |Qi| == |Q
?
i| == c;
dM (Qi) = M(x,Qi)?M(y,Qi);
dM (Q
?
i) = M(x,Q
?
i)?M(y,Q
?
i);
counter(BIN(|dM (Qi)|))++;
if(dM (Qi) * dM (Q
?
i) < 0)
swap counter(BIN(|dM (Qi)|))++;
for each bin b
swap rate(b) = swap counter(b)/counter(b);
Figure 3: Algorithm for computing swap-rates
(i) (ii) (iii) (iv)
UF 0.17 0.48 35.12% 59.30%
c@1 0.09 0.77 11.69% 58.40%
accuracy 0.09 0.68 13.24% 55.00%
Table 2: Results obtained applying the swap method to
accuracy, c@1 and UF at 95% of confidence, with c =
250: (i) Absolute difference required; (ii) Highest value
obtained; (iii) Relative difference required ((i)/(ii)); (iv)
percentage of comparisons that accomplish the required
difference (sensitivity)
Given that Qi and Q
?
i must be disjoint, their size
can only be up to half of the size of the original col-
lection. Thus, we use the value c=250 for our exper-
iment1. Table 2 shows the results obtained by apply-
ing the swap method to accuracy, c@1 and UF, with
c = 250, swap-rate ? 5, and sensitivity given a con-
fidence of 95% (Column (iv)). The range of values
1We use the same size for experiments in Section 4.2 for
homogeneity reasons.
are similar to the ones obtained for other measures
according to (Sakai, 2007a).
According to Column (i), a higher absolute dif-
ference is required for concluding that a system is
better than another using UF. However, the relative
difference is similar to the one required by c@1.
Thus, similar percentage of comparisons using c@1
and UF accomplish the required difference (Column
(iv)). These results show that their sensitivity values
are similar, and higher than the value for accuracy.
4.4 Qualitative evaluation
In addition to the theoretical study, we undertook a
study to interpret the results obtained by real sys-
tems in a real scenario. The aim is to compare the
results of the proposed c@1 measure with accuracy
in order to compare their behavior. For this purpose
we inspected the real systems runs in the data set.
System c@1 accuracy (i) (ii) (iii)
icia091ro 0.58 0.47 237 156 107
uaic092ro 0.47 0.47 236 264 0
loga092de 0.44 0.37 187 230 83
base092de 0.38 0.38 189 311 0
Table 3: Example of system results in QA@CLEF 2009.
(i) number of questions correctly answered; (ii) number
of questions incorrectly answered; (iii) number of unan-
swered questions.
Table 3 shows a couple of examples where two
systems have answered correctly a similar num-
ber of questions. For example, this is the case of
icia091ro and uaic092ro that, therefore, obtain al-
most the same accuracy value. However, icia091ro
has returned less incorrect answers by not respond-
ing some questions. This is the kind of behavior we
want to measure and reward. Table 3 shows how
accuracy is sensitive only to the number of correct
answers whereas c@1 is able to distinguish when
1420
systems keep the number of correct answers but re-
duce the number of incorrect ones by not respond-
ing to some. The same reasoning is applicable to
loga092de compared to base092de for German.
5 Related Work
The decision of leaving a query without response is
related to the system ability to measure accurately its
self-confidence about the correctness of their candi-
date answers. Although there have been one attempt
to make the self-confidence score explicit and use
it (Herrera et al, 2005), rankings are, usually, the
implicit way to evaluate this self-confidence. Mean
Reciprocal Rank (MRR) has traditionally been used
to evaluate Question Answering systems when sev-
eral answers per question were allowed and given
in order (Fukumoto et al, 2002; Voorhees and Tice,
1999). However, as it occurs with Accuracy (propor-
tion of questions correctly answered), the risk of giv-
ing a wrong answer is always preferred better than
not responding.
The QA track at TREC 2001 was the first eval-
uation campaign in which systems were allowed
to leave a question unanswered (Voorhees, 2001).
The main evaluation measure was MRR, but perfor-
mance was also measured by means of the percent-
age of answered questions and the portion of them
that were correctly answered. However, no combi-
nation of these two values into a unique measure was
proposed.
TREC 2002 discarded the idea of including unan-
swered questions in the evaluation. Only one answer
by question was allowed and all answers had to be
ranked according to the system?s self-confidence in
the correctness of the answer. Systems were evalu-
ated by means ofConfidence Weighted Score (CWS),
rewarding those systems able to provide more cor-
rect answers at the top of the ranking (Voorhees,
2002). The formulation of CWS is the following:
CWS = 1
n
n
?
i=1
C(i)
i
(9)
Where n is the number of questions, and C(i) is
the number of correct answers up to the position i in
the ranking. Formally:
C(i) =
i
?
j=1
I(j) (10)
where I(j) is a function that returns 1 if answer j
is correct and 0 if it is not. The formulation of CWS
is inspired by the Average Precision (AP) over the
ranking for one question:
AP = 1
R
?
r
I(r)C(r)
r
(11)
where R is the number of known relevant results
for a topic, and r is a position in the ranking. Since
only one answer per question is requested, R equals
to n (the number of questions) in CWS. However,
in AP formula the summands belong to the posi-
tions of the ranking where there is a relevant result
(product of I(r)), whereas in CWS every position of
the ranking add value to the measure regardless of
whether there is a relevant result or not in that po-
sition. Therefore, CWS gives much more value to
some questions over others: questions whose an-
swers are at the top of the ranking are giving almost
the complete value to CWS, whereas those questions
whose answers are at the bottom of the ranking are
almost not counting in the evaluation.
Although CWS was aimed at promoting the de-
velopment of better self-confidence scores, it was
discussed as a measure for evaluating QA systems
performance. CWS was discarded in the following
campaigns of TREC in favor of accuracy (Voorhees,
2003). Subsequently, accuracy was adopted by the
QA track at the Cross-Language Evaluation Forum
from the beginning (Magnini et al, 2005).
There was an attempt to consider explicitly sys-
tems confidence self-score (Herrera et al, 2005): the
use of the Pearson?s correlation coefficient and the
proposal of measures K and K1 (see Formula 12).
These measures are based in a utility function that
returns -1 if the answer is incorrect and 1 if it is
correct. This positive or negative value is weighted
with the normalized confidence self-score given by
the system to each answer. K is a variation of K1
for being used in evaluations where more than an
answer per question is allowed.
If the self-score is 0, then the answer is ignored
and thus, this measure is permitting to leave a ques-
tion unanswered. A system that always returns a
1421
K1 =
?
i?{correctanswers}
self score(i)?
?
i?{incorrectanswers}
self score(i)
n
? [?1, 1] (12)
self-score equals to 0 (no answer) obtains a K1 value
of 0. However, the final value of K1 is difficult to
interpret: a positive value does not indicate neces-
sarily more correct answers than incorrect ones, but
that the sum of scores of correct answers is higher
than the sum resulting from the scores of incorrect
answers. This could explain the little success of this
measure for evaluating QA systems in favor, again,
of accuracy measure.
Accuracy is the simplest and most intuitive evalu-
ation measure. At the same time is able to reward
those systems showing good performance. How-
ever, together with MRR belongs to the set of mea-
sures that pushes in favor of giving always a re-
sponse, even wrong, since there is no punishment for
it. Thus, the development of better validation tech-
nologies (systems able to decide whether the can-
didate answers are correct or not) is not promoted,
despite new QA architectures require them.
In effect, most QA systems during TREC and
CLEF campaigns had an upper bound of accuracy
around 60%. An explanation for this was the effect
of error propagation in the most extended pipeline
architecture: Passage Retrieval, Answer Extraction,
Answer Ranking. Even with performances higher
than 80% in each step, the overall performance
drops dramatically just because of the product of
partial performances. Thus, a way to break the
pipeline architecture is the development of a mod-
ule able to decide whether the QA system must con-
tinue or not its searching for new candidate answers:
the Answer Validation module. This idea is behind
the architecture of IBM?s Watson (DeepQA project)
that successfully participated at Jeopardy (Ferrucci
et al, 2010).
In 2006, the first Answer Validation Exercise
(AVE) proposed an evaluation task to advance the
state of the art in Answer Validation technologies
(Pen?as et al, 2007). The starting point was the re-
formulation of Answer Validation as a Recognizing
Textual Entailment problem, under the assumption
that hypotheses can be automatically generated by
combining the question with the candidate answer
(Pen?as et al, 2008a). Thus, validation was seen as a
binary classification problem whose evaluation must
deal with unbalanced collections (different propor-
tion of positive and negative examples, correct and
incorrect answers). For this reason, AVE 2006 used
F-measure based on precision and recall for correct
answers selection (Pen?as et al, 2007). Other op-
tion is an evaluation based on the analysis of Re-
ceiver Operating Characteristic (ROC) space, some-
times preferred for classification tasks with unbal-
anced collections. A comparison of both approaches
for Answer Validation evaluation is provided in (Ro-
drigo et al, 2011).
AVE 2007 changed its evaluation methodology
with two objectives: the first one was to bring sys-
tems based on Textual Entailment to the Automatic
Hypothesis Generation problem which is not part it-
self of the Recognising Textual Entailment (RTE)
task but an Answer Validation need. The second
one was an attempt to quantify the gain in QA per-
formance when more sophisticated validation mod-
ules are introduced (Pen?as et al, 2008b). With this
aim, several measures were proposed to assess: the
correct selection of candidate answers, the correct
rejection of wrong answer and finally estimate the
potential gain (in terms of accuracy) that Answer
Validation modules can provide to QA (Rodrigo et
al., 2008). The idea was to give value to the cor-
rectly rejected answers as if they could be correctly
answered with the accuracy shown selecting the cor-
rect answers. This extension of accuracy in the An-
swer Validation scenario inspired the initial develop-
ment of c@1 considering non-response.
6 Conclusions
The central idea of this work is that not respond-
ing has more value than responding incorrectly. This
idea is not new, but despite several attempts in TREC
and CLEF there wasn?t a commonly accepted mea-
1422
sure to assess non-response. We have studied here
an extension of accuracy measure with this feature,
and with a very easy to understand rationale: Unan-
swered questions have the same value as if a pro-
portion of them had been answered correctly, and
the value they add is related to the performance (ac-
curacy) observed over the answered questions. We
have shown that no other estimation of this value
produce a sensible measure.
We have shown also that the proposed measure
c@1 has a good balance of discrimination power,
stability and sensitivity properties. Finally, we have
shown how this measure rewards systems able to
maintain the same number of correct answers and at
the same time reduce the number of incorrect ones,
by leaving some questions unanswered.
Among other tasks, measure c@1 is well suited
for evaluating Reading Comprehension tests, where
multiple choices per question are given, but only one
is correct. Non-response must be assessed if we
want to measure effective reading and not just the
ability to rank options. This is clearly not enough
for the development of reading technologies.
Acknowledgments
This work has been partially supported by the
Research Network MA2VICMR (S2009/TIC-1542)
and Holopedia project (TIN2010-21128-C02).
References
Chris Buckley and Ellen M. Voorhees. 2000. Evalu-
ating evaluation measure stability. In Proceedings of
the 23rd annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 33?40. ACM.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John Prager,
Nico Schlaefer, and Chris Welty. 2010. Building Wat-
son: An Overview of the DeepQA Project. AI Maga-
zine, 31(3).
Junichi Fukumoto, Tsuneaki Kato, and Fumito Masui.
2002. Question and Answering Challenge (QAC-
1): Question Answering Evaluation at NTCIR Work-
shop 3. In Working Notes of the Third NTCIR Work-
shop Meeting Part IV: Question Answering Challenge
(QAC-1), pages 1-10.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo. 2005.
Question Answering Pilot Task at CLEF 2004. InMul-
tilingual Information Access for Text, Speech and Im-
ages, CLEF 2004, Revised Selected Papers., volume
3491 of Lecture Notes in Computer Science, Springer,
pages 581?590.
Bernardo Magnini, Alessandro Vallin, Christelle Ayache,
Gregor Erbach, Anselmo Pen?as, Maarten de Rijke,
Paulo Rocha, Kiril Ivanov Simov, and Richard F. E.
Sutcliffe. 2005. Overview of the CLEF 2004 Multi-
lingual Question Answering Track. InMultilingual In-
formation Access for Text, Speech and Images, CLEF
2004, Revised Selected Papers., volume 3491 of Lec-
ture Notes in Computer Science, Springer, pages 371?
391.
Anselmo Pen?as, A?lvaro Rodrigo, Valent??n Sama, and Fe-
lisa Verdejo. 2007. Overview of the Answer Valida-
tion Exercise 2006. In Evaluation of Multilingual and
Multi-modal Information Retrieval, CLEF 2006, Re-
vised Selected Papers, volume 4730 of Lecture Notes
in Computer Science, Springer, pages 257?264.
Anselmo Pen?as, A?lvaro Rodrigo, Valent??n Sama, and Fe-
lisa Verdejo. 2008a. Testing the Reasoning for Ques-
tion Answering Validation. In Journal of Logic and
Computation. 18(3), pages 459?474.
Anselmo Pen?as, A?lvaro Rodrigo, and Felisa Verdejo.
2008b. Overview of the Answer Validation Exercise
2007. In Advances in Multilingual and Multimodal
Information Retrieval, CLEF 2007, Revised Selected
Papers, volume 5152 of Lecture Notes in Computer
Science, Springer, pages 237?248.
Anselmo Pen?as, Pamela Forner, Richard Sutcliffe, A?lvaro
Rodrigo, Corina Forascu, In?aki Alegria, Danilo Gi-
ampiccolo, Nicolas Moreau, and Petya Osenova.
2010. Overview of ResPubliQA 2009: Question An-
swering Evaluation over European Legislation. In
Multilingual Information Access Evaluation I. Text Re-
trieval Experiments, CLEF 2009, Revised Selected Pa-
pers, volume 6241 of Lecture Notes in Computer Sci-
ence, Springer.
Alvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.
2008. Evaluating Answer Validation in Multi-stream
Question Answering. In Proceedings of the Second In-
ternational Workshop on Evaluating Information Ac-
cess (EVIA 2008).
Alvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.
2011. Evaluating Question Answering Validation as a
classification problem. Language Resources and Eval-
uation, Springer Netherlands (In Press).
Tetsuya Sakai. 2006. Evaluating Evaluation Metrics
based on the Bootstrap. In SIGIR 2006: Proceedings
of the 29th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, Seattle, Washington, USA, August 6-11, 2006,
pages 525?532.
1423
Tetsuya Sakai. 2007a. On the Reliability of Factoid
Question Answering Evaluation. ACM Trans. Asian
Lang. Inf. Process., 6(1).
Tetsuya Sakai. 2007b. On the reliability of information
retrieval metrics based on graded relevance. Inf. Pro-
cess. Manage., 43(2):531?548.
Ellen M. Voorhees and Chris Buckley. 2002. The effect
of Topic Set Size on Retrieval Experiment Error. In SI-
GIR ?02: Proceedings of the 25th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 316?323.
Ellen M. Voorhees and DawnM. Tice. 1999. The TREC-
8 Question Answering Track Evaluation. In Text Re-
trieval Conference TREC-8, pages 83?105.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. In E. M. voorhees, D. K.
Harman, editors: Proceedings of the Tenth Text RE-
trieval Conference (TREC 2001). NIST Special Publi-
cation 500-250.
Ellen M. Voorhees. 2002. Overview of TREC 2002
Question Answering Track. In E.M. Voorhees, L. P.
Buckland, editors: Proceedings of the Eleventh Text
REtrieval Conference (TREC 2002). NIST Publication
500-251.
Ellen M. Voorhees. 2003. Overview of the TREC 2003
Question Answering Track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
1424
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1466?1475,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Domain-Specific Knowledge from Text
Dirk Hovy, Chunliang Zhang, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh, czheng, hovy}@isi.edu
Anselmo Pen?as
UNED NLP and IR Group
Juan del Rosal 16
28040 Madrid, Spain
anselmo@lsi.uned.es
Abstract
Learning by Reading (LbR) aims at enabling
machines to acquire knowledge from and rea-
son about textual input. This requires knowl-
edge about the domain structure (such as en-
tities, classes, and actions) in order to do in-
ference. We present a method to infer this im-
plicit knowledge from unlabeled text. Unlike
previous approaches, we use automatically ex-
tracted classes with a probability distribution
over entities to allow for context-sensitive la-
beling. From a corpus of 1.4m sentences, we
learn about 250k simple propositions about
American football in the form of predicate-
argument structures like ?quarterbacks throw
passes to receivers?. Using several statisti-
cal measures, we show that our model is able
to generalize and explain the data statistically
significantly better than various baseline ap-
proaches. Human subjects judged up to 96.6%
of the resulting propositions to be sensible.
The classes and probabilistic model can be
used in textual enrichment to improve the per-
formance of LbR end-to-end systems.
1 Introduction
The goal of Learning by Reading (LbR) is to enable
a computer to learn about a new domain and then
to reason about it in order to perform such tasks as
question answering, threat assessment, and explana-
tion (Strassel et al, 2010). This requires joint efforts
from Information Extraction, Knowledge Represen-
tation, and logical inference. All these steps depend
on the system having access to basic, often unstated,
foundational knowledge about the domain.
Most documents, however, do not explicitly men-
tion this information in the text, but assume basic
background knowledge about the domain, such as
positions (?quarterback?), titles (?winner?), or ac-
tions (?throw?) for sports game reports. Without
this knowledge, the text will not make sense to the
reader, despite being well-formed English. Luckily,
the information is often implicitly contained in the
document or can be inferred from similar texts.
Our system automatically acquires domain-
specific knowledge (classes and actions) from large
amounts of unlabeled data, and trains a probabilis-
tic model to determine and apply the most infor-
mative classes (quarterback, etc.) at appropriate
levels of generality for unseen data. E.g., from
sentences such as ?Steve Young threw a pass to
Michael Holt?, ?Quarterback Steve Young finished
strong?, and ?Michael Holt, the receiver, left early?
we can learn the classes quarterback and receiver,
and the proposition ?quarterbacks throw passes to
receivers?.
We will thus assume that the implicit knowl-
edge comes in two forms: actions in the form of
predicate-argument structures, and classes as part of
the source data. Our task is to identify and extract
both. Since LbR systems must quickly adapt and
scale well to new domains, we need to be able to
work with large amounts of data and minimal super-
vision. Our approach produces simple propositions
about the domain (see Figure 1 for examples of ac-
tual propositions learned by our system).
American football was the first official evaluation
domain in the DARPA-sponsored Machine Reading
program, and provides the background for a number
1466
of LbR systems (Mulkar-Mehta et al, 2010). Sports
is particularly amenable, since it usually follows a
finite, explicit set of rules. Due to its popularity,
results are easy to evaluate with lay subjects, and
game reports, databases, etc. provide a large amount
of data. The same need for basic knowledge appears
in all domains, though. In music, musicians play in-
struments, in electronics, components constitute cir-
cuits, circuits use electricity, etc.
Teams beat teams
Teams play teams
Quarterbacks throw passes
Teams win games
Teams defeat teams
Receivers catch passes
Quarterbacks complete passes
Quarterbacks throw passes to receivers
Teams play games
Teams lose games
Figure 1: The ten most frequent propositions discovered
by our system for the American football domain
Our approach differs from verb-argument identi-
fication or Named Entity (NE) tagging in several re-
spects. While previous work on verb-argument se-
lection (Pardo et al, 2006; Fan et al, 2010) uses
fixed sets of classes, we cannot know a priori how
many and which classes we will encounter. We
therefore provide a way to derive the appropriate
classes automatically and include a probability dis-
tribution for each of them. Our approach is thus
less restricted and can learn context-dependent, fine-
grained, domain-specific propositions. While a NE-
tagged corpus could produce a general proposition
like ?PERSON throws to PERSON?, our method
enables us to distinguish the arguments and learn
?quarterback throws to receiver? for American foot-
ball and ?outfielder throws to third base? for base-
ball. While in NE tagging each word has only one
correct tag in a given context, we have hierarchical
classes: an entity can be correctly labeled as a player
or a quarterback (and possibly many more classes),
depending on the context. By taking context into
account, we are also able to label each sentence in-
dividually and account for unseen entities without
using external resources.
Our contributions are:
? we use unsupervised learning to train a model
that makes use of automatically extracted
classes to uncover implicit knowledge in the
form of predicate-argument propositions
? we evaluate the explanatory power, generaliza-
tion capability, and sensibility of the proposi-
tions using both statistical measures and human
judges, and compare them to several baselines
? we provide a model and a set of propositions
that can be used to improve the performance
of end-to-end LbR systems via textual enrich-
ment.
2 Methods
INPUT:
Steve Young threw a pass to Michael Holt
1. PARSE INPUT:
2. JOIN NAMES, EXTRACT PREDICATES:
NVN: Steve_Young throw pass 
NVNPN: Steve_Young throw pass to Michael_Holt
3. DECODE TO INFER PROPOSITIONS:
QUARTERBACK throw pass
QUARTERBACK throw pass to RECEIVER
Steve/NNP
Young/NNP
throw/VBD
pass/NN
a/DT
to/TO
Michael/NNP
Holt/NNP
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 2: Illustrated example of different processing steps
Our running example will be ?Steve Young threw
a pass to Michael Holt?. This is an instance of the
underlying proposition ?quarterbacks throw passes
to receivers?, which is not explicitly stated in the
data. A proposition is thus a more general state-
ment about the domain than the sentences it de-
rives. It contains domain-specific classes (quarter-
back, receiver), as well as lexical items (?throw?,
?pass?). In order to reproduce the proposition,
given the input sentences, our system has to not
only identify the classes, but also learn when to
1467
abstract away from the lexical form to the ap-
propriate class and when to keep it (cf. Figure
2, step 3). To facilitate extraction, we focus on
propositions with the following predicate-argument
structures: NOUN-VERB-NOUN (e.g., ?quarter-
backs throw passes?), or NOUN-VERB-NOUN-
PREPOSITION-NOUN (e.g., ?quarterbacks throw
passes to receivers?. There is nothing, though, that
prevents the use of other types of structures as well.
We do not restrict the verbs we consider (Pardo et
al., 2006; Ritter et al, 2010)), which extracts a high
number of hapax structures.
Given a sentence, we want to find the most likely
class for each word and thereby derive the most
likely proposition. Similar to Pardo et al (2006), we
assume the observed data was produced by a process
that generates the proposition and then transforms
the classes into a sentence, possibly adding addi-
tional words. We model this as a Hidden Markov
Model (HMM) with bigram transitions (see Section
2.3) and use the EM algorithm (Dempster et al,
1977) to train it on the observed data, with smooth-
ing to prevent overfitting.
2.1 Data
We use a corpus of about 33k texts on Ameri-
can football, extracted from the New York Times
(Sandhaus, 2008). To identify the articles, we rely
on the provided ?football? keyword classifier. The
resulting corpus comprises 1, 359, 709 sentences
from game reports, background stories, and opin-
ion pieces. In a first step, we parse all documents
with the Stanford dependency parser (De Marneffe
et al, 2006) (see Figure 2, step 1). The output
is lemmatized (collapsing ?throws?, ?threw?, etc.,
into ?throw?), and marked for various dependen-
cies (nsubj, amod, etc.). This enables us to ex-
tract the predicate argument structure, like subject-
verb-object, or additional prepositional phrases (see
Figure 2, step 2). These structures help to sim-
plify the model by discarding additional words like
modifiers, determiners, etc., which are not essen-
tial to the proposition. The same approach is used
by (Brody, 2007). We also concatenate multi-
word names (identified by sequences of NNPs) with
an underscore to form a single token (?Steve/NNP
Young/NNP?? ?Steve Young?).
2.2 Deriving Classes
To derive the classes used for entities, we do not re-
strict ourselves to a fixed set, but derive a domain-
specific set directly from the data. This step is per-
formed simultaneously with the corpus generation
described above. We utilize three syntactic construc-
tions to identify classes, namely nominal modifiers,
copula verbs, and appositions, see below. This is
similar in nature to Hearst?s lexico-syntactic patterns
(Hearst, 1992) and other approaches that derive IS-
A relations from text. While we find it straightfor-
ward to collect classes for entities in this way, we
did not find similar patterns for verbs. Given a suit-
able mechanism, however, these could be incorpo-
rated into our framework as well.
Nominal modifier are common nouns (labeled
NN) that precede proper nouns (labeled NNP), as in
?quarterback/NN Steve/NNP Young/NNP?, where
?quarterback? is the nominal modifier of ?Steve
Young?. Similar information can be gained from ap-
positions (e.g., ?Steve Young, the quarterback of his
team, said...?), and copula verbs (?Steve Young is
the quarterback of the 49ers?). We extract those co-
occurrences and store the proper nouns as entities
and the common nouns as their possible classes. For
each pair of class and entity, we collect counts over
the corpus to derive probability distributions.
Entities for which we do not find any of the above
patterns in our corpus are marked ?UNK?. These
entities are instantiated with the 20 most frequent
classes. All other (non-entity) words (including
verbs) have only their identity as class (i.e., ?pass?
remains ?pass?).
The average number of classes per entity is 6.87.
The total number of distinct classes for entities is
63, 942. This is a huge number to model in our state
space.1 Instead of manually choosing a subset of the
classes we extracted, we defer the task of finding the
best set to the model.
We note, however, that the distribution of classes
for each entity is highly skewed. Due to the unsuper-
vised nature of the extraction process, many of the
extracted classes are hapaxes and/or random noise.
Most entities have only a small number of applicable
classes (a football player usually has one main posi-
1NE taggers usually use a set of only a few dozen classes at
most.
1468
tion, and a few additional roles, such as star, team-
mate, etc.). We handle this by limiting the number of
classes considered to 3 per entity. This constraint re-
duces the total number of distinct classes to 26, 165,
and the average number of classes per entity to 2.53.
The reduction makes for a more tractable model size
without losing too much information. The class al-
phabet is still several magnitudes larger than that for
NE or POS tagging. Alternatively, one could use ex-
ternal resources such as Wikipedia, Yago (Suchanek
et al, 2007), or WordNet++ (Ponzetto and Navigli,
2010) to select the most appropriate classes for each
entity. This is likely to have a positive effect on the
quality of the applicable classes and merits further
research. Here, we focus on the possibilities of a
self-contained system without recurrence to outside
resources.
The number of classes we consider for each entity
also influences the number of possible propositions:
if we consider exactly one class per entity, there will
be little overlap between sentences, and thus no gen-
eralization possible?the model will produce many
distinct propositions. If, on the other hand, we used
only one class for all entities, there will be similar-
ities between many sentences?the model will pro-
duce very few distinct propositions.
2.3 Probabilistic Model
INPUT:
Steve Young threw a pass to Michael Holt
PARSE:
INSTANCES:
Steve_Young throw pass 
Steve_Young throw pass to Michael_Holt
PROPOSITIONS:
Quarterback throw pass
Quarterback throw pass to receiver
Steve
Young
throw
pass
a
to
Michael
Holt
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 3: Graphical model for the running example
We use a generative noisy-channel model to cap-
ture the joint probability of input sentences and their
underlying proposition. Our generative story of how
a sentence s (with words s1, ..., sn) was generated
assumes that a proposition p is generated as a se-
quence of classes p1, ..., pn, with transition proba-
bilities P (pi|pi?1). Each class pi generates a word
si with probability P (si|pi). We allow additional
words x in the sentence which do not depend on any
class in the proposition and are thus generated inde-
pendently with P (x) (cf. model in Figure 3).
Since we observe the co-occurrence counts of
classes and entities in the data, we can fix the emis-
sion parameter P (s|p) in our HMM. Further, we do
not want to generate sentences from propositions, so
we can omit the step that adds the additional words
x in our model. The removal of these words is re-
flected by the preprocessing step that extracts the
structure (cf. Section 2.1).
Our model is thus defined as
P (s,p) =P (p1) ?
n?
i=1
(
P (pi|pi?1) ? P (si|pi)
)
(1)
where si, pi denote the ith word of sentence s and
proposition p, respectively.
3 Evaluation
We want to evaluate how well our model predicts
the data, and how sensible the resulting propositions
are. We define a good model as one that generalizes
well and produces semantically useful propositions.
We encounter two problems. First, since we de-
rive the classes in a data-driven way, we have no
gold standard data available for comparison. Sec-
ond, there is no accepted evaluation measure for this
kind of task. Ultimately, we would like to evaluate
our model externally, such as measuring its impact
on performance of a LbR system. In the absence
thereof, we resort to several complementary mea-
sures, as well as performing an annotation task. We
derive evaluation criteria as follows. A model gener-
alizes well if it can cover (?explain?) all the sentences
in the corpus with a few propositions. This requires
a measure of generality. However, while a proposi-
tion such as ?PERSON does THING?, has excellent
generality, it possesses no discriminating power. We
also need the propositions to partition the sentences
into clusters of semantic similarity, to support effec-
tive inference. This requires a measure of distribu-
tion. Maximal distribution, achieved by assigning
every sentence to a different proposition, however,
is not useful either. We need to find an appropri-
ate level of generality within which the sentences
are clustered into propositions for the best overall
groupings to support inference.
To assess the learned model, we apply the mea-
sures of generalization, entropy, and perplexity (see
1469
Sections 3.2, 3.3, and 3.4). These measures can be
used to compare different systems. We do not at-
tempt to weight or combine the different measures,
but present each in its own right.
Further, to assess label accuracy, we use Ama-
zon?s Mechanical Turk annotators to judge the sen-
sibility of the propositions produced by each sys-
tem (Section 3.5). We reason that if our system
learned to infer the correct classes, then the resulting
propositions should constitute true, general state-
ments about that domain, and thus be judged as sen-
sible.2 This approach allows the effective annotation
of sufficient amounts of data for an evaluation (first
described for NLP in (Snow et al, 2008)).
3.1 Evaluation Data
With the trained model, we use Viterbi decoding to
extract the best class sequence for each example in
the data. This translates the original corpus sen-
tences into propositions. See steps 2 and 3 in Figure
2.
We create two baseline systems from the same
corpus, one which uses the most frequent class
(MFC) for each entity, and another one which uses
a class picked at random from the applicable classes
of each entity.
Ultimately, we are interested in labeling unseen
data from the same domain with the correct class,
so we evaluate separately on the full corpus and
the subset of sentences that contain unknown enti-
ties (i.e., entities for which no class information was
available in the corpus, cf. Section 2.2).
For the latter case, we select all examples con-
taining at least one unknown entity (labeled UNK),
resulting in a subset of 41, 897 sentences, and repeat
the evaluation steps described above. Here, we have
to consider a much larger set of possible classes per
entity (the 20 overall most frequent classes). The
MFC baseline for these cases is the most frequent
of the 20 classes for UNK tokens, while the random
baseline chooses randomly from that set.
3.2 Generalization
Generalization measures how widely applicable the
produced propositions are. A completely lexical ap-
2Unfortunately, if judged insensible, we can not infer
whether our model used the wrong class despite better options,
or whether we simply have not learned the correct label.
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.04 0.01
0.12 0.09
0.25
0.66
Generalization
random
MFC
model
Figure 4: Generalization of models on the data sets
proach, at one extreme, would turn each sentence
into a separate proposition, thus achieving a gener-
alization of 0%. At the other extreme, a model that
produces only one proposition would generalize ex-
tremely well (but would fail to explain the data in
any meaningful way). Both are of course not desir-
able.
We define generalization as
g = 1?
|propositions|
|sentences|
(2)
The results in Figure 4 show that our model is
capable of abstracting away from the lexical form,
achieving a generalization rate of 25% for the full
data set. The baseline approaches do significantly
worse, since they are unable to detect similarities
between lexically different examples, and thus cre-
ate more propositions. Using a two-tailed t-test, the
difference between our model and each baseline is
statistically significant at p < .001.
Generalization on the unknown entity data set is
even higher (65.84%). The difference between the
model and the baselines is again statistically signif-
icant at p < .001. MFC always chooses the same
class for UNK, regardless of context, and performs
much worse. The random baseline chooses between
20 classes per entity instead of 3, and is thus even
less general.
3.3 Normalized Entropy
Entropy is used in information theory to measure
how predictable data is. 0 means the data is com-
pletely predictable. The higher the entropy of our
propositions, the less well they explain the data. We
are looking for models with low entropy. The ex-
treme case of only one proposition has 0 entropy:
1470
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.00 1.000.99 0.99
0.89
0.50
Normalized Entropy
random
MFC
model
Figure 5: Entropy of models on the data sets
we know exactly which sentences are produced by
the proposition.
Entropy is directly influenced by the number of
propositions used by a system.3 In order to compare
different models, we thus define normalized entropy
as
HN =
?
n?
i=0
Pi ? logPi
log n
(3)
where Pi is the coverage of the proposition, or the
percentage of sentences explained by it, and n is the
number of distinct propositions.
The entropy of our model on the full data set is
relatively high with 0.89, see Figure 5. The best
entropy we can hope to achieve given the number
of propositions and sentences is actually 0.80 (by
concentrating the maximum probability mass in one
proposition). The model thus does not perform as
badly as the number might suggest. The entropy of
our model on unseen data is better, with 0.50 (best
possible: 0.41). This might be due to the fact that
we considered more classes for UNK than for regu-
lar entities.
3.4 Perplexity
Since we assume that propositions are valid sen-
tences in our domain, good propositions should have
a higher probability than bad propositions in a lan-
guage model. We can compute this using perplex-
3Note that how many classes we consider per entity influ-
ences how many propositions are produced (cf. Section 2.2),
and thus indirectly puts a bound on entropy.
entropy
Page 1
full data set unknown entities
50.00
51.00
52.00
53.00
54.00
55.00
56.00
57.00
58.00
59.00
60.00 59.52
57.0357.03 57.1556.84
54.92
Perplexity
random
MFC
model
Figure 6: Perplexity of models on the data sets
ity:4
perplexity(data) = 2
? logP (data)
n (4)
where P (data) is the product of the proposition
probabilities, and n is the number of propositions.
We use the uni-, bi-, and trigram counts of the
GoogleGrams corpus (Brants and Franz, 2006) with
simple interpolation to compute the probability of
each proposition.
The results in Figure 6 indicate that the proposi-
tions found by the model are preferable to the ones
found by the baselines. As would be expected, the
sensibility judgements for MFC and model5 (Tables
1 and 2, Section 3.5) are perfectly anti-correlated
(correlation coefficient ?1) with the perplexity for
these systems in each data set. However, due to the
small sample size, this should be interpreted cau-
tiously.
3.5 Sensibility and Label Accuracy
In unsupervised training, the model with the best
data likelihood does not necessarily produce the best
label accuracy. We evaluate label accuracy by pre-
senting subjects with the propositions we obtained
from the Viterbi decoding of the corpus, and ask
them to rate their sensibility. We compare the dif-
ferent systems by computing sensibility as the per-
centage of propositions judged sensible for each sys-
tem. Since the underlying probability distributions
are quite different, we weight the sensibility judge-
ment for each proposition by the likelihood of that
proposition. We report results for both aggregate
4Perplexity also quantifies the uncertainty of the resulting
propositions, where 0 perplexity means no uncertainty.
5We did not collect sensibility judgements for the random
baseline.
1471
accuracy
Page 1
System
90.16 92.13 69.35 70.57 88.84 90.37
94.28 96.55 70.93 70.45 93.06 95.16
100 most frequent random combined
Data set agg maj agg maj agg maj
full
baseline
model
Table 1: Percentage of propositions derived from labeling the full data set that were judged sensible
accuracy
Page 1
System
51.92 51.51 32.39 28.21 50.39 49.66
66.00 69.57 48.14 41.74 64.83 67.76
100 most frequent random combined
Data set agg maj agg maj agg maj
unknown
baseline
model
Table 2: Percentage of propositions derived from labeling unknown entities that were judged sensible
sensibility (using the total number of individual an-
swers), and majority sensibility, where each propo-
sition is scored according to the majority of annota-
tors? decisions.
The model and baseline propositions for the full
data set are both judged highly sensible, achieving
accuracies of 96.6% and 92.1% (cf. Table 1). While
our model did slightly better, the differences are not
statistically significant when using a two-tailed test.
The propositions produced by the model from un-
known entities are less sensible (67.8%), albeit still
significantly above chance level, and the baseline
propositions for the same data set (p < 0.01). Only
49.7% propositions of the baseline were judged sen-
sible (cf. Table 2).
3.5.1 Annotation Task
Our model finds 250, 169 distinct propositions,
the MFC baseline 293, 028. We thus have to restrict
ourselves to a subset in order to judge their sensi-
bility. For each system, we sample the 100 most
frequent propositions and 100 random propositions
found for both the full data set and the unknown enti-
ties6 and have 10 annotators rate each proposition as
sensible or insensible. To identify and omit bad an-
notators (?spammers?), we use the method described
in Section 3.5.2, and measure inter-annotator agree-
ment as described in Section 3.5.3. The details of
this evaluation are given below, the results can be
found in Tables 1 and 2.
The 200 propositions from each of the four sys-
6We omit the random baseline here due to size issues, and
because it is not likely to produce any informative comparison.
tems (model and baseline on both full and unknown
data set), contain 696 distinct propositions. We
break these up into 70 batches (Amazon Turk an-
notation HIT pages) of ten propositions each. For
each proposition, we request 10 annotators. Overall,
148 different annotators participated in our annota-
tion. The annotators are asked to state whether each
proposition represents a sensible statement about
American Football or not. A proposition like ?Quar-
terbacks can throw passes to receivers? should make
sense, while ?Coaches can intercept teams? does
not. To ensure that annotators judge sensibility and
not grammaticality, we format each proposition the
same way, namely pluralizing the nouns and adding
?can? before the verb. In addition, annotators can
state whether a proposition sounds odd, seems un-
grammatical, is a valid sentence, but against the
rules (e.g., ?Coaches can hit players?) or whether
they do not understand it.
3.5.2 Spammers
Some (albeit few) annotators on Mechanical Turk
try to complete tasks as quickly as possible with-
out paying attention to the actual requirements, in-
troducing noise into the data. We have to identify
these spammers before the evaluation. One way is
to include tests. Annotators that fail these tests will
be excluded. We use a repetition (first and last ques-
tion are the same), and a truism (annotators answer-
ing ?no? either do not know about football or just
answered randomly). Alternatively, we can assume
that good annotators, who are the majority, will ex-
hibit similar behavior to one another, while spam-
1472
mers exhibit a deviant answer pattern. To identify
those outliers, we compare each annotator?s agree-
ment to the others and exclude those whose agree-
ment falls more than one standard deviation below
the average overall agreement.
We find that both methods produce similar results.
The first method requires more careful planning, and
the resulting set of annotators still has to be checked
for outliers. The second method has the advantage
that it requires no additional questions. It includes
the risk, though, that one selects a set of bad annota-
tors solely because they agree with one another.
3.5.3 Agreement
agreement
Page 1
0.88 0.76 0.82
? 0.45 0.50 0.48
0.66 0.53 0.58
measure 100 most frequent random combined
agreement
G-index
Table 3: Agreement measures for different samples
We use inter-annotator agreement to quantify the
reliability of the judgments. Apart from the simple
agreement measure, which records how often an-
notators choose the same value for an item, there
are several statistics that qualify this measure by ad-
justing for other factors. One frequently used mea-
sure, Cohen?s ?, has the disadvantage that if there
is prevalence of one answer, ? will be low (or even
negative), despite high agreement (Feinstein and Ci-
cchetti, 1990). This phenomenon, known as the ?
paradox, is a result of the formula?s adjustment for
chance agreement. As shown by Gwet (2008), the
true level of actual chance agreement is realistically
not as high as computed, resulting in the counterin-
tuitive results. We include it for comparative rea-
sons. Another statistic, the G-index (Holley and
Guilford, 1964), avoids the paradox. It assumes that
expected agreement is a function of the number of
choices rather than chance. It uses the same general
formula as ?,
(Pa ? Pe)
(1? Pe)
(5)
where Pa is the actual raw agreement measured, and
Pe is the expected agreement. The difference with
? is that Pe for the G-index is defined as Pe = 1/q,
where q is the number of available categories, in-
stead of expected chance agreement. Under most
conditions, G and ? are equivalent, but in the case
of high raw agreement and few categories, G gives a
more accurate estimation of the agreement. We thus
report raw agreement, ?, and G-index.
Despite early spammer detection, there are still
outliers in the final data, which have to be accounted
for when calculating agreement. We take the same
approach as in the statistical spammer detection and
delete outliers that are more than one standard devi-
ation below the rest of the annotators? average.
The raw agreement for both samples combined is
0.82, G = 0.58, and ? = 0.48. The numbers show
that there is reasonably high agreement on the label
accuracy.
4 Related Research
The approach we describe is similar in nature to un-
supervised verb argument selection/selectional pref-
erences and semantic role labeling, yet goes be-
yond it in several ways. For semantic role label-
ing (Gildea and Jurafsky, 2002; Fleischman et al,
2003), classes have been derived from FrameNet
(Baker et al, 1998). For verb argument detec-
tion, classes are either semi-manually derived from
a repository like WordNet, or from NE taggers
(Pardo et al, 2006; Fan et al, 2010). This allows
for domain-independent systems, but limits the ap-
proach to a fixed set of oftentimes rather inappropri-
ate classes. In contrast, we derive the level of gran-
ularity directly from the data.
Pre-tagging the data with NE classes before train-
ing comes at a cost. It lumps entities together which
can have very different classes (i.e., all people be-
come labeled as PERSON), effectively allowing only
one class per entity. Etzioni et al (2005) resolve the
problem with a web-based approach that learns hi-
erarchies of the NE classes in an unsupervised man-
ner. We do not enforce a taxonomy, but include sta-
tistical knowledge about the distribution of possible
classes over each entity by incorporating a prior dis-
tribution P (class, entity). This enables us to gen-
eralize from the lexical form without restricting our-
selves to one class per entity, which helps to bet-
ter fit the data. In addition, we can distinguish sev-
eral classes for each entity, depending on the context
1473
(e.g., winner vs. quarterback). Ritter et al (2010)
also use an unsupervised model to derive selectional
predicates from unlabeled text. They do not assign
classes altogether, but group similar predicates and
arguments into unlabeled clusters using LDA. Brody
(2007) uses a very similar methodology to establish
relations between clauses and sentences, by cluster-
ing simplified propositions.
Pen?as and Hovy (2010) employ syntactic patterns
to derive classes from unlabeled data in the context
of LbR. They consider a wider range of syntactic
structures, but do not include a probabilistic model
to label new data.
5 Conclusion
We use an unsupervised model to infer domain-
specific classes from a corpus of 1.4m unlabeled
sentences, and applied them to learn 250k propo-
sitions about American football. Unlike previous
approaches, we use automatically extracted classes
with a probability distribution over entities to al-
low for context-sensitive selection of appropriate
classes.
We evaluate both the model qualities and sensibil-
ity of the resulting propositions. Several measures
show that the model has good explanatory power and
generalizes well, significantly outperforming two
baseline approaches, especially where the possible
classes of an entity can only be inferred from the
context.
Human subjects on Amazon?s Mechanical Turk
judged up to 96.6% of the propositions for the full
data set, and 67.8% for data containing unseen enti-
ties as sensible. Inter-annotator agreement was rea-
sonably high (agreement = 0.82, G = 0.58, ? =
0.48).
The probabilistic model and the extracted propo-
sitions can be used to enrich texts and support post-
parsing inference for question answering. We are
currently applying our method to other domains.
Acknowledgements
We would like to thank David Chiang, Victoria Fos-
sum, Daniel Marcu, and Stephen Tratz, as well as the
anonymous ACL reviewers for comments and sug-
gestions to improve the paper. Research supported
in part by Air Force Contract FA8750-09-C-0172
under the DARPA Machine Reading Program.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional linguistics-Volume 1, pages 86?90. Association
for Computational Linguistics Morristown, NJ, USA.
Thorsten Brants and Alex Franz, editors. 2006. The
Google Web 1T 5-gram Corpus Version 1.1. Number
LDC2006T13. Linguistic Data Consortium, Philadel-
phia.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. In Annual Meeting-Association for Com-
putational Linguistics, volume 45, page 448.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006. Citeseer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Oren Etzioni, Michael Cafarella, Doug. Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
James Fan, David Ferrucci, David Gondek, and Aditya
Kalyanpur. 2010. Prismatic: Inducing knowledge
from a large scale lexicalized relation resource. In
Proceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology for
Learning by Reading, pages 122?127, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for FrameNet classi-
fication. In Proceedings of EMNLP, volume 3.
Danies Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
1474
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539?545. Association for Computational Lin-
guistics.
Jasper Wilson Holley and Joy Paul Guilford. 1964. A
Note on the G-Index of Agreement. Educational and
Psychological Measurement, 24(4):749.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006.
Unsupervised Learning of Verb Argument Structures.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 59?70.
Anselmo Pen?as and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. In Pro-
ceedings of the NAACL HLT 2010 First International
Workshop on Formalisms and Methodology for Learn-
ing by Reading, pages 15?23, Los Angeles, California,
June. Association for Computational Linguistics.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522?1531. Association for Computational
Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Evan Sandhaus, editor. 2008. The New York Times Anno-
tated Corpus. Number LDC2008T19. Linguistic Data
Consortium, Philadelphia.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather
Simpson, Robert Schrag, and Jonathan Wright. 2010.
The DARPA Machine Reading Program-Encouraging
Linguistic and Reasoning Research with a Series of
Reading Tasks. In Proceedings of LREC 2010.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697?706. ACM.
1475
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 107?116,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Temporally Anchored Relation Extraction
Guillermo Garrido, Anselmo Pen?as, Bernardo Cabaleiro, and A?lvaro Rodrigo
NLP & IR Group at UNED
Madrid, Spain
{ggarrido,anselmo,bcabaleiro,alvarory}@lsi.uned.es
Abstract
Although much work on relation extraction
has aimed at obtaining static facts, many of
the target relations are actually fluents, as their
validity is naturally anchored to a certain time
period. This paper proposes a methodologi-
cal approach to temporally anchored relation
extraction. Our proposal performs distant su-
pervised learning to extract a set of relations
from a natural language corpus, and anchors
each of them to an interval of temporal va-
lidity, aggregating evidence from documents
supporting the relation. We use a rich graph-
based document-level representation to gener-
ate novel features for this task. Results show
that our implementation for temporal anchor-
ing is able to achieve a 69% of the upper
bound performance imposed by the relation
extraction step. Compared to the state of the
art, the overall system achieves the highest
precision reported.
1 Introduction
A question that arises when extracting a relation is
how to capture its temporal validity: Can we assign a
period of time when the obtained relation held? As
pointed out in (Ling and Weld, 2010), while much
research in automatic relation extraction has focused
on distilling static facts from text, many of the tar-
get relations are in fact fluents, dynamic relations
whose truth value is dependent on time (Russell and
Norvig, 2010).
The Temporally anchored relation extraction
problem consists in, given a natural language text
document corpus, C, a target entity, e, and a target
relation, r, extracting from the corpus the value of
that relation for the entity, and a temporal interval
for which the relation was valid.
In this paper, we introduce a methodological ap-
proach to temporal anchoring of relations automat-
ically extracted from unrestricted text. Our system
(see Figure 1) extracts relational facts from text us-
ing distant supervision (Mintz et al, 2009) and then
anchors the relation to an interval of temporal va-
lidity. The intuition is that a distant supervised sys-
tem can effectively extract relations from the source
text collection, and a straightforward date aggrega-
tion can then be applied to anchor them. We pro-
pose a four step process for temporal anchoring:
(1) represent temporal evidence; (2) select tempo-
ral information relevant to the relation; (3) decide
how a relational fact and its relevant temporal in-
formation are themselves related; and (4) aggregate
imprecise temporal intervals across multiple docu-
ments. In contrast with previous approaches that
aim at intra-document temporal information extrac-
tion (Ling and Weld, 2010), we focus on mining
a corpus aggregating temporal evidences across the
supporting documents.
We address the following research questions:
(1) Validate whether distant supervised learning is
suitable for the task, and evaluate its shortcomings.
(2) Explore whether the use of features extracted
from a document-level rich representation could im-
prove distant supervised learning. (3) Compare the
use of document metadata against temporal expres-
sions within the document for relation temporal an-
choring. (4) Analyze how, in a pipeline architecture,
the propagation of errors limits the overall system?s
107
Training
(1) IR candidate document 
retrieval
(3) Distant supervised
      learning
(5) Relation Extraction
(6) Temporal Anchoring
Document
Collection
Document 
Index
(2) Document 
Representation
(4) Classifiers
Knowledge
Base
Training seeds
< entity, relation name, value >
Training
examples
+ / -
relation 
instances
unlabelled
candidate
Training
Application
Date Extraction
output:
temporally 
anchored 
relations
Date 
Aggregation
Input: Query 
entity
Figure 1: System overview diagram.
performance.
The representation we use for temporal informa-
tion is detailed in section 2; the rich document-level
representation we exploit is described in section 3.
For a query entity and target relation, the system first
performs relation extraction (section 4); then, we
find and aggregate time constraint evidence for the
same relation across different documents, to estab-
lish a temporal validity anchor interval (section 5).
Empirical comparative evaluation of our approach is
introduced in section 6; while some related work is
shown in section 7 and conclusions in section 8.
2 Temporal Anchors
We will denominate relation instance a triple
?entity, relation name, value?. We aim at anchor-
ing relation instances to their temporal validity. We
need a representation flexible enough to capture the
imprecise temporal information available in text,
but expressed in a structured style. Allen?s (1983)
interval-based algebra for temporal representation
and reasoning, underlies much research, such as the
Tempeval challenges (Verhagen et al, 2007; Puste-
jovsky and Verhagen, 2009). Our task is different,
as we focus on obtaining the temporal interval as-
sociated to a fact, rather than reasoning about the
temporal relations among the events appearing in a
single text.
Let us assume that each relation instance is valid
during a certain temporal interval, I = [t0, tf ]. This
sharp temporal interval fails to capture the impreci-
sion of temporal boundaries conveyed in natural lan-
guage text. The Temporal Slot Filling task at TAC-
KBP 2011 (Ji et al, 2011) proposed a 4-tuple rep-
resentation that we will refer to as imprecise anchor
intervals. An imprecise temporal interval is defined
as an ordered 4-tuple of time points: (t1, t2, t3, t4),
with the following semantics: the relation is true for
a period which starts at some point between t1 and
t2 and ends between t3 and t4. It should hold that:
t1 ? t2, t3 ? t4, and t1 ? t4. Any of the four
endpoints can be left unconstrained (t1 or t3 would
be ??, and t2 or t4 would be +?). This represen-
tation is flexible and expressive, although it cannot
capture certain types of information (Ji et al, 2011).
3 Document Representation
We use a rich document representation that employs
a graph structure obtained by augmenting the syn-
tactic dependency analysis of the document with se-
mantic information.
A document D is represented as a document
graph GD; with node set VD and edge set, ED. Each
node v ? VD represents a chunk of text, which is a
sequence of words1. Each node is labeled with a
dictionary of attributes, some of which are common
for every node: the words it contains, their part-of-
speech annotations (POS) and lemmas. Also, a rep-
resentative descriptor, which is a normalized string
value, is generated from the chunks in the node. Cer-
tain nodes are also annotated with one or more types.
There are three families of types: Events (verbs
that describe an action, annotated with tense, polar-
ity and aspect); standardized Time Expressions; and
Named Entities, with additional annotations such as
gender or age.
Edges in the document graph, e ? ED, represent
four kinds of relations between the nodes:
? Syntactic: a dependency relation.
? Coreference: indicates that two chunks refer to
1Most chunks consist in one word; we join words into a
chunk (and a node) in two cases: a multi-word named entity
and a verb and its auxiliaries.
108
David[NNP,David]
NER: PERSON
DESCRIPTOR: 
David
POS: N 
Julia[NNP,Julia]
CLASS:WIFE
NER: PERSON
DESCRIPTOR: 
Julia
POS: N
GENDER:FEMALE 
September[NNP,September] 1979[CD,1979]
NER:DATE
TIMEVALUE:197909
DESCRIPTOR: September 1979
POS: NNP 
wife[NN,wife]
DESCRIPTOR: 
wife
POS: NN 
is[VBZ,be] celebrating[VBG,celebrate]
ASPECT:PROGRESSIVE
TENSE:PRESENT
POLARITY:POS
DESCRIPTOR: celebrate
POS: V 
birthday[NN,birthday]
DESCRIPTOR: 
birthday
POS: NN 
was[VBD,be] born[VBN,bear]
ASPECT:NONE
TENSE:PAST
POLARITY:POS
DESCRIPTOR: bear
POS: V 
arg0
hasClass
prep_in
arg1
arg1
has
INCLUDES
has_wife
Figure 2: Collapsed document graph representation, GC ,
for the sample text document ?David?s wife, Julia, is cel-
ebrating her birthday. She was born in September 1979?.
the same discourse referent.
? Semantic relations between two nodes, such as
hasClass, hasProperty and hasAge.
? Temporal relations between events and time ex-
pressions.
The processing includes dependency parsing,
named entity recognition and coreference reso-
lution, done with the Stanford CoreNLP soft-
ware (Klein and Manning, 2003); and events and
temporal information extraction, via the TARSQI
Toolkit (Verhagen et al, 2005).
The document graph GD is then further trans-
formed into a collapsed document graph, GC . Each
node of GC clusters together coreferent nodes, rep-
resenting a discourse referent. Thus, a node u in GC
is a cluster of nodes u1, . . . , uk of GD. There is an
edge (u, v) in GC if there was an edge between any
of the nodes clustered into u and any of the nodes
v1, . . . , vk? . The coreference edges do not appear in
this representation. Additional semantic information
is also blended into this representation: normaliza-
tion of genitives, semantic class indicators inferred
from appositions and genitives, and gender annota-
tion inferred from pronouns. A final graph example
can be seen in Figure 2.
4 Distant Supervised Relation Extraction
To perform relation extraction, our proposal fol-
lows a distant supervision approach (Mintz et al,
2009), which has also inspired other slot filling sys-
tems (Agirre et al, 2009; Surdeanu et al, 2010).
We capture long distance relations by introducing
a document-level representation and deriving novel
features from deep syntactic and semantic analysis.
Seed harvesting. From a reference Knowledge
Base (KB), we extract a set of relation triples
or seeds: ?entity, relation, value?, where the
relation is one of the target relations. Our
document-level distant supervision assumption is
that if entity and value are found in a document
graph (see section 3), and there is a path connect-
ing them, then the document expresses the relation.
Relation candidates gathering. From a seed triple,
we retrieve candidate documents that contain both
the entity and value, within a span of 20 tokens,
using a standard IR approach. Then, entity and
value are matched to the document graph represen-
tation. We first use approximate string comparison
to find nodes matching the seed entity. After an en-
tity node has been found we use local breadth-first-
search (BFS) to find a matching value and the short-
est connecting path between them. We enforce the
Named Entity type of entity and value to match a
expected type, predefined for the relation.
Our procedure traverses the document graph look-
ing for entity and value nodes meeting those condi-
tions; when found, we generate features for a pos-
itive example for the relation2. If we encounter a
node that matches the expected NE type of the rela-
tion, but does not match the seed value, we generate
a negative example for that relation.
Training. From positive and negative examples, we
generate binary features; some of them are inspired
by previous work (Surdeanu and Ciaramita, 2007;
Mintz et al, 2009; Riedel et al, 2010; Surdeanu et
al., 2010), and others are novel, taking advantage of
our graph representation. Table 1 summarizes our
choice of features. Features appearing in less than 5
training examples were discarded.
Relation instance extraction. Given an input entity
and a target relation, we aim at finding a filler value
for a relation instance. This task is known as Slot
Filling. From the set of retrieved documents relevant
to the query entity, represented as document graphs,
2From the collapsed document graph representation we ob-
tained an average of 9213 positive training examples per slot;
from the uncollapsed document graph, a slightly lower average
of 8178.5 positive examples per slot.
109
Feature name Description
path dependency path between ENTITY and
VALUE in the sentence
X-annotation NE annotations for X
X-pos Part-of-speech annotations for X
X-gov Governor of X in the dependency path
X-mod Modifiers of X in the dependency path
X-has age X is a NE, with an age attribute
X-has class-C X is a NE, with a class C
X-property-P X is a NE, and it has a property P
X-has-Y X is a NE, with a possessive relation with
another NE, Y
X-is-Y X is a NE, in a copula with another NE, Y
X-gender-G X is a NE, and it has gender G
V -tense Tense of the verb V in the path
V -aspect Aspect of the verb V in the path
V -polarity Polarity (positive or negative) of the verb V
Table 1: Features included in the model. X stands for
ENTITY and VALUE. Verb features are generated from
the verbs, V , identified in the path between ENTITY and
VALUE.
we locate matching entities and start a local BFS of
candidate values, generating for them an unlabelled
example. For each of the relations to extract, a bi-
nary classifier (extractor) decides whether the exam-
ple is a valid relation instance. For each particular
relation classifier, only candidates with the expected
entity and value types for the relation were used in
the application phase. Each extractor was a SVM
classifier with linear kernel (Joachims, 2002). All
learning parameters were set to their default values.
The classification process yields a predicted class
label, plus a real number indicating the margin. We
performed an aggregation phase to sum the mar-
gins over distinct occurrences of the same extracted
value. The rationale is that when the same value is
extracted from more than one document, we should
accumulate that evidence.
The output of this phase is the set of extracted re-
lations (positive for each of the classifiers), plus the
documents where the same fact was detected (sup-
porting documents).
5 Temporal Anchoring of Relations
In this section, we propose and discuss a unified
methodological approach for temporal anchoring of
relations. We assume the input is a relation instance
and a set of supporting documents. The task is es-
tablishing a imprecise temporal anchor interval for
the relation.
We present a four-step methodological approach:
(1) representation of intra-document temporal infor-
mation; (2) selection of relevant temporal informa-
tion for the relation; (3) mapping of the link between
relational fact and temporal information into an in-
terval; and (4) aggregation of imprecise intervals.
Temporal representation. The first methodologi-
cal step is to obtain and represent the available intra-
document temporal information; the input is a doc-
ument, and the task is to identify temporal signals
and possible links among them. We use the term link
for a relation between a temporal expression (a date)
and an event; we want to avoid confusion with the
term relation (a relational fact extracted from text).
In our particular implementation:
? We use TARSQI to extract temporal expressions
and link them to events. In particular, TARSQI
uses the following temporal links: included, si-
multaneous, after, before, begun by or ended.
? We focus also on the syntactic pattern [Event-
preposition-Time] within the lexical context of the
candidate entity and value.
? Both are normalized into one from a set of prede-
fined temporal links: within, throughout, begin-
ning, ending, after and before.
Selection of temporal evidence. For each docu-
ment and relational instance, we have to select those
temporal expressions that are relevant.
a. Document-level metadata. The default value
we use is the document creation time (DCT),
if available. The underlying assumption is that
there is a within link from each fact expressed in
the text and the document creation time.
b. Temporal expressions. Temporal evidence
comes also from the temporal expressions
present in the context of a relation. In our par-
ticular implementation, we followed a straight-
forward approach, looking for the time expres-
sion closest in the document graph to the short-
est path between the entity and value nodes. This
search is performed via a limited depth BFS,
starting from the nodes in the path, in order from
value to entity.
Mapping of temporal links into intervals. The
third step is deciding how a relational fact and its rel-
evant temporal information are themselves related.
We have to map this information, expressed in text,
110
Temporal link Constraints mapping
Before t4 = first
After t1 = last
Within and Throughout t2 = first and t3 = last
Beginning t1 = first and t2 = last
Ending t3 = first and t4 = last
Table 2: Mapping from time expression and temporal re-
lation to temporal constraints.
to a temporal representation. We will use the impre-
cise anchor intervals described is section 2.
Let T be a temporal expression identified in the
document or its metadata. Now, the mapping of tem-
poral constraints depends on the temporal link to the
time expression identified; also, the semantics of the
event have to be considered in order to decide the
time period associated to a relation instance. This
step is important because the event could refer just to
the beginning of the relation, its ending, or both. For
instance, it is obvious that having the event marry
is different to having the event divorce, when decid-
ing the temporal constraints associated to the spouse
relation.
Table 2 shows our particular mapping between
temporal links and constraints. In particular, for the
default document creation time, we suppose that a
relation which appears in a document with creation
time d held true at least in that date; that is, we are
assuming a within link, and we map t2 = d, t3 = d.
Inter-document temporal evidence aggregation.
The last step is aggregating all the time constraints
found for the same relation and value across differ-
ent documents. If we found that a relation started af-
ter two dates d and d?, where d? > d, the closest con-
straint to the real start of the relation is d?. Mapped to
temporal constraints, it means that we would choose
the biggest t1 possible. Following the same reason-
ing, we would want to maximize t3. On the other
side, when a relation started before two dates d2 and
d?2, where d
?
2 > d2, the closest constraint is d2 and
we would choose the smallest t2. In summary, we
will maximize t1 and t3 and minimize t2 and t4, so
we will narrow the margins.
6 Evaluation
We have used for our evaluation the dataset com-
piled within the TAC-KBP 2011 Temporal Slot Fill-
ing Task (Ji et al, 2011). We employed as initial
KB the one distributed to participants in the task,
which has been compiled from Wikipedia infoboxes.
It contains 898 triples ?entity, slot type, value? for
100 different entities and up to 8 different slots (re-
lations) per entity3. This gold standard contains the
correct responses pooled from the participant sys-
tems plus a set of responses manually found by
annotators. Each triple has associated a temporal
anchor. The relations had to be extracted from a
domain-general collection of 1.7 million documents.
Our system was one of the five that took part in
the task.We have evaluated the overall system and
the two main components of the architecture: Rela-
tion Extraction, and Temporal Anchoring of the re-
lations. Due to space limitations, the description of
our implementation is very concise; refer to (Garrido
et al, 2011) for further details.
6.1 Evaluation of Relation Extraction
System response in the relation extraction step con-
sists in a set of triples ?entity, slot type, value?.
Performance is measured using precision, recall and
F-measure (harmonic mean) with respect to the 898
triples in the key. Target relations (slots) are poten-
tially list-valued, that is, more than one value can
be valid for a relation (possibly at different points
in time). Only correct values yield any score, and
redundant triples are ignored.
Experiments. We run two different system settings
for the relation extraction step. They differ in the
document representation used (detailed in section3),
in order to empirically assess whether clustering of
discourse referents into single nodes benefits the ex-
traction. In SETTING 1, each document is repre-
sented as a document graph, GD, while in SETTING
2 collapsed document graph representation, GC , is
employed.
Results. Results are shown in Table 3 in the col-
umn Relation Extraction. Both settings have a sim-
ilar performance with a slight increase in the case
of graphs with clustered referents. Although preci-
sion is close to 0.5, recall is lower than 0.1. We have
studied the limits of the assumptions our approach
3There are 7 person relations: cities of residence, state-
orprovinces of residence, countries of residence, employee of,
member of, title, spouse, and an organization relation:
top members/employees.
111
is based on. First, our standard retrieval component
performance limits the overall system?s. As a matter
of example, if we retrieve the first 100 documents
per entity, we find relevant documents only for 62%
of the triples in the key. This number means that no
matter how good relation extraction method is, 38%
of relations will not be found.
Second, the distant supervision assumption un-
derlying our approach is that for a seed relation in-
stance ?entity, relation, value?, any textual men-
tion of entity and value expresses the relation. It
has been shown that this assumption is more often
violated when training knowledge base and docu-
ment collection are of different type, e.g. Wikipedia
and news-wire (Riedel et al, 2010). We have real-
ized that a more determinant factor is the relation
itself and the type of arguments it takes. We ran-
domly sampled 100 training examples per relation,
and manually inspected them to assess if they were
indeed mentions of the relation. While for the re-
lation cities of residence only 30% of the training
examples are expressing the relation, for spouse the
number goes up to 59%. For title, up to 90% of the
examples are correct. This fact explains, at least par-
tially, the zeros we obtain for some relations.
6.2 Evaluation of Temporal Anchoring
Under the evaluation metrics proposed by TAC-KBP
2011, if the value of the relation instance is judged
as correct, the score for temporal anchoring depends
on how well the returned interval matches the one
provided in the key. More precisely, let the correct
imprecise anchor interval in the gold standard key
be Sk = (k1, k2, k3, k4) and the system response be
S = (r1, r2, r3, r4). The absence of a constraint in
t1 or t3 is treated as a value of ??; the absence of
a constraint in t2 or t4 is treated as a value of +?.
Then, let di = |ki ? ri|, for i ? 1, . . . , 4, be the
difference, a real number measured in years. The
score for the system response is:
Q(S) =
1
4
4?
i=1
1
1 + di
The score for a target relation Q(r) is computed
by summing Q(S) over all unique instances of the
relation whose value is correct. If the gold standard
contains N responses, and the system output M re-
sponses, then precision is: P = Q(r)/M , and recall:
R = Q(r)/N ; F1 is the harmonic mean of P and R.
Experiments. We evaluated two different set-
tings for the temporal anchoring step; both use
the collapsed document graph representation, GC
(SETTING 2). The goal of the experiment is two-
fold. First, test the strength of the document creation
time as evidence for temporal anchoring. Second,
test how hard this metadata-level baseline is to beat
using contextual temporal expressions.
The SETTING 2-I assumes a within temporal link
between the document creation time and any relation
expressed inside the document, and aggregates this
information across the documents that we have iden-
tified as supporting the relation. The SETTING 2-II
considers documents content in order to extract tem-
poral links from the context of the text that expresses
the relation. If no temporal expression is found, the
date of the document is used as default. Temporal
links from all supporting documents are mapped into
intervals and aggregated as detailed in section 5.
The performance on relation extraction is an up-
per bound for temporal anchoring, attainable if tem-
poral anchoring is perfect. Thus, we also evaluate
the temporal anchoring performance as the percent-
age the final system achieves with respect to the re-
lation extraction upper bound.
Results. Results are shown in Table 3 under column
Temporal Anchoring. They are low, due to the upper
bound that error propagation in candidate retrieval
and relation extraction imposes upon this step: tem-
porally anchoring alone achives 69% of its upper
bound. This value corresponds to the baseline SET-
TING 2-I, showing its strength. The difference with
SETTING 2-II shows that this baseline is difficult
to beat by considering temporal evidence inside the
document content. There is a reason for this. The
temporal link mapping into time intervals does not
depend only on the type of link, but also on the se-
mantics of the text that expresses the relation as we
pointed out above. We have to decide how to trans-
form the link between relation and temporal expres-
sion into a temporal interval. Learning a model for
this is a hard open research problem that has a strong
adversary in the baseline proposed.
112
Relation Extraction Temporal Anchoring
SETTING 1 SETTING 2 SETTING 2-I SETTING 2-II
P R F P R F P R F % P R F %
(1) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(2) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(3) 0.33 0.02 0.03 0 0 0 0 0 0 0 0 0 0 0
(4) 0.22 0.09 0.13 0.29 0.11 0.16 0.23 0.09 0.13 79 0.21 0.08 0.11 72
(5) 0.53 0.13 0.20 0.54 0.12 0.19 0.34 0.07 0.12 63 0.30 0.06 0.11 56
(6) 0.70 0.12 0.20 0.75 0.13 0.22 0.57 0.10 0.16 76 0.50 0.08 0.14 67
(7) 0.50 0.06 0.10 0.50 0.07 0.12 0.29 0.04 0.07 58 0.25 0.04 0.06 50
(8) 0.25 0.04 0.07 0.20 0.04 0.07 0.15 0.03 0.05 75 0.06 0.01 0.02 30
(9) 0.42 0.08 0.14 0.45 0.08 0.14 0.31 0.06 0.10 69 0.27 0.05 0.09 60
Table 3: Results of experiments for each relation: (1) per:stateorprovinces of residence; (2) per:employee of; (3)
per:countries of residence; (4) per:member of; (5) per:title; (6) org:top members/employees; (7) per:spouse; (8)
per:cities of residence; (9) overall results (calculated as a micro-average).
System # Filled Precision Recall F1
BLENDER2 1206 0.1789 0.3030 0.2250
BLENDER1 1116 0.1796 0.2942 0.2231
BLENDER3 1215 0.1744 0.2976 0.2199
IIRG1 346 0.2457 0.1194 0.1607
Setting 2-1 167 0.2996 0.0703 0.1139
Setting 2-2 167 0.2596 0.0609 0.0986
Stanford 12 5140 0.0233 0.1680 0.0409
Stanford 11 4353 0.0238 0.1453 0.0408
USFD20112 328 0.0152 0.0070 0.0096
USFD20113 127 0.0079 0.0014 0.0024
Table 4: System ID, number of filled responses of the
system, precision, recall and F measure.
6.3 Comparative Evaluation
Our approach was compared with the other four
participants at the KBP Temporal Slot Filling Task
2011. Table 4 shows results sorted by F-measure in
comparison to our two settings (described above).
These official results correspond to a previous
dataset containing 712 triples4.
As shown in column Filled our approach returns
less triples than other systems, explaining low recall.
However, our system achieves the highest precision
for the complete task of temporally anchored rela-
tion extraction. Despite low recall, our system ob-
tains the third best F1 value. This is a very promis-
ing result, since several directions can be explored
to consider more candidates and increase recall.
7 Related Work
Compiling a Knowledge Base of temporally an-
chored facts is an open research challenge (Weikum
et al, 2011). Despite the vast amount of research fo-
cusing on understanding temporal expressions and
4Slot-fillers from human assessors were not considered
their relation to events in natural language, the com-
plete problem of temporally anchored relation ex-
traction remains relatively unexplored. Also, while
much research has focused on single-document ex-
traction, it seems clear that extracting temporally an-
chored relations needs the aggregation of evidences
across multiple documents.
There have been attempts to extend an existing
knowledge base. Wang et al (2010) use regular
expressions to mine Wikipedia infoboxes and cat-
egories and it is not suited for unrestricted text. An
earlier attempt (Zhang et al, 2008), is specific for
business and difficult to generalize to other relations.
Two recent promising works are more related to our
research. Wang et al (2011) uses manually defined
patterns to collect candidate facts and explicit dates,
and re-rank them using a graph label propagation al-
gorithm; their approach is complementary to ours,
as our aim is not to harvest temporal facts but to
extract the relations in which a query entity takes
part; unlike us, they require entity, value, and a ex-
plicit date to appear in the same sentence. Talukdar
et al (2012) focus on the partial task of temporally
anchoring already known facts, showing the useful-
ness of the document creation time as temporal sig-
nal, aggregated across documents.
Earlier work has dealt mainly with partial aspects
of the problem. The TempEval community focused
on the classification of the temporal links between
pairs of events, or an event and a temporal expres-
sion; using shallow features (Mani et al, 2003; La-
pata and Lascarides, 2004; Chambers et al, 2007),
or syntactic-based structured features (Bethard and
Martin, 2007; Pus?cas?u, 2007; Cheng et al, 2007).
Aggregating evidence across different documents
113
to temporally anchor facts has been explored in set-
tings different to Information Extraction, such as
answering of definition questions (Pas?ca, 2008) or
extracting possible dates of well-known historical
events (Schockaert et al, 2010).
Temporal inference or reasoning to solve con-
flicting temporal expressions and induce temporal
order of events has been used in TempEval (Tatu
and Srikanth, 2008; Yoshikawa et al, 2009) and
ACE (Gupta and Ji, 2009) tasks, but focused on
single-document extraction. Ling et al (2010), use
cross-event joint inference to extract temporal facts,
but only inside a single document.
Evaluation campaigns, such as ACE and TAC-
KBP 2011 have had an important role in promoting
this research. While ACE required only to identify
time expressions and classify their relation to events,
KBP requires to infer explicitly the start/end time of
relations, which is a realistic approach in the context
of building time-aware knowledge bases. KBP rep-
resents an important step for the evaluation of tem-
poral information extraction systems. In general, the
participant systems adapted existing slot filling sys-
tems, adding a temporal classification component:
distant supervised (Chen et al, 2010; Surdeanu et
al., 2010) on manually-defined patterns (Byrne and
Dunnion, 2010).
8 Conclusions
This paper introduces the problem of extracting,
from unrestricted natural language text, relational
knowledge anchored to a temporal span, aggregat-
ing temporal evidence from a collection of docu-
ments. Although compiling time-aware knowledge
bases is an important open challenge (Weikum et
al., 2011), it has remained unexplored until very re-
cently (Wang et al, 2011; Talukdar et al, 2012).
We have elucidated the two challenges of the task,
namely relation extraction and temporal anchoring
of the extracted relations.
We have studied how, in a pipeline architecture,
the propagation of errors limits the overall system?s
performance. The performance attainable in the full
task is limited by the quality of the output of the
three main phases: retrieval of candidate passages/
documents, extraction of relations and temporal an-
choring of those.
We have also studied the limits of the distant su-
pervision approach to relation extraction, showing
empirically that its performance depends not only
on the nature of reference knowledge base and doc-
ument corpus (Riedel et al, 2010), but also on the
relation to be extracted. Given a relation between
two arguments, if it is not dominant among textual
expressions of those arguments, the distant supervi-
sion assumption will be more often violated.
We have introduced a novel graph-based docu-
ment level representation, that has allowed us to gen-
erate new features for the task of relation extraction,
capturing long distance structured contexts. Our re-
sults show how, in a document level syntactic repre-
sentation, it yields better results to collapse corefer-
ent nodes.
We have presented a methodological approach
to temporal anchoring composed of: (1) intra-
document temporal information representation; (2)
selection of relation-dependent relevant temporal in-
formation; (3) mapping of temporal links to an inter-
val representation; and (4) aggregation of imprecise
intervals.
Our proposal has been evaluated within a frame-
work that allows for comparability. It has been able
to extract temporally anchored relational informa-
tion with the highest precision among the partici-
pant systems taking part in the competitive evalu-
ation TAC-KBP 2011.
For the temporal anchoring sub-problem, we have
demonstrated the strength of the document creation
time as a temporal signal. It is possible to achieve
a performance of 69% of the upper-bound imposed
by relation extraction by assuming that any relation
mentioned in a document held at the document cre-
ation time (there is a within link between the rela-
tional fact and the document creation time). This
baseline has proved stronger than extracting and an-
alyzing the temporal expressions present in the doc-
ument content.
Acknowledgments
This work has been partially supported by the Span-
ish Ministry of Science and Innovation, through
the project Holopedia (TIN2010-21128-C02), and
the Regional Government of Madrid, through the
project MA2VICMR (S2009/TIC1542).
114
References
Eneko Agirre, Angel X. Chang, Daniel S. Jurafsky,
Christopher D. Manning, Valentin I. Spitkovsky, and
Eric Yeh. 2009. Stanford-UBC at TAC-KBP. In TAC
2009, November.
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26:832?843,
November.
Steven Bethard and James H. Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and se-
mantic features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 129?132, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Lorna Byrne and John Dunnion. 2010. UCD IIRG at
TAC 2010 KBP Slot Filling Task. In Proceedings of
the Third Text Analysis Conference (TAC 2010). NIST,
November.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 173?176, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Wen-Pin Lin, Matthew Snover, Javier Artiles, Marissa
Passantino, and Heng Ji. 2010. CUNY-BLENDER
TAC-KBP2010: Entity linking and slot filling system
description. In Proceedings of the Third Text Analysis
Conference (TAC 2010). NIST, November.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist.japan: temporal relation identifi-
cation using dependency parsed tree. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 245?248, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Guillermo Garrido, Bernardo Cabaleiro, Anselmo Peas,
varo Rodrigo, and Damiano Spina. 2011. A distant
supervised learning system for the TAC-KBP Slot Fill-
ing and Temporal Slot Filling Tasks. In Text Analysis
Conference, TAC 2011 Proceedings Papers.
Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event propaga-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, ACLShort ?09, pages 369?372,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In Text Analysis Conference, TAC 2011 Work-
shop, Notebook Papers.
T. Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines ? Methods, Theory, and
Algorithms. Kluwer/Springer. We used Joachim?s
SVMLight implementation available at http://
svmlight.joachims.org/.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 2003, pages 423?430.
Mirella Lapata and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT 2004.
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence (AAAI-10).
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in news.
In NAACL-Short?03.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL 2009, pages 1003?1011,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M Pas?ca. 2008. Answering Definition Questions via
Temporally-Anchored Text Snippets. Proc. of IJC-
NLP2008.
Georgiana Pus?cas?u. 2007. Wvali: temporal relation
identification by syntactico-semantic analysis. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ?07, pages 484?487,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Pustejovsky and Marc Verhagen. 2009. SemEval-
2010 task 13: evaluating events, time expressions,
and temporal relations (TempEval-2). In Proceed-
ings of the Workshop on Semantic Evaluations: Re-
cent Achievements and Future Directions, DEW ?09,
pages 112?116, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Jose? Balca?zar, Francesco Bonchi,
Aristides Gionis, and Miche`le Sebag, editors, Machine
Learning and Knowledge Discovery in Databases,
volume 6323 of LNCS, pages 148?163. Springer
Berlin / Heidelberg.
Stuart J. Russell and Peter Norvig. 2010. Artificial Intel-
ligence - A Modern Approach (3. internat. ed.). Pear-
son Education.
Steven Schockaert, Martine De Cock, and Etienne Kerre.
2010. Reasoning about fuzzy temporal information
from the web: towards retrieval of historical events.
Soft Computing - A Fusion of Foundations, Method-
ologies and Applications, 14:869?886.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
ACE07, March.
115
Mihai Surdeanu, David McClosky, Julie Tibshirani, John
Bauer, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2010. A simple distant
supervision approach for the tac-kbp slot filling task.
In Proceedings of the Third Text Analysis Conference
(TAC 2010), Gaithersburg, Maryland, USA, Novem-
ber. NIST.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
Proceedings of the Fifth ACM International Confer-
ence on Web Search and Data Mining (WSDM), Seat-
tle, Washington, USA, February. Association for Com-
puting Machinery.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING?08.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Seok Bae Jang, Jessica Littman, Anna
Rumshisky, John Phillips, and James Pustejovsky.
2005. Automating temporal annotation with TARSQI.
In ACLdemo?05.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal re-
lation identification. In SemEval?07.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely YAGO: har-
vesting, querying, and visualizing temporal knowledge
from Wikipedia. In Proceedings of the 13th Inter-
national Conference on Extending Database Technol-
ogy, EDBT ?10, pages 697?700, New York, NY, USA.
ACM.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from textual
web sources by constrained label propagation. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM ?11,
pages 837?846, New York, NY, USA. ACM.
Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.
2011. Temporal knowledge for timely intelligence.
In Malu Castellanos, Umeshwar Dayal, Volker Markl,
Wil Aalst, John Mylopoulos, Michael Rosemann,
Michael J. Shaw, and Clemens Szyperski, editors, En-
abling Real-Time Business Intelligence, volume 84
of Lecture Notes in Business Information Processing,
pages 1?6. Springer Berlin Heidelberg.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ?09, pages 405?413, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Qi Zhang, Fabian M. Suchanek, Lihua Yue, and Gerhard
Weikum. 2008. TOB: Timely ontologies for business
relations. In 11th International Workshop on the Web
and Databases, WebDB.
116
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 15?23,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Semantic Enrichment of Text with Background Knowledge 

Anselmo Pe?as Eduard Hovy
UNED NLP & IR Group USC Information Sciences Institute 
Juan del Rosal, 16 4676 Admiralty Way 
28040 Madrid, Spain Marina del Rey, CA 90292-6695
anselmo@lsi.uned.es hovy@isi.edu
Abstract
Texts are replete with gaps, information omit-
ted since authors assume a certain amount of 
background knowledge.  We describe the kind 
of information (the formalism and methods to 
derive the content) useful for automated fill-
ing of such gaps.  We describe a stepwise pro-
cedure with a detailed example.   
1 Introduction 
Automated understanding of connected text re-
mains an unsolved challenge in NLP.  In contrast 
to systems that harvest information from large col-
lections of text, or that extract only certain pre-
specified kinds of information from single texts, 
the task of extracting and integrating all informa-
tion from a single text, and building a coherent and 
relatively complete representation of its full con-
tent, is still beyond current capabilities.   
A significant obstacle is the fact that text always 
omits information that is important, but that people 
recover effortlessly. Authors leave out information 
that they assume is known to their readers, since its 
inclusion (under the Gricean maxim of minimality) 
would carry an additional, often pragmatic, import. 
The problem is that systems cannot perform the 
recovery since they lack the requisite background 
knowledge and inferential machinery to use it.   
In this research we address the problem of 
automatically recovering such omitted information 
to ?plug the gaps? in text.  To do so, we describe 
the background knowledge required as well as a 
procedure for recognizing where gaps exist and 
determining which kinds of background knowl-
edge are needed.   
We are looking for the synchronization between 
the text representation achievable by current NLP 
and a knowledge representation (KR) scheme that 
can permit further inference for text interpretation.   
1.1 Vision
Clearly, producing a rich text interpretation re-
quires both NLP and KR capabilities.  The strategy 
we explore is the enablement of bidirectional 
communication between the two sides from the 
very beginning of the text processing. We assume 
that the KR system doesn?t require a full represen-
tation of the text meaning, but can work with a par-
tial interpretation, namely of the material explicitly 
present in the text, and can then flesh out this in-
terpretation as required for its specific task. Al-
though the NLP system initially provides simpler 
representations (even possibly ambiguous or 
wrong ones), the final result contains the semantics 
of the text according to the working domain.  
In this model, the following questions arise: 
How much can we simplify our initial text repre-
sentation and still permit the attachment of back-
ground knowledge for further inference and 
interpretation?  How should background knowl-
edge be represented for use by the KR system?  
How can the incompleteness and brittleness typical 
of background knowledge (its representational in-
flexibility, or limitation to a single viewpoint or 
expressive phrasing) (Barker 2007) be overcome?  
In what sequence can a KR system enrich an initial 
and/or impoverished reading, and how can the en-
richment benefit subsequent text processing?   
1.2 Approach
Although we are working toward it, we do not yet 
have such a system.  The aim of our current work 
is to rapidly assemble some necessary pieces and 
explore how to (i) attach background knowledge to 
flesh out a simple text representation and (ii) there 
by make explicit the meanings attached to some of 
its syntactic relations.  We begin with an initial 
simple text representation, a background knowl-
edge base corresponding to the text, and a simple 
15
formalized procedure to attach elements from the 
background knowledge to the entities and implicit 
relations present in the initial text representation.   
Surprisingly, we find that some quite simple 
processing can be effective if we are able to con-
textualize the text under interpretation. 
For our exploratory experiments, we are work-
ing with a collection of 30,000 documents in the 
domain of US football. We parsed the collection 
using a standard dependency parser (Marneffe and 
Manning, 2008; Klein and Maning, 2003) and, af-
ter collapsing some syntactic dependencies, ob-
tained the simple textual representations shown in 
Section 2. From them, we built a Background 
Knowledge Base by automatically harvesting 
propositions expressed in the collection (Section 
3). Their frequency in the collection lead the en-
richment process: given a new text in the same 
domain, we build exactly the same kind of repre-
sentation, and attach the background knowledge 
propositions as related to the text (Section 4).  
Since this is an exploratory sketch, we cannot 
provide a quantitative evaluation yet, but the quali-
tative study over some examples suggest that this 
simple framework is promising enough to start a 
long term research (Section 5). Finally, we con-
clude with the next steps we want to follow and the 
kind of evaluation we plan to do.  
2 Text Representation 
The starting text representation must capture the 
first shot of what?s going on in the text, taking 
some excerpts into account and (unfortunately) 
losing others. After the first shot, in accord with 
the purpose of the reading, we will ?contextualize? 
each sentence, expanding its initial representation 
with the relevant related background knowledge in 
our base. 
During this process of making explicit the im-
plicit semantic relations (which we call contextu-
alization or interpretation) it will become apparent 
whether we need to recover some of the discarded 
elements, whether we need to expand some others, 
etc. So the process of interpretation is identified 
with the growing of the context (according to the 
KB) until the interpretation is possible. This is re-
lated to some well-known theories such as the 
Theory of Relevance (Sperber and Wilson, 1995). 
The particular method we envisage is related to 
Interpretation as Abduction (Hobbs et al 1993). 
How can the initial information be represented 
so as to enable the context to grow into an interpre-
tation? We hypothesize that: 
1. Behind certain syntactic dependencies there 
are semantic relations. 
2. In the case of dependencies between nouns, 
this semantic relation can be made more ex-
plicit using verbs and/or prepositions. The 
knowledge base must help us find them. 
We look for a semantic representation close 
enough to the syntactic representation we can ob-
tain from the dependency graph. The main syntac-
tic dependencies we want to represent in order to 
enable enrichment are: 
1. Dependencies between nouns such as noun-
noun compounds (nn) or possessive (poss). 
2. Dependencies between nouns and verbs, 
such as subject and object relations. 
3. Prepositions having two nouns as argu-
ments. Then the preposition becomes the la-
bel for the relation between the two nouns, 
being the object of the preposition the target 
of the relation. 
For these selected elements, we produce two very 
simple transformations of the syntactic dependency 
graph:
1. Invert the direction of the syntactic depend-
ency for the modifiers. Since we work with 
the hypothesis that behind a syntactic de-
pendency there is a semantic relation, we re-
cord the direction of the semantic relation. 
2. Collapse the syntactic dependencies be-
tween verb, subject, and object into a single 
semantic relation. Since we are assuming 
that the verb is the more explicit expression 
of a semantic relation, we fix this in the ini-
tial representation. The subject will be the 
source of the relation and the object will be 
the target of the relation. When the verb has 
more arguments we consider its expansion 
as a new node as referred in Section 4.4.  
Figure 1 shows the initial minimal representa-
tion for the sentence we will use for our discus-
sion:
San_Francisco's Eric_Davis intercepted 
a Steve_Walsh pass on the next series to 
set_up a seven-yard Young touchdown pass 
to Brent_Jones.
Notice that some pieces of the text are lost in the 
initial representation of the text as for example ?on 
the next series? or ?seven-yard?.
16
3    Background Knowledge Base  
The Background Knowledge Base (BKB) is built 
from a collection in the domain of the texts we 
want to semanticize. The collection consists of 
30,826 New York Times news about American 
football, similar to the kind of texts we want to 
interpret. The elements in the BKB (3,022,305 in 
total) are obtained as a result of applying general 
patterns over dependency trees. We take advantage 
of the typed dependencies (Marneffe and Manning, 
2008) produced by the Stanford parser (Klein and 
Maning, 2003). 
3.1 Types of elements in the BKB 
We distinguish three elements in our Background 
Knowledge Base: Entities, Propositions, and Lexi-
cal relations. All of them have associated their fre-
quency in the reference collection. 
Entities
We distinguish between entity classes and entity 
instances:
1. Entity classes: Entity classes are denoted by 
the nouns that participate in a copulative rela-
tion or as noun modifier. In addition, we intro-
duce two special classes: Person and Group. 
These two classes are related to the use of pro-
nouns in text. Pronouns ?I?, ?he? and ?she? are 
linked to class Person. Pronouns ?we? and 
?they? are linked to class Group. For example, 
the occurrence of the pronoun ?he? in ?He 
threw a pass? would produce an additional 
count of the proposition ?person:throw:pass?. 
2. Entity Instances: Entity instances are indicated 
by proper nouns. Proper nouns are identified 
by the part of speech tagging. Some of these 
instances will participate in the ?has-instance? 
relation (see below).   When they participate in 
a proposition they produce proposition in-
stances.
Figure 1. Representation of the sentence: San_Francisco's Eric_Davis intercepted a Steve_Walsh
pass on the next series to set_up a seven-yard Young touchdown pass to Brent_Jones. 
Propositions
Following Clark and Harrison (2009) we call 
propositions the tuples of words that have some 
determined pattern of syntactic relations among 
them. We focus on NVN, NVNPN and NPN 
proposition types. For example, a NVNPN propo-
sition is a full instantiation of: 
Subject:Verb:Object:Prep:Complement
The first three elements are the subject, the verb 
and the direct object. Fourth is the preposition that 
attaches the PP complement to the verb. For sim-
plicity, indirect objects are considered as a Com-
plement with the preposition ?to?. 
The following are the most frequent NVN 
propositions in the BKB ordered by frequency. 
NVN 2322 'NNP':'beat':'NNP' 
NVN 2231 'NNP':'catch':'pass' 
NVN 2093 'NNP':'throw':'pass' 
NVN 1799 'NNP':'score':'touchdown' 
NVN 1792 'NNP':'lead':'NNP' 
NVN 1571 'NNP':'play':'NNP' 
NVN 1534 'NNP':'win':'game' 
NVN 1355 'NNP':'coach':'NNP' 
NVN 1330 'NNP':'replace':'NNP' 
NVN 1322 'NNP':'kick':'goal' 
NVN 1195 'NNP':'win':'NNP' 
NVN 1155 'NNP':'defeat':'NNP' 
NVN 1103 'NNP':'gain':'yard' 
The ?NNP? tag replaces specific proper nouns 
found in the proposition.  
When a sentence has more than one comple-
ment, a new occurrence is counted for each com-
plement. For example, given the sentence 
?Steve_Walsh threw a pass to Brent_Jones 
in the first quarter?, we would add a count to 
each of the following propositions: 
17
Steve_Walsh:throw:pass
Steve_Walsh:throw:pass:to:Brent_Jones
Steve_Walsh:throw:pass:in:quarter
Notice that right now we include only the heads 
of the noun phrases in the propositions. 
We call proposition classes the propositions that 
only involve instance classes (e.g., ?per-
son:throw:pass?), and proposition instances
those that involve at least one entity instance (e.g., 
?Steve_Walsh:throw:pass?).
Proposition instances are useful for the tracking 
of a entity instance. For example, 
?'Steve_Walsh':'supplant':'John_Fourcade':
'as':'quarterback'?. When a proposition in-
stance is found, it is stored also as a proposition 
class replacing the proper nouns by a special word 
(NNP) to indicate the presence of a entity instance. 
The enrichment of the text is based on the use of 
most frequent proposition classes.  
Lexical relations 
At the moment, we make use of the copulative 
verbs (detected by the Stanford?s parser) in order 
to extract ?is?, and ?has-instance? relations: 
1. Is: between two entity classes. They denote a 
kind of identity between both entity classes, 
but not in any specific hierarchical relation 
such as hyponymy. Neither is a relation of 
synonymy. As a result, is somehow a kind of 
underspecified relation that groups those more 
specific. For example, if we ask the BKB what 
a ?receiver? is, the most frequent relations are: 
290 'person':is:'receiver' 
29 'player':is:'receiver' 
16 'pick':is:'receiver' 
15 'one':is:'receiver' 
14 'receiver':is:'target' 
8 'end':is:'receiver' 
7 'back':is:'receiver' 
6 'position':is:'receiver' 
The number indicates the number of times the 
relation appears explicitly in the collection. 
2. Has-instance: between an entity class and an 
entity instance. For example, if we ask for in-
stances of team, the top 10 instances with more 
support in the collection are: 
192 'team':has-instance:'Jets' 
189 'team':has-instance:'Giants' 
43 'team':has-instance:'Eagles' 
40 'team':has-instance:'Bills' 
36 'team':has-instance:'Colts' 
35 'team':has-instance:'Miami' 
35 'team':has-instance:'Vikings' 
34 'team':has-instance:'Cowboys' 
32 'team':has-instance:'Patriots' 
31 'team':has-instance:'Dallas' 
But we can ask also for the possible classes of 
an instance. For example, all the entity classes for 
?Eric_Davis? are: 
12 'cornerback':has-instance:'Eric_Davis' 
1 'hand':has-instance:'Eric_Davis' 
1 'back':has-instance:'Eric_Davis'  
There are other lexical relations as ?part-of? and 
?is-value-of? in which we are still working. For 
example, the most frequent ?is-value-of? relations 
are:
5178 '[0-9]-[0-9]':is-value-of:'lead' 
3996 '[0-9]-[0-9]':is-value-of:'record' 
2824 '[0-9]-[0-9]':is-value-of:'loss' 
1225 '[0-9]-[0-9]':is-value-of:'season' 
4 Enrichment procedure 
The goal of the enrichment procedure is to deter-
mine what kind of events and entities are involved 
in the text, and what semantic relations are hidden 
by some syntactic dependencies such as noun-noun 
compound or some prepositions. 
4.1 Fusion of nodes 
Sometimes, the syntactic dependency ties two or 
more words that form a single concept. This is the 
case with multiword terms such as ?tight end?, 
?field goal?, ?running back?, etc. In these cases, 
the meaning of the compound is beyond the syn-
tactic dependency. Thus, we shouldn?t look for its 
explicit meaning. Instead, we activate the fusion of 
the nodes into a single one. 
However, there are some open issues related to 
the cases were fusion is not preferred. Otherwise, 
the process could be done with standard measures 
like mutual information, before the parsing step 
(and possibly improving its results). 
The question is whether the fusion of the words 
into a single expression allows or not the consid-
eration of possible paraphrases. For example, in 
the case of ?field:nn:goal?, we don?t find other 
ways to express the concept in the BKB. However, 
in the case of ?touchdown:nn:pass? we can find, 
for example, ?pass:for:touchdown? a significant 
amount of times, and we want to identify them as 
equivalent expressions. For this reason, we find not 
convenient to fuse these cases. 
18
4.2 Building context for instances 
Suppose we wish to determine what kind of entity 
?Steve Walsh? is in the context of the syntactic 
dependency ?Steve_Walsh:nn:pass?. First, we 
look into the BKB for the possible entity classes of 
Steve_Walsh previously found in the collection. In 
this particular case, the most frequent class is 
?quarterback?:
40 'quarterback':has-instance:'Steve_Walsh' 
2 'junior':has-instance:'Steve_Walsh' 
But, what happens if we see ?Steve_Walsh? for 
the first time? Then we need to find evidence from 
other entities in the same syntactic context. We 
found that ?Marino?, ?Kelly?, ?Elway?,
?Dan_Marino?, etc. appear in the same kind of 
proposition (?N:nn:pass?) where we found 
?Steve_Walsh?, each of them supported by 24, 17, 
15 and 10 occurrences respectively. However, 
some of the names can be ambiguous. For exam-
ple, searching for ?Kelly? in our BKB yields: 
153 'quarterback':has-instance:'Jim_Kelly' 
19 'linebacker':has-instance:'Joe_Kelly' 
17 'quarterback':has-instance:'Kelly' 
14 'quarterback':has-instance:'Kelly_Stouffer' 
10 'quarterback':has-instance:'Kelly_Ryan' 
8 'quarterback':has-instance:'Kelly_Holcomb' 
7 'cornerback':has-instance:'Brian_Kelly'  
Whereas others are not so ambiguous: 
113 'quarterback':has-instance:'Dan_Marino' 
6 'passer':has-instance:'Dan_Marino' 
5 'player':has-instance:'Dan_Marino'  
Taking this into account, we are able to infer that 
the most plausible class for an entity involved in a 
?NNP:nn:pass? proposition is a quarterback. 
4.3 Building context for dependencies 
Now we want to determine the meaning behind 
such syntactic dependencies as 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?, 
?Young:nn:pass? or ?pass:to:Brent_Jones?. 
We have two ways for adding more meaning to 
these syntactic dependencies: find the most appro-
priate prepositions to describe them, and find the 
most appropriate verbs. Whether one, the other or 
both is more useful has to be determined during the 
reasoning system development. 
Finding the prepositions 
There are several types of propositions in the 
BKB that involve prepositions. The most relevant 
are NPN and NVNPN. In the case of ?touch-
down:nn:pass?, preposition ?for? is clearly the best 
interpretation for the ?nn? dependency: 
NPN 712 'pass':'for':'touchdown' 
NPN 24 'pass':'include':'touchdown' 
NPN 3 'pass':'with':'touchdown' 
NPN 2 'pass':'of':'touchdown' 
NPN 1 'pass':'in':'touchdown' 
NPN 1 'pass':'follow':'touchdown' 
NPN 1 'pass':'to':'touchdown' 
In the case of ?Steve_Walsh:nn:pass? and 
?Young:nn:pass?, assuming they are quarterbacks, 
we can ask for all the prepositions between ?pass? 
and ?quarterback?: 
NPN 23 'pass':'from':'quarterback' 
NPN 14 'pass':'by':'quarterback' 
NPN 2 'pass':'of':'quarterback' 
NPN 1 'pass':'than':'quarterback' 
NPN 1 'pass':'to':'quarterback' 
Notice how lower frequencies involve more 
noisy options. 
If we don?t have any evidence on the instance 
class, and we know only that they are instances, 
the pertinent query to the BKB obtains: 
NPN 1305 'pass':'to':'NNP' 
NPN 1085 'pass':'from':'NNP' 
NPN 147 'pass':'by':'NNP' 
NPN 144 'pass':'for':'NNP' 
In the case of ?Young:nn:pass? (in ?Young 
pass to Brent Jones?), there exists already the 
preposition ?to? (?pass:to:Brent_Jones?), so the 
most promising choice become the second, 
?pass:from:Young?, which has one order of magni-
tude more occurrences than the following. 
In the case of ?Steve_Walsh:nn:pass? (in ?Eric 
Davis intercepted a Steve Walsh pass?) we can use 
additional information: we know that 
?Eric_Davis:intercept:pass?. So, we can try to 
find the appropriate preposition using NVNPN 
propositions in the following way: 
Eric_Davis:intercept:pass:P:Steve_Walsh?
Asking the BKB about the propositions that in-
volve two instances with ?intercept? and ?pass? we 
get:
NVNPN 48 'NNP':'intercept':'pass':'by':'NNP' 
NVNPN 26 'NNP':'intercept':'pass':'at':'NNP' 
NVNPN 12 'NNP':'intercept':'pass':'from':'NNP' 
We could also query the BKB with the classes 
we already found for ?Eric_Davis? (cornerback, 
player, person): 
NVNPN 11 'person':'intercept':'pass':'by':'NNP' 
NVNPN 4 'person':'intercept':'pass':'at':'NNP' 
NVNPN 2 'person':'intercept':'pass':'in':'NNP' 
19
NVNPN 2 'person':'intercept':'pass':'against':'NNP' 
NVNPN 1 'cornerback':'intercept':'pass':'by':'NNP' 
All these queries accumulate evidence over a cor-
rect preposition ?by? (?pass:by:Steve_Walsh?). 
However, an explicit entity classification would 
make the procedure more robust. 
Finding the verbs 
Now the exercise is to find a verb able to give 
meaning to the syntactic dependencies such as 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?, 
?Young:nn:pass? or ?pass:to:Brent_Jones?.
We can ask the BKB what instances (NNP) do 
with passes. The most frequent propositions are: 
NVN 2241 'NNP':'catch':'pass' 
NVN 2106 'NNP':'throw':'pass' 
NVN 844 'NNP':'complete':'pass' 
NVN 434 'NNP':'intercept':'pass' 
NVNPN 758 'NNP':'throw':'pass':'to':'NNP' 
NVNPN 562 'NNP':'catch':'pass':'for':'yard' 
NVNPN 338 'NNP':'complete':'pass':'to':'NNP' 
NVNPN 255 'NNP':'catch':'pass':'from':'NNP' 
Considering the evidence of ?Brent_Jones? be-
ing instance of ?end? (tight end), if we ask the 
BKB about the most frequent relations between 
?end? and ?pass? we find: 
NVN 28 'end':'catch':'pass' 
NVN 6 'end':'drop':'pass' 
So, in this case, the BKB suggests that the syn-
tactic dependency ?pass:to:Brent_Jones? means 
?Brent_Jones is an end catching a pass?. Or in 
other words, that ?Brent_Jones? has a role of 
?catch-ER? with respect to ?pass?. 
If we want to accumulate more evidence on this 
we can consider NVNPN propositions including 
touchdown. We only find evidence for the most 
general classes (NNP and person): 
NVNPN 189 'NNP':'catch':'pass':'for':'touchdown' 
NVNPN 26 'NNP':'complete':'pass':'for':'touchdown' 
NVNPN 84 'person':'catch':'pass':'for':'touchdown' 
NVNPN 18 'person':'complete':'pass':'for':'touchdown' 
This means, that when we have ?touchdown?, 
we don?t have counting for the second option 
?Brent_Jones:drop:pass?, while ?catch? becomes 
stronger.
In the case of ?Steve_Walsh:nn:pass? we hy-
pothesize that ?Steve_Walsh? is a quarterback. 
Asking the BKB about the most plausible relation 
between a quarterback and a pass we find: Figure 2. Graphical representation of the enriched 
text.
20
NVN 98 'quarterback':'throw':'pass' 
NVN 27 'quarterback':'complete':'pass' 
Again, if we take into account that it is a 
?touchdown:nn:pass?, then only the second op-
tion ?Steve_Walsh:complete:pass? is consistent 
with the NVNPN propositions. 
So, in this case, the BKB suggests that the syn-
tactic dependency ?Steve_Walsh:nn:pass? means 
?Steve_Walsh is a quarterback completing a pass?. 
Finally, with respect to ?touchdown:nn:pass?, 
we can ask about the verbs that relate them: 
NVN 14 'pass':'set_up':'touchdown' 
NVN 6 'pass':'score':'touchdown' 
NVN 5 'pass':'produce':'touchdown' 
Figure 2 shows the graphical representation of 
the sentence after some enrichment. 
4.4 Expansion of relations 
Sometimes, the sentence shows a verb with several 
arguments. In our example, we have 
?Eric_David:intercept:pass:on:series?. In 
these cases, the relation can be expanded and be-
come a node. 
In our example, the new node is the eventuality 
of ?intercept? (let?s say ?intercept-ION?), 
?Eric_Davis? is the ?intercept-ER? and ?pass? is 
the ?intercept-ED?. Then, we can attach the miss-
ing information to the new node (see Figure 3).  
Figure 3. Expansion of the "intercept" relation.  
In addition, we can proceed with the expansion 
of the context considering this new node. For ex-
ample, we are working with the hypothesis that 
?Steve_Walsh? is an instance of quarterback and 
thus, its most plausible relations with pass are 
?throw? and ?complete?. However, now we can 
ask about the most frequent relation between 
?quarterback? and ?interception?. The most fre-
quent is ?quarterback:throw:interception?
supported 35 times in the collection. From this, 
two actions can be done: reinforce the hypothesis 
of ?throw:pass? instead of ?complete:pass?, and 
add the hypothesis that 
?Steve_Walsh:throw:interception?.
Finally, notice that since ?set_up? doesn?t need 
to accommodate more arguments, we can maintain 
the collapsed edge. 
4.5 Constraining the interpretations 
Some of the inferences being performed are local 
in the sense that they involve only an entity and a 
relation. However, these local inferences must be 
coherent both with the sentence and the complete 
document. 
To ensure this coherence we can use additional 
information as a way to constrain different hy-
potheses. In section 4.3 we showed the use of 
NVNPN propositions to constrain NVN ones. 
 Another example is the case of 
?Eric_Davis:intercept:pass?. We can ask the 
BKB for the entity classes that participate in such 
kind of proposition: 
NVN 75 'person':'intercept':'pass' 
NVN 14 'cornerback':'intercept':'pass' 
NVN 11 'defense':'intercept':'pass' 
NVN 8 'safety':'intercept':'pass' 
NVN 7 'group':'intercept':'pass' 
NVN 5 'linebacker':'intercept':'pass' 
So the local inference for the kind of entity 
?Eric_Davis? is (cornerback) must be coherent 
with the fact that it intercepted a pass. In this case 
?cornerback? and ?person? are properly reinforced. 
In some sense, we are using these additional con-
strains as shallow selectional preferences. 
5 Evaluation
The evaluation of the enrichment process is a chal-
lenge by itself. Eventually, we will use extrinsic 
measures such as system performance on a QA 
task, applied first after reading a text, and then a 
second time after the enrichment process. This will 
measure the ability of the system to absorb and use 
knowledge across texts to enrich the interpretation 
of the target text.  In the near term, however, it re-
mains unclear which intrinsic evaluation measures 
to apply.  It is not informative simply to count the 
number of additional relations one can attach to 
representation elements, or to count the increase in 
degree of interlinking of the nodes in the represen-
tation of a paragraph.   
21
6 Related Work 
To build the knowledge base we take an approach 
closely related to DART (Clark and Harrison, 
2009) which in turn is related to KNEXT (Van 
Durme and Schubert, 2008). It is also more dis-
tantly related to TextRunner (Banko et al 2007). 
Like DART, we make use of a dependency 
parser instead of partial parsing. So we capture 
phrase heads instead complete phrases. The main 
differences between the generation of our BKB 
and the generation of DART are: 
1. We use the dependencies involving copula-
tive verbs as a source of evidence for ?is? 
and ?has-instance? relations. 
2. Instead of replacing proper nouns by ?per-
son?, ?place?, or ?organization?, we con-
sider all of them just as instances in our 
BKB. Furthermore, when a proposition con-
tains a proper noun, we count it twice: one 
as the original proposition instance, and a 
second replacing the proper nouns with a 
generic tag indicating that there was a name. 
3. We make use of the modifiers that involve 
an instance (proper noun) to add counting to 
the ?has-instance? relation. 
4. Instead of replacing pronouns by ?person? 
or ?thing?, we replace them by ?person?, 
?group? or ?thing?, taking advantage of the 
preposition number. This is particular useful 
for the domain of football where players and 
teams are central. 
5. We add a new set of propositions that relate 
two clauses in the same sentence (e.g., 
Floyd:break:takle:add:touchdown). We 
tagged these propositions NVV, NVNV, 
NVVN and NVNVN. 
6. Instead of an unrestricted domain collection, 
we consider documents closely related to the 
domain in which we want to interpret texts. 
The consideration of a specific domain collec-
tion seems a very powerful option. Ambiguity is 
reduced inside a domain so the counting for propo-
sitions is more robust. Also frequency distribution 
of propositions is different from one domain into 
another. For example, the list of the most frequent 
NVN propositions in our BKB (see Section 3.1) is, 
by itself, an indication of the most salient and im-
portant events in the American football domain. 
7 Conclusion and Future Work
The task of inferring omitted but necessary infor-
mation is a significant part of automated text inter-
pretation. In this paper we show that even simple 
kinds of information, gleaned relatively straight-
forwardly from a parsed corpus, can be quite use-
ful.  Though they are still lexical and not even 
starting to be semantic, propositions consisting of 
verbs as relations between nouns seem to provide a 
surprising amount of utility.  It remains a research 
problem to determine what kinds and levels of 
knowledge are most useful in the long run.   
In the paper, we discuss only the propositions 
that are grounded in instantial statements about 
players and events.  But for true learning by read-
ing, a system has to be able to recognize when the 
input expresses general rules, and to formulate 
such input as axioms or inferences.  In addition, 
augmenting that is the significant challenge of 
generalizing certain kinds of instantial propositions 
to produce inferences.  At which point, for exam-
ple, should the system decide that ?all football 
players have teams?, and how should it do so? 
How to do so remains a topic for future work.   
A further topic of investigation is the time at 
which expansion should occur.  Doing so at ques-
tion time, in the manner of traditional task-oriented 
back-chaining inference, is the obvious choice, but 
some limited amount of forward chaining at read-
ing time seems appropriate too, especially if it can 
significantly assist with text processing tasks, in 
the manner of expectation-driven understanding.    
Finally, as discussed above, the evaluation of 
our reading augmentation procedures remains to be 
developed.
Acknowledgments 
We are grateful to Hans Chalupsky and David 
Farwell for their comments and input along this 
work. This work has been partially supported by 
the Spanish Government through the "Programa 
Nacional de Movilidad de Recursos Humanos del 
Plan Nacional de I+D+i 2008-2011 (Grant 
PR2009-0020). We acknowledge the support of 
DARPA contract number: FA8750-09-C-0172. 
22
References
1. Banko, M., Cafarella, M., Soderland, S., 
Broadhead, M., Etzioni, O. 2007. Open Infor-
mation Extraction from the Web. IJCAI 2007. 
2. Barker, K. 2007. Building Models by Reading 
Texts. Invited talk at the AAAI 2007 Spring 
Symposium on Machine Reading, Stanford 
University. 
3. Clark, P. and Harrison, P. 2009. Large-scale 
extraction and use of knowledge from text. 
The Fifth International Conference on Knowl-
edge Capture (K-CAP 2009). 
http://www.cs.utexas.edu/users/pclark/dart/ 
4. Hobbs, J.R., Stickel, M., Appelt, D. and Mar-
tin, P., 1993. Interpretation as Abduction. Arti-
ficial Intelligence, Vol. 63, Nos. 1-2, pp. 69-
142.
http://www.isi.edu/~hobbs/interp-abduct-ai.pdf 
5. Klein, D. and Manning, C.D. 2003. Accurate 
Unlexicalized Parsing. Proceedings of the 41st 
Meeting of the Association for Computational 
Linguistics, pp. 423-430 
6. Marneffe, M. and Manning, C.D. 2008. The 
Stanford typed dependencies representation. In 
COLING 2008 Workshop on Cross-framework 
and Cross-domain Parser Evaluation. 
7. Sperber, D. and Wilson, D. 1995. Relevance: 
Communication and cognition (2nd ed.) Ox-
ford, Blackwell. 
8. Van Durme, B., Schubert, L. 2008. Open 
Knowledge Extraction through Compositional 
Language Processing. Symposium on Seman-
tics in Systems for Text Processing, STEP 
2008. 
23
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 43?47,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Detecting compositionality using semantic
vector space models based on syntactic context.
Shared task system description?
Guillermo Garrido
NLP & IR Group at UNED
Madrid, Spain
ggarrido@lsi.uned.es
Anselmo Pen?as
NLP & IR Group at UNED
Madrid, Spain
anselmo@lsi.uned.es
Abstract
This paper reports on the participation of the
NLP GROUP at UNED in the DiSCo?2011
compositionality evaluation task. The aim of
the task is to predict compositionality judge-
ments assigned by human raters to candidate
phrases, in English and German, from three
common grammatical relations: adjective-
noun, subject-verb and subject-object.
Our participation is restricted to adjective-
noun relations in English. We explore the
use of syntactic-based contexts obtained from
large corpora to build classifiers that model
the compositionality of the semantics of such
pairs.
1 Introduction
This paper reports on the NLP GROUP at UNED ?s
participation in DiSCo?2011 Shared Task. We at-
tempt to model the notion of compositionality from
analyzing language use in large corpora. In doing
this, we are assuming the distributional hypothesis:
words that occur in similar contexts tend to have
similar meanings (Harris, 1954). For a review of
the field, see (Turney and Pantel, 2010).
1.1 Approach
In previous approaches to compositionality detec-
tion, different kinds of information have been used:
morphological, lexical, syntactic, and distributional.
? This work has been partially supported by the Spanish
Ministry of Science and Innovation, through the project Holo-
pedia (TIN2010-21128-C02), and the Regional Government of
Madrid, through the project MA2VICMR (S2009/TIC1542).
For our participation, we are interested in exploring,
exclusively, the reach of pure syntactic information
to explain semantics.
Our approach draws from the Background
Knowledge Base representation of texts introduced
in (Pen?as and Hovy, 2010). We hypothesize that
behind syntactic dependencies in natural language
there are semantic relations; and that syntactic con-
texts can be leveraged to represent meaning, particu-
larly of nouns. A system could learn these semantic
relations from large quantities of natural language
text, to build an independent semantic resource, a
Background Knowledge Base (BKB) (Pen?as and
Hovy, 2010).
From a dependency-parsed corpus, we automat-
ically harvest meaning-bearing patterns, matching
the dependency trees to a set of pre-specified syn-
tactic patterns, similarly to (Pado and Lapata, 2007).
Patterns are matched to dependency trees to produce
propositions, carriers of minimal semantic units.
Their frequency in the collection is the fundamen-
tal source of our representation.
Our participation, due to time constraints, is re-
stricted to adjective-noun pairs in English.
2 System Description
Our hypothesis can be spelled out as: words (or
word compounds) with similar syntactic contexts are
semantically similar.
The intuition behind our approach is that non-
compositional compounds are units of meaning.
Then, the meaning of an adjective-noun combina-
tion that is not compositional should be different
from the meaning of the noun alone; for similar
43
approaches, see (Baldwin et al, 2003; Katz and
Giesbrecht, 2006; Mitchell and Lapata, 2010). We
propose studying the distributional semantics of a
adjective-noun compound; in particular, we will rep-
resent it via its syntactic contexts.
2.1 Adjective-noun compounds
Given a particular adjective-noun compound, de-
noted ?a, n?, we want to measure its composition-
ality by comparing its syntactic contexts to those of
the noun: ?n?. After exploring the dataset we real-
ized that considering nouns alone introduced noise,
as contexts of the target and different meanings of
the noun might be hard to separate; in order to soften
this problem we decided to compare the occurrences
of the ?a, n? pair to those of the noun with a different
adjective.
Given a dependency-parsed corpus C, we denote
N the set of all nouns occurring in C and A the set of
all adjectives. An adjective-noun pair, ?a, n?, is an
occurrence in the dependency parse of the sentence
of an arc (a, n), where n is the governor of an adjec-
tival relation, with a as modifier. We define the com-
plementary of ?a, n? as the set of all adjective-noun
pairs with the same noun but a different adjective:
?ac, n? = {?b, n? such that b ? A, b 6= a}
In order to detect compositionality, we compare
the semantics of ?a, n? to those of its complemen-
tary ?ac, n?. We use syntactic context as the repre-
sentation of these compounds? semantics.
We call target pairs those ?a, n? in which we are
interested, as they appear in the training, validation,
or test sets for the task. For each of them, its com-
plementary target is: ?ac, n?.
We model the syntactic contexts of any ?a, n? pair
as a set of vectors in a set of vector spaces defined as
follows. After inspection of the corpus, and its de-
pendency parse annotation layer, we manually spec-
ified a few syntactic relations, which we consider
codify the relevant syntactic relations in which an
?a, n? takes part. For each of these syntactic rela-
tions, we built a vector space model, and we repre-
sented as a vector in it each of the target patterns,
and each of their respective complementary targets.
To compute compositionality of a target, we calcu-
lated the cosine similarity between the target vec-
tor and the target?s complementary vector. So, for
each syntactic relation, and for each target, we have
a value of its similarity to the complementary tar-
get. These similarity values are considered features,
from which to learn the compositionality of targets.
For results comparability, we used the PukWaC
corpus1 as dataset. PukWaC adds to UkWaC a layer
of syntactic dependency annotation. The corpus has
been POS-tagged and lemmatized with the TreeTag-
ger2. The dependency parse was done with Malt-
Parser (Nivre and Scholz, 2004).
2.2 Implementation details
We defined a set of 19 syntactic patterns that define
interesting relations in which an ?a, n? pair might
take part, trying to exploit the dependencies pro-
duced by the MaltParser (Nivre and Scholz, 2004),
including:
? Relations to a verb, other than the auxiliary to
be and to have: subject; object; indirect object;
subject of a passive construction; logical sub-
ject of a passive construction.
? The relations defined in the previous point, en-
riched with a noun that acts as the other element
of a [subject-verb-object] or [subject-passive
verb-logical subject] construction.
? Collapsed prepositional complexes.
? Noun complexes.
? As subject or object of the verb to be.
? Modified by a second adjective.
? As modifier of a possessive.
The paths were defined manually to match our in-
tuitions of which are the paths that best describe the
context of an ?a, n?pair, similarly to (Pado and Lap-
ata, 2007). For each of the patterns, the set of words
that are related through it to the target ?a, n? define
the target?s context.
For most of our processing, we used simple pro-
grams implemented in Prolog and Python. We im-
plemented Prolog programs to model the depen-
dency parsed sentences of the full PUkWaC corpus,
and to match and extract these patterns from them.
After an aggregating step, where proper nouns, num-
bers and dates are substituted by place-holder vari-
1Available at http://wacky.sslmit.unibo.
it
2http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
44
ables, they amount to over 16 million instances,
representing the syntactic relations in which every
?a, n? pair in the corpus takes part. In further pro-
cessing, only those that affect the target pairs, or the
nouns in them, have to be taken into account.
As described above, each pattern we have defined
yields a vector space, where each target and its com-
plementary are represented as a vector. The base
vectors of the vector space model for a pattern are
the words that are syntactic contexts, with that syn-
tactic pattern, of any target in the target set3.
The value of the coordinate for a target and a base
vector is the frequency of the context word as related
to the target by the pattern. All frequencies were
locally scaled using logarithms4.
For each syntactic pattern, and for each target
and complementary, we have two vectors, represent-
ing their meanings in the vector space distributional
model. The complementary vector, in particular,
represents the centroid (average) of the meanings of
all ?b, n? pairs, that share the noun with the target
but have a different adjective, b
We propose that a target will be more composi-
tional if its meaning is more similar to the meaning
of the centroid of its complementary, that codifies
the general meaning of that noun (whenever it ap-
pears with a different adjective).
For each syntactic pattern and target, we can com-
pute the cosine similarity to the complementary tar-
get, and obtain a value to use as a feature of the com-
positionality of the target. Those features will be
used to train a classifier, being the compositionality
score of each sample the label to be learnt.
We used RapidMiner5 (Mierswa et al, 2006) as
our Machine Learning framework. The classifiers
we have used, that are described below, are the im-
plementations available in RapidMiner.
3It would have been possible to consider a common vector
space, using all patterns as base vectors. We decided not to do
so after realising that a single similarity value for a target and
its complementary was not by itself a signal strong enough to
predict the compositionality score. A second objective was to
assess the relative importance of different syntactic contexts for
the task.
4We did not attempt any global weighting. We leave this for
future work.
5http://rapid-i.com
2.3 Feature selection
From the 19 original features, inspection of the cor-
relation to the compositionality score label showed
that some of them were not to be expected to have
much predictive power, while some of them were
too sparse in the collection.
We decided to perform feature selection previ-
ous to all subsequent learning steps. We used
RapidMiner genetic algorithm for feature selection6.
Among the patterns which features were not selected
were those where the ?a, n? pair appears in prepo-
sitional complexes, in noun complexes, as indirect
object, as subject or object of the verb to be, and as
subject of a possessive. Among those selected were
subject and objects of both active and passive con-
structions, and the object of possessives.
2.4 Runs description
Numeric scores For the numeric evaluation task,
we built a regression model by means of a SVM
classifier. We used RapidMiner?s implementation
of mySVMLearner (Ru?ping, 2000), that is based on
the optimization algorithm of SVM-light (Joachims,
1998). We used the default parameters for the clas-
sifier. A simple dot product kernel seemed to ob-
tain the best results in 10-fold cross validation over
the union of the provided train and validation re-
sults. For the three runs, we used identical settings,
optimizing different quality measures in each run:
absolute error (RUN SCORE-1), Pearson?s correla-
tion coefficient (RUN SCORE-2), and Spearman?s
rho (RUN SCORE-3). The choice of a SVM classifier
was motivated by the objective of learning a good
parametric classifier model. In initial experiments,
SVM showed to perform better than other possible
choices, like logistic regression. In hindsight, the
relatively small size of the dataset might be a reason
for the relatively poor results. Experimenting with
other approaches is left for future work.
Coarse scores For the coarse scoring, we decided
to build a different set of classifiers, that would learn
the nominal 3-valued compositionality label. The
classifiers built in our initial experiments turned out
6The mutation step switches features on and off, while the
crossover step interchanges used features. Selection is done
randomly. The algorithm used to evaluate each of the feature
subsets was a SVM identical as the one described below.
45
Run avg4 r ?
RUN-SCORE-1 16.395 0.483 0.487
RUN-SCORE-2 15.874 0.475 0.463
RUN-SCORE-3 16.318 0.494 0.486
baseline 17.857 ? ?
Table 1: TRAINING. Numeric score runs results on 10-fold
cross-validation for the training set. avg4: average absolute
error; r: Pearson?s correlation;?: Spearman?s rho.
Run avg4 r ?
RUN-SCORE-1 17.016 0.237 0.267
RUN-SCORE-2 17.180 0.217 0.219
RUN-SCORE-3 17.289 0.180 0.189
baseline 17.370 ? ?
Table 2: TEST. Numeric score runs for the test set. Only
for the en-ADJ-NN samples. avg4: average absolute error; r:
Pearson?s correlation;?: Spearman?s rho.
to lazily choose the most frequent class (?high?) for
most of the test samples. In an attempt to overcome
this situation and possibly learn non linearly separa-
ble classes, we tried neural network classifiers7. In
hindsight, from seeing the very poor performance of
this classifiers on the test set, it is clear that any per-
formance gains were due to over-fitting on the train-
ing set.
For RUN COARSE-2, we binned the numeric
scores obtained in RUN-SCORE-1, dividing the score
space in three equal sized parts; we decided not to
assume the same distribution of the three labels for
the training and test sets. The results were worse
than the numeric scores, due to the fact that the 3
classes are not equally sized.
2.5 Results
Results in the training phase For all our training,
we performed 10-fold cross validation. For refer-
ence, we report the results as evaluated by averag-
ing over the 10 splits of the union of the provided
training and validation set in Table 1. We compared
against a dummy baseline: return as constant score
the average of the scores in the training and valida-
7For RUN COARSE-1, we used AutoMLP (Breuel and
Shafait, 2010), an algorithm that learns a neural network, op-
timizing both the learning rate and number of hidden nodes of
the network. For RUN COARSE-3, we learnt a simple neural net-
work model, by means of a feed-forward neural network trained
by a backpropagation algorithm (multi-layer perceptron), with
a hidden layer with sigmoid type and size 8.
tion sample sets.
Disappointingly, the resulting classifiers seemed
to be quite lazy, yielding values significatively close
to the average of the compositionality label in the
training and validation set.
The AutoMNLP and neural network seemed to
perform reasonably, and better than other classifiers
we tried (e.g., SVM based). We were wary, though,
of the risk of having learnt an over-fitted model; un-
fortunately, the results on the test set confirmed that:
for instance, the accuracy of RUN-SCORE-3 for the
training set was 0.548, but for the test set it was only
0.327.
Results in the test phase After the task results
were distributed, we verified that our numeric score
runs, for the subtask en-ADJ-NN performed quite
well: fifth among the 17 valid submissions for the
subtask, using the average point difference as quality
measure. Nevertheless, in terms of ranking correla-
tion scores, our system performs presumably worse,
although separate correlation results for the en-ADJ-
NN subtask were not available to us at the time of
writing this report.
Our naive baseline turns out to be strong in terms
of average point score. Of course, the ranking corre-
lation of such a baseline is none; using ranking cor-
relation as quality measure would be more sensible,
given that it discards such a baseline.
3 Conclusions
We obtained modest results in the task. Our three
numeric runs obtained results very similar to each
other. Only taking part in the en-ADJ-NN subtask,
we obtained the 5th best of a total of 17 valid sys-
tems in average point difference. Nevertheless, in
terms ranking correlation scores, our systems seem
to perform worse. The modifications we tried to spe-
cialize for coarse scoring were unsuccessful, yield-
ing poor results.
A few conclusions we can draw at this moment
are: our system could benefit from global frequency
weighting schemes that we did not try but that have
shown to be successful in the past; the relatively
small size of the dataset has not allowed us to learn a
better classifier; finally, we believe the ranking cor-
relation quality measures are more sensible than the
point difference for this particular task.
46
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions: analysis, acquisition and treatment - Volume 18,
MWE ?03, pages 89?96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thomas Breuel and Faisal Shafait. 2010. Automlp: Sim-
ple, effective, fully automated learning rate and size
adjustment. In The Learning Workshop. Online, 4.
Zellig S. Harris. 1954. Distributional structure. Word,
pages 146?162.
Thorsten Joachims. 1998. Making large-scale svm learn-
ing practical. LS8-Report 24, Universita?t Dortmund,
LS VIII-Report.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin
Scholz, and Timm Euler. 2006. Yale: rapid prototyp-
ing for complex data mining tasks. In KDD?06, pages
935?940.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. COLING ?04.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
Based Construction of Semantic Space Models. Com-
putational Linguistics, 33(2):161?199, jun.
Anselmo Pen?as and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. pages
15?23, jun.
Stefan Ru?ping. 2000. mySVM-Manual.
http://www-ai.cs.uni-dortmund.de
/SOFTWARE/MYSVM/.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141?188.
47
