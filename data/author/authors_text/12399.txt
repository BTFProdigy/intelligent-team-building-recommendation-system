Proceedings of NAACL HLT 2009: Demonstrations, pages 9?12,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
 STAT: Speech Transcription Analysis Tool Stephen A. Kunath Program in Linguistics 3e4  George Mason University Fairfax, VA  22030 skunath@gmu.edu Steven H. Weinberger Program in Linguistics 3e4  George Mason University Fairfax, VA  22030 weinberg@gmu.edu  Abstract The Speech Transcription Analysis Tool (STAT) is an open source tool for aligning and comparing two phonetically transcribed texts of human speech.  The output analysis is a parameterized set of phonological differ-ences.  These differences are based upon a se-lectable set of binary phonetic features such as [voice], [continuant], [high], etc.  STAT was initially designed to provide sets of phonological speech patterns in the compari-sons of various English accents found in the Speech Accent Archive http://accent.gmu.edu, but its scope and utility expand to matters of language assessment, phonetic training, foren-sic linguistics, and speech recognition.  1 Introduction The theoretical and practical value of studying human accented speech is of interest to language teachers, linguists, and computational linguists.  It is also part of the research program behind the Speech Accent Archive (http://accent.gmu.edu) housed at George Mason University. The Archive is a growing database of English speech varieties that contains more than 1,100 samples of native and non-native speakers reading from the same English paragraph.  The non-native speakers of English come from more than 250 language back-grounds and include a variety of different levels of English speech abilities. The native samples dem-onstrate the various dialects of English speech from around the world.  All samples include pho-netic transcriptions, phonological generalizations, demographic and geographic information.  For comparison purposes, the Archive also includes 
phonetic sound inventories from more than 200 world languages so that researchers can perform various contrastive analyses and accented speech studies.  No matter how subtle an accent is, human lis-teners can immediately and automatically notice that speakers are different.  For example, Chinese speakers of English sound different from French speakers of English.  The Speech Accent Archive stores and presents data that specifies and codifies these speech differences at the phonetic segment level.  Trained human linguists compare a standard speech sample with phonetically transcribed speech samples from each (non-standard or non-native) speaker and distill from this analysis a set of phonological speech patterns (PSPs) for each speaker.  Essentially, the task is to discover the precise factors or features responsible for humans to categorize say, a Vietnamese speaker of English differently from a so-called standard English speaker. While such analyses are theoretically and practically valuable, the process of comparing two phonetically transcribed speech samples requires explicit training, is time-consuming, and is difficult to update. 2 Phonological Speech Patterns As an example of how we manually derive the PSPs for a non-native English speaker, we begin by comparing the narrow phonetic transcription of a ?standard? North American English sample (1), with a representative non-native speaker of English (here a Vietnamese speaker (2)): (1) [p?l ?ii?z k??l? st?l? ?sk? ?? b???? ?ii?z ????z w??? f???m ?? st?? s?ks spu ?unz ?v f??? sno? p?ii?z fa??v ??k sl ???bz ?v blu? ?ii?z ?n me?bi ? sn?k? f? h? b???? b??b wii ?l?so nii???? sm?l? p?l ??st?k? sne?k ? ?n? b??? t??? f????? f? ?? k???dz 
9
?ii k??n sk?uup? ?ii?z ????z ??nt? ???ii ???d? b???z ? ?? n wii w?l? ?o? miit h? w??nzde? ?t? ?? t???e??n ste????n]  (2) [pli kol? st?l? as x? t ?? b??? ?i ?????s w?d ? x?? f??m ? st ??? s?xs spu?n ?f f??? no? pi?z fai? t ??k ??sl?p? ? ?lu ?i?s e ?n me?bi ? sn?k? f? x?? b???? b?? wi ?l?s? ni?t ?? psm??l? pl?st?k snex ?n? bix t?? f??x f? ?? ki?s ?i k?e ??n sku? l? ??????s ?nt? t?i? ??d ? b??z ? ?n wi ? wil ?o mit? x? w?nz ?de? a ??s t?e??n ste???n]  Each of these phonetic transcriptions are con-structed by 3 to 4 trained linguists, and disagree-ments are settled by consensus. As is the case with all such transcriptions, they remain works in pro-gress. Two of these trained linguists do a pencil and paper word-by-word comparison of the two transcriptions in (1) and (2).  Their analysis of the data may find the following PSPs listed in (3):  (3) (a) final obstruent devoicing ([?i?s]) (b) non aspiration ([pi?z]) (c) final consonant deletion  ([pli]) (d) vowel epenthesis  ([??sl?p?]) (e) substitution of [x] for velars and glot-tals ([bix]) This is just a partial list.  Some speakers may have more, and some speakers may have less.   But the essential claim here is that each speaker?s English accent is the sum of their PSPs.   There are certain problems associated with this manual process.  Foremost among them is the cost and time to train linguists to perform uniform PSP analyses.  Analysts must know what to look for?they must decide what is important and what should be ignored.  This brings us to the second drawback of manual analysis: the lack of a quick and parameterized method of comparison. If researchers need to test hypotheses about ad-ditional but uncatalogued PSPs, or if they need to simply search for a defined subset of PSPs, addi-tional manual analyses are necessary. A third prob-lem appears in the proper selection of one arbitrary standard ?base? sample for the comparisons.  At times researchers may want to compare non-natives with American English native samples, and at other times they may need to compare non-
natives with British, or other varieties of native English.  This requires multiple manual compari-sons, and they take human time and energy.  Fi-nally, as mentioned above, narrow phonetic transcriptions may need to be modified as collabo-rators join the analysis.  But when these are changed, they necessitate concomitant change in the register of PSPs.   Automating PSP generation not only solves these problems, but also opens up new research possibilities.  3 An Automated System: Research Poten-tials We have developed a computational tool that will automatically compare two phonetically-transcribed speech samples and generate a set of PSPs describing the speech differences. Automat-ing the comparison process will be of great use to the archive and to any speech scientist who tran-scribes and analyzes spoken language. It will allow fast and pointed comparisons of any two phoneti-cally transcribed speech samples.  Instead of sim-ply comparing a ?standard? North American native speaker and a non-native speaker, it will be quite simple to perform many accent comparisons, in-cluding those between a native British English speaker and a non-native speaker. It will also be possible to quickly and easily derive a composite result.  That is, after a number of analyses, we can determine what a typical Russian speaker of Eng-lish will do with his vowels and consonants.  This promises to be a great empirical improvement over the pronouncements that are currently offered in the appendices of various ESL teacher-training textbooks.   For the analysis of individual speakers, this tool has direct use in matters of linguistic assessment.  It will be useful in the fields of ESL pronunciation assessment (Anderson-Hsieh, Johnson, and Kohler, 1992). These kinds of assessments will naturally lead to a theory of weighted PSPs.   The tool also serves as a fast and systematic method of checking human transcription accuracy and thereby facilitates better methods of phonetic transcription (Cucchiarini, 1996; Shriberg, Hinke, & Trost-Steffen, 1987).  Finally, the tool can provide a needed human factor diagnostic to guide research in spectro-
10
graphic speech analysis.  And because speech rec-ognition and speaker identification programs must ultimately deal with different accented speech, the results from the STAT analyses will contribute to this work (Bartkova & Jouvet, 2007; Deshpande, Chikkerur, & Govindaraju, 2005).  4 System Overview Linguists who transcribe speech into a phonetic representation may use a tool such as PRAAT, to play the audio source file and a text editor to input the transcription. The result is normally a Unicode text file that has an IPA transcription of the audio file. STAT provides linguists with an easy way to play back an audio source file and share it with other linguists. A key feature that STAT provides in addition to transcription tools is a mechanism to manage a corpus of phonetic transcriptions. Once a corpus of phonetic transcriptions is created, lin-guists can use STAT?s phonological speech pattern analysis tools to describe differences between dif-ferent speakers? accents. The STAT system incorporates several distinct components. Users interact with the system pri-marily via a web interface. All user interfaces are implemented with Ruby on Rails and various JavaScript libraries. Backend processes and algo-rithms are implemented in Java. An open source web application bundle including the front-end web interfaces and backend libraries will be made available as an open source library suitable for use in other applications in the future. We believe that the transcription alignment and speech pattern analysis components of STAT make it a unique tool for linguists studying speech processes.  4.1 Language Management The language management component of STAT provides basic transcribed audio corpus manage-ment. This module allows a user to define a new speaker source language, e.g. Japanese, and specify attributes of the language, e.g. a phonetic inven-tory. All transcriptions are then associated with a speaker source language. STAT offers robust search capabilities that allow a linguist to search by things such as speaker demographics, phonetic in-ventories, phonological speech processes, and speech quality assessments.   
  Figure 1: STAT provides an initial alignment and asso-ciated PSPs. Provided alignments and PSPs can be manually changed by a linguist, recomputed, and anno-tated. 4.2 Transcription Management Whenever a transcription is to be made by lin-guists, a new transcription record is created, asso-ciated with a source language, and the audio file is attached to the transcription record. Once the audio file has been made available, linguists are able to use a web interface to play the audio recording and create phonetic transcriptions. The transcription management interface then allows a senior linguist to adjudicate differences between transcriptions and select an authoritative transcription. 4.3 Transcription Alignment and Analysis Once an authoritative transcription for a speaker has been created a linguist can then compare the transcription with the previously transcribed speech of another speaker. This alignment process is the core of the system. The first stage of the comparison is to create a word and phone level alignment between the two transcriptions. The alignment is performed by our special implementa-tion of Kondrak?s phonetic alignment algorithm (Kondrak, 2000). The output from this part of the system is a complete phone-to-phone to alignment of two transcriptions. Figure 1 shows an example alignment with PSPs that a linguist is able to make adjustments to or mark correct. After alignment a linguist can perform an assessment of the speaker?s speech abilities and make other notes. To help linguists who do work with a variety of different languages and research needs, the settings for the phonemic cluster parser, phoneme distance measures, and alignment algorithm coefficient can 
11
be easily changed inside of STAT.  Linguists can also control the set of constraints used for the phonological speech patterns analysis.  4.4 Phonological Speech Pattern Analysis Once the transcription alignment has been com-pleted, the phonological speech pattern analysis can begin. This analysis evaluates all phonetic dif-ferences between the two transcriptions under analysis. These differences are then processed by our algorithm and used to determine unique phonological speech patterns. All potential phonological speech patterns are returned to the linguist for verification. As the system encounters and  stores more and more phonological speech pattern analyses for a particular language, general descriptions are made about peoples? accents from  a particular language background. 5 Future Work Our initial design of STAT uses manually deter-mined weights of phonological features used to align transcriptions and determine phonological speech processes. In the next major release of STAT we intend to integrate automated methods to propose weight settings based on language selec-tions. We are currently planning on integrating a spectrographic analysis mechanism that will allow for the transcriptions to be time synchronized with the original speech sample. After this we will be investigating the integration of several speaker ac-cent identification algorithms. We will also be in-vestigating applications of this tool to help speech pathologists in the identification and assessment of disordered speech patterns.  6 References Anderson-Hsieh, J., Johnson, R., & Kohler, K. (1992).  The relationship between native speaker judgments of non-native pronunciation and deviance in segmen-tals, prosody, and syllable structure. Language Learning, 42, 529-555. Bartkova, K., & Jouvet, D. (2007).  On using units trained on foreign data for improved multiple accent speech recognition. Speech Communication, 49, 836-846. 
Cucchiarini, C. (1996). Assessing transcription agree-ment: methodological aspects. Clinical Linguistics & Phonetics, 10, 131-155. Deshpande, S., Chikkerur, S., & Govindaraju, V. (2005). Accent classification in speech.  Proceedings of the 4th IEEE Workshop on Automatic Identification Advanced Technologies. Kondrak, G. (2000). A new algorithm for the alignment of phonetic sequences. In Proceedings of the First Conference on North American Chapter of the Asso-ciation For Computational Linguistics (Seattle, Washington, April 29 - May 04, 2000). ACM Inter-national Conference Proceeding Series, vol. 4. Morgan Kaufmann Publishers, San Francisco, CA, 288-295. Shriberg, L., Hinke, R., & Trost-Steffen, C. (1987). A procedure to select and train persons for narrow pho-netic transcription by consensus. Clinical Linguistics & Phonetics, 1, 171-189.  
12
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 168?171,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Wisdom of the Crowd?s Ear: Speech Accent Rating and Annotation with Amazon Mechanical Turk   Stephen A. Kunath Steven H. Weinberger Linguistics Department Georgetown University Washington, D.C. 20057 Program in Linguistics 3e4 George Mason University Fairfax, VA  22030 sak68@georgetown.edu weinberg@gmu.edu       Abstract Human listeners can almost instantaneously judge whether or not another speaker is part of their speech community. The basis of this judgment is the speaker?s accent. Even though humans judge speech accents with ease, it has been tremendously difficult to automatically evaluate and rate accents in any consistent manner. This paper describes an experiment using the Amazon Mechanical Turk to de-velop an automatic speech accent rating dataset.  1 Introduction In linguistics literature and especially in second language acquisition research, the evaluation of human speech accents relies on human judges. Whenever humans listen to the speech of others they are almost instantly able to determine whether the speaker is from the same language community. Indeed, much of the research in accent evaluation relies on native speakers to listen to samples of accented speech and rate the accent severity (An-derson-Hsieh,et. al., 1992; Cunningham-Anderson and Engstrand 1989; Gut, 2007; Koster and Koet 1993; Magen, 1998, Flege, 1995; Munro, 1995, 2001). Two problems arise from the use of this methodology.  One is that the purely linguistic judgments may be infiltrated by certain biases.  So for example, all other things being equal, some native English judges may interpret certain Viet-
namese accents as being more severe than say, Ital-ian accents when listening to the English uttered by speakers from these language backgrounds. The second, and more theoretically interesting problem, is that human judges make these ratings based upon some hidden, abstract knowledge of phonol-ogy.  The mystery of what this knowledge is and contains is real, for as Gut (2007) remarks, ??no exact, comprehensive and universally accepted definition of foreign accent exists? (p75).  The task of this linguistic and computational study is to aid in defining and uncovering this knowledge.  This study aims to develop a method for integrat-ing accent ratings and judgments from a large number of human listeners, provided through Amazon Mechanical Turk(MTurk), to construct a set of training data for an automated speaker accent evaluation system. This data and methodology will be a resource that accent researchers can utilize. It reflects the wisdom of the crowd?s ear to help de-termine the components of speech that different listeners use to rate the accentedness of non-native speakers.  2 Source Data This task required HIT workers to listen to and rate a selection of non-native English speech samples. The source of all the speech samples for this effort was George Mason University?s Speech Accent Archive (http://accent.gmu.edu). The Speech Ac-cent Archive was chosen because of the high qual-ity of samples as well as the fact that each speech 
168
sample had readings of the same elicitation para-graph. This elicitation paragraph was designed to include all of the phonological features considered part of native English speech. Additionally, narrow phonetic transcriptions and phonological generali-zations are available for each sample. Each speaker?s information record contains demographic information and language background information. Three native language groups were selected for this study: Arabic, Mandarin, and Russian. The motivation for this particular selection comes from the fact that each of these languages represents a different language family. These languages contain different phonetic inventories as well as phonological patterns.   3 HIT Description Our HIT consisted of three sections. The first sec-tion asked the worker to describe their own native language background and any foreign language knowledge or experience. Asking about native and foreign language experience allowed us to estimate possible rating bias arising from experience with second language phonology. The second section of the HIT included two rating tasks for use as a base-line and to help the workers get acclimated to the task. Each worker was asked to listen to two audio samples of speakers reading the same elicitation paragraph, one of a native English speaker and one of a native Spanish speaker who started learning English late in life. The rating scale used was a five point Likert scale. After completing the base-line question, workers began the third section and were then asked to listen to fifteen samples of non-native English speakers read the same elicitation paragraph. After listening to each sample the workers were asked to rate the accentedness of the speech on the five point Likert scale. The five-point scale rates native accent as a 1 and heavy accent as a 5. Workers were additionally asked to group each speech sample into different native language categories. For this question they were presented with 3 language family groups: A, B, and C. Based on their perception of each speech sample they would attempt to categorize the fifteen speakers into distinct groups native language groups.   
4 Worker Requirements and Cost Due to the type of questions contained in our HIT we came up with several worker requirements for the HIT. The first and most important requirement was that HIT workers be located inside of the USA so as to limit the number of non-native English speakers. This requirement also helped to increase the likelihood that the listener would be familiar with varieties of English speech accents common in America. Additionally, due to the size of the 
task we had a requirement that any worker must have at least a 65% approval record for previous HITs on other MTurk tasks. After looking at other-comparably difficult tasks we decided to offer our first HIT at $0.75. Subsequent HITs decreased the offered price to $0.50 for the task.  5 HIT Results Two HITs were issued for this task. Each HIT had 25 workers. Average time for each worker on this task was approximately 12.5 minutes. Initial data analysis showed that users correctly carried out the tasks. Baseline question results, shown in figure 1, indicated that virtually every worker agreed that the native English speaker sample was a native speaker of English. The ratings of the baseline Spanish showed that workers generally agreed that it was heavily accented speech. In addition to the 
 
 Figure 1. Mechanical Turk workers ratings of 2 baseline samples: English 1 and Spanish 11. The numbers on the horizontal axis represent the how native-like the speaker was rated. A (1) indicates that the speaker sounds like a native English speaker. A (5) indicates the presence of a heavy accent.            baseline samples 
169
high quality of baseline evaluations, workers con-sistently provided their own native and foreign language information.  Ratings of the speech samples in each question, as seen in Figure 2, showed relatively consistent evaluations across workers. A more detailed statis-tical analysis of inter-worker ratings and groupings is currently underway, but the initial statistical tests show that there was a consistent correlation be-tween certain phonological speech patterns and ratings of accentedness.   
6 Future Work This experiment has already provided a wealth of information on how human?s rate accents and how consistent those ratings are across a large number of listeners. Currently, we are integrating the ac-cent ratings with the phonetic transcriptions and the list of identified phonological speech processes to construct a set of features that are correlated with accent ratings. We have begun to capitalize on the Mechanical Turk paradigm and are con-structing a qualification test to help us better un-derstand inter-worker agreement on accent rating.  
  Figure 2. Workers accent ratings for all speech samples. The horizontal axis indicates the accentedness rating: (1) is a native English accent and (5) is heavily accented. The vertical axis indicates the number of HIT workers that provided the same rating for the sample. The numbers at the end of each language name represent the Speech Accent Archive sample id for the language, e.g. Mandarin.1 indicates that the sample was the Mandarin 1 speaker on the Archive. 
170
This qualification test will include a larger sample of Native English speech data as well as a broader selection of foreign accents. In this new qualifica-tion test workers will be presented with a scale to rate the speakers accent from native-like to heavily accented. Additionally, the user will be asked to group the samples into native language families.  Once the user passes this qualification test they will then be able to work on HITs that are consid-erably shorter than the original long-form HIT de-scribed in this paper. In the new HITs workers will listen to one or more speech samples at a time and both rate and, if required, attempt to group the sample relative to other speech samples. The selec-tion criteria for these new samples will be based on the presence of phonological speech processes that have the highest correlation with accent ratings.   Acknowledgments The authors would like to thank Amazon.com and the workshop organizers for providing MTurk credits to perform this research. References  Anderson-Hsieh, J., Johnson, R., & Kohler, K. 1992. The relationship between native speaker judgments of non-native pronunciation and deviance in segmen-tals, prosody, and syllable structure. Language Learning, 42, 529-555. Cunningham-Anderson, U., and Engstrand, E., 1989. Perceived strength and identity of foreign accent in Swedish. Phonetica, 46, 138-154.  Flege, James E., Murray J. Munro, and Ian R.A. MacKay (1995). Factors Affecting Strength of Per-ceived Foreign Accent in a Second Language. Jour-nal of the Acoustical Society of America, 97, 5, pp 3125-3134. Gut, U. 2007. Foreign Accent. In C. Muller, (ed.), Speaker Classification I. Berlin: Springer. Koster, C., and Koet, T. 1993. The evaluation of accent in the English of Dutchmen. Language Learning, 43, 1, 69-92. Lippi-Green, R. 1997. English with an Accent. New York: Routledge. Magen, H. 1998. The perception of foreign-accented speech. Journal of Phonetics, 26, 381-400. Munro, Murray J. (1995). Nonsegmental Factors in For-eign Accent: Ratings of Filtered Speech. Studies in Second Language Acquisition, 17, pp 17-34. Munro, Murray J. and Tracey M. Derwing (2001). Modeling Perceptions of the Accentedness and Com-
prehensibility of L2 Speech: The Role of Speaking Rate. Studies in Second Language Acquisition, 23, pp 451-468. Scovel, T. 1995. Differentiation, recognition, and identi-fication in the discrimination of foreign accents. In J.Archibald (ed), Phonological Acquisition and Phonological Theory.  Hillsdale, NJ: Lawrence Erl-baum.  
171
