Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 649?656,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Word Alignment in English-Hindi Parallel Corpus Using Recency-Vector
Approach: Some Studies
Niladri Chatterjee
Department of Mathematics
Indian Institute of Technology Delhi
Hauz Khas, New Delhi
INDIA - 110016
niladri iitd@yahoo.com
Saumya Agrawal
Department of Mathematics
Indian Institute of Technology
Kharagpur, West Bengal
INDIA - 721302
saumya agrawal2000@yahoo.co.in
Abstract
Word alignment using recency-vector
based approach has recently become pop-
ular. One major advantage of these tech-
niques is that unlike other approaches they
perform well even if the size of the par-
allel corpora is small. This makes these
algorithms worth-studying for languages
where resources are scarce. In this work
we studied the performance of two very
popular recency-vector based approaches,
proposed in (Fung and McKeown, 1994)
and (Somers, 1998), respectively, for word
alignment in English-Hindi parallel cor-
pus. But performance of the above al-
gorithms was not found to be satisfac-
tory. However, subsequent addition of
some new constraints improved the perfor-
mance of the recency-vector based align-
ment technique significantly for the said
corpus. The present paper discusses the
new version of the algorithm and its per-
formance in detail.
1 Introduction
Several approaches including statistical tech-
niques (Gale and Church, 1991; Brown et al,
1993), lexical techniques (Huang and Choi, 2000;
Tiedemann, 2003) and hybrid techniques (Ahren-
berg et al, 2000), have been pursued to design
schemes for word alignment which aims at estab-
lishing links between words of a source language
and a target language in a parallel corpus. All
these schemes rely heavily on rich linguistic re-
sources, either in the form of huge data of parallel
texts or various language/grammar related tools,
such as parser, tagger, morphological analyser etc.
Recency vector based approach has been pro-
posed as an alternative strategy for word align-
ment. Approaches based on recency vectors typ-
ically consider the positions of the word in the
corresponding texts rather than sentence bound-
aries. Two algorithms of this type can be found in
(Fung and McKeown, 1994) and (Somers, 1998).
The algorithms first compute the position vector
Vw for the word w in the text. Typically, Vw is
of the form ?p1p2 . . . pk?, where the pis indicate
the positions of the word w in a text T . A new
vector Rw, called the recency vector, is computed
using the position vector Vw, and is defined as
?p2?p1, p3?p2, . . . , pk?pk?1?. In order to com-
pute the alignment of a given word in the source
language text, the recency vector of the word is
compared with the recency vector of each target
language word and the similarity between them is
measured by computing a matching cost associ-
ated with the recency vectors using dynamic pro-
gramming. The target language word having the
least cost is selected as the aligned word.
The results given in the above references show
that the algorithms worked quite well in aligning
words in parallel corpora of language pairs con-
sisting of various European languages and Chi-
nese, Japanese, taken pair-wise. Precision of about
70% could be achieved using these algorithms.
The major advantage of this approach is that it can
work even on a relatively small dataset and it does
not rely on rich language resources.
The above advantage motivated us to study
the effectiveness of these algorithms for aligning
words in English-Hindi parallel texts. The corpus
used for this work is described in Table 1. It has
been made manually from three different sources:
children?s storybooks, English to Hindi translation
book material, and advertisements. We shall call
649
the three corpora as Storybook corpus, Sentence
corpus and Advertisement corpus, respectively.
2 Word Alignment Algorithm: Recency
Vector Based Approach
DK-vec algorithm given in (Fung and McKeown,
1994) uses the following dynamic programming
based approach to compute the matching cost
C(n,m) of two vectors v1 and v2 of lengths n and
m, respectively. The cost is calculated recursively
using the following formula,
C(i, j) = |(v1(i)? v2(j)|+min{C(i? 1, j),
C(i? 1, j ? 1), C(i, j ? 1)}
where i and j have values from 2 to n and 2 to
m respectively, n and m being the number of dis-
tinct words in source and target language corpus
respectively. Note that vl(k) denotes the kth entry
of the vector vl, for l = 1 and 2. The costs are
initialised as follows.
C(1, 1) = |v1(1)? v2(1)|;
C(i, 1) = |v1(i)? v2(1)|+ C(i? 1, 1);
C(1, j) = |v1(1)? v2(j)|+ C(1, j ? 1);
The word in the target language that has the
minimum normalized cost (C(n,m)/(n + m)) is
taken as the translation of the word considered in
the source text.
One major shortcoming of the above scheme is
its high computational complexity i.e. O(mn). A
variation of the above scheme has been proposed
in (Somers, 1998) which has a much lower com-
putational complexity O(min(m,n)). In this new
scheme, a distance called Levenshtein distance(S)
is successively measured using :
S = S +min{|v1(i+ 1)? v2(j)|,
|v1(i+1)?v2(j+1)|, |v1(i)?v2(j+1)|}
The word in the target text having the minimum
value of S (Levenshtein difference) is considered
to be the translation of the word in the source text.
2.1 Constraints Used in the Dynamic
Programming Algorithms
In order to reduce the complexity of the dynamic
programming algorithm certain constraints have
been proposed in (Fung and McKeown, 1994).
1. Starting Point Constraint: The constraint im-
posed is: |first-occurrence of source language
word (w1) - first-occurrence of target lan-
guage word w2| < 12?(length of the text).
2. Euclidean distance constraint: The con-
straint imposed is:?(m1 ?m2)2 + (s1 ? s2)2 < T , where mj
and sj are the mean and standard deviation,
respectively, of the recency vector of wj , j =
1 or 2. Here, T is some predefined threshold:
3. Length Constraint: The constraint imposed
is: 12 ? f2 < f1 < 2 ? f2, where f1 and f2 are
the frequencies of occurrence of w1 and w2,
in their respective texts.
2.2 Experiments with DK-vec Algorithm
The results of the application of this algorithm
have been very poor when applied on the three
English to Hindi parallel corpora mentioned above
without imposing any constraints.
We then experimented by varying the values of
the parameters in the constraints in order to ob-
serve their effects on the accuracy of alignment.
As was suggested in (Somers, 1998), we also ob-
served that the Euclidean distance constraint is
not very beneficial when the corpus size is small.
So this constraint has not been considered in our
subsequent experiments. Starting point constraint
imposes a range within which the search for the
matching word is restricted. Although Fung and
McKeown suggested the range to be half of the
length of the text, we felt that the optimum value
of this range will vary from text to text depend-
ing on the type of corpus, length ratio of the two
texts etc. Table 2 shows the results obtained on
applying the DK vec algorithm on Sentence cor-
pus for different lower values of range. Similar
results were obtained for the other two corpora.
The maximum increase observed in the F-score is
around 0.062 for the Sentence corpus, 0.03 for the
Story book corpus and 0.05 for the Advertisement
corpus. None of these improvements can be con-
sidered to be significant.
2.3 Experiments with Somers? Algorithm
The algorithm provided by Somers works by first
finding all the minimum score word pairs using
dynamic programming, and then applying three
filters Multiple Alignment Selection filter, Best
Alignment Score Selection filter and Frequency
Range constraint to the raw results to increase the
accuracy of alignment.
The Multiple Alignment Selection(MAS) filter
takes care of situations where a single target lan-
guage word is aligned with the number of source
650
Corpora English corpus Hindi corpus
Total words Distinct words Total words Distinct words
Storybook corpus 6545 1079 7381 1587
Sentence corpus 8541 1186 9070 1461
Advertisement corpus 3709 1307 4009 1410
Table 1: Details of English-Hindi Parallel Corpora
Range Available Proposed Correct P% R% F-score
50 516 430 34 7.91 6.59 0.077
150 516 481 51 10.60 09.88 0.102
250 516 506 98 19.37 18.99 0.192
500 516 514 100 19.46 19.38 0.194
700 516 515 94 18.25 18.22 0.182
800 516 515 108 20.97 20.93 0.209
900 516 515 88 17.09 17.05 0.171
1000 516 516 100 19.38 19.38 0.194
2000 516 516 81 15.70 15.70 0.157
4535 516 516 76 14.73 14.73 0.147
Table 2: Results of DK-vec Algorithm on Sentence Corpus for different range
language words. Somers has suggested that in
such cases only the word pair that has the mini-
mum alignment score should be considered. Table
3 provides results (see column F-score old) when
the raw output is passed through the MAS filters
for the three corpora. Note that for all the three
corpora a variety of frequency ranges have been
considered, and we have observed that the results
obtained are slightly better when the MAS filter
has been used.
The best F-score is obtained when frequency
range is high i.e. 100-150, 100-200. But here
the words are very few in number and are primar-
ily pronoun, determiner or conjunction which are
not significant from alignment perspective. Also,
it was observed that when medium frequency
ranges, such as 30-50, are used the best result,
in terms of precision, is around 20-28% for the
three corpora. However, since the corpus size is
small, here too the available and proposed aligned
word pairs are very few (below 25). Lower fre-
quency ranges (viz. 2-20 and its sub-ranges) re-
sult in the highest number of aligned pairs. We
noticd that these aligned word pairs are typically
verb, adjective, noun and adverb. But here too
the performance of the algorithm may be consid-
ered to be unsatisfactory. Although Somers has
recommended words in the frequency ranges 10-
30 to be considered for alignment, we have con-
sidered lower frequency words too in our experi-
ments. This is because the corpus size being small
we would otherwise have effectively overlooked
many small-frequency words (e.g. noun, verb,
adjective) that are significant from the alignment
point of view.
Somers has further observed that if the Best
Alignment Score Selection (BASS) filter is ap-
plied to yield the first few best results of alignment
the overall quality of the result improves. Figure
1 shows the results of the experiments done for
different alignment score cut-off without consid-
ering the Frequency Range constraint on the three
corpora. However, it was observed that the perfor-
mance of the algorithm reduced slightly on intro-
ducing this BASS filter.
The above experiments suggest that the perfor-
mance of the two algorithms is rather poor in the
context of English-Hindi parallel texts as com-
pared to other language pairs as shown by Fung
and Somers. In the following section we discuss
the reasons for the low recall and precision values.
2.4 Why Recall and Precision are Low
We observed that the primary reason for the poor
performance of the above algorithms in English
- Hindi context is the presence of multiple Hindi
equivalents for the same English word. This can
happen primarily due to three reasons:
651
Figure 1: Results of Somers? Algorithm and Improved approach for different score cut-off
Declension of Adjective: Declensions of adjec-
tives are not present in English grammar. No mor-
phological variation in adjectives takes place along
with the number and gender of the noun. But,
in Hindi, adjectives may have such declensions.
For example, the Hindi for ?black? is kaalaa when
the noun is masculine singular number (e.g. black
horse ? kaalaa ghodaa). But the Hindi translation
of ?black horses? is kaale ghode; whereas ?black
mare? is translated as kaalii ghodii. Thus the same
English word ?black? may have three Hindi equiv-
alents kaalaa, kaalii, and kale which are to be used
judiciously by considering the number and gender
of the noun concerned.
Declensions of Pronouns and Nouns: Nouns or
pronouns may also have different declensions de-
pending upon the case endings and/or the gender
and number of the object. For example, the same
English word ?my? may have different forms (e.g.
meraa, merii, mere) when translated in Hindi.
For illustration, while ?my book? is translated as
? merii kitaab, the translation of ?my name? is
meraa naam. This happens because naam is mas-
culine in Hindi, while kitaab is feminine. (Note
that in Hindi there is no concept of Neuter gen-
der). Similar declensions may be found with re-
spect to nouns too. For example, the Hindi equiv-
alent of the word ?hour? is ghantaa. In plural
form it becomes ghante (e.g. ?two hours? ? do
ghante). But when used in a prepositional phrase,
it becomes ghanto. Thus the Hindi translation for
?in two hours? is do ghanto mein.
Verb Morphology: Morphology of verbs in
Hindi depends upon the gender, number and per-
son of the subject. There are 11 possible suffixes
(e.g taa, te, tii, egaa) in Hindi that may be at-
tached to the root Verb to render morphological
variations. For illustration,
I read. ? main padtaa hoon (Masculine) but
main padtii hoon (Feminine)
You read. ? tum padte ho (Masculine) or
tum padtii ho (Feminine)
He will read. ? wah padegaa.
Due to the presence of multiple Hindi equiva-
lents, the frequencies of word occurrences differ
significantly, and thereby jeopardize the calcula-
tions. As a consequence, many English words are
wrongly aligned.
In the following section we describe certain
measures that we propose for improving the effi-
ciency of the recency vector based algorithms for
word alignment in English - Hindi parallel texts.
3 Improvements in Word Alignment
In order to take care of morphological variations,
we propose to use root words instead of various
declensions of the word. For the present work this
has been done manually for Hindi. However, al-
gorithms similar to Porter?s algorithm may be de-
veloped for Hindi too for cleaning a Hindi text of
morphological inflections (Ramanathan and Rao,
2003). The modified text, for both English and
Hindi, are then subjected to word alignment.
Table 4 gives the details about the root word
corpus used to improve the result of word align-
ment. Here the total number of words for the three
types of corpora is greater than the total number
of words in the original corpus (Table 1). This is
because of the presence of words like ?I?ll? in the
English corpus which have been taken as ?I shall?
in the root word corpus. Also words like Unkaa
have been taken as Un kaa in the Hindi root word
corpus, leading to an increase in the corpus size.
652
Since we observed (see Section 2.2) that Eu-
clidean distance constraint does not add signifi-
cantly to the performance, we propose not to use
this constraint for English-Hindi word alignment.
However, we propose to impose both frequency
range constraint and length constraint (see Sec-
tion 2.1 and Section 2.3). Instead of the starting
point constraint, we have introduced a new con-
straint, viz. segment constraint, to localise the
search for the matching words. The starting point
constraint expresses range in terms of number of
words. However, it has been observed (see sec-
tion 2.2) that the optimum value of the range varies
with the nature of text. Hence no value for range
may be identified that applies uniformly on differ-
ent corpora. Also for noisy corpora the segment
constraint is expected to yield better results as the
search here is localised better. The proposed seg-
ment constraint expresses range in terms of seg-
ments. In order to impose this constraint, first the
parallel texts are aligned at sentence level. The
search for a target language word is then restricted
to few segments above and below the current one.
Use of sententially aligned corpora for word
alignment has already been recommended in
(Brown et al, 1993). However, the requirement
there is quite stringent ? all the sentences are to
be correctly aligned. The segment constraint pro-
posed herein works well even if the text alignment
is not perfect. Use of roughly aligned corpora has
also been proposed in (Dagan and Gale, 1993) for
word alignment in bilingual corpora, where statis-
tical techniques have been used as the underlying
alignment scheme. In this work, the sentence level
alignment algorithm given in (Gale and Church,
1991) has been used for applying segment con-
straint. As shown in Table 5, the alignment ob-
tained using this algorithm is not very good (only
70% precision for Storybook corpus). The three
aligned root word corpora are then subjected to
segment constraint in our experiments.
Next important decision we need to take which
dynamic programming algorithm should be used.
Results shown in Section 2.2 and 2.3 demonstrate
that the performance of DK-vec algorithm and
Somers? algorithm are almost at par. Hence keep-
ing in view the improved computational complex-
ity, we choose to use Levenshtein distance as used
in Somers? algorithm for comparing recency vec-
tors. In the following subsection we discuss the
experimental results of the proposed approach.
3.1 Experimental Results and Comparison
with Existing algorithms
We have conducted experiments to determine the
number of segments above and below the current
segment that should be considered for searching
the match of a word for each corpus. In this re-
spect we define i-segment constraint in which the
search is restricted to the segments k ? i to k + i
of the target language corpus when the word un-
der consideration is in the segment k of the source
language corpus.
Evidently, the value of i depends on the accu-
racy of sentence alignment. Table 5 suggests that
the quality of alignment is different for the three
corpora that we considered. Due to the very high
precision and recall for Sentence corpus we have
restricted our search to the kth segment only, i.e.
the value of i is 0. However, since the results are
not so good for the Storybook and Advertisement
corpora we found after experimenting that the best
results were obtained when i was 1. During the
experiments it was observed that as the number
of segments was lowered or increased from the
optimum segment the accuracy of alignment de-
creased continuously by around 10% for low fre-
quency ranges for the three corpora and remained
almost same for high frequency ranges.
Table 3 shows the results obtained when seg-
ment constraint is applied on the three corpora
at optimum segment range for various frequency
ranges. A comparison between the F-score given
by algorithm in (Somers, 1998) (the column F-
score old in the table) and the F-score obtained
by applying the improved scheme (the column F-
score new in the table) indicate that the results
have improved significantly for low frequency
ranges.
It is observed that the accuracy of alignment for
almost 95% of the available words has increased
significantly. This accounts for words within low
frequency range of 2?40 for Sentence corpus, 2?
30 for Storybook corpus, and 2?20 for Advertise-
ment corpus. Also, most of the correct word pairs
given by the modified approach are verbs, adjec-
tives or nouns. Also it was observed that as the
noise in the corpus increased the results became
poorer. This accounts for the lowest F-score val-
ues for advertisement corpus. The Sentence cor-
pus, however, has been found to be the least noisy,
and highest precision and recall values were ob-
tained with this corpus.
653
Using Somers? second filter on each corpus for
the optimum segment we found that the results at
low scores were better as shown in Figure 1. The
word pairs obtained after applying the modified
approach can be used as anchor points for further
alignment as well as for vocabulary extraction. In
case of the Sentence corpus, best result for anchor
points for further alignment lies at the score cut
off 1000 where precision and recall are 86.88%
and 80.35% respectively. Hence F-score is 0.835
which is very high as compared to 0.173 obtained
by Somers? approach and indicates an improve-
ment of 382.65%. Also, here the number of cor-
rect word pairs is 198, whereas the algorithms in
(Fung and McKeown, 1994) and (Somers, 1998)
gave only 62 and 61 correct word pairs, respec-
tively. Hence the results are very useful for vo-
cabulary extraction as well. Similarly, Figure 2
and Figure 3 show significant improvements for
the other two corpora. At any score cut-off, the
modified approach gives better results than the al-
gorithms proposed in (Somers, 1998).
4 Conclusion
This paper focuses on developing suitable word
alignment schemes in parallel texts where the size
of the corpus is not large. In languages, where
rich linguistic tools are yet to be developed, or
available freely, such an algorithm may prove to
be beneficial for various NLP activities, such as,
vocabulary extraction, alignment etc. This work
considers word alignment in English - Hindi par-
allel corpus, where the size of the corpus used is
about 18 thousand words for English and 20 thou-
sand words for Hindi.
The paucity of the resources suggests that sta-
tistical techniques are not suitable for the task.
On the other hand, Lexicon-based approaches are
highly resource-dependent. As a consequence,
they could not be considered as suitable schemes.
Recency vector based approaches provide a suit-
able alternative. Variations of this approach have
already been used for word alignment in parallel
texts involving European languages and Chinese,
Japanese. However, our initial experiments with
these algorithms on English-Hindi did not produce
good results. In order to improve their perfor-
mances certain measures have been taken. The
proposed algorithm improved the performance
manifold. This approach can be used for word
alignment in language pairs like English-Hindi.
Since the available corpus size is rather small
we could not compare the results obtained with
various other word alignment algorithms proposed
in the literature. In particular we like to compare
the proposed scheme with the famous IBM mod-
els. We hope that with a much larger corpus size
we shall be able to make the necessary compar-
isons in near future.
References
L. Ahrenberg, M. Merkel, A. Sagvall Hein, and
J.Tiedemann. 2000. Evaluation of word alignment
systems. In Proc. 2nd International conference on
Linguistic resources and Evaluation (LREC-2000),
volume 3, pages 1255?1261, Athens, Greece.
P. Brown, S. A. Della Pietra, V. J. Della Pietra, , and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
K. W. Church Dagan, I. and W. A. Gale. 1993. Robust
bilingual word alignment for machine aided transla-
tion. In Proc. Workshop on Very Large Corpora:
Academic and Industrial Perspectives, pages 1?8,
Columbus, Ohio.
P. Fung and K. McKeown. 1994. Aligning noisy par-
allel corpora across language groups: Word pair fea-
ture matching by dynamic time warping. In Tech-
nology Partnerships for Crossing the Language Bar-
rier: Proc. First conference of the Association for
Machine Translation in the Americas, pages 81?88,
Columbia, Maryland.
W. A. Gale and K. W. Church. 1991. Identifying word
correspondences in parallel texts. In Proc. Fourth
DARPA Workshop on Speech and Natural Language,
pages 152?157. Morgan Kaufmann Publishers, Inc.
Jin-Xia Huang and Key-Sun Choi. 2000. Chinese ko-
rean word alignment based on linguistic compari-
son. In Proc. 38th annual meeting of the association
of computational linguistic, pages 392?399, Hong
Kong.
Ananthakrishnan Ramanathan and Durgesh D. Rao.
2003. A lightweight stemmer for hindi. In Proc.
Workshop of Computational Linguistics for South
Asian Languages -Expanding Synergies with Eu-
rope, EACL-2003, pages 42?48, Budapest, Hungary.
H Somers. 1998. Further experiments in bilingual text
alignment. International Journal of Corpus Linguis-
tics, 3:115?150.
Jo?rg Tiedemann. 2003. Combining clues word align-
ment. In Proc. 10th Conference of The European
Chapter of the Association for Computational Lin-
guistics, pages 339?346, Budapest, Hungary.
654
Segment Constraint: 0-segment (Sentence Corpus)
Frequency a p c P% R% F-score F-score %
range (new) (old) increase
2-5 285 181 141 77.90 49.74 0.61 0.118 416.90
3-5 147 108 81 75.00 55.10 0.64 0.169 278.69
3-10 211 152 119 78.29 56.40 0.61 0.168 263.10
5-20 146 103 79 76.70 54.12 0.64 0.216 196.29
10-20 49 35 29 82.86 59.18 0.69 0.233 196.14
20-30 19 12 9 75.00 47.37 0.58 0.270 114.62
30-50 14 8 6 75.00 42.86 0.55 0.229 140.17
40-50 4 2 2 100.00 50.00 0.67 0.222 201.80
50-100 15 12 8 66.67 53.33 0.59 0.392 50.51
100-200 6 5 5 100.00 83.33 0.91 0.91 -
200-300 3 3 3 100.00 100.00 1.00 1.00 -
Segment Constraint: 1-segment (Story book Corpus)
2-5 281 184 89 48.37 31.67 0.38 0.039 874.35
3-5 143 108 52 48.15 36.36 0.41 0.042 876.19
5-10 125 89 35 39.39 28.00 0.33 0.090 266.67
10-20 75 50 25 50.00 33.33 0.40 0.115 247.83
10-30 117 76 39 51.32 33.33 0.41 0.114 259.65
20-30 32 23 11 47.83 34.38 0.37 0.041 802.43
30-40 14 8 2 25.00 14.29 0.18 0.100 80
40-50 7 7 2 28.57 28.57 0.29 0.200 45.00
50-100 11 10 2 20.00 18.18 0.19 0.110 72.72
100-200 5 5 2 40.00 40.00 0.40 0.444 -
Segment Constraint: 1-segment (Advertisement Corpus)
2-5 411 250 67 26.80 16.30 0.20 0.035 471.43
3-5 189 145 41 28.28 21.69 0.25 0.073 242.47
3-10 237 172 48 27.91 20.03 0.23 0.075 206.67
5-20 107 73 27 36.99 25.23 0.30 0.141 112.77
10-20 31 22 6 27.27 19.35 0.23 0.229 4.37
10-30 40 28 8 32.14 22.50 0.26 0.247 5.26
30-40 3 2 1 50.00 33.33 0.40 0.222 80.18
30-50 3 2 1 50.00 33.33 0.40 0.222 80.18
50-100 4 3 1 33.33 25.00 0.29 0.178 60.60
100-200 2 2 0 0 0 - 1.000 -
Table 3: Comparison of experimental results with Segment Constraint on the three Engish-Hindi parallel
corpora
Corpora English corpus Hindi corpus
Total words Distinct words Total words Distinct words
Storybook corpus 6609 895 7606 1100
Advertisement corpus 3795 1213 4057 1198
Sentence corpus 8540 1012 9159 1152
Table 4: Experimental root word parallel corpora of English -Hindi
655
Different Corpora Actual alignment Alignment given Correct alignment R% P%
in text by system given by system
Advertisement corpus 323 358 253 78.32 70.68
Storybook corpus 609 546 476 78.16 87.18
Sentence corpus 4548 4548 4458 98.02 98.02
Table 5: Results of Church and Gale Algorithm for Sentence level Alignment
Figure 2: Alignment Results for Sentence Corpus
Figure 3: Alignment Results for Story Book Corpus
656
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 301?308,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Parsing Aligned Parallel Corpus by Projecting Syntactic Relations from
Annotated Source Corpus
Shailly Goyal Niladri Chatterjee
Department of Mathematics
Indian Institute of Technology Delhi
Hauz Khas, New Delhi - 110 016, India
{shailly goyal, niladri iitd}@yahoo.com
Abstract
Example-based parsing has already been
proposed in literature. In particular, at-
tempts are being made to develop tech-
niques for language pairs where the source
and target languages are different, e.g.
Direct Projection Algorithm (Hwa et al,
2005). This enables one to develop parsed
corpus for target languages having fewer
linguistic tools with the help of a resource-
rich source language. The DPA algo-
rithm works on the assumption of Di-
rect Correspondence which simply means
that the relation between two words of
the source language sentence can be pro-
jected directly between the correspond-
ing words of the parallel target language
sentence. However, we find that this as-
sumption does not hold good all the time.
This leads to wrong parsed structure of the
target language sentence. As a solution
we propose an algorithm called pseudo
DPA (pDPA) that can work even if Direct
Correspondence assumption is not guaran-
teed. The proposed algorithm works in a
recursive manner by considering the em-
bedded phrase structures from outermost
level to the innermost. The present work
discusses the pDPA algorithm, and illus-
trates it with respect to English-Hindi lan-
guage pair. Link Grammar based pars-
ing has been considered as the underlying
parsing scheme for this work.
1 Introduction
Example-based approaches for developing parsers
have already been proposed in literature. These
approaches either use examples from the same lan-
guage, e.g., (Bod et al, 2003; Streiter, 2002), or
they try to imitate the parse of a given sentence
using the parse of the corresponding sentence in
some other language (Hwa et al, 2005; Yarowsky
and Ngai, 2001). In particular, Hwa et al (2005)
have proposed a scheme called direct projection
algorithm (DPA) which assumes that the relation
between two words in the source language sen-
tence is preserved across the corresponding words
in the parallel target language. This is called Di-
rect Correspondence Assumption (DCA).
However, with respect to Indian languages we
observed that the DCA does not hold good all the
time. In order to overcome the difficulty, in this
work, we propose an algorithm based on a vari-
ation of the DCA, which we call pseudo Direct
Correspondence Assumption (pDCA). Through
pDCA the syntactic knowledge can be transferred
even if not all syntactic relations may be projected
directly from the source language to the target lan-
guage in toto. Further, the proposed algorithm
projects the relations between phrases instead of
projecting relations between words. Keeping in
line with (Hwa et al, 2005), we call this algorithm
as pseudo Direct Projection Algorithm (pDPA).
The present work discusses the proposed pars-
ing scheme for a new (target) language with the
help of a parser that is already available for a
language (source) and using word-aligned paral-
lel corpus of the two languages under considera-
tion. We propose that the syntactic relationships
between the chunks of the input sentence T (of
the target language) are given depending upon the
relationships of the corresponding chunks in the
translation S of T . Along with the parsed struc-
ture of the input, the system also outputs the con-
stituent structure (phrases) of the given input sen-
301
tence.
In this work, we first discuss the proposed
scheme in a general framework. We illustrate the
scheme with respect to parsing of Hindi sentences
using the Link Grammar (LG) based parser for En-
glish and the experimental results are discussed.
Before that in the following section we discuss
Link Grammar briefly.
2 Link Grammar and Phrases
Link grammar (LG) is a theory of syntax which
builds simple relations between pairs of words,
rather than constructing constituents in tree-like
hierarchy. For example, in an SVO language like
English, the verb forms a subject link (S-) to some
word on its left, and an object link (O+) with some
word on its right. Nouns make the subject link
(S+) to some word (verb) on its right, or object
link (O-) to some word on its left.
The English Link Grammar Parser (Sleator and
Temperley, 1991) is a syntactic parser of English
based on LG. Given a sentence, the system as-
signs to it a syntactic structure, which consists of
a set of labeled links connecting pairs of words.
The parser also produces a ?constituent? represen-
tation of a sentence (showing noun phrases, verb
phrases, etc.). It is a dictionary-based system in
which each word in the dictionary is associated
with a set of links. Most of the links have some
associated suffixes to provide various information
(e.g., gender (m/f), number (s/p)), describing
some properties of the underlying word. The En-
glish link parser lists total of 107 links. Table
1 gives a list of some important links of English
LG along with the information about the words on
their left/right and some suffixes.
Link Word in Left Word in Right Suffixes
A Premodifier Noun -
D Determiners Nouns s/m,c/u
J Preposition Object of the prepo-
sition
s/p
M Noun Post-nominal Modi-
fier
p/v/g/a
MV Verbs/adjectives Modifying phrase p/a/i/
l/x
O Transitive verb Direct or indirect ob-
ject
s/p
P Forms of ?be? Complement of ?be? p/v/g/a
PP Forms of ?have? Past participle -
S Subject Finite verb s/p, i, g
Table 1: Some English Links and Their Suffixes
As an example, consider the syntactic struc-
ture and constituent representation of the sentence
given below.
+--------Ss--------+
| +----Jp---+ |
+--Ds-+-Mp-+ +-Dmc-+ +-Pa-+
| | | | | | |
the teacher of the boys is good
(S (NP (NP The teacher)
(PP of (NP the boys)))
(VP is)
(ADJP good).)
It may be noted that in the phrase structure of
the above sentence, verb phrase as obtained from
the phrase parser has been modified to some ex-
tent. The algorithm discussed in this work as-
sumes verb phrases as the main verb along with
all the auxiliary verbs.
For ease of presentation and understanding, we
classify phrase relations as Inter-Phrase and Intra-
phrase relations. Since the phrases are often em-
bedded, different levels of phrase relations are ob-
tained. From the outermost level to the innermost,
we call them as ?first level?, ?second level? of re-
lations and so on. One should note that an ith level
Intra-phrase relation may become Inter-phrase re-
lation at a higher level.
As an example, consider the parsing and phrase
structure of the English sentence given above.
In the first level the Inter-phrase relations (cor-
responding to the phrases ?the teacher of
the boys?, ?is? and ?good?) are Ss and Pa
and the remaining links are Intra-phrase relations.
In the second level the only Inter-phrase rela-
tionship is Mp (connecting ?the teacher? and
?the boys?), and the Intra-phrase relations are
Ds, Jp and Dmc. In third and the last level, Jp is
the Inter-phrase relationship and Dmc is the Intra-
phrase relation (corresponding to ?of? and ?the
boys?).
The algorithm proposed in Section 4 uses
pDCA to first establish the relations of the tar-
get language corresponding to the first-level Inter-
phrase relations of the source language sentence.
Then recursively it assigns the relations corre-
sponding to the inner level relations.
3 DCA vis-a`-vis pDCA
Direct Correspondence Assumption (DCA) states
that the relation between words in source language
sentence can be projected as the relations between
corresponding words in the (literal) translation in
the target language. Direct Projection Algorithm
302
(DPA), which is based on DCA, is a straightfor-
ward projection procedure in which the dependen-
cies in an English sentence are projected to the
sentence?s translation, using the word-level align-
ments as a bridge. DPA also uses some monolin-
gual knowledge specific to the projected-to lan-
guage. This knowledge is applied in the form of
Post-Projection transformation.
However with respect to many language pairs
syntactic relationships between the words cannot
always be imitated to project a parse structure
from source language to target language. For il-
lustration consider the sentence given in Figure 1.
We try to project the links from English to Hindi
in Figure 1(a) and Hindi to Bangla in Figure 1(b).
For Hindi sentence, links are given as discussed by
Goyal and Chatterjee (2005a; 2005b).
(a)
(b)
Figure 1: Failure of DCA
We observe that in the parse structure of the tar-
get language sentences, neither all relations are
correct nor the parse tree is complete. Thus, we
observe that DPA leads to, if not wrong, a very
shallow parse structure. Further, Figure 1(b) sug-
gests that DCA fails not only for languages be-
longing to different families (English-Hindi), but
also for languages belonging to the same family
(Hindi-Bangla).
Hence it is necessary that the parsing algo-
rithm should be able to differentiate between the
links which can be projected directly and the
links which cannot. Further it needs to identify
the chunks of the target language sentence that
cannot be linked even after projecting the links
from the source language sentence. Thus we pro-
pose pseudo Direct Correspondence Assumption
(pDCA) where not all relations can be projected
directly. The projection algorithm needs to take
care of the following three categories of links:
Category 1: Relationship between two chunks
in the source language can be projected to the tar-
get language with minor or no changes (for ex-
ample, subject-verb, object-verb relationships in
the above illustration). It may be noted that since
except for some suffix differences (due to mor-
phological variations), the relation is same in the
source and the target language.
Category 2: Relationship between two chunks
in the source language can be projected to the
target language with major changes. For ex-
ample, in the English sentence given in Figure
2(a), the relationship between the girl and in
the white dress is Mp, i.e. ?nominal mod-
ifier (preposition phrase)?. In the corresponding
phrases ladkii and safed kapde waalii of Hindi,
although the relationship is same, i.e., ?nominal
modifier?, the type of nominal modifier is chang-
ing to waalaa/waale/waalii-adjective. If the dis-
tinction between the types of nominal modifiers is
not maintained, the parsing will be very shallow.
Hence the modification in the link is necessary.
Category 3: Relationship between two chunks
in the target language is either entirely different
or can not be captured from the relationship be-
tween the corresponding chunk(s) in the source
language. For example, the relationship between
the main verb and the auxiliary verb of the Hindi
sentence in Figure 2(a) can not be defined us-
ing the English parsing. Such phrases should be
parsed independently.
The proposed algorithm is based on the above-
described concept of pDCA which gives the parse
structure of the sentences given in Fig. 2.
While working with Indian languages, we found
that outermost Inter-phrase relations usually be-
long to Category 1, and remaining relations be-
long to Category 2. Generally an innermost Intra-
phrase relation (like verb phrase) belongs to Cate-
gory 3. Thus, outermost Inter-phrase relations can
usually be projected to target language directly, in-
nermost Intra-phrase relations for the target lan-
guage which are independent of the source lan-
guage should be decided on the basis of language
specific study and remaining relationship should
303
(a)
(b)
Figure 2: Parsing Using pDCA
be modified before projection from source to tar-
get language.
4 The Proposed Algorithm
DPA (Hwa et al, 2005) discusses projection pro-
cedure for five different cases of word align-
ment of source-target language: one-to-one, one-
to-none, one-to-many, many-to-one and many-to-
many. As discussed earlier, DPA is not sufficient
for many cases. For example, in case of one-to-
many alignment, the proposed solution is to first
create a new empty word that is set as head of
all multiply aligned words in target language sen-
tence, and then the relation is projected accord-
ingly. But, in such cases, relations between these
multiply-aligned words can not be given, and thus
the resulting parsing becomes shallow. The pro-
posed algorithm (pDPA) overcomes these short-
comings as well.
The pDPA works in the following way. It re-
cursively identifies the phrases of the target lan-
guage sentence, and assigns the links between
the two phrases/words of the target language sen-
tence by using the links between the correspond-
ing phrases/words in the source language sen-
tence. It may be noted that link between phrases
means link between the head words of the corre-
sponding phrases. Assignment of links starts from
the outermost level phrases. Syntactic relations
between the constituents of the target language
phrase(s) for which the syntactic structure does
not correspond with the corresponding phrase(s)
in the target language are given independently. A
list of link rules is maintained which keeps the in-
formation about modification(s) required in a link
while projecting from the source language to the
target language. These rules are limited to closed
category words, to parts of speech projected from
source language, or to easily enumerated lexical
categories.
Figure 3 describes the algorithm. The algorithm
takes an input sentence (T ) and the parsing and the
constituent structure of its parallel sentence (S).
Further S and T are assumed to be word-aligned.
Initially, S and T are passed to the module Project-
From(), which identifies the constituent phrases of
S and the relations between them. Then each set
of phrases and relations is passed to the module
ParseFrom(). ParseFrom() module takes as input
two source phrases/words, relation between them,
and corresponding target phrases. It projects the
corresponding relations in the target language sen-
tence T . ParseFromSpecial() module is required
if the relation between phrases of source language
can not be projected so directly to the target lan-
guage. Module Parse() assigns links between the
constituent words of the target language phrases
? P . Notations used in the algorithm are as fol-
lows:
? By T ? ? S? we mean that T ? is aligned with
S?, T ? and S? being some text in the target
and source language, respectively.
? Given a language, the head of a phrase is usu-
ally defined as the keyword of the phrase. For
example, for a verb phrase, the head word is
the main verb.
? P is the exhaustive set of target language
phrases for which Intra-phrase relations are
independent of the corresponding source lan-
guage phrases.
? Rule list R is the list of source-target lan-
guage specific rules which specifies the mod-
ifications in the source language relations to
be projected appropriately in the target lan-
guage.
? Given the parse and constituent structure of a
text S, ?ij = ?Si, Sj , L?, where L is the re-
lation between the constituent phrases/words
Si and Sj of S. ??ij = ?Ti, Tj?, Ti ? Si and
Tj ? Sj . Further, ?ij = ??ij ,??ij?.
304
ProjectFrom(S?, T ?): // S? is a source
// language sentence or phrase, T ? ? S?
{
IF T ? ?P
THEN Parse(T ?);
ELSE
S? = {S1, S2, . . . , Sn}; // Sis are
//constituent phrases/words of S?
T ? = {T1, T2, . . . , Tn} // Ti ? Si
Find all ?ij = ?Si, Sj , L? from S? and
corresponding ??ij = {Ti, Tj} from T ?;
?ij = ??ij ,??ij?
For all i, j, push (S ,?ij);
While !empty(S )
? = pop(S );
IF L /? L
THEN ParseFrom(?);
ELSE ParseFromSpecial(?);
}
Parse(T ?): // T ? is a target language phrase
{
Assign links between constituent words of T ?
using target language specific rules;
}
ParseFrom(?): // ? = ??,???;
// ? = ?S1, S2, L?; ?? = ?T1, T2?;
{
IF T1 6= {?} & T2 6= {?} THEN
Find head words t1 ? T1 and t2 ? T2;
Assign relation L? between t1 and t2; // L?
//is target language link corresponding
//to L identified using R
IF T1 is a phrase and not already parsed
THEN ProjectFrom(S1, T1);
IF T2 is a phrase and not already parsed
THEN ProjectFrom(S2, T2);
}
ParseFromSpecial(?): // ? = ??,???;
// ? = ?S1, S2, L?; ?? = ?T1, T2?;
{
Use target language specific rules to identify if
the relation between T1 and T2 is given by L?;
IF true THEN ParseFrom(?);
ELSE
Assign required relations using rules;
IF T1 is a phrase and not already parsed
THEN ProjectFrom(S1, T1);
IF T2 is a phrase and not already parsed
THEN ProjectFrom(S2, T2);
}
Figure 3: pseudo Direct Projection Algorithm
? S is a stack of ?ijs.
? L is the set of source language relations
whose occurrence in parse of some S? may
lead to different structure of T ?, where T ? ?
S?.
In the following sections we discuss in detail the
scheme for parsing Hindi sentences using parse
structure of the corresponding English sentence.
Along with the parse structure of the input, the
phrase structure is also obtained.
5 Case study: English to Hindi
Prior requirements for developing a parsing
scheme for the target language using the proposed
algorithm are: development of target language
links, word alignment technique, phrase identifi-
cation procedure, creation of rule set R, morpho-
logical analysis, development of ParseFromSpe-
cial() module. In this section we discuss these de-
tails for adapting a parser for Hindi using English
LG based parser.
Hindi Links. Goyal and Chatterjee (2005a;
2005b) have developed links for Hindi Link Gram-
mar along with their suffixes. Some of the Hindi
links are briefly discussed in the Table 2. It may
be noted that due to the free word order of Hindi,
direction can not be specified for some links, i.e.,
for such links ?Word in Left? and ?Word in Right?
(second and third column of Table 2) shall be read
as ?Word on one side? and ?Word on the other
side?, respectively.
Link Word in Left Word in Right Directed
S Subject Main verb NO
SN ne Main verb NO
O Object Main verb NO
J noun/pronoun postposition YES
MV verb modifier Main verb NO
MA Adjective Noun YES
ME aa-e-ii form of
verb
Noun YES
MW waalaa/waale/
waalii
Noun YES
PT taa-te-tii form of
verb
declension of
verb honaa
YES
D Determiner Head noun YES
Table 2: Some Hindi Links
Word Alignment. The algorithm requires that
the source and target language sentences are
word aligned. Some English-Hindi word align-
ment algorithms have already been developed, e.g.
305
(Aswani and Gaizauskas, 2005). However, for the
current implementation alignment has been done
manually with the help of an online English-Hindi
dictionary1.
Identification of Phrases and Head Words.
Verb Phrases. Corresponding to any main verb
vi present in the Hindi sentence, a verb phrase is
formed by considering all the auxiliary verbs fol-
lowing it. A list of Hindi auxiliary verbs, along
with the linkage requirements has been main-
tained. This list is used to identify and link verb
phrases. Main verb of the verb phrase is consid-
ered to be the head word.
Noun and Postposition2 Phrases. English NP
is translated in Hindi as either NP or PP3. Also,
English PP can be translated as either NP or PP. If
the Hindi noun is followed by any postposition,
then that postposition is attached with the noun
to get a PP. In this case the postposition is con-
sidered as the head. Hindi NP corresponding to
some English NP is the maximal span of the words
(in Hindi sentence) aligned with the words in the
corresponding English NP. The Hindi noun whose
English translation is involved in establishing the
Inter-phrase link is the head word. Note that if the
last word (noun) in this Hindi NP is followed by
any postposition (resulting in some PP), then that
postposition is also included in the NP concerned .
In this case the postposition is the head of the NP.
The system maintains a list of Hindi postpositions
to identify Hindi PPs.
For example, consider the translation pair the
lady in the room had cooked the
food? kamre (room) mein (in) baiThii huii (-)
aurat (lady) ne (-) khaanaa (food) banaayaa
(cooked) thaa (-).
The phrase structure of the English sen-
tence is (NP1 (NP2 the lady) (PP1
in (NP3 the room))) (V P1 had
cooked) (NP4 the food).
Here, some of the Hindi phrases are as follows:
kamre mein and aurat ne are identified as Hindi
PP corresponding to English PP1 and NP2. The
words mein and ne are considered as their head
words, respectively. Since the maximal span of
1www.sanskrit.gde.to/hindi/dict/eng-hin-itrans.html
2In Hindi prepositions are used immediately after the
noun. Thus, we refer to them as ?postposition?.
3PP for English is preposition phrase and for Hindi it
stands for postposition phrase.
translation of words of English NP1 is kamre mein
baiThii huii aurat which is followed by postposi-
tion ne, the Hindi phrase corresponding to NP1
is kamre mein baiThii huii aurat ne with ne as
the head word. As huii and thii, which follow
the verbs baiThii4 and banaayaa respectively, are
present in the auxiliary verb list, Hindi VPs are
obtained as baiThii huii and banaayaa thaa (cor-
responding to V P1).
Phrase Set P . Hindi verb phrase and postposi-
tion phrases are linked independent of the corre-
sponding phrases in the English sentence. Thus,
P = {V P, PP}.
Rule List R. Below we enlist some of the rules
defined for parsing Hindi sentences using the En-
glish links (E-links) of the parallel English sen-
tences. Note that these rules are dependent on the
target language.
Corresponding to E-link S: If the Hindi subject is
followed by ne, then the subject makes a Jn link
with ne, and ne makes an SN link with the verb.
Corresponding to E-link O: If the Hindi object is
followed by ko, then the object makes a Jk link
with ko, and ko makes an OK link with the verb.
Corresponding to E-links M, MX: English NPs
may have preposition phrase, present participle,
past participle or adjective as postnominal modi-
fiers which are translated as prenominal modifiers,
or as relative clause in Hindi. The structure of
postnominal modifier, however, may not be pre-
served in the Hindi sentence. If the sentence is not
complex, then the corresponding Hindi link may
be one of MA (adjective), MP (postposition phrase),
MT (present participle), ME (past participle), or MW
(waalaa/waale/waalii-adjective). An appropriate
link is to be assigned in Hindi sentence after iden-
tification of the structure of the nominal modifier.
These cases are handled in the module ParseFrom-
Special(). The segment of the module that handles
English Mp link is given in Figure 4.
Further, since morphological information of
Hindi words can not be always extracted using cor-
responding English sentence, a morphological an-
alyzer is required to extract the information5. For
the current implementation, morphological infor-
4We observe that English PP as postnominal modifier may
be translated as verbal prenominal modifier in Hindi and in
such cases some unaligned word is effectively a verb.
5For Hindi, some work is being carried out in this direc-
tion, e.g., http://ccat.sas.upenn.edu/plc/ tamilweb/hindi.html
306
ParseFromSpecial(?): // ? = ??,???;
// ? = ?S1, S2, L?; ?? = ?T1, T2?;
{
IF L = Mp THEN //S1 and S2 are NP and PP, resp.
IF T2 is followed by some verb, v, not aligned with
any word in S THEN
T3 = VP corresponding to v;
Parse(T3);
Find head word t1 ? T1;
Assign MT/ME link between v and t1;
Assign MVp link between postposition (in T2)
and v;
ProjectFrom(S1, T1); ProjectFrom(S2, T2);
ELSE
ParseFrom(?);
ELSE
Check for other cases of L;
}
Figure 4: ParseFromSpecial() for ?Mp? Link
mation is being extracted using some rules in sim-
pler cases, and manually for more complex cases.
5.1 Illustration with an Example
Consider the English sentence (S) the girl
in the room drew a picture, its parsed
and constituent structure as given in Figure 5. Fur-
ther, the corresponding Hindi sentence (T ), and
the word-alignment is also given.
Figure 5: An Example
The step-by-step parsing of the sentence as per
the pDPA is given below.
ProjectFrom(S, T ):
S = {S1, S2, S3}, where S1, S2, S3 are the
phrases the girl in the room, drew and
a picture, respectively. From the definition of
Hindi phrases, corresponding Ti?s are identified as
?kamre mein baithii laDkii ne?, ?banaayaa? and
?ek chitr?. From the parse structure of S, ??s are
obtained as ?12 = ??S1, S2,Ss?, ?T1, T2?? and
?23 = ??S2, S3,Os?, ?T2, T3??. These ??s are
pushed in the stack S and further processing is
done one-by-one for each of them. We show the
further process for the ?12.
Since Ss /? L , ParseFrom(?12) is executed.
ParseFrom(?12):
The algorithm identifies t1 = ne, t2 = banaayaa.
The Hindi link corresponding to Ss will be SN.
The module ProjectFrom(S1, T1) is then called.
ProjectFrom(S1, T1):
S1 = {S11, S12}, where S11 and S12 are the
girl and in the room, respectively. Corre-
sponding T11 and T12 are ladkii ne and kamre
mein. Thus, ? = ??S11, S12,Mp?, ?T11, T12??.
Since L = Mp ? L , ParseFromSpecial(?) is
called.
ParseFromSpecial(?): (Refer to Figure 4)
Since T2 is followed by an unaligned verb
baithii, the algorithm finds T3 as baithii, and
t1 as ne. It assigns ME link between baithii
and ne. Further, MVp link is assigned between
mein and baithii. Then ProjectFrom(S11, T11) and
ProjectFrom(S12, T12) are called. Since both T11
and T12 ? S , J and Jn links are assigned be-
tween constituent words of T11 and T12, respec-
tively, using Hindi-specific rules.
Similarly, ?23 is parsed.
The final parse and phrase structure of the sen-
tence are obtained as given in Figure 6.
Figure 6: Parsing of Example Sentence
6 Experimental Results
Currently the system can handle the following
types of phrases in different simple sentences.
Noun Phrase. There can be four basic elements
of an English NP6: determiner, pre-modifier, noun
(essential), post-modifier. The system can han-
dle any combination of the following: adjective,
noun, present participle or past participle as pre-
modifier, and adjective, present participle, past
participle or preposition phrase as post-modifier.
Note that some of these cases may be translated as
complex sentence in Hindi (e.g., (book on the
table ? jo kitaab mej par rakhii hai). We are
working upon such cases.
6Pronouns as NPs are simple.
307
Verb Phrase. The system can handle all the four
aspects (indefinite, continuous, perfect and perfect
continuous) for all three tenses. Other cases of
VPs (e.g., modals, passives, compound verbs) can
be handled easily by just identifying and putting
the corresponding auxiliary verbs and their link-
ing requirements in the auxiliary verb list.
Since the system is not fully automated yet, we
could not test our system on a large corpus. The
system has been tested on about 200 sentences
following the specific phrase structures mentioned
above. These sentences have been taken randomly
from translation books, stories books and adver-
tisement materials. These sentences were manu-
ally parsed and a total of 1347 links were obtained.
These links were compared with the system?s out-
put. Table 3 summarizes the findings.
Correct Links : 1254
Links with wrong suffix : 47
Wrong links : 22
Links missing : 31
Table 3: Experimental Results
After analyzing the results, we found that
? For some links, suffixes were wrong. This
was due to insufficiency of rules identifying
morphological information.
? Due to incompleteness of some cases of
ParseFromSpecial() module, some wrong
links were assigned. Also, some links which
should not have been projected, were pro-
jected in the Hindi sentence. We are working
towards exploring these cases in detail.
? Some links were found missing in the pars-
ing since corresponding sentence structures
are yet to be considered in the scheme.
7 Concluding Remarks
The present work focuses on development of Ex-
ample based parsing scheme for a pair of lan-
guages in general, and for English to Hindi in par-
ticular.
Although the current work is motivated by
(Hwa et al, 2005), the algorithm proposed herein
provides a more generalized version of the projec-
tion algorithm by making use of some target lan-
guage specific rules while projecting links. This
provide more flexibility in the projection algo-
rithm. The flexibility comes from the fact that un-
like DPA the algorithm can project links from the
source language to the target language even if the
translations are not literal. Use of rules at the pro-
jection level gives more robust parsing and reduces
the need of post-editing. The proposed scheme
should work for other target languages also pro-
vided the relevant rules can be identified. Fur-
ther, since LG can be converted to Dependency
Grammar (DG) (Sleator and Temperley, 1991),
this work can be easily extended for languages for
which DG implementation is available.
At present, we have focused on developing
parsing scheme for simple sentences. Work has to
be done to parse complex sentences. Once a size-
able parsed corpus is generated, it can be used for
developing the parser for a target language using
bootstrapping. We are currently working on these
lines for developing a Hindi parser.
References
Niraj Aswani and Robert Gaizauskas. 2005. A hy-
brid approach to aligning sentences and words in
English-Hindi parallel corpora. In ACL 2005 Work-
shop on Building and Using Parallel Texts: Data-
driven machine translation and Beyond.
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-Oriented Parsing. Stanford: CSLI Pub-
lications.
Shailly Goyal and Niladri Chatterjee. 2005a. Study of
Hindi noun phrase morphology for developing a link
grammar based parser. Language in India, 5.
Shailly Goyal and Niladri Chatterjee. 2005b. Towards
developing a link grammar based parser for Hindi.
In Proc. of Workshop on Morphology, Bombay.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across parallel
texts. Natural Language Engineering, 11(3):311?
325, September.
Daniel Sleator and Davy Temperley. 1991. Parsing
English with a link grammar. Computer Science
technical report CMU-CS-91-196, Carnegie Mellon
University, October.
Oliver Streiter. 2002. Abduction, induction and
memorizing in corpus-based parsing. In ESSLLI-
2002 Workshop on Machine Learning Approaches
in Computational Linguistics,, Trento, Italy.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In NAACL-2001,
pages 200?207.
308
