A System to Solve Language Tests for Second Grade Students 
Manami Saito 
Nagaoka University of Technology 
saito@nlp.nagaokaut.ac.jp 
Kazuhide Yamamoto 
Nagaoka University of Technology 
yamamoto@fw.ipsj.or.jp 
Satoshi Sekine 
New York University 
Language Craft 
sekine@cs.nyu.edu 
Hitoshi Isahara 
National Institute of Information and Com-
munications Technology 
isahara@nict.go.jp 
 
 
Abstract 
This paper describes a system which 
solves language tests for second grade 
students (7 years old). In Japan, there 
are materials for students to measure 
understanding of what they studied, 
just like SAT for high school students 
in US. We use textbooks for the stu-
dents as the target material of this study. 
Questions in the materials are classified 
into four types: questions about Chi-
nese character (Kanji), about word 
knowledge, reading comprehension, 
and composition. This program doesn?t 
resolve the composition and some other 
questions which are not easy to be im-
plemented in text forms. We built a 
subsystem for each finer type of ques-
tions. As a result, we achieved 55% - 
83% accuracy in answering questions 
in unseen materials. 
1 Introduction 
This paper describes a system which solves lan-
guage tests for second grade students (7 years 
old). We have the following two objections. 
First, we aim to realize the NLP technologies 
into the form which can be easily observed by 
ordinary people. It is difficult to evaluate NLP 
technology clearly by ordinary people. Thus, we 
set the target to answer second grade Japanese 
language test, as an example of intelligible ap-
plication to ordinary people. The ability of this 
program will be shown by scores which are fa-
miliar to ordinary people. 
Second aim is to observe the problems of the 
NLP technologies by degrading the level of tar-
get materials. Those of the current NLP research 
are usually difficult, such as newspapers or tech-
nological texts. They require high accuracy lan-
guage processing, complex world knowledge or 
semantic processing. The NLP problems would 
become more apparent when we degrade the 
target materials. Although questions for second 
grade students also require world knowledge, it 
is expected that the questions become simpler 
and are resolved without tangled techniques.  
2 Related Works 
Hirschman et al (1999) and Charniak et al 
(2000) proposed systems to solve ?Reading 
Comprehension.? Hirschman et al (1999) de-
veloped ?Deep Read,? which is a system to se-
lect sentences in the text which include answers 
to a question. In their experiments, the types of 
questions are limited to ?When,? ?Who? and so 
on. The system is basically an information re-
trieval system which selects a sentence, instead 
of a document, based on the bag-of-words 
method. That system retrieves the sentence con-
taining the answer at 30-40% of the time on the 
tests of third to sixth grade materials. In short, 
Deep Read is very restricted compared to our 
system. Charniak et al (2000) built a system 
improved over the Deep Read by giving more 
weights for verb and subject, and introduced 
heuristic rules for ?Why? question. Though, the 
essential target and method are the same as that 
of Deep Read. 
43
3 Question Classification 
First, we bought five language test books for 
second grade students and one of them, pub-
lished by KUMON, was used as a training text 
to develop our system. The other four books are 
referred occasionally. Second, we classified the 
questions in the training text into four types: 
questions about Chinese character (Kanji), ques-
tions on word knowledge, reading comprehen-
sion, and composition. We will call these types 
as major types. Each of the ?major types? is 
classified into several ?minor types.? Table 1 
shows four major types and their minor types. In 
practice, each minor type farther has different 
style of questions; such as description question, 
choice question, and true-false question. The 
questions can be classified into approximately 
100 categories. We observed that some ques-
tions in other books are mostly similar; however 
there are several questions which are not cov-
ered by the categories. 
 
Major type Minor type 
Kanji Reading, Writing, Radical, The 
order of writing, Classification 
Word knowl-
edge 
Katakana, How to use Kana, Ap-
propriate Noun, To fill blanks for 
Verb, Adjective, and Adjunct, 
Synonym, Antonym, Particle, 
Conjunction, Onomatopoeia, Po-
lite Expression, Punctuation mark
Reading com-
prehension 
Who, What, When, Where, How, 
Why question, Extract specific 
phrases, Progress order of a story, 
Prose and Verse  
Composition Constructing sentence, How to 
write composition 
Table 1. Question types 
4 Answering questions and evaluation 
result 
In this section, we describe the programs to 
solve the questions for each of the 100 catego-
ries and these evaluation results. First we classi-
fied questions. Some questions are difficult to 
cover by the system such as the stroke order of 
writing Kanji. For about 90% of all the question 
types other than such questions, we created pro-
grams for basically one or two categories of 
questions. There are 47 programs for the catego-
ries found in the training data. 
In each section of 4.1 to 4.3, we describe 
how to solve questions of typical types and the 
evaluation results. The evaluation results of the 
total system will be reported in the following 
section. 
Table 2 to 4 show the distributions of minor 
types in each major type, ?Kanji,? ?Word 
knowledge,? and ?Reading comprehension,? in 
the training data and the evaluation results. 
Training and evaluation data have no overlap.  
4.1 Kanji questions 
Most of the Kanji questions are categorized into 
?reading? and ?writing.? Morphological analysis 
is used in the questions of both reading and writ-
ing Kanji; we found that large corpus is effec-
tive to choose the answer from Kanji candidates 
given from dictionary. Table 2 shows it in detail. 
This system cannot answer to the questions 
which are asking the order of writing Kanji, be-
cause it is difficult to put it into digital format.  
The system made 7 errors out of 334 ques-
tions. The most of the questions are the errors in 
reading Kanji by morphological analysis. 
In particular, morphological analysis is the 
only effective method to answer questions on 
this type. It would be very helpful, if we had a 
large corpus considering reading information, 
but there is no such corpus.  
 
Training 
data 
Test data Ques-
tion type
The rate 
of Q in 
training 
data[%]
The used 
knowledge 
and tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Reading 27 Kanji dictionary, 
Morphological 
analysis 
 
96(100) 
 
6(8,8) 
Writing 61 Word diction-
ary, Large 
corpus 
 
220(222) 
 
63(66,66) 
Order of 
writing 
6 -  
0(20) 
 
0(0,2) 
Combi-
nation 
of Kanji 
parts 
3 -  
0(10) 
 
0(0,0) 
Classify 
Kanji 
3 Word diction-
ary, Thesaurus 
 
11(12) 
 
0(0,0) 
Total 100 - 327(364) 69(74,76) 
Table 2. Question types for Kanji 
4.2  Word knowledge questions 
The word knowledge question dealt with vo-
cabulary, different kinds of words and the struc-
ture of sentence. These don?t include Kanji 
questions and reading comprehension. Table 3 
shows different types of questions in this type. 
44
For the questions on antonym, the system can 
answer correctly by choosing most relevant an-
swer candidate using the large corpus out of 
multiple candidates found in the antonym dic-
tionary. 
The questions about synonyms ask relations 
of priority/inferiority between words and choos-
ing the word in a different group. These ques-
tions can usually be answered using thesaurus. 
Ex.1 shows a question about particle, Japa-
nese postposition, which asks to select the most 
appropriate particle for the sentence. 
The system produces all possible sentences 
with the particle choices, and finds most likely 
sentences in a corpus. In Ex.1, all combinations 
are not in a corpus, therefore shorter parts of the 
sentence are used to find in the corpus (e.g. ??
?? (1) ????, ???? (2) ????). In this 
case, the most frequent particle for (1) is ??? in 
a corpus, so this system outputs incorrect answer. 
Ex.1 [?/?/?]????()??????? 
?????? 
 Select particle which fits the sentence from 
{wo,to,ni} 
[1] ??? (1) ??? (2) ??? 
apple-(1) orange-(2) buy 
(1) correct=? (to) system=? (wo) 
(2) correct=? (wo) system=? (wo) 
 
The questions of Katakana can be answered 
mostly by the dictionary. The accuracy of this 
type is not so high, found in Table 2, because 
there are questions asking the origin of the 
words, most Katakana words in Japanese has a 
few origins: borrowed words, onomatopoeia, 
and others. Because we don?t have such knowl-
edge, we could not answer those questions.   
The questions of onomatopoeia include those 
shown in Ex.2. The system uses co-occurrence 
of words in the given sentence and each answer 
candidate to choose the correct answer in Ex.2, 
?????.? However, it was not chosen be-
cause the co-occurrence frequency of ????
?,? the word in the sentence, and ?????,? 
incorrect answer, is higher.  
Ex.2 ??? ???? ???? ??? 
? [ ] ?? ??????? ??????? 
Choose the most appropriate onomatopoeia 
(1) ??? ??? ???? ????  
????(A large object is rowing slowly) 
[??????????????] 
The questions of word knowledge are classi-
fied into 29 types. We made a subsystem for 
each type. As there are possibly more types in 
other books, making a subsystem for each type 
is very costly. One of the future directions of 
this study is to solve this problem. 
Training 
data 
Test data Question 
type 
The rate 
of Q in 
training 
data[%] 
The used 
knowledge and 
tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Anonym 18 Antonym 
dictionary, 
Large corpus 
26(27) 12(15,21) 
Synonym 11 Thesaurus 14(17) 34(44,83) 
Particle 19 Large corpus 25(28) 16(17,17) 
Katakana 25 Word diction-
ary, Morpho-
logical analysis 
18(37) 19(22,52) 
Onomato-
poeia 
19 Large corpus, 
Morphological 
analysis 
18(29) 16(20,31) 
Structure 
of sen-
tence 
5 Morphological 
analysis 
7(7) 20(22,22) 
How to 
use kana 
2 - 0(3) 0(0,19) 
Dictation 
of verb 
2 - 0(3) 0(0,0) 
Total 100 - 108(151) 117(140,245)
Table 3. Question types for Word knowledge 
4.3 Reading comprehension questions 
The reading comprehension questions need to 
read a story and answer questions on the story. 
We will describe five typical techniques that are 
used at different types of questions, shown in 
Table 4. 
Pattern matching (a) is used in questions to 
fill blanks in an expression which describes a 
part of the story. In general, the sequence of 
word used in the matching is the entire expres-
sion, but if no match was found, smaller por-
tions are used in the matching. 
Ex.3 Fill blanks in the expression 
Story?partial??????? ???? 
?? ?? ????????? ????  
?? ???? ?????? 
(In a few days, the flower withers and gradu-
ally changes its color to black.) 
Expression???? (1)?(2) ?? ????? 
(The flower (1) and change its color to (2).) 
Answer?(1) ???? (withers) 
(2) ???? (black) 
The effectiveness of this technique is found 
in this example. The other methods will be 
needed when questions will be more difficult. At 
the time, this technique is very useful to solve 
many questions in reading comprehension.  
45
When the question is ?Why? question, key-
words such as ????  (thus)? and ????
(because)? are used. 
For example, when questions start with 
?When (??)? and ?Where (??),? we can 
restrict the type of answer word to time or loca-
tion, respectively. If the question includes the 
expression of ????? (??? is a particle to 
indicate direction/specification), the answer is 
also likely to be expressed with ?ni? right after 
the location in the story. (The kind of NE 
(Named Entity) and particle right after word 
(b)) 
For the questions asking the time or location 
about the entire story, this system outputs the 
appropriate type of the word which appeared 
first in the story. Although there are mistakes 
due to morphological analysis and NE extraction, 
this technique is also consistently very useful.  
The technique which is partial matching 
with keywords (c) is used to seek an answer 
from story for ?how,? ?why? or ?of what? ques-
tions. Keywords from the question are used to 
locate the answer in the story. 
Ex.4 ???????? ?????????
? ???
Frequency in the large corpus is used to 
find the appropriate sentence conjunction. (d) 
Answer is chosen by comparing the mutual in-
formation of the candidate conjunctions and the 
last basic block of the previous sentence. How-
ever, this technique turns out to be not effective. 
Discourse analysis considering wider context is 
required to solve this question. 
The technique which uses distance between 
keywords in question and answers (e) is sup-
plementary to the abovementioned methods. If 
multiple answers are found, the answer candi-
date that is the closest in the story text to the 
keywords in questions is generated. These key-
words are content words and unknown words in 
the text. This technique is found very effective. 
In Table 4, the column ?Used method? shows 
the techniques used to solve the type of ques-
tions, in the order of priority. ?f? in the table 
denotes means that we use a method which was 
not mentioned above.  
????(How big are chicks when 
they hatched?) 
Text?partial??????????? ??  
 ???????????? ??????? 
(The size of chicks when they hatched is about 
the size of your thumb.) 
 
 
 Answer?????? ???? (size of 
thumb)  
 
The rate of Training data Test data
questions in Used corrent wns. corrent ans.
training data [%] methods (total) (known type Q, total)
Who said 5 b,a,f
The others 0 b,e,f
Like what c,b,e
Of what c,f,e
What doing c,a,f
What is a,e
What do A say b,a,f,e
Whole story b
Part of story -
Whole story b
Part of story b,f,c
16 c,f 11(18) 0(1, 1)
10 c 8(11) 0(0, 1)
2 b,c,f 1(2) 0(0, 0)
10 a 10(12) 4(9, 9)
4 - 0(5) 0(0, 0)
2 d 1(2) 1(3, 3)
10 f 8(11) 0(0, 0)
10 f 7(12) 3(3, 3)
1 - 0(1) 0(0, 6)
100 - 74(116) 10(22, 34)
Why
How
How long, how often, how large
Total
Paragraph
The others
To fill blanks
Not have interrogative pronoun
Conjunction
Progress order of a story
Where 4 3(5) 0(1
When 4 3(5) 0(0
Who
Question type
17(26) 1(1, 6)What 22
5(6) 1(4, 4)
, 1)
, 0)
 
Table 4. Question types for Reading comprehension 
46
5 Evaluation 
We collected questions for the test from differ-
ent books of the training data. The proposition 
of the number of questions for different sections 
is not the same as that of the training data. Table 
2 to 4 show the evaluation results in the test data 
for each type. Table 5 shows the summary of the 
evaluation result. In the test, we use only the 
questions of the type in training data. The tables 
also show the total number of questions, the 
number of questions which are solved correctly, 
and the number of questions which are not one 
of the types the system targeted (not a type in 
the training data). 
The ratio of the questions covered by the sys-
tem, questions in test data which have the same 
type in the training data are 97.4% in Kanji, 
57.1% in word knowledge, and 64.7% in read-
ing comprehension. It indicates that about a half 
of the questions in word knowledge isn?t cov-
ered. As the result, accuracy on the questions of 
the covered types in word knowledge is 83.6%, 
but it drops to 47.8% for the entire questions. It 
is because our system classified the questions 
into many small types and builds a subsystem 
for each type. 
The accuracy for the questions of covered 
type is 83.4%. In particular, for the questions of 
Kanji and word knowledge, the scores in the test 
data are nearly the same as those in the training 
data. It presents that the accuracy of the system 
is provided that the question is in the covered 
type. However, the score of reading comprehen-
sion is lower in the test data. We believe that 
this is mainly due to the small test data of read-
ing comprehension (only 34) and that the accu-
racy for ?Who? questions and the questions to 
fill blanks in the test data are quite difficult com-
pared to the training data. 
Num. Num. Num. RCA * RCA* RCA*
of of of (known (total? in total
all Q known corrent type Q) [%] of known
type Q ans. [%] type Q [%]
Kanji 76 74 69 93.2 90.8 89.8
Word knowledge 245 140 117 83.6 47.8 71.5
Reading 
Comprehension
Total 355 235 196 83.4 55.2 80.3
45.5 29.4 63.834 22 10
 
Table 5. Evaluations at test data 
 
* Rate of Correct Answer 
6 Discussions 
We will discuss the problems and future direc-
tions found by the experiments. 
6.1 Recognition and Classification of Ques-
tions 
In order to solve a question in language test, 
students have to recognize the type of the ques-
tion. The current system skips this process. In 
this system, we set up about 100 types of ques-
tions referring the training data and a subpro-
gram solves questions corresponding to each 
type. There are two problems to be solved. First, 
we have to design the appropriate classification 
and avoid unknown types in the test data. From 
the experiment, we found that the current types 
are not enough to solve this problem. Second, 
the program has to classify the questions auto-
matically. We are building this system and are 
forecasting it quite optimistically once a good 
format is provided. 
6.2 Effectiveness of Large Corpus 
The large corpus of newspapers and the Web are 
used effectively in many different cases. We 
will describe several examples. 
In Japanese, there are different Kanji for the 
same reading. For example, Kanji for ???(au: 
to see, to solve correctly) are ???(to see)? or 
???(to solve correctly)? for ?????(to see 
people)? and ??????(to solve an answer 
correctly),? respectively. This type of questions 
can be solved by counting the expressions with 
Kanji in the corpus. It is similar to word sense 
disambiguation. 
In the questions of particle complement, such 
as ??? (umbrella) ??/?/? (locative-, con-
junctive-, and objective particles) ?? (home) 
??/?/?????? (to left) ? (Intentional 
sentence is ?I left the umbrella at home?)?, it can 
be solved by counting the expressions with each 
particle in a corpus. This method is mentioned in 
Matsui?2004?but the evaluation result was 
not reported. When the answer is not found for 
the entire expression, the answer is searched by 
deleting some contexts. Most questions of filling 
blank types, similar strategy is helpful to find 
the correct answer. 
In summary, the experiments showed that the 
large corpus is quite useful in several types of 
47
questions. We believe it would be quite difficult 
to achieve the same accuracy by compiled 
knowledge, such as a dictionary of verbs, anto-
nyms, synonyms, and relation words, and a the-
saurus.  
6.3 World Knowledge 
The questions sometimes need various types of 
world knowledge. For example, ?A student en-
ters junior high school after graduated from 
elementary school.? And ?People become happy, 
if he receives something nice from someone.? It 
is a difficult problem how to describe and how 
use that knowledge. Another type of world 
knowledge includes origin of words, such as 
foreign borrowed word or onomatopoeia. As far 
as we know, there is no comprehensive knowl-
edge of such in electronic form. It is required to 
design attributes of world knowledge and to use 
them flexibly when applying then to solve the 
questions. 
6.4 Difference between Reading Compre-
hension and Question Answering 
The current QA systems identify the NE type of 
questions and seek the answer candidate of the 
type. However, the questions in the reading 
comprehension don?t limit the answer types to 
person and organization, even if the question is 
?Who? type question. For example, ?raccoon 
dog behind our house? or ?the moon? can be the 
answer. Also, the answer is not always a noun 
phrase, but can be a clause, for example, ?the 
time when new leaves growing on a branch? for 
questions asking time. There are different kinds 
of questions, which are asking not the time of 
specific event but the time or season of the en-
tire story. For example ?When is this story 
about?? In this case, the question can?t be an-
swered by just extracting a noun phrase.  
However, at the moment, we can?t conclude 
if the question can or cannot be answered with-
out really understanding it. Sometime, we can 
find a correct answer without reading the story 
down the line or understanding the story per-
fectly. It is one of the future works. 
6.5 Other techniques: discourse and 
anaphora 
Some techniques other than morphological 
analysis, frequency of appearance in a corpus, 
and question answering methods are used in our 
system. We raise two issues. One of those is the 
discourse analysis. It is required in the questions 
to assign the order of paragraphs, and to select 
appropriate sentence conjunction. The other is 
anaphora analysis, which is very important, not 
only to indicate the antecedent, but also to find 
the link of mentions of entities.  
7 Conclusion 
 several inter-
esting NLP problems were found. 
Hi
Comprehension system?.  
Ch
er-based Language Understanding 
K.
of 
K. 
atural Language Processing, 2004, 
 
We develop a system to solve questions of sec-
ond grade language tests. Our objectives are to 
demonstrate the NLP technologies to ordinary 
people, and to observe the problems of NLP by 
degrading the level of target materials. We 
achieved 55% - 83% accuracy and
References 
rschman, L., Light, M., Breck, E. and Burger, J. D. 
?Deep READ: a Reading 
ACL, 1999, pp 325-332. 
arniak et al, ?Reading Comprehension Programs 
in a Statistical-language-Processing Class?. Work-
shop on Reading Comprehension Tests as Evalua-
tion for Comput
Systems. 2000. 
 Matsui: ?Search Technologies on WWW which 
utilize search engines?. (In Japanese) Journal 
Japanese Language, February, 2004, pp 34-43. 
Yoshihira, Y. Takeda, S. Sekine: ?KWIC System 
on WEB documents?, (In Japanese) 10th Annual 
Meeting of N
pp 137-139. 
 
F  
(http://languagecraft.jp/dennou/) 
igure 1. A Snapshot of the system
48
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 133?136,
New York, June 2006. c?2006 Association for Computational Linguistics
Using Phrasal Patterns to Identify Discourse Relations  
Manami Saito 
Nagaoka University of  
Technology 
Niigata, JP 9402188 
saito@nlp.nagaokaut.ac.jp 
Kazuhide Yamamoto 
Nagaoka University of 
Technology 
Niigata, JP 9402188 
yamamoto@fw.ipsj.or.jp
Satoshi Sekine 
New York University 
New York, NY 10003 
sekine@cs.nyu.edu 
 
Abstract 
This paper describes a system which 
identifies discourse relations between two 
successive sentences in Japanese. On top 
of the lexical information previously 
proposed, we used phrasal pattern 
information. Adding phrasal information 
improves the system's accuracy 12%, 
from 53% to 65%. 
1 Introduction 
Identifying discourse relations is important for 
many applications, such as text/conversation 
understanding, single/multi-document 
summarization and question answering. (Marcu 
and Echihabi 2002) proposed a method to identify 
discourse relations between text segments using 
Na?ve Bayes classifiers trained on a huge corpus. 
They showed that lexical pair information 
extracted from massive amounts of data can have a 
major impact. 
We developed a system which identifies the 
discourse relation between two successive 
sentences in Japanese. On top of the lexical 
information previously proposed, we added phrasal 
pattern information. A phrasal pattern includes at 
least three phrases (bunsetsu segments) from two 
sentences, where function words are mandatory 
and content words are optional. For example, if the 
first sentence is ?X should have done Y? and the 
second sentence is ?A did B?, then we found it 
very likely that the discourse relation is 
CONTRAST (89% in our Japanese corpus). 
2 Discourse Relation Definitions 
There have been many definitions of discourse 
relation, for example (Wolf 2005) and (Ichikawa 
1987) in Japanese. We basically used Ichikawa?s 
classes and categorized 167 cue phrases in the 
ChaSen dictionary (IPADIC, Ver.2.7.0), as shown 
in Table 1. Ambiguous cue phrases were 
categorized into multiple classes. There are 7 
classes, but the OTHER class will be ignored in the 
following experiment, as its frequency is very 
small. 
Table 1. Discourse relations 
Discourse     
relation 
Examples of cue phrase 
(English translation) 
Freq. in  
corpus [%]
ELABORATION and, also, then, moreover 43.0 
CONTRAST although, but, while 32.2 
CAUSE-
EFFECT 
because, and so, thus, 
therefore 12.1 
EQUIVALENCE in fact, alternatively, similarly 6.0 
CHANGE-
TOPIC 
by the way, incidentally, 
and now, meanwhile, well 5.1 
EXAMPLE for example, for instance 1.5 
OTHER most of all, in general 0.2 
 
3 Identification using Lexical Information 
The system has two components; one is to identify 
the discourse relation using lexical information, 
described in this section, and the other is to 
identify it using phrasal patterns, described in the 
next section. 
A pair of words in two consecutive sentences 
can be a clue to identify the discourse relation of 
those sentences. For example, the CONTRAST 
relation may hold between two sentences which 
133
have antonyms, such as ?ideal? and ?reality? in 
Example 1. Also, the EXAMPLE relation may 
hold when the second sentence has hyponyms of a 
word in the first sentence. For example, ?gift shop?, 
?department store?, and ?supermarket? are 
hyponyms of ?store? in Example 2.  
Ex1) 
a. It is ideal that people all over the world 
accept independence and associate on an 
equal footing with each other. 
b. (However,) Reality is not that simple. 
Ex2) 
a. Every town has many stores.  
b. (For example,) Gift shops, department 
stores, and supermarkets are the main 
stores. 
 
In our experiment, we used a corpus from the 
Web (about 20G of text) and 38 years of 
newspapers. We extracted pairs of sentences in 
which an unambiguous discourse cue phrase 
appears at the beginning of the second sentence. 
We extracted about 1,300,000 sentence pairs from 
the Web and about 150,000 pairs from newspapers.  
300 pairs (50 of each discourse relation) were set 
aside as a test corpus. 
3.1 Extracting Word Pairs 
Word pairs are extracted from two sentences; i.e. 
one word from each sentence. In order to reduce 
noise, the words are restricted to common nouns, 
verbal nouns, verbs, and adjectives. Also, the word 
pairs are restricted to particular kinds of POS 
combinations in order to reduce the impact of word 
pairs which are not expected to be useful in 
discourse relation identification. We confined the 
combinations to the pairs involving the same part 
of speech and those between verb and adjective, 
and between verb and verbal noun. 
All of the extracted word pairs are used in base 
form. In addition, each word is annotated with a 
positive or negative label. If a phrase segment 
includes negative words like ?not?, the words in 
the same segment are annotated with a negative 
label. Otherwise, words are annotated with a 
positive label. We don?t consider double negatives. 
In Example 1-b, ?simple? is annotated with a 
negative, as it includes ?not? in the same segment. 
3.2 Score Calculation 
All possible word pairs are extracted from the 
sentence pairs and the frequencies of pairs are 
counted for each discourse relation. For a new 
(test) sentence pair, two types of score are 
calculated for each discourse relation based on all 
of the word pairs found in the two sentences. The 
scores are given by formulas (1) and (2). Here 
Freq(dr, wp) is the frequency of word pair (wp) in 
the discourse relation (dr). Score1 is the fraction of 
the given discourse relation among all the word 
pairs in the sentences. Score2 incorporates an 
adjustment based on the rate (RateDR) of the 
discourse relation in the corpus, i.e. the third 
column in Table 1. The score actually compares 
the ratio of a discourse relation in the particular 
word pairs against the ratio in the entire corpus. It 
helps the low frequency discourse relations get 
better scores.  
 
( )
( )
?
?
=
wpdr
wp
wpdrFreq
wpDRFreq
DRScore
,
1 ),(
,
                 (1) 
 
( )
( )
DR
wpdr
wp
RatewpdrFreq
wpDRFreq
DRScore ?= ?
?
,
2 ),(
,
 (2) 
 
4 Identification using Phrasal Pattern 
We can sometimes identify the discourse relation 
between two sentences from fragments of the two 
sentences. For example, the CONTRAST relation 
is likely to hold between the pair of fragments ?? 
should have done ?.? and ?? did ?.?, and the 
EXAMPLE relation is likely to hold between the 
pair of fragments ?There is?? and ?Those are ? 
and so on.?.  Here ??? represents any sequence of 
words. The above examples indicate that the 
discourse relation between two sentences can be 
recognized using fragments of the sentences even 
if there are no clues based on the sort of content 
words involved in the word pairs.  Accumulating 
such fragments in Japanese, we observe that these 
fragments actually form a phrasal pattern. A phrase 
(bunsetsu) in Japanese is a basic component of 
sentences, and consists of one or more content 
words and zero or more function words. We 
134
specify that a phrasal pattern contain at least three 
subphrases, with at least one from each sentence. 
Each subphrase contains the function words of the 
phrase, and may also include accompanying 
content words. We describe the method to create 
patterns in three steps using an example sentence 
pair (Example 3) which actually has the 
CONTRAST relation. 
Ex3)  
a. ?kanojo-no kokoro-ni donna omoi-ga at-ta-
ka-ha wakara-nai.? (No one knows what 
feeling she had in her mind.) 
b. ?sore-ha totemo yuuki-ga iru koto-dat-ta-
ni-chigai-nai.? (I think that she must have 
needed courage.) 
 
1) Deleting unnecessary phrases 
Noun modifiers using ?no? (a typical particle for a 
noun modifier) are excised from the sentences, as 
they are generally not useful to identify a discourse 
relation. For example, in the compound phrase 
?kanozyo-no (her) kokoro (mind)? in Example 3, 
the first phrase (her), which just modifies a noun 
(mind), is excised. Also, all of the phrases which 
modify excised phrases, and all but the last phrase 
in a conjunctive clause are excised.  
 
2) Restricting phrasal pattern 
In order to avoid meaningless phrases, we restrict 
the phrase participants to components matching the 
following regular expression pattern. Here, noun-x 
means all types of nouns except common nouns, i.e. 
verbal nouns, proper nouns, pronouns, etc. 
 
?(noun-x | verb | adjective)? (particle | auxiliary 
verb | period)+$?, or ?adverb$? 
 
3) Combining phrases and selecting words in a 
phrase 
All possible combinations of phrases including at 
least one phrase from each sentence and at least 
three phrases in total are extracted from a pair of 
sentences in order to build up phrasal patterns. For 
each phrase which satisfies the regular expression 
in 2), the subphrases to be used in phrasal patterns 
are selected based on the following four criteria (A 
to D). In each criterion, a sample of the result 
pattern (using all the phrases in Example 3) is 
expressed in bold face. Note that it is quite difficult 
to translate those patterns into English as many 
function words in Japanese are encoded as a 
position in English. We hope readers understand 
the procedure intuitively. 
 
A) Use all components in each phrase 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
B) Remove verbal noun and proper noun 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
C) In addition, remove verb and adjective 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
D) In addition, remove adverb and remaining noun 
kanojo-no kokoro-ni donna omoi-ga at-ta-ka-ha wakara-nai. 
sore-ha totemo yuuki-ga iru koto-dat-ta-ni-chigai-nai. 
 
4.1 Score Calculation 
By taking combinations of 3 or more subphrases 
produced as described above, 348 distinct patterns 
can be created for the sentences in Example 3; all 
of them are counted with frequency 1 for the 
CONTRAST relation. Like the score calculation 
using lexical information, we count the frequency 
of patterns for each discourse relation over the 
entire corpus. Patterns appearing more than 1000 
times are not used, as those are found not useful to 
distinguish discourse relations. 
The scores are calculated replacing Freq(dr, 
wp) in formulas (1) and (2) by Freq(dr, pp). Here, 
pp is a phrasal pattern and Freq(dr, pp) is the 
number of times discourse relation dr connects 
sentences for which phrasal pattern pp is matched. 
These scores will be called Score3 and Score4, 
respectively. 
5 Evaluation 
The system identifies one of six discourse relations, 
described in Table 1, for a test sentence pair. Using 
the 300 sentence pairs set aside earlier (50 of each 
discourse relation type), we ran two experiments 
for comparison purposes: one using only lexical 
information, the other using phrasal patterns as 
well. In the experiment using only lexical 
information, the system selects the relation 
maximizing Score2 (this did better than Score1).  In 
the other, the system chooses a relation as follows: 
if one relation maximizes both Score1 and Score2, 
135
choose that relation; else, if one relation maximizes 
both Score3 and Score4, choose that relation; else 
choose the relation maximizing Score2. 
Table 2 shows the result. For all discourse relations, 
the results using phrasal patterns are better or the 
same. When we consider the frequency of 
discourse relations, i.e. 43% for ELABORATION, 
32% for CONTRAST etc., the weighted accuracy 
was 53% using only lexical information, which is 
comparable to the similar experiment by (Marcu 
and Echihabi 2002) of 49.7%. Using phrasal 
patterns, the accuracy improves 12% to 65%. Note 
that the baseline accuracy (by always selecting the 
most frequent relation) is 43%, so the improvement 
is significant. 
Table 2. The result 
Discourse relation Lexical info. Only 
With phrasal 
pattern 
ELABORATION 44% (22/50) 52% (26/50) 
CONTRAST 62% (31/50) 86% (43/50) 
CAUSE-EFFECT 56% (28/50) 56% (28/50) 
EQUIVALENCE 58% (29/50) 58% (29/50) 
CHANGE-TOPIC 66% (33/50) 72% (36/50) 
EXAMPLE 56% (28/50) 60% (30/50) 
Total 57% (171/300) 64% (192/300)
Weighted accuracy 53% 65% 
 
Since they are more frequent in the corpus, 
ELABORATION and CONTRAST are more 
likely to be selected by Score1 or Score3. But 
adjusting the influence of rate bias using Score2 
and Score4, it sometimes identifies the other 
relations.  
The system makes many mistakes, but people 
also may not be able to identify a discourse 
relation just using the two sentences if the cue 
phrase is deleted. We asked three human subjects 
(two of them are not authors of this paper) to do 
the same task. The total (un-weighted) accuracies 
are 63, 54 and 48%, which are about the same or 
even lower than the system performance. Note that 
the subjects are allowed to annotate more than one 
relation (Actually, they did it for 3% to 30% of the 
data). If the correct relation is included among 
their N choices, then 1/N is credited to the accuracy 
count. We measured inter annotator agreements. 
The average of the inter-annotator agreements is 
69%. We also measured the system performance 
on the data where all three subjects identified the 
correct relation, or two of them identified the 
correct relation and so on (Table 3). We can see 
the correlation between the number of subjects 
who answered correctly and the system accuracy. 
In short, we can observe from the result and the 
analyses that the system works as well as a human 
does under the condition that only two sentences 
can be read. 
Table 3. Accuracy for different agreements 
# of  subjects correct 3 2 1 0 
System accuracy 71% 63% 60% 47%
. 
6 Conclusion 
In this paper, we proposed a system which 
identifies discourse relations between two 
successive sentences in Japanese. On top of the 
lexical information previously proposed, we used 
phrasal pattern information. Using phrasal 
information improves accuracy 12%, from 53% to 
65%. The accuracy is comparable to human 
performance. There are many future directions, 
which include 1) applying other machine learning 
methods, 2) analyzing discourse relation 
categorization strategy, and 3) including a longer 
context beyond two sentences. 
Acknowledgements 
This research was partially supported by the 
National Science Foundation under Grant IIS-
00325657. This paper does not necessarily reflect 
the position of the U.S. Government. We would 
like to thank Prof. Ralph Grishman, New York 
University, who provided useful suggestions and 
discussions. 
References 
Daniel Marcu and Abdessamad Echihabi. 2002. An 
Unsupervised Approach to Recognizing Discourse 
Relations, Proceedings of the 40th Annual Meeting of 
the Association for Computational Linguistics, 368-
375. 
Florian Wolf and Edward Gibson. 2005. Representing 
Discourse Coherence: A Corpus-Based Study, 
Computational Linguistics, 31(2):249-287. 
Takashi Ichikawa. 1978. Syntactic Overview for 
Japanese Education, Kyo-iku publishing, 65-67 (in 
Japanese). 
136
