Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 483?489, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBM: Combining lexicon-based ML and heuristics
for Social Media Polarities
Carlos Rodr??guez-Penagos, Jordi Atserias, Joan Codina-Filba`,
David Garc??a-Narbona, Jens Grivolla, Patrik Lambert, Roser Saur??
Barcelona Media
Av. Diagonal 177, Barcelona 08018
Corresponding author: carlos.rodriguez@barcelonamedia.org
Abstract
This paper describes the system implemented
by Fundacio? Barcelona Media (FBM) for clas-
sifying the polarity of opinion expressions in
tweets and SMSs, and which is supported by
a UIMA pipeline for rich linguistic and sen-
timent annotations. FBM participated in the
SEMEVAL 2013 Task 2 on polarity classifi-
cation. It ranked 5th in Task A (constrained
track) using an ensemble system combining
ML algorithms with dictionary-based heuris-
tics, and 7th (Task B, constrained) using an
SVM classifier with features derived from the
linguistic annotations and some heuristics.
1 Introduction
We introduce the FBM system for classifying the
polarity of short user-generated text (tweets and
SMSs), which participated in the two subtasks of
SEMEVAL 2013 Task 2 on Sentiment Analysis in
Twitter. These are: Task A. Contextual Polarity Dis-
ambiguation, and Task B. Message Polarity Classifi-
cation. The former aimed at classifying the polarity
of already identified opinion expressions (or cues),
whereas the latter consisted in classifying the polar-
ity of the whole text (Wilson et al, 2013).
The literature agrees on two main approaches for
classifying opinion expressions: using supervised
learning methods and applying dictionary/rule-
based knowledge (see (Liu, 2012) for an overview).
Each of them on its own has been used in work-
able systems, and a principled combination of both
of them can yield good results on noisy data, since
generally one (dictionaries/rules) offers good preci-
sion while the other (ML) is able to discover unseen
examples and thus enhances recall.
FBM combined both approaches in order to bene-
fit from their respective strengths and compensating
as much as possible their weaknesses. For Task A
we used linguistic (lexical and syntactic) annotations
to implement both types of approaches. On the one
hand, we built machine learning classifiers based on
Support Vector Machines (SVMs) and Conditional
Random Fields (CRFs). On the other, we imple-
mented a basic classification system mainly based
on polarity dictionaries and negation information, as
well as simple decision tree-like heuristics extracted
from the training data. For task B we trained an
SVM classifier using some of the annotations from
Task A.
The paper first presents the process of data com-
pilation and preprocessing (section 2), and then de-
scribes the systems for Tasks A (section 3) and B
(section 4). Results and conclusions are discussed
in the last section.
2 Data Compilation and Processing
2.1 Making data available
The corpus of SMSs was provided to the partici-
pants by the organizers of the task. As for the corpus
of tweets, legal restrictions on twitter data distribu-
tion required the participants to download the tex-
tual contents of the corpus from a list of tweet ids.
We retrieved the tweet text using the official twit-
ter API instead of script provided by the organizers,
but not all the tweets were available for download
483
due to restrictions of different types (e.g. geograph-
ical), or because the twitter account was temporarily
suspended. In total, we managed to retrieve 10,764
tweets out of 11,777 ids provided by the organizers
(91.4%). It is worth pointing out that the restric-
tions on tweets distribution can become an issue for
future users of the dataset, as the amount of avail-
able tweets will diminish over time. By contrast, the
twitter test corpus was distributed with the full text
to avoid those problems.
2.2 Leveraging the data with rich linguistic
information
We applied the same linguistic processing to both
corpora (SMSs and tweets), even though the SMS
test data presents very different characteristics from
the twitter data, not only because of what can be ap-
preciated as genre differences, but also due to the
fact that is apparently written in Singaporean En-
glish, which differs significantly from American or
British English. No efforts were made to adapt
our linguistic processing modules and dictionaries
to this data.
Tweets and SMSs were processed with a UIMA1-
based pipeline consisting of a set of linguistic and
opinion-oriented modules, which includes:
Basic linguistic processing: Sentence segmen-
tation, tokenization, POS-tagging, lemmatiza-
tion.
Syntax: Dependency parsing.
Lexicon-based annotations:
? Basic polarity, distinguishing among: positive,
negative, and neutral, as encoded in Wilson et
al. (2010).
? Polarity strength, using the score for pos-
itive and negative polarity in SentiWordnet
3.0 (Baccianella et al, 2010). Each Sen-
tiWordNet synset has an associated triplet of
numerical scores (positive, negative,
and objective) expressing the intensity of
positive, negative and objective polarity of the
terms it contains. They range from 0.0 to 1.0,
and their sum is 1.0 for each synset (Esuli and
Sebastiani, 2007). We selected only the synset
1http://uima.apache.org/uima-specification.html
with positive or negative scores higher than 0.5,
containing a total of 16,791 words.
? Subjectiviy clues, from Wilson et al (2010),
which are classified as weak or strong depend-
ing on their degree of subjectivity.
? Sentiment expressions, from the Linguistic In-
quiry and Word Count (LIWC) 2001 Dictio-
nary (Pennebaker et al, 2001).
? In-house compiled lexicons of negation mark-
ers (such as ?no?, ?never?, ?none?) and quanti-
fiers (?all?, ?many?, etc.), the latter further clas-
sified into low, medium and high according to
their quantification degree.
The different classifiers employed by FBM con-
structed their vectors from this output to learn global
and contextual polarities.
3 Task A: Ensemble System
Our system combined Machine Learning and rule-
based approaches. The aim was to combine the
strengths of each individual component while avoid-
ing as much as possible their weaknesses. In what
follows we describe each system component as well
as the way the ensemble system worked out the col-
lective decisions.
3.1 Conditional Random Fields
One of the classifiers uses the Conditional Random
Fields implementation of a biomedical Named En-
tity Recognition system (JNET from JulieLab) 2, ex-
ploiting the classification capabilities of the system
(rather than its span detection) by strongly associat-
ing already defined ?marked instances? with a polar-
ity, and exploring a 5-word window. It uses depen-
dency labels, POS tags, polar words, sentiwordnet
and LWIC sentiment annotations, as well as indica-
tions for quantifiers and negation markers.
3.2 Support Vector Machines
This classifier was implemented using an SVM algo-
rithm with a linear kernel and the C parameter set to
0.2 (determined using a 5 fold cross-validation). The
features set includes those that we used in RepLab
2http://www.julielab.de
484
2012 (Chenlo et al, 2012) (including number of:
characters, words, links, hashtags, positive and neg-
ative emoticons, question-exclamation marks, ad-
jectives, nouns, verbs, adverbs, uppercased words,
words with duplicated vowels), plus a set of new
features at tweet level obtained from the linguistic
annotations: number of high/medium/low polarity
quantifiers, number of positive and negative polar
words, sentiwordnet applied to both the cue and the
whole tweet.
Moreover, the RepLab polarity calculation based
on different dictionaries was modified to take into
account negation (in a 3-word window) potentially
inverting the polarity (negPol). This polarity mea-
sure was applied to the cue and to the whole tweet,
thus generating two additional features.
3.3 Heuristic Approach
In task A, in parallel to the supervised learning sys-
tem, we developed a method (named Heur) based
on polarity dictionary lookup and simple heuristics
(see Figure 1) taking into account opinion words
as well as negation markers and quantifiers. These
heuristics were implemented so as to maximize the
number of correct positive and negative labels in the
training data. To this end, we calculated the aggre-
gate polarity of a cue segment as the sum of word
polarities found in the polarity lexicon. The aggre-
gate values in the training set ranged from -3 to +3,
taking respectively 1, 0 and -1 as the polarity of pos-
itive, neutral and negative words. The label distri-
bution of cue segments with an aggregate polarity
value of -1 is shown in Table 1.
Aggregate polarity -1
Negation no yes
negative 1,032 30
neutral 37 4
positive 178 71
Table 1: Cue segment polarity statistics in training data
for an aggregate polarity value of -1.
In this case, if no negation is present in the cue
segment, a majority (1,032) of examples had the
negative label. In case there was at least a negation, a
majority (71) of examples had a positive label. This
behaviour was observed with all negative aggregate
1: if has polar word(CUE) then
2: polarity= lex(P)-0.5*lex(QP)
3: -lex(N)+0.5*lex(QN)
4: if polarity>0 then
5: if has negation(CUE) then negative
6: else positive
7: end if
8: else if polarity<0 then
9: if has negation(CUE) then positive
10: else negative
11: end if
12: else
13: if has negation(CUE) then positive
14: else negative
15: end if
16: end if
17: else if has negation(CUE) then negative
18: else
19: polarity= tlex(P)-0.5*tlex(QP)
20: -tlex(N)+0.5*tlex(QN)
21: if polarity<0 then negative
22: else if tlex(NEU)>0 then neutral
23: else if polarity>0 then positive
24: else if has negemo(CUE) then negative
25: else if has posemo(CUE) then positive
26: else unknwn
27: end if
28: end if
Figure 1: Heuristics used by the lexicon-based system to
classify the polarity of a segment marked up as opinion
cue (Task A).
polarity values in training data, yielding the rule in
lines 8 to 11 of Figure 1. Similar rules were ex-
tracted for the other aggregate polarity values (lines
4 to 16 of Figure 1).
Figure 1 details the complete classification algo-
rithm. Note (lines 1 to 17) that we first rely on the
basic polarity lexicon annotations (described in sec-
tion 2). The final aggregate polarity formula (lines
2-3) was refined to distinguish sentiment words
which act as quantifiers, such as pretty in pretty mad.
The word pretty is both a positive polar word and a
quantifier. We want its polarity to be positive in case
it occurs in isolation, but less than one so that the
sum with a following negative polar word (such as
mad) be negative. We thus give this kind of words
a polarity of 0.5 by substracting 0.5 for each polar
word which is also a quantifier. In the polarity for-
mula of lines 2-3, lex(X) refers to the number of
words annotated as X, P and N refer respectively
to positive and negative polar words, and QP and
485
QN refer to positive and negative polar words which
are also quantifiers. Quantifiers which are not polar
words are not taken into account because they are
not likely to change the opinion polarity.
In case that no annotations from the basic polar-
ity, quantifiers, and negative markers lexicons are
found (lines 18 to 28), we look up in dictionaries
built from the training data (tlex in lines 19-20).
To build these dictionaries, we counted how many
times each word was labeled positive, negative and
neutral. We considered that a word has a given po-
larity if the number of times it was assigned to this
class is greater than the number of times it was as-
signed to any other class by a given threshold. We
calculated the polarity in the same way as before,
but now with the counts from the lexicon automati-
cally compiled from the training data. To improve
the recall of the dictionary lookup, we performed
some text normalization: lowercasing, deletion of
repeated characters (such as gooood) and deletion of
the hashtag ?#? character. Finally, if no polar word
is found in the automatically compiled lexicon, we
look at the sentiment annotations (extracted from the
LIWC dictionary).
3.4 Ensemble Voting Algorithm
As already mentioned, we combined the results from
the described polarity methods to build a collective
decision. Table 2 shows the performance (in terms
of F1 measure) of the different single methods over
the tweet test data.
SVM Heur Heur+ CRF
Test 80.74 83.47 84.62 62.85
Table 2: Twitter Task A results for different methods
Although the heuristic method outperforms the
ML methods, they are not only different in nature
(ML vs. heuristic) but also use different information
(see Table 5). This suggests that the ensemble solu-
tion will be complementary and capable of obtaining
better results than any of the individual methods by
itself.
The development set was used to calculate the en-
semble response given the individual votes of the
different systems in a way similar to the behavior
knowledge space method (Huang and Suen, 1993).
Table 3 shows an example of how the assemble
voting is built. For each method vote combina-
tion (SVM-Heuristics-CRF) the number of positives
/ negatives / neutral is calculated in the development
data. The ensemble (EV) selects the vote that max-
imizes the number of correct votes in the develop-
ment data (in bold).
SVM Heur CRF EV
# Instances
pos neg neu
? + ? ? 0 6 0
? ? + ? 1 23 2
? ? ? ? 3 125 2
? u + + 1 0 0
+ u n ? 0 1 0
+ ? + + 17 13 2
+ + + + 314 18 17
+ ? n + 3 1 0
Table 3: Oracle building example (EV: Ensemble Vote,
+:positive, ?:negative, n:neutral, u:unknown)
The test data contains some combination of votes
that were not seen in the development data. Thus,
in order to deal with these unseen combinations of
votes in the test set we use the following backup
heuristics based on the preformance figures of the
individual methods: Use the vote of the heuristic
method. If this method does not vote (u), then se-
lect the SVM vote.
Table 4 shows the results of the proposed ensem-
ble method, the well-known majority voting and the
upper bound of this ensemble method (calculated
with the same strategy over the test data), over the
development and test tweet data
Ensemble Majority Upper
Voting Voting Bound
Dev 85.48 81.31 85.48
Test 85.50 82.70 89.37
Table 4: Results for different ensemble strategies
In the development corpus, the upper bound and
ensemble results are the same, given that they ap-
ply the same knowledge. The difference is in the
test dataset, where the ensemble voting is calculated
based on the knowledge obtained from the develop-
ment corpus, while the upper bound uses the knowl-
edge that can be derived from the test corpus.
486
Table 5 illustrates the features used by each com-
ponent.
SVM SVM CRF Heur
(task A) (task B)
word ? ? ?
lemma
pos ? ?
deps ?
pol ? ? ? ?
polW ?
sent ? ? ?
sentiwn ? ? ?
quant ? ? ? ?
neg ? ? ? ?
links ?
hashTags ?
Table 5: Information used (pos: part-of-speech; deps: de-
pendencies; pol: basic polarity classification; polW: basic
polarity word; sent: LIWC sentiments; sentwn: Senti-
Wordnet; quant/neg: quantifiers and negation markers.)
4 Task B: A Support Vector
Machine-based System
The system presented for task B is based on ML us-
ing a SVM model. The feature vector used as input
for the SVM component is composed of the annota-
tions provided by the linguistic annotation pipeline,
extended with a feature obtained by applying nega-
tion to the next polar words (window of size 3).
The features used do not include the words (or
their lemmas) because the number of tweets avail-
able for training is small (104) compared to the num-
ber of different words (4 ? 104). A model based on
bag-of-words would suffer from overfitting and thus
be very domain and time-dependent. If the train and
test sets were randomly selected from a bigger set,
the use of words could increase the model?s accu-
racy, but the model would also be too narrowly ap-
plied to this specific dataset.
From the annotation pipeline we extracted as fea-
tures: the polar words (PolW) and their basic po-
larity (Pol); the sentiment annotations from LIWC
(Sent); the negation markers (Neg) and quantifiers
(Quant). The model was trained using Weka (Hall
et al, 2009).
The model used is SVM with the C parameter set
to 1.0 and applying a 10 fold cross-validation. The
option of doing first a model to discriminate polar
and neutral tweets was discarded because Weka al-
ready does that when training classifiers for more
than two training classes, and the combination of the
two classifiers (a first one between polar and opin-
ionated and a second one between positive and neg-
ative) would produce the same results.
5 Results and Discussion
The results of our system in each subcorpus and task
are presented in Table 5 (average of the F1-measure
over the classes positive and negative, constrained
track), with the ranking achieved in the competition
in parentheses.
Tweet Corpus SMS Corpus
Task A 0.86 (5th) 0.73 (11th)
Task B 0.61 (7th) 0.47 (28th)
Table 6: FBM system performance (F1 average over pos-
itive and negative classes, constrained track) and rankings
Given the differences in style and vocabularies be-
tween the SMS and tweet corpora, and the fact that
we made not effort whatsoever to adapt our system
or models to them, the drop in performance from
one to the other is considerable, but to be expected
since domain customization is an important aspect
of opinion mining.
Task A: The confusion matrix in Table 7 shows
an acceptable performance for the most frequent
classes in the corpus (with an error of 7.75% and
19.5% for postive and negative cues, respectively)
and a very poor job for neutral cues (98.1% of er-
ror), clearly a minority class in the training corpus
(5% of the data).
GOLD: Pos Neg Neu
SYSTEM: Pos 2,522 296 126
Neg 206 1,240 31
Neu 6 5 3
Table 7: Task A confusion matrix
Given the skewed distribution of polarity cate-
gories in the test corpus, however, neutral mistakes
amount to only 23% of our system error, and so we
487
focus our analysis on the problems in positive and
negative cues, respectively amounting to 31.7% and
44.8% of the total error. There are 2 main sources of
error:
? Limitations of the dictionaries employed,
which were short in covering somewhat fre-
quent slang words (e.g., wacky, baddest, shit-
loads), expressions (e.g., ouch, yukk, C?MON),
or phrases (e.g., over the top), some of which
express a particular polarity but contain a word
expressing just the opposite (have a blast, to
want something bad/ly).
? Problems in UGC processing, mainly related to
normalization (e.g., fooooool) and tokenization
(Perfect...not sure), which put at risk the cor-
rect identification of lexical elements that are
crucial for polarity classification.
Task B: The average F-score of positive and neg-
ative classes was 0.62 in the development set (that
was included in the training set) and the averaged F-
score for the test set was 0.61 (so they are very simi-
lar). If focusing on precision and recall, the positive
and negative classes have higher precision but lower
recall in the test set. We think that this low degrada-
tion of perfomance indicates the model?s potential
for generalization.
6 Conclusions
From our results, we can conclude that the use of
ensemble combination of orthogonal methods pro-
vides good performance for Task A. Similar results
could be expected for Task B (judging from mix-
ing dictionaries and ML in similar tasks at RepLab
2012 (Chenlo et al, 2012)). The ML methods that
we applied for Task B are essentially additive, and
hence have difficulties in applying features such as
polarity shifters. To overcome this, one of the fea-
tures includes negation of polar words when a polar-
ity shifter is near.
Overall, the SemEval Tasks have make evident the
usual challenges when mining opinions from Social
Media channels: noisy text, irregular grammar and
orthography, highly specific lingo, etc. Moreover,
temporal dependencies can affect the performance if
the training and test data have been gathered at dif-
ferent times, as is the case with text of such a volatile
nature as tweets and SMSs.
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
35.00%
40.00%
45.00%
50.00%
train
dev
test
Figure 2: Distribution of tweets over time
The histogram in Figure 2 shows that this also ap-
plies to the Semeval tweets dataset. It illustrates the
distribution of tweets over time (extrapolated from
the sequential ids) in the 3 subcorpora (train, devel-
opment and test), showing some divergence between
the test corpus on the one hand, and the develop-
ment and training corpora on the other. Neverthe-
less, our system shows little performance degrada-
tion between development and testing results, as at-
tested in Table 4 (ensemble voting column).
Our work here and at other competitions already
cited validate a system that combines stochastic and
symbolic methodologies in a principled, data-driven
approach. Time and domain dependencies of Social
Media data make system and model generalization
highly desirable, and our system hybrid nature also
contribute to this objective.
Acknowledgments
This work has been partially funded by the Spanish
Government project Holopedia, TIN2010-21128-
C02-02, the CENIT program project Social Media,
CEN-20101037, and the Marie Curie Reintegration
Grant PIRG04-GA-2008-239414.
488
References
Baccianella, Stefano, Andrea Esuli and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of the 7th conference on International
Language Resources and Evaluation, Valletta, Malta.
Chenlo, Jose M., Jordi Atserias, Carlos Rodr??guez-
Penagos and Roi Blanco. 2012. FBM-Yahoo!
at RepLab 2012. In: P. Forner, J. Karlgren,
C. Womser-Hacker (eds.) CLEF 2012 Evalua-
tion Labs and Workshop, Online Working Notes.
http://clef2012.org/index.php?page=Pages/procee-
dings.php.
Esuli, Andrea and Fabrizio Sebastiani. 2007. SEN-
TIWORDNET: a high-coverage lexical resource for
opinion mining. Technical Report ISTI-PP-002/2007,
Institute of Information Science and Technologies
(ISTI) of the Italian National Research Council
(CNR).
Hall, Mark, Frank Eibe, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann and Ian H. Witten. 2009.
The WEKA data mining software: an update. In:
ACM SIGKDD Explorations Newsletter, 1: 10?18.
Huang, Y. S. and C. Y. Suen. 1993. Behavior-knowledge
space method for combination of multiple classifiers.
In Proceedings of IEEE Computer Vision and Pattern
Recognition, 347?352.
Liu, Bing. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
(5-1), 1?167.
Pennebaker, James W., Martha E. Francis and Roger
J. Booth. 2001. Linguistic inquiry and word count:
LIWC 2001. Mahway: Lawrence Erlbaum Asso-
ciates.
Wilson, Theresa, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov and Alan. Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13.
Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.
2010. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35(3), 399?433.
489
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 19?27,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
AutoLearn?s authoring tool: a piece of cake for teachers  
 
 
Mart? Quixal1, Susanne Preu?3, David Garc?a-Narbona2, Jose R. Boullosa2 
  
1 Voice and Language Group,  
2 Advanced Development Group 
Barcelona Media Centre d?Innovaci? 
Diagonal, 177, E-08018 Barcelona, Spain 
{marti.quixal,david.garcian, 
beto.boullosa}@barcelonamedia.org  
3 GFAI 
Martin-Luther-Str. 14 
Saarbr?cken, Germany 
susannep@iai.uni-sb.de  
  
Abstract 
This paper1 presents AutoLearn?s authoring 
tool: AutoTutor, a software solution that en-
ables teachers (content creators) to develop 
language learning activities including auto-
matic feedback generation without the need of 
being a programmer. The software has been 
designed and implemented on the basis of 
processing pipelines developed in previous 
work. A group of teachers has been trained to 
use the technology and the accompanying 
methodology, and has used materials created 
by them in their courses in real instruction set-
tings, which served as an initial evaluation. 
The paper is structured in four sections: Sec-
tion 1 introduces and contextualizes the re-
search work. Section 2 describes the solution, 
its architecture and its components, and spe-
cifically the way the NLP resources are cre-
ated automatically with teacher input. Section 
3 describes and analyses a case study using 
the tool to create and test a language learning 
activity. Finally Section 4 concludes with re-
marks on the work done and connections to 
related work, and with future work. 
1 Introduction 
Over the past four decades there have been several 
hundreds of CALL (Computer-Aided Language 
Learning) projects, often linked to CALL practice 
(Levy 1997), and within the last twenty years a 
considerable number of them focused on the use of 
                                                          
1
 Research funded by the Lifelong Learning Programme 2007-
2013 (AUTOLEARN, 2007-3625/001-001). 
NLP in the context of CALL (Amaral and Meur-
ers, in preparation). Despite this, there is an appall-
ing absence of parser-based CALL in real 
instruction settings, which has been partially at-
tributed to a certain negligence of the pedagogical 
needs (Amaral and Meurers, in preparation). In 
contrast, projects and systems that were pedagogi-
cally informed succeeded, yielded and are yielding 
interesting results, and are evolving for over a dec-
ade now (Nagata 2002; Nagata 2009; Heift 2001; 
Heift 2003; Heift 2005; Amaral and Meurers, in 
preparation). According to Amaral and Meurers 
successful projects were able to restrict learner 
production in terms of NLP complexity by limiting 
the scope of the learning activities to language-
oriented (as opposed to communicative-oriented) 
or translation exercises, or by providing feedback 
on formal aspects of language in content oriented 
activities, always under pedagogical considerations 
?focus on form. 
Our proposal is a step forward in this direction 
in two ways: a) it allows for feedback generation 
focusing both on formal and content (communica-
tive-oriented) aspects of language learning activi-
ties, and b) it provides teachers with a tool and a 
methodology ?both evolving? for them to gain 
autonomy in the creation of parser-based CALL 
activities ?which by the way has a long tradition in 
CALL (Levy 1997, chap. 2). The goal is to shape 
language technologies to the needs of the teachers, 
and truly ready-to-hand. 
1.1 Related work and research context 
The extent to which pedagogues appreciate and 
require autonomy in the design and creation of 
CALL activities can be traced in the historical 
19
overview offered by (Levy 1997, 16, 17, 19, 23 
and 38). Moreover, parallel research shows that the 
integration of CALL in the learning context is 
critical to ensure the success of whatever materials 
are offered to learners (Levy 1997, 200-203; Polis-
ca 2006). 
AutoTutor goes beyond tools such as Hot Pota-
toes, eXelearning or JClic2 in that it offers the pos-
sibility of authoring NLP-based CALL activities. It 
is also more ambitious than other authoring tools 
developed for the creation of activities in intelli-
gent tutoring systems. Chen and Tokuda (2003) 
and R?sener (2009) present authoring tools for 
translation exercises, where expected learner input 
is much more controlled (by the sentence in the 
source language).  
Heift and Toole (2002) present Tutor Assistant, 
which enables to create activities such as build-a-
sentence, drag-and-drop and fill-in-the-blank. An 
important difference between AutoTutor and Tutor 
Assistant is that the latter is a bit more restrictive in 
terms of the linguistic objects that can be used. It 
also presents a lighter complexity in the modelling 
of the underlying correction modules. However, 
the system underlying Tutor Assistant provides 
with more complex student adaptation functional-
ities (Heift 2003) and would be complementary in 
terms of overall system functionalities. 
                                                          
2
 http://hotpot.uvic.ca/, http://sourceforge.net/apps/trac/exe/wiki, 
http://clic.xtec.cat/es/jclic/index.htm. 
2 AutoTutor: AutoLearn?s authoring 
software 
AutoTutor is a web-based software solution to 
assist non-NLP experts in the creation of language 
learning activities using NLP-intensive processing 
techniques. The process includes a simplified 
specification of the means to automatically create 
the resources used to analyse learner input for each 
exercise. The goal is to use computational devices 
to analyse learner production and to be able to go 
beyond ?yes-or-no? answers providing additional 
feedback focused both on form and content. 
This research work is framed within the 
AutoLearn project, a follow up of the ALLES pro-
ject (Schmidt et al, 2004, Quixal et al, 2006). 
AutoLearn?s aim was to exploit in a larger scale a 
subset of the technologies developed in ALLES in 
real instruction settings. Estrada et al (2009) de-
scribe how, in AutoLearn?s first evaluation phase, 
the topics of the activities were not attractive 
enough for learners and how learner activity de-
creased within the same learning unit across exer-
cises. Both observations ?together with what it has 
been shown with respect to the integration of inde-
pendent language learning, see above ?  impelled 
us to develop AutoTutor, which allows teachers to 
create their own learning units. 
As reflected in Figure 1, AutoTutor consists 
primarily of two pieces of software: AutoTutor 
Activity Creation Kit (ATACK) and AutoTutor 
Activity Player (ATAP). ATACK, an authoring 
Figure 1. AutoTutor software architecture. 
20
tool, provides teachers with the ability to create 
parser-based CALL exercises and define the corre-
sponding exercise specifications for the generation 
of automated feedback. ATAP allows teachers to 
insert, track and manage those exercises in Moodle 
(http://moodle.org), giving learners the possibility 
to visualize and answer them. Both ATACK and 
ATAP share a common infrastructure of NLP ser-
vices which provides the basic methods for gener-
ating, storing and using NLP tools. Access to those 
methods is made through XML-RPC calls. 
2.1 AutoTutor Activity Creation Kit 
ATACK is divided in two components: a GUI that 
allows content creators to enter the text, questions 
and instructions to be presented to learners in order 
to elicit answers from them; and an NLP resource 
creation module that automatically generates the 
resources that will be used for the automated feed-
back. Through the GUI, teachers are also able to 
define a set of expected correct answers for each 
question, and, optionally, specific customized 
feedback and sample answers. 
To encode linguistic and conceptual variation in 
the expected answers, teachers are required to turn 
them into linguistic patterns using blocks. Blocks 
represent abstract concepts, and contain the con-
crete chunks linked to those concepts. Within a 
block one can define alternative linguistic struc-
tures representing the same concept. By combining 
and ordering blocks, teachers can define the se-
quences of text that correspond to the expected 
correct answers ?i.e., they can provide the seeds 
for answer modelling. 
Modelling answers 
Given an exercise where learners are required 
to answer the question ?From an architecture point 
of view, what makes Hagia Sophia in Istanbul so 
famous according to its Wikipedia entry??, the 
following answers would be accepted: 
1. {The Hagia Sophia/The old mosque} is 
famous for its massive dome. 
2. The reputation of {the Hagia Sophia/the 
old mosque} is due to its massive dome. 
To model these possible answers, one would 
use four blocks (see Figure 2) corresponding to 
WHO (Hagia Sophia), WHAT (Famousness), and 
WHY (Dome), and complementary linguistic ex-
pressions such as ?is due to?. Thus, the possible 
correct block sequences would be (indices corre-
sponding to Figure 2): 
a) B1 B2.A B4 
b) B2.B B1 B3 B4 
Block B1 is an example of interchangeable al-
ternatives (the Hagia Sophia or the old mosque), 
which do not require any further condition to ap-
ply. In contrast, block B2 is an instance of a syn-
tactic variation of the concept. Famousness can be 
expressed through an adjective or through a verb 
(in our example), but each of the choices requires a 
different sentence structure.  
Alternative texts in a block with no variants (as 
in B1) exploit the paradigmatic properties of lan-
guage, while alternative texts in a block with two 
variants as in B2 account for its syntagmatic prop-
erties, reflected in the block sequences. Interest-
ingly, this sort of splitting of a sentence into blocks 
is information-driven and simplifies the linguistic 
expertise needed for the exercise specifications. 
2.2 Automatic generation of exercise-specific 
NLP-resources 
Figure 3 shows how the teacher?s input is con-
verted into NLP-components. Predefined system 
components present plain borders, and the result-
ing ones present hyphenised borders. The figure 
also reflects the need for answer and error model-
ling resources. 
NLP resource generation process 
B2 (FAMOUSNESS) 
B: 
the reputation of 
A: 
is famous for 
B1 (SOPHIA) 
the Hagia Sophia 
the old mosque 
B3 (DUE) 
is due to 
B4 (CAUSE) 
its massive dome 
Figure 2 Blocks as specified in AutoTutor GUI. 
21
The generation of the NLP resources is possible 
through the processing of the teacher?s input with 
three modules:  the morphological analysis module 
performs a lexicon lookup and determines un-
known words that are entered into the exercise-
specific lexicon; the disambiguation of base form 
module, disambiguates base forms, e.g. ?better? is 
disambiguated between verb and adjective depend-
ing on the context in preparation of customized 
feedback.  
The last and most important module in the ar-
chitecture is the match settings component, which 
determines the linguistic features and structures to 
be used by the content matching and the exercise-
specific error checking modules (see Figure 4). 
Using relaxation techniques, the parsing of learner 
input is flexible enough to recognize structures 
including incorrect word forms and incorrect, 
missing or additional items such as determiners, 
prepositions or digits, or even longish chunks of 
text with no correspondence the specified answers. 
The match settings component contains rules that 
later on trigger the input for the exercise-specific 
error checking.  
The match settings component consists of 
KURD rules (Carl et al 1998). Thus it can be 
modified and extended by a computational linguist 
any time without the need of a programmer. 
Once the exercise?s questions and expected an-
swers have been defined, ATACK allows for the 
generation of the NLP resources needed for the 
automatic correction of that exercise. The right-
hand side of Figure 3 shows which the generated 
resources are: 
? An exercise-specific lexicon to handle un-
known words 
? A content matching module based on the 
KURD formalism to define several lin-
guistically-motivated layers with different 
levels of relaxation (using word, lemma, 
and grammatical features) for determining 
the matching between the learner input and 
the expected answers  
? A customized feedback module for teacher-
defined exercise-specific feedback  
? An exercise-specific error checking mod-
ule for context-dependent errors linked to 
language aspects in the expected answers 
? A general content evaluation component 
that checks whether the analysis performed 
by the content matching module conforms 
to the specified block orders 
2.3 AutoTutor Activity Player (ATAP) 
With ATAP learners have access to the contents 
enhanced with automatic tutoring previously cre-
ated by teachers. ATAP consists of a) a client GUI 
for learners, integrated in Moodle, to answer exer-
cises and track their own activity; b) a client GUI 
for teachers, also integrated in Moodle, used to 
manage and track learning resources and learner 
Teacher input (GUI) 
ERROR MODEL 
ANSWER MODEL 
Morph. 
analysis 
Morph. 
analysis 
Customized 
feedback 
Match 
settings 
General content 
evaluation 
Content mat-
ching 
Disam. of 
base form 
Exercise-specific lexicon 
Exercise-specific 
error checking 
Blocks (word 
chunks) 
Teacher defined 
error modelling 
Block order 
Figure 3. Processing schema and components of the customizable NLP resources of ATACK 
22
activity; and c) a backend module, integrated into 
the AutoTutor NLP Services Infrastructure, re-
sponsible for parsing the learner?s input and gener-
ating feedback messages. 
Figure 4 describes the two steps involved in the 
NLP-based feedback generation: the NLP compo-
nents created through ATACK ?in hyphenised 
rectangles? are combined with general built-in 
NLP-based correction modules. 
2.4 The feedback generation software 
Feedback is provided to learners in two steps, 
which is reflected in Figure 4 by the two parts, the 
upper and lower part, called General Checking and 
Exercise Specific Checking respectively. The for-
mer consists in the application of standard spell 
and grammar checkers. The latter consists in the 
application of the NLP resources automatically 
generated with the teacher?s input. 
Content matching module 
The text chunks (blocks) that the teacher has en-
tered into ATACK?s GUI are converted into 
KURD rules. KURD provides with sophisticated 
linguistically-oriented matching and action opera-
tors. These operators are used to model (predict-
able) learner text. The content matching module is 
designed to be able to parse learners input with 
different degrees of correctness combining both 
relaxation techniques and mal-rules. For instance, 
it detects the presence of both correct and incorrect 
word forms, but it also detects incorrect words 
belonging to a range of closed or open word 
classes ?mainly prepositions, determiners, modal 
verbs and digits? which can be used to issue a cor-
responding linguistically motivated error messages 
like ?Preposition wrong in this context?, in a con-
text where the preposition is determined by the 
relevant communicative situation. 
Error types that are more complex to handle in 
technical terms involve mismatches between the 
amount of expected elements and the actual 
amount of informational elements in the learner?s 
answer. Such mismatches arise on the grammatical 
level if a composite verb form is used instead of a 
simple one, or when items such as determiners or 
commas are missing or redundant. The system also 
accounts for additional modifiers and other words 
interspersed in the learner?s answer.  
The matching strategy uses underspecified 
empty slots to fit in textual material in between the 
correct linguistic structures. Missing words are 
handled by a layer of matching in which certain 
elements, mainly grammatical function words such 
as determiners or auxiliary verbs, are optional.  
Incorrect word choice in open and closed word 
classes is handled by matching on more abstract 
linguistic features instead of lexeme features. 
The interaction between KURD-based linguis-
tically-driven triggers in the content matching 
module and the rules in the exercise-specific error 
checking (see below) module allows for specific 
mal-rule based error correction. 
Customized feedback 
Teachers can create specific error messages for 
simple linguistic patterns (containing errors or 
searching for missing items) ranging from one or 
two word structures to more complex word-based 
linguistic structures. Technically, error patterns are 
Morph. 
analysis 
Spell 
checking 
Grammar 
checking 
Lexicon Exercise-specific lexicon 
Customized 
feedback 
Exercise-specific 
error checking 
General content 
evaluation 
Content 
matching 
EXERCISE-SPECIFIC CHECKING (TWO) 
GENERAL CHECKING (ONE) 
Figure 4. Processing schema of the NLP resources to generate automatic feedback. 
23
implemented as KURD rules linked to a specific 
error message. These rules have preference over 
the rules applied by any other later module. 
Exercise-specific error checking 
Teachers do not encode all the exercise-specific 
errors themselves because a set of KURD rules for 
the detection of prototypical errors is encoded ?this 
module uses the triggers set by the content match-
ing component. Exercise-specific linguistic errors 
handled in this module have in common that they 
result in sentences that are likely to be wrong ei-
ther from a formal (but context-dependent) point of 
view or from an informational point of view.  
General content evaluation 
Since the contents are specified by the blocks cre-
ated by teachers, the evaluation has a final step in 
which the system checks whether the learner?s 
answer contains all the necessary information that 
belongs to a valid block sequence.  
This module checks for correct order in infor-
mation blocks, for blending structures (mixtures of 
two possible correct structures), missing informa-
tion and extra words (which do not always imply 
an error). The messages generated with this com-
ponent pertain to the level of completeness and 
adequacy of the answer in terms of content. 
3 Usage and evaluation 
AutoTutor has been used by a group of seven 
content creators ?university and school teachers? 
for a period of three months. They developed over 
20 activities for learning units on topics such as 
business and finance, sustainable production and 
consumption, and new technologies. Those activi-
ties contain listening and reading comprehension 
activities, short-text writing activities, enabling 
tasks on composition writing aspects, etc. whose 
answers must be expressed in relatively free an-
swers consisting of one sentence. In November 
2009, these activities were used in real instruction 
settings with approximately 600 learners of Eng-
lish and German. Furthermore, an evaluation of 
both teacher and learner satisfaction and system 
performance was carried out. 
We briefly describe the process of creating the 
materials by one of the (secondary school) teachers 
participating in the content creation process and 
evaluate the results of system performance in one 
activity created by this same teacher. 
3.1 Content creation: training and practice 
To start the process teachers received a 4-hour 
training course (in two sessions) where they were 
taught how to plan, pedagogically speaking, a 
learning sequence including activities to be cor-
rected using automatically generated feedback. We 
required them to develop autonomous learning 
units if possible. And we invited them to get hold 
of any available technology or platform functional-
ity to implement their ideas (and partially offered 
support to them too), convinced that technology 
had to be a means rather than a goal in itself. The 
course also included an overview of NLP tech-
niques and a specific course on the mechanics of 
ATACK (the authoring tool) and ATAP (the activ-
ity management and deployment tool).  
During this training we learned that most teach-
ers do not plan how activities will be assessed: that 
is, they often do not think of the concrete answers 
to the possible questions they will pose to learners. 
They do not need to, since they have all the knowl-
edge required to correct learner production any 
place, any time in their heads (the learner, the ac-
tivity and the expert model) no matter if the learner 
production is written or oral. This is crucial since it 
requires a change in normal working routine. 
After the initial training they created learning 
materials. During creation we interacted with them 
to make sure that they were not designing activities 
whose answers were simply impossible to model. 
For instance, the secondary school teacher who 
prepared the activity on sustainable production and 
consumption provided us with a listening compre-
hension activity including questions such as: 
1) Which is your attitude concerning respon-
sible consumption? How do you deal with 
recycling? Do you think yours is an eco-
logical home? Are you doing your best to 
reduce your ecological footprint? Make a 
list with 10 things you could do at home to 
reduce, reuse o recycle waste at home. 
All these things were asked in one sole instruc-
tion, to be answered in one sole text area. We then 
talked to the teacher and argued with her the kinds 
of things that could be modelled using simple one-
sentence answers. We ended up reducing the input 
provided to learners to perform the activity to one 
24
video (initially a text and a video) and prompting 
learners with the following three questions: 
1) Explain in your words what the ecological 
footprint is. 
2) What should be the role of retailers accord-
ing to Timo M?kel?? 
3) Why should producers and service provid-
ers use the Ecolabel? 
Similar interventions were done in other activi-
ties created by other content creators. But some of 
them were able to create activities which could be 
used almost straightforwardly. 
3.2 System evaluation 
The materials created by teachers were then 
used in their courses. In the setting that we analyse 
learners of English as a second language were 
Catalan and Spanish native speakers between 15 
and 17 years old that attended a regular first year 
of Batxillerat (first course for those preparing to 
enter university studies). They had all been learn-
ing English for more than five years, and according 
to their teacher their CEF level was between A2 
and B1. They were all digital literates and they all 
used the computer on a weekly basis for their stud-
ies or leisure (80% daily). 
We analyse briefly the results obtained for two 
of the questions in one of the activities created by 
the school teacher who authored the learning unit 
on sustainable production and consumption, 
namely questions 1) and 2) above. This learning 
unit was offered to a group of 25 learners. 
Overall system performance 
Table 1 reflects the number of attempts performed 
by learners trying to answer the two questions 
evaluated here: correct, partially correct and incor-
rect answers are almost equally distributed (around 
30% each) and non-evaluated answers are roughly 
10%. In non-evaluated answers we include basi-
cally answers where learners made a bad use of the 
system (e.g., answers in a language other than the 
one learned) or answers which were exactly the 
same as the previous one for two attempts in a row, 
which can interpreted in several ways (misunder-
standing of the feedback, usability problems with 
the interface, problems with pop-up windows, etc.) 
that fall out of the scope of the current analysis. 
Table 2 and Table 3 show the number of mes-
sages issued by the system for correct, partially 
correct and incorrect answers for each of the two 
questions analyzed. The tables distinguish between 
Form Messages and Content Messages, and Real 
Form Errors and Real Content Errors ?a crucial 
distinction given our claim that using AutoTutor 
more open questions could be tackled.3 
QST CORR. PART. INCORR. INV. TOT 
1ST 36 23 12 2 73 
2ND 14 29 36 21 100 
ALL 50 (29%) 52(30%) 48(28%) 23(13%) 173 
Table 1. Correct, partially correct and incorrect answers. 
Table 2 and Table 3 show that the contrast be-
tween issued feedback messages (most commonly 
error messages, but sometimes rather pieces of 
advice or suggestions) and real problems found in 
the answers is generally balanced in formal prob-
lems (31:15, 8:7 and 41:39 for Table 2; and 6:8, 
29:18, and 20:21 for Table 3) independently of the 
correctness of the answer.  
On the contrary, the contrast between issued 
messages and content problems is much more un-
balanced in correct and partially correct answers 
(139:71 and 84:42 for Table 2; and 45:20 and 
110:57 for Table 3) and more balanced for incor-
rect answers (30:18 for Table 2; and 93:77 for 
Table 3). 
 
MESSAGES REAL ERRORS 
Form Cont Form Cont 
CORRECT ANSWERS 31 139 15 71 
PARTIALLY CORRECT 8 84 7 42 
INCORRECT ANSWERS 41 30 39 18 
 TOTAL ANSWERS 80 253 61 131 
Table 2. Messages issued vs. real errors for question 1 
in the answers produced by learners. 
 
MESSAGES REAL ERRORS 
Form Cont Form Cont 
CORRECT ANSWERS 6 45 8 20 
PARTIALLY CORRECT 29 110 18 57 
INCORRECT ANSWERS 20 93 21 77 
TOTAL ANSWERS 55 248 47 154 
Table 3. Messages issued vs. real errors for question 2 
in the answers produced by learners. 
This indicates that generally speaking the sys-
tem behaved more confidently in the detection of 
formal errors than in the detection of content er-
rors. 
                                                          
3
 A proper evaluation would require manual correction of the 
activities by a number of teachers and the corresponding 
evaluation process. 
25
System feedback analysis 
To analyze the system?s feedback we looked into 
the answers and the feedback proposed by the sys-
tem and annotated each answer with one or more 
of the tags corresponding to a possible cause of 
misbehaviour. The possible causes and its absolute 
frequency are listed in Table 4. 
The less frequent ones are bad use of the system 
on the learner side, bad guidance (misleading the 
learner to an improper answer or to a more com-
plex way of getting to it), connection failure, and 
message drawing attention on form when the error 
was on content. 
MISBEHAVIOUR QUESTION 1 QUESTION 2 
CONN-FAIL 1 0 
BAD-USE 1 1 
FRM-INSTOF-CONT 2 1 
BAD-GUIDE 4 2 
OOV 11 13 
WRNG-DIAG 11 20 
FRM-STRICT 33 20 
ARTIF-SEP 0 61 
SPECS-POOR 1 62 
Table 4. Frequent sources of system errors. 
The most frequent causes of system misbehav-
iour are out-of-vocabulary words, wrong diagno-
ses, and corrections too restrictive with respect to 
form. 
Two interesting causes of misbehaviour and in 
fact the most frequent ones were artificial separa-
tion and poor specifications. The former refers to 
the system dividing answer parts into smaller parts 
(and therefore generation of a larger number of 
issued messages). For instance in a sentence like 
(as an answer to question 2) 
The retailers need to make sure that whatever 
they label or they put in shelf is understandable 
to consumers.4 
the system would generate six different feedback 
messages informing that some words were not 
expected (even if correct) and some were found but 
not in the expected location or form. 
In this same sentence above we find examples 
of too poor specifications, where, for instance, it 
was not foreseen that retailers was used in the 
answer. These two kinds of errors reflect the flaws 
of the current system: artificial separation reflects a 
lack of generalization capacity of the underlying 
                                                          
4
 One of the expected possible answers was ?They need to 
make sure that whatever they label and whatever they put in 
the shelves is understood by consumers?. 
parser, and poor specifications reflect the incom-
pleteness of the information provided by novice 
users, teachers acting as material designers. 
4 Concluding remarks 
This paper describes software that provides 
non-NLP experts with a means to utilize and cus-
tomize NLP-intensive resources using an authoring 
tool for language instruction activities. Its usability 
and usefulness have been tested in real instruction 
settings and are currently being evaluated and ana-
lyzed. Initial analyses show that the technology 
and methodology proposed allow teachers to create 
contents including automatic generation feedback 
without the need of being neither a programmer 
nor an NLP expert.  
Moreover, system performance shows a reason-
able confidence in error detection given the imma-
turity of the tool and of its users ?following 
Shneiderman and Plaisant?s terminology (2006). 
There is room for improvement in the way to re-
duce false positives related with poor specifica-
tions. It is quite some work for exercise designers 
to foresee a reasonable range of linguistic alterna-
tives for each answer. One could further support 
them in the design of materials with added func-
tionalities ?using strategies such as shallow seman-
tic parsing, as in (Bailey and Meurers, 2008), or 
adding functionalities on the user interface that 
allow teachers to easily feed exercise models or 
specific feedback messages using learner answers. 
The architecture presented allows for portability 
into other languages (English and German already 
available), with a relative simplicity provided that 
the lexicon for the language exists and contains 
basic morpho-syntactic information. Moreover, 
having developed it as a Moodle extension makes 
it available to a wide community of teachers and 
learners. The modularity of ATACK and ATAP 
makes them easy to integrate in other Learning 
Management Systems. 
In the longer term we plan to improve AutoTu-
tor?s configurability so that its behaviour can be 
defined following pedagogical criteria. One of the 
aspects to be improved is that a computational 
linguist is needed to add new global error types to 
be handled or new linguistic phenomena to be con-
sidered in terms of block order. If such a system is 
used by wider audiences, then statistically driven 
techniques might be employed gradually, probably 
26
in combination with symbolic techniques ?the 
usage of the tool will provide with invaluable 
learner corpora. In the meantime AutoTutor pro-
vides with a means to have automatic correction 
and feedback generation for those areas and text 
genres where corpus or native speaker text is 
scarce, and experiments show it could be realisti-
cally used in real instruction settings. 
Acknowledgments 
We want to thank the secondary school teachers 
who enthusiastically volunteered in the creation 
and usage of AutoLearn materials: Eli Garrabou 
(Fundaci? Llor), M?nica Castanyer, Montse Pada-
reda (Fundaci? GEM) and Anna Campillo (Escola 
Sant Gervasi). We also want to thank their learn-
ers, who took the time and made the effort to go 
through them. We also thank two anonymous re-
viewers for their useful comments. 
References 
Amaral, Luiz A., and Detmar Meurers. On Using Intel-
ligent Computer-Assisted Language Learning in 
Real-Life Foreign Language Teaching and Learning 
(Submitted). 
Bailey, Stacey and Detmar Meurers (2008) Diagnosing 
meaning errors in short answers to reading compre-
hension question. In Proceedings of the Third ACL 
Workshop on Innovative Use of NLP for Building 
Educational Applications, pages 107?115, Columbus, 
Ohio, USA, June 2008. 
Carl, Michael, and Antje Schmidt-Wigger (1998). Shal-
low Post Morphological Processing with KURD. In 
Proceedings of NeMLaP'98, Sydney. 
Chen, Liang and Naoyuki Tokuda (2003) A New Tem-
plate-Template-enhanced ICALL System for a Sec-
ond Language Composition Course. CALICO 
Journal, Vol. 20, No. 3: May 2003. 
Estrada, M., R. Navarro-Prieto, M. Quixal (2009) Com-
bined evaluation of a virtual learning environment: 
use of qualitative methods and log interpretation to 
evaluate a computer mediated language course. In 
Proceedings of International Conference on Educa-
tion and New Learning Technologies, EDULEARN 
09. Barcelona (Spain), 6th-8th July, 2009. 
Heift, Trude. 2001. Intelligent Language Tutoring Sys-
tems for Grammar Practice. Zeitschrift f?r Interkul-
turellen Fremdsprachenunterricht 6, no. 2. 
http://www.ualberta.ca/~german/ejournal/ heift2.htm. 
???. 2003. Multiple learner errors and meaningful 
feedback: A challenge for ICALL systems. CALICO 
Journal 20, no. 3: 533-548. 
???. 2005. Corrective Feedback and Learner Uptake 
in CALL. ReCALL Journal 17, no. 1: 32-46. 
Heift, Trude, and Mathias Schulze. 2007. Errors and 
Intelligence in Computer-Assisted Language Learn-
ing: Parsers and Pedagogues. New York: Routledge. 
Levy, Michael. 1997. Computer-Assisted Language 
Learning. Context and Conceptualization. Oxford: 
Oxford University Press. 
Nagata, Noriko. 2002. BANZAI: An Application of 
Natural Language Processingto Web based 
Language Learning. CALICO Journal 19, no. 3: 583-
599. 
???. 2009. Robo-Sensei?s NLP-Based Error Detec-
tion and Feedback Generation. CALICO Journal 26, 
no. 3: 562-579. 
Polisca, Elena. 2006. Facilitating the Learning Process: 
An Evaluation of the Use and Benefits of a Virtual 
Learning Environment (VLE)-enhanced Independent 
Language-learning Program (ILLP). CALICO Jour-
nal 23, no.3: 499-51. 
Quixal, M., T. Badia, B. Boullosa, L. D?az, and A. Rug-
gia. (2006). Strategies for the Generation of Indi-
vidualised Feedback in Distance Language Learning. 
In Proceedings of the Workshop on Language-
Enabled Technology and Development and Evalua-
tion of Robust Spoken Dialogue Systems of ECAI 
2006. Riva del Garda, Italy, Sept. 2006. 
R?sener, C.: ?A linguistic intelligent system for tech-
nology enhanced learning in vocational training ? the 
ILLU project?. In Cress, U.; Dimitrova, V.; Specht, 
M. (Eds.): Learning in the Synergy of Multiple Dis-
ciplines. 4th European Conference on Technology 
Enhanced Learning, EC-TEL 2009 Nice, France, 
Sept. 29 ? Oct. 2, 2009. Lecture Notes in Computer 
Science. Programming and Software Engineering, 
Vol. 5794, 2009, XVIII, p. 813, Springer, Berlin. 
Schmidt, P., S. Garnier, M. Sharwood, T. Badia, L. 
D?az, M. Quixal, A. Ruggia, A. S. Valderrabanos, A. 
J. Cruz, E. Torrejon, C. Rico, J. Jimenez. (2004) 
ALLES: Integrating NLP in ICALL Applications. In 
Proceedings of Fourth International Conference on 
Language Resources and Evaluation. Lisbon, vol. VI 
p. 1888-1891. ISBN: 2-9517408-1-6. 
Shneiderman, B. and C. Plaisant. (2006) Strategies for 
evaluating information visualization tools: multi-
dimensional in-depth long-term case studies. BELIV 
?06: Proceedings of the 2006 AVI workshop on Be-
yond time and errors: novel evaluation methods for 
information visualization, May 2006. 
Toole, J. & Heift, T. (2002). The Tutor Assistant: An 
Authoring System for a Web-based Intelligent Lan-
guage Tutor. Computer Assisted Language Learning, 
15(4), 373-86. 
27
