Semi-Supervised Maximum Entropy Based Approach to Acronym and
Abbreviation Normalization in Medical Texts
Serguei Pakhomov, Ph.D.
Mayo Foundation, Rochester, MN
 pakhomov.sergey@mayo.edu
Abstract
Text normalization is an important
aspect of successful information
retrieval from medical documents
such as clinical notes, radiology
reports and discharge summaries. In
the medical domain, a significant part
of the general problem of text
normalization is abbreviation and
acronym disambiguation.  Numerous
abbreviations are used routinely
throughout such texts and knowing
their meaning is critical to data
retrieval from the document. In this
paper I will demonstrate a method of
automatically generating training data
for Maximum Entropy (ME) modeling
of abbreviations and acronyms and
will show that using ME modeling is a
promising technique for abbreviation
and acronym normalization. I report
on the results of an experiment
involving training a number of ME
models used to normalize
abbreviations and acronyms on a
sample of 10,000 rheumatology notes
with ~89% accuracy.
1 Introduction and Background
Text normalization is an important aspect of
successful information retrieval from
medical documents such as clinical notes,
radiology reports and discharge summaries,
to name a few. In the medical domain, a
significant part of the general problem of
text normalization is abbreviation and
acronym1 disambiguation.  Numerous
abbreviations are used routinely throughout
such texts and identifying their meaning is
critical to understanding of the document.
The problem is that abbreviations are highly
ambiguous with respect to their meaning.
For example,  according to UMLS?2 (2001),
RA may stand for ?rheumatoid arthritis?,
?renal artery?, ?right atrium?, ?right atrial?,
?refractory anemia?, ?radioactive?, ?right
arm?, ?rheumatic arthritis,? etc. Liu et al
(2001) show that 33% of abbreviations
listed in UMLS are ambiguous. In addition
to problems with text interpretation,
Friedman, et al (2001) also point out that
abbreviations constitute a major source of
errors in a system that automatically
generates lexicons for medical NLP
applications.
Ideally, when looking for documents
containing ?rheumatoid arthritis?, we want
to retrieve everything that has a mention of
RA in the sense of ?rheumatoid arthritis?
but not those documents where RA means
?right atrial.? In a way, abbreviation
normalization problem is a special case of
the word sense disambiguation (WSD)
problem. Modern approaches to WSD
include supervised machine learning
techniques, where some amount of training
                                                
1 To save space and for ease of presentation, I will
use the word ?abbreviation? to mean both
?abbreviation? and ?acronym? since the two could be
used interchangeably for the purposes described in
this paper.
2 Unified Medical Language System?, a database
containing biomedical information and a tools
repository developed at the National Library of
Medicine to help helath professionals as well as
medical informatics researchers.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 160-167.
                         Proceedings of the 40th Annual Meeting of the Association for
data is marked up by hand and is used to
train a classifier. One such technique
involves using a decision tree classifier
(Black 1988). On the other side of the
spectrum, the fully unsupervised learning
methods such as clustering have also been
successfully used (Shutze 1998). A hybrid
class of machine learning techniques for
WSD relies on a small set of hand labeled
data used to bootstrap a larger corpus of
training data (Hearst 1991, Yarowski 1995).
Regardless of the technique that is used for
WSD, the most important part of the process
is the context in which the word appears
(Ide and Veronis 1998). This is also true for
abbreviation normalization.
For the problem at hand, one way to
take context into account is to encode the
type of discourse in which the abbreviation
occurs, where discourse is defined narrowly
as the type of the medical document and the
medical specialty, into a set of explicit rules.
If we see RA in a cardiology report, then it
can be normalized to ?right atrial?;
otherwise, if it occurs in the context of a
rheumatology note, it is likely to mean
?rheumatoid arthritis? or ?rheumatic
arthritis.? This method of explicitely using
global context to resolve the abbreviation
ambiguity in suffers from at least three
major drawbacks from the standpoint of
automation. First of all, it requires a
database of abbreviations and their
expansions linked with possible contexts in
which particular expansions can be used,
which is an error-prone labor intensive task.
Second, it requires a rule-based system for
assigning correct expansions to their
abbreviations, which is likely to become
fairly large and difficult to maintain. Third,
the distinctions made between various
meanings are bound to be very coarse. We
may be able to distinguish correctly between
?rheumatoid arthritis? and ?right atrial?
since the two are likely to occur in clearly
separable contexts; however, distinguishing
between ?rheumatoid arthritis? and ?right
arm? becomes more of a challenge and may
require introducing additional rules to
further complicate the system.
The approach I am investigating falls
into the hybrid category of bootstrapping or
semi-supervised approaches to training
classifiers; however, it uses a different
notion of bootstrapping from that of Hearst
(1991) and Yarowski (1995). The
bootstrapping portion of this approach
consists of using a hand crafted table of
abbreviations and their expansions pertinent
to the medical domain. This should not be
confused with dictionary or semantic
network approaches. The table of
abbreviations and their expansions is just a
simple list representing a one-to-many
relationship between abbreviations and their
possible ?meanings? that is used to
automatically label the training data.
To disambiguate the ?meaning? of
abbreviations I am using a Maximum
Entropy (ME) classifier. Maximum Entropy
modeling has been used successfully in the
recent years for various NLP tasks such as
sentence boundary detection, part-of-speech
tagging, punctuation normalization, etc.
(Berger 1996, Ratnaparkhi 1996, 1998,
Mikheev 1998, 2000). In this paper I will
demonstrate using Maximum Entropy for a
mostly data driven process of abbreviation
normalization in the medical domain.
In the following sections, I will briefly
describe Maximum Entropy as a statistical
technique. I will also describe the process of
automatically generating training data for
ME modeling and present examples of
training and testing data obtained from a
medical sub-domain of rheumatology.
Finally, I will discuss the training and
testing process and present the results of
testing the ME models trained on two
different data sets. One set contains one
abbreviation per training/testing corpus and
the other -- multiple abbreviations per
corpus. Both sets show around 89%
accuracy results when tested on the held-out
data.
2 Clinical Data
The data that was used for this study
consists of a corpus of ~10,000 clinical
notes (medical dictations) extracted at
random from a larger corpus of 171,000
notes (~400,000 words) and encompasses
one of many medical specialties at the Mayo
Clinic ? rheumatology. In the Mayo Clinic?s
setting, each clinical note is a document
recording  information pertinent to treatment
of a patient that consists of a number of
subsections such as Chief Complaint (CC),
History of Present Illness (HPI),
Impresssion/Report/Plan (IP), Final
Diagnoses (DX)3, to name a few. In clinical
settings other than the Mayo Clinic, the
notes may have different segmentation and
section headings; however, most clinical
notes in most clinical settings do have some
sort of segmentation and contain some sort
of discourse markers, such as CC, HPI, etc.,
that can be useful clues to tasks such as the
one discussed in this paper. Theoretically, it
is possible that an abbreviation such as PA
may stand for ?paternal aunt? in the context
of Family History (FH), and ?polyarthritis?
in the Final Diagnoses context. ME
technique lends itself to modeling
information that comes from a number of
heterogeneous sources such as various
levels of local and discourse context.
3 Methods
One of the challenging tasks in text
normalization discussed in the literature is
the detection of abbreviations in unrestricted
text. Various techniques, including ME,
have proven useful for detecting
abbreviations with varying degrees of
success. (Mikheev 1998, 2000, Park and
                                                
3 This format is specific to the Mayo Clinic. Probably
the most commonly used format outside of Mayo is
the so-called SOAP format that stands for Subjective,
Objective, Assessment, Plan. The idea is the same,
but the granularity is lower.
Byrd 2001) It is important to mention that
the methods described in this paper are
different from abbreviation detection;
however, they are meant to operate in
tandem with abbreviation detection
methods.
Two types of methods will be
discussed in this section. First, I will briefly
introduce the Maximum Entropy modeling
technique and then the method I used for
generating the training data for ME
modeling.
3.1 Maximum Entropy
This section presents a brief description of
ME. A more detailed and informative
description can be found in Berger (1996)4,
Ratnaparkhi (1998), Manning and Shutze
(2000) to name just a few.
Maximum Entropy is a relatively
new statistical technique to Natural
Language Processing, although the notion of
maximum entropy has been around for a
long time. One of the useful aspects of this
technique is that it allows to predefine the
characteristics of the objects being modeled.
The modeling involves a set of predefined
features or constraints on the training data
and uniformly distributes the probability
space between the candidates that do not
conform to the constraints. Since the
entropy of a uniform distribution is at its
maximum, hence the name of the modeling
technique.
Features are represented by indicator
functions of the following kind5:
(1)  
?
?
?
==
= otherwise
ycandxoifcoF ,0
,1),(
Where ?o? stands for outcome and ?c?
stands for context. This function maps
contexts and outcomes to a binary set. For
                                                
4 This paper presents an Improved Iterative Scaling
but covers the Generalized Iterative Scaling as well.
5 Borrowed from Ratnaparkhi implementation of
POS tagger.
example, to take a simplified part-of-speech
tagging example, if y = ?the? and x=?noun?,
then F(o,c) = 1, where y is the word
immediately preceding x. This means that in
the context of ?the? the next word is
classified as a noun.
To find the maximum entropy
distribution the Generalized Iterative
Scaling (GIS) algorithm is used, which is a
procedure for finding the maximum entropy
distribution that conforms to the constraints
imposed by the empirical distribution of the
modeled properties in the training data6.
For the study presented in this paper, I used
an implementation of ME that is similar to
that of Ratnaparkhi?s and has been
developed as part of the open source Maxent
1.2.4 package7. (Jason Baldridge, Tom
Morton, and Gann Bierner,
http://maxent.sourceforge.net). In the
Maxent implementation, features are
reduced to contextual predicates,
represented by the variable y in (1). Just as
an example, one of such contextual
predicates could be the type of discourse
that the outcome ?o? occurs in:  PA 
paternal aunt | y = FH; PA  polyarthritis |
y = DX. Of course, using discourse markers
as the only contextual predicate may not be
sufficient. Other features such as the words
surrounding the abbreviation in question
may have to be considered as well.
For this study two kinds of models
were trained for each data set: local context
models (LCM) and combo (CM) models.
The former were built by training on the
sentence-level context only defined as two
preceding (wi-2,wi-1)  and  two following
(wi+1,wi+2) words surrounding an
abbreviation expansion. The latter kind is a
model trained on a combination of sentence
and section level contexts defined simply as
                                                
6 A consice step-by-step description and an
explanation of the algorithm itself can be found in
Manning and Shutze (2000).
7 The ContextGenerator class of the maxent package
was modified to allow for the features discussed in
this paper.
the heading of the section in which an
abbreviation expansion was found.
3.2 Generating simulated training data
In order to generate the training data,
first, I identify potential candidates for  an
abbreviation by taking the list of expansions
from a UMLS database and applying it to
the raw corpus of text data in the following
manner. The expansions for each
abbreviation found in the UMLS?s LRABR
table are loaded into a hash indexed by the
abbreviation.
ABBR EXPANSIONS FOUND IN
DATA
NR normal range; no radiation; no
recurrence; no refill; nurse; nerve
root; no response; no report;
nonreactive; nonresponder
PA Polyarteritis; pseudomonas
aeruginosa; polyarthritis;
pathology; pulmonary artery;
procainamide; paternal aunt; panic
attack; pyruvic acid; paranoia;
pernicious anemia; physician
assistant; pantothenic acid; plasma
aldosterone; periarteritis
PN Penicillin; pneumonia; polyarteritis
nodosa; peripheral neuropathy;
peripheral nerve; polyneuropathy
pyelonephritis; polyneuritis;
parenteral nutrition; positional
nystagmus; periarteritis nodosa
BD band; twice a day; bundle
INF Infection; infected; infusion;
interferon; inferior; infant; infective
RA Rheumatoid arthritis; renal artery;
radioactive; right arm; right atrium;
refractory anemia; rheumatic
arthritis; right atrial
Table 1. Expansions found in the training
data and their abbreviations found in
UMLS.
The raw text of clinical notes is input
and filtered through a dynamic sliding-
window buffer whose maximum window
size is set to the maximum length of any
abbreviation expansion in the UMLS. When
a match to an expansion is found, the
expansion and it?s context are recorded in a
training file as if the expansion were an
actual abbreviation. The file is fed to the
ME modeling software. In this particular
implementation, the context of 7 words to
the left and 7 words to the right of the found
expansion as well as the section label in
which the expansion occurs are recorded;
however, not all of this context ended up
being used in this study.
This methodology makes a reasonable
assumption that given an abbreviation and
one of it?s expansions, the two are likely to
have similar distribution. For example, if we
encounter a phrase like ?rheumatoid
arthritis?, it is likely that the context
surrounding the use of an expanded phrase
?rheumatoid arthritis? is similar to the
context surrounding the use of the
abbreviation ?RA? when it is used to refer to
rheumatoid arthritis. The following
subsection provides additional motivation
for using expansions to simulate
abbreviations.
3.2.1 Distribution of abbreviations compared
to the distribution of their expansions
Just to get an idea of how similar are the
contexts in which abbreviations and their
expansions occur, I conducted the following
limited experiment. I processed a corpus of
all available rheumatology notes (171,000)
and recorded immediate contexts composed
of words in positions {wi-1, wi-2 ,wi+1, wi+2}
for one unambiguous abbreviation ? DJD
(degenerative joint disease). Here wi is
either the abbreviation DJD or its multiword
expansion ?degenerative joint disease.?
Since this abbreviation has only one
possible expansion, we can rely entirely on
finding the strings ?DJD? and ?degenerative
joint disease? in the corpus without having
to disambiguate the abbreviation by hand in
each instance. For each instance of the
strings ?DJD? and ?degenerative joint
disease?, I recorded the frequency with
which words (tokens) in positions wi-1, wi-2,
wi+1 and wi+2 occur with that string as well as
the number of unique strings (types) in these
positions.
It turns out that ?DJD? occurs 2906
times , ?degenerative joint disease? occurs
2517 times. Of the 2906 occurrences of
DJD, there were 204 types that occurred
immediately prior to mention of DJD (wi-1
position) and 115 types that occurred
immediately after (wi+1 position). Of the
2517 occurrences of ?degenerative joint
disease?, there were 207 types that occurred
immediately prior to mention of the
expansion (wi-1 position) and 141 words that
occurred immediately after (wi+1 position).
The overlap between DJD and its expansion
is 115 types in wi-1 position and 66 types in
wi+1 position. Table 2 summarizes the results
for all four {wi-1, wi-2 ,wi+1, wi+2} positions.
Context Context
overlap
N of
unique
contexts
Context
similarity
(%)
Wi-1
DJD 115 204 56
degen. joint dis 115 207 55
Mean 55.5
Wi+1
DJD 66 115 50
degen. joint dis 66 141 46
Mean 48
Wi-2
DJD 189 371 50
degen. joint dis 189 410 46
Mean 48
Wi+2
DJD 126 245 51
degen. joint dis 126 301 41
Mean 46
Total 49.37
Table 2. DJD vs. ?degenerative joint
disease? distribution comparison.
On average, the overlap between the
contexts in which DJD and ?degenerative
joint disease? occur is around 50%, which is
a considerable number because this overlap
covers on average 91% of all occurrences in
wi-1 and wi+1 as well as wi-2 and wi+2
positions.
3.2.2 Data sets
One of the questions that arose during
implementation is whether it would be better
to build a large set of small ME models
trained on sub-corpora containing context
for each abbreviation of interest separately
or if it would be more beneficial to train one
model on a single corpus with contexts for
multiple abbreviations.
This was motivated by the idea that
ME models trained on corpora focused on a
single abbreviation may perform more
accurately; even though such approach may
be computationally expensive.
ABBR N OF UMLS
EXPANSIONS
N OF
OBSERVED
EXPANSIONS
NR 23 10
PA 72 15
PN 28 11
BD 30 3
INF 13 7
RA 28 8
Mean 32.33 9
Table 3. A comparison between UMLS
expansions for 6 abbreviations and the
expansions actually found in the training
data.
For this study, I generated two sets of
data. The first set (Set A) is composed of
training and testing data for 6 abbreviations
(NR, PA, PN, BD, INF, RA), where each
training/testing subset contains only one
abbreviation per corpus. resulting in six
subsets. Table 1 shows the potential
expansions for these abbreviations that were
actually found in the training corpora.
Not all of the possible expansions
found in the UMLS for a given
abbreviations will be found in the text of the
clinical notes. Table 3 shows the number of
expansions actually found in the
rheumatology training data for each of the 6
abbreviations listed in Table 1 as well as the
expansions found for a given abbreviation in
the UMLS database.
The UMLS database has on average
3 times more variability in possible
expansions that were actually found in the
given set of training data. This is not
surprising because the training data was
derived from a relatively small subset of
10,000 notes.
The other set (Set B) is similar to the
first corpus of training events; however, it is
not limited to just one abbreviation sample
per corpus. Instead, it is compiled of
training samples containing expansions from
69 abbreviations. The abbreviations to
include in the training/testing were selected
based on the following criteria:
a. has at least two expansions
b. has 100-1000 training data samples
The data compiled for each set and
subset was split at random in the 80/20
fashion into training and testing data. The
two types of ME models (LCM and CM)
were trained for each subset on 100
iterations through the data with no cutoff
(all training samples used in training).
4 Testing
To summarize the goals of this study, one of
the main questions in this study is whether
local sentence-level context can be used
successfully to disambiguate abbreviation
expansion. Another question that naturally
arose from the structure of the data used for
this study is whether more global section-
level context indicated by section headings
such as ?chief complaint?, ?history of
present illness? ,  etc., would have an effect
on the accuracy of predicting the
abbreviation expansion. Finally, the third
question is whether it is more beneficial to
construct multiple ME models limited to a
single abbreviation. To answer these
questions, 4 sets of tests were conducted:
1. Local Context Model and Set A
2. Combo Model and Set A
3. Local Context Model and Set B
4. Combo Model and Set B
4.1 Results
Table 3 summarizes the results of training
Local Context models with the data from
Set A (one abbreviation per corpus).
ABBR Acc.(%)
Test
Event
Train
Events
Out. Predic.
 NR 87.87 139.6 495.7 10.8 580.4
 PN 77.05 166.2 612.7 11 722.5
 BD 98.49 174.4 724.6 3 704.8
 PA 86.45 182.8 653.3 13.9 707.1
 INF 87.33 196.2 819.3 6.9 950.3
 RA 97.67 924.6 2535 7.6 1549.4
Mean 89.14 297.3 973.43 8.87 869.08
Table 3. Local Context Model and Set A
results
The results in Table 3 show that, on average,
after a ten-fold cross-validation test, the
expansions for the given 6 abbreviations
have been predicted correctly 89.14%.
ABBR Acc.(%)
Test
Event
Train
Events
Out. Predic.
 NR 89.515 139.6 504.6 10.8 589.4
 PN 78.739 166.2 618.7 11 746.1
 BD 98.39 174.4 736.6 3 713.8
 PA 86.193 182.8 692.2 13.9 717
 INF 87.409 196.2 842.3 7 959.8
 RA 97.693 924.6 2704 7.6 1559.4
Mean 89.66 297.3 1016.4 8.88 880.92
Table 4. Combo Model and Set A results
Table 3 as well as table 4 display the
accuracy, the number of training and testing
events/samples, the number of outcomes
(possible expansions for a given
abbreviation) and the number of contextual
predicates averaged across 10 iterations of
the cross-validation test.
Table 4 presents the results of the
Combo approach with the data also from Set
A. The results of the combined discourse +
local context approach are only slightly
better that those of the sentence-level only
approach.
Table 5 displays the results for the set
of tests performed on data containing
multiple abbreviations ? Set B but contrasts
the Local Context Model with the Combo
Model.
Acc.
(%)
Test
Event
Train
Event
Out. Pred.
LCM 89.169 ~4791 ~21999 ~250 ~9400
CM 89.015 ~4792 ~22000 ~251 ~9401
Table 5. Local Context Model
performance contrasted to Combo model
performance on Set B
The first row shows that the LCM model
performs with 89.17% accuracy. CM?s
result is very close: 89.01%. Just as with
Tables 3 and 4, the statistics reported in
Table 5 are averaged across 10 iterations of
cross-validation.
5 Discussion
The results of this study suggest that using
Maximum Entropy modeling for
abbreviation disambiguation is a promising
avenue of research as well as technical
implementation for text normalization tasks
involving abbreviations. Several
observations can be made about the results
of this study. First of all, the accuracy
results on the small pilot sample of 6
abbreviations as well as the larger sample
with 69 abbreviations are quite encouraging
in light of the fact that the training of the
ME models is largely unsupervised8.
                                                
8 With the exception of having to have a database of
acronym/abbreviations and their expansions which
has to be compiled by hand. However, once such list
is compiled, any amount of data can be used for
training with no manual annotation.
Another observation is that it
appears that using section-level context is
not really beneficial to abbreviation
expansion disambiguation in this case. The
results, however, are not by any means
conclusive. It is entirely possible that using
section headings as indicators of discourse
context will prove to be beneficial on a
larger corpus of data with more than 69
abbreviations.
The abbreviation/acronym database in
the UMLS tends to be more comprehensive
than most practical applications would
require. For example, the Mayo Clinic
regards the proliferation of abbreviations
and acronyms with multiple meanings as a
serious patient safety concern and makes
efforts to ensure that only the ?approved?
abbreviations (these tend to have lower
ambiguity) are used in clinical practice,
which would also make the task of their
normalization easier and more accurate. It
may still be necessary to use a combination
of the UMLS?s and a particular clinic?s
abbreviation lists  in order to avoid missing
occasional abbreviations that occur in the
text but have not made it to the approved
clinic?s list. This issue also remains to be
investigated.
6 Future Work
In the future, I am planning to test
the assumption that abbreviations and their
expansions occur in similar contexts by
testing on hand-labeled data. I also plan to
vary the size of the window used for
determining the local context from two
words on each side of the expression in
question as well as the cutoff used during
ME training. It will also be necessary to
extend this approach to other medical and
possibly non-medical domains with larger
data sets. Finally, I will experiment with
combining the UMLS abbreviations table
with the Mayo Clinic specific abbreviations.
References
Baldridge, J., Morton, T., and Bierner, G URL:
http://maxent.sourceforge.net
Berger,  A., Della Pietra, S., and Della Pietra, V.
(1996). A maximum entropy approach to
natural language processing. Computational
Linguistics, 22(1):39-71.
Black, E. (1988). An experiment in computational
discrinmination of English word senses.
IBM Journal of Research and Development,
32(2), 185-194.
Friedman, C., Liu, H., Shagina, L., Johnson, S. and
Hripcsack, G. (2001) Evaluating the UMLS
as a Source of Lexical Knowledge for
Medical Language Processing. In Proc
AMIA 2001.
Hearst, M. (1991). Noun homograph disambiguation
using local context in large text corpora. In
Proc. 7th Annual Conference of the
University of Waterloo Center for the new
OED and Text Research, Oxford.
Ide, N and Veronis, J. (1998). Word sense
disambiguation: the state of the art.
Computational Linguistics, 24(1).
Liu, H., Lussier, Y., and Friedman, C. (2001) A
Study of Abbreviations in UMLS. In Proc.
AMIA 2001.
Mikheev, A. (2000). Document Centered Approach
to Text Normalization. In Proc.  SIGIR
2000.
Mikheev, A. (1998). Feature Lattices for Maximum
Entropy Modeling. In Proc. ACL 1998.
Manning, C. and Shutze H. (1999). Foundations of
Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Park, Y and Byrd, R. (2001). Hybrid text Mining for
Finding Abbreviations and their Definitions.
In Proc. EMNLP 2001.
Ratnaparkhi A. (1996). A maximum entropy part of
speech tagger. In Proceedings of the
conference on empirical methods in natural
language processing, May 1996, University
of Pennsylvania
Ratnaparkhi A. (1998). Maximum Entropy Models
for Natural Language Ambiguity
Resolution. Ph. D. Thesis, U of Penn.
Jurafski D. and Martin J. (2000). Speech and
Language Processing. Prentice Hall, NJ.
Yarowski, D. (1995). Unsupervised word sense
disambiguation rivaling supervised
methods. In Proc. ACL-95, 189-196.
UMLS. (2001). UMLS Knowledge Sources (12th
ed.). Bethesda (MD) : National Library of
Medicine.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 25?28, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
High Throughput Modularized NLP System for Clinical Text 
 
Serguei Pakhomov James Buntrock Patrick Duffy 
Mayo College of Medicine Division of Biomedical Infor-
matics 
Division of Biomedical 
Informatics 
Mayo Clinic Mayo Clinic Mayo Clinic 
Rochester, MN, 55905 Rochester, MN, 55905 Rochester, MN, 55905 
pakhomov@mayo.edu Buntrock@mayo.edu duffp@mayo.edu 
  
Abstract 
This paper presents the results of the de-
velopment of a high throughput, real time 
modularized text analysis and information 
retrieval system that identifies clinically 
relevant entities in clinical notes, maps 
the entities to several standardized no-
menclatures and makes them available for 
subsequent information retrieval and data 
mining. The performance of the system 
was validated on a small collection of 351 
documents partitioned into 4 query topics 
and manually examined by 3 physicians 
and 3 nurse abstractors for relevance to 
the query topics. We find that simple key 
phrase searching results in 73% recall and 
77% precision. A combination of NLP 
approaches to indexing improve the recall 
to 92%, while lowering the precision to 
67%.   
1 Introduction 
Until recently the NLP systems developed for 
processing clinical texts have been narrowly fo-
cused on a specific type of document such as radi-
ology reports [1], discharge summaries [2], 
medline abstracts [3], pathology reports [4]. In ad-
dition to being developed for a specific task, these 
systems tend to fairly monolithic in that their com-
ponents have fairly strict dependencies on each 
other, which make plug-and-play functionality dif-
ficult. NLP researchers and systems developers in 
the field realize that modularized approaches are 
beneficial for component reuse and more rapid de-
velopment and advancement of NLP technology. 
In addition to the issue of modularity, the NLP sys-
tems development efforts are starting to take scal-
ability into account. The Mayo Clinic?s repository 
of clinical notes contains over 16 million docu-
ments growing at the rate of 50K documents per 
week. The time and space required for processing 
these large amounts of data impose constraints on 
the complexity of NLP systems.  
Another engineering challenge is to make the 
NLP systems work in real time. This is particularly 
important in a clinical environment for patient re-
cruitment or patient identification for clinical re-
search use cases. In order to satisfy this 
requirement, a text processing system has to inter-
face with the Electronic Health Record (EHR) sys-
tem in real time and process documents 
immediately after they become available electroni-
cally. All of these are non-trivial issues and are 
currently being addressed in the community. In this 
poster we present the design and architecture of a 
large-scale, highly modularized, real-time enabled 
text analysis system as well as experimental vali-
dation results.  
2 System Description 
Mayo Clinic and IBM have collaborated on a 
Text Analytics project as part of a strategic Life 
Sciences and Computational Biology partnership.  
The goal of the Text Analytics collaboration was to 
provide a text analysis system that would index 
and retrieve clinical documents at the Mayo Clinic.   
The Text Analytics architecture leveraged ex-
isting interface feeds for clinical documents by 
routing them to the warehouse.  A work manager 
was written using messaging queues to distribute 
work for text analysis for real-time and bulk proc-
essing (see Figure 1).   Additional text analysis 
engines can be configured and added with appro-
priate hardware to increase document throughput 
of the system.    
25
 
Figure 1- Text Analysis Process Flow 
For deployment of text analysis engines we tested 
two configurations.  During the development phase 
we used synchronous messaging using Apache 
Web Server with Tomcat/Axis.  The Apache Web 
server provided a round robin mechanism to dis-
tributed SOAP requests for text analysis.  This test-
ing was deployed on a 20 CPU Beowulf cluster 
using AMD Athlon? processors running Linux 
operating system.  For production deployment we 
used Message Driven Beans (MDBs)using IBM 
Websphere Application Server? (WAS) and IBM 
Websphere Message Queue?.  The text engines 
were deployed on 2-CPU blade servers with 4Gb 
RAM.  Each WAS instance had two MDBs with 
text analysis engines. 
Work was distributed using message queues.  Each 
text analysis engine was deployed to function in-
dependent of other engines.   A total of 20 blade 
servers were configured for text processing.  The 
average document throughput for each blade was 
20 documents per minute. 
 
The text analysis engine was designed by concep-
tually breaking up the task into granular functions 
that could be implemented as components to be 
assembled into a text processing system.  
To implement the components we used an 
IBM AlphaWorks package called Unstructured 
Information Management Architecture (UIMA).  
UIMA is a software architecture that defines roles, 
interface, and communications of components for 
natural language processing.  The four main UIMA 
services include: acquisition, unstructured informa-
tion analysis, structured information access, and 
component discovery.  For the Mayo project we 
used the first three services.  The ability to custom-
ize annotator sequences was advantageous during 
the design process.  Also, the ability to add annota-
tors for specific dictionaries amounted only in mi-
nor work. Once annotators are written to 
conformance, UIMA provides pipeline develop-
ment and permits the developer to quickly custom-
ize processing to a specific task.  The final annota-
tor layout is depicted in Figure 2. 
The context free tokenizer is a finite state 
transducer that parses the document text into the 
smallest meaningful spans of text. A token is a set 
of characters that can be classified into one of 
these categories: word, punctuation, number, con-
traction, possessive, symbol without taking into 
account any additional context. 
The context sensitive spell corrector annotator 
is used for automatic spell correction on word to-
kens.  This annotator uses a combination of iso-
lated-word and context-sensitive statistical 
approaches to rank the possible suggestions [5].  
The suggestion with the highest ranking is stored 
as a feature of a token.   
 
Figure 2 ? Text Analysis Pipeline 
The lexical normalizer annotator is applied 
only to words, possessives, and contractions.  It 
generates a canonical form by using the National 
Library of Medicine UMLS Lexical Variant Gen-
erator (LVG) tool1. Apart from generating lexical 
variants and stemming optimized for the biomedi-
cal domain, it also generates a list of lemma entries 
with Penn Treebank tags as input for the POS tag-
ger. 
The sentence detector annotator parses the 
document text into sentences.  The sentence detec-
tor is based on a Maximum Entropy classifier 
technology2 and is trained to recognize sentence 
boundaries from hand annotated data. 
                                                          
1 http://umlslex.nlm.nih.gov  
2 http://maxent.sourceforge.net/ 
26
The context dependent tokenizer uses context 
to detect complex tokens such as dates, times, and 
problem lists3.  
The part of speech (POS) pre-tagger annotator 
is intended to execute prior to the POS tagger an-
notator.  The pre-tagger loads a list of words that 
are unambiguous with respect to POS and have 
predetermined Penn Treebank tags.  Words in the 
document text are tagged with these predetermined 
tags.  The POS tagger can ignore these words and 
focus on the remaining syntactically ambiguous 
words. 
The POS tagger annotator attaches a part of 
speech tag to each token. The current version of 
the POS tagger is from IBM based on Hidden 
Markov models technology.  This tagger has been 
trained on a combination of the Penn Treebank 
corpus of general English and a corpus of manually 
tagged clinical data developed at the Mayo Clinic 
[6], [7]. 
The shallow parser annotator makes higher 
level constructs at the phrase level.  The Shallow 
Parser is from IBM.  The shallow parser uses a set 
of rules operating on tokens and their part-of-
speech category to identify linguistic phrases in the 
text such as noun phrases, verb phrases, and adjec-
tival phrases.   
The dictionary named entity annotator uses a 
set of enriched dictionaries (SNOMED-CT, MeSH, 
RxNorm and Mayo Synonym Clusters (MSC) to 
lookup named entities in the document text.  These 
named entities include drugs, diagnoses, signs, and 
symptoms.  The MSC database contains a set of 
clusters each consisting of diagnostic statements 
that are considered to be synonymous. Synonymy 
here is defined as two or more terms that have been 
manually classified to the same category in the 
Mayo Master Sheet repository, which contains 
over 20 million manually coded diagnostic state-
ments. These diagnostic statements are used as 
entry terms for dictionary lookup. A set of Mayo 
compiled dictionaries are also used to detect ab-
breviations and hyphenated terms.  
The abbreviation disambiguation annotator at-
tempts to detect and expand abbreviations and ac-
ronyms based on Maximum Entropy classifiers 
trained on automatically generated data [8].  
                                                          
3 Problem lists typically consist of numbered items in the Im-
pression/Report/Plan section of the clinical notes  
The negation annotator assigns a certainty at-
tribute to each named entity with the exception of 
drugs. This annotator is based on a generalized 
version of Chapman?s NegEx algorithm [9].   
The ML (Machine Learning) Named Entity 
annotator is based on a Na?ve Bayes classifier 
trained on a combination of the UMLS entry terms 
and the MCS where each diagnostic statement is 
represented as a bag-of-words and used as a train-
ing sample for generating a Naive Bayes classifier 
which assigns MCS id?s to noun phrases identified 
in the text of clinical notes. The architecture of this 
component is given in Figure 3. 
Text 
Dictionary Lookup 
Found 
Noun Phrase Head identifier 
Na?ve Bayes classifier 
Best guess cluster
Mayo Synonym Clusters
M001|cholangeocarcinoma 
M001|bile duct cancer 
M001|? Y N 
 
Figure 3. ML Named Entity Classifier 
 
The text of a clinical note is first looked up in the 
MSC database using the dictionary named entity 
annotator. If a span of text matched something in 
the database, then the span is marked as a named 
entity annotation and the appropriate cluster ID is 
assigned to it. The portions of text where no match 
was found continue to be processed with a named 
entity identification algorithm that relies on the 
output of the shallow parser annotator to find 
noun phrases whose heads are on a list of nouns 
that exist in the MSC database as individual manu-
ally coded entries. For example, a noun phrase 
such as ?metastasized cholangiocarcinoma? will be 
identified as a named entity and subsequently 
automatically classified, but a noun phrase such as 
?patient?s father? will not.  
3 Evaluation 
The system performance was evaluated using a 
collection of 351 documents partitioned into 4 top-
ics: pulmonary fibrosis, cholangiocarcinoma, dia-
betes mellitus and congestive heart failure. Each of 
27
the topics contained approximately 90 documents 
that were manually examined by three nurse ab-
stractors and three physicians. Each note was 
marked as either relevant or not relevant to a given 
topic. In order to establish the reliability of this test 
corpus, we used a standard weighted Kappa statis-
tic [10]. The overall Kappa for the four topics were 
0.59 for pulmonary fibrosis, 0.79 for cholangiocar-
cinoma, 0.79 for diabetes mellitus and 0.59 for 
congestive heart failure. We ran a set of queries for 
each of the 4 topics on the partition generated for 
that topic. Each query used the primary term that 
represented the topic. For example, for pulmonary 
fibrosis, only the term ?pulmonary fibrosis? was 
used while other closely related terms such as ?in-
terstitial pneumonitis? were excluded. The baseline 
query was executed using the term as a key phrase 
on the original text of the documents. The rest of 
the queries were executed using the concept id?s 
automatically generated for each primary term. On 
the back end, the text of the clinical notes was an-
notated with the Metamap program [3] for the 
UMLS concepts and the ML Named Entity annota-
tor for MSC cluster id?s. On the front end, the 
UMLS concept id?s were generated via the UMLS 
Knowledge Server online and the MSC id?s were 
generated using a combination of the same Na?ve 
Bayes classifier and the same dictionary lookup 
mechanism as were used to annotate the clinical 
notes. We also tested a query that combined 
Metamap and MSC annotations and query parame-
ters. Recall, precision and f-score (?=0.5) were 
calculated for each query. The results are summa-
rized in Table 1. 
 Precision Recall F-score 
Key Phrase 0.77 0.73 0.749467 
MSC cluster 0.67 0.89 0.764487 
Metamap 0.71 0.84 0.769548 
Metamap+MSC 0.67 0.92 0.775346 
Table 1. Performance of different annotation methods. 
 
The f-score results are fairly close for all methods; 
however, the recall is highest for the method that 
combines Metamap and the MSC methodology. 
This is particularly important for using this system 
in recruiting patients for epidemiological research 
for disease  incidence or disease prevalence studies 
and clinical trials where recall is valued more than 
precision. A combination of Metamap and MSC 
annotations and queries produced the highest recall 
which shows that these systems are complemen-
tary. The modular design of our system makes it 
easy to incorporate complementary annotation sys-
tems like Metamap into the annotation process. 
Acknowledgements 
The authors wish to thank the Mayo Clinic 
Emeritus Staff Physicians and Nurse Abstractors 
who served as experts for this study.  The authors 
also wish to thank Patrick Duffy for programming 
support and David Hodge for statistical analysis 
and interpretation. 
References  
1. Friedman, C., et al, A general natural-language text 
processor for clinical radiology. Journal of Ameri-
can Medical Informatics Association, 1994. 1(2): p. 
161-174. 
2. Friedman, C. Towards a Comprehensive Medical 
Language Processing System: Methods and Issues. 
in American Medical Informatics Association 
(AMIA). 1997. 
3. Aronson, A. Effective mapping of biomedical text to 
the UMLS Metathesaurus: the MetaMap program. in 
Proceedings of the 2001 AMIA Annual Symposium. 
2001. Washington, DC. 
4. Mitchell, K. and R. Crowley. GNegEx ? Implemen-
tation and Evaluation of a Negation Tagger for the 
Shared Pathology Iinformatics Network. in Advanc-
ing Practice, Instruction and Innovation through In-
formatics (APIII). 2003. 
5. Thompson-McInness, B., S. Pakhomov, and T. 
Pedersen. Automating Spelling Correction Tools Us-
ing Bigram Statistics. in Medinfo Symposium. 2004. 
San Francisco, CA, USA. 
6. Coden, A., et al, Domain-specific language models 
and lexicons for tagging. In print in Journal of Bio-
medical Informatics, 2005. 
7. Pakhomov, S., A. Coden, and C. Chute, Developing 
a Corpus of Clinical Notes Manually Annotated for 
Part-of-Speech. To appear in International Journal of 
Medical Informatics, 2005(Special Issue on Natural 
Language Processing in Biomedical Applications). 
8. Pakhomov, S. Semi-Supervised Maximum Entropy 
Based Approach to Acronym and Abbreviation Nor-
malization in Medical Texts. in 40th Meeting of the 
Association for Computational Linguistics (ACL 
2002). 2002. Philadelohia, PA. 
9. Chapman, W.W., et al Evaluation of Negation 
Phrases in Narrative Clinical Reports. in American 
Medical Informatics Association. 2001. Washington, 
DC, USA. 
10. Landis, J.R. and G.G. Koch, The Measurement of 
Observer Agreement for Categorical Data. Biomet-
rics, 1977. 33: p. 159-174. 
28
Identification of Patients with Congestive Heart Failure using a
binary classifier: a case study.
Serguei V. Pakhomov
Division of Medical
Informatics Research
Mayo Foundation
pakhomov@mayo.edu
James Buntrock
Division of Medical
Informatics Research
Mayo Foundation
buntrock@mayo.edu
Christopher G. Chute
Division of Medical
Informatics Research
Mayo Foundation
chute@mayo.edu
Abstract
This paper addresses a very specific
problem that happens to be  common in
health science research. We present a
machine learning based method for
identifying patients diagnosed with
congestive heart failure and other related
conditions by automatically classifying
clinical notes. This method relies on a
Perceptron neural network classifier
trained on comparable amounts of
positive and negative samples of clinical
notes previously categorized by human
experts. The documents are represented as
feature vectors where features are a mix
of single words and concept mappings to
MeSH and HICDA ontologies. The
method is designed and implemented to
support a particular epidemiological study
but has broader implications for clinical
research. In this paper, we describe the
method and present experimental
classification results based on
classification accuracy and positive
predictive value.
1 Introduction
Epidemiological research frequently has to deal
with collecting a comprehensive set of human
subjects that are deemed relevant for a particular
study. For example, the research focused on
patients with congestive heart failure needs to
identify all possible candidates for the study so that
the candidates could be asked to participate. One
of the requirements of a study like that is the
completeness of the subject pool. In many cases,
such as disease incidence or prevalence studies, it
is not acceptable for the investigator to miss any of
the candidates. The identification of the candidates
relies on a large number of sources some of which
do not exist in an electronic format, but it may start
with the clinical notes dictated by the treating
physician.   
Another aspect of candidate identification is
prospective patient recruitment. Prospective
recruitment is based on inclusion or exclusion
criteria and is of great interest to physicians for
enabling just-in-time treatment, clinic trial
enrollment, or research study options for patients.
At Mayo Clinic most clinical documents are
transcribed within 24 hours of patient consultation.
This electronic narration serves as resource for
enabling prospective recruitment based on criteria
present in clinical document.
Probably the most basic approach to
identification of candidates for recruitment is to
develop a set of terms whose presence in the note
may be indicative of the diagnoses of interest.
This term set can be used as a filtering mechanism
by either searching on an indexed collection of
clinical notes or simply by doing term spotting if
the size of the collection would allow it. For
example, in case of congestive heart failure, one
could define the following set of search terms:
?CHF?, ?heart failure?, ?cardiomyopathy?,
?volume overload?, ?fluid overload?, ?pulmonary
edema?, etc. The number of possible variants is
virtually unlimited, which is the inherent problem
with this approach. It would be hard to guarantee
the completeness of this set to begin with, which is
further complicated by morphological and spelling
variants. This problem is serious because it affects
the recall, which is especially important in
epidemiological studies.
Another problem is that such term spotting or
indexing approach would have to be intelligent
enough to identify the search terms in negated and
other contexts that would render documents
containing these terms irrelevant. A note
containing ?no evidence of heart failure? should
not be retrieved, for example. Identifying negation
reliably and, more importantly, its scope is far
from trivial and is in fact a notoriously difficult
problem in Linguistics [1]. This problem is slightly
less serious than the completeness problem since it
only affects precision which is less important in the
given context than recall.
In order to be able to correctly identify whether
a given patient note contains evidence that the
patient is relevant to a congestive heart failure
study, one has to ?understand? the note. Currently,
there are no systems capable of human-like
?understanding? of natural language; however,
there are methods that allow at least partial
solutions to the language understanding problem
once the problem is constrained in very specific
ways.  One such constraint is to treat language
understanding as a classification problem and to
use available machine learning approaches to
automatic classification to solve the problem.
Clearly, this is a very limited view of language
understanding but we hypothesize that it is
sufficient for the purposes referred to in this paper.
2 Previous work
The classification problems that have been
investigated in the past are just as varied as the
machine learning algorithms that have been used to
solve these problems. Linear Least Squares Fit [2],
Support Vector Machines, Decision trees,
Bayesean learning [3], symbolic rule induction [4],
maximum entropy [5], expert networks [6] are just
a few that have been applied to classifying e-mail,
Web pages, newswire articles, medical reports
among other documents.
Aronow et al [7] have investigated a problem
very similar to the one described in this papers.
They developed an ad hoc classifier based on a
variation of relevance feedback technique for
mammogram reports where the reports were
classified into three ?bins?: relevant, irrelevant and
unsure. One of the features of the text processing
system they used had to do with the ability to
detect and take into account negated elements of
the reports.
Wilcox et al [8] have experimented with a
number of classification algorithms for identifying
clinical conditions such as congestive heart failure,
chronic obstructive pulmonary disease, etc. in
raidograph reports. They found that using an NLP
system such as MedLEE (Medical Language
Extraction and Encoding System) and domain
knowledge sources such as UMLS? [9] for feature
extraction can significantly improve classification
accuracy over the baseline where single words are
used to represent training samples.
Jain and Friedman [10] have demonstrated the
feasibility of using MedLEE for classifying
mammogram reports. Unlike Wilcox  [8], this
work does not use an automatic classifier, instead,
it uses the NLP system to identify findings that are
considered suspicious for breast cancer.
3 NaiveBayes vs. Perceptron
We experimented with two widely used
machine learning algorithms, Perceptron and Na?ve
Bayes, in order to train models capable of
distinguishing between clinical notes that contain
sufficient evidence of the patient having the
diagnosis of congestive heart failure (positive
examples) from notes that do not contain such
evidence (negative examples). The choice of the
problem was dictated by a specific grant aimed at
studying patients with congestive heart failure.
The choice of the algorithms was largely
dictated by efficiency considerations. Both
Perceptron and Na?ve Bayes belong to a family of
linear classifiers which tend to be computationally
more manageable on large feature sets like the one
we are addressing than other algorithms. Damerau
et al [11] show on the Reuters corpus that sparse
feature implementations of linear algorithms are
capable of handling large feature sets. We used a
sparse feature implementation of these two
algorithms available in the SNoW (Sparse
Networks of Winnows) Version 2.1.2 package
[12]. Perceptron and Na?ve Bayes classifiers.
Perceptron is a simple iterative learning
algorithm that represents in its simplest form a
two-layer (input/output) neural network where
each node in the input layer is connected to each
node in the output layer. A detailed description can
be found in [13] and [14]. There are several well
known limitations of this algorithm. The most
significant is that the simple Perceptron is unable
to learn non-linearly separable problems. In order
for this algorithm to work, one should be able to
draw a hyperplane in the training data feature
space that will linearly separate positive examples
from negative. With large multidimensional feature
spaces, it is hard to know a priori whether the
space is linearly separable; however, a good
indication of that can be gleaned from the
classification accuracy testing on several folds of
training/testing data. If the accuracy results show
large fluctuations between folds, then that would
be a good indication that the space is not linearly
separable. On the other hand if the standard
deviation on such a cross-validation task is
relatively small, then one could be reasonably
certain that Perceptron is a usable technique for the
problem.
The other less serious limitation is that there is
a chance that the algorithm will falsely conclude
convergence in a local minimum on the error
function curve without reaching the global
minimum, which could also account for low or
inconsistent accuracy results. This limitation is less
serious because it can be controlled to some extent
with the learning rate parameter, which sets the
amount by which the weights are adjusted each
time Perceptron makes a classification error during
training [14].
Na?ve Bayes does not have the limitations of
Perceptron, but does have limitations of its own.
The Bayes decision rule chooses the class that
maximizes the conditional probability of the class
given the context in which it occurs:
(1) C` = argmax  )|()(
1
CVPCP
n
j
j?
=
Here, C` is the chosen category, C is the set of
all categories and Vj is the context. Na?ve Bayes
decision algorithm makes a simplifying
assumption that the words in Vj are independent of
each other. A particular implementation of the
Na?ve Bayes decision rule based on the
independence assumption to text categorization
and word sense disambiguation problems is also
known as ?bag of words? approach [13]. This
approach does not attempt to take into account any
sort of possible dependency between the individual
words in any given context, in fact it assumes that
the word ?heart? and the word ?failure?, for
example, occur completely independently of each
other. Theoretically, such assumption makes Na?ve
Bayes classifiers very unappealing for text
categorization problems, but in practice it has been
shown to perform well on a much greater range of
domains than the theory would support.
The common feature between the two
techniques is that both are linear classifiers and are
relatively efficient which makes them attractive for
learning from large feature sets with lots of
training samples.
4 CHF pilot study
As part of preliminary grant work to investigate
and evaluate incidence, outcome, and etiology
trends of heart failure, a pilot study for prospective
recruitment using term spotting techniques was
tested.  Prospective recruitment was needed for
rapid case identification with 24 hours of newly
diagnosed heart failure patients.
Within Mayo Clinic approximately 75% of
clinical dictations are electronically transcribed on
the date of diagnosis allowing them to be
processed using natural language techniques.
Using the terms ?cardiomyopathy, heart failure,
congestive heart failure, pulmonary edema,
decompensated heart failure, volume overload, and
fluid overload? all electronic outpatient,
emergency department, and hospital dismissal
notes were processed.  These results were reviewed
by trained nurse abstractors to determine if this
technique could provide identification of patients
with clinically active heart failure.  Using the term
spotting technique no cases were omitted as
compared to standard human diagnostic coding
methods of final diagnosis.  This pilot provided a
valid basis for using term spotting for prospective
recruitment; however, the nurse abstractors
reported filtering out a large number of documents
that were irrelevant to the query, thus indicating
that there was room for improvement especially in
precision. These were not quantified at the time.
The results derived from the test sets used for the
study described in this paper display similar
tendencies.
5 Human Expert Agreement
For testing a classifier, it is important to have a test
bed that contains positive as well as negative
examples that have been annotated by human
experts. It is also important to establish  some sort
of an agreement between annotators. For this study
we used a test bed created with a specific focus on
the diagnosis regarding the patient described
within the medical document for a separate pilot
study of agreement between annotators (de Groen
et al, p. c.).
One of the topics selected for this test bed
creation study included congestive heart failure.
For each topic, 90 documents were selected for
evaluation. Seventy of the 90 documents were
chosen from documents with a high likelihood of
containing diagnostic information regarding the
topic of inquiry. Specifically, thirty-five
documents were randomly selected from a pool of
documents based on a coded final diagnosis; thirty-
five documents were randomly selected from a
pool of documents based on a textual retrieval of
lexical surface forms (term spotting). The final
twenty documents were randomly selected from
the remaining documents, not originally included
in the coded or text identified collections. A group
of Emeritus physicians acted as the human experts
for this annotation task. The experts were
instructed to determine whether the information
contained in the clinical note could support
inclusion of the patient in a clinical/research
investigation, if such investigation was centered on
patients having - at the time the note was created -
the topic of inquiry.
Each document was judged by three physicians
on the following scale: (confirmed-probable-
indeterminate-probably not-definitely not). For the
purposes of our study we collapsed ?confirmed?
and ?probable? categories into one ?positive?
category. We also collapsed ?probably not? and
?definitely not? into a ?negative? category. The
?indeterminate? category happened to include such
artifacts as differential diagnosis as well as
uncertain judgements and therefore was ignored
for our purposes. The agreement on this particular
topic happened to be low: only 31% of the
instances were agreed upon by all three experts;
therefore, we decided to use the agreed upon
subset of the notes only for testing our approach.
The low level of agreement was partly attributable
to the breadth of the topic and, partly, to how the
instructions were interpreted by the experts.
Despite the low level of agreement, we were able
to select a subset of 26 documents where all three
annotators agreed. These were the documents
where all three annotators assigned either the
?positive? or the ?negative? category. 7 documents
were judged as ?positive? and 19 were judged  as
?negative? by all three experts.
6 Feature extraction
Arguably, the most important part of training any
text document classifier is extracting relevant
features from the training data. The resulting data
set looks like a set of feature vectors where each
vector should represent all the relevant information
encoded in the document and as little as possible of
the irrelevant information. To capture the relevant
information and give it more weight, we used two
classification schemes: MeSH (Medical Subjects
Headings) [15]and HICDA (Hospital International
Classification of Diseases Adaptation) [16]. The
MeSH classification is available as part of the
UMLS (Unified Medical Language System)
compiled and distributed by the National Library
of Medicine (NLM) [9]. HICDA is a hierarchical
classification with 19 root nodes and 4,334 leaf
nodes. Since 1975, it has been loosely expanded  to
comprise 35,676 rubrics or leaf nodes. It is an
adaptations of ICD-8, which is the 8th edition of the
International Classification of Diseases. HICDA
contains primarily diagnostic statements, whereas
MeSH is not limited to diagnostic statements and
therefore the two complement each other. It should
also be noted that, for mapping the text of clinical
notes to these two ontologies, in addition to the
text phrases present in HICDA and MeSH, some
lexical and syntactic variants found empirically in
medical texts were also added. For MeSH, these
variants were derived from MEDLINE articles by
UMLS developers and for HICDA, the variants
came from coded diagnoses. Having these lexical
and syntactic variants in conjunction with text
lemmatization made the job of mapping relatively
easy. Text lemmatization was done using the
Lexical Variant Generator?s (lvg1) ?norm? function
also developed at NLM.
For the purposes of this experiment, we
represented each document as a mixed set of
features of the following types: (MeSH code
mappings, HICDA code mapping, Single word
tokens, Demographic data). First, MeSH and
HICDA mappings were identified by stemming
and lowercasing all words in the notes and finding
                                                          
1 umlslex.nlm.nih.gov
their matches in the two ontologies. Next, stop
words were deleted from the text that remained
unmapped. The remaining words were treated as
single word token features. In addition to these
lexical features, we used a set of demographic
features such as age, gender, service code (the type
of specialty provider where the patient was seen (e.
g. ?cardiology?)) and death indicator (whether the
patient was alive at the time the note was created).
Since age is a continuous feature, we had to
discretize it by introducing ranges A-N arbitrarily
distributed across 5 year intervals from 0 to over
70 years old. For this experiment, features that
occurred less than 2 times were ignored. The
extracted feature ?vocabulary? consists of 11,118
unique features. Table 1 shows the breakdown of
the feature vocabulary by type.
Feature type N features Proportion
MeSH headings 6631 60 %
HICDA categories 2721 24 %
Single words 1635 15 %
Demographic features 131 01 %
Totals 11,118 100 %
Table 1 Breakdown of training features by type.
7 Experimental Setup
Both Na?ve Bayes and Perceptron were trained on
the same data and tested using a 10-fold cross-
validation technique as well as a held-out test set
of 26 notes mentioned in section 4.
7.1 Data
Two types of annotated testing/training data were
used in this study. The first type (Type I) is the
data generated by medical coders for the purpose
of conceptual indexing of the clinical notes. The
second type (Type II) is the data annotated by
Emeritus physicians (experts).
For Type I data, a set of clinical notes for 6
months of year 2001 was collected resulting in a
corpus of 1,117,284 notes. Most of these notes
contain a set of final diagnoses established by the
physician and coded using the HICDA
classification by specially trained staff. The coding
makes it easy to extract a set of notes whose final
diagnoses suggests that the patient has congestive
heart failure or a closely related condition or
symptom like pulmonary edema. Once this
positive set was extracted (2945 notes), the
remainder was randomized and a similar set of
negative samples was extracted (4675 notes). The
total size of the corpus is 7620 notes. Each note
was then run through feature extraction and the
resulting set was split into 10 train/test folds by
randomly selecting 20% of the 7620 notes to set
aside for testing for each fold.
Type II data set was split into two subsets: a
complete agreement (TypeII-CA) set and a partial
agreement set (TypeII-PA). The complete
agreement set was created by taking 26 notes that
were reliably categorized by the experts with
respect to congestive heart failure specifically.
These 26 notes represent a set where all three
annotators agreed at least to a large extent on the
categorization. ?A large extent? here means that all
three annotators labeled the positive samples as
either ?confirmed? or ?probable? and the negative
samples as either ?probably not? or ?definitely
not?. The set contains 7 positive and 19 negative
samples. The partial agreement set was created by
labeling all samples for which at least one expert
made a positive judgement and no experts made a
?negative? judgement as ?positive? and then
labeling all samples for which at least one expert
made a negative judgement and no experts made a
positive judgements as ?negative?. This procedure
resulted in reducing the initial set of 90 samples to
74 of which 21 were positive and 53 were negative
for congestive heart failure. This partial agreement
set is obviously weaker in its reliability but it does
provide substantially more data to test on and
would enable us to judge, at the very least, the
consistency of the automatic classifiers being
tested.   
7.2 Training
The following parameters were used for training
the classifiers. Na?ve Bayes was used with the
default smoothing parameter of 15. For Perceptron,
the most optimal combination of parameters was to
have the learning rate set at 0.0001 (very small
increments in weights), the error threshold was set
at 15. The algorithm with these settings was run for
1000 iterations.
7.3 Results
Standard classifier accuracy computation [13] for
binary classifiers was used.
(2) FNFPTNTP
TNTPAcc
+++
+
?= 100
Where TP represents the number of times the
classifier guessed a correct positive value (true
positives), TN is the number of times the classifier
correctly guessed a negative value (true negatives),
FP is the number of times the classifier predicted a
positive value but the correct value was negative
(false positives) and the FN (false negatives) is the
inverse of FP.
In addition to standard accuracy, positive
predictive value was also used. It is defined as:
(3) FPTP
TPPPV
+
= *100
Where TP+FP constitute all positive samples in
the test data set. We are interested in positive
predictive value because of the strong preference
towards perfect recall in document retrieval for
epidemiological studies, even if it comes at the
expense of precision. The rule is that it is better to
identify irrelevant data that can be discarded upon
review than to miss any of the relevant patients.
First, we established a baseline by running a a
very simple term spotter that looked for the CHF-
related terms mentioned in Section 2 (and their
normalized variants) in the collection of
normalized2 documents from the Type II data set.
The accuracy of the term spotter is 56% on Type
II-CA set and 54% on Type II-PA set. Positive
predictive value on Type II-CA set is 85% and on
Type II-PA set ? 71%. The positive predictive
value on Type II-CA set reflects the spotter
missing only 1 document out of 7 identified as
positive by the experts. The results are summarized
in Tables 3 and 4.
The results of testing the two classifiers are
presented in Table 2. Na?ve Bayes algorithm
achieves 82.2% accuracy, whereas Perceptron gets
86.5%. The standard deviation on the Perceptron
classifier results appears to be relatively small,
which leads us to believe that this particular
classification problem is linearly separable. The
difference of 4.3% happens to be statistically
significant as evidenced by a t-test at 0.01
                                                          
2 normalization was done with the lvg stemmer
(umlslex.nlm.nih.gov)
Fold Na?ve Bayes Perceptron Delta
PPV (%) Acc (%) PPV (%) Acc (%) PPV (%) Acc (%)
1 89.21 84.06 78.42 88.39 -10.79 4.33
2 88.16 82.41 74.88 85.30 -13.28 2.89
3 89.34 82.74 75.74 86.09 -13.61 3.35
4 90.77 82.02 79.62 87.07 -11.15 5.05
5 90.54 82.07 76.51 86.54 -14.03 4.47
6 89.55 82.74 80.27 87.40 -9.29 4.66
7 88.16 82.41 74.88 85.30 -13.28 2.89
8 88.10 81.16 78.62 86.28 -9.48 5.12
9 89.26 81.69 79.36 86.68 -9.90 4.99
10 88.12 80.45 76.59 85.89 -11.53 5.44
Mean 89.12 82.18 77.49 86.49 -11.63 4.32
Stdev 0.99 0.009 2.01 0.02
Table 2. Classification test results illustrating the differences between Perceptron and Na?ve Bayes.
confidence level. The difference in the positive
predictive value is also significant, however, is it
inversely related to the difference in accuracy.
Perceptron models perform on average 11 absolute
percentage points worse than Na?ve Bayes models.
Table 1 shows results that represent the
accuracy of the classifiers on classifying the Type I
test data that has been generated by medical
coders. Clearly, Type I data is not generated in
exactly the same way as Type II. Although Type I
data is captured reliably and is highly accurate,
Type II data is classified specifically with respect
to congestive heart failure only, by expert
physicians and, we believe, reflects the nature of
the task at hand a little better.
In order to test the classifiers on Type II data,
we re-trained them on the full set of 7620 notes of
Type I data using the same parameters as were
used for the 10-fold cross-validation test. The
results of testing the classifiers on Type II-CA data
(complete agreement) are presented in Table 3.
Classifier PPV (%) Acc (%)
NaiveBayes 100 69.2
Perceptron 85 76.92
TermSpotter 85 56
 Table 3. Test results for Type II-CA data
(annotated by retired physicians with complete
agreement).
These results are consistent with the ones
displayed in Table 2 in that Perceptron tends to be
more accurate overall but less so in predicting
positive samples. Table 4 summarizes the same
results for Type II-PA test set and the results
appear to be oriented in the same general direction
as the ones reported in Table 2 and 3.
Classifier PPV (%) Acc (%)
NaiveBayes 95 57
Perceptron 86 65
TermSpotter 71 54
 Table 4. Test results for Type II-PA data
(annotated by retired physicians with partial
agreement).
From a practical standpoint, the results
presented here are interesting in that they suggest
that the most accurate classifier may not be the
most useful for a given task. In our case, if we
were to use these classifiers for routing a stream of
electronic clinical notes, the gains in precision that
would be attained with the more accurate classifier
would most likely be wiped out by the losses in
recall since recall is more important for our
particular task than precision. However, for a
different task that may be more focused on
precision, obviously, Perceptron would be a better
choice.
Finally, both Perceptron and Na?ve Bayes
performance appears to be superior to the baseline
performance of the term spotter. Clearly such
comparison is only an indicator because the term
spotter is very simple. It is possible that a more
sophisticated term spotting algorithm may be able
to infer semantic relations between various terms
and be able to compensate for misspellings and
carry out other functions resulting possibly in
better performance. However, even the most
sophisticated term spotter will only be as good as
the initial list of terms supplied to it. The
advantage of automatic classification lies in the
fact that classifiers encode the terminological
information implicitly which alleviates the need to
rely on managing lists of terms and the risk of such
lists being incomplete. The disadvantage of
automatic classification is that the classifier?s
performance is heavily data dependent, which
raises the need for sufficient amounts of annotated
training data and limits this methodology to
environments where such data is available.
The error analysis of the misclassified notes
shows that a more intelligent feature selection
process that takes into account discourse
characteristics and semantics of negation in the
clinical notes is required. For example, one of the
misclassified notes contained ?no evidence of
CHF? as part of the History of Present Illness
(HPI) section. Clearly, the presence of a particular
concept in a clinical note is not always relevant.
For example, various terms and concepts may
appear in the Review of Systems (ROS) section of
the note; however, the ROS section is often used as
a preset template and may have little to do with the
present condition. Same is true for other sections
such as Family History, Surgical History, etc. It is
not clear at this point which sections are to be
included in the feature selection process. The
choice will most likely be task specific.
The current study did not use any negation
identification, which we think accounted for some
of the errors. As one of the future steps, we are
planning to implement a negation detector such as
the NegExpander used by Aronow et al[7].
8 Conclusion
In this paper, we have presented a methodology for
generating on-demand binary classifiers for
filtering clinical patient notes with respect to a
particular condition of interest to a clinical
investigator. Implementation of this approach is
feasible in environments where some quantity of
coded clinical notes can be used as training data.
We have experimented with HICDA codes;
however, other coding schemes may be usable or
even more usable as well.
We do not claim that either Na?ve Bayes or the
Perceptron are the best possible classifiers that
could be used for the task of identifying patients
with certain conditions. All we show is that either
one of these two classifiers is reasonably suitable
for the task and has the benefits of computational
efficiency and simplicity. The results of the
experiments with the classifiers suggest that
although Perceptron has higher accuracy than the
Na?ve Bayes classifier overall, its positive
predictive value is significantly lower.  The latter
result makes it less usable for a practical binary
classification task focused on identifying patient
records that have evidence of congestive heart
failure. It may be worth while pursuing an
approach that would use the two classifiers in
tandem. The classifier with the highest PPV would
be used to make the first cut to maximize recall
and the more accurate classifier would be used to
rank the output for subsequent review.
Acknowledgements
We are thankful to the investigators working on the
?Heart Failure? grant RO1-HL-72435 who have
provided valuable input and recommendations for
this research.
References
1. Horn, L., A Natural History of Negation. 1989,
Chicago: University Of Chicago Press.
2. Yang, Y. and C. Chute. A linear least squares fit
mapping method for information retrieval from
natural language texts. in 14th International
Conference on Computational Linguistics
(COLING). 1992.
3. Lewis, D. Naive (Bayes) at forty: The
independence assumption in information
retrieval. in ECML-98. 1998.
4. Johnson, D., et al, A deci-sion-tree-based
symbolic rule induction system for text
categorization. IBM Systems Journal, 2002.
41(3).
5. Nigam, K., J. Lafferty, and A. McCullum. Using
Maximum Entropy for Text Classification. in
IJCAI-99 Workshop on Machine Learning for
Information Filtering. 1999.
6. Yang, Y. Expert Network: Combining Word-
based Matching and Human Experiences in
Text Categorization and Retrieval. in SIGIR.
1994.
7. Aronow, D., F. Fangfang, and, and B. Croft, Ad
Hoc Classification of Radiology Reports.
Journal of Medical Informatics Association,
1999. 6(5).
8. Wilcox, A., et al Using Knowledge Sources to
Improve Classification of Medical Text
Reports. in KDD-2000. 2000.
9. NLM, UMLS. 2001, National Library of
Medicine.
10. Jain, N. and C. Friedman. Identification of
finding suspiciois for breast cancer based on
natural language processing of mammogram
reports. in AMIA. 1997.
11. Damerau, F., et al Experiments in High
Dimensional Text Categorization. in ACM
SIGIR International Conference on Information
Re-trieval. 2002.
12. Carlson, A.J., et al, SNoW User's Guide,
Cognitive Computations Group - University of
Illinois at Urbana/Champaign.
13. Manning, C. and H. Shutze, Foundations of
Statistical Natural Language Processing. 1999,
Cambridge, MA: MIT Press.
14. Anderson, J., Introduction to Neural Networks.
1995, Boston: MIT Press.
15. NLM, Fact sheet Medical Subject Headings
(MeSH?). 2000.
16. Commission on Professional and Hospital
Activities, Hospital Adaptation of ICDA. 2nd
ed. Vol. 1. 1973, Ann Arbor, MI: Commission
on Professional and Hospital Activities.
Exploring adjectival modification in biomedical discourse 
across two genres 
Olivier Bodenreider Serguei V. Pakhomov 
Lister Hill National Center 
for Biomedical Communications 
National Library of Medicine 
Bethesda, Maryland, 20894 ? USA 
Division of Medical Informatics Research 
Department of Health Sciences Research 
Mayo Clinic 
Rochester, Minnesota, 55905 ? USA 
olivier@nlm.nih.gov Pakhomov.Serguei@mayo.edu 
 
 
Abstract 
Objectives: To explore the phenomenon 
of adjectival modification in biomedical 
discourse across two genres: the biomedi-
cal literature and patient records. Meth-
ods: Adjectival modifiers are removed 
from phrases extracted from two corpora 
(three million noun phrases extracted 
from MEDLINE, on the one hand, and 
clinical notes from the Mayo Clinic, on 
the other). The original phrases, the adjec-
tives extracted, and the resulting demodi-
fied phrases are compared across the two 
corpora after normalization. Quantitative 
comparisons (frequency of occurrence) 
are performed on the whole domain. 
Qualitative comparisons are performed on 
the two subdomains (disorders and proce-
dures). Results: Although the average 
number of adjectives per phrase is equiva-
lent in the two corpora (1.4), there are 
more adjective types in MAYO than in 
MEDLINE for disorders and procedures. 
For disorder phrases, the 38% of adjective 
types common to the two corpora account 
for 85% of the occurrences. The predomi-
nance of adjectives in one corpus is ana-
lyzed. Discussion: Potential applications 
of this approach are discussed, namely 
terminology acquisition, information re-
trieval, and genre characterization. 
1 Introduction 
In previous studies, we demonstrated the feasi-
bility of using NLP techniques such as shallow 
parsing of adjectival modification for identifying 
hierarchical relations among biomedical terms 
(Bodenreider et al, 2001) and for extending an 
existing biomedical terminology (Bodenreider et 
al., 2002). In these studies, the corpus was bio-
medical terminology or phrases extracted from the 
biomedical literature. 
Other authors have explored adjectival modifi-
cation in a clinical corpus. Chute and Elkin (1997) 
note, based on empirical observation of clinical 
data, that many clinical terms are accompanied by 
modifiers, including adjectives. The authors make 
a distinction between clinical modifiers (such as 
chronic, severe, and acute) and operational or ad-
ministrative qualifiers (such as no evidence of, his-
tory of, and status post). It appears that the class of 
clinical modifiers consists primarily of adjectives 
that provide specific information regarding condi-
tion and are distributed on a scale. They suggest 
that operational modifiers be kept separate from 
the terms themselves in order to avoid combinato-
rial explosion. 
Taking this idea one step further, we believe 
that, besides operational modifiers, other adjectives 
encountered in clinical phrases could receive a 
special treatment in applications such as informa-
tion retrieval. For example, adjectives expressing 
nuances useful only in the context of clinical care 
could be removed from the phrase when searching 
the biomedical literature. This is the case of adjec-
tives expressing degree of certainty (e.g., prob-
able). In other cases, adjectives specific to clinical 
phrases can be mapped to synonyms or closely 
related modifiers (e.g., greenish sputum, green 
sputum). The ability to map stylistic variations of 
the same adjective becomes especially important to 
establishing links between clinical records and sci-
entific literature, which actually has significant 
implications for improving patient care in clinical 
practice as well as health science research. Finally, 
adjectives absent from the biomedical literature or 
terminologies may denote recent phenomena, not 
yet integrated in terminologies. 
Knowledge about these classes of adjectives 
may help map across genres. Conversely, studying 
adjectival modification across genres may help 
identify adjectives whose representation varies 
across genres, possibly denoting one of these phe-
nomena. 
In the present paper, we explore the phenome-
non of adjectival modification across two genres: 
the biomedical literature and patient records. The 
expected outcome of this study is to obtain a better 
characterization of adjectival modification in bio-
medical phrases of various origins, in order to fully 
take advantage of this phenomenon in applications 
such as the automatic construction of terminology 
and ontology resources and the retrieval of clinical 
documents. 
2 Background 
Adjectival modification as well as lexical seman-
tics of adjectives has been studied extensively in 
the linguistic and NLP literature. Most approaches 
have been directed at creating adjective taxono-
mies and other ways of classifying and represent-
ing adjectives according to their properties and 
function. Raskin and Niernburg (1995) provide a 
comprehensive overview of the various approaches 
that have been taken to description, classification 
and representation of adjectives. 
From the NLP standpoint, Fellbaum (1993) par-
titions adjectives in WordNet? 1 into two large 
classes: descriptive and relational. Descriptive ad-
jectives ?ascribe a value of an attribute to a noun? 
(p.27) (i.e., big child) while relational adjectives 
are usually derived from and are somehow associ-
ated with a noun (i.e., musical child). Another 
prominent distinction has to do with whether an 
adjective can express continuous (scalar) or dis-
crete (non-scalar) values. Raskin and Niernburg 
(1996) point out that for text meaning representa-
                                                          
1
 www.cogsci.princeton.edu/~wn/ 
tion for computational semantics, the most impor-
tant distinction to make is between scalar and non-
scalar. They also present a method for incorporat-
ing the semantics of the modifier adjective into the 
semantics of the modified noun by representing 
nouns as frames with elements such as 
ATTRIBUTE_SIZE than can be filled in by the 
semantic content of the modifying adjectives. 
The major contribution of this study is to ex-
plore adjectival modification across two genres in 
the biomedical domain. Our approach is essentially 
practical and oriented towards applied perspec-
tives. 
3 Resources 
The two genres compared in this study are the 
biomedical literature and patient records. More 
precisely, we use MEDLINE as our bibliographic 
corpus and clinical notes recorded at the Mayo 
Clinic as our clinical corpus. 
MEDLINE? 2, the U.S. National Library of 
Medicine?s (NLM) premier bibliographic database, 
contains over twelve million references to articles 
from more than 4,600 worldwide journals in life 
sciences with a concentration on biomedicine. 
Srinivasan et al (2002) performed a shallow syn-
tactic analysis on the entire MEDLINE collection, 
using only titles and abstracts in English. From the 
175 million noun phrase types identified in their 
study, we selected the subset of ?simple? phrases, 
i.e., noun phrases excluding prepositional modifi-
cation or any other complex feature. In this study, 
a randomly selected subset of three million of these 
simple noun phrases constitutes our bibliographic 
corpus. 
The Mayo Clinic is a group medical practice in 
the United States and spans all recognized medical 
care settings and specialties. Currently over 50,000 
patient visits occur each week that generate 40,000 
medical documentation entries in Mayo electronic 
record that principally consists of text narratives. 
The current size of the collection is approaching 
fifteen million notes and each note has on average 
200 to 250 words of text. For this study we consid-
ered only the most current sample of the clinical 
notes collection ? 1,783,377 documents recorded 
in 2002. Only simple noun phrases of the same 
type extracted from MEDLINE were extracted 
                                                          
2
 www.ncbi.nlm.nih.gov/entrez/query.fcgi 
from this corpus, resulting in a set of 9,665,942 
phrases. A randomly selected subset of three mil-
lion of these simple noun phrases constitutes our 
clinical corpus. 
In both cases, the noun phrases were first nor-
malized for case, so that the two subsets studied 
represent three million noun phrase types each. 
 
Another resource used in this study is the Uni-
fied Medical Language System? 3 (UMLS?) 
Metathesaurus?. The Metathesaurus, also devel-
oped by NLM, is organized by concept or mean-
ing. A concept is defined as a cluster of terms 
representing the same meaning (synonyms, lexical 
variants, acronyms, translations). The 14th edition 
(2003AA) of the UMLS Metathesaurus contains 
over 1.75 million unique English terms drawn from 
more than sixty families of medical vocabularies, 
and organized in some 875,000 concepts. 
In the UMLS, each concept is categorized by 
semantic types from the Semantic Network. 
McCray et al (2001) designed groupings of se-
mantic types that provide a partition the Metathe-
saurus and, therefore, can be used to extract 
consistent sets of concepts corresponding to a sub-
domain, such as disorders or procedures. 
4 Methods 
In order to compare the linguistic phenomenon of 
adjectival modification across two corpora of noun 
phrases, we first extracted the adjectives after 
submitting the phrases to a shallow syntactic 
analysis and normalizing the head noun of the 
phrase for inflectional variation. Then, we com-
pared across corpora the adjectives on the one hand 
and the ?demodified? noun phrases4 (i.e., noun 
phrases from which the adjectives have been re-
moved) on the other. In order to address the size of 
these corpora, we limited the focus of our study to 
a significant subdomain of clinical medicine: dis-
orders and procedures. 
4.1 Extracting adjectives 
Figure 1 illustrates the sequence of methods 
used for extracting adjectives from the original 
noun phrases. It also presents the number of 
phrases present before and after each of the four 
steps detailed below. 
                                                          
3
 umlsinfo.nlm.nih.gov 
4
 also referred to as ?nested terms? in the literature 
Step 1. Syntactic analysis 
The phrases in our bibliographic and clinical 
samples were then submitted to an underspecified 
syntactic analysis described by Rindflesch et al 
(2000) that draws on a stochastic tagger (see 
(Cutting et al, 1992) for details) as well as the 
SPECIALIST Lexicon5, a large syntactic lexicon 
of both general and medical English that is distrib-
uted with the UMLS. Although not perfect, this 
combination of resources effectively addresses the 
phenomenon of part-of-speech ambiguity in Eng-
lish.  
The resulting syntactic structure identifies the 
head and modifiers for the noun phrase analyzed. 
Each modifier is also labeled as being adjectival, 
adverbial, or nominal. Although all types of modi-
fication in the simple English noun phrase were 
labeled, only adjectives and nouns were selected 
for further analysis in this study. For example, the 
phrase abnormal esophageal motility study was 
analyzed as: 
 
[[mod([abnormal,adj]), 
  mod([esophageal,adj]), 
  mod([motility,noun]), 
  head([study,noun])]] 
 
The result of the syntactic analysis was used to 
select the noun phrases suitable for studying the 
adjectival modification phenomenon, i.e., phrases 
having the following structure: (adj+, noun*, 
head). The phrase is required to start with an ad-
jectival modifier, possibly followed by other adjec-
tives and end with a head noun, possibly preceded 
by other nouns. This specification excludes both 
simple phrases (e.g., one isolated noun) and com-
plex phrases, not suitable for our analysis. 
Step 2. Normalizing the head noun 
In order to compare phrases across corpora, we 
normalized the head noun for inflectional variation 
in each noun phrase. As a result, the two noun 
phrases cerebrovascular accident (in MAYO) and 
cerebrovascular accidents (in MEDLINE) are con-
sidered equivalent. When both the singular and the 
plural form of a phrase appear in the same corpus, 
only the singular form is considered for further 
processing. In practice, to normalize head nouns, 
we used the program lvg6, developed at NLM and 
distributed with the UMLS. 
                                                          
5
 umlslex.nlm.nih.gov 
6
 umlslex.nlm.nih.gov (lvg parameters used: -f:b -CR:oc) 



1,329,225
(adj+, noun*, head)
phrases
3,000,000
randomly selected
?simple? phrases
syntactic
analysis
1,322,403
normalized phrases
normalize
head noun
remove
adjectives
select sub-
domain

2,826,395
demodified
phrases
72,324
adjective
types
Disorders
18,370 adjectives
279,182 dem. terms
Procedures
16,098 adjectives
160,207 dem. terms
1,641,350
(adj+, noun*, head)
phrases
3,000,000
randomly selected
?simple? phrases
syntactic
analysis
1,575,478
normalized phrases
normalize
head noun
remove
adjectives
3,092,340
demodified
phrases
44,268
adjective
types
	
select sub-
domain
Disorders
16,486 adjectives
714,257 dem. terms
Procedures
11,630 adjectives
242,326 dem. terms
 
Figure 1. Summary of the methods. 
 
Step 3. Creating demodified phrases 
When adjectives are identified in a phrase O, a set 
of demodified phrases {T1, T2,?,Tn} is created by 
removing from phrase O any combinations of ad-
jectival modifiers found in it. While the structure 
of the demodified phrases remains syntactically 
correct, the semantics of some phrases may be 
anomalous, especially when adjectives other than 
the leftmost are removed. Since most of them are 
semantically valid, we found it convenient to keep 
all demodified phrases for further analysis. De-
modified phrases with incorrect semantics will be 
filtered out later in the experiment, since they will 
appear with a lower frequency. 
The number of demodified phrases derived 
from a given phrase is 2m ? 1, m being the number 
of adjectives in the phrase. For example, the phrase 
acute respiratory infection syndrome starts with 
the two adjectival modifiers acute and respiratory, 
so that the following three demodified phrases are 
generated respiratory infection syndrome, acute 
infection syndrome, and infection syndrome. 
Step 4. Restricting to disorders and procedures 
Because of the large size of the two corpora, we 
only performed a quantitative analysis of adjectival 
modification for the whole biomedical domain. We 
restricted the qualitative study to disorders and 
procedures. These represent a significant subdo-
main of clinical medicine, yet are small enough to 
be able to perform at least a somewhat detailed 
analysis. 
All phrases, original and demodified, were 
mapped to the UMLS Metathesaurus by first at-
tempting an exact match between phrases and 
Metathesaurus concepts. If an exact match failed, 
normalization was then attempted. This process 
makes the input and target terms potentially com-
patible by eliminating such inessential differences 
as inflection, case and hyphen variation, as well as 
word order variation. From the phrases mapping to 
some concept in the UMLS, we selected those for 
which the semantic category of the concept 
mapped to corresponded to the subdomains of in-
terest. In practice, for a phrase to be considered a 
procedure, it had to map to a UMLS concept and 
the semantic type of this concept had to belong to 
the semantic group Procedures. The same principle 
was used for selecting disorders, using the seman-
tic group Disorders. For example, the demodified 
phrase arthroscopic surgery (derived from decom-
pressive arthroscopic surgery) is considered a pro-
cedure because it maps, as a synonym, to the 
concept Surgical Procedures, Arthroscopic, whose 
semantic group is Procedures. Exceptionally (32 
UMLS concepts), a term may name both a disorder 
and a procedure. These terms are simply counted 
twice, once with Disorders and once with Proce-
dures. 
4.2 Comparing corpora 
In order to investigate the characteristics of each 
corpus (noun phrases extracted from the biomedi-
cal literature and from patient records), we used 
two kinds of comparisons: quantitative and qualita-
tive. The quantitative part consists of comparing 
frequencies of adjectives and demodified phrases 
across corpora, for the whole corpus as well as on 
specific subsets (Disorders and Procedures). In the 
qualitative part, we examined only phrases form 
the subdomains of Disorders and Procedures. 
Quantitative comparisons 
As mentioned earlier, the head noun of each phrase 
was normalized for inflectional variation (see Step 
2 above). The purpose of normalizing the head 
noun is two-fold. First, it contributes to identifying 
phrase variants within each corpus, resulting in 
accurate counts of phrase types after duplicates had 
been removed. Second, it provides a simple means 
(string match) for identifying equivalent phrases 
across corpora. 
We computed the number of original phrases, 
adjectives, and demodified phrases in each corpus, 
counting tokens and types in each category. Addi-
tionally, we explored similarities between the two 
genres by computing the number of phrases and 
adjectives common to the two corpora (intersec-
tion). Finally, we computed the number of phrase 
and adjective types for the two corpora taken to-
gether (union) in order to better characterize the 
whole domain. From these frequencies, we derived 
additional parameters such as the ratio of the num-
ber of adjectives to the number of original phrases. 
Qualitative comparisons 
We first extracted adjectives from the original 
phrases corresponding to Disorders and Procedures 
and computed their frequency of occurrence. Be-
cause phrases must map to a UMLS term in order 
to be identified as members of a subdomain, only 
the adjectives present in biomedical terms can be 
analyzed. For this reason, their rank will be studied 
rather than their frequency7. 
In order to better represent the whole spectrum 
of adjectives present in the two corpora, we then 
turned to the demodified phrases instead of the 
original phrases. In this second part, the condition 
                                                          
7
 rank n simply corresponds to the nth highest frequency 
for a phrase to be considered a member of a sub-
domain was that the demodified phrase (not the 
entire phrase) map to a UMLS term. However, 
some adjectives may be overrepresented when sev-
eral demodified phrases map to a UMLS term in 
the subdomains considered. For example, the 
phrase abdominal vascular reconstructive surgery, 
once demodified, maps to both vascular surgery 
(with modifiers abdominal and reconstructive) and 
reconstructive surgery (with modifiers abdominal 
and vascular). In this case, the adjective abdominal 
was counted twice. 
For each adjective, we determined the corpus in 
which it was predominantly used. If more than half 
of the occurrences appear in one corpus, the adjec-
tive is considered predominant in this corpus. 
When more than half of the occurrences appear in 
both corpora, the adjective is considered common 
to the two corpora. 
5 Results 
5.1 Extracting adjectives 
Out of the 3 million simple noun phrases randomly 
selected from MEDLINE, 1,322,403 phrase types 
were selected for further processing. Out of these, 
72,324 adjective types (1,916,530 tokens) were 
extracted and 2,826,395 demodified phrases were 
generated. 1,575,478 phrase types were selected 
from the 3 million noun phrases in the MAYO 
corpus. Out of these, 44,268 adjective types 
(2,209,778 tokens) were extracted and 3,092,340 
demodified phrases were generated. Details about 
the number of phrases selected at each step of the 
processing are given in Figure 1. 
5.2 Comparing corpora 
Quantitative results 
The number of original phrases (Table 1), adjec-
tives (Table 2), and demodified phrases (Table 3) 
are presented below in tabular format. Counts are 
broken down by corpus (MEDLINE and MAYO), on 
the one hand, and by subdomain (Disorders and 
Prodedures), on the other. Tables also include re-
sults obtained on the whole corpus (All), i.e., with-
out subsetting, and on the union of the two corpora 
(Together). Except for original phrases (Table 1), 
which, by design, are phrase types, Table 2 and 
Table 3 contain the numbers of types (upper left) 
and tokens (lower right). 
The number of adjectives per phrase ranges 
from 1 to 16 in MEDLINE and from 1 to 7 for 
MAYO when the whole corpus is considered. The 
maximum number of adjectives per phrase is 6 or 7 
for the various subsets. Phrases containing so many 
adjectives may look syntactically and semantically 
suspicious. While some of them denote extraction 
errors (often due to inappropriate part-of-speech 
tagging), most correspond to valid phrases and re-
flect the complexity of the biomedical domain 
(e.g., diastolic systolic mean middle cerebral ar-
tery blood flow velocity and combined enteral par-
enteral synthetic hypercaloric nutrition). The 
distribution of the number of adjectives per phrase 
is plotted in Figure 2. 
Although the number of phrases processed is 
slightly more important for MAYO (1,575,476) 
than for MEDLINE (1,322,403), and although the 
ratio of the number of adjective tokens extracted to 
the number of original phrases is roughly similar in 
the two corpora (1.45 for MEDLINE and 1.40 for 
MAYO), there are significantly more adjective 
types in MEDLINE (72,324) than in MAYO 
(44,268). A difference in the opposite direction is 
observed in the Disorders and Procedures subsets, 
where the number of adjective types is higher in 
MAYO than in MEDLINE, while the average number 
of adjectives per phrase is still slightly higher in 
MEDLINE (1.27 vs. 1.21 for Disorders and 1.21 vs. 
1.14 for Procedures). This finding requires further 
investigation. 
Despite reducing the variation by normalizing 
head nouns for inflection, less than 3% of the 
original phrases are common to the two corpora. 
This proportion is significantly higher for the sub-
set of disorder and procedure phrases where up to 
one third of MEDLINE phrases can be found in the 
MAYO corpus. Not surprisingly, the proportion of 
adjectives in common is higher. Overall, 44% of 
the adjectives in MAYO are also found in MEDLINE 
and up to 75% of the adjectives in MEDLINE are 
also found in MAYO (for disorders). Interestingly, 
the adjectives common to both corpora are also the 
most frequent. For example, as shown in Table 2, 
the 1,584 adjective types in common in the subset 
Disorders account for 38% of all adjectives for 
Disorders (4,148), but the corresponding 25,557 
adjective tokens account for 85% of all tokens 
(30,046). 
 
Table 1 ? Number of original phrases (types), for 
Disorders (Di) and Procedures (Pr) 
 
 MEDLINE MAYO Together Common 
Di 4,941 19,641 22,774 1,808 
Pr 1,534 4,959 6,028 465 
All 1,322,403 1,575,476 2,857,848 40,031 
 
Table 2 ? Number of adjectives (types [top] and 
tokens [bottom]), for Disorders (Di) and Proce-
dures (Pr) 
 
 MEDLINE MAYO Together Common 
  2,048   3,684   4,148   1,584 Di 6,299 23,747 30,046 25,557 
     902   1,499   1,790      611 Pr 1,852 5,667 7,519 5,683 
72,324 44,268 97,762 18,830 All 1,916,530 2,209,778 4,126,308 3,885,852 
 
Table 3 ? Number of demodified phrases (types 
[top] and tokens [bottom]), for Disorders (Di) and 
Procedures (Pr) 
 
 MEDLINE MAYO Together Common 
22,031 24,719 34,302 12,448 Di 174,548 463,097 637,645 571,041 
  9,850   8,595 13,691 4,754 Pr 101,323 166,180 267,503 241,790 
1,487,889 1,047,772 2,403,504 132,157 All 2,826,395 3,092,340 5,918,735 2,709,100 
 
 
 
0%
10%
20%
30%
40%
50%
60%
70%
1 2 3 4 1 2 3 4 1 2 3 4
  
Number of adjectives per phrase MEDLINE MAYO
all disorders procedures
 
 
Figure 2. Distribution of the number of adjectives 
per phrase 
 
Qualitative results 
The list of the most frequent adjectives found in 
the original phrases corresponding to Disorders 
and Procedures in the UMLS is given in Table 4, 
with their rank in each corpus. Interestingly, most 
high-ranking adjectives are found in both corpora. 
 
Table 4 ? Rank of the most frequent adjectives in 
MEDLINE (ME) and MAYO (Ma) 
 
Disorders ME Ma Procedures ME Ma 
chronic 2 2 total 1 2 
normal 3 1 surgical 2 3 
acute 4 3 partial 5 1 
congenital 1 8 serum 4 5 
increased 6 5 patient 13 4 
abnormal 8 4 percutaneous 3 15 
neonatal 17 >100 renal 12 7 
decreased 11 7 pulmonary 10 12 
pulmonary 10 9 ultrasound >100 22 
benign 7 13 general >100 23 
renal 9 11 cardiac 16 8 
recurrent 15 6 spinal 11 14 
multiple 12 10 radical 14 13 
increasing 14 12 evoked 29 >100 
malignant 5 27 coronary 8 24 
fetal 33 >100 femoral >100 33 
nasal >100 33 studied 33 >100 
joint 18 18 aortic >100 34 
intracranial 40 >100 fluid 7 27 
positive 24 17 abdominal 24 11 
 
Considering not the original phrases, but de-
modified phrases corresponding to disorders and 
procedures, most adjectives with a frequency 
greater than 10 are found in the two corpora (86% 
for disorder and 80% for procedures). However, 
their representation may differ largely across cor-
pora. Examining the contexts of adjectives for Dis-
orders (4978 adjectives with a frequency greater 
than 10), we found that 40% of the adjectives ap-
pear predominantly in MAYO (e.g., mild, possible, 
recent, probable, questionable, greenish), 20% 
predominantly in MEDLINE (e.g., experimental, 
human, neonatal, canine, intracellular), while 40% 
share most of their contexts across the two corpora 
(e.g., acute, chronic, recurrent). The repartition of 
the demodified phrases for Disorders (8263 
phrases with a frequency greater than 10) is some-
what different. 65% of the demodified phrases ap-
pear predominantly in MAYO (e.g., discomfort, 
tenderness, low back pain, chest pain, diarrhea), 
15% predominantly in MEDLINE (e.g., resistance, 
strain, vesicle, hyperthermia), while 20% share 
most of their contexts across the two corpora (e.g., 
disease, lesion, pain, symptom, abnormality). 
6 Applications 
In this section, we briefly examine some of the 
applications that may benefit from a better knowl-
edge of adjectival modification in biomedical dis-
course: genre characterization, terminology and 
ontology acquisition, and information retrieval. 
Genre characterization 
Knowledge about adjectives and demodified 
phrases predominantly associated with one corpus 
may be useful to characterize corpora, and in this 
experiment, genres. Although limited, this study 
suggests, for example, that a clinical corpus con-
tains markers for uncertainty (e.g., possible, prob-
able, questionable) and non-specific symptoms 
(e.g., discomfort, low back pain). On the other 
hand, in a broad bibliographic corpus, precisions 
about organism or age groups must be given (e.g., 
human, canine, neonatal). Interestingly, while the 
term fever is found with no predominance in either 
corpus, its more scientific synonyms hyperthermia 
and pyrexia are used predominantly in MEDLINE. If 
corroborated, this finding may suggest that, al-
though both scientific publications and medical 
records are geared toward peers, the language used 
in scientific publications tends to be more special-
ized. 
Terminology and ontology acquisition 
The method described in this paper constitutes a 
useful technique for adapting existing terminol-
ogies and ontologies with empirically derived 
terms from a new subdomain. First, demodified 
phrases are more likely to be mapped to another 
corpus. And second, because adjectival modifica-
tion often denotes a hyponymic relation between a 
phrase without modifier and a modified phrase, the 
modified phrase can be linked as a candidate hy-
ponym to the phrase without modifier 
(Bodenreider et al, 2002). 
This approach could be used, for example, for 
adapting biomedical terminologies to subtle clini-
cal nuances. When used with exactly the same 
subdomain the existing terminology comes from, 
this technique could enable regular updates of the 
terminology provided that current textual data is 
used for phrase extraction. 
The approach is currently limited to simple ad-
jectival modification; however, this is a self-
imposed limitation. Theoretically, the same meth-
odology can be adapted to work on nominal, 
prepositional phrase and other types of modifica-
tion. 
Information retrieval 
Terminologies as well as ontologies are frequently 
used for information or document retrieval in the 
domains for which such terminologies or ontolo-
gies are available. Medicine is one such domain 
where there are numerous terminological re-
sources. Integrated in a system such as the UMLS, 
these resources provide, for example, many syno-
nyms for each concept, increasing the chances of 
retrieving documents from a given term. However, 
most terms in these resources are pre-coordinated 
and may not include all the variants needed in 
various contexts. Moreover, most terms are noun 
phrases and, while synonyms are often given for 
nouns, it may not be the case for their modifiers. 
For example, while the various synonyms for fever 
(e.g., hyperthermia and pyrexia) are present in the 
UMLS, there is no greenish variant for green spu-
tum. Nor can there systematically be a variant de-
noting uncertainty. Therefore, identifying classes 
of adjectives that can be either ignored (e.g., uncer-
tainty markers) or mapped to other adjectives (e.g., 
greenish to green) would increase the performance 
of information retrieval systems operating on clini-
cal corpora. In light of these findings, existing ter-
minologies and ontologies can provide a core of 
medical concepts common to most subdomains; 
whereas the methodology described here can be 
used to tailor the general-purpose terminological 
resources to accommodate subdomain-specific 
terminology services. 
7 Conclusions 
In conclusion, adjectival modification plays an im-
portant role in biomedical texts, and knowledge 
about this phenomenon can be exploited in appli-
cations such as the retrieval of biomedical docu-
ments and for developing terminology services in 
the biomedical domain. 
In the future, we would like to identify patterns 
in biomedical terms and phrases based, in part, on 
classes of adjectival modifiers. Creating such a 
model for terms would constitute a generative ap-
proach to biomedical terminology, contrasting with 
the lists of precoordinated terms populating most 
terminology systems in the biomedical domain. 
References 
Bodenreider, O., Burgun, A., and Rindflesch, T. C. 
(2001). Lexically-suggested hyponymic relations 
among medical terms and their representation in the 
UMLS. Proceedings of TIA'2001 "Terminology and 
Artificial Intelligence", 11-21. 
Bodenreider, O., Rindflesch, T. C., and Burgun, A. 
(2002). Unsupervised, corpus-based method for ex-
tending a biomedical terminology. Proceedings of 
the ACL'2002 Workshop "Natural Language Proc-
essing in the Biomedical Domain", 53-60. 
Chute, C. G., and Elkin, P. L. (1997). A clinically de-
rived terminology: qualification to reduction. Proc 
AMIA Annu Fall Symp, 570-574. 
Cutting, D. R., Kupiec, J., Pedersen, J. O., and Sibun, P. 
(1992). A practical part-of-speech tagger. Proceed-
ings of the Third Conference on Applied Natural 
Language Processing, 133-140. 
Fellbaum, C. (1993). Five Papers on WordNet: Adjec-
tives in Wordnet, D. Gross, ed. 
McCray, A. T., Burgun, A., and Bodenreider, O. (2001). 
Aggregating UMLS semantic types for reducing 
conceptual complexity. Medinfo 10, 216-220. 
Raskin, V., and Niernburg, S. (1995). Lexical Semantics 
of Adjectives: A Microtheory of Adjectival Meaning. 
Memoranda In Cognitive and Computer Science 
MCCS-95-288. 
Raskin, V., and Niernburg, S. (1996). Adjectival Modi-
fication in Text Meaning Representation. Proceed-
ings of COLING '96, 842-847. 
Rindflesch, T. C., Rajan, J. V., and Hunter, L. (2000). 
Extracting molecular binding relationships from 
biomedical text. In "Proceedings of the 6th Applied 
Natural Language Processing Conference" (San 
Francisco, Morgan Kaufmann Publishers), pp. 188-
195. 
Srinivasan, S., Rindflesch, T. C., Hole, W. T., Aronson, 
A. R., and Mork, J. G. (2002). Finding UMLS 
Metathesaurus concepts in MEDLINE. Proc AMIA 
Symp, 727-731. 
 
Creating a Test Corpus of Clinical Notes Manually Tagged for Part-of-Speech 
Information 
Serguei  PAKHOMOV 
Division of Medical Informatics 
Research, Mayo Clinic  
Rochester, MN 
Pakhomov.Serguei@mayo.edu 
Anni  CODEN 
IBM, T.J. Watson Research 
Center, 
Hawthorne, NY 10532 
anni@us.ibm.com  
Christopher   CHUTE 
Division of Medical 
Informatics Research, Mayo 
Clinic Rochester, MN 
Chute@mayo.edu 
 
Abstract 
This paper presents a project whose main goal 
is to construct a corpus of clinical text 
manually annotated for part-of-speech 
information. We describe and discuss the 
process of training three domain experts to 
perform linguistic annotation. We list some of 
the challenges as well as encouraging results 
pertaining to inter-rater agreement and 
consistency of annotation. We also present 
preliminary experimental results indicating the 
necessity for adapting state-of-the-art POS 
taggers to the sublanguage domain of medical 
text. 
1 Introduction 
Having reliable part-of-speech (POS) 
information is critical to successful implementation 
of Natural Language Processing (NLP) techniques 
for processing unrestricted text in the biomedical 
domain. State-of-the-art automated POS taggers 
achieve accuracy of 93% - 98% and the most 
successful implementations are based on statistical 
approaches to POS tagging. Taggers based on 
Hidden Markoff Model (HMM) technology 
currently appear to be in the lead. The prime public 
domain examples of such implementations include 
the Trigrams?n?Tags tagger (Brandts 2000), Xerox 
tagger (Cutting et al 1992) and LT POS tagger 
(Mikheev 1997). Maximum Entropy (MaxEnt) 
based taggers also seem to perform very well        
(Ratnaparkhi 1996, Jason Baldridge, Tom Morton, 
and Gann Bierner  http://maxent.sourceforge.net ).  
One of the issues with statistical POS taggers is 
that most of them need a representative amount of 
hand-labeled training data either in the form of a 
comprehensive lexicon and a corpus of untagged 
data or a large corpus of text annotated for POS or 
a combination of the two. Currently, most of the 
POS tagger accuracy reports are based on the 
experiments involving Penn Treebank data 
(Marcus, 1993). The texts in Treebank represent 
the general English domain. It is not entirely clear 
how representative the general English language 
vocabulary and structure are of a specialized sub-
domain such as clinical reports.  
A well-recognized problem is that the accuracy 
of all current POS taggers drops dramatically on 
unknown words. For example, while the TnT 
tagger performs at 97% accuracy on known words 
in the Treebank, the accuracy drops to 89% on 
unknown words (Brandts, 2000). The LT POS 
tagger is reported to perform at 93.6-94.3% 
accuracy on known words and at 87.7-88.7% on 
unknown words using a cascading unknown word 
?guesser? (Mikheev, 1997). The overall results for 
both of  these taggers are much closer to the high 
end of the spectrum because the rate of the 
unknown words in the tests performed on the Penn 
Treebank corpus is generally relatively low ? 2.9% 
(Brandts, 2000). From these results, we can 
conclude that the higher the rate of unknown 
vocabulary, the lower the overall accuracy will be, 
necessitating the adaptation of the taggers trained 
on Penn Treebank  to sublanguage domains with 
vocabulary that is substantially different from the 
one represented by the Penn Treebank corpus.  
Based on the observable differences between 
the clinical and the general English  discourse and 
POS tagging accuracy results on unknown 
vocabulary, it is reasonable to assume that a tagger 
trained on general English may not perform as well 
on clinical notes, where the percentage of unknown 
words will increase. To test this assumption, a 
?gold standard? corpus of clinical notes needs to be 
manually annotated for POS information. The 
issues with the annotation process constitute the 
primary focus of this paper. 
We describe an effort to train three medical 
coding experts to mark the text of clinical notes for 
part-of-speech information. The motivation for 
using medical coders rather than trained linguists is 
threefold. First of all, due to confidentiality 
restrictions, in order to develop a corpus of hand 
labeled data from clinical notes one can only use 
personnel authorized to access patient information. 
The only way to avoid it, is to anonymize the notes 
prior to POS tagging which in itself is a difficult 
and expensive process (Ruch et al 2000). Second, 
medical coding experts are well familiar with 
62
clinical discourse, which helps especially with 
annotating medicine specific vocabulary. Third, 
the fact that POS tagging can be viewed as a 
classification task makes the medical coding 
experts highly suitable because their primary 
occupation and expertise is in classifying patient 
records for subsequent retrieval.    
We show that, given a good set of guidelines, 
medical coding experts can be trained in a limited 
amount of time to perform a linguistic task such as 
POS annotation at a high level of agreement on 
both clinical notes and Penn Treebank data. 
Finally, we report on a set of training experiments 
performed with the TnT tagger (Brandts, 2000) 
using the Penn Treebank as well as the newly 
developed medical corpus.. 
2 Annotation  
Prior to this study, the three annotators who 
participated in it had a substantial experience in 
coding clinical diagnoses but virtually no 
experience in POS markup. The training process 
consisted of a general and rather superficial 
introduction to the issues in linguistics as well as 
some formal training using the POS tagging 
guidelines developed by Santoriny (1991) for 
tagging Penn Treebank data. The formal training 
was followed by informal discussions of the data 
and difficult cases pertinent to the clinical notes 
domain which often resulted in slight 
modifications to the Penn Treebank guidelines. 
The annotation process consisted of 
preprocessing and editing. The pre-processing 
includes sentence boundary detection, tokenization 
and priming with part-of-speech tags generated by 
a MaxEnt tagger (Maxent 1.2.4 package (Baldridge 
et al)) trained on Penn Treebank data. 
Automatically annotated notes were then presented 
to the domain experts for editing. 
3 Annotator agreement 
In order to establish reliability of the data, we 
need to ensure internal as well as external 
consistency of the annotation. First of all, we need 
to make sure that the annotators agree amongst 
themselves (internal consistency) on how they 
mark up text for part-of-speech information. 
Second, we need to find out how closely the 
annotators generating data for this study agree with 
the annotators of an established project such as 
Penn Treebank (external consistency). If both tests 
show relatively high levels of agreement, then we 
can safely assume that the annotators in this study 
are able to generate part-of-speech tags for 
biomedical data that will be consistent with a 
widely recognized standard and can work 
independently of each other thus tripling the 
amount of manually annotated data.  
3.1 Methods 
Two types of measures of consistency were 
computed ? absolute agreement and Kappa 
coefficient. The absolute agreement (Abs Agr) was 
calculated by dividing the total number of times all 
annotators agreed on  a tag over the total number 
of tags. 
Kappa coefficient is given in (1) (Carletta 1996) 
(1) 
)(1
)()(
EP
EPAPKappa ?
?=  
 
where P(A) is the proportion of times the 
annotators actually agree and P(E) is the 
proportion of times the annotators are expected to 
agree due to chance3. 
The Absolute Agreement is most informative 
when computed over several sets of labels and 
where one of the sets represents the ?authoritative? 
set. In this case, the ratio of matches among all the 
sets including the ?authoritative? set to the total 
number of labels shows how close the other sets 
are to the ?authoritative? one. The Kappa statistic 
is useful in measuring how consistent the 
annotators are compared to each other as opposed 
to an authority standard.   
3.2 Annotator consistency 
In order to test for internal consistency, we 
analyzed inter-annotator agreement where the three 
annotators tagged the same small corpus of clinical 
dictations.  
 
File ID Abs agr. Kappa N Samples 
1137689 93.24% 0.9527 755 
1165875 94.59% 0.9622 795 
1283904 89.79% 0.9302 392 
1284881 90.42% 0.9328 397 
1307526 84.43% 0.8943 347 
Total   2686 
Average 90.49% 0.9344  
Table 1. Annotator agreement results based on 5 
clinical notes 
                                                     
3 A  very detailed explanation of the terms used in the formula for 
Kappa computation as well as concrete examples of how it is 
computed are provided in Poessio and Vieira (1988). 
63
The results were compared and the Kappa-
statistic was used to calculate the inter-annotator 
agreement. The results of this experiment are 
summarized in Table 1. For the absolute 
agreement, we computed the ratio of how many 
times all three annotators agreed on a tag for a 
given token to the total number of tags. 
Based on the small pilot sample of 5 clinical 
notes (2686 words), the Kappa test showed a very 
high agreement coefficient ? 0.93. An acceptable 
agreement for most NLP classification tasks lies 
between 0.7 and 0.8 (Carletta 1996, Poessio and 
Vieira 1988). Absolute agreement numbers are 
consistent with high Kappa as they show an 
average of 90% of all tags in the test documents 
assigned exactly the same way by all three 
annotators. 
The external consistency with the Penn Treebank 
annotation was computed using a small random 
sample of 939 words from the Penn Treebank 
Corpus annotated for POS information.  
 
Annotator Abs agr 
A1 88.17% 
A2 87.85% 
A3 87.85% 
Average 87.95% 
Table 2. Absolute agreement results based on 5 
clinical notes with an ?authority? label set. 
The results in Table 2 show that the three 
annotators are on average 88% consistent with the 
annotators of the Penn Treebank corpus.  
3.3 Descriptive statistics for the corpus of 
clinical notes 
  The annotation process resulted in a corpus of 
273 clinical notes annotated with POS tags. The 
corpus contains 100650 tokens from 8702 types 
distributed across 7299 sentences. Table 3 displays 
frequency counts for the top most frequent 
syntactic categories. 
Category Count % total 
NN 18372 18% 
IN 8963 9% 
JJ 8851 9% 
DT 6796 7% 
NNP 4794 5% 
Table 3 Syntactic category distribution in the 
corpus of clinical notes. 
The distribution of syntactic categories suggests 
the predominance of nominal categories, which is 
consistent with the nature of clinical notes 
reporting on various patient characteristics such as 
disorders, signs and symptoms. 
Another important descriptive characteristic of 
this corpus is that the average sentence length is 
13.79 tokens per sentence, which is relatively short 
as compared to the Treebank corpus where the 
average sentence length is 24.16 tokens per 
sentence. This supports our informal observation 
of the clinical notes data containing multiple 
sentence fragments and short diagnostic 
statements. Shorter sentence length implies greater 
number of inter-sentential transitions and therefore 
is likely to present a challenge for a stochastic 
process.   
4 Training a POS tagger on medical data 
In order to test some of our assumptions 
regarding how the differences between general 
English language and the language of clinical notes 
may affect POS tagging, we have trained the 
HMM-based TnT tagger (Brandts, 2000) with 
default parameters at the tri-gram level both on 
Penn Treebank and the clinical notes data. We 
should also note that the tagger relies on a 
sophisticated ?unknown? word guessing algorithm 
which computes the likelihood of a tag based on 
the N last letters of the word, which is meant to 
leverage the word?s morphology in a purely 
statistical manner.  
The clinical notes data was split at random 10 
times in 80/20 fashion where 80% of the sentences 
were used for training and 20% were used for 
testing. This technique is a variation on the classic 
10-fold validation and appears to be more suitable 
for smaller amounts of data.  
We conducted two experiments. First, we 
computed the correctness of the Treebank model 
on each fold of the clinical notes data. We tested 
the Treebank model on the 10 folds rather than the 
whole corpus of clinical notes in order to produce 
correctness results on exactly the same test data as 
would be used for validation tests of models build 
from the clinical notes data. Then, we computed 
the correctness of each of the 10 models trained on 
each training fold of the clinical notes data using 
the corresponding testing fold of the same data for 
testing. 
 
Table 4 Correctness results for the Treebank 
model. 
Correctness was computed simply as the 
percentage of correct tag assignments of the POS 
tagger (hits) to the total number of tokens in the 
test set. Table 4 summarizes the results of testing 
the Treebank model, while Table 5 summarizes the 
Split Hits Total Correctness 
Average 21826.3 24309 89.79% 
64
testing results for the models trained on the clinical 
notes. 
 
The average correctness of the Treebank model 
tested on clinical notes is ~88%, which is 
considerably lower than the state-of-the-art 
performance of the TnT tagger - ~96%. Training 
the tagger on a relatively small amount of clinical 
notes data brings the performance much closer to 
the state-of-the-art ? ~95%. 
 
Table 5 Correctness results for the clinical notes 
model. 
5 Discussion 
The results of this pilot project are encouraging. 
It is clear that with appropriate supervision, people 
who are well familiar with medical content can be 
reliably trained to carry out some of the tasks 
traditionally done by trained linguists.  
This study also indicates that an automatic POS 
tagger trained on data that does not include clinical 
documents may not perform as well as a tagger 
trained on data from the same domain. A 
comparison between the Treebank and the clinical 
notes data shows that the clinical notes corpus 
contains 3,239 lexical items that are not found in 
Treebank. The Treebank corpus contains over 
40,000 lexical items that are not found in the 
corpus of clinical notes. 5,463 lexical items are 
found in both corpora.  In addition to this 37% out-
of-vocabulary rate (words in clinical notes but not 
the Treebank corpus), the picture is further 
complicated by the differences between the n-gram 
tag transitions within the two corpora. For 
example, the likelihood of a DT ? NN bigram is 1 
in Treebank and 0.75 in the clinical notes corpus. 
On the other hand, JJ ? NN transition in the 
clinical notes is 1 but in the Treebank corpus it has 
a likelihood of 0.73. This is just to illustrate the 
fact that not only the ?unknown? out-of-vocabulary 
items may be responsible for the decreased 
accuracy of POS taggers trained on general 
English domain and tested on the clinical notes 
domain, but the actual n-gram statistics may be a 
major contributing factor.    
6 Conclusion 
Several questions remain unresolved. First of all, 
it is unclear how much domain specific data is 
enough to achieve state-of-the-art performance on 
POS tagging. Second, given that it is somewhat 
easier to develop lexicons for POS tagging than to 
annotate corpora, we need to find out how 
important the corpus statistics are as opposed to a 
domain specific lexicon. In other words, can we 
achieve state-of-the-art performance in a 
specialized domain by simply adding the 
vocabulary from the domain to the POS tagger?s 
lexicon? We intend to address both of these 
questions with further experimentation. 
7 Acknowledgements 
Our thanks go to Barbara Abbot, Pauline Funk 
and Debora Albrecht for their persistent efforts in 
the difficult task of corpus annotation. This work 
has been carried out under the NLM Training 
Grant # T15  LM07041-19.  
References 
Baldridge, J., Morton, T., and Bierner, G URL: 
http://maxent.sourceforge.net 
Brandts, T (2000) ?TnT ? A Statistical Part-of-Speech 
Tagger.? In Proc. NAACL/ANLP-2000. 
Carletta,  J.  (1996). Assiessing agreement on 
classification tasks: The Kappa statistic. 
Computational Linguistics, 22(2) pp. 249-254.  
Cutting, D., Kupiec, J., Pedersen, J, and Sibun, P. A 
(1992). Practical POS Tagger. In Proc. ANLP?92. 
Jurafski D. and Martin J. (2000). Speech and Language 
Processing. Prentice Hall, NJ. 
Manning, C. and Shutze H. (1999). Foundations of 
Statistical Natural Language Processing. MIT Press, 
Cambridge, MA. 
Marcus, M., B. Santorini, and M. A. Marcinkiewicz 
(1993). Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics 19, 
297-352. 
Mikheev, A. (1997). Automatic Rule Induction for 
Unknown-Word Guessing. Computational Linguistics 
23(3): 405-423  
Poessio, M. and Vieira, R. (1988). ?A corpus based 
investigation of definite description use? 
Computational Linguistics, pp 186-215. 
Ratnaparkhi A. (1996). A maximum entropy part of 
speech tagger. In Proceedings of the conference on 
empirical methods in natural language processing, 
May 1996, University of Pennsylvania 
Ruch P, Baud RH, Rassinoux AM, Bouillon P, Robert 
G. Medical document anonymization with a semantic 
lexicon. Proc AMIA Symp. 2000; 729-33. 
Santorini B. (1991). Part-of-Speech Tagging Guidelines 
for the Penn Treebank Project. Technical Report. 
Department of Computer and Information Science, 
University of Pennsylvania. 
UMLS. (2001). UMLS Knowledge Sources (12th ed.). 
Bethesda (MD): National Library of Medicine.   
 
 
Split Hits Total Correctness 
Average 23018.4 24309 94.69% 
65
Generating Training Data for Medical Dictations 
 
Sergey Pakhomov 
University of Minnesota, MN 
pakhomov.sergey@mayo.edu 
Michael Schonwetter 
Linguistech Consortium, NJ 
MSchonwetter@qwest.net 
Joan Bachenko 
Linguistech Consortium,NJ 
bachenko@mnic.net 
 
   
 
Abstract 
 
In automatic speech recognition (ASR) enabled 
applications for medical dictations, corpora of 
literal transcriptions of speech are critical for 
training both speaker independent and speaker 
adapted acoustic models.  Obtaining these 
transcriptions is both costly and time consuming.  
Non-literal transcriptions, on the other hand, are 
easy to obtain because they are generated in the 
normal course of a medical transcription operation.  
This paper presents a method of automatically 
generating texts that can take the place of literal 
transcriptions for training acoustic and language 
models.  ATRS1 is an automatic transcription 
reconstruction system that can produce near-literal 
transcriptions with almost no human labor. We will 
show that (i) adapted acoustic models trained on 
ATRS data perform as well as or better than 
adapted acoustic models trained on literal 
transcriptions (as measured by recognition 
accuracy) and (ii) language models trained on 
ATRS data have lower perplexity than language 
models trained on non-literal data. 
 
Introduction 
 
Dictation applications of automatic speech 
recognition (ASR) require literal transcriptions of 
speech in order to train both speaker independent 
and speaker adapted acoustic models.  Literal 
transcriptions may also be used to train stochastic 
language models that need to perform well on 
spontaneous or disfluent speech.  With the 
exception of personal desktop systems, however, 
obtaining these transcriptions is costly and time 
consuming since they must be produced manually 
                                                        
1
 patent pending (Serial No.: 09/487398) 
by humans educated for the task.  The high cost 
makes literal transcription unworkable for ASR 
applications that require adapted acoustic models 
for thousands of talkers as well as accurate 
language models for idiosyncratic natural speech.  
 
Non-literal transcriptions, on the other hand, are 
easy to obtain because they are generated in the 
normal course of a medical transcription operation.  
It has been previously shown by Wightman and 
Harder (1999) that the non-literal transcriptions can 
be successfully used in acoustic adaptation. 
However, non-literal transcriptions are incomplete.  
They exclude many utterances that commonly 
occur in medical dictation?filled pauses, 
repetitions, repairs, ungrammatical phrases, 
pleasantries, asides to the transcriptionist, etc.  
Depending on the talker, such material may 
constitute a significant portion of the dictation. 
 
We present a method of automatically generating 
texts that can take the place of literal transcriptions 
for training acoustic and language models.  ATRS 
is an automatic transcription reconstruction system 
that can produce near-literal transcriptions with 
almost no human labor.  
 
The following sections will describe ATRS and 
present experimental results from language and 
acoustic modeling. We will show that (i) adapted 
acoustic models trained on ATRS data perform as 
well as or better than adapted acoustic models 
trained on literal transcriptions (as measured by 
recognition accuracy) and (ii) language models 
trained on ATRS data have lower perplexity than 
language models trained on non-literal data. Data 
used in the experiments comes from medical 
dictations.  All of the dictations are telephone 
speech. 
 
1  Dictation Applications of ASR 
 
The application for our work is medical dictation 
over the telephone.  Medical dictation differs from 
other telephony based ASR applications, e.g. airline 
reservation systems, because the talkers are repeat 
users and utterances are long.  Dictations usually 
consist of 1-30 minutes of speech.  The talkers call 
in 3-5 days per week and produce between 1 and 12 
dictations each day they call.  Hence a medical 
dictation operation has access to hours of speech 
for each talker.   
 
Spontaneous telephone speech presents additional 
challenges that are caused partly by a poor acoustic 
signal and partly by the disfluent nature of 
spontaneous speech. A number of researchers have 
noted the effects of disfluencies on speech 
recognition and have suggested various approaches 
to dealing with them at language modeling and 
post-processing stages. (Shriberg 1994, Shriberg 
1996, Stolcke and Shriberg 1996, Stolcke et al 
1998, Shriberg and Stolcke 1996, Siu and 
Ostendorf 1996, Heeman et al 1996) Medical over-
the-telephone dictations can be classified as 
spontaneous or quasi-spontaneous discourse 
(Pakhomov 1999, Pakhomov and Savova 1999). 
Most physicians do not read a script prepared in 
advance, instead, they engage in spontaneous 
monologues that display the full spectrum of 
disfluencies found in conversational dialogs in 
addition to other "disfluencies" characteristic of 
dictated speech. An example of the latter is when a 
physician gives instructions to the transcriptionist 
to modify something in the preceding discourse, 
sometimes as far as several paragraphs back. 
 
Most ASR dictation applications focus on desktop 
users; for example, Dragon, IBM, Philips and 
Lernout & Hauspie all sell desktop dictation 
recognizers that work on high quality microphone 
speech.  Typically, the desktop system builds an 
adapted acoustic model if the talker "enrolls", i.e. 
reads a prepared script that serves as a literal 
transcription.  Forced alignment of the script and 
the speech provides the input to acoustic model 
adaptation.   
 
Enrollment makes it relatively easy to obtain literal 
transcriptions for adaptation.  However, enrollment 
is not feasible for dictation over the telephone 
primarily because most physicians will refuse to 
take the time to enroll.  The alternative is to hire 
humans who will type literal transcriptions of 
dictation until enough have been accumulated to 
build an adapted model, an impractical solution for 
a large scale operation that processes speech from 
thousands of talkers.  ATRS is appealing because it 
can generate an approximation of literal 
transcription that can replace enrollment scripts and 
the need for manually generated literal 
transcriptions. 
 
2 Three Classes of Training Data 
 
In this paper, training texts for language and 
acoustic models fall into three categories: 
 
Non-Literal:
 Non-literal transcripts present the 
meaning of what was spoken in a written form 
appropriate for the domain.  In a commercial 
medical transcription operation, the non-literal 
transcript will present the dictation in a format 
appropriate for a medical record.  This typically 
involves (i.) ignoring filled pauses, pleasantries, 
and repeats; (ii.) acting on directions for repairs 
("delete the second paragraph and put this in 
instead..."); (iii.) adding non-dictated punctuation; 
(iv.) correcting grammatical errors; and (v.) re-
formatting certain phrases such as "Lung are 
Clear", to a standard form such as "Lungs - Clear". 
 
Literal:
 Literal transcriptions are exact 
transcriptions of what was spoken.  This includes 
any elements not found in the non-literal transcript, 
such as filled pauses (um's and ah's), pleasantries 
and body noises ("thank you very much, just a 
moment, cough"), repeats, fragments, repairs and 
directions for repairs, and asides ("make that 
bold").  Literal transcriptions require significant 
human effort, and therefore are expensive to 
produce.  Even though they are carefully prepared, 
some errors will be present in the result. 
 
In their study of how humans deal with transcribing 
spoken discourse, Lindsay and O'Connell (1995) 
have found that literal transcripts were "far from 
verbatim." (p.111) They find that the transcribers in 
their study tended to have the most difficulty 
transcribing hesitation phenomena, followed by 
sentence fragments, adverbs and conjunctions and, 
finally, nouns, verbs, adjectives and prepositions. 
Our informal observations made from the 
transcripts produced by highly trained medical 
transcriptionists suggest approximately 5% error 
margin and a gradation of errors similar to 
the one found by Lindsay and O'Connell. 
 
Semi-Literal: Semi-literal transcripts are derived 
using non-literal transcripts, the recognizer output, 
a set of grammars, a dictionary, and an interpreter 
to integrate the recognized material into the non-
literal transcription.  Semi-literal transcripts will 
more closely resemble the literal transcripts, as 
many of the elements missing from the non-literal 
transcripts will be restored. 
 
3 Model Adaptation 
 
It is well known that ASR systems perform best 
when acoustic models are adapted to a particular 
talker?s speech.  This is why commercial desktop 
systems use enrollment.  Although less widely 
applied, language model adaptation based on linear 
interpolation is an effective technique for tailoring 
stochastic grammars to particular domains of 
discourse and to particular speakers (Savova et al 
(2000), Weng et al (1997)).  
 
The training texts used in acoustic modeling come 
from recognizer-generated texts, literal 
transcriptions or non-literal transcriptions.  Within 
the family of transformation and combined 
approaches to acoustic modeling (Digalakis and 
Neumeyer (1996), Strom (1996), Wightman and 
Harder (1999), Hazen and Glass (1997)) three basic 
adaptation methods can be identified: unsupervised, 
supervised, or semi-supervised.  Each adaptation 
method depends on a different type of training text.  
What follows will briefly introduce the three 
methods. 
 
Unsupervised adaptation relies on the 
recognizer?s output as the text guiding the 
adaptation.  Efficacy of unsupervised adaptation 
fully depends on the recognition accuracy.  As 
Wightman and Harder (1999) pointed out, 
unsupervised adaptation works well in laboratory 
conditions when the speech signal has large 
bandwidth and is relatively ?clean? of background 
noise, throat clearings, and other disturbances.  In 
laboratory conditions, the errors introduced by 
unsupervised adaptation can be averaged out by 
using more data (Zavaliagkos and Colthurst, 1997); 
however, in a telephony operation with degraded 
input that is not feasible.  
 
Supervised adaptation is dependent on literal 
transcription availability and is widely used in 
enrollment in most desktop ASR systems.  A 
speaker?s speech sample is transcribed verbatim 
and then the speech signal is aligned with 
pronunciations frame by frame for each individual 
word.  A speaker independent model is augmented 
to include the observations resulting from the 
alignment. 
 
Semi-supervised adaptation rests on the idea that 
the speech signal can be partially aligned by using 
of the recognition output and the non-literal 
transcription.  A significant problem with semi-
supervised adaptation is that only the speech that 
the recognizer already recognizes successfully ends 
up being used for adaptation.  This reinforces what 
is already well represented in the model.  
Wightman and Harder (1999) report that semi-
supervised adaptation has a positive side effect of 
excluding those segments of speech that were mis-
recognized for reasons other than a poor acoustic 
model.  They note that background noise and 
speech disfluency are detrimental to the 
unsupervised adaptation.   
 
In addition to the two problems with semi-
supervised adaptation pointed out by Wightman 
and Harder, we find one more potential problem.  
As a result of matching the word labels produced 
by the recognizer and the non-literal transcription, 
some words may be skipped which may introduce 
unnatural phone transitions at word boundaries.  
 
Language model adaptation is not an appropriate 
domain for acoustic adaptation methods.  However, 
adapted language models can be loosely described 
as supervised or unsupervised, based on the types 
of training texts?literal or non-literal?that were 
used in building the model. 
 
In the following sections we will describe the 
system of generating data that is well suited for 
acoustic and language adaptation and present 
results of experimental evaluation of this system. 
 
3.2 Generating semi-literal data 
ATRS is based on reconstruction of non-literal 
transcriptions to train utterance specific language 
models.  First, a non-literal transcription is used to 
train an augmented probabilistic finite state model 
(APFSM) which is, in turn, used by the recognizer 
to re-recognize the exact same utterance that the 
non-literal transcription was generated from.  The 
APFSM is constructed by linear interpolation of a 
finite state model where all transitional 
probabilities are equal to 1 with two other 
stochastic models.   
 
One of the two models is a background model that 
accounts for expressions such as greetings, 
thanking, false starts and repairs.  A list of these 
out-of-transcription expressions is derived by 
comparing already existing literal transcriptions 
with their non-literal transcription counterparts.  
The other model represents the same non-literal 
transcription populated with filled pauses (FP) 
(?um?s and ah?s?) using a stochastic FP model 
derived from a relatively large corpus of literal 
transcriptions (Pakhomov, 1999, Pakhomov and 
Savova, 1999). 
 
 
 
Interpolation weights are established empirically by 
calculating the resulting model?s perplexity against 
held out data.  Out-of-vocabulary (OOV) items are 
handled provisionally by generating on-the-fly 
pronunciations based on the existing dictionary 
spelling-pronunciation alignments. The result of 
interpolating these two background models is that 
some of the transitional probabilities found in the 
finite state model are no longer 1. 
 
The language model so derived can now be used to 
produce a transcription that is likely to be more true 
to what has actually been said than the non-literal 
transcription that we started to work with. 
 
Further refinement of the new semi-literal 
transcription is carried out by using dynamic 
programming alignment on the recognizer?s 
hypothesis (HYP) and the non-literal transcription 
that is used as reference (REF).  The alignment 
results in each HYP label being designated as a 
MATCH, a DELETION, a SUBSTITUTION or an 
INSERTION.  Those labels present in the HYP 
stream that do not align with anything in the REF 
stream are designated as insertions and are assumed 
to represent the out-of-transcription elements of the 
dictation.  Those labels that do align but do not 
match are designated as substitutions.  Finally, the 
labels found in the REF stream that do not align 
with anything in the HYP stream are designated as 
deletions.   
 
 
The final semi-literal transcription is constructed 
differently depending on the intended purpose of  
 
 
Figure { SEQ Figure \* ARABIC } Percent improvement in true data representation 
of ATRS reconstruction vs. Non-Literal data 
the transcription.  If the transcription will be used 
for acoustic modeling, then the MATCHES, the  
REF portion of SUBSTITUTIONS and the HYP 
portion of only those INSERTIONS that represent 
punctuation and filled pauses make it into the final  
semi-literal transcription.  It is important to filter 
out everything else because acoustic modeling is 
very sensitive to misalignment errors.  Language 
modeling, on the other hand, is less sensitive to 
alignment errors; therefore, INSERTIONS and 
DELETIONS can be introduced into the semi-
literal transcription. 
 
One method of ascertaining the quality of semi-
literal reconstruction is to measure its alignment 
errors against literal data using a dynamic 
programming application.  By measuring the 
correctness spread between ATRS and literal data, 
as well as the correctness spread between non-
literal and literal data, the ATRS alignment 
correctness rate was observed to be 4.4% higher 
absolute over 774 dictation files tested. Chart 1 
summarizes the results. The X axis represents the 
number of dictations in each bin displayed along 
the Y axis representing the % improvement over 
the non-literal counterparts. The results showed 
nearly all ATRS files had better alignment 
correctness than their non-literal counterparts.  The 
majority of the reconstructed dictations resemble 
literal transcriptions between 1% and 8% better 
than their non-literal counterparts.  These results 
are statistically significant as evidenced by a t-test 
at 0.05 confidence level.  Much of the increase in 
alignment can be attributed to the introduction of 
filled pauses by ATRS.  However, ignoring filled 
pauses, we have observed informally that the 
correctness still improves in ATRS files versus 
non-literal. 
 
In the following sections we will address acoustic 
and language modeling and show that semi-literal 
training data is a good substitute for literal data.   
 
 
4 Experimental results 
 
The usefulness of semi-literal transcriptions was 
evaluated in two ways: acoustic adaptation and 
language modeling.  
 
4.1 Adapted acoustic model evaluation 
Three speaker adapted acoustic models were 
trained for each of the 5 talkers in this study using 
the three types of label files and evaluated on the 
talker?s testing data. 
 
4.1.1 Setup 
The data collected for each talker were split into 
testing and training. 
Training Data 
45-55 minutes of audio data was collected for each 
of the six talkers in this experiment: 
 
A female 
B female 
C male 
D male 
F female 
 
All talkers are native speakers of English, two 
males and three females. 
 
Non-literal transcriptions
 of this data were 
obtained in the course of normal transcription 
operation where trained medical transcriptionists 
record the dictations while filtering out disfluency, 
asides and ungrammatical utterances. 
 
Literal transcriptions
 were obtained by having 5 
medical transcriptionists specially trained not to 
filter out disfluency and asides transcribe all the 
dictations used in this study. 
 
Semi-literal transcriptions
 were obtained with the 
system described in section 5 of this paper. 
 
Testing Data  
Three dictations (0.5 ? 2 min) each were pulled out 
of the Literal transcriptions training set and set 
aside for each talker for testing. 
Recognition and evaluation software and 
formalism 
 
Software licensed from Entropic Laboratory was 
used for performing recognition, evaluating 
accuracy and acoustic adaptation. (Valtchev, et al 
(1998)). Adapted models were trained using MLLR 
technique (Legetter and Woodland, (1996)) 
available as part of the Entropic package.  
 
Recognition accuracy and correctness reported in 
this study were calculated according to the 
following formulas: 
 
(1) Acc = hits ? insertions / total words 
(2) Correctness = hits / total words 
 
 
4.1.2 Experiment 
The following Acoustic Models were trained via 
adaptation with a general SI model for each talker 
using all available data (except for the testing data). 
Each model?s name reflects the kind of label data 
that was used for training. 
 
LITERAL 
 
Each audio file was aligned with the corresponding 
literal transcription.  
 
NON-LITERAL 
 
Each audio file was recognized using SI acoustic 
and language models. The recognition output was 
aligned with the non-literal transcription using 
dynamic programming. Only those portions of 
audio that corresponded to direct matches in the 
alignment were used to produce alignments for 
acoustic modeling. This method was originally used 
for medical dictations by Wightman and Harder 
(1999). 
 
SEMI-LITERAL 
 
Each audio file has been processed to produce a 
semi-literal transcription that was then aligned with 
recognition output generated in the process of 
creating semi-literal transcriptions. The portions of 
the audio corresponding to matching segments were 
used for acoustic adaptation training. 
 
The SI model had been trained on all available at 
the time (12 hours)2 similar medical dictations to 
the ones used in this study. The data for the 
                                                        
2
 Although 50-100 hours of data for SI modeling is the 
industry standard, the population we are dealing with is 
highly homogeneous and reasonable results can be 
obtained with lesser amount of data. 
speakers in this study were not used in training the 
SI model.  
 
4.1.3 Results 
Table 1 shows the test results. As expected, both 
recognition accuracy and correctness increase with 
any of the three kinds of adaptation. Adaptation 
using Literal transcriptions yields an overall 
10.84% absolute gain in correctness and 11.49% in 
accuracy over the baseline. 
 
Adaptation using Non-literal transcriptions yields 
an overall 6.36 % absolute gain in correctness and 
5.23 % in accuracy over the baseline. Adaptation 
with Semi-literal transcriptions yields an overall 
11.39 % absolute gain in correctness and 11.05 % 
in accuracy over the baseline. No statistical 
significance tests were performed on this data. 
 
Table 1. Recognition results for three adaptation 
methods 
 
4.1.4 Discussion 
The results of this experiment provide additional 
support for using automatically generated semi-
literal transcriptions as a viable (and possibly 
superior) substitute for literal data. The fact that 
three SEMI-LITERAL adapted AM?s out of 5 
performed better than their LITERAL counterparts 
seems to indicate that there may be undesirable 
noise either in the literal transcriptions or in the 
corresponding audio. It may also be due to the 
relatively small amount of training data used for SI 
modeling thus providing a baseline that can be 
improved with little effort. However, the results 
still indicate that generating semi-literal 
transcriptions may help eliminate the undesirable 
noise and, at the same time, get the benefits of 
broader coverage that semi-literal transcripts can 
afford over NON-LITERAL transcriptions. 
 
 Baseline (SI) 
% 
Literal       
% 
Semi-literal 
% 
Non-literal       
% 
Talker Cor Acc Cor Acc Cor  Acc Cor Acc 
A 58.76 48.47 66.57 58.09 68 58.28 64.76 51.8 
B 41.28 32.2 58.36 49.46 64.59 56.22 55.87 44.66 
C 57.22 54.99 64.38 61.54 61.25 59.31 60.65 58.71 
D 56.86 51.47 68.69 63.3 65.91 59.13 64.69 58.26 
F 54.83 43.69 61.97 53.57 64.7 54.41 61.13 48.73 
         
AVG 52.49 44.81 63.33 56.3 63.81 55.86 58.85 50.04 
4.2 Language Model Evaluation 
For ASR applications where there are significant 
discrepancies between an utterance and its formal 
transcription, the inclusion of literal data in the 
language model can reduce language model 
perplexity and improve recognition accuracy.  In 
medical transcription, the non-literal texts typically 
depart from what has actually been said.  Hence if 
the talker says "lungs are clear" or "lungs sound 
pretty clear", the typed transcription is likely to 
have "Lungs - clear".  In addition, as we noted 
earlier, the non-literal transcription will omit 
disfluencies and asides and will correct 
grammatical errors. 
 
Literal and semi-literal texts can be added onto 
language model training data or interpolated into 
an existing language model. Below we will present 
results of a language modeling experiment that 
compares language models built from literal, semi-
literal and non-literal versions of the same training 
set.  The results substantiate our claim that 
automatically generated semi-literal transcription 
can lead to a significant improvement in language 
model quality. 
 
In order to test the proposed method?s suitability 
for language modeling, we constructed three 
trigram language models and used perplexity as the 
measure of the models? goodness. 
 
Setup 
The following models were trained on three 
versions of a 270,000-word corpus.  The size of the 
training corpus is dictated by availability of literal 
transcriptions.  The vocabulary was derived from a 
combination of all three corpora to keep the OOV 
rate constant. 
 
LLM ? language model built from a corpus of 
literal transcriptions  
NLM ? language model built from non-literal 
transcriptions 
SLM ? language model built from semi-literal 
transcriptions  
 
Approximately 5,000-word literal transcriptions 
corpus consisting of 24 dictations was set aside for 
testing 
 
Results 
The results of perplexity tests of the three models 
on the held-out data at 3-gram level are 
summarized in Table 2. The tests were carried out 
using the Entropic Transcriber Toolkit 
 
It is apparent that SLM yields considerably better 
perplexity than NLM, which indicates that although 
semi-literal transcriptions are not as good as actual  
literal transcriptions, they are more suitable for  
 
Table 2.  Perplexity tests on LLM, NLM, SLM 
 
language modeling than non-literal transcriptions.  
These results are obtained with 270,000 words of 
training data; however, the typical amount is 
dozens of million. We would expect the differences 
in perplexity to become smaller with larger 
amounts of training data. 
 
Conclusions and future work 
 
We have described ATRS, a system for 
reconstructing semi-literal transcriptions 
automatically.  ATRS texts can be used as a 
substitute for literal transcriptions when the cost 
and time required for generating literal 
transcriptions are infeasible, e.g. in a telephony 
based transcription operation that processes 
thousands of acoustic and language models.  Texts 
produced with ATRS were used in training speaker 
adapted acoustic models, speaker independent 
acoustic models and language models.  
Experimental results show that models built from 
ATRS training data yield performance results that 
are equivalent to those obtained with models 
trained on literal transcriptions. In the future, we 
will address the issue of the amount of training data 
for the SI model. Also, current ATRS system does 
not take advantage of various confidence scores 
available in leading recognition engines. We 
believe that using such confidence measures can 
improve the generation of semi-literal transcriptions 
considerably. We would also like to investigate the 
point at which the size of the various kinds of data 
 Perplexity OOV rate (%) 
LLM 185 2.61 
NLM 613 2.61 
SLM 313 2.61 
used for adaptation stops making improvements in 
recognition accuracy.  
 
Acknowledgements 
We would like to thank the anonymous reviewers 
of this paper for very helpful feedback. We thank 
Guergana Savova for excellent suggestions and 
enthusiastic support. We would also like to thank 
Jim Wu for valuable input.   
 
References 
 
Digalakis, V and Neumyer, L. (1996). Speaker 
Adaptation Using Combined Transformation 
and Baysean Mehtods. IEEE Trans. Speech and 
Audio Processing. 
Hazen, T and Glass, J (1997). A Comparison of Novel 
Techniques for Instantaneous Speaker 
Adaptation. In Proc. Eurospeech ?97. 
Heeman, P., Loken-Kim, K and Allen J. (1996). 
Combining the Detection and Correction of 
Speech Repairs. In Proc. ICSLP ?96. 
Huang, X.  and Lee, K (1993).  On Speaker ?
Independent, Speaker-Dependent, and Speaker-
Adaptive Speech Recognition.  In IEEE 
Transactions on Speech and Audio processing, 
Vol.  1, No.  2, pp.  150 ? 157.   
Legetter, C. and Woodland, P. (1996). Maximum 
Likelihood Linear Regression for Speaker 
Adaptation of Continuous Density HMM?s. In 
Computer Speech and Language , 9, (171-186). 
Pakhomov, S.  (1999).  Modeling Filled Pauses in 
Medical Transcriptions.  In Student Section of 
Proc.  ACL?99. 
Pakhomov, S and Savova, G.  (1999).  Filled Pause 
Modeling in Quasi-Spontaneous Speech.  In 
Proc.  Disfluency in Spontaneous Speech 
Workshop at ICPHIS ?99. 
Savova, G, Schonwetter, M. and Pakhomov, S. (2000).  
Improving language model perplexity and 
recognition accuracy for medical dictations via 
within-domaininterpolation with literal and 
semi-literal corpora " In Proc. ICSLP ?00. 
Shriberg, E. 1994 Preliminaries to a Theory of Speech 
Disfluencies. Ph. D. thesis, University of 
California at Berkely. 
Shriberg, E. and Stolcke, A. (1996). Word Predictability 
after Hesitations: A Corpus-based Study. In 
Proc. ICSLP ?96. 
Siu, M and Ostendorf, M. (1996). Modeling Disfluencies 
in Conversational Speech. In Proc. ICSLP ?96. 
Stolcke, A. and Shriberg, E. (1996). Statistical Language 
Modeling for Speech Disfluencies. In proc. 
ICASSP ?96. 
Stolcke A., Shriberg E., Bates R., Ostendorf M., Hakkani 
D., Plauche M., Tur G., and Lu  Y. (1998). 
Automatic Detection of Sentence Boundaries 
and Disfluencies based on Recognized Words. 
Proc. Intl. Conf. on Spoken Language 
Processing. 
Str?m, N (1996): "Speaker Adaptation by Modeling the 
Speaker Variation in a Continuous Speech 
Recognition System," In Proc. ICSLP '96, 
Philadelphia, pp. 989-992.  
Valtchev, V.  Kershaw, D.  and Odell, J.  (1998).  The 
Truetalk Transcriber Book.  Entropic 
Cambridge Research Laboratory, Cambridge, 
England. 
Wightman, C.  W.  and Harder T.  A.  (1999).  Semi-
Supervised Adaptation of Acoustic Models for 
Large-Volume Dictation? In Proc. Eurospeech 
?98. pp 1371-1374. 
Weng, F.,  Stolcke, A., Sankar, A.  (1997). Hub4 
Language Modeling Using Domain 
Interpolation and Data Clustering. Proc. 
DARPA Speech Recognition Workshop, pp. 
147-151, Chantilly, VA. 
 
 
  
  
 
 
BioNLP 2007: Biological, translational, and clinical language processing, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
Determining the Syntactic Structure of Medical Terms in Clinical Notes
Bridget T. McInnes
Dept. of Computer Science
and Engineering
University of Minnesota
Minneapolis, MN, 55455
bthomson@cs.umn.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota Duluth
Duluth, MN, 55812
tpederse@d.umn.edu
Serguei V. Pakhomov
Dept. of Pharmaceutical Care
and Health Systems Center
for Health Informatics
University of Minnesota
Minneapolis, MN, 55455
pakh0002@umn.edu
Abstract
This paper demonstrates a method for de-
termining the syntactic structure of medi-
cal terms. We use a model-fitting method
based on the Log Likelihood Ratio to clas-
sify three-word medical terms as right or
left-branching. We validate this method by
computing the agreement between the clas-
sification produced by the method and man-
ually annotated classifications. The results
show an agreement of 75% - 83%. This
method may be used effectively to enable
a wide range of applications that depend
on the semantic interpretation of medical
terms including automatic mapping of terms
to standardized vocabularies and induction
of terminologies from unstructured medical
text.
1 Introduction
Most medical concepts are expressed via a domain
specific terminology that can either be explicitly
agreed upon or extracted empirically from domain
specific text. Regardless of how it is constructed,
a terminology serves as a foundation for informa-
tion encoding, processing and exchange in a special-
ized sub-language such as medicine. Concepts in the
medical domain are encoded through a variety of lin-
guistic forms, the most typical and widely accepted
is the noun phrase (NP). In some even further spe-
cialized subdomains within medicine, such as nurs-
ing and surgery, an argument can be made that some
concepts are represented by an entire predication
rather than encapsulated within a single nominal-
ized expression. For example, in order to describe
someone?s ability to lift objects 5 pounds or heav-
ier above their head, it may be necessary to use a
term consisting of a predicate such as [LIFT] and a
set of arguments corresponding to various thematic
roles such as <PATIENT> and <PATH> (Ruggieri
et al, 2004). In this paper, we address typical med-
ical terms encoded as noun phrases (NPs) that are
often structurally ambiguous, as in Example 1, and
discuss a case for extending the proposed method to
non-nominalized terms as well.
small1 bowel2 obstruction3 (1)
The NP in Example 1 can have at least two interpre-
tations depending on the syntactic analysis:
[[small1 bowel2] obstruction3] (2)
[small1 [bowel2 obstruction3]] (3)
The term in Example 2 denotes an obstruction in
the small bowel, which is a diagnosable disorder;
whereas, the term in Example 3 refers to a small un-
specified obstruction in the bowel.
Unlike the truly ambiguous general English cases
such as the classical ?American History Professor?
where the appropriate interpretation depends on the
context, medical terms, such as in Example 1, tend
to have only one appropriate interpretation. The
context, in this case, is the discourse domain of
medicine. From the standpoint of the English lan-
guage, the interpretation that follows from Example
3 is certainly plausible, but unlikely in the context
of a medical term. The syntax of a term only shows
9
what interpretations are possible without restricting
them to any particular one. From the syntactic anal-
ysis, we know that the term in Example 1 has the po-
tential for being ambiguous; however, we also know
that it does have an intended interpretation by virtue
of being an entry term in a standardized terminology
with a unique identifier anchoring its meaning. What
we do not know is which syntactic structure gen-
erated that interpretation. Being able to determine
the structure consistent with the intended interpreta-
tion of a clinical term can improve the analysis of
unrestricted medical text and subsequently improve
the accuracy of Natural Language Processing (NLP)
tasks that depend on semantic interpretation.
To address this problem, we propose to use a
model-fitting method which utilizes an existing sta-
tistical measure, the Log Likelihood Ratio. We val-
idate the application of this method on a corpus
of manually annotated noun-phrase-based medical
terms. First, we present previous work on structural
ambiguity resolution. Second, we describe the Log
Likelihood Ratio and then its application to deter-
mining the structure of medical terms. Third, we
describe the training corpus and discuss the compi-
lation of a test set of medical terms and human ex-
pert annotation of those terms. Last, we present the
results of a preliminary validation of the method and
discuss several possible future directions.
2 Previous Work
The problem of resolving structural ambiguity has
been previously addressed in the computational lin-
guistics literature. There are multiple approaches
ranging from purely statistical (Ratnaparkhi, 1998),
to hybrid approaches that take into account the lexi-
cal semantics of the verb (Hindle and Rooth, 1993),
to corpus-based, which is the approach discussed
in this paper. (Marcus, 1980) presents an early ex-
ample of a corpus-based approach to syntactic am-
biguity resolution. One type of structural ambigu-
ity that has received much attention has to do with
nominal compounds as seen in the work of (Resnik,
1993), (Resnik and Hearst, 1993), (Pustejovsky et
al., 1993), and (Lauer, 1995).
(Lauer, 1995) points out that the existing ap-
proaches to resolving the ambiguity of noun phrases
fall roughly into two camps: adjacency and de-
pendency. The proponents of the adjacency model
((Liberman and Sproat, 1992), (Resnik, 1993) and
(Pustejovsky et al, 1993)) argue that, given a three
word noun phrase XYZ, there are two possible an-
alyzes [[XY]Z] and [X[YZ]]. The correct analysis
is chosen based on the ?acceptability? of the adja-
cent bigrams A[XY] and A[YZ]. If A[XY] is more
acceptable than A[YZ], then the left-branching anal-
ysis [[XY]Z] is preferred.
(Lauer and Dras, 1994) and (Lauer, 1995) address
the issue of structural ambiguity by developing a de-
pendency model where instead of computing the ac-
ceptability of A[YZ] one would compute the accept-
ability of A[XZ]. (Lauer, 1995) argues that the de-
pendency model is not only more intuitive than the
adjacency model, but also yields better results. (La-
pata and Keller, 2004) results also support this as-
sertion.
The difference between the approaches within the
two models is the computation of acceptability. Pro-
posals for computing acceptability (or preference)
include raw frequency counts ((Evans and Zhai,
1996) and (Lapata and Keller, 2004)), Latent Se-
mantic Indexing ((Buckeridge and Sutcliffe, 2002))
and statistical measures of association ((Lapata et
al., 1999) and (Nakov and Hearst, 2005)).
One of the main problems with using frequency
counts or statistical methods for structural ambigu-
ity resolution is the sparseness of data; however,
(Resnik and Hearst, 1993) used conceptual associa-
tions (associations between groups of terms deemed
to form conceptual units) in order to alleviate this
problem. (Lapata and Keller, 2004) use the doc-
ument counts returned by WWW search engines.
(Nakov and Hearst, 2005) use the ?2 measure based
on statistics obtained from WWW search engines to
compute values to determine acceptability of a syn-
tactic analysis for nominal compounds. This method
is tested using a set of general English nominal com-
pounds developed by (Lauer, 1995) as well as a set
of nominal compounds extracted from MEDLINE
abstracts.
The novel contribution of our study is in demon-
strating and validating a corpus-based method for
determining the syntactic structure of medical terms
that relies on using the statistical measure of asso-
ciation, the Log Likelihood Ratio, described in the
following section.
10
3 Log Likelihood Ratio
The Log Likelihood Ratio (G2) is a ?goodness of
fit? statistic first proposed by (Wilks, 1938) to test if
a given piece of data is a sample from a set of data
with a specific distribution described by a hypothe-
sized model. It was later applied by (Dunning, 1993)
as a way to determine if a sequence of N words (N-
gram) came from an independently distributed sam-
ple.
(Pedersen et al, 1996) pointed out that there ex-
ists theoretical assumptions underlying the G2 mea-
sure that were being violated therefore making them
unreliable for significance testing. (Moore, 2004)
provided additional evidence that although G2 may
not be useful for determining the significance of an
event, its near equivalence to mutual information
makes it an appropriate measure of word associa-
tion. (McInnes, 2004) applied G2 to the task of ex-
tracting three and four word collocations from raw
text.
G2, formally defined for trigrams in Equation 4,
compares the observed frequency counts with the
counts that would be expected if the words in the
trigram (3-gram; a sequence of three words) corre-
sponded to the hypothesized model.
G2 = 2 ?
?
x,y,z
nxyz ? log(
nxyz
mxyz
) (4)
The parameter nxyz is the observed frequency of
the trigram where x, y, and z respectively represent
the occurrence of the first, second and third words
in the trigram. The variable mxyz is the expected
frequency of the trigram which is calculated based
on the hypothesized model. This calculation varies
depending on the model used. Often the hypothe-
sized model used is the independence model which
assumes that the words in the trigram occur together
by chance. The calculation of the expected values
based on this model is as follows:
mxyz = nx++ ? n+y+ ? n++z/n+++ (5)
The parameter, n+++, is the total number of tri-
grams that exist in the training data, and nx++,
n+y+, and n++z are the individual marginal counts
of seeing words x, y, and z in their respective posi-
tions in a trigram. A G2 score reflects the degree to
which the observed and expected values diverge. A
G2 score of zero implies that the observed values are
equal to the expected and the trigram is represented
perfectly by the hypothesized model. Hence, we
would say that the data ?fits? the model. Therefore,
the higher the G2 score, the less likely the words
in the trigram are represented by the hypothesized
model.
4 Methods
4.1 Applying Log Likelihood to Structural
Disambiguation
The independence model is the only hypothesized
model used for bigrams (2-gram; a sequence of
two words). As the number of words in an N-
gram grows, the number of hypothesized models
also grows. The expected values for a trigram can
be based on four models. The first model is the
independence model discussed above. The second
is the model based on the probability that the first
word and the second word in the trigram are depen-
dent and independent of the third word. The third
model is based on the probability that the second
and third words are dependent and independent of
the first word. The last model is based on the prob-
ability that the first and third words are dependent
and independent of the second word. Table 1 shows
the different models for the trigram XYZ.
Table 1: Models for the trigram XYZ
Model 1 P(XYZ) / P(X) P(Y) P(Z)
Model 2 P(XYZ) / P(XY) P(Z)
Model 3 P(XYZ) / P(X) / P(YZ)
Model 4 P(XYZ) / P(XZ) P(Y)
Slightly different formulas are used to calculate
the expected values for the different hypothesized
models. The expected values for Model 1 (the in-
dependence model) are given above in Equation 5.
The calculation of expected values for Model 2, 3, 4
are seen in Equations 6, 7, 8 respectively.
mxyz = nxy+ ? n++z/n+++ (6)
mxyz = nx++ ? n+yz/n+++ (7)
mxyz = nx+z ? n+y+/n+++ (8)
The parameter nxy+ is the number of times words
x and y occur in their respective positions, n+yz is
11
the number of times words y and z occur in their
respective positions and nx+z is the number of times
that words x and z occur in their respective positions
in the trigram.
The hypothesized models result in different ex-
pected values which results in a different G2 score.
A G2 score of zero implies that the data are perfectly
represented by the hypothesized model and the ob-
served values are equal to the expected. Therefore,
the model that returns the lowest score for a given
trigram is the model that best represents the struc-
ture of that trigram, and hence, best ?fits? the trigram.
For example, Table 2 shows the scores returned for
each of the four hypothesized models for the trigram
?small bowel obstruction?.
Table 2: Example for the term ?small bowel obstruc-
tion?
Model G2 score Model G2 score
Model 1 11,635.45 Model 2 5,169.81
Model 3 8,532.90 Model 4 7,249.90
The smallest G2 score is returned by Model 2
which is based on the first and second words be-
ing dependent and independent of the third. Based
on the data, Model 2 best represents or ?fits? the tri-
gram, ?small bowel obstruction?. In this particular
case that happens to be the correct analysis.
The frequency counts and G2 scores for each
model were obtained using the N-gram Statistics
Package 1 (Banerjee and Pedersen, 2003).
4.2 Data
The data for this study was collected from two
sources: the Mayo Clinic clinical notes and
SNOMED-CT terminology (Stearns et al, 2001).
4.2.1 Clinical Notes
The corpus used in this study consists of over
100,000 clinical notes covering a variety of ma-
jor medical specialties at the Mayo Clinic. These
notes document each patient-physician contact and
are typically dictated over the telephone. They range
in length from a few lines to several pages of text
and represent a quasi-spontaneous discourse where
the dictations are made partly from notes and partly
1http://www.d.umn.edu/ tpederse/nsp.html
from memory. At the Mayo Clinic, the dictations
are transcribed by trained personnel and are stored
in the patient?s chart electronically.
4.2.2 SNOMED-CT
SNOMED-CT (Systematized Nomenclature of
Medicine, Clinical Terminology) is an ontologi-
cal resource produced by the College of American
Pathologists and distributed as part of the Unified
Medical Language System2 (UMLS) Metathesaurus
maintained by the National Library of Medicine.
SNOMED-CT is the single largest source of clini-
cal terms in the UMLS and as such lends itself well
to the analysis of terms found in clinical reports.
SNOMED-CT is used for many applications in-
cluding indexing electronic medical records, ICU
monitoring, clinical decision support, clinical trials,
computerized physician order entry, disease surveil-
lance, image indexing and consumer health informa-
tion services. The version of SNOMED-CT used in
this study consists of more than 361,800 unique con-
cepts with over 975,000 descriptions (entry terms)
(SNOMED-CT Fact Sheet, 2004).
4.3 Testset of Three Word Terms
We used SNOMED-CT to compile a list of terms
in order to develop a test set to validate the G2
method. The test set was created by extracting all
trigrams from the corpus of clinical notes and all
three word terms found in SNOMED-CT. The inter-
section of the SNOMED-CT terms and the trigrams
found in the clinical notes was further restricted to
include only simple noun phrases that consist of a
head noun modified with a set of other nominal or
adjectival elements including adjectives and present
and past participles. Adverbial modification of ad-
jectives was also permitted (e.g. ?partially edentu-
lous maxilla?). Noun phrases with nested prepo-
sitional phrases such as ?fear of flying? as well as
three word terms that are not noun phrases such as
?does not eat? or ?unable to walk? were excluded
from the test set. The resulting test set contains 710
items.
The intended interpretation of each three word
term (trigram) was determined by arriving at a
2Unified Medical Language System is a compendium of
over 130 controlled medical vocabularies encompassing over
one million concepts.
12
consensus between two medical index experts
(kappa=0.704). These experts have over ten years of
experience with classifying medical diagnoses and
are highly qualified to carry out the task of deter-
mining the intended syntactic structure of a clinical
term.
Table 3: Four Types of Syntactic Structures of Tri-
gram Terms
left-branching ((XY)Z):
[[urinary tract] infection]
[[right sided] weakness]
right-branching (X(YZ)):
[chronic [back pain]]
[low [blood pressure]]
non-branching ((X)(Y)(Z)):
[[follicular][thyroid][carcinoma]]
[[serum][dioxin][level]]
monolithic (XYZ):
[difficulty finding words]
[serous otitis media]
In the process of annotating the test set of tri-
grams, four types of terms emerged (Table 3). The
first two types are left and right-branching where the
left-branching phrases contain a left-adjoining group
that modifies the head of the noun phrase. The right-
branching phrases contain a right-adjoining group
that forms the kernel or the head of the noun phrase
and is modified by the remaining word on the left.
The non-branching type is where the phrase contains
a head noun that is independently modified by the
other two words. For example, in ?follicular thyroid
carcinoma?, the experts felt that ?carcinoma? was
modified by both ?follicular? and ?thyroid? indepen-
dently, where the former denotes the type of cancer
and the latter denotes its location. This intuition is
reflected in some formal medical classification sys-
tems such as the Hospital International Classifica-
tion of Disease Adaptation (HICDA) where cancers
are typically classified with at least two categories -
one for location and one for the type of malignancy.
This type of pattern is rare. We were able to iden-
tify only six examples out of the 710 terms. The
monolithic type captures the intuition that the terms
function as a collocation and are not decomposable
into subunits. For example, ?leg length discrepancy?
denotes a specific disorder where one leg is of a dif-
ferent length from the other. Various combinations
of subunits within this term result in nonsensical ex-
pressions.
Table 4: Distribution of term types in the test set
Type Count %total
Left-branching 251 35.5
Right-branching 378 53.4
Non-branching 6 0.8
Monolithic 73 10.3
Total 708 100
Finally, there were two terms for which no con-
sensus could be reached: ?heart irregularly irregu-
lar? and ?subacute combined degeneration?. These
cases were excluded from the final set. Table 4
shows the distribution of the four types of terms in
the test set.
5 Evaluation
We hypothesize that general English typically has
a specific syntactic structure in the medical domain,
which provides a single semantic interpretation. The
patterns observed in the set of 710 medical terms
described in the previous section suggest that the
G2 method offers an intuitive way to determine the
structure of a term that underlies its syntactic struc-
ture.
Table 5: G2 Model Descriptions
left-branching Model 2 [ [XY] Z ]
right-branching Model 3 [ X [YZ] ]
The left and right-branching patterns roughly cor-
respond to Models 2 and 3 in Table 5. Models 1
and 4 do not really correspond to any of the pat-
terns we were able to identify in the set of terms.
Model 1 would represent a term where words are
completely independent of each other, which is an
unlikely scenario given that we are working with
terms whose composition is dependent by definition.
This is not to say that in other applications (e.g.,
syntactic parsing) this model would not be relevant.
Model 4 suggests dependence between the outer
edges of a term and their independence from the
13
Figure 1: Comparison of the results with two base-
lines: L-branching and R-branching assumptions
middle word, which is not motivated from the stand-
point of a traditional context free grammar which
prohibits branch crossing. However, this model may
be welcome in a dependency grammar paradigm.
One of the goals of this study is to test an ap-
plication of the G2 method trained on a corpus of
medical data to distinguish between left and right-
branching patterns. The method ought to suggest
the most likely analysis for an NP-based medical
term based on the empirical distribution of the term
and its components. As part of the evaluation, we
compute the G2 scores for each of the terms in the
test set, and picked the model with the lowest score
to represent the structural pattern of the term. We
compared these results with manually identified pat-
terns. At this preliminary stage, we cast the problem
of identifying the structure of a three word medical
term as a binary classification task where a term is
considered to be either left or right-branching, ef-
fectively forcing all terms to either be represented
by either Model 2 or Model 3.
6 Results and Discussion
In order to validate the G2 method for determin-
ing the structure of medical terms, we calculated
the agreement between human experts? interpreta-
tion of the syntactic structure of the terms and the
interpretation suggested by the G2 method. The
agreement was computed as the ratio of match-
ing interpretations to the total number of terms be-
ing interpreted. We used two baselines, one estab-
lished by assuming that each term is left-branching
and the other by assuming that each term is right-
branching. As is clear from Table 4, the left-
branching baseline is 35.5% and the right-branching
baseline is 53.4% meaning that if we simply as-
sign left-branching pattern to each three word term,
we would agree with human experts 35.5% of the
time. The G2 method correctly identifies 185 tri-
grams as being left-branching (Model 2) and 345 tri-
grams as being right-branching (Model 3). There are
116 right-branching trigrams incorrectly identified
as left-branching, and 62 left-branching trigrams in-
correctly identified as right- branching. Thus the
method and the human experts agreed on 530 (75%)
terms out of 708 (kappa=0.473), which is better than
both baselines (Figure 1). We did not find any over-
lap between the terms that human experts annotated
as non-branching and the terms whose corpus dis-
tribution can be represented by Model 4 ([[XZ]Y]).
This is not surprising as this pattern is very rare.
Most of the terms are represented by either Model 2
(left-branching) or Model 3 (right-branching). The
monolithic terms that the human experts felt were
not decomposable constitute 10% of all terms and
may be handled through some other mechanism
such as collocation extraction or dictionary lookup.
Excluding monolithic terms from testing results in
83.5% overall agreement (kappa=0.664).
We observed that 53% of the terms in our test
set are right-branching while only 35% are left-
branching. (Resnik, 1993) found between 64% and
67% of nominal compounds to be left-branching and
used that finding to establish a baseline for his exper-
iments with structural ambiguity resolution. (Nakov
and Hearst, 2005) also report a similar percentage
(66.8%) of left-branching noun compounds. Our
test set is not limited to nominal compounds, which
may account for the fact that a slight majority of the
terms are found to be right-branching as adjectival
modification in English is typically located to the
left of the head noun. This may also help explain
the fact that the method tends to have higher agree-
ment within the set of right-branching terms (85%)
vs. left-branching (62%).
We also observed that many of the terms marked
as monolithic by the experts are of Latin origin such
as the term in Example 9 or describe the functional
14
status of a patient such as the term in Example 10.
erythema1 ab2 igne3 (9)
difficulty1 swallowing2 solids3 (10)
Example 10 merits further discussion as it illus-
trates another potential application of the method
in the domain of functional status terminology. As
was mentioned in the introduction, functional status
terms may be be represented as a predication with
a set of arguments. Such view of functional status
terminology lends itself well to a frame-based repre-
sentation of functional status terms in the context of
a database such as FrameNet 3 or PropBank4. One of
the challenging issues in representing functional sta-
tus terminology in terms of frames is the distinction
between the core predicate and the frame elements
(Ruggieri et al, 2004). It is not always clear what
lexical material should be part of the core predicate
and what lexical material should be part of one or
more arguments. Consider the term in Example 10
which represents a nominalized form of a predica-
tion. Conceivably, we could analyze this term as a
frame shown in Example 11 where the predication
consists of a predicate [DIFFICULTY] and two ar-
guments. Alternatively, Example 12 presents a dif-
ferent analysis where the predicate is a specific kind
of difficulty with a single argument.
[P:DIFFICULTY]
[ARG1:SWALLOWING<ACTIVITY>]
[ARG2:SOLIDS<PATIENT>]
(11)
[P:SWALLOWING DIFFICULTY]
[ARG1: SOLIDS<PATIENT>]
(12)
The analysis dictates the shape of the frames
and how the frames would fit into a network of
frames. The G2 method identifies Example 10 as
left-branching (Model 2), which suggests that it
would be possible to have a parent DIFFICULTY
frame and a child CLIMBING DIFFICULTY that
would inherit form its parent. An example where
this is not possible is the term ?difficulty staying
asleep? where it would probably be nonsensical or at
least impractical to have a predicate such as [STAY-
ING DIFFICULTY]. It would be more intuitive to
3http://www.icsi.berkeley.edu/framenet/
4http://www.cis.upenn.edu/ ace/
assign this term to the DIFFICULTY frame with
a frame element whose lexical content is ?staying
asleep?. The method appropriately identifies the
term ?difficulty staying asleep? as right-branching
(Model 3) where the words ?staying asleep? are
grouped together. This is an example based on in-
formal observations; however, it does suggest a util-
ity in constructing frame-based representation of at
least some clinical terms.
7 Limitations
The main limitation of the G2 method is the expo-
nential growth in the number of models to be evalu-
ated with the growth in the length of the term. This
limitation can be partly alleviated by either only con-
sidering adjacent models and limiting the length to
5-6 words, or using a forward or backward sequen-
tial search proposed by (Pedersen et al, 1997) for
the problem of selecting models for the Word Sense
Disambiguation task.
8 Conclusions and Future Work
This paper presented a simple but effective method
based on G2 to determine the internal structure of
three-word noun phrase medical terms. The abil-
ity to determine the syntactic structure that gives
rise to a particular semantic interpretation of a med-
ical term may enable accurate mapping of unstruc-
tured medical text to standardized terminologies and
nomenclatures. Future directions to improve the ac-
curacy of our method include determining how other
measures of association, such as dice coefficient and
?2, perform on this task. We feel that there is a pos-
sibility that no single measure performs best over all
types of terms. In that case, we plan to investigate in-
corporating the different measures into an ensemble-
based algorithm.
We believe the model-fitting method is not lim-
ited to structural ambiguity resolution. This method
could be applied to automatic term extraction and
automatic text indexing of terms from a standard-
ized vocabulary. More broadly, the principles of us-
ing distributional characteristics of word sequences
derived from large corpora may be applied to unsu-
pervised syntactic parsing.
15
Acknowledgments
We thank Barbara Abbott, Debra Albrecht and
Pauline Funk for their contribution to annotating the
test set and discussing aspects of medical terms.
This research was supported in part by the
NLM Training Grant in Medical Informatics (T15
LM07041-19). Ted Pedersen?s participation in this
project was supported by the NSF Faculty Early Ca-
reer Development Award (#0092784).
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistic Package. In
Proc. of the Fourth International Conference on Intel-
ligent Text Processing and Computational Linguistics,
Mexico City, February.
A.M. Buckeridge and R.F.E. Sutcliffe. 2002. Disam-
biguating noun compounds with latent semantic index-
ing. International Conference On Computational Lin-
guistics, pages 1?7.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
D.A. Evans and C. Zhai. 1996. Noun-phrase analysis in
unrestricted text for information retrieval. Proc. of the
34th conference of ACL, pages 17?24.
D. Hindle and M. Rooth. 1993. Structural Ambigu-
ity and Lexical Relations. Computational Linguistics,
19(1):103?120.
M. Lapata and F. Keller. 2004. The Web as a Base-
line: Evaluaing the Performance of Unsupervised
Web-based Models for a Range of NLP Tasks. Proc.
of HLT-NAACL, pages 121?128.
M. Lapata, S. McDonald, and F. Keller. 1999. Determi-
nants of Adjective-Noun Plausibility. Proc. of the 9th
Conference of the European Chapter of ACL, 30:36.
M. Lauer and M. Dras. 1994. A Probabilistic Model of
Compound Nouns. Proc. of the 7th Australian Joint
Conference on AI.
M. Lauer. 1995. Corpus Statistics Meet the Noun Com-
pound: Some Empirical Results. Proc. of the 33rd An-
nual Meeting of ACL, pages 47?55.
M. Liberman and R. Sproat. 1992. The stress and struc-
ture of modified noun phrases in English. Lexical Mat-
ters, CSLI Lecture Notes, 24:131?181.
M.P. Marcus. 1980. Theory of Syntactic Recognition
for Natural Languages. MIT Press Cambridge, MA,
USA.
B.T. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master?s thesis,
University of Minnesota.
R. Moore. 2004. On log-likelihood-ratios and the sig-
nificance of rare events. In Dekang Lin and Dekai
Wu, editors, Proc. of EMNLP 2004, pages 333?340,
Barcelona, Spain, July. Association for Computational
Linguistics.
P. Nakov and M. Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound
bracketing. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 17?24, Ann Arbor, Michigan, June. As-
sociation for Computational Linguistics.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Howard Shrobe and Ted
Senator, editors, Proc. of the Thirteenth National Con-
ference on Artificial Intelligence and the Eighth Inno-
vative Applications of Artificial Intelligence Confer-
ence, Vol. 2, pages 455?460, Menlo Park, California.
AAAI Press.
T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequen-
tial model selection for word sense disambiguation. In
Proc. of the Fifth Conference on Applied Natural Lan-
guage Processing, pages 388?395, Washington, DC,
April.
J. Pustejovsky, P. Anick, and S. Bergler. 1993. Lexi-
cal semantic techniques for corpus analysis. Compu-
tational Linguistics, 19(2):331?358.
A. Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Lnaguage Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
P. Resnik and M. Hearst. 1993. Structural Ambiguity
and Conceptual Relations. Proc. of the Workshop on
Very Large Corpora: Academic and Industrial Per-
spectives, June, 22(1993):58?64.
P.S. Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D. the-
sis, University of Pennsylvania.
A.P. Ruggieri, S. Pakhomov, and C.G. Chute. 2004. A
Corpus Driven Approach Applying the ?Frame Se-
mantic? Method for Modeling Functional Status Ter-
minology. Proc. of MedInfo, 11(Pt 1):434?438.
M.Q. Stearns, C. Price, KA Spackman, and AY Wang.
2001. SNOMED clinical terms: overview of the de-
velopment process and project status. Proc AMIA
Symp, pages 662?6.
S. S. Wilks. 1938. The large-sample distribution of the
likelihood ratio for testing composite hypotheses. The
Annals of Mathematical Statistics, 9(1):60?62, March.
16
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 28?31,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
UMLS::Similarity: Measuring the Relatedness
and Similarity of Biomedical Concepts
Bridget T. McInnes? & Ying Liu
Minnesota Supercomputing Institute
University of Minnesota
Minneapolis, MN 55455
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Genevieve B. Melton
Institute for Health Informatics
University of Minnesota
Minneapolis, MN 55455
Serguei V. Pakhomov
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Abstract
UMLS::Similarity is freely available open
source software that allows a user to mea-
sure the semantic similarity or relatedness of
biomedical terms found in the Unified Medi-
cal Language System (UMLS). It is written in
Perl and can be used via a command line in-
terface, an API, or a Web interface.
1 Introduction
UMLS::Similarity1 implements a number of seman-
tic similarity and relatedness measures that are based
on the structure and content of the Unified Medical
Language System. The UMLS is a data warehouse
that provides a unified view of many medical termi-
nologies, ontologies and other lexical resources, and
is also freely available from the National Library of
Medicine.2
Measures of semantic similarity quantify the de-
gree to which two terms are similar based on their
proximity in an is-a hierarchy. These measures are
often based on the distance between the two con-
cepts and their common ancestor. For example, lung
disease and Goodpasture?s Syndrome share the con-
cept disease as a common ancestor. Or in general
English, scalpel and switchblade would be consid-
ered very similar since both are nearby descendents
of the concept knife.
However, concepts that are not technically similar
can still be very closely related. For example, Good-
pasture?s Syndrome and Doxycycline are not similar
?Contact author : bthomson@umn.edu.
1http://umls-similarity.sourceforge.net
2http://www.nlm.nih.gov/research/umls/
since they do not have a nearby common ancestor,
but they are very closely related since Doxycycline
is a possible treatment for Goodpasture?s Syndrome.
A more general example might be elbow and arm,
while they are not similar, an elbow is a part-of an
arm and is therefore very closely related. Measures
of relatedness quantify these types of relationships
by using information beyond that which is found
in an is-a hierarchy, which the UMLS contains in
abundance.
2 Related Work
Measures of semantic similarity and relatedness
have been used in a number of different biomedi-
cal and clinical applications. Early work relied on
the Gene Ontology (GO)3, which is a hierarchy of
terms used to describe genomic information. For
example, (Lord et al, 2003) measured the similar-
ity of gene sequence data and used this in an appli-
cation for conducting semantic searches of textual
resources. (Guo et al, 2006) used semantic simi-
larity measures to identify direct and indirect pro-
tein interactions within human regulatory pathways.
(Ne?ve?ol et al, 2006) used semantic similarity mea-
sures based on MeSH (Medical Subject Headings)4
to evaluate automatic indexing of biomedical arti-
cles by measuring the similarity between their rec-
ommended terms and the gold standard index terms.
UMLS::Similarity was first released in 2009, and
since that time has been used in various different
applications. (Sahay and Ram, 2010) used it in a
3http://www.geneontology.org/
4http://www.ncbi.nlm.nih.gov/mesh
28
health information search and recommendation sys-
tem. (Zhang et al, 2011) used the measures to
identify redundancy within clinical records, while
(Mathur and Dinakarpandian, 2011) used them to
help identify similar diseases. UMLS::Similarity
has also enabled the development and evaluation
of new measures by allowing them to be compared
to existing methods, e.g., (Pivovarov and Elhadad,
2012). Finally, UMLS::Similarity can serve as a
building block in other NLP systems, for exam-
ple UMLS::SenseRelate (McInnes et al, 2011) is a
word sense disambiguation system for medical text
based on semantic similarity and relatedness.
3 UMLS::Similarity
UMLS::Similarity is a descendent of Word-
Net::Similarity (Pedersen et al, 2004), which
implements various measures of similarity and
relatedness for WordNet.5 However, the structure,
nature, and size of the UMLS is quite different from
WordNet, and the adaptations from WordNet were
not always straightforward. One very significant
difference, for example, is that the UMLS is stored
in a MySQL database while WordNet has its own
customized storage format. As a result, the core
of UMLS::Similarity is different and offers a
great deal of functionality specific to the UMLS.
Table 1 lists the measures currently provided in
UMLS::Similarity (as of version 1.27).
The Web interface provides a subset of the func-
tionality offered by the API and command line inter-
face, and allows a user to utilize UMLS::Similarity
without requiring the installation of the UMLS
(which is an admittedly time?consuming process).
4 Unified Medical Language System
The UMLS is a data warehouse that includes over
100 different biomedical and clinical data resources.
One of the largest individual sources is the System-
atized Nomenclature of Medicine?Clinical Terms
(SNOMED CT), a comprehensive terminology cre-
ated for the electronic exchange of clinical health in-
formation. Perhaps the most fine?grained source is
the Foundational Model of Anatomy (FMA), an on-
tology created for biomedical and clinical research.
One of the most popular sources is MeSH (MSH), a
5http://wordnet.princeton.edu/
Table 1: UMLS::Similarity Measures
Type Citation Name
Similarity
(Rada et al, 1989) path
(Caviedes and Cimino, 2004) cdist
(Wu and Palmer, 1994) wup
(Leacock and Chodorow, 1998) lch
(Nguyen and Al-Mubaid, 2006) nam
(Zhong et al, 2002) zhong
(Resnik, 1995) res
(Lin, 1998) lin
(Jiang and Conrath, 1997) jcn
Relatedness
(Banerjee and Pedersen, 2003) lesk
(Patwardhan and Pedersen, 2006) vector
terminology that is used for indexing medical jour-
nal articles in PubMed.
These many different resources are semi-
automatically combined into the Metathesaurus,
which provides a unified view of nearly 3,000,000
different concepts. This is very important since the
same concept can exist in multiple different sources.
For example, the concept Autonomic nerve exists in
both SNOMED CT and FMA. The Metathesaurus
assigns synonymous concepts from multiple sources
a single Concept Unique Identifier (CUI). Thus
both Autonomic nerve concepts in SNOMED CT
and FMA are assigned the same CUI (C0206250).
These shared CUIs essentially merge multiple
sources into a single resource in the Metathesaurus.
Some sources in the Metathesaurus contain addi-
tional information about the concept such as syn-
onyms, definitions,6 and related concepts. Paren-
t/child (PAR/CHD) and broader/narrower (RB/RN)
are the main types of hierarchical relations between
concepts in the Metathesaurus. Parent/child rela-
tions are already defined in the sources before they
are integrated into the UMLS, whereas broader/-
narrower relations are added by the UMLS edi-
tors. For example, Splanchnic nerve has an is-a
relation with Autonomic nerve in FMA. This re-
lation is carried forward in the Metathesaurus by
creating a parent/child relation between the CUIs
C0037991 [Splanchnic nerve] and C0206250 [Au-
tonomic nerve].
6However, not all concepts in the UMLS have a definition.
29
Table 2: Similarity scores for finger and arm
Source Relations CUIs path cdist wup lch nam zhong res lin jcn
FMA PAR/CHD 82,071 0.14 0.14 0.69 1.84 0.15 0.06 0.82 0.34 0.35
SNOMED CT PAR/CHD 321,357 0.20 0.20 0.73 2.45 0.15 0.16 2.16 0.62 0.48
MSH PAR/CHD 26,685 0.25 0.25 0.76 2.30 0.18 0.19 2.03 0.68 0.55
5 Demonstration System
The UMLS::Similarity Web interface7 allows a user
to enter two terms or UMLS CUIs as input in term
boxes. The user can choose to calculate similarity or
relatedness by clicking on the Calculate Similarity
or Calculate Relatedness button. The user can also
choose which UMLS sources and relations should
be used in the calculation. For example, if the terms
finger and arm are entered and the Compute Simi-
larity button is pressed, the following is output:
View D e f i n i t i o n s
View S h o r t e s t Pa th
R e s u l t s :
The s i m i l a r i t y o f f i n g e r
( C0016129 ) and arm ( C0446516 )
u s i n g Pa th Length ( p a t h ) i s
0 . 2 5 .
Using :
SAB : : i n c l u d e MSH
REL : : i n c l u d e PAR/CHD
The Results show the terms and their assigned
CUIs. If a term has multiple possible CUIs associ-
ated with it, UMLS::Similarity returns the CUI pair
that obtained the highest similarity score. In this
case, finger was assigned CUI C0016129 and arm
assigned CUI C0449516 and the resulting similarity
score for the path measure using the MeSH hierar-
chy was 0.25.
Additionally, the paths between the concepts and
their definitions are shown. The View Definitions
and View Shortest Path buttons show the definition
and shortest path between the concepts in a sepa-
rate window. In the example above, the shortest path
between finger (C0016129) and arm (C0446516) is
C0016129 (Finger, NOS) => C0018563 (Hand,
NOS) => C1140618 (Extremity, Upper) =>
7http://atlas.ahc.umn.edu/
C0446516 (Upper arm), and one of the definitions
shown for arm (C0446516) is The superior part
of the upper extremity between the shoulder and
the elbow.
SAB :: include and REL :: include are config-
uration parameters that define the sources and rela-
tions used to find the paths between the two CUIs
when measuring similarity. In the example above,
similarity was calculated using PAR/CHD relations
in the MeSH hierarchy.
All similarity measures default to the use of
MeSH as the source (SAB) with PAR/CHD rela-
tions. While these are reasonable defaults, for many
use cases these should be changed. Table 2 shows
the similarity scores returned for each measure us-
ing different sources. It also shows the number of
CUIs connected via PAR/CHD relations per source.
A similar view is displayed when pressing the
Compute Relatedness button:
View D e f i n i t i o n s
View S h o r t e s t Pa th
R e s u l t s :
The r e l a t e d n e s s o f f i n g e r
( C0016129 ) and arm ( C0446516 )
u s i n g Vec to r Measure ( v e c t o r )
i s 0 . 5 5 1 3 .
Using :
SABDEF : : i n c l u d e
UMLS ALL
RELDEF : : i n c l u d e
CUI /PAR/CHD/RB/RN
Relatedness measures differ from similarity in
their use of the SABDEF and RELDEF parameters.
SABDEF :: include andRELDEF :: include define
the source(s) and relation(s) used to extract defini-
tions for the relatedness measures. In this example,
the definitions come from any source in the UMLS
and include not only the definition of the concept but
30
Table 3: Relatedness scores for finger and arm
Source Relations lesk vector
UMLS ALLCUI/PAR/CHD/RB/RN10,607 0.55
UMLS ALLCUI 39 0.05
also the definition of its PAR/CHD and RB/RN rela-
tions. Table 3 shows the relatedness scores returned
for each of the relatedness measures using just the
concept?s definition (CUI) from all of the sources in
the UMLS (UMLS ALL) and when the definitions
are extended to include the definitions of the con-
cept?s PAR/CHD and RB/RN relations.
6 Acknowledgments
This work was supported by the National Insti-
tute of Health, National Library of Medicine Grant
#R01LM009623-01. It was carried out in part using
computing resources at the University of Minnesota
Supercomputing Institute.
The results reported here are based on the
2012AA version of the UMLS and were computed
using version 1.23 of UMLS::Similarity and version
1.27 of UMLS::Interface.
References
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco,
August.
J.E. Caviedes and J.J. Cimino. 2004. Towards the devel-
opment of a conceptual distance metric for the umls.
Journal of Biomedical Informatics, 37(2):77?85.
X. Guo, R. Liu, C.D. Shriver, H. Hu, and M.N. Lieb-
man. 2006. Assessing semantic similarity measures
for the characterization of human regulatory pathways.
Bioinformatics, 22(8):967?973.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings on Intl Conf on Research in CL, pages pp. 19?33.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An electronic lexical database,
49(2):265?283.
D. Lin. 1998. An information-theoretic definition of
similarity. In Intl Conf ML Proc., pages 296?304.
PW Lord, RD Stevens, A. Brass, and CA Goble. 2003.
Semantic similarity measures as tools for exploring the
gene ontology. In Pacific Symposium on Biocomput-
ing, volume 8, pages 601?612.
S. Mathur and D. Dinakarpandian. 2011. Finding dis-
ease similarity based on implicit semantic similarity.
Journal of Biomedical Informatics, 45(2):363?371.
B.T. McInnes, T. Pedersen, Y. Liu, S. Pakhomov, and
G. Melton. 2011. Knowledge-based method for deter-
mining the meaning of ambiguous biomedical terms
using information content measures of similarity. In
Proceedings of the Annual Symposium of the Ameri-
canMedical Informatics Association, pages 895 ? 904,
Washington, DC.
A. Ne?ve?ol, K. Zeng, and O. Bodenreider. 2006. Besides
Precision & Recall: ExploringAlternative Approaches
to Evaluating an Automatic Indexing Tool for MED-
LINE. In AMIA Annu Symp Proc., page 589.
H.A. Nguyen and H. Al-Mubaid. 2006. New ontology-
based semantic similarity measure for the biomedical
domain. In Proc of the IEEE Intl Conf on Granular
Computing, pages 623?628.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proc of the EACL 2006 Work-
shop Making Sense of Sense, pages 1?8.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In The Annual Meeting of the HLT and
NAACL: Demonstration Papers, pages 38?41.
R. Pivovarov and N. Elhadad. 2012. A hybrid
knowledge-based and data-driven approach to iden-
tifying semantically similar concepts. Journal of
Biomedical Informatics, 45(3):471?481.
R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. IEEE Transactions on Systems, Man, and Cyber-
netics, 19(1):17?30.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
the 14th Intl Joint Conf on AI, pages 448?453.
S. Sahay and A. Ram. 2010. Socio-semantic health in-
formation access. In Proceedings of the AAAI Spring
Symposium on AI and Health Communication.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd Meeting of ACL,
pages 133?138, Las Cruces, NM, June.
R. Zhang, S. Pakhomov, B.T. McInnes, and G.B. Melton.
2011. Evaluating measures of redundancy in clinical
texts. In AMIA Annual Symposium Proceedings, vol-
ume 2011, page 1612.
J. Zhong, H. Zhu, J. Li, and Y. Yu. 2002. Concep-
tual graph matching for semantic search. Proceedings
of the 10th International Conference on Conceptual
Structures, pages 92?106.
31
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 884?889,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Computerized Analysis of a Verbal Fluency Test
James O. Ryan1, Serguei Pakhomov1, Susan Marino1,
Charles Bernick2, and Sarah Banks2
1 College of Pharmacy, University of Minnesota
2 Lou Ruvo Center for Brain Health, Cleveland Clinic
{ryanx765, pakh0002, marin007}@umn.edu
{bernicc, bankss2}@ccf.org
Abstract
We present a system for automated pho-
netic clustering analysis of cognitive tests
of phonemic verbal fluency, on which one
must name words starting with a specific
letter (e.g., ?F?) for one minute. Test re-
sponses are typically subjected to man-
ual phonetic clustering analysis that is
labor-intensive and subject to inter-rater
variability. Our system provides an au-
tomated alternative. In a pilot study,
we applied this system to tests of 55
novice and experienced professional fight-
ers (boxers and mixed martial artists) and
found that experienced fighters produced
significantly longer chains of phonetically
similar words, while no differences were
found in the total number of words pro-
duced. These findings are preliminary, but
strongly suggest that our system can be
used to detect subtle signs of brain damage
due to repetitive head trauma in individu-
als that are otherwise unimpaired.
1 Introduction
The neuropsychological test of phonemic verbal
fluency (PVF) consists of asking the patient to
generate as many words as he or she can in a lim-
ited time (usually 60 seconds) that begin with a
specific letter of the alphabet (Benton et al, 1989).
This test has been used extensively as part of larger
cognitive test batteries to study cognitive impair-
ment resulting from a number of neurological con-
ditions, including Parkinson?s and Huntington?s
diseases, various forms of dementia, and traumatic
brain injury (Troyer et al, 1998a,b; Raskin et al,
1992; Ho et al, 2002). Patients with these dis-
orders tend to generate significantly fewer words
on this test than do healthy individuals. Prior
studies have also found that clustering (the degree
to which patients generate groups of phonetically
similar words) and switching (transitioning from
one cluster to the next) behaviors are also sensi-
tive to the effects of these neurological conditions.
Contact sports such as boxing, mixed martial
arts, football, and hockey are well known for
high prevalence of repetitive head trauma. In re-
cent years, the long-term effects of repetitive head
trauma in athletes has become the subject of inten-
sive research. In general, repetitive head trauma
is a known risk factor for chronic traumatic en-
cephalopathy (CTE), a devastating and untreat-
able condition that ultimately results in permanent
disability and premature death (Omalu et al, 2010;
Gavett et al, 2011). However, little is currently
known about the relationship between the amount
of exposure to head injury and the magnitude of
risk for developing these conditions. Furthermore,
the development of new behavioral methods aimed
at detection of subtle early signs of brain impair-
ment is an active area of research.
The PVF test is an excellent target for this re-
search because it is very easy to administer and has
been shown to be sensitive to the effects of acute
traumatic brain injury (Raskin and Rearick, 1996).
However, a major obstacle to using this test widely
for early detection of brain impairment is that clus-
tering and switching analyses needed to detect
these subtle changes have to be done manually.
These manual approaches are extremely labor-
intensive, and are therefore limited in the types of
clustering analyses that can be performed. Manual
methods are also not scalable to large numbers of
tests and are subject to inter-rater variability, mak-
ing the results difficult to compare across subjects,
as well as across different studies. Moreover, tra-
ditional manual clustering and switching analyses
rely primarily on word orthography to determine
phonetic similarity (e.g., by comparing the first
two letters of two words), rather than phonetic rep-
resentations, which would be prohibitively time-
884
Figure 1: High-level system architecture and
workflow.
consuming to obtain by hand.
Phonetic similarity has been investigated in ap-
plication to a number of research areas, including
spelling correction (Toutanova and Moore, 2002),
machine translation (Knight and Graehl, 1998;
Kondrak et al, 2003), cross-lingual information
retrieval (Melamed, 1999; Fujii and Ishikawa,
2001), language acquisition (Somers, 1998), his-
torical linguistics (Raman et al, 1997), and social-
media informatics (Liu et al, 2012); we propose a
novel clinical application.
Our objective was to develop and pilot-test a
relatively simple, but robust, system for automatic
identification of word clusters, based on phonetic
content, that uses the CMU Pronouncing Dictio-
nary, a decision tree-based algorithm for gener-
ating pronunciations for out-of-dictionary words,
and two different approaches to calculating pho-
netic similarity between words.
We first describe the system architecture and
our phonetic-similarity computation methods, and
then present the results of a pilot study, using data
from professional fighters, demonstrating the util-
ity of this system for early detection of subtle signs
of brain impairment.
2 Automated Clustering Analysis
Figure 1 shows the high-level architecture and
workflow of our system.
2.1 Pronunciation Dictionary
We use a dictionary developed for speech recog-
nition and synthesis applications at the Carnegie
Mellon University (CMUdict). CMUdict contains
phonetic transcriptions, using a phone set based on
ARPABET (Rabiner and Juang, 1993), for North
American English word pronunciations (Weide,
1998). We used the latest version, cmudict.0.7a,
which contains 133,746 entries.
From the full set of entries in CMUdict,
we removed alternative pronunciations for each
word, leaving a single phonetic representation for
each heteronymous set. Additionally, all vowel
symbols were stripped of numeric stress mark-
ings (e.g., AH1 ? AH), and all multicharacter
phone symbols were converted to arbitrary single-
character symbols, in lowercase to distinguish
these symbols from the original single-character
ARPABET symbols (e.g., AH ? c). Finally,
whitespace between the symbols constituting each
phonetic representation was removed, yielding
compact phonetic-representation strings suitable
for computing our similarity measures.
To illustrate, the CMUdict pronunciation entry
for the word phonetic, [F AH0 N EH1 T IH0
K], would be represented as FcNiTmK.
2.2 Similarity Computation
Our system uses two methods for determining
phonetic similarity: edit distance and a common-
biphone check. Each of these methods gives a
measure of similarity for a pair of phonetic repre-
sentations, which we respectively call a phonetic-
similarity score (PSS) and a common-biphone
score (CBS).
For PSS, we first compute the Levenshtein
distance (Levenshtein, 1966) between compact
phonetic-representation strings and normalize that
to the length of the longer string; then, that value
is subtracted from 1. PSS values range from 0 to
1, with higher scores indicating greater similarity.
The CBS is binary, with a score of 1 given for two
phonetic representations that have a common ini-
tial and/or final biphone, and 0 for two strings that
have neither in common.
885
Figure 2: Phonetic chain and common-biphone
chain (below) for an example PVF response.
2.3 Phonetic Clustering
We distinguish between two ways of defining pho-
netic clusters. Traditionally, any sequence of n
words in a PVF response is deemed to form a clus-
ter if all pairwise word combinations for that se-
quence are determined to be phonetically similar
by some metric. In addition to this method, we
developed a less stringent approach in which we
define chains instead of clusters.
A chain comprises a sequence for which the
phonetic representation of each word is similar
to that of the word immediately prior to it in the
chain (unless it is chain-initial) and the word sub-
sequent to it (unless it is chain-final). Lone words
that do not belong to any cluster constitute sin-
gleton clusters. We call chains based on the edit-
distance method phonetic chains, and chains based
on the common-biphone method common-biphone
chains; both are illustrated in Figure 2.
Unlike the binary CBS method, the PSS
method produces continuous edit-distance values,
and therefore requires a threshold for categorizing
a word pair as similar or dissimilar. We determine
the threshold empirically for each letter by taking
a random sample of 1000 words starting with that
letter in CMUdict, computing PSS scores for each
pairwise combination (n = 499, 500), and then
setting the threshold as the value separating the
upper quintile of these scores. With the common-
biphone method, two words are considered pho-
netically similar simply if their CBS is 1.
2.4 System Overview
Our system is written in Python, and is available
online.1 The system accepts transcriptions of a
1http://rxinformatics.umn.edu/
downloads.html
PVF response for a specific letter and, as a pre-
processing step, removes any words that do not be-
gin with that letter. After pre-processing, all words
are phoneticized by dictionary lookup in our mod-
ified CMUdict. For out-of-dictionary words, we
automatically generate a phonetic representation
with a decision tree-based grapheme-to-phoneme
algorithm trained on the CMUdict (Pagel et al,
1998).
Next, PSSs and CBSs are computed sequen-
tially for each pair of contiguous phonetic rep-
resentations, and are used in their respective
methods to compute the following measures:
mean pairwise similarity score (MPSS), mean
chain length (MCL), and maximum chain length
(MXCL). Singletons are included in these calcula-
tions as chains of length 1.
We also calculate equivalent measures for clus-
ters, but do not present these results here due to
space limitations, as they are similar to those for
chains. In addition to these measures, our sys-
tem produces a count of the total number of words
that start with the letter specified for the PVF test
(WCNT), and a count of repeated words (RCNT).
3 Pilot Study
3.1 Participants
We used PVF tests from 55 boxers and mixed
martial artists (4 women, 51 men; mean age 27.7
y.o., SD 6.0) that participated in the Professional
Fighters Brain Health Study (PFBH). The PFBH
is a longitudinal study of unarmed active profes-
sional fighters, retired professional fighters, and
age/education matched controls (Bernick et al, in
press). It is designed to enroll over 400 partici-
pants over the next five years. The 55 participants
in our pilot represent a sample from the first wave
of assessments, conducted in summer of 2012. All
55 participants were fluent speakers of English and
were able to read at at least a 4th-grade level. None
of these participants fought in a professional or
amateur competition within 45 days prior to test-
ing.
3.2 Methods
Each participant?s professional fighting history
was used to determine his or her total number of
pro fights and number of fights per year. These
figures were used to construct a composite fight-
exposure index as a summary measure of cumula-
tive traumatic exposure, as follows.
886
0.0
0.1
0.2
0.3
Common Biphone Edit?Distance
mPSS
Fighter_GroupHigh ExposureLow Exposure
(a) Mean pairwise similarity score
0
1
2
3
Common Biphone Edit?Distance Manual
mCL
Fighter_GroupHigh ExposureLow Exposure
(b) Mean chain/cluster length
0
2
4
Common Biphone Edit?Distance Manual
mxCL
Fighter_GroupHigh ExposureLow Exposure
(c) Max chain/cluster length
Figure 3: Computation-method and exposure-group comparisons showing significant differences be-
tween the low- and high-exposure fighter groups on MPSS, MCL, and MXCL measures. Error bars
represent 95% confidence intervals around the means.
Fighters with zero professional fights were as-
signed a score of 0; fighters with between 1 and 15
total fights, but only one or fewer fights per year,
were assigned a score of 1; fighters with 1-15 to-
tal fights, and more than one fight per year, got a
score of 2; fighters with more than 15 total fights,
but only one or fewer fights per year, got a score
of 3; remaining fighters, with more than 15 fights
and more than one fight per year, were assigned
the highest score of 4.
Due to the relatively small sample size in our
pilot study, we combined groups with scores of
0 and 1 to constitute the low-exposure group
(n = 25), and the rest were assigned to the high-
exposure group (n = 30).
All participants underwent a cognitive test bat-
tery that included the PVF test (letter ?F?). Their
responses were processed by our system, and
means for our chaining variables of interest, as
well as counts of total words and repetitions,
were compared across the low- and high-exposure
groups. Additionally, all 55 PVF responses were
subjected to manual phonetic clustering analysis,
following the methodology of Troyer et al (1997).
With this approach, clusters are used instead of
chains, and two words are considered phonetically
similar if they meet any of the following condi-
tions: they begin with the same two orthographic
letters; they rhyme; they differ by only a vowel
sound (e.g., flip and flop); or they are homophones.
For each clustering method, the differences in
means between the groups were tested for sta-
tistical significance using one-way ANOVA ad-
justed for the effects of age and years of education.
Spearman correlation was used to test for associ-
ations between continuous variables, due to non-
linearity, and to directly compare manually deter-
mined clustering measures with corresponding au-
tomatically determined chain measures.
4 Results
The results of comparisons between the clustering
methods, as well as between the low- and high-
exposure groups, are illustrated in Figure 3.2
We found a significant difference (p < 0.02)
in MPSS between the high- and low-exposure
groups using the common-biphone method (0.15
vs. 0.11), while with edit distance the difference
was small (0.29 vs. 0.28) and not significant (Fig-
ure 3a). Due to infeasibility, MPSS was not calcu-
lated manually.
Mean chain sizes determined by the common-
biphone method correlated with manually deter-
mined cluster sizes more strongly than did chain
sizes determined by edit distance (? = 0.73, p <
0.01 vs. ? = 0.48, p < 0.01). Comparisons of
maximum chain and cluster sizes showed a sim-
ilar pattern (? = 0.71, p < 0.01 vs. ? = 0.39,
p < 0.01).
Both automatic methods showed significant dif-
ferences (p < 0.01) between the two groups in
MCL and MXCL, with each finding longer chains
in the high-exposure group (Figure 3b, 3c); how-
ever, slightly larger differences were observed us-
ing the common-biphone method (MCL: 2.79 vs.
2.21 by common-biphone method, 3.23 vs. 2.80
by edit-distance method; MXCL: 3.94 vs. 2.64 by
2Clustering measures rely on chains for our automatic
methods, and on clusters for manual analysis.
887
common biphone, 4.94 vs. 3.76 by edit distance).
Group differences for manually determined MCL
and MXCL were also significant (p < 0.05 and
p < 0.02, respectively), but less so (MCL: 1.71
vs. 1.46; MXCL: 4.0 vs. 3.04).
5 Discussion
While manual phonetic clustering analysis yielded
significant differences between the low- and high-
exposure fighter groups, our automatic approach,
which utilizes phonetic word representations, ap-
pears to be more sensitive to these differences; it
also appears to produce less variability on cluster-
ing measures. Furthermore, as discussed above,
automatic analysis is much less labor-intensive,
and thus is more scalable to large numbers of tests.
Moreover, our system is not prone to human error
during analysis, nor to inter-rater variability.
Of the two automatic clustering methods, the
common-biphone method, which uses binary sim-
ilarity values, found greater differences between
groups in MPSS, MCL, and MXCL; thus, it ap-
pears to be more sensitive than the edit-distance
method in detecting group differences. Common-
biphone measures were also found to better cor-
relate with manual measures; however, both au-
tomated methods disagreed with the manual ap-
proach to some extent. The fact that the auto-
mated common-biphone method shows significant
differences between group means, while having
less variability in measurements, suggests that it
may be a more suitable measure of phonetic clus-
tering than the traditional manual method.
These results are particularly important in light
of the difference in WCNT means between low-
and high-exposure groups being small and not sig-
nificant (WCNT: 17.6, SD 5.1 vs. 18.7, SD 4.7;
p = 0.24). Other studies that used manual cluster-
ing and switching analyses reported significantly
more switches for healthy controls than for indi-
viduals with neurological conditions (Troyer et al,
1997). These studies also reported differences in
the total number of words produced, likely due to
investigating already impaired individuals.
Our findings show that the low- and high-
exposure groups produced similar numbers of
words, but the high-exposure group tended to
produce longer sequences of phonetically simi-
lar words. The latter phenomenon may be inter-
preted as a mild form of perseverative (stuck-in-
set/repetitive) behavior that is characteristic of dis-
orders involving damage to frontal and subcortical
brain structures.
To test this interpretation, we correlated MCL
and MXCL, the two measures with greatest dif-
ferences between low- and high-exposure fighters,
with the count of repeated words (RCNT). The
resulting correlations were 0.41 (p = 0.01) and
0.48 (p < 0.001), respectively, which supports the
perseverative-behavior interpretation of our find-
ings.
Clearly, these findings are preliminary and need
to be confirmed in larger samples; however, they
plainly demonstrate the utility of our fully auto-
mated and quantifiable approach to characteriz-
ing and measuring clustering behavior on PVF
tests. Pending further clinical validation, this sys-
tem may be used for large-scale screening for sub-
tle signs of certain types of brain damage or de-
generation not only in contact-sports athletes, but
also in the general population.
6 Acknowledgements
We thank the anonymous reviewers for their in-
sightful feedback.
References
Atsushi Fujii and Tetsuya Ishikawa. 2001.
Japanese/English cross-language information
retrieval: Exploration of query translation and
transliteration. In Computers and the Humanities
35.4.
A.L. Benton, K.D. Hamsher, and A.B. Sivan. 1989.
Multilingual aphasia examination.
C. Bernick, S.J. Banks, S. Jones, W. Shin, M. Phillips,
M. Lowe, M. Modic. In press. Professional Fight-
ers Brain Health Study: Rationale and methods. In
American Journal of Epidemiology.
Brandon E. Gavett, Robert A. Stern, and Ann C. Mc-
Kee. 2011. Chronic traumatic encephalopathy: A
potential late effect of sport-related concussive and
subconcussive head trauma. In Clinics in Sports
Medicine 30, no. 1.
Aileen K. Ho, Barbara J. Sahakian, Trevor W. Rob-
bins, Roger A. Barker, Anne E. Rosser, and John R.
Hodges. 2002. Verbal fluency in Huntington?s dis-
ease: A longitudinal analysis of phonemic and se-
mantic clustering and switching. In Neuropsycholo-
gia 40, no. 8.
Vladimir I. Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet Physics Doklady, vol. 10.
888
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1. Association for Computa-
tional Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. In Computational Linguistics 24.4.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology: Companion Volume of the Proceed-
ings of HLT-NAACL 2003. Association for Compu-
tational Linguistics.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. In Computational Linguis-
tics 25.1.
Bennet I. Omalu, Julian Bailes, Jennifer Lynn Ham-
mers, and Robert P. Fitzsimmons. 2010. Chronic
traumatic encephalopathy, suicides and parasuicides
in professional American athletes: The role of the
forensic pathologist. In The American Journal of
Forensic Medicine and Pathology 31, no. 2.
Vincent Pagel, Kevin Lenzo, and Alan Black. 1998.
Letter to sound rules for accented lexicon compres-
sion.
Lawrence Rabiner and Biing-Hwang Juang. 1993. Fun-
damentals of speech recognition.
Anand Raman, John Newman, and Jon Patrick.
1997. A complexity measure for diachronic Chi-
nese phonology. In Proceedings of the SIGPHON97
Workshop on Computational Linguistics at the
ACL97/EACL97.
Sarah A. Raskin, Martin Sliwinski, and Joan C. Borod.
1992. Clustering strategies on tasks of verbal fluency
in Parkinson?s disease. In Neuropsychologia 30, no.
1.
Sarah A. Raskin and Elizabeth Rearick. 1996. Verbal
fluency in individuals with mild traumatic brain in-
jury. In Neuropsychology 10, no. 3.
Harold L. Somers. 1998. Similarity metrics for align-
ing children?s articulation data. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics-Volume 2. Asso-
ciation for Computational Linguistics.
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correc-
tion. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics.
Angela K. Troyer, Morris Moscovitch, and Gor-
don Winocur. 1997. Clustering and switching as
two components of verbal fluency: Evidence from
younger and older healthy adults. In Neuropsychol-
ogy, 11.
Angela K. Troyer, Morris Moscovitch, Gordon
Winocur, Michael P. Alexander, and Don Stuss.
1998a. Clustering and switching on verbal fluency:
The effects of focal frontal- and temporal-lobe le-
sions. In Neuropsychologia.
Angela K. Troyer, Morris Moscovitch, Gordon
Winocur, Larry Leach, and Morris Freedman.
1998b. Clustering and switching on verbal fluency
tests in Alzheimer?s and Parkinson?s disease. In
Journal of the International Neuropsychological So-
ciety 4, no. 2.
Robert Weide. 2008. Carnegie Mel-
lon Pronouncing Dictionary, v. 0.7a.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
889
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 46?52,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automated Identification of Synonyms in Biomedical  
Acronym Sense Inventories  
?
Genevieve B. Melton SungRim Moon 
Institute for Health Informatics & Dept of Surgery Institute for Health Informatics 
University of Minnesota University of Minnesota 
Minneapolis, MN 55455 USA Minneapolis, MN 55455 USA 
gmelton@umn.edu moonx086@umn.edu 
  
Bridget McInnes  Serguei Pakhomov 
College of Pharmacy College of Pharmacy 
University of Minnesota University of Minnesota 
Minneapolis, MN 55455 USA Minneapolis, MN 55455 USA 
bthomson@umn.edu  pakh0002@umn.edu 
?
  
Abstract 
Acronyms are increasingly prevalent in bio-
medical text, and the task of acronym disam-
biguation is fundamentally important for 
biomedical natural language processing sys-
tems. Several groups have generated sense in-
ventories of acronym long form expansions 
from the biomedical literature. Long form 
sense inventories, however, may contain con-
ceptually redundant expansions that negative-
ly affect their quality. Our approach to 
improving sense inventories consists of map-
ping long form expansions to concepts in the 
Unified Medical Language System (UMLS) 
with subsequent application of a semantic si-
milarity algorithm based upon conceptual 
overlap. We evaluated this approach on a ref-
erence standard developed for ten acronyms. 
A total of 119 of 155 (78%) long forms 
mapped to concepts in the UMLS. Our ap-
proach identified synonymous long forms 
with a sensitivity of 70.2% and a positive pre-
dictive value of 96.3%. Although further re-
finements are needed, this study demonstrates 
the potential value of using automated tech-
niques to merge synonymous biomedical 
acronym long forms to improve the quality of 
biomedical acronym sense inventories. 
1 Introduction 
Acronyms and abbreviations are increasingly used 
in biomedical text. This is in large part due to the 
expansive growth of the biomedical literature esti-
mated to be close to one million articles annually 
(Stead et al 2005). Ambiguous acronyms represent 
a challenge to both human readers and compute-
rized processing systems for resolving the 
acronym?s meaning within a particular context. For 
any given acronym, there are often multiple possi-
ble long form expansions. Techniques to determine 
the context-specific meaning or sense of an ambi-
guous acronym are fundamentally important for 
biomedical natural language processing and can 
assist with important tasks such as information re-
trieval and information extraction (Friedman 
2000). 
Acronym ambiguity resolution represents a spe-
cial case of word sense disambiguation (WSD) 
with unique challenges. In particular, there are in-
creasing numbers of new acronyms (i.e., short 
forms) as well as increasing numbers of new 
senses (i.e., long forms) for existing acronyms 
within biomedical text. Acronyms in biomedicine 
also range from those that are common, to those 
that are infrequent which appear to be created in an 
ad hoc fashion resulting essentially in neologisms 
distinct to small sets of biomedical discourse.  
Sense inventories are important tools that can 
assist in the task of disambiguation of acronyms 
and abbreviations. The relative formal nature of 
biomedical literature discourse lends itself well to 
building these inventories because long forms are 
typically contained within the text itself, providing 
a ?definition? on its first mention in an article, next 
to a parenthetical expression containing the short 
form or vice versa (Schwartz and Hearst 2003). In 
contrast, clinical documents are less structured and 
46
typically lack expanded long forms for acronyms 
and abbreviations, leaving sense inventories based 
on documents in the clinical domain not as well 
developed as the sense inventories developed from 
the biomedical literature (Pakhomov et al 2005).  
Compilation of sense inventories for acronyms 
in clinical documents typically relies on vocabula-
ries contained in the Unified Medical Language 
System (UMLS) as well as other resources such as 
ADAM (Zhou et al 2006). However, with the ad-
vantage of using rich and diverse resources like 
ADAM and the UMLS comes the challenge of 
having to identify and merge synonymous long 
form expansions which can occur for a given short 
form. Having synonymous long forms in a sense 
inventory for a given acronym poses a problem for 
automated acronym disambiguation because the 
sense inventory dictates that the disambiguation 
algorithm must be able to distinguish between se-
mantically equivalent senses. This is an important 
problem to address because effective identification 
of synonymous long forms allows for a clean sense 
inventory, and it creates the ability for long form 
expansions to be combined while preserving the 
variety of expression occurring in natural lan-
guage. By automating the merging of synonymous 
expansions and building a high quality sense in-
ventory, the task of acronym disambiguation will 
be improved resulting in better biomedical NLP 
system performance.  
Our approach to reducing multiple synonymous 
variants of the same long form for a set of ten bio-
medical acronyms is based on mapping sense in-
ventories for biomedical acronyms to the UMLS 
and using a semantic similarity algorithm based on 
conceptual overlap. This study is an exploratory 
evaluation of this approach on a manually created 
reference standard.  
2 Background  
2.1 Similarity measures in biomedicine 
The area of semantic similarity in biomedicine 
is a major area within biomedical NLP and know-
ledge representation research. Semantic similarity 
aids NLP systems, improves the performance of 
information retrieval tasks, and helps to reveal im-
portant latent relationships between biomedical 
concepts. Several investigators have studied con-
ceptual similarity and have used relationships in 
controlled biomedical terminologies, empiric sta-
tistical data from biomedical text, and other know-
ledge sources (Lee et al 2008; Caviedes and 
Cimino 2004). However, most of these techniques 
focus on generating measures between a single pair 
of concepts and do not deal directly with the task 
of comparing two groups of concepts.  
Patient similarity represents an important ana-
logous problem that deals with sets of concepts. 
The approach used by Melton et al (2006) was to 
represent each patient case as a set of nodes within 
a controlled biomedical terminology (SNOMED 
CT). The investigators then applied several meas-
ures to ascertain similarity between patient cases. 
These measures ranged from techniques indepen-
dent of the controlled terminology (i.e. set overlap 
or Hamming distance) to methods heavily reliant 
upon the controlled terminology based upon path 
traversal between pair of nodes using defined rela-
tionships (either IS-A relationships or other seman-
tic relationships) within the terminology.  
2.2 Lesk algorithm for measuring similarity 
using sets of definitional words 
A variety of techniques have been used for the 
general problem of WSD that range from highly 
labor intensive that depend upon human data tag-
ging (i.e. supervised learning) to unsupervised ap-
proaches that are completely automated and rely 
upon non-human sources of information, such as 
context and other semantic features of the sur-
rounding text or definitional data.  
The Lesk algorithm (Lesk 1986) is one example 
of an unsupervised method that uses dictionary 
information to perform WSD. This algorithm uses 
the observation that words co-occurring in a sen-
tence refer to the same topic and that dictionary 
definition words will have topically related senses, 
as well. The classic form of this algorithm returns a 
measure of word overlap. Lesk depends upon find-
ing common words between dictionary definitions. 
One shortcoming of Lesk, however, it that it can 
perform worse for words with terse, few word de-
finitions.  
As a modification of Lesk, researchers have 
proposed using WordNet (Felbaum 1998) to en-
hance its performance. WordNet has additional 
semantic information that can aid in the task of 
disambiguation, such as relationships between the 
term of interest and other terms. Banerjee and Pe-
47
dersen (2002) demonstrated that modifications to 
Lesk improved performance significantly with the 
addition of semantic relationship information. 
2.3 Biomedical literature sense inventories 
A number of acronym and abbreviation sense in-
ventories have been developed from the biomedi-
cal literature using a variety of approaches. Chang 
et al (2002) developed the Stanford biomedical 
abbreviation server1 using titles and abstracts from 
MEDLINE, lexical heuristic rules, and supervised 
logistic regression to align text and extract short 
form/long form pairs that matched well with 
acronym short form letters. Similarly, Adar (2004) 
developed the Simple and Robust Abbreviation 
Dictionary (SaRAD)2. This inventory, in addition 
to providing the abbreviation and definition, also 
clusters long forms using an N-gram approach 
along with classification rules to disambiguate de-
finitions. This resource, while analogous with re-
spect to its goal of merging and aligning long form 
expansions, is not freely available. Adar measured 
a normalized similarity between N-gram sets and 
then clustered long forms to create a clustered 
sense inventory resource. 
One of the most comprehensive biomedical 
acronym and abbreviation databases is ADAM 
(Zhou et al 2006) an open source database3 that 
we used for this study. Once identified, short 
form/long form pairs were filtered statistically with 
a rule of length ratio and an empirically-based cut-
off value.  This sense inventory is based on  
MEDLINE titles and abstracts from 2006 and con-
sists of over 59 thousand abbreviation/long form 
pairs. The authors report high precision with 
ADAM (97%) and up to 33% novel abbreviations 
not contained within the UMLS or Stanford Ab-
breviation dictionary.  
2.4 MetaMap resource for automated map-
ping to the UMLS 
An important resource for mapping words and 
phrases to the UMLS Metathesaurus is MetaMap. 
This resource was developed at the National Li-
brary of Medicine (Aronson 2001) to map text of 
biomedical abstracts to the UMLS. MetaMap uses 
                                                          
1 http://abbreviation.stanford.edu 
2 http://www.hpl.hp.com/shl/projects/abbrev.html 
3 http://arrowsmith.psych.uic.edu 
a knowledge intensive approach that relies upon 
computational linguistic, statistical, and symbol-
ic/lexical techniques. While MetaMap was initially 
developed to help with indexing of biomedical lite-
rature, it has been applied and expanded success-
fully to a number of diverse applications including 
clinical text.  
With each mapping, an evaluation function 
based upon centrality, variation, coverage, and co-
hesiveness generates a score for a given mapping 
from 0 to 1000 (strongest match). A cut-off score 
of 900 or greater is considered to represent a good 
conceptual match for MetaMap and was used in 
this study as the threshold to select valid mappings. 
3  Methods  
Ten randomly selected acronyms with between 10 
to 20 long forms were selected from the ADAM 
resource database for this pilot study.  
3.1 Long form mappings to UMLS 
Each acronym long-form was mapped to the 
UMLS with MetaMap using two settings. First, 
MetaMap was run with its default setting on each 
long form expansion. Second, MetaMap was run in 
its ?browse mode? (options ?-zogm?) which allows 
for term processing, overmatches, concept gaps, 
and ignores word order. 
Processing each long form with MetaMap then 
resulted in a set of Concept Unique Identifiers 
(CUIs) representing the long form. Each CUI with 
a score over 900 was included in the overall set of 
CUIs for a particular long form expansion. For a 
given pair of long form expansions the two sets of 
CUIs that each long form mapped to were com-
pared for concept overlap, in an analogous fashion 
to the Lesk algorithm. The overlap between con-
cept sets was calculated between each pair of long 
form expansions and expressed as a ratio: 
 
 ????????????? ????????? ???????????????? ?????????? ????????????? .   
 
For this study, an overlap of 50% or greater was 
considered to indicate a potential synonymous pair. 
Now let us assume that we have two concept 
sets: The first one is {A, B} and the second one is 
{A, B, C}, with each CUI having a score over 900. 
In this example, the overlap of concepts for the 
first concept set between it and the other is 100%, 
and for the second that is 66.7%. Because overlaps 
48
are greater than 50%, they are a potential syn-
onymous pair, and the overlap ratio is calculated as 
?????
????? ?
?
? = 1 (100%). 
3.2 Expert-derived reference standard 
Two physicians were asked to judge the similarity 
between each pair combination of long forms ex-
pansions on a continuous scale for our initial refer-
ence standard. Physicians were instructed to rate 
pairs of long forms for conceptual similarity. Long 
forms were presented on a large LCD touch-screen 
display (Hewlett-Packard TouchSmart 22? desk-
top) along with a continuous scale for the physi-
cians to rate long form pairs as dissimilar (far left 
screen) or highly similar (far right screen). The 
rating was measured on a scale from 1 to 1500 pix-
els representing the maximum width of the touch 
sensitive area of the display (along the x-
coordinate). Inter-rater agreement was assessed 
using Pearson correlation.  
Expert scores were then averaged and plotted 
on a histogram to visualize expert ratings. We sub-
sequently used a univariate clustering approach 
based on the R implementation of the Partitioning 
Around Medoids (PAM) method to estimate a cut-
off point between similar and dissimilar terms 
based on the vector of the average responses by the 
two physicians. The responses were clustered into 
two and three clusters based on an informal obser-
vation of the distribution of responses on the histo-
gram showing evidence of at least a bimodal and 
possibly a trimodal distribution.  
As a quality measure, a third physician manual-
ly reviewed the mean similarity ratings of the first 
two physicians to assess whether their similarity 
judgments represented the degree of synonymy 
between long form expansions necessary to war-
rant merging the long form expansions. This re-
view was done using a binary scale (0=not 
synonymous, 1=synonymous). 
3.3 Evaluation of automated methods  
Long form pair determinations based on the map-
pings to the UMLS were compared to our refer-
ence standard as described in Section 3.2. We 
calculated overall results of all long form pair 
comparisons and on all long form pairs that 
mapped to the UMLS with MetaMap. Performance 
is reported as sensitivity, specificity, and positive 
predictive value.  
4 Results 
A total of 10 random acronyms were used in this 
study. All long forms for these 10 acronyms were 
from the sense inventory ADAM (Zhou et al, 
2006). This resulted in a total of 155 long form 
expansions (median 16.5 per acronym, range 11-
19) (Table 1).  
Acronym N of LF  
expansions 
LF expansions 
mapped by MetaMap 
Total 155 119 (78%) 
ALT 13 9 (70%) 
CK 14 9 (64%) 
CSF 11 7 (74%) 
CTA 19 14 (74%) 
MN 19 17 (89%) 
NG 17 15 (88%) 
PCR 17 8 (47%) 
PET 17 15 (88%) 
RV 16 14 (88%) 
TTP 12 11(92%) 
Table 1. Number of acronym long forms in 
ADAM and mapping to the UMLS 
4.1 Long form mappings to UMLS 
The default mode of MetaMap resulted in 119 
(78%) long forms with mappings to the UMLS 
with MetaMap (Table 1). Use of MetaMap?s 
browse mode did not increase the total number of 
mapped long forms but did change some of the 
mapped concepts returned by MetaMap (not de-
picted).  
 
Acronym N pairs Pearson r 
Total 1125 0.78* 
ALT 78 0.79* 
CK 91 0.77* 
CSF 55 0.80* 
CTA 136 0.92* 
MN 171 0.69* 
NG 136 0.68* 
PCR 136 0.89* 
PET 136 0.78* 
RV 120 0.67* 
TTP 66 0.76* 
Table 2. Pearson correlation coefficient for ratings over-
all and for individual acronyms. *p<0.0001 
 
49
 
Figure 1. Two-way and three-way clustering solution of 
expert ratings of long form pairs. 
4.2 Expert-derived reference standard  
For the 1125 total comparison pairs, two raters as-
sessed similarity between long form pairs on a con-
tinuous scale. The overall mean correlation 
between the two raters was 0.78 (standard devia-
tion 0.08). Pearson correlation coefficients for each 
acronym are depicted in Table 2. 
 
Two-way and three-way clustering demonstrat-
ed an empirically determined ?cutoff? of 525 pix-
els from the left of the screen. This separation 
point between clusters (designated as ?low cutoff?) 
was evident on both the two-way and three-way 
clustering approaches using the PAM method to 
estimate a cut-off point between similar and dissi-
milar terms based on the vector of the average res-
ponses by the two physicians (Figure 1). Intuitively 
this low cutoff includes manual ratings indicative 
of moderate to low similarity (as 525 pixels along 
a 1500 pixel-wide scale is approximately one-third 
of the way from the left ?dissimilar? edge of the 
touch-sensitive screen). To isolate terms that were 
rated as highly similar, we also created an arbitrary 
?high cutoff? of 1200 pixels. 
 
 
Figure 2. Examples of terms originally rated as highly 
similar but not synonymous by the curating physician. 
 
Expert curation of the ratings by the third phy-
sician demonstrated that conceptual similarity rat-
ings were sometimes not equivalent to synonymy 
that would warrant the collapse of long form pairs. 
Of 1125 total pairs of long forms, 70 (6%) origi-
CTA: 
   ?CT hepatic arteriography?     ?CT angiography? 
MN:    
   ?median nerve?         ?motor neuron?  
RV:    
    ?rabies virus?           ?rotavirus? 
    ?right ventricular free wall?    ?right ventricle?                 
TTP:  
    ?thiamine triphosphate?          ?thymidine triphosphate? 
Default Mode: MetaMap Browse Mode: MetaMap 
All LF Mapped LF only All LF Mapped LF only 
High Cutoff 
Sensitivity  21.6% 39.6% 23.8% 43.8% 
Specificity  98.1% 96.8% 99.4% 99.0% 
PPV  48.7% 48.7% 77.8% 77.8% 
NPV  93.6% 95.5% 93.9% 95.9% 
Expert Curation 
Sensitivity  34.3% 64.9% 37.1% 70.2% 
Specificity  98.6% 97.7% 99.9% 99.8% 
PPV 61.5% 61.5% 96.3% 96.3% 
NPV  95.8% 98.0% 96.0% 98.3% 
 
Table 3. Performance of automated techniques for merging biomedical long form senses  
for all long forms and for long forms that mapped to the UMLS only.  
PPV, positive predictive value; NPV, negative predictive value. 
50
nally classified as similar were re-classified as 
conceptually different by the third physician. Sev-
eral examples of long form pairs that were origi-
nally rated as highly similar but were judged as not 
synonymous are contained in Figure 2. 
4.3 Evaluation of automated methods 
The performance of our algorithm is shown in Ta-
ble 3 using MetaMap in the default mode and 
browse mode and then applying our reference 
standard using the ?low cutoff?, ?high cutoff?, and 
expert curation (Table 3). Performance is reported 
for all 155 long forms (All LF) and for the subset 
of 119 long forms that mapped to the UMLS 
(Mapped LF only).  Compared to the ?low cutoff? 
reference standard, the ?high cutoff? and expert 
curation were positively associated with more con-
sistent performance. The browse mode identified 
fewer potential terms to merge and had higher ac-
curacy than the default MetaMap mode.   
5 Conclusions  
The results of this pilot study are promising and 
demonstrate high positive predictive value and 
moderate sensitivity for our algorithm, which indi-
cates to us that this technique with some additional 
modifications has value. We found that mapping 
long form expansions to a controlled terminology 
to not be straightforward. Although approximately 
80% of long forms mapped, another 20% were not 
converted to UMLS concepts. Because each long 
form resulted in multiple paired comparisons, a 
20% loss of mappings resulted globally in a 40% 
loss in overall system performance. While long 
form expansions were entered into MetaMap using 
a partially normalized representation of the long 
form, it is possible that additional normalization 
will improve our mapping. 
An important observation from our expert-
derived reference standard was that terms judged 
by physicians as semantically highly similar may 
not necessarily be synonymous (Figure 2). While 
semantic similarity is analogous, there may be 
some fundamentally different cognitive determina-
tions between similarity and synonymy for human 
raters.  
The current technique that we present compares 
sets of mapped concepts in an analogous fashion to 
the Lesk algorithm and other measures of similari-
ty between groups of concepts previously reported. 
This study did not utilize features of the controlled 
terminology nor statistical information about the 
text to help improve performance. Despite the lack 
of additional refinement to the presented tech-
niques, we found a flat overlap measure to be 
moderately effective in our evaluation. 
6 Future Work 
There are several lines of investigation that we will 
pursue as an extension of this study. The most ob-
vious would be to use semantic similarity measures 
between pairs of concepts that capitalize upon fea-
tures and relationships in the controlled terminolo-
gy. We can also expand upon the type of similarity 
measures for the overall long form comparison 
which requires a measure of similarity between 
groups of concepts. In addition, an empiric weight-
ing scheme based on statistical information of 
common senses may be helpful for concept map-
pings to place more or less emphasis on important 
or less important concepts. We plan to determine 
the impact of automatically reduced sense invento-
ries on the evaluation of WSD algorithms used for 
medical acronym disambiguation.   
Finally, we would like to utilize this work to 
help improve the contents of a sense inventory that 
we are currently developing for acronyms and ab-
breviations. This sense inventory is primarily 
based on clinical documents but incorporates in-
formation from a number of diverse sources in-
cluding ADAM, the UMLS, and a standard 
medical dictionary with abbreviations and acro-
nyms.  
Acknowledgments 
This work was supported by the University of 
Minnesota Institute for Health Informatics and De-
partment of Surgery and by the National Library of 
Medicine (#R01 LM009623-01). We would like to 
thank Fairview Health Services for ongoing sup-
port of this research. 
References  
Eytan Adar (2004) SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics 20:527?33. 
Alan R Aronson (2001) Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: the MetaMap 
program. Proc AMIA Symp. 2001:17-21. 
51
Satanjeev Banerjee, Ted Pedersen. 2002. An Adapted 
Lesk Algorithm for Word Sense Disambiguation Us-
ing WordNet, Proceedings of the Third International 
Conference on Computational Linguistics and Intel-
ligent Text Processing, p.136-145, February 17-23. 
Jorge E. Caviedes JE, James J Cimino. (2004) Towards 
the development of a conceptual distance metric for 
the UMLS. J Biomed Inform. Apr;37(2):77?85. 
Jeffrey T Chang, Hinrich Schutze, Russ B Altman 
(2001) Creating an online dictionary of abbreviations 
from Medline. J Am Med Inform Assoc 9:612?20. 
Christiane Fellbaum, editor. 1998. WordNet: An elec-
tronic lexical database. MIT Press. 
Carol Friedman. 2000. A broad-coverage natural lan-
guage processing system. Proc AMIA Symp., 270?
274. 
Wei-Nchih Lee, Nigam Shah, Karanjot Sundlass, Mark 
Musen (2008) Comparison of Ontology-based Se-
mantic-Similarity Measures. AMIA Annu Symp 
Proc. 2008. 384?388. 
Michael E. Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: How to tell a 
pine cone from a ice cream cone. In Proceedings of 
SIGDOC ?86. 
Genevieve B. Melton, Simon Parsons, Frances P. Mor-
rison, Adam S. Rothschild, Marianthi Markatou, 
George Hripcsak. 2006. Inter-patient distance metrics 
using SNOMED CT defining relationships, Journal 
of Biomedical Informatics, 39(6), 697-705.  
Serguei Pakhomov, Ted Pedersen, Christopher G. 
Chute. 2005. Abbreviation and Acronym Disambigu-
ation in Clinical Discourse. American Medical In-
formatics Association Annual Symposium, 589-593. 
Ariel S Schwartz and Marti A. Hearst. 2003. A Simple 
Algorithm for Identifying Abbreviation Definitions 
in Biomedical Text. Pacific Symposium on Biocom-
puting p451-462. 
William W Stead, Brian J Kelly, Robert M Kolodner. 
2005. Achievable steps toward building a National 
Health Information infrastructure in the United 
States. J. Am. Med. Inform. Assoc., 12, 113?120. 
Wei Zhou, Vetle I Torvik, Neil R Smalheiser (2006) 
ADAM: Another database of abbreviations in Med-
line. Bioinformatics 22:2813? 8. 
52
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 145?153,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using Second-order Vectors in a
Knowledge-based Method for Acronym Disambiguation
Bridget T. McInnes?
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Ying Liu
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Serguei V. Pakhomov
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Genevieve B. Melton
Institute for Health Informatics
University of Minnesota
Minneapolis, MN 55455
Abstract
In this paper, we introduce a knowledge-based
method to disambiguate biomedical acronyms
using second-order co-occurrence vectors. We
create these vectors using information about a
long-form obtained from the Unified Medical
Language System and Medline. We evaluate
this method on a dataset of 18 acronyms found
in biomedical text. Our method achieves an
overall accuracy of 89%. The results show
that using second-order features provide a dis-
tinct representation of the long-form and po-
tentially enhances automated disambiguation.
1 Introduction
Word Sense Disambiguation (WSD) is the task
of automatically identifying the appropriate sense of
a word with multiple senses. For example, the word
culture could refer to anthropological culture
(e.g., the culture of the Mayan civilization), or a
laboratory culture (e.g., cell culture).
Acronym disambiguation is the task of automat-
ically identifying the contextually appropriate long-
form of an ambiguous acronym. For example, the
acronym MS could refer to the disease Multiple Scle-
rosis, the drug Morphine Sulfate, or the state Missis-
sippi, among others. Acronym disambiguation can
be viewed as a special case of WSD, although, un-
like terms, acronyms tend to be complete phrases
or expressions, therefore collocation features are
not as easily identified. For example, the feature
rate when disambiguating the term interest, as in
?Contact author : bthomson@umn.edu.
interest rate, may not be available. Acronyms also
tend to be noun phrases, therefore syntactic features
do not provide relevant information for the purposes
of disambiguation.
Identifying the correct long-form of an acronym
is important not only for the retrieval of information
but the understanding of the information by the re-
cipient. In general English, Park and Byrd (2001)
note that acronym disambiguation is not widely
studied because acronyms are not as prevalent in lit-
erature and newspaper articles as they are in specific
domains such as government, law, and biomedicine.
In the biomedical sublanguage domain, acronym
disambiguation is an extensively studied problem.
Pakhomov (2002) note acronyms in biomedical lit-
erature tend to be used much more frequently than in
news media or general English literature, and tend
to be highly ambiguous. For example, the Uni-
fied Medical Language System (UMLS), which in-
cludes one of the largest terminology resources in
the biomedical domain, contains 11 possible long-
forms of the acronym MS in addition to the four
examples used above. Liu et al (2001) show that
33% of acronyms are ambiguous in the UMLS. In a
subsequent study, Liu et al (2002a) found that 80%
of all acronyms found in Medline, a large repository
of abstracts from biomedical journals, are ambigu-
ous. Wren and Garner (2002) found that there exist
174,000 unique acronyms in the Medline abstracts
in which 36% of them are ambiguous. The authors
also estimated that the number of unique acronyms
is increasing at a rate of 11,000 per year.
Supervised and semi-supervised methods have
been used successfully for acronym disambiguation
145
but are limited in scope due to the need for sufficient
training data. Liu et al (2004) state that an acronym
could have approximately 16 possible long-forms in
Medline but could not obtain a sufficient number of
instances for each of the acronym-long-form pairs
for their experiments. Stevenson et al (2009) cite
a similar problem indicating that acronym disam-
biguation methods that do not require training data,
regardless if it is created manually or automatically,
are needed.
In this paper, we introduce a novel knowledge-
based method to disambiguate acronyms using
second-order co-occurrence vectors. This method
does not rely on training data, and therefore, is not
limited to disambiguating only commonly occurring
possible long-forms. These vectors are created us-
ing the first-order features obtained from the UMLS
about the acronym?s long-forms and second-order
features obtained from Medline. We show that us-
ing second-order features provide a distinct repre-
sentation of the long-form for the purposes of dis-
ambiguation and obtains a significantly higher dis-
ambiguation accuracy than using first order features.
2 Unified Medical Language System
The Unified Medical Language System (UMLS) is
a data warehouse that stores a number of distinct
biomedical and clinical resources. One such re-
source, used in this work, is the Metathesaurus.
The Metathesaurus contains biomedical and clin-
ical concepts from over 100 disparate terminol-
ogy sources that have been semi-automatically in-
tegrated into a single resource containing a wide
range of biomedical and clinical information. For
example, it contains the Systematized Nomencla-
ture of Medicine?Clinical Terms (SNOMED CT),
which is a comprehensive clinical terminology cre-
ated for the electronic exchange of clinical health
information, the Foundational Model of Anatomy
(FMA), which is an ontology of anatomical concepts
created specifically for biomedical and clinical re-
search, and MEDLINEPLUS, which is a terminol-
ogy source containing health related concepts cre-
ated specifically for consumers of health services.
The concepts in these sources can overlap. For
example, the concept Autonomic nerve exists in both
SNOMED CT and FMA. The Metathesaurus assigns
the synonymous concepts from the various sources
a Concept Unique Identifiers (CUIs). Thus both
the Autonomic nerve concepts in SNOMED CT and
FMA are assigned the same CUI (C0206250). This
allows multiple sources in the Metathesaurus to be
treated as a single resource.
Some sources in the Metathesaurus contain ad-
ditional information about the concept such as a
concept?s synonyms, its definition and its related
concepts. There are two main types of relations
in the Metathesaurus that we use: the parent/child
and broader/narrower relations. A parent/child re-
lation is a hierarchical relation between two con-
cepts that has been explicitly defined in one of the
sources. For example, the concept Splanchnic nerve
has an is-a relation with the concept Autonomic
nerve in FMA. This relation is carried forward to
the CUI level creating a parent/child relations be-
tween the CUIs C0037991 (Splanchnic nerve) and
C0206250 (Autonomic nerve) in the Metathesaurus.
A broader/narrower relation is a hierarchical relation
that does not explicitly come from a source but is
created by the UMLS editors. We use the entire
UMLS including the RB/RN and PAR/CHD rela-
tions in this work.
3 Medline
Medline (Medical Literature Analysis and Retrieval
System Online) is a bibliographic database contain-
ing over 18.5 million citations to journal articles
in the biomedical domain which is maintained by
the National Library of Medicine (NLM). The 2010
Medline Baseline, used in this study, encompasses
approximately 5,200 journals starting from 1948 and
is 73 Gigabytes; containing 2,612,767 unique uni-
grams and 55,286,187 unique bigrams. The majority
of the publications are scholarly journals but a small
number of newspapers, and magazines are included.
4 Acronym Disambiguation
Existing acronym disambiguation methods can be
classified into two categories: form-based and
context-based methods. Form-based methods, such
as the methods proposed by Taghva and Gilbreth
(1999), Pustejovsky et al (2001), Schwartz and
Hearst (2003) and Nadeau and Turney (2005), dis-
ambiguate the acronym by comparing its letters di-
146
rectly to the initial letters in the possible long-forms
and, therefore, would have difficulties in distin-
guishing between acronyms with similar long-forms
(e.g., RA referring to Refractory anemia or Rheuma-
toid arthritis).
In contrast, context-based methods disambiguate
between acronyms based on the context in which the
acronym is used with the assumption that the context
surrounding the acronym would be different for each
of the possible long-forms. In the remainder of this
section, we discuss these types of methods in more
detail.
4.1 Context-based Acronym Disambiguation
Methods
Liu et al (2001) and Liu et al (2002b) introduce
a semi-supervised method in which training and
test data are automatically created by extracting ab-
stracts from Medline that contain the acronym?s
long-forms. The authors use collocations and a bag-
of-words approach to train a Naive Bayes algorithm
and report an accuracy of 97%. This method be-
gins to treat acronym disambiguation as more of a
WSD problem by looking at the context in which
the acronym exists to determine its long-form, rather
than the long-form itself. In a subsequent study, Liu
et al (2004) explore using additional features and
machine learning algorithms and report an accuracy
of 99% using the Naive Bayes.
Joshi (2006) expands on Liu, et als work. They
evaluate additional machine learning algorithms us-
ing unigrams, bigrams and trigrams as features.
They found that given their feature set, SVMs ob-
tain the highest accuracy (97%).
Stevenson et al (2009) re-recreate this dataset us-
ing the method described in Liu et al (2001) to auto-
matically create training data for their method which
uses a mixture of linguistics features (e.g., colloca-
tions, unigrams, bigrams and trigrams) in combina-
tion with the biomedical features CUIs and Medi-
cal Subject Headings, which are terms manually as-
signed to Medline abstracts for indexing purposes.
The authors evaluate the Naive Bayes, SVM and
Vector Space Model (VSM) described by Agirre and
Martinez (2004), and report that VSM obtained the
highest accuracy (99%).
Pakhomov (2002) also developed a semi-
supervised method in which training data was
automatically created by first identifying the long-
form found in the text of clinical reports, replacing
the long-form with the acronym to use as training
data. A maximum entropy model trained and tested
on a corpus of 10,000 clinical notes achieved an
accuracy of 89%. In a subsequent study, Pakhomov
et al (2005) evaluate obtaining training data from
three sources: Medline, clinical records and the
world wide web finding using a combination of
instances from clinical records and the web obtained
the highest accuracy.
Joshi et al (2006) compare using the Naive
Bayes, Decision trees and SVM on ambiguous
acronyms found in clinical reports. The authors
use the part-of-speech, the unigrams and the bi-
grams of the context surrounding the acronym as
features. They evaluate their method on 7,738
manually disambiguated instances of 15 ambiguous
acronyms obtaining an accuracy of over 90% for
each acronym.
5 Word Sense Disambiguation
Many knowledge-based WSD methods have been
developed to disambiguate terms which are closely
related to the work presented in this paper. Lesk
(1986) proposes a definition overlap method in
which the appropriate sense of an ambiguous term
was determined based on the overlap between its
definition in a machine readable dictionary (MRD).
Ide and Ve?ronis (1998) note that this work provided
a basis for most future MRD disambiguation meth-
ods; including the one presented in this paper.
Banerjee and Pedersen (2002) use the Lesk?s
overlap method to determine the relatedness be-
tween two concepts (synsets) in WordNet. They ex-
tend the method to not only include the definition
(gloss) of the two synsets in the overlap but also the
glosses of related synsets.
Wilks et al (1990) expand upon Lesk?s method by
calculating the number of times the words in the def-
inition co-occur with the ambiguous words. In their
method, a vector is created using the co-occurrence
information for the ambiguous word and each of its
possible senses. The similarity is then calculated be-
tween the ambiguous word?s vector and each of the
sense vectors. The sense whose vector is most simi-
lar is assigned to the ambiguous word.
147
0
.3
0 0 0 0 0 0disphosphoric
glucose
fructose
phosphoric
esters
changed
effect
0 0 0 0 0
glycolyte
en
zym
es
co
m
bined
decreases
intensity
acid
0
m
etabolites
FEATURES
0 0 0 0 .2 0acid 0 0 0 .1 0 0
0 0 0 0 .5 0 0esters 0 0 0 0 0 0
0 .1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0
fructose
0 0 0 0 0 0
0 0 0 0 0 0 0
diphosphate
0 0 0 0 0 0isomer
0 0 0 0 0 0 0prevalent 0 0 0 0 0 0
0 .1 0 .3 .5 .2 02nd order vector forFructose Diphosphate 0 0 0 .1 0 0
Ex
te
nd
ed
 D
ef
in
iti
on

fo
r F
ru
ct
os
e 
Di
ph
os
ph
at
e
Figure 1: 2nd Order Vector for Fructose Diphosphate (FDP)
Patwardhan and Pedersen (2006) introduce a vec-
tor measure to determine the relatedness between
pairs of concepts. In this measure, a second order
co-occurrence vector is created for each concept us-
ing the words in each of the concepts definition and
calculating the cosine between the two vectors. This
method has been used in the task of WSD by calcu-
lating the relatedness between each possible sense
of the ambiguous word and its surrounding context.
The context whose sum is the most similar is as-
signed to the ambiguous word.
Second-order co-occurrence vectors were first in-
troduced by Schu?tze (1992) for the task of word
sense discrimination and later extended by Puran-
dare and Pedersen (2004). As noted by Pedersen
(2010), disambiguation requires a sense-inventory
in which the long-forms are known ahead of time,
where as in discrimination this information is not
known a priori.
6 Method
In our method, a second-order co-occurrence vec-
tor is created for each possible long-form of the
acronym, and the acronym itself. The appropriate
long-form of the acronym is then determined by
computing a cosine between the vector represent-
ing the ambiguous acronym and each of the vectors
representing the long-forms. The long-form whose
vector has the smallest angle between it and the
acronym vector is chosen as the most likely long-
form of the acronym.
To create a second-order vector for a long-form,
we first obtain a textual description of the long-form
in the UMLS, which we refer to as the extended defi-
nition. Each long-form, from our evaluation set, was
mapped to a concept in the UMLS, therefore, we use
the long-form?s definition plus the definition of its
parent/children and narrow/broader relations and the
terms in the long-form.
We include the definition of the related concepts
because not all concepts in the UMLS have a defini-
tion. In our evaluation dataset, not a single acronym
has a definition for each possible long-form. On
average, each extended definition contains approx-
imately 453 words. A short example of the extended
definition for the acronym FDP when referring to
148
fructose diphosphate is: ? Diphosphoric acid esters
of fructose. The fructose diphosphate isomer is most
prevalent. fructose diphosphate.?
After the extended definition is obtained, we cre-
ate the second-order vector by first creating a word
by word co-occurrence matrix in which the rows
represent the content words in the long-forms, ex-
tended definition, and the columns represent words
that co-occur in Medline abstracts with the words in
the definition. Each cell in this matrix contains the
Log Likelihood Ratio (Dunning (1993)) of the word
found in the row and the word in the column. Sec-
ond, each word in the long-forms, extended defini-
tion is replaced by its corresponding vector, as given
in the co-occurrence matrix. The centroid of these
vectors constitutes the second order co-occurrence
vector used to represent the long-form.
For example, given the example corpus contain-
ing two instances: 1) The metabolites, glucose fruc-
tose and their phosphoric acid esters are changed
due to the effect of glycolytic enzymes, and 2)
The phosphoric acid combined with metabolites de-
creases the intensity. Figure 1 shows how the
second-order co-occurrence vector is created for the
long-form fructose diphosphate using the extended
definition and features from our given corpus above.
The second-order co-occurrence vector for the
ambiguous acronym is created in a similar fashion,
only rather than using words in the extended defini-
tion, we use the words surrounding the acronym in
the instance.
Vector methods are subject to noise introduced by
features that do not distinguish between the differ-
ent long-forms of the acronym. To reduce this type
of noise, we select the features to use in the second
order co-occurrence vectors based on the following
criteria: 1) second order feature cannot be a stop-
word, and 2) second order feature must occur at least
twice in the feature extraction dataset and not occur
more than 150 times. We also experiment with the
location of the second-order feature with respect to
the first-order feature by varying the window size of
zero, four, six and ten words to the right and the left
of the first-order feature. The experiments in this
paper were conducted using CuiTools v0.15. 1
Our method is different from other context-based
1http://cuitools.sourceforge.net
acronym disambiguation methods discussed in the
related work because it does not require annotated
training data for each acronym that needs to be dis-
ambiguated. Our method differs from the method
proposed by Wilks et al (1990) in two fundamen-
tal aspects: 1) using the extended definition of
the possible long-forms of an acronym, and 2) using
second-order vectors to represent the instance con-
taining the acronym and each of the acronym?s pos-
sible long-forms.
7 Data
7.1 Acronym Dataset
We evaluated our method on the ?Abbrev? dataset 2
made available by Stevenson et al (2009). The
acronyms and long-forms in the data were initially
presented by Liu et al (2001). Stevenson et al
(2009) automatically re-created this dataset by iden-
tifying the acronyms and long-forms in Medline ab-
stracts and replacing the long-form in the abstract
with its acronym. Each abstract contains approxi-
mately 216 words. The dataset consists of three sub-
sets containing 100 instances, 200 instances and 300
instances of the ambiguous acronym referred to as
Abbrev.100, Abbrev.200, Abbrev.300, respectively.
The acronyms long-forms were manually mapped to
concepts in the UMLS by Stevenson, et al
A sufficient number of instances were not found
for each of the 21 ambiguous acronyms by Steven-
son et al (2009). For example, ?ASP? only con-
tained 71 instances and therefore not included in any
of the subsets. ?ANA? and ?FDP? only contained
just over 100 instances and therefore, are only in-
cluded in the Abbrev.100 subset. ?ACE?, ?ASP?
and ?CSF? were also excluded because several of
the acronyms? long-forms did not occur frequently
enough in Medline to create a balanced dataset.
We evaluate our method on the same subsets that
Stevenson et al (2009) used to evaluate their super-
vised method. The average number of long-forms
per acronym is 2.6 and the average majority sense
across all subsets is 70%.
7.2 Feature Extraction Dataset
We use abstracts from Medline, containing ambigu-
ous acronym or long-form, to create the second-
2http://nlp.shef.ac.uk/BioWSD/downloads/corpora
149
order co-occurrence vectors for our method as de-
scribed in Section 6. Table 1 shows the number of
Medline abstracts extracted for the acronyms.
Acronyms # Abstracts Acronym # Abstracts
ANA 3,267 APC 11,192
BPD 3,260 BSA 10,500
CAT 44,703 CML 8,777
CMV 13,733 DIP 2,912
EMG 16,779 FDP 1,677
LAM 1,572 MAC 6,528
MCP 2,826 PCA 11,044
PCP 5,996 PEG 10,416
PVC 2,780 RSV 5,091
Table 1: Feature Extraction Data for Acronyms
8 Results
Table 2 compares the majority sense baseline and the
first-order baseline with the results obtained using
our method on the Acronym Datasets (Abbrev.100,
Abbrev.200 and Abbrev.300) using a window size
of zero, four, six and ten. Differences between the
means of disambiguation accuracy produced by var-
ious approaches were tested for statistical signifi-
cance using the pair-wise Student?s t-tests with the
significance threshold set to 0.01.
Window Abbrev
Size 100 200 300
Maj. Sense Baseline 0.70 0.70 0.70
1-order Baseline 0.57 0.61 0.61
Our Method
0 0.83 0.83 0.81
4 0.86 0.87 0.86
6 0.88 0.90 0.89
10 0.88 0.90 0.89
Table 2: Overall Disambiguation Results
The majority sense baseline is often used to evalu-
ate supervised learning algorithms and indicates the
accuracy that would be achieved by assigning the
most frequent sense (long-form) to every instance.
The results in Table 2 demonstrate that our method is
significantly more accurate than the majority sense
baseline (p ? 0.01).
We compare the results using second-order vec-
tors to first-order vectors. Table 2 shows that ac-
curacy of the second-order results is significantly
higher than the first-order results (p ? 0.01).
The results in Table 2 also show that, as the win-
dow size grows from zero to six, the accuracy of the
system increases and plateaus at a window size of
ten. There is no statistically significant difference
between using a window size of six and ten but there
is a significant difference between a window size of
zero and six, as well as four and six (p ? 0.01).
Acronym # Long Abbrev Abbrev Abbrev
forms 100 200 300
ANA 3 0.84
APC 3 0.88 0.87 0.87
BPD 3 0.96 0.95 0.95
BSA 2 0.95 0.93 0.92
CAT 2 0.88 0.87 0.87
CML 2 0.81 0.84 0.83
CMV 2 0.98 0.98 0.98
DIP 2 0.98 0.98
EMG 2 0.88 0.89 0.88
FDP 4 0.65
LAM 2 0.86 0.87 0.88
MAC 4 0.94 0.95 0.95
MCP 4 0.73 0.67 0.68
PCA 4 0.78 0.79 0.79
PCP 2 0.97 0.96 0.96
PEG 2 0.89 0.89 0.88
PVC 2 0.95 0.95
RSV 2 0.97 0.98 0.98
Table 3: Individual Results using a Window Size of 6.
9 Error Analysis
Table 3 shows the results obtained by our method for
the individual acronyms using a window size of six,
and the number of possible long-forms per acronym.
Of the 18 acronyms, three obtain an accuracy below
80 percent: FDP, MCP and PCA.
FPD has four possible long-forms: Fructose
Diphosphate (E1), Formycin Diphosphate (E2), Fib-
rinogen Degradation Product (E3) and Flexor Dig-
itorum Profundus (E4). The confusion matrix in
Table 4 shows that the method was unable to dis-
tinguish between the two long-forms, E1 and E2,
which are both diphosphates, nor E2 and E3.
Long-Form E1 E2 E3 E4
E1: Fructose Diphosphate
E2: Formycin Diphosphate 5 2 11 19
E3: Fibrinogen Degradation Product 4
E4: Flexor Digitorum Profundus 59
Table 4: FDP Confusion Matrix
MCP also has four possible long-forms: Multicat-
alytic Protease (E1), Metoclopramide (E2), Mono-
cyte Chemoattractant Protein (E3) and Membrane
150
Cofactor Protein (E4). The confusion matrix in Ta-
ble 5 shows that the method was not able to distin-
guish between E3 and E4, which are both proteins,
and E1, which is a protease (an enzyme that breaks
down a protein).
Long-Form E1 E2 E3 E4
E1: Multicatalytic Protease 1 5 6 1
E2: Metoclopramide 15
E3: Monocyte Chemoattractant Protein 1 3 44 11
E4: Membrane Cofactor Protein 13
Table 5: MCP Confusion Matrix
PCA has four possible long-forms: Passive Cu-
taneous Anaphylaxis (E1), Patient Controlled Anal-
gesia (E2), Principal Component Analysis (E3), and
Posterior Cerebral Artery (E4). The confusion ma-
trix in Table 6 shows that the method was not able
to distinguish between E2 and E3. Analyzing the
extended definitions of the concepts showed that E2
includes the definition to the concept Pain Manage-
ment. The words in this definition overlap with
many of the words used in E3s extended definition.
Long-Form E1 E2 E3 E4
E1:Passive Cutaneous Anaphylaxis 18 6 1
E2:Patient Controlled Analgesia 5 15
E3:Principal Component Analysis 48
E4:Posterior Cerebral Artery 7
Table 6: PCA Confusion Matrix
10 Comparison with Previous Work
Of the previously developed methods, Liu et al
(2004) and Stevenson et al (2009) evaluated their
semi-supervised methods on the same dataset as we
used for the current study. A direct comparison
can not be made between our method and Liu et al
(2004) because we do not have an exact duplication
of the dataset that they use. Their results are com-
parable to Stevenson et al (2009) with both report-
ing results in the high 90s. Our results are directly
comparable to Stevenson et al (2009) who report
an overall accuracy of 98%, 98% and 99% on the
Abbrev.100, Abbrev.200 and Abbrev.300 datasets
respectively. This is approximately 10 percentage
points higher than our results.
The advantage of the methods proposed by
Stevenson et al (2009) and Liu et al (2004) is that
they are semi-supervised which have been shown to
obtain higher accuracies than methods that do not
use statistical machine learning algorithms. The dis-
advantage is that sufficient training data are required
for each possible acronym-long-form pair. Liu et
al. (2004) state that an acronym could have approxi-
mately 16 possible long-forms in Medline but a suf-
ficient number of instances for each of the acronym-
long-form pairs were not found in Medline and,
therefore, evaluated their method on 15 out of the
original 34 acronyms. Stevenson et al (2009) cite
a similar problem in re-creating this dataset. This
shows the limitation to these methods is that a suffi-
cient number of training examples can not be ob-
tained for each acronym that needs to be disam-
biguated. The method proposed in the paper does
not have this limitation and can be used to disam-
biguate any acronym in Medline.
11 Discussion
In this paper, we presented a novel method to disam-
biguate acronyms in biomedical text using second-
order features extracted from the UMLS and Med-
line. The results show that using second-order fea-
tures provide a distinct representation of the long-
form that is useful for disambiguation.
We believe that this is because biomedical text
contains technical terminology that has a rich source
of co-occurrence information associated with them
due to their compositionality. Using second-order
information works reasonably well because when
the terms in the extended definition are broken up
into their individual words, information is not being
lost. For example, the term Patient Controlled Anal-
gesia can be understood by taking the union of the
meanings of the three terms and coming up with an
appropriate definition of the term (patient has con-
trol over their analgesia).
We evaluated various window sizes to extract the
second-order co-occurrence information from, and
found using locally occurring words obtains a higher
accuracy. This is consistent with the finding reported
by Choueka and Lusignan (1985) who conducted an
experiment to determine what size window is needed
for humans to determine the appropriate sense of an
ambiguous word.
The amount of data used to extract the second-
151
order features for each ambiguous acronym varied
depending on its occurrence in Medline. Table 1 in
Section 7.2 shows the number of abstracts in Med-
line used for each acronym. We compared the accu-
racy obtained by our method using a window size of
six on the Abbrev.100 dataset with the number of ab-
stracts in the feature extraction data. We found that
the accuracy was not correlated with the amount of
data used (r = 0.07). This confirms that it is not the
quantity but the content of the contextual informa-
tion that determines the accuracy of disambiguation.
We compared using second-order features and
first-order features showing that the second-order re-
sults obtained a significantly higher accuracy. We
believe that this is because the definitions of the pos-
sible concepts are too sparse to provide enough in-
formation to distinguish between them. This find-
ing coincides to that of Purandare and Pedersen
(2004) and Pedersen (2010) who found that with
large amounts of data, first-order vectors perform
better than second-order vectors, but second-order
vectors are a good option when large amounts of
data are not available.
The results of the error analysis indicate that
for some acronyms using the extended definition
does not provide sufficient information to make
finer grained distinctions between the long-forms.
This result also indicates that, although many long-
forms of acronyms can be considered coarse-grained
senses, this is not always the case. For example, the
analysis of MCP showed that two of its possible
long-forms are proteins which are difficult to differ-
entiate from given the context.
The results of the error analysis also show that
indicative collocation features for acronyms are not
easily identified because acronyms tend to be com-
plete phrases. For example, two of the possible
long-forms of DF are Fructose Diphosphate and
Formycin Diphosphate.
Two main limitations of this work must be men-
tioned to facilitate the interpretation of the results.
The first is the small number of acronyms and the
small number of long-forms per acronym in the
dataset; however, the acronyms in this dataset are
representative of the kinds of acronyms one would
expect to see in biomedical text. The second limita-
tion is that the dataset contains only those acronyms
whose long-forms were found in Medline abstracts.
The main goal of this paper was to determine if the
context found in the long-forms, extended definition
was distinct enough to distinguish between them us-
ing second-order vectors. For this purpose, we feel
that the dataset was sufficient although a more ex-
tensive dataset may be needed in the future for im-
proved coverage.
12 Future Work
In the future, we plan to explore three different
avenues. The first avenue is to look at obtaining
contextual descriptions of the possible long-forms
from resources other than the UMLS such as the
MetaMapped Medline baseline and WordNet. The
second avenue is limiting the features that are used
in the instance vectors. The first-order features in
the instance vector contain the words from the entire
abstract. As previously mentioned, vector methods
are subject to noise, therefore, in the future we plan
to explore using only those words that are co-located
next to the ambiguous acronym. The third avenue is
expanding the vector to allow for terms. Currently,
we use word vectors, in the future, we plan to extend
the method to use terms, as identified by the UMLS,
as features rather than single words.
We also plan to test our approach in the clinical
domain. We believe that acronym disambiguation
may be more difficult in this domain due to the in-
crease amount of long-forms as seen in the datasets
used by Joshi et al (2006) and Pakhomov (2002).
13 Conclusions
Our study constitutes a significant step forward in
the area of automatic acronym ambiguity resolu-
tion, as it will enable the incorporation of scalable
acronym disambiguation into NLP systems used for
indexing and retrieval of documents in specialized
domains such as medicine. The advantage of our
method over previous methods is that it does not re-
quire manually annotated training for each acronym
to be disambiguated while still obtaining an overall
accuracy of 89%.
Acknowledgments
This work was supported by the National Insti-
tute of Health, National Library of Medicine Grant
#R01LM009623-01.
152
References
E. Agirre and D. Martinez. 2004. The Basque Country
University system: English and Basque tasks. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), pages 44?48.
S. Banerjee and T. Pedersen. 2002. An adapted lesk al-
gorithm for word sense disambiguation using Word-
Net. In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, pages 136?145.
Y. Choueka and S. Lusignan. 1985. Disambiguation
by short contexts. Computers and the Humanities,
19(3):147?157.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
N. Ide and J. Ve?ronis. 1998. Introduction to the special
issue on word sense disambiguation: the state of the
art. Computational Linguistics, 24(1):2?40.
M. Joshi, S. Pakhomov, T. Pedersen, and C.G. Chute.
2006. A comparative study of supervised learning as
applied to acronym expansion in clinical reports. In
Proceedings of the Annual Symposium of AMIA, pages
399?403.
M. Joshi. 2006. Kernel Methods for Word Sense Disam-
biguation and Abbreviation Expansion. Master?s the-
sis, University of Minnesota.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. Proceedings of the 5th Annual
International Conference on Systems Documentation,
pages 24?26.
H. Liu, YA. Lussier, and C. Friedman. 2001. Disam-
biguating ambiguous biomedical terms in biomedical
narrative text: an unsupervised method. Journal of
Biomedical Informatics, 34(4):249?261.
H. Liu, A.R. Aronson, and C. Friedman. 2002a. A study
of abbreviations in MEDLINE abstracts. In Proceed-
ings of the Annual Symposium of AMIA, pages 464?
468.
H. Liu, S.B. Johnson, and C. Friedman. 2002b. Au-
tomatic resolution of ambiguous terms based on ma-
chine learning and conceptual relations in the UMLS.
JAMIA, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A multi-
aspect comparison study of supervised word sense dis-
ambiguation. JAMIA, 11(4):320?331.
D. Nadeau and P. Turney. 2005. A supervised learning
approach to acronym identification. In Proceedings
of the 18th Canadian Conference on Artificial Intelli-
gence, pages 319?329.
S. Pakhomov, T. Pedersen, and C.G. Chute. 2005. Ab-
breviation and acronym disambiguation in clinical dis-
course. In Proceedings of the Annual Symposium of
AMIA, pages 589?593.
S. Pakhomov. 2002. Semi-supervised maximum en-
tropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Y. Park and R.J. Byrd. 2001. Hybrid text mining for find-
ing abbreviations and their definitions. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 126?133.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concepts. In Proceedings of the EACL 2006
Workshop Making Sense of Sense - Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8.
T. Pedersen. 2010. The effect of different context repre-
sentations on word sense discrimination in biomedical
texts. In Proceedings of the 1st ACM International IHI
Symposium, pages 56?65.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL),
pages 41?48.
J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,
M. Morrell, and A. Rumshisky. 2001. Extraction and
disambiguation of acronym-meaning pairs in medline.
Unpublished manuscript.
H. Schu?tze. 1992. Dimensions of meaning. In Proceed-
ings of the 1992 ACM/IEEE Conference on Supercom-
puting, pages 787?796.
A.S. Schwartz and M.A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of the Pacific Sym-
posium on Biocomputing (PSB), pages 451?462.
M. Stevenson, Y. Guo, A. Al Amri, and R. Gaizauskas.
2009. Disambiguation of biomedical abbreviations.
In Proceedings of the ACL BioNLP Workshop, pages
71?79.
K. Taghva and J. Gilbreth. 1999. Recognizing acronyms
and their definitions. ISRI UNLV, 1:191?198.
Y. Wilks, D. Fass, C.M. Guo, J.E. McDonald, T. Plate,
and B.M. Slator. 1990. Providing machine tractable
dictionary tools. Machine Translation, 5(2):99?154.
J.D. Wren and H.R. Garner. 2002. Heuristics for iden-
tification of acronym-definition patterns within text:
towards an automated construction of comprehensive
acronym-definition dictionaries. Methods of Informa-
tion in Medicine, 41(5):426?434.
153
