Proceedings of the 12th European Workshop on Natural Language Generation, pages 82?89,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
An Alignment-capable Microplanner for Natural Language Generation
Hendrik Buschmeier, Kirsten Bergmann and Stefan Kopp
Sociable Agents Group, CITEC, Bielefeld University
PO-Box 10 01 31, 33501 Bielefeld, Germany
{hbuschme, kbergman, skopp}@TechFak.Uni-Bielefeld.DE
Abstract
Alignment of interlocutors is a well known
psycholinguistic phenomenon of great rel-
evance for dialogue systems in general and
natural language generation in particular.
In this paper, we present the alignment-
capable microplanner SPUD prime. Us-
ing a priming-based model of interactive
alignment, it is flexible enough to model
the alignment behaviour of human speak-
ers to a high degree. This will allow for
further investigation of which parameters
are important to model alignment and how
the human?computer interaction changes
when the computer aligns to its users.
1 Introduction
A well known phenomenon in dialogue situations
is alignment of the interlocutors. An illustrative
example is given by Levelt and Kelter (1982), who
telephoned shops and either asked the question
?What time does your shop close?? or the ques-
tion ?At what time does your shop close??. The
answers were likely to mirror the form of the ques-
tion. When asked ?At what . . . ??, answers tended
to begin with the preposition ?at? (e.g., ?At five
o?clock.?). Conversely, when asked ?What . . . ??,
answers tended to begin without the preposition
(e.g., ?Five o?clock.?). Similar alignment phenom-
ena can be observed in many aspects of speech pro-
duction inter alia in syntactic and lexical choice.
Pickering and Garrod (2004) present the inter-
active alignment model bringing together all align-
ment phenomena of speech processing in dialogue.
According to this model, human language com-
prehension and production are greatly facilitated
by alignment of the interlocutors during conversa-
tion. The process of alignment is explained through
mutual priming of the interlocutors? linguistic rep-
resentations. Thus, it is automatic, efficient, and
non-conscious. A stronger claim of the authors is
that alignment ? in combination with routines and
a dialogue lexicon ? is a prerequisite for fluent
speech production in humans.
Alignment effects also occur in human?com-
puter interaction. Brennan (1991) and Branigan
et al (in press) present evidence that syntactic
structures and lexical items used by a computer
are subsequently adopted by users. For this reason,
alignment is an important concept for natural lan-
guage human?computer interaction in general, and
for dialogue systems with natural language gener-
ation in particular. Integrating ideas from the in-
teractive alignment model into the microplanning
component of natural language generation systems
should be beneficial for several reasons. First, mi-
croplanning may become more efficient since the
subsets of rules or lexical items in the dialogue
lexicon that have been used before can be prefer-
entially searched. Second, due to self-alignment,
the output of the system can become more con-
sistent and therefore easier to understand for the
user. Finally, mutual alignment of user and dia-
logue system might make the conversation itself
more natural and, presumably, cognitively more
lightweight for the user.
In this paper we present a computational model
for parts of the interactive alignment model that
are particularly important in the context of natural
language generation. We describe how this model
has been incorporated into the existing SPUD lite
system (Stone et al, 2003; Stone, 2002) to yield
the alignment-capable microplanner SPUD prime.
In Section 2 we describe previous approaches to
integrate alignment into natural language genera-
tion. In Sections 3 and 4, we present our priming-
based model of alignment and its implementation
in SPUD prime. In Section 5, we describe results
of an evaluation on a corpus of task-oriented dia-
logue, and in Section 6 we conclude our work and
describe possible future directions.
82
2 Related Work
Computational modelling is an important method-
ology for evaluating and testing psycholinguistic
theories. Thus, it is certainly not a new idea to
implement the interactive alignment model compu-
tationally. Indeed, a call for ?explicit computational
models? is made as early as in the open peer com-
mentary on Pickering and Garrod?s (2004) paper.
Brockmann et al (2005) and Isard et al (2006)
present a ?massive over-generation? approach to
modelling alignment and individuality in natural
language generation. Their system generates a
huge number of alternative sentences ? up to
3000 ? and evaluates each of these sentences with
a trigram model consisting of two parts: a default
language model computed from a large corpus and
a cache model which is calculated from the user?s
last utterance. The default language model is lin-
early interpolated with the cache model, whose in-
fluence on the resulting combined language model
is determined by a weighting factor ? ? [0,1] that
controls the amount of alignment the system exhib-
its.
Purver et al (2006) take a more formal approach.
They use an implementation of the Dynamic Syn-
tax formalism, which uses the same representations
and mechanisms for parsing as well as for genera-
tion of natural language, and extend it with a model
of context. In their model, context consists of two
distinct representations: a record of the semantic
trees generated and parsed so far and a record of
the transformation actions used for the construction
of these semantic trees. Re-use of semantic trees
and actions is used to model many dialogue phe-
nomena in Dynamic Syntax and can also explain
alignment. Thus, the authors declare alignment to
be a corollary of context re-use. In particular, re-use
of actions is assumed to have a considerable influ-
ence on alignment in natural language generation.
Instead of looking through the complete lexicon
each time a lexical item is chosen, this kind of lex-
ical search is only necessary if no action ? which
constructed the same meaning in the given con-
text before ? exists in the record. If such an action
exists, it can simply be re-used, which obviously
leads to alignment.
A completely different approach to alignment
in natural language generation is presented by de
Jong et al (2008), whose goal is to make a vir-
tual museum guide more believable by aligning
to the user?s level of politeness and formality. In
order to achieve this, the virtual guide analyses sev-
eral features of the user?s utterance and generates a
reply with the same level of politeness and formal-
ity. According to the authors, lexical and syntactic
alignment occur automatically because the lexical
items and syntactic constructions to choose from
are constrained by the linguistic style adopted.
Finally, Bateman (2006) advocates another pro-
posal according to which alignment in dialogue is
predictable for it is an inherently social activity.
Following the social-semiotic view of language,
Bateman suggests to model alignment as arising
from register and microregister. More specifically,
in his opinion priming of a linguistic representation
is comparable with pre-selecting a microregister
that must be considered when generating an utter-
ance in a particular social context.
The approaches presented above primarily focus
on the linguistic aspects of alignment in natural
language generation. The work of Brockmann et
al. (2005) and Isard et al (2006) concentrates on
the surface form of language, Bateman (2006) sees
alignment arising from social-semiotic aspects, and
Purver et al (2006) are primarily interested in fit-
ting alignment into a formal linguistic framework.
In this paper we adopt a more psycholinguistic and
cognitive stance on alignment. Pickering and Gar-
rod (2004) propose that low-level priming is the
basic mechanism underlying interactive alignment.
Here, we propose that computational modelling of
these priming mechanisms also opens up an inter-
esting and new perspective for alignment in natural
language generation.
3 A Priming-based Model of Alignment
We are interested here in those parts of the inter-
active alignment model that are most relevant for
microplanning in natural language generation and
it is out of our scope to model all the facets and
details of direct/repetition priming in the alignment
of linguistic representations. For instance, exact
timing effects are likely to be not even relevant as,
in an actual system, it does not matter how many
milliseconds faster the retrieval of a primed lexical
item is in contrast to the retrieval of an item that
is not primed. For this reason we adopt an ideal-
ised view, in which priming of linguistic structures
results from two basic activation mechanisms:
Temporary activation This kind of activation
should increase abruptly and then decrease
slowly over time until it reaches zero again.
83
Permanent activation This kind of activation
should increase by a certain quantity and then
maintain the new level.
These two mechanisms of priming are in ac-
cordance with empirical findings. Branigan et al
(1999) present evidence for rapid decay of activa-
tion of primed syntactic structures, whereas Bock
and Griffin (2000) report evidence for their long(er)
term activation. In any case, Reitter (2008) found
both types of priming in his analysis of several
corpora, with temporary activation being the more
important one. The assumption that both mechan-
isms play a role in dialogue is also supported by
Brennan and Clark (1996) whose terminology will
be followed in this paper: temporary priming will
be called ?recency of use effects? and permanent
priming will be called ?frequency of use effects?.
Reitter (2008) assumes the repetition probability
of primed syntactic structures to depend logarith-
mically on the distance between priming and usage.
Here, recency of use effects are modelled by a
more general exponential decay function, modified
to meet the needs for modelling activation decay of
primed structures:
ta(?r) = exp
(
?
?r?1
?
)
, (1)
?r ? N+; ? > 0; ta ? [0,1]
ta(?r) is the temporary activation value of a lin-
guistic structure depending on the distance ?r
between the current time T and the time r at which
the structure was primed. The slope of the function
is determined by the parameter ? . Additionally, the
function is shifted right in order to yield an activa-
tion value of 1 for ?r = 1. This shift is due to the
assumption of discrete time steps with a minimal
distance of 1. A plot of ta(?r) with different values
for ? is given in Figure 1a.
Using exponential decay to model temporary ac-
tivation appears to be a sensible choice that is often
used to model natural processes. The advantage of
this model of temporary activation lies in its flexib-
ility. By changing the slope parameter ? , different
empirical findings as well as variation among hu-
mans can be modelled easily.
Next, a mathematical model for frequency of use
effects is needed. To prevent that frequency effects
lead to an ever increasing activation value, a max-
imum activation level exists. This is also found in
Reitter?s (2008) corpus studies, which indicate that
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  3  5  7  9  11  13  15
T
em
po
ra
ry
 A
ct
iv
at
io
n 
 ta
(?
r)
Recency Distance  ?r
(a)
? = 1
2
4
8
16
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  3  5  7  9  11  13  15
Pe
rm
an
en
t A
ct
iv
at
io
n 
 p
a(
 f 
)
Frequency Counter f
(b)
? = 1
2
4
8
16
Figure 1: Plots of the mathematical functions that
model recency and frequency effects. Plot (a) dis-
plays temporary activation depending on the re-
cency of priming. Plot (b) shows permanent activ-
ation depending on the frequency count. Both are
shown for different values of the slope parameter
? respectively ? .
the frequency effect is inversely connected to the
recency effect. Here, we model recency effects with
a general exponential saturation function, modified
to meet the requirements for modelling permanent
activation of linguistic structures:
pa( f ) = 1? exp
(
?
f ?1
?
)
, (2)
f ? N+; ? > 0; pa ? [0,1]
The most important point to note here is that the
permanent activation value pa( f ) is not a function
of time but a function of the frequency-counter f
attached to each linguistic structure. Whenever a
structure is primed, its counter is increased by the
value of 1. Again, the slope of the function is de-
termined by the parameter ? and the function is
84
shifted right in order to get an activation value of
0 for f = 1. A plot of equation (2) with different
slope parameters is given in Figure 1b. Similar to
the advantages of the model of temporary activa-
tion, this model for frequency effects is very flex-
ible so that different empirical findings and human
individuality can be expressed easily.
Now, both priming models need to be combined
for a model of alignment. We opted for a weighted
linear combination of temporary and permanent
activation:
ca(?r, f ) = ? ? ta(?r)+(1??) ? pa( f ), (3)
0? ? ? 1; ca ? [0,1]
Different values of ? allow different forms of align-
ment. With a value of ? = 0.5 recency and fre-
quency effects are equally important, with a value
of ? = 1 alignment depends on recency only, and
with a value of ? = 0 alignment is governed solely
by frequency. Being able to adjust the influence
of the different sorts of priming on alignment is
crucial as it has not yet been empirically determ-
ined to what extent recency and frequency of use
affect alignment (in Section 5.2 we will exploit this
flexibility for matching empircial data).
In contrast to the models of alignment presented
in Section 2, the computational alignment model
presented here will not only consider alignment
between the interlocutors (interpersonal- or other-
alignment), but also alignment to oneself (intra-
personal- or self-alignment). Pickering et al (2003)
present results from three experiments which sug-
gest self-alignment to be even more important than
other-alignment. In our model, self-alignment is
accounted for with the same priming-based mech-
anisms. To this end, four counters are attached to
each linguistic structure:
? ?rs: recency of use by the system itself
? ?ro: recency of use by the interlocutor
? fs: frequency of use by the system itself
? fo: frequency of use by the interlocutor
The overall activation value of the structure is
a linear combination of the combined activation
value ca(?rs, fs) and the combined activation value
ca(?ro, fo) from equation (3):
act(?rs, fs,?ro, fo) =
? ? (? ? ca(?rs, fs)+(1??) ? ca(?ro, fo)),
(4)
0? ? ,? ? 1; act ? [0,1]
Again, by changing the factor ? , smooth interpola-
tion between pure self-alignment (? = 1) and pure
other-alignment (? = 0) is possible, which can ac-
count for different empirical findings or human
individual differences. Furthermore, the strength
of alignment is modelled with a scaling factor ? ,
which determines whether alignment is considered
during generation (? > 0) or not (? = 0).
4 The Alignment-capable Microplanner
SPUD prime
The previously described priming-based model of
alignment has been implemented by extending
the integrated microplanning system SPUD lite
(Stone, 2002). SPUD lite is a lightweight Prolog
re-implementation of the SPUD microplanning sys-
tem (Stone et al, 2003) based on the context-free
tree rewriting grammar formalism TAGLET. Not
only the microplanner itself, but also the linguistic
structures (the initial TAGLET trees) are represen-
ted as Prolog clauses.
SPUD lite carries out the different microplan-
ning tasks (lexical choice, syntactic choice, refer-
ring expression generation and aggregation) at once
by treating microplanning as a search problem. Dur-
ing generation it tries to find an utterance which is
in accordance with the constraints set by its input
(a grammar, a knowledge base and a query). This is
done by searching the search space spanned by the
linguistic grammar rules and the knowledge base
until a goal state is found. Non-goal search states
are preliminary utterances that are extended by one
linguistic structure in each step until a syntactically
complete utterance is found which conveys all the
specified communicative goals. Since this search
space is large even for relatively small grammars,
a heuristic greedy search strategy is utilised.
Our alignment-capable microplanner SPUD
prime extends SPUD lite in several ways. First, we
altered the predicate for the initial TAGLET trees
by adding a unique identifier ID as well as counters
for self/other-recency/frequency values (rs, fs, ro
and fo; see Section 3). The activation value of an
initial tree is then calculated with equation (4).
Furthermore, we have created a mechanism that
enables SPUD lite to change the recency and fre-
quency information attached to the initial trees on-
line during generation. This is done in three steps
with the help of Prolog?s meta-programming cap-
abilities: Firstly, the clause of a tree is retrieved
85
from the knowledge base. Secondly, it is retrac-
ted from the knowledge base. Finally, the clause
is (re-)asserted in the knowledge base with up-
dated recency and frequency information. As a
welcome side effect of this procedure, primed ini-
tial trees are moved to the top of the knowledge
base and ? since Prolog evaluates clauses and facts
in the order of their appearance in the knowledge
base ? they can be accessed earlier than unprimed
initial trees or initial trees that were primed longer
ago. Thus, in SPUD prime recency of priming dir-
ectly influences the access of linguistic structures.
Most importantly, the activation values of the ini-
tial trees are considered during generation. Thus, in
addition to the evaluation measures used by SPUD
lite?s heuristic state evaluation function, the mean
activation value
act(S) =
?Ni=1 actti(?rsti , fsti ,?roti , foti )
N
of the N initial trees {t1, . . . , tN} of a given search
state S is taken into account as a further evaluation
measure. Hence, when SPUD prime evaluates (oth-
erwise equal) successor search states, the one with
the highest mean activation value is chosen as the
next current state.
5 Evaluation
In order to find out whether our priming-based
alignment model and its implementation work as
intended, we evaluated SPUD prime on a corpus
that was collected in an experiment designed to
investigate the alignment behaviour of humans in
a controlled fashion (Wei? et al, 2008). The part
of the corpus that we used consists of eight recor-
ded and transcribed dialogues between two inter-
locutors that play the ?Jigsaw Map Game?, a task
in which different objects have to be placed cor-
rectly on a table. Speakers take turns in explaining
each other where to place the next object in re-
lation to the objects that are already on the table.
Each speaker has to learn a set of name?object rela-
tions before the game, such that both use the same
names for all but three objects. Due to this precon-
dition, both speakers use the same lexical referring
expressions for most objects and the speaker?s lex-
ical alignment behaviour for the differently named
objects can be observed easily.
In our evaluation, we concentrate on the gener-
ation of nouns by simulating the uses of the three
differently learned nouns in the eight dialogues
from the perspective of all sixteen interlocutors.
In each test, SPUD prime plays the role of one
of the speakers talking to a simulated interlocutor
who behaves exactly as in the real experiment.
With this test setup we examined, first, how well
SPUD prime can model the alignment behaviour
of a real speaker in a real dialogue context and,
second, whether our model is flexible enough to
consistently emulate different speakers with differ-
ent alignment behaviour.
In order to find the best model (i.e., the best
parameter set {?,? ,?,?}) for each speaker, we
simulated all tests with all parameter combinations
and counted the number of mismatches between
our model?s choice and the real speaker?s choice.
To make this exhaustive search possible, we limit
the set of values for the parameters ? and ? to
{1,2,4,6,8,10,14,18,24,30} and the set of values
for the parameters ? and ? to {0,0.1,0.2, ...,1},
resulting in a total of 112?102 = 12100 different
parameter sets. Since we want to investigate align-
ment, ? is constantly set to 1.
5.1 An Illustrative Example
To illustrate our evaluation method, we first present
and discuss the simulation of one particular dia-
logue (from the Jigsaw Map Game corpus) from
the perspective of participant (A). Before the exper-
iment started, both interlocutors learned the name?
object relations ?Raute? (rhombus), ?Ring? (ring),
?Schraube? (bolt) and ?Wu?rfel? (cube), additionally
participant (A) learned ?Spielfigur? (token), ?Ball?
(sphere) and ?Block? (cuboid) and participant (B)
learned ?Ma?nnchen? (token), ?Kugel? (sphere) and
?Klotz? (cuboid). In our simulation, we focus on the
use of the differently learned names (the targets)
and not on the other names (non-targets). Table 1
shows the sequence of target nouns as they oc-
curred in the real dialogue (non-targets omitted).
For each parameter set {?,? ,?,?} the dialogue
is simulated in the following way:
? When participant (A) used a referring non-
target noun in the dialogue, self-priming of
the corresponding rule(s) in SPUD prime?s
knowledge base is simulated (i.e., the recency
and frequency counters are increased).
? When participant (A) used a referring target
noun in the dialogue, SPUD prime is queried
to generate a noun for the target object. Then
it is noted whether the noun actually generated
86
B: der Klotz 14 A: der Klotz
1 A: die Spielfigur 15 A: die Kugel
2 A: der Klotz 16 A: der Klotz
B: das Ma?nnchen B: der Klotz
B: der Klotz B: die Kugel
3 A: die Spielfigur B: der Klotz
B: das Ma?nnchen 17 A: der Klotz
4 A: das Ma?nnchen B: das Ma?nnchen
5 A: das Ma?nnchen B: der Klotz
6 A: das Ma?nnchen 18 A: das Ma?nnchen
7 A: das Ma?nnchen 19 A: der Klotz
8 A: das Ma?nnchen B: das Ma?nnchen
B: das Ma?nnchen 20 A: der Ball
9 A: das Ma?nnchen 21 A: das Ma?nnchen
10 A: der Ball B: der Ball
B: der Ball B: das Ma?nnchen
11 A: der Ball 22 A: die Kugel
12 A: der Ball 23 A: der Ball
B: die Kugel B: der Klotz
B: das Ma?nnchen 24 A: der Ball
13 A: der Ball B: der Klotz
B: die Kugel 25 A: der Klotz
Table 1: Sequence of referring target nouns used by
participants (A) and (B) in our example dialogue.
is the noun used in the actual dialogue (match)
or not (mismatch).
? When participant (B) used a referring noun
(target or non-target), priming of the corres-
ponding rule(s) in SPUD prime?s knowledge
base is simulated.
The evaluation measure for a specific parameter
set is the number of mismatches it produces when
simulating a dialogue. Thus the parameter set (or
rather sets) which produce the least number of mis-
matches are the ones that best model the particular
speaker under consideration. For participant (A)
of our example dialogue the distribution of para-
meter sets p producing m mismatches is shown in
Table 2. Four parameter sets produce only two mis-
matches (in phrase 15 and 22; cf. Table 1) and thus
our priming-based alignment model can account
for 92% of the target nouns produced by speaker
(A). However, it must be noted that these two mis-
matches occur at points in the dialogue where the
alignment behaviour of (A) is not straightforward.
At target noun 15, both interlocutors have already
used the name ?Ball? and then both switch to ?Ku-
gel?. The mismatch at target 22 is a special case: (A)
used ?Kugel? and immediately corrected himself to
?Ball?, the name he learned prior to the experiment.
It seems as if the task instruction, to use the learned
nouns, suddenly became prevalent.
m 0 1 2 3 4 5
# p 0 0 4 833 3777 2248
m 6 7 8 9 10 . . .
# p 3204 1105 478 148 294 0
Table 2: Number of parameter sets p leading to m
mismatches for participant (A) in dialogue 7.
5.2 Simulation Results
To evaluate our alignment-capable microplanner,
we simulated the noun production for each of the
interlocutors from the experiment. One dialogue
has been excluded from the data analysis as the
dialogue partners used nouns that none of them had
learned in the priming phase. For each of the re-
maining 14 interlocutors we varied the parameters
? , ? , ? and ? as described above to identify those
parameter set(s) which result in the least number
of mismatches.
Each interlocutor produced between 18 and 32
target nouns (N=14, M=23.071, SD=3.936). Our
simulation runs contain between 0 and 19 mis-
matches overall (N=169400, M=6.35, SD=3.398).
The minimal number of mismatches for each
speaker simulation ranges between 0 and 6 (N=14,
M=2.286, SD=1.684). That is, our model can sim-
ulate a mean of 89.9% of all target nouns (N=14,
M=.899, Min=.667, Max=1.000, SD=.082), which
is an improvement of 24.6% on the baseline con-
dition (alignment switched off), where 65.3% of
the target nouns are generated correctly (N=14,
M=.653, Min=.360, Max=1.000, SD=.071). As
already illustrated in Section 5.1, mismatches typic-
ally occur at points in the dialogue where the align-
ment behaviour of the interlocutor is not straight-
forward.
As displayed in Table 3 the parameter assign-
ments resulting in least mismatches differ consid-
erably from speaker to speaker. However, there are
some remarkable trends to be observed in the data.
As concerns the parameter ? , which determines
the combination of self- and other-alignment, the
majority of values are in the upper range of the
interval [0,1]. For 8 of 14 speakers the mean is
above 0.7 with relatively low standard deviations.
Only for one speaker (P13) the mean ? is below
0.3. Thus, the parameter values indicate a consider-
able tendency toward self-alignment in contrast to
other-alignment.
For the parameter ? that interpolates between
recency and frequency effects of priming, the res-
87
? ? ? ?
m # p M SD M SD M SD M SD
P13 2 4 3.0 1.155 19.5 9.14 .1 .0 .3 .0
P14 1 72 5.53 1.52 14.32 9.61 .819 .040 .901 .108
P17 1 200 1.66 .823 12.94 9.529 .353 .169 .955 .069
P18 3 2445 15.37 8.758 10.98 9.76 .597 .211 .706 .236
P19 0 4321 11.81 9.492 11.01 8.929 .824 .148 .387 .291
P20 2 8 1.0 .0 15.75 9.285 .737 .052 .388 .146
P23 6 987 6.85 6.681 12.08 9.354 .331 .374 .4 .33
P24 3 256 12.95 9.703 13.63 8.937 .537 .201 .468 .298
P39 5 1 1.0 .0 2.0 .0 .9 .0 .8 .0
P40 0 3504 12.08 9.33 10.30 8.753 .843 .147 .343 .282
P41 2 609 11.37 8.475 15.34 8.921 .770 .106 .655 .213
P42 3 30 6.0 1.486 17.53 9.016 .783 .059 .760 .122
P47 2 326 13.75 7.794 13.53 9.508 .772 .095 .816 .166
P48 2 2478 12.87 9.545 10.74 8.538 .764 .175 .166 .148
Table 3: Mean parameter values for those simulation runs which result in a minimal number of mismatches
for each speaker.
ults are less revealing. For two speaker simulations
(P13 and P48) the mean ? is 0.3 or lower, for an-
other four speaker simulations the mean ? is above
0.7. That is, our model produces good matching be-
haviour in adopting different alignment strategies,
depending either primarily on frequency or recency,
respectively. All other simulations, however, are
characterised by a mean ? in the medium range
along with a relatively high standard deviation.
6 Conclusion
In this paper, we introduced a priming-based model
of alignment which focusses more on the psycho-
linguistic aspects of interactive alignment and mod-
els recency and frequency of use effects ? as pro-
posed by Reitter (2008) and Brennan and Clark
(1996) ? as well as the difference between intraper-
sonal and interpersonal alignment (Pickering et al,
2003; Pickering and Garrod, 2004). The presented
model is fully parameterisable and can account for
different empirical findings and ?personalities?. It
has been implemented in the SPUD prime micro-
planner which activates linguistic rules by changing
its knowledge base on-line and considers the ac-
tivation values of those rules used in constructing
the current utterance by using their mean activation
value as an additional feature in its state evaluation
function.
We evaluated our alignment model and its im-
plementation in SPUD prime on a corpus of task-
oriented dialogue collected in an experimental
setup especially designed for alignment research.
The results of this evaluation show that our priming-
based model of alignment is flexible enough to sim-
ulate the alignment behaviour of different human
speakers (generating target nouns) in the experi-
mental setting. It should be noted, however, that
our model tries to give a purely mechanistic ex-
planation of lexical and syntactic choice and that
it, therefore, cannot explain alignment phenomena
that are due to social factors (e.g., politeness, rela-
tionship, etc.), audience design or cases, in which a
speaker consciously decides whether to align or not
(e.g., whether to use a word or its synonym). While
the evaluation has shown that our model can repro-
duce human alignment behaviour to a high degree,
it remains to be investigated which influence each
parameter exerts and how exactly the parameters
vary across individual speakers.
Nevertheless, the development of the alignment-
capable microplanner is only one step in the dir-
ection of an intuitive natural language human?
computer interaction system. In order to reach this
goal, the next step is to combine SPUD prime with
a natural language understanding system, which
should ideally work with the same linguistic rep-
resentations so that the linguistic structures used
by the interlocutor could be primed automatically.
This work is underway.
Furthermore, user studies should be carried
out in order to evaluate SPUD prime in a more
sophisticated way. Branigan et al (in press) found
that human?computer alignment was even stronger
than human?human alignment. But how would
the alignment behaviour of human interlocutors
change if the computer they are speaking to also
aligns to them? Further, would integration of an
alignment-capable dialogue system into a computer
interface make the interaction more natural? And
would an embodied conversational agent appear
88
more resonant and more sociable (Kopp, 2008) if
it aligned to users during conversation? The work
presented here provides a starting point for the
investigation of these questions.
Acknowledgements ? This research is supported
by the Deutsche Forschungsgemeinschaft (DFG) in
the Center of Excellence in ?Cognitive Interaction
Technology? (CITEC) as well as in the Collabor-
ative Research Center 673 ?Alignment in Commu-
nication?. We also thank Petra Wei? for making
the ?Jigsaw Map Game? corpus available and three
anonymous reviewers for their helpful comments.
References
John A. Bateman. 2006. A social-semiotic view of
interactive alignment and its computational instanti-
ation: A brief position statement and proposal. In
Kerstin Fischer, editor, How People Talk to Com-
puters, Robots and Other Artificial Communication
Partners, SFB/TR 8 Report No. 010-09/2006, pages
157?170, Bremen, Germany.
J. Kathryn Bock and Zenzi M. Griffin. 2000. The per-
sistence of structural priming: Transient activation
or implicit learning? Journal of Experimental Psy-
chology: General, 129(2):177?192.
Holly P. Branigan, Martin J. Pickering, and Alexan-
dra A. Cleland. 1999. Syntactic priming in written
production: Evidence for rapid decay. Psychonomic
Bulletin & Review, 6(4):635?640.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
and Janet F. McLean. in press. Linguistic alignment
between people and computers. Journal of Pragmat-
ics.
Susan E. Brennan and Herbert H. Clark. 1996.
Conceptual pacts and lexical choice in conversa-
tion. Journal of Experimental Psychology: Learn-
ing, Memory, and Cognition, 22(6):1482?1493.
Susan E. Brennan. 1991. Conversation with and
through computers. User Modeling and User-Adapt-
ed Interaction, 1(1):67?86.
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Modelling alignment for af-
fective dialogue. In Proc. of the Workshop on Adapt-
ing the Interaction Style to Affective Factors at the
10th Int. Conf. on User Modeling.
Markus A. de Jong, Marie?t Theune, and Dennis Hofs.
2008. Politeness and alignment in dialogues with
a virtual guide. In Proc. of the 7th Int. Conf. on
Autonomous Agents and Multiagent Systems, pages
207?214.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and alignment in generated dia-
logues. In Proc. of the 4th Int. Natural Language
Generation Conf., pages 25?32.
Stefan Kopp. 2008. From communicators to reson-
ators ? Making embodied conversational agents so-
ciable. In Proc. of the Speech and Face to Face
Communication Workshop in Memory of Christian
Beno??t, pages 34?36.
Willem J. M. Levelt and Stephanie Kelter. 1982. Sur-
face form and memory in question answering. Cog-
nitive Psychology, 14(1):78?106.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, 27(2):169?226.
Martin J. Pickering, Holly P. Branigan, and Janet F.
McLean. 2003. Dialogue structure and the activ-
ation of syntactic information. In Proc. of the 9th
Annual Conf. on Architectures and Mechanisms for
Language Processing, page 126.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computation,
4(2?3):289?326.
David Reitter. 2008. Context Effects in Language Pro-
duction: Models of Syntactic Priming in Dialogue
Corpora. Ph.D. thesis, University of Edinburgh.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with communicative intentions: The SPUD sys-
tem. Computational Intelligence, 19(4):311?381.
Matthew Stone. 2002. Lexicalized grammar 101. In
Proc. of the ACL-02 Workshop on Effective Tools
and Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages
77?84.
Petra Wei?, Thies Pfeiffer, Gesche Schaffranietz, and
Gert Rickheit. 2008. Coordination in dialog: Align-
ment of object naming in the Jigsaw Map Game. In
Proc. of the 8th Annual Conf. of the Cognitive Sci-
ence Society of Germany, pages 4?20.
89
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 51?54,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Middleware for Incremental Processing in Conversational Agents
David Schlangen?, Timo Baumann?, Hendrik Buschmeier?, Okko Bu??
Stefan Kopp?, Gabriel Skantze?, Ramin Yaghoubzadeh?
?University of Potsdam ?Bielefeld University ?KTH, Stockholm
Germany Germany Sweden
david.schlangen@uni-potsdam.de
Abstract
We describe work done at three sites on
designing conversational agents capable of
incremental processing. We focus on the
?middleware? layer in these systems, which
takes care of passing around and maintain-
ing incremental information between the
modules of such agents. All implementa-
tions are based on the abstract model of
incremental dialogue processing proposed
by Schlangen and Skantze (2009), and the
paper shows what different instantiations
of the model can look like given specific
requirements and application areas.
1 Introduction
Schlangen and Skantze (2009) recently proposed
an abstract model of incremental dialogue process-
ing. While this model introduces useful concepts
(briefly reviewed in the next section), it does not
talk about how to actually implement such sys-
tems. We report here work done at three different
sites on setting up conversational agents capable
of incremental processing, inspired by the abstract
model. More specifically, we discuss what may
be called the ?middleware? layer in such systems,
which takes care of passing around and maintaining
incremental information between the modules of
such agents. The three approaches illustrate a range
of choices available in the implementation of such
a middle layer. We will make our software avail-
able as development kits in the hope of fostering
further research on incremental systems.1
In the next section, we briefly review the abstract
model. We then describe the implementations cre-
ated at Uni Bielefeld (BF), KTH Stockholm (KTH)
and Uni Potsdam (UP). We close with a brief dis-
cussion of similarities and differences, and an out-
look on further work.
1Links to the three packages described here can be found
at http://purl.org/net/Middlewares-SIGdial2010.
2 The IU-Model of Incremental Processing
Schlangen and Skantze (2009) model incremental
systems as consisting of a network of processing
modules. Each module has a left buffer, a proces-
sor, and a right buffer, where the normal mode of
processing is to take input from the left buffer, pro-
cess it, and provide output in the right buffer, from
where it goes to the next module?s left buffer. (Top-
down, expectation-based processing would work
in the opposite direction.) Modules exchange incre-
mental units (IUs), which are the smallest ?chunks?
of information that can trigger connected modules
into action. IUs typically are part of larger units;
e.g., individual words as parts of an utterance, or
frame elements as part of the representation of an
utterance meaning. This relation of being part of
the same larger unit is recorded through same level
links; the information that was used in creating a
given IU is linked to it via grounded in links. Mod-
ules have to be able to react to three basic situa-
tions: that IUs are added to a buffer, which triggers
processing; that IUs that were erroneously hypothe-
sised by an earlier module are revoked, which may
trigger a revision of a module?s own output; and
that modules signal that they commit to an IU, that
is, won?t revoke it anymore (or, respectively, expect
it to not be revoked anymore).
Implementations of this model then have to re-
alise the actual details of this information flow, and
must make available the basic module operations.
3 Sociable Agents Architecture
BF?s implementation is based on the ?D-Bus? mes-
sage bus system (Pennington et al, 2007), which
is used for remote procedure calls and the bi-
directional synchronisation of IUs, either locally
between processes or over the network. The bus sys-
tem provides proxies, which make the interface of
a local object accessible remotely without copying
data, thus ensuring that any access is guaranteed to
yield up-to-date information. D-Bus bindings exist
for most major programming languages, allowing
51
for interoperability across various systems.
IUs exist as objects implementing a D-Bus in-
terface, and are made available to other modules
by publishing them on the bus. Modules are ob-
jects comprising a main thread and right and left
buffers for holding own IUs and foreign IU proxies,
respectively. Modules can co-exist in one process
as threads or occupy one process each?even dis-
tributed across a network.
A dedicated Relay D-Bus object on the network
is responsible for module administration and up-
date notifications. At connection time, modules
register with the relay, providing a list of IU cat-
egories and/or module names they are interested
in. Category interests create loose functional links
while module interests produce more static ones.
Whenever a module chooses to publish informa-
tion, it places a new IU in its right buffer, while
removal of an IU from the right buffer corresponds
to retraction. The relay is notified of such changes
and in turn invokes a notification callback in all
interested modules synchronising their left buffers
by immediately and transparently creating or re-
moving proxies of those IUs.
IUs consist of the fields described in the abstract
model, and an additional category field which the
relay can use to identify the set of interested mod-
ules to notify. They furthermore feature an optional
custom lifetime, on the expiration of which they
are automatically retracted.
Incremental changes to IUs are simply realised
by changing their attributes: regardless of their lo-
cation in either a right or left buffer, the same setter
functions apply (e.g., set payload). These generate
relay-transported update messages which commu-
nicate the ID of the changed IU. Received update
messages concerning self-owned and remotely-
owned objects are discerned automatically to allow
for special treatment of own IUs. The complete
process is illustrated in Figure 1.
Current state and discussion. Our support for
bi-directional IU editing is an extension to the con-
cepts of the general model. It allows higher-level
modules with a better knowledge of context to re-
vise uncertain information offered by lower levels.
Information can flow both ways, bottom-up and
top-down, thus allowing for diagnostic and causal
networks linked through category interests.
Coming from the field of embodied conversa-
tional agents, and being especially interested in
modelling human-like communication, for exam-
A B
C
IU
IU proxy
Write access
Relay
Data access
Update notification
RBuf LBuf
Interest sets
Figure 1: Data access on the IU proxies is transparently dele-
gated over the D-Bus; module A has published an IU. B and C
are registered in the corresponding interest set, thus receiving
a proxy of this IU in their left buffer. When B changes the IU,
A and C receive update notifications.
ple for on-line production of listener backchannel
feedback, we constantly have to take incremen-
tally changing uncertain input into account. Using
the presented framework consistently as a network
communication layer, we are currently modelling
an entire cognitive architecture for virtual agents,
based on the principle of incremental processing.
The decision for D-Bus as the transportation
layer has enabled us to quickly develop ver-
sions for Python, C++ and Java, and produced
straightforward-to-use libraries for the creation of
IU-exchanging modules: the simplest fully-fledged
module might only consist of a periodically in-
voked main loop callback function and any subset
of the four handlers for IU events (added, removed,
updated, committed).
4 Inpro Toolkit
The InproTK developed at UP offers flexibility on
how tightly or loosely modules are coupled in a
system. It provides mechanisms for sending IU up-
dates between processes via a messaging protocol
(we have used OAA [Cheyer and Martin, 2001], but
other communication layers could also be used) as
well as for using shared memory within one (Java)
process. InproTK follows an event-based model,
where modules create events, for which other mod-
ules can register as Listeners. Module networks are
configured via a system configuration file which
specifies which modules listen to which.
Modules push information to their right, hence
the interface for inter-module communication is
called PushBuffer. (At the moment, InproTK only
implements left-to-right IU flow.) The PushBuffer
interface defines a hypothesis-change method
which a module will call for all its listening mod-
ules. A hypothesis change is (redundantly) charac-
terised by passing both the complete current buffer
state (a list of IUs) as well as the delta between
52
the previous and the current state, leaving listen-
ing modules a choice of how to implement their
internal update.
Modules can be fully event-driven, only trig-
gered into action by being notified of a hypothesis
change, or they can run persistently, in order to cre-
ate endogenous events like time-outs. Event-driven
modules can run concurrently in separate threads or
can be called sequentially by a push buffer (which
may seem to run counter the spirit of incremental
processing, but can be advantageous for very quick
computations for which the overhead of creating
threads should be avoided).
IUs are typed objects, where the base class IU
specifies the links (same-level, grounded-in) that
allow to create the IU network and handles the
assignment of unique IDs. The payload and addi-
tional properties of an IU are specified for the IU?s
type. A design principle here is to make all relevant
information available, while avoiding replication.
For instance, an IU holding a bit of semantic rep-
resentation can query which interval of input data
it is based on, where this information is retrieved
from the appropriate IUs by automatically follow-
ing the grounded-in links. IU networks ground out
in BaseData, which contains user-side input such
as speech from the microphone, derived ASR fea-
ture vectors, camera feeds from a webcam, derived
gaze information, etc., in several streams that can
be accessed based on their timing information.
Besides IU communication as described in the
abstract model, the toolkit also provides a separate
communication track along which signals, which
are any kind of information that is not seen as incre-
mental hypotheses about a larger whole but as infor-
mation about a single current event, can be passed
between modules. This communication track also
follows the observer/listener model, where proces-
sors define interfaces that listeners can implement.
Finally, InproTK also comes with an extensive
set of monitoring and profiling modules which can
be linked into the module network at any point and
allow to stream data to disk or to visualise it online
through a viewing tool (ANON 2009), as well as
different ways to simulate input (e.g., typed or read
from a file) for bulk testing.
Current state and discussion. InproTK is cur-
rently used in our development of an incremental
multimodal conversational system. It is usable in its
current state, but still evolves. We have built and in-
tegrated modules for various tasks (post-processing
of ASR output, symbolic and statistical natural lan-
guage understanding [ANON 2009a,b,c]). The con-
figuration system and the availability of monitoring
and visualisation tools enables us to quickly test
different setups and compare different implementa-
tions of the same tasks.
5 Jindigo
Jindigo is a Java-based framework for implement-
ing and experimenting with incremental dialogue
systems currently being developed at KTH. In
Jindigo, all modules run as separate threads within
a single Java process (although the modules them-
selves may of course communicate with external
processes). Similarly to InproTK, IUs are mod-
elled as typed objects. The modules in the system
are also typed objects, but buffers are not. Instead,
a buffer can be regarded as a set of IUs that are
connected by (typed) same-level links. Since all
modules have access to the same memory space,
they can follow the same-level links to examine
(and possibly alter) the buffer. Update messages
between modules are relayed based on a system
specification that defines which types of update
messages from a specific module go where. Since
the modules run asynchronously, update messages
do not directly invoke methods in other modules,
but are put on the input queues of the receiving
modules. The update messages are then processed
by each module in their own thread.
Jindigo implements a model for updating buffers
that is slightly different than the two previous ap-
proaches. In this approach, IUs are connected by
predecessor links, which gives each IU (words,
widest spanning phrases from the parser, commu-
nicative acts, etc), a position in a (chronologically)
ordered stream. Positional information is reified by
super-imposing a network of position nodes over
the IU network, with the IUs being associated with
edges in that network. These positional nodes then
give us names for certain update stages, and so
revisions can be efficiently encoded by reference
to these nodes. An example can make this clearer.
Figure 2 shows five update steps in the right buffer
of an incremental ASR module. By reference to po-
sitional nodes, we can communicate easily (a) what
the newest committed IU is (indicated in the figure
as a shaded node) and (b) what the newest non-
revoked or active IU is (i.e., the ?right edge? (RE);
indicated in the figure as a node with a dashed line).
So, the change between the state at time t1 and t2
is signalled by RE taking on a different value. This
53
Figure 2: The right buffer of an ASR module, and update
messages at different time-steps.
value (w3) has not been seen before, and so the
consuming module can infer that the network has
been extended; it can find out which IUs have been
added by going back from the new RE to the last
previously seen position (in this case, w2). At t3, a
retraction of a hypothesis is signalled by a return to
a previous state, w2. All consuming modules have
to do now is to return to an internal state linked
to this previous input state. Commitment is repre-
sented similarly through a pointer to the rightmost
committed node; in the figure, that is for example
w5 at t5.
Since information about whether an IU has been
revoked or committed is not stored in the IU it-
self, all IUs can (if desirable) be defined as im-
mutable objects. This way, the pitfalls of having
asynchronous processes altering and accessing the
state of the IUs may be avoided (while, however,
more new IUs have to be created, as compared to
altering old ones). Note also that this model sup-
ports parallel hypotheses as well, in which case the
positional network would turn into a lattice.
The framework supports different types of up-
date messages and buffers. For example, a parser
may incrementally send NPs to a reference reso-
lution (RR) module that has access to a domain
model, in order to prune the chart. Thus, informa-
tion may go both left-to-right and right-to-left. In
the buffer between these modules, the order be-
tween the NPs that are to be annotated is not im-
portant and there is no point in revoking such IUs
(since they do not affect the RR module?s state).
Current state and discussion. Jindigo uses con-
cepts from (Skantze, 2007), but has been rebuilt
from ground up to support incrementality. A range
of modules for ASR, semantic interpretation, TTS,
monitoring, etc., have been implemented within
the framework, allowing us to do experiments
with complete systems interacting with users. We
are currently using the framework to implement a
model of incremental speech production.
6 Discussion
The three implementations of the abstract IU model
presented above show that concrete requirements
and application areas result in different design de-
cisions and focal points.
While BF?s approach is loosely coupled and han-
dles exchange of IUs via shared objects and a me-
diating module, KTH?s implementation is rather
closely coupled and publishes IUs through a single
buffer that lies in shared memory. UP?s approach
is somewhat in between: it abstracts away from the
transportation layer and enables message passing-
based communication as well as shared memory
transparently through one interface.
The differences in the underlying module com-
munication infrastructure affect the way incremen-
tal IU updates are handled in the systems. In BF?s
framework modules holding an IU in one of their
buffers just get notified when one of the IU?s fields
changed. Conversely, KTH?s IUs are immutable
and new information always results in new IUs
being published and a change to the graph repre-
sentation of the buffer?but this allows an efficient
coupling of module states and cheap revoke op-
erations. Again, UP?s implementation lies in the
middle. Here both the whole new state and the delta
between the old and new buffer is communicated,
which leads to flexibility in how consumers can be
implemented, but also potentially to some commu-
nication overhead.
In future work, we will explore if further gener-
alisations can be extracted from the different im-
plementations presented here. For now, we hope
that the reference architectures presented here can
already be an inspiration for further work on incre-
mental conversational systems.
References
Adam Cheyer and David Martin. 2001. The open
agent architecture. Journal of Autonomous Agents
and Multi-Agent Systems, 4(1):143?148, March.
H. Pennington, A. Carlsson, and A. Larsson. 2007.
D-Bus Specification Version 0.12. http://dbus.free-
desktop.org/doc/dbus-specification.html.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of EACL 2009, Athens,
Greece.
Gabriel Skantze. 2007. Error Handling in Spoken Dia-
logue Systems. Ph.D. thesis, KTH, Stockholm, Swe-
den, November.
54
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 88?97,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
 
Regulating Dialogue with Gestures?Towards an Empirically Grounded 
Simulation with Conversational Agents 
 
Kirsten Bergmann1,2 Hannes Rieser1 Stefan Kopp1,2
 
1 Collaborative Research Center 673 ?Alignment in Communication?, Bielefeld University 
2 Center of Excellence ?Cognitive Interaction Technology?(CITEC), Bielefeld University 
   
{kbergman,skopp}@TechFak.Uni-Bielefeld.DE 
hannes.rieser@Uni-Bielefeld.DE 
 
 
 
Abstract 
Although not very well investigated, a crucial as-
pect of gesture use in dialogues is to regulate the 
organisation of the interaction. People use gestures 
decisively, for example to indicate that they want 
someone to take the turn, to 'brush away' what 
someone else said, or to acknowledge others' con-
tributions. We present first insights from a corpus-
based investigation of how gestures are used to 
regulate dialogue, and we provide first results from 
an account to capture these phenomena in agent-
based communication simulations. By advancing a 
model for autonomous gesture generation to also 
cover gesture interpretation, this account enables a 
full gesture turn exchange cycle of generation, un-
derstanding and acceptance/generation in virtual 
conversational agents. 
1 Motivation 
Research on gestures must combine empirical, 
theoretical and simulation methods to investigate 
form, content and function of gestures in relation 
to speech. Our work is based on a corpus of multi-
modal data, the Bielefeld Speech and Gesture 
Alignment corpus of route-description dialogues 
(SAGA corpus, L?cking et al 2010). The point of 
departure of our research has been work on iconic 
and deictic gestures over many years. In this paper 
we focus on a not very well investigated function 
of gestures which we have repeatedly observed in 
this corpus, namely, the regulation of dialogue.  
Most of current gesture research is oriented to-
wards the semiotics of a Peircean tradition as can  
for instance be seen from McNeill?s ?Kendon?s 
continuum? (McNeill 1992, p. 37). As a conse-
quence of this Peircian orientation, gestures have 
been viewed as single signs interfacing with 
speech. Going beyond the integration of in-
put/output modalities in single speech-gesture 
compositions (Johnston and Bangalore, 2005), lit-
tle effort has been spent on the investigation of 
sequences of gestures and speech-gesture composi-
tion both within and across speakers (Hahn and 
Rieser 2010, Rieser 2010). Furthermore, research 
of gesture meaning was restricted to the contribu-
tion of gesture content to propositional content. An 
exception to this research line has been the work of 
Bavelas et al (1992, 1995). It is characterised by 
two features, a functional perspective on gesture in 
opposition to purely classificatory and typological 
ones and an interest to systematically investigate 
the role of gesture in interaction. In particular, 
Bavelas et al (1992) proposed a distinction be-
tween ?topic gestures? and ?interactive gestures?: 
Topic gestures depict semantic information di-
rectly related to the topic of discourse, while inter-
active gestures refer to some aspect of the process 
of conversing with another person. Interactive ges-
tures include delivery gestures (e.g. marking in-
formation status as new, shared, digression), citing 
gestures (acknowledging others? prior contribu-
tions), seeking gestures (seeking agreement, or 
help in finding a word), and turn coordination ges-
 
88
 tures (e.g. taking or giving the turn). Gill et al 
(1999) noted similar functions of gesture use, add-
ing body movements to the repertoire of pragmatic 
acts used in dialogue act theory (e.g. turn-taking, 
grounding, acknowledgements).  
We aim to find out how gestures are related to and 
help regulate the structure of dialogue. We will call 
these gestures `discourse gestures?. Relevant re-
search questions in this respect are the following: 
How can gesture support next speaker selection if 
this follows regular turn distribution mechanisms 
such as current speaker selects next? From the dia-
logues in SAGA we know that averting next 
speaker?s self-selection is of similar importance as 
handing over the floor to the next speaker. So, how 
can averting self-selection of other be accom-
plished gesturally? A still different problem is how 
gesture is utilised to establish an epistemically 
transparent, reliable common ground, say a tight 
world of mutual belief. A precondition for that is 
how gesture can help to indicate a gesturer?s stance 
to the information he provides. Natural language 
has words to indicate degrees of confidence in in-
formation such as probably, seemingly, approxi-
mately, perhaps, believe, know, guess etc. Can ges-
tures acquire this function as well?  
All these issues can be synopsised as follows: How 
can gestures?apart from their manifest contribu-
tion to propositional content?be used to push the 
dialogue machinery forward? In our research, ges-
ture simulation and theory of speech-gesture inte-
gration are developed in tandem. Up to now, both  
have been tied to occurrences of single gestures 
and their embedding in dialogue acts. In this paper, 
we present first steps along both methodological 
strands to explore the use and function of gesture 
in dialogue. We start with an empirical perspective 
on discourse gestures in section 2. In section 3 we 
briefly describe our gesture simulation model 
which so far simulates gesture use employing the 
virtual agent MAX independent of discourse struc-
tures. Section 4 analyses a corpus example of a 
minimal discourse which is regulated mainly by 
gestures of the two interactants. This provides the 
basis for our proposed extension of the gesture 
generation approach to capture the discourse func-
tion of gestures as described in section 5. This ex-
tension will encompass a novel approach to em-
ploy the very generation model used for gesture 
production, and hence all the heuristic gesture 
knowledge it captures, also for gesture interpreta-
tion in dialogue. Section 6 discusses the difference 
between pure interactive gestures and discourse 
gestures and proposes further steps that need to be 
taken to elucidate how gestures are used as a vehi-
cle for regulating dialogue.  
2 Empirical Work on Discourse Gestures 
In looking for discourse gestures we started from 
the rated annotation of 6000 gestures in the SAGA 
corpus. We managed to annotate and rate about 
5000 of them according to traditional criteria using 
practices and fine-grained gesture morphology like 
hand-shape and wrist-movement. About 1000 ges-
tures could not be easily subsumed under the tradi-
tional gesture types (iconics, deictics, metaphorics, 
beats). Furthermore, they were observed to corre-
late with discourse properties such as current 
speaker?s producing his contribution or non-
regular interruption by other speaker.  
For purposes of the classification of the remaining 
1000 gestures we established the following func-
tional working definition: `Discourse gestures? are 
gestures tied up with properties or functions of 
agents? contributions in dialogue such as success-
fully producing current turn, establishing coher-
ence across different speakers? turns by gestural 
reference or indicating who will be next speaker.  
What did we use for dialogue structure? Being fa-
miliar with dialogue models such as SDRT (Asher 
and Lascarides, 2003), PTT (Poesio and Traum, 
1997), and KoS (Ginzburg, 2011) we soon found 
that these were too restricted to serve descriptive 
purposes. So we oriented our ?classification of dia-
logue gesture enterprise? on the well known turn 
taking organisation model of Sacks et al (1974) 
and Levinson?s (1983) discussion of it. However, it 
soon turned out that even these approaches were 
too normative for the SAGA data: This is due to 
the fact that dialogue participants develop enor-
mous creativity in establishing new rules of con-
tent production and of addressing violations of 
prima facie rules.  
Rules of turn-taking, for example, are not hard and 
fast rules, they can be skirted if the need arises, 
albeit there is a convention that this has to be ac-
knowledged and negotiated. A very clear example 
of an allowed interruption of an on-going produc-
tion is a quickly inserted clarification request serv-
ing the communicative goals of current speaker 
and the aims of the dialogue in general. Another 
 
89
 problem with the Sacks et al model consists in the 
following fact: Since its origination many dialogue 
regularities have been discovered which cannot be 
easily founded on a phenomenological or observa-
tional stratum which is essentially semantics-free. 
This can for example be seen from the develop-
ment of the notion of grounding and common 
ground as originally discussed by Stalnaker (1978), 
Clark (1996) and others. Nevertheless, grounding 
(roughly, coming to agree on the meaning of what 
has been said (see e.g. Traum, 1999; Roque and 
Traum, 2008;  Ginzburg 2011, ch. 4.2 for the op-
tions available) generates verbal structure and ver-
bal structure interfaces with gesture. Other exam-
ples in this class are acknowledgements or accepts 
discussed in more detail below. 
How did we decide on which distinctions of ges-
ture annotation have to be used for characterising 
discourse gestures? In other words, how did we 
conceive of the map between gestures of a certain 
sort and discourse structures? First of all we ob-
served that two types of discourse gestures emerge 
from the SAGA data. Some of them come with 
their own global shape and are close to emblems, 
(i.e. conveyors of stable meaning like the victory 
sign). This is true for example of the ?brush aside 
or brush away? gesture shown in Figure 1 (left), 
indicating a gesturer?s assessment of the down-
rated relevance of information, actions or situa-
tions. Discourse gestures of the second class ex-
ploit the means of, for instance, referring gestures 
or iconic gestures. An example of an iconic gesture 
in this role will be discussed to some extent in sec-
tion 4. Its simulation will be described in sections 3 
and 5.  
Here we explain the phenomenon with respect to 
referring pointing gestures which are easier to fig-
ure out (see Figure 1 (right)). Their usage as under 
focus here is not tied to the information under dis-
cussion but to objects in the immediate discourse 
situation, preferably to the participants of the dia-
logue. These uses have a Gricean flavour in the 
following way: Only considerations of relevance 
and co-occurrence with a turn transition relevance 
place together indicate that prima facie not general 
reference is at stake but indication of next speaker 
role. It wouldn?t make sense to point to the other 
person singling her or him out by indexing, be-
cause her or his identity is clear and well estab-
lished through the on-going interaction. Thus we 
see that a gestural device associated with estab-
lished morphological features, pointing, acquires a 
new function, namely indicating the role of next-
speaker. 
Figure 1: Examples of discourse gestures: the brush-away 
gesture (left) and situated pointing to the upper part of the 
interlocutor?s torso (right) used for next speaker selection in 
a ?Gricean? sense (see text for explanation). 
Now both classes of gestures, ?brush away? used 
to indicate informational or other non-relevance 
and pointing, indicating the role of being next 
speaker exploit the motor equipment of the hands. 
For this reason, annotation of discourse gestures 
can safely be based on the classification schemas 
we have developed for practices like indexing, 
shaping or modelling and for the fine-grained mo-
tor behaviour of the hands as exhibited by palm 
orientation, back-of-hand trajectory etc. In work by 
Hahn & Rieser (2009-2011) the following broad 
classes of discourse gestures were established. We 
briefly comment upon these classes of gestures 
found in the SAGA corpus relevant for dialogue 
structure and interaction:  
? Managing of own turn: A speaker may in-
dicate how successful he is in editing out his 
current production.  
? Mechanisms of next-speaker selection as 
proposed in classical CA research, for in-
stance, pointing to the other?s torso is often 
used as a means to indicate next speaker.  
? In grounding acts and feed-back especially 
iconic gestures are used to convey proposi-
tional content. 
? Clarification requests to work on contribu-
tions: An addressee may indicate the need 
for a quick interruption using a pointing to 
demand a clarification. In contrast, a current 
speaker can ward off the addressee?s incipi-
ent interruption using a palm-up gesture di-
 
90
 rected against the intruder thus setting up a 
?fence?.  
? Evidentials for establishing a confidence 
leve: There are fairly characteristic gestures 
indicating the confidence a speaker has in 
the information he is able to convey. 
? Handling of non-canonical moves by dis-
course participants: Interaction sequences 
consisting of attempts by other speaker to in-
terrupt and to thwart this intention by current 
speaker or to give way to it show how dis-
course participants handle non-canonical 
moves.  
? Assessment of relevance by discourse par-
ticipants: Speakers provide an assessment 
of which information is central and which 
one they want to consider as subsidiary. 
? An indication of topical information with 
respect to time, place or objects is fre-
quently given by pointing or by ?placing ob-
jects? into the gesture space.  
 
We know that this list is open and could, more-
over, depend on the corpus. In this paper the focus 
will be on grounding acts and feedback (see sec-
tions 3-5). The reason is that this way we can pro-
vide an extension of existing work on the simula-
tion of gesture production in a fairly direct manner. 
 
3 Simulating Gesture Use: The Genera-
tion Perspective 
Our starting point to simulate gestural behavior in 
dialogue is a gesture generation system which is 
able to simulate speaker-specific use of iconic ges-
tures given (1) a communicative intention, (2) dis-
course contextual information, and (3) an imagistic 
representation of the object to be described. Our 
approach is based on empirical evidence that 
iconic gesture production in humans is influenced 
by several factors. Apparently, iconic gestures 
communicate through iconicity, that is their physi-
cal form depicts object features such as shape or 
spatial properties. Recent findings indicate that a 
gesture?s form is also influenced by a number of 
contextual constraints such as information struc-
ture (see for instance Cassell and Prevost, 1996), or 
the use of more general gestural representation 
techniques such as shaping or drawing is decisive. 
In addition, inter-subjective differences in gestur-
ing are pertinent. There is, for example, wide vari-
ability in how much individuals gesture when they 
speak. Similarly, inter-subjective differences are 
found in preferences for particular representation 
techniques or low-level morphological features 
such as handshape or handedness (Bergmann & 
Figure 2: Schema of a gesture generation network in which 
gesture production choices are considered either 
probabilistically (chance nodes drawn as ovals) or rule-based 
(decision nodes drawn as rectangles). Each choice is 
depending on a number of contextual variables. The links are 
either learned from speaker-specific corpus data (dotted lines) 
or defined in a set of if-then rules (solid lines). 
 
 
Kopp, 2009).  
To meet the challenge of considering general and 
individual patterns in gesture use, we have pro-
posed GNetIc, a gesture net specialised for iconic 
gestures (Bergmann & Kopp, 2009a), in which we 
model the process of gesture formulation with 
Bayesian decision networks (BDNs) that supple-
ment standard Bayesian networks by decision 
nodes. This formalism provides a representation of 
a finite sequential decision problem, combining 
probabilistic and rule-based decision-making. Each 
decision to be made in the formation of an iconic 
gesture (e.g., whether or not to gesture at all or 
which representation technique to use) is repre-
sented in the network either as a decision node 
(rule-based) or as a chance node with a specific 
probability distribution. Factors which contribute 
to these choices (e.g., visuo-spatial referent fea-
tures) are taken as input to the model (see Figure 2) 
The structure of the network as well as local condi-
tional probability tables are learned from the 
SAGA corpus by means of automated machine 
 
 
91
 learning techniques and supplemented with rule-
based decision making. Individual as well as gen-
eral networks are learned from the SAGA corpus 
by means of automated machine learning tech-
niques and supplemented with rule-based decision 
making. So far, three different factors have been 
incorporated into this model: discourse context, the 
previously performed gesture, and features of the 
referent. The latter are extracted from a hierarchi-
cal representation called Imagistic Description 
Trees (IDT), which is designed to cover all deci-
sive visuo-spatial features of objects one finds in 
iconic gestures (Sowa & Wachsmuth, 2009). Each 
node in an IDT contains an imagistic description  
which holds a schema representing the shape of an 
object or object part. Features extracted from this 
representation in order to capture the main charac-
teristics of a gesture?s referent are whether an ob-
ject can be decomposed into detailed subparts 
(whole-part relations), whether it has any symmet-
rical axes, its main axis, its position in the VR 
stimulus, and its shape properties extracted on the 
 are not only present in the 
ly in terms of likeability, competence and 
communicative intent 
 describe the landmark townhall with respect to 
cular speaker) 
sulting in a posterior distribution of probabilities 
nique is decided to be ?drawing?, to be 
basis of so called multimodal concepts (see Berg-
mann & Kopp, 2008). 
Analyzing the GNetIc modelling results enabled us 
to gain novel insights into the production process 
of iconic gestures: the resulting networks for indi-
vidual speakers differ in their structure and in their 
conditional probability distributions, revealing that  
individual differences
overt gestures, but also in the production process 
they originate from.  
The GNetIc model has been extensively evaluated. 
First, in a prediction-based evaluation, the auto-
matically generated gestures were compared 
against their empirically observed counterparts, 
which yielded very promising results (Bergmann & 
Kopp, 2010). Second, we evaluated the GNetIc 
models in a perception-based evaluation study with 
human addressees. Results showed that GNetIc-
generated gestures actually helped to increase the 
perceived quality of object descriptions given by 
MAX. Moreover, gesturing behaviour generated 
with individual speaker networks was rated more 
positive
human-likeness (Bergmann, Kopp & Eyssel, 
2010). 
GNetIc gesture formulation has been embedded in 
a larger production architecture for speech and ges-
ture production. This architecture comprises mod-
ules that carry out content planning, formulation, 
and realisation for speech and gesture separately, 
but in close and systematic coordination (Berg-
mann & Kopp, 2009). To illustrate gesture genera-
tion on the basis of GNetIc models, consider the 
following example starting upon the arrival of a 
message which specifies the 
to
its characteristic properties:  
 
   lmDescrProperty (townhall-1). 
 
Based on this communicative intention, the imag-
istic description of the involved object gets acti-
vated and the agent adopts a spatial perspective 
towards it from which the object is to be described 
(see Figure 3). The representation is analyzed for 
referent features required by the GNetIc model: 
position, main axis, symmetry, number of subparts, 
and shape properties. Regarding the latter, a unifi-
cation of the imagistic townhall-1 representation 
and a set of underspecified shape property repre-
sentations (e.g. for ?longish?, ?round? etc.) reveals 
?U-shaped? as the most salient property to be de-
picted. All evidence available  (referent features, 
discourse context, previous gesture and linguistic 
context) is propagated through the network 
(learned from the data of  one parti
re
for the values in each chance node.  
 
 
Figure 3: The townhall in the virtual world (left) and sche-
matic of the corresponding IDT content (right); activated parts 
are marked. 
 
This way, it is first decided to generate a gesture in 
the current discourse situation at all, the represen-
ation techt
realized with both hands and the pointing hand-
shape ASL-G. Next, the model?s decision nodes 
are employed to decide on the palm and back of 
hand (BoH) orientation as well as movement type 
and direction: as typical in drawing gestures, the 
palm is oriented downwards and the BoH away 
from the speaker?s body. These gesture features are 
combined with a linear movement  consisting of 
two segments per hand (to the right and backwards 
with the right hand; accordingly mirror-
symmetrical with the left hand) to depict the shape 
of the townhall.  
Accompanying speech is generated from selected 
propositional facts using an NLG engine. Syn-
 
92
 chrony between speech and gesture follows co-
expressivity and is set to hold between the gesture 
 
would approach the town-hall and 
 initial 
sequent 
transition 
ore than a repetition of the word 
 
Router: Das ist dann das Rathaus [placing]. 
This is then the townhall [placing]. 
 Das ist ein u-f?rmiges Geb?ude [drawing].  
That is a U-shaped building [drawing]. 
 Du blickst praktisch da rein [shaping].Y
stroke (depicting the U-shape property) and corre-
sponding linguistic element. These values are used 
to fill the slots of a gesture feature matrix which is 
transformed into an XML representation to be real-
ized with the virtual agent MAX (see Figure 4).  
 
 
Figure 4: Specification (left) and realization (right) of an 
autonomously generated drawing gesture which depicts the U-
haped townhall. s
4 Example of a Minimal Discourse 
To start with the analysis of how gestures are not
only employed to carry referential content but also 
to regulate dialogue and discourse, we first present 
a datum from the SAGA corpus showing how the 
Follower?s gesture aligns with the Router?s gesture 
to indicate acknowledgement or accept. The situa-
tion is as follows: the Router describes to the Fol-
lower that he 
how it looks to him. A transcription of the
dialogue passage by the Router and the sub
crucial speech-gesture annotation, including the 
Follower, in ELAN looks as displayed in Figure 5 
(placing, drawing, and shaping are names of anno-
tated gestural representation techniques). 
A short comment on the data might be in order: 
When introducing the townhall as a U-shaped 
building, the Router draws the boundary of it, 
namely a ?U?. He then goes on to describe how the 
on-looker apprehends the building. This is accom-
panied by a forward-oriented direction gesture with 
both hands, mimicking into it. In principle, all the 
information necessary to identify the townhall 
from a front perspective is given by then. There is 
a short pause and we also have a turn 
relevance place here. However, there is no feed-
back by the Follower at this point. Therefore the 
Router selects a typical pattern for self-repairs or 
continuations in German, a that is construction in 
the guise of a propositional apposition. Overlap-
ping the production of kind, he produces a three-
dimensional partial U-shaped object maintaining 
the same perspective as in his first drawing of the 
U-shaped border.  
Observe that the Follower already gives feedback 
after front. The most decisive contribution is the 
Follower?s acknowledgement, however. She imi-
tates the Router?s gesture but from her perspective 
as a potential observer. Also, at the level of single 
form features, she performs the gesture differently. 
(different movement direction, different symmetry) 
The imitating gesture overlaps with her nod and 
her contribution OK. It is important to see that her 
gesture provides m
townhall could possibly give. It refers at the same 
time to the town-hall (standing for a discourse ref-
erent) and provides the information of a U-shape 
indicating property, in other words, it expresses the 
propositional information ?This building being U-
shaped? with this building acting as a definite 
anaphora to the occurrence of a building in the first 
part of the Router?s contribution. Hence, assessed 
from a dialogue perspective the following happens: 
The grounding process triggered by the Follower?s 
acknowledgement amounts to mutual belief among 
Router and Follower that the town hall is U-shaped 
and the approaching on-looker on the route per-
ceives it from the open side of the U. 
o
look practically there into it  [shaping]. 
 Das heisst, es hat vorne so zwei Buchtungen
That is, it has to the front kind of two bulges. 
 und geht hinten zusammen dann.and closes i
the rear then. 
Figure 5: Example showing the Router?s and the Fol-
lower?s gestures and their crucial exchange in terms of 
the Router?s assertion and the Follower?s acknowl-
edgement.  
 
93
 Figure 6: Overview of the production and understanding cycle in the simulation model.
 
5 Extending the Simulation: The Under-
standing-Acceptance/Generation Cycle 
How can we go beyond the simulation of isolated 
speaker-specific gestures towards the generation of 
gestures in dialogues? We build on our findings in 
 the corpus study, briefly taken up here again (see
list in section 2 and the respective comments): 
Gesture helps in structuring the dialogue support-
n
ment of the current speaker?s (Router?s or Fol-
 
re 5 (R1) and the sub-
e fact that the BDN 
-
ing next speaker selection or indicating non-regular 
co tributions of other speaker. It enables assess-
lower?s) communicative intentions by the ad-
dressee, for example of whether the Router wants 
to keep the turn but indicates current memory and 
recapitulation problems thus appealing to the ad-
dressee?s cooperation. In addition, appraisal of the 
reliability of the information given by the Router 
can be read off from some of the Router?s gestures. 
Finally, as shown in section 4, gestures comple-
menting or even replacing verbal information is 
used in acknowledgements. 
Building on these observations, our goal is to 
simulate such dialogic interaction with two virtual 
agents (Router and Follower), each of whom pro-
vided with a speaker-specific GNetIc model. In the 
minimal discourse example Router and Follower  
use similar gestures which, notably, differ with 
respect to some details (e.g. speaker?s perspective). 
In the simulation we essentially capture the 
Router?s contribution in Figu
sequent acknowledgement by the Follower (F1). In 
order to vary the Router?s gesturing behavior we 
use the representation technique of drawing instead 
of shaping in the simulation. 
What we need to extend the model with is an 
analysis of the Follower?s understanding of the 
Router?s gesture. Psychologically plausible but 
beyond commonly specialised technical ap-
proaches, we want to employ the same model of an 
agent?s ?gesture knowledge? for both generating 
and understanding gestures. For an overview of the 
production and understanding cycle see Figure 6.  
Here we can make use of th
formalism allows for two different types of infer-
ence, causal inferences that follow the causal inter 
actions from cause to effect, and diagnostic infer-
ences that allow for introducing evidence for ef-
fects and infer the most likely causes of these ef-
fects. This bi-directional use of BDNs could be 
complementary to approaches of plan/intention 
recognition such as in Geib and Goldman (2003). 
To model a use of gestures for regulation as ob-
served with the Follower F1, the Router agent?s 
gestural activity is set as evidence for the output 
nodes of the Follower?s BDN. A diagnostic infer-
ence then yields the most likely causes, that is, the 
most likely referent properties and values of dis-
course contextual variables. In other words, we 
employ the same speaker-specific GNetIc model 
for generation and for understanding. That is, in
formation about the physical appearance of the 
 
94
 Router?s gesture (as specified in Figure 4) is pro-
vided as evidence for the Follower?s GNetIc model 
revealing?correctly?that the gesture?s representa-
tion technique is ?drawing? and the shape property 
is ?U-shaped?.  
Notably, just as the gesture generation process has 
to make choices between similarly probable alter-
natives, not all diagnostic inferences which are 
drawn by employing the Follower agent?s GNetIc 
model are necessarily in line with the evidence 
from which the Router agent?s gesture was origi-
nally generated. For instance, the communicative 
goal as inferred by the Follower agent is 
?lmDescrPosition? (with a likelihood of .65) in-
simulate such iconic ges-
nts 
gue structure such as 
ext speaker selection or acknowledgement and 
outer?s 
e? 
posed in classical CA research 
back 
 participants 
stead of ?lmDescrProperty?. Nevertheless, the in-
ferred knowledge reveals an underspecified repre-
sentation of the referent (see Figure 7) as well as 
the most likely specification of the discourse con-
text. That way, the Follower agent develops his 
own hypothesis of the Router agent?s communica-
tive goal and the content being depicted gesturally.  
This hypothesis is forwarded to the follower 
agent?s dialogue manager, which responds to such 
declaratives by the Router with an acknowledge-
ment grounding act. Now the very same generation 
process as described in section 3 sets in. The Fol-
lower agent?s feedback is generated by employing 
his GNetIc model for causal inference. The result-
ing gesture is, notably, different from the Router 
agent?s gesture: it is a two-handed shaping gesture 
with handshape ASL-C. Movement type and 
movement features are the same as in the Router 
agent?s drawing gesture. Palm and BoH orientation 
are different due to representation technique spe-
cific patterns which are implemented in the deci-
sion nodes (see Figure 7). This case of using iconic 
gesture for regulating dialogue has been success-
fully implemented using GNetIc and the overall 
production architecture. 
6 Discussion and further research agenda 
In this paper we addressed the dialogue-regulating 
function of gestures. Based on empirical observa-
tions of interactional patterns from the SAGA cor-
pus, the starting points for the simulation of these 
gestures were non-interactional propositional ones 
such as iconics used to describe routes or land-
marks. We achieved to 
tures used in their function as acknowledgeme
shown in section 3 which clearly transcends their 
mere representational task. 
 
 
Figure 7: Imagistic representation of what the Follower un-
derstood from the Router?s gestural depiction of the townhall 
(left) and the simulation of the Follower?s autonomously gen-
erated shaping gesture used as an acknowledgement. 
 
We first note that we draw a distinction between 
gestures relevant for dialo
n
those which focus on influencing the social climate 
among the dialogue participants. We did not have 
many of the latter in SAGA but observed some 
which we classified as ?calming down? and ?don?t 
bother?. In certain communication cultures also 
touching the other?s body is accepted. 
As for a research agenda to elucidate further the 
functions of gestures in dialogue, we do not go too 
deeply into matters of dialogue theory here. We 
already have shown that gestures accompanying 
base-line information, being part of the R
report or the Follower?s uptake can be modelled in 
PTT (Poesio and Rieser 2009, Rieser and Poesio 
2009), if one assumes a unified representation for 
verbal and gestural meaning. Here we concentrate 
on how the simulation work can be pushed forward 
based on theoretical analyses of empirical data.  
Note that on the list of discourse gestures given in 
section 2 the following items are tied to Router?s 
behaviour and can be generated in an autonomous 
fashion: 
? managing of own turn 
? evidentials for establishing a confidence 
level 
? assessment of relevance by discourse par-
ticipants 
? indication of topicality with respect to time, 
place or objects.  
Observe, however, that these will also have an im-
pact on the mental state of the Follower as is e.g., 
obvious for evidentials or the ?brush away gestur
(Figure 1). Relevant for the sequencing of multi-
modal contributions are clearly the following: 
? mechanisms of next-speaker selection as 
pro
? grounding acts and feed
? handling of non-canonical moves by dis-
course
 
95
 ? clarification requests to work on contribu-
tions. 
Th
of adj ing a current and a next 
cep-
tan
this ki ation. 
Ac
Thi r
the
es-
ogue. Personality and 
lletin, 21(4):394?405 
 
 Kopp, S. (2009). Increasing expres-
siveness for virtual agents?Autonomous generation 
d gesture in spatial description tasks. In 
 
n-
n-
  
Ca
Cla
Ge
Gil
gy 
Gin
ress).  
Ha 9-2011): Dialogue Struc-
Ha ch-
ing Gesture 
Lev 983). Pragmatics. Cambridge Uni-
L?c
 M. Kipp et al (Eds.), 
Mc
Poe
nd et al (Eds.), Proceedings of the 13th Work-
Rie
and Wachsmuth (Eds.), 
Rie io, M. (2009). Interactive Gesture in 
Poe ordi-
, 1?89 
Ro  
d-
Sac
king 
Sta  
Sow 9). A computational 
guage and Dialogue, pages 132?
146. Oxford University Press. 
ese are intrinsically involved in the production 
acency pairs, hav
contribution and it is on these that simulation will 
focus on in future work. In combination with an 
information state-based multimodal discourse re-
cord (Traum & Larsson, 2003), the implementated 
cycle of generation, understanding and ac
ce/generation provides the basis for modeling 
nd of gesture-based discourse regul
knowledgments 
esearch is partially ss upported by the DFG in 
 CRC 673 ?Alignment in Communication? and 
the Center of Excellence ?Cognitive Interaction 
Technology?. 
References  
Asher, N. and Lascarides, A. (2003). The Logic of Con-
versation. Cambridge University Press 
Bavelas, J., Chovil, N., Lawrie, D., and Wade, A. 
(1992). Interactive gestures. Discourse Processes, 
15(4):469?491. 
Bavelas, J., Chovil N., Coated, L., Roe, L. (1995). G
tures Specialised for Dial
Social Psychology Bu
Bergmann, K., & Kopp, S. (2010). Modelling the Pro-
duction of Co-Verbal Iconic Gestures by Learning
Bayesian Decision Networks. Applied Artificial In-
telligence, 24(6):530?551. 
Bergmann, K. &
of speech an
Proceedings of AAMAS 2009, pages 361?368.  
Bergmann, K. & Kopp, S. (2009a). GNetIc?Using
Bayesian Decision Networks for iconic gesture ge
eration. In Proceedings of the 9th International Co
ference on Intelligent Virtual Agents, pages 76?89.
rgmann, K., KoppBe , S., and Eyssel, F. (2010). Indi-
vidualized gesturing outperforms average gesturing?
Evaluating gesture production in virtual humans. In 
Proceedings of IVA 2010, pages 104?117, Ber-
lin/Heidelberg. Springer.  
ssell, J. and S. Prevost (1996). Distribution of Seman-
tic Features Across Speech and Gesture by Humans 
and Computers. Proceedings of the Workshop on the 
Integration of Gesture in Language and Speech. 
rk, H.H. (1996). Using Language. CUP 
ib, C., Goldman, R.,(2003). Recognizing Plan/Goal 
Abandonment. In Proceedings of the International 
Joint Conference on Artificial Intelligence (IJCAI), 
pp. 1515?1517. 
l, S. P., Kawamori, M., Katagiri, Y., and Shimojima, 
A. (1999). Pragmatics of body moves. In Proceed-
ings of the 3rd International Cognitive Technolo
Conference, pages 345?358.  
zburg, J. (2011). The Interactive Stance. Meaning 
for Conversation. Oxford University Press (in p
hn, F. and Rieser, H. (200
ture Gestures and Interactive Gestures. Manual, 1st 
version. CRC 673 Working Paper. Bielefeld Univer-
sity 
hn, F. and Rieser, H. (2010): Explaining Spee
Gesture Alignment in MM Dialogue Us
Typology. In P. Lupowski and M. Purver (Eds.), As-
pects of Semantics and Pragmatics of Dialogue. 
SemDial 2010, pp. 99?111. 
inson, St. C. (1
versity Press. 
king, A., Bergmann, K., Hahn, F., Kopp, S., & Rie-
ser, H. (2010): The Bielefeld Speech and Gesture 
Alignment Corpus (SaGA). In
LREC 2010 Workshop: Multimodal Corpora. 
Neill, D. (1992). Hand and Mind. Chicago Univer-
sity Press. 
sio, M. & Rieser, H. (2009). Anaphora and Direct 
Reference: Empirical Evidence from Pointing. In J. 
Edlu
shop on the Semantics and Pragmatics of Dialogue 
(DiaHolmia) (pp. 35?43). Stockholm, Sweden. 
ser, H. (2010). On Factoring out a Gesture Typology 
from the Bielefeld Speech-And-Gesture-Alignment 
Corpus (SAGA). In Kopp 
Proceedings of GW 2009. Springer, pp. 47?61. 
ser, H. & Poes
Dialogue: a PTT Model. In P. Healey et al (Eds.), 
Proceedings of the SIGDIAL 2009 Conference (pp. 
87?96). London, UK: ACL. 
sio, M. and Rieser, H. (2010). Completions, co
nation and alignment in dialogue. Dialogue and Dis-
course 1(1)
Poesio, M. and Traum, D. (1997). Conversational ac-
tions and discourse situations. Computational Intel-
ligence, 13(3): 309?347 
que, A. and Traum, D. (2008). Degrees of Grounding
Based on Evidence of Understanding. In Procee
ings of the 9th SIGdial Workshop on Discourse and 
Dialogue, pp. 54?63 
ks, H., Schegloff, E., Jefferson, G. (1974). A sim-
plest systematics for the organization of turn-ta
for conversation. Language, 50: 696?735 
lnaker, R. (1978): Assertion. In Cole, P. (Ed.) Syntax
and Semantics 9: Pragmatics, pp. 315?322. 
a, T. and Wachsmuth, I. (200
model for the representation an processing of shape 
in coverbal iconic gestures. In K. Coventry et al 
(Eds.), Spatial Lan
 
96
 Traum, D. (1999). Computational models of groundin
in collaborative systems. In Working Notes of AAAI 
Fall Symposium on Psychol
g 
ogical Models of Com-
Tra e 
elt (Eds.), Current and New 
munication, pp. 124?131. 
um, D., & Larsson, S. (2003). The information stat
approach to dialogue management. In R.W. Smith 
and J.C.J. van Kuppev
Directions in Discourse & Dialogue (pp. 325?353). 
Kluwer Academic Publishers. 
 
97
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 12?16,
Utica, May 2012. c?2012 Association for Computational Linguistics
Referring in Installments: A Corpus Study of Spoken Object References
in an Interactive Virtual Environment
Kristina Striegnitz?, Hendrik Buschmeier?and Stefan Kopp?
?Computer Science Department, Union College, Schenectady, NY
striegnk@union.edu
?Sociable Agents Group ? CITEC, Bielefeld University, Germany
{hbuschme,skopp}@techfak.uni-bielefeld.de
Abstract
Commonly, the result of referring expression
generation algorithms is a single noun phrase.
In interactive settings with a shared workspace,
however, human dialog partners often split re-
ferring expressions into installments that adapt
to changes in the context and to actions of their
partners. We present a corpus of human?human
interactions in the GIVE-2 setting in which in-
structions are spoken. A first study of object
descriptions in this corpus shows that refer-
ences in installments are quite common in this
scenario and suggests that contextual factors
partly determine their use. We discuss what
new challenges this creates for NLG systems.
1 Introduction
Referring expression generation is classically consid-
ered to be the problem of producing a single noun
phrase that uniquely identifies a referent (Krahmer
and van Deemter, 2012). This approach is well suited
for non-interactive, static contexts, but recently, there
has been increased interest in generation for situated
dialog (Stoia, 2007; Striegnitz et al, 2011).
Most human language use takes place in dynamic
situations, and psycholinguistic research on human?
human dialog has proposed that the production of
referring expressions should rather be seen as a pro-
cess that not only depends on the context and the
choices of the speaker, but also on the reactions of
the addressee. Thus the result is often not a single
noun phrase but a sequence of installments (Clark
and Wilkes-Gibbs, 1986), consisting of multiple utter-
ances which may be interleaved with feedback from
the addressee. In a setting where the dialog partners
have access to a common workspace, they, further-
more, carefully monitor each other?s non-linugistic
actions, which often replace verbal feedback (Clark
and Krych, 2004; Gergle et al, 2004). The following
example from our data illustrates this. A is instructing
B to press a particular button.
(1) A: the blue button
B: [moves and then hesitates]
A: the one you see on your right
B: [starts moving again]
A: press that one
While computational models of this behavior are still
scarce, some first steps have been taken. Stoia (2007)
studies instruction giving in a virtual environment
and finds that references to target objects are often
not made when they first become visible. Instead in-
teraction partners are navigated to a spot from where
an easier description is possible. Garoufi and Koller
(2010) develop a planning-based approach of this be-
havior. But once their system decides to generate a
referring expression, it is delivered in one unit.
Thompson (2009), on the other hand, proposes a
game-theoretic model to predict how noun phrases
are split up into installments. While Thompson did
not specify how the necessary parameters to calculate
the utility of an utterance are derived from the context
and did not implement the model, it provides a good
theoretical basis for an implementation.
The GIVE Challenge is a recent shared task on sit-
uated generation (Striegnitz et al, 2011). In the GIVE
scenario a human user goes on a treasure hunt in a
virtual environment. He or she has to press a series of
buttons that unlock doors and open a safe. The chal-
lenge for the NLG systems is to generate instructions
in real-time to guide the user to the goal. The instruc-
tions are presented to the user as written text, which
12
means that there is less opportunity for interleaving
language and actions than with spoken instructions.
While some systems generate sentence fragments in
certain situations (e.g., not this one when the user
is moving towards the wrong button), instructions
are generally produced as complete sentences and
replaced with a new full sentence when the context
changes (a strategy which would not work for spoken
instructions). Nevertheless, timing issues are a cause
for errors that is cited by several teams who devel-
oped systems for the GIVE challenge, and generating
appropriate feedback has been an important concern
for almost all teams (see the system descriptions in
(Belz et al, 2011)). Unfortunately, no systematic er-
ror analysis has been done for the interactions from
the GIVE challenges. Anecdotally, however, not re-
acting to signs of confusion in the user?s actions at
all or reacting too late seem to be common causes for
problems. Furthermore, we have found that the strat-
egy of replacing instructions with complete sentences
to account for a change in context can lead to con-
fusion because it seems unclear to the user whether
this new instruction is a correction or an elaboration.
In this paper we report on a study of the com-
municative behavior of human dyads in the GIVE
environment where instead of written text instruction
givers use unrestricted spoken language to direct in-
struction followers through the world. We find that
often multiple installments are used to identify a ref-
erent and that the instruction givers are highly respon-
sive to context changes and the instruction followers?
actions. Our goal is to inform the development of a
generation system that generates object descriptions
in installments while taking into account the actions
of its interaction partner.
2 A corpus of spoken instructions in a
virtual environment
Data collection method The setup of this study
was similar to the one used to collect the GIVE-2
corpus of typed instructions (Gargett et al, 2010).
Instruction followers (IFs) used the standard GIVE-2
client to interact with the virtual environment. In-
struction givers (IGs) could observe the followers?
position and actions in the world using an interactive
map, and they were also provided with the same 3D
view into the scene that the IFs saw on their screen.
Differently from the normal GIVE-2 scenario, the
IGs did not type their instructions but gave spoken
instructions, which were audio recorded as well as
streamed to the IFs over the network. A log of the IFs?
position, orientation and actions that was updated ev-
ery 200ms was recorded in a database.
Participants were recruited in pairs on Bielefeld
University?s campus and received a compensation
of six euros each. They were randomly assigned
to the roles of IG and IF and were seated and in-
structed separately. To become familiar with the task,
they switched roles in a first, shorter training world.
These interactions were later used to devise and test
the annotation schemes. They then played two dif-
ferent worlds in their assigned roles. After the first
round, they received a questionnaire assessing the
quality of the interaction; after the second round, they
completed the Santa Barbara sense of direction test
(Hegarty et al, 2006) and answered some questions
about themselves.
Annotations The recorded instructions of the IGs
were transcribed and segmented into utterances (by
identifying speech pauses longer than 300ms) using
Praat (Boersma and Weenink, 2011). We then created
videos showing the IGs? map view as well as the IFs?
scene view and aligned the audio and transcriptions
with them. The data was further annotated by the first
two authors using ELAN (Wittenburg et al, 2006).
Most importantly for this paper, we classified ut-
terances into the following types:
(i) move (MV) ? instruction to turn or to move
(ii) manipulate (MNP) ? instruction to manipulate an object
(e.g., press a button)
(iii) reference (REF) ? utterance referring to an object
(iv) stop ? instruction to stop moving
(v) warning ? telling the user to not do something
(vi) acknowledgment (ACK) ? affirmative feedback
(vii) communication management (CM) ? indicating that the
IG is planning (e.g., uhmm, just a moment, sooo etc.)
(viii) negative acknowledgment ? indicating a mistake on the
player?s part
(ix) other ? anything else
A few utterances which contained both move and
press instructions were further split, but in general
we picked the label that fit best (using the above list
as a precedence order to make a decision if two labels
fit equally well). The inter-annotator agreement for
utterance types was ? = 0.89 (Cohen?s kappa), which
13
is considered to be very good. Since the categories
were of quite different sizes (cf. Table 1), which may
skew the ? statistic, we also calculated the kappa per
category. It was satisfactory for all ?interesting? cate-
gories. The agreement for category REF was ? = 0.77
and the agreement for other was ? = 0.58. The kappa
values for all other categories were 0.84 or greater.
We reviewed all cases with differing annotations and
reached a consensus, which is the basis for all results
presented in this paper. Furthermore, we collapsed
the labels warning, negative acknowledgment and
other which only occurred rarely.
To support a later more in depth analysis, we also
annotated what types of properties are used in object
descriptions, the givenness status of information in
instructions, and whether an utterance is giving pos-
itive or negative feedback on a user action (even if
not explicitly labeled as (negative) acknowledgment).
Finally, information about the IF?s movements and
actions in the world as well as the visible context was
automatically calculated from the GIVE log files and
integrated into the annotation.
Collected data We collected interactions between
eight pairs. Due to failures of the network connection
and some initial problems with the GIVE software,
only four pairs were recorded completely, so that
we currently have data from eight interactions with
four different IGs. We are in the process of collect-
ing additional data in order to achieve a corpus size
that will allow for a more detailed statistical analy-
sis. Furthermore, we are collecting data in English
to be able to make comparisons with the existing
corpus of written instructions in the GIVE world and
to make the data more easily accessible to a wider
audience. The corpus will be made freely available
at http://purl.org/net/sgive-corpus.
Participants were between 20 and 30 years old and
all of them are native German speakers. Two of the
IGs are male and two female; three of the IFs are
female. The mean length of the interactions is 5.24
minutes (SD= 1.86), and the IGs on average use 325
words (SD = 91).
Table 1 gives an overview of the kinds of ut-
terances used by the IGs. While the general pic-
ture is similar for all speakers, there are statisti-
cally significant differences between the frequen-
cies with which different IGs use the utterance types
Table 1: Overall frequency of utterance types.
utterance type count %
MV 334 46.58
MNP 66 9.21
REF 65 9.07
stop 38 5.30
ACK 92 12.83
CM 97 13.53
other 25 3.49
Table 2: Transitional probabilities for utterance types.
M
V
M
N
P
R
E
F
st
op
A
C
K
C
M
ot
he
r
IF pr
es
s
MV .53 .08 .06 .06 .15 .08 .03 .00
MNP .02 .03 .09 .02 .02 .02 .02 .80
REF .00 .33 .19 .02 .14 .00 .02 .30
stop .47 .03 .18 .03 .03 .16 .11 .00
ACK .64 .08 .09 .03 .01 .10 .00 .05
CM .53 .05 .10 .08 .01 .18 .05 .00
other .44 .04 .12 .12 .08 .16 .00 .04
IF press .21 .01 .00 .01 .36 .36 .04 .00
(?2 = 78.82, p ? 0.001). We did not find a signifi-
cant differences (in terms of the utterance types used)
between the two worlds that we used or between the
two rounds that each pair played.
3 How instruction givers describe objects
We now examine how interaction partners establish
what the next target button is. Overall, there are 76
utterance sequences in the data that identify a target
button and lead to the IF pressing that button. We
discuss a selection of seven representative examples.
(2) IG: und dann dr?ckst du den ganz rechten Knopf den
blauen (and then you press the rightmost button the
blue one; MNP)
IF: [goes across the room and does it]
In (2) the IG generates a referring expression iden-
tifying the target and integrates it into an object ma-
nipulation instruction. In our data, 55% of the tar-
get buttons (42 out of 76) get identified in this way
(which fits into the traditional view of referring ex-
pression generation). In all other cases a sequence of
at least two, and in 14% of the cases more than two,
utterances is used.
The transitional probabilities between utterance
types shown in Table 2 suggest what some common
patterns may be. For example, even though move
instructions are so prevalent in our data, they are
uncommon after reference or manipulate utterances.
14
Instead, two thirds of the reference utterances are
followed by object manipulation instruction, another
reference or an acknowledgement. In the remaining
cases, IFs press a button in response to the reference.
(3) IG: vor dir der blaue Knopf (in front of you the blue button;
REF)
IF: [moves across the room toward the button]
IG: drauf dr?cken (press it; MNP)
(4) IG: und auf der rechten Seite sind zwei rote Kn?pfe (and
on the right are two red buttons; REF)
IF: [turns and starts moving towards the buttons]
IG: und den linken davon dr?ckst du (and you press the left
one; MNP)
In (3) and (4) a first reference utterance is followed
by a separate object manipulation utterance. While
in (3) the first reference uniquely identifies the target,
in (4) the first utterance simply directs the player?s
attention to a group of buttons. The second utterance
then picks out the target.
(5) IG: dreh dich nach links etwas (turn left a little; MV)
IF: [turns left] there are two red buttons in front of him
(and some other red buttons to his right)
IG: so, da siehst du zwei rote Schalter (so now you see two
red buttons; REF)
IF: [moves towards buttons]
IG: und den rechten davon dr?ckst du (and you press the
right one; MNP)
IF: [moves closer, but more towards the left one]
IG: rechts (right; REF)
Stoia (2007) observed that IGs use move instruc-
tions to focus the IF?s attention on a particular area.
This is also common in our data. For instance in (5),
the IF is asked to turn to directly face the group of
buttons containing the target. (5) also shows how IGs
monitor their partners? actions and respond to them.
The IF is moving towards the wrong button causing
the IG to repeat part of the previous description.
(6) IG: den blauen Schalter (the blue button; REF)
IF: [moves and then stops]
IG: den du rechts siehst (the one you see on your right;
REF)
IF: [starts moving again]
IG: den dr?cken (press that one; MNP)
Similarly, in (6) the IG produces an elaboration
when the IF stops moving towards the target, indicat-
ing her confusion.
(7) IG: und jetzt rechts an der (and now to the right on the;
REF)
IF: [turns right, is facing the wall with the target button]
IG: ja . . . genau . . . an der Wand den blauen Knopf (yes
. . . right . . . on the wall the blue button; ACK, REF)
IF: [moves towards button]
IG: einmal dr?cken (press once; MNP)
In (7) the IG inserts affirmative feedback when
the IF reacts correctly to a portion of his utterance.
As can be seen in Table 2, reference utterances are
relatively often followed by affirmative feedback.
(8) IF: [enters room, stops, looks around, ends up looking at
the target]
IG: ja genau den gr?nen Knopf neben der Lampe dr?cken
(yes right, press the green button next to the lamp;
MNP)
IGs can also take advantage of IF actions that are
not in direct response to an utterance. This happens
in (8). The IF enters a new room and looks around.
When she looks towards the target, the IG seizes the
opportunity and produces affirmative feedback.
4 Conclusions and future work
We have described a corpus of spoken instructions in
the GIVE scenario which we are currently building
and which we will make available once it is com-
pleted. This corpus differs from other corpora of task-
oriented dialog (specifically, the MapTask corpus
(Anderson et al, 1991), the TRAINS corpus (Hee-
man and Allen, 1995), the Monroe corpus (Stent,
2000)) in that the IG could observe the IF?s actions
in real-time. This led to interactions in which in-
structions are given in installments and linguistic and
non-linguistic actions are interleaved.
This poses interesting new questions for NLG sys-
tems, which we have illustrated by discussing the
patterns of utterance sequences that IGs and IFs use
in our corpus to agree on the objects that need to
be manipulated. In line with results from psycholin-
guistics, we found that the information necessary to
establish a reference is often expressed in multiple
installments and that IGs carefully monitor how their
partners react to their instructions and quickly re-
spond by giving feedback, repeating information or
elaborating on previous utterance when necessary.
The NLG system thus needs to be able to de-
cide when a complete identifying description can
be given in one utterance and when a description in
installments is more effective. Stoia (2007) as well
as Garoufi and Koller (2010) have addressed this
question, but their approaches only make a choice be-
tween generating an instruction to move or a uniquely
identifying referring expression. They do not con-
sider cases in which another type of utterance, for
instance, one that refers to a group of objects or gives
15
an initial ambiguous description, is used to draw the
attention of the IF to a particular area and they do not
generate referring expressions in installments.
The system, furthermore, needs to be able to in-
terpret the IF?s actions and decide when to insert an
acknowledgment, elaboration or correction. It then
has to decide how to formulate this feedback. The
addressee, e.g., needs to be able to distinguish elabo-
rations from corrections. If the feedback was inserted
in the middle of a sentence, if finally has to decide
whether this sentence should be completed and how
the remainder may have to be adapted.
Once we have finished the corpus collection, we
plan to use it to study and address the questions dis-
cussed above. We are planning on building on the
work by Stoia (2007) on using machine learning tech-
niques to develop a model that takes into account var-
ious contextual factors and on the work by Thompson
(2009) on generating references in installments. The
set-up under which the corpus was collected, further-
more, lends itself well to Wizard-of-Oz studies to test
the effectiveness of different interactive strategies for
describing objects.
Acknowledgments This research was supported
by the Deutsche Forschungsgemeinschaft (DFG) in
the Center of Excellence in ?Cognitive Interaction
Technology? (CITEC) and by the Skidmore Union
Network which was funded through an ADVANCE
grant from the National Science Foundation.
References
Anne H. Anderson, Miles Bader, Ellen Gurman Bard, Eliz-
abeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen
Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller,
Catherine Sotillo, Henry S. Thompson, and Regina
Weinert. 1991. The HCRC map task corpus. Lan-
guage and Speech, 34:351?366.
Anja Belz, Albert Gatt, Alexander Koller, and Kristina
Striegnitz, editors. 2011. Proceedings of the Genera-
tion Challenges Session at the 13th European Workshop
on Natural Language Generation, Nancy, France.
Paul Boersma and David Weenink. 2011. Praat: doing
phonetics by computer. Computer program. Retrieved
May 2011, from http://www.praat.org/.
Herbert H. Clark and Meredyth A. Krych. 2004. Speaking
while monitoring addressees for understanding. Jour-
nal of Memory and Language, 50:62?81.
Herbert H Clark and Deanna Wilkes-Gibbs. 1986. Refer-
ring as a collaborative process. Cognition, 22:1?39.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus
of giving instructions in virtual environments. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC?10), pages
2401?2406, Valletta, Malta.
Konstantina Garoufi and Alexander Koller. 2010. Au-
tomated planning for situated natural language gener-
ation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1573?1582, Uppsala, Sweden.
Darren Gergle, Robert E. Kraut, and Susan R. Fussell.
2004. Action as language in a shared visual space. In
Proceedings of the 2004 ACM Conference on Computer
Supported Cooperative Work, pages 487?496, Chicago,
IL.
Peter A. Heeman and James Allen. 1995. The Trains 93
dialogues. Technical Report Trains 94-2, Computer Sci-
ence Department, University of Rochester, Rochester,
NY.
Mary Hegarty, Daniel R. Montello, Anthony E. Richard-
son, Toru Ishikawa, and Kristin Lovelace. 2006. Spa-
tial abilities at different scales: Individual differences in
aptitude-test performance and spatial-layout learning.
Intelligence, 34:151?176.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38:173?218.
Amanda Stent. 2000. The Monroe corpus. Technical
Report 728/TN 99-2, Computer Science Department,
University of Rochester, Rochester, NY.
Laura Stoia. 2007. Noun Phrase Generation for Situated
Dialogs. Ph.D. thesis, Graduate School of The Ohio
State University, Columbus, OH.
Kristina Striegnitz, Alexandre Denis, Andrew Gargett,
Konstantina Garoufi, Alexander Koller, and Mari?t The-
une. 2011. Report on the second second challenge on
generating instructions in virtual environments (GIVE-
2.5). In Proceedings of the Generation Challenges
Session at the 13th European Workshop on Natural
Language Generation, pages 270?279, Nancy, France.
Will Thompson. 2009. A Game-Theoretic Model of
Grounding for Referential Communication Tasks. Ph.D.
thesis, Northwestern University, Evanston, IL.
Peter Wittenburg, Hennie Brugman, Albert Russel, Alex
Klassmann, and Han Sloetjes. 2006. ELAN: A pro-
fessional framework for multimodality research. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), pages
1556?1559, Genoa, Italy.
16
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 295?303,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Combining Incremental Language Generation and
Incremental Speech Synthesis for Adaptive Information Presentation
Hendrik Buschmeier1, Timo Baumann3, Benjamin Dosch, Stefan Kopp1, David Schlangen2
1Sociable Agents Group, CITEC and Faculty of Technology, Bielefeld University
2Dialogue Systems Group, Faculty of Linguistics and Literary Studies, Bielefeld University
{hbuschme,bdosch,skopp,david.schlangen}@uni-bielefeld.de
3Natural Language Systems Division, Department of Informatics, University of Hamburg
baumann@informatik.uni-hamburg.de
Abstract
Participants in a conversation are normally re-
ceptive to their surroundings and their inter-
locutors, even while they are speaking and can,
if necessary, adapt their ongoing utterance. Typ-
ical dialogue systems are not receptive and can-
not adapt while uttering. We present combin-
able components for incremental natural lan-
guage generation and incremental speech syn-
thesis and demonstrate the flexibility they can
achieve with an example system that adapts to
a listener?s acoustic understanding problems
by pausing, repeating and possibly rephrasing
problematic parts of an utterance. In an eval-
uation, this system was rated as significantly
more natural than two systems representing the
current state of the art that either ignore the
interrupting event or just pause; it also has a
lower response time.
1 Introduction
Current spoken dialogue systems often produce pre-
scripted system utterances or use templates with vari-
able substitution during language generation. If a
dialogue system uses grammar-based generation at
all, it produces complete utterances that are then syn-
thesised and realised in one big chunk. As systems
become increasingly more conversational, however,
the need arises to make output generation1 more flex-
ible. In particular, capabilities for incrementally gen-
erating output become desirable, for two kinds of
reasons.
(a) In situations where fast system responses are
important, production of output can begin before the
1We will use the term ?output generation? here to cover both
natural language generation and speech synthesis.
content that is to be presented is fully specified ? even
if what is being produced is just a turn-taking signal
(Skantze and Hjalmarsson, 2010).
(b) A system that produces its output incrementally
can react to events happening while it is realising an
utterance. This can be beneficial in domains where
the state of the world that the system relays informa-
tion about can change mid-utterance, so that a need
may arise to adapt while speaking. It should also
improve naturalness by allowing the system to react
to dialogue phenomena such as concurrent feedback
signals from the user (Buschmeier and Kopp, 2011).
We present work towards enabling such capabil-
ities. We have implemented and connected a com-
ponent for incremental natural language genera-
tion (iNLG) that works with specifications of sub-
utterance-sized communicative intentions and a com-
ponent for incremental speech synthesis (iSS) that can
handle sub-utterance-sized input and modifications
to not-yet-spoken parts of the utterance with very low
latencies. To explore whether such an output genera-
tion capability can indeed be advantageous, we have
created a test system that can react to random noise
events that occur during a system utterance by repeat-
ing and modifying the last sub-utterance chunk. In
an evaluation, we found that this system is in general
more reactive than a non-incremental variant and that
humans rate its behaviour to be more natural than
two non-incremental and non-responsive systems.
2 Related Work
Psycholinguistic research has identified incremen-
tality as an important property of human language
production early on and it has been incorporated into
several models (e. g., Kempen and Hoenkamp, 1987;
295
Levelt, 1989). Guhe (2007) presents a computational
model of incremental conceptualisation. However,
work on iNLG itself is rare, partly because NLG re-
search focusses on text (instead of spoken language).
Notable exceptions are the in-depth analysis of
requirements for and properties of incremental gen-
eration by Kilger and Finkler (1995), who describe
the LTAG-based incremental syntactic generator VM-
GEN. It takes incremental input, processes it and pro-
duces output as soon as at least a prefix of the final
sentence is syntactically complete. If VM-GEN no-
tices that it committed itself to a prefix too early, it
can initiate an overt repair. More recently, Skantze
and Hjalmarsson (2010) presented a system that per-
forms incremental generation in the context of a spo-
ken dialogue system. It can already start to produce
output when the user has not yet finished speaking
and only a preliminary interpretation exists. By flexi-
bly changing what to say and by being able to make
self-repairs the system can recover from situations
where it selected and committed on an inadequate
speech plan. Both systems, however, are not able
to flexibly adapt the language that they generate to
changing requirements due to changes in the situation
or changing needs on the side of the user.
Real-time on-the-fly control of speech synthesis
is rare, especially the full integration into a dialogue
system. Matsuyama et al (2010) describe a system
that feeds back to the dialogue system the word at
which it has been interrupted by a barge-in. Edlund
(2008) additionally enables a system to continue at
the point where it was interrupted. He also outlines
some requirements for incremental speech synthe-
sis: to give constant feedback about what has been
delivered, to be interruptible (and possibly continue
from that position), and to run in real time. Edlund?s
system, which uses diphone synthesis, performed
non-incrementally before delivery starts. We go be-
yond this in also enabling changes during delivery
and conducting synthesis steps just-in-time.
Dutoit et al (2011) present an incremental HMM
optimiser which allows to change pitch and tempo
of upcoming phonemes. However, as that system is
fed from a (non-incrementally produced) label file, it
cannot easily be used in an incremental system.
A predecessor of our iSS component (which was
not yet fully incremental on the HMM level) is de-
scribed in detail in (Baumann and Schlangen, 2012a).
3 Incremental and Adaptive NLG
3.1 The SPUD microplanning framework
The NLG component presented here is based on
the SPUD microplanning framework (Stone et al,
2003) and realised in DeVault?s (2008) implemen-
tation ?Java SPUD?. SPUD frames microplannig as
a constraint satisfaction problem, solving the tasks
that are involved in generating a sentence (lexical
and syntactic choice, referring expression generation
and aggregation) in an integrated manner. Genera-
tion starts from a communicative goal that specifies
constraints for the final utterance. The generation pro-
cess is further shaped by (a) general constraints that
model pragmatic properties of language use such as
the Gricean maxims (a principle called ?textual econ-
omy?); (b) specific constraints imposed through the
communicative status of the propositions to be com-
municated (i. e., what knowledge can be presupposed
and what needs to be communicated explicitly); and
(c) linguistic resources (a context-free tree rewriting
formalism based on LTAG; Stone, 2002).
To deal efficiently with the infinite search space
spanned by the linguistic resources, SPUD uses a
heuristic search algorithm to find an utterance that
satisfies the imposed constraints (Stone et al, [2003]
describe the heuristic function). In each search step,
the algorithm expands the ?provisional? utterance by
adding the linguistic resource that maximally reduces
the estimated distance to the final utterance.
If the generation process runs into a dead-end state,
it could in principle deal with the situation by track-
ing back and expanding a different branch. This,
however, is impractical, as it becomes impossible
to project when ? if at all ? generation will finish.
Hence, in that case, SPUD stops without providing a
result, delegating the problem back to the preceding
component in the generation pipeline.
3.2 Partially incremental generation
SPUD generates utterances incrementally in the sense
that the completeness of the provisional utterance
increases monotonically with every step. This, how-
ever, does not mean that the surface structure of pro-
visional utterances is constructed incrementally (i. e.,
from left to right) as well, which would only be pos-
sible, if (a) the search strategy would always expand
the leftmost non-lexicalised node in the provisional
296
Utterance IC
1
IC
2
IC
n
 ?
Utterance
outline
IMPT
1
IMPT
2
IMPT
n
 ?
  MCP
? {U
1
, ?}
? KB
1
? {U
i
, ?}
? KB
2
? {U
k
, ?}
? KB
n
  MPP
 ?state
t
Figure 1: Incremental microplanning consists of two pro-
cesses, micro content planning (MCP) and microplanning-
proper (MPP). The former provides incremental microplan-
ning tasks from an utterance outline to the latter, which
incrementally transforms them into communicative intent
and intonation unit-sized chunks of natural language.
utterance first and if (b) the linguistic resources are
specified (and ordered) in a way that allows left-to-
right expansion of the trees in all possible situations.
In practice, both requirements are difficult to meet
and full word-by-word incrementality in natural lan-
guage microplanning is not within reach in the SPUD
framework. Because of this, we take a slightly more
coarse grained approach to incremental microplan-
ning and choose chunks of the size of intonation
phrases instead of words as our incremental units.
We say that our microplanner does ?partially incre-
mental generation?.
Our incremental microplanner comprises two inter-
acting processes, micro content planning and micro-
planning-proper (MCP and MPP; schematised in Fig-
ure 1), each of which fulfils a distinct task and oper-
ates on different structures.
MCP takes as input utterance outlines that describe
the communicative goal (a set of desired updates Ux)
intended to be communicated in an utterance and the
presuppositions and private knowledge needed to do
so. Importantly, utterance outlines specify how the
communicative goal can be decomposed into an or-
dered list of incremental microplanning-tasks IMPTx.
Each such task comprises (a) a subset of the commu-
nicative goal?s desired updates that belong together
and fit into one intonation unit sized chunk of speech
and (b) knowledge KBx used in generation.
MPP takes one incremental microplanning-task at
a time and uses SPUD to generate the IMPT?s commu-
nicative intent as well as its linguistic surface form
ICx. The communiciative intent is added to a repre-
sentation (?state? in Figure 1) that is shared between
the two processes. While processing the IMPTs of
an utterance outline, MCP can access this representa-
tion, which holds information about all the desired
updates that were achieved before, and thus knows
that a desired update that is shared between subse-
quent IMPTs has already been communicated. MPP
can also take this information into account during
generation. This makes it possible that an utterance
is coherent and adheres to pragmatic principles even
though generation can only take local decisions.
3.3 Adaptive generation
Being able to generate language in sub-utterance
chunks makes it possible to dynamically adapt later
increments of an utterance to changes in the situa-
tion that occur while the utterance is being realised.
Decisions about these adaptations need not be taken
almost until the preceding increment finishes, mak-
ing the generation process very responsive. This is
important to be able to deal with interactive dialogue
phenomena, such as communicative feedback of the
interlocutor (Allwood et al, 1992) or compound con-
tributions (Howes et al, 2011), in a timely manner.
Adaptation may happen in both parts of incremen-
tal microplanning. In MCP, adaptation takes the form
of dynamically changing the choice of which IMPT to
generate next or changing the internal structure of an
IMPT; adaptation in MPP changes the choices the gen-
eration process makes while transforming IMPTs into
communicative intent and surface form. Adaptation
in MCP is triggered top-down, by higher-level pro-
cesses such as dialogue management. Adaptation in
MPP on the other hand depends on the task given and
on the status of the knowledge used during generation.
The details are then governed by global parameter
settings MPP uses during generation.
If there is, for example, reason for the system to
believe that the current increment was not commu-
nicated clearly because of noise in the transmission
channel, the MCP process might delay future IMPTs
and initiate a repair of the current one by re-inserting
it at the beginning of the list of upcoming IMPTs of
this utterance outline. The MPP process? next task
is then to re-generate the same IMPT again. Due to
297
Table 1: Surface forms generated from the same IMPT (de-
sired updates = {hasSubject(event6, ?Vorlesung
Linguistik?)}; KB = {event6}) but with different
levels of verbosity.
Verbosity Generated sub-utterance chunk
0 ?Vorlesung Linguistik?
(lecture Linguistcs)
1 ?Betreff: Vorlesung Linguistik?
(subject: lecture Linguistics)
2 ?mit dem Betreff Vorlesung Linguistik?
(with the subject: lecture Linguistics)
changes in the state information and situation that
influence microplanning, the resulting communica-
tive intent and surface form might then differ from
its previous result.
3.4 Adaptation mechanisms
As a proof of concept, we integrated several adapta-
tion mechanism into our NLG-microplanning system.
The goal of these mechanisms is to respond to a dia-
logue partner?s changing abilities to perceive and/or
understand the information the system wants to con-
vey. Some of the mechanisms operate on the level of
MCP, others on the level of MPP. The mechanisms are
implemented either with the knowledge and its con-
versational status used in generation or by altering
the decision structure of SPUD?s search algorithm?s
heuristic function. Similar to the approach of flexi-
ble NLG described by Walker et al (2007), most of
the mechanism are conditioned upon individual flags,
that in our case depend on a numeric value that repre-
sents the level of understanding the system attributes
to the user. Here we describe the two most relevant
mechanisms used to adapt verbosity and redundancy.
Verbosity The first mechanism aims at influenc-
ing the length of a sub-utterance chunk by making
it either more or less verbose. The idea is that actual
language use of human speakers seldom adheres to
the idealised principle of textual economy. This is
not only the case for reasons of cognitive constraints
during speech production, but also because words
and phrases that do not contribute much to an utter-
ance?s semantics can serve a function, for example by
drawing attention to specific aspects of an utterance
or by giving the listener time to process.
To be able to vary utterance verbosity, we anno-
tated the linguistic resources in our system with val-
ues of their verbosity (these are hand-crafted similar
to the rule?s annotation with production costs). Dur-
ing generation in MPP the values of all linguistic re-
sources used in a (provisional) utterance are added up
and used as one factor in SPUD?s heuristic function.
When comparing two provisional utterances that only
deviate in their verbosity value, the one that is nearer
to a requested verbosity level is chosen. Depend-
ing on this level, more or less verbose constructions
are chosen and it is decided whether sub-utterance
chunks are enriched with additional words. Table 1
shows the sub-utterance chunk ?Betreff: Vorlesung
Linguistik? (subject: lecture Linguistics) generated
with different levels of verbosity.
Redundancy The second adaptation mechanism is
redundancy. Again, redundancy is something that an
ideal utterance does not contain and by design SPUD
penalises the use of redundancy in its heuristic func-
tion. Two provisional utterances being equal, the one
exhibiting less redundancy is normally preferred. But
similar to verbosity, redundancy serves communica-
tive functions in actual language use. It can highlight
important information, it can increase the probability
of the message being understood (Reiter and Sripada,
2002) and it is often used to repair misunderstanding
(Baker et al, 2008).
In incremental microplanning, redundant informa-
tion can be present both within one sub-utterance
chunk (e. g., ?tomorrow, March 26, . . . ? vs. ?tomorrow
. . . ?) or across IMPTs. For the former case, we modi-
fied SPUD?s search heuristic in order to conditionally
either prefer an utterance that contains redundant in-
formation or an utterance that only contains what is
absolutely necessary. In the latter case, redundancy
only becomes an option when later IMPTs enable the
choice of repeating information previously conveyed
and therefore already established as shared knowl-
edge. This is controlled via the internal structure of
an IMPT and thus decided on the level of MCP.
4 Incremental Speech Synthesis
In this section we describe our component for incre-
mental speech synthesis. We extend Edlund?s (2008)
requirements specification cited in Section 2, requir-
ing additionally that an iSS supports changes to as-yet
298
unspoken parts of an ongoing utterance.
We believe that the iSS?s requirements of inter-
ruptability, changeability, responsiveness, and feed-
back are best resolved by a processing paradigm in
which processing takes place just-in-time, i. e., tak-
ing processing steps as late as possible such as to
avoid re-processing if assumptions change. Before
we describe these ideas in detail, we give a short
background on speech synthesis in general.
4.1 Background on speech synthesis
Text-to-speech (TTS) synthesis functions in a top-
down processing approach, starting on the utterance
level and descending onto words and phonemes, in
order to make good decisions (Taylor, 2009). For
example, top-down modelling is necessary to assign
stress patterns and sentence-level intonation which
ultimately lead to pitch and duration contours, and to
model co-articulation effects.
TTS systems start out assigning intonation patterns
to the utterance?s words and then generate a target
phoneme sequence which is annotated with the tar-
gets? durations and pitch contour; all of this is called
the linguistic pre-processing step. The synthesis step
proper can be executed in one of several ways with
HMM-based and unit-selection synthesis currently
producing the perceptually best results.
In HMM-based synthesis, the target sequence is
first turned into a sequence of HMM states. A global
optimisation then determines a stream of vocoding
features that optimise both HMM emission probabili-
ties and continuity constraints (Tokuda et al, 2000).
The stream may also be enhanced to consider global
variance of features (Toda and Tokuda, 2007). The
parameter frames are then fed to a vocoder which
generates the final speech audio signal.
Unit-selection, in contrast, searches for the best
sequence of (variably sized) units of speech in a
large, annotated corpus, aiming to find a sequence
that closely matches the target sequence while having
few and if possible smooth joints between units.
We follow the HMM-based approach for our com-
ponent for the following reasons: (a) even though
only global optimisation is optimal for both tech-
niques, the influence of look-ahead on the continuity
constraints of HMM-based synthesis is linear leading
to a linear loss in optimality with smaller look-aheads
(whereas unit-selection with limited look-ahead may
Figure 2: Hierarchical structure of incremental units de-
scribing an example utterance as it is being produced
during delivery.
jump erratically between completely different unit se-
quences). (b) HMM-based synthesis nicely separates
the production of vocoding parameter frames from
the production of the speech audio signal which al-
lows for fine-grained concurrent processing (see next
subsection). (c) Parameters in the vocoding frames
are partially independent. This allows us to indepen-
dently manipulate, e. g., pitch without altering other
parameters or deteriorating speech quality (in unit-
selection, a completely different unit sequence might
become optimal even for slight changes of pitch).
4.2 Incrementalising speech synthesis
As explained in the previous subsection, speech syn-
thesis is performed top-down, starting at the utterance
and progressing down to the word, target and finally,
in the HMM approach, vocoding parameter and signal
processing levels. It is, however, not necessary that
all details at one level of processing are worked out
before starting to process at the next lower level. To
be precise, some syntactic structure is sufficient to
produce sentence-level intonation, but all words need
not be known. Likewise, post-lexical phonological
processes can be computed as long as a local context
of one word is available and vocoding parameter com-
putation (which must model co-articulation effects)
should in turn be satisfied with about one phoneme of
context. Vocoding itself does not need any lookahead
at all (aside from audio buffering considerations).
Thus, we generate our data structures incremen-
tally in a top-down and left-to-right fashion with dif-
ferent amounts of pre-planning and we do this using
several processing modules that work concurrently.
This results in a ?triangular? structure as shown in
299
Figure 2. At the top stands a pragmatic plan for the
full utterance from which a syntactic plan can be de-
vised. This plan is filled with words, as they become
available. On the vocoding parameter level, only a
few frames into the future have been computed so
far ? even though much more context is already avail-
able. That is, we generate structure just-in-time, only
shortly before it is needed by the next processor. This
holds very similarly for the vocoding step that pro-
duces the speech signal just-in-time.
The just-in-time processing approach, combined
with the increasing temporal granularity of units to-
wards the lower levels has several advantages: (a) lit-
tle utterance-initial processing (only what is neces-
sary to produce the beginning of the signal) allows for
very responsive systems; and (b) changes to the ini-
tial plan result only in a modest processing overhead
because little structure has to be re-computed.
4.3 Technical overview
As a basis, we use MaryTTS (Schr?der and Trouvain,
2003), but replace Mary?s internal data structures
and processing strategies with structures from our
incremental SDS architecture, the INPROTK toolkit
(Schlangen et al, 2010; Baumann and Schlangen,
2012b), which implements the IU model for incre-
mental dialogue processing (Schlangen and Skantze,
2009). The model conceptualises ? and the toolkit
implements ? incremental processing as the process-
ing of incremental units (IUs), which are the smallest
?chunks? of information at a specific level (the boxes
in Figure 2). IUs are interconnected to form a network
(e. g., words keep links to their associated phonemes
and neighbouring words and vice-versa) which repre-
sents the system?s information state.
The component is fed with chunk IUs which con-
tain some words to be synthesised (on their own or
appended to an ongoing utterance). For simplicity,
all units below the chunk level are currently gener-
ated using Mary?s (non-incremental) linguistic pre-
processing capabilities to obtain the target phoneme
sequence. For continuations, the preceding parts of
the utterance are taken into account when generating
prosodic characteristics for the new chunk. Also, our
component is able to revoke and exchange chunks
(or unspoken parts thereof) to change what is to be
spoken; this capability however is not used in the
example system presented in Section 5.
The lowest level module of our component is what
may be called a crawling vocoder, which actively
moves along the phoneme IU layer and executes two
processing steps: (a) for each phoneme it generates
the sequence of HMM parameter frames using a local
optimisation technique (using up to four neighbour-
ing phonemes as context) similar to the one described
by Dutoit et al (2011); and (b) vocoding the HMM
parameters into an audio stream which contains the
actual speech signal.
IUs have a ?progress? field which is set by the
crawling vocoder to one of ?upcoming?, ?ongoing?,
or ?completed?, as applicable. IUs provide a generic
update mechanism to support notification about
progress changes in delivery. The next section de-
scribes how this is used to drive the system.
5 Integrating iNLG and iSS for Adaptive
Information Presentation
Integrating incremental microplanning with incre-
mental speech synthesis in one incremental output
generation architecture allows us to test and explore
how their capabilities act in a coordinated way. As a
first example, we implemented a system that presents
information about events in an appointment database
(e. g., new, conflicting or rescheduled appointments)
and is able to cope with external noise burst events,
as they might for example occur on a bad telephone
line or when using a dialogue system next to a busy
street. The focus is on the incremental capabilities of
the system which enable its adaptive behaviour.
5.1 Component interplay
iNLG and iSS are implemented as IU modules in the
INPROTK architecture. The control flow of the sys-
tem (Figure 3) is managed without special coupling
between the modules, relying only on the left-to-right
processing capabilities of INPROTK combined with
its generic IU update mechanism for transporting
feedback from iSS to iNLG. Both modules can be
(and have been) combined with other IU modules.
To communicate an appointment event, the iNLG
module starts by generating two initial chunk IUs,
the first to be expressed immediately, the second as
additional prosodic context (chunk lengths differ with
an average of about 4 words). The iNLG registers as a
?progress listener? on each chunkIU, which registers
300
Figure 3: Information flow (dashed lines) between iNLG
and iSS components (rounded boxes) and incremental
units (rectangular boxes). The vocoder crawls along with
time and triggers the updates.
as a progress listener on a phonemeIUnear its end.
Shortly before iSS finishes speaking the chunk, iNLG
is thus informed and can generate and send the next
chunk to iSS just-in-time.
If adaptation to noise is needed, iNLG re-generates
and re-sends the previous chunk, taking altered pa-
rameters into account. Again, a subsequent chunk
is immediately pre-generated for additional prosodic
context. This way of generating sub-utterance chunks
ensures that there is always one chunk lookahead to
allow the iSS module to compute an adequate in-
tonation for the current chunk, while maintaining
the single chunk as increment size for the system
and minimising redundant work on the side of iNLG
(this lookahead is not required for iSS; but if it is un-
available, sub-utterance chunks may be inadequately
connected prosodically).
5.2 Responding to a noise event
A third module, the noise detector connects to both
iSS and iNLG. On noise onset, it informs iSS to inter-
rupt the ongoing utterance after the current word (this
works by breaking the links between words so that
the crawling vocoder finishes after the currently ongo-
ing word). Once a noise burst ends, iNLG is informed,
re-generates the interrupted sub-utterance chunk with
the verbosity level decreased by one and the assumed
understanding value increased by one (this degree
of adaptation results in a noticeable difference, it is,
however, not based on empirical study). The values
are then reset, the following chunk is generated and
both chunks are sent to iSS.
It should be noted, that we have not implemented
a real noise source and noise detector. Instead, our
random noise simulator generates bursts of noise of
1000 ms after a random time interval (between 2 and
Table 2: Processing time per processing step before deliv-
ery can begin (in ms; averaged over nine stimuli taking the
median of three runs for each stimulus; calculated from
log messages; code paths preheated for optimisation).
non-incr. incr.
NLG-microplanning 361 52
Synthesis (ling. pre-processing) 217 4472
Synthesis (HMM and vocoding) 1004 21
total response time 1582 519
5 seconds) and directly informs the system 300 ms
after noise starts and ends. We think it is reasonable
to assume that a real noise detector should be able to
give accurate information with a similar delay.
6 Evaluation
6.1 Quantitative evaluation
One important argument in favour of incremental
processing is the possibility of speeding up system
response time, which for non-incremental systems
is the sum of the times taken by all processors to
do their work. An incremental system, in contrast,
can fold large amounts of its processing time into the
ongoing speech output; what matters is the sum of
the onset times of each processor, i. e., the time until
a first output becomes available to the next processor.
Table 2 summarises the runtime for the three major
steps in output production of our system using nine
utterances from our domain. Both NLG and speech
synthesis? onset times are greatly reduced in the in-
cremental system.2 Combined, they reduce system
response time by more than a second. This is mostly
due to the almost complete folding of HMM opti-
misation and vocoding times into the spoken utter-
ance. NLG profits from the fact that at the beginning
of an utterance only two chunks have to be gener-
ated (instead of an average of 6.5 chunks in the non-
incremental system) and that the first chunk is often
very simple.
6.2 Subjective evaluation
To further test whether the system?s behaviour in
noisy situations resembles that of a human speaker
2The iSS component by mistake takes the symbolic pre-
processing step twice. Unfortunately, we found this bug only
after creating the stimuli used in the subjective evaluation.
301
in a similar situation, we let humans rate utterances
produced by the fully incremental, adaptive system
and utterances produced by two non-incremental
and less responsive variants (we have not used non-
incremental TTS in combination with iNLG as another
possible base-line as pretests showed this to sound
very unnatural due to the missing prosodic linkage be-
tween phrases). The participants were to rate whether
they agree to the statement ?I found the behaviour of
the system in this situation as I would expect it from
a human speaker? on a 7-point Likert-scale.
In condition A, full utterances were generated non-
incrementally, synthesised non-incrementally and
played without responding to noise-interruptions in
the channel (as if the system did not notice them).
Utterances in condition B were generated and synthe-
sised as in condition A, but playback responded to the
noisy channel, stopping when the noise was noticed
and continuing when noise ended. For condition C,
utterances were generated with the fully incremental
and adaptive system described in Section 5. Upon
noise detection, speech synthesis is interrupted and,
when the noise ends, iNLG will re-generate the in-
terrupted sub-utterance chunk ? using the adaptation
strategy outlined in Section 5.2. This then triggers
iSS into action and shortly after, the system contin-
ues speaking. Nine system runs, each producing a
different utterance from the calendar domain, were
recorded in each of the three conditions, resulting in
a total of 27 stimuli.
Before the actual stimuli were presented, partici-
pants listened to two example stimuli without noise
interruptions to get an impression of how an aver-
age utterance produced by the system sounds. After
the presentation of these two examples, the 27 stim-
uli were presented in the same random order. Par-
ticipants listened once to each stimulus and rated it
immediately after every presentation.
Twelve PhD-students (3 female, 9 male; mean age
30.5 years; 11 with German as one of their first lan-
guages; none with uncorrected hearing impairment)
from Bielefeld University participated in our study
and listened to and rated the 27 stimuli.
A Friedman rank sum test revealed a highly sig-
nificant difference between the perceived human-
likeness of the three systems (?2 = 151, p < .0001).
Median values of stimulus ratings in the conditions
A, B and C were 2, 2 and 6 respectively, indicat-
ing that the fully incremental system was rated con-
siderably more human-like. This was also shown
through a post-hoc analysis with Wilcoxon signed
rank tests which found no significant difference be-
tween condition A and B (V = 1191.5, p = .91)3.
Conditions A and C, however, differed highly signifi-
cantly (V = 82, p < .0001), as did conditions B and
C (V = 22.5, p < .0001) ? even after applying a Bon-
ferroni correction to correct for a possible cumulation
of ?-errors.
7 Conclusion
We have presented what is ? to the best of our knowl-
edge ? the first integrated component for incremental
NLG and speech synthesis and demonstrated the flex-
ibility that an incremental approach to output gener-
ation for speech systems offers by implementing a
system that can repair understanding problems.
From the evaluation we can conclude that incre-
mental output generation (both iNLG and iSS in iso-
lation or combined) is able to greatly speed up sys-
tem response time and can be used as a means to
speed up system response even in an otherwise non-
incremental system. Furthermore, we showed that the
behaviour of our fully incremental and adaptive sys-
tem was perceived as significantly more human-like
than the non-incremental and the non-incremental
but responsive baseline systems.
The understanding problem that our demonstra-
tor system tackled was of the simplest kind, namely
acoustic non-understanding, objectively detectable
as the presence of noise. In principle, however, the
same mechanisms of stopping and rephrasing can be
used to tackle more subjective understanding prob-
lems as can be signalled by linguistic feedback. Our
incremental output generation component gives us an
ideal basis to explore such problems in future work.
Acknowledgements This research is partially sup-
ported by the Deutsche Forschungsgemeinschaft
(DFG) in the Center of Excellence in ?Cognitive Inter-
action Technology? (CITEC) and through an Emmy
Noether Fellowship to the last author.
3This suggests that it does not matter whether a system re-
sponds to problems in the communication channel by waiting or
totally ignores these problems. Notice, however, that we did not
test recall of the calendar events. In that case, condition B should
outperform A, as some information was clearly inaudible in A.
302
References
Jens Allwood, Joakim Nivre, and Elisabeth Ahls?n. 1992.
On the semantics and pragmatics of linguistic feedback.
Journal of Semantics, 9:1?26.
Rachel Baker, Alastair Gill, and Justine Cassell. 2008.
Reactive redundancy and listener comprehension in
direction-giving. In Proceedings of the 9th SIGdial
Workshop on Discourse and Dialogue, pages 37?45,
Columbus, OH.
Timo Baumann and David Schlangen. 2012a. INPRO_iSS:
A component for just-in-time incremental speech syn-
thesis. In Proceedings of ACL System Demonstrations,
Jeju, South Korea.
Timo Baumann and David Schlangen. 2012b. The
INPROTK 2012 release. In Proceedings of the NAACL-
HLT Workshop on Future directions and needs in the
Spoken Dialog Community: Tools and Data, pages 29?
32, Montr?al, Canada.
Hendrik Buschmeier and Stefan Kopp. 2011. Towards
conversational agents that attend to and adapt to com-
municative user feedback. In Proceedings of the 11th
International Conference on Intelligent Virtual Agents,
pages 169?182, Reykjavik, Iceland.
David DeVault. 2008. Contribution Tracking: Partici-
pating in Task-oriented Dialogue Under Uncertainty.
Ph.D. thesis, Rutgers, The State University of New Jer-
sey, New Brunswick, NJ.
Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nicolas
d?Alessandro, and Benjamin Picart. 2011. pHTS for
Max/MSP: A streaming architecture for statistical para-
metric speech synthesis. Technical Report 1, numediart
Research Program on Digital Art Technologies, Mons,
Belgium.
Jens Edlund. 2008. Incremental speech synthesis. In
Second Swedish Language Technology Conference,
pages 53?54, Stockholm, Sweden, November. System
Demonstration.
Markus Guhe. 2007. Incremental Conceptualization for
Language Production. Lawrence Erlbaum, Mahwah,
NJ.
Christine Howes, Matthew Purver, Patrick G. T. Healey,
Gregory Mills, and Eleni Gregoromichelaki. 2011. On
incrementality in dialogue: Evidence from compound
contributions. Discourse & Dialogue, 2:279?311.
Gerard Kempen and Edward Hoenkamp. 1987. An incre-
mental procedural grammar for sentence formulation.
Cognitive Science, 11:201?258.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical
Report RR-95-11, Deutsches Forschungszentrum f?r
K?nstliche Intelligenz, Saarbr?cken, Germany.
Willem J. M. Levelt. 1989. Speaking: From Intention to
Articulation. The MIT Press, Cambridge, UK.
Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda,
Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno.
2010. Analyzing user utterances in barge-in-able spo-
ken dialogue system for improving identification accu-
racy. In Proceedings of INTERSPEECH 2010, pages
3050?3053, Makuhari, Japan.
Ehud Reiter and Somayajulu Sripada. 2002. Human vari-
ation and lexical choice. Computational Linguistics,
28:545?553.
David Schlangen and Gabriel Skantze. 2009. A general,
abstract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 710?718, Athens, Greece.
David Schlangen, Timo Baumann, Hendrik Buschmeier,
Okko Bu?, Stefan Kopp, Gabriel Skantze, and Ramin
Yaghoubzadeh. 2010. Middleware for incremental
processing in conversational agents. In Proceedings of
SIGdial 2010: the 11th Annual Meeting of the Special
Interest Group in Discourse and Dialogue, pages 51?
54, Tokyo, Japan.
Marc Schr?der and J?rgen Trouvain. 2003. The Ger-
man text-to-speech synthesis system MARY: A tool
for research, development and teaching. International
Journal of Speech Technology, 6:365?377.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
incremental speech generation in dialogue systems. In
Proceedings of SIGDIAL 2010: the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 1?8, Tokyo, Japan.
Matthew Stone, Christine Doran, Bonnie Webber, Tonia
Bleam, and Martha Palmer. 2003. Microplanning with
communicative intentions: The SPUD system. Compu-
tational Intelligence, 19:311?381.
Matthew Stone. 2002. Lexicalized grammar 101. In
Proceedings of the ACL-02 Workshop on Effective Tools
and Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages 77?
84, Philadelphia, PA.
Paul Taylor. 2009. Text-to-Speech Synthesis. Cambridge
Univ Press, Cambridge, UK.
Tomoki Toda and Keiichi Tokuda. 2007. A speech param-
eter generation algorithm considering global variance
for HMM-based speech synthesis. IEICE TRANSAC-
TIONS on Information and Systems, 90:816?824.
Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko,
Takao Kobayashi, and Tadashi Kitamura. 2000.
Speech parameter generation algorithms for HMM-
based speech synthesis. In Proceedings of ICASSP
2000, pages 1315?1318, Istanbul, Turkey.
Marylin Walker, Amanda Stent, Fran?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research, 30:413?456.
303
Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 13?17,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Toward a Virtual Assistant for Vulnerable Users:
Designing Careful Interaction
Ramin Yaghoubzadeh
Sociable Agents Group, CITEC
Bielefeld University; PO Box 100131
33501 Bielefeld, Germany
ryaghoub@techfak.uni-bielefeld.de
Stefan Kopp
Sociable Agents Group, CITEC
Bielefeld University; PO Box 100131
33501 Bielefeld, Germany
skopp@techfak.uni-bielefeld.de
Abstract
The VASA project develops a multimodal as-
sistive system mediated by a virtual agent that
is intended to foster autonomy of communica-
tion and activity management in older people
and people with disabilities. Assistive systems
intended for these user groups have to take
their individual vulnerabilities into account. A
variety of psychic, emotional as well as behav-
ioral conditions can manifest at the same time.
Systems that fail to take them into account
might not only fail at joint tasks, but also risk
damage to their interlocutors. We identify im-
portant conditions and disorders and analyze
their immediate consequences for the design
of careful assistive systems.
1 Introduction
In 2001, the World Health Organization consoli-
dated previous taxonomies of somatic and mental
functions and the everyday needs of human beings
into the comprehensive International Classification
of Functioning, Disability and Health ICF (WHO,
2001). Older people, as well as people with im-
pairments, often need support from others to satisfy
those basic needs, among which are activities related
to self-care, to mobility, but also to communication
and management of the daily activities and the so-
cial environment. For many older people, a catas-
trophic event, most often either a fall or the passing
of their spouse, leads to their sudden loss of auton-
omy and subsequent submission into stationary care.
In the latter case, the loss of their day structure is
frequently the intermediate cause. The same effect
can be observed for many disabled people of all ages
who must make a transition from assisted living to
stationary care. Here, specialized systems that assist
in preserving autonomy in a spectrum of daily need
fulfillment can potentially be of great benefit.
The present paper introduces the VASA project
(?Virtual Assistants and their Social Acceptability?),
which in cooperation with a health-care foundation
examines how both older patients and people with
various impediments, congenital or acquired, both
in stationary and assisted living, can be provided
with technical assistance to maintain autonomy for
as long as possible. Importantly, we are not focus-
ing on physical assistance, but on supporting a per-
son?s capability for organizing a social environment
(WHO ICF d9205: ?Socializing?) and managing the
day structure generally (d230: ?Carrying out daily
routine?). These two tasks turned out to be crucial in
our analysis with the health care personnel. We thus
aim to develop an assistive system for (1) managing
daily routine and weekly appointments with the user,
and (2) accessing a video call interface for contact-
ing acquaintances or care personnel (d360: ?Using
communication devices and techniques?).
But how should such a system meet its user, and
what criteria should guide the system interface de-
sign? Research has shown that older users are
far more likely to employ a ?social? conversation
style with a system (Wolters et al, 2009). The
VASA project explores the use of a ?virtual assis-
tant?, an humanoid conversational agent that fea-
tures natural-language and touchscreen input and
human-like communicative behavior (speech, ges-
ture; see Fig. 1 for the current running prototype).
13
Figure 1: The VASA system. Left side: natural-language
calendar management; right side: video call interface.
In this paper we review work on related systems for
older people and people with disabilities. We then
argue that beside the general goal of maximizing us-
ability for this specific user group, there is an en-
hanced vulnerability of these users that calls for spe-
cial care in interaction design; we substantiate this
view by an analysis of potential mental conditions
of the prospective users along with discussions of
what requirements arise from them.
2 Related work
Generally, assistive systems are driven by task rea-
soning systems as well as components for human-
computer interaction, which can be specialized for
older or disabled persons. Modern systems that
attempt to provide a ?natural? interaction are be-
ing developed and evaluated, including touch-screen
and haptic interfaces and interfaces capable of un-
derstanding and generating natural language, all of
them providing an immediacy between communica-
tive intentions and their execution that makes them
suitable especially for users without technical exper-
tise, with reduced sensorimotor skills or reduced ca-
pability for learning new interaction paradigms, as is
frequently the case with older or impaired persons.
The performance of such systems in terms of suit-
able operation in interaction, successful task com-
pletion, and user-reported satisfaction, has been
subject to systematic evaluation under controlled
conditions: The performance of speech recogni-
tion systems has been compared between base-line
users and people with varying degrees of dysarthria
(breathiness, dysfluencies, involuntary noises). Off-
the-shelf speech recognition systems have higher
failure rates with dysarthric speakers (Raghaven-
dra et al, 2001). Mildly and moderately dysarthric
speakers can attain a recognition accuracy of 80%
in dictation systems, breath exercises and phonation
training improve performance (Young and Mihai-
lidis, 2010). Vipperla et al (2009) compared speech
recognition for younger and older users, reporting
an 11% baseline increase in word error rates for the
latter group, attributed to both acoustic and linguis-
tic causes. The Stardust project succeeded in very
high single-word recognition rates on small dictio-
naries in patients with severe dysarthria, enabling
them to control their environment by voice (Hawley
et al, 2007). Fager et al (2010) implemented a mul-
timodal prototype system that combined ASR with
a word prediction model and a capability to enter an
initial letter, leading to an accuracy of > 80%; not-
ing that other conditions, such as a reduced visual
field or ataxia, had to be addressed with technical
solutions for each individual. Jian et al (2011) de-
signed a system for direction giving for seniors, sug-
gesting specific design guidelines addressing typi-
cal perceptive, motor, attentive and cognitive impair-
ments of older users. The evaluation of their multi-
modal system (speech and touch/image) led to posi-
tive results with respect to effectivity, efficiency and
user-reported satisfaction.
3 Careful Interaction with Vulnerable
People: Analysis
The more autonomously assistive systems act, the
higher the potential negative effects they can conse-
quentially cause. This is especially true for robotic
systems, since their extension into the physical
world entails possible harmful effects if proper rea-
soning or safety precautions should be breached by
unanticipated events. But even without physical ma-
nipulation, real damage can still be done. This might
be due to misunderstandings, leading to wrong as-
sumptions in the system, and hence to actions be-
ing performed on behalf, but actually to the detri-
ment, of the user. It might, however, also be due to
the wrong things being communicated, or communi-
cated in an inappropriate manner, leading to unnec-
essary negative appraisal, discomfort, or triggering
of a negative psychic condition in the user. While
unlikely to cause damage in an interaction with the
average healthy interactant, this issue is of the ut-
most importance for many potential user groups.
14
Frail or potentially unstable users are arguably
among those who can derive the greatest benefits
from easily accessible assistive systems, enabling
them to perform tasks which they might else not,
or no longer, perform, thus preserving their auton-
omy. However, they are at the same time affected by
a multitude of possible cognitive, psychic and emo-
tional conditions and behavioral anomalies that can
occur simultaneously. Each of these conditions en-
tails special constraints for interactive systems, ei-
ther for the interaction channels, for the contents,
or for both. Several factors have been accounted
for in existing systems: Reduced perceptive faculty
(vision, hearing), reduced motor abilities (ataxia),
and attention and memory impairments, mitigated
by best-practice rules (Jian et al, 2011). Attempts
to account for users with mild dementia have been
made, such as in the ISISEMD project. Avoiding
a deep hierarchy of dialogue structures and provid-
ing extra information (repetition, paraphrase) rather
than maximum parsimony are paramount in cases of
impaired memory and abstraction faculty, whereas
people with learning difficulties need a system that
operates without extensive training (of the user).
For systems that strive to provide long-term sup-
port to a specific person, adaptation to that per-
son is of vital importance ? by employing user
models that are adapted either manually or using
learning algorithms. System behavior should be
adapted both in the content provided as well as the
form it is provided in, to enable a working rela-
tionship that is both effective and pleasant for the
user (Yaghoubzadeh and Kopp, 2011). This alone
however is insufficient; since the vulnerability of
the actual clientele in VASA is considerable, each
of the encountered mental conditions has to be an-
alyzed and additional dialogue constraints be en-
forced before autonomous interactions can be per-
mitted. There is a variety of such factors that
have not yet been comprehensively addressed, but
might cause critical damage to some interactants
if not considered. The following section captures
the most frequently encountered phenomena, which
were identified in dialogue with care personnel:
? Depression and Bipolar Disorder: Roughly
ten percent of the population suffer from de-
pression at some point in their lives. Depres-
sion increases the risk for suicide ten- to twen-
tyfold (Sadock et al, 2007). Bipolar disor-
der manifests in episodic effects, where sen-
sations of racing thoughts and heightened ac-
tivity (mania) and listlessness and social pas-
sivity (severe depression) alternate or occur si-
multaneously; depressive relapses in particular
are points of vulnerability (Hill and Shepherd,
2009). There are successes in detecting depres-
sive states from facial and voice cues at > 80%
rate (Cohn et al, 2009). A good practice is
to employ mitigation strategies when breaking
bad news to the user (Brown and Levinson,
1987; Fraser, 1980), e.g. by presenting obliga-
tions as options (Williams, 2011), or present-
ing the ?bad news? simultaneously with ?good
news?. We provide for discussion another re-
quirement for interactive systems in this case:
The system must not produce ambiguously in-
terpretable answers ? consider a catastrophic
answer of ?okay? as an affirmative response to
a wrongly parsed utterance that was actually an
expression of intent for suicide, a frequent phe-
nomenon with risk patients (Kelly, 2009).
? Borderline Personality Disorder: This type
of disorders, characterized by emotional insta-
bility, can lead to anxiety, social insecurity and
depression, but also inappropriate outbursts of
anger. Anger management techniques are em-
ployed to inhibit the expression of such anger
(Swaffer and Hollin, 2009). An assistive sys-
tem should be able to cope with impulses of
anger, and as a bare minimum interrupt the in-
teraction and offer to resume it at a later point.
The EmoVoice system, for instance, can clas-
sify emotional features in natural language with
good rates (Vogt et al, 2008), and could be
used to identify anger.
? Epilepsy: Patients with acquired brain injuries
frequently suffer from epilepsy. Even short (pe-
tit mal) epileptic seizures can lead to tempo-
rary absence and periods of confusion and dis-
orientation (APA, 2000). In such a situation,
the patient may utter irrational sentences or be
silent altogether. An assistive system should be
able to detect these irrational deviations from
15
the course of conversation, and fill the user in
again, abort the conversation, or call for help.
? Panic: Proneness to panic attacks can result
from a multitude of afflictions and is hard to
predict. In the event of a panicking interactant,
the system should not take steps that could fur-
ther exacerbate the situation. According to lit-
erature (Gournay and Denford, 2009), panic at-
tacks are generally unable to do any real harm
and subside quickly. Therefore, passivity from
the system?s side, in a neutral mode, is the mini-
mal appropriate behavior. Panicking people are
most likely not able to perform in interaction as
successfully as usual ? systems that should still
be operable by a user in this situation must pro-
vide minimalistic shortcuts to essential features
(i.e. a ?panic button? for emergencies).
? Anxiety: Special care must be taken in the de-
sign of systems aimed at people with social
anxiety. Interactants might be hesitant to open
a conversation even with an artificial system.
The system could take the initiative by simply
opening with a short utterance about the task
domain (Williams, 2011).
? Phobias and Impulse Control Disorders:
Phobic disorders and obsessive-compulsive
disorders can be triggered by environmental
cues (Gournay and Denford, 2009). User inter-
faces have to take this into account, and avoid
presenting stimuli that could act as potential
triggers (e.g. people with an insect phobia
should neither be presented with pictures of in-
sects, nor their verbal mention). The same pre-
cautions are valid in the case of addictions.
Any interactive system, and in particular systems
that do not only provide information but can also be
made to perform tasks autonomously on behalf of
the user, must be designed with all possible afflic-
tions of all possible users in mind, not only as a wise
legal precaution, but also as an ethical obligation to
the designer. We argue that, quite unlike the ?best
practices? of user interface design, there is no degree
of optionality to the implementation of the above
constraints and countermeasures, but that it must be
performed with all musterable diligence. Some con-
straints are especially hard to meet in open-world
systems (e.g. with free Internet access), since the
contents presented are harder to predict.
Note that the set of conditions presented above
is by no means comprehensive. For instance, we
have, for now, altogether omitted an incorporation
of autism-spectrum disorders or of functional psy-
choses such as schizophrenia, paraphrenia and para-
noia ? which are not uncommon in the older popu-
lation (Ashton and Keady, 2009).
4 Summary
The VASA project is developing a multimodal
natural-language agent-mediated assistance system
for older people and patients with disabilities for
enhancing their autonomy in the everyday tasks
of communication and activity management. The
clientele is afflicted with a variety of cognitive, psy-
chic, and emotional conditions that have to be dealt
with with extreme care and entail a necessity for spe-
cific safety mechanisms which will be implemented
for VASA in coordination with the care person-
nel. We attempted to identify common conditions
of older and impaired patients that should be con-
sidered and resolved in any assistive system (or in-
deed any autonomous interactive system) that might
communicate with them. Factors that could lead
to a detrimental outcome of such an interaction in-
clude depression, emotional instability, disorienta-
tion, panic, anxiety and phobia. Some constraints
on the design rationale for such systems can pro-
vide a mitigation of those risks: avoiding ambigu-
ity in the system?s utterances, coping with anger, ir-
rationality and panic by employing appropriate sys-
tem responses, capability for system-side initiative,
and preventing inadvertent stimulation of disorders.
Since the field of potential interactants for generic
assistive systems is vast, as any inspection of a larger
health-care institution will show, more discussion in
the research community should aim at establishing a
stable ontology of their special needs and the rami-
fications for the design of careful assistive systems.
Acknowledgments
This research is supported by the Deutsche
Forschungsgemeinschaft (DFG) in the Center of
Excellence in Cognitive Interaction Technology
(CITEC).
16
References
American Psychiatric Association. 2000. Diagnostic
and Statistical Manual of Mental Disorders DSM-IV-
TR, Fourth Edition. American Psychiatric Publishing,
Inc., Arlington, VA.
Peter Ashton and John Keady. 2009. Mental disorders
of older people. Newell & Gournay (eds.), Mental
Health Nursing: an Evidence-Based Approach, 341?
370. Churchill Livingstone, Philadelphia, PA.
Paul Brown and Stephen Levinson. 1987. Politeness.
Some Universals in Language Usage. Cambridge Uni-
versity Press, Cambridge.
Jeffrey F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara
Tejera Padilla, Feng Zhou, and Fernando de la Torre.
2009. Detecting Depression from Facial Actions and
Vocal Prosody. Proceedings of the 3rd International
Conference on Affective Computing and Intelligent
Interaction and Workshops (ACII 2009), 1?7. IEEE,
Amsterdam.
Susan K. Fager, David R. Beukelman, Tom Jakobs, and
John-Paul Hosom. 2010. Evaluation of a Speech
Recognition Prototype for Speakers with Moderate
and Severe Dysarthria: A Preliminary Report Aug-
mentative and Alternative Communication, 26(4):267-
277.
Bruce Fraser. 1980. Conversational mitigation. Journal
of Pragmatics, 4:341?350.
Kevin Gournay and Lindsay Denford. 2009. Pho-
bias and Rituals. Newell & Gournay (eds.), Mental
Health Nursing: an Evidence-Based Approach, 207?
224. Churchill Livingstone, Philadelphia, PA.
Mark S. Hawley, Pam Enderby, Phil Green, Stuart Cun-
ningham, Simon Brownsell, James Carmichael, Mark
Parker, Athanassios Hatzis, Peter ONeill, and Rebecca
Palmer. 2007. A speech-controlled environmental
control system for people with severe dysarthria. Med-
ical Engineering & Physics, 29(5):586?593.
Robert Gareth Hill and Geoff Shepherd. 2009. Disorders
of Mood: Depression and Mania. Newell & Gournay
(eds.), Mental Health Nursing: an Evidence-Based
Approach, 165?185. Churchill Livingstone, Philadel-
phia, PA.
Cui Jian, Nadine Sasse, Nicole von Steinbu?chel-
Rheinwall, Frank Schafmeister, Hui Shi, Carsten
Rachuy, and Holger Schmidt. 2011. Towards effec-
tive, efficient and elderly-friendly multimodal interac-
tion. Proceedings of the 4th International Conference
on Pervasive Technologies Related to Assistive Envi-
ronments (PETRA 2011), article 45, 1?8. ACM, New
York, NY.
Sarah Kelly. 2009. Suicide and Self-Harm. Newell &
Gournay (eds.), Mental Health Nursing: an Evidence-
Based Approach, 187?206. Churchill Livingstone,
Philadelphia, PA.
Parimala Raghavendra, Elisabet Rosengren, and Sheri
Hunnicutt. 2001. An investigation of different de-
grees of dysarthric speech as input to speaker-adaptive
and speaker-dependent recognition systems. Augmen-
tative and Alternative Communication, 17(4):265-275.
Benjamin J. Sadock, Harold I. Kaplan, and Virginia A.
Sadock. 2007. Kaplan & Sadock?s Synopsis of Psychi-
atry: Behavioral Sciences/Clinical Psychiatry. Lip-
pincott Williams & Wilkins, Philadelphia.
Tracey Swaffer and Clive R. Hollin. 2009. Anger and
Impulse Control. Newell & Gournay (eds.), Mental
Health Nursing: an Evidence-Based Approach, 267?
289. Churchill Livingstone, Philadelphia, PA.
Ravichander Vipperla, Maria Wolters, Kallirroi Georgila,
and Steve Renals. 2009. Speech input from older
users in smart environments: Challenges and perspec-
tives. HCI (6): Universal Access in Human-Computer
Interaction, Intelligent and Ubiquitous Interaction En-
vironments , LNCS 5615:117?126. Springer, Heidel-
berg.
Thurid Vogt, Elisabeth Andre?, and Nikolaus Bee. 2008.
EmoVoice - A framework for online recognition of
emotions from voice. Proceedings of the 4th IEEE
tutorial and research workshop on Perception and In-
teractive Technologies for Speech-Based Systems (PIT
2008), 188?199. Springer, Heidelberg.
Val Williams. 2011. Disability and discourse : analysing
inclusive conversation with people with intellectual
disabilities. Wiley-Blackwell, Chichester, West Sus-
sex / Malden, MA.
World Health Organization. 2001. International Clas-
sification of Functioning, Disability and Health: ICF,
WHO, Geneva, Switzerland.
Maria Wolters, Kallirroi Georgila, Johanna D. Moore,
and Sarah E. MacPherson. 2009. Being Old Doesn?t
Mean Acting Old: How Older Users Interact with Spo-
ken Dialog Systems. ACM Transactions on Accessible
Computing (TACCESS), 2(1):1?39.
Ramin Yaghoubzadeh and Stefan Kopp. 2011. Creat-
ing familiarity through adaptive behavior generation
in human-agent interaction. Proceedings of the 11th
International Conference on Intelligent Virtual Agents
(IVA 2011), LNCS(LNAI) 6895:195?201. Springer,
Heidelberg.
Victoria Young and Alex Mihailidis. 2010. Difficulties in
Automatic Speech Recognition of Dysarthric Speakers
and Implications for Speech-Based Applications Used
by the Elderly: A Literature Review. Assistive Tech-
nology, 22(2):99?112.
17
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 68?72,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Situationally Aware In-Car Information Presentation
Using Incremental Speech Generation: Safer, and More Effective
Spyros Kousidis
1
, Casey Kennington
1,2
, Timo Baumann
4
, Hendrik Buschmeier
2,3
,
Stefan Kopp
2,3
, and David Schlangen
1
1
Dialogue Systems Group,
2
CITEC,
3
Sociable Agents Group ? Bielefeld University
4
Department of Informatics, Natural Language Systems Division ? University of Hamburg
spyros.kousidis@uni-bielefeld.de
Abstract
Holding non-co-located conversations
while driving is dangerous (Horrey and
Wickens, 2006; Strayer et al., 2006),
much more so than conversations with
physically present, ?situated? interlocutors
(Drews et al., 2004). In-car dialogue
systems typically resemble non-co-located
conversations more, and share their
negative impact (Strayer et al., 2013). We
implemented and tested a simple strategy
for making in-car dialogue systems aware
of the driving situation, by giving them
the capability to interrupt themselves
when a dangerous situation is detected,
and resume when over. We show that this
improves both driving performance and
recall of system-presented information,
compared to a non-adaptive strategy.
1 Introduction
Imagine you are driving on a relatively free high-
way at a constant speed and you are talking with the
person next to you. Suddenly, you need to overtake
another car. This requires more attention from you;
you check the mirrors before you change lanes, and
again before you change back. Plausibly, an attent-
ive passenger would have noticed your attention
being focused more on the driving, and reacted to
this by interrupting their conversational contribu-
tion, resuming when back on the original lane.
Using a driving simulation setup, we implemen-
ted a dialogue system that realises this strategy. By
employing incremental output generation, the sys-
tem can interrupt and flexibly resume its output.
We tested the system using a variation of a stand-
ard driving task, and found that it improved both
driving performance and recall, as compared to a
non-adaptive baseline system.
Figure 1: Overview of our system setup: human
controls actions of a virtual car; events are sent to
DM, which controls the speech output.
2 The Setup
2.1 The Situated In-Car System
Figure 1 shows an overview of our system setup,
with its main components: a) the driving simulator
that presents via computer graphics the driving task
to the user; b) the dialogue system, that presents,
via voice output, information to the user (here, cal-
endar entries).
Driving Simulation For the driving simulator,
we used the OpenDS Toolkit,
1
connected to a steer-
ing wheel and a board with an acceleration and
brake pedal, using standard video game hardware.
We developed our own simple driving scenarios
(derived from the ?ReactionTest? task, which is dis-
tributed together with OpenDS) that specified the
driving task and timing of the concurrent speech,
as described below. We modified OpenDS to pass
real-time data (e.g. car position/velocity/events in
the simulation, such as a gate becoming visible
or a lane change) using the mint.tools architec-
ture (Kousidis et al., 2013). In addition, we have
bridged INPROTK (Baumann and Schlangen, 2012)
with mint.tools via the Robotics Service Bus (RSB,
Wienke and Wrede (2011)) framework.
1http://www.opends.eu/
68
Figure 2: Driver?s view during experiment. The
green signal on the signal-bridge indicates the tar-
get lane.
Dialogue System Using INPROTK, we imple-
mented a simple dialogue system. The notion of
?dialogue? is used with some liberty here: the user
did not interact directly with the system but rather
indirectly (and non-intentionally) via driving ac-
tions. Nevertheless, we used the same modularisa-
tion as in more typical dialogue systems by using a
dialoge management (DM) component that controls
the system actions based on the user actions. We
integrated OpenDial (Lison, 2012) as the DM into
INPROTK,
2
though we only used it to make simple,
deterministic decisions (there was no learned dia-
logue policy) based on the state of the simulator
(see below). We used the incremental output gen-
eration capabilities of INPROTK, as described in
(Buschmeier et al., 2012).
3 Experiment
We evaluated the adaptation strategy in a driving
simulation setup, where subjects performed a 30
minute, simulated drive along a straight, five-lane
road, during which they were occasionally faced
with two types of additional tasks: a lane-change
task and a memory task, which aim to measure the
driving performance and the driver?s ability to pay
attention to speech while driving, respectively. The
two tasks occured in isolation or simultaneoulsy.
The Lane-Change Task The driving task we
used is a variant of the well-known lane-change
task (LCT), which is standardised in (ISO, 2010):
It requires the driver to react to a green light posi-
tioned on a signal gate above the road (see Figure 2).
The driver (otherwise instructed to remain in the
middle lane) must move to the lane indicated by
2
OpenDial can be found at http://opendial.
googlecode.com/.
Table 1: Experiment conditions.
Lane Change Presentation mode Abbreviation
Yes CONTROL CONTROL_LANE
Yes ADAPTIVE ADAPTIVE_LANE
Yes NO_TALK NO_TALK_LANE
No CONTROL CONTROL_EMPTY
the green light, remain there until a tone is sounded,
and then return again to the middle lane. OpenDS
gives a success or fail result to this task depending
on whether the target lane was reached within 10
seconds (if at all) and the car was in the middle lane
when the signal became visible. We also added a
speed constraint: the car maintained 40 km/h when
the pedal was not pressed, with a top speed of 70
km/h when fully pressed. During a Lane-change,
the driver was to maintain a speed of 60 km/h, thus
adding to the cognitive load.
The Memory Task We tested the attention of
the drivers to the generated speech using a simple
true-false memory task. The DM generated utter-
ances such as ?am Samstag den siebzehnten Mai
12 Uhr 15 bis 14 Uhr 15 hast du ?gemeinsam Essen
im Westend mit Martin? ? (on Saturday the 17th
of May from 12:15?14:15 you are meeting Mar-
tin for Lunch). Each utterance had 5 information
tokens: day, time, activity, location and partner,
spoken by a female voice. After utterance comple-
tion, and while no driving distraction occurred, a
confirmation question was asked by a male voice,
e.g. ?Richtig oder Falsch? ? Freitag? (Right or
wrong? ? Friday). The subject was then required
to answer true or false by pressing one of two re-
spective buttons on the steering wheel. The token
of the confirmation question was chosen randomly,
although tokens near the beginning of the utterance
(day and time) were given a higher probability of
occurrence. The starting time of the utterance re-
lative to the gate was varied randomly between 3
and 6 seconds before visibility. Figure 3 gives a
schematic overview of the task and describes the
strategy we implemented for interrupting and re-
suming speech, triggered by the driving situation.
3.1 Conditions
Table 1 shows the 4 experiment conditions, de-
noting if a lane change was signalled, and what
presentation strategy was used. Each condition ap-
peared exactly 11 times in the scenario, for a total
of 44 episodes. The order of episodes was randomly
69
t1
t
2
suc
gate
lane t
3
0
1
2
3
4
am Samstag den siebzehn- den siebzehnten Mai ?
am Samstag den siebzehnten Mai um 12 Uhr hast du ?Besprechung mit Peter?
ADAPTIVE
CONTROL
Figure 3: Top view of driving task: as the car moves to the right over time, speech begins at t
1
, the gate with
the lane-change indicator becomes visible at t
2
, where in the adaptive version speech pauses. Successful
lane change is detected at suc; successful change back to the middle lane is detected at lane, and resumes.
(If no change back is detected, the interruption times out at t
3
). All red-dotted lines denote events sent
from OpenDS to the Dialogue Manager.
generated for each subject. With this design, sub-
jects perceive conditions to be entirely random.
3.2 Dependent Variables
The dependent variables for the Memory task
are (a) whether the subject?s answer was correct
(true/false), and (b) the response delay, which is
the time from the end of the clarification ques-
tion to the time the true or false button was
pressed. For the driving task, the dependent vari-
ables are the OpenDS performance measurements
success/failure (as defined above) and reaction time
(time to reach the target lane).
3.3 Procedure
After signing a consent form, subjects were led into
the experiment room, where seat position and audio
level were adjusted, and were given written instruc-
tions. Next, the OpenDS scenario was initiated. The
scenario started with 10 successive lane-change sig-
nal gates without speech, for driving training. An
experimenter provided feedback during training
while the subjects familiarized themselves with the
driving task. Following the training gates came a
clearly-marked ?START? gate, signifying the be-
ginning of the experiment to the subjects (at this
point, the experimenter left). There was a ?FINISH?
gate at the end of the scenario. The whole stretch of
road was 23 km and took approximately 30 minutes
to complete. After the driving task, the subjects
were given a questionnaire, which asked them to
identify the information presentation strategies and
assign a preference.
Table 2: Subjects?
judgement of task
difficulty.
Diff. Freq.
4 (easy) 8
3 7
2 1
1 (hard) 1
Table 3: Subjects? system
preference.
Preference Freq.
ADAPTIVE 3
CONTROL 9
Neither 5
4 Results
In total, 17 subjects (8 male, 9 female, aged 19-
36) participated in the study. All of the subjects
were native German speakers affiliated with AN-
ONYMIZED University. As reported in the post-test
questionnaire, all held a driving license, two had
previous experience with driving simulators and
only one had previous experience with spoken dia-
logue systems. Table 2 shows the subjects? assess-
ment of difficulty, while Table 3 shows their prefer-
ence between the different strategies. Most subjects
found the task relatively easy and either prefer the
speech not to adapt or have no preference.
Memory task The overall percentages of correct
answers to the system?s recall questions (across all
subjects) are shown in Table 4. We see that the sub-
jects? performance in this task is considerably bet-
ter when the system adapts to the driving situation
(ADAPTIVE_LANE condition) rather than speaking
through the lane change (CONTROL_LANE con-
dition). In fact, the performance in the ADAPT-
IVE_LANE condition is closer to the control upper
70
Table 4: Performance in memory task per condi-
tion.
Condition Percentage
CONTROL_EMPTY 169/180 (93.9%)
ADAPTIVE_LANE 156/172 (90.7%)
CONTROL_LANE 150/178 (84.3%)
Table 5: Success in driving task per condition (as
reported by OpenDS).
Condition Success
NOTALK_LANE 175/185 (94.6%)
ADAPTIVE_LANE 165/174 (94.8%)
CONTROL_LANE 165/180 (91.7%)
bound (CONTROL_EMPTY condition). We tested
significance of the results using a generalized lin-
ear mixed model with CONDITION and SUBJECT
as factors, which yields a p-value of 0.027 when
compared against a null model in which only SUB-
JECT is a factor. No significant effects of between-
subjects factors gender, difficulty or preference
were found. In addition, the within-subject variable
time did not have any significant effect (subjects do
not improve in the memory task with time).
The average response delay (from the end of
the recall question to the button press) per condi-
tion across all subjects is shown in Figure 4. Sub-
jects reply slower to the recall questions in the
CONTROL_LANE condition, while their perform-
ance in the ADAPTIVE_LANE condition is indis-
tinguishable from the CONTROL_EMPTY condi-
tion (in which there is no distraction). Addition-
ally, there is a general decreasing trend of response
delay with time, which means that users get ac-
quainted with the task (type of information, format
of question) over time. Both factors (condition
and time) are significant (repeated measures AN-
OVA, 2x2 factorial design, F
condition
= 3.858, p =
0.0359,F
time
= 4.672, p= 0.00662). No significant
effects were found for any of the between-subject
factors (gender, difficulty, preference).
Driving task The success rate in the lane-change
task per condition is shown in Table 5. Here too
we find that the performance is lower in the CON-
TROL_LANE condition, while ADAPTIVE_LANE
does not seem to affect driving performance, when
compared to the NOTALK_LANE condition. The
effect is significant (p = 0.01231) using the same
GLMM approach and factors as above.
ADAPTIVE_LANE CONTROL_EMPTY CONTROL_LANECondition0
500
1000
1500
2000
2500
3000
3500
4000
User
 Res
pons
e De
lay (
ms)
Figure 4: User answer response delay under three
conditions.
5 Discussion, Conclusions, Future Work
We have developed and tested a driving simula-
tion scenario where information is presented by a
spoken dialogue system. Our system has the unique
ability (compared to today?s commercial systems)
to adapt its speech to the driving situation: it in-
terrupts itself when a dangerous situation occurs
and later resumes with an appropriate continuation.
Using this strategy, information presentation had
no impact on driving, and dangerous situations no
impact on information recall. In contrast, a system
that blindly spoke while the driver was distracted
by the lane-change task resulted in worse perform-
ance in both tasks: subjects made more errors in
the memory task and also failed more of the lane-
change tasks, which could prove dangerous in a
real situation.
Interestingly, very few of the subjects preferred
the adaptive version of the system in the post-task
questionnaire. Among the reasons that they gave
for this was their inability to control the interrup-
tions/resumptions of the system. We plan to ad-
dress the issue of control by allowing future ver-
sions of our system to accept user signals, such as
speech or head gestures; it will be interesting to see
whether this will impact driving performance or not.
Further, more sophisticated presentation strategies
(e.g., controlling the complexity of the generated
language in accordance to the driving situation) can
be tested in this framework.
Acknowledgments This research was partly sup-
ported by the Deutsche Forschungsgemeinschaft
(DFG) in the CRC 673 ?Alignment in Communic-
71
ation? and the Center of Excellence in ?Cognit-
ive Interaction Technology? (CITEC). The authors
would like to thank Oliver Eckmeier and Michael
Bartholdt for helping implement the system setup,
as well as Gerdis Anderson and Fabian Wohlge-
muth for assisting as experimenters.
References
Timo Baumann and David Schlangen. 2012. The In-
proTK 2012 release. In NAACL-HLT Workshop on
Future directions and needs in the Spoken Dialog
Community: Tools and Data (SDCTD 2012), pages
29?32, Montr?al, Canada.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Combining incremental language generation and in-
cremental speech synthesis for adaptive information
presentation. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 295?303, Seoul, South Korea.
Frank A. Drews, Monisha Pasupathi, and David L.
Strayer. 2004. Passenger and cell-phone conver-
sations in simulated driving. In Proceedings of the
48th Annual Meeting of the Human Factors and Er-
gonomics Society, pages 2210?2212, New Orleans,
USA.
William J. Horrey and Christopher D. Wickens. 2006.
Examining the impact of cell phone conversations
on driving using meta-analytic techniques. Human
Factors, 48:196?205.
ISO. 2010. Road vehicles ? Ergonomic aspects of
transport information and control systems ? Simu-
lated lane change test to assess in-vehicle second-
ary task demand. ISO 26022:2010, Geneva, Switzer-
land.
Spyros Kousidis, Thies Pfeiffer, and David Schlangen.
2013. MINT.tools: Tools and adaptors supporting
acquisition, annotation and analysis of multimodal
corpora. In Interspeech 2013, Lyon, France. ISCA.
Pierre Lison. 2012. Probabilistic dialogue models with
prior domain knowledge. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 179?188, Seoul, South
Korea.
David L Strayer, Frank A Drews, and Dennis J Crouch.
2006. A comparison of the cell phone driver and the
drunk driver. Human Factors, 48:381?91.
David L Strayer, Joel M Cooper, Jonna Turrill, James
Coleman, and Nate Medeiros. 2013. Measuring
cognitive distraction in the automobile. Technical
report, AAA Foundation for Traffice Safety.
J Wienke and S Wrede. 2011. A middleware for col-
laborative research in experimental robotics. In Sys-
tem Integration (SII), 2011 IEEE/SICE International
Symposium on, pages 1183?1190.
72
