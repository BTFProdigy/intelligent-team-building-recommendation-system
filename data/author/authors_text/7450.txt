Acquisii:ion of a Language Computationall Model for NItA" 
Svetlana St IERI'~MI{TYI!VA 
Computing Research l,aboratory 
New Mexico State University 
Las Cruces, NM, USA ,~8003 
lana(a;crl.nmsu.edu 
Sergei NIP, ENBURG 
(?omputing Research I+aboratory 
New Mexico State University 
l.as Cruces, NM, USA 88003 
sergei@crl.nmsu.edu 
Aiistrael 
This paper describes art approach to actively 
acquire a language computational model. 
The purpose of this acquisition is rapid 
development of NLP systems. The model is 
created with the syntax module of the Boas 
knowledge elicitation system for a quick 
ramp tip o f  a standard transfer-.based 
machine translation system from \[, into 
English. 
\[ ntroduetlon 
Resource acquisition for NI.P systems is a well. 
known bottleneck in language engineering. It 
would be a clear advantage to have a 
methodology that could provide a nmch cheaper 
way of NI,I" resources acquisition. The 
methodology should be universal in the sense 
that it could be applied to any language and 
require no skilled labour of lm)fossionals. Our 
approach attempts just that. 
We describe it on the example of the syntax 
module of the Boas knowledge elicitation 
system tbr a quick ramp tip of a standard 
transfer-based machine tran<;laliori system from 
any langnage into English (Nirenburg 1998). 
This work is a parl of an ongoing project 
devoted to the creation of resources tbr NI,P by 
eliciting knowledge \[i-on-i intbrnlanis. 
1 Other Work on Synta)~ Acquisition 
i'\]xperinlents in "single -.slop" automatic 
acquisitioil of knowledge have been amoni~ lhe 
most lhshional)le topics in NI,I ) over the past 
decade. ()no can mention work on automalic 
acquisition of phrase structure usim3 distribution 
analyst,,; 0h-ill ci al 1990). "\[hc problerns with 
the current fully automatic corpus-based 
approaches include difficulties of maintaining 
any system based on them, due to the 
opaqueness of the method and the data to the 
language ngineer. At the present time, the most 
promising NLP systems include elements of 
both corpus-based and human knowledge-based 
methods. One example is acquisition of Twisted 
Pair Grannnar (Jones and ttavrilla 1998) for a 
pair of English and a source language (SL). 
Another example of a mixture of corpus-based 
and human knowledge-based methods is a 
system to generate a I,exicalized Tree-Adjoining 
Gramn-iar (F. Xia et al 1999) automatically from 
all abstract specification of a language. Grossly 
sin-lplil~/ing and generalizing due to lack of 
space, one can state that these experiments are 
seldon-i comprehensive in coverage and their 
results ate not yet directly useful iri 
comprehensive applications, uch as MT. 
7 Al:quisitim~ of Syntax in Boas 
2.1 Miethodolo~ies for Selection of Syntax 
Parameters 
In general, tile issue of the selection of 
parameters tbr grmnmar acquisition is one of the 
main problems tbr which there is rio single 
answer. Parameters applicable to more than one 
language are studied m the field of language 
universals as well as lhe principles-and- 
parameters ap\[)roach (Chomsky 1981) arid its 
successors ((Tholnsky 1995). Widely devised as 
the ba:ds of universal granlmar, the principles-. 
and--parameters approach has Ibcused on the 
uiliversaliiy of coitaill I()rn-ial grammatical i-riles 
within thai particular approach rather on iho 
sub~tarllive and exhaustive lisl of universal 
parameters., a subset of which is applicable to 
each natural hm,<.~uage., along with lhcii ? 
l l l l  
corresponding sets of values, such as a 
parameter set of nominal cases. In some other 
approaches, parameters and parameter values are 
either not sought out or are expected to be 
obtained automatically (e <, Brown et al 1990; 
Goldstein 1998), and, while holding promise for 
the tiittire as a potential component of an 
elicitation system, cannot, at this time, lbnn the 
basis of an entire system of this kind. 
lit order to ensure uniformity and systematicity 
of operation of a language knowledge licitation 
system, such as Boas, it is desirable to come tip 
with a comprehensive list of all possible 
parameters in natural lalguages and, for each 
such parameter, to create a cumulative list of its 
possible values in all the languages that Boas 
can expect as SLs. Three basic methodological 
approaches are used in Boas. 
Expectation-driven methodology: covering the 
material by collecting cross-linguistic 
information on lexical and grammatical 
parameters, including their possible values and 
realizations, and asking the user to choose what 
holds in SL; while it is beyond the means of the 
current prqiect to check all extant languages fbr 
possible new parameters~ we have included 
infomlation from 25 languages. 
Goal-driven methodology: in the spirit of the 
"demand-side" approach to NLP (Nirenburg 
1996) Boas was tailored lbr elicitation of Mr  
relevant parameters rather than any syntactic 
parameters that can be postulated. A parameter 
was considered to be relevant if it was necessary 
tbr the parser and the generator used in MT in 
the Expedition project (http:/tcrl,NMSU.Edu/ 
expeditiorl/). 
The parser used is a heuristic clause chunker 
developed at NMSU CP,\[, which replaces the 
complex system of phrase structure rules in a 
traditional '2 erammar and uses language specific 
information, among thent word order (SVO vs. 
SOV), clause element (sul!ject, o\[!iect, etc.) 
marking, agreement marking, nouil phrase 
structure pattern, position of a head. 
l)ata-driven methodology: prtmlpiillg the user 
by English words and phrases and requesting 
translatioris or othcr rcnderin,,s in SI.; data- 
driven acquisition is the first choice, wherever 
l'easible, because it is the easiest ype of work 
lbr the userst; In Boas, data-driven acquisition is 
guided by the resident English knowledge 
sources.  
2.2 Types of Syntax Parameters in Boas  
The parameters which are elicited through the 
syntax module of Boas include 2 what we call 
diagnostic and restricting parameters. 
Diagnostic parameters are those whose values 
help determine clause structure lbr correct 
structural transfer and translation of clause 
constituents. For example, in languages which 
use grammatical case, the subject is usually 
marked by the nominative, ergative or absolutive 
case; direct objects are usually marked by the 
accusative case, etc. \]he list of the currently 
used diagnostic parameters in Boas includes: 
bask sentence structure parameters: word 
order preferences, grammatical fimctions 
(subject marking direct object marking, indirect 
ol:tiect marking, complement marking, adverbial 
rnarking, verb marking), clause element 
agreement marking, clause boundary marking, 
and bask noun phrase structure parameters: 
POS patterns with head marking, phrase 
boundary marking, noun phrase component 
agreement 
Restricting parameters determine the scope of 
usage of diagnostic parameters. Some of the 
diagnostic paralneter values can only occur 
simultaneously with certain restricting parameter 
vatues. For exainple, in languages with the 
ergative construction the case of grammatical 
subject is restricted by the tense and aspect of 
the main verb (Mel'chuk 1998). 
t l{emember: they are not stipposed to be trained 
linguists but are  expected to be able to translate 
between the source language and \['nglish. 
2Such iraditionally naolphological paramctcr,~ >;part. 
of speech, number, gender, w)ice, aspect, etc. arc 
elicited l7 the naorphological module of Boas and arc 
prerequisites \[Bl the syntax module. 
1112 
2?3 The Flicitatior~ Procedure 
PrereqnisRes fl~r syntax elicitationo l)ata that 
drives syntax elicitation is obtained at earlier 
stages of elicitation, namely morphology o- 
parameters :-;UCll as Part of speech, (lender, 
Number, Person, Voice, Aspect, etc., as well as 
value sets tbr those parameters; lexieal 
acquisition of a small SL-English le?ieon to 
help work with the examples; the entries in the 
dictiotmry contain all the word forum and feature 
vahies of a SL lexeine and its English 
equivalentS? amt a very small corpus of 
carefllliy preselected and pretagged English 
noun phrases and sentences, used as examples? 
The inventory of tags and represeritation 
format. The tags for NPs include head and 
parameter values: The parameter (feature) set 
consists of Part of speech, Case, Number, 
Gender, Animacy arid I)efiniteness (the values 
of the latter two may pose restrictions on 
agreement of NP components). Every NP is 
represented in tile Boas knowledge base in the 
fbmi era  typed feature structure as illustrated by 
the following example (the sign "#" inarks the 
head): 
\["a good #boy"-: \[struct.ure:nouz_,--phrase\] 
\[ "a"- \[pos :determiner, 
number:sizlgular, root::"a"\]\] 
\["good"= \[pos : adjective, 
root : "good" \] \] 
\["boy"- \ [pos:noun,case:nominat ive 4, 
number : s ingu\]_ar, an imacy : anilna te, 
root: "boy", head:l\]\]\]  
Two kinds of tags are used for sentence 
taggirtg tags that t-efi:r to the whole seutence 
and tags for clause elen~ents. Sentences are 
assigned yah.los of such restricting parameters a ; 
3We inchlde hl the prerequisite knowledge as much 
overtly listed linguistic information as possiMe, to 
avoid the necessity of atmmmtic morphological 
analysis and generation which caililot guar'_iiltec abso-- 
\[utcly correct results. This is possible title tO a Sll/all 
size of  the Icxhson used for syntax exarnples. 
'<As we rise i:i set o\[" t-lnglish NPs out of  context, we 
believe tl-lat every phra,'~c will be understood as being 
hi tile noininative case. 
"clause type," "?voice," "tense" and "aspect". 
(Ganse elements are tagged with the vahie of the 
diagnostic paraineter "'syntactic functiotf' and 
wllues of tile restricting parameters "chtuse 
element realizatiol<" "animacy" and 
"definiteness". Clause elements also inherit 
sontellce lags. Senloncos are tagged in Boas as 
shown by the following exatnple (the 17.)im of 
representation is ;l typed feature structure): 
\["the boy give<~ a book to his teacher":: 
\[structure:sentence, form:af f i rmat ive,e l  
ause-type:main, voice: act ive 
tense:present, aspect: indef in i te\]  
\["the boy"= if unct ion:subject ,  
real izat ion:noun-phrase,  
animacy : animate, 
def in i teness:def in i te,  head- 
root : "boy" \] \] 
\["gives"= \[function:verb, 
real izat ion:verb, head-root: "give"\]\] 
\["a book"= \ [ funct ion:direct-object,  
real izat ion : noun-phrase, 
animacy : inanimate, 
def in i teness: indef in i te,  head- 
root : "boo\]<" \] \] 
\["t:o his teacher"- 
\[ function : indirect-obj ect, 
real izat ion:preposi t ional -phrase,  
animacy : animate, 
def in i teness:def in i te,  head- 
root : "teacher" \] \] \] 
Following tile expectation-driven methodology 
tile sets (if pretagged noun phrases and sentences 
are sclected to cover many though, admittedly, 
not all expected cotnbinations of parameter 
wihles for every phrase or sentence. The 
fbllowing two examples fiirther illustrate the 
Boas elicitation procedure. 
Noun phrase pattern eiieitation. The user i~ 
given a short deiinition of a noun phrase and 
asked to translate a given English phraso~ for 
example "a Xood  t~r)l' '" into S|. using tile words 
given in a small lexicon of selccled SI, lexical 
items translated Ii'om t'nglisil. In case of the 
Russian hmguage tile resuh would be: a good boy 
1113 
---> horoshij malchik. Next, Boas atitomatically 
looks tip every input SL woM in the lexicon and 
assigns part of speech and feature vahie tags to 
all the components of SL noun phrases. English 
translations of SL words help record the 
comparative order of noun phrase pattern 
constituents in SL and English and automatically 
assigns the head marker to that element of the 
SL noun phrase which is the mmslation of the 
English head. This is the final result of SL noun 
phrase pattern elicitation tbr a given English 
phrase. It includes a SL noun phrase pattern to 
be used in an MT parser and a pattern transfer 
inlbnnation for an English generator. Possible 
ambiguities, i.e., multiple sets of feature values 
for one word is resolved actively. 1he module 
can also actively check correctness of noun 
phrase translations. 
Clause structure elicitation includes order of 
the words, subject markers (diagnostic feature 
values or particles), direct object markers, verb 
markers, and clause element agreement. Just like 
in the case of noun phrases, the user is asked to 
translate a given English phrase into SL using 
the words given in the lexicon. For the English 
sentence used in the example above the Russian 
translation will be: 
the boy gives a book to his  teacher --- 
> malch ik  daet knigu uchi te l ju  
As soon as this is done, Boas presents the user 
with English phrases corresponding to clause 
elements of the translated sentence, so that for 
every English-SL pair of sentences the user 
types in (or drags from the sentence translation) 
corresponding SL phrases, thus aligning clause 
elements.After the ractive alignment is done, the 
system automatically: 
? transfers the clause element ags fiom 
English to SL 5. 
* nmrks the heads of every SI, chmse 
elernent, and 
o assigns feature values to the heads of 
clause elements. 
STiffs proved to be working in our experiment with I 1 
langtmgcs, such as French, Spanish, German, Rus- 
Si;:ill, tJkiliiili.~tll. Scrbo-Croatian, Chinese, l>crsiurl, 
Turkish, Arabic, and \[ lindi. 
assigns sentence restricting parameter 
values (clause type, voice, tense and 
aspect, the last three are ligature values 
of the verb). 
In the case of assignment of multiple sets of 
feature values the user is asked to disambiguate . 
them. As a result, every SL clause element is 
now tagged with certain values of diagnostic and 
restricting tags. The system stores these results 
as mternal knowledge represenmtion, i  the fi, mn 
of a feature structure, for further processing. For 
example, tbr the above English-Russian 
sentence pair the mediate results (not shown to 
the user) will be: 
\["malchik daet knigu 
uchitelju":\[ s t ructure :sentence ,  
form: a f f i rmat ive ,  c lause-  
type :main, vo ice  : active, 
tense :present, 
aspect  : imper fect ive  \] 
\[ "malchik '= 
\[function: subject, realization : noun-- 
phrase, animacy:animate, 
head-l, root:'malchik', 
case:nominative, number:singular, 
gender:masculir~e, person:third\]\] 
\["dae~'= \[function:verb, 
realization:verb, head-root:"davat'" 
,number:singular, 
person:third\]\] 
\["kniqu"- \[function:direct-object, 
realization:noun-phrase, 
animacy:inanimate, head-root:'kniga', 
case:accusative, number:singular, 
gender:feminine, person:third\]\] 
\ ["uchitel ju"= \[function:indirect- 
object, realization:noun phrase, 
animacy:animate, head-root:"uchitel'", 
case:dative, number:singular, 
gender:masculine, person:third\]\]\] 
This data is fiu-ther automatically processed to 
obtain tile kind of knowledge which can be tised 
in tile parser or generator, that is, rules (not seen 
by the user), where the t l,,"~ht-hand side centares 
a diagnostic parameter value (word oMer, clause 
element marking, agreement marking, etc.) and 
1114 
the lefi-lmnd side contains the vahtes of 
restricting parameters which condition the use of 
the COiTesponding diagnostic parameter valtte. A 
sample rule for the Russian example above isas 
lbllows: 
DirectObjectMarkerl= SL.Ru\].e\[ 
!hs: SentenceForm\[affirmative\] 
ClauseType\[main\] 
Voice\[active\] 
Tense\[present\] 
Aspect\[imperfective\] 
Subject\[realization:noun-phrase 
animacy:animate\] 
DirectObject\[realization:noun. 
phrase animacy:inanimate\], 
rhs:<:SLDirectObjectMarker\[case:accus 
ative\] :>\]; 
These results are presented to the user for 
approval in a readable form? In Russian these 
rifles mean the tbllowing: 
in the a././b+mative s ntence, mai/7 claltse, active 
voice, present ense, when the xuO/ect is realized 
as NP mid animate and direct c:/?/ect i:+" r'caliT.;ed 
as NI" and itumimcite, 
+ word order is SV(); 
? subject is in nominative case; 
* direct object is in accusative case; 
subject agrees with verb in number and 
person. 
After all the sentence translations are processed 
in this way, the rules with the same right-hand 
side are automatically combined. At the next 
stage of processing the set of values tbr every 
restricting parameter in the right-hand side of the 
combined rule is checked on completeness. This 
means that in Rttssian in the affinnative main 
clause the prelbrred word order is SVO. The 
final result:; are presented l+or the ttser lbr 
al->t-woval or editing. 
Conclusion 
Boas i+; implemented as a WWW-based Ihce, 
using IHMI+, Java Scripts and Purl. \]ks of 
November 1999, the coverage of Boas inchtdes 
the elicitation of inflectional moq~hology, 
moq'~hotactics+, opcn.-chms and closed-.class 
lexical items. Work on tokenization and proper 
names, syntax and feature and syntactic transfer +
is under way. Initial experiments have been 
completed on producing operational knowledge 
from the declarative knowledge licited through 
Boas. Testing and ewduation of the sysem have 
been platmed, and its results will be reported 
separately. 
Acknowledgments 
Research for this paper was supported in part by 
Contract MDA904-97-C-3976 from the US 
Department of Defense. Thanks to Jim Cowie 
and R6mi Zajac lbr many fi-uitful discussions of 
the issues related both to Boas proper and to the 
MT environment in which it operates. 
References 
Brill, E., D Magerman, M Marcus and B Santorini. 
(1990) Deducing Linguistic Structure from the 
Statistics of Large Corpora. Proceedings of the 
29th Annual Meeting of the Association for 
Computational Linguistics. Berkeley. CA. 
Brown, P., J+ Cocke, 5. Della Pietra, V. Delhi Pietra, 
F. Jelinek, J.D. l+afferty, P,.\[.. Mercer and P.S. 
Roossin. 1990. A statistical approach to machine 
translation. Computational 1.ingttistics, 16: 79-85. 
Chomsky, N. 1981. \[.ecturcs on Government and 
Binding. Dordrecht: Foris. 
Chomsky, N. 1995. The Minimalist Program. 
Cambridge, MA: Mrr Press. 
Goldsmith, J. 1998. Unsupervised l~carning of the 
Morphology of a NatLtral Language. http://humani- 
tics.uchicago.edu/facuhy/gohtsnlith/Atttonaorpholo 
gy/Papcr.doc 
Jones, D. and R.Hawilla. 1998. Twisted Pair 
(\]rammar: Support for Rapid Development of 
Machine Translation fin lmw Density l.anguagcs. 
AMTA'gg. 
Mcl'cuk I. 1988. Dependency Syntax: Theory and 
Practice. State University of New York 1Press, 
Albany. 
Nircnbt,rg, Scrgci 1996. Supply-side and demand- 
side Icxical semantics. Introduction to the 
Workshop on thcadth and Depth of Semantic 
LcxJcollS at AC|f196. 
Xia, Fei, M. Pahner, and K.Vijay-Shankcr. 1999. 
Towards SCllli-atltonlatic (hammar l)evelopmcnt 
Proceedings of tile Natnral |,angnl.{e Processing 
Pacific Rim Symposium. Bc(jing, China. 
1115 
Natural Language Analysis of Patent Claims 
Svetlana Sheremetyeva 
Department of Computational Linguistics 
Copenhagen Business School,  
Bernhard Bangs Alle 17 B,  
DK-2000, Denmark 
lanaconsult@mail.dk 
 
 
 
 
Abstract 
We propose a NLP methodology for ana-
lyzing patent claims that combines sym-
bolic grammar formalisms with data-
intensive methods while enhancing analy-
sis robustness. The output of our analyzer 
is a shallow interlingual representation 
that captures both the structure and con-
tent of a claim text. The methodology can 
be used in any patent-related application, 
such as machine translation, improving 
readability of patent claims, information 
retrieval, extraction, summarization, gen-
eration, etc. The methodology should be 
universal in the sense that it could be ap-
plied to any language, other parts of pat-
ent documentation and text as such. 
1 Introduction 
An exploding volume of patent applications makes 
essential the use of adequate patent processing 
tools that could provide for better results in any 
field of patent related activity. NLP techniques 
associated with specificity of patent domain have 
promise for improving the quality of patent docu-
ment processing. 
     Though it is generally recognized that the patent 
domain features overwhelmingly long and com-
plex sentences and peculiar style (Kando, 2000) 
only a few researchers really rely on the linguistic 
specificity of patent style (vs. technical style) when 
processing patent documentation  (Shnimory et al, 
2002; Gnasa and Woch, 2002; Fujii and Ishikawa, 
2002). 
     Developing natural language analyzers for pat-
ents (with at least one or any combination of mor-
phological, syntactic and semantic modules) is a 
basic task. The ultimate task of such analysis is to 
build a kind of possibly unambiguous content rep-
resentation that could further be used to produce 
higher quality applications.  
Broad coverage syntactic parsers with good 
performance have recently become available 
(Charniak, 2000; Collins, 2000), but they are not 
trained for patents. Semantic parsing is considera-
bly less developed and shows a trend to rely on 
ontologies rather then semantic primitives. (Gnasa 
and Woch, 2002). 
This paper reports on on-going project whose 
goal is to propose a NLP methodology and an ana-
lyzer for patent claims. The claim is the focal point 
of a patent disclosure, - it describes essential fea-
tures of the invention and is the actual subject of 
legal protection. 
The methodology we suggest combines sym-
bolic grammar formalisms with data-intensive 
knowledge while enhancing analysis robustness. 
The output of our analyzer is a shallow interlingual 
representation that captures both the structure and 
content of a claim text. It can be used in any pat-
ent-related application, such as machine translation 
improving readability of patent claims, information 
retrieval, extraction, summarization, generation, 
etc. The methodology should be universal in the 
sense that it could be applied to any language, 
other parts of patent documentation and text as 
such.  
In what follows we first consider the knowledge 
base of our model describing in turn a flexible 
depth lexicon, grammar formalism, and language 
of knowledge representation for the final parse. We 
then focus on the analysis algorithm as a multi-
component procedure. To illustrate the potential of 
the methodology we further sketch two of its pos-
sible applications, namely, machine translation and 
an application for improving the readability of pat-
ent claims. We conclude with the description of the 
project status and future work. 
2 Knowledge  
The structure and content of the knowledge 
base has been designed to a) help solve analysis 
problems, ? different kinds of ambiguity, ? and 
b) minimize the knowledge acquisition effort by 
drawing heavily on the patent claim linguistic re-
strictions. 
A patent claim shares technical terminology 
with the rest of a patent but differs greatly in its 
content and syntax. It must be formulated accord-
ing to a set of precise syntactic, lexical and stylistic 
guidelines as specified by the German Patent Of-
fice at the turn of the last century and commonly 
accepted in the U.S., Japan, and other countries. 
The claim describes essential features of the inven-
tion in the obligatory form of a single extended 
nominal sentence, which frequently includes long 
and telescopically embedded predicate phrases. A 
US patent claim that we will further use as an ex-
ample in our description is shown in Figure 1. 
 
A cassette for holding excess lengths of light 
waveguides in a splice area comprising a cover 
part and a pot-shaped bottom part having a bottom 
disk and a rim extending perpendicular to said 
bottom disk, said cover and bottom parts are su-
perimposed to   enclose jointly an area forming a 
magazine for excess lengths of light waveguides, 
said cover part being rotatable in said bottom part, 
two guide slots formed in said cover part, said 
slots being approximately radially directed, guide 
members disposed on said cover part, a splice 
holder mounted on said cover part to form a ro-
tatable splice holder. 
 
Figure 1. A US patent claim text.  
 
In our system the knowledge is coded in the sys-
tem lexicon, which has been acquired from two 
kinds of corpora, - a corpus of complete patent dis-
closures and a corpus of patent claims. The lexicon 
consists of two parts: a shallow lexicon of lexical 
units and a deep (information-rich) lexicon of 
predicates. Predicates in our model are words, 
which are used to describe interrelations between 
the elements of invention. They are mainly verbs, 
but can also be adjectives or prepositions. 
2.1 Shallow Lexicon 
The word list for this lexicon was automatically 
acquired from a 5 million-word corpus of a US 
patent web site. A semi-automatic supertagging 
procedure was used to label these lexemes with 
their supertags.  
Supertagging is a process of tagging lexemes 
with labels (or supertags), which code richer in-
formation than standard POS tags. The use of su-
pertags, as noted in (Joshi and Srinivas, 1994) 
localizes some crucial linguistic dependencies, and 
thus show significant performance gains. The con-
tent of a supertag differs from work to work and is 
tailored for the needs of an application. For exam-
ple, Joshi and Srinivas (1994) who seem to coin 
this term use elementary trees of Lexicalized Tree-
Adjoining Grammar for supertagging lexical items. 
In (Gnasa and Woch, 2002) it is grammatical struc-
tures of the ontology that are used as supertags. 
In our model a supertag codes morphological 
information (such as POS and inflection type) and 
semantic information, an ontological concept, de-
fining a word membership in a certain semantic 
class (such as object, process, substance, etc.). For 
example, the supertag Nf shows that a word is a 
noun in singular (N), means a process (f), and does 
not end in ?ing. This supertag will be assigned, for 
example, to such words as activation or alignment. 
At present we use 23 supertags that are combina-
tions of 1 to 4 features out of a set of 19 semantic, 
morphological and syntactic features for 14 parts 
of speech. For example, the feature structure of 
noun supertags is as follows: 
 
Tag [ POS[Noun  
                  [object   [plural, singular]  
 process [-ing, other[plural, singular]] 
substance [plural, singular] 
other       [plural, singular]]]]]  
 
In this lexicon the number of semantic classes 
(concepts) is domain based. The ?depth? of su-
pertags is specific for every part of speech and 
codes only that amount of the knowledge that is 
believed to be sufficient for our analysis procedure. 
That means that we do not assign equally ?deep? 
supertags for every word in this lexicon. For ex-
ample, supertags for verbs include only morpho-
logical features such as verb forms (-ing form, -ed 
form, irregular form, finite form). For finite forms 
we further code the number feature (plural or sin-
gular). Semantic knowledge about verbs is found 
in the predicate lexicon. 
2.2 Predicate Lexicon      
 This lexicon contains reach and very elaborated 
linguistic knowledge about claim predicates and 
covers both the lexical and, crucially for our sys-
tem, the syntactic and semantic knowledge. Our 
approach to syntax is, thus, fully lexicalist. Below, 
as an example, we describe the predicate lexicon 
for claims on apparatuses. It was manually ac-
quired from the corpus of 1000 US patent claims. 
     Every entry includes the morphological, seman-
tic and syntactic knowledge.  
      Morphological knowledge contains a list of 
practically all forms of a predicate that could only 
be found in the claim corpus.  
Semantic knowledge is coded by associating 
every predicate with a concept of a domain-tuned 
ontology and with a set of case-roles. The semantic 
status of every case-role is defined as ?agent?, 
?place?, ?mode?, etc. The distinguishing feature of 
the case frames in our knowledge base is that 
within the case frame of every predicate the case 
roles are ranked according their weight calculated 
on the basis of the frequency of their occurrence in 
actual corpus together with the predicate. The set 
of case-roles is not necessarily the same for every 
predicate.  
Syntactic knowledge includes the knowledge 
about linearization patterns of predicates that codes 
both the knowledge about co-occurrences of predi-
cates and case-roles and the knowledge about their 
liner order in the claim text. Thus, for example, the 
following phrase from an actual claim: (1: the 
splice holder) *: is arranged (3: on the cover part) 
(4: to form a rotatable splice holder) (where 1, 3 
and 4 are case role ranks and ?*? shows the posi-
tion of the predicate), will match the linearization 
pattern (1  * 3 4).  Not all case-roles defined for a 
predicate co-occur every time it appears in the 
claim text. Syntactic knowledge in the predicate 
dictionary also includes sets of most probable fill-
ers of case-roles in terms of types of phrases and 
lexical preferences. 
2.3 Grammar and Knowledge Representation 
In an attempt to bypass weaknesses of different 
types of grammars the grammar description in our 
model is a mixture of context free lexicalized 
Phrase Structure Grammar and Dependency 
Grammar formalisms.  
Our Phrase Structure Grammar consists of a 
number of rewriting rules and is specified over a 
space of supertags. The grammar is augmented 
with local information, such as lexical preference 
and some of rhetorical knowledge, - the knowledge 
about claim segments, anchored to tabulations, 
commas and a period (there can only be one rhet-
orically meaningful period in a claim which is just 
one sentence). This allows the description of such 
phrases as, for example, ?several rotating, spin-
ning and twisting elements?. The head of a phrase 
(its most important lexical item) is assigned by a 
grammar rule used to make up this phrase.  
The second component of our grammar is a 
version of Dependency Grammar. It is specified 
over the space of phrases (NP, PP, etc.) and a resi-
due of ?ungrammatical? words, i.e., words that do 
not satisfy any of the rules of our Phrase Structure 
Grammar. 
The Dependency Grammar in our model is a 
strongly lexicalized case-role grammar. All syntac-
tic and semantic knowledge within this grammar is 
anchored to one type of lexemes, namely predi-
cates  (see Section 2.2). This grammar assigns a 
final parse (representation) to a claim sentence in 
the form: 
 
text::={ template){template}* 
template::={label predicate-class predicate ((case-
role)(case-role)*} 
case-role::= (rank status value)  
value::= phrase{(phrase(word supertag)*)}* 
where label is a unique identifier of the elemen-
tary predicate-argument structure (by convention, 
marked by the number of its predicate as it appears 
in the claim sentence, predicate-class is a label of 
an ontological concept, predicate is a string corre-
sponding to a predicate from the system lexicon, 
case-roles are ranked according to the frequency 
of their cooccurrence with each predicate in the 
training corpus, status is a semantic status of a 
case-role, such as agent, theme, place, instrument, 
etc., and value is a string which fills a case-role. 
Supertag is a tag, which conveys both morphologi-
cal information and semantic knowledge as speci-
fied in the shallow lexicon (see Section 2.1). Word 
and phrase are a word and phrase (NPs, PPs, etc.) 
in a standard understanding. The representation is 
thus quite informative and captures to a large ex-
tent both morpho-syntactic and semantic properties 
of the claim. 
For some purposes such set of predicate tem-
plates can be used as a final claim representation 
but it is also possible to output a unified represen-
tation of a patent claim as a tree of predicate-
argument templates. 
3 Analysis algorithm 
The analyzer takes a claim text as input and after a 
sequence of analysis procedures produces a set of 
internal knowledge structures in the form of predi-
cate-argument templates filled with chunked and 
supertagged natural language strings. The imple-
mentation of an experimental version is being car-
ried out in C++. In further description we will use 
the example of a claim text shown in Figure 1. 
The basic analysis scenario for the patent claim 
consists of the following sequence of procedures: 
? Tokenization 
? Supertagging 
? Chunking 
? Determining dependencies 
Every procedure relies on a certain amount of 
static knowledge of the model and on the dynamic 
knowledge collected by the previous analyzing 
procedures.  
The top-level procedure of the claim analyser is 
tokenization. It detects tabulation and punctuation 
flagging them with different types of ?border? tags. 
Following that runs the supertagging procedure, - 
a look-up of words in the shallow
 
 
 
 
 
 
Figure 2. A screenshot of the developer tool interface, which shows traces of chunking noun, prepo-
sitional, adverbial, gerundial and infinitival phrases in the claim text shown in Figure 1.   
lexicon (see Section 2.1). It generates all possible 
assignments of supertags to words.  
Then the supertag disambiguation procedure at-
tempts to disambiguate multiple supertags. It uses 
constraint-based hand-crafted rules to eliminate 
impossible supertags for a given word in a 5-word 
window context with the supertag in question in 
the middle. The rules use both lexical, ?supertag? 
and  ?border? tags knowledge about the context. 
The disambiguation rules are of several types, not 
only ?reductionistic? ones. For example, substitu-
tion rules may change the tag ?Present Plural? into 
?Infinitive? (We do not have the ?Infinitive? fea-
ture in the supertag feature space). If there are still 
ambiguities pending after this step of disambigua-
tion the program outputs the most frequent reading 
in the multiple supertag. 
After the supertags are disambiguated the chunk-
ing procedure switches on. Chunking  is carried  
out  by  matching  the strings of  supertags 
against patterns in the right hand side of the rules 
in the PG component of our grammar. ?Border? 
tags are included in the conditioning knowledge. 
    During the chunking procedure we use only a 
subset of PG rewriting rules. This subset includes 
neither the basic rule ?S = NP+VP?, nor any rules 
for rewriting VP.  This means that at this stage of 
analysis we cover only those sentence components 
that are not predicates of any clause (be it a main 
clause or a subordinate/relative clause). We thus do 
not consider it the task of the chunking procedure 
to give any description of syntactic dependencies.  
     The chunking procedure is a succession of 
processing steps itself starting with the simple-
noun-phrase procedure, followed the complex- 
noun-phrase procedure, which integrates simple 
noun phrases into more complex structures (those 
including prepositions and conjunctions). Then the 
prepositional-, adverbial-, infinitival- and gerun-
dial-phrase procedures switch on in turn.  
 
 
 
 
Figure 3. A fragment of the final parse of the sentence in Figure 1. Fillers of the ?direct-obj? 
case-role are long distance dependencies of the predicate ?comprising?. 
 
The order of the calls to these component proce-
dures in the chunking algorithm is established to 
minimize the processing time and effort. The or-
dering is based on a set of heuristics, such as the 
following. Noun phrases are chunked first as they 
are the most frequent types of phrases and many 
other phrases build around them. Figure 1 is a 
screenshot of the interface of the analysis grammar 
acquisition tool. It shows traces of chunking noun, 
prepositional, adverbial, gerundial and infinitival 
phrases in the example of a claim text shown in the 
left pane of Figure 3.   
The next step in claim analysis is the procedure 
determining dependencies.  At this step in addition 
to PG we start using our DG mechanism. The pro-
cedure determining dependencies falls into two 
components: determining elementary (one predi-
cate) predicate-argument structures and unifying 
these structures into a tree. In this paper we?ll limit 
ourselves to a detailed description of the first of 
these tasks. 
The elementary predicate structure procedure, 
in turn, consists of three components, which are 
described below.  
The fist find-predicate component searches for 
all possible predicate-pattern matches over the 
?residue? of ?free? words in a chunked claim and 
returns flagged predicates of elementary predicate-
argument structures. The analyzer is capable to 
extract distantly located parts of one predicate (e.g. 
?is arranged? from ?A is substantially vertically 
arranged on B?).  
The second find-case-roles component retrieves 
semantic (case-roles) and syntactic dependencies 
(such as syntactic subject), requiring that all and 
only dependent elements (chunked phrases in our 
case) be present within the same predicate struc-
ture.  
The rules can use a 5-phrase context with the 
phrase in question in the middle. The conditioning 
knowledge is very rich at this stage. It includes 
syntactic and lexical knowledge about phrase con-
stituents, knowledge about supertags and ?border? 
tags, and all the knowledge about the properties of 
a predicate as specified in the predicate dictionary. 
This rich feature space allows quite a good per-
formance in solving the most difficult analysis 
problems such as, recovery of empty syntactic 
nodes, long distance dependencies, disambiguation 
of PP attachment and parallel structures. There can 
several matches between the set of case-roles asso-
ciated with a particular phrase within one predicate 
structure. This type of ambiguity can be resolved 
with the probabilistic knowledge about case-role 
weights from the predicate dictionary given the 
meaning of a predicate.  
       If a predicate is has several meanings then the 
procedure disambiguate predicate starts, which 
relies on all the static and dynamic knowledge col-
lected so far. During this procedure, once a predi-
cate is disambiguated it is possible to correct a 
case-role status of a phrase if it does not fit the 
predicate description in the lexicon. 
Figure 3 shows the result of assigning case-
roles to the predicates of the claim in Figure 1. The 
set of predicate-arguments structures conforms the 
format of knowledge representation given in Sec-
tion 2.3. As we have already mentioned the ana-
lyzer might stop at this point. It can also proceed 
further and unify this set of predicate structures 
into a tree. We do not describe this rather complex 
procedure here and note only that for this purpose 
we can reuse the planning component of the gen-
erator described in (Sheremetyeva and Nirenburg, 
1996). 
4 Examples of possible applications 
In general, the final parse in the format shown in 
Figure 3 can be used in any patent related applica-
tion. It is impossible to give a detailed description 
of these applications in one paper. We thus limit 
ourselves to sketching just two of them, - machine 
translation and improving the readability of patent 
claims. 
    Long and complex sentences, of which patent 
claims are an ultimate example, are often men-
tioned as sentences of extremely low translatability 
(Gdaniec, 1994). One strategy currently used to 
cope with the problem in the MT frame is to auto-
matically limit the number of words in a sentence 
by cutting it into segments on the basis of the 
punctuation only. In general this results in too few 
phrase boundaries (and some incorrect ones, e.g. 
enumerations). Another well-known strategy is 
pre-editing and postediting or/ and using controlled 
language, which can be problematic for the MT 
user. It is difficult to judge 
whether current MT systems use more sophisti-
cated parsing strategies to deal with the problems 
caused by the length and complexity of 
 
 
Figure 4. A screenshot of the user interface of a prototype application for improving the readability 
of patent claims. The right pane shows an input claim (see Figure 1) chunked into predicates and 
other phrases (case-role fillers). The structure of complex phrases can be deployed by clicking on 
the ?+? sign.  The right pane contains the claim text a set of simple sentences.  
 
of real life utterances as most system descriptions 
are done on the examples of simple sentences. 
To test our analysis module for its applicability 
for machine translation we used the generation 
module of our previous application, - AutoPat, - a 
computer system for authoring patent claims 
(Sheremetyeva, 2003), and modeled a translation 
experiment within one (English) language, thus 
avoiding (for now) transfer problems 1  to better 
concentrate on the analysis proper. Raw claim sen-
tences were input into the analyzer, and parsed. 
The parse was input into the AutoPat generator, 
which due to its architecture output the ?transla-
tion? in two formats, - as a single sentence, which 
is required when a claim is supposed to be in-
                                                          
1 The transfer module (currently under development) 
transfers every individual SL parse structure into an 
equivalent TL structure keeping the format of its repre-
sentation. It then ?glues? the individual structures into a 
tree to output translation as one sentence or generates a 
set of simple sentences directly from the parse in Figure 
3. 
cluded in a patent document, and as a set of simple 
sentences in TL. The modules proved to be com-
patible and the results of such ?translation? showed 
a reasonably small number of failures, mainly due 
to the incompleteness of analysis rules. 
 The second type of the translation output (a set 
of sentences), shows how to use our analyzer in a 
separate (unilingual or multilingual) application for 
improving the readability of patent claims, which 
is relevant, for example, for information dissemi-
nation. Figure 4 is a screenshot of the user inter-
face of a prototype of such an application.  
We are aware of two efforts to deal with the 
problem of claim readability. Shnimory et. al 
(2002) investigate NLP technologies to improve 
readability of Japanese patent claims concentrating 
on rhetorical structure analysis. This approach uses 
shallow analysis techniques (cue phrases) to seg-
ment the claim into more readable parts and visual-
izes a patent claim in the form of a rhetorical 
structure tree. This differs from our final output, 
which seems to be easier to read. Shnimory et. al 
(cf.) refer to another NLP research in Japan di-
rected towards dependency analysis of patent 
claims to support analytical reading of patent 
claims. Unfortunately the author of this paper can-
not read in Japanese. We thus cannot judge our-
selves how well the latter approach works. 
5 Status and Future Work 
The analyzer is in the late stages of implementation 
as of May 2003. The static knowledge sources 
have been compiled for the domain of patents 
about apparatuses. The morphological analysis and 
syntactic chunking are operational and well tested. 
The case-role dependency detection is being cur-
rently tested and updated. The compatibility of the 
analyzer and fully operational generator has been 
proved and tested. First experiments have been 
done to use the analyzer for such applications as 
machine translation and improving claim readabil-
ity. We have not yet made a large-scale evaluation 
of our analysis module. This leaves the comparison 
between other parsers and our approach as a future 
work. The preliminary results show a reasonably 
small number of failures, mainly due to the incom-
pleteness of analysis rules that are being improved 
and augmented with larger involvement of predi-
cate knowledge. 
We intend to a) add an optional interactive 
module to the analyzer (that would allow for hu-
man interference into the process of analysis to 
improve its quality), and complete the integration 
of the analyzer into a machine translation system 
and an application for improving claim readability. 
Another direction of work is developing applica-
tions in a variety of languages (software localiza-
tion); b) develop a patent search and extraction 
facility on the basis of the patent sublanguage and 
our parsing strategy.   
References 
Akiro Shnimori, Manabu Okumura,Yuzo Marukawa, 
and Makoto IwaYama. 2002. Rethorical Structure 
Analysis of Japanese Patent Claims Using Cue 
Phrases. Proceedings of the Third NTRCIR Work-
shop. 
Aravind K.Joshi and Bangalore Srinivas. 1994. Disam-
biguation of Super Parts of Speech (or Supertags): 
Almost Parsing. http://acl.ldc.upenn.edu/C/C94/C94-
1024.pdf 
Atstushi Fujii and Tetsuya Ishikawa. 2002. NTCIR-3 
Patent Retrieval Experiments at ULIS. Proceedings 
of the Third NTRCIR Workshop. 
Claudia Gdaniec. 1994. The Logos Translatability Index 
in Technology  Partnerships for Crossing the Lan-
guage Barrier. Proceedings of the First Conference 
of the Association for Machine Translation in the 
Americas (AMTA). 
Don Blaheta and Eugene Charniak. 2000. Assigning 
Function Tags to Parsed Text. Proceedings of the 
North American Chapter of the Association of Com-
putational Linguistics. 
Eugene Charniak. 2000. A Maximum-entropy-inspired 
Parser. Proceedings of the North American Chapter 
of the Association of Computational Linguistics. 
Michael Collins. 2000. Discriminative Reranking for 
Natural Language Parsing. Machine Learning: Pro-
ceedings of the Seventeenth International  Confer-
ence (ICML 2000), Stanford California. USA 
Melanie Gnasa and Jens Woch. 2002. Architecture of a 
knowledge based interactive Information Retrieval 
System. http://konvens2002.dfki.de/cd/pdf/12P-
gnasa.pdf  
Noriko Kando. 2000. What Shall we Evaluate? Prelimi-
nary Discussion for the NTCIR Patent IR Challenge 
(PIC) Based on the Brainstorming with the Special-
ized Intermediaries in Patent Searching and Patent 
Attorneys. Proceedings of the ACM SIGIR 2000 
Workshop on Patent Retrieval in conjunction with 
The 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information 
retrieval. Athens. Greece 
Svetlana Sheremetyeva and Sergei Nirenburg. 1996. 
Generating Patent Claims. Proceedings of the 8th In-
ternational Workshop on Natural Language Genera-
tion. Herstmonceux, Sussex, UK. 
Svetlana Sheremetyeva 2003. Towards Designing Natu-
ral Language Interfaces. Proceedings of the 4th Inter-
national Conference ?Computational Linguistics and 
Intelligent Text Processing? Mexico City, Mexico 
Application Adaptive Electronic Dictionary with Intelligent Interface 
Svetlana Sheremetyeva 
Copenhagen Business School, 
LanA Consulting 
Madvigs Alle, 9, 2 
Copenhagen, Denmark, DK-1829 
lanaconsult@mail.dk 
 
Abstract 
The paper presents an electronic dictionary 
that can be adapted to the needs of different 
NLP applications. It suggests some ways to 
save on software customisation and 
acquisition effort through an intelligent 
developer interface. The emphasis is made on 
the flexibility of data representation, handling 
and access speed. 
1 Introduction 
In this paper we try to contribute to the problem  
of electronic dictionaries with a case study, - 
TransDict, - a multilingual lexicon for a family of 
patent-related NLP applications, such as AutoPat, 
APTrans and AutoRead1. TransDict thus conforms 
to the ?Multilingual-Specialized? dictionary 
paradigm (S?rasset, 1993), but it can also be used 
as a stand alone tool and adapted for other 
language related tasks, e.g., training computational 
linguists.  
The motivation to focuse on application tuned 
dictionaries is that though developing reusable full-
sized knowledge bases for NLP systems is highly 
desirable this process is extremely expensive and 
time consuming, and  reusability is not guaranteed. 
If an NLP system uses a restricted sublanguage, 
and, thus, can operate with smaller-scale 
dictionaries, the scope of acquisition and 
development effort will decrease correspondingly. 
Dictionary software should be adaptable to the 
specificity of sublanguages. 
The languages that are currently covered are 
English and Danish but TransDict can easily be 
extended to a multiple number of other languages.
 
TransDict features a powerful environment for 
acquisition, editing, browsing, defaulting and 
coherence checking. It is implemented in C++ as 
an integral part of 32-bit Windows applications for 
Windows 95/98/2000/NT.  
                                                     
1
 AutoPat, APTrans, AutoRead, - computer systems 
for authoring, translation and improving readability of 
paten claims, correspondingly (Sheremetyeva, 2003) 
2 Related work 
A vast amount of research in the field of 
electronic dictionaries concentrate on data 
unification, representation, organization and 
management with the major focus on 
multilingual dictionaries as, for example, in 
(Wong, 2000; Boitet et al,2002).  
Multilingual electronic dictionaries often 
include a database of cross-referenced unilingual 
dictionaries with the use of interlingua such as 
ontology (Onyshkevich and Nirenburg, 1994)) 
or a pivotal language (Boitet et al,cf.).  
The architecture of such dictionaries normally 
include a lexical database and a set of tools for 
data management, - visualisers, editors, 
defaulters, etc. (Khatchadourian, 1992). A 
user-friendly interface is one of the major issues 
still uderdeveloped (Bilac and Zock, 2003).  
XML and SGML data representation 
languages (Boitet et al, cf.) have been a 
successful approach to facilitate the export of 
electronic dictionaries to different applications 
though many dictionaries use their own internal 
data representation formats (Fedder, 1992).
  
Finally, it is desirable for electronic 
dictionaries to be stand-alone modules with 
defined interfaces for interaction with other 
linguistic applications (Pointer project report, 
http://www.computing.surrey.ac.uk/ai/pointer).   
3 Overview of TransDict 
3.1 Feature space 
   TransDict is originally built over a set of features 
relevant for the patent  applications including: 
    Semantic features: SEM_Cl - semantic class, 
CASE_ROLEs, - a set of case-roles associated 
with a lexeme, if any). 
    Syntactic features: FILLERs, - sets of most 
probable fillers of case-roles in terms of types of 
phrases and lexical preferences. 
  Linking features: PATTERNs, - linearization 
patterns of lexemes that code both the knowledge 
about co-occurrences of lexemes with their case-
roles and the knowledge about their linear order. 
 Figure 1. An overall architecture of TransDict. 
 
Morphological features: POS, - part of speech, 
MORPH, - wordforms, number, gender, etc.; the 
sets of parts of speech and wordforms are domain 
and application specific (Sheremetyeva, cf.).  
Rank feature: RANK, - corpus-based frequency 
within one semantic class. The more frequent is a 
lexeme, the less its rank. 
3.2 Organization and architecture 
   TransDict includes cross-referenced 
monolingual lexicons for every language. A 
monolingual dictionary consists of a set of entries. 
An entry identifies lexical information for one 
meaning of a lexeme of a given language. Every 
entry is maximally defined as a tree of features: 
  
SEM-CL[Language[POS RANK  
[MORPH CASE_ROLE  FILLER  PATTERN] 
 
The CASE_ROLE , FILLER and  PATTERN 
features might not be specified in certain entries, 
e.g., for nouns-physical objects. 
A maximal entry has the following fields: 
entry::=  
semantics SEM_CL 
language LANGUAGE 
part of speech POS 
major-form string TAG 
other-forms {string TAG}+  
case-frame {CASE_ROLE}+ 
filler {CASE_ROLE{FILLER}+}+ 
patterns {PATTERN}+ 
frequency  RANK  
translation{cross-linguistic equivalent entry 
index}+ 
 
TAG is a label to code several features: POS, 
number, inflection type and semantic class: 
object, event, etc., providing for powerful tagging. 
The architecture of TransDict is shown in 
Figure 1. All information is stored in TransDict 
internal formats: in data files and index files. The 
developer works with the Main Dictionary File 
(MDF) visualised by the interface (Figure 2). 
  
Figure 2. A screenshot of the TransDict interface displaying the entry for the lexeme ?connected? 
 
When the lexicographer saves the data multiple 
extractions from MDF are automatically created. 
These extractions contain different data subsets 
relevant for different processing steps (tagging, 
disambiguation, transfer and generation). The 
extractions are created for every language and for 
every pair of languages. They are linked to 
applications by special DLL (dynamic link 
library) functions that access only one of the 
dictionary extractions for every processing step. 
This approach gives a significant increase in 
access speed and processing, which is crucial for 
real world systems. This and the fact that 
TransDict is implemented for PC motivated our 
choice not to use the SQL database and XML 
(which would have slowed down the application 
performance). It does not mean, however, that 
TransDict could not be used in the on-line regime. 
An interface and a dll can be written for this 
purpose. 
4 Supporting tools 
We developed the following TransDict tools: 
Data importer/merger imports wordlists and/or 
feature values from external files and applications. 
For example, the tool is pipelined to a tagger and 
to AutoPat and AutoTrans user interfaces, to 
automatically import unknown words. 
 Defaulter automatically assignes entry 
structures and some of feature values to entries.  
Editor a) edits feature values in an entry and b) 
edits dictionary settings, - languages, semantic 
classes, parts of speech, wordforms and their tags. 
Any change of settings automatically propagates 
to corresponding entries.  
Morphological generator automatically 
generates wordforms for a given word base form. 
Content and format checker reveals incomplete 
and/or bad formatted entries.  
Look-up tool performs wild card search and 
search on any combination of specified 
parameters. 
5  Interface design 
A lexicographer interacts with the lexicon by an 
extemely user-friendly interface (Figure2). The 
left pane of the interface screen contains a 
scrollable list of lexeme base forms2 in a selected 
language. A click on a language bookmark over  
                                                     
2
 For convenience other wordforms are not included 
in this list but can be displayed on mouse click.  
  
 
Figure 3. A fragment of English and Danish 
equivalent entries as shown in the interface. 
 
the morphological zone displays an entry in the 
selected language equivalent to a highlighted 
word in the left column. All supporting tools are 
accessed  through the interface menus.  
The ?Add? button calls pop-up menus where 
the developer is prompted to select a semantic 
class and part-of speech. This done, an entry with 
a relevant structure, tags and default values will 
be displayed. After the user types in a base form 
all other wordforms are automatically generated 
on mouse click. The developer is to review the 
default knowledge and edit it if necessary. The 
content and format checker take care of correct 
descriptions with different kinds of alert messages 
and rewriting support. Powerful search can be 
done both in a look-up and edit mode. 
 Changing the dictionary settings can easily 
change a base form status of a  wordform, the 
structure of the entry and other specification 
parameters. Figure 3 shows how the default noun 
entry with two slots for its morphological forms: 
singular and plural, is reset for Danish where 
definiteness is expressed morphologically, thus 
duplicating the number of members of the noun 
paradigm compared with English. 
6 Conclusion 
In this paper we described an on-going project 
on developing a multilingual electronic 
dictionary, - TransDict, integrated with patent 
domain applications. We focused on such effort 
saving strategies as knowledge organization, 
access, reusability, support tools and interface 
design. As of now (April 2004) the dictionary 
program including intelligent application adaptive 
interface integrated with supporting tools and 
external applications, - AutoPat, AutoTrans, 
AutoRead (Sheremetyeva, cf.) is fully 
implemented and tested. This ?shell? can now be 
used to create any number of dictionaries with 
different feature spaces.   
The TransDict patent domain knowledge base 
currently contains about 60,000 completed 
English entries and around 100 equivalent Danish 
entries that are directly used in testing analysis, 
transfer and generation modules for the English-
Danish machine translation system.  We plan to 
increase the English-Danish knowledge base to a 
product size level by December 2004. 
  TransDict (with patent domain or other 
knowledge) can be used as a stand-alone tool, for 
other applications e.g., for training computational 
linguists.  
References  
 S.Bilac and M.Zock. 2003. Towards a user-
friendly dictionary interface. Papillon 2003 
Workshop, 3-5 July, NII, Sapporo, Japan. 
C.Boitet, M.Mangeot-Lerebours and G.S?rasset. 
2002. The PAPILLON project: cooperatively 
building a multilingual lexical data-base to 
derive open source dictionaries & lexicons. 
Proceedings of the 2nd Workshop NLPXML 
2002, Post COLING 2002 Workshop. Taipei. 
L.Fedder.1992, The Multilex Internal Format. 
Multilex report, June. 
H. Khatchadourian 1992, Tools, functional 
specifications. Multilex report, February. 
B. Onyshkevich and S. Nirenburg. 1994. The 
lexicon in the scheme of KBMT things. 
Thechnical report MCCS-94-277, CRL, NMSU. 
G. S?rasset. 1993. Recent Trends of Electronic 
Dictionary Research and Development in 
Europe, Technical Memorandum Electronic 
Dictionary Research (EDR), Tokyo, Japan. 
S. Sheremetyeva. 2003. Natural Language 
Analysis of Patent Claims. Proceedings of the 
workshop  ?Patent Corpus Processing? in 
conjunction with 41st Annual Meeting of the 
Association for Computational Linguistics (ACL 
2003), Sapporo. Japan, July 7-12. 
K.Wong.2000. Multilingual Electronic Dictionary 
Project.http://www.csse.monash.edu.au/hons/pr
ojects/2000/Kevin.Wong/ksgw.htm 
Workshop on Humans and Computer-assisted Translation, pages 22?27,
Gothenburg, Sweden, 26 April 2014. c?2014 Association for Computational Linguistics
On-The-Fly Translator Assistant                                             
(Readability and Terminology Handling) 
 
 
Svetlana Sheremetyeva 
National Research South Ural State University / pr.Lenina 74, 454080 
Chelyabinsk, Russia 
LanA Consulting ApS/ Moellekrog 4, Vejby, 3210, Copenhagen, Denmark 
lanaconsult@mail.dk 
 
  
 
Abstract 
This paper describes a new methodology for 
developing CAT tools that assist translators of 
technical and scientific texts by (i) on-the-fly 
highlight of nominal and verbal terminology in a 
source language (SL) document that lifts possible 
syntactic ambiguity and thus essentially raises the 
document readability and (ii) simultaneous 
translation of all SL document one- and multi-
component lexical units.  The methodology is 
based on a language-independent hybrid extraction 
technique used for document analysis, and 
language-dependent shallow linguistic knowledge. 
It is targeted at intelligent output and 
computationally attractive properties. The approach 
is illustrated by its implementation into a CAT tool 
for the Russian-English language pair. Such tools 
can also be integrated into full MT systems. 
1 Introduction 
Exploding volume of professional publications 
demand operative international exchange of 
scientific and technical information and thus put 
in focus operativeness and quality of translation 
services. In spite of the great progress of MT that 
saves translation time, required translation 
quality so far cannot be achieved without human 
judgment (Koehn, 2009). Therefore in great 
demand are CAT tools designed to support and 
facilitate human translation.  
CAT tools are developed to automate 
postediting and often involve controlled 
language. The most popular tools are translation 
memory (TM) tools whose function is to save the 
translation units in a database so that they can be 
re-used through special "fuzzy search" features. 
The efficiency of TM (as well as translation 
quality as such) is directly related to the problem 
of the comprehensiveness of multilingual 
lexicons.  A translator who, as a rule, does not 
possess enough of expert knowledge in a 
scientific or technological domain spends about 
75% of time for translating terminology, which 
do not guarantee the correctness of translation 
equivalents she/he uses.  The percentage of 
mistakes in translating professional terminology 
reaches 40% (Kudashev, 2007). It is therefore 
essential to develop methodologies that could 
help human translators solve this problem, the 
huge resource being the Internet, if properly 
used.  In this paper we suggest one of the 
possible ways to do so.  
We would like to address the importance of 
text readability in the human translation 
performance. Readability relates to (though does 
not coincide with)   the notion of translatability    
in MT research. Readability in human translation 
is associated with the level of clarity of a SL text 
for human understanding.  Every translator 
knows how difficult it can be to understand 
professional texts, not only because of the 
abundance of terminology but also due to 
complex syntax and syntactic ambiguity. The 
ultimate example of a low readability text is the 
patent claim (Shinmori et al., 2003) that is 
written in the form of one nominal sentence with 
extremely complex ?inhuman? syntactic 
structure that can run for a page or more.  Low 
readability is often the case with scientific and 
technical papers as well.  
22
In this paper we describe our effort to develop a 
portable between domains and languages CAT 
tool that can on-the-fly improve the readability 
of professional texts and provide for reliable 
terminology translation.  
We paid special attention to multiword noun 
terminology, the most frequent and important 
terminological unit in special texts that can rarely 
be   found in full in existing lexicons. When 
translated properly, multicomponent NPs do not 
only provide for the correct understanding of the 
corresponding target language (TL) term but in 
many cases lift syntactic ambiguity.  
The tool can find a broad application, e.g., it 
can be useful for any non-SL speaker for a quick 
document digest. The settings of the tool allow 
the extraction of keyword translation pairs in 
case it is needed, e.g., for search purposes. It can 
also be integrated into a full MT system.  
We implemented our methodology into a 
fully functional tool for the Russian-English 
language pair and conducted experiments for 
other domains and language pairs.  In selecting 
Russian as a first SL we were motivated by two 
major considerations. Firstly, Russia has a huge 
pool of scientific and technical papers which are 
unavailable for non-Russian speakers without 
turning to expensive translation services. 
Secondly, our scientific challenge was to develop 
a hybrid methodology applicable to inflecting 
languages. Popular SMT and hybrid techniques 
working well on configurational and 
morphologically poor languages, such as 
English, fail on non-configurational languages 
with rich morphology (Sharoff, 2004). Russian is 
an ultimate example of such a language. It has a 
free word order; a typical Russian word has from 
9 (for nouns) up to 50 forms (for verbs). In what 
follows we first present the tool and then 
describe the underlying methodology. 
 
 
 
 
Figure 1. A screenshot of the Russian-to-English CAT tool user interface at the bookmark ?show all?. 
The left pane displays a SL interactive text of a scientific paper in mathematical modelling with 
explicitly marked (bold faced) nominal terminology and verbs (in blue). The left pane contains the 
alphabetically ordered list of all 1-4 component Russian terms with their English equivalents. On the 
top of the right pane there is a type-in area which permits searching for the translations of terms longer 
than 4 words in the tool knowledge base. The second bookmark on the top of the Ru-En equivalent 
area allows opening a user dictionary for the user to collect terms she/he might need in the future.
23
2 The Tool  
The tool takes a SL text an as input and on the 
fly produces output at two levels: 
? a marked-up interactive SL text with  
highlighted multi-component nominal and 
verbal terminology (NPs and VPs); 
? a list of all single- and multi-component SL-
TL units found in the input text.  
Text mark-up improves input readability and 
helps translator quicker and better understand the 
syntactic structure of the input. This feature 
combined with on-the-fly translation of all  1-4  
component SL text lexical units reduces 
translation time and effort and raises translation 
quality. The tool can be used as an e-dictionary 
where terms are searched through a type-in area 
in the user interface.  
Translation equivalents are normalized as 
follows. SL NPs are outputted in nominative 
singular, while VPs are presented in a finite form 
keeping the SL voice, tense and number features. 
For example, in  the Russian-to-English tool  the 
Russian VP wordform ????????????????_past 
participle, perfective, plural (literally ?done?) 
will be outputted as ??????????????_ finite, 
past, plural = ?were mounted?.  
 
 
 
 
The tool user interface has a lot of effort-saving 
functionalities. A click on a unit in the marked 
up input text in the left pane highlights its TL 
equivalent in the alphabetically sorted list of 
translations on the right pane. It is possible to 
create user dictionaries accumulating 
terminology from different texts, saving these 
dictionaries and projects, etc.   A screenshot of 
the user interface in shown in Figure 1. 
3 Methodology and Development Issues 
3.1 Architecture 
 The overall architecture of the tool is shown in 
Figure 2. The tool engine consists of a shallow 
analyzer including three fully automatic 
modules, - a SL hybrid NP extractor, shallow 
parser and imbedded machine translation module 
meant to translate terminology. The knowledge 
base contains shallow linguistic knowledge, - 
lexicons and rules.  
The NP extractor is a hybrid stand-alone tool 
pipelined to the system. We built it following the 
methodology of NP extraction for the English 
language as described in (Sheremetyeva, 2009) 
and ported it to the Russian language. 
 
 
 
 
Figure 2. The architecture of the CAT tool. 
 
24
 The extraction methodology combines statistical 
techniques, heuristics and very shallow linguistic 
knowledge. The knowledge base consists of a 
number of unilingual  lexicons, - sort of extended 
lists of stop words forbidden in particular (first, 
middle or last) positions in a  typed lexical unit 
(Russian NP in our case).  
NP extraction procedure starts with n-gram 
calculation and then removes n-grams, which 
cannot be NPs by successive matching 
components of calculated n-grams against the 
stop lexicons. The extraction itself thus neither 
requires such demanding NLP procedures, as 
tagging, morphological normalization, POS 
pattern match, etc., nor does it rely on statistical 
counts (statistical counts are only used to sort out 
keywords). The latter makes this extraction 
methodology suitable for inflecting languages 
(Russian in our case) where frequencies of n-
grams are low.   
Porting the NP extractor from English to 
Russian consisted in substituting English stop 
lexicons of the tool with the Russian equivalents. 
We did this by translating each of the English 
stop lists into Russian using a free online system 
PROMT (http://www.translate.ru) followed by 
manual brush-up.   
The NP extractor does not rely on a 
preconstructed corpus, works on small texts, 
does not miss low frequency units and can 
reliably extract all NPs from an input text. We 
excluded a lemmatizer from the original 
extraction algorithm    and    kept     all extracted 
Russian NPs in their textual forms. The noun 
phrases thus extracted are of 1 to 4 components 
due to the limitations of the extractor that uses a 
4-gram model. The extractor was also used for 
lexicon acquisition. 
     The shallow parser consists of an NP 
chunker, VP chunker and tagger.  The first users 
the knowledge dynamically produced by the NP 
extractor (lists of all NPs of an input text in their 
text form). The VP chunker and tagger turn to 
the Russian entries of the tool bilingual lexicon.  
The tagger is actually a supertagger as it assigns 
supertags coding all morphological features, such 
as part-of-speech, number, gender, tense, etc.  
     The machine translation module translates 
text chunks into English using simple transfer 
and generation rules working over the space of 
supertags as found in the CAT tool bilingual 
lexicon. 
3.2 Bilingual lexicon 
To ensure correct terminology translation the 
bilingual lexicon of the tool should necessarily 
be tuned to a specific domain for which it is to be 
used. The lexicon is organized as a set of shallow 
cross-referenced monolingual entries of lexical 
units listed with their part-of-speech class and 
explicit paradigms of domain-relevant 
wordforms.   This is the type of resource that, 
once build for some other purpose, can be simply 
fed into the system. Acquisition of this type of 
knowledge for every new pair of languages is 
what existing SMT tools can provide either in 
advance or on the fly, as reported in (2012 et 
al.,). In our work striving for correctness we 
combined automatic techniques with manual 
check and manual acquisition. 
The Russian vocabulary was created in two 
steps.   First, an initial corpus of Russian 
scientific papers on mathematical modelling of 
approximately 80 000 wordforms was acquired 
on Internet. We then ported the NP extractor 
described above to other Russian parts-of-speech 
and automatically extracted domain specific 
typed lexical units (NPs, VPs, ADJs, etc) 
consisting of 1 up to 4 components from the 
corpus. These automatically extracted lists of 
lexemes were further checked by human 
acquirers and 14 000 of them were used as a seed 
Russian vocabulary. 
The seed vocabulary was then used to acquire 
longer Russian lexemes both from the initial 
corpus, and the Internet, which is in fact an 
unlimited corpus. The following methodology 
was applied. The seed lexical units were used as 
keywords in the Internet search engines.  New 
Russian terminological units including seed 
terms highlighted in the two first pages of the 
search results were included in the lexicon.  For 
example, for the seed (key) term 
????????????????? the following multi-
component terms popped-up on the Internet: 
???????????????? ??????????? ????????, 
???????????????? ?????? ? ???????????? 
???????, ???????????????? ????-?????????,  
etc. As a result, the seed Russian vocabulary was 
extended to 60 000 single- and multi-component 
units up to seven-eight words long. 
Lexical acquisition of English equivalents was 
done based on existing domain lexicons, 
parallel/comparable corpora and raw Internet 
resources. The last needs to be explained. In case 
neither existing lexicons, nor parallel/comparable 
corpora could provide for a reliable English 
25
equivalent, which was mostly the case with long 
terms, translation hypotheses were made based 
on different combinations of translation variants 
of component words. Every translation 
hypothesis was then checked in the Internet 
search engine. If an engine (we used Google) 
showed a translation version in the search results, 
the hypothesis was considered confirmed and the 
English equivalent was included in the tool 
lexicon. For example, the Russian term ??????? 
????????????? ???????? could not be found in 
any of existing lexicons, the following English 
equivalents of the Russian term components 
were found:  
??? ? swarm; ????????????? - conception, 
expression, representation, performance, 
configuration; ??????? ? bit, fraction, particle, 
shard, corpuscle. 
If you create a translation hypothesis by using 
the first translation variant for every component 
of the Russian term you will get: ?swarm 
conception of a bit? or ?bit swarm conception?. 
Used as key words in Google, the search results 
do not contain these words combined in a term. 
This translation hypothesis was rejected. Another 
hypothesis ?particle swarm representation? used 
as key words in Google gives the English term 
?Particle Swarm Optimization and Priority 
Representation? from the paper on mathematical 
modelling  by Philip Brooks, a native English 
speaker.  ?Particle swarm representation? is 
accepted as a correct English translation of the 
Russian term ??????? ????????????? 
????????. Though tedious, this methodology 
allowed careful detection of the up-to-date 
highly reliable translation that could hardly 
be achieved otherwise. 
3.3 Workflow 
The raw SL document first goes to the automatic 
NP extractor, which produces a list of one- to 
four component noun phrases. The dynamically 
created NP list is then used as knowledge for the 
NP chunker, which by matching the extracted list 
against the input text chunks (brackets) noun 
phrases in the document. The morphological 
tagger completes morphological analysis of these 
chunks by looking them up in the NP entries of 
the tool lexicon. The text strings between 
chunked NPs is then supplied to the VP chunker 
that matches this input against verb wordforms, 
as listed in the morphological zones of verb 
entries. In case of a match the text string is 
chunked as VP and a corresponding supertag 
from the lexicon is assigned. The text strings 
which were left between NP and VP chunks are 
then looked up in the rest of the entries of the 
lexicon and tagged. The fact that in every 
chunking/tagging pass only the type-relevant 
lexicon entries are searched practically lifts the 
ambiguity problem in morphological analysis. 
Finally, based on classified chunk borders, the 
document is turned into an interactive 
(?clickable?) text with NP and VP phrases 
highlighted in different colours. 
The output of the shallow analysis stage (fully 
(super) tagged lexical units) is passed to the 
machine translation module that following 
simple rules generates SL-TL lexical pairs for all 
the lexica of the text (See Figure 1).  
4 Status and Conclusions 
The viability of the methodology we have 
described was proved by its implementation in a 
Russian-English CAT tool for the domain of 
scientific papers on mathematical modelling. The 
tool is fully developed. The domain bilingual 
static knowledge sources have been carefully 
crafted based on corpora analysis and internet 
resources. The programming shell of the tool is 
language independent and provides for 
knowledge administration in all the tool modules 
to improve their performance.  
The extractor of Russian nominal terminology 
currently performs with 98, 4 % of recall and 96, 
1% precision. The shallow clunker based on the 
extraction results and lexicon shows even higher 
accuracy. This is explained, on the one hand, by 
the high performance of the NP extractor, and, 
on the other hand, by the nature of inflecting 
languages.  Rich morphology turns out to be an 
advantage in our approach. Great variety of 
morphological forms lowers ambiguity between 
NP components and verb paradigms.  
We could not yet find any publications 
describing research meant for similar output. 
This leaves the comparison between other 
methodologies/tools and ours as a future work. In 
general user evaluation results show a reasonably 
small number of failures that are being improved 
by brushing up the bilingual lexicon. 
We intend to a) improve the quality of the tool 
by updating the tool knowledge based on the user 
feedback; b) integrate the tool into a full MT 
system and  c) develop a search facility on the 
basis of the our extraction strategy. 
26
References  
Enache Ramona, Cristina Espana-Bonet, Aarne 
Ranta, Llu?s Marquez. 2012. A Hybrid System for 
Patent Translation.  Proceedings of the EAMT 
Conference.  Trento..Italy, May 
Koehn Philipp. 2009. A process study of computer-
aided translation, Philipp Koehn, Machine 
Translation Journal, 2009, volume 23, 
number 4, pages 241-263 
Kudashev Igor S. 2007. Desining Translation 
Dictionaris of Special Lexica /I.S.Kudashev. ? 
Helsinki University Print,  ? 445 p. 
Sharoff,  Serge . 2004. What is at stake: a case study 
of Russian expressions starting with a preposition. 
Proceedings of the ACL Workshop on 
Multiword Expressions: Integrating 
Processing, July. 
Sheremetyeva, Svetlana. 2009. On Extracting 
Multiword NP Terminology for MT. Proceedings 
of the EAMT Conference. Barcelona, Spain, 
May. 
Shinmori A., Okumura M., Marukawa Y. Iwayama 
M. 2003. Patent Claim Processing for Readability - 
Structure Analysis and Term Explanation, 
Workshop on Patent Corpus Processing. 
conjunction with ACL 2003, Sapporo. Japan, 
July.
 
27
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 15?20,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Controlled Authoring In A Hybrid Russian-English Machine 
Translation System 
 
 
Svetlana Sheremetyeva 
National Research South Ural State University / pr.Lenina 74, 454080 
Chelyabinsk, Russia 
LanA Consulting ApS/ Moellekrog 4, Vejby, 3210, Copenhagen, Denmark 
lanaconsult@mail.dk  
 
  
 
Abstract 
In this paper we describe the design and 
deployment of a controlled authoring module in 
REPAT, a hybrid Russian-English machine 
translation system for patent claims. Controlled 
authoring is an interactive procedure that is 
interwoven with hybrid parsing and simplifies the 
automatic stage of analysis. Implemented in a pre-
editing tool the controlled authoring module can be 
stand-alone and pipelined to any foreign MT 
system. Although applied to the Russian-English 
language pair in the patent domain, the approach 
described is not specific for the Russian language 
and can be applied for other languages, domains 
and types of machine translation application. 
1 Introduction 
MT systems have become an inherent part of 
translation activities in spite of general 
understanding that it is impossible to get high 
quality machine translation (MT) without human 
judgment (Koehn, 2009). In addition to lexical 
ambiguity, among the linguistic phenomena that 
lower translatability indicators (Underwood and 
Jongejan, 2001) is the syntactic complexity of a 
source text, of which the patent claim whose 
sentence can run for a page or so is an ultimate 
example. 
A wide range of activities can be found in the 
area of developing different techniques to ?help? 
an MT engine cope with the ambiguity and 
complexity of the natural language. Recent work 
investigated the inclusion of interactive 
computer-human communication at each step of 
the translation process by, e.g., showing the user 
various ?paths? among all translations of a 
sentence (Koehn, cf.), or keyboard-driving the 
user to select the best translation (Macklovitch, 
2006). One of the latest publications reports on 
Patent statistical machine translation (SMT) from 
English to French where the user drives the 
segmentation of the input text (Pouliquen et.al, 
2011). Another trend to cope with the source text 
complexity is to rewrite a source text into a 
controlled language (CL) to ensure that the MT 
input conforms to the desired vocabulary and 
grammar constraints. When a controlled 
language is introduced, the number of parses per 
sentence can be reduced dramatically compared 
to the case when a general lexicon and grammar 
are used to parse specialized domain texts. 
Controlled language software is developed 
with different levels of automation and normally 
involves interactive authoring (Nyberg et al., 
2003). The users (authors) have to be taught the 
CL guidelines in order to accurately use an 
appropriate lexicon and grammar during 
authoring. In line with these studies is the 
research on developing pre-editing rules, e.g., 
textual patterns that reformulate the source text 
in order to improve the source text translatability 
and MT output. Such rules implemented in a 
software formalism are applied for controlled 
language authoring (Bredenkamp et al. 2000; 
Rayner et al. 2012).  
This paper focuses on the design, deployment 
and utilization of a controlled language in the 
implementation of the hybrid REPAT 
environment for machine translation of patent 
15
claims from Russian into English. In selecting 
Russian as a source language we were motivated 
by two major considerations.  Firstly, Russia has 
a huge pool of patents which are unavailable for 
non-Russian speakers without turning to 
expensive translation services. The situation is of 
great disadvantage for international technical 
knowledge assimilation, dissemination, 
protection of inventor?s rights and patenting of 
new inventions. Secondly, in an attempt to find 
ways that could lower efforts in developing MT 
systems involving inflecting languages, for 
which statistical techniques normally fail 
(Sharoff, 2004), we were challenged to develop a 
hybrid technique for parsing morphologically 
rich languages on the example of such a highly 
inflecting language as Russian. 
In what follows we first give an overview of 
the   REPAT machine translation environment 
and then focuse on the components of the system 
which are responsible for controlled authoring of 
the source texts with complex syntactic structure, 
such as patent claims. These components raise 
the translatability of patent claims and, second, 
improve their readability in both source and 
target languages, which for patent claims is of 
great importance. It is well known that an 
extremely complex syntactic structure of the 
patent claim is a problematic issue for 
understanding (readability) even in a source 
language (Shinmori et al., 2003), let alone in 
translation.  
2 REPAT environment overview 
The REPAT system takes a Russian patent 
claim as input and produces translations at two 
major levels, the level of terminology (not just 
any chunks), and the text level. Full translation 
of a patent claim is output in two formats, - in the 
form of one sentence meeting all legal 
requirements to the claim text, and as a better 
readable set of simple sentences in the target 
language. In Figure 3 an example of the REPAT 
output is shown for a fragment of a Russian 
claim given below: 
 
??????????????? ?????????? ?????????? 
????????????? ? ???????????? ? ????????, 
??????????? ???, ??? ? ??????? ????????? ??? 
??????, ???????????? ????????? ? ???????? ?? 
????????, ? ??????? ??????????? ??????????? 
???? ????? ??? ?????????????? ????? ??????? 
??? ??????????? ??????? ??????...  
 
The system also improves the readability of a 
source claim by decomposing it into a set of 
simple sentences that can be useful for a 
posteditor to better understand the input and thus 
control the quality of claim translation. The 
REPAT translation environment includes hybrid 
modules for source language analysis, controlled 
authoring, terminology management, knowledge 
development and rule-based modules for transfer 
and target text generation. All modules work on 
controlled language which is built into the 
system. The overall architecture of the system is 
shown in Figure 1. The workflow includes these 
main steps: 
Source claim shallow analysis based on 
hybrid techniques. It serves two purposes : a) the 
on-the-fly translation of terminology; this can be 
used by a non-SL speaker for digest, and b) the 
preparation of a raw document for authoring in 
case a full claim translation is needed; the input 
is made interactive and the nominal and 
predicate terms are highlighted, the predicate 
terminology is linked to the knowledge base.  
Terminology update. The document is checked 
against the system bilingual lexicon and 
unknown words are flagged. If needed the 
lexicon can be updated. 
Authoring. The document is authored to 
conform the controlled lexicon and grammar. 
Unknown words are either avoided or flagged. 
The source claim syntactic structure is 
simplified. The simplification also serves the 
purpose of improving the readability of a source 
language claim.  
Document processing and translation. This 
includes document parsing into a formal content 
representation, generation of a source claim in a 
controlled language, crosslinguistic transfer and 
generation of the target text. The full translation 
is output in two controlled syntax formats, a) as 
one complex sentence meeting all legal 
requirements to the claim text, and d) as a better 
readable set of simple sentences that might meet 
the needs of the user in case the translation is 
needed to assimilate technical knowledge rather 
than to be included in a patent document. The 
simplified syntactic presentation of translation 
can be useful for further automatic claim 
processing, e.g., when translation into other 
languages is needed. 
 
 
16
  
 
Figure 1. An overall architecture of the hybrid REPAT system. 
3 Controlled language 
The system controlled language specifies 
constraints on the lexicon and constraints on the 
complexity of sentences. It draws heavily on the 
patent claim sublanguage on devices in 
automobile industry, and in addition to the 
universal phenomena affecting translatability 
(Underwood and Jongejan, cf.) it addresses the 
REPAT engine-specific constraints. 
Constraints of the REPAT controlled language 
are mainly coded in the corpus-based system 
lexicon, where ambiguous terms, that 
unavoidably emerge in any doimain are split in 
different lexemes, each having only one domain 
meaning. Where possible ambiguous lexemes are 
put in the lexicon as components of longer 
terms/phrases with one meaning. To 
disambiguate the residue of ambiguous terms we 
have created  a  method for disambiguation of 
lexical items that supports interactive 
disambiguation by the user through the system 
user interface.   
Grammar restrictions on the structure of 
sentences are set by an implicitly controlled 
grammar which is associated with a controlled 
set of predicate/argument patterns in the system 
lexicon rather than with syntactic sentence-level 
constraints. The patterns code domain-based 
information on the most frequent co-occurrences 
of predicates in finite forms with their case-roles, 
as well as their linear order in the claim text.  For 
example, the pattern (1 x 3 x 2) corresponds to 
such clam fragment as  
1:boards  x: are 3:rotatably x: mounted 2: on 
the pillars 
The controlled language restrictions are 
imposed on the source text semi-automatically. 
The system prompts the user to make correct 
authoring decisions by providing structural 
templates from the system knowledge base and 
by raising the users? awareness about the 
linguistic phenomena that can increase the 
17
potential problems in machine translation. For 
example, the users are encouraged to repeat a 
preposition or a noun in conjoined constructions, 
limit the use of pronouns and conjunctions, put 
participles specifying a noun in postposition, etc.  
4 Analyzer and authoring engine 
Authoring engine is interwoven with the 
system hybrid analyzer. The analyzer performs 
two tasks in the REPAT system. It analyzers the 
input text into a formal internal representation 
and provides environment for authoring. In 
particular, the analyzer performs the following 
authoring-related steps: 
Segmentation and lexicalization. The input 
text is chunked into noun phrases (NPs) 
predicate phrases (VPs) and other types of 
lexical units. Every chunk is lexicalized by 
associating it with a known lexicon entry.  
The source NPs are chunked based on the 
dynamic knowledge automatically produced by a 
stand-alone hybrid extractor, the core of the 
REPAT shallow parsing component. It was 
ported to the Russian language following the 
methodology of NP extraction for English 
described in (Sheremetyeva 2009). The 
extraction methodology combines statistical 
techniques, heuristics and a shallow linguistic 
knowledge. The extractor does not rely on a 
preconstructed corpus, works on small texts, 
does not miss low frequency units and can 
reliably extract all NPs from an input text. The 
extraction results do not deteriorate when the 
extraction methodology is applied to inflecting 
languages (Russian in our case).  
The NPs are chunked by matching the 
extractor output (lists the source claim NPs in 
their text form) against the claim text. Here the 
language rich inflection properties turn to be an 
advantage: the NP chunking procedure proves to 
be very robust with practically no ambiguity. 
NPs excluded, the rest of the claim lexica is 
chunked by the lexicon look-up practically 
without (ambiguity) problems. The analyzer thus 
trigs highlighting of the nominal and verbal 
terminology, flags unknown words and provides 
means for lexical disambiguation. All lexicalized 
chunks are tagged with supertags coding sets of 
typed features as found in the morphological 
zones of the lexicon. 
Automatic and Interactive Disambiguation. 
Ambiguity of lexical units are resolved, either 
via a) automatic selection of the most likely 
meaning, using a set of disambiguation 
heuristics, or b) interactive clarification with the 
user. Syntactic ambiguity is to be resolved by 
human-computer interaction with strong 
computer support in the form of predicate 
templates to be filled with claim segments. 
Content representation. A formal internal 
representation of the source  claim content is 
built in the following two steps:  
Construction of the underspecified internal 
representations resulting from the authoring 
procedure of calling and filling predicate 
templates by the user. A predicate template is a 
visualization of a corresponding predicate case-
role pattern in the system lexicon. The main slot 
in the template corresponds to the predicate, 
while other slots represent case-roles. By 
supplying fillers into the slots of predicate 
templates the user in fact puts syntactic borders 
between the argument phrases and determines 
the dependency relations between the predicates 
and their arguments.  
Automatic completion of tagging and 
recursive chunking by the deep parser 
component that works over the set of the 
disambiguating features of the underspecified 
content representation. The final parse, a set of 
tagged predicate/argument structures, is then 
submitted into a) the source language generator 
that outputs a source claim in a more readable 
format of simple sentences, and b) to the transfer 
module and then to the target language generator, 
that outputs translations in two formats.  
5 Authoring Interface 
A screenshot of the REPAT authoring interface 
is shown in Figure 2. In the left pane it shows an 
interactive source claim with nominal and 
predicate terminology highlighted in different 
colours. Unknown words, if any, will be flagged. 
The user is encouraged not to use such words 
and remove the flag. In case the user considers 
them necessary, the flag stays (the terms are 
passed to the developer for lexicon update). The 
highlighted terminology improves the input 
readability and helps the user quicker and better 
understand the input content and structure. To 
simplify the input structure the user clicks on a 
predicate and gets a pop-up template whose slots 
are to be filled out with texts strings. Predicate 
templates are generated based on the case-role 
patterns in the system lexicon.  
 
18
 Figure 2. A screenshot of the user interface showing the authoring set up for a fragment of the Russian 
claim given in Section 2. The source text  with visualized terms is shown in the left pane. In the 
middle is the template for the Russian predicate ???????? (is).  The English translations for the 
terminology are shown in the bottom of the right pane.  
 
Figure 3. The two translation variants of the patent claim fragment given in Section 2. On the top the 
claim translation into English in the legal format of one nominal sentence is shown. In the middle the 
?better readable? claim translation in the form of simple sentences is displayed. In the bottom the 
authored Russian input text is given.  
19
The main slot of the template is automatically 
filled with a predicate in a finite form, not 
withstanding in which form the predicate was 
used in the text. Other predicate slots are 
referenced to particular case-roles whose 
semantic statuses are explained to the user by the 
questions next to the predicate slots. The user 
can either drag-and-drop appropriate segments 
from the interactive claim text or simply type the 
text in the slots. During the process of filling the 
template the system shows translations of the 
lexica used in the bottom of the right pane. In 
case a unit put in the slot is not found in the 
lexicon, it is flagged. The user is encouraged to 
either avoid using a problematic unit or 
substitute it with a synonym known to the 
system.  Once the template is filled, the system 
automatically generates a grammatically correct 
simple sentence in the source language and 
displays it for control. In addition to constraining 
the complexity of the sentence structure 
predicate templates also put certain constraints 
on the phrase level. As templates are meant for 
simple sentences only, coordination of verbal 
phrases (predicates) that may be ambiguous is 
avoided. Prepositions or particles attached to the 
verb are put to the main (predicate) template slot 
that resolves a possible attachment ambiguity.  
The authoring procedure completed, the 
underspecified content representation built by the 
analyzer ?behind the scenes? is passed to the 
other modules of the REPAT for translation. The 
authored claim in the source language can also 
be saved and input in any foreign MT system.  
Conclusions 
We presented an authoring environment 
integrated in the hybrid PATMT system for 
translating patent claims. The efficiency of the 
system is conditioned by the controlled language 
framework. The controlled language data are 
created based on the domain-specific analysis of 
the patent corpus on devices in automobile 
industry. The constraints of the controlled 
language are embedded into the system 
knowledge base and included into a 
comprehensive, self-paced training material.  
The authoring environment is interwoven with 
hybrid analysis components specially developed 
for inflecting languages. Rich morphology turns 
out to be an advantage in our approach. A great 
variety of morphological forms significantly 
lowers ambiguity in source text chunking and 
lexicalization.  
The system is implemented in the programming 
language C++ for the Windows operational 
environment. 
References  
Bredenkamp, A., Crysmann, B., and Petrea, M. 2000. 
Looking for Errors: A Declarative Formalism for 
Resource-Adaptive Language Checking. 
Proceedings of LREC 2000. Athens, Greece. 
 Koehn Philipp. 2009. A process study of computer-
aided translation, Philipp Koehn, Machine 
Translation Journal, 2009, volume 23, number 4. 
Macklovitch, Elliott. 2006. TransType2: The last 
word. In proceedings of LREC06, Genoa, May.  
Nyberg E., T Mitamura, D. Svoboda, J. Ko, K. Baker, 
J. Micher 2003. An Integrated system for Source 
language Checking, Analysis and Terminology 
management. Proceedings of Machine 
Translation Summit IX, September. New-
Orleans.USA           
Pouliquen Bruno, Christophe Mazenc Aldo Iorio. 
2011. Tapta: A user-driven translation system for 
patent documents based on domain-aware 
Statistical Machine. Proceedings of the EAMT 
Conference.  Leuven, Belgium, May. 
Rayner, M., Bouillon, P., and Haddow, B. 2012. 
Using Source-Language Transformations to 
Address Register Mismatches in SMT. In 
Proceedings of the Conference of the 
Association for Machine Translation in the 
Americas (AMTA), October, San Diego, USA. 
Sharoff, S. 2004. What is at stake: a case study of 
Russian expressions starting with a preposition. In: 
Proceedings of the Second ACL Workshop on 
Multiword Expressions Integrating 
Processing. 
Sheremetyeva S. 2009    On Extracting Multiword NP 
Terminology for MT.  Proceedings of the 
Thirteen Conference of European Association 
of Machine Translation, Barcelona, Spain. May 
14-15 
Shinmori A., Okumura M., Marukawa Y. Iwayama 
M. 2003. Patent Claim Processing for Readability - 
Structure Analysis and Term Explanation, 
Workshop on Patent Corpus Processing. 
conjunction with ACL 2003, Sapporo. Japan, 
July. 
Underwood N.L. and Jongejan B. 2001. 
Translatability Checker: A Tool to Help Decide 
Whether to Use MT. Proceedings of MT Summit 
VIII, Santiago de Compostela, Spain. 
 
20
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 41?52,
Dublin, Ireland, August 24th 2014.
Automatic Text Simplification For Handling Intellectual Property                       
(The Case of Multiple Patent Claims)                        
Svetlana Sheremetyeva 
National Research South Ural State University, pr. Lenina 76, 454080 Chelyabinsk, Russia 
LanA Consulting ApS, Moellekrog 4, Vejby, 3210, Copenhagen, Denmark 
lanaconsult@mail.dk 
 
  
Abstract 
Handling intellectual property involves the cognitive process of understanding the innovation de-
scribed in the body of patent claims. In this paper we present an on-going project on a multi-level text 
simplification to assist experts in this complex task. Two levels of simplification procedure are de-
scribed. The macro-level simplification results in the visualization of the hierarchy of multiple claims. 
The micro-level simplification includes visualization of the claim terminology, decomposition of the 
claim complex structure into a set of simple sentences and building a graph explicitly showing the in-
terrelations of the invention elements. The methodology is implemented in an experimental text sim-
plifying computer system. The motivation underlying this research is to develop tools that could in-
crease the overall productivity of human users and machines in processing patent applications. 
1 Introduction 
In today's highly-competitive marketplace much of industrial companies? true worth relates to intellec-
tual property protected by patents. However, a great deal of patents is not used to raise standards 
across industries as much as they could. In US alone more than 95% of all active patents are not li-
censed to a single third party and do not earn the first dollar of licensing revenue. Part of the problem 
is that patents can be difficult to understand and value as they are written in dense, arcane legal lan-
guage that only a technical expert can read (http://patentproperties.com/patentinnovations.html).  
Moreover, even patent experts, whose task is to conduct analysis of patent documents, e.g., for 
novelty, scope of protection or value can spend quite a time and effort to clearly understand a crucial 
part of a patent document, claims. The patent claim is the only part of a patent that defines the scope of 
inventor?s rights. Linguistically the claim is the most difficult information carrier. Patent law demands 
the claim to be written as a single albeit very complex and long sentence, no matter that it might run 
for a page or so. Figure 1 shows a short fragment of a claim, just to illustrate what is said above.  
 
Claim 1. A grinding tool for profile strips of wood or the like, comprising a plurality of grinding segments 
arranged in at least two rows; at least two base bodies, each associated with one of said rows of said grinding 
elements, said base bodies being movable relative to one another, said grinding segments of one of said rows 
being offset relative to said grinding segments of the other of said rows so that said rows of said grinding seg-
ments are insertable into one another over at least a part of a respective length thereof;?..and clamping means 
including two clamping elements associated with and located at each side of a respective one of said base bodies 
so as to engage said grinding segments, said two clamping elements including an inner clamping element which 
is basket-shaped and has a plurality of webs which are spaced from one another by respective angular distances 
and lie under said grinding segment receivers, and another clamping element which has a plurality of interme-
diate spaces into which said webs of said inner basket-shaped clamping element are insertable. 
 
Figure 1. A fragment of Claim1 of the US patent 4,777,771. This patent has 24 claims.        
 
The limited space of this paper does not allow us enclosing in the current description a real life patent 
claim section, but an interested reader can consult any patent bank site.  
Place licence statement here for the camera-ready version, see Section ?Licence Statement? of the instructions for preparing a 
manuscript (coling2014.pdf). 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
 
41
This problem of patent expertise is further complicated by the fact that a patent document, as a rule, 
contains not just one but a large number of claims that should be read and interpreted as a whole. 
Anybody who has seen patent claims at least once will find it unnecessary to calculate claim readabil-
ity indices to get persuaded that the claim text is extremely low readable. Traditional readability for-
mulas normally take into account the number of words per sentence or/and the number of ?hard?, be it 
long or low frequency, words per sentence (Kincaid, Fishburne, Rogers, & Chissom, 1975; Brown, 
1998; Greenfield, 2004).  Both the first and the second ratio will be equal to the number of words in a 
claim sentence where practically all words are ?hard? terms, some of them used for the first time. The 
same goes for the claim syntactic structure.  
Patent experts attending to their examination tasks normally perform simplification of a claim text 
manually. Evidently, there is a great need for tools that could automate this process. The need has al-
ready attracted attention of R&D groups working in the field of text processing. Given the linguistic 
complexity of the claim it is not surprising that practically all reports related to the patent/claim sim-
plification research describe on-going projects rather than completed studies or development (see Sec-
tion 2 for references). In this paper we attempt to complement existing achievements by presenting our 
research in the area and suggest text simplification techniques to facilitate understanding/readability of 
both, the whole section of multiple claims in a patent document, and an individual claim.  
The specificity of our approach is primarily motivated and conditioned by the fact that in patent 
examination patent experts cannot afford analyzing a simplified claim text where the content has been 
changed during the simplification procedure. Not a single word in the claim could be changed or omit-
ted. Even the use of synonyms, let alone the omission of claim structural elements (pruning), can 
change the scope of the invention and result in patent infringement and, hence, court cases. All these 
put our work out of the mainstream in the text simplification research. However it meets the definition 
of text simplification as a process of making the text more comprehensible for a targeted audience.  It 
should be also noted that though this study is primarily addressed to patent experts, our simplification 
solutions might be useful for both laypeople and machines meant to automatically process patents, 
e.g., information retrieval or machine translation systems.  
The rest of the paper is organized as follows. Section 2 is devoted to related work.  Section 3 dis-
cusses challenges in the field of claim simplification. Section 4 describes our approach to claim sim-
plification on a macro-level that addresses the whole body of multiple patent claims. In sections 4 and 
5 we suggest some solutions to the simplification of a single claim, which we call a micro-level sim-
plification. Further in Section 6 we present evaluation results and summarize our on-going research in 
Conclusions. 
2 Related work 
 Research on automatic text simplification aims at developing techniques and tools that could make 
texts more comprehensible for certain types of targeted audience/readers. The mainstream of text sim-
plification is developing methodologies and tools for general types of texts that address people with 
special needs, such as poor literacy readers (Aluisio et al. 2010), readers with mild cognitive impair-
ment (Dell'Orletta et al., 2011), elderly people (Bott et al., 2012), language learners of different levels 
(Crossley and McNamara, 2008) or just ?regular? readers (Graesser et al., 2004). Text simplification is 
most often performed on the sentence level.  Simplifying texts to provide more comprehensible input 
to a targeted audience the developers generally work within two approaches: an intuitive approach and 
a structural approach. An intuitive approach relies mainly on the developers? intuition and experience 
(Allen, 2009) that leads to using less lexical diversity, less sophisticated words, less syntactic com-
plexity, and greater cohesion. A structural approach depends on the use of structure and word lists that 
are predefined by the intelligence level, as typically found in targeted readers. The latter is defined by 
readability formulas. Traditional readability formulas are simple algorithms that measure text readabil-
ity based on sentence length and word length. Later research on readability suggests formulas that re-
flect the psycholinguistic and cognitive processes of reading (Crossley et al.2011). 
       At the linguistic level, simplified texts are largely modified to control the complexity of the lexi-
con and the syntax. Automated text simplification tools are trying to achieve this purpose by combin-
ing linguistic and statistical techniques and penalize writers for polysyllabic words and long, complex 
sentences. (Siddharthan, 2002) describe the implementation of the three stages - analysis, transforma-
42
tion and regeneration, system that lay particular emphasis on the discourse level aspects of syntactic 
simplification. Some works on text simplification use parallel corpora of original and simplified sen-
tences (Petersen & Ostendorf, 2007).  There are works where text simplification is treated as a "trans-
lation task within a RBMT (Takao and Sumita. 2003). In (Specia, 2010) text simplification is devel-
oped in the Statistical Machine Translation framework, given a parallel corpus of original and simpli-
fied texts, aligned at the sentence level.  In (Poornima et al.2011) a rule based technique is proposed to 
simplify the complex sentences based on connectives like relative pronouns, coordinating and subor-
dinating conjunctions. Sentence simplification is expressed as the list of sub-sentences that are por-
tions of the original sentence. (Bott, et al., 2012) describe a hybrid automatic text simplification sys-
tem which combines a rule based core module with a statistical support module that controls the appli-
cation of rules in the wrong contexts.  
       The approaches to patent claim simplification can be roughly put into two groups. Studies of the 
first group try to adapt to the patent domain general text simplification techniques and involve lexical 
and/or structural substitution, pruning, paraphrasing, etc. For example, in (Shinmori et al., 2003) the 
discourse structure of the patent claim is built by means of a rule-based technique; each discourse 
segment is then paraphrased. In (Mille and Wanner, 2008) the claim sentence (by means of lexical and 
punctuation clues) is segmented into clausal units, that are then compressed into a summary. The sim-
plification methods proposed by this group of researches to some extent change the original content of 
the claim that might not always be desirable, especially for patent experts.    
      Another group of studies focuses on segmenting, reformatting or highlighting certain parts of the 
patent claim without changing the content of the original. For example, in one of the earlier works  a 
rule-based technique was developed for decomposing the complex sentence of a claim into a set of 
simple sentences while  preserving the initial content (Sheremetyeva, 2003).  Most recently (Shinmori 
et al., 2012) suggested aligning claim phrases with explanatory text from the description section, while   
(Ferraro et al., 2014) proposed an approach that involves highlighting the claim segments borders and 
reformatting the original text so as to emphasis segments with the identified border marker. This ap-
proach does not involve any syntactic restructuring, just visualization of claim segments.  
     In general, due to the linguistic complexity of patent claims all research on automatic claim simpli-
fication make extensive use of rule-based methods possibly augmented with statistical techniques.  
Text segmentation is performed on two levels.  First the claim in segmented into 3 information-
relevant parts, the preamble, transition and body and then the claim body is further segmented into 
smaller parts, often clausal structures.  
      To the best of our knowledge practically all publications on claim simplification consider individ-
ual claims, while in real life most patents contain multiple interrelated claims of different types and a 
patent reader has to understand the whole range of information in the claim section. The cited studies 
address laypeople that are not trained to read patent claims. However, there is also a great demand for 
claim readability tools among patent experts who have to perform thorough and tedious work on claim 
analysis for different examination tasks on a daily basis. When accessing the prototype systems or 
methodologies, the developers normally evaluate the correctness of their own intuitive understanding 
how a simplified claim should look. No studies on end-user requirements or user-centered evaluation 
have been reported so far. In our work among others we have tried to address the above issues.  
     Our research includes the following steps: 
? Extraction of expert knowledge about their needs and procedure of claim analysis 
? Acquisition of linguistic knowledge about the patent claim sublanguage 
? Developing a prototype claim simplification system that meets expert expectations. 
3 Challenges in claim simplification  
In preparing for this research we have investigated professional instructions (Pressman. 2006; Radack, 
1995) on how to read patent claims and conducted extensive interviews with patent experts of several 
companies in the US and Europe handling intellectual property1. The recommendations are as follows. 
The first step towards understanding a claim is to identify its information parts, preamble, transition 
and the body. Another recommendation is to identify and mark the elements of the invention spelled 
                                                 
1
 The confidentiality policy of these companies does not allow us discosing them in this paper. 
43
out in the body of the claim. Element markup is useful not only for proper understanding of the claim 
but also because claims have to be supported by the description. Any terms used in claims must be 
found in the description. Hence, there is a demand to automate patent terminology extraction that 
could underlie terminology markup, e.g., by highlighting.  
In real practice the examiners manually decompose the claim in a tree with noun terminology and 
predicates (verbs, adjectives and prepositions) on separate indented lines to clearly see the invention 
elements and their interrelations.  Hence there is a need to automate the construction of such element-
relation diagrams for every particular claim. The experts we have interviewed were also very enthusi-
astic about a tool that could decompose a complex claim sentence into a set of simple sentences-
features of the invention, provided the content of the claim is preserved. It is evident that building such 
a tool is a much more demanding task than any other as it clearly cannot rely on statistical methods 
only but also requires extensive linguistic knowledge and rule-based techniques.  
Most of patents contain a large number of claims that can claim experts have to interpret related to 
each other. There are two basic types of claims: the independent claims, which stand on their own, and 
the dependent claims, which depend on one or several claims and should be interpreted in conjunction 
with their parents. Any dependent claim which refers to more than one other claim is a multiply de-
pendent claim that should also be visualized in a simplifying tool.  
Based on the extracted expert demands and analyzing procedures we suggest two levels of patent 
claim simplification that should necessarily preserve the claim section content: 
? the macro-level simplification resulting in the visualization of the hierarchy of claims explicitly 
showing their interdependence (type: dependent/independent, parents and children) 
? the micro-level simplification of one claim that includes 
o visualization of the claim terminology  
o decomposition of a claim complex structure into a set of simple sentences 
o building a diagram explicitly showing the interrelations of invention elements. 
The micro-level claim simplification is extremely challenging as cannot but require the NLP tech-
niques and elaborate and extensive linguistic resources that for our purpose do not exist so far.  
4 Macro-level simplification 
The macro-level simplification improves the readability of the whole section of multiple claims in a 
patent document. For this purpose we have developed a patent macro-analyzer that takes as input a 
whole patent document and outputs the hierarchy of claims with a lot of accompanying information 
relevant for patent examination.  In particular, the macro-analyzer automatically performs the follow-
ing successive steps: 
 
? Segmentation of the claim section from the rest of the input patent document 
? Segmentation of individual claims from the body of the claim section  
? Identification of the type of every segmented claim as independent or dependent 
? Identification of all children (one or multiple) for every individual claim 
? Identification of all parents (one or multiple) for every dependent claim 
? Construction of an hierarchical tree of claims 
 
The macro-analyzer is rule-based and uses the knowledge extracted from a 9mio wordform corpus of 
US and European patents2 in the English language.  The knowledge for macro-simplification is very 
shallow and it includes: 
      Clues signaling on the start of the Claims section such as location (the claim section of a patent 
comes after the description at the end of the patent document) and a list of delimiting expressions, 
such as ?We claim?, ? I claim?, ? claim?, ?what we claim is?, etc.  
      Clues signaling on the start of every individual claim that include numbering, formatting, punctua-
tion and a list of delimiting expressions. The claims are set forth as separately numbered paragraphs in 
                                                 
2
 This is justified by the similarity of structures of different national patents due to the similarity of writing rules imposed by 
Patent Law throughout the world.  
 
44
a single-sentence format. Each claim begins with a capital letter and with a number. The first claim of 
an issued patent is always numbered "1," with each claim thereafter following in an ascending se-
quence of Arabic numerals (1, 2, and 3) from broad claims to narrow claims.  
 
 
 
Figure.2. A screenshot of the tree of claims fragment visualized in the user interface. The number of 
dependent and independent claims is shown on the top.  The   right pane is an interactive window 
which displays the input patent text; this text can be scrolled and/or edited right in there. The left pane 
shows a tree with claims as nodes.  Clicks on the coloured square buttons next to claim nodes allow 
displaying/hiding the claim text The numbers on the right of a claim node list claims dependent on the 
claim in question.  The tree of claims is collapsible and expendable in different ways. The ?+? and ?-
?are the usual ?expand? and ?collapse? tree buttons. The coloured square buttons on the right allow 
getting truncated sub-trees of the main claim tree. 
 
        Clues signaling on the dependent claim that include a list of reference expressions. The text of a 
dependent claim always starts with a number (this clue is common to all types of claims) and a spe-
cific reference expression of the type "2. The machine of Claim 1,?" . The wording of a multiply de-
pendent claim reference expression could be, for example, "5. A gadget according to claims 3 or 4, 
further comprising...?. Multiply dependent claims may depend on other claims which do not necessar-
ily follow one another. For example, dependent claims can be referenced as  ?14. A compound of any 
of claims 1-9 or 13,?.?  There may be also reference expressions like ?17. An invention as in previ-
ous claims??. Though variable, the number of dependent claim reference expressions is still limited, 
so that they can be rather exhaustively acquired and explicitly listed in the analyzer knowledge base. 
      Clues signaling on the parents of dependent claims that are in fact contained in the dependent 
claims reference expressions. The sets of parents of different dependent claims can be different, the 
same or intersect. That does not always let build a single root tree of claims for a patent. In compli-
cated cases the macro-analysis can result in a forest of root trees of claims. 
45
      Clues signaling on the children of the claims that do not need to be acquired, the analyzer calcu-
lates them from the dependent claims reference expressions. 
        The type of knowledge required for macro-analysis of the claim section and high structural simi-
larity of national patents imposed by Patent law make the analysis algorithm practically language-
independent. The only thing which is required to port the macro-analyzer from English into any other 
language is to change the lexicon of reference expressions. Such lexicons should  certainly be acquired 
for every particular language by corpus analysis, which is pretty straight forward. 
        The macro-analyzer is implemented as a module of an end-user tool that visualizes the results of 
macro-analysis in the form of a tree structure as shown in Figure 2. The visualized tree is highlighted 
in a way that facilitates the understanding of multiple claim interrelations and allows grasping a lot of 
claim-related information ?at a glance? thus improving the readability of the claim section. The inde-
pendent claims in the tree nodes are highlighted in blue, while dependent claims are presented in red. 
Lists of children are displayed in black to the right of their parent claim nodes, the parents of a multi-
ply dependent claims are shown in red on the left of multiply-dependent claim nodes. The nodes cor-
responding to multiply-dependent claims are highlighted in red. The interface program does supple-
mentary math and displays a total number of claims, as well as the number of independent and de-
pendent claims, correspondingly, and displays them in the status bar.  The independent claims are 
bookmarked.  
       The user can navigate the claim tree, which can collapse/expand in different combinations to dis-
play the subtrees of independent claims, claim children, parents, or ascenders. There are special but-
tons next to each claim node that allow to partially or fully display claim texts. The input text of a 
whole patent is displayed on the right interactive pane of the interface. These functionalities allow in-
teractively aligning claims with certain parts of the description for consistency check or editing. The 
macro-analyzer for the English language is currently available as a standalone tool.   
5 Micro-level claim simplification 
5.1 The knowledge 
Micro-level simplification at each of its stages is done by means of a specific combination of rule-
based and statistical techniques and relies on linguistic knowledge of different depth. This knowledge 
is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and 
is mostly coded in the system lexicon as well as in analysis and generation rules.  Different modules of 
the micro-level simplification component use specific parts and types of linguistic knowledge included 
in the lexicon and their own specific sets of rules.  
The word list for the lexicon was automatically acquired from a 9 million-word corpus of a US and 
European patents available to us from our previous projects and patent web sites. A semi-automatic 
supertagging procedure was used to label these lexemes with their supertags. A supertag codes mor-
phological information (such as POS and inflection type) and semantic information, an ontological 
concept, defining a word membership in a certain semantic class (such as object, process, substance, 
etc.). For example, the supertag Nf shows that a word is a noun in singular (N), means a process (f), 
and does not end in ?ing. This supertag will be assigned, for example, to such words as activation 
or alignment. At present we use 23 supertags that are combinations of 1 to 4 features out of a set 
of 19 semantic, morphological and syntactic features for 14 parts of speech. For example, the feature 
structure of noun supertags is as follows: Tag [ POS[Noun [object [plural, singular] process [-ing, 
other[plural, singular]] substance [plural, singular] other [plural, singular]]]]].  
The ?depth? of supertags is specific for every part of speech and codes only that amount of the 
knowledge that is believed to be sufficient for our analysis procedure. The units of the system lexicon 
are described with a different level of depth. A deep (information-rich) description is only assigned to 
predicates. Other types of lexemes are only assigned morphological information.  
      Predicates in our system are words, which are used to describe interrelations between the elements 
of the invention. They are mainly verbs, but can also be adjectives or prepositions.  A predicate entry 
covers both the lexical, and, crucially for our system, the syntactic and semantic knowledge. The mor-
phological knowledge includes partial paradigms of explicitly listed predicate wordforms as found in 
the patent corpora. Syntactic and semantic knowledge relevant for our task is included in the 
CASE_ROLEs and PATTERNs fields of predicate entries. The CASE_ROLEs field lists a set of the 
46
corpus-based predicate case-roles such as agent, theme, place, instrument, etc. The PATTERNs code 
domain-based information on the most frequent co-occurrences of predicates with their case-roles, as 
well as their linear order in the claim text.  For example, the pattern (1 x 3 x 2) corresponds to such 
clam fragment as  1:boards x:are 3:rotatably x:mounted 2:on the pillars. 
The processing algorithms and rules for every stage of micro-simplification will be described in the 
corresponding sections below. 
5.2 Terminology visualization 
The readability of patent claims increases if the reader can spot the terminology at a glance. It is im-
portant not only in the process of claim examination for novelty but also for a quick check of whether 
the claim text complies the writing rules prescribed by the Patent law. Claims have to be supported by 
the patent description, which means that any terms used in the claims must be found in the description. 
To facilitate these tasks we simplify the claim text by automatically highlighting its nominal terms 
with the subsequent highlighting of these terms in the patent description. In case a certain claim term 
is not found in the description a warning message is given. This task is performed based on the results 
of a shallow analysis performed by a hybrid NP extractor and NP and predicate term chunkers which 
in succession run on the same claim text.  
      To extract (and then highlight) nominal terminology we use the NP extractor described in (Shere-
metyeva, 2009). The extraction methodology combines statistical techniques, heuristics and a very 
shallow linguistic knowledge extracted from the main system lexicon (see Section 5.1). The NP ex-
tractor knowledge base consists of a number of unilingual lexicons, - sort of extended lists of stop 
words forbidden in particular (first, middle or last) positions in a typed lexical unit (NP in our case). 
These lists of stopwords are automatically extracted from the morphological zones of the entries of 
relevant parts-of-speech. 
       The NP extraction procedure starts with n-gram calculation and then removes those n-grams that 
cannot be NPs from the list of all calculated n-grams. This is done by successive matching the compo-
nents of calculated n-grams against the stop lexicons. The NP extraction itself thus neither requires 
such demanding NLP procedures, as tagging, morphological normalization, POS pattern match, etc., 
nor does it rely on statistical counts (statistical counts are only used to sort out keywords which is not 
needed in our case). The advantages of this extractor are in that it does not rely on a preconstructed 
corpus, works well on small texts, does not miss low frequency units and can reliably extract all NPs 
from an input text. The noun phrases thus extracted are of 1 to 4 components due to the limitations of 
the extractor that uses a 4-gram model. A small adaptation of the extractor has been made to have it 
better suite the current task.  First, we excluded a lemmatizer from the original extraction algorithm  
and kept all extracted NPs in their textual forms and, second, we updated the tool knowledge so as to 
allow NPs being extracted form a claim text with articles and determiners (?said?, this?, etc;)  if pre-
sent. It was done to avoid the ambiguity in the subsequent NP chunking in the claim text.  
The chunker users the knowledge dynamically produced by the extractor (lists of all NPs with de-
terminers in their text form as found in the claim text in question). The NPs are chunked in the claim 
text by matching the extractor output against the claim text. The predicate terminology is chunked by 
the main lexicon predicate entries look-up practically without (ambiguity) problems.  The chucked 
nominal and predicate terminology is visualized in the user interface by highlighting them in the claim 
text (see Figure 3, left pane). The same dynamic knowledge is used to check for the claim noun and 
predicate terminology in the text of the description. In case of a failure a warning message about in-
consistency is displayed.  
5.3 One-sentence-to-many decomposition  
Decomposition of one syntactically complex claim sentence into a set of simple sentences is done in 
two takes. First the claim is segmented into the preamble, transition and body text, and then the   pre-
amble and claim body are further segmented into simple sentences. 
The first segmentation is pretty straight forward and is performed based on the knowledge about 
transition expressions explicitly listed in the system knowledge base. The list of corpus-based transi-
tion expressions covers both the US and European rules for writing claims.  In the US claims the tran-
sitions basically used are: "comprising", "which comprises," "consisting of," and "consisting essen-
47
tially of." Modern claims follow a format whereby the preamble is separated from the transitional term 
by a comma, while the transitional term is separated from the body by a colon.  
Under the European Patent Convention a claim can be written according to the so-called "two-part 
form" where the claim text is divided into a generic part that contains old knowledge and a difference 
part that contains novel features of the invention.  The delimiting expressions are "characterized in 
that" or "characterized by". If the European format is used, what is called the "preamble" is different 
from the meaning of ?preamble" under the U.S. patent law. In an independent claim in Europe, the 
preamble is everything which precedes the delimiting expression. The preamble in Europe is some-
times also called "pre-characterizing portion?. It can contain a text of a certain length and syntactic 
complexity. The preamble can therefore require decomposition (simplification) as well. 
 
 
 
Figure 3. A screenshot of ?Decomposition? page of the user interface. The left pane shows the input 
claim text with highlighted terminology.  Predicates are in blue, the nominal terminology is boldfaced. 
The right pane visualizes a simplified claim text in the form of simple sentences. The content of the 
texts in both panes is the same. 
 
Decomposition of the generic/preamble and difference/body parts of the claim text demands much 
more sophisticated techniques than those used at previous levels of simplification. It is performed by 
the deep analyzer that  in full uses the knowledge of the lexicon described in Section 5.1.   
The deep analyzer includes a disambiguating supertagger, typed phrase chunker based on PSG rules 
and DPG-based predicate/argument dependency identifier. It superficially performs the NLP analysis 
procedure as described in (Sheremetyeva 2003). However, the original procedure of the NLP claim 
analysis presented in the cited paper was significantly modified and simplified by introducing the shal-
low analyzer (see section 5.2) at the pre-deep-NLP analysis stage. This made the analysis procedure 
more robust and less computationally demanding. 
The workflow of the current analyzing procedure is as follows. A raw claim is first pre-processed 
by the shallow analyzer that extracts and chunks claim nominal phrases and predicates as presented in 
Section 5.2.   
The claim, thus partially parsed and tagged is then input into the preexisting deep analyzer, which 
completes super tagging, recursive chunking and defines predicate/argument dependencies. The output 
48
of the analyzer is a shallow interlingual representation where the content of every nascent simple sen-
tence is represented by a separate predicate/argument structure (proposition) in the form  
 
proposition::={label predicate-class predicate ((case-role)(case-role))*} 
case-role::= (rank status value) 
value::= phrase{(phrase(word supertag)*)}* 
 
The final parse, a set of fully tagged predicate/argument structures, is then submitted into the generator 
that transforms every predicate/argument structure into a simple sentence. The generator determines 
the order of sentences, the order of words in the nascent sentences taking care of morphological forms 
and agreement.  The order of the sentences follows the order of predicates in the claim. The order of 
the words in a sentence is defined by the knowledge in the PATTERNs zones of the predicate entries 
of the lexicon. Morphological synthesis and agreement are rule-based. The generic part and novelty 
parts of the claim are generated separately. The micro-level of simplification is illustrated in Figure 3. 
5.4  Text-to-diagram simplification 
Simplification of a claim text into a diagram is performed based of the internal claim representation as 
shown in Section 5.3. We here used the automatic text planner of the claim generator that was devel-
oped as a module of a patent MT system (Sheremetyeva, 2007).   
 
 
 
Figure 4. This screenshot of the ?Diagram? page of the user interface which displays a conceptual 
schema of the invention underlying the claim text.  
 
The planner runs over the output of the deep analyzer in the form of a set of separate predi-
cate/argument structures and unifies separate predicate-argument structures into a hierarchical struc-
ture in the form of a single root tree or a forest of trees. The planning stage is guided by the constraints 
on the patent claim sublanguage. The unified trees of predicate structures are visualized for the reader 
in the form of a diagram with explicitly listed invention elements and their relations as in Figure 4. 
49
6 Evaluation 
Given that no reliable evaluation metrics exist so far for text simplification we performed a prelimi-
nary qualitative evaluation of our methodology based on human judgment (as in all cited works on 
claim simplification). Some of the researchers admit avoiding qualitative evaluation due to the lack of 
resources that would have made it possible (Mille and Wanner, 2008). The number of patents the au-
thors use to evaluate their methodologies might seem quite limited, e.g., (Mille and Wanner, 2008) 
report evaluation results based on 30 patents; in (Bouayad-Agha et al.) the test corpus consisted of 29 
patents; (Ferraro et al. 2014) inspected 38 patent documents, but again, the reason is the immense 
complexity and length of the patent claims. 
      There is no need to use readability formulas to prove the higher comprehensibility of the output of 
our macro- and micro level simplifiers as compared to the original claim section texts. These formulas 
are not applicable to the macro-level simplification. As for the micro-level simplification, the 
terminology of the original and simplified claims is kept unchanged and it is evident that simple and 
short sentences are ?simpler? than long and complex ones. 
      We evaluate our methodology with a view to preserving the claim content and grammaticality as 
bad syntax can change the content of the claim with all the legal consequences. We asked human 
annotators (5 linguist students and 3 patent experts) to grade the simplification results according to 
these two criteria. The architecture of our system allows evaluating each component independently.   
      The quality evaluation method of nominal and predicate terminology extraction/highlighting con-
sisted in comparing our results with a gold reference list. The gold lists of multi-component nominal 
terms and predicate terms were built manually by linguist students from the patent corpus of 72000 
words for which it was feasible to create a gold standard. The number of multi-component NPs does 
not include the number of those NPs that only appear inside longer nominal phrases. The evaluation 
results of the extraction are in Table 1. 
 
Table 1. Results of the extraction of nominal and predicate terminology  
 
    Multicomponent NPs        Predicates 
Total number  of gold  terms               1425            1272 
Total extracted phrases              1476            1186 
Correct terms              1394            1154 
Missed terms                  67                54 
Incorrect phrases                  24                 - 
 
 Most of the missed NPs are longer than 4 words; they are missed because we limited ourselves to a 4-
gram extraction model. The problem can be fixed by widening the extraction window which might 
increase the computation time. As for predicates, no incorrect terms were extracted because they were 
only searched against the predicate entries in the system lexicon in the ?residue? of the claim text after 
NP extraction. Extraction mistakes can be corrected by updating the knowledge of the NP extractor. 
     The macro-level simplification (construction of the hierarchical trees of claims) was tested on 25 
patents (each having from 7 to 98 claims of different kind). The performance at this level of simplifi-
cation was practically perfect (i.e., for detecting the beginning and end of the claim section in a patent, 
the accuracy percentage is 100 and the trees of claims for every patent were also 100% correct. The 
result is explained by that the very shallow and closed knowledge required for this simplification pro-
cedure was completely covered in the lexicon.  
  Decomposition of a long claim sentence is undergoing extensive testing, further extension and 
knowledge update. It was feasible to test the methodology on the material of the first (most representa-
tive) claims of 25 patents containing from 5 to 10 predicates (meaning that claims should be decom-
posed into from 5 to 10 simple sentences, correspondingly). The total number of the resulting simple 
sentences is 147 out of which 93 sentences were correct. The problems are mainly due to the insuffi-
cient coverage of the rules identifying predicate/argument relations of syntactic chunks as output by 
the deep parser.  However, these problems can be solved by the knowledge extension and brush-up. 
Already in their present state this simplifying component shows promising performance. 
50
Building diagrams is performed by the planning component of a fully operational generator (see 
section 5.3). It is completely conditioned by the parser and correlates with the claim decomposition. 
Once the decomposition into simple sentences is correct, the diagram is correct as well. 
7 Conclusions  
In this paper, we have presented a methodology for the simplification of both the whole section of pat-
ent claims and individual claims. The simplification improves the readability of patent clams by the 
following: building a hierarchy of multiple claims with relevant accompanying information; highlight-
ing the claim/patent nominal and predicate terminology; decomposing long and complex sentences of 
individual claims into a set of simple sentences preserving the content of the claim; building claim dia-
grams graphically visualizing interrelations of the invention elements.   
      Based on the methodology an experimental claim simplification tool was developed. As of today 
the programming shell of the tool is completed and provides for knowledge administration in all mod-
ules of the system to improve their performance. The static knowledge sources have been compiled for 
the domain of patents about apparatuses and chemical substances. The morphological analysis of Eng-
lish is fully operational and well tested. The English generator is also operational. The evaluation re-
sults suggest that our system produce much more readable output when compared to the original 
claims, and that the preservation of the claim content and grammaticality are positively rated by the 
annotators. The tool is currently undergoing an extensive extension and evaluation. However, already 
in it present state it provides for promising performance. The research is primarily targeted to patent 
experts, but can also be useful for laypeople and for automatic patent processing. 
References 
Aluisio S., Specia L., Gasperin C. and Scarton C. 2010. Readability assessment for text simplification. Proceed-
ings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applica-
tions, pp.1?9. 
Bott S., Saggion H. and Figueroa D. 2012. A Hybrid System for Spanish Text Simplification. NAACL-HLT 2012 
Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 75?84, Montreal, 
Canada, June 7?8, 2012. c 2012 Association for Computational Linguistics 
Bouayad-Agha N., Casamayor G.,Ferraro G., and Wanner L. 2009 Simplification of Patent ClaimSentences for 
Their Paraphrasing and Summarization. Proceedings of the Twenty-Second International FLAIRS Conference. 
Brown, J. D. (1998). An EFL readability index. JALT Journal, 20, 7?36. 
Crossley, S. A. & McNamara, D. S. 2008. Assessing Second Language Reading Texts at the Intermediate Level: 
An approximate replication of Crossley, Louwerse, McCarthy, and McNamara. Language Teaching, 41 (3), 
409?229. 
Crossley, S. A., Allen D. B. and McNamara D. S. 2011. Text readability and intuitive simplification: A compari-
son of readability formulas. Reading in a Foreign Language. April 2011, V. 23, No. 1. pp. 84?101 
Dell?Orletta, F., Montemagni S., and Venturi G. 2011. READ?IT: Assessing Readability of Italian Texts with a 
View to Text Simplification. Proceedings of the 2nd Workshop on Speech and Language Processing for As-
sistive Technologies, Edinburgh, Scotland, UK, 2011, pp. 73-83. 
Ferraro G., Suominen H., and Nualart J. 2014.Segmentation of patent claims for improving their readability Pro-
ceedings of the 3rd Worshop on Predicting and Improving Text Readability for Target Reader Populations 
(PITR) @ EACL 2014, pp. 66?73. Gothenburg, Sweden, April 26-30 2014. c 2014 Association for Computa-
tional Linguistics 
Graesser, A. C., McNamara, D. D., Louwerse, M. L., and Cai, Z. 2004. Coh-Metrix: Analysis of text on cohesion 
and language. Behavior Research Methods, Instruments, & Computers, 36, 193?202. 
Greenfield, J. (2004). Readability formulas for EFL. JALT Journal, 26, 5?24. 
Mille S. and Wanner L. Multilingual Summarization in Practice: The Case of Patent Claims 12th EAMT confer-
ence, 22-23 September 2008, Hamburg, Germany 
51
Kincaid, J. P., Fishburne, R. P., Rogers, R. L. & Chissom, B. S. 1975. Derivation of new readability formulas 
(Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy enlisted personnel, 
Research Branch Report 8?75, Millington, TN: Naval Technical Training, U. S. Naval Air Station, Memphis,. 
Petersen, S., and Ostendorf, M. 2007 Text simplification for language learners: a corpus analysis. Proceedings of 
Workshop on Speech and Language Technology for Education 
Poornima C , Dhanalakshmi V, Anand Kumar M, and Soman K. P.  2011. Rule based Sentence Simplification 
for English to Tamil Machine Translation System. International Journal of Computer Applications (0975 ? 
8887) Volume 25? No.8, July 2011. 
Pressman D. 2006. Patent It Yourself. Nolo, Berkeley, CA. 
Rada D. V.  1995. Reading and understanding patent claims. JOM, 47(11):69?69. 
Siddharthan, A. 2002. An Architecture for a Text Simplification System. Proceedings of the Language Engi-
neering Conference (LEC'02), Hyderabad, India, IEEE Computer Society pp. 64. 
Sheremetyeva S. 1999. A Flexible Approach To Multi-Lingual Knowledge Acquisition For NLG.. Proceedings 
of the 7th European Workshop on Natural Language Generation. Toulouse. (France) May 13-15. 
Sheremetyeva S. 2003. Natural language analysis of patent claims. Proceedings of the ACL 2003 Workshop on 
Patent Processing, ACL ?03, Stroudsburg, PA, USA. Association for Computational Linguistics. 
Sheremetyeva S. 2007. On Portability of Resources for Quick Ramp-Up of Multilingual MT for Patent Claims. 
Proceedings of the workshop on Patent Translation in conjunction with MT Summit XI, Copenhagen, Den-
mark, September 10-14 
Sheremetyeva S. 2009.    On Extracting Multiword NP Terminology for MT.  Proceedings of the Thirteenth 
Conference of  European Association of Machine Translation (EAMT-2009). Barcelona, Spain. May 14-15. 
Shinmori, A.,  Okumura M., Marukawa Y., and Iwayama M. (2003). Patent claim processing for readability: 
structure analysis and term explanation.  Proceedings of the ACL-2003 Workshop on Patent Corpus Process-
ing, volume 20 of PATENT 03, pp. 56?65, Stroudsburg, PA, USA. Association for Computational Linguistics. 
Shinmori, A.,  Okumura M., Marukawa Y. 2012. Aligning patent claims with the?detailed description? for read-
ability. Journal of Natural Language Processing, 12(3):111?128. 
Takao D. and Sumita E.  2003. ?Input sentence splitting and translation?, Proceedings of the Workshop on Build-
ing and using parallel Texts, HLT-NAACL 2003. 
Zhu, Z., Bernhard, D. and Gurevych, I. A. 2010. Monolingual Tree-based Translation Model for Sentence Sim-
plification. Proceedings of The 23rd International Conference on Computational Linguistics (COLING), Au-
gust 2010. Beijing, Chin.a 
 
52
