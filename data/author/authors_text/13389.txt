Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 43?51,
Beijing, August 2010
A Voting Mechanism for Named Entity Translation in English?
Chinese Question Answering 
Ling-Xiang Tang1, Shlomo Geva1, Andrew Trotman2, Yue Xu1 
 
1Faculty of Science and Technology 
Queensland University of Technology 
 {l4.tang,s.geva,yue.xu}@qut.edu.au
 
2Department of Computer Science 
University of Otago 
 
 
andrew@cs.otago.ac.nz
 
 
 
Abstract 
In this paper, we describe a voting 
mechanism for accurate named entity 
(NE) translation in English?Chinese 
question answering (QA). This mecha-
nism involves translations from three 
different sources: machine translation, 
online encyclopaedia, and web docu-
ments. The translation with the highest 
number of votes is selected. We evalu-
ated this approach using test collection, 
topics and assessment results from the 
NTCIR-8 evaluation forum. This 
mechanism achieved 95% accuracy in 
NEs translation and 0.3756 MAP in 
English?Chinese cross-lingual infor-
mation retrieval of QA.  
1  Introduction 
Nowadays, it is easy for people to access 
multi-lingual information on the Internet. Key 
term searching on an information retrieval (IR) 
system is common for information lookup. 
However, when people try to look for answers 
in a different language, it is more natural and 
comfortable for them to provide the IR system 
with questions in their own natural languages 
(e.g. looking for a Chinese answer with an 
English question: ?what is Taiji??). Cross-
lingual question answering (CLQA) tries to 
satisfy such needs by directly finding the cor-
rect answer for the question in a different lan-
guage.  
In order to return a cross-lingual answer, a 
CLQA system needs to understand the ques-
tion, choose proper query terms, and then ex-
tract correct answers. Cross-lingual informa-
tion retrieval (CLIR) plays a very important 
role in this process because the relevancy of 
retrieved documents (or passages) affects the 
accuracy of the answers. 
A simple approach to achieving CLIR is to 
translate the query into the language of the tar-
get documents and then to use a monolingual 
IR system to locate the relevant ones. How-
ever, it is essential but difficult to translate the 
question correctly. Currently, machine transla-
tion (MT) can achieve very high accuracy 
when translating general text. However, the 
complex phrases and possible ambiguities pre-
sent in a question challenge general purpose 
MT approaches. Out-of-vocabulary (OOV) 
terms are particularly problematic. So the key 
for successful CLQA is being able to correctly 
translate all terms in the question, especially 
the OOV phrases. 
In this paper, we discuss an approach for 
accurate question translation that targets the 
OOV phrases and uses a translation voting 
mechanism. This mechanism involves transla-
tions from three different sources: machine 
translation, online encyclopaedia, and web 
documents. The translation with the highest 
number of votes is selected. To demonstrate 
this mechanism, we use Google Translate 
43
(GT)1 as the MT source, Wikipedia as the en-
cyclopaedia source, and Google web search 
engine to retrieve Wikipedia links and relevant 
Web document snippets.  
English questions on the Chinese corpus for 
CLQA are used to illustrate of this approach. 
Finally, the approach is examined and evalu-
ated in terms of translation accuracy and re-
sulting CLIR performance using the test col-
lection, topics and assessment results from 
NTCIR-82. 
English Question Templates (QTs) 
who [is | was | were | will], what is the definition of, 
what is the [relationship | interrelationship | inter-
relationship]  [of | between], what links are there, 
what link is there, what [is | was | are | were | does | 
happened], when [is | was | were |  will | did | do],  
where [will | is | are | were], how [is | was | were | 
did], why [does | is | was | do | did | were | can | 
had], which [is | was | year], please list, describe 
[relationship | interrelationship | inter-relationship]  
[of | between], could you [please | EMPTY] give 
short description[s] to, who, where, what, which, 
how, describe, explain 
Chinese QT Counterparts 
???????, ??????, ??????, ????
??, ?????,?????, ?????, ?????, 
?????, ?????, ????, ????,????, 
????, ????, ????, ????, ????, ?
???, ????, ????, ????, ????, ??
??, ????, ????,????, ????, ???
?, ????, ????, ???,???, ???, ??
?, ???, ???,???, ???, ???, ???, 
???, ???, ???,???,???, ???,???,
???,??,??, ??,??, ??, ??, ??, ??, 
??, ??, ??, ??, ??, ??, ??, ??, ?
?,??, ?? 
Table 1. Question templates 
2 CLIR Issue and Related Work 
In CLIR, retrieving documents with a cross-
lingual query with out-of-vocabulary phrases 
has always been difficult. To resolve this prob-
lem, an external resource such as Web or 
Wikipedia is often used to discover the possi-
ble translation for the OOV term. Wikipedia 
and other Web documents are thought of as 
treasure troves for OOV problem solving be-
cause they potentially cover the most recent 
OOV terms. 
                                                 
1 http://translate.google.com. 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. 
The Web-based translation method was 
shown to be an effective way to solve the OOV 
phrase problem (Chen et al, 2000; Lu et al, 
2007; Zhang & Vines, 2004; Zhang et al, 
2005). The idea behind this method is that a 
term/phrase and its corresponding translation 
normally co-exist in the same document be-
cause authors often provide the new terms? 
translation for easy reading.  
In Wikipedia the language links provided 
for each entry cover most popular written lan-
guages, therefore, it was used to solve a low 
coverage issue on named entities in Eu-
roWordNet (Ferr?ndez et al, 2007); a number 
of research groups (Chan et al, 2007; Shi et 
al., 2008; Su et al, 2007; Tatsunori Mori, 
2007) employed Wikipedia to tackle OOV 
problems in the NTCIR evaluation forum. 
3 CLQA Question Analysis 
Questions for CLQA can be very complex. For 
example, ?What is the relationship between the 
movie "Riding Alone for Thousands of Miles" 
and ZHANG Yimou??. In this example, it is 
important to recognise two named entities 
("Riding Alone for Thousands of Miles" and 
?ZHANG Yimou?) and to translate them pre-
cisely.  
In order to recognise the NEs in the ques-
tion, first, English question template phrases in 
Table 1 are removed from question; next, we 
use the Stanford NLP POS tagger (The Stan-
ford Natural Language Processing Group, 
2010) to identify the named entities; then 
translate them accordingly. Chinese question 
template phrases are also pruned from the 
translated question at the end to reduce the 
noise words in the final query.  
There are three scenarios in which a term or 
phrase is considered a named entity. First, it is 
consecutively labelled NNP or NNPS (Univer-
sity of Pennsylvania, 2010). Second, term(s) 
are grouped by quotation marks.  For example, 
to extract a named entity from the example 
question above, three steps are needed: 
1. Remove the question template phrase 
?What is the relationship between? from 
the question. 
2. Process the remaining using the POS tag-
ger, giving ?the_DT movie_NN ``_`` Rid-
ing_NNP Alone_NNP for_IN Thou-
44
sands_NNS of_IN Miles_NNP 
``_``and_CC ZHANG_NNP Yimou_NNP 
?_.? 
3. ?Riding Alone for Thousands of Miles? is 
between two tags (``) and so is an entity, 
and the phrase ?ZHANG Yimou?, as indi-
cated by two consecutive NNP tags is also 
a named entity. 
Third, if a named entity recognised in the two 
scenarios above is followed in the question by 
a phrase enclosed in bracket pairs, this phrase 
will be used as a tip term providing additional 
information about this named entity. For in-
stance, in the question ?Who is David Ho (Da-i 
Ho)??, ?Da-i Ho? is the tip term of the named 
entity ?David Ho?.  
4 A Voting Mechanism for Named 
Entity Translation (VMNET) 
Observations have been made:  
? Wikipedia has over 100,000 Chinese en-
tries describing various up-to-date events, 
people, organizations, locations, and facts. 
Most importantly, there are links between 
English articles and their Chinese counter-
parts.  
? When people post information on the 
Internet, they often provide a translation 
(where necessary) in the same document.  
These pages contain bilingual phrase pairs.  
For example, if an English term/phrase is 
used in a Chinese article, it is often fol-
lowed by its Chinese translation enclosed 
in parentheses. 
? A web search engine such as Google can 
identify Wikipedia entries, and return 
popular bi-lingual web document snippets 
that are closely related to the query.  
? Statistical machine translation relying on 
parallel corpus such as Google Translate 
can achieve very high translation accuracy. 
Given these observations, there could be up 
to three different sources from which we can 
obtain translations for a named entity; the task 
is to find the best one.  
4.1 VMNET Algorithm 
A Google search on the extracted named entity 
is performed to return related Wikipedia links 
and bilingual web document snippets.   Then 
from the results of Web search and MT, three 
different translations could be acquired. 
Wikipedia Translation 
The Chinese equivalent Wikipedia pages 
could be found by following the language links 
in English pages. The title of the discovered 
Chinese Wikipedia page is then used as the 
Wikipedia translation.  
Bilingual Clue Text Translation 
 The Chinese text contained in the snippets 
returned by the search engine is processed for 
bilingual clue text translation. The phrase in a 
different language enclosed in parentheses 
which come directly after the named entity is 
used as a candidate translation. For example, 
from a web document snippet, ?YouTube - 
Sean Chen (???) dunks on Yao Ming??, ?
???? can be extracted and used as a candi-
date translation of ?Sean Chen?, who is a bas-
ket ball player from Taiwan. 
Machine Translation 
In the meantime, translations for the named 
entity and its tip term (if there is one) are also 
retrieved using Google Translate.  
Regarding the translation using Wikipedia, 
the number of results could be more than one 
because of ambiguity. So for a given named 
entity, we could have at least one, but possibly 
more than three candidate translations. 
With all possible candidate translations, the 
best one then can be selected. Translations 
from all three sources are equally weighted. 
Each translation contributes one vote, and the 
votes for identical translation are cumulated. 
The best translation is the one with the highest 
number of votes. In the case of a tie, the first 
choice of the best translation is the Wikipedia 
translation if only one Wiki-entry is found; 
otherwise, the priority for choosing the best is 
bilingual clue text translation, then machine 
translation. 
4.2 Query Generation with VMNET 
Because terms can have multiple meanings, 
ambiguity often occurs if only a single term is 
given in machine translation. A state-of-the-art 
MT toolkit/service could perform better if 
more contextual information is provided. So a 
better translation is possible if the whole sen-
tence is given (e.g. the question). For this rea-
45
son, the machine translation of the question is 
the whole query and not with the templates 
removed. 
However, issues arise: 1) how do we know 
if all the named entities in question are trans-
lated correctly? 2) if there is an error in named 
entity translation, how can it be fixed? Particu-
larly for case 2, the translation for the whole 
question is considered acceptable, except for 
the named entity translation part. We intend to 
keep most of the translation and replace the 
bad named entity translation with the good 
one. But finding the incorrect named entity 
translation is difficult because the translation 
for a named entity can be different in different 
contexts. The missing boundaries in Chinese 
sentences make the problem harder. To solve 
this, when a translation error is detected, the 
question is reformatted by replacing all the 
named entities with some nonsense strings 
containing special characters as place holders. 
These place holders remain unchanged during 
the translation process.  The good NE transla-
tions then can be put back for the nearly trans-
lated question.  
 Given an English question Q, the detailed 
steps for the Chinese query generation are as 
following: 
1. Retrieve machine translation Tmt for the 
whole question from Google Translate.   
2. Remove question template phrase from 
question. 
3. Process the remaining using the POS tag-
ger.  
4. Extract the named entities from the tagged 
words using the method discussed in Sec-
tion 3. 
5. Replace each named entity in question Q 
with a special string Si,(i =0,1,2,..) which 
makes nonsense in translation and is 
formed by a few non-alphabet characters. 
In our experiments, Si is created by joining 
a double quote character with a ^ character 
and the named entity id (a number, starting 
from 0, then increasing by 1 in order of 
occurrence of the named entity) followed 
by another double quote character. The fi-
nal Si, becomes ?^id?. The resulting ques-
tion is used as Qs.  
6. Retrieve machine translation Tqs for Qs 
from Google Translate.  Since Si consists 
of special characters, it remains unchanged 
in Tqs. 
7. Start the VMNET loop for each named 
entity. 
8. With an option set to return both English 
and Chinese results, Google the named en-
tity and its tip term (if there is one).  
9. If there are any English Wikipedia links in 
the top 10 search results, then retrieve 
them all. Else, jump to step 12. 
10. Retrieve all the corresponding Chinese 
Wikipedia articles by following the lan-
guages links in the English pages. If none, 
then jump to step 12. 
11. Save the title NETwiki(i) of each Chinese 
Wikipedia article  Wiki(i). 
12. Process the search results again to locate a 
bilingual clue text translation candidate - 
NETct, as discussed in Section 4.1. 
13. Retrieve machine translation NETmt, and 
NETtip for this named entity and its tip term 
(if there is one). 
14. Gather all candidate translations: NET-
wiki(*), NETct, NETtip, and NETmt  for vot-
ing. The translation with the highest num-
ber of votes is considered the best 
(NETbest). If there is a tie, NETbest is then 
assigned the translation with the highest 
priority. The priority order of candidate 
translation is NETwiki(0) (if 
sizeof(NETwiki(*))=1)  >  NETct  > NETmt. It 
means when a tie occurs and if there are 
more than one Wikipedia translation, all 
the Wikipedia translations are skipped. 
15. If Tmt does not contain NETbest, it is then 
considered a faulty translation. 
16. Replace Si  in Tqs with NETbest. 
17. If NETbest is different from any NETwiki(i) 
but can be found in the content of a 
Wikipedia article (Wiki(i)), then the corre-
sponding NETwiki(i)  is used as an addi-
tional query term, and appended to the fi-
nal Chinese query. 
18. Continue the VMNET loop and jump back 
to step 8 until no more named entities re-
main in the question. 
19. If Tmt was considered a faulty translation, 
use Tqs as the final translation of Q. Other-
wise, just use Tmt. The Chinese question 
template phrases are pruned from the 
translation for the final query generation.  
46
A short question translation example is 
given below: 
? For the question ?What is the relationship 
between the movie "Riding Alone for 
Thousands of Miles" and ZHANG Yi-
mou??, retrieving its Chinese translation  
from a MT service, we get the following: 
???????????????????
????. 
? The translation for the movie name "Rid-
ing Alone for Thousands of Miles" of 
?ZHANG Yimou? is however incorrect. 
? Since the question is also reformatted into 
?What is the relationship between the 
movie "^0" and ?^1???, machine transla-
tion returns a second translation:  ????
???????^ 0???^ 1?? 
? VMNET obtains the correct translations:  
????? and ???, for two named en-
tities "Riding Alone for Thousands of 
Miles" and ?ZHANG Yimou? respectively. 
? Replace the place holders with the correct 
translations in the second translation and 
give the final Chinese translation: ???
??????????????????
??? 
5 Information Retrieval 
5.1 Chinese Document Processing 
Approaches to Chinese text indexing vary: 
Unigrams, bigrams and whole words are all 
commonly used as tokens. The performance of 
various IR systems using different segmenta-
tion algorithms or techniques varies as well 
(Chen et al, 1997; Robert & Kwok, 2002). It 
was seen in prior experiments that using an 
indexing technique requiring no dictionary can 
have similar performance to word-based index-
ing (Chen, et al, 1997). Using bigrams that 
exhibit high mutual information and unigrams 
as index terms can achieve good results. Moti-
vated by indexing efficiency and without the 
need for Chinese text segmentation, we use 
both bigrams and unigrams as indexing units 
for our Chinese IR experiments. 
5.2 Weighting Model 
A slightly modified BM25 ranking function 
was used for document ordering.  
When calculating the inverse document fre-
quency, we use: 
           
 
 
   (1) 
where N is the number of documents in the 
corpus, and n is the document frequency of 
query term  . The retrieval status value of a 
document d with respect to query            
is given as: 
 
          
 
                 
                  ?       
      
     
 
          
 
   
           (2) 
where          is the term frequency of term 
   in document d;        is the length of 
document d in words and avgdl is the mean 
document length. The number of bigrams is 
included in the document length. The values of 
the tuneable parameters    and b used in our 
experiments are 0.7 and 0.3 respectively. 
6 CLIR Experiment 
6.1 Test Collection and Topics 
Table 2 gives the statistics of the test collection 
and the topics used in our experiments. The 
collection contains 308,845 documents in sim-
plified Chinese from Xinhua News. There are 
in total 100 topics consisting of both English 
and Chinese questions. This is a NTCIR-8 col-
lection for ACLIA task.  
Corpus #docs #topics 
Xinhua Chinese (simplified) 308,845 100 
Table 2. Statistics of test corpus and topics 
6.2 Evaluation Measures 
The evaluation of VMNET performance cov-
ers two main aspects: translation accuracy and 
CLIR performance.  
As we focus on named entity translation, the 
translation accuracy is measured using the pre-
cision of translated named entities at the topic 
level. So the translation precision -P is defined 
as: 
   
 
 
    (3) 
where c is the number of topics in which all 
the named entities are correctly translated; N is 
the number of topics evaluated. 
47
The effectiveness of different translation 
methods can be further measured by the result-
ing CLIR performance. In NTCIR-8, CLIR 
performance is measured using the mean aver-
age precision. The MAP values are obtained 
by running the ir4qa_eval2 toolkit with the 
assessment results 3  on experimental run 
s(NTCIR Project, 2010).  MAP is computed 
using only 73 topics due to an insufficient 
number of relevant document found for the 
other 27 topics (Sakai et al, 2010). This is the 
case for all NTCIR-8 ACLIA submissions and 
not our decision. 
It also must be noted that there are five top-
ics that have misspelled terms in their English 
questions. The misspelled terms in those 5 top-
ics are given in Table 3. It is interesting to see 
how different translations cope with misspelled 
terms and how this affects the CLIR result.  
Topic ID Misspelling Correction 
ACLIA2-CS-0024 Qingling Qinling 
ACLIA2-CS-0035 Initials D Initial D 
ACLIA2-CS-0066 Kasianov Kasyanov 
ACLIA2-CS-0074 
Northern 
Territories 
northern 
territories 
ACLIA2-CS-0075 Kashimir Kashmir 
Table 3. The misspelled terms in topics 
6.3 CLIR Experiment runs 
A few experimental runs were created for 
VMNET and CLIR system performance 
evaluation. Their details are listed in Table 7.  
Those with name *CS-CS* are the Chinese 
monolingual IR runs; and those with the name 
*EN-CS* are the English-to-Chinese CLIR 
runs. Mono-lingual IR runs are used for 
benchmarking our CLIR system performance.  
7 Results and Discussion 
7.1 Translation Evaluation 
The translations in our experiments using 
Google Translate reflect only the results re-
trieved at the time of the experiments because 
Google Translate is believed to be improved 
over time. 
The result of the final translation evaluation 
on the 100 topics is given in Table 4.  Google 
Translate had difficulties in 13 topics. If all 
                                                 
3 http://research.nii.ac.jp/ntcir/ntcir-ws8/ws-en.html. 
thirteen named entities in those topics where 
Google Translate failed are considered OOV 
terms, the portion of topics with OOV phrases 
is relatively small. Regardless, there is an 8% 
improvement achieved by VMNET reaching 
95% precision.   
Method c N P 
Google Translate 87 100 87% 
VMNET 95 100 95% 
Table 4. Translation Evaluation Results 
There are in total 14 topics in which Google 
Translate or VMNET failed to correctly trans-
late all named entities. These topics are listed 
in Table 8. Interestingly, for topic (ACLIA2-
CS-0066) with the misspelled term 
?Kasianov?, VMNET still managed to find a 
correct translation (??????????
???????). This has to be attributed to 
the search engine?s capability in handling mis-
spellings. On the other hand, Google Translate 
was correct in its translation of ?Northern Ter-
ritories? of Japan, but VMNET incorrectly 
chose ?Northern Territory? (of Australia). For 
the rest of the misspelled phrases (Qingling, 
Initials D, Kashimir), neither Google Translate 
nor VMNET could pick the correct translation. 
7.2 IR Evaluation 
The MAP values of all experimental runs cor-
responding to each query processing technique 
and Chinese indexing strategy are given in Ta-
ble 5. The results of mono-lingual runs give 
benchmarking scores for CLIR runs.  
As expected, the highest MAP 0.4681 is 
achieved by the monolingual run VMNET-CS-
CS-01-T, in which the questions were manu-
ally segmented and all the noise words were 
removed.  
It is encouraging to see that the automatic 
run VMNET-CS-CS-02-T with only question 
template phrase removal has a slightly lower 
MAP 0.4419 than that (0.4488) of the best per-
formance CS-CS run in the NTCIR-8 evalua-
tion forum (Sakai, et al, 2010). 
If unigrams were used as the only indexing 
units, the MAP of VMNET-CS-CS-04-T 
dropped from 0.4681 to 0.3406. On the other 
hand, all runs using bigrams as indexing units 
either exclusively or jointly performed very 
well. The MAP of run VMNET-CS-CS-05-T 
using bigrams only is 0.4653, which is slightly 
48
lower than that of the top performer run 
VMNET-CS-CS-01-T, which used two forms 
of indexing units. However, retrieval perform-
ance could be maximised by using both uni-
grams and bigrams as indexing units. 
The highest MAP (0.3756) of a CLIR run is 
achieved by run VMNET-EN-CS-03-T, which 
used VMNET for translation. Comparing it to 
our manual run VMNET-CS-CS-01-T, there is 
around 9% performance degradation as a result 
of the influence of noise words in the ques-
tions, and the possible information loss or 
added noise due to English-to-Chinese transla-
tion, even though the named entities translation 
precision is relatively high. 
The best EN-CS CLIR run (MAP 0.4209)  
in all submissions to the NTCIR-8 ACLIA task 
used the same indexing technique (bigrams 
and unigrams) and ranking function (BM25) as 
run VMNET-EN-CS-03-T but with ?query 
expansion based on RSV? (Sakai, et al, 2010).  
The MAP difference 4.5% between the forum 
best run and our CLIR best run could suggest 
that using query expansion is an effective way 
to improve the CLIR system performance.  
Runs VMNET-EN-CS-01-T and VMNET-
EN-CS-04-T, that both used Google Translate 
provide direct comparisons with runs 
VMNET-EN-CS-02-T and VMNET-EN-CS-
03-T, respectively, which employed VMNET 
for translation. All runs using VMNET per-
formed better than the runs using Google 
Translate.  
Run Name MAP 
NTCIR-8 CS-CS BEST 0.4488 
VMNET-CS-CS-01-T 0.4681 
VMNET-CS-CS-02-T 0.4419 
VMNET-CS-CS-03-T 0.4189 
VMNET-CS-CS-04-T 0.3406 
VMNET-CS-CS-05-T 0.4653 
NTCIR-8 EN-CS BEST 0.4209 
VMNET-EN-CS-01-T 0.3161 
VMNET-EN-CS-02-T 0.3408 
VMNET-EN-CS-03-T 0.3756 
VMNET-EN-CS-04-T 0.3449 
Table 5. Results of all experimental runs 
The different performances between CLIR 
runs using Google Translate and VMENT is 
the joint result of the translation improvement 
and other translation differences. As shown in 
Table 8, VMNET found the correct transla-
tions for 8 more topics than Google Translate. 
It should be noted that there are two topics 
(ACLIA2-CS-0008 and ACLIA2-CS-0088) 
not included in the final CLIR evaluation (Sa-
kai, et al, 2010). Also, there is one phrase, 
?Kenneth Yen (K. T. Yen) (???)?, which 
VMNET couldn?t find the correct translation 
for, but it detected a highly associated term 
?Yulon - ?????, an automaker company in 
Taiwan; Kenneth Yen is the CEO of Yulon. 
Although Yulon is not a correct translation, it is 
still a good query term because it is then possi-
ble to find the correct answer for the question: 
?Who is Kenneth Yen??. However, this topic 
was not included in the NTCIR-8 IR4QA 
evaluation.  
Moreover, it is possible to have multiple 
explanations for a term. In order to discover as 
many question-related documents as possible, 
alternative translations found by VMNET are 
also used as additional query terms. They are 
shown in Table 6. For example, ?? is the 
Chinese term for DINK in Mainland China, 
but ??? is used in Taiwan. Furthermore, 
because VMNET gives the Wikipedia transla-
tion the highest priority if only one entry is 
found, a person?s full name is used in person 
name translation rather than the short com-
monly used name.  For example, Cheney (for-
mer vice president of U.S.) is translated into?
???? rather than just??.  
NE VMNET Wiki Title 
Princess Nori ???? ???? 
DINK ?? ??? 
BSE ??? ?????? 
Three Gorges Dam ???? ???? 
Table 6. Alternative translations 
The biggest difference, 3.07%, between 
runs that used different translation is from runs 
VMNET-EN-CS-03-T and VMNET-EN-CS-
04-T, which both pruned the question template 
phrase for simple query processing. Although 
the performance improvement is not obvious, 
the correct translations and the additional 
query terms found by VMNET are still very 
valuable. 
8 Conclusions 
General machine translation can already 
achieve very good translation results, but with 
our proposed approach we can further improve 
the translation accuracy. With a proper adjust-
49
ment of this approach, it could be used in a 
situation where there is a need for higher pre-
cision of complex phrase translation.  
The results from our CLIR experiments in-
dicate that VMNET is also capable of provid-
ing high quality query terms. A CLIR system 
can achieve good results for answer finding by 
using the VMNET for translation, simple in-
dexing technique (bigrams and unigrams), and 
plain question template phrase pruning.  
 
Run Name Indexing  
Units 
Query Processing 
VMNET-CS-CS-01-T U + B Manually segment the question and remove all the noise words  
VMNET-CS-CS-02-T U + B Prune the question template phrase 
VMNET-CS-CS-03-T U + B Use the whole question without doing any extra processing work 
VMNET-CS-CS-04-T U As VMNET-CS-CS-01-T 
VMNET-CS-CS-05-T B As VMNET-CS-CS-01-T 
VMNET-EN-CS-01-T U + B Use Google Translate on the whole question and use the entire translation 
as query 
VMNET-EN-CS-02-T U + B Use VMNET translation result without doing any further processing 
VMNET-EN-CS-03-T U + B As above, but prune the Chinese question template from translation 
VMNET-EN-CS-04-T U + B Use Google Translate  on  the whole question and prune the Chinese ques-
tion template phrase from the translation 
Table 7. The experimental runs. For indexing units, U means unigrams; B means bigrams. 
 
 
Topic ID Question with OOV Phrases  Correct  GT VMNET  
ACLIA2-CS-0002 What is the relationship between the movie 
"Riding Alone for Thousands of Miles" 
and ZHANG Yimou? 
????
? 
??????? ????? 
ACLIA2-CS-0008 Who is LI Yuchun? ??? ??? ??? 
ACLIA2-CS-0024 Why does Qingling build "panda corridor 
zone" 
?? ??? ??? 
ACLIA2-CS-0035 Please list the events related to the movie 
"Initials D". 
??? D ?? D??? ?? D??
? 
ACLIA2-CS-0036 Please list the movies in which Zhao Wei 
participated. 
?? ?? ?? 
ACLIA2-CS-0038 What is the relationship between Xia Yu 
and Yuan Quan. 
?? ??? ?? 
ACLIA2-CS-0048 Who is Sean Chen(Chen Shin-An)? ??? ???????? ??? 
ACLIA2-CS-0049 Who is Lung Yingtai? ??? ??? ??? 
ACLIA2-CS-0057 What is the disputes between China and 
Japan for the undersea natural gas field in 
the East China Sea? 
?? ????? ?? 
ACLIA2-CS-0066 What is the relationship between two Rus-
sian politicians, Kasianov and Putin? 
????
? 
Kasianov ???
?????
?????
???? 
ACLIA2-CS-0074 Where are Japan's Northern Territories 
located? 
???? ???? ??? 
ACLIA2-CS-0075 Which countries have borders in the Ka-
shimir region? 
???? Kashimir Kashimir 
ACLIA2-CS-0088 What is the relationship between the 
Golden Globe Awards and Broken-back 
Mountain? 
??? ????? ??? 
ACLIA2-CS-0089 What is the relationship between Kenneth 
Yen(K. T. Yen) and China? 
??? ????????
??? 
???? 
Table 8. The differences between Google Translate and VMNET translation of OOV 
phrases in which GT or VMNET was wrong. 
50
 References 
Chan, Y.-C., Chen, K.-H., & Lu, W.-H. (2007). 
Extracting and Ranking Question-Focused 
Terms Using the Titles of Wikipedia Articles. 
Paper presented at the NTCIR-6. 
Chen, A., He, J., Xu, L., Gey, F. C., & Meggs, J. 
(1997, 1997). Chinese text retrieval without 
using a dictionary. Paper presented at the SIGIR 
'97: Proceedings of the 20th annual international 
ACM SIGIR conference on Research and 
development in information retrieval. 
Chen, A., Jiang, H., & Gey, F. (2000). Combining 
multiple sources for short query translation in 
Chinese-English cross-language information 
retrieval. 17-23. 
Ferr?ndez, S., Toral, A., Ferr?ndez, ?., Ferr?ndez, 
A., & Mu?oz, R. (2007). Applying Wikipedia?s 
Multilingual Knowledge to Cross?Lingual 
Question Answering Natural Language 
Processing and Information Systems (pp. 352-
363). 
Lu, C., Xu, Y., & Geva, S. (2007). Translation 
disambiguation in web-based translation 
extraction for English?Chinese CLIR. 819-823. 
NTCIR Project. (2010). Tools. from 
http://research.nii.ac.jp/ntcir/tools/tools-en.html 
Robert, W. P. L., & Kwok, K. L. (2002). A 
comparison of Chinese document indexing 
strategies and retrieval models. ACM 
Transactions on Asian Language Information 
Processing (TALIP), 1(3), 225-268. 
Sakai, T., Shima, H., Kando, N., Song, R., Lin, C.-
J., Mitamura, T., et al (2010). Overview of 
NTCIR-8 ACLIA IR4QA. Paper presented at the 
Proceedings of NTCIR-8, to appear. 
Shi, L., Nie, J.-Y., & Cao, G. (2008). RALI 
Experiments in IR4QA at NTCIR-7. Paper 
presented at the NTCIR-7. 
Su, C.-Y., Lin, T.-C., & Wu, S.-H. (2007). Using 
Wikipedia to Translate OOV Terms on MLIR. 
Paper presented at the NTCIR-6. 
Tatsunori Mori, K. T. (2007). A method of Cross-
Lingual Question-Answering Based on Machine 
Translation and Noun Phrase Translation using 
Web documents. Paper presented at the NTCIR-
6. 
The Stanford Natural Language Processing Group. 
(2010). Stanford Log-linear Part-Of-Speech 
Tagger. from 
http://nlp.stanford.edu/software/tagger.shtml 
University of Pennsylvania. (2010). POS tags. from 
http://bioie.ldc.upenn.edu/wiki/index.php/POS_t
ags 
Zhang, Y., & Vines, P. (2004). Using the web for 
automated translation extraction in cross-
language information retrieval. Paper presented 
at the Proceedings of the 27th annual 
international ACM SIGIR conference on 
Research and development in information 
retrieval.  
Zhang, Y., Vines, P., & Zobel, J. (2005). Chinese 
OOV translation and post-translation query 
expansion in Chinese?English cross-lingual 
information retrieval. ACM Transactions on 
Asian Language Information Processing 
(TALIP), 4(2), 57-77. 
 
 
51
A Boundary-Oriented Chinese Segmentation Method Using N-
Gram Mutual Information 
Ling-Xiang Tang1, Shlomo Geva1, Andrew Trotman2, Yue Xu1 
 
1Faculty of Science and Technology 
Queensland University of Technology 
Brisbane, Australia 
{l4.tang,s.geva,yue.xu}@qut.edu.au 
2Department of Computer Science 
University of Otago 
Dunedin, New Zealand 
 andrew@cs.otago.ac.nz 
 
Abstract 
This paper describes our participation 
in the Chinese word segmentation task 
of CIPS-SIGHAN 2010. We imple-
mented an n-gram mutual information 
(NGMI) based segmentation algorithm 
with the mixed-up features from unsu-
pervised, supervised and dictionary-
based segmentation methods. This al-
gorithm is also combined with a simple 
strategy for out-of-vocabulary (OOV) 
word recognition. The evaluation for 
both open and closed training shows 
encouraging results of our system. The 
results for OOV word recognition in 
closed training evaluation were how-
ever found unsatisfactory. 
1 Introduction 
Chinese segmentation has been an interesting 
research topic for decades. Lots of delicate 
methods are used for providing good Chinese 
segmentation. In general, on the basis of the 
required human effort, Chinese word segmen-
tation approaches can be classified into two 
categories: supervised and unsupervised.  
Particularly, supervised segmentation meth-
ods can achieve a very high precision on the 
targeted knowledge domain with the help of 
training corpus?the manually segmented text 
collection. On the other hand, unsupervised 
methods are suitable for more general Chinese 
segmentation where there is no or limited 
training data available. The resulting segmen-
tation accuracy with unsupervised methods 
may not be very satisfying, but the human ef-
fort for creating the training data set is not ab-
solutely required. 
In the Chinese word segmentation task of 
CIPS-SIGHAN 2010, the focus is on the per-
formance of Chinese segmentation on cross-
domain text. There are in total two types of 
evaluations: closed and open. We participated 
in both closed and open training evaluation 
tasks and both simplified and traditional 
Chinse segmentation subtasks. For the closed 
training evaluation, the provided resource for 
system training is limited and using external 
resources such as trained segmentation soft-
ware, corpus, dictionaries, lexicons, etc are 
forbidden; especially, human-encoded rules 
specified in the segmentation algorithm are not 
allowed.  
For the bakeoff of this year, we imple-
mented a boundary-oriented NGMI-based al-
gorithm with the mixed-up features from su-
pervised, unsupervised and dictionary-based 
methods for the segmentation of cross-domain 
text. In order to detect words not in the training 
corpus, we also used a simple strategy for out-
of-vocabulary word recognition.  
2 A Boundary-Oriented Segmentation 
Method 
2.1 N-Gram Mutual Information 
It is a challenge to segment text that is out-of-
domain for supervised methods which are 
good at the segmentation for the text that has 
been seen segmented before. On the other hand, 
unsupervised segmentation methods could help 
to discover words even if they are not in vo-
cabularies. To conquer the goal of segmenting 
text that is out-of-domain and to take advan-
tage of the training corpus, we use n-gram mu-
tual information (NGMI)(Tang et al, 2009) ?
an unsupervised boundary-oriented segmenta-
tion method and make it trainable for cross-
domain text segmentation.  
As an unsupervised segmentation approach, 
NGMI is derived from the character-based mu-
tual information(Sproat & Shih, 1990), but 
unlike its ancestor it can additionally recognise 
words longer than two characters. Generally, 
mutual information is used to measure the as-
sociation strength of two adjoining entities 
(characters or words) in a given corpus. The 
stronger association, the more likely it is that 
they should be together. The association score 
MI for the adjacent two entities (x and y) is 
calculated as: 
           
        
 
       
 
       
 
     
     
        
 
(1) 
where freq(x) is the frequency of entity x oc-
curring in the given corpus; freq(xy) is the fre-
quency of entity xy (x followed by y) occurring 
in the corpus; N is the size of entities in the 
given corpus; p(x) is an estimate of the prob-
ability of entity x occurring in corpus, calcu-
lated as freq(x)/N. 
NGMI separates words by choosing the 
most probable boundaries in the unsegmented 
text with the help of a frequency table of n-
gram string patterns. Such a frequency table 
can be built from any selected text.  
The main concept of NGMI is to find the 
boundaries between words by combining con-
textual information rather than looking for just 
words. Any place between two Chinese char-
acters could be a possible boundary. To find 
the most rightful ones, boundary confidence 
(BC) is introduced to measure the confidence 
of having words separated correctly. In other 
words, BC measures the association level of 
the left and right characters around each possi-
ble boundary to decide whether the boundary 
should be actually placed. 
For any input string, suppose that we have: 
                           (2) 
The boundary confidence of a possible 
boundary ( | )  is defined as: 
                      (3) 
where L and R are the adjoining left and right 
segments with up to two characters from each 
side of the boundary ( | ) and            , 
        ; and NGMImin is calculated as: 
                             
                
                
                     
(4) 
Basically, NGMImin considers mutual informa-
tion of k (k=2, or k= 4) pairs of segments 
around the boundary; the one with the lowest 
value is used as the score of boundary confi-
dence. Those segment pairs used in NGMImin 
calculation are named adjoining segment pairs 
(ASPs). Each ASP consists of a pair of adjoin-
ing segments. 
For the boundary confidence of the bounda-
ries at the beginning or end of an input string, 
we can retrieve only one character from one 
side of the boundary. So for these two kinds of 
boundaries differently we have: 
                                     
                              
(5) 
                    
                                    
                                             (6) 
For any possible boundary the lower confi-
dence score it has, the more likely it is an ac-
tual boundary. A threshold then can be set to 
decide whether a boundary should be placed. 
So even without a lexicon, it is still probable to 
segment text with a certain precision which 
just simply means the suggested words are all 
out-of-vocabulary. Hence, NGMI can be sub-
sequently used for OOV word recognition. 
2.2 Supervised NGMI 
The idea of making NGMI trainable is to turn 
the segmented text into a word based fre-
quency table. It is a table that records only 
words, adjoining word pairs and their frequen-
cies. For example, given a piece of training 
text ? ?A B C E B C A B? (where A, B, C and 
E are n-gram Chinese words), its frequency 
table should look like the following: 
A|B 2 
B|C 2 
C|E 1 
C|A 1 
A 2 
B 3 
C 2 
E 1 
Also, when doing the boundary confidence 
computation, any substrings (children) of the 
words (parents) in this table are set to have the 
same frequency as their parents?.  
3 Segmentation System Design 
3.1 Frequency Table and Its Alignment 
In order to resolve ambiguity and also recog-
nise OOV terms, statistical information of n-
gram string patterns in test files should be col-
lected. There are in total two groups of fre-
quency information used in the segmentation. 
One is from the training data, recording the 
frequency information of the actual words and 
the adjoining word pairs; the other is from the 
unsegmented text, containing frequency infor-
mation of all possible n-gram patterns.  
However, the statistical data collected from 
the unsegmented test file contains many noise 
patterns. It is necessary to remove those noise 
patterns from the table to avoid negative im-
pact on the final BC computation. Therefore, 
an alignment of the pattern frequencies ob-
tained from the test file is performed to reduce 
noise.  
The frequency alignment is conducted in a 
few steps. First, build a frequency table of all 
string patterns for the unsegmented text includ-
ing those having a frequency of one. Second, 
the frequency table is sorted by the frequency 
and the length of the patterns. Longer patterns 
have a higher ranking than the shorter ones; for 
patterns of same length the ones having higher 
frequency are ranked higher than those having 
lower. Next, starting from the beginning of the 
table where the longest and the most frequent 
pattern have the highest ranking, retrieve one 
record each time and remove from the table all 
its sub-patterns which have the same frequency 
as its parent?s.  
After such a frequency alignment is done, 
two frequency tables are merged into one and 
ready for the final boundary confidence calcu-
lation. 
3.2 Segmentation  
In the training and the system testing stages, 
the segmentation results using boundary confi-
dence alone for word disambiguation were 
found unsatisfactory. Trying to achieve as high 
performance as possible, the overall word 
segmentation for the bakeoff is done by using 
a hybrid algorithm which is a combination of 
NGMI for general word segmentation, and the 
backward maximum match (BMM) method for 
the final word disambiguation.  
Since it is common for a Chinese document 
containing various types of characters: Chi-
nese, digit, alphabet and characters from other 
languages, segmentation needs to be consid-
ered for two particular forms of Chinese 
words: 1) words containing non-Chinese char-
acters such as numbers or letters; and 2) words 
containing purely Chinese characters.  
In order to simplify the process of overall 
segmentation, boundaries are automatically 
added to the places in which Chinese charac-
ters precede non-Chinese characters. Addition-
ally, for words containing numbers or letters, 
we only search those begin with numbers or 
letters and end with Chinese character(s) 
against the given lexicons. If the search fails, 
the part with all non-Chinese characters re-
mains the same and a boundary is added be-
tween the non-Chinese character and the Chi-
nese character.   
For example, to segment a sentence 
?????????????????, it 
consists of there following main steps: 
? First, because of ?? |??, only ????
???? requires initial segmentation. 
? Next, find a matched word ??????? 
in a given lexicon. 
? Last, segment ??????. 
So the critical part of the segmentation algo-
rithm is to segment strings with purely Chinese 
characters.  
By already knowing the actual word infor-
mation (i.e. a vocabulary from the labelled 
training data), it can be set in our algorithm 
that when computing BCs each possible 
boundary is assigned with a score falling in 
one of the following four BC categories: 
? INSEPARATABLE 
? THRESHOLD 
? normal boundary confidence score 
? ABSOLUTE-BOUNDARY 
INSEPARATABLE means the characters 
around the possible boundary are a part of an 
actual word; ABSOLUTE-BOUNDARY 
means the adjoining segments pairs are not 
seen in any words or string patterns. 
THRESHOLD is a threshold value that is 
given to a possible boundary for which only 
one of ASPs can be found in the word pair ta-
ble, and its length is greater than two. 
After finishing all BC computations for an 
input string, it then can be broken down into 
segments separated by the boundaries having a 
BC score that is lower than or equals to the 
threshold value. For each segment, it can be 
checked if it is a word in the vocabulary or if it 
is an OOV term using an OOV judgement 
formula that will be discussed in Section 3.3. If 
a segment is not a word or an OOV term, it 
means there is an ambiguity in that segment. 
For example, given a sentence ???????, 
the substring ????? inside the sentence can 
be either segmented into ???  | ?? or ?? | 
???. 
To disambiguate it, a segment is divided 
into two chunks at the place having the lowest 
BC score. If one of the chunks is a word or 
OOV term, this two-chunk breaking-down op-
eration continues on the remaining non-word 
chunk until both divided chunks are words, or 
none of them is a word or an OOV term. After 
this recursive operation is finished, if there are 
still non-word chunks left they will be further 
segmented using the BMM method. 
The overall segmentation algorithm for an 
all-Chinese string can be summarised as fol-
lows: 
1) Compute BC for each possible boundary. 
2) Input string becomes segments that are 
separated by the boundaries having a 
low BC score (not higher than the 
threshold). 
3) For each remaining non-word segment 
resulting from step 2, it gets recursively 
broken down into two chunks at the 
place having the lowest BC among this 
segment based on the scores from step 1. 
This breaking-down-into-two-chunk 
loop continues on the non-word chunk if 
the other is a word or an OOV term; 
otherwise, all the remaining non-word 
chucks are further segmented using the 
backward maximum match method. 
3.3 OOV Word Recognition 
We use a simple strategy for OOV word detec-
tion. It is assumed that an n-gram string pattern 
can be qualified as an OOV word if it repeats 
frequently within only a short span of text or a 
few documents. So to recognise OOV words, 
the statistical data extracted from the unseg-
mented text needs to contain not only pattern 
frequency information but also document fre-
quency information. However, the documents 
in the test data are boundary-less. To obtain 
document frequencies for string patterns, we 
separate test files into a set of virtual docu-
ments by splitting them according size. The 
size of the virtual document (VDS) is adjust-
able.  
For a given non-word string pattern S, we 
then can compute its probability of being an 
OOV term by using: 
         
  
  
   (7) 
where tf is the term frequency of the string pat-
tern S; df is the virtual document frequency of 
the string pattern. Then S is considered an 
OOV candidate, if it satisfies: 
                    (8) 
where OOV_THRES is an adjustable threshold 
value used to filter out the patterns with lower 
probability of being OOV words. However, 
using this strategy could have side effects on 
the segmentation performance because not all 
the suggested OOV words could be correct. 
4 Experiments 
4.1 Experimental Environment 
OS GNU/Linux 2.6.32.11-99.fc12.x86_64 
CPU 
Intel(R) Core(TM)2 Duo CPU     
E6550  @ 2.33GHz 
MEM 8G memory  
BUILD DEBUG build without optimisation 
Table 1. Software and Hardware Environ-
ment. 
The information of operating system and 
hardware used in the experiments is given in 
Table 1. 
4.2 Parameters Settings 
Parameter Value 
N # of words in training corpus  
THRESHOLD log(1/N) 
VDS 10,000bytes 
OOV_THRES 2.3 
Table 2. System settings used in both closed 
and open training evaluation. 
Table 2 shows the parameters used in the sys-
tem for segmentation and OOV recognition. 
4.3 Closed and Open Training 
For both closed and open training evaluations, 
the algorithm and parameters used for segmen-
tation and OOV detection are exactly the same. 
This is true except for an extra dictionary - cc-
cedict(MDBG) being used in the open training 
evaluation. 
4.4 Segmentation Efficiency 
SUBTASK DOMAIN TIME 
simplified 
(closed) 
A 2m19.841s 
B 2m1.405s 
C 1m57.819s 
D 1m54.375s 
simplified 
(open) 
A 3m52.726s 
B 3m20.907s 
C 3m10.398s 
D 3m22.866s 
traditional 
(closed) 
A 2m33.448s 
B 2m56.056s 
C 3m7.103s 
D 3m14.286s 
traditional A 3m14.595s 
(open) B 3m41.634s 
C 3m53.839s 
D 4m10.099s 
Table 3. The execution time of segmenta-
tions for four different domains in both 
simplified and traditional Chinese subtasks. 
Table 3 shows the execution time of all tasks 
for generating the segmentation outputs. The 
execution time listed in the table includes the 
time for loading the training frequency table, 
building the frequency table from the test file, 
and producing the actual segmentation results. 
5 Evaluation 
5.1 Segmentation Results 
Simplified Chinese 
Task R P F1 ROOV RROOV RRIV 
A (c) 0.907 0.862 0.884 0.069 0.206 0.959 
A (o) 0.869 0.873 0.871 0.069 0.657 0.885 
B (c) 0.876 0.844 0.86 0.152 0.457 0.951 
B (o) 0.859 0.878 0.868 0.152 0.668 0.893 
C (c) 0.885 0.804 0.842 0.110 0.218 0.967 
C (o) 0.865 0.846 0.855 0.110 0.559 0.903 
D (c) 0.904 0.865 0.884 0.087 0.321 0.960 
D (o) 0.853 0.850 0.851 0.087 0.438 0.893 
Traditional Chinese 
Task R P F1 ROOV RROOV RRIV 
A (c) 0.864 0.789 0.825 0.094 0.105 0.943 
A (o) 0.804 0.722 0.761 0.094 0.234 0.863 
B (c) 0.868 0.85 0.859 0.094 0.316 0.926 
B (o) 0.789 0.736 0.761 0.094 0.35 0.834 
C (c) 0.871 0.815 0.842 0.075 0.115 0.932 
C (o) 0.811 0.74 0.774 0.075 0.254 0.856 
D (c) 0.875 0.834 0.854 0.068 0.169 0.926 
D (o) 0.811 0.753 0.781 0.068 0.235 0.853 
Table 4. The segmentation results for four 
domains in both closed and open training 
evaluations. (c) ? closed; (o) ? open;  A - Lit-
erature; B ? Computer; C ? Medicine; D ? 
Finance. ROOV is the OOV rate in the test 
file.  
In the Chinese word segmentation task of 
CIPS-SIGHAN 2010, the system performance 
is measured by five metrics: recall (R), preci-
sion (P), F-measure (F1), recall rate of OOV 
words (RROOV), and recall rate of words in vo-
cabulary (RRIV). 
The official results of our system for both 
open and closed training evaluation are given 
in Table 4. The recall rates, precision values, 
and F1-scores of all tasks show promising re-
sults of our system in the segmentation for 
cross-domain text. However, the gaps between 
our scores and the bakeoff bests also suggest 
that there is still plenty of room for perform-
ance improvements in our system. 
The OOV recall rates (RRoov) showed in 
Table 4 demonstrate that the OOV recognition 
strategy used in our system can achieve a cer-
tain level of OOV word discovery in closed 
training evaluation. The overall result for the 
OOV word recognition is not very satisfactory 
if comparing it with the best result from other 
bakeoff participants. But for the open training 
evaluation the OOV recall rate picked up sig-
nificantly, which indicates that the extra dic-
tionary - cc-cedict covers a fair amount of 
terms for various domains. 
5.2 Possible Further Improvements 
Due to finishing the implementation of our 
segmentation system in a short time, we be-
lieve that there might be many program bugs 
which had negative effects on our system and 
leaded to producing results not as expected.  In 
an analysis of the segmentation outputs, words 
starting with numbers were found incorrectly 
segmented because of the different encodings 
used in the training and test files for digits. 
Moreover, the disambiguation in breaking 
down a non-word segment which contains at 
least an n-gram word could lead to an all-
single-character-word segmentation. This 
should certainly be avoided. 
Also, the current OOV word recognition 
strategy may detect a few good OOV words, 
but also introduces incorrect segmentation 
consistently through the whole input text if 
OOV words are mistakenly identified. If this 
OOV word recognition used in our system can 
be further improved, it can help to alleviate the 
problem of performance deterioration. 
For the open training, if language rules can 
be encoded in both word segmentation and 
OOV word recognition, it certainly is another 
beneficial method to improve the overall preci-
sion and recall rate. 
6 Conclusions 
In this paper, we describe a novel hybrid 
boundary-oriented NGMI-based segmentation 
method, which combines a simple strategy for 
OOV word recognition. The evaluation results 
show reasonable performance of our system in 
cross-domain text segmentation even with the 
negative effects from system bugs and the 
OOV word recognition strategy. It is believed 
that the segmentation system can be improved 
by fixing the existing program bugs, and hav-
ing a better OOV word recognition strategy. 
Performance can also be further improved by 
incorporating language or domain specific 
knowledge into the system. 
References 
MDBG. CC-CEDICT download. from 
http://www.mdbg.net/chindict/chindict.php?pag
e=cc-cedict 
Sproat, Richard, and Chilin Shih. 1990. A statistical 
method for finding word boundaries in Chinese 
text. Computer Processing of Chinese &amp; 
Oriental Languages, 4(4): 336-351. 
Tang, Ling-Xiang, Shlomo Geva, Yue Xu, and 
Andrew Trotman. 2009. Word Segmentation for 
Chinese Wikipedia Using N-Gram Mutual 
Information. Paper presented at the 14th 
Australasian Document Computing Symposium 
(ADCS 2009).  
 
 
 
