Reusing an ontology to generate numeral classifiers 
Francis Bond* 
NTT Communication Science Laboratories 
2-4 Hikari-dai, Kyoto 619-0237, JAPAN 
bond@cs lab .kec l  .n t t .  co .  jp  
Kyonghee Paik 
Center for the Study of Language and Information 
Stanford University, CA 94305-2150, USA 
kpa ik@usa ,  net  
Abstract 
In this paper, we present a solution to the prob- 
lem of generating Japanese nmneral classifiers us- 
ing semantic lasses from an ontology. Most nouns 
must take a numeral classifier when they are quan- 
tiffed in languages uch as Chinese, Japanese, Ko- 
rean, Malay and Thai. In order to select an appro- 
priate classifier, we propose an algorithm which as- 
sociates classifiers with semantic lasses and uses 
inheritance to list only those classifiers which have 
to be listed. It generates sortal classifiers with au ac- 
curacy of 81%. We reuse the ontology provided by 
Goi-Taikei - -  a Japanese lexicon, and show that it 
is a reasonable choice for this task, requiring infor- 
mation to be entered for less than 6% of individual 
nouns .  
1 Introduction 
In this paper we consider two questions. The th'st is: 
how to generate numeral classifiers uch as piece in 
2 pieces qfl)aper? To do this we use a semantic hier- 
archy originally developed for a different ask. The 
second is: how far can such a hierarchy be reused? 
In English, uncountable nouns cannot be directly 
modified by numerals, instead the noun nmst be 
embedded in a noun phrase headed by a classi- 
tier. Knowing when to do this is a language Sl0e- 
cific property. For example, French deux renseigne- 
merit must be translated as two pieces of information 
in English. \] Iu many languages, including most 
South-East Asian lauguages, Chinese, Japanese and 
Korean, the majority of nouns are uncountable and 
nmst be quantified by numeral classifier combina- 
tions. These languages typically have many differ- 
ent classifiers. There has been some work on the 
analysis of numeral classifiers in natural language 
processing, particularly for Japanese (Asahioka et 
al., 1990; Kamei and Muraki, 1995; Bond et al, 
* Visiting CSLI, Stanford University (1999-2000). 
I Numeral-classilier combinations are shown in bold, the 
noun phrases they quantify are underlined. 
1996; Bond et al, 1998; Yokoyama and Ochiai, 
1999), but very little on their generation. We could 
only find one paper on generating classifiers in Thai 
(Sornlertlamvanich et al, 1994). One immediate 
application fox the generation of classifiers is ma- 
chine translation, and we shall take examples flom 
there, but it is in fact needed fox" the generation 
of any quantified noun phrase with an uncountable 
head noun. 
The second question we address is: how far can 
an ontology be reused for a difl%rent task to the one 
it was originally designed fox. There are several 
large ontologies now in use (WordNet (Fellb~mm, 
1998); Goi-Taikei (lkehara et al, 1997); Mikrokos- 
rues (Nirenburg, 1989)) and it is impractical to re- 
build one fox" every application. Howevel, there is 
no guarantee that an ontology built fox one task will 
be useful for another. 
The paper is structured as follows. In Section 2, 
we discuss tile properties of numeral classifiers in 
more detail and suggest an ilnproved algorithm fox" 
generating them. Seclion 3 introduces the ontology 
we have chosen, the Goi-Taikei ontology (ikehara ct 
al., 1997). Then we show how to use the ontology to 
generate classifiers in Section 4. Finally, we discuss 
how well it performs in Section 5. 
2 Generating Numeral Classifiers 
In this section we introduce the properties of nu- 
meral classifiers, focusing on Japanese, then give 
an algorithm to generate classiliers. Japanesc was 
chosen because of tile wealth of published ata on 
Japanese classifiers and the availability of a large 
lexicon with semantic lasses marked. 
2.1 What are Numeral Classifiers 
Japanese is a language where most nouns can not 
be directly modified by numerals, instead, nouns 
are modified by a numeral-classifier combinatiou as 
shown in (1).2 
2~Vc use lhe fo l low ing  abbrev ia t ions :  NOM : nominat ive ;  
ACC = accusat ive ;  AI)N = adnomina l ;  CI. = c lass i l ier ;  ARGSTR 
90 
(l) 2-tsfH10 denshimC~ru 
2-(:L-ADN emai\[ 
2 pieces of emai\] 
2 emails 
In Japanese, numeral classifiers arc a subclass o\[ 
nouns. The main properly dislinguishillg them from 
t)rolotypical nouns is thai lhey cannot sland alone. 
Typically they postiix to numerals, forming a quan- 
tilter l)hrase. Japanese also allows them to combine 
with the quantifier st7 "some" or tile interrogative 
nani "what" (2). We will call all such Colnbinations 
ot' a numeral/quantifier/interrogative with a numeral 
classifier a numeral-classitier combination. 
(2) a. 2-hiki"2 animals" (Numeral) 
b. sO-hiki "solne animals" (Quantilier) 
c. nan-biki "how Illauy mfimals" (inlcr- 
rogali ve) 
Classiliers have different l)rOl)erlies del)ending on 
their use. There are live ulajor types: serial which 
classify the kind o1: tile noun phrase tile}, (.\]uan- 
lily (such as -/all "piece"); evenl which arc used 
to quantify events (such as -kai " l i l l lC" ) ;  me i l s l l -  
ral which ~lre used to measure lhc U.IIIOtlllt Of  SOl l le  
property (such its senchi "-cm"), group which refer 
to a collection of melnbers (such as -inure gloup ), 
and taxononfic which force (he noun phrase to be 
inlerpreted as a generic kind (such as -.vim "kind"). 
We propose the l:ollowing basic struclurc for sor- 
tal classiliers (3). The lexical slructtlre we adopt is 
an extension ot' Pustejovsky's (1995) generative l x- 
icon, with tile addition of an explicit quantilication 
relationship (Bond and Paik, 1997). 
\[ \[AR(;I x : numera l+ 
ARGSTR (3) / LD-ARG1 y: 
,,la.,.qlicrLOUANT Cluant:5- f 5.es (x,  y)  
There are two variables in the argument strut- 
lure: the numeral, quantifier or interrogative (repre- 
sented by numera2+) ,  and the noun phrase being 
classilied. Because the noun phrase being classilied 
can be omitted in context, it is a default argun-lent, 
one which participales in tile logical expressions in 
the qualia, but is not necessarily expressed syntacti- 
cally. 
= argunle l l l  s lructurc;  AR(; = argument ;  \[)-AR(; = default  m-gu- 
menl ,  QUANT = quant i l icat ion.  
Serial classiliers differ from each other in tile re- 
strictions they place on the quantilied variable 7V. 
For example tile classilier -nin adds tile restriction 
y :human.  That is, it can only be used to classify 
human referents. 
Japanese has two number systems: a Sine- 
Japanese one based on Chinese for example, ichi 
"Olle",lli "\[wo",s(lll "lhree", etc., and ~tll alternative 
nalive-Jal)anesc ystem, for example, hitotsu "one" 
fitlalsu "two",milsu "three", etc. In Japanese tile lla- 
live system only exists for the numbers from one 
to ten. Most classitiers combine with the Chinese 
lorms, howevm; different classiliers select Sine- 
Japanese for some numerals, for example, ni-hiki 
"two-el", and most classifiers undergo some form 
of sound change (such as -hiki to -biki in (2)). 
Wc will not bc concerned wilh these morllhological 
changes, we refer interested reMers to Backhouse 
(1993, I 1 g-122) for more discussion. 
Numeral classiliers characteristically premodify 
the noun phrases they quantify, linked by an adhere- 
inal case marker, as in (4); or appear 't\]oating' as 
adverbial phrases, lypically to before the verb: (5). 
The choice between pre-nominal and lloming quan- 
lifters is hu'gcly driven by discourse related consid- 
erations (1)owning, 1996). In this paper we concen- 
lrale on (he semantic ontribution of the quantiliers, 
and ignore tile discourse ffects. 
(4) 2-isii-no tegami-o yonda 
2-CI.-AI)N letter-A{:c read 
I read two letters 
(5) tcgami-o 2-tsii yonda 
letter-ACC 2<:L read 
I read two letters 
Quantilier phrases can also function as noun 
l)hrascs on their own, with anaphoric or deictic ref- 
erence, when what is being quantilied is recover- 
able from the context. For example (7) is accept- 
able if the letters have already been referred to, or 
arc clearly visible. 
(6) \[some background with letters salient\] 
(7) 2-tsfi-o yonda (Japanese) 
2-CI,-ACC read 
1 read two letters 
In the pre-nonlinal construction tile relation be- 
tween ihe target noun phrase and quantilier is 
explicit. For muneral-classilier combinations the 
91 
quantification can be of the object denoted by the 
noun phrase itself as in (8); or of a sub-part of it as 
in (9) (see Bond and Pai l  (1997) for a fuller discus- 
sion). 
(8) 3-tsfi-no tegami 
3-CL-ADN letter 
3 letters 
(9) 3-mai-no tegami 
3-CL-ADN letter 
a 3 page letter 
2.2 An Algorithm to Generate Numeral 
Classifiers 
The only published algorithm to generate classifiers 
is that of Sornlertlamvanich et al (1994). They 
propose to generate classifiers in Thai as follows: 
First create a lexicon with default classifiers listed 
for as many nouns as possible. This was done by 
automatically extracting noun classifier pairs from 
a sense-tagged corpus, and taking the classifier that 
appeared most often with each sense of a noun. 3 
Then, the most fiequent classifier is listed for each 
semantic lass. Generation is then simple: if a noun 
has a default classifier in the lexicon, then use it, 
otherwise use the default classifier associated with 
its semantic lass. 
Unfortunately, no detailed results were given as 
to the size of the concept hierarchy, the number of 
nodes in it or the number of nouns for which clas- 
sifiers were found. As the generation procedure 
was not ilnplemented, there was no overall accuracy 
given for the system. 
As a default, Sornlertlamvanich et al's algorithm 
is useful. However, it does not cover several ex- 
ceptional cases, so we have refined it further. The 
extended algorithm is shown in Figure 1. 
Firstly, we have made explicit what to do when 
a noun is a member of more than one semantic 
class or of no semantic lass. In the lexicon we 
used, nouns are, on average, inembers of 2 seman- 
tic classes. Howevm; the semantic lasses are or- 
dered so that the most typical use comes first. For 
example, usagi "rabbit" is marked as both animal 
and meat, with animal coming first (Fignre 3). 
In this case, we would take the classifier associated 
3111 fact, Thai also has a great many group classiliers, much 
like heM, flock and pack in English. Therefore ach noun has 
tWO classifiers, a sortal classifier and a group classifier listed. 
Japanese does not, so we will not discuss the generation of 
group classiliers here. 
with the first semantic lass. However, in the case of 
usagi it is not counted with the default classifier for 
animals -hiki, but with that for birds -wa, this must 
be listed as an exception. 
Secondly, we have added a method for generating 
classifiers that quantify coordinate noun phrases. 
These commonly appear in appositive noun phrases 
such as ABC-to XYC-no 2-sha "the two companies, 
ABC and XYZ". 
1. For a simple noun phrase 
(a) If the head noun has a default classifier in 
the lexicon: 
use the noun's default classifier 
(b) Else if it exists, use the defimlt classifier 
of the head noun's first listed semantic 
class (the class's default classifier) 
(c) Else use the residual classifier -tsu 
2. For a coordinate noun phrase 
generate the classifier for each noun phrase 
use the most frequent classifier 
Figure 1: Algorithm to generate numeral classifiers 
In addition, we investigate to what degree we 
could use inheritance to remove redundancy from 
the lexicon, ff a noun's default classifier is the same 
as the default classifier for its semantic lass, then 
there is no need to list it in the lexicon. This makes 
the lexicon smaller and it is easier to add new en- 
tries. Any display of the lexical item (such as for 
maintenance or if the lexicon is used as a human 
aid), should automatically generate the classifier 
from the semantic lass. Alternatively (and equiv- 
alently), in a lexicon with multiple inheritance and 
defaults, the class's default classifier can be added 
as a defeasible constraint on all lnembers of the se- 
mantic class. 
3 The  Go i -Ta ike i  Onto logy  
We used tim ontology provided by Goi-Taikei - -  A 
Japanese Lexicon (Ikehara et al, 1997). We choose 
it because of its rich ontology, its extensive use in 
many other NLP applications, its wide coverage of 
Japanese, and tile fact that it is being extended to 
other numeral classifier languages, such as Malay. 
The ontology has several hierarchies of concepts: 
92 
with both is-a and has-a rehttionshil)s. 2,710 se- 
mantic classes (12-level t'ee structure) for common 
nouns, 200 chtsses (9-level tree structure) for proper 
nouns and 108 classes for predicates. We show the 
top three levels of the common norm ontology in 
Figure 2. Words can be assigned to semantic lasses 
anywhere in the hierarchy. Not all semantic lasses 
have words assigned to them. 
The semantic classes are used in the Jalmnese 
word semantic dictionary to classify nouns, verbs 
and adjectives. The dictionary inchtdes 100,000 
common nouns, 70,000 technical terms, 200,000 
proper nouns and 30,000 other words: 400,000 
words in all. The semantic lasses al'e also used as 
selectional restrictions on the arguments o1' predi- 
cates in a separate predicate dictionary, with around 
17,000 entries. 
Figure 3 shows an example of one record of the 
Japanese semantic word dictionary, with the addi- 
tion of the new I)I{FAU1\]I" CLASSIFIFA{ lield (under- 
lined for elnphasis). 
Each record has an index form, pronunciation, 
a canonical form, part-of-speech and semantic 
classes. Each word can have up to five common 
iloun classes and ten proper noun chtsses, hi the 
case of usagi "rabbit", there are two common noun 
classes and no proper noun classes. 
4 Maplfing Classiliers to the Onto logy  
In this section we investigate how l'ar the seman- 
tic classes can be used to predict default classiticrs 
for nouns. Because most sortal classifiers select 
for some kind of semantic lass, we thought that 
nouns grouped together under the same senmntic 
class should share the same classifier. 
We associated classifiers with semantic classes 
by hand. This took around two weeks. We found 
that, while some classes were covered by a single 
classifier, around 20% required more than one. For 
example, 1056:song is counted only by -kyoku 
"tune", and 989 :waker  veh icZe  by only by - 
seki "ship", but the class \[961:weapon\] had menl- 
bet's counted by -hen "long thin", -chO "knife", -.fitri 
"swords", -ki "machines" and more. 
We show the most flequeut numeral classifiers in 
Table 1. We ended up with 47 classifiers used as 
semantic lasses' default classifiers. This is in line 
with the fact that most speakers of Japanese know 
and use between 30 and 80 sortal classifiers (l)own- 
ing, 1996). Of course, we expect o add more clas- 
sifters at the noun level. 
801 semantic lasses turned out not to have clas- 
siliers. This included chtsses with no words associ- 
ated with them, and those that only contained nouns 
with referents o abstract we considered them to be 
uncountable, such as greed, lethargy, etc. 
We used the default chtssifiers assigned to the se- 
mantic classes to generate defeasible del'aults for the 
noun entries in the common and technical term dic- 
tionaries (172,506 words in all). We did this in order 
to look at the distribution of classifiers over words in 
the lexicon. In the actual generation this would be 
done dynamically, after the semantic lasses have 
been disambiguated. The distributions of classifiers 
were similar to those of the semantic lasses, al- 
though there was a higher proportion counted with 
the residual classilier -tsu, and the classifier for ma- 
chines -ekti. This may be an artifact of the 70,000 
word technical term dictionary. As further esearch, 
wc would like to calculate the distribution of classi- 
\[iers in some text, althottgh we expect it to depend 
greatly on the genre. 
The mapping we created is not complete because 
some of the semantic lasses have nouns which do 
not share the same classifiers. We have to add lnore 
specific defaults at the noun level. As well as more 
specific sortal classifiers, there are cases where a 
group classifier may be more appropriate. For ex- 
ample, among the nouns counted with -~zi~ there are 
entries such as couple, twins and so on which are 
often counted with -kumi "pair". 
In addition, the choice o1' classilier can depend on 
factors other than just semantic lass, for example, 
hire "people" can be counted by either -nin or -mei, 
the only difference being that -mei is more polite. 
it was difficult to assign default classifiers to 
the semantic lasses that referred to events. These 
chtsses mainly include deverbal nouns (e.g. konomi 
"liking") and nominal verbs (e.g., benkyO "study"). 
These can stand for both the action or the result of 
the action: e.g. kenkyl7 "a study/research". In these 
cases, every application we considered would dis- 
tinguish between event and sortal classification in 
the input, so it was only necessary to choose a clas- 
sifier for the result of the action. 
5 Evaluation and Discuss ion 
The algorithm was tested oil a 3700 sentence tna- 
claine translation test set of Japanese with English 
translatious, although we only used the JapaneseJ 
~The test set is available at www.kec l .n t t . co . jp /  
i c l /mtg / resources .  
93 
1 : noun  
2 : concrete  
3 : 388 : 533 : 
agent  p lace  ob jec t  
i000 : abst rac t  
1001 : 1235 : 2422 : 
abst rac t  th ing  event  re la t ion  
Figure 2: Top three levels of the Goi-Taikei Common Noun Ontology 
;'CCo;'H 
-INDEX FORM 
PRONUNCIATION 
CANONICAL FORM 
PAP.T OF SPEECH 
\])IEFAULT CLASSIFIER 
SEMANTIC CLASSES 
ey +)-:~ (usagi) 
"5 ~ -~"/usagi/ 
~, (usagi) 
noun 
~g~ (-wa) 
COMMON NOUN 537:beast  \] 
843 meat /egg\ ]  
Figure 3: Japanese Lexical Entry for rabbit "usagi" 
We only considered sentences with a noun phrase 
modified by a sortal classifier. Noun phrases modi- 
lied by group classifiers, such as -soku "pair" were 
not evaluated, as we reasoned that the presence of 
such a classifier would be marked in the input to the 
generator. We also did not consider the anaphoric 
use of numeral classifiers. Although there were 
ninny anaphoric examples, resolving them requires 
robust anaphor esolution, which is a separate prob- 
lem. We estimate that we would achieve the same 
accuracy with the anaphoric examples if their ref- 
erents were known, unfortunately the test set did 
not always include the full context, so we could not 
identify the referents and test this. A typical exam- 
ple of anaphoric use is (10). 
(1o) shukka-ga ruiseki-de 500-hon-wo 
shipment-NOM cumulative 500-CL-ACC 
toppa-shita 
roached 
Cumulative shipments reached 500 ?bar- 
rels/rolls/logs/... 
In total, there were 90 noun phrases modified by a 
sortal classilier. Our test of the algoritlml was done 
by hand, as we have no Japanese generator. We as- 
sumed as input only the fact that a classifier was 
required, and the semantic lasses of the head noun 
given in the lexicon. Using only the default classi- 
tiers predicted by the senmntic lass, we were able 
to generate 73 (81%) correctly. A classifier was only 
judged to be correct if it was exactly the stone as 
that in the original test set. This was ahnost double 
the base line of generating the most common clas- 
sifter (-nin) for all noun phrases, which would have 
achieved 41%. The results, with a breakdown of the 
errors, are summarized in Table 2. 
In this small sample, 6 out of 90 (6.7%) of noun 
phrases needed to have tim default classifier marked 
for the nouu. In fact, there were only 4 different 
nouns, as two were repeated. We therefore stinmte 
that fewer than 6% of nouns will need to have their 
own default classifier marked. Had the default clas- 
sifier for these nouns been marked in the lexicon, 
our accuracy would have been 88%, the maxinmm 
achievable for our method. 
94 
CI.ASSIFIER P, eferents class|lied Semantic Class (2,710) Noun (172,506) 
No. % Example No. % 
None Uncountable referents 794 29.3 3 : agent  34,548 20.0 
-kai (IN) events 703 25.9 1699 :v i s i t  35,050 20.3 
-tsu (O) abstract/general objects 565 20.9 2 : concrete  52,921 30.1 
-nin (J,,) person 298 11.0 5 :person 8,545 4.9 
-ko ({\])iI) concrete objecls 124 4.6 854 : ed ib le  f ru i t  14,380 8.3 
-hen (J?) long thin objecls 52 1.9 673 : t ree  3,775 2.1 
-mai (~)  fiat objecls 32 1.2 7 70 : paper  2,807 1.6 
-teki (}l',;~J) liquid 21 0.8 652 : tear  1,219 0.7 
-dai (?i't) lnechanic itclnS/furniture 18 0.7 9 62 : mach inery  5,087 2.9 
-hiki (l;ri) animals 12 0.6 537 :beast  1,361 0.8 
Other 38 classifiers 91 3.4 12,813 7.4 
Table 1 : Japanese Numeral Classiliers and associated Semantic Classes 
P, esult % No. 
Correctly generated 81% 73 
Incorrectly generated 19% 17 
Total 100% 90 
Breakdown o1' En'ors 
Noun needs default classilier - -  6 
Target not in lexicon, bad entry 4 
()ther errors 7 
Table 2: P, esults of atplying the algorilhm 
Looking at it from allolher point of view, the 
Goi-Taikei ontology, although initially designed i'or 
.lapanese analysis, was also useftfl for generating 
Japanese numeral chtssifiers. We consider that it 
would be equally useful for the same task with Ko- 
l'can, or even lhe tmrelaled language Mahty. 
We generated the residual classilier -tsu for nouns 
not in the lexicon, this proved to be a bad choice 
lbr three unknown words. If we had a me(hod o1: 
deducing senlanlic chtsses for tlnknown words wc 
couM have used it to predict the classiiicr more 
successfully. 1;or example, kikan-l&vhika "insti- 
tutional investor ''5 was not in the dictionary, and 
so we used the senmntic class for lOshika "in- 
vestor", which was 175 : investor ,  a sub-type 
of 5 :person .  Had kikan-toshika "institutional 
investor" been marked as a subtype of company, 
or if we had deduced the semantic lass from the 
modifier, then we would have been able to gener- 
5hmlitufional illvcStOl'S are \[inancial institutions tha! invest 
savin~,s of individuals and non-lina.ncial companies in the fi- 
nancial nmrkets. 
ate tho correct classifior -sha. In ono case, wc felt 
lho default ordering of the semantic lasses should 
have been reversed: 673: t ree  was listed before 
854 : ed ib le  f ru i t  for ringo "apple". 
The remaining errors were moro problematic. 
There was one cxamplc, 80,O00-nin-amari-no 
.vl,#nlei "about 80,000 signatures", wlfich could be 
ueated as rel:ercnt tlansfof: shomei "signature" 
was being counted wilh the classifier for people. 
Another l)ossiblc analysis is that the classilier is 
the head of a referential noun phrase with deic- 
tic/almphoric reference, equivalent to the si,qnaluJws 
oJ'ahold SO, 000 people. A COUlJe were quile literary 
in slylc: for example lOnen-no loshi "10 years (Lit: 
10 years of years)", where the loshi "year" lmrt is 
redundant, and would not normally be used. in two 
of the errors the residual classilier was used instead 
of (he more specific default. Shhnoio (1997) prc~ 
dicls flint this will happen in expressions where lhe 
amot ln l  is being emphasized more than what is be- 
ing counted. Intuitively, lifts applied in both cases, 
but we were ul\]able to identify any features we could 
exploit 1o make this judgment autolnatically. 
A more adwmced semantic analysis may be able 
lo dynamically delermine the appropriate semantic 
class for cases of rel'ercnt transfer, unknown words, 
or words whose semantic lass can be restricted by 
context Our algorithm, which ideally generates the 
classifier from this dynamically determined seman- 
tic class allows us to generate the correct classilier 
in context, whereas using a default listed for a noun 
does not. This was our original mot|wit|on 1'oi" gen- 
erating chtssitiers 1?o111 seman(ic lasses, rather than 
using a classifier lis(ed wilh each noun as Sornlert- 
95 
lamvanich et al (1994) do. 
In this paper we have concentrated on solving 
the problem of generating appropriate Japanese nu- 
meral classifiers using an ontology. 11\] future work, 
we would like to investigate in more detail the con- 
ditions under which a classifier needs to be gener- 
ate& 
6 Conclusion 
In this paper, we presented an algorithm to generate 
Japanese numeral classifiers. It was shown to select 
the correct sortal classifier 81% of the time. The al- 
gorithm uses the ontology provided by Goi-Taikei, 
a Japanese lexicon, and shows how accurately se- 
mantic lasses can predict numeral classifiers for the 
nouns they subsume. We also show how we can 
improve the accuracy and efficiency ftmher through 
solving other natural language processing problems, 
in particular, referent ransfer, anaphor esolution 
and word sense disambiguation. 
Acknowledgments 
The authors thank Kentaro Ogura, Timothy Bald- 
win, Virach Sornlertlamvanich and the anonymous 
reviewers for their helpful comments. 
References 
Yoshimi Asahioka, Hideki Hirakawa, and Shin-ya 
Amano. 1990. Semantic lassification and an 
analyzing system of Japanese numerical expres- 
sions. 1PSJ SIG Notes 90-NL-78, 90(64):129- 
136, July. (in Japanese). 
A. E. Backhouse. 1993. The Japatwse Language: 
An h~troduction. Oxford University Press. 
Francis Bond and Kyonghee Paik. 1997. Classify- 
ing correspondence in Japanese and Korean. In 
3rd Pacific Association for Computational Lin- 
guistics Conference: PACLING-97, pages 58-67. 
Meisei University, Tokyo, Japan. 
Francis Bond, Kentaro Ogura, and Satom Ikehara. 
1996. Classifiers in Japanese-to-English machine 
translation. In 16th International Coqference on 
Computational Linguistics: COLING-96, pages 
125-130, Copenhagen, August. (h t tp : / /  
xxx. lanl. gov/abs/cmp- ig/9608014). 
Francis Bond, Daniela Kurz, and Satoshi Shirai. 
1998. Anchoring floating quantifiers in Japanese- 
to-English machine translation. In 36th Anmtal 
Meeting of the Association .for Computational 
Linguistics and 17th hzternational Conference 
on Computational Linguistics: COLING/ACL- 
98, pages 152-159, Montreal, Canada. 
Pamela Downing. 1996. Nunwral Class(tier Sys- 
tems, the case of Japanese. Jolm Benjamins, Am- 
sterdam. 
Christine Fellbamn, editor. 1998. WordNet: An 
Electronic Lexical Database. MIT Press. 
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, 
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, 
Yoshifumi Ooyama, and Yoshihiko Hayashi. 
1997. Goi-Taikei - -  A Japanese Lexicon.. 
Iwanami Shoten, Tokyo. 5 volumes/CDROM. 
Shin-ichiro Kamei and Kazunori Muraki. 1995. An 
analysis of NP-like quantifiers in Japanese. In 
First Natural l_zmguage Processing Pactific Rim 
Symposium: NLPRS-95, volume 1, pages 163- 
167. 
Sergei Nirenburg. 1989. KBMT-89 - -  a 
knowledge-based MT proiect at Carnegie 
Mellon University. pages 141-147, Aug. 16-18. 
James Pusteiovsky. 1995. The Generative Lexicon. 
MIT Press. 
Mitsuaki Shimojo. 1997. The role of the general 
category in the maintenance of numeral classi- 
fier systems: The case of tsu and ko in Japanese. 
Linguistics, 35(4). (h t tp  : / / i f rm.  g locom.  
ac. jp/doc/sOl. 001 .html). 
Virach Sornlertlamvanich, Wantanee Pantachat, 
and Surapant Meknavin. 1994. Classifier 
assignment by corpus-based approach. In 
15th International Conference on Computa- 
tional Linguistics: COLING-94, pages 556- 
561, August. (h t tp : / /xxx .  lan l .gov /  
abs/cmp- ig/9411027). 
Shoichi Yokoyalna and Takeru Ochiai. 1999. 
Aimai-na sfiry6shi-o fukumu meishiku-no 
kaisekih6 \[a method for analysing noun phrases 
with ambiguous quantifiers.\]. In 5th Almual 
Meeting of the Association for Natural Lzmguage 
Processing, pages 550-553. The Association for 
Natural Language Processing. (in Japanese). 
,+a:~--:~,5o NNT ' JZNN,~N~,  q~NN, t3 
7%, 6 %~cr)~l~l ,~Uc  r) b'~Jb~'\[~l~%-5: 
96 
Using an Ontology to Determine English Countability
Francis Bond? and Caitlin Vatikiotis-Bateson??
* bond@cslab.kecl.ntt.co.jp ** caitlinvb@yahoo.com
2-4 Hikari-dai Seika-cho, Kyoto, Japan 619-0237
NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
Abstract
In this paper we show to what degree the count-
ability of English nouns is predictable from their
semantics. We found that at 78% of nouns?
countability could be predicted using an ontol-
ogy of 2,710 nodes. We also show how this
predictability can be used to aid non-native
speakers to determine the countability of En-
glish nouns when building a bilingual machine
translation lexicon.
1 Introduction
In English, nouns heading noun phrases are typ-
ically either countable or uncountable (also
called count and mass). Countable nouns can
be modified by denumerators, prototypically
numbers, and have a morphologically marked
plural form: one dog , two dogs. Uncountable
nouns cannot be modified by denumerators, but
can be modified by unspecific quantifiers such
as much, and do not show any number dis-
tinction (prototypically being singular): * one
equipment , some equipment , *two equipments.
Knowledge of countability is important when
translating from a source language without
obligatory number and countability distinctions
to a target language that does make num-
ber distinctions. Some examples are Japanese-
to-English (Ehara and Tanaka, 1993; Bond,
2001), Japanese-to-German (Siegel, 1996), and
Chinese-to-English.
For a system generating English, it is impor-
tant to know the countability of the head noun,
as this determines whether it can become plural,
and the range of possible determiners. Knowl-
edge of countability is particularly important in
machine translation, because the closest trans-
??This research was done while the second author was
visiting the NTT Communication Science Laboratories
lation equivalent may have different countabil-
ity from the source noun. Many languages, such
as Chinese and Japanese, do not mark count-
ability, which means that the choice of count-
ability will be largely the responsibility of the
generation component.
In this paper, we measure how well seman-
tic classes predict countability. Obviously, the
answer depends both on how many countability
distinctions are made, and how many semantic
classes are used. If every sense of every word
belongs to its own semantic class, then seman-
tic classes will uniquely, although not usefully,
predict countability. This is effectively the po-
sition taken by Wierzbicka (1988), where the
semantics of a noun, given in the Natural Se-
mantic Metalanguage, always provides enough
information to predict the countability. On the
other hand, if there are only a handful of se-
mantic classes, then they will have little pre-
dictive power. We first define countability, and
discuss its semantic motivation (? 2). Then we
present the lexical resources used in our exper-
iment (? 3), including the ontology of 2,710 se-
mantic classes. Next, we describe the experi-
ment, which uses the semantic classes of words
in a Japanese-to-English transfer dictionary to
predict their countability (? 4). Finally, we
present the results and discuss the theoretical
and practical implications in (? 5).
2 Linguistic Background
Grammatical countability is motivated by
the semantic distinction between object
and substance reference (also known as
bounded/non-bounded or individuated/
non-individuated). Imai and Gentner (1997)
show that the presence of countability in En-
glish and its absence in Japanese influences how
native speakers conceptualize unknown nouns
as objects or substances. There is definitely
some link between countability and conceptual-
ization, but it is a subject of contention among
linguists as to how far grammatical countability
is motivated and how much it is arbitrary. Jack-
endoff (1991) assumes countability and number
to be fully motivated, and shows various rules
for conversion between countable and uncount-
able meanings, but does not discuss any of the
problematic exceptions.
The prevailing position in the natural lan-
guage processing community is to effectively
treat countability as though it were arbitrary
and encode it as a lexical property of nouns.
Copestake (1992) has gone some way toward
representing countability at the semantic level
using a type form with subtypes countable
and uncountable with further subtypes below
these. Words that undergo conversion between
different values of form can be linked with lexi-
cal rules, such as the grinding rule that links a
countable animal with its uncountable inter-
pretation as meat. These are not, however di-
rectly linked to a full ontology. Therefore there
is no direct connection between being an animal
and being countable.
Bond et al (1994) suggested a division of
countability into five major types, based on
Allan (1980)?s noun countability preferences
(NCPs). Nouns which rarely undergo conver-
sion are marked as either fully countable,
uncountable or plural only. Nouns that are
non-specified are marked as either strongly
countable (for count nouns that can be con-
verted to mass, such as cake) or weakly
countable (for mass nouns that are readily con-
vertible to count, such as beer). Conversion is
triggered by surrounding context. Noun phrases
headed by uncountable nouns can be converted
to countable noun phrases by generating clas-
sifiers: one piece of equipment , as described in
Bond and Ikehara (1996).
Full knowledge of the referent of a noun
phrase is not enough to predict countability.
There is also language-specific knowledge re-
quired. There are at least three sources of ev-
idence for this: the first is that different lan-
guages encode the countability of the same ref-
erent in different ways. To use Allan (1980)?s
example, there is nothing about the concept de-
noted by lightning that rules out *a lightning
being interpreted as a flash of lightning . In
both German and French (which distinguish be-
tween countable and uncountable uses of words)
the translation equivalents of lightning are fully
countable (ein Blitz and un e?clair respectively).
Even within the same language, the same ref-
erent can be encoded countably or uncount-
ably: clothes/clothing , things/stuff , jobs/work .
The second evidence comes from the psycho-
linguistic studies of Imai and Gentner (1997)
who show that speakers of Japanese and En-
glish characterize the same referent in different
ways depending on whether they consider it to
be countable (more common for English speak-
ers) or uncountable (more common for Japanese
speakers). Further evidence comes from the En-
glish of non-native speakers, particularly those
whose native grammar does not mark countabil-
ity. Presumably, their knowledge of the world
is just as complete as English native speakers,
but they tend to have difficulty with the English
specific conceptual encoding of countability.
In the next section (? 3) we describe the re-
sources we use to measure the predictability of
countability by meaning, and then describe our
experiment (? 4).
3 Resources
We use the five noun countability classes of
Bond et al (1994), and the 2,710 seman-
tic classes used in the Japanese-to-English ma-
chine translation system ALT-J/E (Ikehara et
al., 1991). These are combined in the machine
translation lexicons, allowing us to quantify how
well semantic classes predict countability.
3.1 Semantic Transfer Dictionary
We use the common noun part of ALT-J/E?s
Japanese-to-English semantic transfer dictio-
nary. It contains 71,833 linked Japanese-
English pairs. A simplified example of the entry
for usagi ?rabbit? is given in Figure 1. Each
record of the dictionary has a Japanese index
form, a sense number, an English index form,
English syntactic information, English seman-
tic information, domain information and so on.
English syntactic information includes the part
of speech, noun countability preference, default
number, default article and whether the noun
is inherently possessed. The semantic informa-
tion includes common and proper noun seman-
tic classes. In this example, there are two se-
mantic classes: animal subsumed by living
thing, and meat subsumed by foodstuff .
Because the dictionary was developed for
a Japanese-to-English machine translation sys-
tem, many of the English translations are longer
than the Japanese source terms: many concepts
encoded in a single lexical item in Japanese may
need multiple words in English. Of the 71,833
entries, 41,285 are multi-word expressions in
English (57.4%).
3.2 Semantic Ontology
ALT-J/E?s ontology classifies concepts to use
in expressing relationships between words. The
meanings of common nouns are given in terms of
a semantic hierarchy of 2,710 nodes. Each node
in the hierarchy represents a semantic class.
Edges in the hierarchy represent is-a or has-
a relationships, so that the child of a semantic
class related by an is-a relation is subsumed
by it. For example, organ is-a body-part.
The semantic hierarchy and the Japanese dic-
tionary marked with it have been published as
Goi-Taikei: A Japanese Lexicon (Ikehara et al,
1997).
The semantic classes are primarily used to
distinguish between word-senses using the se-
lectional restrictions which predicates place on
their arguments. Countability has not been
used as a criterion in deciding which word
should go into which class. In fact, because
the dictionary has been built mainly by native
Japanese speakers, who do not have reliable in-
tuitions on countability, it was not possible to
use countability to help decide into which class
to put a given word.
Although the dictionary has been extensively
used in a machine translation system, errors still
exist. A detailed examination of user dictionar-
ies with the same information content, made by
the same lexicographers who built the lexicon,
found errors in 11?21% of the entries (Ikehara
et al, 1995). A particularly common source of
errors was words being placed one level too high
or low in the hierarchy. The same study found
that 90% of words entered into a user dictio-
nary could be automatically assigned to lexical
classes with 13?25% errors, although words were
assigned to too many semantic classes 32?56%
of the time (the range in errors is due to differ-
ent results from different domains: newspapers
and software manuals).
3.3 Noun Countability Preferences
Nouns in the dictionary are marked with one
of five major countability preference classes:
fully countable, strongly countable,
weakly countable, uncountable and plural
only, described at length in Bond (2001).
In addition to countability, default values
for number and classifier (cl) are also part
of the lexicon. The classes and additional
features are summarized in Table 1, along with
their distribution in ALT-J/E?s common noun
dictionary.1 The most common NCP is fully
countable, followed by uncountable.
The two most basic types are fully
countable and uncountable. Fully countable
nouns such as knife have both singular and plu-
ral forms, and cannot be used with determiners
such as much, little, a little, less and overmuch.
Uncountable nouns, such as furniture, have no
plural form, and can be used with much.
Between these two extremes there are a vast
number of nouns, such as cake, that can be
used in both countable and uncountable noun
phrases. They have both singular and plu-
ral forms, and can also be used with much.
Whether such nouns will be used countably or
uncountably depends on whether their refer-
ent is being thought of as made up of discrete
units or not. As it is not always possible to
determine this explicitly when translating from
Japanese to English, we divide these nouns into
two groups: strongly countable, those that
refer to discrete entities by default, such as cake,
and weakly countable, those that refer to non-
bounded referents by default, such as beer . At
present, these distinctions were made by the
lexicographers? intuition, as there are no large
sense-tagged corpora to train from.
In fact, almost all English nouns can be used
in uncountable environments, for example, if
they are given the ground interpretation. The
only exception is classifiers such as piece or bit ,
which refer to quanta, and thus have no un-
countable interpretation.
Language users are sensitive to relative fre-
quencies of variant forms and senses of lexi-
cal items (Briscoe and Copestake, 1999, p511).
The division into fully, strongly, weakly
1We ignore the two subclasses in this paper:
collective nouns are treated as fully countable and
semi-countable as uncountable.
??
?
?
?
?
?
?
?
?
?
?
Index usagi
sense 1
?
?
?
?
?
?
?
?
?
English Translation rabbit
Part of Speech noun
Noun Countability Pref. strongly countable
Default Number singular
Semantic Classes
[
common noun animal, meat
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Japanese-English Noun Lexical Entry (usagi ? rabbit)
Table 1: Noun Countability Preferences
Noun Countability Code Example Default Default # %
Preference Number Classifier
fully countable CO knife sg ? 47,255 65.8
strongly countable BC cake sg ? 3,110 4.3
weakly countable BU beer sg ? 3,377 4.7
uncountable UC furniture sg piece 15,435 21.5
plural only PT scissors pl pair 2,107 2.9
and uncountable is, in effect, as a coarse way
of reflecting this variation for noun countability.
The last major type of countability prefer-
ence is plural only: nouns that only have a
plural form, such as scissors. They can neither
be denumerated nor modified by much. plural
only are further divided depending on what
classifier they take. For example, pair plural
only nouns use pair as a classifier when they
are denumerated: a pair of scissors. This is
motivated by the shape of the referent: pair
plural only nouns are things that have a bi-
partite structure. Such words only use a sin-
gular form when used as modifiers (a scissor
movement). Other plural only such as clothes
use the plural form even as modifiers (a clothes
horse). In this case, the base (uninflected) form
is clothes, and the plural form is zero-derived
from it. The word clothes cannot be denumer-
ated at all. If clothes must be counted, then
a countable word of similar meaning is substi-
tuted, or clothing is used with a classifier: a
garment, a suit, a piece of clothing .
Information this detailed about noun count-
ability preferences is not found in standard
dictionaries. To enter this information into
the transfer lexicon, a single (Australian) En-
glish native speaker with some knowledge of
Japanese examined all of the entries in Goi-
Taikei?s common-noun dictionary and deter-
mined appropriate values for their countability
preferences.
4 Experiment and Results
To test how well the semantic classes predict the
countability preferences, we carried out a series
of experiments.
We ran the experiments under several condi-
tions, to test the effect of combinations of se-
mantic classes and single-word or multi-word
entries. In all cases the baseline was to
give the most frequently occurring noun count-
ability preference (which was always fully
countable).
In the experiments, we use five NCPs (fully,
strongly, weakly countable, uncountable
and plural only), we do not consider default
number in any of the experiments.
For each combination of semantic classes
in the lexicon, we calculated the most com-
mon NCP. Ties are resolved as follows: fully
countable beats strongly countable beats
weakly countable beats uncountable beats
plural only. For example, consider the se-
mantic class 910:tableware with four mem-
bers: shokki ? tableware (UC), youshokki ?
dinner set (CO), youshokki ? Western-style
tableware (UC) and toukirui ? crockery (UC).
Conditions Entries % Range Baseline
Training=Test all 77.9 76.8?78.6 65.8
Tenfold Cross Validation all 71.2 69.8?72.1 65.8
Tenfold Cross Validation single-word 66.6 65.6?67.7 58.6
Tenfold Cross Validation multi-word 74.8 73.9?75.8 71.1
Table 2: Results
The most common NCP is UC, so the NCP as-
sociated with this class is uncountable.
In our first experiment, we calculated the per-
centage of entries whose NCP was the same
as the most common one. For example,
the NCP associated with the semantic class
910:tableware is uncountable. This is correct
for three out of the four words in this semantic
class. This is equivalent to testing on the train-
ing data, and gives a measure of how well se-
mantic classes actually predict noun countabil-
ity in ALT-J/E?s lexicon: 77.9% of the time.
This is better than the base-line of all fully
countable which would give 65.8%. All the re-
sults are presented in Table 2.
In order to test how useful countability would
be in predicting the countability of unknown
words, we tested the system using stratified
ten-fold cross validation. That is, we divided
the common noun dictionary into ten sets, then
tested on each set in turn, with the other nine-
tenths of the data used as the training set. In
order to ensure an even distribution, the data
was stratified by sorting according to semantic
class with every 10th item included in the same
set. If the combination of semantic classes was
not found in the test set, we took the count-
ability to be the overall most common NCP:
fully countable. This occurred 11.6% of the
time. Using only nine tenths of the data,
the accuracy went down to 71.2%, 5.4% above
the baseline. In this case the training set for
910:tableware will still always contain a ma-
jority of uncountable nouns, so it will be asso-
ciated with UC. This will be correct for all the
words in the class except youshokki ? dinner
set (CO).
Finally, we divided the dictionary into single
and multiple word entries (looked at from the
English side) and re-tested. It was much harder
to predict countability for single words (66.6%)
than it was for multi-word expressions (74.8%).
We will discuss the reason for this in the next
section.
5 Discussion
The upper bound of 78% was lower than we
expected. There were some problems with
the granularity of the hierarchy. In English,
the class names of heterogeneous collections
of objects tend to be uncountable, while the
names of the actual objects are countable.
For example, the following terms are all hy-
ponyms of tableware in Wordnet (Fellbaum,
1998): cutlery, chopsticks, crockery, dishware,
dinnerware, glassware, glasswork, gold plate,
service, tea set, . . . . Most of the entries
are either uncountable, or multi-word expres-
sions headed by group classifiers, such as ser-
vice and set . The words below these classes
are almost all countable, with a sprinkling of
plural only (like tongs). Thus in the three
levels of the hierarchy, two are mainly un-
countable, and below that mainly countable.
However, ALT-J/E?s ontology only has two
levels here: 910:tableware has four daugh-
ters, all leaf nodes in the semantic hierarchy:
911:crockery, 912:cookware, 913:cutlery
and 914:tableware (other). The majority
NCPs for all four of these classes are fully
countable. The question arises as to whether
words such as cutlery should be in the upper or
lower level. Using countability as an additional
criterion for deciding which class to add a word
to makes the task more constrained, and there-
fore more consistent. In this case, we would add
cutlery to the parent node 910:tableware, on
the basis of its countability (or add a new layer
to the ontology).
Adding countability as a criterion would also
help to solve the problem of words being entered
in a class one level too high or too low, as noted
in Section 3.2.
We were resigned to getting almost all of the
pair plural only wrong, and we did, but they
amount to less than 3% of the total. Although
there are some functional similarities, such as
a large percentage of 820:clothes for the
lower body, it was more common to get one or
two in an otherwise large group, such as tongs in
the 913:cutlery class, which is overwhelmingly
fully countable. Because the major differen-
tiator is physical shape, which is not included in
our semantic hierarchy, these words cannot be
learned by our method. This is another argu-
ment for the importance of representing phys-
ical shape so that it is accessible for linguistic
processing.
We had expected single word entries to be
easier to predict than multiple word entries, be-
cause of the lack of influence of modifiers. How-
ever, the experiment showed the opposite. In-
vestigating the reason found that single word
entries tended to have more semantic classes per
word (1.38 vs 1.34) and more varied combina-
tions of semantic classes. This meant that there
were 5.1 entries per combination to train on for
the multi-word entries, but only 3.7 for the sin-
gle word entries. Therefore, it was harder to
train for the single word entries.
As can be seen in the case of tableware given
above, there were classes where the single-word
and multi-word expressions in the same seman-
tic class had different countabilities. Therefore,
even though there were fewer training exam-
ples, learning the NCPs differently for single
and multi-word expressions and then combing
the results gave an improved score: 72.0%.
Finally, there were also substantial numbers
of genuine errors, such as  
	 sofuto
kara? which has two translations soft colour and
soft collar . Their semantic classes should have
been hue and clothing respectively, but the
semantic labels were reversed. In this case the
countability preferences were correct, but the
semantic classes incorrect.
An initial analysis of the erroneous predic-
tions suggested that the upper bound with all
genuine errors in the lexicon removed would be
closer to 85% than 78%. We speculate that
this would be true for languages other than En-
glish because is not specifically tuned to En-
glish, it was developed for Japanese analysis.
Unfortunately we do not have a large lexicon of
French, German or some other countable lan-
guage marked with the same ontology to test
on.
5.1 Further Work
First, we would like to look more at multi-
word expressions. There is a general trend
for the head of a multiword expression to de-
termine the overall countability, which we did
not exploit. Modifiers can also be informative,
particularly for quantified expressions such as
zasshoku ? various colors whose English part
must be countable as it is explicitly denumer-
ated.
Second, we would like to investigate further
the relation between under-specified semantics
and countability. Words such as usagi ? rab-
bit are marked with the semantic classes for
animal and meat, and the single NCP strongly
countable. It may be better to explicitly iden-
tify countability with the animal sense, and un-
countability with the meat sense. In this way,
we could learn NCPs for each semantic class
individually (ignoring plural only) and look
at ways of combining them, or of dynamically
assigning countability during sense disambigua-
tion. Learning NCPs for each class individually
could also help to predict NCPs for entries with
idiosyncratic combinations, for which training
data may not be found.
Finally, from a psycho-linguistic point of
view, it would be interesting to test whether un-
predictable countabilities (that is those words
whose countability is not motivated by their se-
mantic class) are in fact harder for non-native
speakers to use, and more likely to be translated
incorrectly by humans.
5.2 Applications
In general, many errors in countability that had
been overlooked by the lexicographers in the
original compilation of the lexicon and its subse-
quent revisions became obvious when looking at
the words grouped by semantic class and noun
countability preference. Most entries were made
by Japanese native speakers, who do not make
countability distinctions. They were checked
by a native speaker of English, who in turn
did not always understand the Japanese source
word, and thus was unable to identify the cor-
rect sense.
Adding a checker to the dictionary tools,
which warns if the semantic class does not pre-
dict the assigned countability, would help to
avoid such errors. Such a tool could also be
used for fine tuning the position of words in the
hierarchy, and spotting flat-out errors.
Another application of these results is in au-
tomatically predicting the countability of un-
known words. It is possible to automatically
predict semantic classes up to 80% of the time
(Ikehara et al, 1995). These semantic classes
could then be used to predict the countability
at a level substantially above the baseline.
6 Conclusions
Even with a limited ontology and noisy lexicon,
semantics does predict countability around 78%
of the time. Therefore countability is shown to
correlate with semantics. This semantic motiva-
tion can be used to build tools to (a) automat-
ically predict countability for unknown words,
and (b) serve as a check on consistency when
building a dictionary.
Acknowledgments
The authors would like to thank the other mem-
bers of the NTT Machine Translation Research
Group, as well as the NTT Linguistic Media
Group, Timothy Baldwin and Ann Copestake.
This research was supported by the research
collaboration between the NTT Communica-
tion Science Laboratories, Nippon Telegraph
and Telephone Corporation and CSLI, Stanford
University.
References
Keith Allan. 1980. Nouns and countability. Lan-
guage, 56(3):541?67.
Francis Bond and Satoru Ikehara. 1996. When and
how to disambiguate? ? countability in machine
translation ?. In International Seminar on Mul-
timodal Interactive Disambiguation: MIDDIM-
96, pages 29?40, Grenoble. (Reprint of MIDDIM-
1996).
Francis Bond and Kentaro Ogura. 1998. Reference
in Japanese-to-English machine translation. Ma-
chine Translation, 13(2?3):107?134.
Francis Bond and Kyonghee Paik. 1997. Classifying
correspondence in Japanese and Korean. In 3rd
Pacific Association for Computational Linguistics
Conference: PACLING-97, pages 58?67. Meisei
University, Tokyo, Japan.
Francis Bond, Kentaro Ogura, and Satoru Ikehara.
1994. Countability and number in Japanese-to-
English machine translation. In 15th Interna-
tional Conference on Computational Linguistics:
COLING-94, pages 32?38, Kyoto. (http://xxx.
lanl.gov/abs/cmp-lg/9511001).
Francis Bond. 2001. Determiners and Number in
English contrasted with Japanese ? as exemplified
in Machine Translation. Ph.D. thesis, University
of Queensland, Brisbane, Australia.
Ted Briscoe and Ann Copestake. 1999. Lexical
rules in constraint-based grammars. Computa-
tional Linguistics, 25(4):487?526.
Ann Copestake. 1992. The Representation of Lexi-
cal Semantic Information. Ph.D. thesis, Univer-
sity of Sussex, Brighton.
Terumasa Ehara and Hozumi Tanaka. 1993.
Kikaihonyaku-ni-okeru shizengengo shori (natu-
ral language processing in machine translation).
Journal of Information Processing Society of
Japan, 34(10):1266?1273. (in Japanese).
Christine Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hi-
romi Nakaiwa. 1991. Toward an MT system with-
out pre-editing ? effects of new methods in ALT-
J/E?. In Third Machine Translation Summit:
MT Summit III, pages 101?106, Washington DC.
(http://xxx.lanl.gov/abs/cmp-lg/9510008).
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, Fran-
cis Bond, and Yoshie Omi. 1995. Automatic de-
termination of semantic attributes for user de-
fined words in Japanese-to-English machine trans-
lation. Journal of Natural Language Processing,
2(1):3?17. (in Japanese).
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei ? A Japanese Lexicon. Iwanami
Shoten, Tokyo. 5 volumes/CDROM.
Mutsumi Imai and Dedre Gentner. 1997. A crosslin-
guistic study of early word meaning: Univer-
sal ontology and linguistic influence. Cognition,
62:169?200.
Ray Jackendoff. 1991. Parts and boundaries. In
Beth Levin and Steven Pinker, editors, Lexical
and Conceptual Semantics, pages 1?45. Blackwell
Publishers, Cambridge, MA & Oxford, UK.
Kazumi Kawamura, Yasuhiro Katagiri, and
Masahiro Miyazaki. 1995. Multi-dimensional
thesaurus wth various facets,. In IEICE Technical
Report NLC94-48, pages 33?40. (in Japanese).
Melanie Siegel. 1996. Definiteness and number
in Japanese to German machine translation. In
D. Gibbon, editor, Natural Language Processing
and Speech Technology, pages 137?142. Mouton
de Gruyter, Berlin.
Anna Wierzbicka. 1988. The Semantics of Gram-
mar. John Benjamins, Amsterdam.
Acquiring an Ontology for a Fundamental Vocabulary
Francis Bond? and Eric Nichols ?? and Sanae Fujita? and Takaaki Tanaka?
* {bond,fujita,takaaki}@cslab.kecl.ntt.co.jp ** eric-n@is.naist.jp
* NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
** Nara Advanced Institute of Science and Technology
Abstract
In this paper we describe the extraction of
thesaurus information from parsed dictio-
nary definition sentences. The main data
for our experiments comes from Lexeed,
a Japanese semantic dictionary, and the
Hinoki treebank built on it. The dictio-
nary is parsed using a head-driven phrase
structure grammar of Japanese. Knowledge
is extracted from the semantic representa-
tion (Minimal Recursion Semantics). This
makes the extraction process language inde-
pendent.
1 Introduction
In this paper we describe a method of acquiring
a thesaurus and other useful information from
a machine-readable dictionary. The research is
part of a project to construct a fundamental
vocabulary knowledge-base of Japanese:
a resource that will include rich syntactic and
semantic descriptions of the core vocabulary of
Japanese. In this paper we describe the auto-
matic acquisition of a thesaurus from the dic-
tionary definition sentences. The basic method
has a long pedigree (Copestake, 1990; Tsuru-
maru et al, 1991; Rigau et al, 1997). The
main difference from earlier work is that we use
a mono-stratal grammar (Head-Driven Phrase
Structure Grammar: Pollard and Sag (1994))
where the syntax and semantics are represented
in the same structure. Our extraction can thus
be done directly on the semantic output of the
parser.
In the first stage, we extract the thesaurus
backbone of our ontology, consisting mainly of
hypernym links, although other links are also
extracted (e.g., domain). We also link our
??Some of this research was done while the second
author was visiting the NTT Communication Science
Laboratories
extracted thesaurus to an existing ontology of
Japanese: the Goi-Taikei ontology (Ikehara et
al., 1997). This allows us to use tools that ex-
ploit the Goi-Taikei ontology, and also to extend
it and reveal gaps.
The immediate application for our ontology
is in improving the performance of stochastic
models for parsing (see Bond et al (2004) for
further discussion) and word sense disambigua-
tion. However, this paper discusses only the
construction of the ontology.
We are using the Lexeed semantic database
of Japanese (Kasahara et al (2004), next sec-
tion), a machine readable dictionary consisting
of headwords and their definitions for the 28,000
most familiar open class words of Japanese,
with all the definitions using only those 28,000
words (and some function words). We are pars-
ing the definition sentences using an HPSG
Japanese grammar and parser and treebanking
the results into the Hinoki treebank (Bond et
al., 2004). We then train a statistical model
on the treebank and use it to parse the remain-
ing definition sentences, and extract an ontology
from them.
In the next phase, we will sense tag the defini-
tion sentences and use this information and the
thesaurus to build a model that combines syn-
tactic and semantic information. We will also
produce a richer ontology ? by combining infor-
mation for word senses not only from their own
definition sentences but also from definition sen-
tences that use them (Dolan et al, 1993), and
by extracting selectional preferences. Once we
have done this for the core vocabulary, we will
look at ways of extending our lexicon and on-
tology to less familiar words.
In this paper we present the details of the
ontology extraction. In the following section we
give more information about Lexeed and the Hi-
noki treebank.We then detail our method for ex-
tracting knowledge from the parsed dictionary
definitions (? 3). Finally, we discuss the results
and outline our future research (? 4).
2 Resources
2.1 The Lexeed Semantic Database of
Japanese
The Lexeed Semantic Database of Japanese is
a machine readable dictionary that covers the
most common words in Japanese (Kasahara et
al., 2004). It is built based on a series of psy-
cholinguistic experiments where words from two
existing machine-readable dictionaries were pre-
sented to multiple subjects who ranked them on
a familiarity scale from one to seven, with seven
being the most familiar (Amano and Kondo,
1999). Lexeed consists of all open class words
with a familiarity greater than or equal to five.
The size, in words, senses and defining sentences
is given in Table 1.
Table 1: The Size of Lexeed
Headwords 28,300
Senses 46,300
Defining Sentences 81,000
The definition sentences for these sentences
were rewritten by four different analysts to use
only the 28,000 familiar words and the best
definition chosen by a second set of analysts.
Not all words were used in definition sentences:
the defining vocabulary is 16,900 different words
(60% of all possible words were actually used in
the definition sentences). An example entry for
the word   doraiba? ?driver? is given in
Figure 1, with English glosses added. The un-
derlined material was not in Lexeed originally,
we extract it in this paper. doraiba? ?driver?
has a familiarity of 6.55, and three senses. The
first sense was originally defined as just the syn-
onym nejimawashi ?screwdriver?, which has a
familiarity below 5.0. This was rewritten to the
explanation: ?A tool for inserting and removing
screws?.
2.2 The Hinoki Treebank
In order to produce semantic representations
we are using an open source HPSG grammar
of Japanese: JACY (Siegel and Bender, 2002),
which we have extended to cover the dictio-
nary definition sentences (Bond et al, 2004).
We have treebanked 23,000 sentences using the
[incr tsdb()] profiling environment (Oepen and
Carroll, 2000) and used them to train a parse
ranking model for the PET parser (Callmeier,
2002) to selectively rank the parser output.
These tools, and the grammar, are available
from the Deep Linguistic Processing with HPSG
Initiative (DELPH-IN: http://www.delph-in.
net/).
We use this parser to parse the defining sen-
tences into a full meaning representation using
minimal recursion semantics (MRS: Copestake
et al (2001)).
3 Ontology Extraction
In this section we present our work on creating
an ontology. Past research on knowledge ac-
quisition from definition sentences in Japanese
has primarily dealt with the task of automat-
ically generating hierarchical structures. Tsu-
rumaru et al (1991) developed a system for
automatic thesaurus construction based on in-
formation derived from analysis of the terminal
clauses of definition sentences. It was success-
ful in classifying hyponym, meronym, and syn-
onym relationships between words. However,
it lacked any concrete evaluation of the accu-
racy of the hierarchies created, and only linked
words not senses. More recently Tokunaga et
al. (2001) created a thesaurus from a machine-
readable dictionary and combined it with an ex-
isting thesaurus (Ikehara et al, 1997).
For other languages, early work for En-
glish linked senses exploiting dictionary domain
codes and other heuristics (Copestake, 1990),
and more recent work links senses for Spanish
and French using more general WSD techniques
(Rigau et al, 1997). Our goal is similar. We
wish to link each word sense in the fundamen-
tal vocabulary into an ontology. The ontology is
primarily a hierarchy of hyponym (is-a) rela-
tions, but also contains several other relation-
ships, such as abbreviation, synonym and
domain.
We extract the relations from the semantic
output of the parsed definition sentences. The
output is written in Minimal Recursion Seman-
tics (Copestake et al, 2001). Previous work has
successfully used regular expressions, both for
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Headword   doraiba-
POS noun Lexical-type noun-lex
Familiarity 6.5 [1?7]
Sense 1
?
?
?
?
?
?
?
?
?
?
Definition
?
?
?
?
?
S1 	
 /  / 
screw turn (screwdriver)
S1? 	
 /  /  / Integration of a Lexical Type Database with a Linguistically Interpreted
Corpus
Chikara Hashimoto,? Francis Bond,? Takaaki Tanaka,? Melanie Siegel?
? Graduate School of Informatics, Kyoto University
? Machine Translation Research Group, NTT Communication Science Laboratories
? Language Technology Lab, DFKI
? hasimoto@pine.kuee.kyoto-u.ac.jp
?{takaaki,bond}@cslab.kecl.ntt.co.jp
? siegel@dfki.de
Abstract
We have constructed a large scale
and detailed database of lexical types
in Japanese from a treebank that in-
cludes detailed linguistic information.
The database helps treebank annota-
tors and grammar developers to share
precise knowledge about the grammat-
ical status of words that constitute the
treebank, allowing for consistent large
scale treebanking and grammar devel-
opment. In this paper, we report on
the motivation and methodology of the
database construction.
1 Introduction
Treebanks constructed with detailed linguistic in-
formation play an important role in various as-
pects of natural language processing; for exam-
ple, grammatical knowledge acquisition; world
knowledge acquisition (Bond et al, 2004b);
and statistical language model induction. Such
treebanks are typically semi-automatically con-
structed by a linguistically rich computational
grammar.
A detailed grammar in turn is a fundamen-
tal component for precise natural language pro-
cessing. It provides not only detailed syntactic
and morphological information on linguistic ex-
pressions but also precise and usually language-
independent semantic structures of them.
However, such a deep linguistic treebank and
a grammar are often difficult to keep consistent
through development cycles. This is both because
multiple people, often in different locations, par-
ticipate in a development activity, and because
deep linguistic treebanks and grammars are com-
plicated by nature. Thus, it is often the case that
developers lose sight of the current state of the
treebank and grammar, resulting in inconsistency.
We have constructed a linguistically enriched
treebank named ?Hinoki? (Bond et al, 2004a),
which is based on the same framework as the
Redwoods treebank (Oepen et al, 2002) and uses
the Japanese grammar JACY (Siegel and Ben-
der, 2002) to construct the treebank.1 In the con-
struction process, we have also encountered the
problem just mentioned. We are aiming to re-
solve this problem, which we expect many other
project groups that are constructing detailed lin-
guistic treebanks have encountered. Our strategy
is to take a ?snapshot? of one important aspect of
the treebank and grammar for each development
cycle. To be more precise, we extract informa-
tion about lexical items that are being used in tree-
banking from the treebank and grammar and con-
vert it into an electronically accesible structured
database (the lexical-type database). Such a snap-
shot, the database, certainly helps treebank anno-
tators and grammar developers to share precise
and detailed knowledge of the treebank and gram-
mar and thus to make them consistent throughout
the development cycle.2
Lexical items whose information is included
1Currently, the Hinoki treebank contains about 121,000
sentences (about 10 words per sentence).
2We think we also need another snapshot, that of the
grammar rules and principles being used. In this paper, how-
ever, we do not deal with it, and hopefully we will report on
it some other time.
31
in the database are grouped together according
to their grammatical behavior, and we will refer
to each of the groups as a lexical type in the
rest of the paper. A typical lexical item con-
sists of an identifier, and then a triple consist-
ing of the orthography, lexical-type and predicate:
e.g., inu n 1 = ? ???, common-noun-lex,
dog n animal?. The grammar treats all mem-
bers of the same lexical type in the same way. the
lexical type is the locus of syntactic and structural
semantic information. Examples of lexical types
will be described in ?2.
The database could also benefit a wide range
of language researchers, not just those who are
treebanking. As the development of the treebank
and grammar proceeds, together they describe the
language (Japanese in this study) with increasing
accuracy. As a result, the database that we ob-
tain from the sophisticated treebank and grammar
can be thought of as showing us the real view of
the Japanese lexicon. Thus, though many of the
details of the treebank and grammar are frame-
work dependent, the database will provide NLP
researchers who are aiming at deep linguistic pro-
cessing of Japanese with a basic and reliable ref-
erence Japanese lexicon. The correctness can be
verified by examining the treebanked examples.
Such a resource is useful for Japanese language
teachers, lexicographers, and linguists, in addi-
tion to NLP researchers.
The next section describes the framework of
treebanking and motivates the need for the lexical
type database. The third section discusses what
information the lexical type database should con-
tain to facilitate treebanking and grammar devel-
opment; illustrates the contents of the database;
and shows how the database is created. The fourth
section discusses the usefulness of the lexical type
database for many purposes other than treebank-
ing. An overview of related works follows in the
fifth section. Finally, we conclude the paper with
a discussion of our plans for future work.
2 Background to the Database
The treebanking process is illustrated in Fig-
ure 1. As the figure shows, our treebank is
semi-automatically generated by a computational
grammar (and a parser). Each sentence is parsed
and the intended reading chosen from the possi-
Development (refinement)
GRAMMAR
Treebanking (manual annotation)
TREEBANK
automatic parsingfeedback
Figure 1: Treebanking Cycles
ble interpretations. In doing so, we find the gram-
mar?s flaws such as insufficient coverage and spu-
rious ambiguities. The feedback allows us to re-
fine the grammar so that it can have wider cov-
erage and be more appropriately restricted. Cur-
rently this process is carried out by several people,
distributed over four continents.
Although most treebanks are rarely updated,
we consider the updating an integral part of the
process. Thus our treebank is dynamic in the
sense of Oepen et al (2004).
As is often the case with detailed linguistic
treebanking, our grammar and treebank consist
of very fine-grained linguistic information. For
example, our grammar, hence our treebank, dis-
tinguishes several usages of the Japanese dative
marker ni. The Japanese sentence (1) can repre-
sent the two meanings described in (1a) and (1b).
Lexical type names for each usage of ni are writ-
ten in typewriter font.3
(1) hanasiai-wa
discussion-TOP
sinya-ni
midnight-DAT
itaru
reach
a. ?The discussion comes (to a conclusion)
at midnight.?
ni as adv-p-lex-1
b. ?The discussion continues until mid-
night.?
ni as ga-wo-ni-p-lex
The dative phrase, sinya-ni (midnight-DAT), can
act as either an adjunct (1a)4 or an object of itaru
?reach? (1b). Below is an example showing other
usages of ni.
(2) Ken-wa
-TOP
yuka-o
floor-ACC
kirei-ni
clean-DAT
migaku
polish
3These are actual names of the lexical types implemented
in our grammar and might not be understandable to people
in general.
4The object, a conclusion, is expressed by a phonolog-
ically null pronoun. This is the so-called ?pro-drop? phe-
nomenon.
32
a. ?Ken polishes a floor clean.?
(The floor is clean.)
ni as naadj2adv-end-lex
b. ?Ken cleanly polishes a floor.?
(His way of polishing the floor is clean.)
ni as adv-p-lex-6
The dative phrase, kirei-ni (clean-DAT), is used as
an adjunct in both (2a) and (2b), but their usages
and meanings are different. The usage in (2b) is
an ordinary adverb that describes the manner of
Ken?s polishing the floor as clean, while in (2a)
the dative phrase describes the resulting situation
of the floor after polishing as clean. In addition,
the nis in (1) and (2) are different in that the for-
mer takes nouns as its complement while the lat-
ter takes adjectives. Thus, the four usages in (1a),
(1b), (2a) and (2b) must be distinguished so that
we can obtain correct syntactic structures and se-
mantic representations. In our terms, these nis are
said to belong to different lexical types.5 Simi-
larly, our grammar distinguishes usages of other
words, notably functional ones.
However, as we augment the grammar with
finer distinctions, the grammar becomes more and
more opaque and difficult to maintain, and so is
the treebank. This is problematic in three ways.
Firstly, when we annotate parser outputs of one
sentence, we have to see which parse is correct
for the sentence. Consequently, we have to distin-
guish which word usage is correct for each word
in the sentence. However, this task is not always
trivial, since our grammar?s word usage distinc-
tion is very fine grained as shown above. Sec-
ondly, when we add a word to the grammar to
get wider coverage, we have to see which lexical
type the word belongs to. That is, we are required
to be familiar with lexical types of the grammar.
Thirdly, in collaborative grammar development, it
sometimes happens that a developer accidentally
introduces a new lexical type that represents over-
lapping functionality with an existing type. This
causes spurious ambiguity. As a result, the gram-
mar will be unnecessarily bloated, and the tree-
bank will also be easily inconsistent. Again, we
see that comprehensive knowledge of the gram-
mar?s lexical types is indispensable.
5Usages of the Japanese dative marker, ni, are extensively
discussed in, for example, Sadakane and Koizumi (1995).
In summary, it is important to make clear (i)
what lexical types are assumed in a grammar and
a treebank and (ii) how differently they are used
from each other, so that we can make the treebank
annotation and grammar development consistent.
Our solution to the problem is to construct a
lexical type database of a treebank and a gram-
mar. The database is expected to give us explicit
information on (i) what lexical types are imple-
mented in the grammar and are used in the tree-
bank and (ii) how a word is used in Japanese and
is distinguished from other words.
3 Architecture of the Database
This section details the content of the
database and the method of its construc-
tion. The database itself is on-line at
http://pc1.ku-ntt-unet.ocn.ne.
jp/tomcat/lextypeDB/.
3.1 Content of the Database
First of all, what information should be included
in such a database to help treebank annotators and
grammar developers to work consistently? Ob-
viously, once we construct an electronic lexicon,
whatever information it includes, we can easily
see what lexical types are assumed in the gram-
mar and treebank. But we have to carefully con-
sider what to include in the database to make it
clear how each of the lexical types are used and
distinguished.
We include five kinds of information:
(3) Contents of the Database
a. Linguistic discussion
i Name
ii Definition
iii Criteria to judge a word as belong-
ing to a given lexical type
iv Reference to relevant literature
b. Exemplification
i Words that appear in a treebank
ii Sentences in a treebank that contain
the words
c. Implementation
i The portion of grammar source file
that corresponds to the usage
33
ii Comments related to the portion
iii TODOs
d. Links to ?confusing? lexical types
e. Links to other dictionaries
That is, we describe each lexical type in
depth (3a?3c) and present users (treebank an-
notators and grammar developers) explicit links
to other lexical types that share homony-
mous words (3d) (e.g. adv-p-lex-1 vs
ga-wo-ni-case-p-lex in (1)) to make it
clear what distinguishes between them. Further,
we present correspondences to other computa-
tional dictionaries (3e).
Linguistic discussion To understand lexical
types precisely, linguistic observations and anal-
yses are a basic source of information.
Firstly, the requirements for naming lexical-
types in a computational system (3ai) are that
they be short (so that they can be displayed in
large trees) and easily distinguishable. Type
names are not necessarily understandable for any-
one but the developers, so it is useful to link
them to more conventional names. For example
ga-wo-ni-p-lex is a Case Particle (???).
Next, the definition field (3aii) contains a
widely accepted definition statement of the lexi-
cal type. For example, ga-wo-ni-p-lex (1b)
can be defined as ?a particle that indicates that a
noun it attaches to functions as an argument of a
predicate.? Users can grasp the main characteris-
tics from this.
Thirdly, the criteria field (3aiii) provides users
with means of investigating whether a given word
belongs to the class. That is, it provides posi-
tive and negative usage examples. By such us-
age examples, developers can easily find dif-
ferences among lexical types. For example,
adv-p-lex-1 (1a) subcategorizes for nouns,
while adv-p-lex-6 (2b) subcategorizes for
adjectives. Sentences like (1a) and (2b) that fit
such criteria should also be treebanked so that
they can be used to test that the grammar covers
what it claims. This is especially important for
regression testing after new development.
Finally, the reference field (3aiv) points to rep-
resentative papers or books dealing with the lex-
ical type. This allows the grammar developers to
quickly check against existing analyses, and al-
lows users as well to find more information.
Exemplification Examples help users under-
stand lexical types concretely. As we have con-
structed a treebank that is annotated with linguis-
tic information, we can automatically extract rele-
vant examples exhaustively. We give the database
two kinds of examples: words, that are instances
of the lexical types (3bi), and sentences, tree-
banked examples that contain the words (3bii).
This link to the linguistically annotated corpus
examples helps treebankers to check for consis-
tency, and grammar developers to check that the
lexical types are grounded in the corpus data.
Implementation Grammar developers need to
know the actual implementation of lexical types
(3ci). Comments about the implementation (3cii)
are also helpful to ascertain the current status.
Although this section is necessarily framework-
dependent information, all project groups that are
constructing detailed linguistic treebanks need to
document this kind of information. We take our
examples from JACY (Siegel and Bender, 2002),
a large grammar of Japanese built in the HPSG
framework. As actual implementations are gen-
erally incomplete, we use this resource to store
notes about what remains to be done. TODOs
(3ciii) should be explicitly stated to inform gram-
mar developers of what they have to do next.
We currently show the actual TDL defini-
tion, its parent type or types, category of
the head (SYNSEM.LOCAL.CAT.HEAD), valency
(SYNSEM.LOCAL.CAT.VAL), and the semantic
type (SYNSEM.LOCAL.CONT).
Links to ?confusing? lexical types For users to
distinguish phonologically identical but syntacti-
cally or semantically distinct words, it is impor-
tant to link confusing lexical types to one another
within the database. For example, the four lexical
types in (1) and (2) are connected with each other
in terms of ni. That way, users can compare those
words in detail and make a reliable decision when
trying to disambiguate usage examples.6
6Note that this information is not explicitly stored in
the database. Rather, it is dynamically compiled from the
database together with a lexicon database, one of the com-
ponent databases explained below, when triggered by a user
query. User queries are words like ni.
34
Links to other dictionaries This information
helps us to compare our grammar?s treatment
with that of other dictionaries. This compar-
ison would then facilitate understanding of
lexical types and extension of the lexicon. We
currently link lexical types of our grammar
to those of ChaSen (Matsumoto et al, 2000),
Juman (Kurohashi and Nagao, 2003), ALT-
J/E (Ikehara et al, 1991) and EDICT (Breen,
2004). For example, ga-wo-ni-case-p-lex
is linked to ChaSen?s ??-???-??
(particle-case particle-general),
Juman?s ??? (case particle), and
ALT-J/E?s ???-???-???????
(adjunct-case particle-noun/par-
ticle suffix).
Figure 2 shows the document generated from
the lexical type database that describes the lexical
type, ga-wo-ni-p-lex.
3.2 Method of Database Construction
The next question is how to construct such a
database. Needless to say, fully manual construc-
tion of the database is not realistic, since there
are about 300 lexical types and more than 30,000
words in our grammar. In addition, we assume
that we will refer to the database each time we
annotate parser outputs to build the treebank and
that we develop the grammar based on the tree-
banking result. Thus the database construction
process must be quick enough not to delay the
treebanking and grammar development cycles.
To meet the requirement, our method of con-
struction for the lexical type database is semi-
automatic; most of the database content is con-
structed automatically, while the rest must be en-
tered manually. This is depicted in Figure 3.
? Content that is constructed automatically
? Lexical Type ID (Grammar DB)
? Exemplification (3b) (Treebank DB)
? Implementation (3ci,ii) (Grammar DB)
? Link to ?confusing? lexical types (3d)
(Lexicon DB)
? Link to Other Lexicons (3e) (OtherLex
DB)
? Content that is constructed manually
? Linguistic discussion (3a)
? TODOs (3ciii)
3.2.1 Component Databases
To understand the construction process, de-
scription of the four databases that feed the lex-
ical type database is in order. These are the gram-
mar database, the treebank database, the lexicon
database, and the OtherLex database.
? The grammar database contains the actual
implementation of the grammar, written as
typed feature structures using TDL (Krieger
and Schafer, 1994). Although it contains the
whole implementation (lexical types, phrasal
types, types for principles and so on), only
lexical types are relevant to our task.
? The lexicon database gives us mappings be-
tween words in the grammar, their orthogra-
phy, and their lexical types. Thus we can see
what words belong to a given lexical type.
The data could be stored as TDL, but we
use the Postgresql lexdb (Copestake et al,
2004), which simplifies access.
? The treebank database stores all treebank in-
formation, including syntactic derivations,
words, and the lexical type for each word.
The main treebank is stored as structured
text using the [incr tsdb()] (Oepen et al,
2002). We have also exported the deriva-
tion trees for the treebanked sentences into
an SQL database for easy access. The leaves
of the parse data consist of words, and their
lexicon IDs, stored with the ID of the sen-
tence in which the word appears.
? We also use databases from other sources,
such as ChaSen, Juman and Edict.
3.2.2 Automatic Construction
Next we move on to describe the automatic
construction. Firstly, we collect all lexical types
assumed in the grammar and treebank from the
grammar database. Each type constitutes the ID
of a record of the lexical type database.
Secondly, we extract words that are judged to
belong to a given lexical type and sentences that
contains the words (Example (3b)) from the tree-
bank database compiled from the Hinoki tree-
bank (Bond et al, 2004a). The parsed sentences
35
???, ga-wo-ni-p-lex (?,?,?)
Linguistic Discussion
ga-wo-ni-p-lex particles attach to a noun and indicate what grammatical relation (e.g., subject or object)
the noun takes on in relation to a predicate. It does not mean anything by itself.
Right Wrong
????????? ???????????
?????????? 10???????
Literature
[1] Koichi Takezawa. A Configurational Approach to Case Marking in Japanese. Ph.D. dissertation,
University of Washington, 1987.
[ bib ]
[2] Shigeru Miyagawa. Structure and Case Marking in Japanese (Syntax and Semantics 22). Academic
Press, 1989.
[ bib ]
Examples
Lexical Entries (6)
? (ga),? (ni-case),? (o)
Example Sentences (54280)
Examples for? (ga)
?????????? ??????????????????????????????
?????????
??????????????????????????? ?????
???????????????
Examples for? (ni-case)
???????? ?????
????????
???????????
Examples for? (o)
?????
?????????
?????????
More Examples
TDL Summary
TDL Definition
ga-wo-ni-p-lex := case-p-lex & 
                [SYNSEM.LOCAL.CAT.VAL.COMPS.FIRST.LOCAL.CAT.HEAD noun_head].
Supertype Head Category Valency Content
case-p-lex overt-case-p_head p_sat mrs
TODO
Dative subjects of stative predicates are not recognized.
"????????????"
See also mental-stem-lex.
Links
CHASEN?s Lexical type JUMAN?s Lexical type ALT-J/E?s Lexical type
??-???-?? ??? ???-???-???????
Lexical Type List
Figure 2: Screenshot of the lexical type ga-wo-ni-p-lex
36
Manual
Input
Grammar DB
- Lexical Type ID
- Source
OtherLex DB
- Other Lex ID
- Other Lex Type
- Orthography
Lexical Type DB
- Lexical Type ID
- Linguistic Discussion
- Exemplification
- Implementation
- TODOs
- Other Lexicons
Lexicon DB
- Lexicon ID
- Orthography
- Lexical Type ID
Treebank DB
- Lexicon ID
- Orthography
- Sentence ID
User
:
-)
OtherLex
Interface
Query
?Confusing?
Links
Figure 3: The Lexical Type Database Construction
can be seen in various forms: plain text, phrase
structure trees, derivation trees, and minimal re-
cursion semantics representations. We use com-
ponents from the Heart-of-Gold middleware to
present these as HTML (Callmeier et al, 2004).
Thirdly, implementation information except for
TODOs is extracted from the grammar database
(3ci,ii).
Fourthly, in order to establish ?confusing? lex-
ical type links (3d), we collect from the lexicon
database homonyms of a word that users enter as
a query. To be more precise, the lexicon database
presents all the words with the same orthogra-
phy as the query but belonging to different lexical
types. These lexical types are then linked to each
other as ?confusing? in terms of the query word.
Fifthly, we construct links between our lexical
types and POS?s of other lexicons such as ChaSen
from OtherLex DB (3e). To do this, we prepare
an interface (a mapping table) between our lexi-
cal type system and the other lexicon?s POS sys-
tem. As this is a finite mapping it could be made
manually, but we semi-automate its construction.
The similarity between types in the two databases
(JACY and some other lexicon ) is calculated as
the Dice coefficient, where W (LA) is the number
of words W in lexical type L:
sim(LA, LB) =
2? |(W (LA ? LB)|
|W (LA)|+ |W (LB)|
(1)
The Dice coefficient was chosen because of its
generality and ease of calculation. Any pair
where sim(LA, LB) is above a threshold should
potentially be mapped. The threshold must be set
low, as the granularity of different systems can
vary widely.
3.2.3 Manual Construction
Linguistic discussion (3a) and implementation
TODOs (3ciii) have to be entered manually. Lin-
guistic discussion is especially difficult to collect
exhaustively since the task requires an extensive
background in linguistics. We have several lin-
guists in our group, and our achievements in this
task owe much to them. We plan to make the in-
terface open, and encourage the participation of
anyone interested in the task.
The on-line documentation is designed to com-
plement the full grammar documentation (Siegel,
2004). The grammar documentation gives a top
down view of the grammar, giving the overall mo-
tivation for the analyses. The lexical-type docu-
mentation gives bottom up documentation. It can
easily be updated along with the grammar.
Writing implementation TODOs also requires
expertise in grammar development and linguis-
tic background. But grammar developers usually
take notes on what remains to be done for each
lexical type anyway, so this is a relatively simple
task.
After the database is first constructed, how is
it put to use and updated in the treebanking cy-
cles described in Figure 1? Figure 4 illustrates
this. Each time the grammar is revised based on
treebank annotation feedback, grammar develop-
ers consult the database to see the current status
of the grammar. After finishing the revision, the
grammar and lexicon DBs are updated, as are the
corresponding fields of the lexical type database.
Each time the treebank is annotated, annotators
can consult the database to make sure the chosen
parse is correct. Following annotation, the tree-
bank DB is updated, and so is the lexical type
database. In parallel to this, collaborators who are
37
Development (refinement)
GRAMMAR
Treebanking (manual annotation)
TREEBANK
automatic
parsingfeedback
LEXICAL TYPE
DATABASE
WWW
Reference
Updating Grammar and Lexicon DBs
Reference
Updating Treebank DB
Linguistic Discussion
Figure 4: Database Construction Intergrated with Treebanking Cycles
ChaSenJuman ALT-J/E
The Lexical Type
Database
EDICT LexicalResource2Lexical
Resource1
ChaSen
InterfaceJumanInterface
ALT-J/E
Interface
EDICT
Interface Interface2Interface1
Figure 5: Synthesis of Lexical Resources
familiar with linguistics continue to enter relevant
linguistic discussions via the WWW.
4 Lexical Type Database as a General
Linguistic Resource
In this section, we discuss some of the ways the
database can benefit people other than treebank
annotators and grammar developers.
One way is by serving as a link to other lexi-
cal resources. As mentioned in the previous sec-
tion, our database includes links to ChaSen, Ju-
man, ALT-J/E, and EDICT. Currently, in Japanese
NLP (and more generally), various lexical re-
sources have been developed, but their intercor-
respondences are not always clear. These lexical
resources often play complementary roles, so syn-
thesizing them seamlessly will make a Japanese
lexicon with the widest and deepest knowledge
ever. Among our plans is to realize this by
means of the lexical type database. Consider Fig-
ure 5. Assuming that most lexical resources con-
tain lexical type information, no matter how fine
or coarse grained it is, it is natural to think that
the lexical type database can act as a ?hub? that
links those lexical resources together. This will
be achieved by preparing interfaces between the
lexical type database and each of the lexical re-
sources. Clearly, this is an intelligent way to syn-
thesize lexical resources. Otherwise, we have to
prepare nC2 interfaces to synthesize n resources.
The problem is that construction of such an inter-
face is time consuming. We need to further test
generic ways to do this, such as with similarity
scores, though we will not go on further with this
issue in this paper.
Apart from NLP, how can the database be used?
In the short term our database is intended to pro-
vide annotators and grammar developers with a
clear picture of the current status of the treebank
and the grammar. In the long term, we expect to
create successively better approximations of the
Japanese language, as long as our deep linguistic
broad coverage grammar describes Japanese syn-
tax and semantics precisely. Consequently, the
database would be of use to anyone who needs an
accurate description of Japanese. Japanese lan-
guage teachers can use its detailed descriptions
of word usages, the links to other words, and the
real examples from the treebank to show for stu-
dents subtle differences among words that look
the same but are grammatically different. Lexi-
cographers can take advantage of its comprehen-
siveness and the real examples to compile a dic-
tionary that contains full linguistic explanations.
The confidence in the linguistic descriptions is
based on the combination of the precise grammar
linked to the detailed treebank. Each improves the
other through the treebank annotation and gram-
mar development cycle as depicted in Figure 1.
5 Related Work
Tsuchiya et al (2005) have been constructing a
database that summarizes multiword functional
38
expressions in Japanese. That describes each
expression?s linguistic behavior, usage and ex-
amples in depth. Notable differences between
their database and ours are that their database is
mostly constructed manually while ours is con-
structed semi-automatically, and that they target
only functional expressions while we deal with all
kinds of lexical types.
Hypertextual Grammar development (Dini and
Mazzini, 1997) attempted a similar task, but fo-
cused on documenting the grammar, not on link-
ing it to a dynamic treebank. They suggested cre-
ating the documentation in the same file along
with the grammar, in the style of literate program-
ming. This is an attractive approach, especially
for grammars that change constantly. However,
we prefer the flexibility of combining different
knowledge sources (the grammar, treebank and
linguistic description, in addition to external re-
sources).
The Montage project (Bender et al, 2004) aims
to develop a suite of software whose primary au-
dience is field linguists working on underdocu-
mented languages. Among their tasks is to fa-
cilitate traditional grammatical description from
annotated texts by means of one of their products,
the Grammar export tool. Although in the paper
there is little explicit detail about what the ?tradi-
tional grammatical description? is, they seem to
share a similar goal with us: in the case of Mon-
tage, making grammatical knowledge assumed in
underdocumented languages explicit, while in our
case making lexical types assumed in the treebank
and the computational grammar understandable
to humans. Also, some tools they use are used
in our project as well. Consequently, their pro-
cess of grammatical description and documenta-
tion looks quite similar to ours. The difference
is that their target is underdocumented languages
whose grammatical knowledge has so far not been
made clear enough, while we target a familiar
language, Japanese, that is well understood but
whose computational implementation is so large
and complex as to be difficult to fully compre-
hend.
Another notable related work is the COMLEX
syntax project (Macleod et al, 1994). Their goal
is to create a moderately-broad-coverage lexicon
recording the syntactic features of English words
for purposes of computational language analysis.
They employed elves (?elf? = enterer of lexical
features) to create such a lexicon by hand. Natu-
rally, the manual input task is error-prone. Thus
they needed to prepare a document that describes
word usages by which they intended to reduce
elves? errors. It is evident that the document
plays a role similar to our lexical type database,
but there are important divergences between the
two. First, while their document seems to be con-
structed manually (words chosen as examples of
lexical types in the documentation are not always
in the lexicon!), the construction process of our
database is semi-automated. Second, somewhat
relatedly, our database is electronically accessible
and well-structured. Thus it allows more flexi-
ble queries than a simple document. Third, unlike
COMLEX, all the lexical types in the database
are actually derived from the working Japanese
grammar with which we are building the tree-
bank. That is, all the lexical types are defined
formally. Fourth, examples in our database are all
real ones in that they actually appear in the tree-
bank, while most of the COMLEX examples were
created specifically for the project. Finally, we are
dealing with all kinds of lexical types that appear
in the treebank, but the COMLEX project targets
only nouns, adjectives, and verbs.
6 Future Work
We are currently experimenting with moving
some of the information (in particular the type
name and criteria) into the actual grammar files,
in the same way as Dini and Mazzini (1997). This
would make it easier to keep the information in
sync with the actual grammar.
We have discussed the motivation, contents and
construction of the lexical type database. We plan
to evaluate the database (i) by measuring tree-
bank inter-annotator agreement and (ii) by evalu-
ating the coverage, the amount of spurious ambi-
guity, and efficiency of the grammar before and
after introducing the database in the treebank-
ing and grammar development cycles. We ex-
pect that treebank annotators will be more con-
sistent when they can refer to the database and
that grammar developers can more easily find the
grammar?s flaws (like lack of lexical items and
overlapping implementations of the same lexical
39
type) by looking into the database.
Although this paper deals with a lexical type
database of Japanese, the importance of such a
database certainly holds for any large scale deep
grammar. We use the tools from the DELPH-
IN collaboration7 and plan to make our addi-
tions available for groups working with other lan-
guages. In particular, we plan to construct a lex-
ical type database for the Redwoods treebank,
which is semi-automatically constructed from the
English Resource Grammar (ERG) (Flickinger,
2000).
Acknowledgements
We would like to thank the other members
of Machine Translation Research Group, Dan
Flickinger, Stephen Oepen, and Jason Katz-
Brown for their stimulating discussion.
References
Emily M. Bender, Dan Flickinger, Jeff Good, and Ivan A.
Sag. 2004. Montage: Leveraging Advances in Grammar
Engineering, Linguistic Ontologies, and Mark-up for the
Documentation of Underdescribed Languages. In Pro-
ceedings of the Workshop on First Steps for the Documen-
tation of Minority Languages: Computational Linguistic
Tools for Morphology, Lexicon and Corpus Compilation,
LREC2004, Lisbon, Portugal.
Francis Bond, Sanae Fujita, Chikara Hashimoto, Shigeko
Nariyama, Eric Nichols, Akira Ohtani, Takaaki Tanaka,
and Shigeaki Amano. 2004a. The Hinoki Treebank
? Toward Text Understanding. In Proceedings of the
5th International Workshop on Linguistically Interpreted
Corpora (LINC-04), pages 7?10, Geneva.
Francis Bond, Eric Nichols, and Sanae Fujita Takaaki
Tanaka. 2004b. Acquiring an Ontology for a Funda-
mental Vocabulary. In 20th International Conference
on Computational Linguistics (COLING-2004), pages
1319?1325, Geneva.
J. W. Breen. 2004. JMDict: a Japanese-mulitlingual dictio-
nary. In Coling 2004 Workshop on Multilingual Linguis-
tic Resources, pages 71?78, Geneva.
Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, and
Melanie Siegel. 2004. The DeepThought core archi-
tecture framework. In Proceedings of LREC-2004, vol-
ume IV, Lisbon.
Ann Copestake, Fabre Lambeau, Benjamin Waldron, Fran-
cis Bond, Dan Flickinger, and Stephan Oepen. 2004. A
lexicon module for a grammar development environment.
In 4th International Conference on Language Resources
and Evaluation (LREC 2004), volume IV, pages 1111?
1114, Lisbon.
7http://www.delph-in.net/
Luca Dini and Giampolo Mazzini. 1997. Hypertextual
Grammar Development. In Computational Environments
for Grammar Development and Linguistic Engineering,
pages 24?29, Madrid. ACL.
Dan Flickinger. 2000. On building a more effi cient gram-
mar by exploiting types. Natural Language Engineering,
6 (1) (Special Issue on Efficient Proceeding with HPSG,
pages 15?28.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing ? effects of new methods in ALT-J/E?. In Third
Machine Translation Summit: MT Summit III, pages 101?
106, Washington DC. (http://xxx.lanl.gov/
abs/cmp-lg/9510008).
Hans-Ulrich Krieger and Ulrich Schafer. 1994. T DL ? a
type description language for constraint-based grammars.
In Proceedings of the 15th International Conference on
Computational Linguistics.
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. chapter 14, pages 249?260.
Catherine Macleod, Ralph Grishman, and Adam Meyers.
1994. The Comlex Syntax Project: The First Year. In
Proceedings of the 1994 ARPA Human Language Tech-
nology Workshop.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, Kazuma Takaoka, and
Masayuki Asahara, 2000. Morphological Analysis Sys-
tem ChaSen version 2.2.1 Manual. Nara Institute of Sci-
ence and Technology, Dec.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2002. LinGO Redwoods: A
Rich and Dynamic Treebank for HPSG. In Proceedings
of The First Workshop on Treebanks and Linguistic The-
ories, pages 139?149, Sozopol, Bulgaria.
Stephan Oepen, Dan Flickinger, and Francis Bond. 2004.
Towards Holistic Grammar Engineering and Testing.
Grafting Treebank Maintenance into the Grammar Re-
vision Cycle. In Proceedings of the IJCNLP Workshop
Beyond Shallow Analysis, Hainan,China.
Kumi Sadakane and Masatoshi Koizumi. 1995. On the na-
ture of the ?dative? particle ni in Japanese. Linguistics,
33:5?33.
Melanie Siegel and Emily M. Bender. 2002. Effi cient Deep
Processing of Japanese. In Proceedings of the 3rd Work-
shop on Asian Language Resources and International
Standardization, Taipei, Taiwan.
Melanie Siegel. 2004. JACY a practical Japanese HPSG.
ms.
Masatoshi Tsuchiya, Takehito Utsuro, Suguru Matsuyoshi,
Satoshi Sato, and Seiichi Nakagawa. 2005. A corpus
for classifying usages of japanese compound functional
expressions. In Proceedings of Pacific Association for
Computational Linguistics 2005, Tokyo, Japan.
40
Learning the Countability of English Nouns from Corpus Data
Timothy Baldwin
CSLI
Stanford University
Stanford, CA, 94305
tbaldwin@csli.stanford.edu
Francis Bond
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
Kyoto, Japan
bond@cslab.kecl.ntt.co.jp
Abstract
This paper describes a method for learn-
ing the countability preferences of English
nouns from raw text corpora. The method
maps the corpus-attested lexico-syntactic
properties of each noun onto a feature
vector, and uses a suite of memory-based
classifiers to predict membership in 4
countability classes. We were able to as-
sign countability to English nouns with a
precision of 94.6%.
1 Introduction
This paper is concerned with the task of knowledge-
rich lexical acquisition from unannotated corpora,
focusing on the case of countability in English.
Knowledge-rich lexical acquisition takes unstruc-
tured text and extracts out linguistically-precise cat-
egorisations of word and expression types. By
combining this with a grammar, we can build
broad-coverage deep-processing tools with a min-
imum of human effort. This research is close
in spirit to the work of Light (1996) on classi-
fying the semantics of derivational affixes, and
Siegel and McKeown (2000) on learning verb as-
pect.
In English, nouns heading noun phrases are typ-
ically either countable or uncountable (also called
count and mass). Countable nouns can be modi-
fied by denumerators, prototypically numbers, and
have a morphologically marked plural form: one
dog, two dogs. Uncountable nouns cannot be modi-
fied by denumerators, but can be modified by unspe-
cific quantifiers such as much, and do not show any
number distinction (prototypically being singular):
*one equipment, some equipment, *two equipments.
Many nouns can be used in countable or uncountable
environments, with differences in interpretation.
We call the lexical property that determines which
uses a noun can have the noun?s countability prefer-
ence. Knowledge of countability preferences is im-
portant both for the analysis and generation of En-
glish. In analysis, it helps to constrain the inter-
pretations of parses. In generation, the countabil-
ity preference determines whether a noun can be-
come plural, and the range of possible determin-
ers. Knowledge of countability is particularly im-
portant in machine translation, because the closest
translation equivalent may have different countabil-
ity from the source noun. Many languages, such
as Chinese and Japanese, do not mark countability,
which means that the choice of countability will be
largely the responsibility of the generation compo-
nent (Bond, 2001). In addition, knowledge of count-
ability obtained from examples of use is an impor-
tant resource for dictionary construction.
In this paper, we learn the countability prefer-
ences of English nouns from unannotated corpora.
We first annotate them automatically, and then train
classifiers using a set of gold standard data, taken
from COMLEX (Grishman et al, 1998) and the trans-
fer dictionaries used by the machine translation sys-
tem ALT-J/E (Ikehara et al, 1991). The classifiers
and their training are described in more detail in
Baldwin and Bond (2003). These are then run over
the corpus to extract nouns as members of four
classes ? countable: dog; uncountable: furniture; bi-
partite: [pair of] scissors and plural only: clothes.
We first discuss countability in more detail (? 2).
Then we present the lexical resources used in our ex-
periment (? 3). Next, we describe the learning pro-
cess (? 4). We then present our results and evalu-
ation (? 5). Finally, we discuss the theoretical and
practical implications (? 6).
2 Background
Grammatical countability is motivated by the se-
mantic distinction between object and substance
reference (also known as bounded/non-bounded or
individuated/non-individuated). It is a subject of
contention among linguists as to how far grammat-
ical countability is semantically motivated and how
much it is arbitrary (Wierzbicka, 1988).
The prevailing position in the natural language
processing community is effectively to treat count-
ability as though it were arbitrary and encode it as
a lexical property of nouns. The study of countabil-
ity is complicated by the fact that most nouns can
have their countability changed: either converted by
a lexical rule or embedded in another noun phrase.
An example of conversion is the so-called universal
packager, a rule which takes an uncountable noun
with an interpretation as a substance, and returns a
countable noun interpreted as a portion of the sub-
stance: I would like two beers. An example of em-
bedding is the use of a classifier, e.g. uncountable
nouns can be embedded in countable noun phrases
as complements of classifiers: one piece of equip-
ment.
Bond et al (1994) suggested a division of count-
ability into five major types, based on Allan (1980)?s
noun countability preferences (NCPs). Nouns which
rarely undergo conversion are marked as either fully
countable, uncountable or plural only. Fully countable
nouns have both singular and plural forms, and can-
not be used with determiners such as much, little, a
little, less and overmuch. Uncountable nouns, such
as furniture, have no plural form, and can be used
with much. Plural only nouns never head a singular
noun phrase: goods, scissors.
Nouns that are readily converted are marked as ei-
ther strongly countable (for countable nouns that can
be converted to uncountable, such as cake) or weakly
countable (for uncountable nouns that are readily
convertible to countable, such as beer).
NLP systems must list countability for at least
some nouns, because full knowledge of the refer-
ent of a noun phrase is not enough to predict count-
ability. There is also a language-specific knowl-
edge requirement. This can be shown most sim-
ply by comparing languages: different languages en-
code the countability of the same referent in dif-
ferent ways. There is nothing about the concept
denoted by lightning, e.g., that rules out *a light-
ning being interpreted as a flash of lightning. In-
deed, the German and French translation equivalents
are fully countable (ein Blitz and un e?clair respec-
tively). Even within the same language, the same
referent can be encoded countably or uncountably:
clothes/clothing, things/stuff , jobs/work.
Therefore, we must learn countability classes
from usage examples in corpora. There are several
impediments to this approach. The first is that words
are frequently converted to different countabilities,
sometimes in such a way that other native speak-
ers will dispute the validity of the new usage. We
do not necessarily wish to learn such rare examples,
and may not need to learn more common conver-
sions either, as they can be handled by regular lexi-
cal rules (Copestake and Briscoe, 1995). The second
problem is that some constructions affect the appar-
ent countability of their head: for example, nouns
denoting a role, which are typically countable, can
appear without an article in some constructions (e.g.
We elected him treasurer). The third is that different
senses of a word may have different countabilities:
interest ?a sense of concern with and curiosity? is
normally countable, whereas interest ?fixed charge
for borrowing money? is uncountable.
There have been at several earlier approaches
to the automatic determination of countabil-
ity. Bond and Vatikiotis-Bateson (2002) determine
a noun?s countability preferences from its seman-
tic class, and show that semantics predicts (5-way)
countability 78% of the time with their ontology.
O?Hara et al (2003) get better results (89.5%) using
the much larger Cyc ontology, although they only
distinguish between countable and uncountable.
Schwartz (2002) created an automatic countabil-
ity tagger (ACT) to learn noun countabilities from
the British National Corpus. ACT looks at deter-
miner co-occurrence in singular noun chunks, and
classifies the noun if and only if it occurs with a de-
terminer which can modify only countable or un-
countable nouns. The method has a coverage of
around 50%, and agrees with COMLEX for 68% of
the nouns marked countable and with the ALT-J/E
lexicon for 88%. Agreement was worse for uncount-
able nouns (6% and 44% respectively).
3 Resources
Information about noun countability was obtained
from two sources. One was COMLEX 3.0 (Grish-
man et al, 1998), which has around 22,000 noun
entries. Of these, 12,922 are marked as being count-
able (COUNTABLE) and 4,976 as being uncountable
(NCOLLECTIVE or :PLURAL *NONE*). The remainder
are unmarked for countability.
The other was the common noun part of ALT-
J/E?s Japanese-to-English semantic transfer dictio-
nary (Bond, 2001). It contains 71,833 linked
Japanese-English pairs, each of which has a value
for the noun countability preference of the English
noun. Considering only unique English entries with
different countability and ignoring all other informa-
tion gave 56,245 entries. Nouns in the ALT-J/E dic-
tionary are marked with one of the five major count-
ability preference classes described in Section 2. In
addition to countability, default values for number
and classifier (e.g. blade for grass: blade of grass)
are also part of the lexicon.
We classify words into four possible classes, with
some words belonging to multiple classes. The first
class is countable: COMLEX?s COUNTABLE and ALT-
J/E?s fully, strongly and weakly countable. The sec-
ond class is uncountable: COMLEX?s NCOLLECTIVE or
:PLURAL *NONE* and ALT-J/E?s strongly and weakly
countable and uncountable.
The third class is bipartite nouns. These can only
be plural when they head a noun phrase (trousers),
but singular when used as a modifier (trouser leg).
When they are denumerated they use pair: a pair of
scissors. COMLEX does not have a feature to mark
bipartite nouns; trouser, for example, is listed as
countable. Nouns in ALT-J/E marked plural only with
a default classifier of pair are classified as bipartite.
The last class is plural only nouns: those that only
have a plural form, such as goods. They can nei-
ther be denumerated nor modified by much. Many
of these nouns, such as clothes, use the plural form
even as modifiers (a clothes horse). The word
clothes cannot be denumerated at all. Nouns marked
:SINGULAR *NONE* in COMLEX and nouns in ALT-
J/E marked plural only without the default classifier
pair are classified as plural only. There was some
noise in the ALT-J/E data, so this class was hand-
checked, giving a total of 104 entries; 84 of these
were attested in the training data.
Our classification of countability is a subset of
ALT-J/E?s, in that we use only the three basic ALT-
J/E classes of countable, uncountable and plural only,
(although we treat bipartite as a separate class, not a
subclass). As we derive our countability classifica-
tions from corpus evidence, it is possible to recon-
struct countability preferences (i.e. fully, strongly, or
weakly countable) from the relative token occurrence
of the different countabilities for that noun.
In order to get an idea of the intrinsic difficulty of
the countability learning task, we tested the agree-
ment between the two resources in the form of clas-
sification accuracy. That is, we calculate the average
proportion of (both positive and negative) countabil-
ity classifications over which the two methods agree.
E.g., COMLEX lists tomato as being only countable
where ALT-J/E lists it as being both countable and un-
countable. Agreement for this one noun, therefore, is
3
4 , as there is agreement for the classes of countable,
plural only and bipartite (with implicit agreement as
to negative membership for the latter two classes),
but not for uncountable. Averaging over the total set
of nouns countability-classified in both lexicons, the
mean was 93.8%. Almost half of the disagreements
came from words with two countabilities in ALT-J/E
but only one in COMLEX.
4 Learning Countability
The basic methodology employed in this research is
to identify lexical and/or constructional features as-
sociated with the countability classes, and determine
the relative corpus occurrence of those features for
each noun. We then feed the noun feature vectors
into a classifier and make a judgement on the mem-
bership of the given noun in each countability class.
In order to extract the feature values from corpus
data, we need the basic phrase structure, and partic-
ularly noun phrase structure, of the source text. We
use three different sources for this phrase structure:
part-of-speech tagged data, chunked data and fully-
parsed data, as detailed below.
The corpus of choice throughout this paper is the
written component of the British National Corpus
(BNC version 2, Burnard (2000)), totalling around
90m w-units (POS-tagged items). We chose this be-
cause of its good coverage of different usages of En-
glish, and thus of different countabilities. The only
component of the original annotation we make use
of is the sentence tokenisation.
Below, we outline the features used in this re-
search and methods of describing feature interac-
tion, along with the pre-processing tools and ex-
traction techniques, and the classifier architecture.
The full range of different classifier architectures
tested as part of this research, and the experi-
ments to choose between them are described in
Baldwin and Bond (2003).
4.1 Feature space
For each target noun, we compute a fixed-length
feature vector based on a variety of features intended
to capture linguistic constraints and/or preferences
associated with particular countability classes. The
feature space is partitioned up into feature clusters,
each of which is conditioned on the occurrence of
the target noun in a given construction.
Feature clusters take the form of one- or two-
dimensional feature matrices, with each dimension
describing a lexical or syntactic property of the
construction in question. In the case of a one-
dimensional feature cluster (e.g. noun occurring in
singular or plural form), each component feature
feats in the cluster is translated into the 3-tuple:
Feature cluster
(base feature no.) Countable Uncountable Bipartite Plural only
Head number (2) S,P S P P
Modifier number (2) S,P S S P
Subj?V agreement (2 ? 2) [S,S],[P,P] [S,S] [P,P] [P,P]
Coordinate number (2? 2) [S,S],[P,S],[P,P] [S,S],[S,P] [P,S],[P,P] [P,S],[P,P]
N of N (11 ? 2) [100s,P], . . . [lack,S], . . . [pair,P], . . . [rate,P], . . .
PPs (52 ? 2) [per,-DET], . . . [in,-DET], . . . ? ?
Pronoun (12 ? 2) [it,S],[they,P], . . . [it,S], . . . [they,P], . . . [they,P], . . .
Singular determiners (10) a,each, . . . much, . . . ? ?
Plural determiners (12) many, few, . . . ? ? many, . . .
Neutral determiners (11? 2) [less,P], . . . [BARE,S], . . . [enough,P], . . . [all,P], . . .
Table 1: Predicted feature-correlations for each feature cluster (S=singular, P=plural)
?freq(feats|word),
freq(feats|word)
freq(word) ,
freq(feats|word)
?
ifreq(feati|word)
?
In the case of a two-dimensional feature cluster
(e.g. subject-position noun number vs. verb number
agreement), each component feature feat s,t is trans-
lated into the 5-tuple:
?freq(feats,t|word),
freq(feats,t|word)
freq(word) ,
freq(feats,t|word)
?
i,j freq(feati,j |word)
,
freq(feats,t|word)
?
ifreq(feati,t|word)
,
freq(feats,t|word)
?
j freq(feats,j |word)
?
See Baldwin and Bond (2003) for further details.
The following is a brief description of each fea-
ture cluster and its dimensionality (1D or 2D). A
summary of the number of base features and predic-
tion of positive feature correlations with countability
classes is presented in Table 1.
Head noun number:1D the number of the target
noun when it heads an NP (e.g. a shaggy dog
= SINGULAR)
Modifier noun number:1D the number of the tar-
get noun when a modifier in an NP (e.g. dog
food = SINGULAR)
Subject?verb agreement:2D the number of the tar-
get noun in subject position vs. number agree-
ment on the governing verb (e.g. the dog barks
= ?SINGULAR,SINGULAR?)
Coordinate noun number:2D the number of the
target noun vs. the number of the head
nouns of conjuncts (e.g. dogs and mud =
?PLURAL,SINGULAR?)
N of N constructions:2D the number of the target
noun (N?) vs. the type of the N? in an N?
of N? construction (e.g. the type of dog =
?TYPE,SINGULAR?). We have identified a total
of 11 N? types for use in this feature cluster
(e.g. COLLECTIVE, LACK, TEMPORAL).
Occurrence in PPs:2D the presence or absence of
a determiner (?DET) when the target noun oc-
curs in singular form in a PP (e.g. per dog
= ?per,?DET?). This feature cluster exploits
the fact that countable nouns occur determin-
erless in singular form with only very partic-
ular prepositions (e.g. by bus, *on bus, *with
bus) whereas with uncountable nouns, there are
fewer restrictions on what prepositions a target
noun can occur with (e.g. on furniture, with fur-
niture, ?by furniture).
Pronoun co-occurrence:2D what personal and
possessive pronouns occur in the same sen-
tence as singular and plural instances of the
target noun (e.g. The dog ate its dinner =
?its,SINGULAR?). This is a proxy for pronoun
binding effects, and is determined over a total
of 12 third-person pronoun forms (normalised
for case, e.g. he, their, itself ).
Singular determiners:1D what singular-selecting
determiners occur in NPs headed by the tar-
get noun in singular form (e.g. a dog = a).
All singular-selecting determiners considered
are compatible with only countable (e.g. an-
other, each) or uncountable nouns (e.g. much,
little). Determiners compatible with either are
excluded from the feature cluster (cf. this dog,
this information). Note that the term ?deter-
miner? is used loosely here and below to denote
an amalgam of simplex determiners (e.g. a), the
null determiner, complex determiners (e.g. all
the), numeric expressions (e.g. one), and adjec-
tives (e.g. numerous), as relevant to the partic-
ular feature cluster.
Plural determiners:1D what plural-selecting deter-
miners occur in NPs headed by the target noun
in plural form (e.g. few dogs = few). As
with singular determiners, we focus on those
plural-selecting determiners which are compat-
ible with a proper subset of count, plural only
and bipartite nouns.
Non-bounded determiners:2D what non-bounded
determiners occur in NPs headed by the target
noun, and what is the number of the target noun
for each (e.g. more dogs = ?more,PLURAL?).
Here again, we restrict our focus to non-
bounded determiners that select for singular-
form uncountable nouns (e.g. sufficient furni-
ture) and plural-form countable, plural only
and bipartite nouns (e.g. sufficient dogs).
The above feature clusters produce a combined
total of 1,284 individual feature values.
4.2 Feature extraction
In order to extract the features described above,
we need some mechanism for detecting NP and
PP boundaries, determining subject?verb agreement
and deconstructing NPs in order to recover con-
juncts and noun-modifier data. We adopt three ap-
proaches. First, we use part-of-speech (POS) tagged
data and POS-based templates to extract out the nec-
essary information. Second, we use chunk data
to determine NP and PP boundaries, and medium-
recall chunk adjacency templates to recover inter-
phrasal dependency. Third, we fully parse the data
and simply read off all necessary data from the de-
pendency output.
With the POS extraction method, we first Penn-
tagged the BNC using an fnTBL-based tagger (Ngai
and Florian, 2001), training over the Brown and
WSJ corpora with some spelling, number and hy-
phenation normalisation. We then lemmatised this
data using a version of morph (Minnen et al, 2001)
customised to the Penn POS tagset. Finally, we
implemented a range of high-precision, low-recall
POS-based templates to extract out the features from
the processed data. For example, NPs are in many
cases recoverable with the following Perl-style reg-
ular expression over Penn POS tags: (PDT)* DT
(RB|JJ[RS]?|NNS?)* NNS? [?N].
For the chunker, we ran fnTBL over the lem-
matised tagged data, training over CoNLL 2000-
style (Tjong Kim Sang and Buchholz, 2000) chunk-
converted versions of the full Brown and WSJ cor-
pora. For the NP-internal features (e.g. determin-
ers, head number), we used the noun chunks directly,
or applied POS-based templates locally within noun
chunks. For inter-chunk features (e.g. subject?verb
agreement), we looked at only adjacent chunk pairs
so as to maintain a high level of precision.
As the full parser, we used RASP (Briscoe and
Carroll, 2002), a robust tag sequence grammar-
based parser. RASP?s grammatical relation output
function provides the phrase structure in the form
of lemmatised dependency tuples, from which it is
possible to read off the feature information. RASP
has the advantage that recall is high, although pre-
cision is potentially lower than chunking or tagging
as the parser is forced into resolving phrase attach-
ment ambiguities and committing to a single phrase
structure analysis.
Although all three systems map onto an identi-
cal feature space, the feature vectors generated for a
given target noun diverge in content due to the dif-
ferent feature extraction methodologies. In addition,
we only consider nouns that occur at least 10 times
as head of an NP, causing slight disparities in the
target noun type space for the three systems. There
were sufficient instances found by all three systems
for 20,530 common nouns (out of 33,050 for which
at least one system found sufficient instances).
4.3 Classifier architecture
The classifier design employed in this research is
four parallel supervised classifiers, one for each
countability class. This allows us to classify a sin-
gle noun into multiple countability classes, e.g. de-
mand is both countable and uncountable. Thus,
rather than classifying a given target noun accord-
ing to the unique most plausible countability class,
we attempt to capture its full range of countabilities.
Note that the proposed classifier design is that which
was found by Baldwin and Bond (2003) to be opti-
mal for the task, out of a wide range of classifier
architectures.
In order to discourage the classifiers from over-
training on negative evidence, we constructed the
gold-standard training data from unambiguously
negative exemplars and potentially ambiguous pos-
itive exemplars. That is, we would like classifiers
to judge a target noun as not belonging to a given
countability class only in the absence of positive ev-
idence for that class. This was achieved in the case
of countable nouns, for instance, by extracting all
countable nouns from each of the ALT-J/E and COM-
LEX lexicons. As positive training exemplars, we
then took the intersection of those nouns listed as
countable in both lexicons (irrespective of member-
ship in alternate countability classes); negative train-
ing exemplars, on the other hand, were those con-
tained in both lexicons but not classified as count-
Class Positive data Negative data Baseline
Countable 4,342 1,476 .746
Uncountable 1,519 5,471 .783
Bipartite 35 5,639 .994
Plural only 84 5,639 .985
Table 2: Details of the gold-standard data
able in either.1 The uncountable gold-standard data
was constructed in a similar fashion. We used the
ALT-J/E lexicon as our source of plural only and bi-
partite nouns, using all the instances listed as our
positive exemplars. The set of negative exemplars
was constructed in each case by taking the intersec-
tion of nouns not contained in the given countability
class in ALT-J/E, with all annotated nouns with non-
identical singular and plural forms in COMLEX.
Having extracted the positive and negative exem-
plar noun lists for each countability class, we filtered
out all noun lemmata not occurring in the BNC.
The final make-up of the gold-standard data for
each of the countability classes is listed in Table 2,
along with a baseline classification accuracy for
each class (?Baseline?), based on the relative fre-
quency of the majority class (positive or negative).
That is, for bipartite nouns, we achieve a 99.4% clas-
sification accuracy by arbitrarily classifying every
training instance as negative.
The supervised classifiers were built using
TiMBL version 4.2 (Daelemans et al, 2002), a
memory-based classification system based on the k-
nearest neighbour algorithm. As a result of exten-
sive parameter optimisation, we settled on the de-
fault configuration for TiMBL with k set to 9. 2
5 Results and Evaluation
Evaluation is broken down into two components.
First, we determine the optimal classifier configura-
tion for each countability class by way of stratified
cross-validation over the gold-standard data. We
then run each classifier in optimised configuration
over the remaining target nouns for which we have
feature vectors.
5.1 Cross-validated results
First, we ran the classifiers over the full feature set
for the three feature extraction methods. In each
case, we quantify the classifier performance by way
1Any nouns not annotated for countability in COMLEX were
ignored in this process so as to assure genuinely negative
exemplars.
2We additionally experimented with the kernel-based
TinySVM system, but found TiMBL to be superior in all cases.
Class System Accuracy (e.r.) F-score
Tagger? .928 (.715) .953
Chunker .933 (.734) .956Countable
RASP? .923 (.698) .950
Combined .939 (.759) .960
Tagger .945 (.746) .876
Chunker? .945 (.747) .876Uncountable
RASP? .944 (.743) .872
Combined .952 (.779) .892
Tagger .997 (.489) .752
Chunker .997 (.460) .704Bipartite
RASP .997 (.488) .700
Combined .996 (.403) .722
Tagger .989 (.275) .558
Chunker .990 (.299) .568Plural only
RASP? .989 (.227) .415
Combined .990 (.323) .582
Table 3: Cross-validation results
of 10-fold stratified cross-validation over the gold-
standard data for each countability class. The fi-
nal classification accuracy and F-score3 are averaged
over the 10 iterations.
The cross-validated results for each classifier are
presented in Table 3, broken down into the differ-
ent feature extraction methods. For each, in addi-
tion to the F-score and classification accuracy, we
present the relative error reduction (e.r.) in classifi-
cation accuracy over the majority-class baseline for
that gold-standard set (see Table 2). For each count-
ability class, we additionally ran the classifier over
the concatenated feature vectors for the three basic
feature extraction methods, producing a 3,852-value
feature space (?Combined?).
Given the high baseline classification accuracies
for each gold-standard dataset, the most revealing
statistics in Table 3 are the error reduction and F-
score values. In all cases other than bipartite, the
combined system outperformed the individual sys-
tems. The difference in F-score is statistically sig-
nificant (based on the two-tailed t-test, p < .05) for
the asterisked systems in Table 3. For the bipartite
class, the difference in F-score is not statistically sig-
nificant between any system pairing.
There is surprisingly little separating the tagger-,
chunker- and RASP-based feature extraction meth-
ods. This is largely due to the precision/recall trade-
off noted above for the different systems.
5.2 Open data results
We next turn to the task of classifying all unseen
common nouns using the gold-standard data and the
best-performing classifier configurations for each
3Calculated according to: 2?precision ?recallprecision+recall
 0
 0.2
 0.4
 0.6
 0.8
 1
 10  100  1000  10000
 0.2
 0.4
 0.6
 0.8
 1
precision
recall
P
re
ci
si
on
R
ec
al
l
Mean frequency
Figure 1: Precision?recall curve for countable nouns
countability class (indicated in bold in Table 3).4
Here, the baseline method is to classify every noun
as being uniquely countable.
There were 11,499 feature-mapped common
nouns not contained in the union of the gold-
standard datasets. Of these, the classifiers were able
to classify 10,355 (90.0%): 7,974 (77.0%) as count-
able (e.g. alchemist), 2,588 (25.0%) as uncountable
(e.g. ingenuity), 9 (0.1%) as bipartite (e.g. head-
phones), and 80 (0.8%) as plural only (e.g. dam-
ages). Only 139 nouns were assigned to multiple
countability classes.
We evaluated the classifier outputs in two ways.
In the first, we compared the classifier output to the
combined COMLEX and ALT-J/E lexicons: a lexicon
with countability information for 63,581 nouns. The
classifiers found a match for 4,982 of the nouns. The
predicted countability was judged correct 94.6% of
the time. This is marginally above the level of match
between ALT-J/E and COMLEX (93.8%) and substan-
tially above the baseline of all-countable at 89.7%
(error reduction = 47.6%).
To gain a better understanding of the classifier
performance, we analysed the correlation between
corpus frequency of a given target noun and its pre-
cision/recall for the countable class.5 To do this,
we listed the 11,499 unannotated nouns in increas-
ing order of corpus occurrence, and worked through
the ranking calculating the mean precision and re-
call over each partition of 500 nouns. This resulted
in the precision?recall graph given in Figure 1, from
which it is evident that mean recall is proportional
and precision inversely proportional to corpus fre-
4In each case, the classifier is run over the best-
500 features as selected by the method described in
Baldwin and Bond (2003) rather than the full feature set, purely
in the interests of reducing processing time. Based on cross-
validated results over the training data, the resultant difference
in performance is not statistically significant.
5We similarly analysed the uncountable class and found the
same basic trend.
quency. That is, for lower-frequency nouns, the clas-
sifier tends to rampantly classify nouns as count-
able, while for higher-frequency nouns, the classi-
fier tends to be extremely conservative in positively
classifying nouns. One possible explanation for this
is that, based on the training data, the frequency
of a noun is proportional to the number of count-
ability classes it belongs to. Thus, for the more
frequent nouns, evidence for alternate countability
classes can cloud the judgement of a given classifier.
In secondary evaluation, the authors used BNC
corpus evidence to blind-annotate 100 randomly-
selected nouns from the test data, and tested the cor-
relation with the system output. This is intended
to test the ability of the system to capture corpus-
attested usages of nouns, rather than independent
lexicographic intuitions as are described in the COM-
LEX and ALT-J/E lexicons. Of the 100, 28 were clas-
sified by the annotators into two or more groups
(mainly countable and uncountable). On this set,
the baseline of all-countable was 87.8%, and the
classifiers gave an agreement of 92.4% (37.7% e.r.),
agreement with the dictionaries was also 92.4%.
Again, the main source of errors was the classi-
fier only returning a single countability for each
noun. To put this figure in proper perspective, we
also hand-annotated 100 randomly-selected nouns
from the training data (that is words in our com-
bined lexicon) according to BNC corpus evidence.
Here, we tested the correlation between the manual
judgements and the combined ALT-J/E and COMLEX
dictionaries. For this dataset, the baseline of all-
countable was 80.5%, and agreement with the dic-
tionaries was a modest 86.8% (32.3% e.r.). Based
on this limited evaluation, therefore, our automated
method is able to capture corpus-attested count-
abilities with greater precision than a manually-
generated static repository of countability data.
6 Discussion
The above results demonstrate the utility of the
proposed method in learning noun countability
from corpus data. In the final system configu-
ration, the system accuracy was 94.6%, compar-
ing favourably with the 78% accuracy reported
by Bond and Vatikiotis-Bateson (2002), 89.5% of
O?Hara et al (2003), and also the noun token-based
results of Schwartz (2002).
At the moment we are merely classifying nouns
into the four classes. The next step is to store the
distribution of countability for each target noun and
build a representation of each noun?s countability
preferences. We have made initial steps in this direc-
tion, by isolating token instances strongly support-
ing a given countability class analysis for that target
noun. We plan to estimate the overall frequency of
the different countabilities based on this evidence.
This would represent a continuous equivalent of the
discrete 5-way scale employed in ALT-J/E, tunable to
different corpora/domains.
For future work we intend to: investigate further
the relation between meaning and countability, and
the possibility of using countability information to
prune the search space in word sense disambigua-
tion; describe and extract countability-idiosyncratic
constructions, such as determinerless PPs and role-
nouns; investigate the use of a grammar that distin-
guishes between countable and uncountable uses of
nouns; and in combination with such a grammar, in-
vestigate the effect of lexical rules on countability.
7 Conclusion
We have proposed a knowledge-rich lexical acqui-
sition technique for multi-classifying a given noun
according to four countability classes. The tech-
nique operates over a range of feature clusters draw-
ing on pre-processed corpus data, which are then fed
into independent classifiers for each of the count-
ability classes. The classifiers were able to selec-
tively classify the countability preference of English
nouns with a precision of 94.6%.
Acknowledgements
This material is based upon work supported by the National
Science Foundation under Grant No. BCS-0094638 and also
the Research Collaboration between NTT Communication Sci-
ence Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University. We would like to thank
Leonoor van der Beek, Ann Copestake, Ivan Sag and the three
anonymous reviewers for their valuable input on this research.
References
Keith Allan. 1980. Nouns and countability. Language,
56(3):541?67.
Timothy Baldwin and Francis Bond. 2003. A plethora of meth-
ods for learning English countability. In Proc. of the 2003
Conference on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2003), Sapporo, Japan. (to appear).
Francis Bond and Caitlin Vatikiotis-Bateson. 2002. Using an
ontology to determine English countability. In Proc. of the
19th International Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan.
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994.
Countability and number in Japanese-to-English machine
translation. In Proc. of the 15th International Conference
on Computational Linguistics (COLING ?94), pages 32?8,
Kyoto, Japan.
Francis Bond. 2001. Determiners and Number in English, con-
trasted with Japanese, as exemplified in Machine Transla-
tion. Ph.D. thesis, University of Queensland, Brisbane, Aus-
tralia.
Ted Briscoe and John Carroll. 2002. Robust accurate statistical
annotation of general text. In Proc. of the 3rd International
Conference on Language Resources and Evaluation (LREC
2002), pages 1499?1504, Las Palmas, Canary Islands.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Ann Copestake and Ted Briscoe. 1995. Semi-productive poly-
semy and sense extension. Journal of Semantics, pages 15?
67.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2002. TiMBL: Tilburg memory based
learner, version 4.2, reference guide. ILK technical report
02-01.
Ralph Grishman, Catherine Macleod, and Adam Myers, 1998.
COMLEX Syntax Reference Manual. Proteus Project, NYU.
(http://nlp.cs.nyu.edu/comlex/refman.ps).
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
? effects of new methods in ALT-J/E?. In Proc. of the Third
Machine Translation Summit (MT Summit III), pages 101?
106, Washington DC.
Marc Light. 1996. Morphological cues for lexical semantics.
In Proc. of the 34th Annual Meeting of the ACL, pages 25?
31, Santa Cruz, USA.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?23.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Tom O?Hara, Nancy Salay, Michael Witbrock, Dave Schnei-
der, Bjoern Aldag, Stefano Bertolo, Kathy Panton, Fritz
Lehmann, Matt Smith, David Baxter, Jon Curtis, and Peter
Wagner. 2003. Inducing criteria for mass noun lexical map-
pings using the Cyc KB and its extension to WordNet. In
Proc. of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, the Netherlands.
Lane O.B. Schwartz. 2002. Corpus-based acquisition of head
noun countability features. Master?s thesis, Cambridge Uni-
versity, Cambridge, UK.
Eric V. Siegel and Kathleen McKeown. 2000. Learning meth-
ods to combine linguistic indicators: Improving aspectual
classification and revealing linguistic insights. Computa-
tional Linguistics, 26(4):595?627.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proc.
of the 4th Conference on Computational Natural Language
Learning (CoNLL-2000), Lisbon, Portugal.
Anna Wierzbicka. 1988. The Semantics of Grammar. John
Benjamin.
Proceedings of the 43rd Annual Meeting of the ACL, pages 330?337,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
High Precision Treebanking
? Blazing Useful Trees Using POS Information ?
Takaaki Tanaka,? Francis Bond,? Stephan Oepen,? Sanae Fujita?
? {takaaki, bond, fujita}@cslab.kecl.ntt.co.jp
? oe@csli.stanford.edu
? NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
? Universitetet i Oslo and CSLI, Stanford
Abstract
In this paper we present a quantitative
and qualitative analysis of annotation in
the Hinoki treebank of Japanese, and in-
vestigate a method of speeding annotation
by using part-of-speech tags. The Hinoki
treebank is a Redwoods-style treebank of
Japanese dictionary definition sentences.
5,000 sentences are annotated by three dif-
ferent annotators and the agreement evalu-
ated. An average agreement of 65.4% was
found using strict agreement, and 83.5%
using labeled precision. Exploiting POS
tags allowed the annotators to choose the
best parse with 19.5% fewer decisions.
1 Introduction
It is important for an annotated corpus that the mark-
up is both correct and, in cases where variant anal-
yses could be considered correct, consistent. Con-
siderable research in the field of word sense disam-
biguation has concentrated on showing that the an-
notation of word senses can be done correctly and
consistently, with the normal measure being inter-
annotator agreement (e.g. Kilgariff and Rosenzweig,
2000). Surprisingly, few such studies have been car-
ried out for syntactic annotation, with the notable
exceptions of Brants et al (2003, p 82) for the Ger-
man NeGra Corpus and Civit et al (2003) for the
Spanish Cast3LB corpus. Even such valuable and
widely used corpora as the Penn TreeBank have not
been verified in this way.
We are constructing the Hinoki treebank as part of
a larger project in cognitive and computational lin-
guistics ultimately aimed at natural language under-
standing (Bond et al, 2004). In order to build the ini-
tial syntactic and semantic models, we are treebank-
ing the dictionary definition sentences of the most
familiar 28,000 words of Japanese and building an
ontology from the results.
Arguably the most common method in building a
treebank still is manual annotation, annotators (often
linguistics students) marking up linguistic properties
of words and phrases. In some semi-automated tree-
bank efforts, annotators are aided by POS taggers or
phrase-level chunkers, which can propose mark-up
for manual confirmation, revision, or extension. As
computational grammars and parsers have increased
in coverage and accuracy, an alternate approach has
become feasible, in which utterances are parsed and
the annotator selects the best parse Carter (1997);
Oepen et al (2002) from the full analyses derived
by the grammar.
We adopted the latter approach. There were four
main reasons. The first was that we wanted to de-
velop a precise broad-coverage grammar in tandem
with the treebank, as part of our research into natu-
ral language understanding. Treebanking the output
of the parser allows us to immediately identify prob-
lems in the grammar, and improving the grammar
directly improves the quality of the treebank in a mu-
tually beneficial feedback loop (Oepen et al, 2004).
The second reason is that we wanted to annotate to a
high level of detail, marking not only dependency
and constituent structure but also detailed seman-
tic relations. By using a Japanese grammar (JACY:
Siegel and Bender, 2002) based on a monostratal
theory of grammar (HPSG: Pollard and Sag, 1994)
we could simultaneously annotate syntactic and se-
mantic structure without overburdening the annota-
330
tor. The third reason was that we expected the use
of the grammar to aid in enforcing consistency ?
at the very least all sentences annotated are guaran-
teed to have well-formed parses. The flip side to this
is that any sentences which the parser cannot parse
remain unannotated, at least unless we were to fall
back on full manual mark-up of their analyses. The
final reason was that the discriminants can be used
to update the treebank when the grammar changes,
so that the treebank can be improved along with the
grammar. This kind of dynamic, discriminant-based
treebanking was pioneered in the Redwoods tree-
bank of English (Oepen et al, 2002), so we refer
to it as Redwoods-style treebanking.
In the next section, we give some more details
about the Hinoki Treebank and the data used to eval-
uate the parser (? 2). This is followed by a brief dis-
cussion of treebanking using discriminants (? 3), and
an extension to seed the treebanking using existing
markup (? 4). Finally we present the results of our
evaluation (? 5), followed by some discussion and
outlines for future research.
2 The Hinoki Treebank
The Hinoki treebank currently consists of around
95,000 annotated dictionary definition and example
sentences. The dictionary is the Lexeed Semantic
Database of Japanese (Kasahara et al, 2004), which
consists of all words with a familiarity greater than
or equal to five on a scale of one to seven. This
gives 28,000 words, divided into 46,347 different
senses. Each sense has a definition sentence and
example sentence written using only these 28,000
familiar words (and some function words). Many
senses have more than one sentence in the definition:
there are 81,000 defining sentences in all.
The data used in our evaluation is taken from the
first sentence of the definitions of all words with a
familiarity greater than six (9,854 sentences). The
Japanese grammar JACY was extended until the
coverage was over 80% (Bond et al, 2004).
For evaluation of the treebanking we selected
5,000 of the sentences that could be parsed, and di-
vided them into five 1,000 sentence sets (A?E). Def-
inition sentences tend to vary widely in form de-
pending on the part of speech of the word being de-
fined ? each set was constructed with roughly the
same distribution of defined words, as well as hav-
ing roughly the same length (the average was 9.9,
ranging from 9.5?10.4).
A (simplified) example of an entry (Sense 2 of
  kflaten ?curtain: any barrier to communica-
tion or vision?), and a syntactic view of its parse are
given in Figure 1. There were 6 parses for this def-
inition sentence. The full parse is an HPSG sign,
containing both syntactic and semantic information.
A view of the semantic information is given in Fig-
ure 21.
UTTERANCE
NP
VP N
PP V
NP
DET N CASE-P
    	 	 	


       	 	 	
aru monogoto o kakusu mono
a certain stuff ACC hide thing
Curtain2: ?a thing that hides something?
Figure 1: Syntactic View of the Definition of  

2 kflaten ?curtain?
?h0, x2 {h0 : proposition(h5)
h1 : aru(e1, x1, u0) ?a certain?
h1 : monogoto(x1) ?stuff?
h2 : u def(x1, h1, h6)
h5 : kakusu(e2, x2, x1) ?hide?
h3 : mono(x2) ?thing?
h4 : u def(x2, h3, h7)}?
Figure 2: Semantic View of the Definition of  

2 kflaten ?curtain?
The semantic view shows some ambiguity has
been resolved that is not visible in the purely syn-
tactic view. In Japanese, relative clauses can have
gapped and non-gapped readings. In the gapped
reading (selected here),  mono ?thing? is the sub-
ject of  kakusu ?hide?. In the non-gapped read-
ing there is some unspecified relation between the
thing and the verb phrase. This is similar to the dif-
ference in the two readings of the day he knew in En-
glish: ?the day that he knew about? (gapped) vs ?the
day on which he knew (something)? (non-gapped).
1The semantic representation used is Minimal Recursion Se-
mantics (Copestake et al, Forthcoming). The figure shown here
hides some of the detail of the underspecified scope.
331
Such semantic ambiguity is resolved by selecting the
correct derivation tree that includes the applied rules
in building the tree, as shown in Figure 3. In the next
phase of the Hinoki project, we are concentrating on
acquiring an ontology from these semantic represen-
tations and using it to improve the parse selection
(Bond et al, 2004).
3 Treebanking Using Discriminants
Selection among analyses in our set-up is done
through a choice of elementary discriminants, basic
and mostly independent contrasts between parses.
These are (relatively) easy to judge by annotators.
The system selects features that distinguish between
different parses, and the annotator selects or rejects
the features until only one parse is left. In a small
number of cases, annotation may legitimately leave
more than one parse active (see below). The system
we used for treebanking was the [incr tsdb()] Red-
woods environment2 (Oepen et al, 2002). The num-
ber of decisions for each sentence is proportional
to the log of the number of parses. The number of
decisions required depends on the ambiguity of the
parses and the length of the input. For Hinoki, on av-
erage, the number of decisions presented to the an-
notator was 27.5. However, the average number of
decisions needed to disambiguate each sentence was
only 2.6, plus an additional decision to accept or re-
ject the selected parses3. In general, even a sentence
with 100 parses requires only around 5 decisions and
1,000 parses only around 7 decisions. A graph of
parse results versus number of decisions presented
and required is given in Figure 6.
The primary data stored in the treebank is the
derivation tree: the series of rules and lexical items
the parser used to construct the parse. This, along
with the grammar, can be combined to rebuild the
complete HPSG sign. The annotators task is to se-
lect the appropriate derivation tree or trees. The pos-
sible derivation trees for   2 kflaten ?curtain?
are shown in Figure 3. Nodes in the trees indicate
applied rules, simplified lexical types or words. We
2The [incr tsdb()] system, Japanese and English grammars
and the Redwoods treebank of English are available from the
Deep Linguistic Processing with HPSG Initiative (DELPH-IN:
http://www.delph-in.net/).
3This average is over all sentences, even non-ambiguous
ones, which only require a decision as to whether to accept or
reject.
will use it as an example to explain the annotation
process. Figure 3 also displays POS tag from a sep-
arate tagger, shown in typewriter font.4
This example has two major sources of ambiguity.
One is lexical: aru ?a certain/have/be? is ambigu-
ous between a reading as a determiner ?a certain?
(det-lex) and its use as a verb of possession ?have?
(aru-verb-lex). If it is a verb, this gives rise to
further structural ambiguity in the relative clause, as
discussed in Section 2. Reliable POS tags can thus
resolve some ambiguity, although not all.
Overall, this five-word sentence has 6 parses. The
annotator does not have to examine every tree but is
instead presented with a range of 9 discriminants, as
shown in Figure 4, each local to some segment of
the utterance (word or phrase) and thus presenting a
contrast that can be judged in isolation. Here the first
column shows deduced status of discriminants (typ-
ically toggling one discriminant will rule out oth-
ers), the second actual decisions, the third the dis-
criminating rule or lexical type, the fourth the con-
stituent spanned (with a marker showing segmenta-
tion of daughters, where it is unambiguous), and the
fifth the parse trees which include the rule or lexical
type.
D A
Rules /
Lexical Types
Subtrees /
Lexical items
Parse
Trees
? ? rel-cl-sbj-gap 	
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 65?68,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Implemented Description of Japanese:
The Lexeed Dictionary and the Hinoki Treebank
Sanae Fujita, Takaaki Tanaka, Francis Bond, Hiromi Nakaiwa
NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
{sanae, takaaki, bond, nakaiwa}@cslab.kecl.ntt.co.jp
Abstract
In this paper we describe the current state
of a new Japanese lexical resource: the
Hinoki treebank. The treebank is built
from dictionary definition sentences, and
uses an HPSG based Japanese grammar to
encode both syntactic and semantic infor-
mation. It is combined with an ontology
based on the definition sentences to give a
detailed sense level description of the most
familiar 28,000 words of Japanese.
1 Introduction
In this paper we describe the current state of a
new lexical resource: the Hinoki treebank. The
ultimate goal of our research is natural language
understanding ? we aim to create a system that
can parse text into some useful semantic represen-
tation. This is an ambitious goal, and this pre-
sentation does not present a complete solution,
but rather a road-map to the solution, with some
progress along the way.
The first phase of the project, which we present
here, is to construct a syntactically and semanti-
cally annotated corpus based on the machine read-
able dictionary Lexeed (Kasahara et al, 2004).
This is a hand built self-contained lexicon: it con-
sists of headwords and their definitions for the
most familiar 28,000 words of Japanese. Each
definition and example sentence has been parsed,
and the most appropriate analysis selected. Each
content word in the sentences has been marked
with the appropriate Lexeed sense. The syntac-
tic model is embodied in a grammar, while the se-
mantic model is linked by an ontology. This makes
it possible to test the use of similarity and/or se-
mantic class based back-offs for parsing and gen-
eration with both symbolic grammars and statisti-
cal models.
In order to make the system self sustaining we
base the first growth of our treebank on the dic-
tionary definition sentences themselves. We then
train a statistical model on the treebank and parse
the entire lexicon. From this we induce a the-
saurus. We are currently tagging other genres with
the same information. We will then use this infor-
mation and the thesaurus to build a parsing model
that combines syntactic and semantic information.
We will also produce a richer ontology ? for ex-
ample extracting selectional preferences. In the
last phase, we will look at ways of extending our
lexicon and ontology to less familiar words.
2 The Lexeed Semantic Database of
Japanese
The Lexeed Semantic Database of Japanese con-
sists of all Japanese words with a familiarity
greater than or equal to five on a seven point
scale (Kasahara et al, 2004). This gives 28,000
words in all, with 46,000 different senses. Defini-
tion sentences for these sentences were rewritten
to use only the 28,000 familiar words (and some
function words). The defining vocabulary is ac-
tually 16,900 different words (60% of all possi-
ble words). A simplified example entry for the
last two senses of the word doraibfla
?driver? is given in Figure 1, with English glosses
added, but omitting the example sentences. Lex-
eed itself consists of just the definitions, familiar-
ity and part of speech, all the underlined features
are those added by the Hinoki project.
3 The Hinoki Treebank
The structure of our treebank is inspired by the
Redwoods treebank of English (Oepen et al,
2002) in which utterances are parsed and the anno-
tator selects the best parse from the full analyses
derived by the grammar. We had four main rea-
sons for selecting this approach. The first was that
we wanted to develop a precise broad-coverage
65
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX doraiba?
POS noun Lexical-Type noun-lex
FAMILIARITY 6.5 [1?7] (? 5) Frequency 37 Entropy 0.79
SENSE 1 . . .
SENSE 2
P(S2) = 0.84
?
?
?
?
?
?
?
DEFINITION 1/ / 1/ / 1/
Someone who drives a car.
HYPERNYM 1 hito ?person?
SEM. CLASS ?292:chauffeur/driver? (? ?5:person?)
WORDNET driver1
?
?
?
?
?
?
?
SENSE 3
P(S2) = 0.05
?
?
?
?
?
?
?
?
?
?
DEFINITION 1/ / / 1/ / / 3/ / /
In golf, a long-distance club. A number one wood.
HYPERNYM 3 kurabu ?club?
SEM. CLASS ?921:leisure equipment? (? 921)
WORDNET driver5
DOMAIN 1 gorufu ?golf?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Entry for the Word doraibfla ?driver? (with English glosses)
grammar in tandem with the treebank, as part of
our research into natural language understanding.
Treebanking the output of the parser allows us
to immediately identify problems in the grammar,
and improving the grammar directly improves the
quality of the treebank in a mutually beneficial
feedback loop.
The second reason is that we wanted to annotate
to a high level of detail, marking not only depen-
dency and constituent structure but also detailed
semantic relations. By using a Japanese gram-
mar (JACY: Siegel (2000)) based on a monostratal
theory of grammar (Head Driven Phrase Structure
Grammar) we could simultaneously annotate syn-
tactic and semantic structure without overburden-
ing the annotator. The treebank records the com-
plete syntacto-semantic analysis provided by the
HPSG grammar, along with an annotator?s choice
of the most appropriate parse. From this record,
all kinds of information can be extracted at various
levels of granularity: A simplified example of the
labeled tree, minimal recursion semantics repre-
sentation (MRS) and semantic dependency views
for the definition of 2 doraibfla ?driver?
is given in Figure 2.
The third reason was that use of the grammar as
a base enforces consistency ? all sentences anno-
tated are guaranteed to have well-formed parses.
The last reason was the availability of a reason-
ably robust existing HPSG of Japanese (JACY),
and a wide range of open source tools for de-
veloping the grammars. We made extensive use
of tools from the the Deep Linguistic Process-
ing with HPSG Initiative (DELPH-IN: http://
www.delph-in.net/) These existing resources
enabled us to rapidly develop and test our ap-
proach.
3.1 Syntactic Annotation
The construction of the treebank is a two stage
process. First, the corpus is parsed (in our case
using JACY), and then the annotator selects the
correct analysis (or occasionally rejects all anal-
yses). Selection is done through a choice of dis-
criminants. The system selects features that distin-
guish between different parses, and the annotator
selects or rejects the features until only one parse
is left. The number of decisions for each sentence
is proportional to log2 in the length of the sentence
(Tanaka et al, 2005). Because the disambiguat-
ing choices made by the annotators are saved, it
is possible to semi-automatically update the tree-
bank when the grammar changes. Re-annotation
is only necessary in cases where the parse has be-
come more ambiguous or, more rarely, existing
rules or lexical items have changed so much that
the system cannot reconstruct the parse.
The Lexeed definition sentences were already
POS tagged. We experimented with using the POS
tags to mark trees as good or bad (Tanaka et al,
2005). This enabled us to reduce the number of
annotator decisions by 20%.
One concern with Redwoods style treebanking
is that it is only possible to annotate those trees
that the grammar can parse. Sentences for which
no analysis had been implemented in the grammar
or which fail to parse due to processing constraints
are left unannotated. This makes grammar cov-
66
UTTERANCE
NP
VP N
PP V
N CASE-P V V
jidflosha o unten suru hito
car ACC drive do person
Parse Tree
?h0,x1{h0 :proposition m(h1)
h1 :hito n(x1) ?person?
h2 :ude f q(x1,h1,h6)
h3 : jidosha n(x2) ?car?
h4 :ude f q(x2,h3,h7)
h5 :unten s(e1,x1,x2)}??drive?
MRS
{x1 :
e1 :unten s(ARG1 x1 : hito n,ARG2 x2 : jidosha n)
r1 : proposition m(MARG e1 : unten s)}
Semantic Dependency
Figure 2: Parse Tree, Simplified MRS and Dependency Views for 2 doraibfla ?driver?
erage a significant issue. We extended JACY by
adding the defining vocabulary, and added some
new rules and lexical-types (more detail is given
in Bond et al (2004)). None of the rules are spe-
cific to the dictionary domain. The grammatical
coverage over all sentences is now 86%. Around
12% of the parsed sentences were rejected by the
treebankers due to an incomplete semantic repre-
sentation. The total size of the treebank is cur-
rently 53,600 definition sentences and 36,000 ex-
ample sentences: 89,600 sentences in total.
3.2 Sense Annotation
All open class words were annotated with their
sense by five annotators. Inter-annotator agree-
ment ranges from 0.79 to 0.83. For example, the
word kurabu ?club? is tagged as sense 3 in
the definition sentence for driver3, with the mean-
ing ?golf-club?. For each sense, we calculate the
entropy and per sense probabilities over four cor-
pora: the Lexeed definition and example sentences
and Newspaper text from the Kyoto University and
Senseval 2 corpora (Tanaka et al, 2006).
4 Applications
4.1 Stochastic Parse Ranking
Using the treebanked data, we built a stochastic
parse ranking model. The ranker uses a maximum
entropy learner to train a PCFG over the parse
derivation trees, with the current node, two grand-
parents and several other conditioning features. A
preliminary experiment showed the correct parse
is ranked first 69% of the time (10-fold cross val-
idation on 13,000 sentences; evaluated per sen-
tence). We are now experimenting with extensions
based on constituent weight, hypernym, semantic
class and selectional preferences.
4.2 Ontology Acquisition
To extract hypernyms, we parse the first defini-
tion sentence for each sense (Nichols et al, 2005).
The parser uses the stochastic parse ranking model
learned from the Hinoki treebank, and returns the
semantic representation (MRS) of the first ranked
parse. In cases where JACY fails to return a parse,
we use a dependency parser instead. The highest
scoping real predicate is generally the hypernym.
For example, for doraibfla2 the hypernym is hito
?person? and for doraib fla3 the hypernym is
kurabu ?club? (see Figure 1). We also extract
other relationships, such as synonym and domain.
Because the words are sense tags, we can special-
ize the relations to relations between senses, rather
than just words: ?hypernym: doraiba?3, kurabu3?.
Once we have synonym/hypernym relations, we
can link the lexicon to other lexical resources. For
example, for the manually constructed Japanese
ontology Goi-Taikei (Ikehara et al, 1997) we link
to its semantic classes by the following heuristic:
look up the semantic classes C for both the head-
word (wi) and hypernym(s) (wg). If at least one of
the index word?s classes is subsumed by at least
one of the genus? classes, then we consider the re-
lationship confirmed. To link cross-linguistically,
we look up the headwords and hypernym(s) in a
translation lexicon and compare the set of trans-
lations ci ? C(T (wi)) with WordNet (Fellbaum,
1998)). Although looking up the translation adds
noise, the additional filter of the relationship triple
effectively filters it out again.
Adding the ontology to the dictionary interface
makes a far more flexible resource. For example,
by clicking on the ?hypernym: doraiba?3, goru f u1?
link, it is possible to see a list of all the senses re-
67
lated to golf, a link that is inaccessible in the paper
dictionary.
4.3 Semi-Automatic Grammar
Documentation
A detailed grammar is a fundamental component
for precise natural language processing. It pro-
vides not only detailed syntactic and morphologi-
cal information on linguistic expressions but also
precise and usually language-independent seman-
tic structures of them. To simplify grammar de-
velopment, we take a snapshot of the grammar
used to treebank in each development cycle. From
this we extract information about lexical items
and their types from both the grammar and tree-
bank and convert it into an electronically accesi-
ble structured database (the lexical-type database:
Hashimoto et al, 2005). This allows grammar de-
velopers and treebankers to see comprehensive up-
to-date information about lexical types, including
documentation, syntactic properties (super types,
valence, category and so on), usage examples from
the treebank and links to other dictionaries.
5 Further Work
We are currently concentrating on three tasks. The
first is improving the coverage of the grammar,
so that we can parse more sentences to a cor-
rect parse. The second is improving the knowl-
edge acquisition, in particular learning other in-
formation from the parsed defining sentences ?
such as lexical-types, semantic association scores,
meronyms, and antonyms. The third task is adding
the knowledge of hypernyms into the stochastic
model.
The Hinoki project is being extended in several
ways. For Japanese, we are treebanking other gen-
res, starting with Newspaper text, and increasing
the vocabulary, initially by parsing other machine
readable dictionaries. We are also extending the
approach multilingually with other grammars in
the DELPH-IN group. We have started with the
English Resource Grammar and the Gnu Contem-
porary International Dictionary of English and are
investigating Korean and Norwegian through co-
operation with the Korean Research Grammar and
NorSource.
6 Conclusion
In this paper we have described the current state of
the Hinoki treebank. We have further showed how
it is being used to develop a language-independent
system for acquiring thesauruses from machine-
readable dictionaries.
With the improved the grammar and ontology,
we will use the knowledge learned to extend our
model to words not in Lexeed, using definition
sentences from machine-readable dictionaries or
where they appear within normal text. In this way,
we can grow an extensible lexicon and thesaurus
from Lexeed.
Acknowledgements
We thank the treebankers, Takayuki Kurib-
ayashi, Tomoko Hirata and Koji Yamashita, for
their hard work and attention to detail.
References
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: A treebank for text understanding. In Proceed-
ings of the First International Joint Conference on Natural
Language Processing (IJCNLP-04). Springer Verlag. (in
press).
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Chikara Hashimoto, Francis Bond, Takaaki Tanaka, and
Melanie Siegel. 2005. Integration of a lexical type
database with a linguistically interpreted corpus. In 6th
International Workshop on Linguistically Integrated Cor-
pora (LINC-2005), pages 31?40. Cheju, Korea.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lex-
icon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictio-
naries. In Proceedings of the International Joint Confer-
ence on Artificial Intelligence IJCAI-2005, pages 1111?
1116. Edinburgh.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International Con-
ference on Computational Linguistics: COLING-2002,
pages 1253?7. Taipei, Taiwan.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil: Foundations of Speech-
to-Speech Translation, pages 265? 280. Springer, Berlin,
Germany.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The
Hinoki sensebank ? a large-scale word sense tagged cor-
pus of Japanese ?. In Frontiers in Linguistically Anno-
tated Corpora 2006. Sydney. (ACL Workshop).
Takaaki Tanaka, Francis Bond, Stephan Oepen, and Sanae
Fujita. 2005. High precision treebanking ? blazing useful
trees using POS information. In ACL-2005, pages 330?
337.
68
In: Proceedings Of CoNLL-2000 and LLL-2000, pages 43-48, Lisbon, Portugal, 2000. 
Memory-Based Learning for Article Generation 
Guido  Minnen*  Franc is  Bond t 
Cognitive and Computing Sciences MT Research Group 
University of Sussex NTT Communication Science Labs 
Palmer BN1 9QH, Brighton, UK 2-4 Hikari-dai, Kyoto 619-0237, JAPAN 
Guido. Minnen?cogs. susx. ac. uk bond?cslab, kecl. ntt. co. jp 
Ann Copestake  
CSLI 
Stanford University 
Stanford CA 94305-2150, USA 
aac?csli, stanford, edu 
Abst ract  
Article choice can pose difficult problems in ap- 
plications uch as machine translation and auto- 
mated summarization. In this paper, we investi- 
gate the use of corpus data to collect statistical 
generalizations about article use in English in 
order to be able to generate articles automati- 
cally to supplement a symbolic generator. We 
use data from the Penn Treebank as input to a 
memory-based learner (TiMBL 3.0; Daelemans 
et al, 2000) which predicts whether to gener- 
ate an article with respect o an English base 
noun phrase. We discuss competitive r sults ob- 
tained using a variety of lexical, syntactic and 
semantic features that play an important role in 
automated article generation. 
1 In t roduct ion  
Article choice can pose difficult problems in nat- 
ural language applications. Machine transla- 
tion (MT) is an example of such an applica- 
tion. When translating from a source language 
that lacks articles, such as Japanese or Rus- 
sian, to one that requires them, such as English 
or German, the system must somehow generate 
the source language articles (Bond and Ogura, 
1998). Similarly in automated summarization: 
when sentences or fragments are combined or 
reduced, it is possible that the form of a noun 
phrase (NP) is changed such that a change of 
the article associated with the NP's head be- 
comes necessary. For example, consider the sen- 
tences A talk will be given on Friday about NLP; 
The talk will last .for one hour which might get 
summarized as Friday's NLP talk will last one 
* Visiting CSLI, Stanford University (2000). 
t Visiting CSLI, Stanford University (1999-2000). 
hour. However, given the input sentences, it is 
not clear how to decide not to generate an arti- 
cle for the subject NP in the output sentence. 
Another important application is in the field 
known as augmentative and alternative com- 
munication (AAC). In particular, people who 
have lost the ability to speak sometimes use 
a text-to-speech generator as a prosthetic de- 
vice. But most disabilities which affect speech, 
such as stroke or amyotrophic lateral sclerosis 
(ALS or Lou Gehrig's disease), also cause some 
more general motor impairment, which means 
that prosthesis users cannot achieve a text in- 
put rate comparable to normal typing speeds 
even if they are able to use a keyboard. Many 
have to rely on a slower physical interface (head- 
stick, head-pointer, eye-tracker tc). We are at- 
tempting to use a range of NLP technology to 
improve text input speed for such users. Article 
choice is particularly important for this applica- 
tion: many AAC users drop articles and resort 
to a sort of telegraphese, but this causes degra- 
dation in comprehension f synthetic speech and 
contributes to its perception as unnatural and 
robot-like. Our particular goal is to be able to 
use an article generator in conjunction with a 
symbolic generator for AAC (Copestake, 1997; 
Carroll et al, 1999). 
In this paper we investigate the use of corpus 
data to collect statistical generalizations about 
article use in English so as to be able to gen- 
erate them automatically. We use data from 
the Penn Treebank as input to a memory-based 
learner (TiMBL 3.0; Daelemans et al, 2000) 
that is used to predict whether to generate the 
or alan or no article. 1 We discuss a variety 
of lexical, syntactic and semantic features that 
1We assume a postprocessor to determine whether to 
generate a or an as  described in Minnen et al (2000). 
43 
play an important role in automated article gen- 
eration, and compare our results with other re- 
searchers'. 
The paper is structured as follows. Section 2 
relates our work to that of others. Section 3 
introduces the features we use. Section 4 intro- 
duces the learning method we use. We discuss 
our results in Section 5 and suggest some di- 
rections for future research, then conclude with 
some final remarks in Section 6. 
2 Re la ted  Work  
There has been considerable research on gen- 
erating articles in machine translation sys- 
tems (Gawrofiska, 1990; Murata and Nagao, 
1993; Bond and Ogura, 1998; Heine, 1998). 
These systems use hand-written rules and lex- 
ical information to generate articles. The best 
cited results, 88% accuracy, are quoted by Heine 
(1998) which were obtained with respect o a 
very small corpus of 1,000 sentences in a re- 
stricted omain. 
Knight and Chander (1994) present an ap- 
proach that uses decision trees to determine 
whether to generate the or alan. They do not 
consider the possibility that no article should 
be generated. On the basis of a corpus of 400K 
NP instances derived from the Wall Street Jour- 
nal, they construct decision trees for the 1,600 
most frequent nouns by considering over 30,000 
lexical, syntactic and semantic features. They 
achieve an accuracy of 81% with respect to these 
nouns. By guessing the for the remainder of the 
nouns, they achieve an overall accuracy of 78%. 
3 Features  Determin ing  Automated  
Ar t i c le  Generat ion  
We have extracted 300K base noun phrases 
(NPs) from the Penn Treebank Wall Street 
Journal data (Bies et al, 1995) using the tgrep 
tool. The distribution of these NP instances 
with respect o articles is as follows: the 20.6%, 
a/an 9.4% and 70.0% with no article. 
We experimented with a range of features: 
1. Head of the NP: We consider as the head 
of the NP the rightmost noun in the NP. If an 
NP does not contain a noun, we take the last 
word in the NP as its head. 
2. Part-of-speech (PoS) tag of the head of 
the NP: PoS labels were taken from the Penn 
Treebank. We list the tags that occurred with 
(PP-DIR to/T0 
(NP the/DT problem/NN)) 
Figure 1: An example ofa  prepositional phrase 
annotated with a functionaltag 
the heads of theNPs in Table 1. 
PoS Tag the alan no 
NN 42,806 27,160 53,855 
NNS 10,705 446 58,118 
NNP 6,938 271 47,721 
NNPS 536 2 1,329 
CD 382 180 13,368 
DT 18 0 3,045 
PRP 0 0 21,214 
PRP$ 0 0 25 
EX 0 0 1,073 
IN 0 1 502 
JJ 388 143 931 
JJR 11 1 310 
JJS 184 0 282 
RB 15 41 498 
VBG 43 12 210 
VB 0 1 89 
WDT 2 0 4,812 
WP 0 0 2,759 
Misc. 40 8 269 
Total: 62,068 28,266 210,410 
Table 1: Distribution of NP instances in Wall 
Street Journal data (300,744 NPs in all) 
3. Functional tag of the head of the NP: In 
the Penn Treebank each syntactic ategory can 
be associated with up to four functional tags as 
listed in Table 2. We consider the sequence of 
functional tags associated with the category of 
the NP as a feature; if a constituent has no func- 
tional tag, we give the feature the value NONE. 
4. Category of the constituent embedding the 
NP: We looked at the category of the embedding 
constituent. See Figure 1: The category of the 
constituent embedding the NP the problem is 
PP. 
5. Functional tag of the constituent 
embedding the NP: If the category of the con- 
stituent embedding the NP is associated with 
one or more functional tags, they are used as 
features. The functional tag of the constituent 
embedding the problem in Figure 1 is DIR. 
6. Other determiners of the NP: We looked 
at the presence of a determiner in the NP. By 
definition, an NP in the Penn Treebank can only 
44 
Functional Marks: 
Tag (ft) Text categories 
HLN headlines and datelines 
LST list markers 
TTL titles 
Grammat ica l  functions 
CLF 
N0M 
ADV 
LGS 
PRD 
SUBJ 
TPC 
CLR 
BNF 
DTV 
true clefts 
non NPs that function as NPs 
clausal and NP adverbials 
logical subjects in passives 
nonVP predicates 
surface subject 
topicalized/fronted constituents 
closely related 
beneficiary of action 
dative object 
Semantic roles 
V0C vocatives 
DIR direction and trajectory 
L0C location 
MNR manner 
PRP purpose and reason 
TMP temporal phrases 
PUT locative complement of put 
EXT spatial extent of activity 
Table 2: Functional tags and their mean- 
ing (Santorini, 1990) 
have one determiner (Bies et al, 1995), so we 
expect it to be a good predictor of situations 
where we should not generate an article. 
7. Head countability preferences of the head 
of the NP: In case the head of an NP is a noun 
we also use its countability as a feature. We an- 
ticipate that this is a useful feature because sin- 
gular indefinite countable nouns normally take 
the article a/n, whereas singular indefinite un- 
countable nouns normally take no article: a dog 
vs water. We looked up the countability from 
the transfer lexicon used in the Japanese-to- 
English machine translation system ALT- J /E  
(Ikehara et al, 1991). We used six values for 
the countability feature: FC (fully countable) for 
nouns that have both singular and plural forms 
and can be directly modified by numerals and 
modifiers such as many; UC (uncountable) for 
nouns that have no plural form and can be mod- 
ified by much; SC (strongly countable) for nouns 
that are more often countable than uncount- 
able; WC (weakly countable) for nouns that are 
more often uncountable than countable; and PT 
(pluralia tantum) for nouns that only have plu- 
ral forms, such as for example, scissors (Bond 
et al, 1994). Finally, we used the value UNKNOWN 
if the lexicon did not provide countability infor- 
mation for a noun or if the head of the NP was 
not a noun. 41.4% of the NP instances received 
the value UNKNOWN for this feature. 
8. Semantic classes of the head of the NP: If 
the head of the NP is a noun we also take into 
account its semantic lassification in a large se- 
mantic hierarchy. The underlying idea is that 
the semantic lass of the noun can be used as a 
way to back off in case of unknown head nouns. 
The 2,710 node semantic hierarchy we used was 
also developed in the context of the ALT- J /E 
system (Ikehara et al, 1991). Edges in this hi- 
erarchy represent IS-A or HAS-A relationships. 
In case the semantic lasses associated with two 
nodes stand in the IS-A relation, the semantic 
class associated with the node highest in the hi- 
erarchy subsumes the semantic lass associated 
with the other node. 
Each of the nodes in this part of the hierarchy 
is represented by a boolean feature which is set 
to 1 if that node lies on the path from the root 
of the hierarchy to a particular semantic class. 
Thus, for example, the semantic features of a 
noun in the semantic class organization con- 
sists of a vector of 30 features where the features 
corresponding to the nodes noun, concrete ,  
agent and organization are set to I and all 
other features are set to 0. 2 
4 Memory-based  learn ing  
We used the Tilburg memory based learner 
TiMBL 3.0.1 (Daelemans et al, 2000) to learn 
from examples for generating articles using the 
features discussed above. Memory-based learn- 
ing reads all training instances into memory and 
classifies test instances by extrapolating a class 
from the most similar instance(s) in memory. 
Daelemans et al (1999) have shown that 
for typical natural language tasks, this ap- 
proach has the advantage that it also extrap- 
olates from exceptional and low-frequency in- 
stances. In addition, as a result of automat- 
ically weighing features in the similarity func- 
tion used to determine the class of a test in- 
stance, it allows the user to incorporate large 
2If a noun has multiple senses, we collapse them by 
taking the semantic lasses of a noun to be the union of 
the semantic lasses of all its senses. 
45 
numbers of features from heterogeneous sources: 
When data is sparse, feature weighing embod- 
ies a smoothing-by-similarity effect (Zavrel and 
Daelemans, 1997). 
5 Evaluat ion and Discuss ion 
We tested the features discussed in section 3 
with respect o a number of different memory- 
based learning methods as implemented in the 
TiMBL system (Daelemans et al, 2000). 
We considered two different learning algo- 
rithms. The first, IB1 is a k-nearest neighbour 
algorithm. 3 This can be used with two differ- 
ent metrics to judge the distance between the 
examples: overlap and modified value difference 
metric (MVDM). TiMBL automatically learns 
weights for the features, using one of five dif- 
ferent weighting methods: no weighting, gain 
ratio, information gain, chi-squared and shared 
variance. The second algorithm, IGTREE, stores 
examples in a tree which is pruned according 
to the weightings. This makes it much faster 
and of comparable accuracy. The results for 
these different methods, for k = 1, 4, 16 are dis- 
played in Table 3. IB1 is tested with leave-one- 
out cross-validation, IGTREE with ten-fold cross 
validation. 
The best results were (82.6%) for IB1 with 
the MVDM metric, and either no weighting or 
weighting by gain ratio. IGTREE did not per- 
form as well. We investigated more values of k, 
from 1 to 200, and found they had little influ- 
ence on the accuracy results with k = 4 or 5 
performing slightly better. 
We also tested each of the features described 
in Section 3 in isolation and then all together. 
We used the best performing algorithm from our 
earlier experiment: IB1 with MVDM, gain ratio 
and k = 4. The results of this are given in 
Table 4. 
When interpreting these results it is impor- 
tant to recall the figures provided in Table 1. 
The most common article, for any PoS, was no 
and for many PoS, including pronouns, gener- 
ating no article is always correct. There is more 
variation in NPs headed by common ouns and 
adjectives, and a little in NPs headed by proper 
nouns. Our baseline therefore consists of never 
3Strictly speaking, it is a k nearest distance algo- 
rithm, which looks at all examples in the nearest k dis- 
tances, the number of which may be greater than k. 
Feature Accuracy 
head 80.3% 
head's part-of-speech 70.0% 
NP's functional tag 70.5% 
embedding category 70.0% 
embedding functional tag 70.0% 
determiner present or not 70.0% 
head's countability 70.0% 
head's semantic lasses 72.9% 
hline 
Table 4: Accuracy results by feature 
generating an article: this will be right in 70.0% 
of all cases. 
Looking at the figures in Table 4, we see that 
many of the features investigated id not im- 
prove results above the baseline. Using the head 
of the NP itself to predict the article gave the 
best results of any single feature, raising the ac- 
curacy to 79.4%. The functional tag of the head 
of the NP itself improved results slightly. The 
use of the semantic lasses (72.1%) clearly im- 
proves the results over the baseline thereby indi- 
cating that they capture useful generalizations. 
The results from testing the features in com- 
bination are shown in Table 5. Interestingly, 
features which were not useful on their own, 
proved useful in combination with the head 
noun. The most useful features appear to be the 
category of the embedding constituent (81.1%) 
and the presence or absence of a determiner 
(80.9%). Combining all the features gave an 
accuracy of 82.9%. 
Feature Accuracy 
head+its part-of-speech 80.8% 
head+functional t g of NP 81.1% 
head+embedding category 80.8% 
head+embedding functional tag 81.4% 
head+determiner present or not 81.7% 
head+countability 80.8% 
head+semantic classes 80.8% 
hline all features 83.6% 
all features-semantic classes 83.6% 
Table 5: Accuracy with combined features 
Our best results (82.6%), which used all fea- 
tures are significantly better than the baseline 
of generating no articles (70.0%) or using only 
the head of the NP for training (79.4%). We 
46 
Algor i thm 
k 
Feature  Weight ing  
None Gain ratio Information gain X 2 Shared variance 
IB1 1 83.5% 83.5% 83.3% 83.2% 83.3% 
(MVDM) 4 83.5% 83.6% 83.3% 83.3% 83.3% 
16 83.6% 83.5% 83.2% 83.2% 83.2% 
IB1 1 83.1% 83.5% 83.3% 83.2% 83.3% 
(overlap) 4 82.9% 83.1% 83.1% 83.1% 83.1% 
16 82.9% 83.0% 82.9% 82.9% 82.9% 
IGTREE - -  - -  82.9% 82.5% 82.4% 82.6% 
Table 3: Accuracy results broken down with respect o memory-based learning methods used 
also improve significantly upon earlier esults of 
78% as reported by Knight and Chander (1994), 
which in any case is a simpler task since it only 
involved choice between the and alan. Further, 
our results are competitive with state of the art 
rule-based systems. Because different corpora 
are used to obtain the various results reported 
in the literature and the problem is often de- 
fined differently, detailed comparison is difficult. 
However, the accuracy achieved appears to ap- 
proach the accuracy results achieved with hand- 
written rules. 
In order to test the effect of the size of the 
training data, we tested used the best perform- 
ing algorithm from our earlier experiment (IB1 
with MVDM, gain ratio and k = 4) on various 
subsets of the corpus: the first 10%, the first 
20%, the first 30% and so on to the whole cor- 
pus. The results are given in Table 6. 
Size Accuracy  
10% 80.95% 
20% 81.67% 
30% 82.14% 
4O% 82.45% 
50% 82.69% 
60% 83.04% 
70% 83.17% 
80% 83.24% 
90% 83.45% 
100% 83.58% 
(100% is 300,744 NPs) 
Table 6: Accuracy versus Size of Training Data 
The accuracy is still improving even with 
300,744 NPs, an even larger corpus should give 
even better results. It is important to keep in 
mind that we, like most other researchers, have 
been training and testing on a relatively homo- 
geneous corpus. Furthermore, we took as given 
information about the number of the NP. In 
many applications we will have neither a large 
amount of homogeneous training data nor infor- 
mation about number. 
5.1 Future  Work  
In the near future we intend to further ex- 
tend our approach in various directions. First, 
we plan to investigate other lexical and syn- 
tactic features that might further improve our 
results, such as the existence of pre-modifiers 
like superlative and comparative adjectives, and 
post-modifiers like prepositional phrases, rela- 
tive clauses, and so on. We would also like to in- 
vestigate the effect of additional discourse-based 
features uch as one that incorporates informa- 
tion about whether the referent of a noun phrase 
has been mentioned before. 
Second, we intend to make sure that the fea- 
tures we are using in training and testing will 
be available in the applications we consider. For 
example, in machine translation, the input noun 
phrase may be all dogs, whereas the output 
could be either all dogs or all the dogs. At 
present, words such as all, both, half in our in- 
put are tagged as pre-determiners if there is a 
following determiner (it can only be the or a 
possessive), and determiners if there is no arti- 
cle. To train for a realistic application we need 
to collapse the determiner and pre-determiner 
inputs together in our training data. 
Furthermore, we are interested in training 
on corpora with less markup, like the British 
National Corpus (Burnard, 1995) or even no 
markup at all. By running a PoS tagger and 
then an NP chunker, we should be able to get 
a lot more training data, and thus significantly 
improve our coverage. If we can use plain text 
47 
to train on, then it will be easier to adapt our 
tool quickly to new domains, for which there are 
unlikely to be fully marked up corpora. 
6 Conc lud ing  remarks  
We described a memory-based approach to au- 
tomated article generation that uses a variety of 
lexical, syntactic and semantic features as pro- 
vided by the Penn Treebank Wall Street Jour- 
nal data and a large hand-encoded MT dictio- 
nary. With this approach we achieve an accu- 
racy of 82.6%. We believe that this approach 
is an encouraging first step towards a statistical 
device for automated article generation that can 
be used in a range of applications uch as speech 
prosthesis, machine translation and automated 
summarization. 
Acknowledgments  
The authors would like to thank the Stanford NLP 
reading group, the LinGO project at CSLI, Timo- 
thy Baldwin, Kevin Knight, Chris Manning, Wal- 
ter Daelemans and two anonymous reviewers for 
their helpful comments. This project is in part sup- 
ported by the National Science Foundation under 
grant number IRI-9612682. 
References  
Ann Bies, Mark Fergusona, Karen Katz, and Robert 
MacIntyre, 1995. Bracketing Guidelines for Tree- 
bank H Style. Penn Treebank Project, University 
of Pennsylvania. 
Francis Bond and Kentaro Ogura. 1998. Reference 
in Japanese-to-English machine translation. Ma- 
chine Translation, 13(2-3):107-134. 
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 
1994. Countability and number in Japanese-to- 
English machine translation. In 15th Interna- 
tional Conference on Computational Linguistics: 
COLING-94, pages 32-38, Kyoto. (http: / /xxx.  
lanl. gov/abs/cmp- ig/951 i001). 
Lou Burnard. 1995. User reference guide for the 
British National Corpus. Technical report, Ox- 
ford University Computing Services. 
John Carroll, Ann Copestake, Dan Flickinger, and 
Victor Poznanski. 1999. An efficient chart gen- 
erator for (semi-)lexicalist grammars. In Proceed- 
ings o/ the 7th European Workshop on Natural 
Language Generation (EWNLG'99), pages 86-95, 
Toulouse, France. 
Ann Copestake. 1997. Augmented and alternative 
NLP techniques for augmentative and alternative 
communication. In Proceedings of the ACL work- 
shop on Natural Language Processing for Commu- 
nication Aids, pages 37-42, Madrid. 
Walter Daelemans, Antal van den Bosch, and Jakub 
Zavrel. 1999. Forgetting exceptions i harmful in 
language learning. Machine Learning, 34. 
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, 
and Antal van den Bosch. 2000. TiMBL: Tilburg 
memory based learner, version 3.0, reference 
guide. ILK Technical Report 00-01, ILK, Tilburg, 
The Netherlands. (ILK-0001; h t tp : / / i l k .kub .  
nl). 
Barbara Gawrofiska. 1990. "Translation Great 
Problem" on the problem of inserting articles 
when translating from Russian into Swedish. In 
13th International Conference on Computational 
Linguistics: COLING-90, Helsinki. 
Julia E. Heine. 1998. Definiteness predictions for 
Japanese noun phrases. In 36th Annual Meeting 
o\] the Association \]or Computational Linguistics 
and 17th International Conference on Computa- 
tional Linguistics: COLING/A CL-98, pages 519- 
525, Montreal, Canada. 
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hi- 
romi Nakaiwa. 1991. Toward an MT system with- 
out pre-editing - effects of new methods in ALT- 
J /E - .  In Third Machine Translation Summit: 
MT Summit III, pages 101-106, Washington DC. 
(http ://xxx. lanl. gov/abs/cmp- ig/9510008). 
Kevin Knight and Ishwar Chander. 1994. Au- 
tomated postediting of documents. In Proceed- 
ings of the 12th National Conference on Artificial 
Intelligence: AAAI-9~, pages 779-784, Seattle. 
(http ://xxx. lanl. gov/abs/cmp-ig/9407028). 
Guido Minnen, John Carroll, and Darren Pearce. 
2000. Robust, applied morphological generation. 
In Proceedings of the first International Natural 
Language Genration Conference, Mitzpe Ramon, 
Israel. 
Masaki Murata and Makoto Nagao. 1993. Deter- 
mination of referential property and number of 
nouns in Japanese sentences for machine transla- 
tion into English. In Fifth International Confer- 
ence on Theoretical and Methodological Issues in 
Machine Translation: TMI-93, pages 218-25, Ky- 
oto, July. (ht tp : / /xxx.  laa l .  gov/abs/cmp-lg/ 
9405019). 
Beatrice Santorini. 1990. Part-of-speech tagging 
guidelines for the Penn Treebank Project. Tech- 
nical Report MS-CIS-90-47, Department of Com- 
puter and Information Science, University of 
Pennsylvania. 
Jakub Zavrel and Walter Daelemans. 1997. 
Memory-based learning: Using similarity for 
smoothing. In Proceedings of the 35th Annual 
Meeting of the Association for Computational 
Linguistics, Madrid, Spain. 
48 
Extending the Coverage of a Valency Dictionary
Sanae Fujita and Francis Bond
{sanae, bond}@cslab.kecl.ntt.co.jp
2-4 Hikari-dai Seika-cho, Kyoto, Japan 619-0237
NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
Abstract
Information on subcategorization and selec-
tional restrictions is very important for nat-
ural language processing in tasks such as
monolingual parsing, accurate rule-based ma-
chine translation and automatic summarization.
However, adding this detailed information to a
valency dictionary is both time consuming and
costly.
In this paper we present a method of assign-
ing valency information and selectional restric-
tions to entries in a bilingual dictionary, based
on information in an existing valency dictio-
nary. The method is based on two assump-
tions: words with similar meaning have simi-
lar subcategorization frames and selectional re-
strictions; and words with the same translations
have similar meanings. Based on these assump-
tions, new valency entries are constructed for
words in a plain bilingual dictionary, using en-
tries with similar source-language meaning and
the same target-language translations. We eval-
uate the effects of various measures of similarity.
1 Introduction
One of the severest problems facing machine
translation between Asian languages is the lack
of suitable language resources. Even when
word-lists or simple bilingual dictionaries exist,
it is rare for them to include detailed informa-
tion about the syntax and meaning of words.
In this paper we present a method of adding
new entries to a bilingual valency dictionary.
New entries are based on existing entries, so
have the same amount of detailed information.
The method bootstraps from an initial hand
built lexicon, and allows new entries to be added
cheaply and effectively. Although we will use
Japanese and English as examples, the algo-
rithm is not tied to any particular language
pair or dictionary. The core idea is to add
new entries to the valency dictionary by using
Japanese-English pairs from a plain bilingual
dictionary (without detailed information about
valency or selectional restrictions), and build
new entries for them based on existing entries.
It is well known that detailed information
about verb valency (subcategorization) and se-
lectional restrictions is useful both for monolin-
gual parsing and selection of appropriate trans-
lations in machine translation. As well as being
useful for resolving parsing ambiguities, verb
valency information is particularly important
for complicated processing such as identification
and supplementation of zero pronouns. How-
ever, this information is not encoded in normal
human-readable dictionaries, and is hard to en-
ter manually. Shirai (1999) estimates that at
least 27,000 valency entries are needed to cover
around 80% of Japanese verbs in a typical news-
paper, and we expect this to be true of any lan-
guage. Various methods of creating detailed en-
tries have been suggested, such as the extraction
of candidates from corpora (Manning, 1993; Ut-
suro et al, 1997; Kawahara and Kurohashi,
2001), and the automatic and semi-automatic
induction of semantic constraints (Akiba et al,
2000). However, the automatic construction of
monolingual entries is still far from reaching
the quality of hand-constructed resources. Fur-
ther, large-scale bilingual resources are still rare
enough that it is much harder to automatically
build bilingual entries.
Our work differs from corpus-based work such
as Manning (1993) or Kawahara and Kurohashi
(2001) in that we are using existing lexical re-
sources rather than a corpus. Thus our method
will work for rare words, so long as we can find
them in a bilingual dictionary, and know the
English translation. It does not, however, learn
new frames from usage examples.
In order to demonstrate the utility of the
valency information, we give an example of a
sentence translated with the system default in-
formation (basically a choice between transitive
and intransitive), and the full valency informa-
tion. The verb is   kamei-suru ?order?,
which takes a sentential complement. In (1)1
the underlined part is the sentential comple-
ment. The verb valency entry is the same as 

	 joushin-suru ?report? [NP-ga Cl-to V],
except with the clause marked as to-infinitival.2
The translation with the valency information
is far from perfect, but it is comprehensible.
Without the valency information the translation
is incomprehensible.
(1) 

koku-ou
king

wa
top

kerai
subordinate

ni
dat
  
shutsugeki shiro,
sally forth

to
quot
 
ffA Plethora of Methods for Learning English Countability
Timothy Baldwin
CSLI
Stanford University
Stanford, CA 94305 USA
tbaldwin@csli.stanford.edu
Francis Bond
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
Kyoto, Japan
bond@cslab.kecl.ntt.co.jp
Abstract
This paper compares a range of methods
for classifying words based on linguis-
tic diagnostics, focusing on the task of
learning countabilities for English nouns.
We propose two basic approaches to
feature representation: distribution-based
representation, which simply looks at
the distribution of features in the cor-
pus data, and agreement-based represen-
tation which analyses the level of token-
wise agreement between multiple pre-
processor systems. We additionally com-
pare a single multiclass classifier archi-
tecture with a suite of binary classifiers,
and combine analyses from multiple pre-
processors. Finally, we present and evalu-
ate a feature selection method.
1 Introduction
Lexical acquisition can be described as the process
of populating a grammar skeleton with lexical items,
through a process of mapping word lemmata onto
lexical types described in the grammar. Depending
on the linguistic precision of the base grammar, lex-
ical acquisition can range in complexity from sim-
ple part-of-speech tagging (shallow lexical acquisi-
tion) to the acquisition of selectionally-constrained
subcategorisation frame clusters or constructional
compatibilities (deep lexical acquisition). Our par-
ticular interest is in the latter task of deep lexical
acquisition with respect to English nouns.
We are interested in developing learning tech-
niques for deep lexical acquisition which take a fixed
set of linguistic diagnostics, and classify words ac-
cording to corpus data. We propose a range of gen-
eral techniques for this task, as exemplified over the
task of English countability acquisition. Countabil-
ity is the syntactic property that determines whether
a noun can take singular and plural forms, and af-
fects the range of permissible modifiers. Many
nouns have both countable and uncountable lemmas,
with differences in meaning: I submitted two papers
?documents? (countable) vs. Please use white paper
?substance to be written on? (uncountable).
This research complements that described in
Baldwin and Bond (2003), where we present the lin-
guistic foundations and features drawn upon in the
countability classification task, and motivate the
claim that countability preferences can be learned
from corpus evidence. In this paper, we focus on
the methods used to tackle the task of countability
classification based on this fixed feature set.
The remainder of this paper is structured as fol-
lows. Section 2 outlines the countability classes,
resources and pre-processors. Section 3 presents
two methods of representing the feature space. Sec-
tion 4 details the different classifier designs and the
dataset, which are then evaluated in Section 5. Fi-
nally, we conclude the paper with a discussion in
Section 6.
2 Preliminaries
In this section, we describe the countability classes,
the resources used in this research, and the feature
extraction method. These are described in greater
detail in Baldwin and Bond (2003).
2.1 Countability classes
Nouns are classified as belonging to one or more of
four possible classes: countable, uncountable, plural
only and bipartite. Countable nouns can be modi-
fied by denumerators, prototypically numbers, and
have a morphologically marked plural form: one
dog, two dogs. Uncountable nouns cannot be mod-
ified by denumerators, but can be modified by un-
specific quantifiers such as much; they do not show
any number distinction (prototypically being singu-
lar): *one equipment, some equipment, *two equip-
ments. Plural only nouns only have a plural form,
such as goods, and cannot be either denumerated or
modified by much; many plural only nouns, such
as clothes, use the plural form even as modifiers: a
clothes horse. Bipartite nouns are plural when they
head a noun phrase (trousers), but generally singu-
lar when used as a modifier (trouser leg); they can
be denumerated with the classifier pair: a pair of
scissors.
2.2 Gold standard data
Information about noun countability was obtained
from two sources: COMLEX 3.0 (Grishman et
al., 1998) and the common noun part of ALT-
J/E?s Japanese-to-English semantic transfer dictio-
nary (Ikehara et al, 1991). Of the approximately
22,000 noun entries in COMLEX, 13,622 are marked
as countable, 710 as uncountable and the remainder
are unmarked for countability. ALT-J/E has 56,245
English noun types with distinct countability.
2.3 Feature space
Features used in this research are divided up into
feature clusters, each of which is conditioned on
the occurrence of a target noun in a given construc-
tion. Feature clusters are either one-dimensional
(describing a single multivariate feature) or two-
dimensional (describing the interaction between two
multivariate features), with each dimension describ-
ing a lexical or syntactic property of the construc-
tion in question. An example of a one-dimensional
feature cluster is head noun number, i.e. the num-
ber (singular or plural) of the target noun when it oc-
curs as the head of an NP; an example of a two-
dimensional feature cluster in subject?verb agree-
ment, i.e. the number (singular or plural) of the tar-
get noun when it occurs as head of a subject NP
vs. number agreement on the verb (singular or plu-
ral). Below, we provide a basic description of the
10 feature clusters used in this research and their di-
mensionality ([x]=1-dimensional feature cluster with
x unit features, [x?y]=2-dimensional feature cluster
with x ? y unit features). These represent a total of
206 unit features.
Head noun number:[?] the number of the target
noun when it heads an NP
Modifier noun number:[?] the number of the target
noun when a modifier in an NP
Subject?verb agreement:[???] the number of the
target noun in a subject position vs. number
agreement on the governing verb
Coordinate noun number:[???] the number of the
target noun vs. the number of the head nouns of
conjuncts
N of N constructions:[????] the type of the N? (e.g.
COLLECTIVE, TEMPORAL) vs. the number of the
target noun (N?) in an N? of N? construction
Occurrence in PPs:[????] the preposition type vs.
the presence or absence of a determiner when
the target noun occurs in singular form in a PP
Pronoun co-occurrence:[????] what personal, pos-
sessive and reflexive pronouns (e.g. he, their,
itself ) occur in the same sentence as singular
and plural instances of the target noun
Singular determiners:[??] what singular-selecting
determiners (e.g. a, much) occur in NPs headed
by the target noun in singular form
Plural determiners:[??] what plural-selecting de-
terminers (e.g. many, various) occur in NPs
headed by the target noun in plural form
Non-bounded determiners:[????] what non-
bounded determiners (e.g. more, sufficient)
occur in NPs headed by the target noun, and
what is the number of the target noun for each
2.4 Feature extraction
The values for the features described above were ex-
tracted from the written component of the British
National Corpus (BNC, Burnard (2000)) using three
different pre-processors: (a) a POS tagger, (b) a full-
text chunker and (c) a dependency parser. These are
used independently to test the efficacy of the differ-
ent systems at capturing features used in the clas-
sification process, and in tandem to consolidate the
strengths of the individual methods.
With the POS extraction method, we first tagged
the BNC using an fnTBL-based tagger (Ngai and
Florian, 2001) trained over the Brown and WSJ cor-
pora and based on the Penn POS tagset. We then
lemmatised this data using a Penn tagset-customised
version of morph (Minnen et al, 2001). Finally, we
implemented a range of high-precision, low-recall
POS-based templates to extract out the features from
the processed data.
For the chunker, we ran fnTBL over the lem-
matised tagged data, training over CoNLL 2000-
style (Tjong Kim Sang and Buchholz, 2000) chunk-
converted versions of the full Brown and WSJ cor-
pora. For the NP-internal features (e.g. determin-
ers, head number), we used the noun chunks directly,
or applied POS-based templates locally within noun
chunks. For inter-chunk features (e.g. subject?verb
agreement), we looked at only adjacent chunk pairs
so as to maintain a high level of precision.
We read dependency tuples directly off the output
of RASP (Briscoe and Carroll, 2002b) in grammati-
cal relation mode.1 RASP has the advantage that re-
call is high, although precision is potentially lower
1We used the first parse in the experiments reported here.
An alternative method would be to use weighted dependency
tuples, as described in Briscoe and Carroll (2002a).
than chunking or tagging as the parser is forced into
resolving phrase attachment ambiguities and com-
mitting to a single phrase structure analysis.
After generating the different feature vectors for
each noun based on the above configurations, we fil-
tered out all nouns which did not occur at least 10
times in NP head position in the output of all three
systems. This resulted in a total of 20,530 nouns,
of which 9,031 are contained in the combined COM-
LEX and ALT-J/E lexicons. The evaluation is based
on these 9,031 nouns.
3 Feature representation
We test two basic feature representations in this re-
search: distribution-based, which simply looks at
the relative occurrence of different features in the
corpus data, and agreement-based, which analyses
the level of token-wise agreement between multiple
systems.
3.1 Distribution-based feature representation
In the distribution-based feature representation, we
take each target noun in turn and compare its amal-
gamated value for each unit feature with (a) the val-
ues for other target nouns, and (b) the value of other
unit features within that same feature cluster. That
is, we focus on the relative prominence of features
globally within the corpus and locally within each
feature cluster.
In the case of a one-dimensional feature cluster
(e.g. singular determiners), each unit feature f s for
target noun w is translated into 3 separate feature
values:
corpfreq(f s,w) =
freq(f s|w)
freq(?) (1)
wordfreq(f s,w) =
freq(f s|w)
freq(w) (2)
featfreq(f s,w) =
freq(f s|w)?
ifreq(f i|w)
? (3)
where freq(?) is the frequency of all words in the cor-
pus. That is, for each unit feature we capture the rel-
ative corpus frequency, frequency relative to the tar-
get word frequency, and frequency relative to other
features in the same feature cluster. Thus, for an n-
valued one-dimensional feature cluster, we generate
3n independent feature values.
In the case of a two-dimensional feature ma-
trix (e.g. subject-position noun number vs. verb
number agreement), each unit feature f s,t for tar-
get noun w is translated into corpfreq(f s,t,w),
wordfreq(f s,t,w) and featfreq(f s,t,w) as above,
and 2 additional feature values:
featdimfreq?(f s,t,w) =
freq(f s,t|w)?
ifreq(f i,t|w)
(4)
featdimfreq?(f s,t,w) =
freq(f s,t|w)?
j freq(f s,j |w)
(5)
which represent the featfreq values calculated along
each of the two feature dimensions. Additionally,
we calculate cumulative totals for each row and
column of the feature matrix and describe each as
for the one-dimensional features above (in the form
of 3 values). Thus, for an m ? n-valued two-
dimensional feature cluster, we generate a total of
5mn+ 3(m+ n) independent feature values.
The feature clusters produce a combined total of
1284 individual feature values.
3.2 Agreement-based feature representation
The agreement-based feature representation con-
siders the degree of token agreement between the
features extracted using the three different pre-
processors. This allows us to pinpoint the reliable di-
agnostics within the corpus data and filter out noise
generated by the individual pre-processors.
It is possible to identify the features which
are positively-correlated with a unique countability
class (e.g. occurrence of a singular noun with the
determiner a occurs only for countable nouns), and
for each to determine the token-level agreement be-
tween the different systems. The number of diagnos-
tics considered for each of the countability classes
is: 32 for countable nouns, 19 for uncountable nouns
and 1 for each of plural only and bipartite nouns.
The total number of diagnostics we test agreement
across is thus 53.
The token-level correlation for each feature f s is
calculated fourfold according to relative agreement,
the ? statistic, correlated frequency and correlated
weight. The relative agreement between systems
sys? and sys? wrt f s for target noun w is defined to
be:
agr(f s,w)(sys?, sys?) =
|tok(f s,w)(sys?) ? tok(f s,w)(sys?)|
|tok(f s,w)(sys?) ? tok(f s,w)(sys?)|
where tok (f s,w)(sys i) returns the set of token in-
stances of (f s,w). The ? statistic (Carletta, 1996)
is recast as:
?(f s,w)(sys?, sys?) =
agr(f s,w)(sys?, sys?)?
?
agr(f s,?)(sys?,sys?)
N
??
?
agr(f s,?)(sys?,sys?)
N
In this modified form, ?(f s,w) represents the diver-
gence in relative agreement wrt f s for target noun w ,
relative to the mean relative agreement wrt f s over
all words. Correlated frequency is defined to be:
cfreq(f s,w)(sys?, sys?) =
|tok(f s,w)(sys?) ? tok(f s,w)(sys?)|
freq(w)
It describes the occurrence of tokens in agreement
for (f s,w) relative to the total occurrence of the tar-
get word.
The metrics are used to derive three separate fea-
ture values for each diagnostic over the three pre-
processor system pairings. We additionally calcu-
late the mean value of each metric across the system
pairings and the overall correlated weight for each
countability class C as:
cw(C ,w)(sys?, sys?) =
?
f s?C |tok(f s,w)(sys?) ? tok(f s,w)(sys?)|?
i|tok(f i,w)(sys?) ? tok(f i,w)(sys?)|
Correlated weight describes the occurrence of corre-
lated features in the given countability class relative
to other correlated features.
We test agreement: (a) for each of these diag-
nostics individually and within each countability
class (Agree(Token,?)), and (b) across the amalgam
of diagnostics for each of the countability classes
(Agree(Class,?)). For Agree(Token,?), we calculate
agr , ? and cfreq values for each of the 53 diag-
nostics across the 3 system pairings, and addition-
ally calculate the mean value for each value. We
additionally calculate the overall cw value for each
countability class. This results in a total of 640 fea-
ture values (3? 53? 3 + 53? 3 + 4). In the case
of Agree(Class,?), we average the agr , ? and cfreq
values across each countability class for each of the
three system pairings, and also calculate the mean
value in each case. We further calculate the overall
cw value for each countability class, culminating in
52 feature values (3? 4? 3 + 4? 3 + 4).
4 Classifier Set-up and Evaluation
Below, we outline the different classifiers tested
and describe the process used to generate the gold-
standard data.
4.1 Classifier architectures
We propose a variety of unsupervised and super-
vised classifier architectures for the task of learning
countability, and also a feature selection method. In
all cases, our classifiers are built using TiMBL ver-
sion 4.2 (Daelemans et al, 2002), a memory-based
classification system based on the k-nearest neigh-
bour algorithm. As a result of extensive parame-
ter optimisation, we settled on the default configu-
ration2 for TiMBL with k set to 9.3
2IB1 with weighted overlap, gain ratio-based feature
weighting and equal weighting of neighbours.
3We additionally experimented with the kernel-based
TinySVM system, but found TiMBL to be the marginally supe-
rior performer in all cases, a somewhat surprising result given
the high-dimensionality of the feature space.
Full-feature supervised classifiers
The simplest system architecture applies the su-
pervised learning paradigm to the distribution-based
feature vectors for each of the POS tagger, chun-
ker and RASP (Dist(POS,?), Dist(chunk,?) and
Dist(RASP,?), respectively). For the distribution-
based feature representation, we additionally
combine the outputs of the three pre-processors by:
(a) concatenating the individual distribution-based
feature vectors for the three systems (resulting in
a 3852-element feature vector: Dist(AllCON,?));
and (b) taking the mean over the three systems for
each distribution-based feature value (resulting in
a 1284-element feature vector: Dist(AllMEAN,?)).
The agreement-based feature representation
provides two additional system configurations:
Agree(Class,?) and Agree(Token,?) (see Section
3.2).
Orthogonal to the issue of how to generate the
feature values is the question of how to classify
a given noun according to the different countabil-
ity classes. The two basic options here are to ei-
ther have a single classifier and define multiclasses
according to all observed combinations of count-
ability classes (Dist(?,SINGLE)), or have a suite of
binary classifiers, one for each countability class
(Dist(?,SUITE)). The SINGLE classifier architec-
ture has advantages in terms of speed (a 4? speed-
up over the classifier suite) and simplicity, but runs
into problems with data sparseness for the less-
commonly attested multi-classes given that a single
noun can occur with multiple countabilities. The
SUITE classifier architecture delineates the different
countability classes more directly, but runs the risk
of a noun not being classified according to any of the
four classes.
Feature-selecting supervised classifiers
We improve the performance of the basic classi-
fiers by way of best-N filter-based feature selection.
Feature selection has been shown to improve clas-
sification accuracy over a variety of tasks (Liu and
Motoda, 1988), but in the case of memory-based
learners such as TiMBL, has the additional advan-
tage of accelerating the classification process and re-
ducing memory overhead. The computational com-
plexity of memory-based learners is proportional to
the number of features, so any reduction in the fea-
ture space leads to a proportionate reduction in com-
putational time. For tasks such as countability clas-
sification with a large number of both feature values
and test instances (particularly if we are to classify
all noun types in a given corpus), this speed-up is
vital.
Our feature selection method uses a combined
feature relevance metric to estimate the best-N fea-
tures for each countability class, and then restricts
the classifier to operate over only those N features.
Feature relevance is estimated through analysis of
the correspondence between class and feature val-
ues for a given feature, through metrics including
shared variance and information gain. These indi-
vidual metrics tend to be biased toward particular
features: information gain and gain ratio, e.g., tend
to favour features of higher cardinality (White and
Liu, 1994). In order to minimise such bias, we
generate a feature ranking for each feature selec-
tion metric (based on the relative feature relevance
scores), and simply add the absolute ranks for each
feature together. By re-ranking the features in in-
creasing order of summed rank, we can generate a
generalised feature relevance ranking. We are now
in a position to prune the feature space to a pre-
determined size, by taking the best-N features in the
feature ranking.
The feature selection metrics we combine are
those implemented in TiMBL, namely: shared vari-
ance, chi-square, information gain and gain ratio.
Unsupervised classifier
In order to derive a common baseline for the dif-
ferent systems, we built an unsupervised classifier
which, for each target noun, simply checks to see
if any diagnostic (as used in the agreement-based
feature representation) was detected for each of the
countability classes; even a single occurrence of
a diagnostic is taken to be sufficient evidence for
membership in that countability class. Elementary
system combination is achieved by voting between
the three pre-processor outputs as to whether the tar-
get noun belongs to a given countability class. That
is, the target noun is classified as belonging to a
given countability class iff at least two of the pre-
processors furnish linguistic evidence for member-
ship in that class.
4.2 Training data
Training data was generated independently for the
SINGLE and SUITE classifiers. In each case, we first
extracted all countability-annotated nouns from each
of the ALT-J/E and COMLEX lexicons which are at-
tested at least 10 times in the BNC, and composed
the training data from these pre-filtered sets. In the
case of the SINGLE classifier, we simply classified
words according to the union of all countabilities
from ALT-J/E and COMLEX, resulting in the follow-
ing dataset:
Count Uncount Plural Bipart No. Freq
1 0 0 0 4068 .685
0 1 0 0 1134 .191
0 0 1 0 35 .006
0 0 0 1 10 .002
1 1 0 0 650 .110
1 0 1 0 13 .002
0 1 1 0 13 .002
0 0 1 1 5 .001
1 1 1 0 8 .001
From this, it is evident that some class combinations
(e.g. plural only+bipartite) are highly infrequent, hint-
ing at a problem with data sparseness.
For the SUITE classifier, we generate the positive
exemplars for the countable and uncountable classes
from the intersection of the COMLEX and ALT-J/E
data for that class; negative exemplars, on the other
hand, are those not annotated as belonging to that
class in either lexicon. With the plural only and
bipartite data, COMLEX cannot be used as it does
not describe these two classes. We thus took all
members of each class listed in ALT-J/E as our pos-
itive exemplars, and all remaining nouns with non-
identical singular and plural forms as negative ex-
emplars. This resulted in the following datasets:
Class Positive data Negative data
Countable 4,342 1,476
Uncountable 1,519 5,471
Plural only 84 5,639
Bipartite 35 5,639
5 Evaluation
Evaluation of the supervised classifiers was carried
out based on 10-fold stratified cross-validation over
the relevant dataset, and results presented here are
averaged over the 10 iterations. Classifier perfor-
mance is rated according to classification accuracy
(the proportion of instances classified correctly) and
F-score (? = 1). In the case of the SINGLE classifier,
the class-wise F-score is calculated by decomposing
the multiclass labels into their components. A count-
able+uncountable instance misclassified as countable,
for example, would count as a misclassification in
terms of classification accuracy, a correct classifica-
tion in the calculation of the countable F-score, and a
misclassification in the calculation of the uncountable
F-score. Note that the SINGLE classifier is run over a
different dataset to each member of the SUITE clas-
sifier, and cross-comparison of the classification ac-
curacies is not representative of the relative system
performance (classification accuracies for the SIN-
GLE classifier are given in parentheses to reinforce
this point). Classification accuracies are thus simply
used for classifier comparison within a basic classi-
fier architecture (SINGLE or SUITE), and F-score is
Classifier Accuracy F-score
Majority class .746 .855
Unsupervised .798 .879
Dist(POS,SUITE) .928 .953
Dist(POS,SINGLE) (.850) .940
Dist(chunk,SUITE) .933 .956
Dist(chunk,SINGLE) (.853) .942
Dist(RASP,SUITE) .923 .950
Dist(RASP,SINGLE) (.847) .940
Dist(AllCON,SUITE) .939 .960
Dist(AllCON,SINGLE) (.857) .944
Dist(AllMEAN,SUITE) .937 .959
Agree(Token,SUITE) .902 .936
Agree(Class,SUITE) .911 .941
Table 1: Basic results for countable nouns
Classifier Accuracy F-score
Majority class .783 (.357)
Unsupervised .342 .391
Dist(POS,SUITE) .945 .876
Dist(POS,SINGLE) (.850) .861
Dist(chunk,SUITE) .945 .876
Dist(chunk,SINGLE) (.853) .861
Dist(RASP,SUITE) .944 .872
Dist(RASP,SINGLE) (.847) .851
Dist(AllCON,SUITE) .952 .892
Dist(AllCON,SINGLE) (.857) .873
Dist(AllMEAN,SUITE) .954 .895
Agree(Token,SUITE) .923 .825
Agree(Class,SUITE) .923 .824
Table 2: Basic results for uncountable nouns
the evaluation metric of choice for overall evalua-
tion.
We present the results for two baseline systems
for each countability class: a majority-class clas-
sifier and the unsupervised method. The Majority
class system is run over the binary data used by
the SUITE classifier for the given class, and sim-
ply classifies all instances according to the most
commonly-attested class in that dataset. Irrespective
of the majority class, we calculate the F-score based
on a positive-class classifier, i.e. a classifier which
naively classifies each instance as belonging to the
given class; in the case that the positive class is not
the majority class, the F-score is given in parenthe-
ses.
The results for the different system configurations
over the four countability classes are presented in
Tables 1?4, in which the highest classification accu-
racy and F-score values for each class are presented
in boldface. The classifier Dist(AllCON,SUITE), for
example, applies the distribution-based feature rep-
resentation in a SUITE classifier configuration (i.e.
it tests for binary membership in each countability
class), using the concatenated feature vectors from
each of the tagger, chunker and RASP.
Items of note in the results are:
Classifier Accuracy F-score
Majority class .985 (.023)
Unsupervised .411 .033
Dist(POS,SUITE) .989 .558
Dist(POS,SINGLE) (.850) .479
Dist(chunk,SUITE) .990 .568
Dist(chunk,SINGLE) (.853) .495
Dist(RASP,SUITE) .989 .415
Dist(RASP,SINGLE) (.847) .360
Dist(AllCON,SUITE) .990 .582
Dist(AllCON,SINGLE) (.857) .500
Dist(AllMEAN,SUITE) .990 .575
Agree(Token,SUITE) .988 .409
Agree(Class,SUITE) .988 .401
Table 3: Basic results for plural only nouns
Classifier Accuracy F-score
Majority class .994 (.012)
Unsupervised .931 .137
Dist(POS,SUITE) .997 .752
Dist(POS,SINGLE) (.850) .857
Dist(chunk,SUITE) .997 .704
Dist(chunk,SINGLE) (.853) .865
Dist(RASP,SUITE) .997 .700
Dist(RASP,SINGLE) (.847) .798
Dist(AllCON,SUITE) .996 .723
Dist(AllCON,SINGLE) (.857) .730
Dist(AllMEAN,SUITE) .997 .710
Agree(Token,SUITE) .997 .710
Agree(Class,SUITE) .997 .695
Table 4: Basic results for bipartite nouns
? all system configurations surpass both the
majority-class baseline and unsupervised clas-
sifier in terms of F-score
? for all other than bipartite nouns, the SUITE
classifier outperforms the SINGLE classifier in
terms of F-score
? the best of the distribution-based classifiers
was, without exception, superior to the best of
the agreement-based classifiers
? chunk-based feature extraction generally pro-
duced superior performance to POS tag-based
feature extraction, which was in turn gener-
ally better than RASP-based feature extraction;
statistically significant differences in F-score
(based on the two-tailed t-test, p < .05) were
observed for both chunking and tagging over
RASP for the plural only class, and chunking
over RASP for the countable class
? for the SUITE classifier, system combination
by either concatenation (Dist(AllCON,SUITE))
or averaging over the individual feature val-
ues (Dist(AllMEAN,SUITE)) generally led to a
statistically significant improvement over each
of the individual systems for the countable
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 100  1000
 0.1
 1
 10
 100
F-s
cor
e
Ins
tan
ces
/sec
No. Features (N)
rand-N (countable)best-N (countable)
best-N (uncountable)rand-N (uncountable)
best-N (countable)best-N (uncountable)
Figure 1: Effects of feature selection
and uncountable classes,4 but there was no
statistical difference between these two archi-
tectures for any of the 4 countability classes;
for the SINGLE classifier, system combination
(Dist(AllCON,SUITE)) did not lead to a signifi-
cant performance gain
To evaluate the effects of feature selection, we
graphed the F-score value and processing time (in
instances processed per second5) over values of
N from 25 to the full feature set. We targeted
the Dist(AllCON,SUITE) system for evaluation (3852
features), and ran it over both the countable and un-
countable classes.6 We additionally carried out ran-
dom feature selection as a baseline to compare the
feature selection results against. Note that the x-axis
(N ) and right y-axis (instances/sec) are both log-
arithmic, such that the linear right-decreasing time
curves are indicative of the direct proportionality be-
tween the number of features and processing time.
The differential in F-score for the best-N configura-
tion as compared to the full feature set is statistically
insignificant for N > 100 for countable nouns and
N > 50 for uncountable nouns. That is, feature se-
lection facilitates a relative speed-up of around 30?
without a significant drop in F-score. Comparing the
results for the best-N and rand-N features, the dif-
ference in F-score was statistically significant for all
values of N < 1000. The proposed method of fea-
ture selection thus allows us to maintain the full clas-
sification potential of the feature set while enabling
4No significant performance difference was observed for:
Dist(ChunkMEAN,SUITE) vs. Dist(All?,SUITE) for countable
nouns, and Dist(POSCON,SUITE) vs. Dist(AllCON,SUITE) for
uncountable nouns.
5As evaluated on an AMD Athlon 2100+ CPU with 3GB of
memory.
6We focus exclusively on countable and uncountable nouns
here and in the remainder of supplementary evaluation as these
are by far the most populous countability classes.
Feature COUNTABLE UNCOUNTABLE
space Acc F-score Acc F-score
All features .937 .959 .954 .895
Best-200 .934 .956 .949 .884
Binary .904? .931? .930? .833?
Corpus freq .929 .954 .952 .889
Word freq .933 .956 .954 .896
Feature freq .928 .952? .934? .869?
Table 5: Results for restricted feature sets
a speedup greater than an order of magnitude, po-
tentially making the difference in practical utility for
the proposed method.
To determine the relative impact of the com-
ponent feature values on the performance of the
distribution-based feature representation, we used
the Dist(AllMEAN,SUITE) configuration to build: (a)
a classifier using a single binary value for each
unit feature, based on simple corpus occurrence (Bi-
nary); and (b) 3 separate classifiers based on each of
the corpfreq , wordfreq and featfreq features values
only (without the 2D feature cluster totals). In each
case, the total number of feature values is 206.
The results for each of these classifiers over
countable and uncountable nouns are pre-
sented in Table 5, as compared to the basic
Dist(AllMEAN,SUITE) classifier with all 1,284
features (All features) and also the best-200 features
(Best-200). Results which differ from those for
All features to a level of statistical significance are
asterisked. The binary classifiers performed signif-
icantly worse than All features for both countable
and uncountable nouns, underlining the utility of the
distribution-based feature representation. wordfreq
is marginally superior to corpfreq as a standalone
feature representation, and both of these were on
the whole slightly below the full feature set in
performance (although no significant difference was
observed). featfreq performed slightly worse again,
significantly below the level of the full feature set.
Results for the best-200 classifier were marginally
higher than those for each of the individual feature
representations in the case of the countable class,
but marginally below the results for corpfreq and
wordfreq in the case of the uncountable class. The
differences here are not statistically significant, and
additional evaluation is required to determine the
relative success of feature selection over simply
using wordfreq values, for example.
6 Discussion
There have been at least three earlier approaches
to the automatic determination of countability:
two using semantic cues and one using cor-
pora. Bond and Vatikiotis-Bateson (2002) deter-
mine a noun?s countability preferences?as de-
fined in a 5-way classification?from its se-
mantic class in the ALT-J/E lexicon, and show
that semantics predicts countability 78% of the
time. O?Hara et al (2003) implemented a sim-
ilar approach using the much larger Cyc on-
tology and achieved 89.5% accuracy, mapping
onto the 2 classes of countable and uncount-
able. Schwartz (2002) learned noun countabilities
by looking at determiner occurrence in singular
noun chunks and was able to tag 11.7% of BNC
noun tokens as countable and 39.5% as uncountable,
achieving a noun type agreement of 88% and 44%,
respectively, with the ALT-J/E lexicon. Our results
compare favourably with each of these.
In a separate evaluation, we took the best-
performing classifier (Dist(AllCON,SUITE)) and ran
it over open data, using best-500 feature selection
(Baldwin and Bond, 2003). The output of the
classifier was evaluated relative to hand-annotated
data, and the level of agreement found to be around
92.4%, which is approximately equivalent to the
agreement between COMLEX and ALT-J/E of 93.8%.
In conclusion, we have presented a plethora of
learning techniques for deep lexical acquisition from
corpus data, and applied each to the task of classify-
ing English nouns for countability. We specifically
compared two feature representations, based on rel-
ative feature occurrence and token-level classifica-
tion, and two basic classifier architectures, using a
suite of binary classifiers and a single multi-class
classifier. We also analysed the effects of comb-
ing the output of multiple pre-processors, and pre-
sented a simple feature selection method. Overall,
the best results were obtained using a distribution-
based suite of binary classifiers combining the out-
put of multiple pre-processors.
Acknowledgements
This material is based upon work supported by the National
Science Foundation under Grant No. BCS-0094638 and also
the Research Collaboration between NTT Communication Sci-
ence Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University. We would like to thank
Leonoor van der Beek, Slaven Bilac, Ann Copestake, Ivan Sag
and the three anonymous reviewers for their valuable input on
this research, and John Carroll for providing access to RASP.
References
Timothy Baldwin and Francis Bond. 2003. Learning the count-
ability of English nouns from corpus data. In Proc. of the
41st Annual Meeting of the ACL, Sapporo, Japan. (to ap-
pear).
Francis Bond and Caitlin Vatikiotis-Bateson. 2002. Using an
ontology to determine English countability. In Proc. of the
19th International Conference on Computational Linguistics
(COLING 2002), Taipei, Taiwan.
Ted Briscoe and John Carroll. 2002a. High precision extraction
of grammatical relations. In Proc. of the 19th International
Conference on Computational Linguistics (COLING 2002),
pages 134?140, Taipei, Taiwan.
Ted Briscoe and John Carroll. 2002b. Robust accurate sta-
tistical annotation of general text. In Proc. of the 3rd In-
ternational Conference on Language Resources and Evalu-
ation (LREC 2002), pages 1499?1504, Las Palmas, Canary
Islands.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguistics,
22(2):249?254.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2002. TiMBL: Tilburg memory based
learner, version 4.2, reference guide. ILK technical report
02-01.
Ralph Grishman, Catherine Macleod, and Adam Myers, 1998.
COMLEX Syntax Reference Manual. Proteus Project, NYU.
(http://nlp.cs.nyu.edu/comlex/refman.ps).
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
? effects of new methods in ALT-J/E?. In Proc. of the Third
Machine Translation Summit (MT Summit III), pages 101?
106, Washington DC, USA.
Huan Liu and Hiroshi Motoda. 1988. Feature Extraction, Con-
struction and Selection: A Data Mining Perspective. Kluwer
Academic Publishers.
Guido Minnen, John Carroll, and Darren Pearce. 2001. Ap-
plied morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?23.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Tom O?Hara, Nancy Salay, Michael Witbrock, Dave Schnei-
der, Bjoern Aldag, Stefano Bertolo, Kathy Panton, Fritz
Lehmann, Matt Smith, David Baxter, Jon Curtis, and Peter
Wagner. 2003. Inducing criteria for mass noun lexical map-
pings using the Cyc KB and its extension to WordNet. In
Proc. of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, the Netherlands.
Lane O.B. Schwartz. 2002. Corpus-based acquisition of head
noun countability features. Master?s thesis, Cambridge Uni-
versity, Cambridge, UK.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proc.
of the 4th Conference on Computational Natural Language
Learning (CoNLL-2000), Lisbon, Portugal.
Allan P. White and Wei Zhong Liu. 1994. Bias in information-
based measures in decision tree induction. Machine Learn-
ing, 15(3):321?9.
The Hinoki Treebank: Working Toward Text Understanding
Francis Bond, Sanae Fujita, Chikara Hashimoto,?
Kaname Kasahara, Shigeko Nariyama,? Eric Nichols,?
Akira Ohtani,? Takaaki Tanaka, Shigeaki Amano
NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
?Kobe Shoin Women?s University ?NAIST ?Osaka Gakuin University
{bond, sanae, kaname, takaaki, amano}@cslab.kecl.ntt.co.jp ? chashi@sils.shoin.ac.jp,
?{eric-n, shigeko}@is.naist.jp ?ohtani@utc.osaka-gu.ac.jp
Abstract
In this paper we describe the construction of a
new Japanese lexical resource: the Hinoki treebank.
The treebank is built from dictionary definition sen-
tences, and uses an HPSG based Japanese grammar
to encode the syntactic and semantic information.
We show how this treebank can be used to extract
thesaurus information from definition sentences in
a language-neutral way using minimal recursion se-
mantics.
1 Introduction
In this paper we describe the current state of a new
lexical resource: the Hinoki treebank. The motiva-
tion and initial construction was described in detail
in Bond et al (2004a). The ultimate goal of our re-
search is natural language understanding ? we aim
to create a system that can parse text into some use-
ful semantic representation. Ideally this would be
such that the output can be used to actually update
our semantic models. This is an ambitious goal, and
this paper does not present a completed solution,
but rather a road-map to the solution, with some
progress along the way.
The mid-term goal is to build a thesaurus from
dictionary definition sentences and use it to enhance
a stochastic parse ranking model that combines syn-
tactic and semantic information. In order to do this
the Hinoki project is combining syntactic annotation
with word sense tagging. This will make it possible
to test the use of similarity and/or class based ap-
proaches together with symbolic grammars and sta-
tistical models. Our aim in this is to alleviate data
sparseness. In the Penn Wall Street Journal tree-
bank (Taylor et al, 2003), for example, the words
stocks and skyrocket never appear together. How-
ever, the superordinate concepts capital (? stocks)
and move upward (? skyrocket) often do.
We are constructing the ontology from the ma-
chine readable dictionary Lexeed (Kasahara et al,
2004). This is a hand built self-contained lexicon:
it consists of headwords and their definitions for the
most familiar 28,000 words of Japanese. This set
is large enough to include most basic level words
and covers over 75% of the common word tokens
in a sample of Japanese newspaper text. In order
to make the system self sustaining we base the first
growth of our treebank on the dictionary definition
sentences themselves. We then train a statistical
model on the treebank and parse the entire lexicon.
From this we induce a thesaurus. We are currently
tagging the definition sentences with senses. We will
then use this information and the thesaurus to build
a model that combines syntactic and semantic in-
formation. We will also produce a richer ontology
? for example extracting selectional preferences. In
the last phase, we will look at ways of extending our
lexicon and ontology to less familiar words.
In this paper we present the results from treebank-
ing 38,900 dictionary sentences. We also highlight
two uses of the treebank: building the statistical
models and inducing the thesaurus.
2 The Lexeed Semantic Database of
Japanese
The Lexeed Semantic Database of Japanese consists
of all Japanese words with a familiarity greater than
or equal to five on a seven point scale (Kasahara et
al., 2004). This gives 28,000 words in all, with 46,347
different senses. Definition sentences for these sen-
tences were rewritten to use only the 28,000 familiar
words (and some function words). The defining vo-
cabulary is actually 16,900 different words (60% of
all possible words). An example entry for first two
senses of the word   doraiba? ?driver? is
given in Figure 1, with English glosses added (un-
derlined features are those added by Hinoki).
3 The Hinoki Treebank
The structure of our treebank is inspired by the Red-
woods treebank of English in which utterances are
parsed and the annotator selects the best parse from
the full analyses derived by the grammar (Oepen et
al., 2002). We had four main reasons for selecting
this approach. The first was that we wanted to de-
velop a precise broad-coverage grammar in tandem
with the treebank, as part of our research into nat-
ural language understanding. Treebanking the out-
??
?
?
?
?
?
?
?
?
?
?
?
?
Index   doraiba?
POS noun Lexical-type noun-lex
Familiarity 6.5 [1?7]
Sense 1
?
?
Definition 	
 /  /  /  / A Method of Creating New Bilingual Valency Entries
using Alternations
Sanae Fujita Francis Bond
{sanae, bond}@cslab.kecl.ntt.co.jp
NTT Machine Translation Research Group
NTT Communication Science Laboratories
Nippon Telephone and Telegraph Corporation
Abstract
We present a method that uses alternation data
to add new entries to an existing bilingual valency
lexicon. If the existing lexicon has only one half of
the alternation, then our method constructs the
other half. The new entries have detailed infor-
mation about argument structure and selectional
restrictions. In this paper we focus on one class
of alternations, but our method is applicable to
any alternation. We were able to increase the
coverage of the causative alternation to 98%, and
the new entries gave an overall improvement in
translation quality of 32%.
1 Introduction
Recently, deep linguistic processing, which aims
to provide a useful semantic representation, has
become the focus of more research, as parsing
technologies improve in both speed and robust-
ness (Uszkoreit, 2002). In particular, machine
translation systems still mainly rely on large
hand-crafted lexicons. The knowledge acquisition
bottleneck, however, remains: precise grammars
need information-rich lexicons, such as valency
dictionaries, which are costly to build and extend.
In this paper, we present a method of adding new
entries to an existing bilingual valency dictionary,
using information about verbal alternations.
The classic approach to acquiring lexical infor-
mation is to build resources by hand. This pro-
duces useful resources but is expensive. This is
still the approach taken by large projects such
as FrameNet (Baker et al, 1998) or OntoSem.
Therefore, there is a need to extend these hand-
made resources quickly and economically. An-
other approach is to attempt to learn informa-
tion from corpora. There has been much research
based on this, but due to the inevitable errors,
there are few examples of lexicons being con-
structed fully automatically. Korhonen (2002)
reports that the ceiling on the performance of
mono-lingual subcategorization acquisition from
corpora is generally around 80%, a level that
still requires manual intervention. Yet another
approach is to combine knowledge sources: for
example to build a lexicon and then try to ex-
tend it using corpus data or to enrich mono-
lingual data using multilingual lexicons (Fujita
and Bond, 2002).
The aim of this research is not to create a
lexicon from scratch, but rather to add further
entries to an existing lexicon. We propose a
method of acquiring detailed information about
predicates, including argument structure, seman-
tic restrictions on the arguments and transla-
tion equivalents. It combines two heterogeneous
knowledge sources: an existing bilingual valency
lexicon (the seed lexicon), and information about
verbal alternations.
Most verbs have more than one possible argu-
ment structure (subcat). These can be regular-
ized into pairs of alternations, where two argu-
ment structures link similar semantic roles into
different subcats. Levin (1993) has identified over
80 alternation types for English, and these have
been extended to cover 4,432 verbs in 492 classes
(Dorr, 1997). In this paper, we will consider al-
ternations between transitive (Vt) and intransi-
tive (Vi) uses of verbs, where the subject of the
intransitive verb (S) is the same as the object of
the transitive verb (O) (e.g. the acid dissolved
the metal ? the metal dissolved (in the acid))
(Levin, 1993, 26?33)). We call the subject of the
transitive verb A (ergative) and this alternation
the S=O alternation.
Figure 1 shows a simplified example of an alter-
nating pair in a bilingual valency dictionary (the
valency lexicon from the Japanese-to-English ma-
chine translation system ALT-J/E (Ikehara et
al., 1991)). This includes the subcategorization
frame and selectional restrictions. As shown in
Figure 1, Japanese, unlike English, typically mor-
phologically marks the transitivity alternation.
We chose the S=O alternation because it is one
J-E Entry: 302116
S   N1:?stuff?  nom
X  N3:?stuff?  dat
 Vi 	 tokeru ?dissolve?
S   N1 subject
 Vi dissolve
X  PP in N3
J-E Entry: 508661
A   N1:?people, artifact?  nom
O  N2 ?stuff? 
 acc
X  N3:?inanimate?  dat
 Vi  toku ?dissolve?
A   N1 subject
 Vt dissolve
O  N2 direct object
X
 PP in N3
Figure 1: Vi 	 tokeru ?dissolve? ? Vt  toku ?dissolve?
of the most common types of alternations, mak-
ing up 34% of those discovered by Bond et al
(2002) and has been extensively studied. The
method we present, however, can be used with
any alternation for which lists of alternating verbs
exist.
2 Resources
We use two main resources in this paper: (1) a
seed lexicon of high quality hand-made valency
entries; and (2) lists of verbs that undergo one or
more S=O alternations.
The alternation list includes 449 native
Japanese verbs that take the S=O alterna-
tion, based on data from Jacobsen (1981), Bul-
lock (1999) and the Japanese/English dictionary
EDICT (Breen, 1995). Each entry consists of a
pair of Japanese verbs with one or more English
glosses. Expanding out the English results in 839
Japanese-English pairs in all. Some examples are
given in Table 1.
Intransitive Transitive
Ja En Ja En
 tokeru dissolve  toku dissolve
 naku cry  nakasu make cry
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 10?17,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Multilingual Ontology Acquisition from Multiple MRDs
Eric Nichols?, Francis Bond?, Takaaki Tanaka?, Sanae Fujita?, Dan Flickinger ?
? Nara Inst. of Science and Technology ? NTT Communication Science Labs ? Stanford University
Grad. School of Information Science Natural Language Research Group CSLI
Nara, Japan Keihanna, Japan Stanford, CA
eric-n@is.naist.jp {bond,takaaki,sanae}@cslab.kecl.ntt.co.jp danf@csli.stanford.edu
Abstract
In this paper, we outline the develop-
ment of a system that automatically con-
structs ontologies by extracting knowledge
from dictionary definition sentences us-
ing Robust Minimal Recursion Semantics
(RMRS). Combining deep and shallow
parsing resource through the common for-
malism of RMRS allows us to extract on-
tological relations in greater quantity and
quality than possible with any of the meth-
ods independently. Using this method,
we construct ontologies from two differ-
ent Japanese lexicons and one English lex-
icon. We then link them to existing, hand-
crafted ontologies, aligning them at the
word-sense level. This alignment provides
a representative evaluation of the qual-
ity of the relations being extracted. We
present the results of this ontology con-
struction and discuss how our system was
designed to handle multiple lexicons and
languages.
1 Introduction
Automatic methods of ontology acquisition have a
long history in the field of natural language pro-
cessing. The information contained in ontolo-
gies is important for a number of tasks, for ex-
ample word sense disambiguation, question an-
swering and machine translation. In this paper,
we present the results of experiments conducted
in automatic ontological acquisition over two lan-
guages, English and Japanese, and from three dif-
ferent machine-readable dictionaries.
Useful semantic relations can be extracted from
large corpora using relatively simple patterns (e.g.,
(Pantel et al, 2004)). While large corpora often
contain information not found in lexicons, even a
very large corpus may not include all the familiar
words of a language, let alne those words occur-
ring in useful patterns (Amano and Kondo, 1999).
Therefore it makes sense to also extract data from
machine readable dictionaries (MRDs).
There is a great deal of work on the creation
of ontologies from machine readable dictionaries
(a good summary is (Wilkes et al, 1996)), mainly
for English. Recently, there has also been inter-
est in Japanese (Tokunaga et al, 2001; Nichols
et al, 2005). Most approaches use either a special-
ized parser or a set of regular expressions tuned
to a particular dictionary, often with hundreds of
rules. Agirre et al (2000) extracted taxonomic
relations from a Basque dictionary with high ac-
curacy using Constraint Grammar together with
hand-crafted rules. However, such a system is lim-
ited to one language, and it has yet to be seen
how the rules will scale when deeper semantic re-
lations are extracted. In comparison, as we will
demonstrate, our system produces comparable re-
sults while the framework is immediately applica-
ble to any language with the resources to produce
RMRS. Advances in the state-of-the-art in pars-
ing have made it practical to use deep processing
systems that produce rich syntactic and semantic
analyses to parse lexicons. This high level of se-
mantic information makes it easy to identify the
relations between words that make up an ontol-
ogy. Such an approach was taken by the MindNet
project (Richardson et al, 1998). However, deep
parsing systems often suffer from small lexicons
and large amounts of parse ambiguity, making it
difficult to apply this knowledge broadly.
Our ontology extraction system uses Robust
Minimal Recursion Semantics (RMRS), a formal-
ism that provides a high level of detail while, at
the same time, allowing for the flexibility of un-
derspecification. RMRS encodes syntactic infor-
mation in a general enough manner to make pro-
cessing of and extraction from syntactic phenom-
ena including coordination, relative clause analy-
10
sis and the treatment of argument structure from
verbs and verbal nouns. It provides a common for-
mat for naming semantic relations, allowing them
to be generalized over languages. Because of this,
we are able to extend our system to cover new lan-
guages that have RMRS resourses available with
a minimal amount of effort. The underspecifica-
tion mechanism in RMRS makes it possible for us
to produce input that is compatible with our sys-
tem from a variety of different parsers. By select-
ing parsers of various different levels of robustness
and informativeness, we avoid the coverage prob-
lem that is classically associated with approaches
using deep-processing; using heterogeneous pars-
ing resources maximizes the quality and quantity
of ontological relations extracted. Currently, our
system uses input from parsers from three lev-
els: with morphological analyzers the shallowest,
parsers using Head-driven Phrase Structure Gram-
mars (HPSG) the deepest and dependency parsers
providing a middle ground.
Our system was initially developed for one
Japanese dictionary (Lexeed). The use of the ab-
stract formalism, RMRS, made it easy to extend to
a different Japanese lexicon (Iwanami) and even a
lexicon in a different language (GCIDE).
Section 2 provides a description of RMRS and
the tools used by our system. The ontological ac-
quisition system is presented in Section 3. The re-
sults of evaluating our ontologies by comparison
with existing resources are given in Section 4. We
discuss our findings in Section 5.
2 Resources
2.1 The Lexeed Semantic Database of
Japanese
The Lexeed Semantic Database of Japanese is a
machine readable dictionary that covers the most
familiar open class words in Japanese as measured
by a series of psycholinguistic experiments (Kasa-
hara et al, 2004). Lexeed consists of all open class
words with a familiarity greater than or equal to
five on a scale of one to seven. This gives 28,000
words divided into 46,000 senses and defined with
75,000 definition sentences. All definition sen-
tences and example sentences have been rewritten
to use only the 28,000 familiar open class words.
The definition and example sentences have been
treebanked with the JACY grammar (? 2.4.2).
2.2 The Iwanami Dictionary of Japanese
The Iwanami Kokugo Jiten (Iwanami) (Nishio
et al, 1994) is a concise Japanese dictionary.
A machine tractable version was made avail-
able by the Real World Computing Project for
the SENSEVAL-2 Japanese lexical task (Shirai,
2003). Iwanami has 60,321 headwords and 85,870
word senses. Each sense in the dictionary con-
sists of a sense ID and morphological information
(word segmentation, POS tag, base form and read-
ing, all manually post-edited).
2.3 The Gnu Contemporary International
Dictionary of English
The GNU Collaborative International Dictionary
of English (GCIDE) is a freely available dic-
tionary of English based on Webster?s Revised
Unabridged Dictionary (published in 1913), and
supplemented with entries from WordNet and ad-
ditional submissions from users. It currently
contains over 148,000 definitions. The version
used in this research is formatted in XML and is
available for download from www.ibiblio.org/
webster/.
We arranged the headwords by frequency and
segmented their definition sentences into sub-
sentences by tokenizing on semicolons (;). This
produced a total of 397,460 pairs of headwords
and sub-sentences, for an average of slightly less
than four sub-sentences per definition sentence.
For corpus data, we selected the first 100,000 def-
inition sub-sentences of the headwords with the
highest frequency. This subset of definition sen-
tences contains 12,440 headwords with 36,313
senses, covering approximately 25% of the defi-
nition sentences in the GCIDE. The GCIDE has
the most polysemy of the lexicons used in this re-
search. It averages over 3 senses per word defined
in comparison to Lexeed and Iwanami which both
have less than 2.
2.4 Parsing Resources
We used Robust Minimal Recursion Semantics
(RMRS) designed as part of the Deep Thought
project (Callmeier et al, 2004) as the formal-
ism for our ontological relation extraction en-
gine. We used deep-processing tools from the
Deep Linguistic Processing with HPSG Initiative
(DELPH-IN: http://www.delph-in.net/) as
well as medium- and shallow-processing tools for
Japanese processing (the morphological analyzer
11
ChaSen and the dependency parser CaboCha)
from the Matsumoto Laboratory.
2.4.1 Robust Minimal Recursion Semantics
Robust Minimal Recursion Semantics is a form
of flat semantics which is designed to allow deep
and shallow processing to use a compatible se-
mantic representation, with fine-grained atomic
components of semantic content so shallow meth-
ods can contribute just what they know, yet with
enough expressive power for rich semantic content
including generalized quantifiers (Frank, 2004).
The architecture of the representation is based on
Minimal Recursion Semantics (Copestake et al,
2005), including a bag of labeled elementary pred-
icates (EPs) and their arguments, a list of scoping
constraints which enable scope underspecification,
and a handle that provides a hook into the repre-
sentation.
The representation can be underspecified in
three ways: relationships can be omitted (such
as quantifiers, messages, conjunctions and so on);
predicate-argument relations can be omitted; and
predicate names can be simplified. Predicate
names are defined in such a way as to be as
compatible (predictable) as possible among differ-
ent analysis engines, using a lemma pos subsense
naming convention, where the subsense is optional
and the part-of-speech (pos) for coarse-grained
sense distinctions is drawn from a small set of gen-
eral types (noun, verb, sahen (verbal noun), . . . ).
The predicate unten s (?U unten ?drive?), for
example, is less specific than unten s 2 and thus
subsumes it. In order to simplify the combination
of different analyses, the EPs are indexed to the
corresponding character positions in the original
input sentence.
Examples of deep and shallow results for the
same sentence ?k?U2d0 jido?sha wo
unten suru hito ?a person who drives a car (lit:
car-ACC drive do person)? are given in Figures 1
and 2 (omitting the indexing). Real predicates are
prefixed by an under-bar ( ). The deep parse gives
information about the scope, message types and
argument structure, while the shallow parse gives
little more than a list of real and grammatical pred-
icates with a hook.
2.4.2 Deep Parsers (JACY, ERG and PET)
For both Japanese and English, we used the PET
System for the high-efficiency processing of typed
feature structures (Callmeier, 2000). For Japanese,
we used JACY (Siegel, 2000), for English we used
the English Resource Grammar (ERG: Flickinger
2000).1
JACY The JACY grammar is an HPSG-based
grammar of Japanese which originates from work
done in the Verbmobil project (Siegel, 2000) on
machine translation of spoken dialogues in the do-
main of travel planning. It has since been ex-
tended to accommodate written Japanese and new
domains (such as electronic commerce customer
email and machine readable dictionaries).
The grammar implementation is based on a sys-
tem of types. There are around 900 lexical types
that define the syntactic, semantic and pragmatic
properties of the Japanese words, and 188 types
that define the properties of phrases and lexical
rules. The grammar includes 50 lexical rules
for inflectional and derivational morphology and
47 phrase structure rules. The lexicon contains
around 36,000 lexemes.
The English Resource Grammar (ERG) The
English Resource Grammar (ERG: (Flickinger,
2000)) is a broad-coverage, linguistically precise
grammar of English, developed within the Head-
driven Phrase Structure Grammar (HPSG) frame-
work, and designed for both parsing and gen-
eration. It was also originally launched within
the Verbmobil (Wahlster, 2000) spoken language
machine translation project for the particular do-
mains of meeting scheduling and travel planning.
The ERG has since been substantially extended in
both grammatical and lexical coverage, reaching
80-90% coverage of sizeable corpora in two ad-
ditional domains: electronic commerce customer
email and tourism brochures.
The grammar includes a hand-built lexicon of
23,000 lemmas instantiating 850 lexical types, a
highly schematic set of 150 grammar rules, and a
set of 40 lexical rules, all organized in a rich multi-
ple inheritance hierarchy of some 3000 typed fea-
ture structures. Like other DELPH-IN grammars,
the ERG can be processed by several parsers and
generators, including the LKB (Copestake, 2002)
and PET (Callmeier, 2000). Each successful ERG
analysis of a sentence or fragment includes a fine-
grained semantic representation in MRS.
For the task of parsing the dictionary defini-
tions in GCIDE (the GNU Collaborative Interna-
1Both grammars, the LKB and PET are available at
<http://www.delph-in.net/>.
12
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TEXT ?k?U2d0
TOP h1
RELS
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
proposition m rel
LBL h1
ARG0 e2 tense=present
MARG h3
?
?
?
?
?
jidousha n rel
LBL h4
ARG0 x5
?
?
?
?
?
?
?
udef rel
LBL h6
ARG0 x5
RSTR h7
BODY h8
?
?
?
?
?
?
?
?
?
?
unten s rel
LBL h9
ARG0 e11 tense=present
ARG1 x10
ARG2 x5
?
?
?
?
?
?
?
hito n rel
LBL h12
ARG0 x10
?
?
?
?
?
?
?
udef rel
LBL h13
ARG0 x10
RSTR h14
BODY h15
?
?
?
?
?
?
?
?
proposition m rel
LBL h10001
ARG0 e11 tense=present
MARG h16
?
?
?
?
?
?
unknown rel
LBL h17
ARG0 e2 tense=present
ARG x10
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HCONS {h3 qeq h17,h7 qeq h4,h14 qeq h12,h16 qeq h9}
ING {h12 ing h10001}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: RMRS for the Sense 2 of doraiba- ?driver? (Cabocha/JACY)
?
?
?
?
?
TEXT ?k?U2d0
TOP h9
RELS
?
?
?
?
?
jidousha n rel
LBL h1
ARG0 x2
?
?
?
?
o p rel
LBL h3
ARG0 u4
?
?
?
?
unten s rel
LBL h5
ARG0 e6
?
?
?
?
suru v rel
LBL h7
ARG0 x8
?
?
?
?
hito n rel
LBL h9
ARG0 x10
?
?
?
?
?
?
?
?
?
?
Figure 2: RMRS for the Sense 2 of doraiba- ?driver? (ChaSen)
tional Dictionary of English; see below), the ERG
was minimally extended to include two additional
fragment rules, for gap-containing VPs and PPs
(idiosyncratic to this domain), and additional lex-
ical entries were manually added for all missing
words in the alphabetically first 10,000 definition
sentences.
These first 10,000 sentences were parsed and
then manually tree-banked to provide the train-
ing material for constructing the stochastic model
used for best-only parsing of the rest of the defini-
tion sentences. Using POS-based unknown-word
guessing for missing lexical entries, MRSes were
obtained for about 75% of the first 100,000 defini-
tion sentences.
2.4.3 Medium Parser (CaboCha-RMRS)
For Japanese, we produce RMRS from the de-
pendency parser Cabocha (Kudo and Matsumoto,
2002). The method is similar to that of Spreyer
and Frank (2005), who produce RMRS from de-
tailed German dependencies. CaboCha provides
fairly minimal dependencies: there are three links
(dependent, parallel, apposition) and they link
base phrases (Japanese bunsetsu), marked with
the syntactic and semantic head. The CaboCha-
RMRS parser uses this information, along with
heuristics based on the parts-of-speech, to produce
underspecified RMRSs. CaboCha-RMRS is ca-
pable of making use of HPSG resources, includ-
ing verbal case frames, to further enrich its out-
put. This allows it to produce RMRS that ap-
proaches the granularity of the analyses given by
HPSG parsers. Indeed, CaboCha-RMRS and JACY
give identical parses for the example sentence in
Figure 1. One of our motivations in including a
medium parser in our system is to extract more re-
lations that require special processing; the flexibil-
ity of CaboCha-RMRS and the RMRS formalism
make this possible.
2.4.4 Shallow Parser (ChaSen-RMRS)
The part-of-speech tagger, ChaSen (Matsumoto
et al, 2000) was used for shallow processing of
Japanese. Predicate names were produced by
transliterating the pronunciation field and map-
ping the part-of-speech codes to the RMRS super
types. The part-of-speech codes were also used
to judge whether predicates were real or gram-
matical. Since Japanese is a head-final language,
the hook value was set to be the handle of the
right-most real predicate. This is easy to do for
Japanese, but difficult for English.
3 Ontology Construction
We adopt the ontological relation extraction algo-
rithm used by Nichols et al (2005). Its goal is to
identify the semantic head(s) of a dictionary def-
inition sentence ? the relation(s) that best sum-
marize it. The algorithm does this by traversing
the RMRS structure of a given definition sentence
starting at the HOOK (the highest-scoping seman-
tic relationship) and following its argument struc-
ture. When the algorithm can proceed no fur-
ther, it returns the a tuple consisting of the def-
inition word and the word identified by the se-
13
mantic relation where the algorithm halted. Our
extended algorithm has the following characteris-
tics: sentences with only one content-bearing re-
lation are assumed to identify a synonym; spe-
cial relation processing (? 3.1) is used to gather
meta-information and identify ontological rela-
tions; processing of coordination allows for ex-
traction of multiple ontological relations; filtering
by part-of-speech screens out unlikely relations
(? 3.2).
3.1 Special Relations
Occasionally, relations which provide ontological
meta-information, such as the specification of do-
main or temporal expressions, or which help iden-
tify the type of ontological relation present are en-
countered. Nichols et al (2005) identified these
as special relations. We use a small number of
rules to determine where the semantic head is and
what ontological relation should be extracted. A
sample of the special relations are listed in Ta-
ble 1. This technique follows in a long tradition of
special treatment of certain words that have been
shown to be particularly relevant to the task of
ontology construction or which are semantically
content-free. These words or relations have also
be referred to as ?empty heads?, ?function nouns?,
or ?relators? in the literature (Wilkes et al, 1996).
Our approach generalizes the treatment of these
special relations to rules that are portable for any
RMRS (modulo the language specific predicate
names) giving it portability that cannot be found
in approaches that use regular expressions or spe-
cialized parsers.
Special Predicate (s) Ontological
Japanese English Relation
isshu, hitotsu form, kind, one hypernym
ryaku(shou) abbreviation abbreviation
bubun, ichibu part, peice meronym
meishou name name
keishou ?polite name for? name:honorific
zokushou ?slang for? name:slang
Table 1: Special predicates and their associated
ontological relations
Augmenting the system to work on English def-
inition sentence simply entailed writing rules to
handle special relations that occur in English. Our
system currently has 26 rules for Japanese and 50
rules for English. These rules provide process-
ing of relations like those found in Table 1, and
they also handle processing of coordinate struc-
tures, such as noun phrases joined together with
conjunctions such as and, or, and punctuation.
3.2 Filtering by Part-of-Speech
One of the problems encountered in expanding the
approach in Nichols et al (2005) to handle En-
glish dictionaries is that many of the definition
sentences have a semantic head with a part-of-
speech different than that of the definition word.
We found that differing parts-of-speech often indi-
cated an undesirable ontological relation. One rea-
son such relations can be extracted is when a sen-
tence with a non-defining role, for example indi-
cating usage, is encountered. Definition sentence
for non-content-bearing words such as of or the
also pose problems for extraction.
We avoid these problems by filtering by parts-
of-speech twice in the extraction process. First, we
select candidate sentences for extraction by veri-
fying that the definition word has a content word
POS (i.e. adjective, adverb, noun, or verb). Fi-
nally, before we extract any ontological relation,
we make sure that the definition word and the se-
mantic head are in compatible POS classes.
While adopting this strategy does reduce the
number of total ontological relations that we ac-
quire, it increases their reliability. The addition of
a medium parser gives us more RMRS structures
to extract from, which helps compensate for any
loss in number.
4 Results and Evaluation
We summarize the relationships acquired in Ta-
ble 2. The columns specify source dictionary
and parsing method while the rows show the rela-
tion type. These counts represent the total num-
ber of relations extracted for each source and
method combination. The majority of relations
extracted are synonyms and hypernyms; however,
some higher-level relations such as meronym and
abbreviation are also acquired. It should also
be noted that both the medium and deep meth-
ods were able to extract a fair number of spe-
cial relations. In many cases, the medium method
even extracted more special relations than the deep
method. This is yet another indication of the
flexibility of dependency parsing. Altogether, we
extracted 105,613 unique relations from Lexeed
(for 46,000 senses), 183,927 unique relations from
Iwanami (for 85,870 senses), and 65,593 unique
relations from GCIDE (for 36,313 senses). As can
be expected, a general pattern in our results is that
the shallow method extracts the most relations in
total followed by the medium method, and finally
14
Relation Lexeed Iwanami GCIDE
Shallow Medium Deep Shallow Medium Deep Deep
hypernym 47,549 43,006 41,553 113,120 113,433 66,713 40,583
synonym 12,692 13,126 9,114 31,682 32,261 18,080 21,643
abbreviation 340 429 1,533 739
meronym 235 189 395 202 472
name 100 89 271 140
Table 2: Results of Ontology Extraction
the deep method.
4.1 Verification with Hand-crafted
Ontologies
Because we are interested in comparing lexical se-
mantics across languages, we compared the ex-
tracted ontology with resources in both the same
and different languages.
For Japanese we verified our results by com-
paring the hypernym links to the manually con-
structed Japanese ontology Goi-Taikei (GT). It is
a hierarchy of 2,710 semantic classes, defined for
over 264,312 nouns Ikehara et al (1997). The se-
mantic classes are mostly defined for nouns (and
verbal nouns), although there is some information
for verbs and adjectives. For English, we com-
pared relations to WordNet 2.0 (Fellbaum, 1998).
Comparison for hypernyms is done as follows:
look up the semantic class or synset C for both the
headword (wi) and genus term(s) (wg). If at least
one of the index word?s classes is subsumed by at
least one of the genus? classes, then we consider
the relationship confirmed (1).
?(ch,cg) : {ch ? cg;ch ?C(wh);cg ?C(wg)} (1)
To test cross-linguistically, we looked up the
headwords in a translation lexicon (ALT-J/E (Ike-
hara et al, 1991) and EDICT (Breen, 2004)) and
then did the confirmation on the set of translations
ci ? C(T (wi)). Although looking up the transla-
tion adds noise, the additional filter of the relation-
ship triple effectively filters it out again.
The total figures given in Table 3 do not match
the totals given in Table 2. These totals represent
the number of relations where both the definition
word and semantic head were found in at least one
of the ontologies being used in this comparison.
By comparing these numbers to the totals given
in Section 4, we can get an idea of the coverage
of the ontologies being used in comparison. Lex-
eed has a coverage of approx. 55.74% ( 58,867105,613 ),
with Iwanami the lowest at 48.20% ( 88,662183,927 ), and
GCIDE the highest at 69.85% (45,81465,593 ). It is clear
that there are a lot of relations in each lexicon that
are not covered by the hand-crafted ontologies.
This demonstrates that machine-readable dictio-
naries are still a valuable resource for constructing
ontologies.
4.1.1 Lexeed
Our results using JACY achieve a confirmation
rate of 66.84% for nouns only and 60.67% over-
all (Table 3). This is an improvement over both
Tokunaga et al (2001), who reported 61.4% for
nouns only, and Nichols et al (2005) who reported
63.31% for nouns and 57.74% overall. We also
achieve an impressive 33,333 confirmed relations
for a rate of 56.62% overall. It is important to
note that our total counts include all unique re-
lations regardless of source, unlike Nichols et al
(2005) who take only the relation from the deepest
source whenever multiple relations are extracted.
It is interesting to note that shallow processing out
performs medium with 22,540 verified relations
(59.40%) compared to 21,806 (57.76%). This
would seem to suggest that for the simplest task of
retrieving hyperynms and synonyms, more infor-
mation than that is not necessary. However, since
medium and deep parsing obtain relations not cov-
ered by shallow parsing and can extract special re-
lations, a task that cannot be performed without
syntactic information, it is beneficial to use them
as well.
Agirre et al (2000) reported an error rate of
2.8% in a hand-evaluation of the semantic rela-
tions they automatically extracted from a machine-
readable Basque dictionary. In a similar hand-
evaluation of a stratified sampling of relations ex-
tracted from Lexeed, we achieved an error rate
of 9.2%, demonstrating that our method is also
highly accurate (Nichols et al, 2005).
4.2 Iwanami
Iwanami?s verification results are similar to Lex-
eed?s (Table 3). There are on average around 3%
more verifications and a total of almost 20,000
more verified relations extracted. It is particu-
larly interesting to note that deep processing per-
15
Confirmed Relations in Lexeed
Method / Relation hypernym synonym Total
Shallow 58.55 % ( 16585 / 28328 ) 61.93 % ( 5955 / 9615 ) 59.40 % ( 22540 / 37943 )
Medium 55.97 % ( 15431 / 27570 ) 62.61 % ( 6375 / 10182 ) 57.76 % ( 21806 / 37752 )
Deep 54.78 % ( 4954 / 9043 ) 67.76 % ( 5098 / 7524 ) 60.67 % ( 10052 / 16567 )
All 55.22 % ( 23802 / 43102 ) 60.46 % ( 9531 / 15765 ) 56.62 % ( 33333 / 58867 )
Confirmed Relations in Iwanami
Method / Relation hypernym synonym Total
Shallow 61.20 % ( 35208 / 57533 ) 63.57 % ( 11362 / 17872 ) 61.76 % ( 46570 / 75405 )
Medium 60.69 % ( 35621 / 58698 ) 62.86 % ( 11037 / 17557 ) 61.19 % ( 46658 / 76255 )
Deep 63.59 % ( 22936 / 36068 ) 64.44 % ( 8395 / 13027 ) 63.82 % ( 31331 / 49095 )
All 59.36 % ( 40179 / 67689 ) 61.66 % ( 12931 / 20973 ) 59.90 % ( 53110 / 88662 )
Confirmed Relations in GCIDE
POS / Relation hypernym synonym Total
Adjective 2.88 % ( 37 / 1283 ) 16.77 % ( 705 / 4203 ) 13.53 % ( 742 / 5486 )
Noun 57.60 % ( 7518 / 13053 ) 50.71 % ( 3522 / 6945 ) 55.21 % ( 11040 / 19998 )
Verb 24.22 % ( 3006 / 12411 ) 21.40 % ( 1695 / 7919 ) 23.12 % ( 4701 / 20330 )
Total 39.48 % ( 10561 / 26747 ) 31.06 % ( 5922 / 19067 ) 35.98 % ( 16483 / 45814 )
Table 3: Confirmed Relations, measured against GT and WordNet
forms better here than on Lexeed (63.82% vs
60.67%), even though the grammar was developed
and tested on Lexeed. There are two reasons for
this: The first is that the process of rewriting Lex-
eed to use only familiar words actually makes the
sentences harder to parse. The second is that the
less familiar words in Iwanami have fewer senses,
and easier to parse definition sentences. In any
case, the results support our claims that our onto-
logical relation extraction system is easily adapt-
able to new lexicons.
4.3 GCIDE
At first glance, it would seem that GCIDE has
the most disappointing of the verification results
with overall verification of not even 36% and only
16,483 relations confirmed. However, on closer
inspection one can see that noun hypernyms are a
respectable 57.60% with over 55% for all nouns.
These figures are comparable with the results we
are obtaining with the other lexicons. One should
also bear in mind that the definitions found in
GCIDE can be archaic; after all this dictionary
was first published in 1913. This could be one
cause of parsing errors for ERG. Despite these ob-
stacles, we feel that GCIDE has a lot of poten-
tial for ontological acquisition. A dictionary of
its size and coverage will most likely contain rela-
tions that may not be represented in other sources.
One only has to look at the definition of ??
{? ?driver?/driver to confirm this; GT has
two senses (?screwdriver? and ?vehicle operator?)
Lexeed and Iwanami have 3 senses each (adding
?golf club?), and WordNet has 5 (including ?soft-
ware driver?), but GCIDE has 6, not including
?software driver? but including spanker ?a kind of
sail?. It should be beneficial to propagate these
different senses across ontologies.
5 Discussion and Future Work
We were able to successfully combine deep pro-
cessing of various levels of depth in order to
extract ontological information from lexical re-
sources. We showed that, by using a well defined
semantic representation, the extraction can be gen-
eralized so much that it can be used on very differ-
ent dictionaries from different languages. This is
an improvement on the common approach to using
more and more detailed regular expressions (e.g.
Tokunaga et al (2001)). Although this provides a
quick start, the results are not generally reusable.
In comparison, the shallower RMRS engines are
immediately useful for a variety of other tasks.
However, because the hook is the only syntactic
information returned by the shallow parser, onto-
logical relation extraction is essentially performed
by this hook-identifying heuristic. While this is
sufficient for a large number of sentences, it is not
possible to process special relations with the shal-
low parser since none of the arguments are linked
with the predicates to which they belong. Thus, as
Table 2 shows, our shallow parser is only capable
of retrieving hypernyms and synonyms. It is im-
portant to extract a variety of semantic relations in
order to form a useful ontology. This is one of the
reasons why we use a combination of parsers of
16
different analytic levels rather than depending on
a single resource.
The other innovation of our approach is the
cross-lingual evaluation. As a by-product of
the evaluation we enhance the existing resources
(such as GT or WordNet) by linking them, so
that information can be shared between them. In
this way we can use the cross-lingual links to fill
gaps in the monolingual resources. GT and Word-
Net both lack complete cover - over half the rela-
tions were confirmed with only one resource. This
shows that the machine readable dictionary is a
useful source of these relations.
6 Conclusion
In this paper, we presented the results of experi-
ments conducted in automatic ontological acqui-
sition over two languages, English and Japanese,
and from three different machine-readable dictio-
naries. Our system is unique in combining parsers
of various levels of analysis to generate its input
semantic structures. The system is language ag-
nostic and we give results for both Japanese and
English MRDs. Finally, we presented evaluation
of the ontologies constructed by comparing them
with existing hand-crafted English and Japanese
ontologies.
References
Eneko Agirre, Olatz Ansa, Xabier Arregi, Xabier Artola,
Arantza Diaz de Ilarraza, Mikel Lersundi, David Martinez,
Kepa Sarasola, and Ruben Urizar. 2000. Extraction of
semantic relations from a Basque monolingual dictionary
using Constraint Grammar. In EURALEX 2000.
Shigeaki Amano and Tadahisa Kondo. 1999. Nihongo-no
Goi-Tokusei (Lexical properties of Japanese). Sanseido.
J. W. Breen. 2004. JMDict: a Japanese-multilingual dictio-
nary. In Coling 2004 Workshop on Multilingual Linguistic
Resources, pages 71?78. Geneva.
Ulrich Callmeier. 2000. PET - a platform for experimenta-
tion with efficient HPSG processing techniques. Natural
Language Engineering, 6(1):99?108.
Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, and
Melanie Siegel. 2004. The DeepThought core architecture
framework. In Proceedings of LREC-2004, volume IV.
Lisbon.
Ann Copestake. 2002. Implementing Typed Feature Structure
Grammars. CSLI Publications.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281?
332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28. (Special Issue on Efficient Processing with
HPSG).
Anette Frank. 2004. Constraint-based RMRS construction
from shallow grammars. In 20th International Con-
ference on Computational Linguistics: COLING-2004,
pages 1269?1272. Geneva.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
? effects of new methods in ALT-J/E ?. In Third Ma-
chine Translation Summit: MT Summit III, pages 101?
106. Washington DC. (http://xxx.lanl.gov/abs/
cmp-lg/9510008).
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lex-
icon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In CoNLL 2002:
Proceedings of the 6th Conference on Natural Language
Learning 2002 (COLING 2002 Post-Conference Work-
shops), pages 63?69. Taipei.
Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda,
and Asahara. 2000. Nihongo Keitaiso Kaiseki System:
Chasen. http://chasen.naist.jp/hiki/ChaSen/.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictio-
naries. In Proceedings of the International Joint Confer-
ence on Artificial Intelligence IJCAI-2005, pages 1111?
1116. Edinburgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition. In 20th
International Conference on Computational Linguistics:
COLING-2004, pages 771?777. Geneva.
Stephen D. Richardson, William B. Dolan, and Lucy Van-
derwende. 1998. MindNet: acquiring and structuring se-
mantic information from text. In 36th Annual Meeting
of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics:
COLING/ACL-98, pages 1098?1102. Montreal.
Kiyoaki Shirai. 2003. SENSEVAL-2 Japanese dictionary
task. Journal of Natural Language Processing, 10(3):3?
24. (in Japanese).
Melanie Siegel. 2000. HPSG analysis of Japanese. In
Wahlster (2000), pages 265?280.
Kathrin Spreyer and Anette Frank. 2005. The TIGER RMRS
700 bank: RMRS construction from dependencies. In Pro-
ceedings of the 6th International Workshop on Linguisti-
cally Interpreted Corpora (LINC 2005), pages 1?10. Jeju
Island, Korea.
Takenobu Tokunaga, Yasuhiro Syotu, Hozumi Tanaka, and
Kiyoaki Shirai. 2001. Integration of heterogeneous lan-
guage resources: A monolingual dictionary and a the-
saurus. In Proceedings of the 6th Natural Language Pro-
cessing Pacific Rim Symposium, NLPRS2001, pages 135?
142. Tokyo.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer, Berlin, Germany.
Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.
1996. Electric Words. MIT Press.
17
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 62?69,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Hinoki Sensebank
? A Large-Scale Word Sense Tagged Corpus of Japanese ?
Takaaki Tanaka, Francis Bond and Sanae Fujita
{takaaki,bond,fujita}@cslab.kecl.ntt.co.jp
NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
Abstract
Semantic information is important for
precise word sense disambiguation system
and the kind of semantic analysis used in
sophisticated natural language processing
such as machine translation, question
answering, etc. There are at least two
kinds of semantic information: lexical
semantics for words and phrases and
structural semantics for phrases and
sentences.
We have built a Japanese corpus of over
three million words with both lexical
and structural semantic information. In
this paper, we focus on our method of
annotating the lexical semantics, that is
building a word sense tagged corpus and
its properties.
1 Introduction
While there has been considerable research on
both structural annotation (such as the Penn
Treebank (Taylor et al, 2003) or the Kyoto Corpus
(Kurohashi and Nagao, 2003)) and semantic
annotation (e.g. Senseval: Kilgariff and
Rosenzweig, 2000; Shirai, 2002), there are almost
no corpora that combine both. This makes it
difficult to carry out research on the interaction
between syntax and semantics.
Projects such as the Penn Propbank are adding
structural semantics (i.e. predicate argument
structure) to syntactically annotated corpora,
but not lexical semantic information (i.e. word
senses). Other corpora, such as the English
Redwoods Corpus (Oepen et al, 2002), combine
both syntactic and structural semantics in a
monostratal representation, but still have no
lexical semantics.
In this paper we discuss the (lexical) semantic
annotation for the Hinoki Corpus, which is
part of a larger project in psycho-linguistic
and computational linguistics ultimately aimed at
language understanding (Bond et al, 2004).
2 Corpus Design
In this section we describe the overall design of
the corpus, and is constituent corpora. The basic
aim is to combine structural semantic and lexical
semantic markup in a single corpus. In order to
make the first phase self contained, we started
with dictionary definition and example sentences.
We are currently adding other genre, to make the
langauge description more general, starting with
newspaper text.
2.1 Lexeed: A Japanese Basic Lexicon
We use word sense definitions from Lexeed:
A Japanese Semantic Lexicon (Kasahara et al,
2004). It was built in a series of psycholinguistic
experiments where words from two existing
machine-readable dictionaries were presented to
subjects and they were asked to rank them on a
familiarity scale from one to seven, with seven
being the most familiar (Amano and Kondo,
1999). Lexeed consists of all words with a
familiarity greater than or equal to five. There
are 28,000 words in all. Many words have
multiple senses, there were 46,347 different
senses. Definition sentences for these sentences
were rewritten to use only the 28,000 familiar
words. In the final configuration, 16,900 different
words (60% of all possible words) were actually
used in the definition sentences. An example
entry for the word ??{? doraiba? ?driver?
is given in Figure 1, with English glosses added.
This figure includes the sense annotation and
information derived from it that is described in this
paper.
Table 1 shows the relation between polysemy
and familiarity. The #WS column indicates the
average number of word senses that polysemous
62
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX ??{? doraiba-
POS noun Lexical-Type noun-lex
FAMILIARITY 6.5 [1?7] (? 5) Frequency 37 Entropy 0.79
SENSE 1
(0.11)
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION F11/k/F0?e1/8/  /e&1<1/8/2d/?1/
a tool for inserting and removing screws .
EXAMPLE ?Hf??{?<?GF1k2Z8
he used a small screwdriver to tighten the screws on his glasses.
HYPERNYM ?1 equipment ?tool?
SEM. CLASS ?942:tool/implement? (? ?893:equipment?)
WORDNET screwdriver1
?
?
?
?
?
?
?
?
?
?
?
?
SENSE 2
(0.84)
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION ?1/k/?U1/2d/01/
Someone who drives a car.
EXAMPLE ?H,?C??{?A0???.e8
my father was given an award as a good driver.
HYPERNYM 01 hito ?person?
SEM. CLASS ?292:chauffeur/driver? (? ?5:person?)
WORDNET driver1
?
?
?
?
?
?
?
?
?
?
?
?
SENSE 3
(0.05)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION ???1// /2?? 1/X/G/???3/??/}??/
In golf, a long-distance club. A number one wood.
EXAMPLE ?H??{???????I08
he hit (it) 30 yards with the driver.
HYPERNYM ???3 kurabu ?club?
SEM. CLASS ?921:leisure equipment? (? 921)
WORDNET driver5
DOMAIN ???1 gorufu ?golf?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Entry for the Word doraiba? ?driver? (with English glosses)
words have. Lower familiarity words tend to
have less ambiguity and 70 % of words with a
familiarity of less than 5.5 are monosemous. Most
polysemous words have only two or three senses
as seen in Table 2.
Fam #Words
Poly-
semous #WS
#Mono-
semous(%)
6.5 - 368 182 4.0 186 (50.5)
6.0 - 4,445 1,902 3.4 2,543 (57.2)
5.5 - 9,814 3,502 2.7 6,312 (64.3)
5.0 - 11,430 3,457 2.5 7,973 (69.8)
Table 1: Familiarity vs Word Sense Ambiguity
2.2 Ontology
We also have an ontology built from the parse
results of definitions in Lexeed (Nichols and Bond,
2005). The ontology includes more than 50
thousand relationship between word senses, e.g.
synonym, hypernym, abbreviation, etc.
2.3 Goi-Taikei
As part of the ontology verification, all nominal
and most verbal word senses in Lexeed were
#WS #Words
1 18460
2 6212
3 2040
4 799
5 311
6 187
7 99
8 53
9 35
10 15
11 19
12 13
13 13
14 6
15 6
16 3
17 2
18 3
19 1
20 2
? 21 19
Table 2: Number of Word Senses
linked to semantic classes in the Japanese
thesaurus, Nihongo Goi-Taikei (Ikehara et al,
1997). Common nouns are classified into about
2,700 semantic classes which are organized into a
63
semantic hierarchy.
2.4 Hinoki Treebank
Lexeed definition and example sentences are
syntactically and semantically parsed with HPSG
and correct results are manually selected (Tanaka
et al, 2005). The grammatical coverage over all
sentences is 86%. Around 12% of the parsed
sentences were rejected by the treebankers due
to an incomplete semantic representation. This
process had been done independently of word
sense annotation.
2.5 Target Corpora
We chose two types of corpus to mark up: a
dictionary and two newspapers. Table 3 shows
basic statistics of the target corpora.
The dictionary Lexeed, which defined word
senses, is also used for a target for sense tagging.
Its definition (LXD-DEF) and example (LXD-EX)
sentences consist of basic words and function
words only, i.e. it is self-contained. Therefore,
all content words have headwords in Lexeed, and
all word senses appear in at least one example
sentence.
Both newspaper corpora where taken from the
Mainichi Daily News. One sample (Senseval2)
was the text used for the Japanese dictionary
task in Senseval-2 (Shirai, 2002), which has some
words marked up with word sense tags defined
in the Iwanami lexicon (Nishio et al, 1994).
The second sample was those sentences used in
the Kyoto Corpus (Kyoto), which is marked up
with dependency analyses (Kurohashi and Nagao,
2003). We chose these corpora so that we can
compare our annotation with existing annotation.
Both these corpora were thus already segmented
and annotated with parts-of-speech. However,
they used different morphological analyzers to
the one used in Lexeed, so we had to do some
remapping. E.g. in Kyoto the copula is not split
from nominal-adjectives, whereas in Lexeed it is:
?[9 genkida ?lively? vs ?[9 genki da. This
could be done automatically after we had written
a few rules.
Although the newspapers contain many words
other than basic words, only basic words have
sense tags. Also, a word unit in the newspapers
does not necessarily coincide with the headword
in Lexeed since part-of-speech taggers used for
annotation are different. We do not adjust the word
segmentation and leave it untagged at this stage,
even if it is a part of a basic word or consists of
multiple basic words. For instance, Lexeed has the
compound entry |+^? kahei-kachi ?monetary
value?, however, this word is split into two basic
words in the corpora. In this case, both two words
|+ kahei ?money? and ^? kachi ?value? are
tagged individually.
Corpus Tokens
Content
Words
Basic
Words
%Mono-
semous
LXD-DEF 691,072 318,181 318,181 31.7
LXD-EX 498,977 221,224 221,224 30.5
Senseval2 888,000 692,069 391,010 39.3
Kyoto 969,558 526,760 472,419 36.3
Table 3: Corpus Statistics
The corpora are not fully balanced, but
allow some interesting comparisons. There are
effectively three genres: dictionary definitions,
which tend to be fragments and are often
syntactically highly ambiguous; dictionary
example sentences, which tend to be short
complete sentences, and are easy to parse; and
newspaper text from two different years.
3 Annotation
Each word was annotated by five annotators.
We actually used 15 annotators, divided into 3
groups. None were professional linguists or
lexicographers. All of them had a score above
60 on a Chinese character based vocabulary test
(Amano and Kondo, 1998). We used multiple
annotators to measure the confidence of tags and
the degree of difficulty in identifying senses.
The target words for sense annotation are
the 9,835 headwords having multiple senses in
Lexeed (? 2.1). They have 28,300 senses in
all. Monosemous words were not annotated.
Annotation was done word by word. Annotators
are presented multiple sentences (up to 50) that
contain the same target word, and they keep
tagging that word until occurrences are done. This
enables them to compare various contexts where
a target word appears and helps them to keep the
annotation consistent.
3.1 Tool
A screen shot of the annotation tool is given in
Figure 2. The interface uses frames on a browser,
with all information stored in SQL tables. The left
hand frame lists the words being annotated. Each
word is shown with some context: the surrounding
64
paragraph, and the headword for definition and
example sentences. These can be clicked on to
get more context. The word being annotated is
highlighted in red. For each word, the annotator
chooses its senses or one or more of the other tags
as clickable buttons. It is also possible to choose
one tag as the default for all entries on the screen.
The right hand side frame has the dictionary
definitions for the word being tagged in the top
frame, and a lower frame with instructions. A
single word may be annotated with senses from
more than one headword. For example ?? is
divided into two headwords basu ?bus? and basu
?bass?, both of which are presented.
As we used a tab-capable browser, it was easy
for the annotators to call up more information in
different tabs. This proved to be quite popular.
3.2 Markup
Annotators choose the most suitable sense in the
given context from the senses that the word have
in lexicon. Preferably, they select a single sense
for a word, although they can mark up multiple
tags if the words have multiple meanings or are
truly ambiguous in the contexts.
When they cannot choose a sense in some
reasons, they choose one or more of the following
special tags.
o other sense: an appropriate sense is not found
in a lexicon. Relatively novel concepts (e.g.
??{? doraiba? ?driver? for ?software
driver?) are given this tag.
c multiword expressions (compound / idiom): the
target word is a part of a non-compositional
compound or idiom.
p proper noun: the word is a proper noun.
x homonym: an appropriate entry is not found
in a lexicon, because a target is different
from head words in a lexicon (e.g. only a
headword ?? bass ?bus? is present in a
lexicon for ??basu ?bass?).
e analysis error: the word segmentation or part-
of-speech is incorrect due to errors in pre-
annotation of the corpus.
3.3 Feedback
One of the things that the annotators found hard
was not knowing how well they were doing. As
they were creating a gold standard, there was
initially no way of knowing how correct they were.
We also did not know at the start of the annotation
how fast senses could or should be annotated (a
test of the tool gave us an initial estimate of around
400 tokens/day).
To answer these questions, and to provide
feedback for the annotators, twice a day we
calculated and graphed the speed (in words/day)
and majority agreement (how often an annotator
agrees with the majority of annotators for each
token, measured over all words annotated so
far). Each annotator could see a graph with
their results labelled, and the other annotators
made anonymous. The results are grouped into
three groups of five annotators. Each group
is annotating a different set of words, but we
included them all in the feedback. The order
within each group is sorted by agreement, as we
wished to emphasise the importance of agreement
over speed. An example of a graph is given
in Figure 3. When this feedback was given,
this particular annotator has the second worst
agreement score in their subgroup (90.27%) and is
reasonably fast (1799 words/day) ? they should
slow down and think more.
The annotators welcomed this feedback, and
complained when our script failed to produce it.
There was an enormous variation in speed: the
fastest annotator was 4 times as fast as the slowest,
with no appreciable difference in agreement.
After providing the feedback, the average speed
increased considerably, as the slowest annotators
agonized less over their decisions. The final
average speed was around 1,500 tokens/day, with
the fastest annotator still almost twice as fast as the
slowest.
4 Inter-Annotator Agreement
We employ inter-annotator agreement as our core
measure of annotation consistency, in the same
way we did for treebank evaluation (Tanaka et al,
2005). This agreement is calculated as the average
of pairwise agreement. Let wi be a word in a set of
content words W and wi, j be the jth occurrence of
a word wi. Average pairwise agreement between
the sense tags of wi, j each pair of annotators
marked up a(wi, j) is:
a(wi, j) =
?k (mi, j(sik)C2)
nwi, j C2
(1)
where nwi, j(? 2) is the number of annotators that
tag the word wi, j, and mi, j(sik) is the number
65
Figure 2: Sense Annotation tool (word ?( shibaraku ?briefly?)
Figure 3: Sample feedback provided to an annotator
of sense tags sik for the word wi, j. Hence, the
agreement of the word wi is the average of awi, j
over all occurrences in a corpus:
a(wi) =
? j a(wi, j)
Nwi
(2)
66
where Nwi is the frequency of the word wi in a
corpus.
Table 4 shows statistics about the annotation
results. The average numbers of word senses in
the newspapers are lower than the ones in the
dictionary and, therefore, the token agreement
of the newspapers is higher than those of the
dictionary sentences. %Unanimous indicates the
ratio of tokens vs types for which all annotators
(normally five) choose the same sense. Snyder
and Palmer (2004) report 62% of all word types
on the English all-words task at SENSEVAL-3
were labelled unanimously. It is hard to directly
compare with our task since their corpus has only
2,212 words tagged by two or three annotators.
4.1 Familiarity
As seen in Table 5, the agreement per type
does not vary much by familiarity. This was
an unexpected result. Even though the average
polysemy is high, there are still many highly
familiar words with very good agreement.
Fam
Agreement
token (type) #WS %Monosem
6.5 - .723 (.846) 7.00 22.6
6.0 - .780 (.846) 5.82 28.0
5.5 - .813 (.853) 3.79 42.4
5.0 - .821 (.850) 3.84 46.2
ALL .787 (.850) 5.18 34.5
Table 5: Inter-Annotator Agreement (LXD-DEF)
4.2 Part-of-Speech
Table 6 shows the agreement according to part of
speech. Nouns and verbal nouns (vn) have the
highest agreements, similar to the results for the
English all-words task at SENSEVAL-3 (Snyder
and Palmer, 2004). In contrast, adjectives have as
low agreement as verbs, although the agreement
of adjectives was the highest and that of verbs
was the lowest in English. This partly reflects
differences in the part of speech divisions between
Japanese and English. Adjectives in Japanese are
much close in behaviour to verbs (e.g. they can
head sentences) and includes many words that are
translated as verbs in English.
4.3 Entropy
Entropy is directly related to the difficulty in
identifing senses as shown in Table 7.
POS Agreement (type) #WS %Monosemous
n .803 (.851) 2.86 62.9
v .772 (.844) 3.65 34.0
vn .849 (.865) 2.54 61.0
adj .770 (.810) 3.58 48.3
adv .648 (.833) 3.08 46.4
others .615 (.789) 3.19 50.8
Table 6: POS vs Inter-Annotator Agreement (LXD-
DEF)
Entropy Agreement (type) #Words #WS
2 - .672 84 14.2
1 - .758 1096 4.38
0.5 - .809 1627 2.88
0.05 - .891 495 3.19
0 - .890 13778 2.56
Table 7: Entropy vs Agreement
4.4 Sense Lumping
Low agreement words have some senses that are
difficult to distinguish from each other: these
senses often have the same hypernyms. For
example, the agreement rate of s kusabana
?grass/flower? in LXD-DEF is only 33.7 %.
It has three senses whose semantic class is
similar: kusabana1 ?flower that blooms in grass?,
kusabana2 ?grass that has flowers? and souka1
?grass and flowers? (hypernyms flower1, grass1
and flower1 & grass1 respectively).
In order to investigate the effect of semantic
similarity on agreement, we lumped similar word
senses based on hypernym and semantic class.
We use hypernyms from the ontology (? 2.1) and
semantic classes in Goi-Taikei (? 2.3), to regard
the word senses that have the same hypernyms or
belong to the same semantic classes as the same
senses.
Table 8 shows the distribution after sense
lumping. Table 9 shows the agreement with
lumped senses. Note that this was done with an
automatically derived ontology that has not been
fully hand corrected.
As is expected, the overall agreement increased,
from 0.787 to 0.829 using the ontology, and
to 0.835 using the coarse-grained Goi-Taikei
semantic classes. For many applications, we
expect that this level of disambiguation is all that
is required.
4.5 Special Tags
Table 10 shows the ratio of special tags and
multiple tags to all tags. These results show
67
Corpus
Annotated
Tokens #WS
Agreement
token (type)
%Unanimous
token (type) Kappa
LXD-DEF 199,268 5.18 .787 (.850) 62.8 (41.1) 0.58
LXD-EX 126,966 5.00 .820 (.871) 69.1 (53.2) 0.65
Senseval2 223,983 4.07 .832 (.833) 73.9 (45.8) 0.52
Kyoto 268,597 3.93 .833 (.828) 71.5 (46.1) 0.50
Table 4: Basic Annotation Statistics
Corpus %Other Sense %MWE %Homonym %Proper Noun %Error %Multiple Tags
LXD-DEF 4.2 1.5 0.084 0.046 0.92 11.9
LXD-EX 2.3 0.44 0.035 0.0018 0.43 11.6
Senseval2 9.3 5.6 4.1 8.7 5.7 7.9
Kyoto 9.8 7.9 3.3 9.0 5.5 9.3
Table 10: Special Tags and Multiple Tags
Fam
Agreement
token (type) #WS %Monosem
6.5 - .772 (.863) 6.37 25.6
6.0 - .830 (.868) 5.16 31.5
5.5 - .836 (.872) 3.50 45.6
5.0 - .863 (.866) 3.76 58.7
ALL .829 (.869) 4.72 39.1
Lumping together Hypernyms
(4,380 senses compressed into 1,900 senses)
Fam
Agreement
token (type) #WS %Monosem
6.5 - .775 (.890) 6.05 26.8
6.0 - .835 (.891) 4.94 36.4
5.5 - .855 (.894) 3.29 50.6
5.0 - .852 (.888) 3.46 49.9
ALL .835 (.891) 4.48 41.7
Lumping together Semantic Classes
(8,691 senses compressed into 4,030 senses)
Table 8: Sense Lumping Results (LXD-DEF)
(LXD-DEF)
Agreement
token (type) #WS %Monosem
no lumping .698 (.816) 8.81 0.0
lumping .811 (.910) 8.24 20.0
Hypernum Lumping
(LXD-DEF)
Agreement
token (type) #WS %Monosem
no lumping .751 (.814) 7.09 0.0
lumping .840 (.925) 5.99 21.9
Semantic Class Lumping
Table 9: Lumped Sense Agreement (LXD-DEF)
the differences in corpus characteristics between
dictionary and newspaper. The higher ratios of
Other Sense and Homonym at newspapers indicate
that the words whose surface form is in a
dictionary are frequently used for the different
meanings in real text, e.g. ? gin ?silver? is
used for the abbrebiation of ? ginkou ?bank?.
%Multiple Tags is the percentage of tokens for
which at least one annotator marked multiple tags.
5 Discussion
5.1 Comparison with Senseval-2 corpus
The Senseval-2 Japanese dictionary task
annotation used senses from a different dictionary
(Shirai, 2002). In the evaluation, 100 test words
were selected from three groups with different
entropy bands (Kurohashi and Shirai, 2001). Da
is the highest entropy group, which contains the
most hard to tag words, and Dc is the lowest
entropy group.
We compare our results with theirs in Table 11.
The Senseval-2 agreement figures are slightly
higher than our overall. However, it is impossible
to make a direct comparison as the numbers of
annotators (two or three annotators in Senseval vs
more than 5 annotators in our work) and the sense
inventories are different.
5.2 Problems
Two main problems came up when building
the corpora: word segmentation and sense
segmentation. Multiword expressions like
compounds and idioms are tied closely to both
problems.
The word segmentation is the problem of how
to determine an unit expressing a meaning. At
the present stage, it is based on headword in
Lexeed, in particular, only compounds in Lexeed
are recognized, we do not discriminate non-
decomposable compounds with decomposable
ones. However, if the headword unit in the
dictionary is inconsistent, word sense tagging
inherits this problem. For examples, ? ichibu
has two main usage: one + classifier and a part
of something. Lexeed has an entry including both
two senses. However, the former is split into two
68
POS Da Db Dc Total
Hinoki Senseval Hinoki Senseval Hinoki Senseval Hinoki Senseval
noun .768 .809 .784 .786 .848 .957 .806 .859
14.4 13.1 5.0 4.1 3.1 3.8 5.9 5.1
verb .660 .699 .722 .896 .738 .867 .723 .867
16.7 21.8 10.3 9.3 5.2 5.9 9.6 10.9
total .710 .754 .760 .841 .831 .939 .768 .863
15.6 18.8 7.0 6.2 4.2 4.9 7.6 7.9
Table 11: Comparison of Agreement for the Senseval-2 Lexical Sample Task Corpus ( upper row:
agreement, lower row: the number of word senses)
words by our morphological analyser in the same
way as other numeral + classifier.
The second problem is how to mark off
metaphorical meaning from literal meanings.
Currently, this also depends on the Lexeed
definition and it is not necessarily consistent
either. Some words in institutional idioms (Sag
et al, 2002) have the idiom sense in the lexicon
while most words do not. For instance, ?
shippo ?tail of animal?) has a sense for the reading
?weak point? in an idiom ?kY shippo-o
tsukamu ?lit. to grasp the tail, idiom. to find one?s
weak point?, while  ase ?sweat? does not have
a sense for the applicable meaning in the idiom 
k?2 ase-o nagasu ?lit. to sweat, idiom, to work
hard?.
6 Conclusions
We built a corpus of over three million words
which has lexical semantic information. We are
currently using it to build a model for word sense
disambiguation.
Acknowledgement
We thank the 15 sense annotators and 3 treebankers
(Takayuki Kuribayashi, Tomoko Hirata and Koji Yamashita).
References
Anne Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer Academic Publishers.
Shigeaki Amano and Tadahisa Kondo. 1998. Estimation
of mental lexicon size with word familiarity database.
In International Conference on Spoken Language
Processing, volume 5, pages 2119?2122.
Shigeaki Amano and Tadahisa Kondo. 1999. Nihongo-no
Goi-Tokusei (Lexical properties of Japanese). Sanseido.
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The
Hinoki treebank: A treebank for text understanding. In
Proceedings of the First International Joint Conference on
Natural Language Processing (IJCNLP-04), pages 554?
559. Hainan Island.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei
? A Japanese Lexicon. Iwanami Shoten, Tokyo. 5
volumes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic
lexicon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in
Japanese).
Adam Kilgariff and Joseph Rosenzweig. 2000. Framework
and results for English SENSEVAL. Computers and
the Humanities, 34(1?2):15?48. Special Issue on
SENSEVAL.
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. In Abeille? (2003), chapter 14, pages 249?260.
Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2
Japanese task. SIG NLC 2001-10, IEICE. (in Japanese).
Eric Nichols and Francis Bond. 2005. Acquiring ontologies
using deep and shallow processing. In 11th Annual
Meeting of the Association for Natural Language
Processing, pages 494?498. Takamatsu.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International
Conference on Computational Linguistics: COLING-
2002, pages 1253?7. Taipei, Taiwan.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A
pain in the neck for NLP. In Alexander Gelbuk,
editor, Computational Linguistics and Intelligent Text
Processing: Third International Conference: CICLing-
2002, pages 1?15. Springer-Verlag, Hiedelberg/Berlin.
Kiyoaki Shirai. 2002. Construction of a word sense tagged
corpus for SENSEVAL-2 Japanese dictionary task. In
Third International Conference on Language Resources
and Evaluation (LREC-2002), pages 605?608.
Benjamin Snyder and Martha Palmer. 2004. The English all-
words task. In Proceedings of Senseval-3, pages 41?44.
ACL, Barcelona.
Takaaki Tanaka, Francis Bond, Stephan Oepen, and Sanae
Fujita. 2005. High precision treebanking ? blazing useful
trees using POS information. In ACL-2005, pages 330?
337.
Ann Taylor, Mitchel Marcus, and Beatrice Santorini. 2003.
The Penn treebank: an overview. In Abeille? (2003),
chapter 1, pages 5?22.
69
Proceedings of the Workshop on Linguistic Distances, pages 35?42,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Sentence Comparison
using Robust Minimal Recursion Semantics
and an Ontology
Rebecca Dridan
}
and Francis Bond

}
rdrid@csse.unimelb.edu.au

bond@cslab.kecl.ntt.co.jp
}
The University of Melbourne

NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
Abstract
We design and test a sentence com-
parison method using the framework
of Robust Minimal Recursion Seman-
tics which allows us to utilise the deep
parse information produced by Jacy, a
Japanese HPSG based parser and the
lexical information available in our on-
tology. Our method was used for both
paraphrase detection and also for an-
swer sentence selection for question an-
swering. In both tasks, results showed
an improvement over Bag-of-Words, as
well as providing extra information use-
ful to the applications.
1 Introduction
Comparison between sentences is required for
many NLP applications, including question
answering, paraphrasing, text summarization
and entailment tasks. In this paper we show
an RMRS (Robust Minimal Recursion Seman-
tics, see Section 1.1) comparison algorithm
that can be used to compare sentences in
any language that has RMRS generating tools
available. Lexical resources of any language
can be plugged in to give a more accurate and
informative comparison.
The simplest and most commonly used
methods of judging sentence similarity use
word overlap { either looking for matching
word sequences, or comparing a Bag-of-Words
representation of each sentence. Bag-of-Words
discards word order, and any structure desig-
nated by such, so that the cat snored and the
dog slept is equivalent to the dog snored and
the cat slept. Sequence matching on the other
hand requires exact word order matching and
hence the game began quietly and the game qui-
etly began are not considered a match. Neither
method allows for synonym matching.
Hirao et al (2004) showed that they could
get a much more robust comparison using
dependency information rather than Bag-of-
Words, since they could abstract away from
word order but still compare the important
elements of a sentence. Using deep parsing
information, such as dependencies, but also
deep lexical resources where available, enables
a much more informative and robust compar-
ison, which goes beyond lexical similarity. We
use the RMRS framework as our comparison
format because it has the descriptive power to
encode the full semantics, including argument
structure. It also enables easy combination of
deep and shallow information and, due to its
at structure, is easy to manage computation-
ally.
1.1 Robust Minimal Recursion
Semantics
Robust Minimal Recursion Semantics
(RMRS) is a form of at semantics which is
designed to allow deep and shallow processing
to use a compatible semantic representation,
while being rich enough to support gener-
alized quantiers (Frank, 2004). The main
component of an RMRS representation is
a bag of elementary predicates and their
arguments.
An elementary predicate always has a
unique label, a relation type, a relation name
and an ARG0 feature. The example in Fig-
ure 1 has a label of h5 which uniquely identi-
es this predicate. Relation types can either
be realpred for a predicate that relates di-
rectly to a content word from the input text, or
gpred for grammatical predicates which may
not have a direct referent in the text. For ex-
amples in this paper, a realpred is distin-
guished by an underscore ( ) before the rela-
tion name.
The gpred relation names come from a
35
"unten s
lbl h5
arg0 e6
#
Figure 1: Elementary predicate for-2 unten
\drive"
closed-set which specify common grammatical
relations, but the realpred names are formed
from the word in the text they relate to and
this is one way in which RMRS allows under-
specication. A full relation name is of the
form lemma pos sense, where the pos (part
of speech) is drawn from a small set of general
types including noun, verb and sahen (verbal
noun). The sense is a number that identies
the sense of the word within a particular gram-
mar being used. The POS and sense informa-
tion are only used when available and hence
the unten s 1 is more specic but compati-
ble with unten s or even unten.
The arg0 feature (e6 in Figure 1) is the
referential index of the predicate. Predicates
with the same arg0 are said to be referen-
tially co-indexed and therefore have the same
referent in the text.
A shallow parse might provide only the fea-
tures shown in Figure 1, but a deep parse can
also give information about other arguments
as well as scoping constraints. The features
arg1..arg4 specify the indices of the semantic
arguments of the relevant predicate, similar to
PropBank's argument annotation (Kingsbury
et al, 2002). While the RMRS specication
does not dene semantic roles for the argn
features, in practice arg1 is generally used for
the agent and arg2 for the patient. Fea-
tures arg3 and arg4 have less consistency in
their roles.
We will use (1) and (2) as examples of sim-
ilar sentences. They are denition sentences
for one sense of ),'* doraiba- \driver",
taken from two dierent lexicons.
(1) .30 & -2 !% 1
jidosha wo unten suru hito
car acc drive do person
\a person who drives a car"
(2) .30 #" $ -2 /
jidosha nado no unten sha
car etc. adn drive -er
\a driver of cars etc."
Examples of deep and shallow RMRS results
for (1) are given in Figure 2. Deep results for
(2) are given in Figure 3.
2 Algorithm
The matching algorithm is loosely based on
RMRS comparison code included in the LKB
(Copestake, 2002: hhttp://www.delph-in.
net/lkb/i), which was used in Ritchie (2004),
however that code used no outside lexical re-
sources and we have substantially changed the
matching algorithm.
The comparison algorithm is language inde-
pendent and can be used for any RMRS struc-
tures. It rst compares all elementary predi-
cates from the RMRSs to construct a list of
match records and then examines, and poten-
tially alters, the list of match records accord-
ing to constraints encoded in the argn vari-
ables. Using the list of scored matches, the
lowest scoring possible match set is found and,
after further processing on that set, a similar-
ity score is returned. The threshold for de-
ciding whether a pair of sentences should be
considered similar or not can be determined
separately for dierent applications.
2.1 Matching Predicates
The elementary predicates (EPs) of our RMRS
structures are divided into two groups - those
that have a referent in the text, hereafter
known as content EPs, and those that don't.
There are three kinds of content EP: real-
preds, which correspond to content bearing
words that the grammar knows; gpreds with
a carg (constant argument) feature, which
are used to represent proper names and num-
bers; and gpreds with a predicate name start-
ing with generic such as generic verb which
are used for unknown words that have only
been identied by their part of speech. All
other EPs have no referent and are used to
provide information about the content EPs or
about the structure of the sentence as a whole.
These non-content EPs can provide some use-
ful information, but generally only in relation
to other content EPs.
Each content EP of the rst RMRS is com-
pared to all content EPs in the second RMRS,
as shown in Figure 4.
Matches are categorised as exact, syn-
onym, hypernym, hyponym or no match
and a numerical score is assigned. The nu-
36
26
6
6
6
6
6
6
6
6
6
6
6
6
4
text ',)%&+ $*
top h1
rels
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
"
proposition m rel
lbl h1
arg0 e2
marg h3
#"
unknown rel
lbl h4
arg0 e2
arg x5
#

jidousha n
lbl h6
arg0 x7

2
6
4
udef rel
lbl h8
arg0 x7
rstr h9
body h10
3
7
5
2
6
4
unten s
lbl h11
arg0 e13
arg1 u12
arg2 x7
3
7
5

hito n
lbl h14
arg0 x5

2
6
4
udef rel
lbl h15
arg0 x5
rstr h16
body h17
3
7
5
"
proposition m rel
lbl h10001
arg0 e13
marg h18
#
2
6
4
topic rel
lbl h10002
arg0 e19
arg1 e13
arg2 x5
3
7
5
9
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
;
hcons fh3 qeq h4 ; h9 qeq h6 ; h16 qeq h14 ; h18 qeq h11g
ing fh11 ing h10002 ; h14 ing h10001g
3
7
7
7
7
7
7
7
7
7
7
7
7
7
5
2
6
6
6
4
text ',)%&+ $*
top h9
rels
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 477?485, Prague, June 2007. c?2007 Association for Computational Linguistics
Word Sense Disambiguation
Incorporating Lexical and Structural Semantic Information
Takaaki Tanaka? Francis Bond? Timothy Baldwin? Sanae Fujita? Chikara Hashimoto?
? {takaaki, sanae}@cslab.kecl.ntt.co.jp ? bond@nict.go.jp
? tim@csse.unimelb.edu.au ? ch@yz.yamagata-u.ac.jp
? NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
? National Institute of Information and Communications Technology
? The University of Melbourne ? Yamagata University
Abstract
We present results that show that incorporat-
ing lexical and structural semantic informa-
tion is effective for word sense disambigua-
tion. We evaluated the method by using pre-
cise information from a large treebank and
an ontology automatically created from dic-
tionary sentences. Exploiting rich semantic
and structural information improves preci-
sion 2?3%. The most gains are seen with
verbs, with an improvement of 5.7% over a
model using only bag of words and n-gram
features.
1 Introduction
Recently, significant improvements have been made
in combining symbolic and statistical approaches
to various natural language processing tasks. In
parsing, for example, symbolic grammars are be-
ing combined with stochastic models (Riezler et al,
2002; Oepen et al, 2002; Malouf and van Noord,
2004). Statistical techniques have also been shown
to be useful for word sense disambiguation (Steven-
son, 2003). However, to date, there have been
few combinations of sense information together with
symbolic grammars and statistical models. Klein
and Manning (2003) show that much of the gain in
statistical parsing using lexicalized models comes
from the use of a small set of function words.
Features based on general relations provide little
improvement, presumably because the data is too
sparse: in the Penn treebank normally used to train
and test statistical parsers stocks and skyrocket never
appear together. They note that this should motivate
the use of similarity and/or class based approaches:
the superordinate concepts capital (? stocks) and
move upward (? sky rocket) frequently appear to-
gether. However, there has been little success in this
area to date. For example, Xiong et al (2005) use se-
mantic knowledge to parse Chinese, but gain only a
marginal improvement. Focusing on WSD, Steven-
son (2003) and others have shown that the use of
syntactic information (predicate-argument relations)
improve the quality of word sense disambiguation
(WSD). McCarthy and Carroll (2003) have shown
the effectiveness of the selectional preference infor-
mation for WSD. However, there is still little work
on combining WSD and parse selection.
We hypothesize that one of the reasons for the
lack of success is that there has been no resource
annotated with both syntactic (or structural seman-
tic information) and lexical semantic information.
For English, there is the SemCor corpus (Fellbaum,
1998) which is annotated with parse trees and Word-
Net senses, but it is fairly small, and does not ex-
plicitly include any structural semantic information.
Therefore, we decided to construct and use a tree-
bank with both syntactic information (e.g. HPSG
parses) and lexical semantic information (e.g. sense
tags): the Hinoki treebank (Bond et al, 2004). This
can be used to train word sense disambiguation and
parse ranking models using both syntactic and lexi-
cal semantic features. In this paper, we discuss only
word sense disambiguation. Parse ranking is dis-
cussed in Fujita et al (2007).
2 The Hinoki Corpus
The Hinoki corpus consists of the Lexeed Seman-
tic Database of Japanese (Kasahara et al, 2004) and
corpora annotated with syntactic and semantic infor-
477
mation.
2.1 Lexeed
Lexeed is a database built from on a dictionary,
which defines word senses used in the Hinoki cor-
pus and has around 49,000 dictionary definition sen-
tences and 46,000 example sentences which are syn-
tactically and semantically annotated. Lexeed con-
sists of all words with a familiarity greater than or
equal to five on a scale of one to seven. This gives
a fundamental vocabulary of 28,000 words, divided
into 46,347 different senses. Each sense has a defi-
nition sentence and example sentence written using
only these 28,000 familiar words (and some function
words). Many senses have more than one sentence
in the definition: there are 75,000 defining sentences
in all.
A (simplified) example of the entry for?U3 un-
tenshu ?chauffeur? is given in Figure 1. Each word
contains the word itself, its part of speech (POS) and
lexical type(s) in the grammar, and the familiarity
score. Each sense then contains definition and ex-
ample sentences, links to other senses in the lexicon
(such as hypernym), and links to other resources,
such as the Goi-Taikei (Ikehara et al, 1997) and
WordNet (Fellbaum, 1998). Each content word in
the definition and example sentences is annotated
with sense tags from the same lexicon.
2.2 Lexical Semantics Annotation
The lexical semantic annotation uses the sense in-
ventory from Lexeed. All words in the fundamental
vocabulary are tagged with their sense. For example,
the word d& ookii ?big? (in ookiku naru ?grow
up?) is tagged as sense 5 in the example sentence
(Figure 1), with the meaning ?elder, older?.
Each word was annotated by five annotators. We
use the majority choice in case of disagreements
(Tanaka et al, 2006). Inter-annotator agreements
among the five annotators range from 78.7% to
83.3%: the lowest agreement is for the Lexeed def-
inition sentences and the highest is for Kyoto cor-
pus (newspaper text). These agreements reflect the
difficulties in disambiguating word sense over each
corpus and can be considered as the upper bound of
precision for WSD.
Table 1 shows the distribution of word senses ac-
cording to the word familiarity in Lexeed.
Fam #Words
Poly-
semous #WS
#Mono-
semous(%)
6.5 - 368 182 4.0 186 (50.5)
6.0 - 4,445 1,902 3.4 2,543 (57.2)
5.5 - 9,814 3,502 2.7 6,312 (64.3)
5.0 - 11,430 3,457 2.5 7,973 (69.8)
Table 1: Word Senses in Lexeed
2.3 Ontology
The Hinoki corpus comes with an ontology semi-
automatically constructed from the parse results of
definitions in Lexeed (Nichols and Bond, 2005). The
ontology includes more than 80 thousand relation-
ships between word senses, e.g. synonym, hyper-
nym, abbreviation, etc. The hypernym relation for
?U3 untenshu ?chauffeur? is shown in Figure 1.
Hypernym or synonym relations exist for almost all
content words.
2.4 Thesaurus
As part of the ontology verification, all nominal and
most verbal word senses in Lexeed were linked to
semantic classes in the Japanese thesaurus, Nihongo
Goi-Taikei (Ikehara et al, 1997). These were then
hand verified. Goi-Taikei has about 400,000 words
including proper nouns, most nouns are classified
into about 2,700 semantic classes. These seman-
tic classes are arranged in a hierarchical structure
(11 levels). The Goi-Taikei Semantic Class for ?
U3 untenshu ?chauffeur? is shown in Figure 1:
?C292:driver? at level 9 which is subordinate to
?C4:person?.
2.5 Syntactic and Structural Semantics
Annotation
Syntactic annotation is done by selecting the best
parse (or parses) from the full analyses derived by
a broad-coverage precision grammar. The gram-
mar is an HPSG implementation (JACY: Siegel and
Bender, 2002), which provides a high level of de-
tail, marking not only dependency and constituent
structure but also detailed semantic relations. As the
grammar is based on a monostratal theory of gram-
mar (HPSG: Pollard and Sag, 1994) it is possible
to simultaneously annotate syntactic and semantic
structure without overburdening the annotator. Us-
ing a grammar enforces treebank consistency ? all
sentences annotated are guaranteed to have well-
478
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX ?U3 untenshu
POS noun
LEX-TYPE noun-lex
FAMILIARITY 6.2 [1?7] (? 5)
SENSE 1
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION
[
\1 ??1 k?U1 2d04 a person who drives trains and cars
]
EXAMPLE
[
d&(5 C<8b\1 G?U31 Dod6 G%?3 2
I dream of growing up and becoming a train driver
]
HYPERNYM 04 hito ?person?
SEM. CLASS ?292:driver? (? ?4:person?)
WORDNET motorman1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur?
formed parses. The flip side to this is that any sen-
tences which the parser cannot parse remain unan-
notated, at least unless we were to fall back on full
manual mark-up of their analyses. The actual anno-
tation process uses the same tools as the Redwoods
treebank of English (Oepen et al, 2002).
There were 4 parses for the definition sentence
shown in Figure 1. The correct parse, shown as a
phrase structure tree, is shown in Figure 2. The two
sources of ambiguity are the conjunction and the rel-
ative clause. The parser also allows the conjunction
to join to \ densha and 0 hito. In Japanese, rel-
ative clauses can have gapped and non-gapped read-
ings. In the gapped reading (selected here), 0 hito
is the subject of ?U unten ?drive?. In the non-
gapped reading there is some underspecified relation
between the thing and the verb phrase. This is sim-
ilar to the difference in the two readings of the day
he knew in English: ?the day that he knew about?
(gapped) vs ?the day on which he knew (some-
thing)? (non-gapped). Such semantic ambiguity is
resolved by selecting the correct derivation tree that
includes the applied rules in building the tree.
The parse results can be automatically given by
the HPSG parser PET (Callmeier, 2000) with the
Japanese grammar JACY. The current parse ranking
model has an accuracy of 70%: the correct tree is
ranked first 70% of the time (for Lexeed definition
sentences) (Fujita et al, 2007).
The full parse is an HPSG sign, containing both
syntactic and semantic information. A view of the
semantic information is given in Figure 31.
1The specific meaning representation language used in
UTTERANCE
NP
VP N
PP V
NP
PP
N CONJ N CASE-P V V
\ ? ? k ?U 2d 0
densha ya jidousha o unten suru hito
train or car ACC drive do person
?U31 ?chauffeur?: ?a person who drives a train or car?
Figure 2: Syntactic View of the Definition of ?U
31 untenshu ?chauffeur?
The semantic view shows some ambiguity has
been resolved that is not visible in the purely syn-
tactic view.
The semantic view can be further simplified into a
dependency representation, further abstracting away
from quantification, as shown in Figure 4. One of
the advantages of the HPSG sign is that it contains
all this information, making it possible to extract the
particular view needed. In order to make linking to
other resources (such as the sense annotation) easier,
predicates are labeled with pointers back to their po-
sition in the original surface string. For example, the
predicate densha n 1 links to the surface characters
between positions 0 and 3: \.
JACY is Minimal Recursion Semantics (Copestake et al, 2005).
479
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TEXT \??k?U2d0
TOP h1
RELS
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
proposition m rel
LBL h1
ARG0 e2
MARG h3
?
?
?
?
?
?
unknown rel
LBL h4
ARG0 e2
ARG x5
?
?
?
?
?
densha n
LBL h6
ARG0 x7
?
?
?
?
?
?
?
udef rel
LBL h8
ARG0 x7
RSTR h9
BODY h10
?
?
?
?
?
?
?
?
?
?
ya p
LBL h11
ARG0 x13
L-INDEX x7
R-INDEX x12
?
?
?
?
?
?
?
?
?
?
udef rel
LBL h15
ARG0 x12
RSTR h16
BODY h17
?
?
?
?
?
?
?
jidousha n
LBL h18
ARG0 x12
?
?
?
?
?
?
?
udef rel
LBL h19
ARG0 x12
RSTR h20
BODY h21
?
?
?
?
?
?
?
?
?
?
unten s
LBL h22
ARG0 e23 tense=present
ARG1 x5
ARG2 x13
?
?
?
?
?
?
?
hito n
LBL h24
ARG0 x5
?
?
?
?
?
?
?
udef rel
LBL h25
ARG0 x5
RSTR h26
BODY h27
?
?
?
?
?
?
?
?
proposition m rel
LBL h10001
ARG0 e23 tense=present
MARG h28
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HCONS {h3 qeq h4,h9 qeq h6,h16 qeq h11,h20 qeq h18,h26 qeq h24,h28 qeq h22}
ING {h24 ing h10001}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Semantic View of the Definition of ?U31 untenshu ?chauffeur?
_1:proposition_m<0:13>[MARG e2:unknown]
e2:unknown<0:13>[ARG x5:_hito_n]
x7:udef<0:3>[]
x7:densha_n_1<0:3>
x12:udef<4:7>[]
x12:_jidousha_n<4:7>
x13:_ya_p_conj<0:4>[L-INDEX x7:_densha_n_1, R-INDEX x12:_jidousha_n]
e23:_unten_s_2<8:10>[ARG1 x5:_hito_n, ARG2 x13:_ya_p_conj]
x5:udef<12:13>[]
_2:proposition_m<0:13>[MARG e23:_unten_s_2]
Figure 4: Dependency View of the Definition of ?U31 untenshu ?chauffeur?
3 Task
We define the task in this paper as ?allocating the
word sense tags for all content words included in
Lexeed as headwords, in each input sentence?. This
task is a kind of all-words task, however, a unique
point is that we focus on fundamental vocabulary
(basic words) in Lexeed and ignore other words. We
use Lexeed as the sense inventory. There are two
problems in resolving the task: how to build the
model and how to assign the word sense by using
the model for disambiguating the senses. We de-
scribe the word sense selection model we use in sec-
tion 4 and the method of word sense assignment in
section 5.
4 Word Sense Selection Model
All content words (i.e. basic words) in Lexeed are
classified into six groups by part-of-speech: noun,
verb, verbal noun, adjective, adverb, others. We
treat the first five groups as targets of disambiguat-
ing senses. We build five words sense models corre-
sponding to these groups. A model contains senses
for various words, however, features for a word are
discriminated from those for other words so that the
senses irrelevant to a target word are not selected.
For example, an n-gram feature following a target
word ?has-a-tail? for dog is distinct from that for cat.
In the remainder of this section, we describe the
features used in the word sense disambiguation.
First we used simple n-gram collocations, then a bag
of words of all words occurring in the sentence. This
was then enhanced by using ontological information
and predicate argument relations.
4.1 Word Collocations
Word collocations (WORD-Col) are basic and effec-
tive cues for WSD. They can be modelled by n-
gram and bag of words features, which are easily
extracted from a corpus. We used all unigrams, bi-
grams and trigrams which precede and follow the
target words (N-gram) and all content words in the
sentences where the target words occur (BOW).
480
# sample features
C1 ?COLWS:04?
C2 ?COLWSSC:C33:other person?
C3 ?COLWSHYP:0/1?
C4 ?COLWSHYPSC:C5:person?
C1 ?COLWS:\1?
C2 ?COLWSSC:C988:land vehicle?
C3 ?COLWSHYP:?1?
C4 ?COLWSHYPSC:C988:land vehicle?
C1 ?COLWS:?1?
C2 ?COLWSSC:C988:land vehicle?
C3 ?COLWSHYP:2?
C4 ?COLWSHYPSC:C988:land vehicle?
Table 2: Example semantic collocation features
(SEM-Col) extracted from the word sense tagged cor-
pus and the dictionary (Lexeed and GoiTaikei) and
the ontology which have the word senses and the se-
mantic classes linked to the semantic tags. The first
column numbers the feature template corresponding
to each example.
4.2 Semantic Features
We use the semantic information (sense tags and on-
tologies) in two ways. One is to enhance the collo-
cations and the other is to enhance dependency rela-
tions.
4.2.1 Semantic Collocations
Word surface features like N-gram and BOW in-
evitably suffer from data sparseness, therefore, we
generalize them to more abstract words or concepts
and also consider words having the same mean-
ings. We used the ontology described in Sec-
tion 2.3 to get hypernyms and synonyms and the
Goi-Taikei thesaurus to abstract the words to the se-
mantic classes. The superordinate classes at level
3, 4 and 5 are also added in addition to the original
semantic class. For example, \ densha ?train?
and ? jidousha ?automobile? are both gener-
alized to the semantic class ?C988:land vehicle?
(level 7). The superordinate classes are also used:
?C706:inanimate? (level 3), ?C760:artifact?
(level 4) and ?C986:vehicle? (level 5).
4.2.2 Semantic Dependencies
The semantic dependency features are based on
a predicate and its arguments taken from the ele-
mentary dependencies. For example, consider the
semantic dependency representation for densha ya
# sample features for ?U2d1
D1 ?PRED:?U2d, ARG1:0?
D1 ?PRED:?U2d, ARG2:\?
D1 ?PRED:?U2d, ARG2:??
D2 ?PRED:?U2d, ARG1:04?
D2 ?PRED:?U2d, ARG2:\1?
D2 ?PRED:?U2d, ARG2:?1?
D3 ?PRED:?U2d, ARG1SC:C33?
D3 ?PRED:?U2d, ARG2SC:C988?
D4 ?PRED:?U2d, ARG2SYN:???1?
D5 ?PRED:?U2d, ARG1HYP:0/1?
D5 ?PRED:?U2d, ARG2HYP:?1?
D5 ?PRED:?U2d, ARG2HYP:2?
D6 ?PRED:?U2d, ARG1HYPSC:C5?
D6 ?PRED:?U2d, ARG2HYPSC:C988?
D11 ?PRED:?U2d, ARG1:0, ARG2:\?
D22 ?PRED:?U2d, ARG1:04, ARG2:\1?
D23 ?PRED:?U2d, ARG1:04, ARG2:C1460 ?
D24 ?PRED:?U2d, ARG1:04, ARG2SYN:???1?
D32 ?PRED:?U2d, ARG1:C5, ARG2:\1?
D33 ?PRED:?U2d, ARG1:C5, ARG2:C988?
D55 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYP:?1?
D56 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYPSC:C988?
D65 ?PRED:?U2d, ARG1HYPSC:C5 , ARG2HYP:?1?
D322 ?PRED:C2003, ARG1:04, ARG2:\1?
Table 3: Example semantic features extracted from
the dependency tree in Figure 4. The first column
numbers the feature template corresponding to each
example.
jidousha-wo unten suru hito ?a person who drives a
train or car? given in Figure 4. The predicate un-
ten ?drive?, has two arguments: ARG1 hito ?person?
and ARG2 ya ?or?. The coordinate conjunction is
expanded out into its children, giving ARG2 densha
?train? and jidousha ?automobile?.
From these, we produce several features, a sam-
ple of them are shown in Table 3. One has all argu-
ments and their labels (D11). We also produce var-
ious back offs, for example the predicate with only
one argument at a time (D1-D3). Each combination
of predicate and its related argument(s) becomes a
feature.
For the next class of features, we used the sense
information from the corpus combined with the se-
mantic classes in the dictionary to replace each pred-
481
icate by its disambiguated sense, its hypernym, its
synonym (if any) and its semantic class. The seman-
tic classes for\1 and?1 are both ?988:land
vehicle?, while ?U1 is ?2003:motion? and 04
is ?4:human?. We also expand ?1 into its syn-
onym ???1 mo?ta?ka? ?motor car?.
The semantic class features provide a seman-
tic smoothing, as words are binned into the 2,700
classes. The hypernym/synonym features provide
even more smoothing. Both have the effect of mak-
ing more training data available for the disambigua-
tor.
4.3 Domain
Domain information is a simple and sometimes
strong cue for disambiguating the target words
(Gliozzo et al, 2005). For instance, the sense of
the word ?record? is likey to be different in the mu-
sical context, which is recalled by domain-specific
words like ?orchestra?, ?guitar?, than in the sport-
ing context. We use 12 domain categories like ?cul-
ture/art?, ?sport?, etc. which are similar to ones used
in directory search web sites. About 6,000 words
are automatically classified into one of 12 domain
categories by distributions in web sites (Hashimoto
and Kurohashi, 2007) and 10% of them are manually
checked. Polysemous words which belong to multi-
ple domains and neutral words are not classified into
any domain.
5 Search Algorithm
The conditional probability of the word sense for
each word is given by the word sense selection
model described in Section 4. In the initial state,
some of the semantic features, e.g. semantic col-
locations (SEM-Col) and word sense extensions for
semantic dependencies (SEM-Dep) are not available,
since no word senses for polysemous words have
been determined. It is not practical to count all com-
binations of word senses for target words, therefore,
we first try to decide the sense for that word which
is most plausible among all the ambiguous words,
then, disambiguate the next word by using the sense.
We use the beam search algorithm, which is sim-
ilar to that used for decoder in statistical machine
translation (Watanabe, 2004), for finding the plausi-
ble combination of word sense tags.
The algorithm is described as follows. For a pol-
ysemous word set in an input sentence {w1, . . . ,wn},
twik is the k-th word sense of word wi, W is a set
having words to be disambiguated, T is a list of re-
solved word senses. A search node N is defined as
[W,T ] and a score of a node N, s(N) is defined as
the probability that the word sense set T occurs in
the context. The beam search can be done as fol-
lows (beam width is b):
1. Create an initial node N0 = [T0,W0] (T0 = {},
W0 = {}) and insert the node into an initial
queue Q0.
2. For each node N in the queue Q, do the follow-
ing steps.
? For each wi (? W ), create W ?i by picking
out wi from W
? Create new lists T ?1, . . . ,T ?l by adding one
of word sense candidates twi1,. . . ,twil for wi
to T
? Create new nodes [W ?i ,T ?0 ], . . . ,[W ?i ,T ?l ] and
insert them into the queue Q?
3. Sort the nodes in Q? by the score s(N)
4. If the top node W in the queue Q? is empty,
adopt T as the combination of word senses and
terminate. Otherwise, pick out the top b nodes
from Q? and insert them into new queue Q, then
go back to 2
6 Evaluation
We trained and tested on the Lexeed Dictionary Def-
inition (LXD-DEF) and Example sections (LXD-EX) of
the Hinoki corpus (Bond et al, 2007). These have
about 75,000 definition and 46,000 example sen-
tences respectively. Some 54,000 and 36,000 sen-
tences of them are treebanked, i.e., they have the
syntactic trees and structural semantic information.
We used these sentences with the complete informa-
tion and selected 1,000 sentences out of each sen-
tence class as test sets (LXD-DEFtest, LXD-EXtest), and
the remainder is combined and used as a training
set (LXD-ALL). We also tested 1,000 sentences from
the Kyoto Corpus of newspaper text (KYOTOtest).
These sentences have between 3.4 (LXD-EXtest) ? 5.2
(KYOTOtest) polysemous words per sentence on av-
erage.
482
We use a maximum entropy / minimum diver-
gence (MEMD) modeler to train the word sense se-
lection model. We use the open-source Maximun
Entropy Modeling Toolkit2 for training, determining
best-performing convergence thresholds and prior
sizes experimentally. The models for five differ-
ent POSs were trained with each training sets: the
base model is word collocation model (WORD-Col),
and the semantic models built by semantic colloca-
tion (SEM-Col), semantic dependency (SEM-Dep) or
domain with WORD-Col (+SEM-Col, +SEM-Dep and
+DOMAIN).
Figure 5: Learning Curve
7 Results and Discussion
Table 4 shows the precision as the results of the word
sense disambiguation on the combination of LXD-
DEF and LXD-EX (LXD-ALL). The baseline method
selects the senses occurring most frequently in the
training corpus. Each row indicates the results us-
ing the baseline, word collocation (WORD-Col), the
combinations of WORD-Col and one of the seman-
tic features (+SEM-Col, +SEM-Dep and +DOMAIN),
e.g, +SEM-Col gives the results using WORD-Col and
SEM-Col, and all features (FULL).
There are significant improvements over the base-
line and the other results on all corpora. Basic word
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
collocation features (WORD-Col) give a vast improve-
ment. Extending this by using the ontological in-
formation (+SEM-Col) gives a further improvement
over the WORD-Col. Adding the predicate-argument
relationships (+SEM-Dep) improves the results even
more.
Table 6 shows the statistics of the target corpora.
The best result of LXD-DEFtest (80.7%) surpasses the
inter-annotator agreement (78.7%) in building the
Hinoki Sensebank. However, there is a wide gap
between the best results of KYOTOtest (60.4%) and
the inter-annotator agreement (83.3%), this suggests
other information such as the semantic classes for
named entities (including proper nouns and multi-
word expressions (MWE)) and broader contexts are
required. However, a model built on dictionary sen-
tences lacks these features. Even, so there is some
improvement.
The domain features (+DOMAIN) give small con-
tribution to the precision, since only intra-sentence
context is counted in this experiment. Unfortunately
dictiory definition and example sentences do not re-
ally have a useful context. We expect broader con-
text should make the domain features more effective
for the newspaper text (e.g. as in Stevenson (2003)),
Table 5 shows comparison of results of different
POSs. The semantic features (+SEM-Col and +SEM-
Dep) are particularly effective for verb and also give
moderate improvements on the results of the other
POSs.
Figure 5 shows the precisions of LXD-DEFtest in
changing the size of a training corpus, which is di-
vided into five partitions. The precision is saturated
in using four partitions (264,000 tokens).
These results of the dictionary sentences are close
to the best published results for the SENSEVAL-2
task (79.3% by Murata et al (2003) using a com-
bination of simple Bayes learners). However, we
are using a different sense inventory (Lexeed not
Iwanami (Nishio et al, 1994)) and testing over a dif-
ferent corpus, so the results are not directly compa-
rable. In future work, we will test over SENSEVAL-
2 data so that we can compare directly.
None of the SENSEVAL-2 systems used onto-
logical information, despite the fact that the dic-
tionary definition sentences were made available,
and there are several algorithms describing how to
extract such information from MRDs (Tsurumaru
483
Model Test Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULL
LXD-ALL LXD-DEFtest 72.8 78.4 79.8 80.2 78.1 80.7
LXD-EXtest 70.4 75.6 78.7 77.9 76.0 78.8
KYOTOtest 55.6 58.5 60.0 58.8 59.8 60.4
Table 4: The Precision of WSD
POS Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULL
Noun 65.5 68.7 69.6 69.4 68.9 69.8
Verb 60.3 66.9 71.0 70.6 67.7 72.6
VN 72.6 76.2 77.7 74.6 77.6 77.5
Adj 59.9 67.2 69.5 68.9 68.9 69.5
Adv 74.4 78.6 79.8 79.2 78.6 79.8
Table 5: The Precision of WSD (per Part-of-Speech)
et al, 1991; Wilkes et al, 1996; Nichols et al, 2005).
We hypothesize that this is partly due to the way the
task is presented: there was not enough time to ex-
tract and debug an ontology as well as build a dis-
ambiguation system, and there was no ontology dis-
tributed. The CRL system (Murata et al, 2003) used
a syntactic dependency parser as one source of fea-
tures (KNP: Kurohashi and Nagao (2003)), remov-
ing it decreased performance by around 0.6%.
8 Conclusions
We used the Hinoki corpus to test the importance of
lexical and structural information in word sense dis-
ambiguation. We found that basic n-gram features
and collocations provided a great deal of useful in-
formation, but that better results could be gained by
using ontological information and semantic depen-
dencies.
Acknowledgements
We would like to thank the other members of the
NTT Natural Language Research Group NTT Com-
munication Science laboratories for their support.
We would also like to express gratitude to the re-
viewers for their valuable comments and Professor
Zeng Guangping, Wang Daliang and Shen Bin of
the University of Science and Technology Beijing
(USTB) for building the demo system.
References
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: A treebank for text understanding. In Proceed-
ings of the First International Joint Conference on Natural
Language Processing (IJCNLP-04), pages 554?559. Hainan
Island.
Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2007. The Hi-
noki syntactic and semantic treebank of Japanese. Language
Resources and Evaluation. (Special issue on Asian language
technology).
Ulrich Callmeier. 2000. PET - a platform for experimentation
with efficient HPSG processing techniques. Natural Lan-
guage Engineering, 6(1):99?108.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.
2005. Minimal Recursion Semantics. An introduction. Re-
search on Language and Computation, 3(4):281?332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for HPSG
parse selection. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 25?32. Prague, Czech Republic.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL 2005). Ann
Arbor, U.S.
Chikara Hashimoto and Sadao Kurohashi. 2007. Construction
of domain dictionary for fundamental vocaburalry. In Pro-
ceedings of the ACL 2007 Main Conference Poster Sessions.
Association for Computational Linguistics, Prague, Czech
Republic.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In IPSG SIG: 2004-NLC-159, pages 75?82. Tokyo.
(in Japanese).
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 423?430. URL
http://www.aclweb.org/anthology/P03-1054.pdf.
484
Corpus
Annotated
Tokens #WS
Agreement
token (type) %Other Sense %Homonym %MWE %Proper Noun
LXD-DEF 199,268 5.18 .787 (.850) 4.2 0.084 1.5 0.046
LXD-EX 126,966 5.00 .820 (.871) 2.3 0.035 0.4 0.0018
KYOTO 268,597 3.93 .833 (.828) 9.8 3.3 7.9 5.5
Table 6: Corpus Statistics
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing sys-
tem. In Anne Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, chapter 14, pages 249?260. Kluwer Aca-
demic Publishers.
Robert Malouf and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
IJCNLP-04 Workshop: Beyond shallow analyses - For-
malisms and statistical modeling for deep analyses. JST
CREST. URL http://www-tsujii.is.s.u-tokyo.ac.
jp/bsa/papers/malouf.pdf.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computational Linguistics,
29(4):639?654.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and HItoshi Isahara. 2003. CRL at Japanese dictionary-
based task of SENSEVAL-2. Journal of Natural Language
Processing, 10(3):115?143. (in Japanese).
Eric Nichols and Francis Bond. 2005. Acquiring ontologies
using deep and shallow processing. In 11th Annual Meeting
of the Association for Natural Language Processing, pages
494?498. Takamatsu.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictionar-
ies. In Proceedings of the International Joint Conference on
Artificial Intelligence IJCAI-2005, pages 1111?1116. Edin-
burgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani. 1994.
Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dic-
tionary Edition 5]. Iwanami Shoten, Tokyo. (in Japanese).
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International Confer-
ence on Computational Linguistics: COLING-2002, pages
1253?7. Taipei, Taiwan.
Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing
the Wall Street Journal using a Lexical-Functional Grammar
and discriminative estimation techniques. In 41st Annual
Meeting of the Association for Computational Linguistics:
ACL-2003, pages 271?278.
Melanie Siegel and Emily M. Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion at the 19th International Conference on Computational
Linguistics, pages 1?8. Taipei.
Mark Stevenson. 2003. Word Sense Disambiguation. CSLI Pub-
lications.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The Hi-
noki sensebank ? a large-scale word sense tagged corpus of
Japanese ?. In Proceedings of the Workshop on Frontiers in
Linguistically Annotated Corpora 2006, pages 62?69. Syd-
ney. URL http://www.aclweb.org/anthology/W/W06/
W06-0608, (ACL Workshop).
Hiroaki Tsurumaru, Katsunori Takesita, Itami Katsuki, Toshi-
hide Yanagawa, and Sho Yoshida. 1991. An approach to
thesaurus construction from Japanese language dictionary.
In IPSJ SIGNotes Natural Language, volume 83-16, pages
121?128. (in Japanese).
Taro Watanabe. 2004. Example-based Statistical Machine
Translation. Ph.D. thesis, Kyoto University.
Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.
1996. Electric Words. MIT Press.
Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese treebank
with semantic knowledge. In Robert Dale, Jian Su Kam-Fai
Wong and, and Oi Yee Kwong, editors, Natural Language
Processing ? IJCNLP 005: Second International Joint Con-
ference Proceedings, pages 70?81. Springer-Verlag.
485
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929?937,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Hypernym Discovery Based on Distributional Similarity                     
and Hierarchical Structures 
Ichiro Yamada?, Kentaro Torisawa?, Jun?ichi Kazama?, Kow Kuroda?,  
Masaki Murata?, Stijn De Saeger?, Francis Bond? and Asuka Sumida? 
 
?National Institute of Information and Communications Technology 
3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN 
{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp
?Japan Advanced Institute of Science and Technology 
1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPAN 
a-sumida@jaist.ac.jp 
 
Abstract 
This paper presents a new method of devel-
oping a large-scale hyponymy relation data-
base by combining Wikipedia and other Web 
documents. We attach new words to the hy-
ponymy database extracted from Wikipedia 
by using distributional similarity calculated 
from documents on the Web. For a given tar-
get word, our algorithm first finds k similar 
words from the Wikipedia database. Then, 
the hypernyms of these k similar words are 
assigned scores by considering the distribu-
tional similarities and hierarchical distances 
in the Wikipedia database. Finally, new hy-
ponymy relations are output according to the 
scores. In this paper, we tested two distribu-
tional similarities. One is based on raw verb-
noun dependencies (which we call ?RVD?), 
and the other is based on a large-scale clus-
tering of verb-noun dependencies (called 
?CVD?). Our method achieved an attachment 
accuracy of 91.0% for the top 10,000 rela-
tions, and an attachment accuracy of 74.5% 
for the top 100,000 relations when using 
CVD. This was a far better outcome com-
pared to the other baseline approaches. Ex-
cluding the region that had very high scores, 
CVD was found to be more effective than 
RVD. We also confirmed that most relations 
extracted by our method cannot be extracted 
merely by applying the well-known lexico-
syntactic patterns to Web documents. 
1 Introduction 
Large-scale taxonomies such as WordNet (Fell-
baum 1998) play an important role in informa-
tion extraction and question answering. However, 
extremely high costs are borne to manually en-
large and maintain such taxonomies. Thus, appli-
cations using these taxonomies tend to face the 
drawback of data sparseness. This paper presents 
a new method for discovering a large set of hy-
ponymy relations. Here, a word1 X is regarded as 
a hypernym of a word Y if Y is a kind of X or Y 
is an instance of X. We are able to generate 
large-scale hyponymy relations by attaching new 
words to the hyponymy database extracted from 
Wikipedia (referred to as ?Wikipedia relation 
database?) by using distributional similarity cal-
culated from Web documents. Relations ex-
tracted from Wikipedia are relatively clean. On 
the other hand, reliable distributional similarity 
can be calculated using a large number of docu-
ments on the Web. In this paper, we combine the 
advantages of these two resources.  
Using distributional similarity, our algorithm 
first computes k similar words for a target word. 
Then, each k similar word assigns a score to its 
ancestors in the hierarchical structures of the 
Wikipedia relation database. The hypernym that 
has the highest score for the target word is se-
lected as the hypernym of the target word. Figure 
1 is an overview of the proposed approach. 
In the experiment, we extracted hypernyms for 
approximately 670,000 target words that are not 
included in the Wikipedia relation database but 
are found on the Web. We tested two distribu-
tional similarities: one based on raw verb-noun 
dependencies (RVD) and the other based on a 
large-scale clustering of verb-noun dependencies 
(CVD). The experimental results showed that the 
proposed methods were more effective than the 
other baseline approaches. In addition, we con-
firmed that most of the relations extracted by our 
method could not be extracted using the lexico-
syntactic pattern-based method.  
In the remainder of this paper, we first intro-
                                                 
1 In this paper, we use the term ?word? for both ?a 
single-word word? and ?a multi-word word.? 
929
duce some related works in Section 2. Section 3 
describes the Wikipedia relation database. Sec-
tion 4 describes the distributional similarity cal-
culated by the two methods. In Section 5, we 
describe a method to discover an appropriate 
hypernym for each target word. The experimen-
tal results are presented in Section 6 before con-
cluding the paper in Section 7. 
2 Related Works 
Most previous researchers have relied on lex-
ico-syntactic patterns for hyponymy acquisition. 
Lexico-syntactic patterns were first used by 
Hearst (1992). The patterns used by her included 
?NP0 such as NP1,? in which NP0 is a hypernym 
of NP1. Using these patterns as seeds, Hearst dis-
covered new patterns by which to semi-
automatically extract hyponymy relations. Pantel 
et al (2004a) proposed a method to automatical-
ly discover the patterns using a minimal edit dis-
tance. Ando et al (2003) applied predefined lex-
ico-syntactic patterns to Japanese news articles. 
Snow et al (2005) generalized these lexico-
syntactic pattern-based methods by using depen-
dency path features for machine learning. Then, 
they extended the framework such that this me-
thod was capable of making use of heterogenous 
evidence (Snow et al 2006). These pattern-based 
methods require the co-occurrences of a target 
word and the hypernym in a document. It should 
be noted that the requirement of such co-
occurrences actually poses a problem when we 
extract a large set of hyponymy relations since 
they are not frequently observed (Shinzato et al 
2004, Pantel et al 2004b). 
Clustering-based methods have been proposed 
as another approach. Caraballo (1999), Pantel et 
al. (2004b), and Shinzato et al (2004) proposed a 
method to find a common hypernym for word 
classes, which are automatically constructed us-
ing some measures of word similarities or hierar-
chical structures in HTML documents. Etzioni et 
al. (2005) used both a pattern-based approach 
and a clustering-based approach. The required 
amount of co-occurrences is significantly re-
duced due to class-based generalization 
processes. Note that these clustering-based me-
thods obtain the same hypernym for all the words 
in a particular class. This causes a problem for 
selecting an appropriate hypernym for each word 
in the case when the granularity or the construc-
tion of the classes is incorrect. Figure 2 shows 
the drawbacks of the existing approaches. 
Ponzetto et al (2007) and Sumida et al (2008) 
proposed a method for acquiring hyponymy rela-
tions from Wikipedia. This Wikipedia-based ap-
proach can extract a large volume of hyponymy 
relations with high accuracy. However, it is also 
true that this approach does not account for many 
words that usually appear in Web documents; 
this could be because of the unbalanced topics in 
Wikipedia or merely because of the incomplete 
coverage of articles on Wikipedia. Our method 
can target words that frequently appear on the 
Web but are not included in the Wikipedia rela-
tion database, thus making the results of the Wi-
kipedia-based approach richer and more ba-
lanced. Our approach uses distributional similari-
Figure 1: Overview of the proposed approach. 
hypernym : 
Target word:  Selected from the Web 
: word
k similar words
No direct co-occurrences of 
hypernym and hyponym in 
corpora are needed.
Selected from hypernyms in the 
Wikipedia relation database.
A hypernym is selected for 
each word independently.
Wikipedia relation database
Wikipedia-based approach
(Ponzetto et al 2007 and 
Sumida et al 2008)
Hyponymy relations are 
extracted using the layout 
information of Wikipedia.
Wikipedia
Figure 2: Drawbacks in existing approaches for hypo-
nymy acquisition. 
Pattern-based method
(Hearst 1992, Pantel et al 
2004a, Ando et al 2003, 
Snow et al 2005, Snow et al 
2006, and Etzioni et al 2005)
Clustering-based method
(Caraballo 1999, Pantel et al 
2004b, Shinzato et al 2004, 
and Etzioni et al 2005)
DocumentsCorpus/documents
Co-occurrences 
in a pattern are 
needed 
hypernym such as word hypernym ..?   word
word
word
wordword
Word Class
word
The same hypernym 
is selected for all 
words in a class.
930
ty, which is computed based on the noun-verb 
dependency profiles on the Web. The use of dis-
tributional similarity resembles the clustering-
based approach; however, our method can select 
a hypernym for each word independently, and it 
does not suffer from class granularity mismatch 
or the low quality of classes. In addition, our ap-
proach exploits the hierarchical structures of the 
Wikipedia hypernym relations.  
3 Wikipedia Relation Database 
Our Wikipedia relation database is based on the 
extraction method of Sumida et al (2008). They 
proposed a method of automatically acquiring 
hyponymy relations by focusing on the hierar-
chical layout of articles on Wikipedia. By way of 
an example, Figure 3 shows part of the source 
code clipped from the article titled ?Penguin.? 
An article has hierarchical structures composed 
of titles, sections, itemizations, etc. The entire 
article is divided into sections titled ?Anatomy,? 
?Mating habits,? ?Systematics and evolution,? 
?Penguins in popular culture,? and so on. The 
section ?Systematics and evolution? has a sub-
section ?Systematics,? which is further divided 
into ?Aptenodytes,? ?Eudyptes,? and so on. 
Some of these section-subsection relations can be 
regarded as valid hyponymy relations. In this 
article, relations such as the one between ?Apte-
nodytes? and ?Emperor Penguin? and that be-
tween ?Book? and ?Penguins of the World? are 
valid hyponymy relations.  
First, Sumida et al (2008) extracted hypony-
my relation candidates from hierarchical struc-
tures on Wikipedia. Then, they selected proper 
hyponymy relations using a support vector ma-
chine classifier. They used several kinds of fea-
tures for the hyponymy relation candidate, such 
as a POS tag for each word, the appearance of 
morphemes of each word, the distance between 
two words in the hierarchical structures of Wiki-
pedia, and the last character of each word. As a 
result of their experiments, approximately 2.4 
million hyponymy relations in Japanese were 
extracted, with a precision rate of 90.1%.  
Compared to the traditional taxonomies, these 
extracted hyponymy relations have the following 
characteristics (Fellbaum 1998, Bond et al 2008). 
(a) The database includes a more extensive 
vocabulary. 
(b)  The database includes a large number of 
named entities. 
Popular Japanese taxonomies GoiTaikei (Ike-
hara et al 1997) and Bunrui-Goi-Hyo (1996) 
contain approximately 300,000 words and 
96,000 words, respectively. In contrast, the ex-
tracted hyponymy relations contain approximate-
ly 1.2 million hyponyms and are undoubtedly 
much larger than the existing taxonomies. 
Another difference is that since Wikipedia covers 
a large number of named entities, the extracted 
hyponymy relations also contain a large number 
of named entities.  
Note that the extracted relations have a hierar-
chical structure because one hypernym of a cer-
tain word may also be the hyponym of another 
hypernym. However, we observed that the depth 
of the hierarchy, on an average, is extremely 
shallow. To make the hierarchy appropriate for 
our method, we extended these into a deeper hie-
rarchical structure. The extracted relations in-
clude many compound nouns as hypernyms, and 
we decomposed a compound noun into a se-
quence of nouns using a morphological analyzer. 
Since Japanese is a head-final language, the suf-
fix of a noun sequence becomes the hypernym of 
the original compound noun if the suffix forms 
another valid compound noun. We extracted suf-
fixes of compound nouns and manually checked 
whether they were valid compound nouns; then, 
we constructed a hierarchy of compound nouns. 
The hierarchy can be extended such that it in-
cludes the hyponyms of the original hypernym 
and the resulting hierarchy constitutes a hierar-
chical taxonomy. We use this hierarchical tax-
onomy as a target for expansion.2  
                                                 
2  Note that this modification was performed as part of 
another project of ours aimed at constructing a large-scale 
and clean hypernym knowledge base by human annotation. 
We do not think this cost is directly relevant to the method 
proposed here. 
Figure 3: A part of source code clipped from the 
article ?Penguin? in Wikipedia. 
'''Penguins''' are a group of 
[[Aquatic animal|aquatic]], 
[[flightless bird]]s. 
== Anatomy == 
== Mating habits == 
==Systematics and evolution== 
===Systematics=== 
* Aptenodytes 
**[[Emperor Penguin]] 
** [[King Penguin]] 
* Eudyptes 
== Penguins in popular culture == 
== Book == 
* Penguins 
* Penguins of the World 
== Notes == 
* Penguinone 
* the [[Penguin missile]] 
[[Category:Penguins]] 
[[Category:Birds]]
931
4 Distributional Similarity 
The distributional hypothesis states that words 
that occur in similar contexts tend to be semanti-
cally similar (Harris 1985). In this section, we 
first introduce distributional similarity based on 
raw verb-noun dependencies (RVD). To avoid 
the sparseness problem of the co-occurrence of 
verb-noun dependencies, we also use distribu-
tional similarity based on a large-scale clustering 
of verb-noun dependencies (CVD). 
In the experiment mentioned in the following 
section, we used the TSUBAKI corpus (Shinzato 
et al 2008) to calculate distributional similarity. 
This corpus provides a collection of 100 million 
Japanese Web pages containing 6 ? 109
 
sentences. 
4.1 Distributional Similarity Based on RVD 
When calculating the distributional similarity 
based on RVD, we use the triple <v, rel, n>, 
where v is a verb, n is a noun phrase, and rel 
stands for the relation between v and n. In Japa-
nese, a relation rel is represented by postposi-
tions attached to n and the phrase composed of n 
and rel modifies v. Each triple is divided into two 
parts. The first is <v, rel> and the second is n. 
Then, we consider the conditional probability of 
occurrence of the pair <v, rel>: P(<v, rel>|n).  
P(<v, rel>|n) can be regarded as the distribution 
of the grammatical contexts of the noun phrase n. 
The distributional similarity can be defined as 
the distance between these distributions. There 
are several kinds of functions for evaluating the 
distance between two distributions (Lee 1999). 
Our method uses the Jensen-Shannon divergence. 
The Jensen-Shannon divergence between two 
probability distributions, )|( 1nP ?  and )|( 2nP ? , 
can be calculated as follows: 
 
)),
2
)|()|(
||)|((
)
2
)|()|(
||)|(((
2
1
))|(||)|((
21
2
21
1
21
nPnP
nPD
nPnP
nPD
nPnPD
KL
KL
JS
?+??+
?+??=
??
 
 
where DKL indicates the Kullback-Leibler diver-
gence and is defined as follows: 
 
.
)|(
)|(
log)|())|(||)|((
2
1
121 ? ???=?? nP nPnPnPnPDKL  
 
Finally, the distributional similarity between 
two words, n1 and n2, is defined as follows: 
 
)).|(||)|((1),( 2121 nPnPDnnsim JS ???=  
 
This similarity assumes a value from 0 to 1. If 
two words are similar, the value will be close to 
1; if two words have entirely different meanings, 
the value will be 0.
 
In the experiment, we used 1,000,000 noun 
phrases and 100,000 pairs of verbs and postposi-
tions to calculate the probability P(<v, rel>|n) 
from the dependency relations extracted from the 
above-mentioned Web corpus (Shinzato et al 
2008). The probabilities are computed using the 
following equation by modifying for the fre-
quency using the log function: 
 
?
>?<
+><
+><=><
Drelv
nrelvf
nrelvf
nrelvP
,
1),,(log(
1)),,(log(
)|,(
,0),,(if >>< nrelvf
  
where f(<v, rel, n>) is the frequency of a triple 
<v, rel, n> and D is the set defined as { <v, rel > | 
f(<v, rel, n>) > 0 }. In the case of f(<v, rel, n>) = 
0, P(<v, rel>|n) is set to 0.  
Instead of using the observed frequency di-
rectly as in the usual maximum likelihood esti-
mation, we modified it as above. Although this 
might seems strange, this kind of modification is 
common in information retrieval as a term 
weighing method (Manning et al 1999) and  it is 
also applied in some studies to yield better word 
similarities (Terada et al 2006, Kazama et al 
2009). We also adopted this idea in this study. 
4.2 Distributional Similarity Based on CVD 
Rooth et al (1999) and Torisawa (2001) showed 
that EM-based clustering using verb-noun de-
pendencies can produce semantically clean noun 
clusters. We exploit these EM-based clustering 
results as the smoothed contexts for noun n. In 
Torisawa?s model (2001), the probability of oc-
currence of the triple <v, rel, n> is defined as 
follows: 
 
,)()|()|,(
),,(
? ? ><=
><
Aadef aPanParelvP
nrelvP
 
 
where a denotes a hidden class of <v,rel> and n. 
In this equation, the probabilities P(<v,rel>|a), 
P(n|a), and P(a) cannot be calculated directly 
because class a is not observed in a given corpus. 
The EM-based clustering method estimates these 
probabilities using a given corpus. In the E-step, 
932
the probability P(a|<v,rel>) is calculated. In the 
M-step, the probabilities P(<v,rel>|a), P(n|a), 
and P(a) are updated to arrive at the maximum 
likelihood using the results of the E-step. From 
the results of estimation of this EM-based clus-
tering method, we can obtain the probabilities 
P(<v,rel>|a), P(n|a), and P(a) for each <v, rel>, n, 
and a. Then, P(a|n) is calculated by the following 
equation: 
 
.
)()|(
)()|(
)|( ? ?= Aa aPanP
aPanP
naP  
 
P(a|n) can be used to find the class of n. For 
example, the class that has the maximum P(a|n) 
can be regarded as the class to which n belongs. 
Noun phrases that occur with similar pairs 
<v,rel> tend to be classified in the same class. 
Kazama et al (2008) proposed the paralleliza-
tion of this EM-based clustering with the aim of 
enabling large-scale clustering and using the re-
sulting clusters in named entity recognition. Ka-
zama et al (2009) reported the calculation of 
distributional similarity using the clustering re-
sults. The distributional similarity was calculated 
by the Jensen-Shannon divergence, which was 
used in this paper. Similar to the case in Kazama 
et al, we performed word clustering using 
1,000,000 noun phrases and 2,000 classes. Note 
that the frequencies of dependencies were mod-
ified with the log function, as in RVD, described 
in the previous section. 
5 Discovering an Appropriate Hyper-
nym for a Target word 
In the Wikipedia relation database, there are 
about 95,000 hypernyms and about 1.2 million 
hyponyms. In both RVD and CVD, the words 
used were selected according to the number (the 
number of kinds, not the frequency) of <v, rel >s 
that n has dependencies in the data. As a result, 1 
million words were selected. The number of 
common words that are also included in the Wi-
kipedia relation database are as follows: 
 
Hypernyms     28,015 (common hypernyms) 
Hyponyms   175,022 (common hyponyms) 
 
These common hypernyms become candidates 
for hypernyms for a target word. On the other 
hand, the common hyponyms are used as clues 
for identifying appropriate hypernyms. 
In our task, the potential target words are 
about 810,000 in number and are not included in 
the Wikipedia relation database. These include 
some strange words or word phrases that are ex-
tracted due to the failure of morphological analy-
sis. We exclude these words using simple rules. 
Consequently, the number of target words for our 
process is reduced to about 670,000.  
In the following section, we outline the scor-
ing method that uses k similar words to discover 
an appropriate hypernym for a target word. We 
also explain several baseline approaches that use 
distributional similarity. 
5.1 Scoring with k similar Words 
In this approach, we first calculate the similari-
ties between the common hyponyms and a target 
word and select the k most similar common hy-
ponyms. Here, we use a similarity threshold val-
ue Smin to avoid the effect of words having lower 
similarities. If the similarity is less than the thre-
shold value, the word is excluded from the set of 
k similar words. Next, each k similar word votes 
a score to its ancestors in the hierarchical struc-
tures of the Wikipedia relation database. The 
score used to vote for a hypernym nhyper is as fol-
lows: 
 
,),(
)(
)()(
1),(?
??
? ?=
trghyperhypo
hypohyper
nksimilarnDescn
hypotrg
nnr
hyper
nnsimd
nscore
 
 
where ntrg is the target word, Desc(nhyper) is the 
descendant of the hypernym nhyper, ksimilar(ntrg) 
is the k similar word of ntrg, 
1),( ?hypohyper nnrd is a 
penalty that depends on the differences in the 
depth of hierarchy, d is a parameter for the penal-
ty value and has a value between 0 and 1, and 
r(ntrg, nhypo) is the difference in the depth of hie-
rarchy between ntrg and nhypo. sim(ntrg,nhypo) is a 
distributional similarity between ntrg and nhypo.  
As a result of scoring, each hypernym has a 
score for the target word. The hypernym that has 
the highest score for the target word is selected 
as its hypernym. The hyponymy relations thus 
produced are ranked according to the scores. 
Figure 4 shows an example of the scoring 
process. In this example, we use CitroenAX as the 
target word whose hypernym will be identified. 
First, the k similar words are extracted from the 
common hyponyms in the Wikipedia relation: 
Opel Astra, TVR Tuscan, Mitsubishi Minica, and 
Renault Lutecia are extracted. Next, each k simi-
lar word votes a score to its ancestors. The words 
Opel Astra, TVR Tuscan, and Renault Lutecia 
vote to their parent car and the word Mitsubishi 
933
Minica votes to its parent mini-vehicle and its 
grandparent car with a small penalty. Finally, the 
hypernym car, which has the highest score, is 
selected as the hypernym of the target word Ci-
troenAX. 
5.2 Baseline Approaches 
Using distributional similarity, we can also de-
velop the following baseline approaches to dis-
cover hyponymy relations. 
 
Selecting the hypernym of the most similar hy-
ponym (baseline approach 1) 
We use the heuristics that similar words tend to 
have the same hypernym. In this approach, we 
first calculate the similarities between the com-
mon hyponyms and the target word. The com-
mon hyponym most similar to the target word is 
extracted. Then, the parent of the extracted 
common hyponym is regarded as the hypernym 
of the target word. This approach outputs several 
hypernyms when the most similar hyponym has 
several hypernyms. This approach can be consi-
dered to be the same as the scoring method using 
k similar words when k = 1. We use the distribu-
tional similarity between the target word and the 
most similar hyponym in the Wikipedia relation 
database as the score for the appropriateness of 
the resulting hyponymy. 
 
Selecting the most similar hypernym (baseline 
approach 2) 
The distributional similarity between the com-
mon hypernym and the target word is calculated. 
Then, the hypernym that has the highest distribu-
tional similarity is regarded as the hypernym of 
the target word. The similarity is used as the 
score of the appropriateness of the produced hy-
ponymy. 
 
Scoring based on the average similarity of the 
hypernym?s children (baseline approach 3) 
This approach uses the probabilistic distributions 
of the hypernym?s children. We define the prob-
ability )|( hyperchild nP ? characterized by the children 
of the hypernym nhyper, as follows: 
 
,
)(
)()|(
)|(
)(
)(
?
?
?
?
?
=?
hyperhypo
hyperhypo
nChn
hypo
nChn
hypohypo
hyperchild nP
nPnP
nP  
 
where Ch(nhyper) is a set of all children of nhyper. 
Then, distributional similarities between a com-
mon hypernym nhyper and the target word nhypo are 
calculated. The hypernym that has the highest 
distributional similarity is selected as the hyper-
nym of the word. This distributional similarity is 
used as the score of the appropriateness of the 
produced hyponymy. 
If a hypernym has only a few children, the re-
liability of the probabilistic distribution of 
hypernym defined here will be low because the 
Wikipedia relation database includes some incor-
rect relations. For this reason, we use the hyper-
nym only if the number of children it has is more 
than a threshold value.  
6 Experiments 
We evaluated our proposed methods by using it 
in experiments to discover hypernyms from the 
Wikipedia relation database for the target words 
extracted from about 670,000 noun phrases.  
6.1 Parameter Estimation by Preliminary 
Experiments 
In the proposed methods, there are several para-
meters. We performed parameter optimization by 
randomly selecting 694 words as development 
data in our preliminary experiments. The hyper-
nyms of these words were determined manually. 
We adjusted the parameters so that each method 
achieved the best performance for this develop-
ment data. 
The parameters in the scoring method with k 
similar words were adjusted as follows3:  
 (RVD) 
Number of similar words:         k = 100. 
Similarity threshold:           Smin = 0.05. 
Penalty value for ancestors:    d = 0.6. 
                                                 
3 We tested the parameter values k = {100, 200, 300, 400, 
500, 600, 700, 800, 900, 1000}, Smin={0, 0.05, 0.1, 0.15, 0.2, 
0.25, 0.3, 0.35, 0.4} and d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 
0.8, 0.85, 0.9, 0.95, 1.0}. 
Figure 4: Overview of the scoring process.
car
CitroenAX
mini
vehicle
hybrid 
vehicle
Opel 
Astra
Renault 
Lutecia
Mitsubishi
Minica
k similar words
Each k-similar word
votes the score to its 
ancestors in the Wikipedia 
relation database.
Target word selected 
from the Web text (ntrg).
TVR 
Tuscan
: common hypernym(nhyper)
: k similar word &  
common hyponym(nhypo)
x d1
x d0
x d0
934
(CVD) 
Number of similar words:         k = 200. 
Similarity threshold:                Smin = 0.3. 
Penalty value for ancestors:    d = 0.6. 
 
The parameter in baseline approach 3 was ad-
justed as follows: 
Threshold for the number of children: 20. 
6.2 Evaluation of the Experimental Results 
on the Basis of Score Ranking 
Using the adjusted parameters, we conducted 
experiments to extract the hypernym of each tar-
get word with the help of the scoring method 
based on k similar words. In these experiments, 
two kinds of distributional similarity mentioned 
in Section 4 were exploited individually. The 
words that were used in the development data 
were excluded.  
We also conducted a comparative experiment 
in which the parameter value for the penalty of 
the hierarchal difference, d, was set to 0 to clari-
fy the ability of using hierarchal structures in the 
k similar words method. This means each k simi-
lar word votes only to their parent. 
We then judged the quality of each acquired 
hypernym. The evaluation data sets were sam-
pled from the top 1,000, 10,000, 100,000, and 
670,000 results that were ranked according to the 
score of each method. Then, against 200 samples 
that were randomly sampled from each set, one 
of the authors judged whether the hypernym ex-
tracted by each method for the target word was 
correct or not. In this evaluation, if the sentence 
?The target word is a kind of the hypernym? or 
?The target word is an instance of the hypernym? 
was consistent, the extracted hyponymy was 
judged as correct. It should be noted that the out-
puts of the compared methods are combined and 
shuffled to enable fair comparison. In addition, 
baseline approach 1 extracted several hypernyms 
for the target word. In this case, we judged the 
hypernym as correct when the case where one of 
the hypernyms was correct.  
The precision of each result is shown in Table 
1. The results of the k similar words method are 
far better than those of the other baseline me-
thods. In particular, the k similar words method 
with CVD outperformed the methods of the k 
similar words where the parameter value d was 
set to 0 and the method using RVD except for the 
top 1,000 results. This means that the use of hie-
rarchal structures and the clustering process for 
calculating distributional similarity are effective 
for this task. We confirmed the significant differ-
ences of the proposed method (CVD) as com-
pared with all the baseline approaches at the 1% 
significant level by the Fisher?s exact test (Hays 
1988). 
The precision of baseline approach 2 that se-
lected the most similar hypernym was the worst 
among all the methods. There were words that 
were similar to the target word among the hyper-
nyms extracted incorrectly. For example, the 
word semento-kojo (cement factory) was ex-
tracted for the hypernym of the word kuriningu-
kojo (dry cleaning plant). It is difficult to judge 
whether the word is a hypernym or just a similar 
word by using only the similarity measure. 
As for the results of baseline approach 1 using 
the most similar hyponym and baseline approach 
3 using the similarity of the set of hypernym?s 
children, the noise on the Wikipedia relation da-
tabase decreased the precision. Moreover, over-
specified hypernyms were extracted incorrectly 
by these methods. In contrast, the method of 
scoring based on the use of k similar words was 
robust against noise because it uses the voting 
approach for the similarities. Further, this me-
thod can extract hypernyms that are not over-
specific because it uses all descendants for scor-
ing.  
Table 2 shows some examples of relations ex-
tracted by the k similar words method using 
CVD. 
 
Table 1:  Precision of each approach based on the score ranking. CVD represents the method that uses the dis-
tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the 
one based on raw verb-noun dependencies. 
 k-similar words
(CVD) 
k-similar words
(RVD) 
k-similar words
(CVD, d = 0)
Baseline  
approach 1 
(CVD) 
Baseline  
approach 2 
(CVD) 
Baseline  
approach 3 
(CVD) 
1,000 0.940 1.000 0.850 0.730 0.290 0.630 
10,000 0.910 0.875 0.875 0.555 0.300 0.445 
100,000 0.745 0.710 0.730 0.500 0.280 0.435 
670,000 0.520 0.500 0.470 0.345 0.115 0.170 
935
6.3 Investigation of the Extracted Relation 
Overlap with a Conventional Method 
We randomly sampled 300 hyponymy rela-
tions that were extracted correctly using the k 
similar words method exploiting CVD and inves-
tigated whether or not these relations can be ex-
tracted by the conventional method based on the 
lexico-syntactic pattern. The possible hyponymy 
relations were extracted using the pattern-based 
method (Ando et al 2003) from the TSUBAKI 
corpus (Shinzato et al 2008). From a comparison 
of these relations, we found only 57 common 
hyponymy relations. That is, the remaining 243 
hyponymy relations were not included in the 
possible hyponymy relations. This result indi-
cates that our method can acquire the hyponymy 
relations that cannot be extracted by the conven-
tional pattern-based method. 
6.4 Discussions 
We investigated the reason for the errors gener-
ated by the method of scoring using k similar 
words exploiting CVD. We conducted experi-
ments on hypernym extraction targeting 694 
words in the development data mentioned in Sec-
tion 6.1. Among these, 286 relations were ex-
tracted incorrectly. In these relations, there were 
some frequent hypernyms. For example, the 
word sakuhin (work) appeared 28 times and hon 
(book) appeared 20 times. As shown in Table 2, 
hon (book) was also extracted for the target word 
meru-seminah (mail seminar). It is really diffi-
cult even for a human to identify whether the 
title is that of the book or the event. If we can 
identify these difficult hypernyms in advance, we 
can improve precision by excluding them from 
the target hypernyms. This will be one of the top-
ics for future study. 
7 Conclusion 
In this paper, we proposed a method for disco-
vering hyponymy relations between nouns by 
fusing the Wikipedia relation database and words 
from the Web. We demonstrated that the method 
using k similar words has high accuracy. The 
experimental results showed the effectiveness of 
using hierarchal structures and the clustering 
process for calculating distributional similarity 
for this task. The experimental results showed 
that our method could achieve 91.0% attachment 
accuracy for the top 10,000 hyponymy relations 
and 74.5% attachment accuracy for the top 
100,000 relations when using the clustering-
based similarity. We confirmed that most rela-
tions extracted by the proposed method could not 
be handled by the lexico-syntactic pattern-based 
method. Future work will be to filter out difficult 
hypernyms for hyponymy extraction process to 
achieve higher precision. 
References 
M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic 
Extraction of Hyponyms from Newspaper Using 
Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-
NL-157, pp. 77?82 (in Japanese). 
F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto. 
2008. Boot-strapping a WordNet Using Multiple 
Existing WordNets. In the 6th International Confe-
rence on Language Resources and Evaluation 
(LREC), Marrakech.  
Bunruigoihyo. 1996. The National Language Re-
search Institute (in Japanese). 
S. A. Caraballo. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text. In 
Proceedings of the Conference of the Association 
for Computational Linguistics (ACL). 
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld and A. Yates. 2005. 
Unsupervised Named-Entity Extraction from the 
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Table2:  Hypernym discovery results by the k-similar 
words based approach (CVD). The underline indi-
cates the hypernyms which are extracted incorrectly.
Score Target word Extracted hypernym 
58.6 INDIVI burando 
(fashion label)
54.3 kureome (Cleome) hana (flower)
34.4 UOKR  gemu (game)
21.7 Okido (Okido) machi (town)
20.5 Sumatofotsu 
(Smart fortwo) 
kuruma  
(car) 
15.6 Fukagawameshi 
(Fukagawa rice)
ryori (dish) 
8.9 John Barry sakkyokuka 
 (composer)
8.5 JVM sofuto-wea 
(software) 
6.6 metangasu 
(methane gas) 
genso 
(chemical element)
5.4 me-ru semina 
(mail seminar) 
Hon (book) 
3.9 gurometto 
(grommet) 
shohin 
(merchandise)
3.1 supuringubakku  
(spring back) 
gensho 
(phenomenon)
936
Database. Cambridge, MA: MIT Press. 
Z. Harris. 1985. Distributional Structure. In Katz, J. J. 
(ed.) The Philosophy of Linguistics, Oxford Uni-
versity Press, pp. 26?47. 
W. L. Hays. 1988. Statistics: Analyzing Qualitative 
Data, Rinehart and Winston, Inc., Ch. 18, pp. 769?
783. 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of 
the 14th Conference on Computational Linguistics 
(COLING), pp. 539?545.  
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-
kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997. 
Goi-Taikei A Japanese Lexicon, Iwanami Shoten. 
J. Kazama and K. Torisawa. 2008. Inducing Gazet-
teers for Named Entity Recognition by Large-scale 
Clustering of Dependency Relations. In Proceed-
ings of ACL-08: HLT, pp. 407?415. 
J. Kazama, Stijn De Saeger, K. Torisawa and M. Mu-
rata. 2009. Generating a Large-scale Analogy List 
Using a Probabilistic Clustering Based on Noun-
Verb Dependency Profiles. In 15th Annual Meeting 
of the Association for Natural Language 
Processing, C1?3 (in Japanese). 
L. Lee. 1999. Measures of Distributional Similarity. 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, pp. 25?
32. 
C. D. Manning and H. Schutze. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press. 
P. Pantel, D. Ravichandran and E. Hovy. 2004a. To-
wards Terascale Knowledge Acquisition. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics. 
P. Pantel and D. Ravichandran. 2004b. Automatically 
Labeling Semantic Classes. In Proceedings of the 
Human Language Technology and North American 
Chapter of the Association for Computational Lin-
guistics Conference. 
S. P. Ponzetto, and M. Strube. 2007. Deriving a Large 
Scale Taxonomy from Wikipedia. In Proceedings 
of the 22nd National Conference on Artificial Intel-
ligence, pp. 1440?1445. 
M. Rooth, S. Riezler, D. Presher, G. Carroll and F. 
Beil. 1999. Inducing a Semantically Annotated 
Lexicon via EM-based Clustering. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics, pp. 104?111. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypo-
nymy Relations from Web Documents. In Proceed-
ings of HLT-NAACL, pp. 73?80. 
K. Shinzato, D. Kawahara, C. Hashimoto and S. Ku-
rohashi. 2008. A Large-Scale Web Data Collection 
as A Natural Language Processing Infrastructure. 
In the 6th International Conference on Language 
Resources and Evaluation (LREC). 
R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Dis-
covery. NIPS 2005. 
R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogenous Evidence. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th annual 
meeting of the Association for Computational Lin-
guistics, pp. 801?808. 
A. Sumida, N. Yoshinaga and K. Torisawa. 2008. 
Boosting Precision and Recall of Hyponymy Rela-
tion Acquisition from Hierarchical Layouts in Wi-
kipedia. In the 6th International Conference on 
Language Resources and Evaluation (LREC). 
A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool 
for Constructing a Synonym Dictionary using con-
text Information. In proceedings of IPSJ SIG Tech-
nical Reports, vol.2006 No.124, pp. 87-94. (In Jap-
anese). 
K. Torisawa. 2001. An Unsupervised Method for Ca-
nonicalization of Japanese Postpositions. In Pro-
ceedings of the 6th Natural Language Processing 
Pacific Rim Symposium (NLPRS), pp. 211?218. 
K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-
ma, M. Murata, D. Noguchi and A. Sumida. 2008. 
TORISHIKI-KAI, An Autogenerated Web Search 
Directory. In Proceedings of the second interna-
tional symposium on universal communication, pp. 
179?186, 2008. 
937
MRD-based Word Sense Disambiguation: Further#2 Extending#1 Lesk
Timothy Baldwin,? Su Nam Kim,? Francis Bond,? Sanae Fujita,?
David Martinez? and Takaaki Tanaka?
? CSSE
University of Melbourne
VIC 3010 Australia
? NICT
3-5 Hikaridai, Seika-cho
Soraku-gun, Kyoto
619-0289 Japan
? NTT CS Labs
2-4 Hikari-dai, Seika-cho
Soraku-gun, Kyoto
619-0237 Japan
Abstract
This paper reconsiders the task of MRD-
based word sense disambiguation, in extend-
ing the basic Lesk algorithm to investigate
the impact onWSD performance of different
tokenisation schemes, scoring mechanisms,
methods of gloss extension and filtering
methods. In experimentation over the Lex-
eed Sensebank and the Japanese Senseval-
2 dictionary task, we demonstrate that char-
acter bigrams with sense-sensitive gloss ex-
tension over hyponyms and hypernyms en-
hances WSD performance.
1 Introduction
The aim of this work is to develop and extend word
sense disambiguation (WSD) techniques to be ap-
plied to all words in a text. The goal of WSD is
to link occurrences of ambiguous words in specific
contexts to their meanings, usually represented by
a machine readable dictionary (MRD) or a similar
lexical repository. For instance, given the following
Japanese input:
(1) ?????
quiet
?
dog
?
ACC
????
want to keep
?(I) want to keep a quiet dog?
we would hope to identify each component word as
occurring with the sense corresponding to the indi-
cated English glosses.
WSD systems can be classified according to the
knowledge sources they use to build their models. A
top-level distinction is made between supervised and
unsupervised systems. The former rely on training
instances that have been hand-tagged, while the lat-
ter rely on other types of knowledge, such as lexical
databases or untagged corpora. The Senseval evalu-
ation tracks have shown that supervised systems per-
form better when sufficient training data is available,
but they do not scale well to all words in context.
This is known as the knowledge acquisition bottle-
neck, and is the main motivation behind research on
unsupervised techniques (Mihalcea and Chklovski,
2003).
In this paper, we aim to exploit an existing lexical
resource to build an all-words Japanese word-sense
disambiguator. The resource in question is the Lex-
eed Sensebank (Tanaka et al, 2006) and consists of
the 28,000 most familiar words of Japanese, each of
which has one or more basic senses. The senses take
the form of a dictionary definition composed from
the closed vocabulary of the 28,000 words contained
in the dictionary, each of which is further manually
sense annotated according to the Lexeed sense in-
ventory. Lexeed also has a semi-automatically con-
structed ontology.
Through the Lexeed sensebank, we investigate a
number of areas of general interest to theWSD com-
munity. First, we test extensions of the Lesk algo-
rithm (Lesk, 1986) over Japanese, focusing specif-
ically on the impact of the overlap metric and seg-
ment representation on WSD performance. Second,
we propose further extensions of the Lesk algorithm
that make use of disambiguated definitions. In this,
we shed light on the relative benefits we can expect
from hand-tagging dictionary definitions, i.e. in in-
troducing ?semi-supervision? to the disambiguation
task. The proposed method is language independent,
and is equally applicable to the Extended WordNet1
for English, for example.
2 Related work
Our work focuses on unsupervised and semi-
supervised methods that target al words and parts
of speech (POS) in context. We use the term
?unsupervised? to refer to systems that do not
use hand-tagged example sets for each word, in
line with the standard usage in the WSD litera-
ture (Agirre and Edmonds, 2006). We blur the su-
pervised/unsupervised boundary somewhat in com-
bining the basic unsupervised methods with hand-
tagged definitions from Lexeed, in order to measure
the improvement we can expect from sense-tagged
data. We qualify our use of hand-tagged definition
1 http://xwn.hlt.utdallas.edu
775
sentences by claiming that this kind of resource is
less costly to produce than sense-annotated open text
because: (1) the effects of discourse are limited, (2)
syntax is relatively simple, (3) there is significant se-
mantic priming relative to the word being defined,
and (4) there is generally explicit meta-tagging of
the domain in technical definitions. In our experi-
ments, we will make clear when hand-tagged sense
information is being used.
Unsupervised methods rely on different knowl-
edge sources to build their models. Primarily
the following types of lexical resources have been
used for WSD: MRDs, lexical ontologies, and un-
tagged corpora (monolingual corpora, second lan-
guage corpora, and parallel corpora). Although
early approaches focused on exploiting a single re-
source (Lesk, 1986), recent trends show the bene-
fits of combining different knowledge sources, such
as hierarchical relations from an ontology and un-
tagged corpora (McCarthy et al, 2004). In this sum-
mary, we will focus on a few representative systems
that make use of different resources, noting that this
is an area of very active research which we cannot
do true justice to within the confines of this paper.
The Lesk method (Lesk, 1986) is an MRD-based
system that relies on counting the overlap between
the words in the target context and the dictionary
definitions of the senses. In spite of its simplicity,
it has been shown to be a hard baseline for unsu-
pervised methods in Senseval, and it is applicable to
all-words with minimal effort. Banerjee and Peder-
sen (2002) extended the Lesk method for WordNet-
based WSD tasks, to include hierarchical data from
the WordNet ontology (Fellbaum, 1998). They ob-
served that the hierarchical relations significantly
enhance the basic model. Both these methods will
be described extensively in Section 3.1, as our ap-
proach is based on them.
Other notable unsupervised and semi-supervised
approaches are those of McCarthy et al (2004), who
combine ontological relations and untagged corpora
to automatically rank word senses in relation to a
corpus, and Leacock et al (1998) who use untagged
data to build sense-tagged data automatically based
on monosemous words. Parallel corpora have also
been used to avoid the need for hand-tagged data,
e.g. by Chan and Ng (2005).
3 Background
As background to our work, we first describe the ba-
sic and extended Lesk algorithms that form the core
of our approach. Then we present the Lexeed lex-
ical resource we have used in our experiments, and
finally we outline aspects of Japanese relevant for
this work.
3.1 Basic and Extended Lesk
The original Lesk algorithm (Lesk, 1986) performs
WSD by calculating the relative word overlap be-
tween the context of usage of a target word, and the
dictionary definition of each of its senses in a given
MRD. The sense with the highest overlap is then se-
lected as the most plausible hypothesis.
An obvious shortcoming of the original Lesk al-
gorithm is that it requires that the exact words used
in the definitions be included in each usage of the
target word. To redress this shortcoming, Banerjee
and Pedersen (2002) extended the basic algorithm
for WordNet-based WSD tasks to include hierarchi-
cal information, i.e. expanding the definitions to in-
clude definitions of hypernyms and hyponyms of the
synset containing a given sense, and assigning the
same weight to the words sourced from the different
definitions.
Both of these methods can be formalised accord-
ing to the following algorithm, which also forms the
basis of our proposed method:
for each word wi in context w = w1w2...wn do
for each sense si,j and definition di,j of wi do
score(si,j) = overlap(w, di,j)
end for
s?i = arg maxj score(si,j)
end for
3.2 The Lexeed Sensebank
All our experimentation is based on the Lexeed
Sensebank (Tanaka et al, 2006). The Lexeed Sense-
bank consists of all Japanese words above a certain
level of familiarity (as defined by Kasahara et al
(2004)), giving rise to 28,000 words in all, with a to-
tal of 46,000 senses which are similarly filtered for
similarity. The sense granularity is relatively coarse
for most words, with the possible exception of light
verbs, making it well suited to open-domain appli-
cations. Definition sentences for these senses were
rewritten to use only the closed vocabulary of the
28,000 familiar words (and some function words).
Additionally, a single example sentence was man-
ually constructed to exemplify each of the 46,000
senses, once again using the closed vocabulary of the
Lexeed dictionary. Both the definition sentences and
example sentences were then manually sense anno-
tated by 5 native speakers of Japanese, from which a
majority sense was extracted.
776
In addition, an ontology was induced from the
Lexeed dictionary, by parsing the first definition sen-
tence for each sense (Nichols et al, 2005). Hy-
pernyms were determined by identifying the highest
scoping real predicate (i.e. the genus). Other rela-
tion types such as synonymy and domain were also
induced based on trigger patterns in the definition
sentences, although these are too few to be useful
in our research. Because each word is sense tagged,
the relations link senses rather than just words.
3.3 Peculiarities of Japanese
The experiments in this paper focus exclusively
on Japanese WSD. Below, we outline aspects of
Japanese which are relevant to the task.
First, Japanese is a non-segmenting language, i.e.
there is no explicit orthographic representation of
word boundaries. The native rendering of (1), e.g., is???????????. Various packages exist to
automatically segment Japanese strings into words,
and the Lexeed data has been pre-segmented using
ChaSen (Matsumoto et al, 2003).
Second, Japanese is made up of 3 basic alpha-
bets: hiragana, katakana (both syllabic in nature)
and kanji (logographic in nature). The relevance of
these first two observations to WSD is that we can
choose to represent the context of a target word by
way of characters or words.
Third, Japanese has relatively free word order,
or strictly speaking, word order within phrases is
largely fixed but the ordering of phrases governed
by a given predicate is relatively free.
4 Proposed Extensions
We propose extensions to the basic Lesk algorithm
in the orthogonal areas of the scoring mechanism,
tokenisation, extended glosses and filtering.
4.1 Scoring Mechanism
In our algorithm, overlap provides the means to
score a given pairing of context w and definition
di,j . In the original Lesk algorithm, overlap was
simply the sum of words in common between the
two, which Banerjee and Pedersen (2002) modified
by squaring the size of each overlapping sub-string.
While squaring is well motivated in terms of prefer-
ring larger substring matches, it makes the algorithm
computationally expensive. We thus adopt a cheaper
scoring mechanism which normalises relative to the
length of w and di,j , but ignores the length of sub-
string matches. Namely, we use the Dice coefficient.
4.2 Tokenisation
Tokenisation is particularly important in Japanese
because it is a non-segmenting language with a lo-
gographic orthography (kanji). As such, we can
chose to either word tokenise via a word splitter
such as ChaSen, or character tokenise. Charac-
ter and word tokenisation have been compared in
the context of Japanese information retrieval (Fujii
and Croft, 1993) and translation retrieval (Baldwin,
2001), and in both cases, characters have been found
to be the superior representation overall.
Orthogonal to the question of whether to tokenise
into words or characters, we adopt an n-gram seg-
ment representation, in the form of simple unigrams
and simple bigrams. In the case of word tokenisa-
tion and simple bigrams, e.g., example (1) would be
represented as {?????? ,?? ,????? }.
4.3 Extended Glosses
The main direction in which Banerjee and Peder-
sen (2002) successfully extended the Lesk algorithm
was in including hierarchically-adjacent glosses (i.e.
hyponyms and hypernyms). We take this a step
further, in using both the Lexeed ontology and the
sense-disambiguated words in the definition sen-
tences.
The basic form of extended glossing is the simple
Lesk method, where we take the simple definitions
for each sense si,j (i.e. without any gloss extension).
Next, we replicate the Banerjee and Pedersen
(2002) method in extending the glosses to include
words from the definitions for the (immediate) hy-
pernyms and/or hyponyms of each sense si,j .
An extension of the Banerjee and Pedersen (2002)
method which makes use of the sense-annotated def-
initions is to include the words in the definition of
each sense-annotated word dk contained in defini-
tion di,j = d1d2...dm of word sense si,j . That is,
rather than traversing the ontology relative to each
word sense candidate si,j for the target word wi,
we represent each word sense via the original def-
inition plus all definitions of word senses contained
in it (weighting each to give the words in the original
definition greater import than those from definitions
of those word senses). We can then optionally adopt
a similar policy to Banerjee and Pedersen (2002) in
expanding each sense-annotated word dk in the orig-
inal definition relative to the ontology, to include the
immediate hypernyms and/or hyponyms.
We further expand the definitions (+extdef) by
adding the full definition for each sense-tagged word
in the original definition. This can be combined
with the Banerjee and Pedersen (2002) method by
777
also expanding each sense-annotated word dk in the
original definition relative to the ontology, to in-
clude the immediate hypernyms (+hyper) and/or hy-
ponyms (+hypo).
4.4 Filtering
Each word sense in the dictionary is marked with a
word class, and the word splitter similarly POS tags
every definition and input to the system. It is nat-
ural to expect that the POS tag of the target word
should match the word class of the word sense, and
this provides a coarse-grained filter for discriminat-
ing homographs with different word classes.
We also experiment with a stop word-based filter
which ignores a closed set of 18 lexicographic mark-
ers commonly found in definitions (e.g. ? [ryaku]
?an abbreviation for ...?), in line with those used by
Nichols et al (2005) in inducing the ontology.
5 Evaluation
We evaluate our various extensions over two
datasets: (1) the example sentences in the Lexeed
sensebank, and (2) the Senseval-2 Japanese dictio-
nary task (Shirai, 2002).
All results below are reported in terms of sim-
ple precision, following the conventions of Senseval
evaluations. For all experiments, precision and re-
call are identical as our systems have full coverage.
For the two datasets, we use two baselines: a ran-
dom baseline and the first-sense baseline. Note that
the first-sense baseline has been shown to be hard
to beat for unsupervised systems (McCarthy et al,
2004), and it is considered supervised when, as in
this case, the first-sense is the most frequent sense
from hand-tagged corpora.
5.1 Lexeed Example Sentences
The goal of these experiments is to tag all the words
that occur in the example sentences in the Lexeed
Sensebank. The first set of experiments over the
Lexeed Sensebank explores three parameters: the
use of characters vs. words, unigrams vs. bigrams,
and original vs. extended definitions. The results of
the experiments and the baselines are presented in
Table 1.
First, characters are in all cases superior to words
as our segment granularity. The introduction of bi-
grams has a uniformly negative impact for both char-
acters and words, due to the effects of data sparse-
ness. This is somewhat surprising for characters,
given that the median word length is 2 characters,
although the difference between character unigrams
and bigrams is slight.
Extended definitions are also shown to be superior
to simple definitions, although the relative increment
in making use of large amounts of sense annotations
is smaller than that of characters vs. words, suggest-
ing that the considerable effort in sense annotating
the definitions is not commensurate with the final
gain for this simple method.
Note that at this stage, our best-performing
method is roughly equivalent to the unsupervised
(random) baseline, but well below the supervised
(first sense) baseline.
Having found that extended definitions improve
results to a small degree, we turn to our next exper-
iment were we investigate whether the introduction
of ontological relations to expand the original def-
initions further enhances our precision. Here, we
persevere with the use of word and characters (all
unigrams), and experiment with the addition of hy-
pernyms and/or hyponyms, with and without the ex-
tended definitions. We also compare our method
directly with that of Banerjee and Pedersen (2002)
over the Lexeed data, and further test the impact
of the sense annotations, in rerunning our experi-
ments with the ontology in a sense-insensitive man-
ner, i.e. by adding in the union of word-level hyper-
nyms and/or hyponyms. The results are described in
Table 2. The results in brackets are reproduced from
earlier tables.
Adding in the ontology makes a significant dif-
ference to our results, in line with the findings of
Banerjee and Pedersen (2002). Hyponyms are better
discriminators than hypernyms (assuming a given
word sense has a hyponym ? the Lexeed ontology
is relatively flat), partly because while a given word
sense will have (at most) one hypernym, it often has
multiple hyponyms (if any at all). Adding in hyper-
nyms or hyponyms, in fact, has a greater impact on
results than simple extended definitions (+extdef),
especially for words. The best overall results are
produced for the (weighted) combination of all on-
tological relations (i.e. extended definitions, hyper-
nyms and hyponyms), achieving a precision level
above both the unsupervised (random) and super-
vised (first-sense) baselines.
In the interests of getting additional insights into
the import of sense annotations in our method, we
ran both the original Banerjee and Pedersen (2002)
method and a sense-insensitive variant of our pro-
posed method over the same data, the results for
which are also included in Table 2. Simple hy-
ponyms (without extended definitions) and word-
based segments returned the best results out of all
the variants tried, at a precision of 0.656. This com-
pares with a precision of 0.683 achieved for the best
778
UNIGRAMS BIGRAMS
ALL WORDS POLYSEMOUS ALL WORDS POLYSEMOUS
Simple Definitions
CHARACTERS 0.523 0.309 0.486 0.262
WORDS 0.469 0.229 0.444 0.201
Extended Definitions
CHARACTERS 0.526 0.313 0.529 0.323
WORDS 0.489 0.258 0.463 0.227
Table 1: Precision over the Lexeed example sentences using simple/extended definitions and word/character
unigrams and bigrams (best-performing method in boldface)
ALL WORDS POLYSEMOUS
UNSUPERVISED BASELINE: 0.527 0.315
SUPERVISED BASELINE: 0.633 0.460
Banerjee and Pedersen (2002) 0.648 0.492
Ontology expansion (sense-sensitive)
simple (0.469) (0.229)
+extdef (0.489) (0.258)
+hypernyms 0.559 0.363
W +hyponyms 0.655 0.503
+def +hyper 0.577 0.386
+def +hypo 0.649 0.490
+def +hyper +hypo 0.683 0.539
simple (0.523) (0.309)
+extdef (0.526) (0.313)
+hypernyms 0.539 0.334
C +hyponyms 0.641 0.481
+def +hyper 0.563 0.365
+def +hypo 0.671 0.522
+def +hyper +hypo 0.671 0.522
Ontology expansion (sense-insensitive)
+hypernyms 0.548 0.348
+hyponyms 0.656 0.503
W +def +hyper 0.551 0.347
+def +hypo 0.649 0.490
+def + hyper +hypo 0.631 0.464
+hypernyms 0.537 0.332
+hyponyms 0.644 0.485
C +def +hyper 0.542 0.335
+def +hypo 0.644 0.484
+def + hyper +hypo 0.628 0.460
Table 2: Precision over the Lexeed exam-
ple sentences using ontology-based gloss extension
(with/without word sense information) and word
(W) and character (C) unigrams (best-performing
method in boldface)
of the sense-sensitive methods, indicating that sense
information enhances WSD performance. This rein-
forces our expectation that richly annotated lexical
resources improve performance. With richer infor-
mation to work with, character based methods uni-
formly give worse results.
While we don?t present the results here due to rea-
sons of space, POS-based filtering had very little im-
pact on results, due to very few POS-differentiated
homographs in Japanese. Stop word filtering leads
ALL
WORDS
POLYSEMOUS
Baselines
Unsupervised (random) 0.310 0.260
Supervised (first-sense) 0.577 0.555
Ontology expansion (sense-sensitive)
W +def +hyper +hypo 0.624 0.605
C +def +hyper +hypo 0.624 0.605
Ontology expansion (sense-insensitive)
W +def +hyper +hypo 0.602 0.581
C +def +hyper +hypo 0.593 0.572
Table 3: Precision over the Senseval-2 data
to a very slight increment in precision across the
board (of the order of 0.001).
5.2 Senseval-2 Japanese Dictionary Task
In our second set of experiments we apply our pro-
posed method to the Senseval-2 Japanese dictionary
task (Shirai, 2002) in order to calibrate our results
against previously published results for Japanese
WSD. Recall that this is a lexical sample task,
and that our evaluation is relative to Lexeed re-
annotations of the same dataset, although the relative
polysemy for the original data and the re-annotated
version are largely the same (Tanaka et al, 2006).
The first sense baselines (i.e. sense skewing) for the
two sets of annotations differ significantly, however,
with a precision of 0.726 reported for the original
task, and 0.577 for the re-annotated Lexeed vari-
ant. System comparison (Senseval-2 systems vs. our
method) will thus be reported in terms of error rate
reduction relative to the respective first sense base-
lines.
In Table 3, we present the results over the
Senseval-2 data for the best-performing systems
from our earlier experiments. As before, we in-
clude results over both words and characters, and
with sense-sensitive and sense-insensitive ontology
expansion.
Our results largely mirror those of Table 2, al-
though here there is very little to separate words
and characters. All methods surpassed both the ran-
dom and first sense baselines, but the relative impact
779
of sense annotations was if anything even less pro-
nounced than for the example sentence task.
Both sense-sensitiveWSDmethods achieve a pre-
cision of 0.624 over all the target words (with one
target word per sentence), an error reduction rate
of 11.1%. This compares favourably with an error
rate reduction of 21.9% for the best of the WSD
systems in the original Senseval-2 task (Kurohashi
and Shirai, 2001), particularly given that our method
is semi-supervised while the Senseval-2 system is a
conventional supervised word sense disambiguator.
6 Conclusion
In our experiments extending the Lesk algorithm
over Japanese data, we have shown that definition
expansion via an ontology produces a significant
performance gain, confirming results by Banerjee
and Pedersen (2002) for English. We also explored
a new expansion of the Lesk method, by measuring
the contribution of sense-tagged definitions to over-
all disambiguation performance. Using sense infor-
mation doubles the error reduction compared to the
supervised baseline, a constant gain that shows the
importance of precise sense information for error re-
duction.
Our WSD system can be applied to all words in
running text, and is able to improve over the first-
sense baseline for two separate WSD tasks, using
only existing Japanese resources. This full-coverage
system opens the way to explore further enhance-
ments, such as the contribution of extra sense-tagged
examples to the expansion, or the combination of
different WSD algorithms.
For future work, we are also studying the in-
tegration of the WSD tool with other applications
that deal with Japanese text, such as a cross-lingual
glossing tool that aids Japanese learners reading text.
Another application we are working on is the inte-
gration of the WSD system with parse selection for
Japanese grammars.
Acknowledgements
This material is supported by the Research Collaboration be-
tween NTT Communication Science Laboratories, Nippon
Telegraph and Telephone Corporation and the University of
Melbourne. We would like to thank members of the NTT Ma-
chine Translation Group and the three anonymous reviewers for
their valuable input on this research.
References
Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense
Disambiguation: Algorithms and Applications. Springer,
Dordrecht, Netherlands.
Timothy Baldwin. 2001. Low-cost, high-performance transla-
tion retrieval: Dumber is better. In Proc. of the 39th Annual
Meeting of the ACL and 10th Conference of the EACL (ACL-
EACL 2001), pages 18?25, Toulouse, France.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using WordNet. In
Proc. of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-2002),
pages 136?45, Mexico City, Mexico.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proc. of the 20th
National Conference on Artificial Intelligence (AAAI 2005),
pages 1037?42, Pittsburgh, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Hideo Fujii and W. Bruce Croft. 1993. A comparison of index-
ing techniques for Japanese text retrieval. In Proc. of 16th
International ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?93), pages 237?
46, Pittsburgh, USA.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In Proc. of SIG NLC-159, Tokyo, Japan.
Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2
Japanese tasks. In IEICE Technical Report NLC 2001-10,
pages 1?8. (in Japanese).
Claudia Leacock, Martin Chodorow, and George A. Miller.
1998. Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1):147?
65.
Michael Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone from
an ice cream cone. In Proc. of the 1986 SIGDOC Confer-
ence, pages 24?6, Ontario, Canada.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka
Hirano, Hiroshi Matsuda, Kazuma Takaoka, and Masayuki
Asahara. 2003. Japanese Morphological Analysis System
ChaSen Version 2.3.3 Manual. Technical report, NAIST.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proc. of the 42nd Annual Meeting of the ACL, pages 280?
7, Barcelona, Spain.
Rada Mihalcea and Timothy Chklovski. 2003. Open Mind
Word Expert: Creating Large Annotated Data Collections
with Web Users? Help. In Proceedings of the EACL
2003 Workshop on Linguistically Annotated Corpora (LINC
2003), pages 53?61, Budapest, Hungary.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictionar-
ies. In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI-2005), pages 1111?6, Ed-
inburgh, UK.
Kiyoaki Shirai. 2002. Construction of a word sense tagged
corpus for SENSEVAL-2 japanese dictionary task. In Proc.
of the 3rd International Conference on Language Resources
and Evaluation (LREC 2002), pages 605?8, Las Palmas,
Spain.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The
Hinoki sensebank ? a large-scale word sense tagged cor-
pus of Japanese ?. In Proc. of the Workshop on Frontiers
in Linguistically Annotated Corpora 2006, pages 62?9, Syd-
ney, Australia.
780
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 109?112,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Using Generation for Grammar Analysis and Error Detection
Michael Wayne Goodman?
University of Washington
Dept. of Linguistics
Box 354340 Seattle, WA 98195, USA
goodmami@u.washington.edu
Francis Bond
NICT Language Infrastructure Group
3-5 Hikaridai, Seika-cho, So?raku-gun,
Kyoto, 619-0289 Japan
bond@ieee.org
Abstract
We demonstrate that the bidirectionality
of deep grammars, allowing them to gen-
erate as well as parse sentences, can be
used to automatically and effectively iden-
tify errors in the grammars. The system is
tested on two implemented HPSG gram-
mars: Jacy for Japanese, and the ERG for
English. Using this system, we were able
to increase generation coverage in Jacy by
18% (45% to 63%) with only four weeks
of grammar development.
1 Introduction
Linguistically motivated analysis of text provides
much useful information for subsequent process-
ing. However, this is generally at the cost of re-
duced coverage, due both to the difficulty of pro-
viding analyses for all phenomena, and the com-
plexity of implementing these analyses. In this
paper we present a method of identifying prob-
lems in a deep grammar by exploiting the fact that
it can be used for both parsing (interpreting text
into semantics) and generation (realizing seman-
tics as text). Since both parsing and generation use
the same grammar, their performance is closely
related: in general improving the performance or
cover of one direction will also improve the other.
(Flickinger, 2008)
The central idea is that we test the grammar on
a full round trip: parsing text to its semantic repre-
sentation and then generating from it. In general,
any sentence where we cannot reproduce the orig-
inal, or where the generated sentence significantly
differs from the original, identifies a flaw in the
grammar, and with enough examples we can pin-
point the grammar rules causing these problems.
We call our system Egad, which stands for Erro-
neous Generation Analysis and Detection.
?This research was carried out while visiting NICT.
2 Background
This work was inspired by the error mining ap-
proach of van Noord (2004), who identified prob-
lematic input for a grammar by comparing sen-
tences that parsed and those that didn?t from a
large corpus. Our approach takes this idea and fur-
ther applies it to generation. We were also inspired
by the work of Dickinson and Lee (2008), whose
?variation n-gram method? models the likelihood
a particular argument structure (semantic annota-
tion) is accurate given the verb and some context.
We tested Egad on two grammars: Jacy (Siegel,
2000), a Japanese grammar and the English Re-
source Grammar (ERG) (Flickinger, 2000, 2008)
from the DELPH-IN1 group. Both grammars are
written in the Head-driven Phrase Structure Gram-
mar (HPSG) (Pollard and Sag, 1994) framework,
and use Minimal Recursion Semantics (MRS)
(Copestake et al, 2005) for their semantic rep-
resentations. The Tanaka Corpus (Tanaka, 2001)
provides us with English and Japanese sentences.
The specific motivation for this work was to in-
crease the quality and coverage of generated para-
phrases using Jacy and the ERG. Bond et al
(2008) showed they could improve the perfor-
mance of a statistical machine translation system
by training on a corpus that included paraphrased
variations of the English text. We want to do the
same with Japanese text, but Jacy was not able to
produce paraphrases as well (the ERG had 83%
generation coverage, while Jacy had 45%) Im-
proving generation would also greatly benefit X-
to-Japanese machine translation tasks using Jacy.
2.1 Concerning Grammar Performance
There is a difference between the theoretical and
practical power of the grammars. Sometimes the
1Deep Linguistic Processing with HPSG Initiative ? see
http://www.delph-in.net for background informa-
tion, including the list of current participants and pointers to
available resources and documentation
109
parser or generator can reach the memory (i.e.
edge) limit, resulting in a valid result not being
returned. Also, we only look at the top-ranked2
parse and the first five generations for each item.
This is usually not a problem, but it could cause
Egad to report false positives.
HPSG grammars are theoretically symmetric
between parsing and generation, but in practice
this is not always true. For example, to improve
performance, semantically empty lexemes are not
inserted into a generation unless a ?trigger-rule?
defines a context for them. These trigger-rules
may not cover all cases.
3 Grammar Analysis
When analyzing a grammar, Egad looks at all in-
put sentences, parses, and generations processed
by the grammar and uses the information therein
to determine characteristics of these items. These
characteristics are encoded in a vector that can be
used for labeling and searching items. Some char-
acteristics are useful for error mining, while others
are used for grammar analysis.
3.1 Characteristic Types
Egad determines both general characteristics of an
item (parsability and generability), and character-
istics comparing parses with generations.
General characteristics show whether each item
could: be parsed (?parsable?), generate from
parsed semantics (?generable?), generate the orig-
inal parsed sentence (?reproducible?), and gener-
ate other sentences (?paraphrasable?).
For comparative characteristics, Egad com-
pares every generated sentence to the parsed sen-
tence whence its semantics originated, and deter-
mines if the generated sentence uses the same set
of lexemes, derivation tree,3 set of rules, surface
form, and MRS as the original.
3.2 Characteristic Patterns
Having determined all applicable characteristics
for an item or a generated sentence, we encode the
values of those characteristics into a vector. We
call this vector a characteristic pattern, or CP.
An example CP showing general characteristics is:
0010 -----
2Jacy and the ERG both have parse-ranking models.
3In comparing the derivation trees, we only look at phrasal
nodes. Lexemes and surface forms are not compared.
The first four digits are read as: the item is
parsable, generable, not reproducible, and is para-
phrasable. The five following dashes are for com-
parative characteristics and are inapplicable except
for generations.
3.3 Utility of Characteristics
Not all characteristics are useful for all tasks. We
were interested in improving Jacy?s ability to gen-
erate sentences, so we primarily looked at items
that were parsable but ungenerable. In comparing
generated sentences with the original parsed sen-
tence, those with differing semantics often point to
errors, as do those with a different surface form but
the same derivation tree and lexemes (which usu-
ally means an inflectional rule was misapplied).
4 Problematic Rule Detection
Our method for detecting problematic rules is to
train a maximum entropy-based classifier4 with n-
gram paths of rules from a derivation tree as fea-
tures and characteristic patterns as labels. Once
trained, we do feature-selection to look at what
paths of rules are most predictive of certain labels.
4.1 Rule Paths
We extract n-grams over rule paths, or RPs,
which are downward paths along the derivation
tree. (Toutanova et al, 2005) By creating sepa-
rate RPs for each branch in the derivation tree, we
retain some information about the order of rule ap-
plication without overfitting to specific tree struc-
tures. For example, Figure 1 is the derivation tree
for (1). A couple of RPs extracted from the deriva-
tion tree are shown in Figure 2.
(1) ?????
shashin-utsuri-ga
picture-taking-NOM
??
ii
good
(X is) good at taking pictures.
4.2 Building a Model
We build a classification model by using a parsed
or generated sentence?s RPs as features and that
sentence?s CP as a label. The set of RPs includes
n-grams over all specified values of N. The labels
are, to be more accurate, regular expressions of
4We would like to look at using different classifiers here,
such as Decision Trees. We initially chose MaxEnt because
it was easy to implement, and have since had little motivation
to change it because it produced useful results.
110
utterance rule-decl-finite
head subj rule
hf-complement-rule
quantify-n-lrule
compounds-rule
shashin
??
utsuri 1
??
ga
?
unary-vstem-vend-rule
adj-i-lexeme-infl-rule
ii-adj
??
Figure 1: Derivation tree for (1)
quantify-n-lrule ? compounds-rule ? shashin
quantify-n-lrule ? compounds-rule ? utsuri 1
Figure 2: Example RPs extracted from Figure 1
CPs and may be fully specified to a unique CP or
generalize over several.5 The user can weight the
RPs by their N value (e.g. to target unigrams).
4.3 Finding Problematic Rules
After training the model, we have a classifier that
predicts CPs given a set of RPs. What we want,
however, is the RP most strongly associated with
a given CP. The classifier we use provides an easy
method to get the score a given feature has for
some label. We iterate over all RPs, get their score,
then sort them based on the score. To help elim-
inate redundant results, we exclude any RP that
either subsumes or is subsumed by a previous (i.e.
higher ranked) RP.
Given a CP, the RP with the highest score
should indeed be the one most closely associated
to that CP, but it might not lead to the greatest
number of items affected. Fixing the second high-
est ranked RP, for example, may improve more
items than fixing the top ranked one. To help the
grammar developer decide the priority of prob-
lems to fix, we also output the count of items ob-
served with the given CP and RP.
5 Results and Evaluation
We can look at two sets of results: how well
Egad was able to analyze a grammar and detect
errors, and how well a grammar developer could
use Egad to fix a problematic grammar. While the
latter is also influenced by the skill of the gram-
mar developer, we are interested in how well Egad
5For example, /0010 -----/ is fully specified.
/00.. -----/ marginalizes two general characteristics
points to the most significant errors, and how it can
help reduce development time.
5.1 Error Mining
Table 1 lists the ten highest ranked RPs associated
with items that could parse but could not generate
in Jacy. Some RPs appear several times in differ-
ent contexts. We made an effort to decrease the
redundancy, but clearly this could be improved.
From this list of ten problematic RPs, there
are four unique problems: quantify-n-lrule (noun
quantification), no-nspec (noun specification), to-
comp-quotarg (? to quotative particle), and te-
adjunct (verb conjugation). The extra rules listed
in each RP show the context in which each
problem occurs, and this can be informative as
well. For instance, quantify-n-lrule occurs in
two primary contexts (above compounds-rule and
nominal-numcl-rule). The symptoms of the prob-
lem occur in the interation of rules in each context,
but the source of the problem is quantify-n-lrule.
Further, the problems identified are not always
lexically marked. quantify-n-lrule occurs for all
bare noun phrases (ie. without determiners). This
kind of error cannot be accurately identified by us-
ing just word or POS n-grams, we need to use the
actual parse tree.
5.2 Error Correction
Egad greatly facilitated our efforts to find and fix
a wide variety of errors in Jacy. For example, we
restructured semantic predicate hierarchies, fixed
noun quantification, allowed some semantically
empty lexemes to generate in certain contexts,
added pragmatic information to distinguish be-
tween politeness levels in pronouns, allowed im-
peratives to generate, allowed more constructions
for numeral classifiers, and more.
Egad also identified some issues with the ERG:
both over-generation (an under-constrained inflec-
tional rule) and under-generation (sentences with
the construction take {care|charge|. . . } of were
not generating).
5.3 Updated Grammar Statistics
After fixing the most significant problems in Jacy
(outlined in Section 5.2) as reported by Egad,
we obtained new statistics about the grammar?s
coverage and characteristics. Table 2 shows the
original and updated general statistics for Jacy.
We increased generability by 18%, doubled repro-
ducibility, and increased paraphrasability by 17%.
111
Score Count Rule Path N-grams
1.42340952569648 109 hf-complement-rule? quantify-n-lrule? compounds-rule
0.960090299833317 54 hf-complement-rule? quantify-n-lrule? nominal-numcl-rule? head-specifier-rule
0.756227560530811 63 head-specifier-rule? hf-complement-rule? no-nspec? ???
0.739668926140179 62 hf-complement-rule? head-specifier-rule? hf-complement-rule? no-nspec
0.739090261637851 22 hf-complement-rule? hf-adj-i-rule? quantify-n-lrule? compounds-rule
0.694215264789286 36 hf-complement-rule? hf-complement-rule? to-comp-quotarg? ???
0.676244980660372 82 vstem-vend-rule? te-adjunct? ???
0.617621482523537 26 hf-complement-rule? hf-complement-rule? to-comp-varg? ???
0.592260546433334 36 hf-adj-i-rule? hf-complement-rule? quantify-n-lrule? nominal-numcl-rule
0.564790702894285 62 quantify-n-lrule? compounds-rule? vn2n-det-lrule
Table 1: Top 10 RPs for ungenerable items
Original Modified
Parsable 82% 83%
Generable 45% 63%
Reproducible 11% 22%
Paraphrasable 44% 61%
Table 2: Jacy?s improved general statistics
As an added bonus, our work focused on improv-
ing generation also improved parsability by 1%.
Work is now continuing on fixing the remainder
of the identified errors.
6 Future Work
In future iterations of Egad, we would like to ex-
pand our feature set (e.g. information from failed
parses), and make the system more robust, such
as replacing lexical-ids (specific to a lexeme) with
lexical-types, since all lexemes of the same type
should behave identically. A more long-term goal
would allow Egad to analyze the internals of the
grammar and point out specific features within the
grammar rules that are causing problems. Some
of the errors detected by Egad have simple fixes,
and we believe there is room to explore methods
of automatic error correction.
7 Conclusion
We have introduced a system that identifies er-
rors in implemented HPSG grammars, and further
finds and ranks the possible sources of those prob-
lems. This tool can greatly reduce the amount
of time a grammar developer would spend find-
ing bugs, and helps them make informed decisions
about which bugs are best to fix. In effect, we are
substituting cheap CPU time for expensive gram-
mar developer time. Using our system, we were
able to improve Jacy?s absolute generation cover-
age by 18% (45% to 63%) with only four weeks
of grammar development.
8 Acknowledgments
Thanks to NICT for their support, Takayuki Kurib-
ayashi for providing native judgments, and Mar-
cus Dickinson for comments on an early draft.
References
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving statistical machine trans-
lation by paraphrasing the training data. In International
Workshop on Spoken Language Translation, pages 150?
157. Honolulu.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281?
332.
Markus Dickinson and Chong Min Lee. 2008. Detecting
errors in semantic annotation. In Proceedings of the
Sixth International Language Resources and Evaluation
(LREC?08). Marrakech, Morocco.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6(1):15?28. (Special Issue on Efficient Processing with
HPSG).
Dan Flickinger. 2008. The English resource grammar. Tech-
nical Report 2007-7, LOGON, http://www.emmtee.
net/reports/7.pdf. (Draft of 2008-11-30).
Carl Pollard and Ivan A. Sag. 1994. Head Driven
Phrase Structure Grammar. University of Chicago Press,
Chicago.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil: Foundations of Speech-
to-Speech Translation, pages 265 ? 280. Springer, Berlin,
Germany.
Yasuhito Tanaka. 2001. Compilation of a multilingual paral-
lel corpus. In Proceedings of PACLING 2001, pages 265?
268. Kyushu. (http://www.colips.org/afnlp/
archives/pacling2001/pdf/tanaka.pdf).
Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the redwoods corpus. Research on Lan-
guage and Computation, 3(1):83?105.
Gertjan van Noord. 2004. Error mining for wide-coverage
grammar engineering. In 42nd Annual Meeting of the
Association for Computational Linguistics: ACL-2004.
Barcelona.
112
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25?32,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploiting Semantic Information for HPSG Parse Selection
Sanae Fujita,? Francis Bond,? Stephan Oepen,? Takaaki Tanaka?
? {sanae,takaaki}@cslab.kecl.ntt.co.jp, ? bond@ieee.org, ? oe@ifi.uio.no
? NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
? National Institute of Information and Communications Technology (Japan)
? University of Oslo, Department of Informatics (Norway)
Abstract
In this paper we present a framework for
experimentation on parse selection using
syntactic and semantic features. Results
are given for syntactic features, depen-
dency relations and the use of semantic
classes.
1 Introduction
In this paper we investigate the use of semantic in-
formation in parse selection.
Recently, significant improvements have been
made in combining symbolic and statistical ap-
proaches to various natural language processing
tasks. In parsing, for example, symbolic grammars
are combined with stochastic models (Oepen et al,
2004; Malouf and van Noord, 2004). Much of the
gain in statistical parsing using lexicalized models
comes from the use of a small set of function words
(Klein and Manning, 2003). Features based on gen-
eral relations provide little improvement, presum-
ably because the data is too sparse: in the Penn
treebank standardly used to train and test statisti-
cal parsers stocks and skyrocket never appear to-
gether. However, the superordinate concepts capi-
tal (? stocks) and move upward (? sky rocket) fre-
quently appear together, which suggests that using
word senses and their hypernyms as features may be
useful
However, to date, there have been few combina-
tions of sense information together with symbolic
grammars and statistical models. We hypothesize
that one of the reasons for the lack of success is
that there has been no resource annotated with both
syntactic and semantic information. In this paper,
we use a treebank with both syntactic information
(HPSG parses) and semantic information (sense tags
from a lexicon) (Bond et al, 2007). We use this to
train parse selection models using both syntactic and
semantic features. A model trained using syntactic
features combined with semantic information out-
performs a model using purely syntactic information
by a wide margin (69.4% sentence parse accuracy
vs. 63.8% on definition sentences).
2 The Hinoki Corpus
There are now some corpora being built with the
syntactic and semantic information necessary to in-
vestigate the use of semantic information in parse
selection. In English, the OntoNotes project (Hovy
et al, 2006) is combining sense tags with the Penn
treebank. We are using Japanese data from the Hi-
noki Corpus consisting of around 95,000 dictionary
definition and example sentences (Bond et al, 2007)
annotated with both syntactic parses and senses from
the same dictionary.
2.1 Syntactic Annotation
Syntactic annotation in Hinoki is grammar based
corpus annotation done by selecting the best parse
(or parses) from the full analyses derived by a broad-
coverage precision grammar. The grammar is an
HPSG implementation (JACY: Siegel and Bender,
2002), which provides a high level of detail, mark-
ing not only dependency and constituent structure
but also detailed semantic relations. As the gram-
mar is based on a monostratal theory of grammar
(HPSG: Pollard and Sag, 1994), annotation by man-
ual disambiguation determines syntactic and seman-
tic structure at the same time. Using a grammar
25
helps treebank consistency ? all sentences anno-
tated are guaranteed to have well-formed parses.
The flip side to this is that any sentences which the
parser cannot parse remain unannotated, at least un-
less we were to fall back on full manual mark-up of
their analyses. The actual annotation process uses
the same tools as the Redwoods treebank of English
(Oepen et al, 2004).
A (simplified) example of an entry is given in Fig-
ure 1. Each entry contains the word itself, its part
of speech, and its lexical type(s) in the grammar.
Each sense then contains definition and example
sentences, links to other senses in the lexicon (such
as hypernym), and links to other resources, such
as the Goi-Taikei Japanese Lexicon (Ikehara et al,
1997) and WordNet (Fellbaum, 1998). Each content
word of the definition and example sentences is an-
notated with sense tags from the same lexicon.
There were 4 parses for the definition sentence.
The correct parse, shown as a phrase structure tree,
is shown in Figure 2. The two sources of ambigu-
ity are the conjunction and the relative clause. The
parser also allows the conjunction to combine \
densha and 0 hito. In Japanese, relative clauses
can have gapped and non-gapped readings. In the
gapped reading (selected here), 0 hito is the subject
of ?U unten ?drive?. In the non-gapped reading
there is some underspecified relation between the
modifee and the verb phrase. This is similar to the
difference in the two readings of the day he knew
in English: ?the day that he knew about? (gapped)
vs ?the day on which he knew (something)? (non-
gapped). Such semantic ambiguity is resolved by
selecting the correct derivation tree that includes the
applied rules in building the tree (Fig 3).
The semantic representation is Minimal Recur-
sion Semantics (Copestake et al, 2005). We sim-
plify this into a dependency representation, further
abstracting away from quantification, as shown in
Figure 4. One of the advantages of the HPSG sign
is that it contains all this information, making it pos-
sible to extract the particular view needed. In or-
der to make linking to other resources, such as the
sense annotation, easier predicates are labeled with
pointers back to their position in the original sur-
face string. For example, the predicate densha n 1
links to the surface characters between positions 0
and 3:\.
UTTERANCE
NP
VP N
PP V
NP
PP
N CONJ N CASE-P V V
\ ? ? k ?U 2d 0
densha ya jidousha o unten suru hito
train or car ACC drive do person
?U31 ?chauffeur?: ?a person who drives a train or car?
Figure 2: Syntactic View of the Definition of ?U
31 untenshu ?chauffeur?
e2:unknown<0:13>[ARG x5:_hito_n]
x7:densha_n_1<0:3>[]
x12:_jidousha_n<4:7>[]
x13:_ya_p_conj<0:4>[LIDX x7:_densha_n_1,
RIDX x12:_jidousha_n]
e23:_unten_s_2<8:10>[ARG1 x5:_hito_n]
e23:_unten_s_2<8:10>[ARG2 x13:_ya_p_conj]
Figure 4: Simplified Dependency View of the Defi-
nition of ?U31 untenshu ?chauffeur?
2.2 Semantic Annotation
The lexical semantic annotation uses the sense in-
ventory from Lexeed (Kasahara et al, 2004). All
words in the fundamental vocabulary are tagged
with their sense. For example, the wordd& ookii
?big? (of example sentence in Figure 1) is tagged as
sense 5 in the example sentence, with the meaning
?elder, older?.
The word senses are further linked to semantic
classes in a Japanese ontology. The ontology, Goi-
Taikei, consists of a hierarchy of 2,710 semantic
classes, defined for over 264,312 nouns, with a max-
imum depth of 12 (Ikehara et al, 1997). We show
the top 3 levels of the Goi-Taikei common noun on-
tology in Figure 5. The semantic classes are prin-
cipally defined for nouns (including verbal nouns),
although there is some information for verbs and ad-
jectives.
3 Parse Selection
Combining the broad-coverage JACY grammar and
the Hinoki corpus, we build a parse selection model
on top of the symbolic grammar. Given a set of can-
26
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX ?U3 untenshu
POS noun
SENSE 1
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION
[
\1 ??1 k?U1 2d04 a person who drives trains and cars
]
EXAMPLE
[
d&(5 C<8b\1 G?U31 Dod6 G%?3 2
I dream of growing up and becoming a train driver
]
HYPERNYM 04 hito ?person?
SEM. CLASS ?292:driver? (? ?4:person?)
WORDNET motorman1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur?
frag-np
rel-cl-sbj-gap
hd-complement noun-le
hd-complement v-light
hd-complement
hd-complement case-p-acc-le
noun-le conj-le noun-le vn-trans-le v-light-le
\ ? ? k ?U 2d 0
densha ya jidousha o unten suru hito
train or car ACC drive do person
?U31 ?chauffeur?: ?a person who drives a train or car?
Figure 3: Derivation Tree of the Definition of ?U31 untenshu ?chauffeur?
Phrasal nodes are labeled with identifiers of grammar rules, and (pre-terminal) lexical nodes with class names for types of lexical
entries.
Lvl 0 Lvl 1 Lvl 2 Lvl 3
human
agent organization
facility
c
o
n
c
r
e
t
e
place region
natural place
object animate
inanimate
abstract
thing
mental state
noun action
human activity
event phenomenon
natural phen.
a
b
s
t
r
a
c
t
existence
system
relationship
property
relation state
shape
amount
location
time
Figure 5: Top 3 levels of the GoiTaikei Ontology
didate analyses (for some Japanese string) according
to JACY, the goal is to rank parse trees by their prob-
ability: training a stochastic parse selection model
on the available treebank, we estimate statistics of
various features of candidate analyses from the tree-
bank. The definition and selection of features, thus,
is a central parameter in the design of an effective
parse selection model.
3.1 Syntactic Features
The first model that we trained uses syntactic fea-
tures defined over HPSG derivation trees as summa-
rized in Table 1. For the closely related purpose of
parse selection over the English Redwoods treebank,
Toutanova et al (2005) train a discriminative log-
linear model, using features defined over derivation
trees with non-terminals representing the construc-
tion types and lexical types of the HPSG grammar.
The basic feature set of our parse selection model
for Japanese is defined in the same way (correspond-
ing to the PCFG-S model of Toutanova et al (2005)):
each feature capturing a sub-tree from the deriva-
27
# sample features
1 ?0 rel-cl-sbj-gap hd-complement noun-le?
1 ?1 frag-np rel-cl-sbj-gap hd-complement noun-le?
1 ?2 ? frag-np rel-cl-sbj-gap hd-complement noun-le?
2 ?0 rel-cl-sbj-gap hd-complement?
2 ?0 rel-cl-sbj-gap noun-le?
2 ?1 frag-np rel-cl-sbj-gap hd-complement?
2 ?1 frag-np rel-cl-sbj-gap noun-le?
3 ?1 conj-le ya?
3 ?2 noun-le conj-le ya?
3 ?3  noun-le conj-le ya?
4 ?1 conj-le?
4 ?2 noun-le conj-le?
4 ?3  noun-le conj-le?
Table 1: Example structural features extracted from
the derivation tree in Figure 3. The first column
numbers the feature template corresponding to each
example; in the examples, the first integer value
is a parameter to feature templates, i.e. the depth
of grandparenting (types #1 and#2) or n-gram size
(types #3 and #4). The special symbols ? and 
denote the root of the tree and left periphery of the
yield, respectively.
tion limited to depth one. Table 1 shows example
features extracted from our running example (Fig-
ure 3 above) in our MaxEnt models, where the fea-
ture template #1 corresponds to local derivation sub-
trees. We will refer to the parse selection model us-
ing only local structural features as SYN-1.
3.1.1 Dominance Features
To reduce the effects of data sparseness, feature
type #2 in Table 1 provides a back-off to deriva-
tion sub-trees, where the sequence of daughters is
reduced to just the head daughter. Conversely, to
facilitate sampling of larger contexts than just sub-
trees of depth one, feature template #1 allows op-
tional grandparenting, including the upwards chain
of dominating nodes in some features. In our ex-
periments, we found that grandparenting of up to
three dominating nodes gave the best balance of en-
larged context vs. data sparseness. Enriching our ba-
sic model SYN-1 with these features we will hence-
forth call SYN-GP.
3.1.2 N-Gram Features
In addition to these dominance-oriented features
taken from the derivation trees of each parse tree,
our models also include more surface-oriented fea-
tures, viz. n-grams of lexical types with or without
lexicalization. Feature type #3 in Table 1 defines
n-grams of variable size, where (in a loose anal-
ogy to part-of-speech tagging) sequences of lexical
types capture syntactic category assignments. Fea-
ture templates #3 and #4 only differ with regard to
lexicalization, as the former includes the surface to-
ken associated with the rightmost element of each
n-gram (loosely corresponding to the emission prob-
abilities in an HMM tagger). We used a maximum
n-gram size of two in the experiments reported here,
again due to its empirically determined best overall
performance.
3.2 Semantic Features
In order to define semantic parse selection features,
we use a reduction of the full semantic representa-
tion (MRS) into ?variable-free? elementary depen-
dencies. The conversion centrally rests on a notion
of one distinguished variable in each semantic rela-
tion. For most types of relations, the distinguished
variable corresponds to the main index (ARG0 in the
examples above), e.g. an event variable for verbal re-
lations and a referential index for nominals. Assum-
ing further that, by and large, there is a unique re-
lation for each semantic variable for which it serves
as the main index (thus assuming, for example, that
adjectives and adverbs have event variables of their
own, which can be motivated in predicative usages
at least), an MRS can be broken down into a set of
basic dependency tuples of the form shown in Fig-
ure 4 (Oepen and L?nning, 2006).
All predicates are indexed to the position of the
word or words that introduced them in the input sen-
tence (<start:end>). This allows us to link them
to the sense annotations in the corpus.
3.2.1 Basic Semantic Dependencies
The basic semantic model, SEM-Dep, consists of
features based on a predicate and its arguments taken
from the elementary dependencies. For example,
consider the dependencies for densha ya jidousha-
wo unten suru hito ?a person who drives a train or
car? given in Figure 4. The predicate unten ?drive?
has two arguments: ARG1 hito ?person? and ARG2
jidousha ?car?.
From these, we produce several features (See Ta-
ble 2). One has all arguments and their labels (#20).
We also produce various back offs: #21 introduces
28
# sample features
20 ?0 unten s ARG1 hito n 1 ARG2 ya p conj?
20 ?0 ya p conj LIDX densha n 1 RIDX jidousha n 1?
21 ?1 unten s ARG1 hito n 1?
21 ?1 unten s ARG2 jidousha n 1?
21 ?1 ya p conj LIDX densha n 1?
21 ?1 ya p conj RIDX jidousha n 1?
22 ?2 unten s hito n 1 jidousha n 1?
23 ?3 unten s hito n 1?
23 ?3 unten s jidousha n 1?
. . .
Table 2: Example semantic features (SEM-Dep) ex-
tracted from the dependency tree in Figure 4.
only one argument at a time, #22 provides unlabeled
relations, #23 provides one unlabeled relation at a
time and so on.
Each combination of a predicate and its related
argument(s) becomes a feature. These resemble the
basic semantic features used by Toutanova et al
(2005). We further simplify these by collapsing
some non-informative predicates, e.g. the unknown
predicate used in fragments.
3.2.2 Word Sense and Semantic Class
Dependencies
We created two sets of features based only on the
word senses. For SEM-WS we used the sense anno-
tation to replace each underspecified MRS predicate
by a predicate indicating the word sense. This used
the gold standard sense tags. For SEM-Class, we used
the sense annotation to replace each predicate by its
Goi-Taikei semantic class.
In addition, to capture more useful relationships,
conjunctions were followed down into the left and
right daughters, and added as separate features. The
semantic classes for \1densha ?train? and  ?
1jidousha ?car? are both ?988:land vehicle?,
while ?U1 unten ?drive? is ?2003:motion? and
04 hito ?person?is ?4:human?. The sample features
of SEM-Class are shown in Table 3.
These features provide more specific information,
in the case of the word sense, and semantic smooth-
ing in the case of the semantic classes, as words are
binned into only 2,700 classes.
3.2.3 Superordinate Semantic Classes
We further smooth these features by replacing the
semantic classes with their hypernyms at a given
level (SEM-L). We investigated levels 2 to 5. Pred-
# sample features
40 ?0 unten s ARG1 C4 ARG2 C988?
40 ?1 C2003 ARG1 C4 ARG2 C988?
40 ?1 C2003 ARG1 C4 ARG2 C988?
40 ?0 ya p conj LIDX C988 RIDX C988?
41 ?2 unten s ARG1 C4?
41 ?2 unten s ARG2 C988?
. . .
Table 3: Example semantic class features (SEM-
Class).
icates are binned into only 9 classes at level 2, 30
classes at level 3, 136 classes at level 4, and 392
classes at level 5.
For example, at level 3, the hypernym class
for ?988:land vehicle? is ?706:inanimate?,
?2003:motion? is ?1236:human activity?
and ?4:human? is unchanged. So we used
?706:inanimate? and ?1236:human activity?
to make features in the same way as Table 3.
An advantage of these underspecified semantic
classes is that they are more robust to errors in word
sense disambiguation ? fine grained sense distinc-
tions can be ignored.
3.2.4 Valency Dictionary Compatability
The last kind of semantic information we use is
valency information, taken from the Japanese side
of the Goi-Taikei Japanese-English valency dictio-
nary as extended by Fujita and Bond (2004).This va-
lency dictionary has detailed information about the
argument properties of verbs and adjectives, includ-
ing subcategorization and selectional restrictions. A
simplified entry of the Japanese side for ?U2
dunten-suru ?drive? is shown in Figure 6.
Each entry has a predicate and several case-slots.
Each case-slot has information such as grammatical
function, case-marker, case-role (N1, N2, . . . ) and
semantic restrictions. The semantic restrictions are
defined by the Goi-Taikei?s semantic classes.
On the Japanese side of Goi-Taikei?s valency
dictionary, there are 10,146 types of verbs giving
18,512 entries and 1,723 types of adjectives giving
2,618 entries.
The valency based features were constructed by
first finding the most appropriate pattern, and then
recording how well it matched.
To find the most appropriate pattern, we extracted
candidate dictionary entries whose lemma is the
29
PID:300513
? N1 <4:people> "%" ga
? N2 <986:vehicles> "k" o
? ?U2d unten-suru
Figure 6: ?U2dunten-suru ?N1 drive N2?.
PID is the verb?s Pattern ID
# sample features
31 ?0 High?
31 ?1 300513 High?
31 ?2 2?
31 ?3 R:High?
31 ?4 300513 R:High?
32 ?1 unten s High?
32 ?4 unten s R:High?
33 ?5 N1 C High?
33 ?7 C?
. . .
Table 4: Example semantic features (SP)
same as the predicate in the sentence: for exam-
ple we look up all entries for ?U2d unten-
suru ?drive?. Then, for each candidate pattern, we
mapped its arguments to the target predicate?s ar-
guments via case-markers. If the target predicate
has no suitable argument, we mapped to comitative
phrase. Finally, for each candidate patterns, we cal-
culate a matching score1 and select the pattern which
has the best score.
Once we have the most appropriate pattern,
we then construct features that record how good
the match is (Table 4). These include: the to-
tal score, with or without the verb?s Pattern ID
(High/Med/Low/Zero: #31 0,1), the number of filled
arguments (#31 2), the fraction of filled arguments
vs all arguments (High/Med/Low/Zero: #31 3,4),
the score for each argument of the pattern (#32 5)
and the types of matches (#32 5,7).
These scores allow us to use information about
word usage in an exisiting dictionary.
4 Evaluation and Results
We trained and tested on a subset of the dictionary
definition and example sentences in the Hinoki cor-
pus. This consists of those sentences with ambigu-
ous parses which have been annotated so that the
1The scoring method follows Bond and Shirai (1997), and
depends on the goodness of the matches of the arguments.
number of parses has been reduced (Table 5). That
is, we excluded unambiguous sentences (with a sin-
gle parse), and those where the annotators judged
that no parse gave the correct semantics. This does
not necessarily mean that there is a single correct
parse, we allow the annotator to claim that two or
more parses are equally appropriate.
Corpus # Sents Length Parses/Sent
(Ave) (Ave)
Definitions Train 30,345 9.3 190.1
Test 2,790 10.1 177.0
Examples Train 27,081 10.9 74.1
Test 2,587 10.4 47.3
Table 5: Data of Sets for Evaluation
Dictionary definition sentences are a different
genre to other commonly used test sets (e.g news-
paper text in the Penn Treebank or travel dialogues
in Redwoods). However, they are valid examples
of naturally occurring texts and a native speaker can
read and understand them without special training.
The main differences with newspaper text is that
the definition sentences are shorter, contain more
fragments (especially NPs as single utterances) and
fewer quoting and proper names. The main differ-
ences with travel dialogues is the lack of questions.
4.1 A Maximum Entropy Ranker
Log-linear models provide a very flexible frame-
work that has been widely used for a range of tasks
in NLP, including parse selection and reranking for
machine translation. We use a maximum entropy
/ minimum divergence (MEMD) modeler to train
the parse selection model. Specifically, we use the
open-source Toolkit for Advanced Discriminative
Modeling (TADM:2 Malouf, 2002) for training, us-
ing its limited-memory variable metric as the opti-
mization method and determining best-performing
convergence thresholds and prior sizes experimen-
tally. A comparison of this learner with the use
of support vector machines over similar data found
that the SVMs gave comparable results but were far
slower (Baldridge and Osborne, 2007). Because we
are investigating the effects of various different fea-
tures, we chose the faster learner.
2http://tadm.sourceforge.net
30
Method Definitions Examples
Accuracy Features Accuracy Features
(%) (?1000) (%) (?1000)
SYN-1 52.8 7 67.6 8
SYN-GP 62.7 266 76.0 196
SYN-ALL 63.8 316 76.2 245
SYN baseline 16.4 random 22.3 random
SEM-Dep 57.3 1,189 58.7 675
+SEM-WS 56.2 1,904 59.0 1,486
+SEM-Class 57.5 2,018 59.7 1,669
+SEM-L2 60.3 808 62.9 823
+SEM-L3 59.8 876 62.8 879
+SEM-L4 59.9 1,000 62.3 973
+SEM-L5 60.4 1,240 61.3 1,202
+SP 59.1 1,218 68.2 819
+SEM-ALL 62.7 3,384 69.1 2,693
SYN-SEM 69.5 2,476 79.2 2,126
SEM baseline 20.3 random 22.8 random
Table 6: Parse Selection Results
4.2 Results
The results for most of the models discussed in the
previous section are shown in Table 6. The accuracy
is exact match for the entire sentence: a model gets
a point only if its top ranked analysis is the same as
an analysis selected as correct in Hinoki. This is a
stricter metric than component based measures (e.g.,
labelled precision) which award partial credit for in-
correct parses. For the syntactic models, the base-
line (random choice) is 16.4% for the definitions and
22.3% for the examples. Definition sentences are
harder to parse than the example sentences. This
is mainly because they have fewer relative clauses
and coordinate NPs, both large sources of ambigu-
ity. For the semantic and combined models, multiple
sentences can have different parses but the same se-
mantics. In this case all sentences with the correct
semantics are scored as good. This raises the base-
lines to 20.3 and 22.8% respectively.
Even the simplest models (SYN-1 and SEM-Dep)
give a large improvement over the baseline. Adding
grandparenting to the syntactic model has a large
improvement (SYN-GP), but adding lexical n-grams
gave only a slight improvement over this (SYN-ALL).
The effect of smoothing by superordinate seman-
tic classes (SEM-Class), shows a modest improve-
ment. The syntactic model already contains a back-
off to lexical-types, we hypothesize that the seman-
tic classes behave in the same way. Surprisingly, as
we add more data, the very top level of the seman-
tic class hierarchy performs almost as well as the
+
+ +
+ + + + + + +
+
bc
bc
bc
bc
bc
bc
bc
bc
bc
bc
bc
ld
ld
ld
ld
ld
ld
ld
ld
ld
ld
ld
0 20 40 60 80 100
20
30
40
50
60
70
% of training data (30,345 sentences)
Se
n
t.
A
cc
u
ra
cy SYN-SEM
SEM-ALL
SYN-ALL
Figure 7: Learning Curves (Definitions)
more detailed levels. The features using the valency
dictionary (SP) also provide a considerable improve-
ment over the basic dependencies.
Combining all the semantic features (SEM-ALL)
provides a clear improvement, suggesting that the
information is heterogeneous. Finally, combing the
syntactic and semantic features gives the best results
by far (SYN-SEM: SYN-ALL + SEM-Dep + SEM-Class +
SEM-L2 + SP). The definitions sentences are harder
syntactically, and thus get more of a boost from the
semantics. The semantics still improve performance
for the example sentences.
The semantic class based sense features used here
are based on manual annotation, and thus show an
upper bound on the effects of these features. This
is not an absolute upper bound on the use of sense
information ? it may be possible to improve further
through feature engineering. The learning curves
(Fig 7) have not yet flattened out. We can still im-
prove by increasing the size of the training data.
5 Discussion
Bikel (2000) combined sense information and parse
information using a subset of SemCor (with Word-
Net senses and Penn-II treebanks) to produce a com-
bined model. This model did not use semantic de-
pendency relations, but only syntactic dependen-
cies augmented with heads, which suggests that the
deeper structural semantics provided by the HPSG
parser is important. Xiong et al (2005) achieved
only a very minor improvement over a plain syntac-
tic model, using features based on both the corre-
lation between predicates and their arguments, and
between predicates and the hypernyms of their argu-
ments (using HowNet). However, they do not inves-
tigate generalizing to different levels than a word?s
immediate hypernym.
31
Pioneering work by Toutanova et al (2005) and
Baldridge and Osborne (2007) on parse selection for
an English HPSG treebank used simpler semantic
features without sense information, and got a far less
dramatic improvement when they combined syntac-
tic and semantic information.
The use of hand-crafted lexical resources such as
the Goi-Taikei ontology is sometimes criticized on
the grounds that such resources are hard to produce
and scarce. While it is true that valency lexicons
and sense hierarchies are hard to produce, they are
of such value that they have already been created for
all of the languages we know of which have large
treebanks. In fact, there are more languages with
WordNets than large treebanks.
In future work we intend to confirm that we can
get improved results with raw sense disambiguation
results not just the gold standard annotations and test
the results on other sections of the Hinoki corpus.
6 Conclusions
We have shown that sense-based semantic features
combined with ontological information are effec-
tive for parse selection. Training and testing on
the definition subset of the Hinoki corpus, a com-
bined model gave a 5.6% improvement in parse se-
lection accuracy over a model using only syntactic
features (63.8% ? 69.4%). Similar results (76.2%
? 79.2%) were found with example sentences.
References
Jason Baldridge and Miles Osborne. 2007. Active learning and
logarithmic opinion pools for HPSG parse selection. Natural
Language Engineering, 13(1):1?32.
Daniel M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proceedings of the Joint SIG-
DAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 155?163. Hong
Kong.
Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2007. The Hi-
noki syntactic and semantic treebank of Japanese. Language
Resources and Evaluation. (Special issue on Asian language
technology).
Francis Bond and Satoshi Shirai. 1997. Practical and efficient
organization of a large valency dictionary. In Workshop on
Multilingual Information Processing ? Natural Language
Processing Pacific Rim Symposium ?97: NLPRS-97. Phuket.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.
2005. Minimal Recursion Semantics. An introduction. Re-
search on Language and Computation, 3(4):281?332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Sanae Fujita and Francis Bond. 2004. A method of creating
new bilingual valency entries using alternations. In Gilles
Se?rasset, editor, COLING 2004 Multilingual Linguistic Re-
sources, pages 41?48. Geneva.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the Human Language
Technology Conference of the NAACL, Companion Volume:
Short Papers, pages 57?60. Association for Computational
Linguistics, New York City, USA. URL http://www.
aclweb.org/anthology/N/N06/N06-2015.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In IPSG SIG: 2004-NLC-159, pages 75?82. Tokyo.
(in Japanese).
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 423?430. URL
http://www.aclweb.org/anthology/P03-1054.pdf.
Robert Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In CONLL-2002, pages
49?55. Taipei, Taiwan.
Robert Malouf and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
IJCNLP-04 Workshop: Beyond shallow analyses - For-
malisms and statistical modeling for deep analyses. JST
CREST. URL http://www-tsujii.is.s.u-tokyo.ac.
jp/bsa/papers/malouf.pdf.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2004. LinGO redwoods: A rich and
dynamic treebank for HPSG. Research on Language and
Computation, 2(4):575?596.
Stephan Oepen and Jan Tore L?nning. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th International
Conference on Language Resources and Evaluation (LREC
2006). Genoa, Italy.
Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
Melanie Siegel and Emily M. Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion at the 19th International Conference on Computational
Linguistics, pages 1?8. Taipei.
Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the redwoods corpus. Research on Language
and Computation, 3(1):83?105.
Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese treebank
with semantic knowledge. In Robert Dale, Jian Su Kam-Fai
Wong and, and Oi Yee Kwong, editors, Natural Language
Processing ? IJCNLP 005: Second International Joint Con-
ference Proceedings, pages 70?81. Springer-Verlag.
32
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 146?149,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Online Search Interface for the Sejong Korean-Japanese Bilingual 
Corpus and Auto-interpolation of Phrase Alignment 
 
Sanghoun Song 
Korea Univ. 
Anam-dong, Sungbuk-gu, Seoul, 
South Korea  
sanghoun@gmail.com 
Francis Bond 
NICT Language Infrastructure Group 
2-2-2 Hikaridai, Seika-cho, 
Soraku-gun, Kyoto, Japan 
bond@ieee.org 
 
 
 
  
Abstract 
A user-friendly interface to search bilingual 
resources is of great help to NLP developers as 
well as pure-linguists. Using bilingual re-
sources is difficult for linguists who are unfa-
miliar with computation, which hampers capa-
bilities of bilingual resources. NLP developers 
sometimes need a kind of workbench to check 
their resources. The online interface this paper 
introduces can satisfy these needs. In order to 
implement the interface, this research deals 
with how to align Korean and Japanese phras-
es and interpolates them into the original bi-
lingual corpus in an automatic way. 
1 Introduction 
Bilingual or multilingual corpora are significant 
language resources in various language studies, 
such as language education, comparative linguis-
tics, in particular, NLP. What holds the key posi-
tion in bilingual resources is how to align lin-
guistic units between two languages. In this con-
text, three fundamental questions about how to 
harness bilingual resources can be raised; (i) 
which linguistic unit or level should correspond 
to those in the corresponding language? (ii) 
which method should be employed for align-
ment? (iii) which environments should be pre-
pared for users?  
This paper covers these matters related to bi-
lingual resources and their use. The language 
resource that this paper handles is the Sejong Ko-
rean-Japanese Bilingual Corpus (henceforth 
SKJBC). 1  The original version of the SKJBC, 
constructed in a XML format, aligns sentence by 
                                                 
1 The SKJBC is readily available for academic and research 
purposes only.  For information on license conditions and 
others, please contact the National Academy of Korean Language 
(http://www.korean.go.kr/eng/index.jsp). 
sentence or paragraph by paragraph. This re-
search re-organizes and re-aligns the original 
version using GIZA++ (Och and Ney, 2003) and 
Moses (Koehn et al 2007), and interpolates the 
aligning information into each original file auto-
matically. Turning to the interface, this research 
converts the whole data into a database system 
(MySQL) to guarantee data integrity. Building 
on the database, this research implements an on-
line search system accessible without any restric-
tions; dubbed NARA2. 
2 The SKJBC 
The SKJBC had been constructed as a subset of 
the Sejong project3 which had been carried out 
from 1998 to 2007, sponsored by the Korean 
government. The SKJBC is divided into two 
parts; one is the raw corpus annotated only with 
sentence aligning indices, the other is the POS-
tagged corpus, in which the tag set for Korean 
complies with the POS-tagging guideline of the 
Sejong project, and the morphological analysis 
for Japanese is based on ChaSen (Matsumoto et 
al., 1999). This paper is exclusively concerned 
with the latter, because it is highly necessary for 
the phrase alignment to make use of well-
segmented and well-refined data. Table 1 illu-
strates the basic configuration of the SKJBC. 
Since the prime purpose of the Sejong project 
was to build up balanced corpora, the SKJBC 
consists of various genres, as shown in Figure 1. 
This makes a clear difference from other bilin-
gual resources where the data-type is normally 
homogeneous (e.g. newspapers). Moreover, since 
it had been strictly prohibited to manipulate the 
                                                 
2 The name, NARA, has meanings in both Korean and Jap-
anese. It is a local name in Japan; it also means ?a country? 
in Korean. Since the name can properly stands for this re-
search?s goal, the name has been used as a project title. 
3 http://www.sejong.or.kr/eindex.php 
146
original source for any reasons, the data in 
SKJBC fully reflect on the real usage of Korean. 
These characteristics, however, sometimes work 
against computational implementation. Bi-texts 
do not always correspond to each other sentence 
by sentence; we can find out that there are a 
number of cases that a sentence matches two or 
more sentences in the other language or the cor-
responding sentences might be non-existent. In 
other words, it is almost impossible to align all 
the sentences only one-to-one. These cases even-
tually produce the multiple-to-multiple alignment, 
unless annotators discard or separate them artifi-
cially. No artificial manipulation was allowed 
under construction, the SKJBC contains quite a 
few pairs in a multiple-to-multiple relation. 
 
Korean Japanese 
type token type token
document 50 ( KoJa : 38, JaKo : 12 ) 
sentence 4,030  4,038 
word 21,734  43,534  10,452 93,395 morpheme 9,483  101,266  10,223 
Table 1. Configuration of the SKJBC 
 
Figure 1. Composition of the SKJBC 
3 Alignment 
This section is connected with the first question 
raised in section 1; the proper level of alignment. 
Most bilingual corpora, including the SKJBC, 
have been constructed sentence by sentence de-
spite shortcomings, because it costs too much 
time and effort to annotate word or phrase cor-
respondence by hand (Abeill?, 2003). To anno-
tate more specified alignment between two lan-
guages is to enhance the utility value of the re-
source; this research, first of all, considers how 
to align at the level of word and phrase. 
Multiple Alignments: Because of the prob-
lem mentioned in the previous section, the pairs 
which do not match in a one-to-one relation were 
excluded from the target of alignment. Through-
out a preliminary experiment, it was born out 
that, if they remained, they led to a worse result.  
After casting them away, the number of target 
sentences is 3,776, which account for about 86 
percent of the whole data. 
Word vs. Phrase: To make the units equiva-
lent as far as possible is the crucial factor in 
aligning as accurately as possible. One of the 
main issues that should be taken into account in 
aligning Korean and Japanese phrases is word 
boundary. Though Korean and Japanese share 
lots of features, the boundary of word or phrase 
is inconsistent with each other. The general con-
cept to segment words in Korean is the so-called 
ejeol, tantamount to word-spacing, whereas that 
in Japanese is bunsetsu, what we say. The differ-
ence stems from the different writing style; Ko-
rean inserts spacing between words, while Japa-
nese seldom uses spacing. Consequently, each 
word in Korean is virtually equivalent to a phras-
al unit in Japanese, as given in (1-2). 
 
(1) ??? ?/VV+?/EP+?/EF 
wus-ess-ta 
laugh-PAST-DC ?laughed? 
(2) ?? ??/VIN 
? ?/AU 
warat-ta 
laugh-PAST ?langhed? 
 
The first line (i.e. ejeol) in (1) for Korean corres-
ponds to the first and second line (i.e. bunsetsu) 
in (2). Hence, it is the most suitable choice to 
align Korean morphemes (e.g. ? wus) and Japa-
nese bunsetsu (e.g. ?? warat). 
On the other hand, there is a clear cut between 
lemmatized lexical forms and surface forms in 
Japanese, (e.g. ?? and ?? in the above, re-
spectively), whereas there is none in Korean. In 
order to prevent the result from being biased, this 
paper establishes two training sets (i.e. lemma-
tized and surface forms) for alignment. 
Word Sense Disambiguation (WSD): Other 
than the above issues, it is also needed to consid-
er WSD. For example, a Korean word ? salm 
has two meanings; one is ?life? as a nominal ex-
pression, the other is ?boil? as a verbal lexeme, 
which correspond to ? sei, ?? niru, respec-
tively. This research, therefore, makes training 
data composed of each morpheme plus its POS 
tag, such as ??/NNG? and ??/NCPV?. 
4 Auto-interpolation 
Turning to the second question, this part covers 
how to align and annotate. Were it not for auto-
matic processing, it would be painstaking work 
to construct bilingual resources even line by line. 
One popular toolkit to align linguistic units be-
147
tween two languages in an unsupervised way is 
GIZA++. 
Even though GIZA++ yields fairly good 
?word? alignment, much remains still to be done.  
For instance, those who want to study two or 
more languages from a comparative stance are 
certain to need syntactic data which offer more 
information about language diversity than plain 
word-mapping. Besides, Statistical Machine 
Translation (SMT) commonly runs under the 
phrase-based model. This research employs the 
Moses toolkit to establish phrase tables. The 
baseline of this research is the factorless one with 
a five-gram language model. 
In order to measure the accuracy of alignment, 
this research uses the BLEU scoring (Papineni et 
al., 2002) which has been widely used for eva-
luating SMT, under the hypothesis that the 
BLEU score denotes how well-established the 
phrase table is. For the evaluation purpose, 500 
sentences were selected from the SKJBC at ran-
dom, and tested within each SMT baseline, as 
given in Table 2. 
 
KoJa JaKo 
lemmatized 72.72 71.37
surface 72.98 72.83
surface + tag 70.55 68.26
Table 2. BLEU Score 
(3) <link xtargets="1.1.p8.s4 ; 1.1.p14.s3"> 
<phr xtargets="w1 w2 w3 w4 ; w1 w2 w3 w4"> 
<wrd xtargets="w3 ; w1"> 
<wrd xtargets="w5 ; w5"> 
</link> 
(4) <s id=1.1.p8.s4> 
??? kulaya  ?then? 
<w id=w1>??/VV</w>  kule 
<w id=w2>??/EC</w>  yeya 
????.  caywulop-ci  ?be free?  
<w id=w3>???/VA</w>  caywulop ?free? 
<w id=w4>?/EF</w>  ci 
<w id=w5>./SF</w> 
</s> 
(5) <s id=1.1.p14.s3> 
<w id=w1>??</w> ??/NG jiyuu ?freedom? 
<w id=w2>?</w> ?/AU  da  
<w id=w3>??</w> ??/PJC kara  
<w id=w4>?</w> ?/PEN  ne  
<w id=w5>?</w> ?/SYF 
</s> 
 
Korean and Japanese are typologically very 
similar. In particular, they have very similar 
word order, which makes them easy to align us-
ing GIZA++ and Moses. Therefore, we could ex-
pect the baselines to perform well, and Table 2 
proves it. Table 2 indicates the baselines using 
Japanese surface forms are slightly better than 
those using lemmatized forms. The next step is 
to confirm whether or not the baselines with POS 
tags decrease performance. The last line in Table 
2 implies it is not the case, there is a slight de-
cline.  
Building on the last baselines, this research in-
terpolates word and phrase aligning information 
into the original XML files as presented in (3-5), 
which means ?Then, you will be free?. Figure 2 
represents how the online interface this paper 
proposes handles (3-5). 
 
 
Figure 2. Sample of Online Interface 
5 Online Search Interface 
Last but maybe the most important is a user-
friendly interface. Those who have a solid back-
ground in computation could take advantage of 
computational surroundings (e.g. Moses). Most 
linguists, however, are not aware of how to use 
bilingual data so well. It might look uneasy or 
even vague for them to harness bilingual re-
sources for their current research. That means, no 
matter how good the bilingual resource is or no 
matter how well-trained the word or phrase table 
is, unless there is an available interface, the re-
source becomes no more than a very restricted 
one for a small number of people. Bilingual re-
sources are not NLP-dominated ones, admitting 
NLP developers employ them most widely. They 
are also useful in doing comparative language 
research, making learning materials, or even hu-
man translation. Since one of the easiest interface 
in these days would be web-browsers, this re-
search provide a web-interface; NARA (ver. 2).4 
The interface of NARA system looks like a 
common search site (e.g. Google). A simple 
search option takes a position on the front side, 
assuming most of users are unfamiliar with lin-
guistic terms. On the other hand, advanced 
search mode, as given in Figure 3, offers more 
specialized search options. One can search by tag, 
morpheme, or word with specific sub-options, 
such as matching type. One can also select the 
result format such as word, sentence, or span. In 
order to compare the search result in various 
ways, there are some configuration options, such 
as search direction (i.e. KoJa or JaKo), genre, 
source language, etc. 
                                                 
4 http://corpus.mireene.com/nara.php 
148
Turning to the output screen, as shown in Fig-
ure 4, each underlined word has its correspond-
ing word or phrase. When the pointer is over an 
underlined word, the system highlights the re-
lated words and phrases. If it is necessary to 
check out more information (e.g. source), one 
can use ?INFO? buttons. Finally, the interface 
offers a function to save the current result to a 
spreadsheet (MS-Excel). 
6 Conclusion 
Focusing on the Sejong Korean Japanese Bilin-
gual Corpus (SKJBC), this paper covers three 
matters about how to use and show bilingual re-
sources, and provides a user-friendly online in-
terface to search the SKJBC. The NARA inter-
face is applicable to any other bilingual resources 
in further researches, because it has been de-
signed data-independently. We have already used 
it for aligned Korean-English text. 
 
Acknowledgments 
Part of this work was carried out while the first 
author was an intern at the NICT Language In-
frastructure Group. The fist author was also 
sponsored by the BK21 Project (Global Intern-
ship). We owe special thanks to Prof. Jae-Woong 
Choe, Prof. Han-Seop Lee, Dr. Dong-Sung Kim, 
Eric Nichols, Yeolwon Seong, and Inbean Lim. 
References  
Anne Abeill?. 2003. Treebanks. Kluwer Academic 
Publishers, Hingham, MA, USA. 
Franz J. Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models, 
Computational Linguistics, 29(1): 19-51. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. 
Annual Meeting of the ACL. 
Philipp Koehn, Marcello Federico, Wade Shen, Nico-
la Bertoldi, Ond?rej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, Richard 
Zens, Alexandra Constantin, Christine Corbett Mo-
ran, and Evan Herbst. 2007. Moses: Open Source 
Toolkit for Statistical Machine Translation. 
Annual Meeting of the ACL. 
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, 
and Yoshitaka Hirano. 1999. Japanese Morpho-
logical Analysis System ChaSen version 2.0 
Manual. NAIST-ISTR99009. 
Figure 3. Screenshot of Advanced Search Mode 
 
Figure 4. Phrase Alignment for ?a mysterious sea route? 
149
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 1?8,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Enhancing the Japanese WordNet
Francis Bond,? Hitoshi Isahara,? Sanae Fujita,?
Kiyotaka Uchimoto,? Takayuki Kuribayashi? and Kyoko Kanzaki?
? NICT Language Infrastructure Group, ? NICT Language Translation Group
<bond@ieee.org,{isahara,uchimoto,kuribayashi,kanzaki}@nict.go.jp>
? Sanae Fujita, NTT Communications Science Laboratory
<sanae@kecl.cslab.ntt.co.jp>
Abstract
The Japanese WordNet currently has
51,000 synsets with Japanese entries. In
this paper, we discuss three methods of
extending it: increasing the cover, linking
it to examples in corpora and linking it
to other resources (SUMO and GoiTaikei).
In addition, we outline our plans to make
it more useful by adding Japanese defini-
tion sentences to each synset. Finally, we
discuss how releasing the corpus under an
open license has led to the construction
of interfaces in a variety of programming
languages.
1 Introduction
Our goal is to make a semantic lexicon of
Japanese that is both accesible and usable. To
this end we are constructing and releasing the
Japanese WordNet (WN-Ja) (Bond et al, 2008a).
We have almost completed the first stage,
where we automatically translated the English
and Euro WordNets, and are hand correcting it.
We introduce this in Section 2. Currently, we
are extending it in three main areas: the first
is to add more concepts to the Japanese Word-
Net, either by adding Japanese to existing En-
glish synsets or by creating new synsets (? 3).
The second is to link the synsets to text exam-
ples (? 4). Finally, we are linking it to other re-
sources: the Suggested Upper Merged Ontology
(SUMO) (Niles and Pease, 2001), the Japanese
semantic lexicon GoiTaikei (Ikehara et al, 1997),
and a collection of illustrations taken from the
Open ClipArt Library (Phillips, 2005) (? 5).
2 Current State
Currently, the WN-Ja consists of 157,000 senses
(word-synset pairs) 51,000 concepts (synsets) and
81,000 unique Japanese words (version 0.91). The
relational structure (hypernym, meronym, do-
main, . . . ) is based entirely on the English Word-
Net 3.0 (Fellbaum, 1998). We have Japanese
words for 43.0% of the synsets in the English
WordNet. Of these synsets, 45% have been
checked by hand, 8% were automatically cre-
ated by linking through multiple languages and
46% were automatically created by adding non-
ambiguous translations, as described in Bond
et al (2008a). There are some 51,000 synsets with
Japanese candidate words that have not yet been
checked. For up-to-date information on WN-Ja
see: nlpwww.nict.go.jp/wn-ja.
An example of the entry for the synset
02076196-n is shown in Figure 1. Most fields
come from the English WordNet. We have added
the underlined fields (Ja Synonyms, Illustration,
links to GoiTaikei, SUMO) and are currently
adding the translated definition (Def (Ja)). In
the initial automatic construction there were 27
Japanese words associated with the synset,1 in-
cluding many inappropriate translations for other
senses of seal (e.g., ?? hanko ?stamp?). These
were reduced to three after checking: ????,
?? azarashi ?seal? and ?? ? shi-ru ?seal?.
Synsets with? in their names are those for which
there is currently no Japanese entry in the Word-
Net.
The main focus of this year?s work has been
this manual trimming of badly translated words.
The result is a WordNet with a reasonable cov-
erage of common Japanese words. The precision
per sense is just over 90%. We have aimed at high
coverage at the cost of precision for two reasons:
(i) we think that the WordNet must have a rea-
1????, ???, ????, ??, ?, ??, ??, ?
?, ??, ?, ??, ??, ??, ??, ??, ??, ??, ?
?, ???, ?, ???, ??, ??, ??, ??, ?? ?, ?
?, ??, ??, ??, ??, ??, ??,??,?, ??, ??
1
sonable coverage to be useful for NLP tasks and
(ii) we expect to continue refining the accuracy
over the following years. Our strategy is thus dif-
ferent from Euro WordNet (Vossen, 1998), where
initial emphasis was on building a consistent and
complete upper ontology.
3 Increasing Coverage
We are increasing the coverage in two ways. The
first is to continue to manually correct the auto-
matically translated synsets. This is being done
both by hand, as time permits, and also by com-
paring against other resources such as GoiTaikei
and Wikipedia. When we check for poor candi-
dates, we also add in missing words as they occur
to us.
More interestingly, we wish to add synsets for
Japanese concepts that may not be expressed in
the English WordNet. To decide which new con-
cepts to add, we will be guided by the other tasks
we are doing: annotation and linking. We intend
to create new synsets for words found in the cor-
pora we annotate that are not currently covered,
as well as for concepts that we want to link to.
An example for the first is the concept ?? go-
han ?cooked rice?, as opposed to the grain ?
kome ?rice?. An example of the second is???
? ?shinguru ?single: a song usually extracted
from a current or upcoming album to promote
the album?. This is a very common hypernym in
Wikipedia but missing from the English Word-
Net.
As far as possible, we want to coordinate the
creation of new synsets with other projects: for
example KorLex: the Korean WordNet aleady
makes the cooked rice/grain distinction, and the
English WordNet should also have a synset for
this sense of single.
4 Text Annotation
We are in the process of annotating four texts
(Table 1). The first two are translations of Word-
Net annotated English Texts (SemCor and the
WordNet definitions), the third is the Japanese
newspaper text that forms the Kyoto Corpus
and the fourth is an open corpus of bilingual
Japanese-English sentences (Tanaka). In 2009,
we expect to finish translating and annotate all
of SemCor, translate the WordNet definitions and
Name Sentences Words Content Words
SemCor 12,842 224,260 120,000
Definitions 165,977 1,468,347 459,000
Kyoto 38,383 969,558 527,000
Tanaka 147,190 1,151,892 360,000
Table 1: Corpora to be Sense Tagged
start annotation on the Kyoto and Tanaka Cor-
pora.
This annotation is essential for finding missing
senses in the Japanese WordNet, as well as get-
ting the sense distributions that are needed for
supervised word sense disambiguation.
4.1 SemCor
SemCor is a textual corpus in which words have
been both syntactically and semantically tagged.
The texts included in SemCor were extracted
from the Brown corpus (Francis and Kucera,
1979) and then linked to senses in the English
WordNet. The frequencies in this corpus were
used to give the sense frequencies in WordNet
(Fellbaum, 1998). A subset of this corpus (Mul-
tiSemCor) was translated into Italian and used
as a corpus for the Italian WordNet (Bentivogli
et al, 2004). We are translating this subset into
Japanese.
In the same way as Bentivogli et al (2004), we
are exploiting Cross-Language Annotation Trans-
fer to seed the Japanese annotation. For exam-
ple, consider (1)2. The content words answer,
was, simple, honest are tagged in SemCor. They
can be aligned with their translations ?? ko-
tae ?answer?, ?? kantan ?simple?, ?? soc-
choku ?honest? and ??? datta ?was?. This
allows us to tag the Japanese translation with
the same synsets as the English, and thus disam-
biguate them.
(1) His answeri wasj simplek but honestl .
??i ? ??k ???? ??l ? ??
???j ?
However, just because all the English words
have sysnets in WordNet, it is not always the
case for the translations. For example, the En-
glish phrase last night can be translated into ?
? zen?ya ?last-night?. Here the two English
words (and synsets) link to a single Japanese
2Sentence 96 in b13.
2
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Synset 02076196-n
Synonyms
?
?
ja ??, ????, ???
en seal9
fr phoque
?
? Illustration
animal/seal.png
Def (en) ?any of numerous marine mammals that come on shore to breed; chiefly of cold regions?
Def (ja) ???????????????????????????????
Hypernyms ?????/pinniped
Hyponyms ?/crabeater seal ?/eared seal ??/earless seal
GoiTaikei ??537:beast??
SUMO ? Carnivore
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Example Entry for Seal/??
word which has no suitable synset in the English
WordNet. In this case, we need to create a new
synset unique to the Japanese WordNet.3
We chose a translated SemCor as the basis of
annotation for two main reasons: (i) the cor-
pus can be freely redistributed ? we expect
the glosses to be useful as an aligned corpus of
Japanese-English-Italian and (ii) it has other an-
notations associated with it: Brown corpus POS
annotation, Penn Treebank syntactic annotation.
4.2 WordNet Definitions
Our second translated corpus is formed from
the WordNet definitions (and example sentences)
themselves (e.g., the def field shown in Figure 1).
The English definitions have been annotated with
word senses in the Princeton WordNet Gloss Cor-
pus. In the same way that we do for SemCor, we
are translating the definitions and examples, and
using the existing annotation to seed our annota-
tion.
Using the definitions as the base for a sense
annotated corpus is attractive for the following
reasons: (i) the translated corpus can be freely
redistributed ? we expect the definitions to be
useful as an aligned corpus and also to be useful
for many other open lexicons; (ii) the definitions
are useful for Japanese native speakers using the
WordNet, (iii) the definitions are useful for unsu-
pervised sense disambiguation techniques such as
LESK (Baldwin et al, 2008); (iv) other projects
3Arguably, the fact that one says last night (not yester-
day night) for the night proceeding today and tomorrow
night (not next night) for the night following today sug-
gests that these multi-word expressions are lexicalized and
synsets should be created for them in the English Word-
Net. However, in general we expect to create some synsets
that will be unique to the Japanese WordNet.
have also translated synset definitions (e.g. Span-
ish and Korean), so we can hope to create a multi-
lingual corpus here as well and (v) the definitions
can be used as a machine readable dictionary, and
various information extracted from there (Barn-
brook, 2002; Nichols et al, 2006)
4.3 Kyoto Text Corpus
The Kyoto Text Corpus consists of newspaper
text from the Mainichi Newspaper (1995), seg-
mented and annotated with Japanese POS tags
and dependency trees (Kurohashi and Nagao,
2003). The corpus is made up of two parts. The
first consists of 17 full days of articles and the sec-
ond of one year?s editorials. We hope to annotate
at least parts of it during 2009.
Even though the Kyoto Text Corpus is not
freely redistributable, we have chosen to anno-
tate it due to the wealth of annotation associated
with it: dependency trees, predicate-argument re-
lations and co-reference (Iida et al, 2007), trans-
lations into English and Chinese (Uchimoto et al,
2004) and sense annotations from the Hinoki
project (Bond et al, 2006). We also felt it was
important to tag some native Japanese text, not
only translated text.
4.4 Tanaka Corpus
Finally, we will also tag the Tanaka Corpus, an
open corpus of Japanese-English sentence pairs
compiled by Professor Yasuhito Tanaka at Hyogo
University and his students (Tanaka, 2001) and
released into the public domain. The corrected
version we use has around 140,000 sentence pairs.
This corpus is attractive for several reasons.
(i) it is freely redistributable; (ii) it has been in-
dexed to entries in the Japanese-English dictio-
3
nary JMDict (Breen, 2003); (iii) part of it has
also been used in an open HPSG-based treebank
(Bond et al, 2008b); (iv) further, translations in
other languages, most notably French, have been
added by the TATOEBA project.4 Our plan is
to tag this automatically using the tools devel-
oped for the Kyoto corpus annotation, and then
to open the data to the community for refinement.
We give a typical example sentence in (2).
(2) ??????????????????
?Some birds are sitting on the branch of that
tree.? (en)
?Des oiseaux se reposent sur la branche de cet
arbre.? (fr)
5 Linking to other resources
We currently link the Japanese WordNet to three
other resources: the Suggested Upper Merged
Ontology; GoiTaikei, a Japanese Lexicon; and a
collection of pictures from the Open Clip Art Li-
brary (OCAL: Phillips (2005)).
For SUMO we used existing mappings. For the
other resources, we find confident matches auto-
matically and then generalize from them. We find
matches in three ways:
MM Monosemous monolingual matches
e.g. cricket bat or ?? azarashi ?seal?
MB Monosemous bilingual matches
e.g. ????seal?
HH Hypernym/Hyponym pairs
e.g. ?seal ? mammal?
We intend to use the same techniques to link
other resources, such as the concepts from the
EDR lexicon (EDR, 1990) and the automati-
cally extracted hypernym-hyponym links from
Torishiki-kai (Kuroda et al, 2009).
5.1 SUMO
The Suggested Upper Merged Ontology (SUMO)
is a large formal public ontology freely released
by the IEEE (Niles and Pease, 2001).
Because the structure of the Japanese Word-
Net is closely linked to that of the English Word-
Net, we were able to take advantage of the ex-
isting mappings from the English WordNet to
SUMO. There are 102,669 mappings from SUMO
4wwwcyg.utc.fr/tatoeba/
Carnivore Business Competition
Figure 2: SUMO illustrations
to WordNet: 3,593 equivalent, 10,712 where the
WordNet synset subsumes the SUMO concept,
88,065 where the SUMO concept subsumes the
WordNet concept, 293 where the negation of the
SUMO concept subsumes the WordNet synset
and 6 where the negation of the SUMO concept
is equivalent to the WordNet synset. According
to the mapping, synset 02076196-n ?? azarashi
?seal?, shown in Figure 1 is subsumed by the
SUMO concept ??Carnivore??. There is no link
between seal and carnivore in WordNet, which
shows how different ontologies can complement
each other.
Linking to SUMO also allowed us to use the
SUMO illustrations.5 These consist of 12,237
links linking 4,607 concepts to the urls of 10,993
illustrations. These are mainly taken from
from Wikimedia (upload.wikimedia.org), with
around 1,000 from other sources. The pictures
can be linked quite loosely to the concepts. For
example, ??Carnivore?? is illustrated by a lion eat-
ing meat, and ??BusinessCompetition?? by a pic-
ture of Wall Street.
As we wanted our illustrations to be more con-
crete, we only use SUMO illustrations where the
SUMO-WordNet mapping is equivalence. This
gave 4,384 illustrations for 999 synsets.
5.2 GoiTaikei
Linking Goi-Taikei, we used not only the
Japanese dictionary published in Ikehara et al
(1997), but also the Japanese-English dictionary
used in the machine translation system ALT-J/E
(Ikehara et al, 1991). We attempted to match
synsets to semantic categories by matching the
5Available at http://sigmakee.cvs.sourceforge.
net/viewvc/sigmakee/KBs/pictureList.kif, thanks to
Adam Pease for letting us know about them.
4
Japanese, English and English-Japanese pairs to
unambiguous entries in Goi-Taikei. For example,
the synset shown in Figure 1 was automatically
assigned the semantic category ??537:beast??, as
?? appears only once in WN-Ja, with the synset
shown, and once in the Japanese dictionary for
ALT-J/E with a single semantic category.
We are currently evaluating our results against
an earlier attempt to link WordNet and GoiTaikei
that also matched synset entries to words in Goi-
Taikei (Asanoma, 2001), but did not add an extra
constraint (that they must be either monosemous
or match as a hypernym-hyponym pair).
Once we have completed the mapping, we will
use it to check for inconsistencies in the two re-
sources.
5.3 Open ClipArt Library
In order to make the sense distinctions more vis-
ible we have semi-automatically linked synsets
to illustrations from the Open Clip Art Library
(OCAL: Phillips (2005)) using the mappings pro-
duced by Bond et al (2008a).
We manually checked the mappings and added
a goodness score. Illustrations are marked as:
3 the best out of multiple illustrations
2 a good illustration for the synset
1 a suitable illustration, but not perfect
This tag was used for black and white im-
ages, outlines, and so forth.
After the scoring, there were 874 links for 541
synsets (170 scored 1, 642 scored 2 and 62 scored
3). This is only a small subset of illustrations in
OCAL and an even smaller proportion of word-
net. However, because any illustrated synset alo
(in theory) illustrates its hypernyms, we have in-
directly illustrated far more than 541 synsets:
these figures are better than they seem.
There are far fewer OCAL illustrations than
the SUMO linked illustrations. However, they are
in general more representative illustrations (espe-
cially those scored 2 and above), and the source of
the clipart is available as SVG source so it is easy
to manipulate them. We think that this makes
them particularly useful for a variety of tasks.
One is pedagogical ? it is useful to have pic-
tures in learners? dictionaries. Another is in cross-
cultural communication - for example in Pangea,
where children use pictons (small concept repre-
senting pictures) to write messages (Takasaki and
Mori, 2007).
The OCAL illustrations mapped through
WordNet to 541 SUMO concepts. We have given
these links to the SUMO researchers.
6 Interfaces
We released the Japanese WordNet in three for-
mats: tab-delimited text, XML and as an SQLite
database. The license was the same as English
WordNet. This is a permissive license, the data
can be reused within proprietary software on the
condition that the license is distributed with that
software (similar to the MIT X license). The
license is also GPL-compatible, meaning that
the GPL permits combination and redistribution
with software that uses it.
The tab delimited format consists of just a list
of synsets, Japanese words and the type of link
(hand, multi-lingual or monosemous):
02076196-n ?? hand
02076196-n ???? hand
02076196-n ??? hand
We also output in WordNet-LMF (Francopoulo
et al, 2006; Soria et al, 2009), to make the
program easily available for other WordNet re-
searchers. In this case the synset structure was
taken from the English WordNet and the lem-
mas from the Japanese WordNet. Because of the
incomplete coverage, not all synsets contain lem-
mas. This format is used by the Kyoto Project,
and we expect it to become the standard ex-
change format for WordNets (Vossen et al, 2008).
Finally, we also created an SQL database. This
contains information from the English WordNet,
the Japanese WordNet, and links to illustra-
tions. We chose SQLite,6 a self-contained, zero-
configuration, SQL database engine whose source
code is in the public domain. The core structure
is very simple with six tables, as shown in Fig-
ure 3.
As we prepared the release we wrote a perl
module for a basic interface. This was used to
develop a web interface: Figure 4 shows a screen-
shot.
6http://www.sqlite.org
5
word
wordid 
 lang 
 lemma 
 pron 
 pos 
sense
synset 
 wordid 
 lang 
 rank 
 lexid 
 freq 
 src 
1..*1
synset
pos 
 name 
 src 
11..*
synsetDef
synset 
 lang 
 def 
 sid 
11
synlink
synset1 
 synset2 
 link 
 src 
1 1..*
xlink
synset 
 resource 
 xref 
 misc 
 confidence
1 1..*
Figure 3: Database Schema
Figure 4: Web Search Screenshot
6
7 Discussion
In contrast to earlier WordNets, the Japanese
WordNet was released with two known major im-
perfections: (i) the concept hierarchy was en-
tirely based on English with no adaptation to
Japanese and (ii) the data was released with some
unchecked automatically created entries. The re-
sult was a WordNet that did not fully model the
lexical structure of Japanese and was known to
contain an estimated 5% errors. The motivation
behind this was twofold. Firstly, we wanted to try
and take advantage of the open source model. If
the first release was good enough to be useful, we
hoped to (a) let people use it and (b) get feedback
from them which could then be incorporated into
the next release. This is the strategy known as
release early, release often (Raymond, 1999).
Secondly, we anticipated the most common use
of the WordNet to be in checking whether one
word is a hypernym of another. In this case, even
if one word is wrong, it is unlikely that the other
will be, so a small percentage of errors should be
acceptable.
From the practical point of view, the early re-
lease appears to have been a success. The SQL
database proved very popular, and within two
weeks of the first release someone produced a
python API. This was soon followed by inter-
faces in java, ruby, objective C and gauche. We
also received feedback on effective indexing of the
database and some corrections of entries ? these
have been included in the most recent release
(0.91).
The data from the Japanese WordNet has al-
ready been incorporated into other projects. The
first was the Multi-Lingual Semantic Network
(MLSN) (Cook, 2008) a WordNet based net-
work of Arabic, Chinese, English, German and
Japanese. Because both the Japanese WordNet
and MLSN use very open licenses, it is possible
to share entries directly. We have already re-
ceived useful feedback and over a thousand new
entries from MLSN. The second project using our
data is the Asian WordNet (Charoenporn et al,
2008). They have a well developed interface for
collaborative development of linguistic resources,
and we hope to get corrections and additions
from them in the future. Another project us-
ing the Japanese WordNet data is the Language
Grid (Ishida, 2006) which offers the English and
Japanese WordNets as concept dictionaries.
We have also been linked to from other re-
sources. The Japanese-English lexicon project
JMDict (Breen, 2004) now links to the Japanese
WordNet, and members of that project are us-
ing WordNet to suggest new entries. We used
JMDict in the first automatic construction stage,
so it is particularly gratifying to be able to help
JMDict in turn.
Finally, we believe that data about language
should be shared ? language is part of the com-
mon heritage of its speakers. In our case, the
Japanese WordNet was constructed based on the
work that others made available to us and thus we
had a moral obligation to make our results freely
available to others. Further, projects that create
WordNets but do not release them freely hinder
research on lexical semantics in that language ?
people cannot use the unreleased resource, but it
is hard to get funding to duplicate something that
already exists.
In future work, in addition to the planned ex-
tensions listed here, we would like to work on
the following: Explicitly marking lexical variants;
linking to instances in Wikipedia; adding deriva-
tional and antonym links; using the WordNet for
word sense disambiguation.
8 Conclusion
This paper presents the current state of the
Japanese WordNet (157,000 senses, 51,000 con-
cepts and 81,000 unique Japanese words, with
links to SUMO, Goi-Taikei and OCAL) and out-
lined our plans for further work (more words,
links to corpora and other resources). We hope
that WN-Ja will become a useful resource not only
for natural language processing, but also for lan-
guage education/learning and linguistic research.
References
Naoki Asanoma. 2001. Alignment of ontologies:wordnet
and goi-taikei. In NAACL Wokshop on WordNet &
Other Lexical Resources, pages 89?94. Pittsburgh, USA.
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2008. MRD-
based word sense disambiguation: Further extending
Lesk. In Proc. of the 3rd International Joint Conference
on Natural Language Processing (IJCNLP-08), pages
775?780. Hyderabad, India.
Geoff Barnbrook. 2002. Defining Language ? A local
7
grammar of definition sentences. Studies in Corpus Lin-
guistics. John Benjamins.
Luisa Bentivogli, Pamela Forner, and Emanuele Pianta.
2004. Evaluating cross-language annotation transfer in
the MultiSemCor corpus. In 20th International Con-
ference on Computational Linguistics: COLING-2004,
pages 364?370. Geneva.
Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2006. The Hinoki syntactic and semantic treebank of
Japanese. Language Resources and Evaluation, 40(3?
4):253?261. (Special issue on Asian language technol-
ogy).
Francis Bond, Hitoshi Isahara, Kyoko Kanzaki, and Kiy-
otaka Uchimoto. 2008a. Boot-strapping a WordNet
using multiple existing WordNets. In Sixth Interna-
tional conference on Language Resources and Evalua-
tion (LREC 2008). Marrakech.
Francis Bond, Takayuki Kuribayashi, and Chikara
Hashimoto. 2008b. Construction of a free Japanese
treebank based on HPSG. In 14th Annual Meeting of
the Association for Natural Language Processing, pages
241?244. Tokyo. (in Japanese).
James W. Breen. 2003. Word usage examples in an elec-
tronic dictionary. In Papillon (Multi-lingual Dictionary)
Project Workshop. Sapporo.
James W. Breen. 2004. JMDict: a Japanese-multilingual
dictionary. In Coling 2004 Workshop on Multilingual
Linguistic Resources, pages 71?78. Geneva.
Thatsanee Charoenporn, Virach Sornlerlamvanich,
Chumpol Mokarat, and Hitoshi Isahara. 2008. Semi-
automatic compilation of Asian WordNet. In 14th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 1041?1044. Tokyo.
Darren Cook. 2008. MLSN: A multi-lingual semantic net-
work. In 14th Annual Meeting of the Association for
Natural Language Processing, pages 1136?1139. Tokyo.
EDR. 1990. Concept dictionary. Technical report, Japan
Electronic Dictionary Research Institute, Ltd.
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
W. Nelson Francis and Henry Kucera. 1979. BROWN
CORPUS MANUAL. Brown University, Rhode Island,
third edition.
Gil Francopoulo, Monte George, Nicoletta Calzolari, Mon-
ica Monachini, Nuria Bel, Mandy Pet, and Claudia So-
ria. 2006. Lexical markup framework (LMF). In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006). Genoa,
Italy.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference relations.
In ACL Workshop: Linguistic Annotation Workshop,
pages 132?139. Prague.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing ? effects of new methods in ALT-J/E ?. In
Third Machine Translation Summit: MT Summit III,
pages 101?106. Washington DC.
Toru Ishida. 2006. Language grid: An infrastructure for in-
tercultural collaboration. In IEEE/IPSJ Symposium on
Applications and the Internet (SAINT-06), pages 96?
100. (keynote address).
Kow Kuroda, Jae-Ho Lee, Hajime Nozawa, Masaki Mu-
rata, and Kentaro Torisawa. 2009. Manual cleaning of
hypernyms in Torishiki-Kai. In 15th Annual Meeting of
The Association for Natural Language Processing, pages
C1?3. Tottori. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. In Anne Abeille?, editor, Treebanks: Building
and Using Parsed Corpora, chapter 14, pages 249?260.
Kluwer Academic Publishers.
Eric Nichols, Francis Bond, Takaaki Tanaka, Sanae Fu-
jita, and Daniel Flickinger. 2006. Robust ontology ac-
quisition from multiple sources. In Proceedings of the
2nd Workshop on Ontology Learning and Population:
Bridging the Gap between Text and Knowledge, pages
10?17. Sydney.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Chris Welty and Barry Smith, edi-
tors, Proceedings of the 2nd International Conference on
Formal Ontology in Information Systems (FOIS-2001).
Maine.
Jonathan Phillips. 2005. Introduction to the open
clip art library. http://rejon.org/media/writings/
ocalintro/ocal_intro_phillips.html. (accessed
2007-11-01).
Eric S. Raymond. 1999. The Cathedral & the Bazaar.
O?Reilly.
Claudia Soria, Monica Monachini, and Piek Vossen. 2009.
Wordnet-LMF: fleshing out a standardized format for
wordnet interoperability. In Second International Work-
shop on Intercultural Collaboration (IWIC-2009). Stan-
ford.
Toshiyuki Takasaki and Yumiko Mori. 2007. Design and
development of a pictogram communication system for
children around the world. In First International Work-
shop on Intercultural Collaboration (IWIC-2007), pages
144?157. Kyoto.
Yasuhito Tanaka. 2001. Compilation of a multilingual par-
allel corpus. In Proceedings of PACLING 2001, pages
265?268. Kyushu.
Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo, Masaki
Murata, Satoshi Sekine, and Hitoshi Isahara. 2004.
Multilingual aligned parallel treebank corpus reflecting
contextual information and its applications. In Gilles
Se?rasset, editor, COLING 2004 Multilingual Linguistic
Resources, pages 57?64. COLING, Geneva, Switzerland.
P Vossen, E. Agirre, N. Calzolari, C. Fellbaum, S. Hsieh,
C. Huang, H. Isahara, K. Kanzaki, A. Marchetti,
M. Monachini, F. Neri, R. Raffaelli, G. Rigau, and
M. Tescon. 2008. KYOTO: A system for mining,
structuring and distributing knowledge across languages
and cultures. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08). Mar-
rakech, Morocco.
Piek Vossen, editor. 1998. Euro WordNet. Kluwer.
8
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 86?89, Dublin, Ireland, August 23-29 2014.
NTU-MC Toolkit: Annotating a Linguistically Diverse Corpus
Liling Tan
Universit?t des Saarland
Campus, 66123 Saarbr?cken, Germany
alvations@gmail.com
Francis Bond
Nanyang Technological University
14 Nanyang Drive, Singapore 637332
bond@ieee.org
Abstract
The NTU-MC Toolkit is a compilation of tools to annotate the Nanyang Technological University
- Multilingual Corpus (NTU-MC). The NTU-MC is a parallel corpora of linguistically diverse
languages (Arabic, English, Indonesian, Japanese, Korean, Mandarin Chinese, Thai and Viet-
namese). The NTU-MC thrives on the mantra of "more data is better data and more annotation
is better information". Other than increasing parallel data from diverse language pairs, annotat-
ing the corpus with various layers of information allows corpora linguists to discover linguistic
phenomena and provides computational linguists with pre-annotated features for various NLP
tasks. In addition to the agglomeration existing tools into a single python wrapper library, we
have implemented three tools (Mini-segmenter, GaChalign and Indotag) that (i) pro-
vides users with varying analysis of the corpus, (ii) improves the state-of-art performance and
(iii) reimplements a previously unavailable annotation tool as a free and open tool. This paper
briefly describes the wrapper classes available in the toolkit and introduces and demonstrates the
usage of the Mini-segmenter, GaChalign and Indotag.
1 Introduction
The NTU-MC Toolkit was developed in conjunction with the compilation of the Nanyang Technological
University - Multilingual Corpus (NTU-MC) (Tan and Bond, 2012). It is an agglomeration of existing
state-of-art tools into a single python wrapper library. The NTU-MC Toolkit provides python wrapper
classes for tokenizers and Part-of-Speech (POS) taggers for the respectively languages:
? Stanford Segmenter and POS taggers (Arabic and Chinese)
? POSTECH POSTAG/K tagger (Korean)
? tinysegmenter and MeCab (Japanese)
? JVnTextPro (Vietnamese)
Additionally, we implemented three tools to provide complementary or better annotations, viz.:
? Mini-segmenter (Chinese): Dictionary based Chinese segmenter
? GaChalign (Crosslingual): Gale-Church Sentence-level Aligner with variable parameters
? Indotag (Indonesian): Conditional Random Field (CRF) POS tagger.
The following sections of the paper will briefly describe the wrapper classes available in the toolkit (Sec-
tion 2) and introduce and demonstrate the usage of the Mini-segmenter (Section 3), GaChalign
(Section 4) and Indotag (Section 5).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
86
2 Tokenization and POS Tagger Wrappers
Python wrapper classes were written for (i) Stanford Segmenter and POS tagger (Chang et al., 2008;
Toutanova et al., 2003), (ii) POSTECH POSTAG/K tagger (Lee et al., 2002), (iii) tinysegmenter
and MeCab (Kudo et al., 2004) and (iv) JVnTextPro (Nguyen et al., 2010). Although scientifically
uninteresting, it simplifies usage of annotation tools especially for beginner who are new to Natural
Language Processing or python programming. The wrapper classes are also compatible with corpora
readers of the Natural Language Toolkit (NLTK).
Usage Users can either invoke the wrapper classes programmatically
1
:
$ python
?> from ntumc.tk import postech
?> sentence = u"???????????????????????????Tian
Tian Hainanese Chicken Rice????????(Maxwell Food Centre)????
???, ??????????????????????."
?> postech.postagk(sentence)
[(u??????, ?NNP?), (u????, ?JKB?), (u????, ?MAG?), (u????,
?XR?), (u???, ?XSA?), (u???, ?ETM?), (u??????, ?NNG?), ...]
or via command line:
$ echo "???????????????????????????Tian Tian
Hainanese Chicken Rice????????(Maxwell Food Centre)???????,
??????????????????????." > input.txt
$ python ntumc/tk/postech.py input.txt > output.txt
3 Mini-segmenter
The mini-segmenter is dictionary based Chinese segmenter that capitalizes on token length as
heuristics for Chinese text tokenization. The tool includes a dictionary of Singaporean Chinese NEs
crawled from Wikipedia titles and articles on Singapore.
Motivation The mini-segmenter was created to resolve the problem of segmenting localized Chinese
words from the Singaporean variety of Mandarin Chinese in the NTU-MC. After manual inspection,
the Stanford Chinese segmenter
2
was segmenting the Chinese tokens with the wrong word boundary.
For example, the Stanford Chinese word segmenter wrongly tokenized ??? wujielu ?Orchard
road" as ?_?? wu jielu ?black joint-road". Originally, these topological terms were re-segmented
with a manually crafted dictionary built using Wikipedia?s Chinese translations of English names of
Singapore places and streets. Then we found more localized Named Entities (NEs) for person names,
organizations and food terms. Short of building a manually segmented corpus and retraining the Stan-
ford segmenter models, a simple dictionary approach to segmentation could resolve out-of-domain issue.
Innovation A lightweight lexicon/dictionary based Chinese text segmenter. The advantage of us-
ing a lexicon/dictionary for text segmentation is the ability to localize and scale according to the Chinese
variety or domain. The mini-segmenter ranks the token boundaries based on sum of the square of
the tokens? character length,
?
n
i
len(token
i
)
2
, where n is the number of tokens and len(token) is the
character length of each token. This novel scoring is based on the preference for larger chunks than
smaller chunks in a sentence.
Usage The full documentation of the mini-segmenter can be found on https:
//code.google.com/p/mini-segmenter/
1
The example sentence in English, ?One of the most famous Hainanese chicken rice stalls in Singapore, Tian Tian
Hainanaese Chiken Rice is located in the Maxwell Food Centre, with long queues forming in front of the stall every day."
2
both Penn Chinese Treebank (ctb) and Peking University (pku) models
87
Results We evaluate the mini-segmenter output against the Stanford segmenter output with the
fish-head-curry.txt from the NTU-MC which was was previously selected at random as a text
sample for human annotators to verify the tagger accuracy. The Stanford segmenter with Stanford POS
tagger, it achieved 85.94% POS accuracy with 19% mis-segments. Using the mini-segmenter with
the Stanford POS tagger, it achieved 91.27% POS accuracy with 11.43% mis-segments.
4 GaChalign
The GaChalign tool is sentence alignment tool to align sentences given a bitext. The tool is a modifica-
tion of the original Gale-Church algorithm that capitalized on ratio of characters/tokens of two languages
in the bitext to align the sentences (Gale and Church, 1993).
Motivation The Gale-Church algorithm had parameters tuned to suit Indo-European languages more
specifically German-English language pairs. When using state-of-art sentence alignment tool based on
Gale-Church algorithm to align Chinese, Japanese or Korean texts to their respective English texts, the
NTU-MC reported a poor performance in F-measure metrics adheres to standards set by the ARCADE
II project (Chiao et al. 2006). We want to see whether it is possible to improve the algorithm by tune
algorithm using language-pair specific parameters.
Innovation We replaced the mean, variance and penalty parameters from the Gale-Church algorithm
with language-pair specific parameters automatically calculated from a non-aligned corpus.
Results Our experiment with English-Japanese corpus has shown that (i) simply using the calculated
character mean from the unaligned text improves precision and recall of the algorithm; from 61.0%
(default parameters) to 62.0% (language specific) F-scores) and (ii) using language specific penalties
further increased the F-scores to 62.9%. However, aligning syllabic/logographic language (Japanese) to
alphabetic language (English) remains a challenge for Gale-Church algorithm
3
.
5 Indotag
The Indotag is a probabilistic Conditional Random Field (CRF) Bahasa Indonesian Part of Speech
(POS) tagger with the specifications recommended by (Pisceldo et al., 2009). The pre-trained model is
based on the unigram CRF with 2-left and 2-right context features using the Universitas Indonesia?s 1
million word corpus compiled under the Pan Asia Networking Localization (PANL10N) project.
Motivation To reimplement the Indonesian POS tagger described in Pisceldo et al. (2010) using free
and open data and licensing it as open source tool.
Innovation None or not much. An open source reimplementation of a Bahasa Indonesian POS tagger.
Result The IndoTag achieved 78% accuracy when annotating the the fish-head-curry.txt text
sample from the NTU-MC.
6 Discussion
While English POS tagging reports >97% accuracy (Manning, 2011) and sentence alignments for Indo-
European languages performs well at >96% (Gale and Church, 1993; Varga et al., 2007), there is much
room for improvement with regards to POS tagger accuracy for Asian languages and automatic sen-
tence alignments from syllabic/logographic languages to alphabetic ones. Even though the languages in
the NTU-MC are not considered low-resource languages, the tools to annotate them have limited per-
formance. While the maintainers of the NTU-MC continues to push the performance of the individual
tools for these languages, we urge researchers to work on improving NLP tools/application for Asian
languages.
3
Detailed evaluation on the GaChalign experiments can be found on https://code.google.com/p/
gachalign/
88
7 Conclusion
We have introduced the NTU-MC Toolkit that was compiled to annotated the linguistically diverse NTU-
MC. The toolkit agglomerate existing tools into a single python wrapper library. The toolkit also imple-
mented the novel dictionary-based segmenter (Mini-segmenter) to improve state-of-art performance
for Chinese segmentation, an modified Gale-Church algorithm (GaChalign) to improve sentence align-
ments for syllabic-alphabetic language pairs and reimplemented an open source Indotag Bahasa In-
donesian POS tagger.
Acknowledgements
This research was partially funded by a joint JSPS/NTU grant on Revealing Meaning through Multiple
Languages and the Erasmus Mundus Action 2 program MULTI of the European Union, grant agreement
number 2009-5259-5.
The research leading to these results has received funding from the People Programme (Marie
Curie Actions) of the European Union?s Seventh Framework Programme FP7/2007-2013/ under REA
grant agreement n
?
317471.
References
Pi-Chuan Chang, Michel Galley, and Christopher D Manning. 2008. Optimizing chinese word segmentation for
machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation,
pages 224?232. Association for Computational Linguistics.
William A. Gale and Kenneth Ward Church. 1993. A program for aligning sentences in bilingual corpora. Com-
putational Linguistics, 19(1):75?102.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to japanese
morphological analysis. In EMNLP, pages 230?237.
Gary Geunbae Lee, Jeongwon Cha, and Jong-Hyeok Lee. 2002. Syllable-pattern-based unknown-morpheme
segmentation and estimation for hybrid part-of-speech tagging of korean. Computational Linguistics, 28(1):53?
70.
Christopher D Manning. 2011. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In
Computational Linguistics and Intelligent Text Processing, pages 171?189. Springer.
Cam-Tu Nguyen, Xuan-Hieu Phan, and Thu-Trang Nguyen. 2010. Jvntextpro: A java-based vietnamese text
processing tool. http://jvntextpro.sourceforge.net/.
Femphy Pisceldo, Ruli Manurung, and Mirna Adriani. 2009. Probabilistic part-of-speech tagging for bahasa
indonesia.
Liling Tan and Francis Bond. 2012. Building and annotating the linguistically diverse ntu-mc (ntu-multilingual
corpus). In International Journal of Asian Language Processing, 22(4), page 161?174.
Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages
173?180. Association for Computational Linguistics.
Daniel Varga, Peter Halacsy, AndraS Kornai, Viktor Nagy, Laszlo Nemeth, and Viktor TrOn. 2007. Parallel
corpora for medium density languages. Recent Advances in Natural Language Processing IV: Selected Papers
from RANLP 2005, 292:247.
89
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 125?129,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-lingual Parse Disambiguation based on Semantic Correspondence
Lea Frermann
Department of Computational Linguistics
Saarland University
frermann@coli.uni-saarland.de
Francis Bond
Linguistics and Multilingual Studies
Nanyang Technological University
bond@ieee.org
Abstract
We present a system for cross-lingual parse
disambiguation, exploiting the assumption
that the meaning of a sentence remains un-
changed during translation and the fact that
different languages have different ambiguities.
We simultaneously reduce ambiguity in multi-
ple languages in a fully automatic way. Eval-
uation shows that the system reliably discards
dispreferred parses from the raw parser output,
which results in a pre-selection that can speed
up manual treebanking.
1 Introduction
Treebanks, sets of parsed sentences annotated with a
sytactic structure, are an important resource in NLP.
The manual construction of treebanks, where a hu-
man annotator selects a gold parse from all parses
returned by a parser, is a tedious and error prone pro-
cess. We present a system for simultaneous and ac-
curate partial parse disambiguation of multiple lan-
guages. Using the pre-selected set of parses returned
by the system, the treebanking process for multiple
languages can be sped up.
The system operates on an aligned parallel cor-
pus. The languages of the parallel corpus are con-
sidered as mutual semantic tags: As the meaning of
a sentence stays constant during translation, we are
able to resolve ambiguities which exist in only one
of the langauges by only accepting those interpreta-
tions which are licensed by the other language.
In particular, we select one language as the tar-
get language, translate the other language?s seman-
tics for every parse into the target language and thus
align maximally similar semantic representations.
The parses with the most overlapping semantics are
selected as preferred parses.
As an example consider the English sentence They
closed the shop at five, which has the following two
interpretations due to PP attachment ambiguity:1
(1) ?At five, they closed the shop?
close(they, shop); at(close, 5)
(2) ?The shop at five was closed by them?
close(they, shop); at(shop, 5)
The Japanese translation is also ambiguous, but in
a completely different way: it has the possibility of
a zero pronoun (we show the translated semantics).
(3) ?
kare
he
?
ra
PL
?
wa
TOP
?
5
5
?
ji
hour
?
ni
at
?
mise
shop
?
wo
ACC
??
shime
close
?
ta
PAST
?At 5 o?clock, they closed the shop.?
close(they, shop); at(close, 5)
(4) ?At 5 o?clock, as for them, someone closed the shop.?
close(?, shop); at(close, 5)
topic(they,close)
We show the semantic representation of the ambi-
guity with each sentence. Both languages are disam-
biguated by the other language as only the English
interpretation (1) is supported in Japanese, and only
the Japanese interpretation (3) leads to a grammati-
cal English sentence.
2 Related Work
There is no group using exactly the same approach
as ours: automated parallel parse disambiguation
on the basis of semantic analyses. Zhechev and
1In fact it has four, as they can be either plural or the androg-
ynous singular, this is also disambiguated by the Japanese.
125
Way (2008) automatically generate parallel tree-
banks for training of statistical machine translation
(SMT) systems through sub-tree alignment. We do
not aim to carry out the complete treebanking pro-
cess, but to optimize speed and precision of manual
creation of high-quality treebanks.
Wu (1997) and others have tried to simultane-
ously learn grammars from bilingual texts. Burkett
and Klein (2008) induce node-alignments of syntac-
tic trees with a log-linear model, in order to guide
bilingual parsing. Chen et al (2011) translate an
existing treebank using an SMT system and then
project parse results from the treebank to the other
language. This results in a very noisy treebank, that
they then clean. These approaches align at the syn-
tactic level (using CFGs and dependencies respec-
tively).
In contrast to the above approaches, we assume
the existence of grammars and use a semantic rep-
resentation as the appropriate level for cross-lingual
processing. We compare semantic sub-structures, as
those are more straightforwardly comparable across
different languages. As a consequence, our system
is applicable to any combination of languages. The
input is plain parallel text, neither side needs to be
treebanked.
3 Materials and Methods
We use grammars within the grammatical frame-
work of head-driven phrase-structure grammar
(HPSG Pollard and Sag (1994)), with the seman-
tic representation of minimal recursion semantics
(MRS; Copestake et al (2005)). We use two large-
scale HPSG grammars and a Japanese-English ma-
chine translation system, all of which were de-
veloped in the DELPH-IN framework:2 The En-
glish Resource Grammar (ERG; Flickinger (2000))
is used for English parsing, and Jacy (Bender and
Siegel, 2004) for parsing Japanese. For Japanese
to English translation we use Jaen, a semantic-
transfer based machine translation system (Bond
et al, 2011).
3.1 Semantic Interface and Alignment
For the alignment, we convert the MRS struc-
tures into simplified elementary dependency graphs
2http://www.delph-in.net/
x4:pronoun_q[]
e2:_close_v_c[ARG1 x4:pron, ARG2 x9:_shop_n_of]
x9:_the_q[]
e8:_at_p_temp[ARG1 e2, ARG2 x16:_num_hour(5)]
x16:_def_implicit_q[]
Figure 1: EDG for They closed the shop at five.
(EDGs), which abstract away information about
grammatical properties of relations and scopal in-
formation. Preliminary experiments showed that the
former kind of information did not contribute to dis-
ambiguation performance, as number is typically
underspecified in Japanese. As we only consider lo-
cal information in the alignment, scopal information
can be ignored as well. An example EDG is dis-
played in Figure 1.
An EDG consists of a bag of elementary predi-
cates (EPs) which are themselves composed of re-
lations. Each line in Figure 1 corresponds to one
EP. Relations are the elementary building blocks of
the EDG, and loosely correspond to words of the
surface string. EPs consist either of atomic rela-
tions (corresponding to quantifiers), or a predicate-
argument structure which is composed of several re-
lations. During alignment, we only consider non-
atomic EPs, as quantifiers should be considered as
grammatical properties of (lexical) relations, which
we chose to ignore.
Given the EDG representations of the translated
Japanese sentence, and the original target language
EDGs, we can straightforwardly align by matching
substructures of different granularity.
Currently, we align at the predicate level. We are
experimenting with aligning further dependency re-
lation based tuples, which would allow us to resolve
more structural ambiguities.
3.2 The Disambiguation System
Ambiguity in the analyses for both languages is re-
duced on the basis of the semantic analyses returned
for each sentence-pair, and a reduced set of pre-
ferred analyses is returned for both languages. For
each sentence-pair, we (1) parse the English and
the Japanese sentence (MRSE and MRSJ ) (2) trans-
fer the Japanese MRS analyses to English MRSs
(MRSJE) (3) convert the top 11 translated MRSs
126
and the original English MRSs to EDGs3 (EDGE
and EDGJE) (4) align every possible E and JE EDG
combination and determine the set of best aligning
analyses (5) from those, create language specific sets
of preferred parses.
We are comparing semantic representations of the
same language, the English text from the bilingual
corpus and the English machine translation of the
Japanese text. In order to increase robustness of
our alignment system we not only consider com-
plete translations, but also accept partially translated
MRSs in case no complete translation could be pro-
duced. This step significantly increases the recall,
while the partial MRSs proved to be informative
enough for parse disambiguation.
4 Evaluation and Results
We evaluate our model on the task of parse disam-
biguation. We use full sentence match as evaluation
metric, a challenging target.
The Tanaka corpus is used for training and testing
(Tanaka, 2001). It is an open corpus of Japanese-
English sentence pairs. We use version (2008-11)
which contains 147,190 sentence pairs. We hold out
4,500 sentence pairs each for development and test.
For each sentence, we compare the number of the-
oretically possible alignments with the number of
preferred alignments returned by our system. On
average, ambiguity is reduced down to 30%. For
English 3.76 and for Japanese 3.87 parses out of
(at most) 11 analyses remain in the partially disam-
biguated list: both languages benefit equally from
the disambiguation.
We evaluate disambiguation accuracy by counting
the number of times the gold parse was present in the
partially disambiguated set (full sentence match).
Table 1 shows the alignment accuracy results.
The correct parse is included in the reduced set
in 80% of the cases for Japanese, and for 82% of
the cases in English. We match atomic relations
when aligning the semantic structures, which is a
very generic method applicable to the vast major-
ity of sentence pairs. This leads to a recall score of
3These are ranked with a model trained on a hand-
treebanked set. The cutoff was determined empirically: For
both languages the gold parse is included in the top 11 parses in
more than 97% of the cases.
English Japanese
Prec F Prec F
Included 0.820 0.897 0.804 0.887
First Rank 0.659 0.791 0.676 0.803
MRR 0.713 0.829 0.725 0.837
Table 1: Accuracy and F-scores for disambiguation per-
formance of our system. Recall was 99% in every case.
?Included?: inclusion of the gold parse in the reduced set
of parses or not. ?First Rank?: ranking of the preferred
parse as top in the reduced list. ?MRR?: mean reciprocal
rank of the gold parse in the list.
99%, and an F-Score of 89.7% and 88.7% for En-
glish and Japanese, respectively.
The reduced list of parser analyses can be further
ranked by the parse ranking model which is included
in the parsers of the respective languages (the same
models with which we determined the top 11 analy-
ses). Given this ranking, we can evaluate how often
the preferred parse is ranked top in our partially dis-
ambiguated list; results are shown in the two bottom
lines of Table 1.
A ranked list of possible preferred parses whose
top rank corresponds with a high probability to the
gold parse should further speed up the manual tree-
banking process.
Performance in the context of the whole pipeline
The performance of parsers and MT system
strongly influences the end-to-end results of the pre-
sented system. In the results given above, this in-
fluence is ignored. We lose around 29% of our data
because no parse could be produced in one or both
languages, or no translation could be produced. and
a further 5% of the sentences did not have the gold
parse in the original set of analyses (before align-
ment): our system could not possibly select the cor-
rect parse in those cases.
5 Discussion
Our system builds on the output of two parsers and
a machine translation system. We reduce ambiguity
for all sentence pairs where a parse could be cre-
ated for both languages, and for which there was at
least a partial translation. For these sentences, the
cross-lingual alignment component achieves a recall
of above 99%, such that we do not lose any addi-
127
tional data. The parsers and the MT system include
a parse ranking system trained on human gold anno-
tations. We use these models in parsing and transla-
tion to select the top 11 analyses. Our system thus
depends on a range of existing technologies. How-
ever, these technologies are available for a range of
languages, and we use them for efficient extension
of linguistic resources.
The effectiveness of cross-lingual parse disam-
biguation on the basis of semantic alignment highly
depends on the languages of choice. Given that we
exploit the differences between languages, pairs of
less related languages should lead to better disam-
biguation performance. Furthermore, disambiguat-
ing with more than two languages should improve
performance. Some ambiguities may be shared be-
tween languages. 4
One weakness when considering the disam-
biguated sentences as training for a parse ranking
model is that the translation fails on similar kinds of
sentences, so there are some phenomena which we
get no examples of ? the automatically trained tree-
bank does not have a uniform coverage of phenom-
ena. Our models may not discriminate some phe-
nomena at all.
Our system provides large amounts of automati-
cally annotated data at the only cost of CPU time:
so far we have disambiguated 25,000 sentences: 10
times more than the existing hand annotated gold
data. Using the parser output for speeding up man-
ual treebanking is most effective if the gold parse is
reliably included in the reduced set of parses. In-
creasing precision by accepting more than only the
most overlapping parses may lead to more effective
manual treebanking.
The alignment method we propose does not make
any language-specific assumptions, nor is it limited
to align two languages only. The algorithm is very
flexible, and allows for straightforward exploration
of different numbers and combinations of languages.
6 Conclusion and Future Work
Translating a sentence into a different language
changes its surface form, but not its meaning. In
4For example the PP attachment ambiguity in John said that
he went on Tuesday where either the saying or the going could
have happened on Tuesday holds in both English and Japanese.
parallel corpora, one language can be viewed as a
semantic tag of the other language and vice versa,
which allows for disambiguation of phenomena
which are ambiguous in only one of the languages.
We use the above observations for cross-lingual
parse disambiguation. We experimented with the
language pair of English and Japanese, and were
able to accurately reduce ambiguity in parser anal-
yses simultaneously for both languages to 30% of
the starting ambiguity. The remaining parses can be
used as a pre-selection to speed up the manual tree-
banking process.
We started working on an extrinsic evaluation of
the presented system by training a discriminative
parse ranking model on the output of our alignment
process. Augmenting the Gold training data with
our data improves the model. Our next step will
be to evaluate the system as part of the treebanking
process, and optimize the parameters such as disam-
biguation precision vs. amount of disambiguation.
As no language-specific assumptions are hard
coded in our disambiguation system, it would be
very interesting to apply the system to different lan-
guage pairs as well as groups of more than two lan-
guages. Using a group of languages for disambigua-
tion will likely lead to increased and more accurate
disambiguation, as more constraints are imposed on
the data.
Probably the most important goal for future work
is improving the recall achieved in the complete dis-
ambiguation pipeline. Many sentence-pairs cannot
be disambiguated because either no parse can be
generated for one or both languages, or no (par-
tial) translation can be produced. Following the
idea of partial translations, partial parses may be a
valid backoff. For purposes of cross-lingual align-
ment, partial structures may contribute enough in-
formation for disambiguation. There has been work
regarding partial parsing in the HPSG community
(Zhang and Kordoni, 2008), which we would like to
explore. There is also current work on learning more
types and instances of transfer rules (Haugereid and
Bond, 2011).
Finally, we would like to investigate more align-
ment methods, such as dependency relation based
alignment which we started experimenting with, or
EDM-based metrics as presented in (Dridan and
Oepen, 2011).
128
Acknowledgments
This research was supported in part by the Erasmus
Mundus Action 2 program MULTI of the European
Union, grant agreement number 2009-5259-5 and
the the joint JSPS/NTU grant on Revealing Meaning
Using Multiple Languages. We would like to thank
Takayuki Kuribayashi and Dan Flickinger for their
help with the treebanking.
References
Emily M. Bender and Melanie Siegel. 2004. Im-
plementing the syntax of Japanese numeral clas-
sifiers. In Proceedings of the IJC-NLP-2004.
Francis Bond, Stephan Oepen, Eric Nichols, Dan
Flickinger, Erik Velldal, and Petter Haugereid.
2011. Deep open-source machine translation.
Machine Translation, 25(2):87?105.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP, 2008.
Wenliang Chen, Jun?ichi Kazama, Min Zhang,
Yoshimasa Tsuruoka, Yujie Zhang, Yiou Wang,
Kentaro Torisawa, and Haizhou Li. 2011. SMT
helps bitext dependency parsing. In Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP2011), pages 73?83. Edinburgh.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics ?
an introduction. Research on Language and Com-
putation, 3:281?332.
Rebecca Dridan and Stephan Oepen. 2011. Parser
evaluation using elementary dependency match-
ing. In Proceedings of IWPT.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28. (Special Issue on Effi-
cient Processing with HPSG).
Petter Haugereid and Francis Bond. 2011. Extract-
ing transfer rules for multiword expressions from
parallel corpora. In Proceedings of the Work-
shop on Multiword Expressions: from Parsing
and Generation to the Real World.
Carl Pollard and Ivan A. Sag. 1994. Head
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago.
Yasuhito Tanaka. 2001. Compilation of a multilin-
gual parallel corpus. In Proceedings of PACLING
2001.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?403.
Yi Zhang and Valia Kordoni. 2008. Robust parsing
with a large HPSG grammar. In Proceedings of
the Sixth International Conference on Language
Resources and Evaluation (LREC?08).
Ventsislav Zhechev and Andy Way. 2008. Auto-
matic generation of parallel treebanks. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008).
129
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1352?1362,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Linking and Extending an Open Multilingual Wordnet
Francis Bond
Linguistics and Multilingual Studies
Nanyang Technological University
bond@ieee.org
Ryan Foster
Great Achievement Press
ryan_foster@gapbks.com
Abstract
We create an open multilingual wordnet
with large wordnets for over 26 languages
and smaller ones for 57 languages. It is
made by combining wordnets with open li-
cences, data from Wiktionary and the Uni-
code Common Locale Data Repository.
Overall there are over 2 million senses
for over 100 thousand concepts, linking
over 1.4 million words in hundreds of lan-
guages.
1 Introduction
We wish to create a lexicon covering as many lan-
guages as possible, with as much useful informa-
tion as possible. Generally, language resources, to
be useful, must be both accessible (legal to use)
and usable (of sufficient quality, size and with a
documented interface) (Ishida, 2006). We address
both of these concerns in this paper.
One of the many attractions of the semantic net-
work WordNet (Fellbaum, 1998), is that there are
numerous wordnets being built for different lan-
guages. There are, in addition, many projects
for groups of languages: Euro WordNet (Vossen,
1998), BalkaNet (Tufis? et al, 2004), Asian Word-
net (Charoenporn et al, 2008) and more. Al-
though there are over 60 languages for which
wordnets exist in some state of development (Fell-
baum and Vossen, 2012, 316), less than half of
these have released any data, and for those that
have, the data is often not freely accessible (Bond
and Paik, 2012). For those wordnets that are avail-
able, they are of widely varying size and quality,
both in terms of accuracy and richness. Further,
there is very little standardization in terms of for-
mat, what information is included, or license.
The goal of the research outlined in this paper
is to make it possible for a researcher interested in
working on the lexical semantics of a language or
languages to be able to access wordnets for those
languages with a minimum of legal and technical
barriers. In practice this means making it possible
to access multiple wordnets with a common inter-
face. We also use sources of semi-structured data
that have minimal legal restrictions to automati-
cally extend existing freely available wordnets and
to create additional wordnets which can be added
to our open wordnet grid.
Previous studies have leveraged multiple word-
nets and Wiktionary (Wikimedia, 2013) to extend
existing wordnets or create new ones (de Melo and
Weikum, 2009; Hanoka and Sagot, 2012). These
studies passed over the valuable sense groupings
of translations within Wiktionary and merely used
Wiktionary as a source of translations that were
not disambiguated according to sense. The present
study built and extended wordnets by directly link-
ing Wiktionary senses to WordNet senses.
Meyer and Gurevych (2011) demonstrated the
ability to automatically identify many matching
senses in Wiktionary and WordNet based on the
similarity of monolingual features. Our study
combines monolingual features with the disam-
biguating power of multiple languages. In ad-
dition to differences in linking methodology, our
project gives special attention to ensuring the max-
imum re-usability and accessibility of the data and
software released.
Other large scale multilingual lexicons have
been made by linking wordnet to Wikipedia
(Wikipedia, 2013; de Melo and Weikum, 2010;
Navigli and Ponzetto, 2012). Our approach is
complementary to these: in general Wikipedia has
more entities than classes, while Wiktionary has
more classes.
In Section 2 we discuss linking freely available
wordnets to form a single multilingual semantic
network. In Section 3 we extend the wordnets with
data from two sources. We show the results in
Section 4 and then discuss them and outline future
1352
work in Section 5.
2 Linking Multiple Wordnets
In order to make the data from existing wordnet
projects more accessible, we have built a simple
database with information from those wordnets
with licenses that allow redistribution of the data.
These wordnets, their licenses and recent activity
are summarized in Table 1 (sizes for most of them
are shown in Table 2).1
Wordnet Project Lng Licence Type
Albaneto als CC BY a
Arabic WordNet arb CC BY-SA s
DanNet dan wordnet a
Princeton WordNetu eng wordnet a
Persian Wordnet fas free to use u
FinnWordNetu fin CC BY a
WOLFu fra CeCILL-C s
Hebrew Wordneto heb wordnet s
MultiWordNeto ita CC BY a
Japanese Wordnetu jpn wordnet a
Multilingual cat CC BY a
Central eus CC BY-NC-SA n
Repositoryo,u glg CC BY a
spa CC BY a
Wordnet Bahasau ind MIT a
zsm MIT a
Norwegian Wordneto nno wordnet a
nob wordnet a
plWordNeto,u pol wordnet a
OpenWN-PTu por CC BY-SA s
Thai Wordnet tha wordnet a
o Re-released under an open license in 2012
u Updated in 2012
Type: u Unrestricted; a Attribution; s Share-alike;
n Non-commercial
URL: http://casta-net.jp/~kuribayashi/multi/
Table 1: Linked Open Wordnets
The first wordnet developed is the Princeton
WordNet (PWN: Fellbaum, 1998). It is a large
lexical database of English. Open class words
(nouns, verbs, adjectives and adverbs) are grouped
into concepts represented by sets of synonyms
(synsets). Synsets are linked by semantic relations
such as hyponomy and meronomy. PWN is re-
leased under an open license (allowing one to use,
copy, modify and distribute it so long as you prop-
erly acknowledge the copyright).
The majority of freely available wordnets take
the basic structure of the PWN and add new lem-
mas (words) to the existing synsets: the extend
model (Vossen, 2005). For example, dogn:1 is
linked to the lemmas chien in French, anjing in
Malay, and so on. It is widely realized that this
1We have now added Mandarin Chinese.
model is imperfect as different languages lexical-
ize different concepts and link them in different
ways (Fellbaum and Vossen, 2012). Nevertheless,
many projects have found that the overall structure
of PWN serves as a useful scaffold. The fact that,
for example, a dogn:1 is an animaln:1 is language
independent.
In theory, such wordnets can easily be com-
bined into a single resource by using the PWN
synsets as pivots. All languages are linked through
the English wordnet. Because they are linked at
the synset level, the problem of ambiguity one gets
when linking bilingual dictionaries through a com-
mon language is resolved: we are linking senses to
senses.
In practice, linking a new language?s wordnet
into the grid could be problematic for three rea-
sons. The first problem was that the wordnets were
linked to various versions of the Princeton Word-
Net. In order to combine them into a single multi-
lingual structure, we had to map to a common ver-
sion. The second problem was the incredible va-
riety of formats that the wordnets are distributed
in. Almost every project uses a different format.
Even different versions of the same project often
had slightly different formats. The final problem
was legal: not all wordnets have been released un-
der licenses that allow reuse.
The first problem can largely be overcome us-
ing the mapping scripts from Daude et al (2003).
Mapping introduces some distortions, in particu-
lar, when a synset is split, we chose to only map
the translations to the most probable mapping, so
some new synsets will have no translations.
The second problem we are currently solving
through brute force, writing a new script for ev-
ery new project we add. We make these scripts,
along with the reformatted wordnets, freely avail-
able for download. Any problems or bugs found
when converting the wordnets have been reported
back to the original projects, with many of them
fixed in newer releases. We consider this feedback
to be an important part of our work: it means that
other researchers and users do not have to suffer
from the same problems and it encourages projects
to release updates.
The third, legal, problem is being solved by
an ongoing campaign to encourage projects to
(re-)release their data under open licenses. Since
Bond and Paik (2012) surveyed wordnet licenses
in 2011, six projects have newly released data un-
1353
der open licenses and eight projects have updated
their data.
Our combined wordnet includes English (Fell-
baum, 1998); Albanian (Ruci, 2008); Arabic
(Black et al, 2006); Chinese (Huang et al, 2010);
Danish (Pedersen et al, 2009); Finnish (Linde?n
and Carlson., 2010); French (Sagot and Fis?er,
2008); Hebrew (Ordan and Wintner, 2007); In-
donesian and Malaysian (Nurril Hirfana et al,
2011); Italian (Pianta et al, 2002); Japanese
(Isahara et al, 2008); Norwegian (Bokma?l and
Nynorsk: Lars Nygaard 2012, p.c.); Persian (Mon-
tazery and Faili, 2010); Portuguese (de Paiva
and Rademaker, 2012); Polish (Piasecki et al,
2009); Thai (Thoongsup et al, 2009) and Basque,
Catalan, Galician and Spanish from the Multilin-
gual Common Repository (Gonzalez-Agirre et al,
2012).
On our server, the wordnets are all in a shared
sqlite database using the schema produced by
the Japanese WordNet project (Isahara et al,
2008). The database is based on the logical struc-
ture of the Princeton WordNet, with an additional
language attribute for lemmas, examples, defini-
tions and senses. It is a single open multilingual
resource. When we redistribute the data, each
project?s data is made available separately, with a
common format, but separate licenses.
The Scandinavian and Polish wordnets are
based on the merge approach, where indepen-
dent language specific structures are built and then
some synsets linked to PWN. Typically only a
small subset will be linked (due more to resource
limitations than semantic incompatibility).
2.1 Core Concepts
Boyd-Graber et al (2006) created a list of 5,000
core word senses in Princeton WordNet which
represent approximately the 5,000 most frequently
used word senses.2 We use this list to evaluate the
coverage of the wordnets: do they contain words
for the most common concepts? As a very rough
measure of useful coverage, we report the percent-
age of synsets covered from this core list. Because
the list is based on English data, it is of course not
a perfect measure for other languages and cultures.
Note that some wordnet projects have deliberately
targeted the core concepts, which of course boosts
their coverage scores.
2The original list is here from http://wordnetcode.
princeton.edu/standoff-files/core-wordnet.txt;
we converted it to wn30 synsets.
2.2 License Types
The licenses fall into four broad categories: (u)
completely unrestricted, (a) attribution required,
(s) share alike, and (n) non-commercial. The first
category includes any work that is in the public
domain or that the author has released without any
restrictions. The second category allows anyone
to use, adapt, improve, and redistribute the work as
long as one attributes the work in the manner spec-
ified by the copyright holder (without suggesting
an endorsement). The WordNet, MIT, and CC BY
licenses are all in this category. The third category
allows anyone to adapt and improve the licensed
work and redistribute it, but the redistributed work
must be released under the same license. The CC
BY-SA, GPL, GFDL, and CeCILL-C licenses are
of this type. Because derivative works can only
be redistributed under the same license, works li-
censed under any two of these licenses cannot
be combined with each other and legally redis-
tributed. In general, a work formed from the com-
bination of works in category (u) and (a) with a
work in category (s) will be subject to the more re-
strictive terms of the the share alike license. How-
ever, the GPL, GFDL and CeCILL-C are incom-
patible with CC BY.3 The fourth type of license
further forbids the commercial use of a work. The
CC BY-NC and the CC BY-NC-SA licenses are in
this category, they are also incompatible with li-
censes in category (s).
Releasing a work under the more restrictive li-
censes in categories (s) and (n) above substantially
limit and complicate the ability to extend and com-
bine a work into other useful forms. By maintain-
ing a separation of databases released under in-
compatible licenses, we avoid any possible legal
problems. Due to license incompatibilities, it is
impossible to release a single database with all the
wordnets, even though individually they are redis-
tributable. We can currently combine those with
licenses in groups (u) and (a) and the CC BY-
SA wordnets (now everything except French and
Basque).
3 Extending with non-wordnet data
We looked at two sources for automatically adding
new entries. The Unicode Common Locale Data
Repository (CLDR) has reliable information on
languages, territories and dates. Wiktionary is a
3http://www.gnu.org/licenses/license-list.
html\#ccby
1354
general purpose lexicon with much more informa-
tion for many words.
3.1 Unicode Common Locale Data
Repository (CLDR)
We added information on languages, territories
and dates from the Unicode Common Locale Data
Repository (CLDR).4 This is a collection of data
maintained by the Unicode Consortium to support
software internationalization and localization with
locale information on formatting dates, numbers,
currencies, times, and time zones, as well as help
for choosing languages and countries by name. It
has this data for over 194 languages. It is released
under an open license that allows redistribution
with proper attribution (Unicode, Inc., 2012).5
We found data for 122 languages. Most had
around 550 senses (synsets and their lemmas): for
example, for Portuguese: Englishn:1 ingle?s. Some
had only 40 or 50, such as Assamese, which only
has the week days, month names and a few lan-
guage names. The linked data was small enough
to check by hand. When the original CLDR data
is correct the data we generate should be correct.
The idea of using such data is not new. Quah
et al (2001) for example, use Linux locale data
to extend a proprietary English-Malay lexicon.
de Melo and Weikum (2009) also use this data
(and data from a variety of other sources) to build
an enhanced wordnet, in addition adding new
synsets for concepts that are not in not wordnet.
However, when they released the data as LEXVO
(data about languages: CC BY-SA) and UWN (the
universal multilingual wordnet: CC BY-NC-SA),
they added additional license restrictions which
complicate the reuse of the data and make it im-
possible to integrate the data back into the original
wordnet project.
3.2 Wiktionary
Searches for a publicly-available source of Wik-
tionary in a preprocessed, machine-readable for-
mat did not turn up any sources that were recent
and publicly-available.6 Although there are sev-
4http://cldr.unicode.org/
5With the extra requirement that ?there is clear notice in
each modified Data File or in the Software as well as in the
documentation associated with the Data File(s) or Software
that the data or software has been modified.?
6We later learned that McCrae et al (2012) made a release
of Wiktionary in the lemon format (http://datahub.io/
en/dataset/dbnary). They did not, however, release the
code they used to parse Wiktionary.
eral freely-available software programs that are
capable of parsing portions of the English Wik-
tionary, none of the programs that were evaluated
appeared to extract the precise set of information
desired for our task in an easy-to-use format. So
the authors decided to build a custom parser capa-
ble of extracting the information needed for build-
ing open wordnets.
3.2.1 Wiktionary Parser
Since each language edition of Wiktionary is for-
matted in a somewhat unique way, parsers must
be tailored to recognize the structure and format-
ting of each edition on a case-by-case basis. The
authors created a parser tailored to the English
Wiktionary, although it can be extended to handle
other language versions as well. We are releasing
this code under the MIT license.7
The current version of the parser is capable of
extracting headwords, parts of speech, definitions,
synonyms and translations from the XML Wik-
tionary database dumps provided by the Wikime-
dia Foundation.8 Within these large XML files,
the main body of Wiktionary articles are stored in
a Wikitext format, which is a semi-structured for-
mat. Although anyone can edit a Wiktionary page
and use any style of formatting they desire, the
community of users encourages adhering to estab-
lished guidelines, which produces a format that is
generally predictable.
Within the English Wiktionary, synonyms and
translations are both grouped into sense groups
that correspond with definitions in the main sec-
tion. These sense groups are marked by a short
text gloss (short gloss), which is usually an abbre-
viated version of one of the full definitions (full
definition). The parser makes no attempt to match
these short glosses with the full definitions. Data
is simply extracted, cleaned, and then stored in a
relational database or flat file.
Translations proved to be easy to extract due
to the fairly consistent use of a specifically for-
matted translation template. These templates in-
clude a language code derived from ISO standards,
the translation, and optional additional informa-
tion such as gender, transliteration, script, and al-
ternate forms. The parser extracts and retains all
of this potentially valuable information.
Examples of translation templates:
7Available from the Open Multilingual Wordnet Page:
http://casta-net.jp/~kuribayashi/multi/.
8http://dumps.wikimedia.org/
1355
? Finnish: {{t+|fi|sanakirja}}
? French: {{t+|fr|dictionnaire|m}}
To enable later processing, it is necessary to tie
synonyms and translations to their corresponding
short gloss via a unique key. Most parsers simply
use an automatically generated surrogate key or a
key based on the ordered position of data within
a Wiktionary article. Since Wiktionary is con-
stantly changing, the side effect of this approach
is that data extracted from a specific snapshot of
the Wiktionary database can only be meaningfully
used in connection with other data extracted by
the same parser from the exact same snapshot. To
overcome this, we use a unique key that can be
recreated from the data itself, which we call the
defkey. To generate this key, we concatenate the
language code, headword, part of speech, and the
short gloss and use the sha1 hash function (NIST,
2012) to create a unique 40-character hexadecimal
string from the resulting text.
These defkeys are time and technology indepen-
dent, so they allow the ability for researchers to
efficiently share and compare results. Once a link
is established between this defkey and a particu-
lar synset, translations added to Wiktionary at a
later data can be automatically integrated into our
multilingual wordnet. Conversely, if a Wiktionary
contributor changes a short gloss, historical data
connected to the old defkey is preserved while new
data imported at a later time will not be incorrectly
linked to an older definition.
Another feature of our parser is a feedback
mode, which generates a report about poorly for-
mated data that was encountered. These automat-
ically generated reports can be used to create a
quality-enhancing feedback loop with Wiktionary.
3.2.2 Linking Senses
Meyer and Gurevych (2011) showed that auto-
matic alignments between Wiktionary senses and
PWN can be established with reasonable accuracy
and recall by combining multiple text similarity
scores to compare a bag of words based on several
pieces of information linked to a WordNet sense
with another bag of words obtained from a Wik-
tionary entry. In our study we evaluated the poten-
tial for aligning senses based on common transla-
tions in combination with monolingual similarity
features.
In this study we used 20 of the wordnets de-
scribed in Section 2,9 and the Wiktionary data ob-
tained using the parser described in Section 3.2.1.
Before searching for translation matches, we nor-
malized the data to ensure the most accurate pos-
sible overlap count. First, article headwords were
included as English translations of Wiktionary
senses (along with synonyms). Then differences
in language codes were rectified and translations
containing symbolic characters or a mixture of ro-
man and non-roman characters were marked to be
ignored, save a few exceptions. This left approx-
imately 1.4 million sense translations in 20 lan-
guages in our wordnet grid, and nearly 1.3 million
Wiktionary translations in over 1,000 languages.
We then created a list of all possible align-
ments where at least one translation of a wordnet
sense matched a translation of a Wiktionary sense.
This represented a small percentage of the possi-
ble alignments, because definitions in Wiktionary
that do not contain any translations were ignored
in our study. Of more than 500,000 English defini-
tions in Wiktionary, only about 130,000 presently
have associated translations. The resulting graph
contained over 700,000 possible sense alignments.
We calculated a number of similarity scores, the
first two based on similarity in the number of lem-
mas, calculated using the Jaccard index:
sime(sn,sk) =
|E(sk)?E(sn)|
|E(sk)?E(sn)|
(1)
sima(sn,sk) =
|L(sk)?L(sn)|
|L(sk)?L(sn)|
(2)
Where sk,sn are concepts in Wiktionary and word-
net respectively,10 E(s) is the set of English lem-
mas for sense s and L(s) is the set of lemmas in all
languages.
As an initial pruning, we kept only matches
where either: sima ? 0.7 or (sime ? 0.5 and
sima ? 0.5) or, if (|L(sk) ? L(sn)| > 5) then
(sime ? 0.5 and sima ? 0.45). After apply-
ing these filters, approximately 220,000 alignment
candidates remained.
We reviewed a random sample of 551 alignment
candidates. Of these 136 were deemed correctly
aligned. Another 48 we considered possibly close
enough to produce valid translations for wordnet.
All others were marked as incorrect alignments.
9We didn?t use Chinese or Polish, as the wordnets were
added after we had started the evaluation.
10Precisely, synsets in wordnet and senses in Wiktionary.
1356
This development dataset was used to tune re-
fined similarity scores.
simt(sn,sk) =
|L(sk)?L(sn)|?
? |L(sk)?L(sn)|
(3)
simd(sn,sk) = BoW(wndef)?BoW(wkdef)?BoW(wndef)??BoW(wkdef)? (4)
simc(sn,sk) = simt+? simc (5)
simt gives higher weight to concepts that link
through more lemmas, not just a higher proportion
of lemmas.
simd measures the similarity of the definitions
in the two resources, using a cosine similarity
score. We initially used the WordNet gloss and
example sentence(s) for wndef and the short gloss
from Wiktionary for wkdef. This improved the ac-
curacy of the combined ranking score (simc), but
since many of the short glosses are only one or
two words, the sparse input often produced a simd
score of zero even when the candidate alignment
was correct. To improve the accuracy of the simd
component, we also added in the long definitions.
Short glosses were aligned with long definitions
using a similar approach to McCrae et al (2012).
First we search for a match where the short gloss
was a substring of the full definition. If that failed
to produce a single possible alignment, we aligned
the short gloss with the full definition that pro-
duced the greatest cosine similarity score. Finally,
where the short definition was blank and only a
long definition was present, we aligned the two.
The results of this alignment were less than 90%
accurate, so to offset the effects of this noise we in-
cluded both the full definition and the short gloss
in wkdef. For wndef we used the WordNet gloss,
example sentence(s), and synonyms. Even though
the linking of definitions within Wiktionary left
much to be desired, the increased amount of text
improved the accuracy of the definition based sim-
ilarity component of our ranking score.
Our combined ranking score (simc), based on
both overlapping translations and a monolingual
lexical similarity score, was able to outperform
ranking based on either component in isolation.
We expect that an improved alignment of short
glosses to full definitions together with more ac-
curate measures of lexical similarity such as de-
scribed by Meyer and Gurevych (2011) would fur-
ther improve the accuracy of a combined ranking
score. We employed our combined ranking score
first as a filter, where simc ? ?c. The ranking score
is then used to select the best match among com-
peting alignments. Alignments are based on the
belief that a definition within Wiktionary should
only map to a single WordNet synset (if any at
all). In theory, each WordNet synset should rep-
resent a meaning distinguishable from all other
synsets. Because Wiktionary is organized accord-
ing to lemma first, and sense second, multiple def-
initions in separate articles often map to the same
synset. For example mortal ?A human; someone
susceptible to death?, individual ?A person con-
sidered alone . . . ?, and person ?A single human
being; an individual? all align with someonen:1
(00007846-n). However, two distinct definitions
within the same Wiktionary entry should not map
to the same WordNet sense. When there are mul-
tiple possible alignments where only one can be
valid, simc is used to determine the best match.
In addition to using the combined ranking score
as a filter, we found that we could obtain a small
additional increase in accuracy without reducing
recall by also requiring simt ? ?t or simd ? ?d .
To determine ideal values for the weights and
thresholds, we performed several grid searches.
The parameters are interdependent and can pro-
duce reasonable results at a variety of points. Ideal
values also depend on whether we wish to maxi-
mize accuracy or recall. ? is set at 3.2 in order to
achieve an ideal target threshold of ?t = 1. We fi-
nally chose values of ? = 0.7 and ?c = 0.71 which
gave a reasonable balance between accuracy and
recall.
4 Results and Evaluation
We give the data for the 26 wordnets with more
than 10,000 synsets in Table 2. There are a further
57 with more than 1,000; 133 with more than 100,
200 with more than 10 and 645 with more than 1
(although most of the very small languages appear
to be simple errors in the language code entered
into Wiktionary). Individual totals are shown for
synsets and senses from the original wordnets, the
data extracted from Wiktionary, and the merged
data of the wordnets, Wiktionary and CLDR. We
do not show the CLDR data in the table as it is
so small, generally 500-600 synsets for the top
languages. Overall there are 2,040,805 senses for
117,659 concepts, using over 1,400,000 words in
over 1,000 languages.
The smaller wordnets are not of much practi-
cal use, but can still serve as the core of new
1357
Projects Wiktionary Merged (+CLDR)
ISO Language Synsets Senses Core Synsets Senses Core Synsets Senses Core
eng English 117,659 206,978 100 35,400 49,951 75 117,661 213,538 100
fin Finnish 116,763 189,227 100 21,516 31,154 65 116,830 199,435 100
tha Thai 73,350 95,517 81 2,560 3,193 17 73,595 97,390 81
fra French 59,091 102,671 92 20,449 27,150 63 61,258 109,643 95
jpn Japanese 57,179 158,064 95 12,685 19,479 52 59,112 166,617 96
ind Indonesian 52,006 142,488 99 2,390 2,810 17 52,154 143,755 99
cat Catalan 45,826 70,622 81 8,626 10,251 36 48,007 74,806 84
spa Spanish 38,512 57,764 76 18,281 25,310 60 47,737 74,848 86
por Portuguese 41,810 68,285 79 12,331 16,178 53 43,870 74,151 84
zsm Standard Malay 42,766 119,152 99 2,833 3,744 19 43,079 120,686 99
ita Italian 34,728 60,561 83 14,605 18,710 53 38,938 68,827 87
eus Basque 29,413 48,934 71 1,693 1,943 11 29,965 49,945 72
pol Polish 14,008 21,001 30 10,888 13,431 46 20,975 30,943 55
glg Galician 19,312 27,138 36 2,492 2,871 15 20,772 29,136 42
fas Persian 17,759 30,461 41 4,229 5,443 26 20,766 35,318 55
rus Russian 0 0 0 19,983 33,716 64 20,138 34,009 64
deu German 0 0 0 19,675 29,616 64 19,857 29,884 64
cmn Mandarin Chinese 4,913 8,069 28 12,130 19,079 49 15,490 27,113 60
arb Standard Arabic 10,165 21,751 48 6,892 9,337 38 14,861 31,337 63
nld Dutch 0 0 0 13,741 19,709 56 13,950 20,003 56
ces Czech 0 0 0 12,802 15,493 54 13,030 15,813 54
swe Swedish 0 0 0 12,000 16,226 51 12,221 16,512 51
ell Modern Greek 0 0 0 10,308 13,071 44 10,549 13,472 44
dan Danish 4,476 5,859 81 7,290 8,931 35 10,328 13,551 85
nob Norwegian Bokma?l 4,455 5,586 79 7,262 9,170 35 10,322 13,612 83
hun Hungarian 0 0 0 9,964 12,699 45 10,213 13,029 45
Core shows the percentage coverage of the 5,000 core concepts.
Table 2: Merged Wordnets (with more than 10,000 entries)
projects. The bigger wordnets show the data from
Wiktionary (and to a lesser extent CLDR) having
only a small increase in the number of senses. The
biggest change is for the medium size projects,
such as Persian or Arabic, which end up with
much better coverage of the most frequent core
concepts. Major languages such as German or
Russian, which currently do not have open word-
nets get good coverage as well.
The size of the mapping table is the same as the
number of English senses linked (49,951 senses).
We evaluated a random sample of 160 alignments
and found the accuracy to be 90% (Wiktionary
sense maps to the best possible wordnet sense).
We then evaluated samples of the wordnet cre-
ated from Wiktionary for several languages. For
each language we choose 100 random senses, then
checked them against existing wordnets.11 For all
unmatched entries, we then had them checked by
native speakers. The results are given in Table 3.
The sense accuracy is higher than the mapping ac-
curacy: in general, entries with more translations
are linked more accurately, thus raising the av-
erage precision. During the extraction and eval-
11For Chinese we use the wordnet from Xu et al (2008),
which is free for research but cannot be redistributed. For
German we used Euro WordNet (Vossen, 1998).
Language % Matched % Good
Chinese? 46 97
Serbo-Croation?,?? 0 91
Czech? 0 99
English 89 92
German? 19 85
Indonesian 69 97
Korean? 0 96
Japanese 56 90
Russian? 0 99
Average 94.0
Table 3: Precision of Wiktionary-based Wordnets
? Not used to build the mapping from wordnet to Wiktionary.
?? We allow terms used in either Serbian or Croatian.
uation, we noticed several language specific fea-
tures: for example, Serbo-Croatian had a mixture
of Cyrillic and Latin entries. For languages where
one script was clearly dominant, we kept only that,
but really these decisions should be done for each
language by a native speaker.
We make the data available in two ways. The
first is a set of downloads. Each language has up
to three files: the data from the wordnet project
(if it exists), the data from the CLDR and the data
from Wiktionary. They are kept separate in order
1358
to keep the licenses as free as possible. The sec-
ond is as two on-line searches: one using only the
data from the projects, and one with all the data
combined. The combination is done by simple
union.12 We maintain this separation as we can-
not guarantee the quality of the automatically ex-
tracted data. Because the raw data is there it is pos-
sible to combine them in other ways. The simple
structure is easy to manipulate, and there is code
to use this style of data with the popular tool kit
NLTK (Bird et al, 2010).
5 Discussion and Future Work
We have created a large open wordnet of high
quality (85%?99% measured on senses). Twenty
six languages have more than 10,000 concepts
covered, with 42?100% coverage of the most com-
mon core concepts. The data is easily download-
able with minimal restrictions. The overall ac-
curacy is estimated at over 94%, as most of the
original wordnets are hand verified (and so should
be 100% accurate). The high accuracy is largely
thanks to the disambiguating power of the multi-
ple translations, made possible by the many open
wordnets we have access to.
Because we link senses between wordnet and
Wiktionary and then use the translations of the
sense, manually validating this mapping will im-
prove the entries in multiple languages simultane-
ously. As the Wiktionary-wordnet algnment map-
ping is linked to persistent keys it will remain use-
ful even as the resources change. Further, it can be
used to identify and add missing senses to word-
net: unmapped Wiktionary entries are candidates
for new concepts.
The Universal Wordnet (UWN: de Melo and
Weikum, 2009) brings in data from even more re-
sources, and combines them to make a larger re-
source, choosing parameters with slightly lower
precision (just under 90%). It is further linked to
Wikipedia, adding many named entities. We ex-
pect that our work is complementary. Because
we use a different approach, it would be possi-
ble to merge the two if the licenses allowed us
to. However, since the CC BY-SA and CC-BY-
NC-SA licenses are mutually exclusive, the two
works cannot be combined and rereleased unless
relevant parties can relicense the works. There is
no easy way to improve UWN beyond checking
each and every entry, which is expensive. An ad-
12http://casta-net.jp/~kuribayashi/multi/
vantage of our approach, noted above, is that we
can validate the sense matches for English and the
accuracy percolates down to all the languages.
Integrating data from the most recent version
of Wiktionary can be done simply and takes a
few hours. It is therefore feasible to update the
downloadable data regularly. Improvements in ei-
ther the wordnet projects or Wiktionary (or both)
can also result in improved mappings. We further
hope to take advantage of ongoing initiatives in the
global wordnet grid to add new concepts not in the
Princeton WordNet, so that we can expand beyond
an English-centered world view.
By making the data from multiple sources eas-
ily available with minimal restrictions, we hope
that it will be easier to do research that exploits
lexical semantics. In particular, we make the data
easily accessible to the original wordnet projects,
some of whom have already started to merge it
into their own resources. We cannot check the
accuracy of data in all languages, nor, for exam-
ple, check that synsets have the most appropriate
lemmas associated with them. Many languages
have their own orthographic issues (for example
a choice of scripts, or the choice to include vow-
els or not). Our automatic extraction does not deal
with these issues at all. This kind of language spe-
cific quality control is best done by the individual
wordnet projects.
We also consider it important to keep feeding
data back to the individual wordnet projects, as
much of the innovative research comes from them:
the class/instance distinction from PWN; the dis-
tinction between rigid and non-rigid synsets from
the Kyoto Project; domain mappings from the
MultiWordNet (Pianta et al, 2002); representing
orthographic variation from the Japanese Wordnet
(Kuroda et al, 2011); combining close languages
from the Wordnet Bahasa (Nurril Hirfana et al,
2011); and so on. For all of these reasons, we
do not consider automatic extraction from/linking
to Wiktionary a substitute for building languages
specific wordnets.
Further work that this data should allow us to
do include: automatically producing a list of bad
data found in Wiktionary that can be used by Wik-
tionary editors to correct errors; and finding gaps
in wordnet by identifying senses in Wiktionary
that have a large number of translations, but fail to
have any significant alignment with existing word-
net synsets.
1359
We currently only link through the English Wik-
tionary and its translations. It should be possible to
expand the multilingual wordnet in the same way
using Wiktionaries in other languages, which we
would expect to improve coverage.
Finally, Wiktionary contains a lot of useful in-
formation we are not currently using (information
on gender, transliterations, pronunciations, alter-
native spellings and so forth). We can also think
of the aligned definitions as a paraphrase corpus
for English.
We have devoted more space than is usual for
a computational linguistics paper to issues of li-
censing and sustainability. This is deliberate: we
feel papers about lexical resources should be clear
about licensing, and that it should be considered
early on when creating new resources. There are
strong arguments that open data leads to better sci-
ence (Pederson, 2008), and it has been shown that
open resources are cited more (Bond and Paik,
2012). In addition, how to maintain resources over
time is a major unsolved problem. We consider it
important that our wordnet is not just large and ac-
curate but also maintainable and as accessible as
possible.
6 Conclusions
We have created an open multilingual wordnet
with over 26 languages. It is made by combining
wordnets with open licences, data from the Uni-
code Common Locale Data Repository and Wik-
tionary. Overall there are over 2 million senses for
117,659 concepts, using over 1.4 million words in
hundreds of languages.
Acknowledgments
We would like to thank the following for their
help with the evaluation: Le Tuan Anh, Frantis?ek
Kratochv??l, Kyonghee Paik, Zina Pozen, Melanie
Siegel, Stefanie Stadler, Bilyana Shuman, Liling
Tan and Muhammad Zulhelmy bin Mohd Rosman.
References
Stephen Bird, Ewan Klein, and Edward Loper.
2010. Nyumon Shizen Gengo Shori [In-
troduction to Natural Language Processing].
O?Reilly. (translated by Hagiwara, Nakamura
and Mizuno).
W. Black, S. Elkateb, H. Rodriguez, M. Alkhalifa,
P. Vossen, A. Pease, M. Bertran, and C. Fell-
baum. 2006. The Arabic wordnet project. In
Proceedings of LREC 2006.
Francis Bond and Kyonghee Paik. 2012. A survey
of wordnets and their licenses. In Proceedings
of the 6th Global WordNet Conference (GWC
2012). Matsue. 64?71.
Jordan Boyd-Graber, Christiane Fellbaum, Daniel
Osherson, and Robert Schapire. 2006. Adding
dense, weighted connections to WordNet. In
Proceedings of the Third Global WordNet Meet-
ing. Jeju.
Thatsanee Charoenporn, Virach Sornlerlam-
vanich, Chumpol Mokarat, and Hitoshi Isahara.
2008. Semi-automatic compilation of Asian
WordNet. In 14th Annual Meeting of the
Association for Natural Language Processing,
pages 1041?1044. Tokyo.
Jordi Daude, Lluis Padro, and German Rigau.
2003. Validation and tuning of Wordnet map-
ping techniques. In Proceedings of the In-
ternational Conference on Recent Advances
in Natural Language Processing (RANLP?03).
Borovets, Bulgaria.
Gerard de Melo and Gerhard Weikum. 2009. To-
wards a universal wordnet by learning from
combined evidence. In Proceedings of the 18th
ACM Conference on Information and Knowl-
edge Management (CIKM 2009), pages 513?
522. ACM, New York, NY, USA.
Gerard de Melo and Gerhard Weikum. 2010. To-
wards universal multilingual knowledge bases.
In Pushpak Bhattacharyya, Christiane Fell-
baum, and Piek Vossen, editors, Principles,
Construction, and Applications of Multilingual
Wordnets. Proceedings of the 5th Global Word-
Net Conference (GWC 2010), pages 149?156.
Narosa Publishing, New Delhi, India.
Valeria de Paiva and Alexandre Rademaker. 2012.
Revisiting a Brazilian wordnet. In Proceedings
of the 6th Global WordNet Conference (GWC
2012). Matsue.
Christiane Fellbaum and Piek Vossen. 2012. Chal-
lenges for a multilingual wordnet. Lan-
guage Resources and Evaluation, 46(2):313?
326. Doi=10.1007/s10579-012-9186-z.
Christine Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
Aitor Gonzalez-Agirre, Egoitz Laparra, and Ger-
man Rigau. 2012. Multilingual central repos-
1360
itory version 3.0: upgrading a very large lexi-
cal knowledge base. In Proceedings of the 6th
Global WordNet Conference (GWC 2012). Mat-
sue.
Vale?rie Hanoka and Beno??t Sagot. 2012. Wordnet
creation and extension made simple: A multi-
lingual lexicon-based approach using wiki re-
sources. In Proceedings of LREC 2012. Istan-
bul.
Chu-Ren Huang, Shu-Kai Hsieh, Jia-Fei Hong,
Yun-Zhu Chen, I-Li Su, Yong-Xiang Chen, and
Sheng-Wei Huang. 2010. Chinese wordnet:
Design and implementation of a cross-lingual
knowledge processing infrastructure. Journal of
Chinese Information Processing, 24(2):14?23.
(in Chinese).
Hitoshi Isahara, Francis Bond, Kiyotaka Uchi-
moto, Masao Utiyama, and Kyoko Kanzaki.
2008. Development of the Japanese WordNet.
In Sixth International conference on Language
Resources and Evaluation (LREC 2008). Mar-
rakech.
Toru Ishida. 2006. Language grid: An
infrastructure for intercultural collaboration.
In IEEE/IPSJ Symposium on Applications
and the Internet (SAINT-06), pages 96?100.
URL http://langrid.nict.go.jp/file/
langrid20060211.pdf, (keynote address).
Kow Kuroda, Takayuki Kuribayashi, Francis
Bond, Kyoko Kanzaki, and Hitoshi Isahara.
2011. Orthographic variants and multilingual
sense tagging with the Japanese WordNet. In
17th Annual Meeting of the Association for Nat-
ural Language Processing, pages A4?1. Toy-
ohashi.
Krister Linde?n and Lauri Carlson. 2010.
Finnwordnet ? wordnet pa?finska via
o?versa?ttning. LexicoNordica ? Nordic
Journal of Lexicography, 17:119?140. In
Swedish with an English abstract.
John McCrae, Philipp Cimiano, and Elena
Montiel-Ponsoda. 2012. Integrating word-
net and wiktionary with lemon. In Christian
Chiarcos, Sebastian Nordhoff, and Sebastian
Hellman, editors, Linked Data in Linguistics.
Springer.
Christian M. Meyer and Iryna Gurevych. 2011.
What psycholinguists know about chemistry:
Aligning wiktionary and wordnet for increased
domain coverage. In Proceedings of the 5th In-
ternational Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 883?892.
Nurril Hirfana Mohamed Noor, Suerya Sapuan,
and Francis Bond. 2011. Creating the open
Wordnet Bahasa. In Proceedings of the 25th Pa-
cific Asia Conference on Language, Information
and Computation (PACLIC 25), pages 258?267.
Singapore.
Mortaza Montazery and Heshaam Faili. 2010. Au-
tomatic Persian wordnet construction. In 23rd
International conference on computational lin-
guistics, pages 846?850.
Roberto Navigli and Simone Paolo Ponzetto.
2012. BabelNet: The automatic construction,
evaluation and application of a wide-coverage
multilingual semantic network. Artificial Intel-
ligence, 193:217?250.
NIST. 2012. Secure hash standard (shs). Fips pub
180-4, National Institute of Standards and Tech-
nology.
Noam Ordan and Shuly Wintner. 2007. He-
brew wordnet: a test case of aligning lexical
databases across languages. International Jour-
nal of Translation, 19(1):39?58.
B.S Pedersen, S. Nimb, J. Asmussen, N. S?rensen,
L. Trap-Jensen, and H. Lorentzen. 2009. Dan-
Net ? the challenge of compiling a wordnet
for Danish by reusing a monolingual dictionary.
Language Resources and Evaluation.
Ted Pederson. 2008. Empiricism is not a matter
of faith. Computational Linguistics, 34(3):465?
470.
Emanuele Pianta, Luisa Bentivogli, and Christian
Girardi. 2002. Multiwordnet: Developing an
aligned multilingual database. In In Proceed-
ings of the First International Conference on
Global WordNet, pages 293?302. Mysore, In-
dia.
Maciej Piasecki, Stan Szpakowicz, and Bartosz
Broda. 2009. A Wordnet from the Ground Up.
Wroclaw University of Technology Press. URL
http://www.plwordnet.pwr.wroc.pl/
main/content/files/publications/A_
Wordnet_from_the_Ground_Up.pdf, (ISBN
978-83-7493-476-3).
Chiew Kin Quah, Francis Bond, and Takefumi
Yamazaki. 2001. Design and construction
of a machine-tractable Malay-English lexicon.
1361
In Asialex 2001 Proceedings, pages 200?205.
Seoul.
Ervin Ruci. 2008. On the current state of Al-
banet and related applications. Technical report,
University of Vlora. (http://fjalnet.com/
technicalreportalbanet.pdf).
Beno??t Sagot and Darja Fis?er. 2008. Building
a free French wordnet from multilingual re-
sources. In European Language Resources As-
sociation (ELRA), editor, Proceedings of the
Sixth International Language Resources and
Evaluation (LREC?08). Marrakech, Morocco.
Sareewan Thoongsup, Thatsanee Charoenporn,
Kergrit Robkop, Tan Sinthurahat, Chumpol
Mokarat, Virach Sornlertlamvanich, and Hi-
toshi Isahara. 2009. Thai wordnet construction.
In Proceedings of The 7th Workshop on Asian
Language Resources (ALR7), Joint conference
of the 47th Annual Meeting of the Association
for Computational Linguistics (ACL) and the
4th International Joint Conference on Natural
Language Processing (IJCNLP),. Suntec, Sin-
gapore.
Dan Tufis?, Dan Cristea, and Sofia Stamou. 2004.
BalkaNet: Aims, methods, results and perspec-
tives. a general overview. Romanian Journal of
Information Science and Technology, 7(1?2):9?
34.
Unicode, Inc. 2012. Unicode, Inc. license agree-
ment - data files and software. http://www.
unicode.org/copyright.html.
Piek Vossen, editor. 1998. Euro WordNet. Kluwer.
Piek Vossen. 2005. Building wordnets.
http://www.globalwordnet.org/gwa/
BuildingWordnets.ppt.
Wikimedia. 2013. List of wiktionaries.
http://meta.wikimedia.org/w/index.
php?title=Wiktionary&oldid=4729333.
(accessed on 2013-02-14).
Wikipedia. 2013. Wikipedia ? wikipedia,
the free encyclopedia. URL http:
//en.wikipedia.org/w/index.php?
title=Wikipedia&oldid=552515903,
[Online; accessed 30-April-2013].
Renjie Xu, Zhiqiang Gao, Yuzhong Qu, and
Zhisheng Huang. 2008. An integrated approach
for automatic construction of bilingual Chinese-
English WordNet. In 3rd Asian Semantic Web
Conference (ASWC 2008), pages 302?341.
1362
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 167?170, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
XLING: Matching Query Sentences to a Parallel Corpus using  
Topic Models for Word Sense Disambiguation 
 
Liling Tan and Francis Bond 
Division of Linguistics and Multilingual Studies, 
Nanyang Technological University 
14 Nanyang Drive, Singapore 637332 
alvations@gmail.com, bond@ieee.org 
Abstract 
This paper describes the XLING system partici-
pation in SemEval-2013 Crosslingual Word 
Sense Disambiguation task. The XLING system 
introduces a novel approach to skip the sense 
disambiguation step by matching query sentenc-
es to sentences in a parallel corpus using topic 
models; it returns the word alignments as the 
translation for the target polysemous words. 
Although, the topic-model base matching under-
performed, the matching approach showed po-
tential in the simple cosine-based surface simi-
larity matching. 
1 Introduction 
This paper describes the XLING system, an un-
supervised Cross-Lingual Word Sense Disam-
biguation (CLWSD) system based on matching 
query sentence to parallel corpus using topic 
models. CLWSD is the task of disambiguating a 
word given a context by providing the most ap-
propriate translation in different languages 
(Lefever and Hoste, 2013).  
2 Background  
Topic models assume that latent topics exist in 
texts and each semantic topic can be represented 
with a multinomial distribution of words and 
each document can be classified into different 
semantic topics (Hofmann, 1999). Blei et al 
(2003b) introduced a Bayesian version of topic 
modeling using Dirichlet hyper-parameters, La-
tent Dirichlet Allocation (LDA). Using LDA, a 
set of topics can be generated to classify docu-
ments within a corpus. Each topic will contain a 
list of all the words in the vocabulary of the cor-
pus where each word is assigned a probability of 
occurring given a particular topic. 
3 Approach 
We hypothesized that sentences with different 
senses of a polysemous word will be classified 
into different topics during the LDA process. By 
matching the query sentence to the training sen-
tences by LDA induced topics, the most appro-
priate translation for the polysemous word in the 
query sentence should be equivalent to transla-
tion of word in the matched training sentence(s) 
from a parallel corpus. By pursuing this ap-
proach, we escape the traditional mode of dis-
ambiguating a sense using a sense inventory. 
4 System Description 
The XLING_TnT system attempts the matching 
subtask in three steps (1) Topicalize: match-
ing the query sentence to the training sentences 
by the most probable topic. (2) Rank: the 
matching sentences were ranked according to 
the cosine similarity between the query and 
matching sentences. (3) Translate: provides 
the translation of the polysemous word in the 
matched sentence(s) from the parallel corpus.  
4.1 Preprocessing  
The Europarl version 7 corpus bitexts (English-
German, English-Spanish, English-French, Eng-
lish-Italian and English-Dutch) were aligned at 
word-level with GIZA++ (Och and Ney, 2003). 
The translation tables from the word-alignments 
were used to provide the translation of the poly-
semous word in the Translate step.  
The English sentences from the bitexts were 
lemmatized using a dictionary-based lemmatiz-
167
er: xlemma1. After the lemmatization, English 
stopwords2 were removed from the sentences. 
The lemmatized and stop filtered sentences were 
used as document inputs to train the LDA topic 
model in the Topicalize step.  
Previously, topic models had been incorpo-
rated as global context features into a modified 
naive Bayes network with traditional WSD fea-
tures (Cai et al 2007). We try a novel approach 
of integrating local context (N-grams) by using 
pseudo-word sentences as input for topic induc-
tion. Here we neither lemmatize or remove stops 
words.  For example: 
 
Original Europarl sentence: ?Education and 
cultural policies are important tools for creating 
these values? 
 
Lemmatized and stopped: ?education cultural 
policy be important tool create these values? 
 
Ngram pseudo-word: ?education_and_cultural 
and_cultural_policies cultural_policies_are 
are_important_tools important_tools_for 
tools_for_creating for_creating_these creat-
ing_these_values? 
4.2 Topicalize and Match 
The Topicalize step of the system first (i) 
induced a list of topics and trained a topic model 
for each polysemous word using LDA, then (ii) 
allocated the topic with the highest probability 
to each training sentence. 
Finally, at evaluation, (iii) the query sentences 
were assigned the most probable topic inferred 
using the trained topic models. Then the training 
sentences allocated with the same topic were 
considered as matching sentences for the next 
Rank step.  
4.2.1 Topic Induction 
Topic models were trained using Europarl sen-
tences that contain the target polysemous words; 
one model per target word. The topic models 
were induced using LDA by setting the number 
of topics (#topics) as 50, and the alpha and beta 
                                                          
1  http://code.google.com/p/xlemma/ 
2  Using the Page and Article Analyzer stopwords from    
   http://www.ranks.nl/resources/stopwords.html 
hyper-parameters were symmetrically set at 
1.0/#topics. Blei et al (2003) had shown that the 
perplexity plateaus when #topics ? 50; higher 
perplexity means more computing time needed 
to train the model. 
4.2.2 Topic Allocation 
Each sentence was allocated the most probable 
topic induced by LDA. An induced topic con-
tained a ranked list of tuples where the 2nd ele-
ment in each tuple is a word that associated with 
the topic, the 1st element is the probability that 
the associated word will occur given the topic. 
The probabilities are generatively output using 
Variational Bayes algorithm as described in 
Hoffman et al (2010). For example: 
[(0.0208, 'sport'), (0.0172, 'however'), 
(0.0170, 'quite'), (0.0166, 'maritime'), 
(0.0133, 'field'), (0.0133, 'air-transport'), 
(0.0130, 'appear'), (0.0117, 'arrangement'), 
(0.0117, 'pertain'), (0.0111, 'supervision')] 
4.2.3 Topic Inference 
With the trained LDA model, we inferred the 
most probable topic of the query sentence. Then 
we extracted the top-10 sentences from the train-
ing corpus that shared the same top ranking top-
ic.  
The topic induction, allocation and inference 
were done separately on the lemmatized and 
stopped sentences and on the pseudo-word sen-
tence, resulting in two sets of matching sentenc-
es. Only the sentences that were in both sets of 
matches are considered for the Rank step. 
4.3 Rank 
Matched sentences from the Topicalize step 
were converted into term vectors. The vectors 
were reweighted using tf-idf and ranked accord-
ing to the cosine similarity with the query sen-
tences. The top five sentences were piped into 
the Translate step. 
4.4 Translate 
From the matching sentences, the Translate 
step simply checks the GIZA++ word alignment 
table and outputs the translation(s) of the target 
polysemous word. Each matching sentence, 
168
could output more than 1 translation depending 
on the target word alignment. As a simple way 
of filtering stop-words from target European 
languages, translations with less than 4 charac-
ters were removed. This effectively distills misa-
ligned non-content words, such as articles, pro-
nouns, prepositions, etc. To simplify the lemma-
tization of Spanish and French plural noun suf-
fixes, the ?-es? and ?-s? are stemmed from the 
translation outputs.  
 The XLING_TnT system outputs one transla-
tion for each query sentence for the best result 
evaluation. It output the top 5 translations for the 
out-of-five evaluation. 
4.5 Fallback 
For the out-of-five evaluation, if the query re-
turned less than 5 answers, the first fallback3 
appended the lemma of the Most Frequent Sense 
(according to Wordnet) of the target polysemous 
word in their respective language from the Open 
Multilingual Wordnet.4 If the first fallback was 
insufficient, the second fallback appended the 
most frequent translation of the target polyse-
mous word to the queries? responses. 
4.6 Baseline 
We also constructed a baseline for matching sen-
tences by cosine similarity between the lemmas 
of the query sentence and the lemmas of each 
English sentence in the training corpus.5 The 
baseline system is named XLING_SnT (Similar 
and Translate). The cosine similarity is calculat-
ed from the division of the vector product of the 
query and training sentence (i.e. numerator) by 
the root product of the vector?s magnitude 
squared. 
5 Results 
Tables 1 and 2 present the results for the XLING 
system for best and out-of-five evaluation. Our 
system did worse than the task?s baseline, i.e. 
the Most Frequent Translation (MFT) of the tar-
get word for all languages. Moreover the topic 
                                                          
3    Code sample for the fallback can be found at  
     http://goo.gl/PbdK7 
4    http://www.casta-net.jp/~kuribayashi/multi/ 
5  Code-snippet for the baseline can be found at  
     http://pythonfiddle.com/surface-cosine-similarity  
model based matching did worse than the cosine 
similarity matching baseline. The results show 
that matching on topics did not help. However, 
Li et al (2010) and Anaya-Sanchez et al (2007) 
had shown that pure topic model based unsuper-
vised system for WSD should perform a little 
better than Most Frequent Sense baseline in 
coarse-grain English WSD. Hence it was neces-
sary to perform error analysis and tweaking to 
improve the XLING system. 
 
BEST German Spanish French Italian Dutch 
SnT 
 
8.13  
(10.36) 
19.59 
(24.31) 
17.33 
(11.57) 
12.74 
(11.27) 
9.89 
(9.56) 
TnT 
 
5.28 
(5.82) 
18.60 
(24.31) 
16.48 
(11.63) 
10.70 
(7.54) 
7.40 
(8.54) 
MFT 
 
17.43 
(15.30) 
23.23 
(27.48) 
25.74 
(20.19) 
20.21 
(19.88) 
20.66 
(24.15) 
Table 1: Precision and (Mood) for the best evaluation 
 
OOF German Spanish French Italian Dutch 
SnT 
 
23.71 
(30.57) 
44.83 
(50.04) 
38.44 
(32.45) 
32.38 
(29.17) 
27.11 
(27.31) 
TnT 
 
19.13 
(23.54) 
39.52 
(44.96) 
35.3 
(28.02) 
33.28 
(29.61) 
23.27 
(22.98) 
MFT 
 
38.86 
(44.35) 
53.07 
(57.35) 
51.36 
(47.42) 
42.63 
(41.69) 
43.59 
(41.97) 
Table 2: Precision and (Mood) for the oof evaluation 
6 Error Analysis and Modifications 
Statistically, we could improve the robustness of 
the topic models in the Topicalize step by 
(i) tweaking the Dirichlet hyper-parameters to 
alpha = 50/#topics, beta = 0.01 as suggested by 
Wang et al (2009). 
 
 BEST OOF 
 Precision Mood Precision Mood 
German 6.50 6.71 20.98 25.18 
Spanish 14.77 19.43 40.22 45.67 
French 10.79 7.95 31.26 23.37 
Italian 13.10 10.95 36.56 31.94 
Dutch 7.42 7.47 21.66 20.42 
Table 3: Evaluations on Hyper-parameter tweaks 
 
Although the hyperparameters tweaks improves 
the scores for German and Dutch evaluations it 
brings the overall precision and mood precision 
of the other three languages down. Since the 
documents from each language are parallel, this 
169
suggests that there is some language-dependency 
for LDA?s hyperparameters. 
 By going through the individual queries and 
responses, several issues in the translate 
step need to be resolved to achieve higher preci-
sion; (i) German-English and Dutch-English 
word alignments containing compound words 
need to be segmented (e.g. kraftomnibusverkehr 
?kraft omnibus verkehr) and realigned such that 
the target word coach only aligns to omnibus, 
(ii) lemmatization of Italian, German and Dutch 
is crucial is getting the gold answers of the task 
(e.g. XLING answers omnibussen while the gold 
answers allowed omnibus). The use of target 
language lemmatizers, such as TreeTagger 
(Schmid, 1995) would have benefited the sys-
tem. 
7 Discussion 
The main advantage of statistical language inde-
pendent approaches is the ability to scale the 
system in any possible language. However lan-
guage dependent processing remains crucial in 
building an accurate system, especially lemmati-
zation in WSD tasks (e.g. kraftomnibusverkehr). 
We also hypothesize that more context would 
have improved the results of using topics: dis-
ambiguating senses solely from sentential con-
text is artificially hard. 
8 Conclusion 
Our system has approached the CLWSD task in 
an unconventional way of matching query sen-
tences to parallel corpus using topic models. 
Given no improvement from hyper-parameter 
tweaks, it reiterates Boyd-Graber, Blei and 
Zhu?s (2007) assertion that while topic models 
capture polysemous use of words, they do not 
carry explicit notion of senses that is necessary 
for WSD. Thus our approach to match query 
sentences by topics did not perform beyond the 
MFT baseline in the CLWSD evaluation. 
However, the surface cosine baseline, with-
out any incorporation of any sense knowledge, 
had surprisingly achieved performance closer to 
MFT It provides a pilot platform for future work 
to approach the CLWSD as a vector-based doc-
ument retrieval task on parallel corpora and 
providing the translation from the word align-
ments. 
References  
 enry Anaya-   anche , Aurora  ons-Porrata, and 
Rafael Berlanga-Llavori. 2007. Tkb-uo: Using 
sense clustering for wsd. In Proceedings of the 
Fourth International Workshop on Semantic Eval-
uations (SemEval-2007), pp. 322?325. 
Jordan Boyd-Graber, David M. Blei, and Xiaojin 
Zhu. 2007. A Topic Model for Word Sense Dis-
ambiguation. In Proc. of Empirical Methods in 
Natural Language Processing( EMNLP).  
David M. Blei, Andrew Y. Ng, and Michael L. Jor-
dan. 2003. Latent Dirichlet alocation. Journal of 
Machine Learning Research, 3:993?1022. 
Jun-Fu Cai, Wee-Sun Lee and Yee-Whye Teh. 2007. 
Improving word sense disambiguation using topic 
features. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning (EMNLP-CoNLL), pp. 1015?1023. 
Christiane Fellbaum. (ed.) (1998) WordNet: An Elec-
tronic Lexical Database, MIT Press 
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of SIGIR '99, Berkeley, 
CA, USA. 
Matthew Hoffman, David Blei and Francis Bach. 
2010. Online Learning for Latent Dirichlet Alloca-
tion. In Proceedings of NIPS 2010. 
Els Lefever and V?ronique Hoste. 2013. SemEval-
2013 Task 10: Cross-Lingual Word Sense Disam-
biguation, In Proceedings SemEval 2013, in con-
junction with *SEM 2013, Atlanta, USA. 
Linlin Li, Benjamin Roth and Caroline Sporleder. 
Topic Models for Word Sense Disambiguation and 
Token-based Idiom Detection. In Proc. of The 
48th Annual Meeting of the Association for Com-
putational Linguistics (ACL), 2010. Uppsala, 
Sweden. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment 
Models. Computational Linguistics 29:1. pp. 19-
51. 
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German. 
Proceedings of the ACL SIGDAT-Workshop. Dub-
lin, Ireland.  
Yi Wang, Hongjie Bai, Matt Stanton, Wen-Yen 
Chen, Edward Y. Chang. 2009. Plda: Parallel la-
tent dirichlet alocation for large-scale applica-
tions. In Proc. of 5th International Conference on 
Algorithmic Aspects in Information and Manage-
ment. 
170
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541?545,
Dublin, Ireland, August 23-24, 2014.
Sensible: L2 Translation Assistance by Emulating the Manual
Post-Editing Process
Liling Tan, Anne-Kathrin Schumann, Jose M.M. Martinez and Francis Bond
1
Universit?at des Saarland / Campus, Saarbr?ucken, Germany
Nanyang Technological University
1
/ 14 Nanyang Drive, Singapore
alvations@gmail.com, anne.schumann@mx.uni-saarland.de,
j.martinez@mx.uni-saarland.de, bond@ieee.org
Abstract
This paper describes the Post-Editor Z sys-
tem submitted to the L2 writing assis-
tant task in SemEval-2014. The aim of
task is to build a translation assistance
system to translate untranslated sentence
fragments. This is not unlike the task
of post-editing where human translators
improve machine-generated translations.
Post-Editor Z emulates the manual pro-
cess of post-editing by (i) crawling and ex-
tracting parallel sentences that contain the
untranslated fragments from a Web-based
translation memory, (ii) extracting the pos-
sible translations of the fragments indexed
by the translation memory and (iii) apply-
ing simple cosine-based sentence similar-
ity to rank possible translations for the un-
translated fragment.
1 Introduction
In this paper, we present a collaborative submis-
sion between Saarland University and Nanyang
Technological University to the L2 Translation As-
sistant task in SemEval-2014. Our team name
is Sensible and the participating system is Post-
Editor Z (PEZ).
The L2 Translation Assistant task concerns the
translation of an untranslated fragment from a par-
tially translated sentence. For instance, given a
sentence, ?Ich konnte B?arbel noch on the border
in einen letzten S-Bahn-Zug nach Westberlin set-
zen.?, the aim is to provide an appropriate transla-
tion for the underline phrase, i.e. an der Grenze.
The aim of the task is not unlike the task of
post-editing where human translators correct er-
rors provided by machine-generated translations.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The main difference is that in the context of post-
editing the source text is provided. A transla-
tion workflow that incorporates post-editing be-
gins with a source sentence, e.g. ?I could still
sit on the border in the very last tram to West
Berlin.? and the human translator is provided with
a machine-generated translation with untranslated
fragments such as the previous example and some-
times ?fixing? the translation would simply re-
quire substituting the appropriate translation for
the untranslated fragment.
2 Related Tasks and Previous
Approaches
The L2 writing assistant task lies between the
lines of machine translation and crosslingual word
sense disambiguation (CLWSD) or crosslingual
lexical substitution (CLS) (Lefever and Hoste,
2013; Mihalcea et al. 2010).
While CLWSD systems resolve the correct
semantics of the translation by providing the
correct lemma in the target language, CLS at-
tempts to provide also the correct form of the
translation with the right morphology. Machine
translation tasks focus on producing translations
of whole sentences/documents while crosslingual
word sense disambiguation targets a single lexical
item.
Previously, CLWSD systems have tried distri-
butional semantics and string matching methods
(Tan and Bond, 2013), unsupervised clustering of
word alignment vectors (Apidianaki, 2013) and
supervised classification-based approaches trained
on local context features for a window of three
words containing the focus word (van Gompel,
2010; van Gompel and van den Bosch, 2013; Rud-
nick et al., 2013). Interestingly, Carpuat (2013)
approached the CLWSD task with a Statistical MT
system .
Short of concatenating outputs of CLWSD /
CLS outputs and dealing with a reordering issue
541
and responding to the task organizers? call to avoid
implementing a full machine translation system
to tackle the task, we designed PEZ as an Auto-
matic Post-Editor (APE) that attempts to resolve
untranslated fragments.
3 Automatic Post-Editors
APEs target various types of MT errors from de-
terminer selection (Knight and Chander, 1994) to
grammatical agreement (Mare?cek et al., 2011).
Untranslated fragments from machine translations
are the result of out-of-vocabulary (OOV) words.
Previous approaches to the handling of un-
translated fragments include using a pivot lan-
guage to translate the OOV word(s) into a third
language and then back into to the source lan-
guage, thereby extracting paraphrases to OOV
(Callison-burch and Osborne, 2006), combining
sub-lexical/constituent translations of the OOV
word(s) to generate the translation (Huang et al.,
2011) or finding paraphrases of the OOV words
that have available translations (Marton et al.,
2009; Razmara et al., 2013).
1
However the simplest approach to handle un-
translated fragments is to increase the size of par-
allel data. The web is vast and infinite, a human
translator would consult the web when encounter-
ing a word that he/she cannot translate easily. The
most human-like approach to post-editing a for-
eign untranslated fragment is to do a search on
the web or a translation memory and choose the
most appropriate translation of the fragment from
the search result given the context of the machine
translated sentence.
4 Motivation
When post-editing an untranslated fragment, a hu-
man translator would (i) first query a translation
memory or parallel corpus for the untranslated
fragment in the source language, (ii) then attempt
to understand the various context that the fragment
can occur in and (iii) finally he/she would sur-
mise appropriate translations for the untranslated
fragment based on semantic and grammatical con-
straints of the chosen translations.
1
in MT, evaluation is normally performed using automatic
metrics based on automatic evaluation metrics that compares
scores based on string/word similarity between the machine-
generated translation and a reference output, simply remov-
ing OOV would have improved the metric ?scores? of the
system (Habash, 2008; Tan and Pal, 2014).
The PEZ system was designed to emulate the
manual post-editing process by (i) first crawling
a web-based translation memory, (ii) then extract-
ing parallel sentences that contain the untranslated
fragments and the corresponding translations of
the fragments indexed by the translation memory
and (iii) finally ranking them based on cosine sim-
ilarity of the context words.
5 System Description
The PEZ system consists of three components,
viz (i) a Web Translation Memory (WebTM)
crawler, (ii) the XLING reranker and (iii) a longest
ngram/string match module.
5.1 WebTM Crawler
Given the query fragment and the context sen-
tence, ?Die Frau kehrte alone nach Lima zur?uck?,
the crawler queries www.bab.la and returns
sentences containing the untranslated fragment
with various possible tranlsations, e.g:
? isoliert : Darum sollten wir den Kaffee nicht
isoliert betrachten.
? alleine : Die Kommission kann nun aber f?ur
ihr Verhalten nicht alleine die Folgen tragen.
? Allein : Allein in der Europischen Union
sind.
The retrieval mechanism is based on the
fact that the target translations of the queried
word/phrase are bolded on a web-based TM and
thus they can be easily extracted by manipulating
the text between <bold>...</bold> tags. Al-
though the indexed translations were easy to ex-
tract, there were few instances where the transla-
tions were embedded betweeen the bold tags on
the web-based TM.
5.2 XLING Reranker
XLING is a light-weight cosine-based sentence
similarity script used in the previous CLWSD
shared task in SemEval-2013 (Tan and Bond,
2013). Given the sentences from the WebTM
crawler, the reranker first removes all stopwords
from the sentences and then ranks the sentences
based on the number of overlapping stems.
In situations where there are no overlapping
content words from the sentences, XLING falls
back on the most common translation of the un-
translated fragment.
542
en-de en-es fr-en nl-en
acc wac rec acc wac rec acc wac rec acc wac rec
WebTM 0.160 0.184 0.647 0.145 0.175 0.470 0.055 0.067 0.210 0.092 0.099 0.214
XLING 0.152 0.178 0.647 0.141 0.171 0.470 0.055 0.067 0.210 0.088 0.095 0.214
PEZ 0.162 0.233 0.878 0.239 0.351 0.819 0.081 0.116 0.321 0.115 0.152 0.335
Table 1: Results for Best Evaluation of the System Runs.
5.3 Longest Ngram/String Matches
Due to the low coverage of the indexed trans-
lations on the web TM, it is necessary to ex-
tract more candidate translations. Assuming lit-
tle knowledge about the target language, human
translator would find parallel sentences containing
the untranslated fragment and resort to finding re-
peating phrases that occurs among the target lan-
guage sentences.
For instance, when we query the phrase history
book from the context ?Von ihr habe ich mehr gel-
ernt als aus manchem history book.?, the longest
ngram/string matches module retrieves several tar-
get language sentences without any indexed trans-
lation:
? Ich weise darauf hin oder nehme an, dass
dies in den Geschichtsb?uchern auch so
erw?ahnt wird.
? Wenn die Geschichtsb?ucher geschrieben wer-
den wird unser Zeitalter, denke ich, wegen
drei Dingen erinnert werden.
? Ich bin sicher, Pr?asident Mugabe hat sich
nun einen Platz in den Geschichtsb?uchern
gesichert, wenn auch aus den falschen
Gr?unden.
? In den Geschichtsb?uchern wird f?ur jeden
einzelnen Tag der letzten mehr als 227 Jahre
an Gewalttaten oder Tragdien auf dem eu-
rop?aischen Kontinent erinnert.
By simply spotting the repeating word/string
from the target language sentences it is pos-
sible to guess that the possible candidates
for ?history book? are Geschichtsb?ucher or
Geschichtsb?uchern. Computationally, this can
be achieved by looking for the longest matching
ngrams or the longest matching string across the
target language sentences fetched by the WebTM
crawler.
5.4 System Runs
We submitted three system runs to the L2 writing
assistant task in Semeval-2014.
1. WebTM: a baseline configuration which out-
puts the most frequent indexed translation of
the untranslated fragment from the Web TM.
2. XLING: reranks the WebTM outputs based
on cosine similarity.
3. PEZ: similar to the XLING but when the
WebTM fetches no output, the system looks
for longest common substring and reranks the
outputs based on cosine similarity.
6 Evaluation
The evaluation of the task is based on three met-
rics, viz. absolute accuracy (acc), word-based ac-
curacy (wac) and recall (rec).
Absolute accuracy measures the number of
fragments that match the gold translation of the
untranslated fragments. Word-based accuracy as-
signs a score according to the longest consecutive
matching substring between output fragment and
reference fragment; it is computed as such:
wac =
|longestmatch(output,reference)|
max(|output|,|reference|)
Recall accounts for the number of fragments for
which output was given (regardless of whether it
was correct).
7 Results
Table 1 presents the results for the best evalua-
tion scores of the PEZ system runs for the En-
glish to German (en-de), English to Spanish (en-
es), French to English (fr-en) and Dutch to English
(nl-en) evaluations. Figure 1 presents the word ac-
curacy of the system runs for both best and out-of-
five (oof) evaluation
2
.
The results show that using the longest
ngram/string improves the recall and subsequently
the accuracy and word accuracy of the system.
However, this is not true when guessing untrans-
lated fragments from L1 English to L2. This is
due to the low recall of the system when search-
ing for the untranslated fragment in French and
2
Please refer to http://goo.gl/y9f5Na for results
of other competing systems
543
Figure 1: Word Accuracy of System Runs (best on
the left, oof on the right).
Dutch, where the English words/phases indexed in
the TM is much larger than other languages.
8 Error Analysis
We manually inspected the English-German out-
puts from the PEZ system and identified several
particularities of the outputs that account for the
low performance of the system for this language
pair.
8.1 Weird Expressions in the TM
When attempting to translate Nevertheless in the
context of ?Nevertheless hat sich die neue Bun-
desrepublik Deutschland unter amerikanischem
Druck an der militrischen Einmischung auf dem
Balkan beteiligt.? where the gold translation is
Trotzdem or Nichtsdestotrotz. The PEZ system re-
trieves the following sentence pairs that contains a
rarely used expression nichtsdestoweniger from a
literally translated sentence pair in the TM:
? EN: But nevertheless it is a fact that nobody
can really recognize their views in the report.
? DE: Aber nichtsdestoweniger kann sich nie-
mand so recht in dem Bericht wiederfinden.
Another example of weird expression is when
translating ?husband? in the context of ?In der
Silvesternacht sind mein husband und ich auf die
Bahnhofstra?e gegangen.?. PEZ provided a lesser
use yet valid translation Gemahl instead of the
gold translation Mann. In this case, it is also a
matter of register where in a more formal register
one will use Gemahl instead of Mann.
8.2 Missing / Additional Words from
Matches
When extracting candidate translations from the
TM index or longest ngram/string, there are sev-
eral matches where the PEZ system outputs a par-
tial phrase or phrases with additional tokens that
cause the disparity between the absolute accuracy
and word accuracy. An instance of missing words
is as follows:
? Input: Eine genetische Veranlagung
plays a decisive role.
? PEZ: Eine genetische Veranlagung
eine entscheidende rolle.
? Gold: Eine genetische Veranlagung
spielt (dabei) eine entscheidende rolle.
For the addition of superfluous words is as fol-
lows:
? Input: Ger?ate wie Handys sind
not permitted wenn sie nicht unterrichtlichen
Belangen dienen.
? PEZ: Ger?ate wie Handys sind es verboten,
wenn sie nicht unterrichtlichen Belangen
dienen.
? Gold: Ger?ate wie Handys sind verboten
wenn sie nicht unterrichtlichen Belangen di-
enen.
8.3 Case Sensitivity
For the English-German evaluation , there are sev-
eral instances where the PEZ system produces the
correct translation of the phrase but in lower cases
and this resulted in poorer accuracy. This is unique
to German target language and possibly contribut-
ing to the lower scores as compared to the English-
Spanish evaluation.
9 Conclusion
In this paper, we presented the PEZ automatic
post-editor system in the L2 writing assistant task
in SemEval-2014. The PEZ post-editing system is
a resource lean approach to provide translation for
untranslated fragments based on no prior training
data and simple string manipulations from a web-
based translation memory.
The PEZ system attempts to emulate the pro-
cess of a human translator post-editing out-
of-vocabulary words from a machine-generated
544
translation. The best configuration of the PEZ sys-
tem involves a simple string search for the longest
common ngram/string from the target language
sentences without having word/phrasal alignment
and also avoiding the need to handle word reorder-
ing for multi-token untranslated fragments.
Acknowledgements
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union?s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement n
?
317471.
References
Marianna Apidianaki. 2013. Limsi : Cross-lingual
word sense disambiguation using translation sense
clustering. In Second Joint Conference on Lexi-
cal and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
178?182, Atlanta, Georgia, USA, June.
Chris Callison-burch and Miles Osborne. 2006. Im-
proved statistical machine translation using para-
phrases. In In Proceedings of HLT/NAACL-2006,
pages 17?24.
Marine Carpuat. 2013. Nrc: A machine translation ap-
proach to cross-lingual word sense disambiguation
(semeval-2013 task 10). In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 188?192, Atlanta, Georgia, USA, June.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In ACL, pages 57?
60.
Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang,
Shih-Ting Huang, and Jason S. Chang. 2011. Using
sublexical translations to handle the oov problem in
machine translation. ACM Trans. Asian Lang. Inf.
Process., 10(3):16.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In AAAI, pages 779?784.
David Mare?cek, Rudolf Rosa, Petra Galu?s?c?akov?a, and
Ond?rej Bojar. 2011. Two-step translation with
grammatical post-processing. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
WMT ?11, pages 426?432, Stroudsburg, PA, USA.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
EMNLP, pages 381?390.
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1105?
1115, Sofia, Bulgaria, August.
Alex Rudnick, Can Liu, and Michael Gasser. 2013.
Hltdi: Cl-wsd using markov random fields for
semeval-2013 task 10. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 171?177, Atlanta, Georgia, USA, June.
Liling Tan and Francis Bond. 2013. Xling: Match-
ing query sentences to a parallel corpus using topic
models for wsd. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
167?170, Atlanta, Georgia, USA, June.
Liling Tan and Santanu Pal. 2014. Manawi: Using
multi-word expressions and named entities to im-
prove machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
Baltimore, USA, August.
Maarten van Gompel and Antal van den Bosch. 2013.
Wsd2: Parameter optimisation for memory-based
cross-lingual word-sense disambiguation. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 183?187, Atlanta, Geor-
gia, USA, June.
Maarten van Gompel. 2010. Uvt-wsd1: A cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, SemEval ?10, pages 238?241,
Stroudsburg, PA, USA.
545
Proceedings of the 8th Workshop on Asian Language Resources, pages 144?152,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Development of the Korean Resource Grammar:
Towards Grammar Customization
Sanghoun Song
Dept. of Linguistics
Univ. of Washington
sanghoun@uw.edu
Jong-Bok Kim
School of English
Kyung Hee Univ.
jongbok@khu.ac.kr
Francis Bond
Linguistics and Multilingual Studies
Nanyang Technological Univ.
bond@ieee.org
Jaehyung Yang
Computer Engineering
Kangnam Univ.
jhyang@kangnam.ac.kr
Abstract
The Korean Resource Grammar (KRG)
is a computational open-source grammar
of Korean (Kim and Yang, 2003) that has
been constructed within the DELPH-IN
consortium since 2003. This paper re-
ports the second phase of the KRG devel-
opment that moves from a phenomena-
based approach to grammar customiza-
tion using the LinGO Grammar Matrix.
This new phase of development not only
improves the parsing efficiency but also
adds generation capacity, which is nec-
essary for many NLP applications.
1 Introduction
The Korean Resource Grammar (KRG) has been
under development since 2003 (Kim and Yang,
2003) with the aim of building an open source
grammar of Korean. The grammatical frame-
work for the KRG is Head-driven Phrase Struc-
ture Grammar (HPSG: (Pollard and Sag, 1994;
Sag et al, 2003)), a non-derivational, constraint-
based, and surface-oriented grammatical archi-
tecture. The grammar models human languages
as systems of constraints on typed feature struc-
tures. This enables the extension of grammar
in a systematic and efficient way, resulting in
linguistically precise and theoretically motivated
descriptions of languages.
The initial stage of the KRG (hereafter,
KRG1) has covered a large part of the Korean
grammar with fine-grained analyses of HPSG.
However, this version, focusing on linguistic
data with theory-oriented approaches, is unable
to yield efficient parsing or generation. The addi-
tional limit of the KRG1 is its unattested parsing
efficiency with a large scale of naturally occur-
ring data, which is a prerequisite to the practical
uses of the developed grammar in the area of MT.
Such weak points have motivated us to move
the development of KRG to a data-driven ap-
proach from a theory-based one upon which the
KRG1 is couched. In particular, this second
phase of the KRG (henceforth, KRG2) also starts
with two methods: shared grammar libraries (the
Grammar Matrix (Bender et al, 2002; Bender et
al., 2010)) and data-driven expansion (using the
Korean portions of multilingual texts).
Next, we introduce the resources we used
(? 2). this is followed by more detailed motiva-
tion for our extensions (? 3). We then detail how
we use the grammar libraries from the Grammar
Matrix to enable generation (? 2) and then ex-
pand the coverage based on a corpus study (? 5).
2 Background
2.1 Open Source NLP with HPSG
The Deep Linguistic Processing with HPSG
Initiative (DELPH-IN: www.delph-in.net)
provides an open-source collection of tools and
grammars for deep linguistic processing of hu-
man language within the HPSG and MRS (Min-
imal Recursion Semantics (Copestake et al,
2005)) framework. The resources include soft-
ware packages, such as the LKB for parsing and
generation, PET (Callmeier, 2000) for parsing,
and a profiling tool [incr tsdb()] (Oepen, 2001).
There are also several grammars: e.g. ERG; the
144
English Resource Grammar (Flickinger, 2000),
Jacy; a Japanese Grammar (Siegel and Bender,
2002), the Spanish grammar, and so forth. These
along with some pre-compiled versions of pre-
processing or experimental tools are packaged in
the LOGON distribution.1 Most resources are un-
der the MIT license, with some parts under other
open licenses such as the LGPL.2 The KRG has
been constructed within this open-source infras-
tructure, and is released under the MIT license3.
2.2 The Grammar Matrix
The Grammar Matrix (Bender et al, 2002; Ben-
der et al, 2010) offers a well-structured envi-
ronment for the development of precision-based
grammars. This framework plays a role in build-
ing a HPSG/MRS-based grammar in a short
time, and improving it continuously. The Gram-
mar Matrix covers quite a few linguistic phe-
nomena constructed from a typological view.
There is also a starter-kit, the Grammar Matrix
customization system which can build the back-
bone of a computational grammar from a linguis-
tic description.
2.3 A Data-driven Approach
Normally speaking, building up a computational
grammar is painstaking work, because it costs
too much time and effort to develop a grammar
by hand only. An alternative way is a data-driven
approach which ensures ?cheap, fast, and easy?
development. However, this does not mean that
one is better than the other. Each of these two
approaches has its own merits. To achieve the
best or highest performance of parsing and gen-
eration, each needs to complement the other.
3 Directions for Improvement
3.1 Generation for MT
HPSG/MRS-based MT architecture consists of
parsing, transfer, and generation, as assumed in
Figure 1 (Bond et al, 2005). As noted earlier,
1wiki.delph-in.net/moin/LogonTop
2www.opensource.org/licenses/
3It allows people ?. . . without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense,
and/or sell copies? so long as ?The above copyright notice
and this permission notice shall be included . . . ?
Figure 1: HPSG/MRS-based MT Architecture
the KRG1 with no generation function is limited
only to the Source Analysis in Figure 1. In addi-
tion, since its first aim was to develop a Korean
grammar that reflects its individual properties in
detail, the KRG1 lacks compatible semantic rep-
resentations with other grammars such as ERG
and Jacy. The mismatches between the compo-
nents of the KRG1 and those of other grammars
make it difficult to adopt the Korean grammar for
an MT system. To take a representative example,
the KRG1 treats tense information as a feature
type of HEAD, while other grammars incorpo-
rate it into the semantics; thus, during the trans-
fer process in Figure 1, some information will be
missing. In addition, KRG1 used default inheri-
tance, which makes the grammar more compact,
but means it could not used with the faster PET
parser. We will discuss this issue in more detail
in Section 4.1.
Another main issue in the KRG1 is that some
of the defined types and rules in the grammar are
inefficient in generation. Because the declared
types and rules are defined with theoretical mo-
tivations, the run-time for generating any parsing
units within the system takes more than expected
and further causes memory overflow errors to
crop up almost invariably, even though the in-
put is quite simple. This problem is partially due
to the complex morphological inflection system
in the KRG1. Section 4.2 discusses how KRG2,
solves this problem.
Third it is better ?to be robust for parsing and
strict for generation? (Bond et al, 2008). That
means robust rules will apply in parsing, though
the input sentence does not sound perfect, but not
in generation. For example, the sentence (1b),
the colloquial form of the formal, standard sen-
tence (1a), is used more frequently in colloquial
context:
(1) a. ney-ka
you-NOM
cham
really
yeppu-ney.
pretty-DECL
?You are really pretty.?
b. ni-ka cham ippu-ney
The grammar needs to parse both (1a) and
145
(1b) and needs to yield the same MRS be-
cause both sentences convey the same truth-
conditional meaning. However, the KRG1 han-
dles only the legitimate sentence (1a), exclud-
ing (1b). The KRG1 is thus not sophisticated
enough to distinguish these two stylistic differ-
ent sentences. Therefore we need to develop the
generation procedures that can choose a proper
sentence style. Section 4.3 proposes the ?STYLE?
feature structure as the choice device.
3.2 Exploiting Corpora
One of the main motivations for our grammar
improvement is to achieve more balance between
linguistic motivation and practical purpose. We
have first evaluated the coverage and perfor-
mance of the KRG1 using a large size of data
to track down the KRG1?s problems that may
cause parsing inefficiencies and generating clog.
In other words, referring to the experimental re-
sults, we patterned the problematic parts in the
current version. According to the error pattern,
on the one hand, we expanded lexicon from oc-
curring texts in our generalization. On the other
hand, we fixed the previous rules and sometimes
introduced new rules with reference to the occur-
rence in texts.
3.3 How to Improve
In developing the KRG, we have employed two
strategies for improvement; (i) shared grammar
libraries and (ii) exploiting large text corpora.
We share grammar libraries with the Gram-
mar Matrix in the grammar (Bender et al, 2002)
as the foundation of KRG2. The Grammar Ma-
trix provides types and constraints that assist the
grammar in producing well-formed MRS repre-
sentations. The Grammar Matrix customization
system provides with a linguistically-motivated
broad coverage grammar for Korean as well as
the basis for multilingual grammar engineering.
In addition, we exploit naturally occurring texts
as the generalization corpus. We chose as our
corpora Korean texts that have translations avail-
able in English or Japanese, because they can be
the baseline of multilingual MT. Since the data-
driven approach is influenced by data type, mul-
tilingual texts help us make the grammar more
suitable for MT in the long term. In developing
the grammar in the next phrase, we assumed the
following principles:
(2) a. The Grammar Matrix will apply when a judg-
ment about structure (e.g. semantic represen-
tation) is needed.
b. The KRG will apply when a judgment about
Korean is needed.
c. The resulting grammar has to run on both
PET and LKB without any problems.
d. Parsing needs to be accomplished as robustly
as possible, and generation needs to be done
as strictly as possible.
4 Generation
It is hard to alter the structure of the KRG1
from top to bottom in a relatively short time,
mainly because the difficulties arise from con-
verting each grammar module (optimized only
for parsing) into something applicable to gener-
ation, and further from making the grammar run
separately for parsing and generation.
Therefore, we first rebuilt the basic schema of
the KRG1 on the Grammar Matrix customiza-
tion system, and then imported each grammar
module from KRG1 to the matrix-based frame
(?4.1). In addition, we reformed the inflectional
hierarchy assumed in the KRG1, so that the
grammar does not impede generation any longer
(? 4.2). Finally, we introduced the STYLE feature
structure for sentence choice in accordance with
our principles (2c-d) (?4.3).
4.1 Modifying the Modular Structure
The root folder krg contains the basic type
definition language files (*.tdl. In the
KRG2, we subdivided the types.tdl into:
matrix.tdl file which corresponds to gen-
eral principles; korean.tdl with language
particular rules; types-lex.tdl for lex-
ical types and types-ph.tdl for phrasal
types. In addition, we reorganized the KRG1?s
lexicons.tdl file into the lex folder con-
sisting of several sub-files in accordance with the
POS values (e.g.; lex-v.tdl for verbs).
The next step is to revise grammar modules
in order to use the Grammar Matrix to a full ex-
tent. In this process, when inconsistencies arise
between KRG1 and KRG2, we followed (2a-b).
146
We further transplanted each previous module
into the KRG2, while checking the attested test
items used in the KRG1. The test items, con-
sisting of 6,180 grammatical sentences, 118 un-
grammatical sentences, were divided into sub-
groups according to the related phenomena (e.g.
light verb constructions).
4.2 Simplifying the Inflectional Hierarchy
Korean has rigid ordering restrictions in the mor-
phological paradigm for verbs, as shown in (3).
(3) a. V-base + HON + TNS + MOOD + COMP
b. ka-si-ess-ta-ko ?go-HON-PST-DC-COMP?
KRG1 dealt with this ordering of suffixes by us-
ing a type hierarchy that represents a chain of in-
flectional slots (Figure 2: Kim and Yang (2004)).
Figure 2: Korean Verbal Hierarchy
This hierarchy has its own merits, but it is not
so effective for generating sentences. This is be-
cause the hierarchy requires a large number of
calculations in the generation process. Figure 3
and Table 1 explains the difference in computa-
tional complexity according to each structure.In
Figure 3: Calculating Complexity
Figure 3, (a) is similar to Figure 2, while (b) is on
the traditional template approach. Let us com-
pare each complexity to get the target node D.
For convenience? sake, let us assume that each
node has ten constraints to be satisfied. In (a),
since there are three parents nodes (i.e. A, B, and
C) on top of D, D cannot be generated until A,
B, and C are checked previously. Hence, it costs
at least 10,000 (10[A] ?10[B] ?10[C] ?10[D])
calculations. In contrast, in (b), only 100 (10[A]
?10[D]) calculations is enough to generate node
D. That means, the deeper the hierarchy is, the
more the complexity increases. Table 1 shows
(a) requires more than 52 times as much com-
plexity as (b), though they have the same number
of nodes.
Table 1: Complexity of (a) and (b)
(a) (b)
B? 10[A]?10[B?] 100 10[A]?10[B?]
C? 10[A]?10[B]?10[C?] 1,000 10[A]?10[C?]
D? 10[A]?10[B]?10[C]?10[D?] 10,000 10[A]?10[D?]
D 10[A]?10[B]?10[C]?10[D] 10,000 10[A]?10[D]
? 21,100 400
When generation is processed by LKB, all po-
tential inflectional nodes are made before syntac-
tic configurations according to the given MRS.
Thus, if the hierarchy becomes deeper and con-
tains more nodes, complexity of (a)-styled hi-
erarchy grows almost by geometric progres-
sion. This makes generation virtually impossi-
ble, causing memory overflow errors to the gen-
eration within the KRG1.
A fully flat structure (b) is not always supe-
rior to (a). First of all, the flat approach ig-
nores the fact that Korean is an agglutinative
language. Korean morphological paradigm can
yield a wide variety of forms; therefore, to enu-
merate all potential forms is not only undesirable
but also even impossible.
The KRG2 thus follows a hybrid approach (c)
that takes each advantage of (a) and (b). (c) is
more flattened than (a), which lessens computa-
tional complexity. On the other hand, in (c), the
depth of the inflectional hierarchy is fixed as two,
and the skeleton looks like a unary form, though
each major node (marked as a bigger circle) has
its own subtypes (marked as dotted lines). Even
though the depth has been diminished, the hier-
archy is not a perfectly flat structure; therefore, it
can partially represent the austere suffix ordering
in Korean. The hierarchy (c), hereby, curtails the
cost of generation.
In this context, we sought to use the minimum
number of possible inflectional slots for Korean.
We need at least three: root + semantic slot(s)
+ syntactic slot(s). That is, a series of suffixes
147
Table 2: Complexity of (a-c)
Depth Complexity
(a) n ? 3 ? 10,000
(b) n = 1 100
(c) n = 2 10,000
that denote semantic information attaches to the
second slot, and a series of suffixes, likewise,
attaches to the third slot. Since semantic suf-
fixes are, almost invariably, followed by syntac-
tic ones in Korean, this ordering is convincing,
granting that it does not fully represent that there
is also an ordering among semantic forms or syn-
tactic ones. (4) is an example from hierarchy (c).
There are three slots; root ka ?go?, semantic suf-
fixes si-ess, and syntactic ones ta-ko.
(4) a. V-base + (HON+TNS) + (MOOD+COMP)
b. ka-si+ess-ta+ko ?go-HON+PST-DC+COMP?
Assuming there are ten constraints on each node,
the complexity to generate D in (c) is just 10,000.
The measure, of course, is bigger than that of (b),
but the number never increases any more. That
means, all forms at the same depth have equal
complexity, and it is fully predictable. Table 2
compares the complexity from (a) to (c). By con-
verting (a) to (c), we made it possible to generate
with KRG2.
4.3 Choosing a Sentence Style
The choice between formal or informal (collo-
quial) sentence styles depends on context. A ro-
bust parser should cover both styles, but we gen-
erally want a consistent style when generating.
Figure 4: Type Hierarchy of STYLE
In such a case, the grammar resorts to STYLE
to filter out the infelicitous results. The type hi-
erarchy is sketched out in Figure 4. strict is near
to school grammar (e.g. written is a style of
newspapers). On the other hand, some variant
forms that stem from the corresponding canoni-
cal forms falls under robust in Figure 4. For in-
stance, if the text domain for generation is news-
paper, we can select only written as our sentence
choice, which excludes other styled sentences
from our result.
Let us see (1a-b) again. ni ?you? in (1b) is a di-
alect form of ney, but it has been used more pro-
ductively than its canonical form in daily speech.
In that case, we can specify STYLE of ni as di-
alect as given below. In contrast, the neutral
form ney has an unspecified STYLE feature:
ni := n-pn-2nd-non-pl &
[ STEM < ??ni?? >, STYLE dialect ].
ney := n-pn-2nd-non-pl &
[ STEM < ??ney?? > ].
Likewise, since the predicate in (1b) ippu
?pretty? stems from yeppu in (1a), they share
the predicate name ? yeppu a 1 rel? (i.e. the
RMRS standard for predicate names such as
? lemma pos sense rel?), but differ in each
STYLE feature. That means (1a-b) share the
same MRS structure (given below). KRG hereby
can parse (1b) into the same MRS as (1a) and
generate (1a) from it.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
mrs
LTOP h1 h
INDEX e2 e
RELS
?
?
?
?
?
?
?
?
?
person rel
LBL h3 h
ARG0 x4
?
?
?
x
PNG.PER 2nd
PNG.NUM non-pl
?
?
?
?
?
?
?
?
?
?
?
,
?
?
?
?
?
?
?
exist q rel
LBL h5 h
ARG0 x4
RSTR h6 h
BODY h7 h
?
?
?
?
?
?
?
,
?
?
?
?
?
cham d 1 rel
LBL h1
ARG0 e9 e
ARG1 h8 h
?
?
?
?
?
,
?
?
?
?
?
yeppu a 1 rel
LBL h10 h
ARG0 e2
ARG1 x4
?
?
?
?
?
?
HCONS
?
?
?
?
qeq
HARG h6
LARG h3
?
?
?
,
?
?
?
qeq
HARG h8
LARG h10
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: MRS of (1a-b)
These kinds of stylistic differences can take
place at the level of (i) lexicon, (ii) morpholog-
ical combination, and (iii) syntactic configura-
tion. The KRG2 revised each rule with reference
to its style type; therefore, we obtained totally
96 robust rules. As a welcoming result, we could
manipulate our generation, which was successful
respect to (2c-d). Let us call the version recon-
structed so far ?base?.
148
5 Exploiting Corpora
5.1 Resources
This study uses two multilingual corpora; one is
the Sejong Bilingual Corpora: SBC (Kim and
Cho, 2001), and the other is the Basic Travel Ex-
pression Corpus: BTEC (Kikui et al, 2003). We
exploited the Korean parts in each corpus, taking
them as our generalization corpus. Table 3 repre-
sents the configuration of two resources (KoEn:
Korean-English, KoJa: Korean-Japanese):
Table 3: Generalization Corpora
SBC BTEC
Type Bilingual Multilingual
Domain Balanced Corpus Tourism
Words KoEn : 243,788 914,199KoJa : 276.152
T/T ratio KoEn : 27.63 92.7KoJa : 20.28
Avr length KoEn : 16.30 8.46KoJa : 23.30
We also make use of nine test suites sorted
by three types (Each test suite includes 500 sen-
tences). As the first type, we used three test
sets covering overall sentence structures in Ko-
rean; Korean Phrase Structure Grammar (kpsg;
Kim (2004)), Information-based Korean Gram-
mar (ibkg; Chang (1995)), and the SERI test set
(seri; Sung and Jang (1997)).
Second, we randomly extracted sentences
from each corpus, separately from our gener-
alization corpus; two suites were taken from
the Korean-English and Korean-Japanese pair in
SBC (sj-ke and sj-kj, respectively). The other
two suites are from the BTEC-KTEXT (b-k),
and the BTEC-CSTAR (b-c); the former consists
of relatively plain sentences, while the latter is
composed of spoken ones.
Third, we obtained two test suites from sample
sentences in two dictionaries; Korean-English
(dic-ke), and Korean-Japanese (dic-kj). These
suites assume to have at least two advantages
with respect to our evaluation; (i) the sentence
length is longer than that of BTEC as well as
shorter than that of SBC, (ii) the sample sen-
tences on dictionaries are normally made up of
useful expressions for translation.
5.2 Methods
We tried to do experiments and improve the
KRG, following the three steps repeatedly: (i)
evaluating, (ii) identifying, and (iii) exploiting.
In each of the first step, we tried to parse the nine
test suites and generate sentences with the MRS
structures obtained from the parsing results, and
measured their coverage and performance. Here,
?coverage? means how many sentences can be
parsed or generated, and ?performance? repre-
sents how many seconds it takes on average. In
the second step, we identified the most serious
problems. In the third step, we sought to exploit
our generalization corpora in order to remedy the
drawbacks. After that, we repeated the proce-
dures until we obtain the desired results.
5.3 Experiments
So far, we have got two versions; KRG1 and
base. Our further experiments consist of four
phases; lex, MRS, irules, and KRG2.
Expanding the lexicon: To begin with, in or-
der to broaden our coverage, we expanded our
lexical entries with reference to our generaliza-
tion corpus and previous literature. Verbal items
are taken from Song (2007) and Song and Choe
(2008), which classify argument structures of
Korean verbal lexicon into subtypes within the
HPSG framework in a semi-automatic way. The
reason why we do not use our corpus here is
that verbal lexicon commonly requires subcat-
egorization frames, but we cannot induce them
so easily only using corpora. For other word
classes, we extracted lexical items from the POS
tagged SBC and BTEC corpora. Table 4 explains
how many items we extracted from our general-
ization corpus. Let us call this version ?lex?.
Table 4: Expansion of Lexical Items
verbal nouns 4,474
verbs and adjectives 1,216
common nouns 11,752
proper nouns 7,799
adverbs 1,757
numeral words 1,172
MRS: Generation in LKB, as shown in Fig-
ure 1, deploys MRS as the input, which means
our generation performance hinges on the well-
149
formedness of MRS. In other words, if our MRS
is broken somewhere or constructed inefficiently,
generation results is directly affected. For in-
stance, if the semantic representation does not
scope, we will not generate correctly. We were
able to identify such sentences by parsing the
corpora, storing the semantic representations and
then using the semantic well formedness check-
ers in the LKB. We identified all rules and lexi-
cal items that produced ill-formed MRSs using
a small script and fixed them by hand. This had
an immediate and positive effect on coverage as
well as performance in generation. We refer to
these changes as ?MRS?.
Different inflectional forms for sentence
styles: Texts in our daily life are actually com-
posed of various styles. For example, spoken
forms are normally more or less different from
written ones. The difference between them in
Korean is so big that the current version of KRG
can hardly parse spoken forms. Besides, Ko-
rean has lots of compound nouns and derived
words. Therefore, we included these forms into
our inflectional rules and expanded lexical en-
tries again (3,860 compound nouns, 2,791 de-
rived words). This greatly increased parsing cov-
erage. We call this version ?irules?.
Grammaticalized and Lexicalized Forms:
There are still remaining problems, because our
test suites contain some considerable forms.
First, Korean has quite a few grammaticalized
forms; for instance, kupwun is composed of a
definite determiner ku and a classifier for human
pwun ?the person?, but it functions like a sin-
gle word (i.e. a third singular personal pronoun).
In a similar vein, there are not a few lexical-
ized forms as well; for example, a verbal lexeme
kkamek- is composed of kka- ?peel? and mek-
?eat?, but it conveys a sense of ?forget?, rather
than ?peel and eat?. In addition, we also need to
cover idiomatic expressions (e.g. ?thanks?) for
robust parsing. Exploiting our corpus, we added
1,720 grammaticalized or lexicalized forms and
352 idioms. Now, we call this ?KRG2?.
Table 5 compares KRG2 with KRG1, and Fig-
ure 6 shows how many lexical items we have
covered so far.
Table 5: Comparison between KRG1 and KRG2
KRG1 KRG2
# of default types 121 159
# of lexical types 289 593
# of phrasal types 58 106
# of inflectional rules 86 244
# of syntactic rules 36 96
# of lexicon 2,297 39,190
Figure 6: Size of Lexicon
5.4 Evaluation
Table 6 shows the evaluation measure of this
study. ?p? and ?g? stand for ?parsing? and ?gener-
ation?, respectively. ?+? represents the difference
compared to KRG1. Since KRG1 does not gen-
erate, there is no ?g+?.
Table 6: Evaluation
coverage (%) ambiguity
p p+ g s p g
kpsg 77.0 -5.5 55.2 42.5 174.9 144.4
ibkg 61.2 41.8 68.3 41.8 990.5 303.5
seri 71.3 -0.8 65.7 46.8 289.1 128.4
b-k 43.0 32.6 62.8 27.0 1769.4 90.0
b-c 52.2 45.8 59.4 31.0 1175.8 160.6
sj-ke 35.4 31.2 58.2 20.6 358.3 170.3
sj-kj 23.0 19.6 52.2 12.0 585.9 294.9
dic-ke 40.4 31.0 42.6 17.2 1392.7 215.9
dic-kj 34.8 25.2 67.8 23.6 789.3 277.9
avr 48.7 24.5 59.1 28.8 836.2 198.4
On average, the parsing coverage increases
24.5%. The reason why there are negative val-
ues in ?p+? of kpsg and seri is that we discarded
some modules that run counter efficient process-
ing (e.g., the grammar module for handling float-
ing quantifiers sometimes produces too many
ambiguities.). Since KRG1 has been constructed
largely around the test sets, we expected it to
perform well here. If we measure the parsing
coverage again, after excluding the results of
kpsg and seri, it accounts for 32.5%.4 The gen-
eration coverage of KRG2 accounts for almost
60% per parsed sentence on average. Note that
KRG1 could not parse at all. ?s? (short for ?suc-
cess?) means the portion of both parsed and gen-
erated sentences (i.e. ?p???g?), which accounts
4The running times, meanwhile, becomes slower as we
would expect for a grammar with greater coverage. How-
ever, we can make up for it using the PET parser, as shown
in Figure 9.
150
 
0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
KRG1 base lex MRS irules KRG2
%
kpsg
ibkg
seri
b-k
b-c
sj-ke
sj-kj
dic-ke
dic-kj
Figure 7: Parsing Coverage (%)
 
0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
base lex MRS irules KRG2
%
kpsg
ibkg
seri
b-k
b-c
sj-ke
sj-kj
dic-ke
dic-kj
Figure 8: Generation Coverage (%)
for about 29%. Ambiguity means ?# of parses/#
of sentences? for parsing and ?# of realizations/#
of MRSes? for generation. The numbers look
rather big, which should be narrowed down in
our future study.
In addition, we can find out in Table 6 that
there is a coverage ordering with respect to the
type of test suites; ?test sets > BTEC > dic >
SBC?. It is influenced by three factors; (i) lexi-
cal variety, (ii) sentence length, and (iii) text do-
main. This difference implies that it is highly
necessary to use variegated texts in order to im-
prove grammar in a comprehensive way.
Figure 7 to 10 represent how much each exper-
iment in ?5.3 contributes to improvement. First,
let us see Figure 7 and 8. As we anticipated,
lex and irules contribute greatly to the growth of
parsing coverage. In particular, the line of b-c in
Figure 8, which mostly consists of spoken forms,
rises rapidly in irules and KRG2. That implies
Korean parsing largely depends on richness of
lexical rules. On the other hand, as we also ex-
pected, MRS makes a great contribution to gen-
eration coverage (Figure 8). In MRS, the growth
accounts for 22% on average. That implies test-
ing with large corpora must take precedence in
order for coverage to grow.
Figure 9 and 10 shows performance in pars-
ing and generation, respectively. Comparing to
KRG1, our Matrix-based grammars (from base
 
0.0 
5.0 
10.0 
15.0 
20.0 
25.0 
KRG1 base lex MRS irules KRG2
sec.
kpsg
ibkg
seri
b-k
b-c
sj-ke
sj-kj
dic-ke
dic-kj
Figure 9: Parsing Performance (s)
 
0.0 
5.0 
10.0 
15.0 
20.0 
25.0 
base lex MRS irules KRG2
sec.
kpsg
ibkg
seri
b-k
b-c
sj-ke
sj-kj
dic-ke
dic-kj
Figure 10: Generation Performance (s)
to KRG2) yields fairly good performance. It is
mainly because we deployed the PET parser that
runs fast, whereas KRG1 runs only on LKB. Fig-
ure 10, on the other hand, shows that the revi-
sion of MRS also does much to enhance gen-
eration performance, in common with coverage
mentioned before. It decreases the running times
by about 3.1 seconds on average.
6 Conclusion
The newly developed KRG2 has been success-
fully included in the LOGON repository since
July, 2009; thus, it is readily available. In fu-
ture research, we plan to apply the grammar in
an MT system (for which we already have a
prototype). In order to achieve this goal, we
need to construct multilingual treebanks; Korean
(KRG), English (ERG), and Japanese (Jacy).
Acknowledgments
We thank Emily M. Bender, Dan Flickinger,
Jae-Woong Choe, Kiyotaka Uchimoto, Eric
Nichols, Darren Scott Appling, and Stephan
Oepen for comments and suggestions at vari-
ous stages. Parts of this research was conducted
while the first and third authors were at the Na-
tional Institute for Information and Communi-
cations Technologies (NICT), Japan; we thank
NICT for their support. Our thanks also go to
three anonymous reviewers for helpful feedback.
151
References
Bender, Emily M., Dan Flickinger, and Stephan
Oepen. 2002. The Grammar Matrix: An Open-
Source Starter-Kit for the Rapid Development of
Cross-Linguistically Consistent Broad-Coverage
Precision Grammars. In Procedings of the Work-
shop on Grammar Engineering and Evaluation at
the 19th International Conference on Computa-
tional Linguistics.
Bender, Emily M., Scott Drellishak, Antske Fokkens,
Michael Wayne Goodman, Daniel P. Mills, Laurie
Poulson, and Safiyyah Saleem. 2010. Grammar
Prototyping and Testing with the LinGO Grammar
Matrix Customization System. In Proceedings of
ACL 2010 Software Demonstrations.
Bond, Francis, Stephan Oepen, Melanie Siegel, Ann
Copestake, and Dan Flickinger. 2005. Open
Source Machine Translation with DELPH-IN. In
Proceedings of Open-Source Machine Transla-
tion: Workshop at MT Summit X.
Bond, Francis, Eric Nichols, Darren Scott Appling,
and Michael Paul. 2008. Improving Statistical
Machine Translation by Paraphrasing the Train-
ing Data. In Proceedings of the 5th International
Workshop on Spoken Languaeg Translation.
Callmeier, Ulrich. 2000. PET?a Platform for Exper-
imentation with Efficient HPSG Processing Tech-
niques. Natural Language Engineering, 6(1):99?
107.
Chang, Suk-Jin. 1995. Information-based Korean
Grammar. Hanshin Publishing, Seoul.
Copestake, Ann, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Seman-
tics: An Introduction. Research on Language and
Computation, 3(4):281?332.
Flickinger, Dan. 2000. On Building a More Efficient
Grammar by Exploiting Types. Natural Language
Engineering, 6 (1) (Special Issue on Efficient Pro-
cessing with HPSG):15 ? 28.
Kikui, Genichiro, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Creating
corpora for speech-to-speech translation. In Proc.
of the EUROSPEECH03, pages 381?384, Geneve,
Switzerland.
Kim, Se-jung and Nam-ho Cho. 2001. The progress
and prospect of the 21st century Sejong project. In
ICCPOL-2001, pages 9?12, Seoul.
Kim, Jong-Bok and Jaehyung Yang. 2003. Ko-
rean Phrase Structure Grammar and Its Implemen-
tations into the LKB System. In Proceedings of
the 17th Pacific Asia Conference on Language, In-
formation and Computation.
Kim, Jong-Bok and Jaehyung Yang. 2004. Projec-
tions from Morphology to Syntax in the Korean
Resource Grammar: Implementing Typed Feature
Structures. In Lecture Notes in Computer Science,
volume 2945, pages 13?24. Springer-Verlag.
Kim, Jong-Bok. 2004. Korean Phrase Structure
Grammar. Hankwuk Publishing, Seoul.
Oepen, Stephan. 2001. [incr tsdb()] ? competence
and performance laboratory. User manual. Tech-
nical report, Computational Linguistics, Saarland
University.
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. The University of
Chicago Press, Chicago, IL.
Sag, Ivan A., Thomas Wasow, , and Emily M. Bender.
2003. Syntactic Theory: A Formal Introduction.
CSLI Publications, Stanford, CA.
Siegel, Melanie and Emily M. Bender. 2002. Effi-
cient Deep Processing of Japanese. In Proceed-
ings of the 3rd Workshop on Asian Language Re-
sources and International Standardization.
Song, Sanghoun and Jae-Woong Choe. 2008.
Automatic Construction of Korean Verbal Type
Hierarchy using Treebank. In Proceedings of
HPSG2008.
Song, Sanghoun. 2007. A Constraint-based Analysis
of Passive Constructions in Korean. Master?s the-
sis, Korea University, Department of Linguistics.
Sung, Won-Kyung and Myung-Gil Jang. 1997. SERI
Test Suites ?95. In Proceedings of the Confer-
ence on Hanguel and Korean Language Informa-
tion Processing.
152
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 92?100,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Extracting Transfer Rules for Multiword Expressions from Parallel Corpora
Petter Haugereid and Francis Bond
Division of Linguistics and Multilingual Studies,
Nanyang Technological University, Singapore
petterha@ntu.edu.sg,bond@ieee.org
Abstract
This paper presents a procedure for extract-
ing transfer rules for multiword expressions
from parallel corpora for use in a rule based
Japanese-English MT system. We show that
adding the multi-word rules improves transla-
tion quality and sketch ideas for learning more
such rules.
1 Introduction
Because of the great ambiguity of natural language,
it is hard to translate from one language to another.
To deal with this ambiguity it is common to try to
add more context to a word, either in the form of
multi-word translation patterns (Ikehara et al, 1991)
or by adding more context to the translations in sta-
tistical MT systems (Callison-Burch et al, 2005).
In this paper, we present a way to learn large
numbers of multi-word translation rules from either
dictionaries or parallel text, and show their effec-
tiveness in a semantic?transfer-based Japanese-to-
English machine translation system. This research
is similar to work such as Nichols et al (2007). The
novelty lies in (i) the fact that we are learning rules
from parallel text and (ii) that we are learning much
more complex rules.
In Section 2, we outline the semantic transfer ma-
chinery and we introduce the DELPH-IN machine
translation initiative that provided the resources used
in its construction. We describe in more detail how
we learn new rules in Section 3, and show their ef-
fect in Section 4. We briefly discuss the results and
outline future work in Section 5 and, finally, we con-
clude this paper in Section 6.
2 Semantic transfer
All experiments are carried out using Jaen, a se-
mantic transfer based machine translation system
(Bond et al, 2011). The system uses Minimal Re-
cursion Semantics (MRS) as its semantic representa-
tion (Copestake et al, 2005). The transfer process
takes place in three steps. First, a Japanese string is
parsed with the Japanese HPSG grammar, JACY. The
grammar produces an MRS with Japanese predicates.
Second, the Japanese MRS is transferred into an En-
glish MRS. And finally, the English HPSG grammar
ERG generates an English string from the English
MRS.
At each step of the translation process, stochastic
models are used to rank the output. There is a cutoff
at 5, so the maximal amount of generated sentences
is 125 (5x5x5). The final results are reranked using
a combined model (Oepen et al, 2007).
While JACY and the ERG have been developed
over many years, less effort has been put into the
transfer grammar, and this component is currently
the bottleneck of the system. In general, transfer
rules are the bottleneck for any system, and there
is a long history of trying to expand the number of
transfer rules types (Matsuo et al, 1997) and tokens
(Yamada et al, 2002).
In order to increase the coverage of the system
(the number of words that we can translate) we build
rules automatically. We look at strings that have
a high probability of being a translation (identified
from parallel corpora), and see if they fit a pattern
defined in the transfer grammar. A very simple pat-
tern would be that of a noun predicate being trans-
ferred as another noun predicate. The transfer rule
type for this pattern is given in (1). The type makes
92
sure that the LBL and the ARG0 values are kept when
the relation is transferred, while the PRED value is
left underspecified.1
(1)
?
?
?
?
?
?
?
noun-mtr
IN|RELS
?[
LBL h1 , ARG0 x1
]?
OUT|RELS
?[
LBL h1 , ARG0 x1
]?
?
?
?
?
?
?
?
The rule for? (hon)? book, which is a subtype
of noun-mtr, is given in (2).
(2)
?
?
?
?
?
?
?
hon_book
IN|RELS
?[
PRED _hon_n_rel
]?
OUT|RELS
?[
PRED _book_n_of_rel
]?
?
?
?
?
?
?
?
A linguistically more interesting transfer rule is
that for PP ? Adjective transfer (see (3)), which
takes as input 3 relations (the first for the noun, the
second for the postposition, and the third for the
quantifier of the noun, all properly linked), and out-
puts one relation (for the adjective), for example of
an angle ? angular, to give an English-to-English
example. The output adjective relation is given the
same handle, index and external argument as the in-
put postposition, so that the semantic linking with
the rest of the MRS is preserved. In this way, modi-
fiers of the PP will modify the Adjective, and so on.
The use of this transfer rule is demonstrated in Sec-
tion 3.1.2
1the LBL (label) of the relation is a tag, which can be used to
refer to the relation (conventionally written with an h for han-
dle). The ARG0 is the index of the relation. Nouns and deter-
miners have referential indices (conventionally written with an
x), while adjectives and verbs have event indices (written with
an e).
2The HCONS feature has as value a list of qeq constraints
(equality modulo quantifiers), which function is to express that
the label of a relation is equal to a handle in an argument posi-
tion (without unifying them).
(3)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pp-adj_mtr
IN
?
?
?
?
?
?
?
?
?
?
?
?
?
RELS
?
[
LBL h1 , ARG0 x1
]
[
LBL h0 , ARG0 e0 ,
ARG1 ext , ARG2 x1
]
[
ARG0 x1 , RSTR hr
]
?
HCONS
?[
HARG hr , LARG h1
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
OUT|RELS
?[
LBL h0 , ARG0 e0 ,
ARG1 ext
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
3 Procedure
We are using GIZA++ (Och and Ney, 2003) and
Anymalign (Lardilleux and Lepage, 2009) to gener-
ate phrase tables from a collection of four Japanese
English parallel corpora and one bilingual dictio-
nary. The corpora are the Tanaka Corpus (2,930,132
words: Tanaka (2001)), the Japanese Wordnet Cor-
pus (3,355,984 words: Bond et al (2010)), the
Japanese Wikipedia corpus (7,949,605),3 and the
Kyoto University Text Corpus with NICT transla-
tions (1,976,071 words: Uchimoto et al (2004)).
The dictionary is Edict, a Japanese English dictio-
nary (3,822,642 words: Breen (2004)). The word
totals include both English and Japanese words.
We divided the corpora into development, test,
and training data, and extracted the transfer rules
from the training data. The training data of the four
corpora together with the Edict dictionary form a
parallel corpus of 20 million words (9.6 million En-
glish words and 10.4 million Japanese words). The
Japanese text is tokenized and lemmatized with the
MeCab morphological analyzer (Kudo et al, 2004),
and the English text is tokenized and lemmatized
with the Freeling analyzer (Padr? et al, 2010), with
MWE, quantities, dates and sentence segmentation
turned off.
When applying GIZA++ and Anymalign to the
lemmatized parallel corpus they produced phrase ta-
bles with 10,812,423 and 5,765,262 entries, respec-
tively, running GIZA++ with the default MOSES
settings and Anymalign for approximately 16 hours.
3The Japanese-English Bilingual Corpus of Wikipedia?s Ky-
oto Articles: http://alaginrc.nict.go.jp/WikiCorpus/
index_E.html
93
We filtered out the entries with an absolute fre-
quency of 1,4 and which had more than 4 words on
the Japanese side or more than 3 words on the En-
glish side. This left us with 6,040,771 Moses entries
and 3,435,176 Anymalign entries. We then checked
against the Jacy lexicon on the Japanese side and the
ERG lexicon on the English side to ensure that the
source and the target could be parsed/generated by
the MT system. Finally, we filtered out entries with a
translation probability, P(English|Japanese), of less
than 0.1. This gave us 1,376,456 Moses entries and
234,123 Anymalign entries. These were all phrase
table entries with a relatively high probability, con-
taining lexical items known both to the parser and
the generator.
For each of these phrase table entries, we looked
up the lexemes on either side in the Jacy/ERG lexi-
cons, and represented them with the semantic predi-
cate (and their syntactic category).5 Ambiguous lex-
emes were represented with a list of predicates. We
represented each possible surface rule with a list of
all possible semantic predicate rules. So a possible
surface rule with two (two times) ambiguous lexi-
cal items would give four possible semantic rules, a
possible surface rule with three (two times) ambigu-
ous lexical items would give eight possible seman-
tic rules, and so on. A total of 53,960,547 possible
semantic rules were created. After filtering out se-
mantic transfer rules containing English predicates
of probability less than 0.2 compared to the most
frequent predicate associated with the same surface
form, this number was reduced to 26,875,672.6 Each
of these rules consists of two ordered lists of seman-
tic predicates (one for Japanese and one for English).
From these possible semantic transfer rules, we
extracted transfer rules that fitted nine different pat-
4The absolute frequency number can, according to Adrien
Lardilleux (p.c.), be thought of as a confidence score. The
larger, the more accurate and reliable the translation probabili-
ties. 1 is the lowest score.
5As shown in (2), predicates reflect the syntactic category of
the lexical item by means of an infix, e.g. ?_n_? for noun.
6We used a profile of the English training data from the
Tanaka Corpus and the Japanese Wordnet Corpus, parsed with
the ERG grammar, to find the probability of each English pred-
icate, given its surface form. For example the word sleep is
assigned the predicate "_sleep_n_1_rel" 103 times, the predi-
cate "_sleep_v_1_rel" 89 times, and "_sleep_v_in_rel" 2 times.
Hence, semantic transfer rules containing the first two are ac-
cepted, while rules conataining the last are filtered out.
terns. We extracted 81,690 rules from the Moses en-
tries, and 52,344 rules from the Anymalign entries.
The total number of rules extracted was 97,478.
(36,556 rules overlapped.) Once the rule templates
have been selected and the thresholds set, the entire
process is automatic.
The distribution of the extracted rules over the
nine patterns is shown in Table 1.
In the first three patterns, we would simply see if
the predicates had the appropriate ?_n_? and ?_a_?
infixes in them (for nouns and adjectives respec-
tively). 82,651 rules fitted these patterns and were
accepted as transfer rules. The last six patterns were
slightly more complex, and are described below.
3.1 PP? adjective
Japanese PPs headed by the postposition? no ?of?
often correspond to an adjective in English as illus-
trated in (4).
(4) a. ??
small.size
?
of
small
b. ??
music
?
of
musical
In order to extract transfer rules that fit this pat-
tern, we checked for possible semantic rules hav-
ing two predicates on the Japanese side and one on
the English side. The first Japanese predicate would
have the infix ?_n_? (be a noun), and the second
would be ?_no_p_rel? (the predicate of the postpo-
sition ?). The sole English predicate would have
the infix ?_a_? (be an adjective).
3.2 PP? PP
Japanese PPs headed by the postposition ? de
?with/by/in/on/at? are, given certain NP comple-
ments, translated into English PPs headed by the
preposition ?by? (meaning ?by means of?) where the
prepositional object does not have a determiner, as
illustrated in (5).
(5) ????
taxi
?
DE
by taxi
By checking for possible semantic transfer rules
fitting the pattern noun + de_p_rel on the Japanese
94
Input Output Moses Anymalign Merged rules
noun + noun ? noun + noun 34,691 23,333 38,529
noun + noun ? adj + noun 21,129 13,198 23,720
noun + noun ? noun 11,824 12,864 20,402
PP ? adj 753 372 1,022
PP ? PP 131 24 146
verb + NP ? verb + NP 9,985 1,926 10,256
noun + adj ? adj 544 243 566
postp + noun + verb ? verb 1,821 173 1,921
PP + verb ? verb 812 211 916
Total 81,690 52,344 97,478
Table 1: Transfer rule patterns.
side, and the pattern by_p_rel and noun on the En-
glish side, we created PP to PP transfer rules where,
in addition to the predicates stemming from the lex-
ical items, the English determiner was set to the
empty determiner (udef_q_rel). The resulting trans-
fer rule for (5) is illustrated in (6).
(6)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pp_pp_mtr
IN
?
[
PRED _de_p_rel
]
[
PRED udef_q_rel
]
[
PRED _takushii_n_rel
]
?
OUT
?
[
PRED _by_p_means_rel
]
[
PRED udef_q_rel
]
[
PRED _taxi_n_1_rel
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
With this particular pattern we get transfer rules
which prevent us from generating all possible trans-
lations of ? (?with?, ?by?, ?on?, ?in?, or ?at?), and
keeps the quantifier unexpressed.
There are many other possible PP?PP patterns,
such as??? start in/on/at/to ?in the beginning?.
We started with one well known idiomatic English
type, but should learn many more.
3.3 Verb + NP? Verb + NP
Japanese MWEs fitting the pattern noun + object
marker (?) + verb usually are translated into En-
glish MWEs fitting one out of three verb + NP pat-
terns, illustrated in (7). In (7a), the NP has an unex-
pressed quantifier. The English pattern in these cases
will be verb + noun. In (7b), the NP has an indef-
inite article. The English pattern will then be verb
+ _a_q_rel + noun. And in (7c), the NP has defi-
nite article. The English pattern will then be verb +
_the_q_rel + noun.
(7) a. ???
tenisu
tennis
?
wo
ACC
?
shi
do
??
masu
POLITE
play tennis
b. ??
seikei
living
?
wo
ACC
???
tateru
stand up
make a living
c. ??
seme
blame
?
wo
ACC
??
ou
bear
take the blame
By adding these rules to the transfer grammar, we
avoid generating sentences such as I play the ten-
nis and He took a blame. In addition, we are able
to constrain the translations of the individual words,
greatly reducing the transfer search space
3.4 Noun + Adjective? Adjective
Japanese has a multiword expression pattern that is
not found in English. In this pattern, noun +? (ga)
+ adjective usually correspond to English adjectives,
as shown in (8). The pattern is an example of a dou-
ble subject construction. The Japanese adjective has
its subject provided by a noun, but still takes an ex-
ternal subject. Our transfer rule takes this external
95
subject and links it to the subject of the English ad-
jective.
(8) X
X
X
ga
ga
ga
?
se
NOM
?
ga
height
??
takai
NOM high
X is tall
With the new rules, the transfer grammar now cor-
rectly translates (9) as She is very intelligent. and
not Her head is very good., which is the translation
produced by the system without the new multiword
rules. Notice the fact that the adverb modifying the
adjective in Japanese is also modifying the adjective
in English.
(9) ??
kanojo
She
?
wa
TOPIC
??
taihen
very
?
atama
head
?
ga
NOM
??
yoi
good
?
.
.
She is very intelligent.
Because of the flexibility of the rule based sys-
tem, we can also parse, translate and generate many
variants of this, including those where the adverb
comes in the middle of the MWE, or where a dif-
ferent topic marker is used as in (10). We learn the
translation equivalences from text n-grams, but then
match them to complex patterns, thus taking advan-
tage of the ease of processing of simple text, but
still apply them flexibly, with the power of the deep
grammar.
(10) ??
kanojo
She
?
mo
FOCUS
?
atama
head
?
ga
NOM
??
taihen
very
??
yoi
good
?
.
.
She is also very intelligent.
She is very intelligent also.
3.5 Postp + Noun + Verb? Verb / PP + Verb
? Verb
Japanese has two MWE patterns consisting of a
postposition, a noun, and a verb, corresponding to
a verb in English. The first is associated with the
postposition? no ?of? (see (11)), and the second is
associated with the postposition ? ni ?in/on/at/to?
(see (12)).
(11) ??
rekishi
history
?
no
of
??
benkyou
study
?
wo
ACC
??
suru
make
study history
(12) ??
kingyo
goldfish
?
ni
in/on/at/to
??
esa
feed
?
wo
ACC
??
yaru
give
feed the goldfish
In (11), the postposition? no ?of?, the noun?
? benkyou ?study?, and the verb?? suru ?make?
are translated as study, while in (12), the postposi-
tion ? ni ?in/on/at/to?, the noun ?? esa ?feed?,
and the verb?? yaru ?give? are translated as feed.
In both MWE patterns, the noun is marked with the
object marker? wo. The two patterns have differ-
ent analysis: In (11), which has the no-pattern, the
postposition attaches to the noun, and the object of
the postposition?? rekishi ?history? functions as
a second subject of the verb. In (12), which has the
ni-pattern, the postposition attaches to the verb, and
the object of the postposition?? kingyo ?goldfish?
is a part of a PP. Given the different semantic rep-
resentations assigned to the two MWE patterns, we
have created two transfer rule types. We will have a
brief look at the transfer rule type for the no transla-
tion pattern, illustrated in (13).7
(13)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
p+n+arg12_arg12_mtr
IN
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
RELS
?
[
LBL h2 , ARG0 event,
ARG1 x3 , ARG2 x2
]
[
LBL h2 , ARG0 x3
]
[
ARG0 x3 , RSTR h3
]
[
LBL h1 , ARG0 e1 ,
ARG1 x1 , ARG2 x3
]
?
HCONS
?[
HARG h3 , LARG h2
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
OUT|RELS
?[
LBL h1 , ARG0 e1 ,
ARG1 x1 , ARG2 x2
]?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The input of the p+n+arg12_arg12_mtr transfer
rule type consists of (i) a postposition relation, (ii)
a noun relation, (iii) a quantifier (of the the noun),
7The transfer rule type for the ni translation pattern
(pp+arg12_arg12_mtr) is identical to the transfer rule type for
the no translation pattern except from the linking of the postpo-
sition in the input.
96
and (iv) a verb relation (listed as they appear on the
RELS list). The output relation is a verb relation. No-
tice that the ARG1 of the input verb relation is reen-
tered as ARG1 of the output relation ( x1 ), and the
ARG2 of the input postposition relation is reentered
as ARG2 of the output relation ( x2 ). The output re-
lation is also given the same LBL and ARG0 value
as the input verb relation. In this way, the Japanese
MWE is collapsed into one English relation while
semantic links to the rest of the semantic representa-
tion are maintained.
3.6 Summary
Out of the 26,875,672 possible semantic predicate
rules, we extracted 97,478 rules that fitted one of the
nine patterns. These rules were then included in the
transfer grammar of the MT system.
4 Results
The impact of the MWE transfer rules on the MT
system is illustrated in Table 2.
We compare two versions of the system, one with
automatically extracted MWE rules and one with-
out. They both have hand-written MWE and single
word rules as well as automatically extracted sin-
gle word rules extracted from Edict by Nichols et al
(2007).
The additional rules in + MWE are those pro-
duced in Section 3. The system was tested on held
out sections of the Tanaka Corpus (sections 003 to
005). As can be seen from the results, the overall
system is still very much a research prototype, the
coverage being only just over 20%.
Adding the new rules gave small but consistent
increases in both end-to-end coverage (19.3% to
20.1%) and translation quality (17.80% to 18.18%)
measured with NEVA (Forsbom, 2003).8
When we look only at the 105 sentences whose
translations were changed by the new rules the
NEVA increased from 17.1% to 21.36%. Investigat-
ing the effects on development data, we confirmed
that when the new MWE rules hit, they almost al-
ways improved the translation. However, there is
still a problem of data-sparseness, we are missing
8NEVA is an alternative to BLEU that is designed to provide
a more meaningful sentence-level score for short references. It
is calculated identically to BLEU, but leaving out the log and
exponent calculations. We find it correlates highly with BLEU.
instances of rule-types as well as missing many po-
tential rule types.
As an example of the former, we have a pattern
for verb+NP ? verb+NP, but were unable to learn
????? jihi wo negau ?beg for mercy: lit. ask
for compassion?. We had one example in the train-
ing data, and this was not enough to get over our
threshold. As an example of the latter, we do not
currently learn any rules for Adverb+Verb?Verb al-
though this is a common pattern.
5 Discussion and Further Work
The transfer rules learned here are based on co-
occurrence data from corpora and a Japanese-to-
English dictionary. Many of the translations learned
are in fact compositional, especially for the com-
pound noun and verb-object patterns. For exam-
ple, ? ? ?? ana-wo horu ?dig hole? ? dig
a whole would have been translated using existing
rules. In this case the advantage of the MWE rule is
that it reduces the search space, so the system does
not have to consider less likely translations such as
carve the shortages. More interestingly, many of the
rules find non-compositional translations, or those
where the structure cannot be translated word for
word. Some of these are also idiomatic in the source
and target language. One of our long term goals is
to move these expressions into the source and tar-
get grammars. Currently, both Jacy and the ERG
have idiom processing (based on Copestake et al,
2002), but there are few idiomatic entries in their
lexicons. Bilingual data can be a good source for
identifying these monolingual idioms, as it makes
the non-compositionality explicit. An example of
a rule that uses the current idiom machinery is the
(hand-built) rule N-ga chie-wo shiboru ?N squeezes
knowledge??N racks N?s brains, where the subject
is co-indexed with a possessive pronoun modifying
the object: I/You rack my/your brains. Adding such
expressions to the monolingual grammars simplifies
the transfer rules and makes the grammars more use-
ful for other tasks.
In this paper we only presented results for nine
major multi-word transfer rule types. These were
those that appeared often in the training and devel-
opment data. We can straightforwardly extend this
in two ways: by extending the number of rule types
97
Version Parse Transfer Generation Total NEVA (%) F1
coverage coverage coverage coverage
?MWE 3614/4500 1647/3614 870/1647 870/4500 17.80 0.185
(0 rules) (80.3%) (45.6%) (52.8%) (19.3%)
+ adj/n 3614/4500 1704/3614 900/1704 900/4500 17.99 0.189
(83,217 rules) (80.3%) (47.1%) (52.8%) (20.0%)
+ PP 3614/4500 1659/3614 877/1659 877/4500 17.88 0.187
(1,168 rules) (80.3%) (45.9%) (52.9%) (19.5%)
+ verb 3614/4500 1688/3614 885/1688 885/4500 17.89 0.186
(13,093 rules) (80.3%) (46.7%) (52.4%) (19.7%)
+ MWE 3614/4500 1729/3614 906/1729 906/4500 18.18 0.190
(97,478 rules) (80.3%) (47.8%) (52.4%) (20.1%)
Table 2: Coverage of the MT system before and after adding the MWE transfer rules.
and by extending the number of rule instances.
Shirai et al (2001) looked at examples in a
65,500-entry English-Japanese lexicon and esti-
mated that there were at least 80 multi-word
Japanese patterns that translated to a single word
in English. As we are also going from multi-word
to multi-word we expect that there will be even
more than this. Currently, adding another pattern is
roughly an hour?s work (half to make the rule-type in
the transfer engine, half to make the rule matcher in
the rule builder). To add another 100 patterns is thus
6 weeks work. Almost certainly this can be speeded
up by sharing information between the templates.
We therefore estimate that we can greatly reduce the
sparseness of rule-types with four weeks work.
To improve the coverage of rule instances, we
need to look at more data, such as that aligned by
Utiyama and Takahashi (2003).
Neither absolute frequency nor estimated transla-
tion probability give reliable thresholds for deter-
mining whether rules are good or not. Currently
we are investigating two solutions. One is feedback
cleaning, where we investigate the impact of each
new rule and discard those that degrade translation
quality, following the general idea of Imamura et al
(2003). The second is the more traditional human-
in-the loop: presenting each rule and a series of rele-
vant translation pairs to a human and asking them to
judge if it is good or not. Ultimately, we would like
to extend this approach to crowd source the deci-
sions. There are currently two very successful online
collaborative Japanese-English projects (Edict and
Tatoeba, producing lexical entries and multilingual
examples respectively) which indicates that there is
a large pool of interested knowledgeable people.
Finally, we are working in parallel to qualitatively
improve the MWE rules in two ways. The first is to
extend rules using semantic classes, not just words.
This would mean we would need fewer rules, but
each rule would be more powerful. Of course, many
rules are very idiomatic and should trigger on actual
lexemes, but there are many, such as ?????
himei wo negau ?beg for mercy? which allow some
variation ? in this case there are at least three differ-
ent verbs that are commonly used. At a lower level
we need to improve our handling of orthographic
variants so that a rule can match on different forms
of the same word, rather than requiring several rules.
We are working together with the Japanese WordNet
to achieve these goals.
The second approach is to learn complex rules
directly from the parallel text, in a similar way to
(Jellinghaus, 2007) or (Way, 1999). This will be
necessary to catch rules that our templates do not
include, but it is very easy to over-fit the rules to the
translation data. For this reason, we are still con-
straining rules with templates.
98
Resource Availability
The MWE expression rules made here and the ma-
chine translation system that uses them are avail-
able through an open source code repository. In-
stallation details can be found at http://wiki.
delph-in.net/moin/LogonInstallation. The
code to make the rules is undergoing constant re-
vision, when it settles down we intend to also add it
to the repository.
6 Conclusion
This paper presented a procedure for extracting
transfer rules for multiword expressions from paral-
lel corpora for use in a rule based Japanese-English
MT system. We showed that adding the multi-
word rules improves translation coverage (19.3%
to 20.1%) and translation quality (17.8% to 18.2%
NEVA). We show how we can further improve by
learning even more rules.
Acknowledgments
We would like to thank the members of the LO-
GON, and DELPH-IN collaborations for their support
and encouragement. In addition we would like to
thank the developers and maintainers of the other
resources we used in our project, especially JMDict,
Tatoeba, Anymalign and Moses. This project was
supported in part by Nanyang Technological Univer-
sity (through a start-up grant on ?Automatically de-
termining meaning by comparing a text to its trans-
lation?).
References
Francis Bond, Hitoshi Isahara, Kiyotaka Uchimoto,
Takayuki Kuribayashi, and Kyoko Kanzaki. 2010.
Japanese WordNet 1.0. In 16th Annual Meeting of
The Association for Natural Language Process-
ing, pages A5?3. Tokyo.
Francis Bond, Stephan Oepen, Eric Nichols, Dan
Flickinger, Erik Velldal, and Petter Haugereid.
2011. Deep open source machine translation. Ma-
chine Translation. (Special Issue on Open source
Machine Translation, to appear).
James W. Breen. 2004. JMDict: a Japanese-
multilingual dictionary. In Coling 2004 Workshop
on Multilingual Linguistic Resources, pages 71?
78. Geneva.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statistical
machine translation to larger corpora and longer
phrases. In 43nd Annual Meeting of the Associa-
tion for Computational Linguistics: ACL-2005.
Ann Copestake, Dan Flickinger, Carl J. Pol-
lard, and Ivan A. Sag. 2005. Minimal Re-
cursion Semantics: an introduction. Re-
search on Language and Computation, 3(4):281?
332. URL http://lingo.stanford.edu/sag/
papers/copestake.pdf.
Ann Copestake, Fabre Lambeau, Aline Villavicen-
cio, Francis Bond, Timothy Baldwin, Ivan Sag,
and Dan Flickinger. 2002. Multiword expres-
sions: Linguistic precision and reusability. In
Proceedings of the Third International Confer-
ence on Language Resources and Evaluation
(LREC 2002), pages 1941?7. Las Palmas, Canary
Islands.
Eva Forsbom. 2003. Training a super model look-
alike: Featuring edit distance, n-gram occurrence,
and one reference translation. In In Proceedings
of the Workshop on Machine Translation Evalua-
tion. Towards Systemizing MT Evaluation.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and
Hiromi Nakaiwa. 1991. Toward an MT system
without pre-editing ? effects of new methods
in ALT-J/E ?. In Third Machine Translation
Summit: MT Summit III, pages 101?106. Wash-
ington DC. URL http://xxx.lanl.gov/abs/
cmp-lg/9510008.
Kenji Imamura, Eiichiro Sumita, and Yuji Mat-
sumoto. 2003. Feedback cleaning of machine
translation rules using automatic evaluation. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics, pages
447?454. Association for Computational Lin-
guistics, Sapporo, Japan. URL http://www.
aclweb.org/anthology/P03-1057.
Michael Jellinghaus. 2007. Automatic Acquisition of
Semantic Transfer Rules for Machine Translation.
Master?s thesis, Universit?t des Saarlandes.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
99
2004, pages 230?237. Association for Computa-
tional Linguistics, Barcelona, Spain.
Adrien Lardilleux and Yves Lepage. 2009.
Sampling-based multilingual alignment. In
Proceedings of Recent Advances in Natural
Language Processing (RANLP 2009), pages
214?218. Borovets, Bulgaria.
Yoshihiro Matsuo, Satoshi Shirai, Akio Yokoo, and
Satoru Ikehara. 1997. Direct parse tree translation
in cooperation with the transfer method. In Daniel
Joneas and Harold Somers, editors, New Methods
in Language Processing, pages 229?238. UCL
Press, London.
Eric Nichols, Francis Bond, Darren Scott Appling,
and Yuji Matsumoto. 2007. Combining resources
for open source machine translation. In The
11th International Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI-07), pages 134?142. Sk?vde.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Stephan Oepen, Erik Velldal, Jan Tore L?nning,
Paul Meurer, and Victoria Rosen. 2007. Towards
hybrid quality-oriented machine translation. on
linguistics and probabilities in MT. In 11th Inter-
national Conference on Theoretical and Method-
ological Issues in Machine Translation: TMI-
2007, pages 144?153.
Llu?s Padr?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castell?n. 2010. Freeling 2.1:
Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010). La Val-
letta. (http://nlp.lsi.upc.edu/freeling.
Satoshi Shirai, Kazuhide Yamamoto, and Kazutaka
Takao. 2001. Construction of a dictionary to
translate japanese phrases into one english word.
In Proceedings of ICCPOL?2001 (19th Interna-
tional Conference on Computer Processing of
Oriental Languages, pages 3?8. Seoul.
Yasuhito Tanaka. 2001. Compilation of a mul-
tilingual parallel corpus. In Proceedings of
PACLING 2001, pages 265?268. Kyushu.
(http://www.colips.org/afnlp/archives/
pacling2001/pdf/tanaka.pdf).
Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo,
Masaki Murata, Satoshi Sekine, and Hitoshi
Isahara. 2004. Multilingual aligned parallel
treebank corpus reflecting contextual informa-
tion and its applications. In Gilles S?ras-
set, editor, COLING 2004 Multilingual Linguis-
tic Resources, pages 57?64. COLING, Geneva,
Switzerland. URL http://acl.ldc.upenn.
edu/W/W04/W04-2208.bib.
Masao Utiyama and Mayumi Takahashi. 2003.
English-Japanese translation alignment data.
http://www2.nict.go.jp/x/x161/members/
mutiyama/align/index.html.
Andy Way. 1999. A hybrid architecture for robust
MT using LFG-DOP. Journal of Experimental
and Theoretical Artificial Intelligence, 11. Special
Issue on Memory-Based Language Processing.
Setsuo Yamada, Kenji Imamura, and Kazuhide Ya-
mamoto. 2002. Corpus-assisted expansion of
manual mt knowledge. In Ninth International
Conference on Theoretical and Methodological
Issues in Machine Translation: TMI-2002, pages
199?208. Keihanna, Japan.
100
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 20?29,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Enriching Parallel Corpora for Statistical Machine Translation
with Semantic Negation Rephrasing
Dominikus Wetzel
Department of Computational Linguistics
Saarland University
dwetzel@coli.uni-sb.de
Francis Bond
Linguistics and Multilingual Studies
Nanyang Technological University
bond@ieee.org
Abstract
This paper presents an approach to improving
performance of statistical machine translation
by automatically creating new training data
for difficult to translate phenomena. In partic-
ular this contribution is targeted towards tack-
ling the poor performance of a state-of-the-art
system on negated sentences. The corpus ex-
pansion is achieved by high quality rephrasing
of existing sentences to their negated counter-
parts making use of semantic transfer. The
method is designed to work on both sides of
the parallel corpus while preserving the align-
ment. Our results show an overall improve-
ment of 0.16 BLEU points, with a statisti-
cally significant increase of 1.63 BLEU points
when tested on only negated test data.
1 Introduction
Having large and good quality parallel corpora is vi-
tal for the quality of statistical machine translation
(SMT) systems. However, these corpora are expen-
sive to create. Furthermore, certain phenomena are
not very frequent and hence underrepresented in ex-
isting parallel corpora, such as negated sentences,
questions, etc. Due to the lack of such training data,
the SMT systems do not perform as well as they
could. Especially when it comes to negation, it is
important that the basic semantics is preserved, i.e. a
negated statement should not be translated as a pos-
itive one and vice versa.
Given a state-of-the-art baseline Japanese-English
SMT system, a separate evaluation on the seman-
tic level of negative only vs. positive only test data
reveals the considerably poorer performance on the
negative test set. This tendency and the importance
of preserving a negated statement motivates experi-
ments with improving performance on negative sen-
tences.
Providing more training data for negative sen-
tences should even out the discrepancy of the perfor-
mance between the above mentioned negative and
positive test data. We present a method where a
large amount of negative training data is obtained by
rephrasing the original training data. The rephras-
ing is performed on the semantic level to ensure
high reliability and quality of the generated data.
Simple rewriting based on the surface or syntactic
level would require complex language specific rules,
which is not desirable.
Working on the semantic structure exploits the
fact that these representations abstract away from
language specific structures. Thus, our approach
can be easily implemented for other languages, pro-
vided there are grammars available for both lan-
guages involved in the desired parallel corpus. The
DELPH-IN project1 provides various such gram-
mars.
This paper first describes related work in the fol-
lowing section. Section 3 presents a semantic analy-
sis of the data with respect to negation and provides
some distributional statistics. In Section 4 we elab-
orate on the functionality of our rephrasing system
and present different methods for corpus expansion.
The experimental setup and the results are in Sec-
tion 5. A discussion and our conclusion are given in
Section 6 and Section 7, respectively.
2 Related Work
There has been plenty of work on paraphrasing data
in order to overcome the limitations that insuffi-
ciently large or underrepresented phenomena in par-
1www.delph-in.net
20
allel corpora impose on SMT.
Callison-Burch et al (2006) tackle the problem
of unseen phrases in SMT by adding source lan-
guage paraphrases to the phrase table with appropri-
ate probabilities. Both are obtained from additional
parallel corpora, where the translations of the same
foreign language phrase are considered paraphrases.
He et al (2011) use a statistical framework for
paraphrase generation of the source language. A
log-linear model similar to the one used in phrase-
based SMT provides paraphrases which are ranked
based on novelty and fluency. The training corpus
is then expanded by either adding the first best para-
phrase, or n-best paraphrases. The target language is
just copied to provide the required target side of the
paraphrase.
Marton et al (2009) and Gao and Vogel (2011)
create new information by means of shallow seman-
tic methods. The former present an approach to
overcome the problem of unknown words in a low
resource experiment. They base their monolingual
paraphrasing on semantic similarity measures. In
their setting they achieve significantly better trans-
lations. Gao and Vogel (2011) expand the parallel
corpus by creating new information from existing
data. With the use of a monolingual semantic role
labeller one side of the parallel corpus is labelled.
Role-to-word rules are extracted. In sentences con-
taining the frames and semantic roles for which re-
placement rules exist, the corresponding words are
substituted. A support vector machine is used for
filtering the generated paraphrases.
An approach where paraphrases are obtained via
generation from semantic structures is presented in
Nichols et al (2010). It exploits the fact that the gen-
erator produces multiple surface realizations. The
basic set up is similar to our work, however our ap-
proach additionally manipulates, i.e. rephrases the
semantics before generation. Furthermore, we im-
plement parallel rephrasing, changing the meaning
of both source and target text simultaneously.
There is, on the other hand, little work in phrase-
based SMT especially targeting negated sentences.
Collins et al (2005) approach the problem of prop-
erly translating negation in their general reordering
setting. Transformation rules are applied to syntac-
tic trees, so that the source language word order has
a closer resemblance to the target language word or-
der. In particular, the German negation is moved to-
wards the same position as the English one. This
however presumes the existence of at least some
negated training data.
3 Analysis of the Semantic Structure
The linguistic analysis is performed based on the
Head-Driven Phrase Structure Grammar (HPSG)
formalism established in the DELPH-IN project. In
particular we consider the language pair Japanese-
English. Hence, the broad-coverage grammar Jacy
for Japanese (Bender and Siegel, 2004) and the En-
glish Resource Grammar (ERG) (Flickinger, 2000)
are used respectively to parse the data and obtain the
semantics for each sentence.
3.1 Negation in Minimal Recursion Semantics
The formalism that is used to represent the seman-
tics in the DELPH-IN grammars is Minimal Recur-
sion Semantics (MRS) (Copestake et al, 2005). Per
definition, an MRS structure consists of a top han-
dle, a bag of elementary predicates (EP) and a bag of
constraints on handles. EPs represent verbs, their ar-
guments, negations, quantifiers, among others. Fur-
thermore, each EP has a handle with which it can
be identified. Constraints on handles are used to re-
strict EPs such that they are outscoped by negations
or quantifiers.
In a negated sentence, the negated verb is
outscoped by the negation relation EP. Technically,
the negation relation with handle hn takes as its ar-
gument (ARG1) a handle (hx) which is equal mod-
ulo quantifiers to the handle of the verb (hv), written
as the handle constraint: hx =q hv. For visualiza-
tion, an example is given, which shows the relevant
parts of such a negated structure for the sentence
?This may not suit your taste.? (Figure 1). There,
the negated verb has the handle h8. The negation
relation EP with handle h10 outscopes this via the
constraint h12 =q h8.
The rephrasing we propose can be achieved with
little or no knowledge about the specific implemen-
tation choices of the individual grammar. Collecting
a few sample sentences that appear to be negated in
the original data ? by performing a simple surface
string matching ? is enough to reveal the principle
of how negation is implemented. Because negation
21
< e2,
{ h8: _MAY_V_MODAL_REL( ARG0 e2, ARG1 h9 ),
h10: NEG_REL( ARG0 e11, ARG1 h12),
h13: _suit_v_1_rel( ARG0 e14, ARG1 x4, ARG2 x15),
... }
{ h6 =q h3,
h12 =q h8,
h9 =q h13,
... } >
Figure 1: A visualization of the English MRS structure from the sentence ?This may not suit your taste.?.
The irrelevant parts have been omitted. The necessary parts in the corresponding Japanese MRS are the
same.
Japanese
English neg rel no neg rel
neg rel 8.5% 1.4%
no neg rel 9.7% 80.4%
Table 1: Distribution of negation measured by the
presence or absence of a negation relation (neg rel)
for those sentences with parses in both languages.
is represented at the semantic level, both the ERG
and Jacy have very similar analyses, even though
the syntactic realization is very different (negation
in English involves a negative marker such as not
and the use of an auxiliary verb such as do, while in
Japanese it is realized by an auxiliary verb nai).
3.2 Data and Distribution of Negations
The data we use in this work is the Japanese-
English parallel Tanaka corpus (Tanaka, 2001; Bond
et al, 2008). We used the version distributed with
Jacy, which has approximately 150,000 sentence
pairs randomly ordered and divided into 100 pro-
files of 1,500 sentences each (the last one is a lit-
tle short). We summarize the distribution of negated
sentence pairs in Table 1. The data we consider for
these statistics excludes development and test pro-
files (000?005). 84.5% of the input sentence pairs
can be parsed successfully (110,759 out of 139,150).
The table also shows mixed cases where one lan-
guage had a negation relation EP, whereas the other
did not. Mixed cases are especially frequent when
the Japanese side has a negation relation. These
cases have two main causes: lexical negation such
as ?She missed the bus.? being translated with the
equivalent of ?She did not catch the bus.?; and id-
ioms, such as ikanakereba naranai ?I must go (lit:
go-not-if not-become)? where the Japanese expres-
sion of modality includes a negation. Instances of
the latter type form the majority, and should be han-
dled in a newer version of the grammar, they are not
considered further in this work.
4 Method: MRS Rephrasing & Corpus
Expansion
The basic setup of the whole rephrasing system con-
sists of parsing, MRS manipulation, generation and
finally parallel corpus compilation. In the follow-
ing sections, the individual processing modules are
described in detail.
4.1 Parsing
Parsing is done using PET (Callmeier, 2000) a
bottom-up chart parser for unification-based gram-
mars using the English and Japanese Grammars
ERG and Jacy. Since our approach builds on seman-
tic rephrasing, only the MRS structure is required.
We only use the best (first) parse returned by the
parser.
4.2 Rephrasing
This module takes an MRS structure as input and
rephrases it if possible by adding a negation rela-
tion EP to the highest scoping predicate. Adding the
negation relation in our current form does not ex-
plore alternatives, where the negation has scope over
22
other EPs in the MRS, nor are more refined changes
from positive to negative polarity items considered.
Before inserting the negation relation EP into the
existing MRS structure with its required handle con-
straint, we have to identify the EP we want to negate.
The event that is introduced by the highest scoping
verb is used. The event variable e2 is directly acces-
sible at the top of the MRS structure (cf. Figure 1).
The corresponding EP that we want to negate has the
event variable as value of its ARG0 attribute. This
EP has a handle h8 that has to be outscoped by the
negation by means of a handle constraint. Hence, a
new negation relation EP (in the example it got the
handle h10) is inserted with the following condition:
Its ARG1 attribute value has to be token identical to
the left side of a =q constraint. The right side is set
to the just identified handle h8 of the verb.
4.3 Generation
The same grammars used for parsing can also be
used by the generator of the Lexical Knowledge
Builder Environment (Copestake, 2002) to gener-
ate an n-best list of surface realizations given an
MRS structure. However, we only consider the high-
est ranked realization. For the English generation,
a generation ranking model is provided within the
DELPH-IN project, thus providing a more confident
n-best list. For the current Japanese grammar, no
such model is available.
An example of a successful generation can be
found in Table 2. On the English side, two surface
variations are generated. The Japanese realizations
show more variations in honorification and aspect.
We can only negate sentence pairs in both lan-
guages for 13.3% of the training data (18,727). This
is mainly because of the brittleness of the Japanese
generation (Goodman and Bond, 2009). Further,
there are multiple ways of negating sentences and
we do not always select the correct one.
4.4 Expanded Parallel Corpus Compilation
The method for assembling the expanded version of
the parallel corpus for the use as training or devel-
opment data directly influences translation quality.
This is also demonstrated in Nichols et al (2010),
where various versions of padding out the data and
preserving the word distribution are compared. The
reported differences in performance suggest the im-
portance of the method. Therefore, we have experi-
mented with the following versions:
? Append: The obtained negated sentence pairs
are added to the original corpus. Only the high-
est ranked realization per sentence for each lan-
guage is considered. Thus they are aligned with
each other. This leads to the addition of the fol-
lowing sentence pair where bilingual negation
was successful:
(en original,jp original)
(en negated 1,jp negated 1) added
? Padding: In order to preserve the word dis-
tribution as mentioned above, we addition-
ally padded out the sentence pairs by copying,
where no bilingual negation was possible:
(en original,jp original)
(en original,jp original) added
? Replace: For emphasizing the impact of
negated sentences, a variant of Append was
compiled. Instead of adding the original pair of
a successful bilingual negation the former was
replaced by the latter:
(en negated 1,jp negated 1) substi-
tuted
Another way of testing the quality of the gener-
ated rephrases is to include them in the language
model training. The expectation is that when the
rephrases are of good quality, then the language
model will be better and in turn should have posi-
tive result on the overall SMT.
5 Experiments & Evaluation
We experiment with the phrase-based statistical ma-
chine translation toolkit Moses (Koehn et al, 2007)
in order to train a Japanese - English system and
to show the influence of the expanded parallel cor-
pora obtained with negation rephrasing on transla-
tion performance.
5.1 Data
The Tanaka corpus is used as a basis for our exper-
iments. We tokenize and truecase the English side,
the Japanese side is already tokenized and there are
no case distinctions. Sentences longer than 40 to-
kens are removed. For evaluation, the English part
is recased and detokenized.
23
English Japanese
original I aim to be a writer. ????????????
negated I don?t aim to be a writer. ????????????
I do not aim to be a writer. ?????????????
???????????
??????????
???????????
??????????
Table 2: English and Japanese generations of a successfully rephrased sentence pair.
The sentence and token statistics for the original
Tanaka corpus and our various extensions are listed
in Table 3. The original corpus version acts as base-
line data with profiles 006?100 as training and 000?
002 as development data. For the extended systems,
the training data as described in Section 4.4 is used.
The same methods are applied on the development
portion of the Tanaka corpus for tuning. The full test
data has 42,305 English and 53,242 Japanese tokens
and 4,500 sentences and is equal to the Tanaka cor-
pus profiles 003?005.
The language model training data is in almost all
cases equal to the original English Tanaka training
data. Only in the Append + neg LM experiment, the
training data for the language model is equal to the
Append training data, except that it is slightly larger,
since long sentences have not been filtered out. The
expanded language model training data consists of
1,476,231 tokens and 160,069 sentences.
5.2 Different Test Sets
In order to find out the performance of the baseline
and the extended systems on negative sentences, the
test data has to be split up into several subsets, most
notably neg-strict and pos-strict. The former only
contains negated sentences, the latter only positive
sentences. The definition of both is based on the ex-
istence of a negation relation EP in the semantics of
the sentence. In order to obtain the semantic struc-
ture, the sentence pairs have to be parsed success-
fully. This also means, we will have some sentence
pairs for which we cannot make a decision. There-
fore, we provide a third test subset biparse, which
contains all the parsable sentence pairs. This set re-
veals the big jump of BLEU score compared to the
fourth test set al, which is the regular test set of the
Tanaka corpus. A combined dataset with pos-strict-
neg-strict is provided, which is the union of the first
two sets.
5.3 Setup
We use Moses (SVN revision 4293) with Giza++
(Och and Ney, 2003) and the SRILM toolkit 1.5.12
(Stolcke, 2002). The language model is trained as
a 5-order model with Kneser-Ney discounting. The
Giza++ alignment heuristic grow-diag-final-and is
used. All systems are tuned with MERT (Och,
2003). Several tunings for each system are run, the
best performing ones are reported here.
5.4 Results
The results of our experiments can be seen in Ta-
ble 4. The baseline is outperformed by our two best
variations Append and Append + neg LM with re-
spect to the entire test set. The differences in BLEU
points are 0.14 and 0.16, which are not statistically
significant according to the paired bootstrap resam-
pling method (Koehn, 2004).
When looking at the test set neg-strict that only
contains negated sentences, our improvement is
much more apparent. The gain of our best perform-
ing model Append + neg LM compared to the base-
line is at 1.63 BLEU points, which is statistically
significant (p < 0.05). On the other hand there is
a statistically insignificant drop of 0.30 with pos-
strict.
The model with the expanded language model
training data (Append + neg LM) always performs
24
Tokens Sentences
train dev train dev
Baseline 1,300,821 / 1,641,591 42,248 / 52,822 141,147 4,500
Append 1,469,569 / 1,841,139 47,905 / 59,400 159,874 5,121
Padding 2,628,757 / 3,293,246 85,422 / 105,952 282,294 9,000
Replace 1,327,936 / 1,651,655 43,174 / 53,130 141,147 4,500
Table 3: Counts of tokens and sentences of the original Tanaka corpus and our expanded versions. Tokens
are split up in English/Japanese counts.
better than the model under the same conditions ex-
cept language model training data (Append).
When padding out the original data to preserve the
word distribution in Padding, the effect of the addi-
tional negated training pairs is not strong enough.
Both scores on the entire test set, as well as on the
negation specific test set drop below the baseline.
This version performs slightly better overall com-
pared to Replace, however, on neg-strict it is a lot
worse.
We manually checked the neg-strict test data set
of our best performing system Append + neg LM
versus the baseline, checking only whether the nega-
tion was translated or not (ignoring the overall qual-
ity). For 146 sentences, both systems correctly
translated the negation. For 76 sentences both sys-
tems failed to translate the negation. For 33 sen-
tences Append + neg LM translated the negation
where the baseline system did not, and for 30 sen-
tences the baseline system translated the negation
but Append + neg LM did not. Overall, we reduced
the number of critical negation errors from 99 to 96.
Some example sentences are given in Figure 2.
6 Discussion
For identifying the performance of a state-of-the-art
baseline system on negated sentences, we have split
the test data into several distinct sets. The transla-
tion quality drops considerably by about 3 BLEU
points when looking at the negative data compared
to the parsable test data biparse. This big decline
and the difference between performance on negative
vs. positive test data shows that there is great poten-
tial to improve SMT systems by tackling this prob-
lem. Our approach is successful in handling nega-
tions better and thus diminishing the discrepancy of
the two sets.
As the results show, there is only a small decrease
of BLEU score points on the positive test data. And
on the negative test data, the increase is substan-
tially higher. Nevertheless, the overall performance
in terms of BLEU only reflects this high increase to
a certain degree. This can be attributed to the fact
that the test data has a similar distribution to that of
the training data, i.e. the proportion of negative sen-
tences is low. Thus, the big increase gets diluted in
the overall test data.
The results further show that improvement on the
negative test data set comes at the cost of a slight
degradation of performance on the positive data set
and hence also on the full test set. This behaviour
is not surprising due to the fact that a positive and
its negative correspondent only vary very little when
looking at the surface structure. The models trained
with our extended data are aimed at providing one
model which provides a balance between this gain
and the loss.
This notion suggests that one would benefit from
providing two separate translation models, one for
negated input data and one for positive data. In this
setting, the ample amount of negative training data
that we generated through rephrasing could be ex-
ploited even more. A yet higher increase of BLEU
score is expected. This of course requires a prepro-
cessing step that confidently splits up the data ac-
cordingly. However, since we have the grammars at
hand that can reliably determine whether there is a
semantic negation relation in the input, this step can
be solved easily. One small disadvantage with this
idea is that a decision can only be made if the gram-
25
Test data sets all biparse neg-strict pos-strict pos-strict-neg-strict
Sentence counts 4500 3399 285 2684 2969
Baseline 22.87 25.76 22.77 26.60 26.25
Append 23.01 25.78 24.04 26.22 26.25
Append + neg LM 23.03 25.88 24.40 26.30 26.28
Padding 22.74 25.54 22.62 26.35 26.06
Replace 22.55 25.35 23.36 26.00 25.84
Table 4: Japanese-English translation evaluation results of the baseline and our extended systems.
mar of the input language produces a parse for the
input sentence. This however can be circumvented
by backing off to the well balanced model presented
in this work. In other words, we use a positive model
for positive sentences, a negative model for negative
sentences and a balanced model if we are not sure.
Our method depends on two large-scale deep se-
mantic grammars. However, developing such gram-
mars has been made much more efficient with the
emergence of the Grammar Matrix (Bender et al,
2002). There is is already a large collection of work-
ing grammars, which can readily be tried out. In
addition to the ERG and Jacy, there are grammars
for German, French, Korean, Modern Greek, Nor-
wegian, Spanish, Portuguese, and more, with vary-
ing levels of coverage.2
Because parsing, rephrasing and generation do
not have 100% coverage, we cannot produce negated
versions of all sentences. The rephrasing can only
work when both sides of a sentence pair are parsable.
Furthermore, not every rephrased sentence pair can
be successfully realized. However, we still manage
to build far more negated training data than is oth-
erwise available: more than doubling the amount.
This could be further increased by a little more work
on the generation, especially for Jacy. In addition,
we have not made use of all the generated data, i.e.
lower ranked realizations have been discarded even
though they may still be useful.
Furthermore, we have shown in the experiment re-
sults that using our expanded version for language
model training is also of great benefit, since we
could achieve not only an overall increase, but es-
pecially one on negated test data.
2moin.delph-in.net/GrammarCatalogue
7 Conclusion & Future Work
We have presented an approach which alleviates
the negation translation difficulties of phrase-based
SMT. We have tackled the problem by automati-
cally expanding the training data with negated sen-
tence pairs. The additional data has been obtained
by rephrasing existing data based on the semantic
structure of the input.
Our experiments with the phrase-based SMT sys-
tem Moses show small improvements over the base-
line considering the entire test data. A more dis-
tinct look at only negated sentences in the test data
shows a statistically significant improvement of 1.63
BLEU points. The best performing model represents
a good balance of a high BLEU score increase on the
negated test data vs. a statistically insignificant de-
crease on the positive test data, yet achieving a small
overall improvement. Furthermore, it was shown,
that expanding not only the translation training data,
but also the language model training data boosts per-
formance even more.
Our method works on the semantic level and can
be easily adapted to other languages. Having ac-
cess to a deep semantic structure opens possible ex-
tensions along our idea. On the one hand negation
rephrasing could be refined in order to have a higher
generation rate. On the other hand, other phenomena
could also be tackled in the same way: e.g. rephras-
ing declarative statements to interrogatives.
Just for negation, the corpora expanded with our
high quality negations could be combined with the
syntactic reordering strategies presented in Section 2
such that the negation reordering rule has more train-
ing data and thus a bigger influence on the overall
performance.
26
Acknowledgements
This research was supported in part by the Erasmus
Mundus Action 2 program MULTI of the European
Union, grant agreement number 2009-5259-5.
References
Bender, E. M., Flickinger, D., and Oepen, S. (2002).
The grammar matrix: An open-source starter-kit
for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proceedings of the Workshop on Grammar Engi-
neering and Evaluation at the 19th International
Conference on Computational Linguistics, pages
8?14, Taipei, Taiwan.
Bender, E. M. and Siegel, M. (2004). Implement-
ing the syntax of Japanese numeral classifiers. In
Proceedings of the IJC-NLP-2004.
Bond, F., Kuribayashi, T., and Hashimoto, C.
(2008). Construction of a free Japanese tree-
bank based on HPSG. In 14th Annual Meeting
of the Association for Natural Language Process-
ing, pages 241?244, Tokyo. (in Japanese).
Callison-Burch, C., Koehn, P., and Osborne, M.
(2006). Improved statistical machine translation
using paraphrases. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 17?24, New York City,
USA. Association for Computational Linguistics.
Callmeier, U. (2000). PET - a platform for exper-
imentation with efficient HPSG processing tech-
niques. Natural Language Engineering, 6(1):99?
108.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual
Meeting of the ACL, Ann Arbor, Michigan. ACL.
Copestake, A. (2002). Implementing Typed Feature
Structure Grammars. CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., and Sag,
I. A. (2005). Minimal Recursion Semantics ? An
Introduction. Research on Language and Compu-
tation, 3:281?332.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28. (Special Issue on Effi-
cient Processing with HPSG).
Gao, Q. and Vogel, S. (2011). Corpus expansion
for statistical machine translation with semantic
role label substitution rules. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, pages 294?298, Portland, Oregon, USA.
Association for Computational Linguistics.
Goodman, M. W. and Bond, F. (2009). Using gen-
eration for grammar analysis and error detection.
In Joint conference of the 47th Annual Meeting
of the Association for Computational Linguistics
and the 4th International Joint Conference on
Natural Language Processing of the Asian Fed-
eration of Natural Language Processing, pages
109?112, Singapore.
He, W., Zhao, S., Wang, H., and Liu, T. (2011).
Enriching smt training data via paraphrasing. In
Proceedings of 5th International Joint Conference
on Natural Language Processing, pages 803?810,
Chiang Mai, Thailand. Asian Federation of Natu-
ral Language Processing.
Koehn, P. (2004). Statistical Significance Tests for
Machine Translation Evaluation. In Proceed-
ings of EMNLP 2004, pages 388?395, Barcelona,
Spain. Association for Computational Linguis-
tics.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,
Federico, M., Bertoldi, N., Cowan, B., Shen, W.,
Moran, C., Zens, R., Dyer, C., Bojar, O., Con-
stantin, A., and Herbst, E. (2007). Moses: Open
Source Toolkit for Statistical Machine Transla-
tion. In Annual Meeting of the ACL.
Marton, Y., Callison-Burch, C., and Resnik, P.
(2009). Improved statistical machine translation
using monolingually-derived paraphrases. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
381?390, Singapore. Association for Computa-
tional Linguistics.
Nichols, E., Bond, F., Appling, D. S., and Mat-
sumoto, Y. (2010). Paraphrasing Training Data
for Statistical Machine Translation. Journal of
Natural Language Processing, 17(3):101?122.
27
Och, F. J. (2003). Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of ACL, pages 160?167.
Och, F. J. and Ney, H. (2003). A Systematic Com-
parison of Various Statistical Alignment Models.
Computational Linguistics, 29:19?51.
Stolcke, A. (2002). SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on
Spoken Language Processing, volume 2, pages
901?904, Denver.
Tanaka, Y. (2001). Compilation of a multilingual
parallel corpus. In Proceedings of PACLING
2001, pages 265?268, Kyushu.
28
Japanese ???????????????
Baseline They played tennis yesterday.
Append + neg LM They do not play tennis yesterday.
Reference Yesterday they didn?t play tennis, because it rained.
(a) Baseline fails to translate the negation.
Japanese ?????????????????????????
Baseline He is sure to break your promise, I?m sure.
Append + neg LM He never breaks his word, I?m sure.
Reference I?m sure he won?t fail to keep his word.
(b) Correct translation by our system with valid variation of wording.
Japanese ??????????????????????
Baseline I was when I came home, he was asleep.
Append + neg LM I came home when he is not asleep.
Reference He wasn?t sleeping when I came home.
(c) Baseline omits the negation.
Japanese ???????????????
Baseline Money with me.
Append + neg LM I don?t have any money with me.
Reference I don?t have any money with me.
(d) Baseline omits subject, verb and negation.
Japanese ???????????????????
Baseline The???? in Japan, I cannot see it.
Append + neg LM The???? in Japan.
Reference The Southern Cross is not to be seen in Japan.
(e) Our system does not translate a part of the sentence.
Japanese ????????????
Baseline Don?t speak in a loud voice.
Append + neg LM You must speak in a loud voice.
Reference You must not speak loudly.
(f) Our system omits the negation.
Japanese ??????????
Baseline She has no friends.
Append + neg LM She is a friend of mine.
Reference She doesn?t have a boy friend.
(g) Our system does not produce a negation. The object is incorrectly trans-
lated in both systems.
Figure 2: Sentences from the neg-strict test set showing differences between the baseline and our best
performing system Append + neg LM. Examples in (a?d) show improvements, (e?g) show degradations.
29
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67?75,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Extracting Semantic Transfer Rules from Parallel Corpora
with SMT Phrase Aligners
Petter Haugereid and Francis Bond
Linguistics and Multilingual Studies
Nanyang Technological University
petterha@ntu.edu.sg bond@ieee.org
Abstract
This paper presents two procedures for ex-
tracting transfer rules from parallel corpora
for use in a rule-based Japanese-English MT
system. First a ?shallow? method where
the parallel corpus is lemmatized before it is
aligned by a phrase aligner, and then a ?deep?
method where the parallel corpus is parsed by
deep parsers before the resulting predicates
are aligned by phrase aligners. In both pro-
cedures, the phrase tables produced by the
phrase aligners are used to extract semantic
transfer rules. The procedures were employed
on a 10 million word Japanese English paral-
lel corpus and 190,000 semantic transfer rules
were extracted.
1 Introduction
Just like syntactic and semantic information finds its
way into SMT models and contribute to improved
quality of SMT systems, rule-based systems bene-
fit from the inclusion of statistical models, typically
in order to rank the output of the components in-
volved. In this paper, we present another way of im-
proving RBMT systems with the help of SMT tools.
The basic idea is to learn transfer rules from paral-
lel texts: first creating alignments of predicates with
the help of SMT phrase aligners and then extracting
semantic transfer rules from these. We discuss two
procedures for creating the alignments. In the first
procedure the parallel corpus is lemmatized before
it is aligned with two SMT phrase aligners. Then
the aligned lemmas are mapped to predicates with
the help of the lexicons of the parsing grammar and
the generating grammar. Finally, the transfer rules
are extracted from the aligned predicates. In the sec-
ond procedure, the parallel corpus is initially parsed
by the parsing grammar and the generating gram-
mar. The grammars produce semantic representa-
tions, which are represented as strings of predicates.
This gives us a parallel corpus of predicates, about
a third of the size of the original corpus, which we
feed the phrase aligners. The resulting phrase tables
with aligned predicates are finally used for extrac-
tion of semantic transfer rules.
The two procedures complement each other. The
first procedure is more robust and thus learns from
more examples although the resulting rules are less
reliable. Here we extract 127,000 semantic transfer
rules. With the second procedure, which is more ac-
curate but less robust, we extract 113,000 semantic
transfer rules. The union of the procedures gives a
total of 190,000 unique rules for the Japanese En-
glish MT system Jaen.
2 Semantic Transfer
Jaen is a rule-based machine translation system em-
ploying semantic transfer rules. The medium for the
semantic transfer is Minimal Recursion Semantics,
MRS (Copestake et al, 2005). The system consists
of the two HPSG grammars: JACY, which is used
for the parsing of the Japanese input (Siegel and
Bender, 2002) and the ERG, used for the generation
of the English output (Flickinger, 2000). The third
component of the system is the transfer grammar,
which transfers the MRS representation produced by
the Japanese grammar into an MRS representation
that the English grammar can generate from: Jaen
(Bond et al, 2011).
At each step of the translation process, the output
67
Source
Language
Analysis ff
MRS
-







Bitext
?







Grammar
??







Treebank
Controller
Reranker ff
MRS
- Target
Language
Generation







Grammar
??







TreebankffSL?TL
Semantic
Transfer
6MRS
?
Interactive Use
6?
Batch Processing
6?
Figure 1: Architecture of the Jaen MT system.
is ranked by stochastic models. In the default con-
figuration, only the 5 top ranked outputs at each step
are kept, so the maximum number of translations is
125 (5x5x5). There is also a final reranking using a
combined model (Oepen et al, 2007).
The architecture of the MT system is illustrated in
Figure 1, where the contribution of the transfer rule
extraction from parallel corpora is depicted by the
arrow going from Bitext to Semantic Transfer.
Most of the rules in the transfer grammar are
simple predicate changing rules, like the rule for
mapping the predicate ?_hon_n_rel? onto the predi-
cate ?_book_v_1_rel?. Other rules are more com-
plex, and transfers many Japanese relations into
many English relations. In all, there are 61 types
of transfer rules, the most frequent being the rules
for nouns translated into nouns (44,572), noun noun
compounds translated into noun noun compounds
(38,197), and noun noun compounds translated into
adjective plus noun (27,679). 31 transfer rule types
have less than 10 instances. The most common rule
types are given in Table 1.1
1Some of the rule types are extracted by only one ex-
traction method. This holds for the types n_adj+n_mtr,
n+n+n_n+n_mtr, n+n_n_mtr, pp+np_np+pp_mtr, and
arg1+pp_arg1+pp_mtr, adj_pp_mtr, and preposition_mtr.
The lemmatized extraction method extracts rules for triple
compounds n+n+n_n+n. This is currently not done with
the semantic extraction method, since a template for a triple
compound would include 8 relations (each noun also has a
quantifier and there are two compound relations in between),
and the number of input relations are currently limited to 5 (but
can be increased). The rest of the templates are new, and they
have so far only been successfully integrated with the semantic
extraction method.
The transfer grammar has a core set of 1,415
hand-written transfer rules, covering function
words, proper nouns, pronouns, time expressions,
spatial expressions, and the most common open
class items. The rest of the transfer rules (190,356
unique rules) are automatically extracted from par-
allel corpora.
The full system is available from http:
//moin.delph-in.net/LogonTop (different
components have different licenses, all are open
source, mainly LGPL and MIT).
3 Two methods of rule extraction
The parallel corpus we use for rule extraction is
a collection of four Japanese English parallel cor-
pora and one bilingual dictionary. The corpora
are the Tanaka Corpus (2,930,132 words: Tanaka,
2001), the Japanese Wordnet Corpus (3,355,984
words: Bond, Isahara, Uchimoto, Kuribayashi, and
Kanzaki, 2010), the Japanese Wikipedia corpus
(7,949,605 words),2 and the Kyoto University Text
Corpus with NICT translations (1,976,071 words:
Uchimoto et al, 2004). The dictionary is Edict
(3,822,642 words: Breen, 2004). The word totals
include both English and Japanese words.
The corpora were divided into into development,
test, and training data. The training data from the
four corpora plus the bilingual dictionary was used
for rule extraction. The combined corpus used for
rule extraction consists of 9.6 million English words
and 10.4 million Japanese words (20 million words
in total).
3.1 Extraction from a lemmatized parallel
corpus
In the first rule extraction procedure we extracted
transfer rules directly from the surface lemmas of
the parallel text. The four parallel corpora were
tokenized and lemmatized, for Japanese with the
MeCab morphological analyzer (Kudo et al, 2004),
and for English with the Freeling analyzer (Padr?
et al, 2010), with MWE, quantities, dates and sen-
tence segmentation turned off. (The bilingual dic-
tionary was not tokenized and lemmatized, since the
entries in the dictionary are lemmas).
2The Japanese-English Bilingual Corpus of Wikipedia?s
Kyoto Articles: http://alaginrc.nict.go.jp/
WikiCorpus/index_E.html.
68
Rule type Hand Lemma Pred Intersect Union Total
noun_mtr 64 32,033 31,575 19,100 44,508 44,572
n+n_n+n_mtr 0 32,724 18,967 13,494 38,197 38,197
n+n_adj+n_mtr 0 22,777 15,406 10,504 27,679 27,679
arg12+np_arg12+np_mtr 0 9,788 1,774 618 10,944 10,944
arg1_v_mtr 22 8,325 1,031 391 8,965 8,987
pp_pp_mtr 2 146 8,584 19 8,711 8,713
adjective_mtr 27 4,914 4,034 2,183 6,765 6,792
arg12_v_mtr 50 4,720 1,846 646 5,920 5,970
n_adj+n_mtr 1 - 4,695 - 4,695 4,696
n+n_n_mtr 0 2,591 3,273 1,831 4,033 4,033
n+n+n_n+n_mtr 0 3,380 - - 3,376 3,376
n+adj-adj-mtr 2 633 2,586 182 3,037 3,039
n_n+n_mtr 1 - 2,229 - 2,229 2,230
pp-adj_mtr 27 1,008 971 1 1,978 2,005
p+n+arg12_arg12_mtr 1 1,796 101 35 1,862 1,863
pp+np_np+pp_mtr 0 - 1,516 - 1,516 1,516
pp+arg12_arg12_mtr 0 852 62 26 888 888
arg1+pp_arg1+pp_mtr 1 - 296 - 296 297
monotonic_mtr 139 - - - - 139
adj_pp_mtr 0 - 112 - 112 112
preposition_mtr 53 - 34 - 34 87
arg123_v_mtr 3 30 14 8 36 39
Table 1: Most common mtr rule types. The numbers in the Hand column show the number of hand-written rules
for each type. The numbers in the Lemma column, show the number of rules extracted from the lemmatized parallel
corpus. The numbers in the Pred column show the number of rules extracted from the semantic parallel corpus. The
Intersect column, shows the number of intersecting rules of Lemma and Pred, and the Union column show the number
of distinct rules of Lemma and Pred.
We then used MOSES (Koehn et al, 2007) and
Anymalign (Lardilleux and Lepage, 2009) to align
the lemmatized parallel corpus. We got two phrase
tables with 10,812,423 and 5,765,262 entries, re-
spectively. MOSES was run with the default set-
tings, and Anymalign ran for approximately 16
hours.
We selected the entries that had (i) a translation
probability, P(English|Japanese) of more than 0.1,3
(ii) an absolute frequency of more than 1,4 (iii) fewer
than 5 lemmas on the Japanese side and fewer than 4
3This number is set based on a manual inspection of the
transfer rules produced. The output for each transfer rule tem-
plate is inspected, and for some of the templates, in particular
the multi-word expression templates, the threshold is set higher.
4The absolute frequency number can, according to Adrien
Lardilleux (p.c.), be thought of as a confidence score. The
larger, the more accurate and reliable the translation probabili-
ties. 1 is the lowest score.
lemmas on the English side,5 and (iv) lexical entries
for all lemmas in Jacy for Japanese and the ERG for
English. This gave us 2,183,700 Moses entries and
435,259 Anymalign entries, all phrase table entries
with a relatively high probability, containing lexical
items known both to the parser and the generator.
The alignments were a mix of one-to-one-or-
many and many-to-one-or-many. For each lemma
in each alignment, we listed the possible predicates
according to the lexicons of the parsing grammar
(Jacy) and the generating grammar (ERG). Since
many lemmas are ambiguous, we often ended up
with many semantic alignments for each surface
alignment. If a surface alignment contained 3 lem-
mas with two readings each, we would get 8 (2x2x2)
semantic alignments. However, some of the seman-
5These numbers are based on the maximal number of lem-
mas needed for the template matching on either side.
69
tic relations associated with a lemma had very rare
readings. In order to filter out semantic alignments
with such rare readings, we parsed the training cor-
pus and made a list of 1-grams of the semantic rela-
tions in the highest ranked output. Only the relations
that could be linked to a lemma with a probability
of more than 0.2 were considered in the semantic
alignment. The semantic alignments were matched
against 16 templates. Six of the templates are simple
one-to-one mapping templates:
1. noun ? noun
2. adjective ? adjective
3. adjective ? intransitive verb
4. intransitive verb ? intransitive verb
5. transitive verb ? transitive verb
6. ditransitive verb ? ditransitive verb
The rest of the templates have more than one
lemma on the Japanese side and one or more lem-
mas on the English side. In all, we extracted 126,964
rules with this method. Some of these are relatively
simple, such as 7 which takes a noun compound and
translates it into a single noun, or 8 which takes a
VP and translates it into a VP (without checking for
compositionality, if it is a common pattern we will
make a rule for it).
7. n+n? n
(1) ?
minor
???-?
test
??-?
had
?
I had a quiz.
8. arg12+np? arg12+np_mtr
(2) ??
that
??-?
job
??-??-?
finished
?
I finished the job.
Other examples, such as 9 are more complex, here
the rule takes a Japanese noun-adjective combina-
tion and translates it to an adjective, with the exter-
nal argument in Japanese (the so-called second sub-
ject) linked to the subject of the English adjective.
Even though we are applying the templates to learn
rules to lemma n-grams, in the translation system
these rules apply to the semantic representation, so
they can apply to a wide variety of syntactic vari-
ations (we give an example of a relative clause be-
low).
9. n+adj? adj
(3) ?-?
previous
?-?
winter
?-?
snow
???-?
much-be
?
Previous winter was snowy.
(4) ?-?
snow
??
much
?
winter
??-?
was
?
It was a snowy winter.
Given the ambiguity of the lemmas used for the
extraction of transfer rules, we were forced to fil-
ter semantic relations that have a low probability in
order to avoid translations that do not generalize.
One consequence of this is that we were not building
rules that should have been built in cases where an
ambiguous lemma has one dominant reading, and
one or more less frequent, but plausible, readings.
Another consequence is that we were building rules
where the dominant reading is used, but where a less
frequent reading is correct. The method is not very
precise since it is based on simple 1-gram counts,
and we are not considering the context of the indi-
vidual lemma. A way to improve the quality of the
assignment of the relation to the lemma would be to
use a tagger or a parser. However, instead of going
down that path, we decided to parse the whole par-
allel training corpus with the parsing grammar and
the generation grammar of the MT system and pro-
duce a parallel corpus of semantic relations instead
of lemmas. In this way, we use the linguistic gram-
mars as high-precision semantic taggers.
3.2 Extraction from a parallel corpus of
predicates
The second rule extraction procedure is based on a
parallel corpus of semantic representations, rather
than lemmatized sentences. We parsed the train-
ing corpus (1,578,602 items) with the parsing gram-
mar (Jacy) and the generation grammar (ERG) of
the MT system, and got a parse with both grammars
for 630,082 items. The grammars employ statistical
models trained on treebanks in order to select the
most probable analysis. For our semantic corpus,
70
we used the semantic representation of the highest
ranked analysis on either side.
The semantic representation produced by the
ERG for the sentence The white dog barks is given in
Figure 2. The relations in the MRSs are represented
in the order they appear in the analysis.6 In the se-
mantic parallel corpus we kept the predicates, e.g.
_the_q_rel, _white_a_1_rel, and so on, but we did
not keep the information about linking. For verbs,
we attached information about the valency. Verbs
that were analyzed as intransitive, like bark in Fig-
ure 2, were represented with a suffix 1x, where 1
indicates argument 1 and x indicates a referential
index: _bark_v_1_rel@1x. If a verb was analyzed
as being transitive or ditransitive, this would be re-
flected in the suffix: _give_v_1_rel@1x2x3x. The
item corresponding to The white dog barks in the se-
mantic corpus would be _the_q_rel _white_a_1_rel
_dog_n_1_rel _bark_v_1_rel@1x.
The resulting parallel corpus of semantic rep-
resentations consists of 4,712,301 relations for
Japanese and 3,806,316 relations for English. This
means that the size of the semantic parallel corpus
is a little more than a third of the lemmatized paral-
lel corpus. The grammars used for parsing are deep
linguistic grammars, and they do not always perform
very well on out of domain data, like for example the
Japanese Wikipedia corpus. One way to increase the
coverage of the grammars would be to include ro-
bustness rules. This would decrease the reliability
of the assignment of semantic relations, but still be
more reliable than simply using 1-grams to assign
the relation.
The procedure for extracting semantic transfer
rules from the semantic parallel corpus is similar
to the procedure for extraction from the lemmatized
corpus. The major difference is that the semantic
corpus is disambiguated by the grammars.
As with the lemmatized corpus, the semantic par-
allel corpus was aligned with MOSES and Anyma-
lign. They produced 4,830,000 and 4,095,744 align-
ments respectively. Alignments with more than 5
relations on either side and with a probability of
less than 0.01 were filtered out.7 This left us with
6Each predicate has the character span of the corresponding
word(s) attached.
7A manual inspection of the rules produced by the template
matching showed that most of the rules produced for several of
4,898,366 alignments, which were checked against
22 rule templates.8 This produced 112,579 rules,
which is slightly fewer than the number of rules
extracted from the lemmatized corpus (126,964).
49,187 of the rules overlap with the rules extracted
from the lemmatized corpus, which gives us a total
number of unique rules of 190,356. The distribution
of the rules is shown in Table 1.
Some of the more complex transfer
rules types like p+n+arg12_arg12_mtr and
pp+arg12_arg12_mtr were extracted in far greater
numbers from the lemmatized corpus than from
the corpus of semantic representations. This is
partially due to the fact that the method involving
the lemmatized corpus is more robust, which means
that the alignments are done on 3 times as much
data as the method involving the corpus of semantic
predicates. Another reason is that the number
of items that need to be aligned to match these
kinds of multi-word templates is larger when the
rules are extracted from the corpus of semantic
representations. (For example, a noun relation
always has a quantifier binding it, even if there is no
particular word expressing the quantifier.) Since the
number of items to be aligned is bigger, the chance
of getting an alignment with a high probability that
matches the template becomes smaller.
One of the transfer rule templates (pp_pp_mtr)
generates many more rules with the method in-
volving the semantic predicates than the method
involving lemmas. This is because we restricted
the rule to only one preposition pair (_de_p_rel
? _by_p_means_rel) with the lemmatized corpus
method, while all preposition pairs are accepted with
the semantic predicate method since the confidence
in the output of this method is higher.
4 Experiment and Results
In order to compare the methods for rule extraction,
we made three versions of the transfer grammar, one
including only the rules extracted from the lemma-
the templates were good, even with a probability as low as 0.01.
For some of the templates, the threshold was set higher.
8The reason why the number of rule templates is higher with
this extraction method, is that the confidence in the results is
higher. This holds in particular for many-to-one rules, were the
quality of the rules extracted with from the lemmatized corpus
is quite low.
71
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
mrs
LTOP h1 h
INDEX e2 e
RELS
?
?
?
?
?
?
?
?
_the_q_rel<0:3>
LBL h3 h
ARG0 x5 x
RSTR h6 h
BODY h4 h
?
?
?
?
?
?
?
,
?
?
?
?
?
_white_a_1_rel<4:9>
LBL h7 h
ARG0 e8 e
ARG1 x5
?
?
?
?
?
,
?
?
?
_dog_n_1_rel<10:13>
LBL h7
ARG0 x5
?
?
?,
?
?
?
?
?
_bark_v_1_rel<14:20>
LBL h9 h
ARG0 e2
ARG1 x5
?
?
?
?
?
?
HCONS
?
?
?
?
qeq
HARG h6
LARG h7
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: MRS of The white dog barks
tized corpus (Lemm), one including only the rules
extracted from the corpus of semantic representa-
tions (Pred), and one including the union of the two
(Combined). In the Combined grammar, the Lemm
rules with a probability lower than 0.4 were filtered
out if the input relation(s) are already translated by
either handwritten rules or Pred rules since the con-
fidence in the Lemm rules is lower.
Since the two methods for rule extraction involve
different sets of templates, we also made two ver-
sions of the transfer grammar including only the 15
templates used in both Lemm and Pred. These were
named LemmCore and PredCore.
The five versions of the transfer grammar were
tested on sections 003, 004, and 005 of the Tanaka
Corpus (4,500 test sentences), and the results are
shown in Table 2. The table shows how the ver-
sions of Jaen performs with regard to parsing (con-
stant), transfer, generation, and overall coverage. It
also shows the NEVA9 scores of the highest ranked
translated sentences (NEVA), and the highest NEVA
score of the 5 highest ranked translations (Oracle).
The F1 is calculated based on the overall coverage
and the NEVA.
The coverage of Lemm and Pred is the same;
20.8%, but Pred gets a higher NEVA score than
Lemm (21.11 vs. 18.65), and the F1 score is one
percent higher. When the Lemm and Pred rules are
combined in Combined, the coverage is increased
by almost 6%. This increase is due to the fact that
the Lemm and Pred rule sets are relatively compli-
9NEVA (N-gram EVAluation: Forsbom (2003)) is a modi-
fied version of BLEU.
mentary. Although the use of the Lemm and Pred
transfer grammars gives the same coverage (20.8%),
only 648 (14.4%) of the test sentences are translated
by both systems. The NEVA score of Combined is
between that of Lemm and Pred while the F1 score
beats both Lemm and Pred.
When comparing the core versions of Lemm and
Pred, LemmCore and PredCore, we see the same
trend, namely that coverage is about the same and
the NEVA score is higher when the Pred rules are
used.
644 of the test sentences were translated by all
versions of the transfer grammar (Lemm, Pred, and
Combined). Table 3 shows how the different ver-
sions of Jaen perform on these sentences. The re-
sults show that the quality of the transfer rules ex-
tracted from the MRS parallel corpus is higher than
the quality of the transfer rules based on the lemma-
tized parallel corpus. It also shows that there is a
small decrease of quality when the rules from the
lemmatized parallel corpus are added to the rules
from the MRS corpus.
Version NEVA
Lemmatized 20.44
MRS 23.55
Lemma + MRS 23.04
Table 3: NEVA scores of intersecting translations
The two best-performing versions of JaEn, Pred
and Combined, were compared to MOSES (see Ta-
ble 4 and Table 5). The BLEU scores were calcu-
lated with multi-bleu.perl, and the METEOR
72
Parsing Transfer Generation Overall NEVA Oracle F1
LemmCore 3590/4500 1661/3590 930/1661 930/4500 18.65 22.99 19.61
79.8% 46.3% 56.0% 20.7%
Lemm 3590/4500 1674/3590 938/1674 938/4500 18.65 22.99 19.69
79.8% 46.6% 56.0% 20.8%
PredCore 3590/4500 1748/3590 925/1748 925/4500 20.40 24.81 20.48
79.8% 48.7% 52.9% 20.6%
Pred 3590/4500 1782/3589 937/1782 937/4500 21.11 25.75 20.96
79.8% 49.7% 52.6% 20.8%
Combined 3590/4500 2184/3589 1194/2184 1194/4500 19.77 24.00 22.66
79.8% 60.9% 54.7% 26.5%
Table 2: Evaluation of the Tanaka Corpus Test Data
scores were calculated with meteor-1.3.jar
using default settings.10 The human score is a direct
comparison, an evaluator11 was given the Japanese
source, a reference translation and the output from
the two systems, randomly presented as A or B.
They then indicated which they preferred, or if the
quality was the same (in which case each system
gets 0.5). All the translations, including the refer-
ence translations, were tokenized and lower-cased.
In both comparisons, MOSES gets better BLEU and
METEOR scores, while the Jaen translation is pre-
ferred by the human evaluator in 58 out of 100 cases.
BLEU METEOR HUMAN
JaEn First 16.77 28.02 58
MOSES 30.19 31.98 42
Table 4: BLEU Comparison of Jaen loaded with the
Combined rules, and MOSES (1194 items)
BLEU METEOR HUMAN
JaEn 18.34 29.02 58
MOSES 31.37 32.14 42
Table 5: BLEU Comparison of Jaen loaded with the Pred
rules, and MOSES (936 items)
The two systems make different kinds of mis-
takes. The output of Jaen is mostly grammatical,
10The METEOR evaluation metric differs from BLEU in that
it does not only give a score for exact match, but it also gives
partial scores for stem, synonym, and paraphrase matches.
11A Japanese lecturer at NTU, trilingual in English, Japanese
and Korean, not involved in the development of this system, but
with experience in Japanese/Korean MT research.
but it may not always make sense. An example of a
nonsense translation from Jaen is given in (5).12
(5) S: ???????????
R: We eat fish raw.
M: We eat fish raw.
J: We eat fish in the camcorder.
Jaen sometimes gets the arguments wrong:
(6) S: ???????????
R: He was elected president.
M: He was elected president.
J: The president chose him.
The output of Moses on the other hand is more
likely to lack words in the translation, and it is also
more likely to be ungrammatical. A translation with
a missing word is shown in (7).
(7) S: ??????????????
R: The curtains were drawn slowly.
M: The curtain was slowly.
J: The curtain was drawn slowly.
Missing words become extra problematic when a
negation is not transferred:
(8) S: ????????????
R: We shouldn?t have any prejudice.
M: You should have a bias.
J: I shouldn?t have prejudice.
Sometimes the Moses output is lacking so many
words that it is impossible to follow the meaning:
12The examples below are taken from the development data
of the Tanaka Corpus. ?S? stands for ?Source?, ?R? stands for
?Reference translation?, ?M? stands for ?Moses translation,? and
?J? stands for ?Jaen translation.?
73
(9) S: ???????????????
R: Our brains control our activities.
M: The brain to us.
J: The brain is controlling our activities.
Also the output of Moses is more likely to be un-
grammatical, as illustrated in (10) and (11).
(10) S: ?????????????
R: I have a deep love for Japan.
M: I is devoted to Japan.
J: I am deeply loving Japan.
(11) S: ?????????????
R: She wrung the towel dry.
M: She squeezed pressed the towel.
J: She wrung the towel hard.
5 Discussion
In order to get a system with full coverage, Jaen
could be used with Moses as a fallback. This would
combine the precision of the rule-based system with
the robustness of Moses. The coverage and the qual-
ity of Jaen itself can be extended by using more
training data. Our experience is that this holds even
if the training data is from a different domain. By
adding training data, we are incrementally adding
rules to the system. We still build the rules we built
before, plus some more rules extracted from the new
data. Learning rules that are not applicable for the
translation task does not harm or slow down the sys-
tem. Jaen has a rule pre-selection program which,
before each translation task selects the applicable
rules. When the system does a batch translation of
1,500 sentences, the program selects about 15,000 of
the 190,000 automatically extracted rules, and only
these will be loaded. Rules that have been learned
but are not applicable are not used.13
We can also extend the system by adding more
transfer templates. So far, we are using 23 templates,
and by adding new templates for multi-word expres-
sions, we can increase the precision.
The predicate alignments produced from the par-
allel corpus of predicates are relatively precise since
the predicates are assigned by the grammars. This
allows us to extract transfer rules from alignments
13The pre-selection program speeds up the system by a factor
of three.
that are given a low probability (down to 0.01) by
the aligner.
We would also like to get more from the data we
have, by making the parser more robust. Two ap-
proaches that have been shown to work with other
grammars is making more use of morphological in-
formation (Adolphs et al, 2008) or adding robust-
ness rules (Cramer and Zhang, 2010).
6 Conclusion
We have shown how semantic transfer rules can be
learned from parallel corpora that have been aligned
in SMT phrase tables. We employed two strategies.
The first strategy was to lemmatize the parallel cor-
pus and use SMT aligners to create phrase tables of
lemmas. We then looked up the relations associated
with the lemmas using the lexicons of the parser and
generator. This gave us a phrase table of aligned
relations. We were able to extract 127,000 rules
by matching the aligned relations with 16 semantic
transfer rule templates.
The second strategy was to parse the parallel cor-
pus with the parsing grammar and the generating
grammar of the MT system. This gave us a paral-
lel corpus of predicates, which, because of lack of
coverage of the grammars, was about a third the size
of the full corpus. The parallel corpus of predicates
was aligned with SMT aligners, and we got a sec-
ond phrase table of aligned relations. We extracted
113,000 rules by matching the alignments against 22
rule templates. These transfer rules produced the
same number of translation as the rules produced
with the first strategy (20.8%), but they proved to
be more precise.
The two rule extraction methods complement
each other. About 30% of the sentences translated
with one rule set are not translated by the other. By
merging the two rule sets into one, we increased the
coverage of the system to 26.6%. A human evalua-
tor preferred Jaen?s translation to that of Moses for
58 out of a random sample of 100 translations.
References
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Dan Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natu-
ral language parsing. In European Language Re-
74
sources Association (ELRA), editor, Proceedings
of the Sixth International Language Resources
and Evaluation (LREC?08), pages 1380?1387.
Marrakech, Morocco.
Francis Bond, Hitoshi Isahara, Kiyotaka Uchimoto,
Takayuki Kuribayashi, and Kyoko Kanzaki. 2010.
Japanese WordNet 1.0. In 16th Annual Meeting of
the Association for Natural Language Processing,
pages A5?3. Tokyo.
Francis Bond, Stephan Oepen, Eric Nichols, Dan
Flickinger, Erik Velldal, and Petter Haugereid.
2011. Deep open source machine transla-
tion. Machine Translation, 25(2):87?105.
URL http://dx.doi.org/10.1007/
s10590-011-9099-4, (Special Issue on
Open source Machine Translation).
James W. Breen. 2004. JMDict: a Japanese-
multilingual dictionary. In Coling 2004 Workshop
on Multilingual Linguistic Resources, pages 71?
78. Geneva.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Seman-
tics. An introduction. Research on Language and
Computation, 3(4):281?332.
Bart Cramer and Yi Zhang. 2010. Constraining
robust constructions for broad-coverage parsing
with precision grammars. In Proceedings of
COLING-2010, pages 223?231. Beijing.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28. (Special Issue on Effi-
cient Processing with HPSG).
Eva Forsbom. 2003. Training a super model look-
alike: Featuring edit distance, n-gram occurrence,
and one reference translation. In In Proceedings
of the Workshop on Machine Translation Evalua-
tion. Towards Systemizing MT Evaluation.
Philipp Koehn, Wade Shen, Marcello Federico,
Nicola Bertoldi, Chris Callison-Burch, Brooke
Cowan, Chris Dyer, Hieu Hoang, Ondrej Bo-
jar, Richard Zens, Alexandra Constantin, Evan
Herbst, Christine Moran, and Alexandra Birch.
2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of the ACL
2007 Interactive Presentation Sessions. Prague.
URL http://www.statmt.org/moses/.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 230?237. Association for Computa-
tional Linguistics, Barcelona, Spain.
Adrien Lardilleux and Yves Lepage. 2009.
Sampling-based multilingual alignment. In
Proceedings of Recent Advances in Natural
Language Processing (RANLP 2009), pages
214?218. Borovets, Bulgaria.
Stephan Oepen, Erik Velldal, Jan Tore L?nning,
Paul Meurer, and Victoria Rosen. 2007. Towards
hybrid quality-oriented machine translation. on
linguistics and probabilities in MT. In 11th Inter-
national Conference on Theoretical and Method-
ological Issues in Machine Translation: TMI-
2007, pages 144?153.
Llu?s Padr?, Miquel Collado, Samuel Reese, Ma-
rina Lloberes, and Irene Castell?n. 2010. Freel-
ing 2.1: Five years of open-source language pro-
cessing tools. In Proceedings of 7th Language
Resources and Evaluation Conference (LREC
2010). La Valletta. (http://nlp.lsi.upc.
edu/freeling.
Melanie Siegel and Emily M. Bender. 2002. Effi-
cient deep processing of Japanese. In Proceed-
ings of the 3rd Workshop on Asian Language Re-
sources and International Standardization at the
19th International Conference on Computational
Linguistics, pages 1?8. Taipei.
Yasuhito Tanaka. 2001. Compilation of a multilin-
gual parallel corpus. In Proceedings of PACLING
2001, pages 265?268. Kyushu. (http:
//www.colips.org/afnlp/archives/
pacling2001/pdf/tanaka.pdf).
Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo,
Masaki Murata, Satoshi Sekine, and Hitoshi
Isahara. 2004. Multilingual aligned paral-
lel treebank corpus reflecting contextual in-
formation and its applications. In Gilles
S?rasset, editor, COLING 2004 Multilingual
Linguistic Resources, pages 57?64. COLING,
Geneva, Switzerland. URL http://acl.
ldc.upenn.edu/W/W04/W04-2208.bib.
75
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 149?158,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Developing Parallel Sense-tagged Corpora with Wordnets
Francis Bond, Shan Wang,
Eshley Huini Gao, Hazel Shuwen Mok, Jeanette Yiwen Tan
Linguistics and Multilingual Studies, Nanyang Technological University
bondieee.org
Abstract
Semantically annotated corpora play an
important role in natural language pro-
cessing. This paper presents the results
of a pilot study on building a sense-tagged
parallel corpus, part of ongoing construc-
tion of aligned corpora for four languages
(English, Chinese, Japanese, and Indone-
sian) in four domains (story, essay, news,
and tourism) from the NTU-Multilingual
Corpus. Each subcorpus is first sense-
tagged using a wordnet and then these
synsets are linked. Upon the completion
of this project, all annotated corpora will
be made freely available. The multilingual
corpora are designed to not only provide
data for NLP tasks like machine transla-
tion, but also to contribute to the study
of translation shift and bilingual lexicogra-
phy as well as the improvement of mono-
lingual wordnets.
1 Introduction
Large scale annotated corpora play an essen-
tial role in natural language processing (NLP).
Over the years with the efforts of the commu-
nity part-of-speech tagged corpora have achieved
high quality and are widely available. In com-
parison, due to the complexity of semantic an-
notation, sense tagged parallel corpora develop
slowly. However, the growing demands in more
complicated NLP applications such as informa-
tion retrieval, machine translation, and text sum-
marization suggest that such corpora are in great
need. This trend is reflected in the construc-
tion of two types of corpora: (i) parallel cor-
pora: FuSe (Cyrus, 2006), SMULTRON (Volk
et al, 2010), CroCo ( ?Culo et al, 2008), German-
English parallel corpus (Pad? and Erk, 2010), Eu-
roparl corpus (Koehn, 2005), and OPUS (Ny-
gaard and Tiedemann, 2003; Tiedemann and Ny-
gaard, 2004; Tiedemann, 2009, 2012) and (ii)
sense-tagged monolingual corpora: English cor-
pora such as Semcor (Landes et al, 1998); Chi-
nese corpora, such as the crime domain of Sinica
Corpus 3.0 (Wee and Mun, 1999), 1 million word
corpus of People?s Daily (Li et al, 2003), three
months? China Daily (Wu et al, 2006); Japanese
corpora, such as Hinoki Corpus (Bond et al, 2008)
and Japanese SemCor (Bond et al, 2012) and
Dutch Corpora such as the Groningen Meaning
Bank (Basile et al, 2012). Nevertheless, almost no
parallel corpora are sense-tagged. With the excep-
tion of corpora based on translations of SemCor
(Bentivogli et al, 2004; Bond et al, 2012) sense-
tagged corpora are almost always monolingual.
This paper describes ongoing work on the con-
struction of a sense-tagged parallel corpus. It com-
prises four languages (English, Chinese, Japanese,
and Indonesian) in four domains (story, essay,
news, and tourism), taking texts from the NTU-
Multilingual Corpus (Tan and Bond, 2012). For
these subcorpora we first sense tag each text
monolingually and then link the concepts across
the languages. The links themselves are typed and
tell us something of the nature of the translation.
The annotators are primarily multilingual students
from the division of linguistics and multilingual
studies (NTU) with extensive training. In this pa-
per we introduce the planned corpus annotation
and report on the results of a completed pilot: an-
notation and linking of one short story: The Ad-
venture of the Dancing Men in Chinese, English
and Japanese. All concepts that could be were
aligned and their alignments annotated.
The paper is structured as follows. Section 2
reviews existing parallel corpora and sense tagged
corpora that have been built. Section 3 introduces
the resources that we use in our annotation project.
The annotation scheme for the multilingual cor-
pora is laid out in Section 4. In Section 5 we report
149
in detail the results of our pilot study. Section 6
presents our discussion and future work.
2 Related Work
In recent years, with the maturity of part-of-speech
(POS) tagging, more attention has been paid to
the practice of getting parallel corpora and sense-
tagged corpora to promote NLP.
2.1 Parallel Corpora
Several research projects have reported annotated
parallel corpora. Among the first major efforts in
this direction is FuSe (Cyrus, 2006), an English-
German parallel corpus extracted from the EU-
ROPARL corpus (Koehn, 2005). Parallel sen-
tences were first annotated mono-lingually with
POS tags and lemmas; related predicates (e.g.
a verb and its nominalization are then linked).
SMULTRON (Volk et al, 2010) is a parallel tree-
bank of 2,500 sentences from different genres:
a novel, economy texts from several sources, a
user manual and mountaineering reports. Most
of the corpus is German-English-Swedish paral-
lel text, with additional texts in French and Span-
ish. CroCo ( ?Culo et al, 2008) is a German-
English parallel and comparable corpus of a dozen
texts from eight genres, totaling approximately
1,000,000 words. Each sentence is annotated with
phrase structures and grammatical functions, and
words, chunks and phrases are aligned across par-
allel sentences. This resource is limited to two
languages, English and German, and is not sys-
tematically linked to any semantic resource. Pad?
and Erk (2010) have conducted a study of transla-
tion shifts on a German-English parallel corpus of
1,000 sentences from EUROPARL annotated with
semantic frames from FrameNet and word align-
ments. Their aim was to measure the feasibility of
frame annotation projection across languages.
The above corpora have been used for study-
ing translation shift. Plain text parallel corpora are
also widely used in NLP. The Europarl corpus col-
lected the parallel text in 11 official languages of
the European Union (i.e. Danish, German, Greek,
English, Spanish, Finnish, French, Italian, Dutch,
Portuguese, and Swedish) from proceedings of the
European Parliament. Each language is composed
of about 30 million words (Koehn, 2005). Newer
versions have even more languages. OPUS v0.1
contains the documentation of the office package
OpenOffice with a collection of 2,014 files in En-
glish and five translated texts, namely, French,
Spanish, Swedish, German and Japanese. This
corpus consists of 2.6 million words (Nygaard and
Tiedemann, 2003; Tiedemann and Nygaard, 2004;
Tiedemann, 2012). However, when we examined
the Japanese text, we found the translations are of-
ten from different versions of the software and not
synchronized very well.
2.2 Sense Tagged Corpora
Surprisingly few languages have sense tagged cor-
pora. In English, Semcor was built by annotat-
ing texts from the Brown Corpus using the sense
inventory of WordNet 1.6 (Fellbaum, 1998) and
has been mapped to subsequent WordNet versions
(Landes et al, 1998). The Defense Science Or-
ganization (DSO) corpus annotated the 191 most
frequent and ambiguous nouns and verbs from the
combined Brown Corpus and Wall Street Journal
Corpus using WordNet 1.5. The 191 words com-
prise of 70 verbs with an average sense number of
12 and 121 nouns with an average sense number
of 7.8. The verbs and nouns respectively account
for approximately 20% of all verbs and nouns in
any unrestricted English text (Ng and Lee, 1996).
The WordNet Gloss Disambiguation Project uses
Princeton WordNet 3.0 (PWN) to disambiguate its
own definitions and examples.1
In Chinese, Wee and Mun (1999) reported the
annotation of a subset of Sinica Corpus 3.0 using
HowNet. The texts are news covering the crime
domain with 30,000 words. Li et al (2003) an-
notated the semantic knowledge of a 1 million
word corpus from People?s Daily with dependency
grammar. The corpus include domains such as
politics, economy, science, and sports. (Wu et al,
2006) described the sense tagged corpus of Peking
University. They annotated three months of the
People?s Daily using the Semantic Knowledge-
base of Contemporary Chinese (SKCC)2. SKCC
describes the features of a word through attribute-
value pairs, which incorporates distributional in-
formation.
In Japanese, the Hinoki Corpus annotated 9,835
headwords with multiple senses in Lexeed: a
Japanese semantic lexicon (Kasahara et al, 2004)
To measure the conincidence of tags and difficulty
degree in identifying senses, each word was anno-
tated by 5 annotators (Bond et al, 2006).
1
http://wordnet.prineton.edu/glosstag.
shtml
2
http://l.pku.edu.n/l_sem_dit/
150
We only know of two multi-lingual sense-
tagged corpora. One is MultiSemCor, which is
an English/Italian parallel corpus created based
on SemCor (Landes et al, 1998). MultiSemCor
is made of 116 English texts taken from SemCor
with their corresponding 116 Italian translations.
There are 258,499 English tokens and 267,607
Italian tokens. The texts are all aligned at the word
level and content words are annotated with POS,
lemma, and word senses. It has 119,802 English
words semantically annotated from SemCor and
92,820 Italian words are annotated with senses au-
tomatically transferred from English (Bentivogli
et al, 2004). Japanese SemCor is another transla-
tion of the English SemCor, whose senses are pro-
jected across from English. It takes the same texts
in MultiSemCor and translates them into Japanese.
Of the 150,555 content words, 58,265 are sense
tagged either as monosemous words or by pro-
jecting from the English annotation (Bond et al,
2012). The low annotation rate compared to Mul-
tiSemCor reflects both a lack of coverage in the
Japanese wordnet and the greater typological dif-
ference.
Though many efforts have been devoted to the
construction of sense tagged corpora, the major-
ity of the existing corpora are monolingual, rel-
atively small in scale and not all freely available.
To the best of our knowledge, no large scale sense-
tagged parallel corpus for Asian languages exists.
Our project will fill this gap.
3 Resources
This section introduces the wordnets and corpora
we are using for the annotation task.
3.1 Wordnets
Princeton WordNet (PWN) is an English lexical
database created at the Cognitive Science Labo-
ratory of Princeton University. It was developed
from 1985 under the direction of George A. Miller.
It groups nouns, verbs, adjective and adverbs into
synonyms (synsets), most of which are linked to
other synsets through a number of semantic rela-
tions. (Miller, 1998; Fellbaum, 1998). The version
we use in this study is 3.0.
A number of wordnets in various languages
have been built based on and linked to PWN. The
Open Multilingual Wordnet (OMW) project3 cur-
3
http://www.asta-net.jp/~kuribayashi/
multi/
rently provides 22 wordnets (Bond and Paik, 2012;
Bond and Foster, 2013). The Japanese and Indone-
sian wordnets in our project are from OMW pro-
vided by the creators (Isahara et al, 2008, Nurril
Hirfana et al, 2011).
The Chinese wordnet we use is a heavily re-
vised version of the one developed by Southeast
University (Xu et al, 2008). This was automat-
ically constructed from bilingual resources with
minimal hand-checking. It has limited coverage
and is somewhat noisy, we have been revising it
and use this revised version for our annotation.
3.2 Multilingual Corpus
The NTU-multilingual corpus (NTU-MC) is com-
piled at Nanyang Technological University. It
contains eight languages: English (eng), Man-
darin Chinese (cmn), Japanese (jpn), Indonesian
(ind), Korean, Arabic, Vietnamese and Thai (Tan
and Bond, 2012). We selected parallel data
for English, Chinese, Japanese, and Indonesian
from NTU-MC to annotate. The data are from
four genres, namely, short story (two Sherlock
Holmes? Adventures), essay (Raymond, 1999),
news (Kurohashi and Nagao, 2003) and tourism
(Singapore Tourist Board, 2012). The corpus sizes
are shown in Table 1. We show the number of
words and concepts (open class words tagged with
synsets) only for English, the other languages are
comparable in size.
4 Annotation Scheme for Multilingual
Corpora
The annotation task is divided into two phases:
monolingual sense annotation and multilingual
concept alignment.
4.1 Monolingual Sense Annotation
First, the Chinese, Japanese and Indonesian cor-
pora were automatically tokenized and tagged
with parts-of-speech. Secondly, concepts were
tagged with candidate synsets, with multiword ex-
pressions allowing a skip of up to 3 words. Any
match with a wordnet entry was considered a po-
tential concept.
These were then shown to annotators to either
select the appropriate synset, or point out a prob-
lem. The interface for doing sense annotation is
shown in Figure 1.
In Figure 1, the concepts to be annotated are
shown as red and underlined. When clicking on
151
Genre Text Sentences Words Concepts
Eng Cmn Jpn Ind Eng Eng
Story The Adventure of the Dancing Men 599 606 698 ? 11,200 5,300
The Adventure of the Speckled Band 599 612 702 ? 10,600 4,700
Essay The Cathedral and the Bazaar 769 750 773 ? 18,700 8,800
News Mainichi News 2,138 2,138 2,138 ? 55,000 23,200
Tourism Your Singapore (web site) 2,988 2,332 2,723 2,197 74,300 32,600
Table 1: Multilingual corpus size
Figure 1: Tagging the sense of cane.
a concept, its WordNet senses appear to the right
of a screen. The annotator chooses between these
senses or a number of meta-tags: e, s, m, p, u.
Their meaning is explained below.
e error in tokenization
??? should be??
three-toed should be three - toed
s missing sense (not in wordnet)
I program in python ?the computer language?
COMMENT: add link to existing synset
<06898352-n ?programming language?
m bad multiword
(i) if the lemma is a multiword, this tag means
it is not appropriate
(ii) if the lemma is single-word, this tag
means it should be part of a multiword
p POS that should not be tagged (article,
modal, preposition, . . . )
u lemma not in wordnet but POS open class
(tagged automatically)
COMMENT: add or link to existing synset
Missing senses in the wordnets were a major
issue when tagging, especially for Chinese and
Japanese. We allowed the annotators to add candi-
date new senses in the comments; but these were
not made immediately available in the tagging in-
terface. As almost a third of the senses were miss-
ing in Chinese and Japanese, this slowed the anno-
tators down considerably.
Our guidelines for adding new concepts or link-
ing words to existing cover four cases:
= When a word is a synonym of an exist-
ing word, add =synset to the comment:
e.g. for laidback, it is a synonym of
02408011-a ?laid-back, mellow?, so we add
=02408011-a to the comment for laidback.
< When a word is a hyponym/instance of
152
an existing word, mark it with <synset:
For example, python is a hyponym of
06898352-n programming language, so we
add <06898352-n to python
! Mark antonyms with !synset.
? If you cannot come up with a more specific
relationship, just say the word is related in
some way to an existing synset with ?synset;
and add more detail in the comment.
Finally, we have added more options for the
annotators: prn (pronouns) and seven kinds of
named entities: org (organization); loc (location);
per (person); dat (date/time); num (number); oth
(other) and the super type nam (name). These ba-
sically follow Landes et al (1998, p207), with the
addition of number, date/time and name. Name is
used when automatically tagging, it should be spe-
cialized later, but is useful to have when aligning.
Pronouns include both personal and indefinite-
pronouns. Pronouns are not linked to their mono-
lingual antecedents, just made available for cross-
lingual linking.
4.2 Multilingual Concept Alignment
We looked at bitexts: the translated text and its
source (in this case English). Sentences were
already aligned as part of the NTU-Multilingual
Corpus. The initial alignment was done automat-
ically: concepts that are tagged with the same
synset or related synsets (one level of hyponymy)
are directly linked. Then the sentence pairs are
presented to the annotator, using the interface
shown in Figure 2.
In the alignment interface, when you hover over
a concept, its definition from PWN is shown in a
pop-up window at the top. Clicking concepts in
one language and then the other produces a can-
didate alignment: the annotator then choses the
kind of alignment. After concepts are aligned they
are shown in the same color. Both bell and ?
? m?nl?ng ?door bell? have the same synset, so
they are linked with =. Similarly, Watson and ?
? Hu?she?ng ?Watson? refer to the same person,
so they are also connected with =. However, ring
in the English sentence is a noun while the corre-
sponding Chinese word? xia?ng ?ring? is a verb;
so they are linked with the weaker type ?.
We found three issues came up a lot during the
annotation: (i) Monolingual tag errors; (ii) mul-
tiword expression not tagged; (iii) Pronouns not
tagged.
(i) In some cases, the monolingual tag was
not the best choice. Looking at the tagging in
both languages often made it easier to choose be-
tween similar monolingual tags, and the annota-
tors found themselves wanting to retag a number
of entries.
(ii) It was especially common for it to become
clear that things should have been tagged as mul-
tiword expressions. Consider kuchi-wo hiraku
?speak? in (1).
(1) Said he suddenly
a. ????
ho-muzu
Holmes
?
ga
NOM
??
totsuzen
suddenly
?
kuchi
mouth
?
wo
ACC
??
hiraku
open
?Holmes opens his mouth suddenly?
This was originally tagged as ?open mouth? but
in fact it is a multiword expression with the mean-
ing ?say?, and is parallel in meaning to the original
English text. As this concept is lexicalized, the an-
notator grouped the words together and tagged the
new concept to the synset 00941990-v ?express in
speech?. The concepts were then linked together
with ?. It is hard for the monolingual annotator
to consistently notice such multiword expressions:
however, the translation makes them more salient.
(iii) It was often the case that an open class
word in one language would link to a closed class
word in the other, especially to a pronoun. We
see this in (1) where he in English links to ho-
muzu ?Holmes? in Japanese. In order to capture
these correspondences, we allowed the annotator
to also tag named entities, pronouns and interrog-
atives. From now on we will tag these as part of
the initial monolingual alignment.
We tagged the links between concepts with the
types shown in Table 2.
5 Pilot Study Results
A pilot study was conducted using the first story
text: The Adventure of the Dancing Men, a Sher-
lock Holmes short story (Conan Doyle, 1905).
The Japanese version was translated by Otokichi
Mikami and Yu Okubu;4 we got the translated ver-
sion of Chinese from a website which later disap-
peared. Using English text as the source language,
the Japanese and Chinese texts were aligned and
4
http://www.aozora.gr.jp/ards/000009/
ard50713.html
153
Figure 2: Interface for aligning concepts.
manually sense-tagged with reference to their re-
spective wordnets. The number of words and con-
cepts for each language is shown in Table 3.
English Chinese Japanese
Sentences 599 680 698
Words 11,198 11,325 13,483
Concepts 5,267 4,558 4,561
Excluding candidate concepts rejected by the annotators.
Table 3: Concepts in Dancing Men
The relationships between words were tagged
using the symbols in in Table 2. The difficult cases
are similar relation and translation equivalent rela-
tion. Due to translation styles and language diver-
gence, some concepts with related meaning can-
not be directly linked. We give examples in (2)
through (4).
(2) ?How on earth do you know that?? I asked.
a. ?
?
?
????
ittai
on+earth
?
?
,
????
doushite
why
??
sono
that
??=?
koto=wo
thing=ACC
?
?
?
?
?
?
?
to
QUOT
?=?
watashi=wa
me=TOP
??=??
kiki=kaesu
ask=return
?Why on earth do you know that thing?? I ask
in return.
In (2), compared to ask in English, the Japanese
kikikaesu has the additional meaning of ?in re-
turn?: it is a hyponym. We marked their relation
as ? (similar in meaning).
We introduced a new class ? to indicate com-
binations of words or phrases that are translation
equivalents of the original source but are not lex-
icalized enough to be linked in the wordnet. One
example is shown in (3).
(3) be content with my word
154
Type Example
= same concept say ??? iu ?say?
? hypernym wash ?????? araiotosu ?wash out?
?2 2nd level dog ??? doubutsu ?animal?
? hyponym sunlight ?? hikari ?light?
?n nth level
? similar notebook ???? memochou ?notepad?
dulla ???? kusumu ?darken?
? equivalent be content with my word ?
??????????-? ?believe in my words?
! antonym hot ???=?? samu=ku nai ?not cold?
# weak ant. not propose to invest ?
?????? omoi=todomaru ?hold back?
Table 2: Translation Equivalence Types
a. ????=?
watakushi=no
me=of
??=?
kotoba=wo
word=ACC
??=?
shinji=te
believe=ing
?believe in my words?
In this case shinjite ?believe? is being used to
convey the same pragmatic meaning as content
with but they are not close enough in meaning that
we want to link them in the lexicon.
(4) shows some further issues in non-direct
translation.
(4) I am sure that I shall sayh noithing j of the kindk .
a. ????
iyaiya
by+no+means
?
,
,
???
sonnak
that+kindk+of
??
koto j
thing j
?
wa
TOP
??-?
iwah-ni
sayh-NEGi
?
yo
yo
?no no, I will not say that kind of thing?
Sayh noithing j of the kindk becomes roughly
?noti sayh that kindk of thing j?. All the elements
are there, but they are combined in quite a different
structure and some semantic decomposition would
be needed to link them. Chinese and Japanese do
not use negation inside the NP, so this kind of dif-
ference is common. Tagging was made more com-
plicated by the fact that determiners are not part of
wordnet, so it is not clear which parts of the ex-
pression should be tagged.
Though there are many difficult cases, the most
common case was for two concepts to share the
same synset and be directly connected. For
example, notebook is tagged with the synset
06415419-n, defined as ?a book with blank pages
for recording notes or memoranda?. In the
Japanese version, this concept is translated into?
?? bibouroku ?notebook?, with exactly the same
synset (06415419-n). Hence, we linked the words
with the = symbol.
The number of link types after the first round
of cross-lingual annotation (eng-jpn, eng-cmn) is
summarized in Table 4. In the English-Japanese
and English-Chinese corpora, 51.38% and 60.07%
of the concepts have the same synsets: that is,
slightly over half of the concepts can be directly
translated. Around 5% of the concepts in the two
corpora are linked to words close in the hierar-
chy (hyponym/hypernym). There were very few
antonyms (0.5%). Similar relations plus transla-
tion equivalents account for 42.85% and 34.74%
in the two corpora respectively. These parts are
the most challenging for machine translation.
In this first round, when the annotator attempted
to link concepts, it was sometimes the case that
the translation equivalent was a word not excluded
from wordnet by design. Especially common was
cases of common nouns in Japanese and Chinese
being linked to pronouns in English. In studying
how concepts differ across languages, we consider
these of interest. We therefore expanded our tag-
ging effort to include pronouns.
6 Discussion and Future Work
The pilot study showed clearly that cross-lingual
annotation was beneficial not just in finding inter-
esting correspondences across languages but also
in improving the monolingual annotation. In par-
ticular, we found many instances of multiword ex-
pressions that had been missed in the monolingual
annotation. Using a wordnet to sense tag a corpus
is extremely effective in improving the quality of
the wordnet, and tagging and linking parallel text
155
Type Eng-Jpn Eng-Cmn
linked 2,542 2,535
= 1,416 51.58 1,712 60.07
? 990 36.07 862 30.25
? 186 6.78 128 4.49
? 75 2.73 94 3.30
?2 8 0.81 13 1.51
? 63 2.30 39 1.37
?2 10 1.01 18 2.09
! 1 0.04 2 0.07
# 14 0.51 13 0.46
unlinked 2,583 1,898
Table 4: Analysis of links
is an excellent way to improve the quality of the
monolingual annotation. Given how many prob-
lems we found in both wordnet and corpus when
we went over the bilingual annotation, we hypoth-
esize that perhaps one of the reasons WSD is cur-
rently so difficult is that the gold standards are not
yet fully mature. They have definitely not yet gone
through the series of revisions that many syntactic
corpora have, even though the tagging scheme is
far harder.
For this project, we improved our annotation
process in two major ways:
(i) We expanded the scope of the annotation
to include pronouns and named entities interrog-
atives. These will now be tagged from the mono-
lingual annotation stage.
(ii) We improved the tool to make it possible to
add new entries directly to the wordnets, so that
they are available for tagging the remaining text.
Using the comments to add new sense was a bad
idea: synset-ids were cut and pasted, often with a
character missing, and annotators often mistyped
the link type. In addition, for words that appeared
many times, it was tedious to redo it for each word.
We are now testing an improved interface where
annotators add new words to the wordnet directly,
and these then become available for tagging. As a
quality check, the new entries are reviewed by an
expert at the end of each day, who has the option
of amending the entry (and possibly re-tagging).
We are currently tagging the remaining texts
shown in Table 1, with a preliminary release
scheduled for September 2013. For this we are
also investigating ways of improving the auto-
matic cross-lingual annotation: using word level
alignments; using global translation models and
by relaxing the mapping criteria (in particular
allowing linking across parts of speech through
derivational links). When we have finished, we
will also link the Japanese to the Chinese, using
English as a pivot. Finally, we will go through the
non-aligned concepts, and analyze why they can-
not be aligned.
In future work we intend to also add struc-
tural semantic annotation to cover issues such as
quantification. Currently we are experimenting
with Dependency Minimal Recursion Semantics
(DMRS: Copestake et al, 2005; Copestake, 2009)
and looking at ways to also constrain these cross-
linguistically (Frermann and Bond, 2012).
An interesting further extension would be to
look at a level of discourse marking. This would
be motivated by those translations which cannot
be linked at a lower level. In this way we would
become closer to the Groningen Meaning Bank,
which annotates POS, senses, NE, thematic roles,
syntax, semantics and discourse (Basile et al,
2012).
7 Conclusions
This paper presents preliminary results from an
ongoing project to construct large-scale sense-
tagged parallel corpora. Four languages are cho-
sen for the corpora: English, Chinese, Japanese,
and Indonesia. The annotation scheme is divided
into two phrases: monolingual sense annotation
and multilingual concept alignment. A pilot study
was carried out in Chinese, English and Japanese
for the short story The Adventure of the Danc-
ing Men. The results show that in the English-
Japanese and English-Chinese corpora, over half
of the concepts have the same synsets and thus
can be easily translated. However, 42.85% and
34.74% of the concepts in the two corpora can-
not be directly linked, which suggests it is hard for
machine translation. All annotated corpora will be
made freely available through the NTU-MC, in ad-
dition, the changes made to the wordnets will be
released through the individual wordnet projects.
Acknowledgments
This research was supported in part by the MOE
Tier 1 grant Shifted in Translation ? An Empirical
Study of Meaning Change across Languages.
156
References
Valerio Basile, Johan Bos, Kilian Evang, and
Noortje Venhuizen. 2012. Developing a large
semantically annotated corpus. In Proceedings
of the Eighth International Conference on Lan-
guage Resources and Evaluation (LREC 2012),
pages 3196?3200. Istanbul, Turkey.
Luisa Bentivogli, Pamela Forner, and Emanuele
Pianta. 2004. Evaluating cross-language an-
notation transfer in the MultiSemCor corpus.
In 20th International Conference on Computa-
tional Linguistics: COLING-2004, pages 364?
370. Geneva.
Francis Bond, Timothy Baldwin, Richard
Fothergill, and Kiyotaka Uchimoto. 2012.
Japanese SemCor: A sense-tagged corpus of
Japanese. In Proceedings of the 6th Global
WordNet Conference (GWC 2012), pages
56?63. Matsue.
Francis Bond and Ryan Foster. 2013. Linking and
extending an open multilingual wordnet. In 51st
Annual Meeting of the Association for Compu-
tational Linguistics: ACL-2013. Sofia.
Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2006. The Hinoki syntactic and semantic
treebank of Japanese. Language Resources and
Evaluation, 40(3?4):253?261. URL http://
dx.doi.org/10.1007/s10579-007-9036-6,
(Special issue on Asian language technology;
re-issued as DOI s10579-008-9062-z due to
Springer losing the Japanese text).
Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2008. The Hinoki syntactic and semantic
treebank of Japanese. Language Resources and
Evaluation, 42(2):243?251. URL http://
dx.doi.org/10.1007/s10579-008-9062-z,
(Re-issue of DOI 10.1007/s10579-007-9036-6
as Springer lost the Japanese text).
Francis Bond and Kyonghee Paik. 2012. A survey
of wordnets and their licenses. In Proceedings
of the 6th Global WordNet Conference (GWC
2012). Matsue. 64?71.
Arthur Conan Doyle. 1905. The Return of Sher-
lock Homes. George Newnes, London. Project
Gutenberg www.gutenberg.org/files/108/
108-h/108-h.htm.
Ann Copestake. 2009. Slacker semantics: Why
superficiality, dependency and avoidance of
commitment can be the right way to go. In
Proceedings of the 12th Conference of the Eu-
ropean Chapter of the ACL (EACL 2009), pages
1?9. Athens.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Seman-
tics. An introduction. Research on Language
and Computation, 3(4):281?332.
Oliver ?Culo, Silvia Hansen-Schirra, Stella Neu-
mann, and Mihaela Vela. 2008. Empirical
studies on language contrast using the English-
German comparable and parallel CroCo corpus.
In Proceedings of Building and Using Com-
parable Corpora, LREC 2008 Workshop, Mar-
rakesh, Morocco, volume 31, pages 47?51.
Lea Cyrus. 2006. Building a resource for studying
translation shifts. In Proceedings of The Sec-
ond International Conference on Language Re-
sources and Evaluation (LREC-2006).
Christine Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
Lea Frermann and Francis Bond. 2012. Cross-
lingual parse disambiguation based on seman-
tic correspondence. In 50th Annual Meeting of
the Association for Computational Linguistics:
ACL-2012, pages 125?129. Jeju, Korea.
Hitoshi Isahara, Francis Bond, Kiyotaka Uchi-
moto, Masao Utiyama, and Kyoko Kanzaki.
2008. Development of the Japanese WordNet.
In Sixth International conference on Language
Resources and Evaluation (LREC 2008). Mar-
rakech.
Kaname Kasahara, Hiroshi Sato, Francis Bond,
Takaaki Tanaka, Sanae Fujita, Tomoko Kana-
sugi, and Shigeaki Amano. 2004. Construc-
tion of a Japanese semantic lexicon: Lexeed.
In IPSG SIG: 2004-NLC-159, pages 75?82.
Tokyo. (in Japanese).
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Sum-
mit X.
Sadao Kurohashi and Makoto Nagao. 2003. Build-
ing a Japanese parsed corpus ? while improv-
ing the parsing system. In Anne Abeill?, edi-
tor, Treebanks: Building and Using Parsed Cor-
pora, chapter 14, pages 249?260. Kluwer Aca-
demic Publishers.
Shari Landes, Claudia Leacock, and Christiane
Fellbaum. 1998. Building semantic concor-
157
dances. In Fellbaum (1998), chapter 8, pages
199?216.
Mingqin Li, Juanzi Li, Zhendong Dong, Zuoy-
ing Wang, and Dajin Lu. 2003. Building a
large Chinese corpus annotated with seman-
tic dependency. In Proceedings of the sec-
ond SIGHAN workshop on Chinese language
processing-Volume 17, pages 84?91. Associa-
tion for Computational Linguistics.
George Miller. 1998. Foreword. In Fellbaum
(1998), pages xv?xxii.
Nurril Hirfana Mohamed Noor, Suerya Sapuan,
and Francis Bond. 2011. Creating the open
Wordnet Bahasa. In Proceedings of the 25th Pa-
cific Asia Conference on Language, Information
and Computation (PACLIC 25), pages 258?267.
Singapore.
Hwee Tou Ng and Hian Beng Lee. 1996. Inte-
grating multiple knowledge sources to disam-
biguate word sense: An exemplar-based ap-
proach. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguis-
tics, pages 40?47.
Lars Nygaard and J?rg Tiedemann. 2003. OPUS
? an open source parallel corpus. In Proceed-
ings of the 13th Nordic Conference on Compu-
tational Linguistics.
Sebastian Pad? and Katrin Erk. 2010.
Translation shifts and frame-semantic
mismatches: A corpus analysis. Ms:
http://www.nlpado.de/~sebastian/
pub/papers/ijl10_pado_preprint.pdf.
Eric S. Raymond. 1999. The Cathedral & the
Bazaar. O?Reilly.
Singapore Tourist Board. 2012. Your Singapore.
Online: www.yoursingapore.om. [Accessed
2012].
Liling Tan and Francis Bond. 2012. Building and
annotating the linguistically diverse NTU-MC
(NTU-multilingual corpus). International Jour-
nal of Asian Language Processing, 22(4):161?
174.
J?rg Tiedemann. 2009. News from OPUS ?
a collection of multilingual parallel corpora
with tools and interfaces. In N. Nicolov,
K. Bontcheva, G. Angelova, and R. Mitkov,
editors, Recent Advances in Natural Language
Processing, volume 5, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
J?rg Tiedemann. 2012. Parallel data, tools and
interfaces in OPUS. In Proceedings of the
Eight International Conference on Language
Resources and Evaluation (LREC?12), pages
2214?2218.
J?rg Tiedemann and Lars Nygaard. 2004. The
OPUS corpus ? parallel and free. In In Pro-
ceeding of the 4th International Conference on
Language Resources and Evaluation (LREC-4).
Martin Volk, Anne G?hring, Torsten Marek, and
Yvonne Samuelsson. 2010. SMULTRON (ver-
sion 3.0) ? The Stockholm MULtilingual par-
allel TReebank. http://www.l.uzh.h/
researh/paralleltreebanks_en.html.
Gan Kok Wee and Tham Wai Mun. 1999. General
knowledge annotation based on how-net. Com-
putational Linguistics and Chinese Language
Processing, 4(2):39?86.
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shi-
wen Yu. 2006. A chinese corpus with word
sense annotation. In Computer Processing of
Oriental Languages. Beyond the Orient: The
Research Challenges Ahead, pages 414?421.
Springer.
Renjie Xu, Zhiqiang Gao, Yuzhong Qu, and
Zhisheng Huang. 2008. An integrated approach
for automatic construction of bilingual Chinese-
English WordNet. In 3rd Asian Semantic Web
Conference (ASWC 2008), pages 302?341.
158
