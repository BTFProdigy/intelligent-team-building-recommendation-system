Corporator: A tool for creating RSS-based specialized corpora 
C?drick Fairon 
Centre de traitement automatique du langage 
UCLouvain 
Belgique 
cedrick.fairon@uclouvain.be 
 
Abstract 
This paper presents a new approach and a 
software for collecting specialized cor-
pora on the Web. This approach takes 
advantage of a very popular  XML-based 
norm used on the Web for sharing con-
tent among websites: RSS (Really Simple 
Syndication). After a brief introduction to 
RSS, we explain the interest of this type 
of data sources in the framework of cor-
pus development. Finally, we present 
Corporator, an Open Source software 
which was designed for collecting corpus 
from RSS feeds.  
1 Introduction1 
Over the last years, growing needs in the fields 
of Corpus Linguistics and NLP have led to an 
increasing demand for text corpora. The automa-
tion of corpus development has therefore became 
an important and active field of research. Until 
recently, constructing corpora required large 
teams and important means (as text was rarely 
available on electronic support and computer had 
limited capacities). Today, the situation is quite 
different as any published text is recorded, at 
some point of its ?life? on digital media. Also, 
increasing number of electronic publication (tex-
tual databank, CD?ROM, etc.) and the expansion 
of the Internet have made text more accessible 
than ever in our history.  
The Internet is obviously a great source of 
data for corpus development. It is either consid-
ered as a corpus by itself (see the WebCorp Pro-
ject of Renouf, 2003) or as a huge databank in 
which to look for specific texts to be selected and 
                                                
1 I would like to thank CENTAL members who took part in 
the development and the administration of GlossaNet and 
those who contributed to the development of Corporator and 
GlossaRSS. Thanks also to Herlinda Vekemans who helped 
in the preparation of this paper. 
gathered for further treatment. Examples of pro-
jects adopting the latter approach are numerous 
(among many Sekigushi and Yammoto, 2004; 
Emirkanian et al 2004). It is also the goal of the 
WaCky Project for instance which aims at devel-
oping tools ?that will allow linguists to crawl a 
section of the web, process the data, index them 
and search them?2.  
So we have the Internet: it is immense, free, 
easily accessible and can be used for all manner 
of language research (Kilgarriff and Grefenstette, 
2003). But text is so abundant, that it is not so 
easy to find appropriate textual data for a given 
task. For this reason, researchers have been de-
veloping softwares that are able to crawl the Web 
and find sources corresponding to specific crite-
ria. Using clustering algorithms or similarity 
measures, it is possible to select texts that are 
similar to a training set. These techniques can 
achieve good results, but they are sometimes lim-
ited when it comes to distinguishing between 
well-written texts vs. poorly written, or other 
subtle criteria. In any case, it will require filter-
ing and cleaning of the data (Berland and Grabar, 
2002).  
One possibility to address the difficulty to find 
good sources is to avoid ?wide crawling? but 
instead to bind the crawler to manually identified 
Web domains which are updated on a regular 
basis and which offer textual data of good quality 
(this can be seen as ?vertical crawling? as op-
posed to ?horizontal? or ?wide crawling?). This 
is the choice made in the GlossaNet system (Fai-
ron, 1998; 2003). This Web service gives to the 
users access to a linguistics based search engine 
for querying online newspapers (it is based on 
the Open Source corpus processor Unitex3 ? 
Paumier, 2003). Online newspapers are an inter-
esting source of textual data on the Web because 
they are continuously updated and they usually 
publish articles reviewed through a full editorial 
                                                
2 http://wacky.sslmit.unibo.it 
3 http://www-igm.univ-mlv.fr/~unitex/ 
43
process which ensures (a certain) quality of the 
text.  
 
Figure 1. GlossaNet interface 
 
GlossaNet downloads over 100 newspapers (in 
10 languages) on a daily basis and parses them 
like corpora. The Web-based interface4 of this 
service enable the user to select a list of newspa-
pers and to register a query. Every day, the 
user?s query is applied on the updated corpus 
and results are sent by email to the user under 
the form of a concordance. The main limitation 
of GlossaNet is that it works only on a limited 
set of sources which all are of the same kind 
(newspapers). 
In this paper we will present a new approach 
which takes advantage of a very popular XML-
based format used on the Web for sharing con-
tent among websites: RSS (Really Simple Syn-
dication). We will briefly explain what RSS is 
and discuss its possibilities of use for building 
corpora.  
We will also present Corporator, an Open 
Source program we have developed for creating 
RSS-fed specialized corpora. This system is not 
meant to replace broad Web crawling ap-
proaches but rather systems like GlossaNet, 
which collect Web pages from a comparatively 
small set of homogeneous Web sites. 
2 From RSS news feeds to corpora 
2.1 What is RSS 
RSS is the acronym for Really Simple Syndica-
tion5. It is an XML-based format used for facili-
                                                
4 http://glossa.fltr.ucl.ac.be 
5 To be more accurate, ?r? in RSS was initially a reference to 
RDF. In fact, at the beginning of RSS the aim was to enable 
automatic Web site summary and at that time, RSS stood for 
tating news publication on the Web and content 
interchange between websites6. Netscape created 
this standard in 1999, on the basis of Dave 
Winer?s work on the ScriptingNews format (his-
torically the first syndication format used on the 
Web)7. Nowadays many of the press groups 
around the world offer RSS-based news feeds on 
their Web sites which allow easy access to the 
recently published news articles: 
 
FR Le monde :   
http://www.lemonde.fr/web/rss/0,48-0,1-0,0.html 
IT La Repubblica  
http://www.repubblica.it/servizi/rss/index.html  
PT P?blico 
http://www.publico.clix.pt/homepage/site/rss/default.asp  
US New York Times 
http://www.nytimes.com/services/xml/rss/index.html  
ES El Pais :  
http://www.elpais.es/static/rss/index.html  
AF Allafrica.com8 
http://fr.allafrica.com/tools/headlines/rss.html  
etc. 
 
 
                                                                       
RDF Site Summary format. But over the time this standard 
changed for becoming a news syndication tools and the 
RDF headers were removed. 
6 Atom is another standard built with the same objective but 
is more flexible from a technical point of view. For a com-
parison, see http://www.tbray.org/atom/RSS-and-Atom or 
Hammersley (2005). 
7 After 99, many groups were involved in the development 
of RSS and it is finally Harvard which published RSS 2.0 
specifications under Creative Commons License in 2003. 
For further details on the RSS? history, see 
http://blogs.law.harvard.edu/tech/rssVersionHistory/  
8 AllAfrica gathers and indexes content from more than 125 
African press agencies and other sources. 
    
 
Figure 2. Example of RSS feeds proposed by  
Reuters (left) and the New York Times (right) 
44
Figure 2 shows two lists of RSS proposed by 
Reuters and the New York Times respectively. 
Each link points to a RSS file that contains a list 
of articles recently published and corresponding 
to the selected theme or section. RSS files do not 
contain full articles, but only the title, a brief 
summary, the date of publication, and a link to 
the full article available on the publisher Web 
site. On a regular basis (every hour or even more 
frequently), RSS documents are updated with 
fresh content. 
News publishers usually organize news feeds 
by theme (politics, health, business, etc.) and/or 
in accordance with the various sections of the 
newspaper (front page, job offers, editorials, re-
gions, etc.). Sometimes they even create feeds 
for special hot topics such as ?Bird flu?, in Fig-
ure 2 (Reuters).  
There is a clear tendency to increase the num-
ber of available feeds. We can even say that there 
is some kind of competition going on as competi-
tors tend to offer more or better services than the 
others. By proposing accurate feeds of informa-
tion, content publishers try to increase their 
chance to see their content reused and published 
on other websites (see below ?2.2). Another in-
dicator of the attention drawn to RSS applica-
tions is that some group initiatives are taken for 
promoting publishers by publicizing their RSS 
sources. For instance, the French association of 
online publishers (GESTE9) has released an 
Open Source RSS reader10 which includes more 
than 274 French news feeds (among which we 
can find feeds from Le Monde, Lib?ration, 
L?Equipe, ZDNet, etc.). 
2.2 What is RSS? 
RSS is particularly well suited for publishing 
content that can be split into items and that is 
updated regularly. So it is very convenient for 
publishing news, but it is not limited to news. 
There are two main situations of use for RSS. 
First, on the user side, people can use an RSS 
enabled Web client (usually called news aggre-
gator) to read news feeds. Standalone applica-
tions (like BottomFeeder11 ou Feedreader12) co-
exist with plug-ins readers to be added to a regu-
lar Web browser. For example, Wizz RSS News 
Reader is an extension for Firefox. It is illus-
trated in Figure 3: the list of items provided by a 
                                                
9 http://www.geste.fr 
10 AlerteInfo, http://www.geste.fr/alertinfo/home.html 
11 http://www.cincomsmalltalk.com/BottomFeeder/ 
12 http://www.feedreader.com 
RSS is displayed in the left frame. A simple click 
on one item opens the original article in the right 
frame. 
 
 
Figure 3. News aggregator plugin in Firefox 
 
Second, on the Web administrator side, this 
format facilitates the integration in one Web site 
of content provided by another Web site under 
the form of a RSS. Thanks to this format, Google 
can claim to integrate news from 4500 online 
sources updated every 15 minutes13. 
2.3 How does the XML code looks like? 
As can be see in Figure 414, the XML-based for-
mat of RSS is fairly simple. It mainly consists of 
a ?channel? which contains a list of ?items? de-
scribed by a title, a link, a short description (or 
summary), a publication date, etc. This example 
shows only a subset of all the elements (tags) 
described by the standard15.   
 
Figure 4: example of RSS feed 
2.4 Can RSS feed corpora? 
As mentioned above, RSS feeds contain few text. 
They are mainly a list of items, but each item has 
a link pointing to the full article. It is therefore 
                                                
13 http://news.google.com 
14 This example comes from the New York Times ?World 
Business? RSS feed and was simplified to fit our needs.  
15 It is also possible to add elements not described in RSS 
2.0 if they are described in a namespace. 
45
easy to create a kind of ?greedy? RSS reader 
which does not only read the feed, but also 
download each related Web page. This was our 
goal when we developed Corporator, the pro-
gram presented in section 3.  
2.5 Why using RSS feeds? 
The first asset of RSS feeds in the framework of 
corpus development is that they offer pre-
classified documents by theme, genre or other 
categories. If the classification fits the researcher 
needs, it can be used for building a specialized 
corpus. Paquot and Fairon (Forthcoming), for 
instance, used this approach for creating corpora 
of editorials in several languages, which can 
serve as comparable corpora to the ICLE16 argu-
mentative essays, see section 3.1). Classification 
is of course extremely interesting for building 
specialized corpora, but there are two limitations 
of this asset: 
? The classification is not standardized 
among content publishers. So it will re-
quire some work to find equivalent news 
feeds from different publishers. Figure 2 
offers a good illustration of this: the cate-
gories proposed by Reuters and the New 
York Times do not exactly match (even if 
they both have in common some feeds like 
sports or science).  
? We do not have a clear view on how the 
classification is done (manually by the 
authors, by the system administrators, or 
even automatically?).  
A second asset is that RSS are updated on a 
regular basis. As such, an RSS feed provides a 
continuous flow of data that can be easily col-
lected in a corpus. We could call this a dynamic 
corpus (Fairon, 1999) as it will grow over time. 
We could also use the term monitor corpus 
which was proposed by Renouf (1993) and 
which is widely used in the Anglo-Saxon com-
munity of corpus linguistics.  
A third asset is that the quality of the language 
in one feed will be approximately constant. We 
know that one of the difficulties when we crawl 
the Web for finding sources is that we can come 
across any kind of document of different quality. 
By selecting ?trusted? RSS sources, we can in-
sure an adequate quality of the retrieved texts. 
We can also note that RSS feeds comprise the 
title, date of publication and the author?s name of 
                                                
16 See Granger et al (2002). 
the articles referred to. This is also an advantage 
because this information can be difficult to ex-
tract from HTML code (as it is rarely well struc-
tured). As soon as we know the date of publica-
tion, we can easily download only up to date in-
formation, a task that is not always easy with 
regular crawlers. 
On the side of these general assets, it is also 
easy to imagine the interest of this type of 
sources for specific applications such as linguis-
tic survey of the news (neologism identification, 
term extraction, dictionary update, etc.). 
All these advantages would not be very sig-
nificant if the number of sources was limited. 
But as we indicated above, the number of news 
feeds is rapidly and continuously growing, and 
not only on news portals. Specialized websites 
are building index of RSS feeds17 (but we need to 
remark that for the time being traditional search 
engines such as Google, MSN, Yahoo, etc. han-
dle RSS feeds poorly). It is possible to find feeds 
on virtually any domain (cooking, health, sport, 
education, travels, sciences) and in many lan-
guages. 
3 Corporator: a ?greedy? news agreg-
gator 
Corporator18 is a simple command line pro-
gram which is able to read an RSS file, find the 
links in it and download referenced documents. 
All these HTML documents are filtered and 
gathered in one file as illustrated in Figure 5.  
 
 
Figure 5. Corporator Process 
 
 
The filtering step is threefold: 
- it removes HTML tags, comments and 
scripts; 
- it removes (as much as possible) the 
worthless part of the text  (text from ads, 
                                                
17 Here is just a short selection: http://www.newsxs.com,  
http://www.newsisfree.com, http://www.rss-scout.de, 
http://www.2rss.com, http://www.lamoooche.com. 
18 Corporator is an Open Source program written in Perl. It 
was developed on the top of a preexisting Open Sources 
command line RSS reader named  The Yoke. It will be 
shortly made available on CENTAL?s web site: 
http://cental.fltr.ucl.ac.be. 
46
links, options and menu from the original 
Web page)19.  
- it converts the filtered text from its origi-
nal character encoding to UTF8. Corpora-
tor can handle the download of news feeds 
in many languages (and encodings: UTF, 
latin, iso, etc.)20.  
The program can easily be set up in a task 
scheduler so that it runs repeatedly to check if 
new items are available. As long as the task re-
mains scheduled, the corpus will keep on grow-
ing. 
Figure 6 shows a snapshot of the resulting 
corpus. Each downloaded news item is preceded 
by a header that contains information found in 
the RSS feed.  
 
Figure 6. Example or resulting corpus 
 
Corporator is a generic tool, built for 
downloading any feeds in any language. This 
goal of genericity comes along with some 
limitations. For instance, for any item in the RSS 
feed, the program will download only one Web 
page even if, on some particular websites, 
articles can be split over several pages: Reuters21 
for instance splits its longer articles into several 
pages so that each one can fit on the screen. The 
RSS news item will only refer to the first page 
and Corporator will only download that page. It 
will therefore insert an incomplete article in the 
corpus. We are still working on this issue.  
                                                
19 This is obviously the most difficult step. Several options 
have been implemented to improve the accuracy of this 
filter : delete text above the article title, delete text after 
pattern X, delete line if matches pattern X, etc. 
20 It can handle all the encodings supported by the Perl 
modules Encode (for information, see Encode::Supported 
on Cpan). Although, experience shows that using the En-
code can be complicated. 
21 http://today.reuters.com 
3.1 Example of corpus creation 
In order to present a first evaluation of the sys-
tem, we provide in Figure 7 some information 
about an ongoing corpus development project. 
Our aim is to build corpora of editorials in sev-
eral languages, which can serve as comparable 
corpora to the ICLE argumentative essays 
(Paquot and Fairon, forthcoming). We have 
therefore selected ?Editorial?, ?Opinion? and 
other sections of various newspapers, which are 
expected to contain argumentative texts. Figure 7 
gives for four of these sources the number of ar-
ticles22 downloaded between January 1st 2006 
and January 31st 2006 (RSS feed names are given 
between brackets and URLs are listed in the 
footnotes). Tokens were counted using Unitex 
(see above) on the filtered text (i.e. text already 
cleaned from HTML and non-valuable text). 
Figure 7 shows that the amount of text pro-
vided for a given section (here, Opinion) by dif-
ferent publishers can be very different. It also 
illustrates the fact that it is not always possible to 
find corresponding news feeds among different 
publishers: Le Monde, for instance, does not pro-
vide its editorials on a particular news feed. We 
have therefore selected a rubric named Rendez-
vous in replacement (we have considered that it 
contains a text genre of interest to our study). 
 
 
Le Monde23 (Rendez-vous) 
58 articles 
90,208 tokens 
New York Times24 (Opinion) 
 220 articles 
 246,104 tokens 
Washington Post25 (Opinion) 
 95 articles 
 137,566 tokens 
El Pais26 (Opini?n) 
 337 articles 
 399,831 tokens 
 
Figure 7. Download statistics: number of articles 
downloaded in January 2006 
 
                                                
22 This is the number of articles recorded by the program 
after filtering. It may not correspond exactly to the number 
of articles really published on this news feed. 
23 www.lemonde.fr/rss/sequence/0,2-3238,1-0,0.xml 
24 www.nytimes.com/services/xml/rss/nyt/Opinion.xml 
25 www.washingtonpost.com/wp-
dyn/rss/index.html#opinion 
26 www.elpais.es/rss/feed.html?feedId=1003 
47
3.2 Towards an online service 
Linguists may find command line tools hard to 
use. For this reason, we have also developed a 
Web-based interface for facilitating RSS-based 
corpus development. GlossaRSS provides a sim-
ple Web interface in which users can create 
?corpus-acquisition tasks?. They just choose a 
name for the corpus, provide a list of URL corre-
sponding to RSS feeds and activate the down-
load. The corpus will grow automatically over 
time and the user can at any moment log in to 
download the latest version of the corpus. For 
efficiency reasons, the download managing pro-
gram checks that news feeds are downloaded 
only once. If several users require the same feed, 
it will be downloaded once and then appended to 
each corpus. 
 
Figure 8. Online service for building  
RSS-based corpora 
 
This service is being tested and will be made 
public shortly. Furthermore, we plan to integrate 
this procedure to GlossaNet. At the moment, 
GlossaNet provides language specialists with a 
linguistic search engine that can analyze a little 
more than 100 newspapers (as seen in Figure 1, 
users who register a linguistic query can com-
pose a corpus by selecting newspapers in a pre-
defined list). Our goal is to offer the same service 
in the future but on RSS-based corpora. So it will 
be possible to create a new corpus, register a lin-
guistic query and get concordance on a daily or 
weekly basis by email. There is no programming 
difficulty, but there is a clear issue on the side of 
?scalability? (at the present time, GlossaNet 
counts more than 1,300 users and generates more 
than 18,800 queries a day. The computing charge 
would probably be difficult to cope with if each 
user started to build and work on a different cor-
pus). An intermediate approach between the cur-
rent list of newspapers and an open system 
would be to define in GlossaNet some thematic 
corpora that would be fed by RSS from different 
newspapers. 
3.3 From text to RSS-based speech corpora 
The approach presented in this paper focuses on 
text corpora, but could be adapted for collecting 
speech corpora. In fact RSS are also used as a 
way for publishing multimedia files through Web 
feeds named ?podcasts?. Many medias, corpora-
tions or individuals use podcasting for placing 
audio and video files on the Internet. The advan-
tage of podcast compared with streaming or sim-
ple download, is ?integration?. Users can collect 
programs from a variety of sources and subscribe 
to them using a podcast-aware software which 
will regularly check if new content is available. 
This technology has been very successful in the 
last two years and has been rapidly growing in 
importance. Users have found many reasons to 
use it, sometimes creatively: language teachers, 
for example, have found there a very practical 
source of authentic recordings for their lessons.  
Regarding corpus development, the interest of 
podcasting is similar to the ones of text-based 
RSS (categorization, content regularly updated, 
etc.). Another interesting fact is that sometimes 
transcripts are published together with the pod-
cast and it is therefore a great source for creating 
sound/text corpora27.  
Many portals offer lists of poscast28. One of 
the most interesting ones, is Podzinger29 which 
not only indexes podcasts metadata (title, author, 
date, etc.), but uses a speech recognition system 
for indexing podcast content. 
It would require only minor technical adapta-
tion to enable Corporator to deal with podcasts, 
something that will be done shortly. Of course, 
this will only solve the problem of collecting 
sound files, not the problem of converting these 
files into speech data useful for linguistic re-
search. 
4 Conclusion 
Corpora uses and applications are every year 
more numerous in NLP, language teaching, cor-
pus linguistics, etc. and there is therefore a grow-
ing demand for large well-tailored corpora. At 
the same time the Internet has grown enor-
mously, increasing its diversity and its world 
                                                
27 It is even possible to find services that do podcast tran-
scripts (http://castingwords.com). 
28 http://www.podcastingnews.com, http://www.podcast.net, 
etc. 
29 http://www.podzinger.com 
48
wide coverage. It is now an ideal ?ground? for 
finding corpus sources. But these assets (size, 
diversity) is at the same time an issue for finding 
good, reliable, well-written, sources that suit our 
needs. This is the reason why we need to develop 
intelligent source-finder crawlers and other soft-
wares specialized in corpus collection. Our con-
tribution to this effort is to bring the researchers? 
attention to a particularly interesting source of 
text on the Internet: RSS news feeds. The main 
interest of this source is to provide classified lists 
of documents continuously updated and consis-
tent in terms of language quality.  
To build specialized corpora with a traditional 
crawler approach, the process will probably con-
sist in retrieving documents (using a search en-
gine as starting point) and then sorting the re-
trieved documents and selecting the ones that 
pass some kind of validity tests. With RSS-based 
corpus, the approach is different and could be 
summarized as follows: do not sort a list of re-
trieved documents, but retrieve a list of sorted 
documents. This is of course only possible if we 
can find RSS-feeds compatible with the theme 
and/or language we want in our corpus.  
References 
Berland, Sophie and Natalia Grabar. 2002. Assistance 
automatique pour l'homog?n?isation d'un corpus 
Web de sp?cialit?. In Actes  des 6?mes Journ?es in-
ternationales d'analyse statistique des donn?es tex-
tuelles (JADT 2002). Saint-Malo. 
Fairon, C?drick. 1999. Parsing a Web site as a corpus. 
In C. Fairon (ed.). Analyse lexicale et syntaxique: 
Le syst?me INTEX, Lingvisticae Investigationes 
Tome XXII (Volume sp?cial). John Benjamins 
Publishing, Amsterdam/Philadelphia, pp. 327-340. 
Granger, Sylviane, Estelle Dagneaux and Fanny Meu-
nier (eds). 2002. The International Corpus of 
Learner English. CD-ROM and Handbook. Presses 
universitaires de Louvain, Louvain-la-Neuve.  
Hammersley, Ben. 2005. Developing Feeds with RSS 
and Atom. O?Reilly, Sebastopol, CA. 
Kilgarriff, Adam and Gregory Grefenstette. 2003. 
Introduction to the Special Issue on the Web as 
Corpus. Computational Linguistics, Vol. 29(3): 
333-348. 
Paquot, Magali and C?drick Fairon. (forthcoming). 
Investigating L1-induced learner variability: Using 
the Web as a source of L1 comparable data. 
Paumier, S?bastien. 2003. De la reconnaissance de 
formes linguistiques ? l'analyse syntaxique, Ph.D., 
Universit? de Marne-la-Vall?e. 
Renouf, Antoinette. 1993. 'A Word in Time: first find-
ings from the investigation of dynamic text'. In J. 
Aarts, P. de Haan and N. Oostdijk (eds), English 
Language Corpora: Design, Analysis and Exploi-
tation, Rodopi, Amsterdam, pp. 279-288. 
Renouf, Antoinette. 2003. 'WebCorp: providing a 
renewable energy source for corpus linguistics'. In 
S. Granger and S. Petch-Tyson (eds), Extending the 
scope of corpus-based research: new applications, 
new challenges, Rodopi , Amsterdam, pp. 39-58. 
Sekiguchi, Youichi and Kazuhide Yamamoto. 2004. 
'Improving Quality of the Web Corpus'. In Pro-
ceedings of The First International Joint Confer-
ence on Natural Language Processing (IJCNLP-
04), pp. 201-206. 
Emirkanian Louisette, Christophe Fouquer? and Fab-
rice Issac. 2004. Corpus issus du Web : analyse des 
pertinences th?matique et informationnelle. In G. 
Purnelle, C. Fairon and A. Dister (eds), Le Poids 
des mots. Actes des 7?mes Journ?es internationales 
d'analyse statistique des donn?es textuelles (JADT 
2004), Presses universitaires de Louvain, Louvain-
La-Neuve, pp. 390-398. 
49
 50
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 466?477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
An ?AI readability? formula for French as a foreign language
Thomas Franc?ois
IRCS, University of Pennsylvania
3401 Walnut Street Suite 400A Room 423
Philadelphia, PA 19104, USA
frthomas@sas.upenn.edu
Ce?drick Fairon
CENTAL, UCLouvain
Place Blaise Pascal, 1
1348 Louvain-la-Neuve, Belgium
Cedrick.Fairon@uclouvain.be
Abstract
This paper present a new readability formula
for French as a foreign language (FFL), which
relies on 46 textual features representative of
the lexical, syntactic, and semantic levels as
well as some of the specificities of the FFL
context. We report comparisons between sev-
eral techniques for feature selection and var-
ious learning algorithms. Our best model,
based on support vector machines (SVM), sig-
nificantly outperforms previous FFL formulas.
We also found that semantic features behave
poorly in our case, in contrast with some pre-
vious readability studies on English as a first
language.
1 Introduction
Whether in a first language (L1) or a second and for-
eign language (L2), learning to read has been and re-
mains one of the major concerns of education. When
a teacher wants to improve his/her students? reading
skills, he/she uses reading exercises, whether there
are guided or independent. For this practice to be
efficient, it is necessary that the texts suit the level
of students (O?Connor et al2002). This condition
is sometimes difficult to meet for teachers wishing
to get off the beaten tracks by not using texts from
levelled textbooks or readers.
In this context, readability formulas have long
been used to help teachers faster select texts for their
students. These formulas are reproducible meth-
ods that aim at matching readers and texts relative
to their reading difficulty level. The Flesch (1948)
and Dale and Chall (1948) formulas are probably
the best-known examples of those. They are typical
of classic formulas, the first major methodological
paradigm developed in the field during the 40?s and
50?s. They were kept as parsimonious as possible,
using linear regression to combined two, or some-
times, three surface features, such as word mean
length, sentence mean length, or proportion of out-
of-simple-vocabulary words.
Later, some scholars (Kintsch and Vipond, 1979;
Redish and Selzer, 1985) argued that the classic for-
mulas suffer from several shortcomings. These for-
mulas only take into account superficial features, ig-
noring other important aspects contributing to text
difficulty, such as coherence, content density, infer-
ence load, etc. They also omit the interactive as-
pect of the reading process. In the 80?s, a second
paradigm, inspired by structuro-cognitivist theories,
intended to overcome these issues. It focused on
higher textual dimensions, such as inference load
(Kintsch and Vipond, 1979; Kemper, 1983), den-
sity of concepts (Kintsch and Vipond, 1979), or
macrostructure (Meyer, 1982). However, these at-
tempts did not achieve better results than the clas-
sic approach, even though they used more principled
and more complex features.
Recently, a third paradigm, referred to as the ?AI
readability? by Franc?ois (2011a), has emerged in the
field. Studies that are part of this current share three
key features: the use of a large number of texts as-
sessed by experts (coming from textbooks, simpli-
fied newspapers or web resources) as training data ;
the use of NPL-enable features able to capture a
wider range of readability factors, and the combi-
nation of those features through a machine learning
466
algorithm. Since the work of Si and Callan (2001),
this paradigm have spawn several studies for English
(Collins-Thompson and Callan, 2005; Heilman et
al., 2008; Schwarm and Ostendorf, 2005; Feng et
al., 2010).
However, for French, the field is far from being so
thriving. To our knowledge, only two ?AI readabil-
ity? have been designed so far for French L1 and
only one for French as a foreign language (FFL)
(see Section 2). This paper reports some experi-
ments aimed at designing a more efficient readabil-
ity model for FFL. In Section 2, it is further argue
why a new formula was necessary for FFL. Section
3 covers the various methodological steps required
to devise the model, whose results are reported in
Section 4. Finally, Section 5 discusses some inter-
esting insights gained by this work.
2 Readability models for French
Readability of French never enjoyed a large suc-
cess: while readability studies on English dates back
to the 20?s, it is only in 1957 that the French-
speaking world discovered it through the work of
Conquet (1957). Since then, only a few studies fo-
cused on the topic.
The two first French L1 formulas were adap-
tations of the Flesch formula (Kandel and Moles,
1958; de Landsheere, 1963). It is only with
Henry (1975) that French got a model fitting the
particularities of the language. Henry used cloze
tests to assess the level of 60 texts from primary and
secondary school textbooks and trained three for-
mulas on this corpus. It is worth mentioning that
Henry?s formulas have been applied to FFL by Cor-
naire (1988). During the same time, Richaudeau
explored a different path, as a representative of the
structuro-cognitivist paradigm. He used the num-
ber of words recalled by a subject after he/she has
just read a sentence as a device to measure under-
standing and provided an ?efficiency formula? of
texts (Richaudeau, 1979). Although more modern
in its conception, Richaudeau?s hard-to-implement
formula did not achieve the same recognition in the
French speaking world as Henry?s.
After those two major efforts, few works fol-
lowed. It is worth mentioning two more authors:
Mesnager (1989), who designed a classic formula
for children that draw inspiration from the Dale and
Chall (1948) formula and Daoust et al1996), who
developed SATO-CALIBRAGE, a program assessing
text difficulty from the first to the eleventh grade.
It can be considered as the first ?AI formula? for
French L1, since it made use of NLP-enabled fea-
tures. It is also the last formula published for French
L1, if we except the adaptation of the model by
Collins-Thompson and Callan (2004) to French.
As regards to French L2, the literature is even
sparser. Tharp (1939) published a first formula tak-
ing into account one particularity of the L2 context:
the cognates. Those are words sharing a similar
form and meaning across two languages and hav-
ing a facilitating effect in reading. This idea was re-
cently replicated by Uitdenbogerd (2005), who com-
bined a syntactic feature, the mean number of words
per sentence, with the number of cognates per 100
words in her formula. Although taking into account
this effect of the L1 on L2 reading is very interest-
ing, these two studies are confined to a limited audi-
ence: English speakers learning French. As regards
a more generic approach, Franc?ois (2009) recently
published an ?AI formula? for FFL, based on lo-
gistic regression and ten features. Among those, he
stressed the use of verbal tense information as a way
to improve performance. However, the set of fea-
tures he experimented remains limited (about 20).
From all this, it seems clear that FFL readability
needs to be addressed more thoroughly, especially if
we are willing to get a generic model, able to make
predictions for L2 readers with any L1 background.
The rest of this paper describes one such attempt.
3 Design of the formula
The design of an ?AI readability? formula involves
the same three steps as a classification problem.
First, one need to gather a gold-standard corpus
large enough to reliably train the parameters of a
learning algorithm, as described in Section 3.1. The
next step, covered in Section 3.2, consists in defin-
ing a set of predictors, that is to say, linguistic char-
acteristics of the texts that will be used to predict the
difficulty level of new texts. Finally, the best sub-
set of these predictors is combined within a learning
algorithm to obtain the best model possible. Experi-
ments at this level are reported in Section 3.3.
467
3.1 The corpus
A gold-standard for readability consists in texts la-
belled according to their difficulty. For this, it is first
necessary to choose a difficulty scale used for the la-
bels (for English L1, it is usually the 12 grade levels
scale), that also constrains the output of the formula.
Then, each text have to be assessed with a method
able to measure the reading comprehension level of
the target population.
Regarding the scale, an obvious choice for
the foreign language context was the begin-
ner/intermediate/advanced continuum, recently re-
defined in the Common European Framework of
Reference for Languages (CEFR) (Council of Eu-
rope, 2001) as the six following levels: A1 (Break-
through); A2 (Waystage); B1 (Threshold); B2 (Van-
tage); C1 (Effective Operational Proficiency) and C2
(Mastery). This scale has now become the reference
for foreign language education, at least in Europe.
Assessing the reading difficulty of texts with re-
spect to a target population of readers was a more
challenging issue. Several techniques have been
used in the literature, the most important of which
are comprehension tests, cloze tests and expert
judgements. They all postulate a given population of
readers, although relying on expert judgements save
the need for a sample of subjects to take a test. In
this case, texts comes from textbooks whose content
difficulty have been assessed by the publishers.
This last criterion is now mainstream in ?AI read-
ability?, since it is very practical and facilitates the
creation of a large corpus, but it has its own short-
comings. Studies such as van Oosten et al2011)
found that expert agreement on a same corpus of
texts might be insufficient for a classification task.
For this study, we nevertheless relied on expert
judgements, since we needed a large amount of la-
belled texts to ensure a robust statistical learning.
We selected 28 FFL textbooks, published after 2001
and designed for adults or adolescents learning FFL
for general purposes. From those, we extracted
2,160 texts related to a reading comprehension task
and assigned to each of them the same level as the
textbook it came from.
As it was expected from van Oosten et al2011)?s
study, differences in the publishers? conception of
difficulty led to an heterogeneous labelling between
textbooks. This heterogeneity was detected in three
of the six levels (A1, A2, and B1) using ANOVA
based on two classic readability features as inde-
pendent variables: the mean number of words per
sentence and the mean number of letters per word.
A subsequent qualitative analysis revealed that most
of the heterogeneity was coming from textbooks fol-
lowing the new didactic approach recommended by
the CEFR: the task-oriented approach, which fo-
cuses more on the task than the text when labelling
the overall reading activity. Therefore, we decided
to remove those type of textbooks from our corpus,
which amounted to 5 books and 249 texts. The re-
maining 1,852 excerpts were kept for our experi-
ments. Their distribution is displayed in Table 1 as
regard to the number of texts and tokens.
3.2 The predictors
In a second step, every text of the corpus was rep-
resented as a numeric vector of 406 features, each
of them representing a linguistic dimension of the
text as a single number. Their implementation drew
on two different sources of inspiration: the existing
predictors in the English and French literature and
the psycholinguistic literature on the reading pro-
cess. The complete set was classified in four fam-
ilies, depending on the kind of information each one
is supposed to represent. These families were: ?lex-
ical?, ?syntactic?, ?semantic?, and ?specific to FFL
context?. Each of them was further divided in sub-
families, described in the rest of the section 1.
3.2.1 Lexical Features
Lexical features have been shown to be the most
important level of information in many readability
studies (Chall and Dale, 1995; Lorge, 1944). It is
then not surprising that a wide range of lexical pre-
dictors have been developed in the literature. Our
own set comprised the following subfamilies:
Statistics of lexical frequencies: frequencies of
words in a text are a good indicator of the text?s over-
all difficulty (Stenner, 1996). They are usually sum-
marized via the mean, but we also tested the median,
the interquartile range, as well as the 75th and 90th
percentiles.
1Space restrictions did not enable us to formally defined
each variable used in this study. The reader may consult
Franc?ois (2011b) for a more comprehensive description.
468
A1 A2 B1 B2 C1 C2 Total
430(58.561) 380(75.779) 552(176.973) 198(71.701) 184(92.327) 108(35.202) 1, 852(510; 543)
Table 1: Distribution of the number of texts and tokens per level in our corpus.
We used Lexique3 (New et al2007) as our fre-
quency database. It is a lexicon including about
50,000 lemmas and 125,000 inflected forms whose
frequencies were obtained from movie subtitles.
Since French has a rich morphology, we considered
the probabilities of both lemma and inflected forms.
Moreover, following an idea from Elley (1969), we
also computed the above mentioned statistics for
given POS words, such as content word, nouns,
verbs, etc.
Percentage of words not in a reference list: part
of the Dale and Chall (1948)?s formula, this feature
is one of the most famous in readability. For our
experiments, two word lists for FFL were used: the
well-known ? but already dated ? Gougenheim et
al. (1964)?s list and a second one that was found at
the end of one FFL textbook: Alter Ego (Berthet et
al., 2006). Different sizes were also experimented
for both lists.
Word length: mean word length is another classic
feature in readability (Flesch, 1948; Smith, 1961).
We used various statistics based on the number of
letters per word (mean, median, percentiles, etc.).
N-grams models: Si and Callan (2001) shown
that n-grams models can successfully be applied to
readability. We then used both a simple unigram ap-
proach based on the frequencies from Lexique3, and
a more complex bigram model trained on two dif-
ferent corpora: the Google n-grams (Michel et al
2011) and a corpus of newspaper articles from Le
Soir amounting to 5, 000, 000 words 2. Both were
normalized according the length n of the text as fol-
lows:
P (text) =
1
n
n?
i=1
logP (wi|h) (1)
where wi is the ith word and h a limited history of
length 0 (unigram) or 1 (bigram).
2Smoothing algorithms used were respectively the simple
Good-Turing algorithm (Gale and Sampson, 1995) for unigrams
and linear interpolation (Chen and Goodman, 1999) for the bi-
grams.
Lexical diversity: the repetition effect is another
factor known to affect the reading process (Bowers,
2000). It has been mainly implemented through the
classic type-token ratio (TTR) that suffers from be-
ing dependent on the text length. This is why we
defined a normalized TTR, which is the mean score
of several TTRs, computed on text?s fragments of
equal length. This way, long texts were made com-
parable with short ones.
Orthographic neighborhood: we finally sug-
gested a new lexical variable, based on the fact that
some characteristics of the orthographic neighbors 3
of a word are known to impact the reading of this
word (Andrews, 1997). Thirteen predictors were
implemented to account for the number or the fre-
quency of the orthographic neighbors of all words in
a text.
3.2.2 Syntactic features
The syntactic level of information is another tradi-
tional area of investigation in readability. Although
most of the scholars in the field agree that it does not
lead to such efficient predictors as the lexical level,
they have noticed it can be combined with the latter
to improve performance of readability formulas. We
therefore investigated the following subfamilies:
Sentence length: the traditional approach to syn-
tactic difficulty relied on the number of words per
sentence. We have approached it through various
statistics such as the mean, the median, or several
percentiles.
Part of speech ratios: Bormuth (1966) demon-
strated the good predictive power of some POS ra-
tios in a text. We computed 156 ratios based on
TreeTagger?s POS (Schmid, 1994). They operated
as proxies for the syntactic complexity of sentences,
since we did not use features based on a parser 4.
3The orthographic neighbors of a word X have been defined
by Coltheart (1978) as all the words of the same length asX and
varying from it only by one letter (eg. FIST and GIST).
4This choice was motivated as follows. Bormuth (1966),
who performed a manual annotation of the syntactic structures
469
Verbs: although the tense and moods found in a
text have been hardly considered in the field, Car-
reiras et al1997) suggested that verbal aspects are
important while building a mental representation of
a text and therefore impact its understanding. They
help the reader to distinguish between major and
minor elements associated with events described by
these verbs. We therefore replicated and enhanced
the feature set proposed by Franc?ois (2009), consid-
ering either binary indicators or proportions of the
use of tenses or moods in a text.
3.2.3 Semantic features
The importance of semantic and cognitive
factors have been particularly stressed by the
structuro-cognitivist paradigm, although Miller and
Kintsch (1980), as well as Kemper (1983), eventu-
ally admitted not being able to demonstrate the supe-
riority of those new predictors over traditional ones.
More recent work also reported limited evidence of
this alleged superiority (Pitler and Nenkova, 2008;
Feng et al2010). In order to clarify as much as
possible the situation for FFL, we implemented the
following features:
Personnalization level: Dale and Tyler (1934)
suggested that informal texts should be easier to read
and that informality might be assessed through the
type of personal pronouns found in a text. On this as-
sumption, 13 variables were defined to take into ac-
count various personal pronouns proportions in the
text.
Conceptual density: Kintsch et al1975) showed
that the number of propositions as well as the num-
ber of different arguments in a sentence influence
its reading time. Following Kintsch?s propositional
model, we used Denside?es (Lee et al2010) to cap-
ture conceptual complexity. It is a program able to
estimate the mean number of propositions per word
in a text using 35 rules relying on lexical and POS
clues.
in its corpus, noticed that features based on parse trees were
less efficient than classic ones, such as sentence length or part
of speech ratios. Therefore, it seemed unlikely that the infor-
mation collected by means of syntactic parsers, which are still
committing a significant number of errors, at least for French,
would belie these findings.
Lexical cohesion : the level of cohesion in a text
was measured as the average cosine of all pair of
adjacent sentences in the text. Each sentence was
represented by a numeric weighted vector (based on
words) and projected in a vector space. As sug-
gested by Foltz and al. (1998), two methods were
used to define the vector space and weight every
word: the tf-idf (term frequency-inverse document
frequency) and the latent semantic analysis (LSA).
The first approach, called ?word overlap?, corre-
sponds to the ?noun overlap? defined by Graesser et
al. (2004, 199), except that all type of POS are taken
into account. For LSA, we applied a singular value
decomposition (SVD), and after comparing various
sizes with a cross-validation procedure, we retained
a small 15-dimensional space.
3.2.4 Features specific to FFL
Apart from the effect of cognates (Uitdenbogerd,
2005; Tharp, 1939), few features specific to the L2
context were previously investigated. It is probably
because such an approach requires to train a model
for each pair of language of interest and gather suit-
able data for evaluation. Since our study intended to
design a generic model, we focused on specific pre-
dictors affecting L2 reading, whatever the learner?s
mother tongue is:
Multi-word expressions (MWE): MWEs are ac-
knowledged to cause problems to L2 learners for
production (Bahns and Eldaw, 1993). However, the
effect of MWE on the reception side remains un-
clear, especially for beginners. Ozasa et al2007)
tested the mean of the absolute frequency of all
MWEs in a text as an indication of its difficulty,
but it appeared non significant. In a latter experi-
ment involving a larger set of MWE-based predic-
tors, Franc?ois and Watrin (2011) detected a signifi-
cant, but limited effect. We therefore replicated this
set, which includes variables based on the frequen-
cies of MWE, their syntactic structure, their number
or their length. Frequencies were estimated on the
same corpora as the bigram model described above
(Google and Le Soir).
Type of text: Finally, we defined five simple vari-
ables aiming at identifying dialogues, such as pres-
ence of commas, ratio of punctuation, etc. as sug-
gested by Henry (1975). This focus on dialogue was
470
Level of information Tag Description of the variable ?
Lexical
PA-Alterego Proportion of absent words from a list of easy words 0.653
X90FFFC 90th percentile of inflected forms for content words only ?0.643
ML3 Unigram model based on lemmas ?0.553
NLM Mean number of letters per word 0.483
TTR Type-token ratio based on lemma 0.283
MedNeigh+Freq Median number of more frequent neighbor for words ?0.233
Syntactic
NMP Mean number of words per sentence 0.623
NWS90 Length (in words) of the 90th percentile sentence 0.613
LSDaoust Percentage of sentences longer than 30 words (Daoust et al1996) 0.563
PPres Presence of at least one present participle in the text 0.443
PRO.PRE Ratio of pronouns on prepositions ?0.353
PPres-C Proportion of present participle among verbs 0.413
PPasse Presence of at least one past participle 0.393
Impf Presence of at least one imperfect 0.273
Subp Presence of at least one subjunctive present 0.273
Cond Presence of at least one conditional 0.233
Imperatif Presence of at least one imperative 0.02
Subi Presence of at least one subjunctive imperfect 0.05
Semantic
avLocalLsa-Lem Average intersentential cohesion measured via LSA 0.633
PP1P2 Percentage of P1 and P2 personal pronouns ?0.333
Specific
NAColl Proportion of MWE having the structure NOUN ADJ 0.293
BINGUI Presence of commas 0.463
Table 2: Spearman correlation for some predictors in our set with difficulty. A positive correlation means that the
difficulty of texts increases with the value of the predictor. Signification levels are the following 1 :< 0.05; 2 :< 0.01;
and 3 : < 0.001.
explained by their extensive use in foreign language
teaching, especially in the first levels. Furthermore,
even for L1, various scholars stressed the fact that
dialogues are often written in a simpler style and
have a more mundane content (Dolch, 1948; Flesch,
1948).
3.3 The algorithms
The last step in the development of our formula was
to select the most informative subset of features and
combine them in a state-of-the-art machine learn-
ing algorithm. The algorithms originally consid-
ered were six: multinomial and ordinal logistic re-
gression (respectively MLR and OLR), classifica-
tion trees, bagging, boosting (both based on decision
trees) and support vector machine (SVM). However,
since the logistic models and the SVM clearly out-
performed the others three, we will reported only
about those in the next section.
4 Results
The experiments based on this methodology were
twofold. First, we assessed the predictive power
of each of the 406 features, considered in a bivari-
ate relationship with difficulty. Second, we selected
various subsets of features for training models and
compared their performance. The two next sections
summarize the main findings obtained during these
two steps.
4.1 The efficiency of predictors
Spearman correlation was used to assess the effi-
ciency of each predictor, to better account for non-
linear relationships with the criterion. Values for
some variables among the four families are reported
in Table 2. In accordance with the literature, it ap-
peared that the best family of predictors were the
lexical one, followed by the syntactic one. On the
contrary, semantic and specific to FFL features did
not perform so well, with the exception of the LSA-
based feature (avLocalLsa-Lem).
Of all predictors, the best was surprisingly PA-
Alterego, a list-based variable inspired by Dale and
Chall (1948), but adapted to the FFL context, since
the list of easy words used came from a FFL text-
book (Alter Ego 1). This suggests that, although the
predictive power of ?specific to FFL? features was
low, specialization to the FFL context was beneficial
at other levels.
471
4.2 The models
Once the best single predictors were identified, it
was possible to combine several of them in a read-
ability model for comparison. This required some
corpus preparation. Since preliminary experiments
showed that the equal prior probabilities are required
to ensure a unbiased training, the whole corpus was
resampled to get the same number of texts per level
(108), which amounted to a total of 648 texts. We
then split this smaller corpus into two sets. 240 texts
were kept for development purposes, mainly feature
selection and estimation of the meta-parameters ?
and C for the SVM. The remaining 408 texts were
used for evaluating performance of our readability
models.
4.2.1 Selection of the features
Several ways of selecting the smallest ?best? sub-
set of features were compared, given that some
variables are partly redundant when combined to-
gether. The first method was based on the
structuro-cognitivist assumption that readability for-
mulas should include other features than just lexico-
syntactical ones, in order to maximize variety of in-
formation. Therefore, we tried an ?expert? selec-
tion, keeping either the best feature among each of
the four families (set Exp1), or the two best features
(set Exp2) 5.
These ?expert? approaches were compared to an
automatic selection, using either a stepwise proce-
dure 6 for logistic regression (OLR and MLR) or
a built-in regularization (Bishop, 2006, 10) for the
SVM, based on the 46 best predictors inside each
subfamily.
For the sake of comparison, we also defined two
other sets: one that corresponds to a random clas-
sification (the empty subset), and a baseline, based
on two classics predictors (number of letters per
word and number of words per sentence), which
aimed to mimic classic formulas such as those of
5For the syntactic level, since the two best variables be-
longed to the same subfamily (see Section 3.2) and were too
highly intercorrelated, the 90th percentile of the sentence length
(NWS90) was replaced by the best feature from another subfam-
ily: the presence of at least one present participle (PPres).
6In order to suppress as much random effects as possible, the
selection process was repeated 100 times via a bootstrapping
.632 procedure (Tuffe?ry, 2007, 396-371) and only the features
selected at least 50 times out of 100 were kept.
Flesch (1948) or Dale and Chall (1948). A summary
of the features included in each subset is available in
Table 3.
4.2.2 Evaluation of the models
The next step then consisted in training logistic
and SVM models for each of the above subsets.
Their performances, reported in Table 4, were as-
sessed using five measures: the multiple correlation
ratio (R), the accuracy (acc), the adjacent accuracy 7
(adjacc), the root mean square error (rmse) and the
mean absolute error (mae). It should be noted that
each of these measures was estimated through a ten-
fold cross-validation procedure, which allowed us to
compare performances of different models with a T-
test.
The comparison between the models was per-
formed in two steps. First, we computed T-tests
based on adjacc to compare the models based on
a same set of features (either Exp1, Exp2, or Auto).
This allowed us to pick up the best classifier for each
set. In a second step, these three best models were
compared the same way, which resulted in the se-
lection of the very best classifier. The decision of
adopting the adjacent accuracy as a criterion instead
of the accuracy was motivated by our conviction that
our system should rather avoid serious errors (i.e.
larger than one level) than be more accurate, while
sometimes generating terrible mistakes. However, it
appeared that both metrics were mostly consistent.
The performance of the different models are dis-
played in Table 4. It is first interesting to note that
the baseline (based on SVM) already gives interest-
ing results. It reaches a classification accuracy of
34%, which is about twice the random. As regards
the first model (Exp1), based on RLM and including
four predictors, it outperforms the baseline by 5%, a
difference close to significance (t(9) = 1.77; p =
0.055). Therefore, combining variables from sev-
eral families seems to improve performance over the
?classic? baseline, limited to lexico-syntactic fea-
tures.
This finding is reinforced by the SVM model
from Exp2, which includes eight features. It per-
forms significantly better than the baseline (t(9) =
7Heilman et al2008) defined it as ?the proportion of pre-
dictions that were within one level of the human assigned label
for the given text?.
472
Model name Classifieur Set of features
Exp1 OLR, MLR and SVM PA-Alterego + NMP + avLocalLsa-Lem + BINGUI
Exp2 OLR, MLR and SVM PA-Alterego + X90FFFC + NMP + PPres + avLocalLsa-Lem + PP1P2 + BINGUI + NAColl
Auto-OLR OLR PA-Alterego + NMP + PPres + ML3
Auto-MLR MLR
PA-Alterego + Cond + Imperatif + Impf + PPasse + PPres + Subi + Subp
+ BINGUI + TTR + NWS90 + LSDaoust + MedNeigh+Freq
Auto-SVM SVM all the 46 variables
Table 3: Results from the two selection process: expert and automatic. Description of the features can be found in
Table 2.
Model Classifier Parameters R acc adjacc rmse mae
Random / / / 16.6 44.4 / /
Baseline SVM ? = 0.05;C = 25 0.62 34.0 68.2 1.51 1.06
Exp1 RLM / 0.70 39.4 74.2 1.34 0.97
Exp2 SVM ? = 0.002;C = 75 0.73 40.8 77.9 1.28 0.94
Auto-OLR OLR / 0.71 39.6 76.1 1.33 0.96
Auto SVM ? = 0.004;C = 5 0.73 49.1 79.6 1.27 0.90
Table 4: Evaluation measures for the best difficulty model from each feature set (Exp1, Exp2 and Auto), along with
values for a random classification, and the ?classic? baseline.
2.36; p = 0.02), with an accuracy gain of 7%. How-
ever, to that point, it was not clear whether this supe-
riority was indeed a consequence of maximizing the
kind of information brought to the model or merely
the result of the increased number of predictor.
We thus performed another experiment to address
this issue. The model Exp1 was compared with
Auto-OLR, the best ordinal logistic model obtained
through the stepwise selection (see Tables 4 and
3), and previously discarded as a result of the T-
test comparisons. Like Exp1, it also contains four
predictors, but they are all lexical or syntactic fea-
tures. Therefore, this model does not maximize the
type of information. Surprisingly, we observed that
Auto-OLR obtained similar and even slightly bet-
ter performance than Exp1 (+2% for both acc and
adjacc). Thus, the claim that maximizing the source
of information should yield better models did not
stand on our data.
Finally, our best performing model was based on
the Auto feature set and SVM. Its accuracy was in-
creased by 8% in comparison with the Exp2 model,
which is clearly a significant improvement (t(9) =
2.61; p = 0.01), and outperformed the baseline by
15%. As mentioned previously, this model includes
46 features coming from our four families. It is
worth mentioning that the quality of the predictions
is not the same across the levels, as shown in Ta-
ble 5. They are more accurate for classes situated
at both ends of the difficulty scale, namely A1, C1
and C2. For A1, this is explained because texts for
beginners are more typical, having very short sen-
tences and simple words. However, the case of C1
and C2 classes is more surprising and might be due
to some specificities of the learning algorithm.
A1 A2 B1 B2 C1 C2
Adj. acc. 100% 71% 67% 71% 86% 83%
Table 5: Adjacent accuracy per level, computed on one
of the 10 folds. Its adjacent accuracy was 79%, which is
very similar to the average value of the model.
We also assessed the specific contribution of each
family of features in two ways: on one hand, we
trained a model including only the features from this
family; on the other hand, we trained a model in-
cluding all features except those from this family.
Results for the four families are displayed at Table 6.
It appeared that the lexical family was the most
accurate set of predictors (40.5%) and yielded the
highest loss in performance when set aside, espe-
cially for adjacent accuracy. In fact, this was the
only set whose absence significantly impacted ad-
jacent accuracy, suggesting that the other type of
predictors can only improve the accuracy of predic-
tions, but are not able to reduce the amount of crit-
ical mistakes. The second best family was, expect-
edly, the syntactic one. Its accuracy closely match
that of the lexical set, although more severe mistakes
were made, as shown by the drop in adjacent accu-
473
racy. Finally, our two other families was clearly in-
ferior, but they still improved slightly the accuracy
of our model, although not the adjacent accuracy.
Family only All except family
Acc. Adj. acc. Acc. Adj. acc.
Lexical 40.5 75.6 41.1 73.5
Syntactic 39.3 69.5 43.2 78.4
Semantic 28.8 61.5 47.8 79.2
FFL 24.9 58.5 47.8 79.6
Table 6: Accuracy and adjacent accuracy (in percentage)
for models either using only one family of predictors, or
including all 46 features except those of one family.
4.2.3 Comparaison with previous work
Comparisons with other FFL models are difficult
to provide: not only there are few formulas available
for FFL, but some of these focus on a different au-
dience, making comparability low. This is why we
were able to compare our results with only two pre-
vious models.
The first of them is a classic readability formula
by Kandel and Moles (1958), which is an adaptation
of the Flesch (1948) formula for French:
Y = 207? 1.015lp? 0.736lm (2)
where Y is a readability score ranging from 100
(easiest) to 0 (harder); lp is the average number of
words per sentence and lm is the average number
of syllables per 100 words. Although it was not de-
signed for FFL, we considered it, since it is one of
the most well-known formula for French and the two
features combined are very general. Their predic-
tive power should not vary much in both contexts, as
shown by Greenfield (2004) for English. We evalu-
ated it on the same test corpus as our SVM model
and obtained really lower values : a R of 0.55 and
an accuracy of 33%.
The second model was that of Franc?ois (2009),
which is based on a multinomial logistic regression
including ten features: a unigram model similar to
ML3, the number of letters per word, the number of
words per sentence, and binary variables indicating
the presence of a past participle, present participle,
imperfect, infinitive, conditional, future and present
subjunctive tenses in the text. To our knowledge,
this model is the best current generic model avail-
able for FFL. On our data, it yielded an accuracy of
41% and an adjacent accuracy of 72.7%, both esti-
mated through a 10-fold cross-validation procedure.
Therefore, our new approach achieved an accuracy
gain of 8% over this state-of-the-art model, which
was considered as a significant difference (t(9) =
3.72; p = 0.002).
Apart of those two studies, Uitdenbogerd (2005)
also developed recently a FFL formula. However, as
explained previously, this work focused on a spe-
cific category of L2 readers, the English-speakers
learning FFL, which resulted in a different problem.
She reported a higher R than us (0.87 against 0.73).
However, this value might be the training one and
was estimated on a small amount of novel begin-
nings. It is therefore likely that our model generalize
better, especially across genres and L2 readers with
different L1 backgrounds.
5 Discussion and conclusion
In this paper, we introduced a new ?AI readability?
formula for FFL, able to predict the level of texts
according to the largely-spread CEFR scale. Our
model is based on a SVM classifier and combines 46
features corresponding to several levels of linguis-
tic information. Among those, we suggested some
new features: the normalized TTR and the set of
variables based on several characteristics of words?
neighbors. Comparing our approach with two pre-
viously published formulas, our model significantly
outperformed both these works. Therefore, it repre-
sent a robust generic solution for FFL readers will-
ing to find various kind of texts that suit their lin-
guistic abilities.
Besides the creation of a new FFL readability
formula, this study produced two valuable insights.
First, we showed that maximizing the type of lin-
guistic information might not be the best path to go,
since a model based on four lexico-syntactic fea-
tures yielded predictions as accurate as those of a
model relying on our Exp1 set of variables. How-
ever, this finding might be partly accounted by the
lower predictive power of the features from the se-
mantic and specific-to-FFL family, with the notable
exception of the LSA-based predictor (avLocalLsa-
Lem), which is the third best predictor when consid-
ered alone.
This leads us to our second finding, relative to the
474
set of semantic features. Yet their importance was
largely praised in the structuro-cognitivist paradigm
and in most of the recent works, our experiments
cast serious doubts about their efficiency, at least in
a L2 context. Not only the expert models, to which
we imposed the presence of one or two semantic pre-
dictors, did not perform the best, but none of the
features from our semantic set was retained during
the automatic selection of the variables for the lo-
gistic models. On the contrary, in some subsets,
the LSA-based feature was sometimes considered as
collinear with the other variables. Finally and fore-
most, we showed that dropping the semantic features
did not impact significantly the performance of our
best model.
With reservations one may have because of the
limited number of semantic predictors in our set,
these results however raise some concerns about
whether the information coming from semantic vari-
ables is really different from that carried by lexico-
syntactic features. Our results clearly show that
this may not be the case. This conclusion con-
tradicts the assumptions of the structuro-cognitivist
paradigm, but corroborates Chall and Dale (1995)?s
view that the information carried by semantic pre-
dictors is largely correlated with that of lexico-
syntactical ones.
Further investigation on this issue would defi-
nitely be worthwhile, since several facts could ex-
plain these contradictory findings. First, it might be
that semantic and lexical predictors are correlated
because the methods used for the parameterization
of the semantic factors heavily relie on lexical infor-
mation. This is the case for the LSA, as well as for
the propositional approach of the content density.
Alternatively, this difference with other work in
L1 could be due to the L2 context. Chall and
Dale (1995) explained that the lexicon and the syn-
tax are more important for children learning to read
than for more advanced readers, who then become
more sensitive to organisationnal aspects. From the
threshold hypothesis (Alderson, 1984), we know
that before reaching a sufficient level of proficiency,
L2 learners struggle mostly with the lexicon and
the syntactic structures. This might explain why
lexico-syntactic predictors were so predominant in
our experiments. Some further experiments are thus
needed to investigate which of these facts better ac-
count for our findings on the semantic features.
A last avenue of research worth mentioning would
be to develop the family of specific-to-FFL predic-
tors, to determine whether taking into account the
impact of a given L1 language on the readability of
L2 texts would increase performance over a generic
model enough so that tuning efforts are worthwhile.
Acknowledgments
Thomas Franc?ois was an Aspirant F.N.R.S. when
this work was performed. The writing of this paper
was done while being a recipient of a Fellowship of
the Belgian American Educational Foundation. We
thank both for their support. We would also like to
acknowledge the invaluable help of Bernadette De-
hottay for the collection of the corpus used in that
study.
References
J.C. Alderson. 1984. Reading in a foreign language :
a reading problem or a language problem ? In J.C.
Alderson and A.H Urquhart, editors, Reading in a For-
eign Language, pages 1?24. Longman, New York.
S. Andrews. 1997. The effect of orthographic similarity
on lexical retrieval: Resolving neighborhood conflicts.
Psychonomic Bulletin & Review, 4(4):439?461.
J. Bahns and M. Eldaw. 1993. Should We Teach EFL
Students Collocations? System, 21(1):101?14.
A. Berthet, C. Hugot, V. Kizirian, B. Sampsonis, and
M. Waendendries. 2006. Alter Ego 1. Hachette, Paris.
C.M. Bishop. 2006. Pattern recognition and machine
learning. Springer, New York.
J.R. Bormuth. 1966. Readability: A new approach.
Reading research quarterly, 1(3):79?132.
J.S. Bowers. 2000. In defense of abstractionist theories
of repetition priming and word identification. Psycho-
nomic bulletin and review, 7(1):83?99.
M. Carreiras, N. Carriedo, M.A. Alonso, and
A. Ferna?ndez. 1997. The role of verb tense and
verb aspect in the foregrounding of information
during reading. Memory & Cognition, 25(4):438?446.
J.S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brookline
Books, Cambridge.
S. Chen and J. Goodman. 1999. An empirical study of
smoothing techniques for language modeling. Com-
puter Speech and Language, 13(4):359?393.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
475
Proceedings of HLT/NAACL 2004, pages 193?200,
Boston, USA.
K. Collins-Thompson and J. Callan. 2005. Predict-
ing reading difficulty with statistical language models.
Journal of the American Society for Information Sci-
ence and Technology, 56(13):1448?1462.
M. Coltheart. 1978. Lexical access in simple reading
tasks. In G. Underwood, editor, Strategies of infor-
mation processing, pages 151?216. Academic Press,
London.
A. Conquet. 1957. La lisibilite?. Assemble?e Permanente
des CCI de Paris, Paris.
C.M. Cornaire. 1988. La lisibilite? : essai d?application
de la formule courte d?Henry au franc?ais langue
e?trange`re. Canadian Modern Language Review,
44(2):261?273.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Press Syndicate of the University of
Cambridge.
E. Dale and J.S. Chall. 1948. A formula for predicting
readability. Educational research bulletin, 27(1):11?
28.
E. Dale and R.W. Tyler. 1934. A study of the fac-
tors influencing the difficulty of reading materials for
adults of limited reading ability. The Library Quar-
terly, 4:384?412.
F. Daoust, L. Laroche, and L. Ouellet. 1996. SATO-
CALIBRAGE: Pre?sentation d?un outil d?assistance au
choix et a` la re?daction de textes pour l?enseignement.
Revue que?be?coise de linguistique, 25(1):205?234.
G. de Landsheere. 1963. Pour une application des tests
de lisibilite? de Flesch a` la langue franc?aise. Le Travail
Humain, 26:141?154.
E.W. Dolch. 1948. Problems in reading. The Garrard
Press, Champaign : Illinois.
W.B. Elley. 1969. The assessment of readability by
noun frequency counts. Reading Research Quarterly,
4(3):411?427.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A Comparison of Features for Automatic Read-
ability Assessment. In COLING 2010: Poster Volume,
pages 276?284.
R. Flesch. 1948. A new readability yardstick. Journal of
Applied Psychology, 32(3):221?233.
P.W. Foltz, W. Kintsch, and T.K. Landauer. 1998. The
measurement of textual coherence with latent semantic
analysis. Discourse processes, 25(2):285?307.
T. Franc?ois and P. Watrin. 2011. On the contribution
of MWE-based features to a readability formula for
French as a foreign language. In Proceedings of the
International Conference RANLP 2011.
T. Franc?ois. 2009. Combining a statistical language
model with logistic regression to predict the lexical
and syntactic difficulty of texts for FFL. In Proceed-
ings of the 12th Conference of the EACL : Student Re-
search Workshop, pages 19?27.
T. Franc?ois. 2011a. La lisibilite? computationnelle
: un renouveau pour la lisibilite? du franc?ais langue
premie`re et seconde ? International Journal of Ap-
plied Linguistics (ITL), 160:75?99.
T. Franc?ois. 2011b. Les apports du traitement au-
tomatique du langage a` la lisibilite? du franc?ais langue
e?trange`re. Ph.D. thesis, Universite? Catholique de Lou-
vain. Thesis Supervisors : Ce?drick Fairon and Anne
Catherine Simon.
W.A. Gale and G. Sampson. 1995. Good-Turing fre-
quency estimation without tears. Journal of Quantita-
tive Linguistics, 2(3):217?237.
G. Gougenheim, R. Miche?a, P. Rivenc, and A. Sauvageot.
1964. L?e?laboration du franc?ais fondamental (1er
degre?). Didier, Paris.
A.C. Graesser, D.S. McNamara, M.M. Louwerse, and
Z. Cai. 2004. Coh-Metrix: Analysis of text on co-
hesion and language. Behavior Research Methods, In-
struments, & Computers, 36(2):193?202.
J. Greenfield. 2004. Readability formulas for EFL.
Japan Association for Language Teaching, 26(1):5?
24.
M. Heilman, K. Collins-Thompson, and M. Eskenazi.
2008. An analysis of statistical models and features
for reading difficulty prediction. In Proceedings of the
Third Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 1?8.
G. Henry. 1975. Comment mesurer la lisibilite?. Labor,
Bruxelles.
L. Kandel and A. Moles. 1958. Application de l?indice
de Flesch a` la langue franc?aise. Cahiers E?tudes de
Radio-Te?le?vision, 19:253?274.
S. Kemper. 1983. Measuring the inference load of a text.
Journal of Educational Psychology, 75(3):391?401.
W. Kintsch and D. Vipond. 1979. Reading comprehen-
sion and readability in educational practice and psy-
chological theory. In L.G. Nilsson, editor, Perspec-
tives on Memory Research, pages 329?365. Lawrence
Erlbaum, Hillsdale, NJ.
W. Kintsch, E. Kozminsky, W.J. Streby, G. McKoon, and
J.M. Keenan. 1975. Comprehension and recall of text
as a function of content variables1. Journal of Verbal
Learning and Verbal Behavior, 14(2):196?214.
H. Lee, P. Gambette, E. Maille?, and C. Thuillier. 2010.
Denside?es: calcul automatique de la densite? des ide?es
dans un corpus oral. In Actes de la douxime Rencon-
tre des tudiants Chercheurs en Informatique pour le
Traitement Automatique des langues (RECITAL).
476
I. Lorge. 1944. Predicting readability. the Teachers Col-
lege Record, 45(6):404?419.
J. Mesnager. 1989. Lisibilite? des textes pour enfants: un
nouvel outil? Communication et Langages, 79:18?38.
B.J.F. Meyer. 1982. Reading research and the composi-
tion teacher: The importance of plans. College com-
position and communication, 33(1):37?49.
J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K. Gray,
The Google Books Team, J.P. Pickett, D. Hoiberg,
D. Clancy, P. Norvig, J. Orwant, S. Pinker, M.A.
Nowak, and E.L. Aiden. 2011. Quantitative analysis
of culture using millions of digitized books. Science,
331(6014):176?182.
J.R. Miller and W. Kintsch. 1980. Readability and re-
call of short prose passages: A theoretical analysis.
Journal of Experimental Psychology: Human Learn-
ing and Memory, 6(4):335?354.
B. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007.
The use of film subtitles to estimate word frequencies.
Applied Psycholinguistics, 28(04):661?677.
R.E. O?Connor, K.M. Bell, K.R. Harty, L.K. Larkin, S.M.
Sackor, and N. Zigmond. 2002. Teaching reading to
poor readers in the intermediate grades: A comparison
of text difficulty. Journal of Educational Psychology,
94(3):474?485.
T. Ozasa, G. Weir, and M. Fukui. 2007. Measuring read-
ability for Japanese learners of English. In Proceed-
ings of the 12th Conference of Pan-Pacific Association
of Applied Linguistics.
E. Pitler and A. Nenkova. 2008. Revisiting readabil-
ity: A unified framework for predicting text quality. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 186?195.
J.C. Redish and J. Selzer. 1985. The place of readability
formulas in technical communication. Technical com-
munication, 32(4):46?52.
F. Richaudeau. 1979. Une nouvelle formule de lisibilite?.
Communication et Langages, 44:5?26.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of International
Conference on New Methods in Language Processing,
volume 12. Manchester, UK.
S.E. Schwarm and M. Ostendorf. 2005. Reading level
assessment using support vector machines and statis-
tical language models. Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 523?530.
L. Si and J. Callan. 2001. A statistical model for sci-
entific readability. In Proceedings of the Tenth Inter-
national Conference on Information and Knowledge
Management, pages 574?576. ACM New York, NY,
USA.
E.A. Smith. 1961. Devereaux readability index. The
Journal of Educational Research, 54(8):289?303.
A.J. Stenner. 1996. Measuring reading comprehension
with the lexile framework. In Fourth North American
Conference on Adolescent/Adult Literacy.
J.B. Tharp. 1939. The Measurement of Vocabulary Dif-
ficulty. Modern Language Journal, pages 169?178.
S. Tuffe?ry. 2007. Data mining et statistique
de?cisionnelle l?intelligence des donne?es. E?d. Technip,
Paris.
S. Uitdenbogerd. 2005. Readability of French as a for-
eign language and its uses. In Proceedings of the Aus-
tralian Document Computing Symposium, pages 19?
25.
P. van Oosten, V. Hoste, and D. Tanghe. 2011. A pos-
teriori agreement as a quality measure for readabil-
ity prediction systems. In A. Gelbukh, editor, Com-
putational Linguistics and Intelligent Text Processing,
volume 6609 of Lecture Notes in Computer Science,
pages 424?435. Springer, Berlin / Heidelberg.
477
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 770?779,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A hybrid rule/model-based finite-state framework
for normalizing SMS messages
Richard Beaufort1 Sophie Roekhaut2 Louise-Am?lie Cougnon1 C?drick Fairon1
(1) CENTAL, Universit? catholique de Louvain ? 1348 Louvain-la-Neuve, Belgium
{richard.beaufort,louise-amelie.cougnon,cedrick.fairon}@uclouvain.be
(2) TCTS Lab, Universit? de Mons ? 7000 Mons, Belgium
sophie.roekhaut@umons.ac.be
Abstract
In recent years, research in natural
language processing has increasingly
focused on normalizing SMS messages.
Different well-defined approaches have
been proposed, but the problem remains
far from being solved: best systems
achieve a 11% Word Error Rate. This
paper presents a method that shares
similarities with both spell checking
and machine translation approaches. The
normalization part of the system is entirely
based on models trained from a corpus.
Evaluated in French by 10-fold-cross
validation, the system achieves a 9.3%
Word Error Rate and a 0.83 BLEU score.
1 Introduction
Introduced a few years ago, Short Message
Service (SMS) offers the possibility of exchanging
written messages between mobile phones. SMS
has quickly been adopted by users. These
messages often greatly deviate from traditional
spelling conventions. As shown by specialists
(Thurlow and Brown, 2003; Fairon et al,
2006; Bieswanger, 2007), this variability is
due to the simultaneous use of numerous coding
strategies, like phonetic plays (2m1 read ?demain?,
?tomorrow?), phonetic transcriptions (kom instead
of ?comme?, ?like?), consonant skeletons (tjrs
for ?toujours?, ?always?), misapplied, missing
or incorrect separators (j esper for ?j?esp?re?, ?I
hope?; j?croibi1k, instead of ?je crois bien que?,
?I am pretty sure that?), etc. These deviations
are due to three main factors: the small number
of characters allowed per text message by the
service (140 bytes), the constraints of the small
phones? keypads and, last but not least, the fact
that people mostly communicate between friends
and relatives in an informal register.
Whatever their causes, these deviations
considerably hamper any standard natural
language processing (NLP) system, which
stumbles against so many Out-Of-Vocabulary
words. For this reason, as noted by Sproat et al
(2001), an SMS normalization must be performed
before a more conventional NLP process can
be applied. As defined by Yvon (2008), ?SMS
normalization consists in rewriting an SMS text
using a more conventional spelling, in order
to make it more readable for a human or for a
machine.?
The SMS normalization we present here was
developed in the general framework of an SMS-
to-speech synthesis system1. This paper, however,
only focuses on the normalization process.
Evaluated in French, our method shares
similarities with both spell checking and machine
translation. The machine translation-like module
of the system performs the true normalization
task. It is entirely based on models learned from
an SMS corpus and its transcription, aligned
at the character-level in order to get parallel
corpora. Two spell checking-like modules
surround the normalization module. The first
one detects unambiguous tokens, like URLs
or phone numbers, to keep them out of the
normalization. The second one, applied on the
normalized parts only, identifies non-alphabetic
sequences, like punctuations, and labels them
with the corresponding token. This greatly helps
the system?s print module to follow the basic rules
of typography.
This paper is organized as follows. Section 2
proposes an overview of the state of the art.
Section 3 presents the general architecture of
our system, while Section 4 focuses on how we
learn and combine our normalization models.
Section 5 evaluates the system and compares it to
1The Vocalise project.
See cental.fltr.ucl.ac.be/team/projects/vocalise/.
770
previous works. Section 6 draws conclusions and
considers some future possible improvements of
the method.
2 Related work
As highlighted by Kobus et al (2008b), SMS
normalization, up to now, has been handled
through three well-known NLP metaphors: spell
checking, machine translation and automatic
speech recognition. In this section, we only
present the pros and cons of these approaches.
Their results are given in Section 5, focused on
our evaluation.
The spell checking metaphor (Guimier de Neef
et al, 2007; Choudhury et al, 2007; Cook and
Stevenson, 2009) performs the normalization task
on a word-per-word basis. On the assumption
that most words should be correct for the purpose
of communication, its principle is to keep In-
Vocabulary words out of the correction process.
Guimier de Neef et al (2007) proposed a rule-
based system that uses only a few linguistic
resources dedicated to SMS, like specific lexicons
of abbreviations. Choudhury et al (2007)
and Cook and Stevenson (2009) preferred to
implement the noisy channel metaphor (Shannon,
1948), which assumes a communication process
in which a sender emits the intended message
W through an imperfect (noisy) communication
channel, such that the sequence O observed by
the recipient is a noisy version of the original
message. On this basis, the idea is to recover the
intended message W hidden behind the sequences
of observations O, by maximizing:
Wmax = arg maxP (W |O) (1)
= arg max
P (O|W )P (W )
P (O)
where P (O) is ignored because constant,
P (O|W ) models the channel?s noise, and P (W )
models the language of the source. Choudhury et
al. (2007) implemented the noisy channel through
a Hidden-Markov Model (HMM) able to handle
both graphemic variants and phonetic plays as
proposed by (Toutanova and Moore, 2002), while
Cook and Stevenson (2009) enhanced the model
by adapting the channel?s noise P (O|W,wf)
according to a list of predefined observed
word formations {wf}: stylistic variation, word
clipping, phonetic abbreviations, etc. Whatever
the system, the main limitation of the spell
checking approach is the excessive confidence it
places in word boundaries.
The machine translation metaphor, which is
historically the first proposed (Bangalore et al,
2002; Aw et al, 2006), considers the process of
normalizing SMS as a translation task from a
source language (the SMS) to a target language
(its standard written form). This standpoint is
based on the observation that, on the one side,
SMS messages greatly differ from their standard
written forms, and that, on the other side, most
of the errors cross word boundaries and require
a wide context to be handled. On this basis,
Aw et al (2006) proposed a statistical machine
translation model working at the phrase-level,
by splitting sentences into their k most probable
phrases. While this approach achieves really good
results, Kobus et al (2008b) make the assertion
that a phrase-based translation can hardly capture
the lexical creativity observed in SMS messages.
Moreover, the translation framework, which can
handle many-to-many correspondences between
sources and targets, exceeds the needs of SMS
normalization, where the normalization task is
almost deterministic.
Based on this analysis, Kobus et al (2008b)
proposed to handle SMS normalization through
an automatic speech recognition (ASR) metaphor.
The starting point of this approach is the
observation that SMS messages present a lot
of phonetic plays that sometimes make the
SMS word (sr?, mwa) closer to its phonetic
representation ([sKe], [mwa]) than to its standard
written form (serai, ?will be?, moi, ?me?).
Typically, an ASR system tries to discover the
best word sequence within a lattice of weighted
phonetic sequences. Applied to the SMS
normalization task, the ASR metaphor consists
in first converting the SMS message into a phone
lattice, before turning it into a word-based lattice
using a phoneme-to-grapheme dictionary. A
language model is then applied on the word
lattice, and the most probable word sequence is
finally chosen by applying a best-path algorithm
on the lattice. One of the advantages of the
grapheme-to-phoneme conversion is its intrinsic
ability to handle word boundaries. However,
this step also presents an important drawback,
raised by the authors themselves: it prevents
next normalization steps from knowing what
graphemes were in the initial sequence.
771
Our approach, which is detailed in Sections 3
and 4, shares similarities with both the spell
checking approach and the machine translation
principles, trying to combine the advantages
of these methods, while leaving aside their
drawbacks: like in spell checking systems, we
detect unambiguous units of text as soon as
possible and try to rely on word boundaries when
they seem reliable enough; but like in the machine
translation task, our method intrinsically handles
word boundaries in the normalization process if
needed.
3 Overview of the system
3.1 Tools in use
In our system, all lexicons, language models
and sets of rules are compiled into finite-state
machines (FSMs) and combined with the input
text by composition (?). The reader who is
not familiar with FSMs and their fundamental
theoretical properties, like composition, is urged
to consult the state-of-the-art literature (Roche
and Schabes, 1997; Mohri and Riley, 1997; Mohri
et al, 2000; Mohri et al, 2001).
We used our own finite-state tools: a finite-state
machine library and its associated compiler
(Beaufort, 2008). In conformance with the format
of the library, the compiler builds finite-state
machines from weighted rewrite rules, weighted
regular expressions and n-gram models.
3.2 Aims
We formulated four constraints before fixing the
system?s architecture. First, special tokens, like
URLs, phones or currencies, should be identified
as soon as possible, to keep them out of the
normalization process.
Second, word boundaries should be taken into
account, as far as they seem reliable enough. The
idea, here, is to base the decision on a learning
able to catch frequent SMS sequences to include
in a dedicated In-Vocabulary (IV) lexicon.
Third, any other SMS sequence should be
considered as Out-Of-Vocabulary (OOV), on
which in-depth rewritings may be applied.
Fourth, the basic rules of typography and
typesettings should be applied on the normalized
version of the SMS message.
3.3 Architecture
The architecture depicted in Figure 1 directly
relies on these considerations. In short, an
SMS message first goes through three SMS
modules, which normalize its noisy parts.
Then, two standard NLP modules produce a
morphosyntactic analysis of the normalized text.
A last module, finally, takes advantage of this
linguistic analysis either to print a text that follows
the basic rules of typography, or to synthesize the
corresponding speech signal.
Because this paper focuses on the normalization
task, the rest of this section only presents the
SMS modules and the ?smart print? output. The
morphosyntactic analysis, made of state-of-the-art
algorithms, is described in (Beaufort, 2008), and
the text-to-speech synthesis system we use is
presented in (Colotte and Beaufort, 2005).
3.3.1 SMS modules
SMS preprocessing. This module relies
on a set of manually-tuned rewrite rules. It
identifies paragraphs and sentences, but also some
SMS Modules
SMS Preprocessing
SMS Normalization
SMS Postprocessing
Standard NLP Modules
Morphological analysis
Contextual disambiguation
TTS engineSmart print
SMS message
Standard
written message Speech
Figure 1: Architecture of the system
772
unambiguous tokens: URLs, phone numbers,
dates, times, currencies, units of measurement
and, last but not least in the context of SMS,
smileys2. These tokens are kept out of the
normalization process, while any other sequence
of characters is considered ? and labelled ? as
noisy.
SMS normalization. This module only uses
models learned from a training corpus (cf. Section
4). It involves three steps. First, an SMS-
dedicated lexicon look-up, which differentiates
between known and unknown parts of a noisy
token. Second, a rewrite process, which creates a
lattice of weighted solutions. The rewrite model
differs depending on whether the part to rewrite
is known or not. Third, a combination of the
lattice of solutions with a language model, and the
choice of the best sequence of lexical units. At
this stage, the normalization as such is completed.
SMS postprocessing. Like the preprocessor,
the postprocessor relies on a set of manually-
tuned rewrite rules. The module is only applied
on the normalized version of the noisy tokens,
with the intention to identify any non-alphabetic
sequence and to isolate it in a distinct token.
At this stage, for instance, a point becomes a
?strong punctuation?. Apart from the list of
tokens already managed by the preprocessor,
the postprocessor handles as well numeric and
alphanumeric strings, fields of data (like bank
account numbers), punctuations and symbols.
3.3.2 Smart print
The smart print module, based on manually-tuned
rules, checks either the kind of token (chosen
by the SMS pre-/post-processing modules)
or the grammatical category (chosen by the
morphosyntactic analysis) to make the right
typography choices, such as the insertion of
a space after certain tokens (URLs, phone
numbers), the insertion of two spaces after
a strong punctuation (point, question mark,
exclamation mark), the insertion of two carriage
returns at the end of a paragraph, or the upper
case of the initial letter at the beginning of the
sentence.
2Our list contains about 680 smileys.
4 The normalization models
4.1 Overview of the normalization algorithm
Our approach is an approximation of the noisy
channel metaphor (cf. Section 2). It differs
from this general framework, because we adapt
the model of the channel?s noise depending
on whether the noisy token (our sequence
of observations) is In-Vocabulary or Out-Of-
Vocabulary:
P (O|W ) =
?
?
?
PIV (O|W ) if O ? IV
POOV (O|W ) else
(2)
Indeed, our algorithm is based on the assumption
that applying different normalization models to IV
and OOV words should both improve the results
and reduce the processing time.
For this purpose, the first step of the algorithm
consists in composing a noisy token T with an
FST Sp whose task is to differentiate between
sequences of IV words and sequences of OOV
words, by labelling them with a special IV or OOV
marker. The token is then split in n segments sgi
according to these markers:
{sg} = Split(T ? Sp) (3)
In a second step, each segment is composed
with a rewrite model according to its kind: the IV
rewrite modelRIV for sequences of IV words, and
the OOV rewrite model ROOV for sequences of
OOV words:
sg?i =
?
?
?
sgi ?RIV if sgi ? IV
sgi ?ROOV else
(4)
All rewritten segments are then concatenated
together in order to get back the complete token:
T = ni=1(sg
?
i) (5)
where  is the concatenation operator.
The third and last normalization step is applied
on a complete sentence S. All tokens Tj of S
are concatenated together and composed with the
lexical language model LM . The result of this
composition is a word lattice, of which we take
the most probable word sequence S? by applying
a best-path algorithm:
S? = BestPath( (mj=1Tj) ? LM ) (6)
where m is the number of tokens of S. In S?,
each noisy token Tj of S is mapped onto its most
probable normalization.
773
4.2 The corpus alignment
Our normalization models were trained on a
French SMS corpus of 30,000 messages, gathered
in Belgium, semi-automatically anonymized and
manually normalized by the Catholic University
of Louvain (Fairon and Paumier, 2006). Together,
the SMS corpus and its transcription constitute
parallel corpora aligned at the message-level.
However, in order to learn pieces of knowledge
from these corpora, we needed a string alignment
at the character-level.
One way of implementing this string alignment
is to compute the edit-distance of two strings,
which measures the minimum number of
operations (substitutions, insertions, deletions)
required to transform one string into the other
(Levenshtein, 1966). Using this algorithm,
in which each operation gets a cost of 1, two
strings may be aligned in different ways with
the same global cost. This is the case, for
instance, for the SMS form kozer ([koze]) and
its standard transcription caus? (?talked?), as
illustrated by Figure 2. However, from a linguistic
standpoint, alignment (1) is preferable, because
corresponding graphemes are aligned on their first
character.
In order to automatically choose this preferred
alignment, we had to distinguish the three edit-
operations, according to the characters to be
aligned. For that purpose, probabilities were
required. Computing probabilities for each
operation according to the characters to be aligned
was performed through an iterative algorithm
described in (Cougnon and Beaufort, 2009). In
short, this algorithm gradually learns the best way
of aligning strings. On our parallel corpora, it
converged after 7 iterations and provided us with
a result from which the learning could start.
(1) ko_ser (2) k_oser
caus?_ caus?_
(3) ko_ser (4) k_oser
caus_? caus_?
Figure 2: Different equidistant alignments, using
a standard edit-cost of 1. Underscores (?_?) mean
insertion in the upper string, and deletion in the
lower string.
4.3 The split model Sp
In natural language processing, a word is
commonly defined as ?a sequence of alphabetic
characters between separators?, and an IV word is
simply a word that belongs to the lexicon in use.
In SMS messages however, separators are
surely indicative, but not reliable. For this reason,
our definition of the word is far from the previous
one, and originates from the string alignment.
After examining our parallel corpora aligned at
the character-level, we decided to consider as a
word ?the longest sequence of characters parsed
without meeting the same separator on both sides
of the alignment?. For instance, the following
alignment
J esper_ k___tu va_
J?esp?re que tu vas
(I hope that you will)
is split as follows according to our definition:
J esper_ k___tu va_
J?esp?re que tu vas
since the separator in ?J esper? is different
from its transcription, and ?ktu? does not
contain any separator. Thus, this SMS sequence
corresponds to 3 SMS words: [J esper], [ktu] and
[va].
A first parsing of our parallel corpora provided
us with a list of SMS sequences corresponding to
our IV lexicon. The FST Sp is built on this basis:
Sp = ( S? (I|O) ( S+(I|O) )? S? ) ?G (7)
where:
? I is an FST corresponding to the lexicon,
in which IV words are mapped onto the IV
marker.
? O is the complement of I3. In this OOV
lexicon, OOV sequences are mapped onto the
OOV marker.
? S is an FST corresponding to the list of
separators (any non-alphabetic and non-
numeric character), mapped onto a SEP
marker.
3Actually, the true complement of I accepts sequences
with separators, while these sequences were removed from
O.
774
? G is an FST able to detect consecutive
sequences of IV (resp. OOV) words, and to
group them under a unique IV (resp. OOV)
marker. By gathering sequences of IVs and
OOVs, SEP markers disappear from Sp.
Figure 3 illustrates the composition of Sp with
the SMS sequence J esper kcv b1 (J?esp?re que ?a
va bien, ?I hope you are well?). For the example,
we make the assumption that kcv was never seen
during the training.
eJ s p e r k c v b 1'  ' '  ' '  '
IV IVOOV
Figure 3: Application of the split model Sp. The
OOV sequence starts and ends with separators.
4.4 The IV rewrite model RIV
This model is built during a second parsing
of our parallel corpora. In short, the parsing
simply gathers all possible normalizations for
each SMS sequence put, by the first parsing, in
the IV lexicon. Contrary to the first parsing, this
second one processes the corpus without taking
separators into account, in order to make sure
that all possible normalizations are collected.
Each normalization w? for a given SMS
sequence w is weighted as follows:
p(w?|w) =
Occ(w?, w)
Occ(w)
(8)
where Occ(x) is the number of occurrences of x in
the corpus. The FST RIV is then built as follows:
RIV = SIV
? IVR ( SIV
+ IVR )
? SIV
? (9)
where:
? IVR is a weighted lexicon compiled into an
FST, in which each IV sequence is mapped
onto the list of its possible normalizations.
? SIV is a weighted lexicon of separators, in
which each separator is mapped onto the list
of its possible normalizations. The deletion
is often one of the possible normalization of
a separator. Otherwise, the deletion is added
and is weighted by the following smoothed
probability:
p(DEL|w) =
0.1
Occ(w) + 0.1
(10)
4.5 The OOV rewrite model ROOV
In contrast to the other models, this one is not a
regular expression made of weighted lexicons.
It corresponds to a set of weighted rewrite rules
(Chomsky and Halle, 1968; Johnson, 1972; Mohri
and Sproat, 1996) learned from the alignment.
Developed in the framework of generative
phonology, rules take the form
?? ? : ? _ ? / w (11)
which means that the replacement ? ? ? is
only performed when ? is surrounded by ? on
the left and ? on the right, and gets the weight w.
However, in our case, rules take the simpler form
?? ? / w (12)
which means that the replacement ? ? ? is
always performed, whatever the context.
Inputs of our rules (?) are sequences of
1 to 5 characters taken from the SMS side
of the alignment, while outputs (?) are their
corresponding normalizations. Our rules are
sorted in the reverse order of the length of their
inputs: rules with longer inputs come first in the
list.
Long-to-short rule ordering reduces the number
of proposed normalizations for a given SMS
sequence for two reasons:
1. the firing of a rule with a longer input blocks
the firing of any shorter sub-rule. This is due
to a constraint expressed on lists of rewrite
rules: a given rule may be applied only if no
more specific and relevant rule has been met
higher in the list;
2. a rule with a longer input usually has fewer
alternative normalizations than a rule with a
shorter input does, because the longer SMS
sequence likely occurred paired with fewer
alternative normalizations in the training
corpus than did the shorter SMS sequence.
Among the wide set of possible sequences
of 2 to 5 characters gathered from the corpus,
we only kept in our list of rules the sequences
that allowed at least one normalization solely
made of IV words. It is important to notice that
here, we refer to the standard notion of IV word:
while gathering the candidate sequences from the
corpus, we systematically checked each word of
the normalizations against a lexicon of French
775
standard written forms. The lexicon we used
contains about 430,000 inflected forms and is
derived from Morlex4, a French lexical database.
Figure 4 illustrates these principles by focusing
on 3 input sequences: aussi, au and a. As
shown by the Figure, all rules of a set dedicated
to the same input sequence (for instance, aussi)
are optional (??), except the last one, which is
obligatory (?). In our finite-state compiler, this
convention allows the application of all concurrent
normalizations on the same input sequence, as
depicted in Figure 5.
In our real list of OOV rules, the input sequence
a corresponds to 231 normalizations, while au
accepts 43 normalizations and aussi, only 3. This
highlights the interest, in terms of efficiency, of the
long-to-short rule ordering.
4.6 The language model
Our language model is an n-gram of lexical
forms, smoothed by linear interpolation (Chen
and Goodman, 1998), estimated on the normalized
part of our training corpus and compiled into a
weighted FST LMw.
At this point, this FST cannot be combined with
our other models, because it works on lexical units
and not on characters. This problem is solved
by composing LMw with another FST L, which
represents a lexicon mapping each input word,
considered as a string of characters, onto the same
output words, but considered here as a lexical
unit. Lexical units are then permanently removed
from the language model by keeping only the first
projection (the input side) of the composition:
LM = FirstProjection( L ? LMw ) (13)
In this model, special characters, like
punctuations or symbols, are represented by
their categories (light, medium and strong
punctuations, question mark, symbol, etc.), while
special tokens, like URLs or phone numbers,
are handled as token values (URL, phone, etc.)
instead of as sequences of characters. This
reduces the complexity of the model.
As we explained earlier, tokens of a same
sentence S are concatenated together at the end
of the second normalization step. During this
concatenation process, sequences corresponding
to special tokens are automatically replaced by
their token values. Special characters, however,
4See http://bach.arts.kuleuven.be/pmertens/.
"aussi" ?-> "au si" / 8.4113 (*)
"aussi" ?-> "ou si" / 6.6743 (*)
"aussi" -> "aussi" / 0.0189 (*)
...
...
"au" ?-> "ow" / 14.1787
...
"au" ?-> "?t" / 12.5938
"au" ?-> "du" / 12.1787 (*)
"au" ?-> "o" / 11.8568
...
"au" ?-> "on" / 10.8568 (*)
...
"au" ?-> "aud" / 9.9308
"au" ?-> "aux" / 6.1731 (*)
"au" -> "au" / 0.0611 (*)
...
...
"a" ?-> "a d" / 17.8624
"a" ?-> "ation" / 17.8624
"a" ?-> "?ts" / 17.8624
...
"a" ?-> "ablement" / 16.8624
"a" ?-> "anisation" / 16.8624
...
"a" ?-> "u" / 15.5404
"a" ?-> "y a" / 15.5404
...
"a" ?-> "abilit?" / 13.4029
"a" ?-> "?-" / 12.1899
"a" ?-> "ar" / 11.5225
"a" ?-> \DEL / 9.1175
"a" ?-> "?a" / 6.2019
"a" ?-> "?" / 3.5013
"a" -> "a" / 0.3012
Figure 4: Samples from the list of OOV
rules. Rules? weights are negative logarithms
of probabilities: smaller weights are thus better.
Asterisks indicate normalizations solely made of
French IV words.
aa:o/6.67 uu ?:" "/8.41
s/0.02 s s i?:" "
Figure 5: Application of the OOV rules on
the input sequence aussi. All normalizations
corresponding to this sequence were allowed,
while rules corresponding to shorter input
sequences were ignored.
776
are still present in S. For this reason, S is first
composed with an FST Reduce, which maps each
special character onto its corresponding category:
S ?Reduce ? LM (14)
5 Evaluation
The performance and the efficiency of our system
were evaluated on a MacBook Pro with a 2.4 GHz
Intel Core 2 Duo CPU, 4 GB 667 MHz DDR2
SDRAM, running Mac OS X version 10.5.8.
The evaluation was performed on the corpus
of 30,000 French SMS presented in Section 4.2,
by ten-fold cross-validation (Kohavi, 1995). The
principle of this method of evaluation is to split
the initial corpus into 10 subsets of equal size. The
system is then trained 10 times, each time leaving
out one of the subsets from the training corpus, but
using only this omitted subset as test corpus.
The language model of the evaluation is a
3-gram. We did not try a 4-gram. This choice
was motivated by the experiments of Kobus et
al. (2008a), who showed on a French corpus
comparable to ours that, if using a larger language
model is always rewarded, the improvement
quickly decreases with every higher level and is
already quite small between 2-gram and 3-gram.
Table 1 presents the results in terms of
efficiency. The system seems efficient, while we
cannot compare it with other methods, which did
not provide us with this information.
Table 2, part 1, presents the performance of
our approach (Hybrid) and compares it to a trivial
copy-paste (Copy). The system was evaluated
in terms of BLEU score (Papineni et al, 2001),
Word Error Rate (WER) and Sentence Error Rate
(SER). Concerning WER, the table presents the
distribution between substitutions (Sub), deletions
(Del) and insertions (Ins). The copy-paste results
just inform about the real deviation of our corpus
from the traditional spelling conventions, and
highlight the fact that our system is still at pains
to significantly reduce the SER, while results
in terms of WER and BLEU score are quite
encouraging.
Table 2, part 2, provides the results of the
state-of-the-art approaches. The only results truly
comparable to ours are those of Guimier de Neef
et al (2007), who evaluated their approach on
the same corpus as ours5; clearly, our method
5They performed an evaluation without ten-fold cross-
mean dev.
bps 1836.57 159.63
ms/SMS (140b) 76.23 22.34
Table 1: Efficiency of the system.
outperforms theirs. Our results also seem a bit
better than those of Kobus et al (2008a), although
the comparison with this system, also evaluated in
French, is less easy: they combined the French
corpus we used with another one and performed
a single validation, using a bigger training corpus
(36.704 messages) for a test corpus quite similar
to one of our subsets (2.998 SMS). Other systems
were evaluated in English, and results are more
difficult to compare; at least, our results seem in
line with them.
The analysis of the normalizations produced
by our system pointed out that, most often, errors
are contextual and concern the gender (quel(le),
?what?), the number (bisou(s), ?kiss?), the person
([tu t?]inqui?te(s), ?you are worried?) or the
tense (arriv?/arriver, ?arrived?/?to arrive?). That
contextual errors are frequent is not surprising. In
French, as mentioned by Kobus et al (2008b), n-
gram models are unable to catch this information,
as it is generally out of their scope.
On the other hand, this analysis confirmed
our initial assumptions. First, special tokens
(URLs, phones, etc.) are not modified. Second,
agglutinated words are generally split (Pensa ms
? Pense ? mes, ?think to my?), while misapplied
separators tend to be deleted (G t ? J??tais, ?I
was?). Of course, we also found some errors at
word boundaries ([il] l?arrange ? [il] la range,
?[he] arranges? ? ?[he] pits in order?), but they
were fairly rare.
6 Conclusion and perspectives
In this paper, we presented an SMS normalization
framework based on finite-state machines and
developed in the context of an SMS-to-speech
synthesis system. With the intention to avoid
wrong modifications of special tokens and to
handle word boundaries as easily as possible, we
designed a method that shares similarities with
both spell checking and machine translation. Our
validation, because their rule-based system did not need any
training.
777
1. Our approach 2. State of the art
Ten-fold cross-validation, French French English
Copy Hybrid Guimier Kobus 2008 Aw Choud. Cook
x? ? x? ? 2007 1 2? 2006 2006?? 2009??
Sub. 25.90 1.65 6.69 0.45 11.94
Del. 8.24 0.74 1.89 0.31 2.36
Ins. 0.46 0.08 0.72 0.10 2.21
WER 34.59 2.37 9.31 0.78 16.51 10.82 41.00 44.60
SER 85.74 0.87 65.07 1.85 76.05
BLEU 0.47 0.03 0.83 0.01 0.736 0.8 0.81
x?=mean, ?=standard deviation
Table 2: Performance of the system. (?) Kobus 2008-1 corresponds to the ASR-like system, while
Kobus 2008-2 is a combination of this system with a series of open-source machine translation toolkits.
(??) Scores obtained on noisy data only, out of the sentence?s context.
normalization algorithm is original in two ways.
First, it is entirely based on models learned from
a training corpus. Second, the rewrite model
applied to a noisy sequence differs depending on
whether this sequence is known or not.
Evaluated by ten-fold cross-validation, the
system seems efficient, and the performance
in terms of BLEU score and WER are quite
encouraging. However, the SER remains too high,
which emphasizes the fact that the system needs
several improvements.
First of all, the model should take phonetic
similarities into account, because SMS messages
contain a lot of phonetic plays. The phonetic
model, for instance, should know that o, au,
eau, . . . , aux can all be pronounced [o], while
?, ais, ait, . . . , aient are often pronounced [E].
However, unlike Kobus et al (2008a), we feel
that this model must avoid the normalization step
in which the graphemic sequence is converted
into phonemes, because this conversion prevents
the next steps from knowing which graphemes
were in the initial sequence. Instead, we propose
to learn phonetic similarities from a dictionary
of words with phonemic transcriptions, and to
build graphemes-to-graphemes rules. These rules
could then be automatically weighted, by learning
their frequencies from our aligned corpora.
Furthermore, this model should be able to allow
for timbre variation, like [e]?[E], in order to
allow similarities between graphemes frequently
confused in French, like ai ([e]) and ais/ait/aient
([E]). Last but not least, the graphemes-to-
graphemes rules should be contextualized, in
order to reduce the complexity of the model.
It would also be interesting to test the impact of
another lexical language model, learned on non-
SMS sentences. Indeed, the lexical model must
be learned from sequences of standard written
forms, an obvious prerequisite that involves a
major drawback when the corpus is made of SMS
sentences: the corpus must first be transcribed,
an expensive process that reduces the amount
of data on which the model will be trained. For
this reason, we propose to learn a lexical model
from non-SMS sentences. However, the corpus of
external sentences should still share two important
features with the SMS language: it should mimic
the oral language and be as spontaneous as
possible. With this in mind, our intention is
to gather sentences from Internet forums. But
not just any forum, because often forums share
another feature with the SMS language: their
language is noisy. Thus, the idea is to choose
a forum asking its members to pay attention to
spelling mistakes and grammatical errors, and to
avoid the use of the SMS language.
Acknowledgments
This research was funded by grants no. 716619
and 616422 from the Walloon Region of Belgium,
and supported by the Multitel research centre.
We sincerely thank our anonymous reviewers
for their insightful and helpful comments on the
first version of this paper.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text
778
normalization. In Proc. COLING/ACL 2006.
Srinivas Bangalore, Vanessa Murdock, and Giuseppe
Riccardi. 2002. Bootstrapping bilingual data
using consensus translation for a multilingual instant
messaging system. In Proc. the 19th international
conference on Computational linguistics, pages 1?
7, Morristown, NJ, USA.
Richard Beaufort. 2008. Application des
machines ? etats finis en synth?se de la parole.
S?lection d?unit?s non uniformes et correction
orthographique. Ph.D. thesis, FUNDP, Namur,
Belgium, March. 605 pages.
Markus Bieswanger. 2007. abbrevi8 or not 2 abbrevi8:
A contrastive analysis of different space and time-
saving strategies in English and German text
messages. In Texas Linguistics Forum, volume 50.
Stanley F. Chen and Joshua Goodman. 1998.
An empirical study of smoothing techniques for
language modeling. Technical Report 10-98,
Computer Science Group, Harvard University.
Noam Chomsky and Morris Halle. 1968. The sound
pattern of English. Harper and Row, New York, NY.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar1, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. International Journal on
Document Analysis and Recognition, 10(3):157?
174.
Vincent Colotte and Richard Beaufort. 2005.
Linguistic features weighting for a text-to-speech
system without prosody model. In Proc.
Interspeech?05, pages 2549?2552.
Paul Cook and Suzanne Stevenson. 2009. An
unsupervised model for text message normalization.
In Proc. Workshop on Computational Approaches to
Linguistic Creativity, pages 71?78.
Louise-Am?lie Cougnon and Richard Beaufort. 2009.
SSLD: a French SMS to standard language
dictionary. In Sylviane Granger and Magali Paquot,
editors, Proc. eLexicography in the 21st century:
New applications, new challenges (eLEX 2009).
Presses Universitaires de Louvain. To appear.
C?drick Fairon and S?bastien Paumier. 2006. A
translated corpus of 30,000 French SMS. In Proc.
LREC 2006, May.
C?crick. Fairon, Jean R. Klein, and S?bastien Paumier.
2006. Le langage SMS: ?tude d?un corpus
informatis? ? partir de l?enqu?te Faites don de
vos SMS ? la science. Presses Universitaires de
Louvain. 136 pages.
Emilie Guimier de Neef, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS:
?valuation et bilan quantitatif. In Actes de TALN
2007, pages 123?132, Toulouse, France.
C. Douglas Johnson. 1972. Formal aspects of
phonological description. Mouton, The Hague.
Catherine Kobus, Fran?ois Yvon, and G?raldine
Damnati. 2008a. Normalizing SMS: are two
metaphors better than one? In Proc. COLING 2008,
pages 441?448, Manchester, UK.
Catherine Kobus, Fran?ois Yvon, and G?raldine
Damnati. 2008b. Transcrire les SMS comme on
reconna?t la parole. In Actes de la Conf?rence sur
le Traitement Automatique des Langues (TALN?08),
pages 128?138, Avignon, France.
Ron Kohavi. 1995. A study of cross-validation
and bootstrap for accuracy estimation and model
selection. In Proc. IJCAI?95, pages 1137?1143.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics, 10:707?710.
Mehryar Mohri and Michael Riley. 1997. Weighted
determinization and minimization for large
vocabulary speech recognition. In Proc.
Eurospeech?97, pages 131?134.
Mehryar Mohri and Richard Sproat. 1996. An
efficient compiler for weighted rewrite rules. In
Proc. ACL?96, pages 231?238.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer
Science, 231(1):17?32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2001. Generic -removal algorithm for weighted
automata. Lecture Notes in Computer Science,
2088:230?242.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL
2001, pages 311?318.
Emmanuel Roche and Yves Schabes, editors. 1997.
Finite-state language processing. MIT Press,
Cambridge.
Claude E. Shannon. 1948. A mathematical theory of
communication. The Bell System Technical Journal,
27:379?423.
Richard Sproat, A.W. Black, S. Chen, S. Kumar,
M. Ostendorf, and C. Richards. 2001.
Normalization of non-standard words. Computer
Speech & Language, 15(3):287?333.
Crispin Thurlow and Alex Brown. 2003. Generation
txt? The sociolinguistics of young people?s text-
messaging. Discourse Analysis Online, 1(1).
Kristina Toutanova and Robert C. Moore. 2002.
Pronunciation modeling for improved spelling
correction. In Proc. ACL?02, pages 144?151.
Fran?ois Yvon. 2008. Reorthography of SMS
messages. Technical Report 2008, LIMSI/CNRS,
Orsay, France.
779
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 84?89,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine learning and features selection for 
semi-automatic ICD-9-CM encoding
Julia Medori C?drick Fairon
CENTAL
Universit? catholique de Louvain
Place Blaise Pascal, 1
1348 Louvain-la-neuve
CENTAL
Universit? catholique de Louvain
Place Blaise Pascal, 1
1348 Louvain-la-neuve
julia.medori@uclouvain.be cedrick.fairon@uclouvain.be
Abstract
This  paper  describes  the  architecture  of  an 
encoding system which aim is to be implemented 
as  a  coding  help  at  the  Cliniques  universtaires  
Saint-Luc,  a  hospital  in  Brussels.  This  paper 
focuses  on  machine  learning  methods,  more 
specifically, on the appropriate set of attributes to 
be chosen in order to optimize the results of these 
methods.  A  series  of  four  experiments  was 
conducted  on  a  baseline  method:  Na?ve  Bayes 
with varying sets of attributes. These experiments 
showed that a first step consisting in the extraction 
of   information  to  be  coded  (such  as  diseases, 
procedures,  aggravating factors, etc.) is essential. 
It  also demonstrated the importance of stemming 
features.  Restraining  the  classes  to  categories 
resulted in a recall of  81.1 %.
1 Introduction
This  paper  describes  a  series  of  experiments 
carried out within the framework of the CAPADIS 
project.1 This  project  is  the  product  of  a 
collaboration  between  the  UCL  (Universit? 
catholique  de  Louvain,  Belgium)  and  the 
Cliniques  universitaires  Saint-Luc.  Saint-Luc  is 
one of the major hospitals in Belgium. Each year, 
a team of file clerks processes more than 85,000 
patient  discharge summaries  and assigns  to  each 
of them classification codes taken from the ICD-
9-CM (International  Classification of Diseases ? 
1http://www.iwoib.irisnet.be/PRFB/t10/t10_medori_fr.html
Ninth Revision ? Clinical modification ) (PMIC, 
2005). 
The  encoding  of  clinical  notes  (or  patient 
discharge summaries) into nomenclatures such as 
the International Classification of Diseases (ICD) 
is  a  time-consuming,  yet  necessary  task  in 
hospitals. This essential process aims at evaluating 
the  costs  and  budget  in  each  medical  unit.  In 
Belgium,  these  data  are  sent  to  the  National 
Health Department  so as to compute part  of the 
hospital?s funding.
Our  aim  is  to  help  coders  with  their  ever-
growing workload. More and more patients? stays 
need to be encoded while the number  of  coders 
remains the same. Our goal is therefore to develop 
an semi-automatic encoding system where the role 
of the coders would be to check and complete the 
codes provided by the system.
This  paper  focuses  on  machine  learning 
methods as automatic encoding techniques. More 
specifically,  it  focuses  on  the  appropriate  set  of 
attributes  to  be  chosen  in  order  to  optimize  the 
results of these methods.
It  will  therefore  present  the  structure  of  the 
system and compare the results of different inputs 
to the machine learning approach. Section 2 gives 
a  more  detailed  description  of  the  objectives  of 
this  project.  Section 3 gives  an overview of  the 
architecture of the system: first, the extraction part 
will  be  described,  and  then,  the  automatic 
encoding stage will  be discussed.  Section 4 will 
focus  on  the  machine  learning  experiments  that 
84
were conducted. The results will be presented and 
discussed in sections 5 and 6.
2 Objectives
Since  the  early  1990s  and  the  rise  of  the 
computational  linguistics  field,  many  scientists 
have looked into the possible automation of  the 
encoding  process  (Ananiadou  and  McNaught, 
2006;  Ceusters  et  al.,  1994;  Deville  etal.,  1996; 
Friedman  et  al.,  2004;  Sager  et  al.,  1995; 
Zweigenbaum  et  al.,  1995).  Two  different 
approaches  distinguish  themselves  from  one 
another: a symbolic approach as in (Pereira et al, 
2006) and a statistical one. Both methods scored 
highly  in  the  ?Computational  Medicine 
Challenge?  (CMC)  organized  by  the  ?National 
Library  of  Medicine?  in  2007  (Pestian  et  al., 
2007):  among  the  best  three  systems,  two 
combined a statistic and a symbolic approach and 
only one relies only on a symbolic approach. Most 
systems participating took a hybrid approach as in 
(Farkas and Szarvas, 2008). 
During ACL 2007, Aronson (2007) presented 
within the framework of the same challenge, four 
different  approaches,  symbolic,  statistical  and 
hybrid.  His  conclusion  was  that  combining 
different  methods  and  approaches  performed 
better and were more stable than their contributing 
methods. Pakhomov (2006) describes Autocoder, 
an  automatic  encoding  system  implemented  at 
Mayo  Clinic  that  combines  example-based rules 
and  a  machine  learning  module  using  Na?ve 
Bayes.
Within  the  scope  of  this  challenge,  only  a 
limited number of codes were involved. 
The objective of our work is to build such a tool to 
help  the  team  of  coders  from  the  Cliniques  
Universitaires  Saint-Luc.  Three  facts  are 
noteworthy:  the  clinical  notes  we  work  on  are 
written in French; they originate from all medical 
units; and all the codes from the ICD are used in 
the  process  (around  15,000).  Most  studies  are 
limited  on  at  least  one  of  these  criteria:  most 
systems  are  developed  on  English  as  more 
language resources  are  available,  and they often 
focus  on  specific  types  of  notes,  e.g.  the  CMC 
focused on radiology reports.
3 System description
The system is divided into two units: an extraction 
unit  which  aims  at  marking  up  information 
considered  as  relevant  in  the  encoding  process, 
and  an  encoding  unit  which,  from  extracted 
information generates a list of codes.
Figure 1. System structure
Extraction: The system aims  at  reproducing 
the work of human coders.  Coders first  read the 
text, extract all the pieces of information that have 
to  be  encoded,  and  ?translate?  information  into 
codes of the ICD-9-CM. The idea behind our tool 
is  to  recreate  this  process.  The  main  source  of 
information coders  use  are  the patient  discharge 
summaries written by doctors summarizing all that 
happened   during  the  patient?s  stay:  diagnoses, 
procedures, as well as the aggravating factors, the 
patient?s  medical  history,  etc.  These  files  are 
electronic documents written in free text with no 
specific structure.  
We developed a tool which aims at extracting 
the necessary information from these texts: terms 
referring to diseases but also anatomical terms, the 
degree of seriousness or probability of a disease, 
aggravating factors such as smoking, allergies,  or 
other types of information that may influence the 
choice of a code.
There are many ways of referring to the same 
diagnosis  or  procedure,  we  therefore  needed  to 
build specialized dictionaries that would comprise 
as  many  of  these  wordings  as  possible.  The 
Preprocessing
Dictionaries and 
linguistic structures
Morphological 
processing
Context analysis 
Matching lists
Code selection 
according to 
context and 
probabilities
ICD-9-CM
clinical 
notes
clinical notes 
+ 
ordered list 
of codes
extraction
encoding
Manual checking
Machine learning 
module 
85
dictionaries  of  diseases  and  procedures   were 
mainly built  automatically using the  UMLS and 
the  classifications  in  French it  comprises.  Other 
specialized  dictionaries  (anatomical  terms, 
medical  departments,  medications,  etc.)  were 
developed  from  existing  lists.  These  then  were 
gradually completed manually as the development 
of the extraction tool went on.
However,  the  plain  detection  of  terms  is  not 
sufficient.  It  is  important  to  detect  in  which 
context  these  terms  occur.  For  instance,  a 
diagnosis that is negated will not be encoded. The 
identification  of  contexts  required  the  use  of 
finite-state  automata  and  transducers.  These 
transducers  are  represented  by  graphs  that 
describe  the  linguistic  structures  indicating 
specific contexts. These graphs were hand crafted 
using  the  UNITEX  software  tool2 (Paumier, 
2003). An example of a graph matching fractures 
and sprains is presented in figure 2.3 Each path of 
the  graph  describes  a  recognized  linguistic 
structure. 
Graphs were also used to broaden the scope of 
the  terms  detected by dictionaries.  For  instance, 
not only do diseases need to be extracted but, to 
code,  one also needs to know which part  of the 
body is affected. 
Certain  types  of  diagnoses  also  have  to  be 
described via graphs such as smoking as there are 
many ways in which to say that someone smokes 
or not.  Ex: ?he smokes 3 cigarettes a day.? ?He 
used to smoke.? ?Occasionally smokes.? ?Heavy 
smoker.? ?Does not smoke.? 
Figure 2. Example of a UNITEX graph matching 
patterns such as fractures and sprains. 
2 http://www-igm.univ-mlv.fr/~unitex/
3 The grey boxes indicate calls to other graphs. Here, 
d_localisation is a graph matching anatomical terms.
Our  aim  was  to  develop  a  wide-coverage 
system.  We  therefore  focused  mainly  on  the 
General  Internal  Medicine  service  in  order  to 
develop the grammars and dictionaries. It is a very 
diverse department where physicians have to face 
all kinds of diseases.
The graphs and dictionaries on which is based 
our extraction system were built  during the first 
phase of the project. A more detailed description 
and  evaluation  of  the  extraction  process  can  be 
found in (Medori, 2008). 
Encoding: As  was  said  above,  two  main 
approaches to the encoding problem coexist:  the 
symbolic  approach  and  the  statistical  approach. 
Both  have  their  benefits  and  drawbacks.  The 
symbolic approach is a time-consuming approach 
as  it  involves  describing  linguistic  rules  linking 
text to diseases.  The statistical  approach has the 
advantage of being fast to compute but the need 
for a large amount of data often hampers the use 
of  these  methods.  However,  both  methods  give 
reliable results, and a combination of both is the 
option generally favored. In our context, we chose 
to combine both approaches as a large corpus of 
clinical notes is at our disposal. 
Saint-Luc  provided  us  with  a  corpus  of 
166,670  clinical  notes.  The  codes  that  were 
assigned  to  them  by  the  coders  were  also 
provided.  This  corpus  gives  us  the  chance  to 
develop and test statistical methods in a ?real life? 
experiment.4
However, whatever the results, we will need to 
combine  these  methods  with  linguistic  rules. 
There are two main reasons for this : in the near 
future, we will have to face the problem of having 
to switch to another classification. The change to 
ICD-10-CM is  planned  for  2015.  Therefore,  at 
that time, we will not have enough learning data to 
be able to generate the list of codes in a statistical 
manner. The second reason is that there are codes 
that  are seldom assigned and for which we will 
not have enough occurrences in our corpus to be 
able to extract them statistically. 
This paper focuses on the statistical tests that 
were conducted on our corpus. An insight into a 
symbolic  method  using  the  matching  of 
morphemes can be found in (Medori, 2008).
4 In this paper, the experiments were conducted on a smaller 
corpus.  At  a  later  stage,  the  methods  chosen  for  the  final 
system will need to be trained on the full corpus.
86
4 Experiment
As a first encoding experiment, we chose to focus 
on  a  baseline  machine  learning  method:  Na?ve 
Bayes.  This  method  has  often  been  used  and 
proves to be robust.
To conduct this experiment, we used Weka, a 
data mining software5 developed at the University 
of Waikato. For more information on this tool, see 
(Witten, 2004).
In order to test this method we took a sub-set 
of 19,994 discharge summaries from the General 
Internal Medicine department. In order to test how 
necessary the extraction step is, we chose the texts 
from the department on which the development of 
the extraction rules were based.
These  notes  were  assigned  102,855  codes 
which makes up 4,039 distinct codes.
This corpus was then divided into two subsets: 
90% of  the  19,994 patient  discharge  summaries 
were used as the training corpus and 10% as the 
test set.
As with any machine learning method, enough 
data for each class is needed in the training set in 
order to be able to classify correctly.  Therefore, 
we  built  a  classifier  for  each  code  that  was 
manually assigned at least 6 times in our corpus. 
This  resulted  in  1,497  classifiers,  which  means 
that  we did not  have enough data  to  be able  to 
assign 2,542 codes which make up 5% of all the 
assigned codes. 
Four experiments were conducted:
Experiment  1. In  our  first  experiment,  the 
selected  attributes  were  the  terms  that  were 
highlighted  as  diagnoses  by  the  extraction  step. 
The  diagnoses  identified  in  a  negative  context 
were  removed  from  the  features  list.  These 
resulting  list  of  extracted  terms  went  through  a 
normalization  process:  accents  and  stop  words 
were removed; words were decapitalized.
Experiment 2. The second experiment aimed 
at proving the relevance of the stemming of these 
terms.  The  attributes  in  this  experiment  were 
therefore  the  terms  that  were  extracted,  then 
normalized  and  stemmed  using  Snowball 
Stemmer6 which  is  an  implementation  of  the 
Porter algorithm.
Experiment  3. In  this  third  experiment,  we 
wanted to  check  the  relevance of  the  extraction 
5 http://www.cs.waikato.ac.nz/~ml/weka/
6 http://snowball.tartarus.org/
process (see experiments 1 and 2). Therefore, the 
attributes comprised all the words contained in the 
clinical notes apart from stop words. The words 
were stemmed in the same way as the extracted 
terms in experiment 2.
Experiment 4. In all the previous experiments, 
the classes to be assigned consisted in codes. In 
this experiment, classes are reduced to categories 
of codes: represented by the first three digits of a 
code. The attributes are the same as in experiment 
1: extracted terms (non-stemmed). As the system 
is  designed  as  a  coding  help  i.e.  its  aim  is  to 
generate  a list  of  suggested codes,  and not  as  a 
fully  automated  encoding  system,  one  could 
imagine  listing  categories  of  codes  instead  of 
codes themselves and then let the coders look up 
in  the  hierarchy for  the  appropriate  code within 
the selected category.
At the end of each experiment, we end up with 
a list of the 1,497 codes from ICD-9-CM ordered 
by their Na?ve Bayes score for each letter.
The measure that  is  most  interesting here is  the 
recall.  The  list  of  suggested  codes  needs  to 
comprise most of the codes the coder will need so 
that he/she does not have to go elsewhere to find 
the  appropriate  code.  Therefore,  we  kept  three 
measures of recall. It is important to keep the list 
of  codes  to  be  presented  to  the  user  short  and 
manageable.  Larkey  and  Croft  (1995)  used  the 
same measures and set the limit number of codes 
to  20.  This  choice  is  arbitrary but  seems  like  a 
sensible limit. In Saint-Luc, the maximum number 
of  codes  a  file  clerk  can  assign  to  a  patient 
discharge summary is 26 (the principal diagnosis 
is assigned the letter A and all the other codes are 
ordered  according  to  the  other  letters  of  the 
alphabet).  However,  few  reports  are  actually 
assigned 26 codes (15 out of 19,994). The average 
number of codes assigned by the file clerks in our 
set of 19,994 discharge summaries is 6.2.
The  three  measures  of  recall  are Recall10, 
Recall15 and Recall20 which are the measures of 
micro averaged recall if we show the first 10, 15 
and 20 most likely codes respectively.7
7 It should be noted that we keep in the list of suggested codes 
all the codes that tie last with the 10th, 15th and 20th position 
respectively.
87
5 Results
The results of the experiments described above are 
detailed in figure 3.
Rec10 Rec15 Rec20
1(att: extracted terms) 50.4 56.4 60.5
2 (att: stemmed 
extracted terms) 56.1 64.1 69.1
3 (att: all words, 
stemmed) 39.1 40.3 41.4
4 (att: extracted terms
classes : categories) 64.0 75.1 81.1
Figure 3. Recall for each experiment (in %)
Experiment  1. From the results of  this baseline 
experiment,  considering  the  extracted  terms  and 
retaining the 20 most likely codes according to the 
Na?ve Bayesian statistics,  more than 60% of the 
codes manually assigned to the test notes can be 
found in this list. 
Experiment 2. The stemming of the extracted 
terms increased the recall by 8.6%.
Experiment 3. If considering all the words as 
attributes,  the  recall  when  retaining  20  possible 
candidates  is  around 40% while  when attributes 
are  selected  through  the  extraction  process,  the 
recall  increases  to  69% which is  an  increase  of 
about 28%. This result proves that the extraction 
process  is  an  essential  step  in  the  system  and 
clearly improves the performance of the statistical 
encoding unit. 
Experiment  4. When  classes  are  limited  to 
categories,   Recall20  jumps  to  81.1% which  is 
20.6%  more  than  in  experiment  1  which  was 
conducted  with  the  same  attributes  but  where 
classes  were  codes.  This  supports  our  idea  that 
showing a list of categories instead of codes could 
be  an  interesting  alternative  for  coders:  they 
would be shown more codes while keeping the list 
manageable, and then could browse easily into the 
sub-structure of the classification.
6 Discussion
The choice of attributes is important when testing 
machine  learning methods.  In  the  framework  of 
the  development  of  an  encoding  system,  we 
proved that a first step consisting in selecting the 
terms  carrying  the  information  that  needs  to  be 
encoded is essential. We also showed that the use 
of a simple stemming algorithm clearly improves 
the performance of the method.
In the last experiment, classifying the clinical 
notes by categories of codes resulted in a recall of 
81.1%. This reinforces our opinion that, to make 
sure that all the needed codes are present for the 
coder,  we  could  list  categories  and  let  him/her 
browse through the codes from there.
It  is important to put these results in light of 
where the codes originate. Most of the information 
that  needs  to  be  encoded  is  present  in  these 
clinical  notes.  However,  even though efforts  are 
made in order for this to change, many physicians 
still do not compile all the information into these 
notes. Coders therefore still have to look up into 
the whole patient record in order to find additional 
codes.  The  proportion  of  codes  that  cannot  be 
inferred from the clinical notes can be very high. 
A study conducted  by Sabine  Regout,  a  patient 
discharge summary specialist in Saint-Luc, on 250 
clinical notes from 25 medical units, showed that 
in  most  departments,  15  to  20%  of  the  codes 
assigned by the clerks cannot be inferred from the 
notes. This proportion can increase up to 80% in 
some surgery departments. This evaluation proves 
that  without  a  change  of  mind-set  from  the 
physicians,  our  system  can  only  aim  to  be  a 
coding  help  for  file  clerks.  Analyzing  all  the 
different types of documents contained in patient 
records would be a difficult task as they comprise 
a  variety  of  documents  with  different  structures 
and formats,  and some of them are hand-written 
documents. For our experiments, this also means 
that the maximal recall value we will be able to 
get is around 80%.
In  these  experiments,  we  were  not  able  to 
check the inter-annotator agreement but we must 
keep  in  mind  that,  as  in  any  classification  task 
where  humans  set  the  gold-standard,  one  must 
expect some degree of errors and variation in the 
coding.  
Another  observation  influences  the  maximal 
number of codes we will be able to retrieve is that 
we built classifiers for all the codes for which we 
had enough data. This lead to the building of 1497 
classifiers.  This  represents  95% of  all  the codes 
assigned  to  our  test  notes.  This  decreases  our 
maximal recall value by 5%.
The  codes  that  are  seldom  assigned  will 
therefore never show up in our list  of suggested 
codes. This is rather problematic and other non-
88
statistical methods will be needed to make up for 
this.
7 Future work
In the light of these results, the next step will be to 
conduct  an  experiment  on  categories  as  classes 
using stemmed  extracted terms  as  features.  This 
should improve further the 81.1% recall from the 
results of experiment 4.
These experiments were conducted in order to 
select the right features to be used as attributes for 
our  machine  learning  module.  We  chose  Na?ve 
Bayes  as  a  baseline  method.  However,  other 
methods  have  been  tested  in  previous  works 
(Larkey and Croft, 1995) and have proved to give 
good results as well, such as k-nearest neighbors 
or Support Vector Machines.
We saw, at the end of section 6, that symbolic 
methods need to be developed in order to assist 
machine  learning  methods.  Machine  learning 
techniques  have  their  limitations:  they  cannot 
assign codes for which they did not have enough 
data,  and they cannot  face the change to a  new 
nomenclature.  Therefore,  in  the  near  future  we 
will  have  to  develop  a  symbolic  module 
comprising a series of linguistic rules in order to 
do the matching on all codes. A prototype based 
on the matching of morphemes has already been 
developed  but  will  need  to  be  experimented 
further.
The results  of  the  experiments  we conducted 
on  a  machine  learning  method  were  promising. 
Now, combining these two different approaches is 
the next challenging task in our project.
Acknowledgements
The  CAPADIS  project  was  funded  by  the 
government  of  the  Brussels-Capital  Region 
(Institute  for  the  encouragement  of 
Scientific Research  and  Innovation  of  Brussels) 
within  the  framework  of  the  "Prospective 
research for Brussels 2006" program. I would also 
like  to  thank  Beno?t  Debande,  Claire  Beguin, 
Sabine Regout and the Saint-Luc hospital team of 
coders.
References
Ananiadou S., McNaught J.: Introduction to Text Mining in 
Biology.  In  Ananiadou  S.,  McNaught  J.  (eds.)  Text 
Mining  for  Biology  and  Biomedicine,  pp  1--12,  Artech 
House Books (2006).
Aronson  A.  R.:  MetaMap:  Mapping  Text  to  the  UMLS 
Metathesaurus (2006).
Ceusters  W.,  Michel  C.,  Penson  D.,  Mauclet  E.:  Semi-
automated encoding of diagnoses and medical procedures 
combining ICD-9-CM with computational-linguistic tools. 
Ann Med Milit Belg;8(2):53?58 (1994).
Deville G., Herbigniaux E., Mousel P., Thienpont G., W?ry 
M.: ANTHEM: Advanced Natural Language Interface for 
Multilingual Text Generation in Healthcare (1996).
Farkas R., Szarvas G. Automatic construction of rule-based 
ICD-9-CM  coding  systems?,  BMC  Bioinformatics,  9 
(2008).
Friedman  C.,  Shagina  L.,  Lussier  Y.A.,  Hripcsak  G.: 
Automated  encoding  of  clinical  documents  based  on 
natural  language  processing.  J  Am Med  Inform Assoc. 
2004 Sep-Oct;11(5):392--402. Epub 2004 Jun 7 (2004).
Larkey L. S, Croft W. B. Automatic assignment of icd9 codes 
to  discharge  summaries.  Technical  report,  University  of 
Massachusetts at Amherst, Amherst, MA (1995). 
Medori J. From Free Text to ICD: Development of a Coding 
Help, In: Proceedings of Louhi 08, Turku, 3-4 sept 2008 
(2008).
Pakhomov S. V. S., Buntrock J. D., Chute C. G.: Automating 
the Assignment of Diagnosis Codes to Patient Encounters 
Using Example-based and Machine Learning Techniques 
(2006).
Paumier S.  De la reconnaissance de formes linguistiques ? 
l'analyse syntaxique.  PhD thesis.  Universit? de Marne-la-
Vall?e (2003).
Practice  Management  Information  Corporation.  ICD-9-CM 
Hospital  Edition, International Classification of Diseases 
9th  Revision,  Clinical  Modification  (Color-Coded, 
Volumes 1-3, Thumb-Indexed) (2005).
Pereira S., N?v?ol A., Massari P., Joubert M., Darmoni S.J. : 
Construction  of  a  semi-automated  ICD-10  coding  help 
system to optimize medical and economic coding.  Proc. 
MIE. (2006).
Pestian J. P., Brew C., Matykiewicz P.M., Hovermale D.J., 
Johnson  N.,  Cohen  K.B.,  Duch  W.:  A  shared  task 
involving  multi-label  classification  of  clinical  free  text. 
Proceedings of ACL BioNLP; 2007 Jun; Prague (2007).
Sager  N.,  Lyman M.,  Nh?n N.,  Tick L.:  Medical language 
processing: Applications to patient data representation and 
automatic encoding. Methods of Information in Medicine, 
(34):140 -- 146 (1995).
Witten  I.H.,  Frank  E.  Data  Mining:  Practical  Machine 
Learning Tools and Techniques.  San Francisco: Morgan 
Kaufmann Publishers.  2nd edition.  560  pp.  ISBN 0-12-
088407-0 (2005). 
Zweigenbaum P. and Consortium MENELAS: MENELAS: 
coding  and  information  retrieval  from natural  language 
patient discharge summaries. In Laires M. F., Ladeira M. 
J.,  Christensen  J.  P.,  (eds.),  Advances  in  Health 
Telematics,  pages  82-89.  IOS  Press,  Amsterdam,  1995. 
MENELAS Final Edited Progress Report (1995).
89
