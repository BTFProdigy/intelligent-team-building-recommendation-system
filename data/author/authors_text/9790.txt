Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 650?659,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Joint Unsupervised Coreference Resolution with Markov Logic
Hoifung Poon Pedro Domingos
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350, U.S.A.
{hoifung,pedrod}@cs.washington.edu
Abstract
Machine learning approaches to coreference
resolution are typically supervised, and re-
quire expensive labeled data. Some unsuper-
vised approaches have been proposed (e.g.,
Haghighi and Klein (2007)), but they are less
accurate. In this paper, we present the first un-
supervised approach that is competitive with
supervised ones. This is made possible by
performing joint inference across mentions,
in contrast to the pairwise classification typ-
ically used in supervised methods, and by us-
ingMarkov logic as a representation language,
which enables us to easily express relations
like apposition and predicate nominals. On
MUC and ACE datasets, our model outper-
forms Haghigi and Klein?s one using only a
fraction of the training data, and often matches
or exceeds the accuracy of state-of-the-art su-
pervised models.
1 Introduction
The goal of coreference resolution is to identify
mentions (typically noun phrases) that refer to the
same entities. This is a key subtask in many NLP
applications, including information extraction, ques-
tion answering, machine translation, and others. Su-
pervised learning approaches treat the problem as
one of classification: for each pair of mentions,
predict whether they corefer or not (e.g., McCal-
lum & Wellner (2005)). While successful, these
approaches require labeled training data, consisting
of mention pairs and the correct decisions for them.
This limits their applicability.
Unsupervised approaches are attractive due to the
availability of large quantities of unlabeled text.
However, unsupervised coreference resolution is
much more difficult. Haghighi and Klein?s (2007)
model, the most sophisticated to date, still lags su-
pervised ones by a substantial margin. Extending it
appears difficult, due to the limitations of its Dirich-
let process-based representation.
The lack of label information in unsupervised
coreference resolution can potentially be overcome
by performing joint inference, which leverages the
?easy? decisions to help make related ?hard? ones.
Relations that have been exploited in supervised
coreference resolution include transitivity (McCal-
lum & Wellner, 2005) and anaphoricity (Denis &
Baldridge, 2007). However, there is little work to
date on joint inference for unsupervised resolution.
We address this problem using Markov logic,
a powerful and flexible language that combines
probabilistic graphical models and first-order logic
(Richardson & Domingos, 2006). Markov logic
allows us to easily build models involving rela-
tions among mentions, like apposition and predi-
cate nominals. By extending the state-of-the-art al-
gorithms for inference and learning, we developed
the first general-purpose unsupervised learning al-
gorithm for Markov logic, and applied it to unsuper-
vised coreference resolution.
We test our approach on standard MUC and ACE
datasets. Our basic model, trained on a minimum
of data, suffices to outperform Haghighi and Klein?s
(2007) one. Our full model, using apposition and
other relations for joint inference, is often as accu-
rate as the best supervised models, or more.
650
We begin by reviewing the necessary background
on Markov logic. We then describe our Markov
logic network for joint unsupervised coreference
resolution, and the learning and inference algorithms
we used. Finally, we present our experiments and re-
sults.
2 Related Work
Most existing supervised learning approaches for
coreference resolution are suboptimal since they re-
solve each mention pair independently, only impos-
ing transitivity in postprocessing (Ng, 2005). More-
over, many of them break up the resolution step into
subtasks (e.g., first determine whether a mention is
anaphoric, then classify whether it is coreferent with
an antecedent), which further forsakes opportunities
for joint inference that have been shown to be help-
ful (Poon & Domingos, 2007). Using graph parti-
tioning, McCallum & Wellner (2005) incorporated
transitivity into pairwise classification and achieved
the state-of-the-art result on the MUC-6 dataset, but
their approach can only leverage one binary relation
at a time, not arbitrary relations among mentions.
Denis & Baldridge (2007) determined anaphoricity
and pairwise classification jointly using integer pro-
gramming, but they did not incorporate transitivity
or other relations.
While potentially more appealing, unsupervised
learning is very challenging, and unsupervised
coreference resolution systems are still rare to this
date. Prior to our work, the best performance in
unsupervised coreference resolution was achieved
by Haghighi & Klein (2007), using a nonparamet-
ric Bayesian model based on hierarchical Dirichlet
processes. At the heart of their system is a mixture
model with a few linguistically motivated features
such as head words, entity properties and salience.
Their approach is a major step forward in unsuper-
vised coreference resolution, but extending it is chal-
lenging. The main advantage of Dirichlet processes
is that they are exchangeable, allowing parameters
to be integrated out, but Haghighi and Klein forgo
this when they introduce salience. Their model thus
requires Gibbs sampling over both assignments and
parameters, which can be very expensive. Haghighi
and Klein circumvent this by making approxima-
tions that potentially hurt accuracy. At the same
time, the Dirichlet process prior favors skewed clus-
ter sizes and a number of clusters that grows loga-
rithmically with the number of data points, neither of
which seems generally appropriate for coreference
resolution.
Further, deterministic or strong non-deterministic
dependencies cause Gibbs sampling to break down
(Poon & Domingos, 2006), making it difficult to
leverage many linguistic regularities. For exam-
ple, apposition (as in ?Bill Gates, the chairman of
Microsoft?) suggests coreference, and thus the two
mentions it relates should always be placed in the
same cluster. However, Gibbs sampling can only
move one mention at a time from one cluster to
another, and this is unlikely to happen, because it
would require breaking the apposition rule. Blocked
sampling can alleviate this problem by sampling
multiple mentions together, but it requires that the
block size be predetermined to a small fixed number.
When we incorporate apposition and other regular-
ities the blocks can become arbitrarily large, mak-
ing this infeasible. For example, suppose we also
want to leverage predicate nominals (i.e., the sub-
ject and the predicating noun of a copular verb are
likely coreferent). Then a sentence like ?He is Bill
Gates, the chairman of Microsoft? requires a block
of four mentions: ?He?, ?Bill Gates?, ?the chair-
man of Microsoft?, and ?Bill Gates, the chairman
of Microsoft?. Similar difficulties occur with other
inference methods. Thus, extending Haghighi and
Klein?s model to include richer linguistic features is
a challenging problem.
Our approach is instead based on Markov logic,
a powerful representation for joint inference with
uncertainty (Richardson & Domingos, 2006). Like
Haghighi and Klein?s, our model is cluster-based
rather than pairwise, and implicitly imposes tran-
sitivity. We do not predetermine anaphoricity of a
mention, but rather fuse it into the integrated reso-
lution process. As a result, our model is inherently
joint among mentions and subtasks. It shares sev-
eral features with Haghighi & Klein?s model, but re-
moves or refines features where we believe it is ap-
propriate to. Most importantly, our model leverages
apposition and predicate nominals, which Haghighi
& Klein did not use. We show that this can be done
very easily in our framework, and yet results in very
substantial accuracy gains.
651
It is worth noticing that Markov logic is also well
suited for joint inference in supervised systems (e.g.,
transitivity, which tookMcCallum&Wellner (2005)
nontrivial effort to incorporate, can be handled in
Markov logic with the addition of a single formula
(Poon & Domingos, 2008)).
3 Markov Logic
In many NLP applications, there exist rich relations
among objects, and recent work in statistical rela-
tional learning (Getoor & Taskar, 2007) and struc-
tured prediction (Bakir et al, 2007) has shown that
leveraging these can greatly improve accuracy. One
of the most powerful representations for joint infer-
ence is Markov logic, a probabilistic extension of
first-order logic (Richardson & Domingos, 2006). A
Markov logic network (MLN) is a set of weighted
first-order clauses. Together with a set of con-
stants, it defines a Markov network with one node
per ground atom and one feature per ground clause.
The weight of a feature is the weight of the first-
order clause that originated it. The probability of
a state x in such a network is given by P (x) =
(1/Z) exp (
?
i wifi(x)), where Z is a normaliza-
tion constant, wi is the weight of the ith clause,
fi = 1 if the ith clause is true, and fi = 0 other-
wise.
Markov logic makes it possible to compactly
specify probability distributions over complex re-
lational domains. Efficient inference can be per-
formed using MC-SAT (Poon & Domingos, 2006).
MC-SAT is a ?slice sampling? Markov chain Monte
Carlo algorithm. Slice sampling introduces auxil-
iary variables u that decouple the original ones x,
and alternately samples u conditioned on x and vice-
versa. To sample from the slice (the set of states x
consistent with the current u), MC-SAT calls Sam-
pleSAT (Wei et al, 2004), which uses a combina-
tion of satisfiability testing and simulated annealing.
The advantage of using a satisfiability solver (Walk-
SAT) is that it efficiently finds isolated modes in the
distribution, and as a result the Markov chain mixes
very rapidly. The slice sampling scheme ensures
that detailed balance is (approximately) preserved.
MC-SAT is orders of magnitude faster than previous
MCMC algorithms like Gibbs sampling, making ef-
ficient sampling possible on a scale that was previ-
Algorithm 1 MC-SAT(clauses, weights,
num samples)
x(0) ? Satisfy(hard clauses)
for i? 1 to num samples do
M ? ?
for all ck ? clauses satisfied by x(i?1) do
With probability 1? e?wk add ck to M
end for
Sample x(i) ? USAT (M)
end for
ously out of reach.
Algorithm 1 gives pseudo-code for MC-SAT. At
iteration i ? 1, the factor ?k for clause ck is ei-
ther ewk if ck is satisfied in x(i?1), or 1 otherwise.
MC-SAT first samples the auxiliary variable uk uni-
formly from (0, ?k), then samples a new state uni-
formly from the set of states that satisfy ??k ? uk
for all k (the slice). Equivalently, for each k, with
probability 1 ? e?wk the next state must satisfy ck.
In general, we can factorize the probability distribu-
tion in any way that facilitates inference, sample the
uk?s, and make sure that the next state is drawn uni-
formly from solutions that satisfy ??k ? uk for all
factors.
MC-SAT, like most existing relational inference
algorithms, grounds all predicates and clauses, thus
requiring memory and time exponential in the pred-
icate and clause arities. We developed a general
method for producing a ?lazy? version of relational
inference algorithms (Poon & Domingos, 2008),
which carries exactly the same inference steps as the
original algorithm, but only maintains a small sub-
set of ?active? predicates/clauses, grounding more
as needed. We showed that Lazy-MC-SAT, the lazy
version of MC-SAT, reduced memory and time by
orders of magnitude in several domains. We use
Lazy-MC-SAT in this paper.
Supervised learning for Markov logic maximizes
the conditional log-likelihoodL(x, y) = logP (Y =
y|X = x), where Y represents the non-evidence
predicates, X the evidence predicates, and x, y their
values in the training data. For simplicity, from now
on we omit X , whose values are fixed and always
conditioned on. The optimization problem is convex
and a global optimum can be found using gradient
652
descent, with the gradient being
?
?wi
L(y) = ni(y)?
?
y? P (Y = y
?)ni(y?)
= ni(y)? EY [ni].
where ni is the number of true groundings of clause
i. The expected count can be approximated as
EY [ni] ?
1
N
N?
k=1
ni(yk)
where yk are samples generated by MC-SAT. To
combat overfitting, a Gaussian prior is imposed on
all weights.
In practice, it is difficult to tune the learning rate
for gradient descent, especially when the number
of groundings varies widely among clauses. Lowd
& Domingos (2007) used a preconditioned scaled
conjugate gradient algorithm (PSCG) to address this
problem. This estimates the optimal step size in each
step as
? =
?dT g
dTHd + ?dTd
.
where g is the gradient, d the conjugate update direc-
tion, and ? a parameter that is automatically tuned
to trade off second-order information with gradient
descent. H is the Hessian matrix, with the (i, j)th
entry being
?2
?wi?wj
L(y) = EY [ni] ? EY [nj ]? EY [ni ? nj ]
= ?CovY [ni, nj ].
The Hessian can be approximated with the same
samples used for the gradient. Its negative inverse
diagonal is used as the preconditioner.1
The open-source Alchemy package (Kok et al,
2007) provides implementations of existing algo-
rithms for Markov logic. In Section 5, we develop
the first general-purpose unsupervised learning al-
gorithm for Markov logic by extending the existing
algorithms to handle hidden predicates.2
1Lowd & Domingos showed that ? can be computed more
efficiently, without explicitly approximating or storing the Hes-
sian. Readers are referred to their paper for details.
2Alchemy includes a discriminative EM algorithm, but it as-
sumes that only a few values are missing, and cannot handle
completely hidden predicates. Kok & Domingos (2007) applied
Markov logic to relational clustering, but they used hard EM.
4 An MLN for Joint Unsupervised
Coreference Resolution
In this section, we present our MLN for joint unsu-
pervised coreference resolution. Our model deviates
from Haghighi & Klein?s (2007) in several impor-
tant ways. First, our MLN does not model saliences
for proper nouns or nominals, as their influence is
marginal compared to other features; for pronoun
salience, it uses a more intuitive and simpler def-
inition based on distance, and incorporated it as a
prior. Another difference is in identifying heads. For
the ACE datasets, Haghighi and Klein used the gold
heads; for the MUC-6 dataset, where labels are not
available, they crudely picked the rightmost token in
a mention. We show that a better way is to determine
the heads using head rules in a parser. This improves
resolution accuracy and is always applicable. Cru-
cially, our MLN leverages syntactic relations such
as apposition and predicate nominals, which are not
used by Haghighi and Klein. In our approach, what
it takes is just adding two formulas to the MLN.
As common in previous work, we assume that
true mention boundaries are given. We do not as-
sume any other labeled information. In particu-
lar, we do not assume gold name entity recogni-
tion (NER) labels, and unlike Haghighi & Klein
(2007), we do not assume gold mention types (for
ACE datasets, they also used gold head words). We
determined the head of a mention either by taking
its rightmost token, or by using the head rules in a
parser. We detected pronouns using a list.
4.1 Base MLN
The main query predicate is InClust(m, c!), which
is true iff mention m is in cluster c. The ?t!? notation
signifies that for each m, this predicate is true for a
unique value of c. The main evidence predicate is
Head(m, t!), where m is a mention and t a token, and
which is true iff t is the head of m. A key component
in our MLN is a simple head mixture model, where
the mixture component priors are represented by the
unit clause
InClust(+m,+c)
and the head distribution is represented by the head
prediction rule
InClust(m,+c) ? Head(m,+t).
653
All free variables are implicitly universally quanti-
fied. The ?+? notation signifies that the MLN con-
tains an instance of the rule, with a separate weight,
for each value combination of the variables with a
plus sign.
By convention, at each inference step we name
each non-empty cluster after the earliest mention it
contains. This helps break the symmetry among
mentions, which otherwise produces multiple op-
tima and makes learning unnecessarily harder. To
encourage clustering, we impose an exponential
prior on the number of non-empty clusters with
weight ?1.
The above model only clusters mentions with the
same head, and does not work well for pronouns. To
address this, we introduce the predicate IsPrn(m),
which is true iff the mention m is a pronoun, and
adapt the head prediction rule as follows:
?IsPrn(m) ? InClust(m,+c) ? Head(m,+t)
This is always false when m is a pronoun, and thus
applies only to non-pronouns.
Pronouns tend to resolve with men-
tions that are semantically compatible with
them. Thus we introduce predicates that
represent entity type, number, and gender:
Type(x, e!), Number(x, n!), Gender(x, g!),
where x can be either a cluster or mention,
e ? {Person, Organization, Location, Other},
n ? {Singular, Plural} and g ?
{Male, Female, Neuter}. Many of these are
known for pronouns, and some can be inferred
from simple linguistic cues (e.g., ?Ms. Galen?
is a singular female person, while ?XYZ Corp.?
is an organization).3 Entity type assignment is
represented by the unit clause
Type(+x,+e)
and similarly for number and gender. A mention
should agree with its cluster in entity type. This is
ensured by the hard rule (which has infinite weight
and must be satisfied)
InClust(m, c)? (Type(m, e)? Type(c, e))
3We used the following cues: Mr., Ms., Jr., Inc., Corp., cor-
poration, company. The proportions of known properties range
from 14% to 26%.
There are similar hard rules for number and gender.
Different pronouns prefer different entity types,
as represented by
IsPrn(m) ? InClust(m, c)
?Head(m,+t) ? Type(c,+e)
which only applies to pronouns, and whose weight is
positive if pronoun t is likely to assume entity type
e and negative otherwise. There are similar rules for
number and gender.
Aside from semantic compatibility, pronouns tend
to resolve with nearby mentions. To model this, we
impose an exponential prior on the distance (number
of mentions) between a pronoun and its antecedent,
with weight ?1.4 This is similar to Haghighi and
Klein?s treatment of salience, but simpler.
4.2 Full MLN
Syntactic relations among mentions often suggest
coreference. Incorporating such relations into our
MLN is straightforward. We illustrate this with
two examples: apposition and predicate nominals.
We introduce a predicate for apposition, Appo(x, y),
where x, y are mentions, and which is true iff y is an
appositive of x. We then add the rule
Appo(x, y)? (InClust(x, c)? InClust(y, c))
which ensures that x, y are in the same cluster if y is
an appositive of x. Similarly, we introduce a predi-
cate for predicate nominals, PredNom(x, y), and the
corresponding rule.5 The weights of both rules can
be learned from data with a positive prior mean. For
simplicity, in this paper we treat them as hard con-
straints.
4.3 Rule-Based MLN
We also consider a rule-based system that clusters
non-pronouns by their heads, and attaches a pro-
noun to the cluster which has no known conflicting
4For simplicity, if a pronoun has no antecedent, we define
the distance to be?. So a pronoun must have an antecedent in
our model, unless it is the first mention in the document or it can
not resolve with previous mentions without violating hard con-
straints. It is straightforward to soften this with a finite penalty.
5We detected apposition and predicate nominatives using
simple heuristics based on parses, e.g., if (NP, comma, NP) are
the first three children of an NP, then any two of the three noun
phrases are apposition.
654
type, number, or gender, and contains the closest an-
tecedent for the pronoun. This system can be en-
coded in an MLN with just four rules. Three of them
are the ones for enforcing agreement in type, num-
ber, and gender between a cluster and its members,
as defined in the base MLN. The fourth rule is
?IsPrn(m1) ? ?IsPrn(m2)
?Head(m1, h1) ? Head(m2, h2)
?InClust(m1, c1) ? InClust(m2, c2)
? (c1 = c2? h1 = h2).
With a large but not infinite weight (e.g., 100),
this rule has the effect of clustering non-pronouns
by their heads, except when it violates the hard
rules. The MLN can also include the apposition and
predicate-nominal rules. As in the base MLN, we
impose the same exponential prior on the number of
non-empty clusters and that on the distance between
a pronoun and its antecedent. This simple MLN is
remarkably competitive, as we will see in the exper-
iment section.
5 Learning and Inference
Unsupervised learning in Markov logic maximizes
the conditional log-likelihood
L(x, y) = logP (Y = y|X = x)
= log
?
z P (Y = y, Z = z|X = x)
where Z are unknown predicates. In our coref-
erence resolution MLN, Y includes Head and
known groundings of Type, Number and Gender,
Z includes InClust and unknown groundings of
Type, Number, Gender, and X includes IsPrn,
Appo and PredNom. (For simplicity, from now on
we drop X from the formula.) With Z, the opti-
mization problem is no longer convex. However, we
can still find a local optimum using gradient descent,
with the gradient being
?
?wi
L(y) = EZ|y[ni]? EY,Z [ni]
where ni is the number of true groundings of the ith
clause. We extended PSCG for unsupervised learn-
ing. The gradient is the difference of two expec-
tations, each of which can be approximated using
samples generated by MC-SAT. The (i, j)th entry of
the Hessian is now
?2
?wi?wj
L(y) = CovZ|y[ni, nj ]? CovY,Z [ni, nj ]
and the step size can be computed accordingly.
Since our problem is no longer convex, the nega-
tive diagonal Hessian may contain zero or negative
entries, so we first took the absolute values of the
diagonal and added 1, then used the inverse as the
preconditioner. We also adjusted ? more conserva-
tively than Lowd & Domingos (2007).
Notice that when the objects form independent
subsets (in our cases, mentions in each document),
we can process them in parallel and then gather suf-
ficient statistics for learning. We developed an ef-
ficient parallelized implementation of our unsuper-
vised learning algorithm using the message-passing
interface (MPI). Learning in MUC-6 took only one
hour, and in ACE-2004 two and a half.
To reduce burn-in time, we initialized MC-SAT
with the state returned by MaxWalkSAT (Kautz et
al., 1997), rather than a random solution to the hard
clauses. In the existing implementation in Alchemy
(Kok et al, 2007), SampleSAT flips only one atom
in each step, which is inefficient for predicates with
unique-value constraints (e.g., Head(m, c!)). Such
predicates can be viewed as multi-valued predi-
cates (e.g., Head(m) with value ranging over all
c?s) and are prevalent in NLP applications. We
adapted SampleSAT to flip two or more atoms in
each step so that the unique-value constraints are
automatically satisfied. By default, MC-SAT treats
each ground clause as a separate factor while de-
termining the slice. This can be very inefficient
for highly correlated clauses. For example, given
a non-pronoun mention m currently in cluster c and
with head t, among the mixture prior rules involv-
ing m InClust(m, c) is the only one that is satisfied,
and among those head-prediction rules involving m,
?IsPrn(m)?InClust(m, c)?Head(m, t) is the only
one that is satisfied; the factors for these rules mul-
tiply to ? = exp(wm,c + wm,c,t), where wm,c is the
weight for InClust(m, c), and wm,c,t is the weight
for ?IsPrn(m)?InClust(m, c)?Head(m, t), since
an unsatisfied rule contributes a factor of e0 = 1. We
extended MC-SAT to treat each set of mutually ex-
clusive and exhaustive rules as a single factor. E.g.,
for the above m, MC-SAT now samples u uniformly
655
from (0, ?), and requires that in the next state ?? be
no less than u. Equivalently, the new cluster and
head for m should satisfy wm,c? + wm,c?,t? ? log(u).
We extended SampleSAT so that when it consid-
ers flipping any variable involved in such constraints
(e.g., c or t above), it ensures that their new values
still satisfy these constraints.
The final clustering is found using the MaxWalk-
SAT weighted satisfiability solver (Kautz et al,
1997), with the appropriate extensions. We first ran
a MaxWalkSAT pass with only finite-weight formu-
las, then ran another pass with all formulas. We
found that this significantly improved the quality of
the results that MaxWalkSAT returned.
6 Experiments
6.1 System
We implemented our method as an extension to the
Alchemy system (Kok et al, 2007). Since our learn-
ing uses sampling, all results are the average of five
runs using different random seeds. Our optimiza-
tion problem is not convex, so initialization is im-
portant. The core of our model (head mixture) tends
to cluster non-pronouns with the same head. There-
fore, we initialized by setting all weights to zero,
and running the same learning algorithm on the base
MLN, while assuming that in the ground truth, non-
pronouns are clustered by their heads. (Effectively,
the corresponding InClust atoms are assigned to
appropriate values and are included in Y rather than
Z during learning.) We used 30 iterations of PSCG
for learning. (In preliminary experiments, additional
iterations had little effect on coreference accuracy.)
We generated 100 samples using MC-SAT for each
expectation approximation.6
6.2 Methodology
We conducted experiments on MUC-6, ACE-2004,
and ACE Phrase-2 (ACE-2). We evaluated our sys-
tems using two commonly-used scoring programs:
MUC (Vilain et al, 1995) and B3 (Amit & Bald-
win, 1998). To gain more insight, we also report
pairwise resolution scores and mean absolute error
in the number of clusters.
6Each sample actually contains a large number of ground-
ings, so 100 samples yield sufficiently accurate statistics for
learning.
The MUC-6 dataset consists of 30 documents for
testing and 221 for training. To evaluate the contri-
bution of the major components in our model, we
conducted five experiments, each differing from the
previous one in a single aspect. We emphasize that
our approach is unsupervised, and thus the data only
contains raw text plus true mention boundaries.
MLN-1 In this experiment, the base MLN was
used, and the head was chosen crudely as the
rightmost token in a mention. Our system was
run on each test document separately, using a
minimum of training data (the document itself).
MLN-30 Our system was trained on all 30 test doc-
uments together. This tests how much can be
gained by pooling information.
MLN-H The heads were determined using the head
rules in the Stanford parser (Klein & Manning,
2003), plus simple heuristics to handle suffixes
such as ?Corp.? and ?Inc.?
MLN-HA The apposition rule was added.
MLN-HAN The predicate-nominal rule was added.
This is our full model.
We also compared with two rule-based MLNs:
RULE chose the head crudely as the rightmost token
in a mention, and did not include the apposition rule
and predicate-nominal rule; RULE-HAN chose the
head using the head rules in the Stanford parser, and
included the apposition rule and predicate-nominal
rule.
Past results on ACE were obtained on different
releases of the datasets, e.g., Haghighi and Klein
(2007) used the ACE-2004 training corpus, Ng
(2005) and Denis and Baldridge (2007) used ACE
Phrase-2, and Culotta et al (2007) used the ACE-
2004 formal test set. In this paper, we used the
ACE-2004 training corpus and ACE Phrase-2 (ACE-
2) to enable direct comparisons with Haghighi &
Klein (2007), Ng (2005), and Denis and Baldridge
(2007). Due to license restrictions, we were not able
to obtain the ACE-2004 formal test set and so cannot
compare directly to Culotta et al (2007). The En-
glish version of the ACE-2004 training corpus con-
tains two sections, BNEWS and NWIRE, with 220
and 128 documents, respectively. ACE-2 contains a
656
Table 1: Comparison of coreference results in MUC
scores on the MUC-6 dataset.
# Doc. Prec. Rec. F1
H&K 60 80.8 52.8 63.9
H&K 381 80.4 62.4 70.3
M&W 221 - - 73.4
RULE - 76.0 65.9 70.5
RULE-HAN - 81.3 72.7 76.7
MLN-1 1 76.5 66.4 71.1
MLN-30 30 77.5 67.3 72.0
MLN-H 30 81.8 70.1 75.5
MLN-HA 30 82.7 75.1 78.7
MLN-HAN 30 83.0 75.8 79.2
Table 2: Comparison of coreference results in MUC
scores on the ACE-2004 (English) datasets.
EN-BNEWS Prec. Rec. F1
H&K 63.2 61.3 62.3
MLN-HAN 66.8 67.8 67.3
EN-NWIRE Prec. Rec. F1
H&K 66.7 62.3 64.2
MLN-HAN 71.3 70.5 70.9
training set and a test set. In our experiments, we
only used the test set, which contains three sections,
BNEWS, NWIRE, and NPAPER, with 51, 29, and
17 documents, respectively.
6.3 Results
Table 1 compares our system with previous ap-
proaches on the MUC-6 dataset, in MUC scores.
Our approach greatly outperformed Haghighi &
Klein (2007), the state-of-the-art unsupervised sys-
tem. Our system, trained on individual documents,
achieved an F1 score more than 7% higher than
theirs trained on 60 documents, and still outper-
formed it trained on 381 documents. Training on
the 30 test documents together resulted in a signif-
icant gain. (We also ran experiments using more
documents, and the results were similar.) Better
head identification (MLN-H) led to a large improve-
ment in accuracy, which is expected since for men-
tions with a right modifier, the rightmost tokens con-
fuse rather than help coreference (e.g., ?the chair-
man of Microsoft?). Notice that with this improve-
ment our system already outperforms a state-of-the-
Table 3: Comparison of coreference results in MUC
scores on the ACE-2 datasets.
BNEWS Prec. Rec. F1
Ng 67.9 62.2 64.9
D&B 78.0 62.1 69.2
MLN-HAN 68.3 66.6 67.4
NWIRE Prec. Rec. F1
Ng 60.3 50.1 54.7
D&B 75.8 60.8 67.5
MLN-HAN 67.7 67.3 67.4
NPAPER Prec. Rec. F1
Ng 71.4 67.4 69.3
D&B 77.6 68.0 72.5
MLN-HAN 69.2 71.7 70.4
Table 4: Comparison of coreference results in B3 scores
on the ACE-2 datasets.
BNEWS Prec. Rec. F1
Ng 77.1 57.0 65.6
MLN-HAN 70.3 65.3 67.7
NWIRE Prec. Rec. F1
Ng 75.4 59.3 66.4
MLN-HAN 74.7 68.8 71.6
NPAPER Prec. Rec. F1
Ng 75.4 59.3 66.4
MLN-HAN 70.0 66.5 68.2
art supervised system (McCallum & Wellner, 2005).
Leveraging apposition resulted in another large im-
provement, and predicate nominals also helped. Our
full model scores about 9% higher than Haghighi &
Klein (2007), and about 6% higher than McCallum
& Wellner (2005). To our knowledge, this is the best
coreference accuracy reported on MUC-6 to date.7
The B3 scores of MLN-HAN on the MUC-6 dataset
are 77.4 (precision), 67.6 (recall) and 72.2 (F1).
(The other systems did not report B3.) Interest-
ingly, the rule-based MLN (RULE) sufficed to out-
perform Haghighi & Klein (2007), and by using bet-
ter heads and the apposition and predicate-nominal
rules (RULE-HAN), it outperformed McCallum &
Wellner (2005), the supervised system. The MLNs
with learning (MLN-30 and MLN-HAN), on the
7As pointed out by Haghighi & Klein (2007), Luo et al
(2004) obtained a very high accuracy on MUC-6, but their sys-
tem used gold NER features and is not directly comparable.
657
Table 5: Our coreference results in precision, recall, and
F1 for pairwise resolution.
Pairwise Prec. Rec. F1
MUC-6 63.0 57.0 59.9
EN-BNEWS 51.2 36.4 42.5
EN-NWIRE 62.6 38.9 48.0
BNEWS 44.6 32.3 37.5
NWIRE 59.7 42.1 49.4
NPAPER 64.3 43.6 52.0
Table 6: Average gold number of clusters per document
vs. the mean absolute error of our system.
# Clusters MUC-6 EN-BN EN-NW
Gold 15.4 22.3 37.2
Mean Error 4.7 3.0 4.8
# Clusters BNEWS NWIRE NPAPER
Gold 20.4 39.2 55.2
Mean Error 2.5 5.6 6.6
other hand, substantially outperformed the corre-
sponding rule-based ones.
Table 2 compares our system to Haghighi & Klein
(2007) on the ACE-2004 training set in MUC scores.
Again, our system outperformed theirs by a large
margin. The B3 scores of MLN-HAN on the ACE-
2004 dataset are 71.6 (precision), 68.4 (recall) and
70.0 (F1) for BNEWS, and 75.7 (precision), 69.2
(recall) and 72.3 (F1) for NWIRE. (Haghighi &
Klein (2007) did not report B3.) Due to license re-
strictions, we could not compare directly to Culotta
et al (2007), who reported overall B3-F1 of 79.3 on
the formal test set.
Tables 3 and 4 compare our system to two re-
cent supervised systems, Ng (2005) and Denis
& Baldridge (2007). Our approach significantly
outperformed Ng (2005). It tied with Denis &
Baldridge (2007) on NWIRE, and was somewhat
less accurate on BNEWS and NPAPER.
Luo et al (2004) pointed out that one can ob-
tain a very high MUC score simply by lumping all
mentions together. B3 suffers less from this prob-
lem but is not perfect. Thus we also report pairwise
resolution scores (Table 5), the gold number of clus-
ters, and our mean absolute error in the number of
clusters (Table 6). Systems that simply merge all
mentions will have exceedingly low pairwise preci-
sion (far below 50%), and very large errors in the
number of clusters. Our system has fairly good pair-
wise precisions and small mean error in the number
of clusters, which verifies that our results are sound.
6.4 Error Analysis
Many of our system?s remaining errors involve nom-
inals. Additional features should be considered to
distinguish mentions that have the same head but are
different entities. For pronouns, many remaining er-
rors can be corrected using linguistic knowledge like
binding theory and salience hierarchy. Our heuris-
tics for identifying appositives and predicate nomi-
nals also make many errors, which often can be fixed
with additional name entity recognition capabilities
(e.g., given ?Mike Sullivan, VOA News?, it helps to
know that the former is a person and the latter an
organization). The most challenging case involves
phrases with different heads that are both proper
nouns (e.g., ?Mr. Bush? and ?the White House?).
Handling these cases requires domain knowledge
and/or more powerful joint inference.
7 Conclusion
This paper introduces the first unsupervised coref-
erence resolution system that is as accurate as su-
pervised systems. It performs joint inference among
mentions, using relations like apposition and predi-
cate nominals. It uses Markov logic as a representa-
tion language, which allows it to be easily extended
to incorporate additional linguistic and world knowl-
edge. Future directions include incorporating addi-
tional knowledge, conducting joint entity detection
and coreference resolution, and combining corefer-
ence resolution with other NLP tasks.
8 Acknowledgements
We thank the anonymous reviewers for their comments.
This research was funded by DARPA contracts NBCH-
D030010/02-000225, FA8750-07-D-0185, and HR0011-
07-C-0060, DARPA grant FA8750-05-2-0283, NSF grant
IIS-0534881, and ONR grant N-00014-05-1-0313 and
N00014-08-1-0670. The views and conclusions con-
tained in this document are those of the authors and
should not be interpreted as necessarily representing the
official policies, either expressed or implied, of DARPA,
NSF, ONR, or the United States Government.
658
References
Amit, B. & Baldwin, B. 1998. Algorithms for scoring
coreference chains. In Proc. MUC-7.
Bakir, G.; Hofmann, T.; Scho?lkopf, B.; Smola, A.;
Taskar, B. and Vishwanathan, S. (eds.) 2007. Pre-
dicting Structured Data. MIT Press.
Culotta, A.; Wick, M.; Hall, R. and McCallum, A. 2007.
First-order probabilistic models for coreference reso-
lution. In Proc. NAACL-07.
Denis, P. & Baldridge, J. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proc. NAACL-07.
Getoor, L. & Taskar, B. (eds.) 2007. Introduction to
Statistical Relational Learning. MIT Press.
Haghighi, A. & Klein, D. 2007. Unsupervised corefer-
ence resolution in a nonparametric Bayesian model. In
Proc. ACL-07.
Kautz, H.; Selman, B.; and Jiang, Y. 1997. A general
stochastic approach to solving problems with hard and
soft constraints. In The Satisfiability Problem: Theory
and Applications. AMS.
Klein, D. & Manning, C. 2003. Accurate unlexicalized
parsing. In Proc. ACL-03.
Kok, S.; Singla, P.; Richardson, M.; Domingos,
P.; Sumner, M.; Poon, H. & Lowd, D. 2007.
The Alchemy system for statistical relational AI.
http://alchemy.cs.washington.edu/.
Lowd, D. & Domingos, D. 2007. Efficient weight learn-
ing for Markov logic networks. In Proc. PKDD-07.
Luo, X.; Ittycheriah, A.; Jing, H.; Kambhatla, N. and
Roukos, S. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proc. ACL-04.
McCallum, A. & Wellner, B. 2005. Conditional models
of identity uncertainty with application to noun coref-
erence. In Proc. NIPS-04.
Ng, V. 2005. Machine Learning for Coreference Resolu-
tion: From Local Classification to Global Ranking. In
Proc. ACL-05.
Poon, H. & Domingos, P. 2006. Sound and efficient
inference with probabilistic and deterministic depen-
dencies. In Proc. AAAI-06.
Poon, H. & Domingos, P. 2007. Joint inference in infor-
mation extraction. In Proc. AAAI-07.
Poon, H. & Domingos, P. 2008. A general method for
reducing the complexity of relational inference and its
application to MCMC. In Proc. AAAI-08.
Richardson, M. & Domingos, P. 2006. Markov logic
networks. Machine Learning 62:107?136.
Vilain, M.; Burger, J.; Aberdeen, J.; Connolly, D. &
Hirschman, L. 1995. A model-theoretic coreference
scoring scheme. In Proc. MUC-6.
Wei, W.; Erenrich, J. and Selman, B. 2004. Towards
efficient sampling: Exploiting random walk strategies.
In Proc. AAAI-04.
659
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1?10,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Semantic Parsing
Hoifung Poon Pedro Domingos
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350, U.S.A.
{hoifung,pedrod}@cs.washington.edu
Abstract
We present the first unsupervised approach
to the problem of learning a semantic
parser, using Markov logic. Our USP
system transforms dependency trees into
quasi-logical forms, recursively induces
lambda forms from these, and clusters
them to abstract away syntactic variations
of the same meaning. The MAP semantic
parse of a sentence is obtained by recur-
sively assigning its parts to lambda-form
clusters and composing them. We evalu-
ate our approach by using it to extract a
knowledge base from biomedical abstracts
and answer questions. USP substantially
outperforms TextRunner, DIRT and an in-
formed baseline on both precision and re-
call on this task.
1 Introduction
Semantic parsing maps text to formal meaning
representations. This contrasts with semantic role
labeling (Carreras and Marquez, 2004) and other
forms of shallow semantic processing, which do
not aim to produce complete formal meanings.
Traditionally, semantic parsers were constructed
manually, but this is too costly and brittle. Re-
cently, a number of machine learning approaches
have been proposed (Zettlemoyer and Collins,
2005; Mooney, 2007). However, they are super-
vised, and providing the target logical form for
each sentence is costly and difficult to do consis-
tently and with high quality. Unsupervised ap-
proaches have been applied to shallow semantic
tasks (e.g., paraphrasing (Lin and Pantel, 2001),
information extraction (Banko et al, 2007)), but
not to semantic parsing.
In this paper we develop the first unsupervised
approach to semantic parsing, using Markov logic
(Richardson and Domingos, 2006). Our USP sys-
tem starts by clustering tokens of the same type,
and then recursively clusters expressions whose
subexpressions belong to the same clusters. Ex-
periments on a biomedical corpus show that this
approach is able to successfully translate syntac-
tic variations into a logical representation of their
common meaning (e.g., USP learns to map active
and passive voice to the same logical form, etc.).
This in turn allows it to correctly answer many
more questions than systems based on TextRun-
ner (Banko et al, 2007) and DIRT (Lin and Pantel,
2001).
We begin by reviewing the necessary back-
ground on semantic parsing and Markov logic. We
then describe our Markov logic network for un-
supervised semantic parsing, and the learning and
inference algorithms we used. Finally, we present
our experiments and results.
2 Background
2.1 Semantic Parsing
The standard language for formal meaning repre-
sentation is first-order logic. A term is any ex-
pression representing an object in the domain. An
atomic formula or atom is a predicate symbol ap-
plied to a tuple of terms. Formulas are recursively
constructed from atomic formulas using logical
connectives and quantifiers. A lexical entry de-
fines the logical form for a lexical item (e.g., a
word). The semantic parse of a sentence is de-
rived by starting with logical forms in the lexi-
cal entries and recursively composing the mean-
ing of larger fragments from their parts. In tradi-
tional approaches, the lexical entries and meaning-
1
composition rules are both manually constructed.
Below are sample rules in a definite clause gram-
mar (DCG) for parsing the sentence: ?Utah bor-
ders Idaho?.
V erb[?y?x.borders(x, y)]? borders
NP [Utah]? Utah
NP [Idaho]? Idaho
V P [rel(obj)]? V erb[rel] NP [obj]
S[rel(obj)]? NP [obj] V P [rel]
The first three lines are lexical entries. They are
fired upon seeing the individual words. For exam-
ple, the first rule applies to the word ?borders? and
generates syntactic category Verb with the mean-
ing ?y?x.borders(x, y) that represents the next-
to relation. Here, we use the standard lambda-
calculus notation, where ?y?x.borders(x, y)
represents a function that is true for any (x, y)-
pair such that borders(x, y) holds. The last two
rules compose the meanings of sub-parts into that
of the larger part. For example, after the first
and third rules are fired, the fourth rule fires and
generates V P [?y?x.borders(x, y)(Idaho)]; this
meaning simplifies to ?x.borders(x, Idaho) by
the ?-reduction rule, which substitutes the argu-
ment for a variable in a functional application.
A major challenge to semantic parsing is syn-
tactic variations of the same meaning, which
abound in natural languages. For example, the
aforementioned sentence can be rephrased as
?Utah is next to Idaho,??Utah shares a border with
Idaho,? etc. Manually encoding all these varia-
tions into the grammar is tedious and error-prone.
Supervised semantic parsing addresses this issue
by learning to construct the grammar automati-
cally from sample meaning annotations (Mooney,
2007). Existing approaches differ in the meaning
representation languages they use and the amount
of annotation required. In the approach of Zettle-
moyer and Collins (2005), the training data con-
sists of sentences paired with their meanings in
lambda form. A probabilistic combinatory cate-
gorial grammar (PCCG) is learned using a log-
linear model, where the probability of the final
logical form L and meaning-derivation tree T
conditioned on the sentence S is P (L, T |S) =
1
Z
exp (
?
i
w
i
f
i
(L, T, S)). Here Z is the normal-
ization constant and f
i
are the feature functions
with weights w
i
. Candidate lexical entries are gen-
erated by a domain-specific procedure based on
the target logical forms.
The major limitation of supervised approaches
is that they require meaning annotations for ex-
ample sentences. Even in a restricted domain,
doing this consistently and with high quality re-
quires nontrivial effort. For unrestricted text, the
complexity and subjectivity of annotation render it
essentially infeasible; even pre-specifying the tar-
get predicates and objects is very difficult. There-
fore, to apply semantic parsing beyond limited do-
mains, it is crucial to develop unsupervised meth-
ods that do not rely on labeled meanings.
In the past, unsupervised approaches have been
applied to some semantic tasks, but not to seman-
tic parsing. For example, DIRT (Lin and Pan-
tel, 2001) learns paraphrases of binary relations
based on distributional similarity of their argu-
ments; TextRunner (Banko et al, 2007) automati-
cally extracts relational triples in open domains us-
ing a self-trained extractor; SNE applies relational
clustering to generate a semantic network from
TextRunner triples (Kok and Domingos, 2008).
While these systems illustrate the promise of un-
supervised methods, the semantic content they ex-
tract is nonetheless shallow and does not constitute
the complete formal meaning that can be obtained
by a semantic parser.
Another issue is that existing approaches to se-
mantic parsing learn to parse syntax and semantics
together.1 The drawback is that the complexity
in syntactic processing is coupled with semantic
parsing and makes the latter even harder. For ex-
ample, when applying their approach to a different
domain with somewhat less rigid syntax, Zettle-
moyer and Collins (2007) need to introduce new
combinators and new forms of candidate lexical
entries. Ideally, we should leverage the enormous
progress made in syntactic parsing and generate
semantic parses directly from syntactic analysis.
2.2 Markov Logic
In many NLP applications, there exist rich rela-
tions among objects, and recent work in statisti-
cal relational learning (Getoor and Taskar, 2007)
and structured prediction (Bakir et al, 2007) has
shown that leveraging these can greatly improve
accuracy. One of the most powerful representa-
tions for this is Markov logic, which is a proba-
bilistic extension of first-order logic (Richardson
and Domingos, 2006). Markov logic makes it
1The only exception that we are aware of is Ge and
Mooney (2009).
2
possible to compactly specify probability distri-
butions over complex relational domains, and has
been successfully applied to unsupervised corefer-
ence resolution (Poon and Domingos, 2008) and
other tasks. A Markov logic network (MLN) is
a set of weighted first-order clauses. Together
with a set of constants, it defines a Markov net-
work with one node per ground atom and one fea-
ture per ground clause. The weight of a feature
is the weight of the first-order clause that origi-
nated it. The probability of a state x in such a
network is given by the log-linear model P (x) =
1
Z
exp (
?
i
w
i
n
i
(x)), where Z is a normalization
constant, w
i
is the weight of the ith formula, and
n
i
is the number of satisfied groundings.
3 Unsupervised Semantic Parsing with
Markov Logic
Unsupervised semantic parsing (USP) rests on
three key ideas. First, the target predicate and ob-
ject constants, which are pre-specified in super-
vised semantic parsing, can be viewed as clusters
of syntactic variations of the same meaning, and
can be learned from data. For example, borders
represents the next-to relation, and can be viewed
as the cluster of different forms for expressing this
relation, such as ?borders?, ?is next to?, ?share the
border with?; Utah represents the state of Utah,
and can be viewed as the cluster of ?Utah?, ?the
beehive state?, etc.
Second, the identification and clustering of can-
didate forms are integrated with the learning for
meaning composition, where forms that are used
in composition with the same forms are encour-
aged to cluster together, and so are forms that are
composed of the same sub-forms. This amounts to
a novel form of relational clustering, where clus-
tering is done not just on fixed elements in rela-
tional tuples, but on arbitrary forms that are built
up recursively.
Third, while most existing approaches (manual
or supervised learning) learn to parse both syn-
tax and semantics, unsupervised semantic pars-
ing starts directly from syntactic analyses and fo-
cuses solely on translating them to semantic con-
tent. This enables us to leverage advanced syn-
tactic parsers and (indirectly) the available rich re-
sources for them. More importantly, it separates
the complexity in syntactic analysis from the se-
mantic one, and makes the latter much easier to
perform. In particular, meaning composition does
not require domain-specific procedures for gener-
ating candidate lexicons, as is often needed by su-
pervised methods.
The input to our USP system consists of de-
pendency trees of training sentences. Compared
to phrase-structure syntax, dependency trees are
the more appropriate starting point for semantic
processing, as they already exhibit much of the
relation-argument structure at the lexical level.
USP first uses a deterministic procedure to con-
vert dependency trees into quasi-logical forms
(QLFs). The QLFs and their sub-formulas have
natural lambda forms, as will be described later.
Starting with clusters of lambda forms at the atom
level, USP recursively builds up clusters of larger
lambda forms. The final output is a probability
distribution over lambda-form clusters and their
compositions, as well as the MAP semantic parses
of training sentences.
In the remainder of the section, we describe
the details of USP. We first present the procedure
for generating QLFs from dependency trees. We
then introduce their lambda forms and clusters,
and show how semantic parsing works in this set-
ting. Finally, we present the Markov logic net-
work (MLN) used by USP. In the next sections, we
present efficient algorithms for learning and infer-
ence with this MLN.
3.1 Derivation of Quasi-Logical Forms
A dependency tree is a tree where nodes are words
and edges are dependency labels. To derive the
QLF, we convert each node to an unary atom with
the predicate being the lemma plus POS tag (be-
low, we still use the word for simplicity), and each
edge to a binary atom with the predicate being
the dependency label. For example, the node for
Utah becomes Utah(n
1
) and the subject depen-
dency becomes nsubj(n1, n2). Here, the n
i
are
Skolem constants indexed by the nodes. The QLF
for a sentence is the conjunction of the atoms for
the nodes and edges, e.g., the sentence above will
become borders(n
1
) ? Utah(n
2
) ? Idaho(n
3
) ?
nsubj(n
1
, n
2
) ? dobj(n
1
, n
3
).
3.2 Lambda-Form Clusters and Semantic
Parsing in USP
Given a QLF, a relation or an object is repre-
sented by the conjunction of a subset of the atoms.
For example, the next-to relation is represented
by borders(n
1
)? nsubj(n
1
, n
2
)? dobj(n
1
, n
3
),
and the states of Utah and Idaho are represented
3
by Utah(n
2
) and Idaho(n
3
). The meaning com-
position of two sub-formulas is simply their con-
junction. This allows the maximum flexibility in
learning. In particular, lexical entries are no longer
limited to be adjacent words as in Zettlemoyer and
Collins (2005), but can be arbitrary fragments in a
dependency tree.
For every sub-formula F , we define a corre-
sponding lambda form that can be derived by re-
placing every Skolem constant n
i
that does not
appear in any unary atom in F with a unique
lambda variable x
i
. Intuitively, such constants
represent objects introduced somewhere else (by
the unary atoms containing them), and corre-
spond to the arguments of the relation repre-
sented by F . For example, the lambda form
for borders(n
1
) ? nsubj(n
1
, n
2
) ? dobj(n
1
, n
3
)
is ?x
2
?x
3
. borders(n
1
) ? nsubj(n
1
, x
2
) ?
dobj(n
1
, x
3
).
Conceptually, a lambda-form cluster is a set of
semantically interchangeable lambda forms. For
example, to express the meaning that Utah bor-
ders Idaho, we can use any form in the cluster
representing the next-to relation (e.g., ?borders?,
?shares a border with?), any form in the cluster
representing the state of Utah (e.g., ?the beehive
state?), and any form in the cluster representing
the state of Idaho (e.g., ?Idaho?). Conditioned
on the clusters, the choices of individual lambda
forms are independent of each other.
To handle variable number of arguments, we
follow Davidsonian semantics and further de-
compose a lambda form into the core form,
which does not contain any lambda variable
(e.g., borders(n
1
)), and the argument forms,
which contain a single lambda variable (e.g.,
?x
2
.nsubj(n
1
, x
2
) and ?x
3
.dobj(n
1
, x
3
)). Each
lambda-form cluster may contain some number of
argument types, which cluster distinct forms of the
same argument in a relation. For example, in Stan-
ford dependencies, the object of a verb uses the de-
pendency dobj in the active voice, but nsubjpass
in passive.
Lambda-form clusters abstract away syntactic
variations of the same meaning. Given an in-
stance of cluster T with arguments of argument
types A
1
, ? ? ? , A
k
, its abstract lambda form is given
by ?x
1
? ? ??x
k
.T(n) ?
?
k
i=1
A
i
(n, x
i
).
Given a sentence and its QLF, semantic pars-
ing amounts to partitioning the atoms in the QLF,
dividing each part into core form and argument
forms, and then assigning each form to a cluster
or an argument type. The final logical form is de-
rived by composing the abstract lambda forms of
the parts using the ?-reduction rule.2
3.3 The USP MLN
Formally, for a QLF Q, a semantic parse L par-
titions Q into parts p
1
, p
2
, ? ? ? , p
n
; each part p is
assigned to some lambda-form cluster c, and is
further partitioned into core form f and argument
forms f
1
, ? ? ? , f
k
; each argument form is assigned
to an argument type a in c. The USP MLN de-
fines a joint probability distribution over Q and L
by modeling the distributions over forms and ar-
guments given the cluster or argument type.
Before presenting the predicates and formu-
las in our MLN, we should emphasize that they
should not be confused with the atoms and formu-
las in the QLFs, which are represented by reified
constants and variables.
To model distributions over lambda forms,
we introduce the predicates Form(p, f!) and
ArgForm(p, i, f!), where p is a part, i is the in-
dex of an argument, and f is a QLF subformula.
Form(p, f) is true iff part p has core form f, and
ArgForm(p, i, f) is true iff the ith argument in p
has form f.3 The ?f!? notation signifies that each
part or argument can have only one form.
To model distributions over arguments, we in-
troduce three more predicates: ArgType(p, i, a!)
signifies that the ith argument of p is assigned to
argument type a; Arg(p, i, p?) signifies that the
ith argument of p is p?; Number(p, a, n) signifies
that there are n arguments of p that are assigned
to type a. The truth value of Number(p, a, n) is
determined by the ArgType atoms.
Unsupervised semantic parsing can be captured
by four formulas:
p ? +c ? Form(p,+f)
ArgType(p, i,+a) ? ArgForm(p, i,+f)
Arg(p, i, p
?
) ? ArgType(p, i,+a) ? p
?
? +c
?
Number(p,+a,+n)
All free variables are implicitly universally quan-
tified. The ?+? notation signifies that the MLN
contains an instance of the formula, with a sep-
arate weight, for each value combination of the
2Currently, we do not handle quantifier scoping or se-
mantics for specific closed-class words such as determiners.
These will be pursued in future work.
3There are hard constraints to guarantee that these assign-
ments form a legal partition. We omit them for simplicity.
4
variables with a plus sign. The first formula mod-
els the mixture of core forms given the cluster, and
the others model the mixtures of argument forms,
argument types, and argument numbers, respec-
tively, given the argument type.
To encourage clustering and avoid overfitting,
we impose an exponential prior with weight ? on
the number of parameters.4
The MLN above has one problem: it often
clusters expressions that are semantically oppo-
site. For example, it clusters antonyms like ?el-
derly/young?, ?mature/immature?. This issue also
occurs in other semantic-processing systems (e.g.,
DIRT). In general, this is a difficult open problem
that only recently has started to receive some at-
tention (Mohammad et al, 2008). Resolving this
is not the focus of this paper, but we describe a
general heuristic for fixing this problem. We ob-
serve that the problem stems from the lack of nega-
tive features for discovering meanings in contrast.
In natural languages, parallel structures like con-
junctions are one such feature.5 We thus introduce
an exponential prior with weight ? on the number
of conjunctions where the two conjunctive parts
are assigned to the same cluster. To detect con-
junction, we simply used the Stanford dependen-
cies that begin with ?conj?. This proves very ef-
fective, fixing the majority of the errors in our ex-
periments.
4 Inference
Given a sentence and the quasi-logical form Q
derived from its dependency tree, the conditional
probability for a semantic parse L is given by
Pr(L|Q) ? exp (
?
i
w
i
n
i
(L,Q)). The MAP se-
mantic parse is simply argmax
L
?
i
w
i
n
i
(L,Q).
Enumerating all L?s is intractable. It is also un-
necessary, since most partitions will result in parts
whose lambda forms have no cluster they can be
assigned to. Instead, USP uses a greedy algorithm
to search for the MAP parse. First we introduce
some definitions: a partition is called ?-reducible
from p if it can be obtained from the current parti-
tion by recursively ?-reducing the part containing
p with one of its arguments; such a partition is
4Excluding weights of? or??, which signify hard con-
straints.
5For example, in the sentence ?IL-2 inhibits X in A and
induces Y in B?, the conjunction between ?inhibits? and ?in-
duces? suggests that they are different. If ?inhibits? and ?in-
duces? are indeed synonyms, such a sentence will sound awk-
ward and would probably be rephrased as ?IL-2 inhibits X in
A and Y in B?.
Algorithm 1 USP-Parse(MLN, QLF)
Form parts for individual atoms in QLF and as-
sign each to its most probable cluster
repeat
for all parts p in the current partition do
for all partitions that are ?-reducible from
p and feasible do
Find the most probable cluster and argu-
ment type assignments for the new part
and its arguments
end for
end for
Change to the new partition and assignments
with the highest gain in probability
until none of these improve the probability
return current partition and assignments
called feasible if the core form of the new part is
contained in some cluster. For example, consider
the QLF of ?Utah borders Idaho? and assume
that the current partition is ?x
2
x
3
.borders(n
1
) ?
nsubj(n
1
, x
2
) ? dobj(n
1
, x
3
), Utah(n
2
),
Idaho(n
3
). Then the following partition is
?-reducible from the first part in the above
partition: ?x
3
.borders(n
1
) ? nsubj(n
1
, n
2
) ?
Utah(n
2
) ? dobj(n
1
, x
3
), Idaho(n
3
). Whether
this new partition is feasible depends on whether
the core form of the new part ?x
3
.borders(n
1
) ?
nsubj(n
1
, n
2
) ? Utah(n
2
) ? dobj(n
1
, x
3
) (i.e.
borders(n
1
) ? nsubj(n
1
, n
2
) ? Utah(n
2
)) is
contained in some lambda-form cluster.
Algorithm 1 gives pseudo-code for our algo-
rithm. Given part p, finding partitions that are ?-
reducible from p and feasible can be done in time
O(ST ), where S is the size of the clustering in
the number of core forms and T is the maximum
number of atoms in a core form. We omit the proof
here but point out that it is related to the unordered
subtree matching problem which can be solved in
linear time (Kilpelainen, 1992). Inverted indexes
(e.g., from p to eligible core forms) are used to fur-
ther improve the efficiency. For a new part p and
a cluster that contains p?s core form, there are km
ways of assigning p?s m arguments to the k argu-
ment types of the cluster. For larger k and m, this
is very expensive. We therefore approximate it by
assigning each argument to the best type, indepen-
dent of other arguments.
This algorithm is very efficient, and is used re-
peatedly in learning.
5
5 Learning
The learning problem in USP is to maximize the
log-likelihood of observing the QLFs obtained
from the dependency trees, denoted by Q, sum-
ming out the unobserved semantic parses:
L
?
(Q) = logP
?
(Q)
= log
?
L
P
?
(Q,L)
Here, L are the semantic parses, ? are the MLN pa-
rameters, and P
?
(Q,L) are the completion likeli-
hoods. A serious challenge in unsupervised learn-
ing is the identifiability problem (i.e., the opti-
mal parameters are not unique) (Liang and Klein,
2008). This problem is particularly severe for
log-linear models with hard constraints, which are
common in MLNs. For example, in our USP
MLN, conditioned on the fact that p ? c, there is
exactly one value of f that can satisfy the formula
p ? c ? Form(p, f), and if we add some constant
number to the weights of p ? c ? Form(p, f) for
all f, the probability distribution stays the same.6
The learner can be easily confused by the infinitely
many optima, especially in the early stages. To
address this problem, we impose local normaliza-
tion constraints on specific groups of formulas that
are mutually exclusive and exhaustive, i.e., in each
group, we require that
?
k
i=1
e
w
i
= 1, where w
i
are the weights of formulas in the group. Group-
ing is done in such a way as to encourage the
intended mixture behaviors. Specifically, for the
rule p ? +c ? Form(p,+f), all instances given
a fixed c form a group; for each of the remain-
ing three rules, all instances given a fixed a form a
group. Notice that with these constraints the com-
pletion likelihood P (Q,L) can be computed in
closed form for any L. In particular, each formula
group contributes a term equal to the weight of the
currently satisfied formula. In addition, the opti-
mal weights that maximize the completion likeli-
hood P (Q,L) can be derived in closed form us-
ing empirical relative frequencies. E.g., the opti-
mal weight of p ? c? Form(p, f) is log(n
c,f
/n
c
),
where n
c,f
is the number of parts p that satisfy
both p ? c and Form(p, f), and n
c
is the number
of parts p that satisfy p ? c.7 We leverage this fact
for efficient learning in USP.
6Regularizations, e.g., Gaussian priors on weights, allevi-
ate this problem by penalizing large weights, but it remains
true that weights within a short range are roughly equivalent.
7To see this, notice that for a given c, the total contribu-
tion to the completion likelihood from all groundings in its
formula group is
?
f
w
c,f
n
c,f
. In addition,
?
f
n
c,f
= n
c
Algorithm 2 USP-Learn(MLN, QLFs)
Create initial clusters and semantic parses
Merge clusters with the same core form
Agenda? ?
repeat
for all candidate operations O do
Score O by log-likelihood improvement
if score is above a threshold then
Add O to agenda
end if
end for
Execute the highest scoring operation O? in
the agenda
Regenerate MAP parses for affected QLFs
and update agenda and candidate operations
until agenda is empty
return the MLN with learned weights and the
semantic parses
Another major challenge in USP learning is the
summation in the likelihood, which is over all pos-
sible semantic parses for a given dependency tree.
Even an efficient sampler like MC-SAT (Poon and
Domingos, 2006), as used in Poon & Domingos
(2008), would have a hard time generating accu-
rate estimates within a reasonable amount of time.
On the other hand, as already noted in the previous
section, the lambda-form distribution is generally
sparse. Large lambda-forms are rare, as they cor-
respond to complex expressions that are often de-
composable into smaller ones. Moreover, while
ambiguities are present at the lexical level, they
quickly diminish when more words are present.
Therefore, a lambda form can usually only belong
to a small number of clusters, if not a unique one.
We thus simplify the problem by approximating
the sum with the mode, and search instead for the
L and ? that maximize logP
?
(Q,L). Since the op-
timal weights and log-likelihood can be derived in
closed form given the semantic parses L, we sim-
ply search over semantic parses, evaluating them
using log-likelihood.
Algorithm 2 gives pseudo-code for our algo-
rithm. The input consists of an MLN without
weights and the QLFs for the training sentences.
Two operators are used for updating semantic
parses. The first is to merge two clusters, denoted
by MERGE(C
1
, C
2
) for clusters C
1
, C
2
, which does
the following:
and there is the local normalization constraint
?
f
e
w
c,f
= 1.
The optimal weights w
c,f
are easily derived by solving this
constrained optimization problem.
6
1. Create a new cluster C and add all core forms
in C
1
, C
2
to C;
2. Create new argument types for C by merg-
ing those in C
1
, C
2
so as to maximize the log-
likelihood;
3. Remove C
1
, C
2
.
Here, merging two argument types refers to pool-
ing their argument forms to create a new argument
type. Enumerating all possible ways of creating
new argument types is intractable. USP approxi-
mates it by considering one type at a time and ei-
ther creating a new type for it or merging it to types
already considered, whichever maximizes the log-
likelihood. The types are considered in decreasing
order of their numbers of occurrences so that more
information is available for each decision. MERGE
clusters syntactically different expressions whose
meanings appear to be the same according to the
model.
The second operator is to create a new clus-
ter by composing two existing ones, denoted by
COMPOSE(C
R
, C
A
), which does the following:
1. Create a new cluster C;
2. Find all parts r ? C
R
, a ? C
A
such that a is
an argument of r, compose them to r(a) by
?-reduction and add the new part to C;
3. Create new argument types for C from the ar-
gument forms of r(a) so as to maximize the
log-likelihood.
COMPOSE creates clusters of large lambda-forms
if they tend to be composed of the same sub-
forms (e.g., the lambda form for ?is next to?).
These lambda-forms may later be merged with
other clusters (e.g., borders).
At learning time, USP maintains an agenda that
contains operations that have been evaluated and
are pending execution. During initialization, USP
forms a part and creates a new cluster for each
unary atom u(n). It also assigns binary atoms of
the form b(n, n?) to the part as argument forms
and creates a new argument type for each. This
forms the initial clustering and semantic parses.
USP then merges clusters with the same core form
(i.e., the same unary predicate) using MERGE.8 At
each step, USP evaluates the candidate operations
and adds them to the agenda if the improvement is
8Word-sense disambiguation can be handled by including
a new kind of operator that splits a cluster into subclusters.
We leave this to future work.
above a threshold.9 The operation with the highest
score is executed, and the parameters are updated
with the new optimal values. The QLFs which
contain an affected part are reparsed, and opera-
tions in the agenda whose score might be affected
are re-evaluated. These changes are done very ef-
ficiently using inverted indexes. We omit the de-
tails here due to space limitations. USP terminates
when the agenda is empty, and outputs the current
MLN parameters and semantic parses.
USP learning uses the same optimization objec-
tive as hard EM, and is also guaranteed to find a
local optimum since at each step it improves the
log-likelihood. It differs from EM in directly opti-
mizing the likelihood instead of a lower bound.
6 Experiments
6.1 Task
Evaluating unsupervised semantic parsers is dif-
ficult, because there is no predefined formal lan-
guage or gold logical forms for the input sen-
tences. Thus the best way to test them is by using
them for the ultimate goal: answering questions
based on the input corpus. In this paper, we ap-
plied USP to extracting knowledge from biomedi-
cal abstracts and evaluated its performance in an-
swering a set of questions that simulate the in-
formation needs of biomedical researchers. We
used the GENIA dataset (Kim et al, 2003) as
the source for knowledge extraction. It contains
1999 PubMed abstracts and marks all mentions
of biomedical entities according to the GENIA
ontology, such as cell, protein, and DNA. As a
first approximation to the questions a biomedi-
cal researcher might ask, we generated a set of
two thousand questions on relations between enti-
ties. Sample questions are: ?What regulates MIP-
1alpha??, ?What does anti-STAT 1 inhibit??. To
simulate the real information need, we sample the
relations from the 100 most frequently used verbs
(excluding the auxiliary verbs be, have, and do),
and sample the entities from those annotated in
GENIA, both according to their numbers of occur-
rences. We evaluated USP by the number of an-
swers it provided and the accuracy as determined
by manual labeling.10
9We currently set it to 10 to favor precision and guard
against errors due to inexact estimates.
10The labels and questions are available at
http://alchemy.cs.washington.edu/papers/poon09.
7
6.2 Systems
Since USP is the first unsupervised semantic
parser, conducting a meaningful comparison of it
with other systems is not straightforward. Stan-
dard question-answering (QA) benchmarks do not
provide the most appropriate comparison, be-
cause they tend to simultaneously emphasize other
aspects not directly related to semantic pars-
ing. Moreover, most state-of-the-art QA sys-
tems use supervised learning in their key compo-
nents and/or require domain-specific engineering
efforts. The closest available system to USP in
aims and capabilities is TextRunner (Banko et al,
2007), and we compare with it. TextRunner is the
state-of-the-art system for open-domain informa-
tion extraction; its goal is to extract knowledge
from text without using supervised labels. Given
that a central challenge to semantic parsing is re-
solving syntactic variations of the same meaning,
we also compare with RESOLVER (Yates and Et-
zioni, 2009), a state-of-the-art unsupervised sys-
tem based on TextRunner for jointly resolving en-
tities and relations, and DIRT (Lin and Pantel,
2001), which resolves paraphrases of binary rela-
tions. Finally, we also compared to an informed
baseline based on keyword matching.
Keyword: We consider a baseline system based
on keyword matching. The question substring
containing the verb and the available argument is
directly matched with the input text, ignoring case
and morphology. We consider two ways to derive
the answer given a match. The first one (KW) sim-
ply returns the rest of sentence on the other side of
the verb. The second one (KW-SYN) is informed
by syntax: the answer is extracted from the subject
or object of the verb, depending on the question. If
the verb does not contain the expected argument,
the sentence is ignored.
TextRunner: TextRunner inputs text and outputs
relational triples in the form (R,A
1
, A
2
), where
R is the relation string, and A
1
, A
2
the argument
strings. Given a triple and a question, we first
match their relation strings, and then match the
strings for the argument that is present in the ques-
tion. If both match, we return the other argument
string in the triple as an answer. We report results
when exact match is used (TR-EXACT), or when
the triple string can contain the question one as a
substring (TR-SUB).
RESOLVER: RESOLVER (Yates and Etzioni,
2009) inputs TextRunner triples and collectively
resolves coreferent relation and argument strings.
On the GENIA data, using the default parameters,
RESOLVER produces only a few trivial relation
clusters and no argument clusters. This is not sur-
prising, since RESOLVER assumes high redun-
dancy in the data, and will discard any strings with
fewer than 25 extractions. For a fair compari-
son, we also ran RESOLVER using all extractions,
and manually tuned the parameters based on eye-
balling of clustering quality. The best result was
obtained with 25 rounds of execution and with the
entity multiple set to 200 (the default is 30). To an-
swer questions, the only difference from TextRun-
ner is that a question string can match any string
in its cluster. As in TextRunner, we report results
for both exact match (RS-EXACT) and substring
(RS-SUB).
DIRT: The DIRT system inputs a path and returns
a set of similar paths. To use DIRT in question
answering, we queried it to obtain similar paths
for the relation of the question, and used these
paths while matching sentences. We first used
MINIPAR (Lin, 1998) to parse input text using
the same dependencies as DIRT. To determine a
match, we first check if the sentence contains the
question path or one of its DIRT paths. If so, and if
the available argument slot in the question is con-
tained in the one in the sentence, it is a match, and
we return the other argument slot from the sen-
tence if it is present. Ideally, a fair comparison will
require running DIRT on the GENIA text, but we
were not able to obtain the source code. We thus
resorted to using the latest DIRT database released
by the author, which contains paths extracted from
a large corpus with more than 1GB of text. This
puts DIRT in a very advantageous position com-
pared with other systems. In our experiments, we
used the top three similar paths, as including more
results in very low precision.
USP: We built a system for knowledge extrac-
tion and question answering on top of USP. It
generated Stanford dependencies (de Marneffe et
al., 2006) from the input text using the Stan-
ford parser, and then fed these to USP-Learn11,
which produced an MLN with learned weights
and the MAP semantic parses of the input sen-
tences. These MAP parses formed our knowledge
base (KB). To answer questions, the system first
parses the questions12 using USP-Parse with the
11
? and ? are set to ?5 and ?10.
12The question slot is replaced by a dummy word.
8
Table 1: Comparison of question answering re-
sults on the GENIA dataset.
# Total # Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP 334 295 88%
learned MLN, and then matches the question parse
to parses in the KB by testing subsumption (i.e., a
question parse matches a KB one iff the former is
subsumed by the latter). When a match occurs, our
system then looks for arguments of type in accor-
dance with the question. For example, if the ques-
tion is ?What regulates MIP-1alpha??, it searches
for the argument type of the relation that contains
the argument form ?nsubj? for subject. If such an
argument exists for the relation part, it will be re-
turned as the answer.
6.3 Results
Table 1 shows the results for all systems. USP
extracted the highest number of answers, almost
doubling that of the second highest (RS-SUB).
It obtained the highest accuracy at 88%, and
the number of correct answers it extracted is
three times that of the second highest system.
The informed baseline (KW-SYN) did surpris-
ingly well compared to systems other than USP, in
terms of accuracy and number of correct answers.
TextRunner achieved good accuracy when exact
match is used (TR-EXACT), but only obtained a
fraction of the answers compared to USP. With
substring match, its recall substantially improved,
but precision dropped more than 20 points. RE-
SOLVER improved the number of extracted an-
swers by sanctioning more matches based on the
clusters it generated. However, most of those ad-
ditional answers are incorrect due to wrong clus-
tering. DIRT obtained the second highest number
of correct answers, but its precision is quite low
because the similar paths contain many errors.
6.4 Qualitative Analysis
Manual inspection shows that USP is able to re-
solve many nontrivial syntactic variations with-
out user supervision. It consistently resolves the
syntactic difference between active and passive
voices. It successfully identifies many distinct ar-
gument forms that mean the same (e.g., ?X stimu-
lates Y? ? ?Y is stimulated with X?, ?expression
of X? ? ?X expression?). It also resolves many
nouns correctly and forms meaningful groups of
relations. Here are some sample clusters in core
forms:
{investigate, examine, evaluate, analyze, study,
assay}
{diminish, reduce, decrease, attenuate}
{synthesis, production, secretion, release}
{dramatically, substantially, significantly}
An example question-answer pair, together with
the source sentence, is shown below:
Q: What does IL-13 enhance?
A: The 12-lipoxygenase activity of murine
macrophages.
Sentence: The data presented here indicate
that (1) the 12-lipoxygenase activity of murine
macrophages is upregulated in vitro and in vivo
by IL-4 and/or IL-13, . . .
7 Conclusion
This paper introduces the first unsupervised ap-
proach to learning semantic parsers. Our USP
system is based on Markov logic, and recursively
clusters expressions to abstract away syntactic
variations of the same meaning. We have suc-
cessfully applied USP to extracting a knowledge
base from biomedical text and answering ques-
tions based on it.
Directions for future work include: better han-
dling of antonyms, subsumption relations among
expressions, quantifier scoping, more complex
lambda forms, etc.; use of context and discourse
to aid expression clustering and semantic parsing;
more efficient learning and inference; application
to larger corpora; etc.
8 Acknowledgements
We thank the anonymous reviewers for their comments. This
research was partly funded by ARO grant W911NF-08-1-
0242, DARPA contracts FA8750-05-2-0283, FA8750-07-D-
0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-
D030010, NSF grants IIS-0534881 and IIS-0803481, and
ONR grant N00014-08-1-0670. The views and conclusions
contained in this document are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies, either expressed or implied, of ARO, DARPA,
NSF, ONR, or the United States Government.
9
References
G. Bakir, T. Hofmann, B. B. Scho?lkopf, A. Smola,
B. Taskar, S. Vishwanathan, and (eds.). 2007. Pre-
dicting Structured Data. MIT Press, Cambridge,
MA.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the Twentieth International Joint Con-
ference on Artificial Intelligence, pages 2670?2676,
Hyderabad, India. AAAI Press.
Xavier Carreras and Luis Marquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role la-
beling. In Proceedings of the Eighth Conference on
Computational Natural Language Learning, pages
89?97, Boston, MA. ACL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation, pages 449?
454, Genoa, Italy. ELRA.
Ruifang Ge and Raymond J. Mooney. 2009. Learning
a compositional semantic parser using an existing
syntactic parser. In Proceedings of the Forty Sev-
enth Annual Meeting of the Association for Compu-
tational Linguistics, Singapore. ACL.
Lise Getoor and Ben Taskar, editors. 2007. Introduc-
tion to Statistical Relational Learning. MIT Press,
Cambridge, MA.
Pekka Kilpelainen. 1992. Tree Matching Prob-
lems with Applications to Structured Text databases.
Ph.D. Thesis, Department of Computer Science,
University of Helsinki.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus - a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:180?82.
Stanley Kok and Pedro Domingos. 2008. Extract-
ing semantic networks from text via relational clus-
tering. In Proceedings of the Nineteenth European
Conference on Machine Learning, pages 624?639,
Antwerp, Belgium. Springer.
Percy Liang and Dan Klein. 2008. Analyzing the er-
rors of unsupervised learning. In Proceedings of the
Forty Sixth Annual Meeting of the Association for
Computational Linguistics, pages 879?887, Colum-
bus, OH. ACL.
Dekang Lin and Patrick Pantel. 2001. DIRT - dis-
covery of inference rules from text. In Proceedings
of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 323?328, San Francisco, CA. ACM Press.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, Granada, Spain.
ELRA.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 982?991,
Honolulu, HI. ACL.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Proceedings of the Eighth International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 311?324, Mexico City,
Mexico. Springer.
Hoifung Poon and Pedro Domingos. 2006. Sound and
efficient inference with probabilistic and determin-
istic dependencies. In Proceedings of the Twenty
First National Conference on Artificial Intelligence,
pages 458?463, Boston, MA. AAAI Press.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
649?658, Honolulu, HI. ACL.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62:107?136.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34:255?296.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammers. In Proceedings of the Twenty First
Conference on Uncertainty in Artificial Intelligence,
pages 658?666, Edinburgh, Scotland. AUAI Press.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 878?887, Prague, Czech. ACL.
10
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 296?305,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Unsupervised Ontology Induction from Text
Hoifung Poon and Pedro Domingos
Department of Computer Science & Engineering
University of Washington
hoifung,pedrod@cs.washington.edu
Abstract
Extracting knowledge from unstructured
text is a long-standing goal of NLP. Al-
though learning approaches to many of its
subtasks have been developed (e.g., pars-
ing, taxonomy induction, information ex-
traction), all end-to-end solutions to date
require heavy supervision and/or manual
engineering, limiting their scope and scal-
ability. We present OntoUSP, a system that
induces and populates a probabilistic on-
tology using only dependency-parsed text
as input. OntoUSP builds on the USP
unsupervised semantic parser by jointly
forming ISA and IS-PART hierarchies of
lambda-form clusters. The ISA hierar-
chy allows more general knowledge to
be learned, and the use of smoothing for
parameter estimation. We evaluate On-
toUSP by using it to extract a knowledge
base from biomedical abstracts and an-
swer questions. OntoUSP improves on
the recall of USP by 47% and greatly
outperforms previous state-of-the-art ap-
proaches.
1 Introduction
Knowledge acquisition has been a major goal of
NLP since its early days. We would like comput-
ers to be able to read text and express the knowl-
edge it contains in a formal representation, suit-
able for answering questions and solving prob-
lems. However, progress has been difficult. The
earliest approaches were manual, but the sheer
amount of coding and knowledge engineering
needed makes them very costly and limits them to
well-circumscribed domains. More recently, ma-
chine learning approaches to a number of key sub-
problems have been developed (e.g., Snow et al
(2006)), but to date there is no sufficiently auto-
matic end-to-end solution. Most saliently, super-
vised learning requires labeled data, which itself is
costly and infeasible for large-scale, open-domain
knowledge acquisition.
Ideally, we would like to have an end-to-end un-
supervised (or lightly supervised) solution to the
problem of knowledge acquisition from text. The
TextRunner system (Banko et al, 2007) can ex-
tract a large number of ground atoms from the
Web using only a small number of seed patterns
as guidance, but it is unable to extract non-atomic
formulas, and the mass of facts it extracts is un-
structured and very noisy. The USP system (Poon
and Domingos, 2009) can extract formulas and ap-
pears to be fairly robust to noise. However, it is
still limited to extractions for which there is sub-
stantial evidence in the corpus, and in most cor-
pora most pieces of knowledge are stated only
once or a few times, making them very difficult to
extract without supervision. Also, the knowledge
extracted is simply a large set of formulas with-
out ontological structure, and the latter is essential
for compact representation and efficient reasoning
(Staab and Studer, 2004).
We propose OntoUSP (Ontological USP), a sys-
tem that learns an ISA hierarchy over clusters of
logical expressions, and populates it by translat-
ing sentences to logical form. OntoUSP is en-
coded in a few formulas of higher-order Markov
logic (Domingos and Lowd, 2009), and can be
viewed as extending USP with the capability to
perform hierarchical (as opposed to flat) cluster-
ing. This clustering is then used to perform hier-
archical smoothing (a.k.a. shrinkage), greatly in-
creasing the system?s capability to generalize from
296
sparse data.
We begin by reviewing the necessary back-
ground. We then present the OntoUSP Markov
logic network and the inference and learning al-
gorithms used with it. Finally, experiments on
a biomedical knowledge acquisition and question
answering task show that OntoUSP can greatly
outperform USP and previous systems.
2 Background
2.1 Ontology Learning
In general, ontology induction (constructing an
ontology) and ontology population (mapping tex-
tual expressions to concepts and relations in the
ontology) remain difficult open problems (Staab
and Studer, 2004). Recently, ontology learn-
ing has attracted increasing interest in both NLP
and semantic Web communities (Cimiano, 2006;
Maedche, 2002), and a number of machine learn-
ing approaches have been developed (e.g., Snow
et al (2006), Cimiano (2006), Suchanek et al
(2008,2009), Wu & Weld (2008)). However, they
are still limited in several aspects. Most ap-
proaches induce and populate a deterministic on-
tology, which does not capture the inherent un-
certainty among the entities and relations. Be-
sides, many of them either bootstrap from heuris-
tic patterns (e.g., Hearst patterns (Hearst, 1992))
or build on existing structured or semi-structured
knowledge bases (e.g., WordNet (Fellbaum, 1998)
and Wikipedia1), thus are limited in coverage.
Moreover, they often focus on inducing ontology
over individual words rather than arbitrarily large
meaning units (e.g., idioms, phrasal verbs, etc.).
Most importantly, existing approaches typically
separate ontology induction from population and
knowledge extraction, and pursue each task in a
standalone fashion. While computationally effi-
cient, this is suboptimal. The resulted ontology
is disconnected from text and requires additional
effort to map between the two (Tsujii, 2004). In
addition, this fails to leverage the intimate connec-
tions between the three tasks for joint inference
and mutual disambiguiation.
Our approach differs from existing ones in two
main aspects: we induce a probabilistic ontology
from text, and we do so by jointly conducting on-
tology induction, population, and knowledge ex-
traction. Probabilistic modeling handles uncer-
tainty and noise. A joint approach propagates in-
1http : //www.wikipedia.org
formation among the three tasks, uncovers more
implicit information from text, and can potentially
work well even in domains not well covered by
existing resources like WordNet and Wikipedia.
Furthermore, we leverage the ontology for hierar-
chical smoothing and incorporate this smoothing
into the induction process. This facilitates more
accurate parameter estimation and better general-
ization.
Our approach can also leverage existing on-
tologies and knowledge bases to conduct semi-
supervised ontology induction (e.g., by incorpo-
rating existing structures as hard constraints or pe-
nalizing deviation from them).
2.2 Markov Logic
Combining uncertainty handling and joint infer-
ence is the hallmark of the emerging field of statis-
tical relational learning (a.k.a. structured predic-
tion), where a plethora of approaches have been
developed (Getoor and Taskar, 2007; Bakir et al,
2007). In this paper, we use Markov logic (Domin-
gos and Lowd, 2009), which is the leading unify-
ing framework, but other approaches can be used
as well. Markov logic is a probabilistic exten-
sion of first-order logic and can compactly specify
probability distributions over complex relational
domains. It has been successfully applied to un-
supervised learning for various NLP tasks such
as coreference resolution (Poon and Domingos,
2008) and semantic parsing (Poon and Domingos,
2009). A Markov logic network (MLN) is a set of
weighted first-order clauses. Together with a set
of constants, it defines a Markov network with one
node per ground atom and one feature per ground
clause. The weight of a feature is the weight of the
first-order clause that originated it. The probabil-
ity of a state x in such a network is given by the
log-linear model P (x) = 1Z exp (
?
i wini(x)),
where Z is a normalization constant, wi is the
weight of the ith formula, and ni is the number
of satisfied groundings.
2.3 Unsupervised Semantic Parsing
Semantic parsing aims to obtain a complete canon-
ical meaning representation for input sentences. It
can be viewed as a structured prediction problem,
where a semantic parse is formed by partitioning
the input sentence (or a syntactic analysis such as
a dependency tree) into meaning units and assign-
ing each unit to the logical form representing an
entity or relation (Figure 1). In effect, a semantic
297
induces
protein CD11b
nsubj dobj
IL-4nn
induces
protein CD11b
nsubj dobj
IL-4nn
INDUCE
INDUCER INDUCED
IL-4
CD11B
INDUCE(e1)INDUCER(e1,e2) INDUCED(e1,e3)IL-4(e2) CD11B(e3)
IL-4 protein induces CD11b
Structured prediction: Partition + Assignment
Figure 1: An example of semantic parsing. Top:
semantic parsing converts an input sentence into
logical form in Davidsonian semantics. Bottom: a
semantic parse consists of a partition of the depen-
dency tree and an assignment of its parts.
parser extracts knowledge from input text and con-
verts them into logical form (the semantic parse),
which can then be used in logical and probabilistic
inference and support end tasks such as question
answering.
A major challenge to semantic parsing is syn-
tactic and lexical variations of the same mean-
ing, which abound in natural languages. For ex-
ample, the fact that IL-4 protein induces CD11b
can be expressed in a variety of ways, such
as, ?Interleukin-4 enhances the expression of
CD11b?, ?CD11b is upregulated by IL-4?, etc.
Past approaches either manually construct a gram-
mar or require example sentences with meaning
annotation, and do not scale beyond restricted do-
mains.
Recently, we developed the USP system (Poon
and Domingos, 2009), the first unsupervised ap-
proach for semantic parsing.2 USP inputs de-
pendency trees of sentences and first transforms
them into quasi-logical forms (QLFs) by convert-
ing each node to a unary atom and each depen-
dency edge to a binary atom (e.g., the node for
?induces? becomes induces(e1) and the subject
dependency becomes nsubj(e1, e2), where ei?s
are Skolem constants indexed by the nodes.).3
For each sentence, a semantic parse comprises of
a partition of its QLF into subexpressions, each
of which has a naturally corresponding lambda
2In this paper, we use a slightly different formulation of
USP and its MLN to facilitate the exposition of OntoUSP.
3We call these QLFs because they are not true logical
form (the ambiguities are not yet resolved). This is related
to but not identical with the definition in Alshawi (1990).
Object Cluster: INDUCE
induces 0.1
enhances 0.4? ??
Property Cluster: INDUCER
0.5
0.4?
IL-4 0.2
IL-8 0.1?
None 0.1
One 0.8?
nsubj
agent
Core Form
Figure 2: An example of object/property clusters:
INDUCE contains the core-form property cluster
and others, such as the agent argument INDUCER.
form,4 and an assignment of each subexpression
to a lambda-form cluster.
The lambda-form clusters naturally form an IS-
PART hierarchy (Figure 2). An object cluster cor-
responds to semantic concepts or relations such as
INDUCE, and contains a variable number of prop-
erty clusters. A special property cluster of core
forms maintains a distribution over variations in
lambda forms for expressing this concept or rela-
tion. Other property clusters correspond to modi-
fiers or arguments such as INDUCER (the agent ar-
gument of INDUCE), each of which in turn con-
tains three subclusters of property values: the
argument-object subcluster maintains a distribu-
tion over object clusters that may occur in this
argument (e.g., IL ? 4), the argument-form sub-
cluster maintains a distribution over lambda forms
that corresponds to syntactic variations for this ar-
gument (e.g., nsubj in active voice and agent in
passive voice), and the argument-number subclus-
ter maintains a distribution over total numbers of
this argument that may occur in a sentence (e.g.,
zero if the argument is not mentioned).
Effectively, USP simultaneously discovers the
lambda-form clusters and an IS-PART hierarchy
among them. It does so by recursively combining
subexpressions that are composed with or by sim-
ilar subexpressions. The partition breaks a sen-
tence into subexpressions that are meaning units,
and the clustering abstracts away syntactic and
lexical variations for the same meaning. This
novel form of relational clustering is governed by
a joint probability distribution P (T, L) defined in
higher-order5 Markov logic, where T are the input
dependency trees, and L the semantic parses. The
4The lambda form is derived by replacing every Skolem
constant ei that does not appear in any unary atom in the
subexpression with a lambda variable xi that is uniquely in-
dexed by the corresponding node i. For example, the lambda
form for nsubj(e1, e2) is ?x1?x2.nsubj(x1, x2).
5Variables can range over arbitrary lambda forms.
298
main predicates are:
e ? c: expression e is assigned to cluster c;
SubExpr(s, e): s is a subexpression of e;
HasValue(s, v): s is of value v;
IsPart(c, i, p): p is the property cluster in ob-
ject cluster c uniquely indexed by i.
In USP, property clusters in different object clus-
ters use distinct index i?s. As we will see later,
in OntoUSP, property clusters with ISA relation
share the same index i, which corresponds to a
generic semantic frame such as agent and patient.
The probability model of USP can be captured
by two formulas:
x ? +p ? HasValue(x,+v)
e ? c ? SubExpr(x, e) ? x ? p
? ?1i.IsPart(c, i, p).
All free variables are implicitly universally
quantified. The ?+? notation signifies that the
MLN contains an instance of the formula, with
a separate weight, for each value combination of
the variables with a plus sign. The first formula is
the core of the model and represents the mixture
of property values given the cluster. The second
formula ensures that a property cluster must be a
part in the corresponding object cluster; it is a hard
constraint, as signified by the period at the end.
To encourage clustering, USP imposes an expo-
nential prior over the number of parameters.
To parse a new sentence, USP starts by parti-
tioning the QLF into atomic forms, and then hill-
climbs on the probability using a search operator
based on lambda reduction until it finds the max-
imum a posteriori (MAP) parse. During learn-
ing, USP starts with clusters of atomic forms,
maintains the optimal semantic parses according
to current parameters, and hill-climbs on the log-
likelihood of observed QLFs using two search op-
erators:
MERGE(c1, c2) merges clusters c1, c2 into a larger
cluster c by merging the core-form clusters
and argument clusters of c1, c2, respectively.
E.g., c1 = {?induce?}, c2 = {?enhance?},
and c = {?induce?, ?enhance?}.
COMPOSE(c1, c2) creates a new lambda-form
cluster c formed by composing the lambda
forms in c1, c2 into larger ones. E.g., c1 =
{?amino?}, c2 = {?acid?}, and c =
{?amino acid?}.
Each time, USP executes the highest-scored op-
erator and reparses affected sentences using the
new parameters. The output contains the optimal
lambda-form clusters and parameters, as well as
the MAP semantic parses of input sentences.
3 Unsupervised Ontology Induction with
Markov Logic
A major limitation of USP is that it either merges
two object clusters into one, or leaves them sepa-
rate. This is suboptimal, because different object
clusters may still possess substantial commonali-
ties. Modeling these can help extract more gen-
eral knowledge and answer many more questions.
The best way to capture such commonalities is
by forming an ISA hierarchy among the clusters.
For example, INDUCE and INHIBIT are both sub-
concepts of REGULATE. Learning these ISA rela-
tions helps answer questions like ?What regulates
CD11b??, when the text states that ?IL-4 induces
CD11b? or ?AP-1 suppresses CD11b?.
For parameter learning, this is also undesirable.
Without the hierarchical structure, each cluster es-
timates its parameters solely based on its own ob-
servations, which can be extremely sparse. The
better solution is to leverage the hierarchical struc-
ture for smoothing (a.k.a. shrinkage (McCallum et
al., 1998; Gelman and Hill, 2006)). For example,
if we learn that ?super-induce? is a verb and that in
general verbs have active and passive voices, then
even though ?super-induce? only shows up once
in the corpus as in ?AP-1 is super-induced by IL-
4?, by smoothing we can still infer that this proba-
bly means the same as ?IL-4 super-induces AP-1?,
which in turn helps answer questions like ?What
super-induces AP-1?.
OntoUSP overcomes the limitations of USP by
replacing the flat clustering process with a hier-
archical clustering one, and learns an ISA hier-
archy of lambda-form clusters in addition to the
IS-PART one. The output of OntoUSP consists
of an ontology, a semantic parser, and the MAP
parses. In effect, OntoUSP conducts ontology in-
duction, population, and knowledge extraction in a
single integrated process. Specifically, given clus-
ters c1, c2, in addition to merge vs. separate, On-
toUSP evaluates a third option called abstraction,
in which a new object cluster c is created, and ISA
links are added from ci to c; the argument clusters
in c are formed by merging that of ci?s.
In the remainder of the section, we describe the
299
details of OntoUSP. We start by presenting the
OntoUSP MLN. We then describe our inference
algorithm and how to parse a new sentence us-
ing OntoUSP. Finally, we describe the learning al-
gorithm and how OntoUSP induces the ontology
while learning the semantic parser.
3.1 The OntoUSP MLN
The OntoUSP MLN can be obtained by modifying
the USP MLN with three simple changes. First,
we introduce a new predicate IsA(c1, c2), which
is true if cluster c1 is a subconcept of c2. For con-
venience, we stipulate that IsA is reflexive (i.e.,
IsA(c, c) is true for any c). Second, we add two
formulas to the MLN:
IsA(c1, c2) ? IsA(c2, c3) ? IsA(c1, c3).
IsPart(c1, i1, p1) ? IsPart(c2, i2, p2)
? IsA(c1, c2) ? (i1 = i2 ? IsA(p1, p2)).
The first formula simply enforces the transitivity
of ISA relation. The second formula states that if
the ISA relation holds for a pair of object clusters,
it also holds between their corresponding property
clusters. Both are hard constraints. Third, we in-
troduce hierarchical smoothing into the model by
replacing the USP mixture formula
x ? +p ? HasValue(x,+v)
with a new formula
ISA(p1,+p2) ? x ? p1 ? HasValue(x,+v)
Intuitively, for each p2, the weight corresponds to
the delta in log-probability of v comparing to the
prediction according to all ancestors of p2. The
effect of this change is that now the value v of
a subexpression x is not solely determined by its
property cluster p1, but is also smoothed by statis-
tics of all p2 that are super clusters of p1.
Shrinkage takes place via interaction among the
weights of the ISA mixture formula. In particular,
if the weights for some property cluster p are all
zero, it means that values in p are completely pre-
dicted by p?s ancestors. In effect, p is backed off
to its parent.
3.2 Inference
Given the dependency tree T of a sentence, the
conditional probability of a semantic parse L is
given by Pr(L|T ) ? exp (?i wini(T,L)).
The MAP semantic parse is simply
Algorithm 1 OntoUSP-Parse(MLN, T )
Initialize semantic parse L with individual
atoms in the QLF of T
repeat
for all subexpressions e in L do
Evaluate all semantic parses that are
lambda-reducible from e
end for
L? the new semantic parse with the highest
gain in probability
until none of these improve the probability
return L
argmaxL
?
i wini(T, L). Directly enumer-
ating all L?s is intractable. OntoUSP uses the
same inference algorithm as USP by hill-climbing
on the probability of L; in each step, OntoUSP
evaluates the alternative semantic parses that
can be formed by lambda-reducing a current
subexpression with one of its arguments. The only
difference is that OntoUSP uses a different MLN
and so the probabilities and resulting semantic
parses may be different. Algorithm 1 gives
pseudo-code for OntoUSP?s inference algorithm.
3.3 Learning
OntoUSP uses the same learning objective as USP,
i.e., to find parameters ? that maximizes the log-
likelihood of observing the dependency trees T ,
summing out the unobserved semantic parses L:
L?(T ) = logP?(L)
= log?L P?(T, L)
However, the learning problem in OntoUSP is
distinct in two important aspects. First, OntoUSP
learns in addition an ISA hierarchy among the
lambda-form clusters. Second and more impor-
tantly, OntoUSP leverages this hierarchy during
learning to smooth the parameter estimation of in-
dividual clusters, as embodied by the new ISA
mixture formula in the OntoUSP MLN.
OntoUSP faces several new challenges unseen
in previous hierarchical-smoothing approaches.
The ISA hierarchy in OntoUSP is not known in
advance, but needs to be learned as well. Simi-
larly, OntoUSP has no known examples of pop-
ulated facts and rules in the ontology, but has to
infer that in the same joint learning process. Fi-
nally, OntoUSP does not start from well-formed
structured input like relational tuples, but rather
directly from raw text. In sum, OntoUSP tackles a
300
Algorithm 2 OntoUSP-Learn(MLN, T?s)
Initialize with a flat ontology, along with clus-
ters and semantic parses
Merge clusters with the same core form
Agenda ? ?
repeat
for all candidate operations O do
Score O by log-likelihood improvement
if score is above a threshold then
Add O to agenda
end if
end for
Execute the highest scoring operation O? in
the agenda
Regenerate MAP parses for affected trees and
update agenda and candidate operations
until agenda is empty
return the learned ontology and MLN, and the
semantic parses
very hard problem with exceedingly little aid from
user supervision.
To combat these challenges, OntoUSP adopts
a novel form of hierarchical smoothing by inte-
grating it with the search process for identify-
ing the hierarchy. Algorithm 2 gives pseudo-
code for OntoUSP?s learning algorithm. Like
USP, OntoUSP approximates the sum over all
semantic parses with the most probable parse,
and searches for both ? and the MAP semantic
parses L that maximize P?(T,L). In addition to
MERGE and COMPOSE, OntoUSP uses a new opera-
tor ABSTRACT(c1, c2), which does the following:
1. Create an abstract cluster c;
2. Create ISA links from c1, c2 to c;
3. Align property clusters of c1 and c2; for each
aligned pair p1 and p2, either merge them
into a single property cluster, or create an ab-
stract property cluster p in c and create ISA
links from pi to p, so as to maximize log-
likelihood.
Intuitively, c corresponds to a more abstract con-
cept that summarizes similar properties in ci?s.
To add a child cluster c2 to an existing ab-
stract cluster c1, OntoUSP also uses an operator
ADDCHILD(c1, c2) that does the following:
1. Create an ISA link from c2 to c1;
2. For each property cluster of c2, maximize the
log-likelihood by doing one of the following:
merge it with a property cluster in an exist-
ing child of c1; create ISA link from it to
an abstract property cluster in c; leave it un-
changed.
For efficiency, in both operators, the best option
is chosen greedily for each property cluster in c2,
in descending order of cluster size.
Notice that once an abstract cluster is created,
it could be merged with an existing cluster using
MERGE. Thus with the new operators, OntoUSP
is capable of inducing any ISA hierarchy among
abstract and existing clusters. (Of course, the ISA
hierarchy it actually induces depends on the data.)
Learning the shrinkage weights has been ap-
proached in a variety of ways; examples include
EM and cross-validation (McCallum et al, 1998),
hierarchical Bayesian methods (Gelman and Hill,
2006), and maximum entropy with L1 priors
(Dudik et al, 2007). The past methods either only
learn parameters with one or two levels (e.g., in
hierarchical Bayes), or requires significant amount
of computation (e.g., in EM and in L1-regularized
maxent), while also typically assuming a given
hierarchy. In contrast, OntoUSP has to both in-
duce the hierarchy and populate it, with potentially
many levels in the induced hierarchy, starting from
raw text with little user supervision.
Therefore, OntoUSP simplifies the weight
learning problem by adopting standard m-
estimation for smoothing. Namely, the weights
for cluster c are set by counting its observations
plus m fractional samples from its parent distribu-
tion. When c has few observations, its unreliable
statistics can be significantly augmented via the
smoothing by its parent (and in turn to a gradually
smaller degree by its ancestors). m is a hyperpa-
rameter that can be used to trade off bias towards
statistics for parent vs oneself.
OntoUSP also needs to balance between two
conflicting aspects during learning. On one hand,
it should encourage creating abstract clusters to
summarize intrinsic commonalities among the
children. On the other hand, this needs to be heav-
ily regularized to avoid mistaking noise for the sig-
nal. OntoUSP does this by a combination of priors
and thresholding. To encourage the induction of
higher-level nodes and inheritance, OntoUSP im-
poses an exponential prior ? on the number of pa-
rameter slots. Each slot corresponds to a distinct
property value. A child cluster inherits its parent?s
slots (and thus avoids the penalty on them). On-
301
toUSP also stipulates that, in an ABSTRACT opera-
tion, a new property cluster can be created either as
a concrete cluster with full parameterization, or as
an abstract cluster that merely serves for smooth-
ing purposes. To discourage overproposing clus-
ters and ISA links, OntoUSP imposes a large ex-
ponential prior ? on the number of concrete clus-
ters created by ABSTRACT. For abstract cluster, it
sets a cut-off tp and only allows storing a probabil-
ity value no less than tp. Like USP, it also rejects
MERGE and COMPOSE operations that improve log-
likelihood by less than to. These priors and cut-off
values can be tuned to control the granularity of
the induced ontology and clusters.
Concretely, given semantic parses L, OntoUSP
computes the optimal parameters and evaluates
the regularized log-likelihood as follows. Let
wp2,v denote the weight of the ISA mixture for-
mula ISA(p1,+p2)? x ? p1 ? HasValue(x,+v).
For convenience, for each pair of property clus-
ter c and value v, OntoUSP instead computes
and stores w?c,v =
?
ISA(c, a)wa,v, which sums
over all weights for c and its ancestors. (Thus
wc,v = w?c,v ? w?p,v, where p is the parent of
c.) Like USP, OntoUSP imposes local normal-
ization constraints that enable closed-form esti-
mation of the optimal parameters and likelihood.
Specifically, using m-estimation, the optimal w?c,v
is log((m ?ew?p,v +nc,v)/(m+nc)), where p is the
parent of c and n is the count. The log-likelihood
is ?c,v w?c,v ?nc,v, which is then augmented by the
priors.
4 Experiments
4.1 Methodology
Evaluating unsupervised ontology induction is dif-
ficult, because there is no gold ontology for com-
parison. Moreover, our ultimate goal is to aid
knowledge acquisition, rather than just inducing
an ontology for its own sake. Therefore, we
used the same methodology and dataset as the
USP paper to evaluate OntoUSP on its capabil-
ity in knowledge acquisition. Specifically, we ap-
plied OntoUSP to extract knowledge from the GE-
NIA dataset (Kim et al, 2003) and answer ques-
tions, and we evaluated it on the number of ex-
tracted answers and accuracy. GENIA contains
1999 PubMed abstracts.6 The question set con-
6http://www-tsujii.is.s.u-tokyo-
.ac.jp/GENIA/home/wiki.cgi.
tains 2000 questions which were created by sam-
pling verbs and entities according to their frequen-
cies in GENIA. Sample questions include ?What
regulates MIP-1alpha??, ?What does anti-STAT 1
inhibit??. These simple question types were used
to focus the evaluation on the knowledge extrac-
tion aspect, rather than engineering for handling
special question types and/or reasoning.
4.2 Systems
OntoUSP is the first unsupervised approach that
synergistically conducts ontology induction, pop-
ulation, and knowledge extraction. The system
closest in aim and capability is USP. We thus com-
pared OntoUSP with USP and all other systems
evaluated in the USP paper (Poon and Domingos,
2009). Below is a brief description of the systems.
(For more details, see Poon & Domingos (2009).)
Keyword is a baseline system based on keyword
matching. It directly matches the question sub-
string containing the verb and the available argu-
ment with the input text, ignoring case and mor-
phology. Given a match, two ways to derive the
answer were considered: KW simply returns the
rest of sentence on the other side of the verb,
whereas KW-SYN is informed by syntax and ex-
tracts the answer from the subject or object of the
verb, depending on the question (if the expected
argument is absent, the sentence is ignored).
TextRunner (Banko et al, 2007) is the state-of-
the-art system for open-domain information ex-
traction. It inputs text and outputs relational triples
in the form (R,A1, A2), where R is the relation
string, and A1, A2 the argument strings. To an-
swer questions, each triple-question pair is consid-
ered in turn by first matching their relation strings,
and then the available argument strings. If both
match, the remaining argument string in the triple
is returned as an answer. Results were reported
when exact match is used (TR-EXACT), or when
the triple strings may contain the question ones as
substrings (TR-SUB).
RESOLVER (Yates and Etzioni, 2009) inputs
TextRunner triples and collectively resolves coref-
erent relation and argument strings. To answer
questions, the only difference from TextRunner is
that a question string can match any string in its
cluster. As in TextRunner, results were reported
for both exact match (RS-EXACT) and substring
(RS-SUB).
DIRT (Lin and Pantel, 2001) resolves binary rela-
302
Table 1: Comparison of question answering re-
sults on the GENIA dataset. Results for systems
other than OntoUSP are from Poon & Domingos
(2009).
# Total # Correct Accuracy
KW 150 67 45%
KW-SYN 87 67 77%
TR-EXACT 29 23 79%
TR-SUB 152 81 53%
RS-EXACT 53 24 45%
RS-SUB 196 81 41%
DIRT 159 94 59%
USP 334 295 88%
OntoUSP 480 435 91%
tions by inputting a dependency path that signifies
the relation and returns a set of similar paths. To
use DIRT in question answering, it was queried to
obtain similar paths for the relation of the ques-
tion, which were then used to match sentences.
USP (Poon and Domingos, 2009) parses the in-
put text using the Stanford dependency parser
(Klein and Manning, 2003; de Marneffe et al,
2006), learns an MLN for semantic parsing from
the dependency trees, and outputs this MLN and
the MAP semantic parses of the input sentences.
These MAP parses formed the knowledge base
(KB). To answer questions, USP first parses the
questions (with the question slot replaced by a
dummy word), and then matches the question
parse to parses in the KB by testing subsumption.
OntoUSP uses a similar procedure as USP for ex-
tracting knowledge and answering questions, ex-
cept for two changes. First, USP?s learning and
parsing algorithms are replaced with OntoUSP-
Learn and OntoUSP-Parse, respectively. Second,
when OntoUSP matches a question to its KB, it
not only considers the lambda-form cluster of the
question relation, but also all its sub-clusters.7
4.3 Results
Table 1 shows the results comparing OntoUSP
with other systems. While USP already greatly
outperformed other systems in both precision and
recall, OntoUSP further substantially improved on
the recall of USP, without any loss in precision.
In particular, OntoUSP extracted 140 more correct
answers than USP, for a gain of 47% in absolute
7Additional details are available at
http : //alchemy.cs.washington.edu/papers/poon10.
ISA ISA
INHIBIT
induce, enhance, trigger, augment, up-regulate
INDUCE
inhibit, block, suppress, prevent, abolish, abrogate, down-regulate
activate
regulate, control, govern, modulate
ISA
ACTIVATE
REGULATE
Figure 3: A fragment of the induced ISA hierar-
chy, showing the core forms for each cluster (the
cluster labels are added by the authors for illustra-
tion purpose).
recall. Compared to TextRunner (TR-SUB), On-
toUSP gained on precision by 38 points and ex-
tracted more than five times of correct answers.
Manual inspection shows that the induced ISA
hierarchy is the key for the recall gain. Like
USP, OntoUSP discovered the following clusters
(in core forms) that represent some of the core
concepts in biomedical research:
{regulate, control, govern, modulate}
{induce, enhance, trigger, augment, up-
regulate}
{inhibit, block, suppress, prevent, abolish, ab-
rogate, down-regulate}
However, USP formed these as separate clusters,
whereas OntoUSP in addition induces ISA rela-
tions from the INDUCE and INHIBIT clusters to
the REGULATE cluster (Figure 3). This allows
OntoUSP to answer many more questions that
are asked about general regulation events, even
though the text states them with specific regula-
tion directions like ?induce? or ?inhibit?. Below
is an example question-answer pair output by On-
toUSP; neither USP nor any other system were
able to extract the necessary knowledge.
Q: What does IL-2 control?
A: The DEX-mediated IkappaBalpha induc-
tion.
Sentence: Interestingly, the DEX-mediated
IkappaBalpha induction was completely inhibited
by IL-2, but not IL-4, in Th1 cells, while the re-
verse profile was seen in Th2 cells.
OntoUSP also discovered other interesting
commonalities among the clusters. For exam-
ple, both USP and OntoUSP formed a singleton
cluster with core form ?activate?. Although this
cluster may appear similar to the INDUCE clus-
ter, the data in GENIA does not support merg-
ing the two. However, OntoUSP discovered that
303
the ACTIVATE cluster, while not completely resol-
vent with INDUCE, shared very similar distribu-
tions in their agent arguments. In fact, they are
so similar that OntoUSP merges them into a sin-
gle property cluster. It found that the patient ar-
guments of INDUCE and INHIBIT are very similar
and merged them. In turn, OntoUSP formed ISA
links from these three object clusters to REGULATE,
as well as among their property clusters. In-
tuitively, this makes sense. The positive- and
negative-regulation events, as signified by INDUCE
and INHIBIT, often target similar object entities
or processes. However, their agents tend to differ
since in one case they are inducers, and in the other
they are inhibitors. On the other hand, ACTIVATE
and INDUCE share similar agents since they both
signify positive regulation. However, ?activate?
tends to be used more often when the patient ar-
gument is a concrete entity (e.g., cells, genes, pro-
teins), whereas ?induce? and others are also used
with processes and events (e.g., expressions, inhi-
bition, pathways).
USP was able to resolve common syntactic dif-
ferences such as active vs. passive voice. How-
ever, it does so on the basis of individual verbs,
and there is no generalization beyond their clus-
ters. OntoUSP, on the other hand, formed a high-
level cluster with two abstract property clusters,
corresponding to general agent argument and pa-
tient argument. The active-passive alternation is
captured in these clusters, and is inherited by all
descendant clusters, including many rare verbs
like ?super-induce? which only occur once in GE-
NIA and for which there is no way that USP
could have learned about their active-passive al-
ternations. This illustrates the importance of dis-
covering ISA relations and performing hierarchi-
cal smoothing.
4.4 Discussion
OntoUSP is a first step towards joint ontology in-
duction and knowledge extraction. The experi-
mental results demonstrate the promise in this di-
rection. However, we also notice some limitations
in the current system. While OntoUSP induced
meaningful ISA relations among relation clusters
like REGULATE, INDUCE, etc., it was less success-
ful in inducing ISA relations among entity clus-
ters such as specific genes and proteins. This is
probably due to the fact that our model only con-
siders local features such as the parent and argu-
ments. A relation is often manifested as verbs and
has several arguments, whereas an entity typically
appears as an argument of others and has few ar-
guments of its own. As a result, in average, there
is less information available for entities than rela-
tions. Presumably, we can address this limitation
by modeling longer-ranged dependencies such as
grandparents, siblings, etc. This is straightforward
to do in Markov logic.
OntoUSP also uses a rather elaborate scheme
for regularization. We hypothesize that this can
be much simplified and improved by adopting a
principled framework such as Dudik et al (2007).
5 Conclusion
This paper introduced OntoUSP, the first unsuper-
vised end-to-end system for ontology induction
and knowledge extraction from text. OntoUSP
builds on the USP semantic parser by adding the
capability to form hierarchical clusterings of logi-
cal expressions, linked by ISA relations, and us-
ing them for hierarchical smoothing. OntoUSP
greatly outperformed USP and other state-of-the-
art systems in a biomedical knowledge acquisition
task.
Directions for future work include: exploiting
the ontological structure for principled handling of
antonyms and (more generally) expressions with
opposite meanings; developing and testing alter-
nate methods for hierarchical modeling in On-
toUSP; scaling up learning and inference to larger
corpora; investigating the theoretical properties of
OntoUSP?s learning approach and generalizing it
to other tasks; answering questions that require in-
ference over multiple extractions; etc.
6 Acknowledgements
We give warm thanks to the anonymous reviewers for
their comments. This research was partly funded by ARO
grant W911NF-08-1-0242, AFRL contract FA8750-09-C-
0181, DARPA contracts FA8750-05-2-0283, FA8750-07-D-
0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-
D030010, NSF grants IIS-0534881 and IIS-0803481, and
ONR grant N00014-08-1-0670. The views and conclusions
contained in this document are those of the authors and
should not be interpreted as necessarily representing the offi-
cial policies, either expressed or implied, of ARO, DARPA,
NSF, ONR, or the United States Government.
References
Hiyan Alshawi. 1990. Resolving quasi logical forms. Com-
putational Linguistics, 16:133?144.
G. Bakir, T. Hofmann, B. B. Scho?lkopf, A. Smola, B. Taskar,
304
S. Vishwanathan, and (eds.). 2007. Predicting Structured
Data. MIT Press, Cambridge, MA.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open informa-
tion extraction from the web. In Proceedings of the Twen-
tieth International Joint Conference on Artificial Intelli-
gence, pages 2670?2676, Hyderabad, India. AAAI Press.
Philipp Cimiano. 2006. Ontology learning and population
from text. Springer.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
Fifth International Conference on Language Resources
and Evaluation, pages 449?454, Genoa, Italy. ELRA.
Pedro Domingos and Daniel Lowd. 2009. Markov Logic:
An Interface Layer for Artificial Intelligence. Morgan &
Claypool, San Rafael, CA.
Miroslav Dudik, David Blei, and Robert Schapire. 2007. Hi-
erarchical maximum entropy density estimation. In Pro-
ceedings of the Twenty Fourth International Conference
on Machine Learning.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Andrew Gelman and Jennifer Hill. 2006. Data Analysis Us-
ing Regression and Multilevel/Hierarchical Models. Cam-
bridge University Press.
Lise Getoor and Ben Taskar, editors. 2007. Introduction to
Statistical Relational Learning. MIT Press, Cambridge,
MA.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi Tsu-
jii. 2003. GENIA corpus - a semantically annotated cor-
pus for bio-textmining. Bioinformatics, 19:180?82.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Forty First
Annual Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of
inference rules from text. In Proceedings of the Seventh
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 323?328, San Fran-
cisco, CA. ACM Press.
Alexander Maedche. 2002. Ontology learning for the se-
mantic Web. Kluwer Academic Publishers, Boston, Mas-
sachusetts.
Andrew McCallum, Ronald Rosenfeld, Tom Mitchell, and
Andrew Ng. 1998. Improving text classification by
shrinkage in a hierarchy of classes. In Proceedings of the
Fifteenth International Conference on Machine Learning.
Hoifung Poon and Pedro Domingos. 2008. Joint unsuper-
vised coreference resolution with Markov logic. In Pro-
ceedings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 649?658, Honolulu,
HI. ACL.
Hoifung Poon and Pedro Domingos. 2009. Unsupervised
semantic parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing,
pages 1?10, Singapore. ACL.
Rion Snow, Daniel Jurafsky, and Andrew Ng. 2006. Seman-
tic taxonomy induction from heterogenous evidence. In
Proceedings of COLING/ACL 2006.
S. Staab and R. Studer. 2004. Handbook on ontologies.
Springer.
Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2008. Yago - a large ontology from Wikipedia and Word-
Net. Journal of Web Semantics.
Fabian Suchanek, Mauro Sozio, and Gerhard Weikum. 2009.
Sofie: A self-organizing framework for information ex-
traction. In Proceedings of the Eighteenth International
Conference on World Wide Web.
Jun-ichi Tsujii. 2004. Thesaurus or logical ontology, which
do we need for mining text? In Proceedings of the Lan-
guage Resources and Evaluation Conference.
Fei Wu and Daniel S. Weld. 2008. Automatically refining the
wikipedia infobox ontology. In Proceedings of the Seven-
teenth International Conference on World Wide Web, Bei-
jing, China.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
305
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	Implementing Weighted Abduction in Markov Logic
James Blythe
USC ISI
blythe@isi.edu
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Pedro Domingos
University of Washington
pedrod@cs.washington.edu
Rohit J. Kate
University of Wisconsin-Milwaukee
katerj@uwm.edu
Raymond J. Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
Abduction is a method for finding the best explanation for observations. Arguably
the most advanced approach to abduction, especially for natural language processing, is
weighted abduction, which uses logical formulas with costs to guide inference. But it
has no clear probabilistic semantics. In this paper we propose an approach that imple-
ments weighted abduction in Markov logic, which uses weighted first-order formulas to
represent probabilistic knowledge, pointing toward a sound probabilistic semantics for
weighted abduction. Application to a series of challenge problems shows the power and
coverage of our approach.
1 Introduction
Abduction is inference to the best explanation.1 Typically, one uses it to find the best hypothesis ex-
plaining a set of observations, e.g., in diagnosis and plan recognition. In natural language processing the
content of an utterance can be viewed as a set of observations, and the best explanation then constitutes
the interpretation of the utterance. Hobbs et al [7] described a variety of abduction called ?weighted
abduction? for interpreting natural language discourse. The key idea was that the best interpretation of
a text is the best explanation or proof of the logical form of the text, allowing for assumptions. What
counted as ?best? was defined in terms of a cost function which favored proofs with the fewest number of
assumptions and the most salient and plausible axioms, and in which the pervasive redundancy implicit
in natural language discourse was exploited. It was argued in that paper that such interpretation problems
as coreference and syntactic ambiguity resolution, determining the specific meanings of vague predicates
and lexical ambiguity resolution, metonymy resolution, metaphor interpretation, and the recognition of
discourse structure could be seen to ?fall out? of the best abductive proof.
Specifically, weighted abduction has the following features:
1. In a goal expression consisting of an existentially quantified conjunction of positive literals, each
literal is given a cost that represents the utility of proving that literal as opposed to assuming it.
That is, a low cost on a literal will make it more likely for it to be assumed, whereas a high cost
will result in a greater effort to find a proof.
1We are indebted to Jesse Davis, Parag Singla and Marc Sumner for discussions about this work. This research was
supported in part by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force
Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172, in part by the Office of Naval Research under contract
no. N00014-09-1-1029, and in part by the Army Research Office under grant W911NF-08-1-0242. Any opinions, findings, and
conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of
the DARPA, AFRL, ONR, ARO, or the US government.
55
2. Costs are passed back across the implication in Horn clauses according to weights on the conjuncts
in the antecedents. Specifically, if a consequent costs $c and the weight on a conjunct in the
antecedent is v, then the cost on that conjunct will be $vc. Note that if the weights add up to less
than one, backchaining on the rule will be favored, as the cost of the antecedent will be less than
the cost of the consequent. If the weights add up to more than one, backchaining will be disfavored
unless a proof can be found for one or more of the conjuncts in the antecedent, thereby providing
partial evidence for the consequent.
3. Two literals can be factored or unified, where the result is given the minimum cost of the two,
providing no contradiction would result. This is a frequent mechanism for coreference resolution.
In practice, only a shallow or heuristic check for contradiction is done.
4. The lowest-cost proof is the best interpretation, or the best abductive proof of the goal expression.
However, there are two significant problems with weighted abduction as it was originally presented.
First, it required a large knowledge base of commonsense knowledge. This was not available when
weighted abduction was first described, but since that time there have been substantial efforts to build up
knowledge bases for various purposes, and at least two of these have been used with promising results
in an abductive setting?Extended WordNet [6] for question-answering and FrameNet [11] for textual
inference.
The second problem with weighted abduction was that the weights and costs did not have a prob-
abilistic semantics. This, for example, hampers automatic learning of weights from data or existing
resources. That is the issue we address in the present paper.
In the last decade and a half, a number of formalisms for adding uncertain reasoning to predicate logic
have been developed that are well-founded in probability theory. Among the most widely investigated
is Markov logic [14, 4]. In this paper we show how weighted abduction can be implemented in Markov
logic. This demonstrates that Markov logic networks can be used as a powerful mechanism for interpret-
ing natural language discourse, and at the same time provides weighted abduction with something like a
probabilistic semantics.
In Section 2 we briefly describe Markov logic and Markov logic networks. Section 3 then describes
how weighted abduction can be implemented in Markov logic. In Section 4 we describe experiments in
which fourteen published examples of the use of weighted abduction in natural language understanding
are implemented in Markov logic networks, with good results. Section 5 on current and future directions
briefly describes an ongoing experiment in which we are attempting to scale up to apply this procedure
to the textual inference problem with a knowledge base derived from FrameNet with tens of thousands
of axioms.
2 Markov Logic Networks and Related Work
Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order
logic and probabilistic graphical models. A traditional first-order knowledge base can be seen as a set of
hard constraints on the set of possible worlds: if a world violates even one formula, its probability is zero.
In order to soften these constraints, Markov logic attaches a weight to each first-order logic formula in
the knowledge base. Such a set of weighted first-order logic formulae is called a Markov logic network
(MLN). A formula?s weight reflects how strong a constraint it imposes on the set of possible worlds: the
higher the weight, the lower the probability of a world that violates it; however, that probability need not
be zero. An MLN with all infinite weights reduces to a traditional first-order knowledge base with only
hard constraints.
56
Formally, an MLN L is a set of formula?weight pairs (Fi, wi). Given a set of constants, it defines
a joint probability distribution over a set of boolean variables X = (X1, X2...) corresponding to the
possible groundings (using the given constants) of the literals present in the first-order formulae:
P (X = x) = 1Z exp(
?
iwini(x))
where ni(x) is the number of true groundings of Fi in x and Z is a normalization term obtained by
summing P (X = x) over all values of X .
Semantically, an MLN can be viewed as a set of templates for constructing Markov networks [12],
the undirected counterparts of Bayesian networks. An MLN and a set of constants produce a Markov
network in which each ground literal is a node and every pair of ground literals that appear together in
some grounding of some formula are connected by an edge. Different sets of constants produce different
Markov networks; however, there are certain regularities in their structure and parameters. For example,
all groundings of the same formula have the same weight.
Probabilistic inference for an MLN (such as finding the most probable truth assignment for a given
set of ground literals, or finding the probability that a particular formula holds) can be performed by
first producing the ground Markov network and then using well known inference techniques for Markov
networks, like Gibbs sampling. Given a knowledge base as a set of first-order logic formulae, and a
database of training examples each consisting of a set of true ground literals, it is also possible to learn
appropriate weights for the MLN formulae which maximize the probability of the training data. An open-
source software package for MLNs, called Alchemy 2, is also available with many built-in algorithms
for performing inference and learning.
Much of the early work on abduction was done in a purely logical framework (e.g., [13, 3, 9, 10].
Typically the choice between alternative explanations is made on the basis of parsimony; the shortest
proofs with the fewest assumptions are favored. However, a significant limitation of these purely logical
approaches is that they are unable to reason under uncertainty or estimate the likelihood of alternative
explanations. A probabilistic form of abduction is needed in order to account for uncertainty in the
background knowledge and to handle noisy and incomplete observations.
In Bayesian networks [12] background knowledge with its uncertainties is encoded in a directed
graph. Then, given a set of observations, probabilistic inference over the graph structure is done to
compute the posterior probability of alternative explanations. However, Bayesian networks are based on
propositional logic and cannot handle structured representations, hence preventing their use in situations,
characteristic of natural language processing, that involve an unbounded number of entities with a variety
of relations between them.
In recent years there have been a number of proposals attempting to combine the probabilistic nature
of Bayesian networks with structured first-order representations. It is impossible here to review this liter-
ature here. A a good review of much of it can be found in [5], and in [14] there are detailed comparisonss
of various models to MLNs.
Charniak and Shimony [2] define a variant of weighted abduction, called ?cost-based abduction? in
which weights are attached to terms rather than to rules or to antecedents in rules. Thus, the term Pi
has the same cost whatever rule it is used in. The cost of an assignment to the variables in the domain
is the sum of the costs of the variables that are true in the assignment. Charniak and Shimony provide
a probabilistic semantics for their approach by showing how to construct a Bayesian network from a
domain such that a most probable explanation solution to the Bayes net corresponds to a lowest-cost
solution to the abduction problem. However, in natural language applications the utility of proving a
proposition can vary by context; weighted abduction accomodates this, whereas cost-based abduction
does not.2http://alchemy.cs.washington.edu
57
3 Weighted Abduction and MLNs
Kate and Mooney [8] show how logical abduction can be implemented in Markov logic networks. They
use forward inference in MLNs to perform abduction by adding clauses with reverse implications. Uni-
versally quantified variables from the left hand side of rules are converted to existentially quantified
variables in the reversed clause. For example, suppose we have the following rule saying that mosquito
bites transmit malaria:
mosquito(x) ? infected(x,Malaria) ? bite(x, y) ? infected(y,Malaria)
This would be translated into the soft rule
[w] infected(y,Malaria) ? ?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
Where there is more than one possible explanation, they include a closure axiom saying that one of the
explanations must hold. Since blood transfusions also cause malaria, they have the hard rule
infected(y,Malaria) ?
?x[mosquito(x) ? infected(x,Malaria) ? bite(x, y)]
??x[infected(x,Malaria) ? transfuse(Blood, x, y)].
Kate and Mooney also add a soft mutual exclusivity clause that states that no more than one of the
possible explanations is true.
In translating between weighted abduction and Markov logic, we need similarly to specify the axioms
in Markov logic that correspond to a Horn clause axiom in weighted abduction. In addition, we need to
describe the relation between the numbers in weighted abduction and the weights on the Markov logic
axioms. Hobbs et al [7] give only broad, informal guidelines about how the numbers correspond to
probabilities. In this development, we elaborate on how the numbers can be defined more precisely
within these guidelines in a way that links with the weights in Markov logic, thereby pointing to a
probabilistic semantics for the weighted abduction numbers.
There are two sorts of numbers in weighted abduction?the weights on conjuncts in the antecedents
of Horn clause axioms, and the costs on conjuncts in goal expressions, which are existentially quantified
conjunctions of positive literals. We deal first with the weights, then with the costs.
The space of events over which probabilities are taken is the set of proof graphs constituting the best
interpretations of a set of texts in a corpus. Thus, by the probability of p(x) given q(x), we mean the
probability that p(x) will occur in a proof graph in which q(x) occurs.
The translation from weighted abduction axioms to Markov logic axioms can be broken into two
steps. First we consider the ?or? node case, determining the relative costs of axioms that have the same
consequent. Then we look at the ?and? node case, determining how the weights should be distributed
across the conjuncts in the antecedent of a Horn clause, given the total weight for the antecedent.
Weights on Antecedents in Axioms. First consider a set of Horn clause axioms all with the same
consequent, where we collapse the antecedent into a single literal, and for simplicity allow x to stand for
all the universally quantified variables in the antecedent, and assume the consequent to have only those
variables. That is, we convert all axioms of the form
p1(x) ? . . . ? q(x)
into axioms of the form
Ai(x) ? q(x), where p1(x) ? . . . ? Ai(x)
To convert this into Markov logic, we first introduce the hard constraint
Ai(x) ? q(x).
In addition, given a goal of proving q(x), in weighted abduction we will want to backchain on at least
(and usually at most) one of these axioms or we will want simply to assume q(x). Thus, we can introduce
another hard constraint with the disjunction of these antecedents as well as a literal AssumeQ(x) that
means q(x) is assumed rather than proved.
58
q(x) ? A1(x) ? A2(x) ? . . . ? An(x) ? AssumeQ(x).
Then we need to introduce soft constraints to indicate that each of these disjuncts is a possible explana-
tion, or proof, of q(x), with an associated probability, or weight.
[wi] q(x) ? Ai(x), . . .
[w0] q(x) ? AssumeQ(x)
The probability that AssumeQ(x) is true is the conditional probability P0 that none of the antecedents
is true given that q(x) is true.
P0 = P (?[A1(x) ? A2(x) ? . . . ? An(x)] | q(x))
In weighted abduction, when the antecedent weight is greater than one, we prefer assuming the conse-
quent to assuming the antecedent. When the antecedent weight is less than one we prefer to assume the
antecedent. If the probability that an antecedent Ai(x) is the explanation of q(x) is greater than P0, it
should be given a weighted abduction weight vi less than 1, making it more likely to be chosen.3 Cor-
respondingly, if it is less than P0, it should be given a weight vi greater than 1, making it less likely
to be chosen. In general, the weighted abduction weights should be in reverse order of the conditional
probabilities Pi that Ai(x) is the explanation of q(x).
Pi = P (Ai(x) | q(x))
If we assign the weights vi in weighted abduction to be
vi = logPilogP0
then this is consistent with informal guidelines in [7] on the meaning of these weights. We use the logs
of the probabilities rather than the probabilities themselves to moderate the effect of one axiom being
very much more probable than any of the others.
Kate and Mooney [8], in their translation of logical abduction into Markov logic, also include soft
constraints stipulating that the different possible explanations Ai(x) are normally mutually exclusive.
We do not do that here, but we get a kind of soft mutual exclusivity constraint by virtue of the axioms
below that levy a cost for any literal that is taken to be true. In general, more parimonious explanations
will be favored.
Nevertheless, in most cases a single explanation will suffice. When this is true, the probability of
Ai(x) holding when q(x) holds is e
wi
Z . Then a reasonable approximation for the relation between the
weighted abduction weights vi and the Markov logic weights wi is
wi = ?vilogP0
Weights on Conjuncts in Antecedents. Next consider how cost is spread across the conjuncts in the
antecedent of a Horn clause in weighted abduction. Here we use u?s to represent the weighted abduction
weights on the conjuncts.
p1(x)u1 ? p2(x)u2 ? ... ? A(x)
The u?s should somehow represent the semantic contribution of each conjunct to the conclusion. That is,
given that the conjunct is true, what is the probability that it is part of an explanation of the consequent?
Conjuncts with a higher such probability should be given higher weights u; they play a more significant
role in explaining A(x).
Let Pi be the conditional probability of the consequent given the ith conjunct in the antecedent.
Pi = P (A(x)|pi(x))
and let Z be a normalization factor.
Z = ?ni=1 Pi
3We use vi for these weighted abduction weights and wi for Markov logic weights.
59
Let v be the weight of the entire antecedent as determined above.
Then it is consistent with the guidelines in [7] to define the weights on the conjuncts as follows:
ui = vPiZ
The weights ui will sum to v and each will correspond to the semantic contribution of its conjunct to the
consequent.
In Markov logic, weights apply only to axioms as a whole, not parts of axioms. Thus, the single
axiom above must be decomposed into one axiom for each conjunct and the dependencies must be
written as
[wi] pi(x) ? A(x), . . .
The relation between the weighted abduction weights ui and the Markov logic weights wi can be
approximated by
ui = ve
?wi
Z
Costs on Goals. The other numbers in weighted abduction are the costs associated with the conjuncts
in the goal expression. In weighted abduction these costs function as utilities. Some parts of the goal
expression are more important to interpret correctly than others; we should try harder to prove these
parts, rather than simply assuming them. In language it is important to recognize the referential anchor
of an utterance in shared knowledge. Thus, those parts of a sentence most likely to provide this anchor
have the highest utility. If we simply assume them, we lose their connection with what is already known.
Those parts of a sentence most likely to be new information will have a lower cost, because we usually
would not be able to prove them in any case.
Consider the two sentences
The smart man is tall.
The tall man is smart.
The logical form for each of them will be
(?x)[smart(x) ? tall(x) ?man(x)]
In weighted abduction, an interpretation of the sentence is a proof of the logical form, allowing assump-
tions. In the first sentence we want to prove smart(x) to anchor the sentence referentially. Then tall(x)
is new information; it will have to be assumed. We will want to have a high cost on smart(x) to force
the proof procedure to find this referential anchor. The cost on tall(x) will be low, to allow it to be
assumed without expending too much effort in trying to locate that fact in shared knowledge.
In the second sentence, the case is the reverse.
Let?s focus on the first sentence and assume we know that educated people are smart and big people
are tall, and furthermore that John is educated and Bill is big.
educated(x)1.2 ? smart(x)
big(x)1.2 ? tall(x)
educated(J), big(B)
In weighted abduction, the best interpretation will be that the smart man is John, because he is educated,
and we pay the cost for assuming he is tall. The interpretation we want to avoid is one that says x is Bill;
he is tall because he is big, and we pay the cost of assuming he is smart. Weighted abduction with its
differential costs on conjuncts in the goal expression favors the first and disfavors the second.
In weighted abduction, only assumptions cost; literals that are proved cost nothing. When the above
axioms are translated into Markov logic, it would be natural to capture the differential costs by attaching a
negative weight to smart(x) to model the cost associated with assuming it. However, this weight would
apply to any assignment in which smart(J) is true, regardless of whether it was assumed, derived from
60
an assumed fact, or derived from a known fact. A potential solution might be to attach the negative weight
to AssumeSmart(x). But the first axiom above allows us to bypass the negative weight on smart(x).
We can hypothesize that x is Bill, pay a low cost on AssumeEducated(B), derive smart(B), and get
the wrong assignment. Thus it is not enough to attach a negative weight to high-cost conjuncts in the
goal expression. This negative weight would have to be passed back through the whole knowledge base,
making the complexity of setting the weights at problem time in the MLN knowledge base equal to the
complexity of running the inference problem.
An alternative solution, which avoids this problem when the forward inferences are exact, is to use
a set of predicates that express knowing a fact without any assumptions. In the current example, we
would add Ksmart(x) for knowing that an entity is smart. The facts asserted in the data base are now
Keducated(J) and Kbig(B). For each hard axiom involving non-K predicates, we have a correspond-
ing axiom that expresses the relation between the K-predicates, and we have a soft axiom allowing us to
cross the border between the K predicates and their non-K counterparts.
Keducated(x) ? Ksmart(x)., . . .
[w] Ksmart(x) ? smart(x), . . .
Here the positive weight w attached is chosen to counteract the negative weight we would attach to
smart(x) to reflect the high cost of assuming it.
This removes the weight associated with assuming smart(x) regardless of the inference path that
leads to knowing smart(x) (KSmart(x))). Further, this translation takes linear time in the size of
the goal expression to compute, since we do not need to know the equivalent weighted abduction cost
assigned to the possible antecedents of smart(x).
If the initial facts do not include KEducated(B) and instead educated(B) must be assumed, then
the negative weight associated with smart(B) is still present. In this solution, there is no danger that
the inference process can by-pass the cost of assuming smart(B), since it is attached to the required
predicate and can only be removed by inferring KSmart(B).
Finally, there is a tendency in Markov logic networks for assignments of high probability for proposi-
tions for which there is no evidence one way or the other. To suppress this, we associate a small negative
weight with every predicate. In practice, it has turned out that a weight of ?1 effectively suppresses this
behavior.
4 Experimental Results
We have tested our approach on a set of fourteen challenge problems from [7] and subsequent papers,
designed to exercise the principal features of weighted abduction and show its utility for solving natural
language interpretation problems. The knowledge bases used for each of these problems are sparse,
consisting of only the axioms required for solving the problems plus a few distractors.
An example of a relatively simple problem is #5 in the table below, resolving ?he? in the text
I saw my doctor last week. He told me to get more exercise.
where we are given a knowledge base that says a doctor is a person and a male person is a ?he?. Solving
the problem requires assuming the doctor is male.
(?x)[doctor(x)1.2 ? person(x)]
(?x)[male(x).6 ? person(x).6 ? he(x)]
The logical form fragment to prove is (?x)he(x), where we know doctor(D).
A problem of intermediate difficulty (#7) is resolving the three lexical ambiguities in the sentence
The plane taxied to the terminal.
61
where we are given a knowledge base saying that airplanes and wood smoothers are planes, planes
moving on the ground and people taking taxis are both described as ?taxiing?, and computer terminals
and airport terminals are both terminals.
An example of a difficult problem is #12, finding the coherence relation, thereby resolving the pro-
noun ?they?, between the sentences
The police prohibited the women from demonstrating. They feared violence.
The axioms specify relations between fearing, not wanting, and prohibiting, as well as the defeasible
transitivity of causality and the fact that a causal relation between sentences makes the discourse coher-
ent.
The weights in the axioms were mostly distributed evenly across the conjuncts in the antecedents and
summed to 1.2.
For each of these problems, we compare the performance of the method described here with a man-
ually constructed gold standard and also with a method based on Kate and Mooney?s (KM) approach.
In this method, weights were assigned to the reversed clauses based on the negative log of the sum of
weights in the original clause. This approach does not capture different weights for different antecedents
of the same rule, and so has less fidelity to weighted abduction than our approach. In each case, we used
Alchemy?s probabilistic inference to determine the most probable explanation (MPE) [12].
In some of the problems the system should make more than one assumption, so there are 22 assump-
tions in total over all 14 problems in the gold standard. Using our method, 18 of the assumptions were
found, while 15 were found using the KM method. Table 1 shows the number of correct assumptions
found and the running time for the two approaches for each problem. Our method in particular provides
good coverage, with a recall of over 80% of the assumptions made in the gold standard. It has a shorter
running time overall, approximately 5.3 seconds versus 8.7 seconds for the reversal method. This is
largely due to one problem in the test set, problem #9, where the running time for the KM method is
relatively high because the technique finds a less sparse network, leading to larger cliques. There were
two problems in the test set that neither approach could solve. One of these contains predicates that have
a large number of arguments, leading to large clique sizes.
5 Current and Future Directions
In other work [11] we are experimenting with using weighted abduction with a knowledge base with tens
of thousands of axioms derived from FrameNet for solving problems in recognizing textual entailment
(RTE2) from the Pascal dataset [1]. For a direct comparison between standard weighted abduction and
the Markov logic approach described here, we are also experimenting with using the latter on the same
task with the same knowledge base.
For each text-hypothesis pair, the sentences are parsed and a logical form is produced. The output for
the first sentence forms the specific knowledge the system has while the output for the second sentence
is used as the target to be explained. If the cost of the best explanation is below a threshold we take the
target sentence to be true given the initial information.
It is a major challenge to scale our approach to handle all the problems from the RTE2 development
and test sets. We are not yet able to address the most complex of these using inference in Markov logic
networks. However, we have devised a number of pre-processing steps to reduce the complexity of the
resultant network, which significantly increase the number of problems that are tractable.
The FrameNet knowledge base contains a large number of axioms with general coverage. For any
individual entailment problem, most of them are irrelevant and can be removed after a simple graphical
analysis. We are able to remove more irrelevant axioms and predicates with an iterative approach that in
62
Our Method KM Method Gold
Problem score seconds score seconds standard
1 3 300 3 16 3
2 1 250 1 265 1
3 1 234 1 266 1
4 2 234 2 203 2
5 1 218 1 218 1
6 1 218 0 265 1
7 3 300 3 218 3
8 1 200 1 250 1
9 2 421 0 5000 2
10 1 2500 1 1500 3
11 0 0 1
12 0 0 1
13 1 250 1 250 1
14 1 219 1 219 1
Total 18 5344 15 8670 22
Table 1: Performance on each problem in our test set, comparing two encodings of weighted abduction
into Markov logic networks and a gold standard.
each iteration both drops axioms that are shown to be irrelevant and simplifies remaining axioms in such
a way as not to change the probability of entailment.
We also simplify predications by removing unnecessary arguments. The most natural way to convert
FrameNet frames to axioms is to treat a frame as a predicate whose arguments are the frame elements for
all of its roles. After converting to Markov logic, this results in rules having large numbers of existentially
quantified variables in the consequent. This can lead to a combinatorial explosion in the number of
possible ground rules. Many of the variables in the frame predicate are for general use and can be pruned
in the particular entailment. Our approach essentially creates abstractions of the original predicates that
preserve all the information that is relevant to the current problem but greatly reduces the number of
ground instances to consider.
Before implementing these pre-processing steps, only two or three problems could be run to com-
pletion on a Macbook Pro with 8 gigabytes of RAM. After making them, 28 of the initial 100 problems
could be run to completion.
Work on this effort continues.
6 Summary
Weighted abduction is a logical reasoning framework that has been successfully applied to solve a num-
ber of interesting and important problems in computational natural-language semantics ranging from
word sense disambiguation to coreference resolution. However, its method for representing and combin-
ing assumption costs to determine the most preferred explanation is ad hoc and without a firm theoretical
foundation. Markov Logic is a recently developed formalism for combining first-order logic with prob-
abilistic graphical models that has a well-defined formal semantics in terms of specifying a probability
distribution over possible worlds. This paper has presented a method for mapping weighted abduction
63
to Markov logic, thereby providing a sound probabilistic semantics for the approach and also allowing
it to exploit the growing toolbox of inference and learning algorithms available for Markov logic. Com-
plementarily, it has also demonstrated how Markov logic can thereby be applied to help solve important
problems in computational semantics.
References
[1] Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpek-
tor. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual Entailment, Venice, Italy, 2006.
[2] Eugene Charniak and Solomon E. Shimony. Cost-based abduction and map explanation. Artificial Artificial
Intelligence Journal, 66(2):345?374, 1994.
[3] P. T. Cox and T. Pietrzykowski. Causes for events: Their computation and applications. In J. Siekmann,
editor, 8th International Conference on Automated Deduction (CADE-8), Berlin, 1986. Springer-Verlag.
[4] P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Morgan & Claypool,
San Rafael, CA, 2009.
[5] L. Getoor and B. Taskar, editors. Introduction to Statistical Relational Learning. MIT Press, Cambridge,
MA, 2007.
[6] S. Harabagiu and D.I. Moldovan. Lcc?s question answering system. In 11th Text Retrieval Conference,
TREC-11, Gaithersburg, MD., 2002.
[7] Jerry R. Hobbs, Mark E. Stickel, Douglas E. Appelt, and Paul A. Martin. Interpretation as abduction. Artifi-
cial Intelligence, 63(1-2):69?142, 1993.
[8] Rohit Kate and Ray Mooney. Probabilistic abduction using markov logic networks. In IJCAI 09 Workshop
on Plan, Activity and Intent Recognition, 2009.
[9] Hector J. Levesque. A knowledge-level account of abduction. In Eleventh International Joint Conference on
Artificial Intelligence, volume 2, pages 1061?1067, Detroit, Michigan, 1989.
[10] Hwee Tou Ng and Raymond J. Mooney. The role of coherence in constructing and evaluating abductive
explanations. In P. O?Rorke, editor, Working Notes, AAAI Spring Symposium on Automated Abduction,
Stanford, California, March 1990.
[11] E. Ovchinnikova, N. Montazeri, T. Alexandrov, J. Hobbs, M. McCord, and R. Mulkar-Mehta. Abductive
reasoning with a large knowledge base for discourse processing. In Proceedings of the 9th International
Conference on Computational Semantics, Oxford, United Kingdom, 2011.
[12] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann,
San Francisco, CA, 1988.
[13] Harry E. Pople. On the mechanization of abductive logic. In Third International Joint Conference on Artificial
Intelligence, pages 147?152, Stanford, California, August 1973.
[14] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 62:107?136, 2006.
64
