Proceedings of NAACL HLT 2009: Short Papers, pages 141?144,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Faster MT Decoding through Pervasive Laziness
Michael Pust and Kevin Knight
Information Sciences Institute
University of Southern California
lastname@isi.edu
Abstract
Syntax-based MT systems have proven
effective?the models are compelling and
show good room for improvement. However,
decoding involves a slow search. We present
a new lazy-search method that obtains signifi-
cant speedups over a strong baseline, with no
loss in Bleu.
1 Introduction
Syntax-based string-to-tree MT systems have
proven effective?the models are compelling and
show good room for improvement. However, slow
decoding hinders research, as most experiments
involve heavy parameter tuning, which involves
heavy decoding. In this paper, we present a new
method to improve decoding performance, obtain-
ing a significant speedup over a strong baseline with
no loss in Bleu. In scenarios where fast decoding is
more important than optimal Bleu, we obtain better
Bleu for the same time investment. Our baseline
is a full-scale syntax-based MT system with 245m
tree-transducer rules of the kind described in (Gal-
ley et al, 2004), 192 English non-terminal symbols,
an integrated 5-gram language model (LM), and
a decoder that uses state-of-the-art cube pruning
(Chiang, 2007). A sample translation rule is:
S(x0:NP x1:VP) ? x1:VP x0:NP
In CKY string-to-tree decoding, we attack spans
of the input string from shortest to longest. We pop-
ulate each span with a set of edges. An edge contains
a English non-terminal (NT) symbol (NP, VP, etc),
border words for LM combination, pointers to child
edges, and a score. The score is a sum of (1) the
left-child edge score, (2) the right-child edge score,
(3) the score of the translation rule that combined
them, and (4) the target-string LM score. In this pa-
per, we are only concerned with what happens when
constructing edges for a single span [i,j]. The naive
algorithm works like this:
for each split point k
for each edge A in span [i,k]
for each edge B in span [k,j]
for each rule R with RHS = A B
create new edge for span [i,j]
delete all but 1000-best edges
The last step provides a necessary beam. Without
it, edges proliferate beyond available memory and
time. But even with the beam, the naive algorithm
fails, because enumerating all <A,B,R> triples at
each span is too time consuming.
2 Cube Pruning
Cube pruning (Chiang, 2007) solves this problem by
lazily enumerating triples. To work, cube pruning
requires that certain orderings be continually main-
tained at all spans. First, rules are grouped by RHS
into rule sets (eg, all the NP-VP rules are in a set),
and the members of a given set are sorted by rule
score. Second, edges in a span are grouped by NT
into edge sets (eg, all the NP edges are in an edge
set), ordered by edge score.
Consider the sub-problem of building new [i,j]
edges by combining (just) the NP edges over [i,k]
with (just) the VP edges over [k,j], using the avail-
able NP-VP rules. Rather than enumerate all triples,
cube pruning sets up a 3-dimensional cube struc-
ture whose individually-sorted axes are the NP left
edges, the VP right edges, and the NP-VP rules. Be-
cause the corner of the cube (best NP left-edge, best
VP right-edge, best NP-VP rule) is likely the best
edge in the cube, at beam size 1, we would sim-
ply return this edge and terminate, without checking
other triples. We say ?likely? because the corner po-
sition does not take into account the LM portion of
the score.1
After we take the corner and post a new edge from
it, we identify its 3 neighbors in the cube. We com-
1We also employ LM rule and edge forward-heuristics as in
(Chiang, 2007), which improve the sorting.
141
pute their full scores (including LM portion) and
push them onto a priority queue (PQ). We then pop
an item from the PQ, post another new edge, and
push the item?s neighbors onto the PQ. Note that this
PQ grows in size over time. In this way, we explore
the best portion of the cube without enumerating all
its contents. Here is the algorithm:
push(corner, make-edge(corner)) onto PQ
for i = 1 to 1000
pop(position, edge) from top of PQ
post edge to chart
for each n in neighbors(position)
push(n, make-edge(n)) onto PQ
if PQ is empty, break from for-loop
The function make-edge completely scores an edge
(including LM score) before inserting it into the PQ.
Note that in practice, we execute the loop up to 10k
times, to get 1000 edges that are distinct in their NTs
and border words.
In reality, we have to construct many cubes, one
for each combinable left and right edge set for a
given split point, plus all the cubes for all the other
split points. So we maintain a PQ-of-PQs whose el-
ements are cubes.
create each cube, pushing its fully-scored corner
onto the cube?s PQ
push cubes themselves onto a PQ-of-PQs
for i = 1 to 1000:
pop a cube C from the PQ-of-PQs
pop an item from C
post edge to chart
retrieve neighbors, score & push them onto C
push C back onto the PQ-of-PQs
3 Lazy Lists
When we meter the cube pruning algorithm, we find
that over 80% of the time goes to building the initial
queue of cubes, including deriving a corner edge for
each cube?only a small fraction is spent deriving
additional edges via exploring the cubes. For spans
of length 10 or greater, we find that we have to create
more than 1000 cubes, i.e., more than the number of
edges we wish to explore.
Our idea, then, is to create the cubes themselves
lazily. To describe our algorithm, we exploit an ab-
stract data structure called a lazy list (aka generator,
stream, pipe, or iterator), which supports three oper-
ations:
next(list): pops the front item from a list
peek(list): returns the score of the front item
empty(list): returns true if the list is empty
A cube is a lazy list (of edges). For our purposes, a
lazy list can be implemented with a PQ or something
else?we no longer care how the list is populated or
maintained, or even whether there are a finite num-
ber of elements.
Instead of explicitly enumerating all cubes for a
span, we aim to produce a lazy list of cubes. As-
sume for the moment that such a lazy list exists?we
show how to create it in the next section?and call it
L. Let us also say that cubes come off L in order of
their top edges? scores. To get our first edge, we let
C = next(L), and then we call next(C). Now a ques-
tion arises: do we pop the next-best edge off C, or
do we investigate the next cube in L? We can decide
by calling peek(peek(L)). If we choose to pop the
next cube (and then its top edge), then we face an-
other (this time three-way) decision. Bookkeeping
is therefore required if we are to continue to emit
edges in a good order.
We manage the complexity through the abstrac-
tion of a lazy list of lazy lists, to which we routinely
apply a single, key operation called merge-lists. This
operation converts a lazy list of lazy lists of X?s into
a simple lazy list of X?s. X can be anything: edges,
integers, lists, lazy lists, etc.
Figure 1 gives the generic merge-lists algorithm.
The yield function suspends computation and re-
turns to the caller. peek() lets the caller see what is
yielded, next() returns what is yielded and resumes
the loop, and empty() tells if the loop is still active.
We are now free to construct any nested ?list of
lists of lists ... of lists of X? (all lazy) and reduce
it stepwise and automatically to a single lazy list.
Standard cube pruning (Section 2) provides a sim-
ple example: if L is a list of cubes, and each cube is
a lazy list of edges, then merge-lists(L) returns us a
lazy list of edges (M), which is exactly what the de-
coder wants. The decoder can populate a new span
by simply making 1000 calls to next(M).
4 Pervasive Laziness
Now we describe how to generate cubes lazily. As
with standard cube pruning, we need to maintain a
142
merge-lists(L):
(L is a lazy list of lazy lists)
1. set up an empty PQ of lists,
prioritized by peek(list)
2. push next(L) onto PQ
3. pop list L2 off PQ
4. yield pop(L2)
5. if !empty(L2) and peek(L2) is worse than
peek(peek(L)), then push next(L) onto PQ
6. if !empty(L2), then push L2 onto PQ
7. go to step 3
Figure 1: Generic merge-lists algorithm.
small amount of ordering information among edges
in a span, which we exploit in constructing higher-
level spans. Previously, we required that all NP
edges be ordered by score, the same for VP edges,
etc. Now we additionally order whole edge sets
(groups of edges sharing an NT) with respect to each
other, eg, NP > VP > RB > etc. These are ordered
by the top-scoring edges in each set.
Ideally, we would pop cubes off our lazy list in
order of their top edges. Recall that the PQ-of-PQs
in standard cube pruning works this way. We cannot
guarantee this anymore, so we approximate it.
Consider first a single edge set from [i,k], eg, all
the NP edges. We build a lazy list of cubes that all
have a left-NP. Because edge sets from [k,j] are or-
dered with respect to each other, we may find that
it is the VP edge set that contains the best edge in
[k,j]. Pulling in all NP-VP rules, we can now postu-
late a ?best cube,? which generates edges out of left-
NPs and right-VPs. We can either continue making
edge from this cube, or we can ask for a ?second-
best cube? by moving to the next edge set of [k,j],
which might contain all the right-PP edges. Thus,
we have a lazy list of left-NP cubes. Its ordering
is approximate?cubes come off in such a way that
their top edges go from best to worst, but only con-
sidering the left and right child scores, not the rule
scores. This is the same idea followed by standard
cube pruning when it ignores internal LM scores.
We next create similar lazy lists for all the other
[i,k] edge sets (not just NP). We combine these lists
into a higher-level lazy list, whose elements pop off
according to the ordering of edge sets in [i,k]. This
structure contains all edges that can be produced
	












	

		







Figure 2: Organizing lazy lists for the decoder.
from split point k. We call merge-lists recursively
on the structure, leaving us with a single lazy list M
of edges. The decoder can now make 1000 calls to
next(M) to populate the new span.
Edges from other split points, however, must
compete on an equal basis for those 1000 slots. We
therefore produce a separate lazy list for each of the
j ? i ? 1 split points and combine these into an
even higher-level list. Lacking an ordering criterion
among split points, we presently make the top list a
non-lazy one via the PQ-of-PQs structure. Figure 2
shows how our lists are organized.
The quality of our 1000-best edges can be im-
proved. When we organize the higher-level lists by
left edge-sets, we give prominence to the best left
edge-set (eg, NP) over others (eg, VP). If the left
span is relatively short, the contribution of the left
NP to the total score of the new edge is small, so
this prominence is misplaced. Therefore, we repeat
the above process with the higher-level lists orga-
nized by right span instead of left. We merge the
right-oriented and left-oriented structures, making
sure that duplicates are avoided.
Related Work. Huang and Chiang (2007) de-
143
5x108 1x109 1.5x109 2x109 2.5x109 3x109
edges created
42000
43000
44000
45000
m
od
el
 c
os
t
lazy cube generation
exhaustive cube generation
Figure 3: Number of edges produced by the decoder, ver-
sus model cost of 1-best decodings.
scribe a variation of cube pruning called cube grow-
ing, and they apply it to a source-tree to target-
string translator. It is a two pass approach, where
a context-free parser is used to build a source for-
est, and a top down lazy forest expansion is used to
integrate a language model. The expansion recur-
sively calls cubes top-down, in depth first order. The
context-free forest controls which cubes are built,
and acts as a heuristic to minimize the number of
items returned from each cube necessary to generate
k-best derivations at the top.
It is not clear that a decoder such as ours, without
the source-tree constraint, would benefit from this
method, as building a context-free forest consistent
with future language model integration via cubes is
expensive on its own. However, we see potential
integration of both methods in two places: First,
the merge-lists algorithm can be used to lazily pro-
cess any nested for-loops?including vanilla CKY?
provided the iterands of the loops can be priori-
tized. This could speed up the creation of a first-pass
context-free forest. Second, the cubes themselves
could be prioritized in a manner similar to what we
describe, using the context-free forest to prioritize
cube generation rather than antecedent edges in the
chart (since those do not exist yet).
5 Results
We compare our method with standard cube prun-
ing (Chiang, 2007) on a full-scale Arabic/English
syntax-based MT system with an integrated 5-gram
20000 40000 60000 80000
decode time (seconds)
51.2
51.4
51.6
51.8
52
52.2
52.4
52.6
52.8
53
bl
eu
lazy cube generation
exhaustive cube generation
Figure 4: Decoding time versus Bleu.
LM. We report on 500 test sentences of lengths 15-
35. There are three variables of interest: runtime,
model cost (summed across all sentences), and IBM
Bleu. By varying the beam sizes (up to 1350),
we obtain curves that plot edges-produced versus
model-cost, shown in Figure 3. Figure 4 plots Bleu
score against time. We see that we have improved
the way our decoder searches, by teaching it to ex-
plore fewer edges, without sacrificing its ability to
find low-cost edges. This leads to faster decoding
without loss in translation accuracy.
Taken together with cube pruning (Chiang, 2007),
k-best tree extraction (Huang and Chiang, 2005),
and cube growing (Huang and Chiang, 2007), these
results provide evidence that lazy techniques may
penetrate deeper yet into MT decoding and other
NLP search problems.
We would like to thank J. Graehl and D. Chiang
for thoughts and discussions. This work was par-
tially supported under DARPA GALE, Contract No.
HR0011-06-C-0022.
References
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule. In Proc. NAACL-HLT.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. IWPT.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc.
ACL.
144
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 455?460,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Two Easy Improvements to Lexical Weighting
David Chiang and Steve DeNeefe and Michael Pust
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{chiang,sdeneefe,pust}@isi.edu
Abstract
We introduce two simple improvements to the
lexical weighting features of Koehn, Och, and
Marcu  (2003)  for  machine  translation: one
which  smooths  the  probability  of  translating
word f to word e by simplifying English mor-
phology, and one which conditions it  on the
kind of training data that f and e co-occurred
in. These new variations lead to improvements
of up to +0.8 BLEU, with an average improve-
ment of +0.6 BLEU across two language pairs,
two genres, and two translation systems.
1 Introduction
Lexical weighting features (Koehn et al, 2003) es-
timate the probability of a phrase pair or translation
rule word-by-word. In this paper, we introduce two
simple improvements to these features: one which
smooths  the  probability  of  translating  word f to
word e using English morphology, and one which
conditions it on the kind of training data that f and
e co-occurred in. These new variations lead to im-
provements of up to+0.8BLEU, with an average im-
provement of +0.6BLEU across two language pairs,
two genres, and two translation systems.
2 Background
Since there  are  slight  variations  in  how the  lexi-
cal weighting features are computed, we begin by
defining the baseline lexical weighting features. If
f = f1 ? ? ? fn and e = e1 ? ? ? em are a training sentence
pair, let ai (1 ? i ? n) be the (possibly empty) set of
positions in f that ei is aligned to.
First, compute a word translation table from the
word-aligned parallel text: for each sentence pair and
each i, let
c( f j, ei)? c( f j, ei) +
1
|ai|
for j ? ai (1)
c(NULL, ei)? c(NULL, ei) + 1 if |ai| = 0 (2)
Then
t(e | f ) = c( f , e)?
e c( f , e)
(3)
where f can be NULL.
Second, during phrase-pair extraction, store with
each phrase pair the alignments between the words
in the phrase pair. If it is observed with more than
one word alignment pattern, store the most frequent
pattern.
Third, for each phrase pair ( f? , e?, a), compute
t(e? | f? ) =
|e?|
?
i=1
?
?
?
?
?
?
?
?
?
?
?
1
|ai|
?
j?ai
t(e?i | f? j) if |ai| > 0
t(e?i | NULL) otherwise
(4)
This generalizes to synchronous CFG rules in the ob-
vious way.
Similarly, compute the reverse probability t( f? | e?).
Then add two new model features
? log t(e? | f? ) and ? log t( f? | e?)
455
translation
feature (7) (8)
small LM 26.7 24.3
large LM 31.4 28.2
? log t(e? | f? ) 9.3 9.9
? log t( f? | e?) 5.8 6.3
Table 1: Although the language models prefer translation
(8), which translates ?? and ?? as singular nouns, the
lexical weighting features prefer translation (7), which in-
correctly generates plural nouns. All features are negative
log-probabilities, so lower numbers indicate preference.
3 Morphological smoothing
Consider the following example Chinese sentence:
(5) ???
W?n Ji?b?o
Wen Jiabao
??
bi?osh?
said
,
,
,
????
K?t?d?w?
C?te d?Ivoire
?
sh?
is
??
Zh?nggu?
China
?
z?i
in
??
F?izh?u
Africa
?
de
?s
?
h?o
good
??
p?ngy?u
friend
,
,
,
?
h?o
good
??
hu?b?n
partner
.
.
.
(6) Human: Wen Jiabao said that C?te d?Ivoire is
a good friend and a good partner of China?s in
Africa.
(7) MT (baseline): Wen  Jiabao  said  that  Cote
d?Ivoire  is  China?s  good friends, and  good
partners in Africa.
(8) MT (better):Wen Jiabao said that Cote d?Ivoire
is  China?s  good friend and  good partner in
Africa.
The baseline machine translation (7) incorrectly gen-
erates plural nouns. Even though the language mod-
els (LMs) prefer singular nouns, the lexical weight-
ing features prefer plural nouns (Table 1).
1
The reason for this is that the Chinese words do not
have any marking for number. Therefore the infor-
mation needed to mark friend and partner for num-
ber must come from the context. The LMs are able
to capture this context: the 5-gram is China?s good
1
The presence of an extra comma in translation (7) affects
the LM scores only slightly; removing the comma would make
them 26.4 and 32.0.
f e t(e | f ) t( f | e) tm(e | f ) tm( f | e)
?? friends 0.44 0.44 0.47 0.48
?? friend 0.21 0.58 0.19 0.48
?? partners 0.44 0.60 0.40 0.53
?? partner 0.13 0.40 0.17 0.53
Table 2: The morphologically-smoothed lexical weight-
ing features weaken the preference for singular or plural
translations, with the exception of t(friends | ??).
friend is observed in our large LM, and the 4-gram
China?s good friend in our small LM, but China?s
good friends is not observed in either LM. Likewise,
the 5-grams good friend and good partner and good
friends and good partners are both observed in our
LMs, but neither good friend and good partners nor
good friends and good partner is.
By contrast, the lexical weighting tables (Table 2,
columns 3?4), which ignore context, have a strong
preference for plural translations, except in the case
of t(?? | friend). Therefore  we hypothesize  that,
for Chinese-English translation, we should weaken
the lexical weighting features? morphological pref-
erences so that more contextual features can do their
work.
Running a morphological stemmer (Porter, 1980)
on  the  English  side  of  the  parallel  data  gives  a
three-way parallel text: for each sentence, we have
French f, English e, and stemmed English e?. We can
then build two word translation tables, t(e? | f ) and
t(e | e?), and form their product
tm(e | f ) =
?
e?
t(e? | f )t(e | e?) (9)
Similarly, we can compute tm( f | e) in the opposite
direction.
2
(See Table 2, columns 5?6.) These tables
can then be extended to phrase pairs or synchronous
CFG rules as before and added as two new features
of the model:
? log tm(e? | f? ) and ? log tm( f? | e?)
The feature tm(e? | f? ) does still prefer certain word-
forms, as can be seen in Table 2. But because e is
generated from e? and not from f , we are protected
from the situation where a rare f leads to poor esti-
mates for the e.
2
Since the Porter stemmer is deterministic, we always have
t(e? | e) = 1.0, so that tm( f | e) = t( f | e?), as seen in the last
column of Table 2.
456
When  we  applied  an  analogous  approach  to
Arabic-English translation, stemming  both  Arabic
and English, we generated very large lexicon tables,
but saw no statistically significant change in BLEU.
Perhaps this is  not surprising, because  in  Arabic-
English translation (unlike Chinese-English transla-
tion), the source language is morphologically richer
than the target language. So we may benefit from fea-
tures that preserve this information, while smoothing
over morphological differences blurs important dis-
tinctions.
4 Conditioning on provenance
Typical machine translation systems are trained on
a fixed set of training data ranging over a variety of
genres, and if the genre of an input sentence is known
in advance, it is usually advantageous to use model
parameters tuned for that genre.
Consider the following Arabic sentence, from a
weblog (words written left-to-right):
(10) ????
wlEl
perhaps
???
h*A
this
???
AHd
one
???
Ahm
main
??????
Alfrwq
differences
???
byn
between
???
Swr
images
?????
AnZmp
systems
?????
AlHkm
ruling
????????
AlmqtrHp
proposed
.
.
.
(11) Human: Perhaps this is one of the most impor-
tant differences between the images of the pro-
posed ruling systems.
(12) MT (baseline): This may be one of the most
important differences between pictures of the
proposed ruling regimes.
(13) MT (better): Perhaps this is one of the most im-
portant differences between the images of the
proposed regimes.
The Arabic word ???? can be translated asmay or per-
haps (among others), with the latter more common
according to t(e | f ), as shown in Table 3. But some
genres favor perhaps more or less strongly. Thus,
both translations (12) and (13) are good, but the lat-
ter uses a slightly more informal register appropriate
to the genre.
Following Matsoukas et al (2009), we assign each
training sentence pair a set of binary features which
we call s-features:
t(e | f ) ts(e | f )
f e ? nw web bn un
???? may 0.13 0.12 0.16 0.09 0.13
???? perhaps 0.20 0.23 0.32 0.42 0.19
Table 3: Different genres have different preferences for
word translations. Key: nw = newswire, web = Web, bn =
broadcast news, un = United Nations proceedings.
? Whether the sentence pair came from a particu-
lar genre, for example, newswire or web
? Whether the sentence pair came from a particu-
lar collection, for example, FBIS or UN
Matsoukas et  al. (2009)  use these s-features  to
compute  weights  for  each  training  sentence  pair,
which are in turn used for computing various model
features. They found that the sentence-level weights
were most helpful for computing the lexical weight-
ing  features  (p.c.). The  mapping  from  s-features
to  sentence  weights  was  chosen  to  optimize  ex-
pected TER on held-out data. A drawback of this
method is that we must now learn the mapping from
s-features to sentence-weights and then the model
feature weights. Therefore, we tried an alternative
that incorporates s-features into the model itself.
For each s-feature s, we compute new word trans-
lation tables ts(e | f ) and ts( f | e) estimated from
only those sentence pairsf on which s fires, and ex-
tend them to phrases/rules as before. The idea is to
use these probabilities as new features in the model.
However, two  challenges  arise: first, many  word
pairs are unseen for a given s, resulting in zero or
undefined probabilities; second, this adds many new
features for each rule, which requires a lot of space.
To address the problem of unseen word pairs, we
use Witten-Bell smoothing (Witten and Bell, 1991):
t?s(e | f ) = ? f sts(e | f ) + (1 ? ? f s)t(e | f ) (14)
? f s =
c( f , s)
c( f , s) + d( f , s)
(15)
where c( f , s) is the number of times f has been ob-
served in sentences with s-feature s, and d( f , s) is the
number of e types observed aligned to f in sentences
with s-feature s.
For each s-feature s, we add two model features
? log t?s(e? | f? )
t(e? | f? )
and ? log t?s( f? | e?)
t( f? | e?)
457
Arabic-English Chinese-English
newswire web newswire web
system features Dev Test Dev Test Dev Test Dev Test
string-to-string baseline 47.1 43.8 37.1 38.4 28.7 26.0 23.2 25.9
full
2
47.7 44.2
?
37.4 39.0 29.5 26.8 23.8 26.3
string-to-tree baseline 47.3 43.6 37.7 39.6 29.2 26.4 23.0 26.0
full 47.7 44.3 38.3 40.2 29.8 27.1 23.4 26.6
Table 4: Our variations on lexical weighting improve translation quality significantly across 16 different test conditions.
All improvements are significant at the p < 0.01 level, except where marked with an asterisk (?), indicating p < 0.05.
In order to address the space problem, we use the
following heuristic: for any given rule, if the absolute
value of one of these features is less than log 2, we
discard it for that rule.
5 Experiments
Setup We  tested  these  features  on  two  ma-
chine  translation  systems: a  hierarchical  phrase-
based (string-to-string) system (Chiang, 2005) and
a syntax-based (string-to-tree) system (Galley et al,
2004; Galley et al, 2006). For Arabic-English trans-
lation, both systems were trained on 190+220 mil-
lion words of parallel data; for Chinese-English, the
string-to-string system was trained on 240+260 mil-
lion words of parallel data, and the string-to-tree sys-
tem, 58+65 million words. Both used two language
models, one trained on the combined English sides
of the Arabic-English and Chinese-English data, and
one trained on 4 billion words of English data.
The baseline string-to-string system already incor-
porates some simple provenance features: for each
s-feature s, there is a feature P(s | rule). Both base-
line also include a variety of other features (Chiang
et al, 2008; Chiang et al, 2009; Chiang, 2010).
Both systems were trained using MIRA (Cram-
mer et al, 2006; Watanabe et al, 2007; Chiang et al,
2008) on a held-out set, then tested on two more sets
(Dev and Test) disjoint from the data used for rule
extraction and for  MIRA training. These datasets
have roughly 1000?3000 sentences (30,000?70,000
words) and are drawn from test sets from the NIST
MT evaluation and development sets from the GALE
program.
Individual  tests We  first  tested  morphological
smoothing  using  the  string-to-string  system  on
Chinese-English  translation. The  morphologically
smoothed system generated the improved translation
(8) above, and generally gave a small improvement:
task features Dev
Chi-Eng nw baseline 28.7
morph 29.1
We then tested the provenance-conditioned fea-
tures on both Arabic-English and Chinese-English,
again using the string-to-string system:
task features Dev
Ara-Eng nw baseline 47.1
(Matsoukas et al, 2009) 47.3
provenance
2
47.7
Chi-Eng nw baseline 28.7
provenance
2
29.4
The  translations  (12)  and  (13)  come  from  the
Arabic-English baseline and provenance systems.
For Arabic-English, we also compared against lex-
ical  weighting  features  that  use  sentence  weights
kindly provided to us by Matsoukas et al Our fea-
tures performed better, although it should be noted
that those sentence weights had been optimized for
a different translation model.
Combined  tests Finally, we  tested  the  features
across a wider range of tasks. For Chinese-English
translation, we  combined  the  morphologically-
smoothed  and  provenance-conditioned  lexical
weighting  features; for  Arabic-English, we  con-
tinued  to  use  only  the  provenance-conditioned
features. We  tested  using  both  systems, and  on
both  newswire  and  web  genres. The  results  are
shown in Table 4. The features produce statistically
significant improvements across all 16 conditions.
2
In these systems, an error crippled the t( f | e), tm( f | e), and
ts( f | e) features. Time did not permit rerunning all of these sys-
tems with the error fixed, but partial results suggest that it did
not have a significant impact.
458
-0.4-0.3-0.2-0.1 0
 0.1 0.2 0.3 0.4 0.5
-0.8 -0.6 -0.4 -0.2  0  0.2  0.4  0.6  0.8
Web
Newswire
bc bn LDC2005T06 NameEntityLDC2006E24LDC2006E92LDC2006G05LDC2007E08
LDC2007E101LDC2007E103 LDC2008G05lexiconng nwNewsExplorer UNweb
wl
Figure 1: Feature  weights  for  provenance-conditioned  features: string-to-string, Chinese-English, web  versus
newswire. A higher weight indicates a more useful source of information, while a negative weight indicates a less
useful or possibly problematic source. For clarity, only selected points are labeled. The diagonal line indicates where
the two weights would be equal relative to the original t(e | f ) feature weight.
Figure 1 shows the feature weights obtained for
the provenance-conditioned features ts( f | e) in the
string-to-string Chinese-English system, trained on
newswire and web data. On the diagonal are cor-
pora that were equally useful in either genre. Surpris-
ingly, the UN data received strong positive weights,
indicating usefulness in both genres. Two lists  of
named entities received large weights: the LDC list
(LDC2005T34)  in  the  positive  direction  and  the
NewsExplorer  list  in  the  negative  direction, sug-
gesting  that  there  are  noisy  entries  in  the  latter.
The corpus LDC2007E08, which contains parallel
data mined from comparable corpora (Munteanu and
Marcu, 2005), received strong negative weights.
Off the diagonal are corpora favored in only one
genre or the other: above, we see that the wl (we-
blog)  and ng (newsgroup)  genres  are  more help-
ful for web translation, as expected (although web
oddly seems less helpful), as well as LDC2006G05
(LDC/FBIS/NVTC Parallel Text V2.0). Below are
corpora  more  helpful  for  newswire  translation,
like LDC2005T06 (Chinese News Translation Text
Part 1).
6 Conclusion
Many  different  approaches  to  morphology  and
provenance in machine translation are possible. We
have chosen to implement our approach as exten-
sions  to  lexical  weighting  (Koehn  et  al., 2003),
which is nearly ubiquitous, because it is defined at
the level of word alignments. For this reason, the
features we have introduced should be easily ap-
plicable to a wide range of phrase-based, hierarchi-
cal phrase-based, and syntax-based systems. While
the improvements obtained using them are not enor-
mous, we have demonstrated that they help signif-
icantly across many different conditions, and over
very strong baselines. We therefore fully expect that
these  new  features  would  yield  similar  improve-
ments in other systems as well.
Acknowledgements
We would like to thank Spyros Matsoukas and col-
leagues at BBN for providing their sentence-level
weights  and  important  insights  into  their  corpus-
weighting work. This work was supported in part by
DARPA contract HR0011-06-C-0022 under subcon-
tract to BBN Technologies.
459
References
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of  syntactic  and struc-
tural translation features. In Proc. EMNLP 2008, pages
224?233.
David  Chiang, Kevin  Knight, and  Wei  Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. NAACL HLT, pages 218?226.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL 2005,
pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, pages 1443?1452.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
HLT-NAACL 2004, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe,Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich
syntactic translation models. In Proc. COLING-ACL
2006, pages 961?968.
Philipp  Koehn, Franz Josef  Och, and  Daniel  Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127?133.
Spyros  Matsoukas, Antti-Veikko I.  Rosti, and  Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proc. EMNLP 2009,
pages 708?717.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31:477?504.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proc. EMNLP-CoNLL
2007, pages 764?773.
Ian H.  Witten  and  Timothy C.  Bell. 1991. The
zero-frequency problem: Estimating the probabilities
of novel events in adaptive text compression. IEEE
Trans. Information Theory, 37(4):1085?1094.
460
