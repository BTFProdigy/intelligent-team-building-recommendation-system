Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 204?212,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Discriminative Strategies to Integrate Multiword Expression Recognition
and Parsing
Matthieu Constant
Universite? Paris-Est
LIGM, CNRS
France
mconstan@univ-mlv.fr
Anthony Sigogne
Universite? Paris-Est
LIGM, CNRS
France
sigogne@univ-mlv.fr
Patrick Watrin
Universite? de Louvain
CENTAL
Belgium
patrick.watrin
@uclouvain.be
Abstract
The integration of multiword expressions in a
parsing procedure has been shown to improve
accuracy in an artificial context where such
expressions have been perfectly pre-identified.
This paper evaluates two empirical strategies
to integrate multiword units in a real con-
stituency parsing context and shows that the
results are not as promising as has sometimes
been suggested. Firstly, we show that pre-
grouping multiword expressions before pars-
ing with a state-of-the-art recognizer improves
multiword recognition accuracy and unlabeled
attachment score. However, it has no statis-
tically significant impact in terms of F-score
as incorrect multiword expression recognition
has important side effects on parsing. Sec-
ondly, integrating multiword expressions in
the parser grammar followed by a reranker
specific to such expressions slightly improves
all evaluation metrics.
1 Introduction
The integration of Multiword Expressions (MWE)
in real-life applications is crucial because such ex-
pressions have the particularity of having a certain
level of idiomaticity. They form complex lexical
units which, if they are considered, should signifi-
cantly help parsing.
From a theoretical point of view, the integra-
tion of multiword expressions in the parsing pro-
cedure has been studied for different formalisms:
Head-Driven Phrase Structure Grammar (Copestake
et al, 2002), Tree Adjoining Grammars (Schuler
and Joshi, 2011), etc. From an empirical point of
view, their incorporation has also been considered
such as in (Nivre and Nilsson, 2004) for depen-
dency parsing and in (Arun and Keller, 2005) in con-
stituency parsing. Although experiments always re-
lied on a corpus where the MWEs were perfectly
pre-identified, they showed that pre-grouping such
expressions could significantly improve parsing ac-
curacy. Recently, Green et al (2011) proposed in-
tegrating the multiword expressions directly in the
grammar without pre-recognizing them. The gram-
mar was trained with a reference treebank where
MWEs were annotated with a specific non-terminal
node.
Our proposal is to evaluate two discriminative
strategies in a real constituency parsing context:
(a) pre-grouping MWE before parsing; this would
be done with a state-of-the-art recognizer based
on Conditional Random Fields; (b) parsing with
a grammar including MWE identification and then
reranking the output parses thanks to a Maxi-
mum Entropy model integrating MWE-dedicated
features. (a) is the direct realistic implementation of
the standard approach that was shown to reach the
best results (Arun and Keller, 2005). We will evalu-
ate if real MWE recognition (MWER) still positively
impacts parsing, i.e., whether incorrect MWER does
not negatively impact the overall parsing system.
(b) is a more innovative approach to MWER (de-
spite not being new in parsing): we select the final
MWE segmentation after parsing in order to explore
as many parses as possible (as opposed to method
(a)). The experiments were carried out on the French
Treebank (Abeille? et al, 2003) where MWEs are an-
notated.
204
The paper is organized as follows: section 2 is
an overview of the multiword expressions and their
identification in texts; section 3 presents the two dif-
ferent strategies and their associated models; sec-
tion 4 describes the resources used for our exper-
iments (the corpus and the lexical resources); sec-
tion 5 details the features that are incorporated in the
models; section 6 reports on the results obtained.
2 Multiword expressions
2.1 Overview
Multiword expressions are lexical items made up
of multiple lexemes that undergo idiosyncratic con-
straints and therefore offer a certain degree of id-
iomaticity. They cover a wide range of linguistic
phenomena: fixed and semi-fixed expressions, light
verb constructions, phrasal verbs, named entities,
etc. They may be contiguous (e.g. traffic light) or
discontinuous (e.g. John took your argument into
account). They are often divided into two main
classes: multiword expressions defined through lin-
guistic idiomaticity criteria (lexicalized phrases in
the terminology of Sag et al (2002)) and those de-
fined by statistical ones (i.e. simple collocations).
Most linguistic criteria used to determine whether a
combination of words is a MWE are based on syn-
tactic and semantic tests such as the ones described
in (Gross, 1986). For instance, the utterance at night
is a MWE because it does display a strict lexical
restriction (*at day, *at afternoon) and it does not
accept any inserting material (*at cold night, *at
present night). Such linguistically defined expres-
sions may overlap with collocations which are the
combinations of two or more words that cooccur
more often than by chance. Collocations are usu-
ally identified through statistical association mea-
sures. A detailed description of MWEs can be found
in (Baldwin and Nam, 2010).
In this paper, we focus on contiguous MWEs that
form a lexical unit which can be marked by a part-of-
speech tag (e.g. at night is an adverb, because of is a
preposition). They can undergo limited morphologi-
cal and lexical variations ? e.g. traffic (light+lights),
(apple+orange+...) juice ? and usually do not al-
low syntactic variations1 such as inserts (e.g. *at
1Such MWEs may very rarely accept inserts, often limited
to single word modifiers: e.g. in the short term, in the very short
cold night). Such expressions can be analyzed at the
lexical level. In what follows, we use the term com-
pounds to denote such expressions.
2.2 Identification
The idiomaticity property of MWEs makes them
both crucial for Natural Language Processing appli-
cations and difficult to predict. Their actual iden-
tification in texts is therefore fundamental. There
are different ways for achieving this objective. The
simpler approach is lexicon-driven and consists in
looking the MWEs up in an existing lexicon, such
as in (Silberztein, 2000). The main drawback is
that this procedure entirely relies on a lexicon and
is unable to discover unknown MWEs. The use
of collocation statistics is therefore useful. For in-
stance, for each candidate in the text, Watrin and
Franc?ois (2011) compute on the fly its association
score from an external ngram base learnt from a
large raw corpus, and tag it as MWE if the associa-
tion score is greater than a threshold. They reach ex-
cellent scores in the framework of a keyword extrac-
tion task. Within a validation framework (i.e. with
the use of a reference corpus annotated in MWEs),
Ramisch et al (2010) developped a Support Vector
Machine classifier integrating features correspond-
ing to different collocation association measures.
The results were rather low on the Genia corpus
and Green et al (2011) confirmed these bad results
on the French Treebank. This can be explained by
the fact that such a method does not make any dis-
tinctions between the different types of MWEs and
the reference corpora are usually limited to certain
types of MWEs. Furthermore, the lexicon-driven
and collocation-driven approaches do not take the
context into account, and therefore cannot discard
some of the incorrect candidates. A recent trend is
to couple MWE recognition with a linguistic ana-
lyzer: a POS tagger (Constant and Sigogne, 2011)
or a parser (Green et al, 2011). Constant and Si-
gogne (2011) trained a unified Conditional Random
Fields model integrating different standard tagging
features and features based on external lexical re-
sources. They show a general tagging accuracy of
94% on the French Treebank. In terms of Multi-
word expression recognition, the accuracy was not
term.
205
clearly evaluated, but seemed to reach around 70-
80% F-score. Green et al (2011) proposed to in-
clude the MWER in the grammar of the parser. To
do so, the MWEs in the training treebank were anno-
tated with specific non-terminal nodes. They used a
Tree Substitution Grammar instead of a Probabilis-
tic Context-free Grammar (PCFG) with latent anno-
tations in order to capture lexicalized rules as well
as general rules. They showed that this formalism
was more relevant to MWER than PCFG (71% F-
score vs. 69.5%). Both methods have the advantage
of being able to discover new MWEs on the basis
of lexical and syntactic contexts. In this paper, we
will take advantage of the methods described in this
section by integrating them as features of a MWER
model.
3 Two strategies, two discriminative
models
3.1 Pre-grouping Multiword Expressions
MWER can be seen as a sequence labelling task
(like chunking) by using an IOB-like annotation
scheme (Ramshaw and Marcus, 1995). This implies
a theoretical limitation: recognized MWEs must be
contiguous. The proposed annotation scheme is
therefore theoretically weaker than the one proposed
by Green et al (2011) that integrates the MWER in
the grammar and allows for discontinuous MWEs.
Nevertheless, in practice, the compounds we are
dealing with are very rarely discontinuous and if so,
they solely contain a single word insert that can be
easily integrated in the MWE sequence. Constant
and Sigogne (2011) proposed to combine MWE seg-
mentation and part-of-speech tagging into a single
sequence labelling task by assigning to each token a
tag of the form TAG+X where TAG is the part-of-
speech (POS) of the lexical unit the token belongs to
and X is either B (i.e. the token is at the beginning
of the lexical unit) or I (i.e. for the remaining posi-
tions): John/N+B hates/V+B traffic/N+B jams/N+I.
In this paper, as our task consists in jointly locating
and tagging MWEs, we limited the POS tagging to
MWEs only (TAG+B/TAG+I), simple words being
tagged by O (outside): John/O hates/O traffic/N+B
jams/N+I.
For such a task, we used Linear chain Conditional
Ramdom Fields (CRF) that are discriminative prob-
abilistic models introduced by Lafferty et al (2001)
for sequential labelling. Given an input sequence of
tokens x = (x1, x2, ..., xN ) and an output sequence
of labels y = (y1, y2, ..., yN ), the model is defined
as follows:
P?(y|x) =
1
Z(x)
.
N?
t
K?
k
log?k.fk(t, yt, yt?1, x)
where Z(x) is a normalization factor depending
on x. It is based on K features each of them be-
ing defined by a binary function fk depending on
the current position t in x, the current label yt, the
preceding one yt?1 and the whole input sequence
x. The tokens xi of x integrate the lexical value
of this token but can also integrate basic properties
which are computable from this value (for example:
whether it begins with an upper case, it contains a
number, its tags in an external lexicon, etc.). The
feature is activated if a given configuration between
t, yt, yt?1 and x is satisfied (i.e. fk(t, yt, yt?1, x) =
1). Each feature fk is associated with a weight ?k.
The weights are the parameters of the model, to be
estimated. The features used for MWER will be de-
scribed in section 5.
3.2 Reranking
Discriminative reranking consists in reranking the n-
best parses of a baseline parser with a discriminative
model, hence integrating features associated with
each node of the candidate parses. Charniak and
Johnson (2005) introduced different features that
showed significant improvement in general parsing
accuracy (e.g. around +1 point in English). For-
mally, given a sentence s, the reranker selects the
best candidate parse p among a set of candidates
P (s) with respect to a scoring function V?:
p? = argmaxp?P (s)V?(p)
The set of candidates P (s) corresponds to the n-best
parses generated by the baseline parser. The scor-
ing function V? is the scalar product of a parameter
vector ? and a feature vector f :
V?(p) = ?.f(p) =
m?
j=1
?j .fj(p)
where fj(p) corresponds to the number of occur-
rences of the feature fj in the parse p. According to
206
Charniak and Johnson (2005), the first feature f1 is
the probability of p provided by the baseline parser.
The vector ? is estimated during the training stage
from a reference treebank and the baseline parser
ouputs.
In this paper, we slightly deviate from the original
reranker usage, by focusing on improving MWER
in the context of parsing. Given the n-best parses,
we want to select the one with the best MWE seg-
mentation by keeping the overall parsing accuracy as
high as possible. We therefore used MWE-dedicated
features that we describe in section 5. The training
stage was performed by using a Maximum entropy
algorithm as in (Charniak and Johnson, 2005).
4 Resources
4.1 Corpus
The French Treebank2 [FTB] (Abeille? et al, 2003)
is a syntactically annotated corpus made up of jour-
nalistic articles from Le Monde newspaper. We
used the latest edition of the corpus (June 2010)
that we preprocessed with the Stanford Parser pre-
processing tools (Green et al, 2011). It contains
473,904 tokens and 15,917 sentences. One benefit of
this corpus is that its compounds are marked. Their
annotation was driven by linguistic criteria such as
the ones in (Gross, 1986). Compounds are identified
with a specific non-terminal symbol ?MWX? where
X is the part-of-speech of the expression. They have
a flat structure made of the part-of-speech of their
components as shown in figure 1.
MWN


HH
H
N
part
P
de
N
marche?
Figure 1: Subtree of MWE part de marche? (market
share): The MWN node indicates that it is a multiword
noun; it has a flat internal structure N P N (noun ? pre-
prosition ? noun)
The French Treebank is composed of 435,860 lex-
ical units (34,178 types). Among them, 5.3% are
compounds (20.8% for types). In addition, 12.9%
2http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-
fr.php
of the tokens belong to a MWE, which, on average,
has 2.7 tokens. The non-terminal tagset is composed
of 14 part-of-speech labels and 24 phrasal ones (in-
cluding 11 MWE labels). The train/dev/test split is
the same as in (Green et al, 2011): 1,235 sentences
for test, 1,235 for development and 13,347 for train-
ing. The development and test sections are the same
as those generally used for experiments in French,
e.g. (Candito and Crabbe?, 2009).
4.2 Lexical resources
French is a resource-rich language as attested by
the existing morphological dictionaries which in-
clude compounds. In this paper, we use two large-
coverage general-purpose dictionaries: Dela (Cour-
tois, 1990; Courtois et al, 1997) and Lefff (Sagot,
2010). The Dela was manually developed in the
90?s by a team of linguists. We used the distribution
freely available in the platform Unitex3 (Paumier,
2011). It is composed of 840,813 lexical entries in-
cluding 104,350 multiword ones (91,030 multiword
nouns). The compounds present in the resources re-
spect the linguistic criteria defined in (Gross, 1986).
The lefff is a freely available dictionary4 that has
been automatically compiled by drawing from dif-
ferent sources and that has been manually validated.
We used a version with 553,138 lexical entries in-
cluding 26,311 multiword ones (22,673 multiword
nouns). Their different modes of acquisition makes
those two resources complementary. In both, lexical
entries are composed of a inflected form, a lemma,
a part-of-speech and morphological features. The
Dela has an additional feature for most of the mul-
tiword entries: their syntactic surface form. For in-
stance, eau de vie (brandy) has the feature NDN be-
cause it has the internal flat structure noun ? prepo-
sition de ? noun.
In order to compare compounds in these lexical
resources with the ones in the French Treebank, we
applied on the development corpus the dictionar-
ies and the lexicon extracted from the training cor-
pus. By a simple look-up, we obtained a prelimi-
nary lexicon-based MWE segmentation. The results
are provided in table 1. They show that the use of
external resources may improve recall, but they lead
3http://igm.univ-mlv.fr/?unitex
4http://atoll.inria.fr/?sagot/lefff.html
207
to a decrease in precision as numerous MWEs in the
dictionaries are not encoded as such in the reference
corpus; in addition, the FTB suffers from some in-
consistency in the MWE annotations.
T L D T+L T+D T+L+D
recall 75.9 31.7 59.0 77.3 83.4 84.0
precision 61.2 52.0 55.6 58.7 51.2 49.9
f-score 67.8 39.4 57.2 66.8 63.4 62.6
Table 1: Simple context-free application of the lexical
resources on the development corpus: T is the MWE lex-
icon of the training corpus, L is the lefff, D is the Dela.
The given scores solely evaluate MWE segmentation and
not tagging.
In terms of statistical collocations, Watrin and
Franc?ois (2011) described a system that lists all the
potential nominal collocations of a given sentence
along with their association measure. The authors
provided us with a list of 17,315 candidate nominal
collocations occurring in the French treebank with
their log-likelihood and their internal flat structure.
5 MWE-dedicated Features
The two discriminative models described in sec-
tion 3 require MWE-dedicated features. In order to
make these models comparable, we use two compa-
rable sets of feature templates: one adapted to se-
quence labelling (CRF-based MWER) and the other
one adapted to reranking (MaxEnt-based reranker).
The MWER templates are instantiated at each posi-
tion of the input sequence. The reranker templates
are instantiated only for the nodes of the candidate
parse tree, which are leaves dominated by a MWE
node (i.e. the node has a MWE ancestor). We define
a template T as follows:
? MWER: for each position n in the input se-
quence x,
T = f(x, n)/yn
? RERANKER: for each leaf (in position n)
dominated by a MWE node m in the current
parse tree p,
T = f(p, n)/label(m)/pos(p, n)
where f is a function to be defined; yn is the out-
put label at position n; label(m) is the label of node
m and pos(p, n) indicates the position of the word
corresponding to n in the MWE sequence: B (start-
ing position), I (remaining positions).
5.1 Endogenous Features
Endogenous features are features directly extracted
from properties of the words themselves or from a
tool learnt from the training corpus (e.g. a tagger).
Word n-grams. We use word unigrams and bigrams
in order to capture multiwords present in the training
section and to extract lexical cues to discover new
MWEs. For instance, the bigram coup de is often
the prefix of compounds such as coup de pied (kick),
coup de foudre (love at first sight), coup de main
(help).
POS n-grams. We use part-of-speech unigrams
and bigrams in order to capture MWEs with irreg-
ular syntactic structures that might indicate the id-
iomacity of a word sequence. For instance, the POS
sequence preposition ? adverb associated with the
compound depuis peu (recently) is very unusual in
French. We also integrated mixed bigrams made up
of a word and a part-of-speech.
Specific features. Due to their different use, each
model integrates some specific features. In order to
deal with unknown words and special tokens, we in-
corporate standard tagging features in the CRF: low-
ercase forms of the words, word prefixes of length 1
to 4, word suffice of length 1 to 4, whether the word
is capitalized, whether the token has a digit, whether
it is an hyphen. We also add label bigrams. The
reranker models integrate features associated with
each MWE node, the value of which is the com-
pound itself.
5.2 Exogenous Features
Exogenous features are features that are not entirely
derived from the (reference) corpus itself. They are
computed from external data (in our case, our lexical
resources). The lexical resources might be useful to
discover new expressions: usually, expressions that
have standard syntax like nominal compounds and
are difficult to predict from the endogenous features.
The resources are applied to the corpus through a
lexical analysis that generates, for each sentence, a
finite-state automaton TFSA which represents all the
possible analyses. The features are computed from
the automaton TFSA.
Lexicon-based features. We associate each word
with its part-of-speech tags found in our external
morphological lexicon. All tags of a word constitute
208
an ambiguity class ac. If the word belongs to a com-
pound, the compound tag is also incorporated in the
ambiguity class. For instance, the word night (either
a simple noun or a simple adjective) in the context at
night, is associated with the class adj noun adv+I as
it is located inside a compound adverb. This feature
is directly computed from TFSA. The lexical anal-
ysis can lead to a preliminary MWE segmentation
by using a shortest path algorithm that gives priority
to compound analyses. This segmentation is also a
source of features: a word belonging to a compound
segment is assigned different properties such as the
segment part-of-speech mwt and its syntactic struc-
turemws encoded in the lexical resource, its relative
position mwpos in the segment (?B? or ?I?).
Collocation-based features. In our collocation re-
source, each candidate collocation of the French
treebank is associated with its internal syntactic
structure and its association score (log-likelihood).
We divided these candidates into two classes: those
whose score is greater than a threshold and the other
ones. Therefore, a given word in the corpus can be
associated with different properties whether it be-
longs to a potential collocation: the class c and the
internal structure cs of the collocation it belongs to,
its position cpos in the collocation (B: beginning; I:
remaining positions; O: outside). We manually set
the threshold to 150 after some tuning on the devel-
opment corpus.
All feature templates are given in table 2.
Endogenous Features
w(n+ i), i ? {?2,?1, 0, 1, 2}
w(n+ i)/w(n+ i+ 1), i ? {?2,?1, 0, 1}
t(n+ i), i ? {?2,?1, 0, 1, 2}
t(n+ i)/t(n+ i+ 1), i ? {?2,?1, 0, 1}
w(n+ i)/t(n+ j), (i, j) ? {(1, 0), (0, 1), (?1, 0), (0,?1)}
Exogenous Features
ac(n)
mwt(n)/mwpos(n)
mws(n)/mwpos(n)
c(n)/cs(n)/cpos(n)
Table 2: Feature templates (f ) used both in the MWER
and the reranker models: n is the current position in the
sentence, w(i) is the word at position i; t(i) is the part-
of-speech tag of w(i); if the word at absolute position i
is part of a compound in the Shortest Path Segmentation,
mwt(i) and mws(i) are respectively the part-of-speech
tag and the internal structure of the compound,mwpos(i)
indicates its relative position in the compound (B or I).
6 Evaluation
6.1 Experiment Setup
We carried out 3 different experiments. We first
tested a standalone MWE recognizer based on CRF.
We then combined MWE pregrouping based on
this recognizer and the Berkeley parser5 (Petrov
et al, 2006) trained on the FTB where the com-
pounds were concatenated (BKYc). Finally, we
combined the Berkeley parser trained on the FTB
where the compounds are annotated with specific
non-terminals (BKY), and the reranker. In all exper-
iments, we varied the set of features: endo are all en-
dogenous features; coll and lex include all endoge-
nous features plus collocation-based features and
lexicon-based ones, respectively; all is composed of
both endogenous and exogenous features. The CRF
recognizer relies on the software Wapiti6 (Lavergne
et al, 2010) to train and apply the model, and on
the software Unitex (Paumier, 2011) to apply lexical
resources. The part-of-speech tagger used to extract
POS features was lgtagger7 (Constant and Sigogne,
2011). To train the reranker, we used a MaxEnt al-
gorithm8 as in (Charniak and Johnson, 2005).
Results are reported using several standard mea-
sures, the F1score, unlabeled attachment and Leaf
Ancestor scores. The labeled F1score [F1]9, de-
fined by the standard protocol called PARSEVAL
(Black et al, 1991), takes into account the brack-
eting and labeling of nodes. The unlabeled attache-
ment score [UAS] evaluates the quality of unlabeled
5We used the version adapted to French in
the software Bonsai (Candito and Crabbe?, 2009):
http://alpage.inria.fr/statgram/frdep/fr stat dep parsing.html.
The original version is available at:
http://code.google.com/p/berkeleyparser/. We trained the
parser as follows: right binarization, no parent annotation, six
split-merge cycles and default random seed initialisation (8).
6Wapiti can be found at http://wapiti.limsi.fr/. It was con-
figured as follows: rprop algorithm, default L1-penalty value
(0.5), default L2-penalty value (0.00001), default stopping cri-
terion value (0.02%).
7Available at http://igm.univ-
mlv.fr/?mconstan/research/software/.
8We used the following mathematical libraries PETSc et
TAO, freely available at http://www.mcs.anl.gov/petsc/ and
http://www.mcs.anl.gov/research/projects/tao/
9Evalb tool available at http://nlp.cs.nyu.edu/evalb/. We
also used the evaluation by category implemented in the class
EvalbByCat in the Stanford Parser.
209
dependencies between words of the sentence10. And
finally, the Leaf-Ancestor score [LA]11 (Sampson,
2003) computes the similarity between all paths (se-
quence of nodes) from each terminal node to the root
node of the tree. The global score of a generated
parse is equal to the average score of all terminal
nodes. Punctuation tokens are ignored in all met-
rics. The quality of MWE identification was evalu-
ated by computing the F1 score on MWE nodes. We
also evaluated the MWE segmentation by using the
unlabeled F1 score (U). In order to compare both ap-
proaches, parse trees generated by BKYc were auto-
matically transformed in trees with the same MWE
annotation scheme as the trees generated by BKY.
In order to establish the statistical significance of
results between two parsing experiments in terms of
F1 and UAS, we used a unidirectional t-test for two
independent samples12. The statistical significance
between two MWE identification experiments was
established by using the McNemar-s test (Gillick
and Cox, 1989). The results of the two experiments
are considered statistically significant with the com-
puted value p < 0.01.
6.2 Standalone Multiword recognition
The results of the standalone MWE recognizer are
given in table 3. They show that the lexicon-based
system (lex) reaches the best score. Accuracy is im-
proved by an absolute gain of +6.7 points as com-
pared with BKY parser. The strictly endogenous
system has a +4.9 point absolute gain, +5.4 points
when collocations are added. That shows that most
of the work is done by fully automatically acquired
features (as opposed to features coming from a man-
ually constructed lexicon). As expected, lexicon-
based features lead to a 5.3 point recall improve-
ment (with respect to non-lexicon based features)
whereas precision is stable. The more precise sys-
tem is the base one because it almost solely detects
compounds present in the training corpus; neverthe-
less, it is unable to capture new MWEs (it has the
10This score is computed by using the tool available at
http://ilk.uvt.nl/conll/software.html. The constituent trees are
automatically converted into dependency trees with the tool
Bonsai.
11Leaf-ancestor assessment tool available at
http://www.grsampson.net/Resources.html
12Dan Bikel?s tool available at
http://www.cis.upenn.edu/?dbikel/software.html.
lowest recall). BKY parser has the best recall among
the non lexicon-based systems, i.e. it is the best one
to discover new compounds as it is able to precisely
detect irregular syntactic structures that are likely to
be MWEs. Nevertheless, as it does not have a lex-
icalized strategy, it is not able to filter out incorrect
candidates; the precision is therefore very low (the
worst).
P R F1 F1 ? 40 U
base 78.0 68.3 72.8 71.2 74.3
endo 75.5 74.5 75.0 74.0 76.3
coll 76.6 74.4 75.5 74.9 77.0
lex 76.0 79.8 77.8 77.8 79.0
all 76.2 79.2 77.7 77.3 78.8
BKY 67.6 75.1 71.1 70.7 72.5
Stanford* - - - 70.1 -
DP-TSG* - - - 71.1 -
Table 3: MWE identification with CRF: base are the
features corresponding to token properties and word n-
grams. The differences between all systems are statisti-
cally significant with respect to McNemar?s test (Gillick
and Cox, 1989), except lex/all and all/coll;
lex/coll is ?border-line?. The results of the systems
based on the Stanford Parser and the Tree Substitution
Parser (DP-TSG) are reported from (Green et al, 2011).
6.3 Combination of Multiword Expression
Recognition and Parsing
We tested and compared the two proposed dis-
criminative strategies by varying the sets of MWE-
dedicated features. The results are reported in ta-
ble 4. Table 5 compares the parsing systems, by
showing the score differences between each of the
tested system and the BKY parser.
Strat. Feat. Parser F1 LA UAS F1(MWE)
- - BKY 80.61 92.91 82.99 71.1
pre - BKYc 75.47 91.10 76.74 0.0
pre endo BKYc 80.23 92.69 83.62 74.9
pre coll BKYc 80.32 92.73 83.77 75.5
pre lex BKYc 80.66 92.81 84.16 77.4
pre all BKYc 80.51 92.77 84.05 77.2
post endo BKY 80.87 92.94 83.49 72.9
post coll BKY 80.71 92.85 83.16 71.2
post lex BKY 81.08 92.98 83.98 74.5
post all BKY 81.03 92.96 83.97 74.3
pre gold BKYc 83.73 93.77 90.08 95.8
Table 4: Parsing evaluation: pre indicates a MWE pre-
grouping strategy, whereas post is a reranking strategy
with n = 50. The feature gold means that we have ap-
plied the parser on a gold MWE segmentation.
210
?F1 ?UAS ?F1(MWE)
pre post pre post pre post
endo -0.38 +0.26 +0.63 +0.50 +3.8 +1.8
coll -0.29 +0.10 +0.78 +0.17 +4.4 +0.1
lex +0.05 +0.47 +1.17 +0.99 +6.3 +3.4
Table 5: Comparison of the strategies with respect to
BKY parser.
Firstly, we note that the accuracy of the best re-
alistic parsers is much lower than that of a parser
with a golden MWE segmentation13 (-2.65 and -5.92
respectively in terms of F-score and UAS), which
shows the importance of not neglecting MWE recog-
nition in the framework of parsing. Furthermore,
pre-grouping has no statistically significant impact
on the F-score14, whereas reranking leads to a sta-
tistically significant improvement (except for col-
locations). Both strategies also lead to a statisti-
cally significant UAS increase. Whereas both strate-
gies improve the MWE recognition, pre-grouping
is much more accurate (+2-4%); this might be due
to the fact that an unlexicalized parser is limited in
terms of compound identification, even within n-
best analyses (cf. Oracle in table 6). The benefits of
lexicon-based features are confirmed in this experi-
ment, whereas the use of collocations in the rerank-
ing strategy seems to be rejected.
endo coll lex all oracle
n=1 80.61
(71.1)
n=5 80.74 80.88 81.03 81.05 83.17
(71.5) (71.7) (73.4) (73.3) (74.6)
n=20 80.98 80.72 81.09 81.01 84.76
(72.9) (70.6) (73.6) (73.0) (75.5)
n=50 80.87 80.71 81.08 81.03 85.21
(72.9) (71.2) (74.5) (74.3) (76.4)
n=100 80.69 80.53 81.12 80.93 85.54
(72.0) (70.0) (74.4) (73.7) (76.4)
Table 6: Reranker F1 evaluation with respect to n and the
types of features. The F1(MWE) is given in parenthesis.
Table 7 shows the results by category. It indi-
cates that both discriminative strategies are of in-
terest in locating multiword adjectives, determiners
and prepositions; the pre-grouping method appears
to be particularly relevant for multiword nouns and
13The F1(MWE) is not 100% with a golden segmentation be-
cause of tagging errors by the parser.
14Note that we observe an increase of +0.5 in F1 on the de-
velopment corpus with lexicon-based features.
adverbs. However, it performs very poorly in multi-
word verb recognition. In terms of standard parsing
accuracy, the pre-grouping approach has a very het-
erogeneous impact: Adverbial and Adjective Modi-
fier phrases tend to be more accurate; verbal kernels
and higher level constituents such as relative and
subordinate clauses see their accuracy level drop,
which shows that pre-recognition of MWE can have
a negative impact on general parsing accuracy as
MWE errors propagate to higher level constituents.
cat #gold BKY endo lex endo lex
(pre) (pre) (post) (post)
MWET 4 0.0 N/A N/A N/A N/A
MWA 22 37.2 +15.2 +21.3 +0.9 +4.7
MWV 47 62.1 -9.7 -13.2 +1.7 +2.5
MWD 24 62.1 +7.3 +10.2 0.0 +1.2
MWN 860 68.2 +4.0 +7.0 +1.7 +4.2
MWADV 357 72.1 +3.8 +6.4 +3.4 +4.1
MWPRO 31 84.2 -3.5 -0.9 0.0 0.0
MWP 294 79.1 +4.3 +5.8 +0.4 +1.1
MWC 86 85.7 +0.9 +3.7 +0.2 +1.0
Sint 209 47.2 -7.7 -8.7 +0.1 -0.2
AdP 86 48.8 +1.2 +3.0 +3.4 +5.1
Ssub 406 60.8 -1.1 -1.1 -0.3 -0.5
VPpart 541 63.2 -2.8 -2.1 -0.5 -1.6
Srel 408 74.8 -3.4 -3.5 -0.3 -0.6
VPinf 781 75.2 0.0 -0.1 -0.3 -0.3
COORD 904 75.2 +0.2 +0.4 -0.3 -0.4
PP 4906 76.7 -0.8 -0.3 +0.5 +0.7
AP 1482 74.5 +3.2 +3.9 +0.7 +1.6
NP 9023 79.8 -1.1 -0.8 +0.1 +0.2
VN 3089 94.0 -2.0 -1.0 0.0 0.0
Table 7: Evaluation by category with respect to BKY
parser. The BKY column indicates the F1 of BKY parser.
7 Conclusions and Future Work
In this paper, we evaluated two discriminative strate-
gies to integrate Multiword Expression Recognition
in probabilistic parsing: (a) pre-grouping MWEs
with a state-of-the-art recognizer and (b) MWE
identification with a reranker after parsing. We
showed that MWE pre-grouping significantly im-
proves compound recognition and unlabeled depen-
dency annotation, which implies that this strategy
could be useful for dependency parsing. The rerank-
ing procedure evenly improves all evaluation scores.
Future work could consist in combining both strate-
gies: pre-grouping could suggest a set of potential
MWE segmentations in order to make it more flexi-
ble for a parser; final decisions would then be made
by the reranker.
211
Acknowlegments
The authors are very grateful to Spence Green for his
useful help on the treebank, and to Jennifer Thewis-
sen for her careful proof-reading.
References
A. Abeille? and L. Cle?ment and F. Toussenel. 2003.
Building a treebank for French. Treebanks. In A.
Abeille? (Ed.). Kluwer. Dordrecht.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: The case of French. In
ACL.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos, B.
Santorini and T. Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Proceedings of the DARPA Speech
and Natural Language Workshop.
T. Baldwin and K.S. Nam. 2010. Multiword Ex-
pressions. Handbook of Natural Language Process-
ing, Second Edition. CRC Press, Taylor and Francis
Group.
M. -H. Candito and B. Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. Proceedings of IWPT 2009.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05).
M. Constant and A. Sigogne. 2011. MWU-aware Part-
of-Speech Tagging with a CRF model and lexical re-
sources. In Proceedings of the Workshop on Multi-
word Expressions: from Parsing and Generation to the
Real World (MWE?11).
A. Copestake, F. Lambeau, A. Villavicencio, F. Bond,
T. Baldwin, I. Sag, D. Flickinger. 2002. Multi-
word Expressions: Linguistic Precision and Reusabil-
ity. Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002).
B. Courtois. 1990. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise. Vol. 87.
B. Courtois, M. Garrigues, G. Gross, M. Gross, R.
Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-
Montange, M. Silberztein and R. Vive?s. 1997. Dic-
tionnaire e?lectronique DELAC : les mots compose?s bi-
naires. Technical Report. n. 56. LADL, University
Paris 7.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP?89.
S. Green, M.-C. de Marneffe, J. Bauer and C. D. Man-
ning. 2011. Multiword Expression Identification with
Tree Substitution Grammars: A Parsing tour de force
with French. In Empirical Method for Natural Lan-
guage Processing (EMNLP?11).
M. Gross. 1986. Lexicon Grammar. The Representa-
tion of Compound Words. In Proceedings of Compu-
tational Linguistics (COLING?86).
J. Lafferty and A. McCallum and F. Pereira. 2001. Con-
ditional random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML 2001).
T. Lavergne, O. Cappe? and F. Yvon. 2010. Practical Very
Large Scale CRFs. In ACL.
J. Nivre and J. Nilsson. 2004. Multiword units in syntac-
tic parsing. In Methodologies and Evaluation of Mul-
tiword Units in Real-World Applications (MEMURA).
S. Paumier. 2011. Unitex 3.9 documentation.
http://igm.univ-mlv.fr/?unitex.
S. Petrov, L. Barrett, R. Thibaux and D. Klein. 2006.
Learning accurate, compact and interpretable tree an-
notation. In ACL.
C. Ramisch, A. Villavicencio and C. Boitet. 2010. mwe-
toolkit: a framework for multiword expression identi-
fication. In LREC.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the 3rd Workshop on Very Large Corpora.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake and D.
Flickinger. 2002. Multiword Expressions: A Pain in
the Neck for NLP. In CICLING 2002. Springer.
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10).
G. Sampson and A. Babarczy. 2003. A test of the leaf-
ancestor metric for parsing accuracy. Natural Lan-
guage Engineering. Vol. 9 (4).
Seddah D., Candito M.-H. and Crabb B. 2009. Cross-
parser evaluation and tagset variation: a French tree-
bank study. Proceedings of International Workshop
on Parsing Technologies (IWPT?09).
W. Schuler, A. Joshi. 2011. Tree-rewriting models of
multi-word expressions. Proceedings of the Workshop
on Multiword Expressions: from Parsing and Genera-
tion to the Real World (MWE?11).
M. Silberztein. 2000. INTEX: an FST toolbox. Theoret-
ical Computer Science, vol. 231(1).
P. Watrin and T. Franc?ois. 2011. N-gram frequency
database reference to handle MWE extraction in NLP
applications. In Proceedings of the 2011 Workshop on
MultiWord Expressions.
212
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 49?56,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
MWU-aware Part-of-Speech Tagging with a CRF model and lexical
resources
Matthieu Constant
Universite? Paris-Est, LIGM
5, bd Descartes - Champs/Marne
77454 Marne-la-Valle?e cedex 2, France
mconstan@univ-mlv.fr
Anthony Sigogne
Universite? Paris-Est, LIGM
5, bd Descartes - Champs/Marne
77454 Marne-la-Valle?e cedex 2, France
sigogne@univ-mlv.fr
Abstract
This paper describes a new part-of-speech tag-
ger including multiword unit (MWU) identifi-
cation. It is based on a Conditional Random
Field model integrating language-independent
features, as well as features computed from
external lexical resources. It was imple-
mented in a finite-state framework composed
of a preliminary finite-state lexical analysis
and a CRF decoding using weighted finite-
state transducer composition. We showed that
our tagger reaches state-of-the-art results for
French in the standard evaluation conditions
(i.e. each multiword unit is already merged in
a single token). The evaluation of the tagger
integrating MWU recognition clearly shows
the interest of incorporating features based on
MWU resources.
1 Introduction
Part-of-speech (POS) tagging reaches excellent
results thanks to powerful discriminative multi-
feature models such as Conditional Random Fields
(Lafferty et al, 2001), Support Vector Machine
(Gime?nez and Ma?rquez, 2004), Maximum Entropy
(Ratnaparkhi, 1996). Some studies like (Denis and
Sagot, 2009) have shown that featuring these models
by means of external morphosyntactic resources still
improves accuracy. Nevertheless, current taggers
rarely take multiword units such as compound words
into account, whereas they form very frequent lexi-
cal units with strong syntactic and semantic particu-
larities (Sag et al, 2001; Copestake et al, 2002) and
their identification is crucial for applications requir-
ing semantic processing. Indeed, taggers are gen-
erally evaluated on perfectly tokenized texts where
multiword units (MWU) have already been identi-
fied.
Our paper presents a MWU-aware POS tagger
(i.e. a POS tagger including MWU recognition1).
It is based on a Conditional Random Field (CRF)
model that integrates features computed from large-
coverage morphosyntactic lexicons and fine-grained
MWU resources. We implemented it in a finite-state
framework composed of a finite-state lexical ana-
lyzer and a CRF-decoder using weighted transducer
composition.
In section 2, we will first describe statistical tag-
ging based on CRF. Then, in section 3, we will
show how to adapt the tagging models in order to
also identify multiword unit. Next, section 4 will
present the finite-state framework used to implement
the tagger. Section 5 will focus on the description of
our working corpus and the set of lexical resources
used. In section 6, we then evaluate our tagger on
French.
2 Statistical POS tagging with Linear
Chain Conditional Random Fields
Linear chain Conditional Ramdom Fields (CRF) are
discriminative probabilistic models introduced by
(Lafferty et al, 2001) for sequential labelling. Given
an input sequence x = (x1, x2, ..., xN ) and an out-
1This strategy somewhat resembles the popular approach of
joint word segmentation and part-of-speech tagging for Chi-
nese, e.g. (Zhang and Clark, 2008). Moreover, other similar
experiments on the same task for French are reported in (Con-
stant et al, 2011).
49
put sequence of labels y = (y1, y2, ..., yN ), the
model is defined as follows:
P?(y|x) =
1
Z(x) .
N
?
t
K
?
k
?k.fk(t, yt, yt?1, x)
where Z(x) is a normalization factor depending
on x. It is based on K features each of them be-
ing defined by a binary function fk depending on
the current position t in x, the current label yt,
the preceding one yt?1 and the whole input se-
quence x. The feature is activated if a given con-
figuration between t, yt, yt?1 and x is satisfied (i.e.
fk(t, yt, yt?1, x) = 1). Each feature fk is associated
with a weight ?k. The weights are the parameters
of the model. They are estimated during the train-
ing process by maximizing the conditional loglikeli-
hood on a set of examples already labeled (training
data). The decoding procedure consists in labelling
a new input sequence with respect to the model, by
maximizing P (y|x) (or minimizing ?logP (y|x)).
There exist dynamic programming procedures such
as Viterbi algorithm in order to efficiently explore all
labelling possibilities.
Features are defined by combining different prop-
erties of the tokens in the input sequence and the la-
bels at the current position and the preceding one.
Properties of tokens can be either binary or tex-
tual: e.g. token contains a digit, token is capital-
ized (binary property), form of the token, suffix of
size 2 of the token (textual property). Most tag-
gers exclusively use language-independent proper-
ties ? e.g. (Ratnaparkhi, 1996; Toutanova et al,
2003; Gime?nez and Ma?rquez, 2004; Tsuruoka et
al., 2009). It is also possible to integrate language-
dependant properties computed from an external
broad-coverage morphosyntactic lexicon, that are
POS tags found in the lexicon for the given token
(e.g. (Denis and Sagot, 2009)). It is of great interest
to deal with unknown words2 as most of them are
covered by the lexicon, and to somewhat filter the
list of candidate tags for each token. We therefore
added to our system a language-dependent property:
a token is associated with the concatenation of its
possible tags in an external lexicon, i.e. the am-
bibuity class of the token (AC).
2Unknown words are words that did not occur in the training
data.
In practice, we can divide features fk in two
families: while unigram features (uk) do not de-
pend on the preceding tag, i.e. fk(t, yt, yt?1, x) =
uk(t, yt, x), bigram features (bk) depend on both
current and preceding tags, i.e. fk(t, yt, yt?1, x) =
bk(t, yt, yt?1, x). In our practical case, bigrams
exlusively depends on the two tags, i.e. they are in-
dependent from the input sequence and the current
position like in the Hidden Markov Model (HMM)3.
Unigram features can be sub-divided into internal
and contextual ones. Internal features provide solely
characteristics of the current token w0: lexical form
(i.e. its character sequence), lowercase form, suf-
fice, prefix, ambiguity classes in the external lexi-
cons, whether it contains a hyphen, a digit, whether
it is capitalized, all capitalized, multiword. Contex-
tual features indicate characteristics of the surround-
ings of the current token: token unigrams at relative
positions -2,-1,+1 and +2 (w?2, w?1, w+1,w+2); to-
ken bigrams w?1w0, w0w+1 and w?1w+1; ambi-
guity classes at relative positions -2,-1,+1 and +2
(AC?2, AC?1, AC+1,AC+2). The different feature
templates used in our tagger are given in table 2.
Internal unigram features
w0 = X &t0 = T
Lowercase form of w0 = L &t0 = T
Prefix of w0 = P with |P | < 5 &t0 = T
Suffix of w0 = S with |S| < 5 &t0 = T
w0 contains a hyphen &t0 = T
w0 contains a digit &t0 = T
w0 is capitalized &t0 = T
w0 is all capital &t0 = T
w0 is capitalized and BOS4 &t0 = T
w0 is multiword &t0 = T
Lexicon tags AC0 of w0 = A & w0 is multiword &t0 = T
Contextual unigram features
wi = X , i ? {?2,?1, 1, 2} &t0 = T
wiwj = XY , (j, k) ? {(?1, 0), (0, 1), (?1, 1)} &t0 = T
ACi = A & wi is multiword, i ? {?2,?1, 1, 2} &t0 = T
Bigram features
t?1 = T ? &t0 = T
Table 1: Feature templates
3 MWU-aware POS tagging
MWU-aware POS tagging consists in identifying
and labelling lexical units including multiword ones.
3Hidden Markov Models of order n use strong indepen-
dance assumptions: a word only depends on its corresponding
tag, and a tag only depends on its n previous tags. In our case,
n=1.
50
It is somewhat similar to segmentation tasks like
chunking or Named Entity Recognition, that iden-
tify the limits of chunk or Named Entity segments
and classify these segments. By using an IOB5
scheme (Ramshaw and Marcus, 1995), this task is
then equivalent to labelling simple tokens. Each to-
ken is labeled by a tag in the form X+B or X+I,
where X is the POS labelling the lexical unit the to-
ken belongs to. Suffix B indicates that the token is at
the beginning of the lexical unit. Suffix I indicates
an internal position. Suffix O is useless as the end
of a lexical unit corresponds to the beginning of an-
other one (suffix B) or the end of a sentence. Such
procedure therefore determines lexical unit limits, as
well as their POS.
A simple approach is to relabel the training data
in the IOB scheme and to train a new model with the
same feature templates. With such method, most of
multiword units present in the training corpus will
be recognized as such in a new text. The main issue
resides in the identification of unknown multiword
units. It is well known that statistically inferring new
multiword units from a rather small training corpus
is very hard. Most studies in the field prefer finding
methods to automatically extract, from very large
corpus, multiword lexicons, e.g. (Dias, 2003; Caseli
et al, 2010), to be integrated in Natural Language
Processing tools.
In order to improve the number of new multiword
units detected, it is necessary to plug the tagger to
multiword resources (either manually built or auto-
matically extracted). We incorporate new features
computed from such resources. The resources that
we use (cf. section 5) include three exploitable fea-
tures. Each MWU encoded is obligatory assigned
a part-of-speech, and optionally an internal sur-
face structure and a semantic feature. For instance,
the organization name Banque de Chine (Bank of
China) is a proper noun (NPP) with the semantic
feature ORG; the compound noun pouvoir d?achat
(purchasing power) has a syntactic form NPN be-
cause it is composed of a noun (N), a preposition (P)
and a noun (N). By applying these resources to texts,
it is therefore possible to add four new properties
for each token that belongs to a lexical multiword
5I: Inside (segment); O: Outside (segment); B: Beginning
(of segment)
unit: the part-of-speech of the lexical multiword unit
(POS), its internal structure (STRUCT), its semantic
feature (SEM) and its relative position in the IOB
scheme (POSITION). Table 2 shows the encoding
of these properties in an example. The property ex-
traction is performed by a longest-match context-
free lookup in the resources. From these properties,
we use 3 new unigram feature templates shown in
table 3: (1) one combining the MWU part-of-speech
with the relative position; (2) another one depending
on the internal structure and the relative position and
(3) a last one composed of the semantic feature.
FORM POS STRUCT POSITION SEM Translation
un - - O - a
gain - - O - gain
de - - O - of
pouvoir NC NPN B - purchasing
d? NC NPN I -
achat NC NPN I - power
de - - O - of
celles - - O - the ones
de - - O - of
la - - O - the
Banque NPP - B ORG Bank
de NPP - I ORG of
Chine NPP - I ORG China
Table 2: New token properties depending on Multiword
resources
New internal unigram features
POS0/POSITION0 &t0 = T
STRUCT0/POSITION0 &t0 = T
SEM0 &t0 = T
Table 3: New features based on the MW resources
4 A Finite-state Framework
In this section, we describe how we implemented a
unified Finite-State Framework for our MWU-aware
POS tagger. It is organized in two separate clas-
sical stages: a preliminary resource-based lexical
analyzer followed by a CRF-based decoder. The
lexical analyzer outputs an acyclic finite-state trans-
ducer (noted TFST) representing candidate tagging
sequences for a given input. The decoder is in charge
of selecting the most probable one (i.e. the path in
the TFST which has the best probability).
51
4.1 Weighted finite-state transducers
Finite-state technology is a very powerful machin-
ery for Natural Language Processing (Mohri, 1997;
Kornai, 1999; Karttunen, 2001), and in particu-
lar for POS tagging, e.g. (Roche and Schabes,
1995). It is indeed very convenient because it
has simple factorized representations and interest-
ing well-defined mathematical operations. For in-
stance, weighted finite-state transducers (WFST) are
often used to represent probabilistic models such as
Hidden Markov Models. In that case, they map in-
put sequences into output sequences associated with
weights following a probability semiring (R+,+,?,
0, 1) or a log semiring (R ? {??,+?},?log,+,
+?, 0) for numerical stability6. A WFST is a finite-
state automaton which each transition is composed
of an input symbol, an output symbol and a weight.
A path in a WFST is therefore a sequence of consec-
utive transitions of the WFST going from an initial
state to a final state, i.e. it puts a binary relation
between an input sequence and an output sequence
with a weight that is the product of the weights of the
path transitions in a probability semiring (the sum
in the log semiring). Note that a finite-state trans-
ducer is a WFST with no weights. A very nice oper-
ation on WFSTs is composition (Salomaa and Soit-
tola, 1978). Let T1 be a WFST mapping an input
sequence x into an output sequence y with a weight
w1(x, y), and T2 be another WFST mapping a se-
quence y into a sequence z with a weight w2(y, z).
The composition of T1 with T2 results in a WFST T
mapping x into z with a weight w1(x, y).w2(y, z) in
the probability semiring (w1(x, y) + w2(y, z) in the
log semiring).
4.2 Lexical analysis and decoding
The lexical analyzer is driven by lexical resources
represented by finite-state transducers like in (Sil-
berztein, 2000) (cf. section 5) and generates a TFST
containing candidate analyses. Transitions of the
TFST are labeled by a simple token (as input) and
a POS tag (as output). This stage allows for re-
ducing the global ambiguity of the input sentence in
two different ways: (1) tag filtering, i.e. each token
6A semiring K is a 5-tuple (K,?,?, 0?, 1?) where the set K
is equipped with two operations ? and ?; 0? and 1? are their
respective neutral elements. The log semiring is an image of
the probability semiring via the ?log function.
is only assigned its possible tags in the lexical re-
sources; (2) segment filtering, i.e. we only keep lex-
ical multiword units present in the resources. This
implies the use of large-coverage and fine-grained
lexical resources.
The decoding stage selects the most probable path
in the TFST. This involves that the TFST should
be weighted by CRF-based probabilities in order
to apply a shortest path algorithm. Our weighing
procedure consists in composing a WFST encoding
the sentence unigram probabilities (unigram WFST)
and a WFST encoding the bigram probabilities (bi-
gram WFST). The two WFSTs are defined over the
log semiring. The unigram WFST is computed from
the TFST. Each transition corresponds to a (xt,yt)
pair at a given position t in the sentence x. So each
transition is weighted by summing the weights of
the unigram features activated at this position. In our
practical case, bigram features are independent from
the sentence x. The bigram WFST can therefore be
constructed once and for all for the whole tagging
process, in the same way as for order-1 HMM tran-
sition diagrams (Nasr and Volanschi, 2005).
5 Linguistic resources
5.1 French TreeBank
The French Treebank (FTB) is a syntactically an-
notated corpus7 of 569,039 tokens (Abeille? et al,
2003). Each token can be either a punctuation
marker, a number, a simple word or a multiword
unit. At the POS level, it uses a tagset of 14 cate-
gories and 34 sub-categories. This tagset has been
optimized to 29 tags for syntactic parsing (Crabbe?
and Candito, 2008) and reused as a standard in a
POS tagging task (Denis and Sagot, 2009). Below
is a sample of the FTB version annotated in POS.
, PONCT ,
soit CC i.e.
une DET a
augmentation NC raise
de P of
1 , 2 DET 1 , 2
% NC %
par rapport au P+D compared with the
mois NC preceding
pre?ce?dent ADJ month
7It is made of journalistic texts from Le Monde newspaper.
52
Multiword tokens encode multiword units of dif-
ferent types: compound words and named enti-
ties. Compound words mainly include nominals
such as acquis sociaux (social benefits), verbs such
as faire face a` (to face) adverbials like dans l?
imme?diat (right now), prepositions such as en de-
hors de (beside). Some Named Entities are also en-
coded: organization names like Socie?te? suisse de mi-
croe?lectronique et d? horlogerie, family names like
Strauss-Kahn, location names like Afrique du Sud
(South Africa) or New York. For the purpose of our
study, this corpus was divided in three parts: 80%
for training (TRAIN), 10% for development (DEV)
and 10% for testing (TEST).
5.2 Lexical resources
The lexical resources are composed of both mor-
phosyntactic dictionaries and strongly lexicalized
local grammars. Firstly, there are two general-
language dictionaries of simple and multiword
forms: DELA (Courtois, 1990; Courtois et al, 1997)
and Lefff (Sagot, 2010). DELA has been devel-
opped by a team of linguists. Lefff has been au-
tomatically acquired and then manually validated.
It also resulted from the merge of different lexical
sources. In addition, we applied specific manually
built lexicons: Prolex (Piton at al., 1999) contain-
ing toponyms ; others including organization names
and first names (Martineau et al, 2009). Figures on
these dictionaries are detailed in table 4.
Name # simple forms #MW forms
DELA 690,619 272,226
Lefff 553,140 26,311
Prolex 25,190 97,925
Organizations 772 587
First names 22,074 2,220
Table 4: Morphosynctatic dictionaries
This set of dictionaries is completed by a library
of strongly lexicalized local grammars (Gross, 1997;
Silberztein, 2000) that recognize different types of
multiword units such as Named Entities (organiza-
tion names, person names, location names, dates),
locative prepositions, numerical determiners. A lo-
cal grammar is a graph representing a recursive
finite-state transducer, which recognizes sequences
belonging to an algebraic language. Practically, they
describe regular grammars and, as a consequence,
can be compiled into equivalent finite-state trans-
ducers. We used a library of 211 graphs. We man-
ually constructed from those available in the online
library GraalWeb (Constant and Watrin, 2007).
5.3 Lexical resources vs. French Treebank
In this section, we compare the content of the re-
sources described above with the encodings in the
FTB-DEV corpus. We observed that around 97,4%
of lexical units encoded in the corpus (excluding
numbers and punctuation markers) are present in our
lexical resources (in particular, 97% are in the dic-
tionaries). While 5% of the tokens are unknown (i.e.
not present in the training corpus), 1.5% of tokens
are unknown and not present in the lexical resources,
which shows that 70% of unknown words are cov-
ered by our lexical resources.
The segmentation task is mainly driven by the
multiword resources. Therefore, they should match
as much as possible with the multiword units en-
coded in the FTB. Nevertheless, this is practically
very hard to achieve because the definition of MWU
can never be the same between different people as
there exist a continuum between compositional and
non-compositional sequences. In our case, we ob-
served that 75.5% of the multiword units in the FTB-
DEV corpus are in the lexical resources (87.5% in-
cluding training lexicon). This means that 12.5%
of the multiword tokens are totally unknown and,
as a consequence, will be hardly recognized. An-
other significant issue is that many multiword units
present in our resources are not encoded in the FTB.
For instance, many Named Entities like dates, per-
son names, mail addresses, complex numbers are ab-
sent. By applying our lexical resources8 in a longest-
match context-free manner with the platform Unitex
(Paumier, 2011), we manually observed that 30% of
the multiword units found were not considered as
such in the FTB-DEV corpus.
6 Experiments and Evaluation
We firstly evaluated our system for standard tag-
ging without MWU segmentation and compare it
with other available statistical taggers that we all
trained on the FTB-TRAIN corpus. We tested the
8We excluded local grammars recognizing dates, person
names and complex numbers.
53
well-known TreeTagger (Schmid, 1994) based on
probabilistic decision trees, as well as TnT (Brants,
2000) implementing second-order Hidden Markov.
We also compared our system with two existing
discriminative taggers: SVMTool (Gime?nez and
Ma?rquez, 2004) based on Support Vector Models
with language-independent features; MElt (Denis
and Sagot, 2009) based on a Maximum Entropy
model also incorporating language-dependent fea-
ture computed from an external lexicon. The lexicon
used to train and test MElt included all lexical re-
sources9 described in section 5. For our CRF-based
system, we trained two models with CRF++10: (a)
STD using language-independent template features
(i.e. excluding AC-based features); (b) LEX using
all feature templates described in table 2. We note
CRF-STD and CRF-LEX the two related taggers
when no preliminary lexical analysis is performed;
CRF-STD+ and CRF-LEX+ when a lexical analy-
sis is performed. The lexical analysis in our exper-
iment consists in assigning for each token its possi-
ble tags found in the lexical resources11 . Tokens not
found in the resources are assigned all possible tags
in the tagset in order to ensure the system robust-
ness. If no lexical analysis is applied, our system
constructs a TFST representing all possible analyzes
over the tagset. The results obtained on the TEST
corpus are summed up in table 5. Column ACC in-
dicates the tagger accuracy in percentage. We can
observe that our system (CRF-LEX+) outperforms
the other existing taggers, especially MElt whose
authors claimed state-of-the-art results for French.
We can notice the great interest of a lexical analysis
as CRF-STD+ reaches similar results as a MaxEnt
model based on features from an external lexicon.
We then evaluated our MWU-aware tagger
trained on the TRAIN corpus whose complex tokens
have been decomposed in a sequence of simple to-
kens and relabeled in the IOB representation. We
used three different sets of feature templates lead-
9Dictionaries were all put together, as well as with the result
of the application of the local grammars on the corpus.
10CRF++ is an open-source toolkit to train and test CRF mod-
els (http://crfpp.sourceforge.net/). For training, we set the cut-
off threshold for features to 2 and the C value to 1. We also used
the L2 regularization algorithm.
11Practically, as the tagsets of the lexical resources and the
FTB were different, we had to first map tags used in the dictio-
naries into tags belonging to the FTB tagset.
Tagger Model ACC
TnT HMM 96.3
TreeTagger Decision trees 96.4
SVMTool SVM 97.2
CRF-STD CRF 97.4
MElt MaxEnt 97.6
CRF-STD+ CRF 97.6
CRF-LEX CRF 97.7
CRF-LEX+ CRF 97.7
Table 5: Comparison of different taggers for French
ing to three CRF models: CRF-STD,CRF-LEX and
CRF-MWE. The two first ones (STD and LEX) use
the same feature templates as in the previous ex-
periment. MWE includes all feature templates de-
cribed in sections 2 and 3. CRF-MWE+ indicates
that a preliminary lexical analysis is performed be-
fore applying CRF-MWE decoding. The lexical anal-
ysis is achieved by assigning all possible tags of sim-
ple tokens found in our lexical resources, as well as
adding, in the TFST, new transitions corresponding
to MWU segments found in the lexical resources.
We compared the three models with a baseline and
SVMTool that have been learnt on the same training
corpus. The baseline is a simple context-free lookup
in the training MW lexicon, after a standard CRF-
based tagging with no MW segmentation. We eval-
uated each MWU-aware tagger on the decomposed
TEST corpus and computed the f-score, combining
precision and recall12. The results are synthesized
in table 6. The SEG column shows the segmentation
f -score solely taking into account the segment limits
of the identified lexical unit. The TAG column also
accounts for the label assigned. The first observation
is that there is a general drop in the performances for
all taggers, which is not a surprise as regards with
the complexity of MWU recognition (97.7% for the
best standard tagger vs. 94.4% for the best MWU-
aware tagger). Clearly, MWU-aware taggers which
models incorporate features based on external MWU
resources outperform the others. Nevertheless, the
scores for the identification and the tagging of the
MWUs are still rather low: 91%-precision and 71%
recall. We can also see that a preliminary lexical
analysis slightly lower the scores, which is due to
12f-score f = 2prp+r where p is precision and r is recall.
54
missing MWUs in the resources and is a side effect
of missing encodings in the corpus.
Tagger Model TAG SEG
Baseline CRF 91.2 93.6
SVMTool SVM 92.1 94.7
CRF-STD CRF 93.7 95.8
CRF-LEX CRF 93.9 95.9
CRF-MWE CRF 94.4 96.4
CRF-MWE+ CRF 94.3 96.3
Table 6: Evaluation of MWU-aware tagging
With respect to the statistics given in section 5.3,
it appears clearly that the evaluation of MWU-aware
taggers is somewhat biased by the fact that the def-
inition of the multiword units encoded in the FTB
and the ones listed in our lexical resources are not
exactly the same. Nevertheless, this evaluation that
is the first in this context, brings new evidences
on the importance of multiword unit resources for
MWU-aware tagging.
7 Conclusions and Future Work
This paper presented a new part-of-speech tagger in-
cluding multiword unit identification. It is based on
a CRF model integrating language-independent fea-
tures, as well as features computed from external
lexical resources. It was implemented in a finite-
state framework composed of a preliminary finite-
state lexical analysis and a CRF decoding using
weighted finite-state transducer composition. The
tagger is freely available under the LGPL license13.
It allows users to incorporate their own lexicons in
order to easily integrate it in their own applications.
We showed that the tagger reaches state-of-the-art
results for French in the standard evaluation environ-
ment (i.e. each multiword unit is already merged in
a single token). The evaluation of the tagger inte-
grating MWU recognition clearly shows the interest
of incorporating features based on MWU resources.
Nevertheless, as there exist some differences in the
MWU definitions between the lexical resources and
the working corpus, this first experiment requires
further investigations. First of all, we could test our
tagger by incorporating lexicons of MWU automat-
ically extracted from large raw corpora in order to
13http://igm.univ-mlv.fr/?mconstan/research/software
deal with low recall. We could as well combine the
lexical analyzer with a Named Entity Recognizer.
Another step would be to modify the annotations of
the working corpus in order to cover all MWU types
and to make it more homogeneous with our defini-
tion of MWU. Another future work would be to test
semi-CRF models that are well-suited for segmenta-
tion tasks.
References
A. Abeille?, L. Cle?ment and F. Toussenel. 2003. Building
a treebank for French. in A. Abeille? (ed), Treebanks,
Kluwer, Dordrecht.
T. Brants. 2000. TnT - A Statistical Part-of-Speech Tag-
ger. In Proceedings of the Sixth Applied Natural Lan-
guage Processing Conference (ANLP 2000), 224?231.
H. Caseli, C. Ramisch, M. das Graas Volpe Nunes, A.
Villavicencio. 2010. Alignment-based extraction
of multiword expressions. Language Resources and
Evaluation, Springer, vol. 44(1), 59?77.
M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Si-
gogne, S. Billot. 2011. Inte?grer des connaissances lin-
guistiques dans un CRF : application a` l?apprentissage
d?un segmenteur-e?tiqueteur du franc?ais. In Actes de la
Confe?rence sur le traitement automatique des langues
naturelles (TALN?11).
M. Constant and P. Watrin. 2007. Networking Mul-
tiword Units. In Proceedings of the 6th Interna-
tional Conference on Natural Language Processing
(GoTAL?08), Lecture Notes in Artificial Intelligence,
Springer-Verlag, vol. 5221: 120 ? 125.
A. Copestake, F. Lambeau, A. Villavicencio, F. Bond, T.
Baldwin, I. A. Sag and D. Flickinger. 2002. Multi-
word expressions: linguistic precision and reusability.
In Proceedings of the Third conference on Language
Resources and Evaluation (LREC? 02), 1941 ? 1947.
B. Courtois. 1990. Un syste`me de dictionnaires
e?lectroniques pour les mots simples du franc?ais.
Langue Franc?aise, vol. 87: 1941 ? 1947.
B. Courtois, M. Garrigues, G. Gross, M. Gross, R.
Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-
Montange, M. Silberztein, R. Vive?s. 1990. Dictio-
nnaire e?lectronique DELAC : les mots compose?s bi-
naires. Technical report, LADL, University Paris 7,
vol. 56.
B. Crabbe? and M. -H. Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franais. In Pro-
ceedings of Traitement des Langues Naturelles (TALN
2008).
P. Denis et B. Sagot. 2009. Coupling an annotated cor-
pus and a morphosyntactic lexicon for state-of-the-art
55
POS tagging with less human effort. In Proceedings
of the 23rd Pacific Asia Conference on Language, In-
formation and Computation (PACLIC 2009).
G. Dias. 2003. Multiword Unit Hybrid Extraction. In
proceedings of the Workshop on Multiword Expres-
sions of the 41st Annual Meeting of the Association
of Computational Linguistics (ACL 2003), 41?49.
J. Gime?nez and L. Ma?rquez. 2004. SVMTool: A gen-
eral POS tagger generator based on Support Vector
Machines. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
M. Gross. 2007. The construction of local grammars. In
E. Roche and Y. Schabes (eds.). Finite-State Language
Processing. The MIT Press, Cambridge, Mass. 329?
352
L. Karttunen. 2001. Applications of Finite-State Trans-
ducers in Natural Language Processing. In proceed-
ings of the 5th International Conference on Implemen-
tation and Application of Automata (CIAA 2000). Lec-
ture Notes in Computer Science. vol. 2088, Springer,
34?46
A. Kornai (Ed.). 1999. Extended Finite State Models of
Language. Cambridge University Press
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random Fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML 2001), 282?289.
C. Martineau, T. Nakamura, L. Varga and Stavroula Voy-
atzi. 2009. Annotation et normalisation des entite?s
nomme?es. Arena Romanistica. vol. 4:234?243.
M. Mohri 1997. Finite-state transducers in language
and speech processing. Computational Linguistics 23
(2):269?311.
A. Nasr, A. Volanschi. 2005. Integrating a POS Tagger
and a Chunker Implemented as Weighted Finite State
Machines. Finite-State Methods and Natural Lan-
guage Processing, Lecture Notes in Computer Sci-
ence, vol. 4002, Springer 167?178.
S. Paumier. 2011. Unitex 2.1 user manual.
http://igm.univ-mlv.fr/?unitex.
O. Piton, D. Maurel, C. Belleil. 1999. The Prolex Data
Base : Toponyms and gentiles for NLP. In proceedings
of the Third International Workshop on Applications
of Natural Language to Data Bases (NLDB?99), 233?
237.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the 3rd Workshop on Very Large Corpora, 88 ? 94.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 1996), 133 ? 142.
E. Roche, Y. Schabes. 1995. Deterministic part-of-
speech tagging with finite-state transducers. Compu-
tational Linguistics, MIT Press, vol. 21(2), 227?253
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, D.
Flickinger. 2001. Multiword Expressions: A Pain in
the Neck for NLP. In Proceedings of the 3rd Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2002), 1?15
B. Sagot. 2010. The Lefff, a freely available, accurate
and large-coverage lexicon for French. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10).
A. Salomaa, M. Soittola. 1978. Automata-Theoretic As-
pects of Formal Power Series. Springer-Verlag.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. Proceedings of International
Conference on New Methods in Language Processing.
M. Silberztein. 2000. INTEX: an FST toolbox. Theoret-
ical Computer Science, vol. 231 (1): 33?46.
K. Toutanova, D. Klein, C. D. Manning, Y. Yoram
Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. Proceedings of
HLT-NAACL 2003, 252 ? 259.
Y. Tsuruoka, J. Tsujii, S. Ananiadou. 2009. Fast Full
Parsing by Linear-Chain Conditional Random Fields.
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2009), 790?798.
Y. Zhang, S. Clark. 2008. Joint Word Segmentation and
POS Tagging Using a Single Perceptron. Proceedings
of ACL 2008, 888 ? 896.
56
