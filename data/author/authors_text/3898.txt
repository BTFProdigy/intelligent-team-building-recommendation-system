Overview of Patent Retrieval Task at NTCIR-3 
Makoto Iwayama 
Tokyo Institute of 
Technology/Hitachi Ltd. 
iwayama@crl.hitachi.co.jp 
Atsushi Fujii 
University of Tsukuba/Japan 
Science and Technology Corp.
fujii@slis.tsukuba.ac.jp 
Noriko Kando 
National Institute of 
Informatics 
kando@nii.ac.jp 
Akihiko Takano 
National Institute of 
Informatics 
aki@acm.org 
 
 
Abstract 
We describe the overview of patent re-
trieval task at NTCIR-3. The main task was 
the technical survey task, where participants 
tried to retrieve relevant patents to news ar-
ticles. In this paper, we introduce the task 
design, the patent collections, the character-
istics of the submitted systems, and the re-
sults overview. We also arranged the free-
styled task, where participants could try 
anything they want as far as the patent col-
lections were used. We describe the brief 
summaries of the proposals submitted to the 
free-styled task. 
1 Introduction 
In the field of information retrieval, there have 
been held successive evaluation workshops, such 
as TREC [8], CREF [1], and NTCIR [5], to build 
and utilize various kinds of test collections. In the 
Third NTCIR Workshop (NTCIR-3), which was 
held from June 2001 to December 2003, a serious 
effort was first made in the ?Patent Retrieval Task? 
to explore information retrieval targeting patent 
documents. 
The goal of Patent Retrieval Task is to provide 
test collections for enhancing research on patent 
information processing, from patent retrieval to 
patent mining. Although there exist many com-
mercial patent retrieval systems and services, pat-
ent retrieval has not been paid much attention in 
the research field of information retrieval. One of 
the reasons is the lack of test collection on patent. 
TREC used patent documents as a part of the 
document collections, but there was no treatment 
specially applied to the patent collection. 
In SIGIR2000, the first workshop on patent re-
trieval was held [4] and there were many fruitful 
discussions on the current status and future direc-
tions of patent retrieval. The workshop convinced 
us that there was the need of test collections spe-
cifically for patents. 
We then asked for PATOLIS Co. [7] to provide 
patent collections for the patent retrieval task. Con-
sequently, we could release three kinds of patent 
collections; those were two years? Japanese full 
texts, five years? Japanese abstracts, and five 
years? English abstracts. At the same time, we 
could fortunately have cooperation with JIPA (Ja-
pan Intellectual Property Association) [3] in creat-
ing search topics and assessing the relevance. 
Since each member of JIPA belongs to the intellec-
tual property division in her/his company, they are 
all experts in patent searching. All the above 
contributions enabled us to kick off the first 
evaluation workshop designed for patent 
information processing. 
There are various phases and aspects in patent 
information processing. For example, various 
kinds of users (researchers, patent searchers, busi-
ness managers, and so on) search patents for vari-
ous purposes (technical survey, finding conflicting 
applications, buying/selling patents, and so on). 
Corresponding to each situation, an appropriate 
search model should be developed. The standard of 
the relevance judgments may also depend on each 
situation. In some cases, retrieving relevant patents 
is not enough but further analysis on the retrieved 
patents might be necessary. For example, creating 
a patent map of a product would clarify the patent 
relations between the techniques used to make the 
product. Cross-lingual patent retrieval is also im-
portant when applying patents to foreign countries. 
All of these are within scope of our project and this 
task was the first step toward our goal. 
2 Task Design 
In this workshop, we focused on a simple task of 
technical survey. End-users we assumed in the task 
were novice users, for example, business managers. 
The major reason of adopting such general task 
was that we could only use the two years? full texts 
that were not enough for trying more patent-
oriented task like finding conflicting applications 
from patents. 
 
 
Figure 1: Scenario of technology survey 
 
To fit the task to a real situation, we used Japa-
nese news articles as the original sources of search 
topics, so the task was conducting cross-database 
retrieval, searching patents by news articles. The 
task assumed the following situation that is de-
picted in Figure 1. When a business manager looks 
through news articles and is interested in one of 
them, she/he clips it out and asks a searcher to find 
related patents to the clipping. The manager passes 
the clipping to the searcher along with her/his 
memorandum, and this clipping with memorandum 
became the search topic in this task. The memo-
randum helps the searcher to have the exact infor-
mation need the manager has, when the clipping 
contains non-relevant topics or the clipping has 
little description on the information need. Task 
participants played the role of the searcher and 
tried to retrieve relevant patents to the clipping. 
Since the purpose of the searching was technical 
survey, the claim part in patent was not treated 
specifically in assessing the relevance. Patent 
documents were treated as if those were technical 
papers. 
Cross-database retrieval itself is so general that 
techniques investigated in the task can be applied 
to various combinations of databases. This is an-
other purpose of the task. 
We prepared search topics in four languages, 
Japanese, English, Korean, and Chinese (both tra-
ditional and simplified). Participants could try 
cross-lingual patent retrieval by using one of the 
non-Japanese topics. Unfortunately, only two 
groups submitted cross-lingual results and both of 
them used English topics. 
In addition to the technical survey task ex-
plained so far, we arranged the optional task, 
where participants could try anything they want as 
far as they used the patent collections provided. 
One of the purposes of this free-styled task is to 
explore next official tasks. 
3 Characteristics of Patent Applications 
In this section, we briefly review the characteristics 
of patent applications (patent documents). 
? There are structures, for example, claims, 
purposes, effects, and embodiments of the 
invention. 
? Although the claim part is the most impor-
tant in patent, it is written in an unusual style 
especially for Japanese patent; all the sub-
topics are written in single sentence. 
? To enlarge the scope of invention, vague or 
general terms are often used in claims. 
? Patents include much technical terminology. 
Applicants may define and use their original 
terms not used in other patents. 
? There are large variations in length. The 
longest patent in our collections contains 
about 30,000 Japanese words! 
? The search models would be significantly 
different between industries, for example, 
between chemical / pharmaceutical indus-
tries and computers / machinery / electric 
industries. 
? Classification exists. IPC (International Pat-
ent Classification) is the most popular one. 
? The criterion of evaluation depends on the 
purpose of searching. For example, high re-
call is required for finding conflicting appli-
cations. 
? In some industries, images are important to 
judge the relevance. 
Our task focused on few of the above character-
istics. We treated patent documents as technical 
documents rather than legal statements, so we did 
not distinguish between the claim part and the oth-
ers in assessing the relevance. High recall was not 
necessary, so we used the standard averaged preci-
sion to evaluate the results. Few groups used struc-
tures and classifications. Images were not included 
in the patent collections provided. 
4 Patent Collections 
PATOLIS Co. provided and we released the fol-
lowing patent collections. 
? kkh: Publication of unexamined patent ap-
plications (1998, 1999) (in Japanese) 
? jsh: JAPIO Patent Abstracts (1995?1999) 
(in Japanese) 
? paj: Patent Abstracts Japan (1995? 1999) (in 
English) 
?Kkh? contains full texts of unexamined patent 
applications in Japanese. Images were eliminated. 
?Jsh? contains human edited abstracts in Japanese. 
Although all the texts in ?kkh? have the abstracts 
written by the applicants, experts in JAPIO (Japan 
Patent Information Organization) [2] short-
ened/lengthened about half of them to fit the length 
within about 400 Japanese characters. They also 
normalized technical terms if necessary. ?Paj? is 
English translation of ?jsh?. 
translation by  
human experts 
 
modif ication of the original abstracts by 
human experts (JAPIO) 
 
kkh: (98,99) 
Publication of 
unexamined patent 
applications  
(in Japanese) 
 
jsh: (95-99) 
JAPIO Patent 
Abstracts 
(in Japanese) 
 
paj: (95-99) 
Patent Abstracts 
Japan 
(in English) 
 
 
Figure 2: Relationships between the patent col-
lections 
 
Figure 2 shows the relationships between these 
three collections. Here, we see parallel relations, 
for example, full texts vs. abstracts, original ab-
stracts vs. edited abstracts, and Japanese abstracts 
vs. English abstracts. Researchers can use these 
parallel collections for various purposes, for exam-
ple, finding rules of abstracting, creating a term 
normalization dictionary, acquiring translation 
knowledge, and so on. 
Table 1 summarizes the characteristics of the 
three collections. 
 
 kkh jsh paj 
Type Full text Abstract Abstract 
Language Japanese Japanese English 
Years 98,99 95-99 95-99 
Number of 
documents
697,262 1,706,154 1,701,339
Bytes 18139M 1883M 2711M 
 
Table 1: Characteristics of the patent collections 
5 Topics 
JIPA members created topics, six for the dry run 
and 25 for the formal run. Since the topics for the 
dry run were substantially revised after the dry run, 
we decided to re-use those in the formal run. In 
consequence, we had the total 31 topics for the 
formal run. 
Figure 3 is an example of the topics in English 
and Table 2 shows the explanations of the fields in 
the topics. In our task, <ARTICLE> and 
<SUPPLEMENT> correspond to the news clipping 
and the memorandum respectively. 
The topics also contain <DESCRIPTION> and 
<NARRATIVE> fields we are familiar with. Since 
many NTCIR tasks already have the results for 
using <DESCRIPTION> and <NARRATIVE> 
fields, we can compare our results of using these 
fields with the results of other tasks. 
Along with the grade of relevance (i.e., ?A?, 
?B?, ?C?, or ?D?), each judged patent has a mark 
(?S?, ?J?, or ?U?) representing the origin from 
which the patent was retrieved. Table 3 explains 
about the marks. For example, a document with 
?BJ? means that the document was judged as ?par-
tially relevant? (i.e. ?B-?) and only found by ex-
perts in their preliminary search (i.e., ?-J?). 
Here, note that all the submitted runs contrib-
uted to collecting the ?S? patents, but only the top 
30 patents for each run were used. Note also that 
we can restore the patent set retrieved by the man-
ual search (i.e., ?PJ? set) by collecting ?J? and ?U? 
patents. 
 
 
<TOPIC><NUM>P004</NUM><LANG>EN</LANG> 
<PURPOSE>technology survey</PURPOSE> 
<TITLE>Device to judge relative merits by comparing 
codes such as barcodes with each other</TITLE> 
<ARTICLE> 
<A-DOC> 
<A-DOCNO>JA-981031179</A-DOCNO> 
<A-LANG>JA</A-LANG> 
<A-SECTION>Society</A-SECTION> 
<A-AE>No</A-AE> 
<A-WORDS>189</A-WORDS> 
<A-HEADLINE>BANDAI lost a lawsuit for piracy filed by 
EPOCH at Tokyo District Court</A-HEADLINE> 
<A-DATE>1998-10-31</A-DATE> 
<A-TEXT>In settlement of the lawsuit filed by EPOCH 
INC., the toy manufacturer, against BANDAI CO., LTD. As 
compensation of 264 million for damages for infringement 
of a card game patent, the Tokyo District Court ordered 
BANDAI to pay about 114 million on the 30th. The presid-
ing judge, Mr. Yoshiyuki Mori, indicated that some func-
tions including key operation for the "Super Barcode 
Wars" mini game machine manufactured and sold by BANDAI 
CO., LTD. in July, 1992 to March, 1993 fell under the 
"technical range of a patent licensed to EPOCH 
INC.".</A-TEXT> 
</A-DOC> 
</ARTICLE> 
<SUPPLEMENT>Determination of victory or defeat by com-
paring each other's values based on codes from barcode 
readings does not conflict with the patent.</SUPPLEMENT> 
<DESCRIPTION>What kind of devices determines leaders or 
victors by reading several codes such as barcodes and 
comparing the values corresponding to these 
codes?</DESCRIPTION> 
<NARRATIVE>"Super Barcode Wars" is a type of mini game 
machine where recorded barcodes are read in cards fea-
turing characters and the game proceeds in semi-real 
time by operating offence and defense keys. Sample codes 
include barcodes and magnetic codes, but shall not be 
defined as limited only to these.</NARRATIVE> 
<CONCEPT>Sign, barcode, code, superiority or inferior-
ity, victory or defeat, comparison, judgment</CONCEPT> 
<PI>PATENT-KKH-G-H01-333373</PI> 
</TOPIC> 
 
Figure 3: Example of the topics 
 
 
Field Explanation 
<LANG> Language code 
<PURPOSE> Purpose of search 
<TITLE> Concise representation 
of search topic 
<ARTICLE> MAINICHI news article 
in NTCIR format 
<SUPPLEMENT> Supplemental informa-
tion of news article 
<DESCRIPTION> Short description of 
search topic 
<NARRATIVE> Long description of 
search topic 
<CONCEPT> List of keywords 
<PI> Original patents of news 
article 
 
Table 2: Explanations of the fields in topics 
6 
6.1 
Results Overview 
Participants 
Eight groups submitted the 36 runs. One group 
submitted runs only for pooling. We briefly de-
scribe the characteristics of each group. Refer to 
the proceedings of Patent Retrieval Task [6] for 
each detail. 
LAPIN: This group focused on the ?term distil-
lation? in cross-database retrieval, where the dif-
ference between the term frequency in source 
database and that in target database was integrated 
into the overall term weighting. 
SRGDU: This group tried several pseudo rele-
vance feedback methods in the context of patent 
retrieval. The proposed method using Taylor for-
mula was compared with the traditional Rocchio 
method. 
daikyo: This group made long gram-based in-
dex from the patent collections. Compared with the 
traditional gram-based indexing, proposed method 
produce more compact index. 
DTEC: This group searched various kinds of 
abstracts rather than full texts, and compared the 
effectiveness of those. The abstracts were JAPIO 
patent abstracts and the combinations of ?title?, 
?applicant?s abstract?, and ?claims?. Manual and 
automatic runs were compared. 
DOVE: This group also submitted manual and 
automatic runs. In the manual runs, non-relevant 
passages in <ARTICLE> were eliminated manu-
ally. 
IFLAB: This group evaluated their cross-
lingual IR system PRIME through several mono-
lingual runs. They also evaluated their translation 
extraction method by using Japanese-US patent 
families, which were not provided in this task. 
brkly: This group submitted both monolingual 
and cross-lingual runs. In the cross-lingual runs, 
words in English topics were translated into Japa-
nese words by using English-Japanese dictionary 
automatically created by the aligned bilingual cor-
pus (i.e., ?paj? and ?jsh?). Their method of creating 
the dictionary is based on word co-occurrence with 
the association measure. 
sics: This group also submitted cross-lingual 
runs, where they automatically created a cross-
lingual thesaurus form the aligned bilingual corpus, 
?paj? and ?jsh?, and used the thesaurus for word-
based query translation. The Random Indexing 
vector-space technique was used to extract the 
cross-lingual thesaurus. Note that, in both the 
?sics? and the ?brkly? groups, there was no mem-
ber who understands Japanese. 
6.2 
6.3 
6.4 
7 
Recall/Precision 
The recall/precision graphs of the mandatory runs 
are shown in Figure 4, and those of the optional 
runs in Figure 5. In each figure, there are both re-
sults for the strict relevance (?A?) and the relaxed 
relevance (?A? + ?B?). For each run in the figures, 
brief system description is specified; the descrip-
tion includes the searching mode (automatic or 
manual), the topic fields used in query construction, 
and the topic language. 
Topic-by-topic Results 
Figure 6 shows the median of the average preci-
sions for each topic. Figure 7 shows the breakdown 
of the relevance judgments. Detailed analysis on 
each topic will be given by JIPA, where it will be 
discussed about the reasons why systems could not 
find some patents human experts found and vise 
versa. 
Recall of the relevant patents retrieved in 
the preliminary human search 
Figure 8 shows the recall of the relevant patents 
retrieved in the preliminary human search. In the 
process of making pool, we used only the top 30 
documents for each run. Here, we extracted more 
documents from each run and investigated how 
many human retrieving relevant patents could be 
covered by the systems. 
Optional (Free-styled) Task 
The following two groups applied to the optional 
task. Refer to the proceedings of Patent Retrieval 
Task [6] for each detail. 
CRL: This group investigated the method of 
extracting various rules from the existing align-
ments in patents. The ?diff? command of UNIX 
was used to find the alignments between JAPIO 
patent abstracts and the original abstracts by appli-
cants, between claims and embodiments, and be-
tween different claims in an application. 
TIT: This group focused on the unusual style of 
Japanese claims, and tried to automatically struc-
ture the claims to raise the readability of claims. 
Rhetorical structure analysis was applied for this 
purpose. 
8 Summary and Future Directions 
In this paper, we described the overview of patent 
retrieval task at NTCIR-3. We are planning to con-
tinue our effort for the next patent retrieval task 
along with the following directions. 
? Longer range of years will be covered. 
? Purpose of search would shift to more real 
one, for example, searching conflicting ap-
plications.  
Acknowledgements 
 
We are grateful to PATOLIS Co. for providing the 
patent collections of this task. We also thank all the 
members of JIPA who created the topics and as-
sessed the relevance. Without their expertise in 
patent, this task would not be realized. Lastly, we 
thank all the participants for their contributions to 
this task. 
References 
[1] CLEF (Cross Language Evaluation Forum) 
(http://clef.iei.pi.cnr.it/) 
[2] JAPIO (Japan Patent Information Organization) 
(http://www.japio.or.jp/) 
[3] JIPA (Japan Intellectual Property Association) 
(http://www.jipa.or.jp/) 
[4] ACM-SIGIR Workshop on Patent Retrieval, or-
ganized by Mun-Kew Leong and Noriko Kando, 
2000. 
(http://research.nii.ac.jp/ntcir/sigir2000ws/) 
[5] NTCIR (NII-NACSIS Test Collection for IR Sys-
tems) 
(http://research.nii.ac.jp/ntcir/index-en.html) 
[6] Proceedings of the Third NTCIR Workshop on 
Research in Information Retrieval, Automatic Text 
Summarization and Question Answering, 2003. 
[7] PATOLIS Co.                                       
 (http://www.patolis.co.jp/e-index.html) 
[8] TREC (Text Retrieval Conference) 
(http://trec.nist.gov/) 
A, mandatory
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN4(A)
DTEC1(M)
DOVE4(M)
brklypat1(A)
daikyo(M)
SRGDU5(A)
IFLAB6(A)
brklypat3(A,E)
A: auto
M: manual
E: English topics
A+B, mandatory
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN4(A)
DOVE4(M)
DTEC1(M)
daikyo(M)
brklypat1(A)
SRGDU3(A)
IFLAB6(A)
brklypat3(A,E)
A: auto
M: manual
E: English topics
 
Figure 4: Recall/Precision of mandatory runs
A, optional
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN1(A,TDNC)
brklypat2(A,DN)
DOVE3(A,DN)
DOVE2(A,D)
SRGDU6(A,DN)
IFLAB2(A,D)
IFLAB3(A,DN)
IFLAB7(A,T)
brklypat4(A,DN,E)
A: auto
M : manual
T: TITLE
D: DESCRIPTION
N: NARRATIVE
C: CONCEPT
E: English Topics
 
 A+B, optional
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
recall
pr
ec
is
io
n
LAPIN1(A,TDNC)
DOVE3(A,DN)
brklypat2(A,DN)
DOVE2(A,D)
IFLAB2(A,D)
SRGDU4(A,DN)
IFLAB4(A,DN)
IFLAB7(A,T)
brklypat4(A,DN,E)
A: auto
M : manual
T: TITLE
D: DESCRIPTION
N: NARRATIVE
C: CONCEPT
E: English Topics
 
Figure 5: Recall/Precision of optional runs 
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1 2 3 4 5 6 7 8 9 1011 12 1314 15 16 1718 19 202122 23 24 25 26 272829 30 31
topic ID
m
ed
ia
n o
f 
a
v
e
ra
ge
 
pre
ci
si
on
s
A
A+B
 
Figure 6: Median of average precisions (all runs) 
 
0
50
10
150
20
250
30
350
400
450
BS
AS
BJ
AJ
BU
AU
BS 1 34 44 2 1543 9 29 7 3 6 15 0 6 2 2 15 5 4 0 1 0 1 7 151726 0 0 1 16
AS 0 5 27 2 0 0 55 0 1 0 1 57 6 37 0 2 0 6 9 1 0 1 1 10 49 11 5 35 0 5 15
BJ 10 7 2 2 2 0 9 42 10 3 0 47 22 2 8 4 9 36 0 5 0 0 2 3 3 16 18 3 0 2 19
AJ 4 0 0 0 0 0 23101711 1 17312 10 1 1 2 11 0 0 2 4 2 10812 6 7 7 0 1 16
BU 7 13 4 4 6 0 5 33 7 4 3 29 5 10 3 1 8 18 2 1 1 0 4 16 6 16 38 7 5 7 4
AU 22 13 6 10 15 12 27 23 18 15 4 101 22 17 5 15 5 17 36 4 8 4 4 72 49 12 19 40 6 3 16
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31
topic ID
n
um
be
r 
of 
d
oc
um
en
ts
 
Figure 7: Breakdown of relevance judgments  
Recall of A J+AU
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 100 200 300 400 500 600 700 800 900 1000
ranking
re
ca
ll a ll
m andatory
m andatory (au to)
Recall  of AJ+AU+BJ+BU
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 100 200 300 400 500 600 700 800 900 1000
ranking
re
ca
ll a ll
m andatory
m andatory (au to)
 
Figure 8: Recall of the relevant patents retrieved in the preliminary human search 
Handling Information Access Dialogue through QA Technologies
? A novel challenge for open-domain question answering ?
Tsuneaki Kato
The University of Tokyo
kato@boz.c.u-tokyo.ac.jp
Fumito Masui
Mie University
masui@ai.info.mie-u.ac.jp
Jun?ichi Fukumoto
Ritsumeikan University
fukumoto@media.ritsumei.ac.jp
Noriko Kando
National Institute of Informatics
kando@nii.ac.jp
Abstract
A novel challenge for evaluating open-domain
question answering technologies is proposed.
In this challenge, question answering systems
are supposed to be used interactively to an-
swer a series of related questions, whereas in
the conventional setting, systems answer iso-
lated questions one by one. Such an interac-
tion occurs in the case of gathering informa-
tion for a report on a specific topic, or when
browsing information of interest to the user. In
this paper, first, we explain the design of the
challenge. We then discuss its reality and show
how the capabilities measured by the challenge
are useful and important in practical situations,
and that the difficulty of the challenge is proper
for evaluating the current state of open-domain
question answering technologies.
1 Introduction
Open-domain question answering technologies allow
users to ask a question in natural language and obtain
the answer itself rather than a list of documents that con-
tain the answer. These technologies make it possible
to retrieve information itself rather than merely docu-
ments, and will lead to new styles of information access
(Voorhees, 2000).
The recent research on open-domain question answer-
ing concentrates on answering factoid questions one by
one in isolation from each other. Such systems that an-
swer isolated factoid questions are the most basic level of
question answering technologies, and will lead to more
sophisticated technologies that can be used by profes-
sional reporters and information analysts. On some stage
of that sophistication, a cub reporter writing an article on
a specific topic will be able to translate the main issue ad-
dressed by his report into a set of simpler questions and
then pose those questions to the question answering sys-
tem (Burger et al, 2001).
In addition, there is a relation between multi-document
summarization and question answering. In his lecture,
Eduard Hovy mentioned that multi-document summa-
rization may be able to be reduced into a series of ques-
tion answering (Hovy, 2001). In SUMMAC, an intrinsic
evaluation was conducted which measures the extent to
which a summary provides answers to a set of obligatory
questions on a given topic (Mani et al, 1998). Those sug-
gest such question answering systems that can answer a
series of related questions would surely be a useful aid to
summarization work by human and by machine.
Against this background, question answering systems
need to be able to answer a series of questions, which
have a common topic and/or share a local context. In
this paper, we propose a challenge to measure objectively
and quantitatively such an ability of question answering
systems. We call this challenge QACIAD (Question An-
swering Challenge for Information Access Dialogue). In
this challenge, question answering systems are used in-
teractively to participate in dialogues for accessing infor-
mation. Such information access dialogue occurs such
as when gathering information for a report on a specific
topic, or when browsing information of interest to the
user. Actually, in QACIAD, the interaction is only simu-
lated and systems answer a series of questions in a batch
mode. Although such a simulation may neglect the in-
herent dynamics of dialogue, it is a practical compromise
for objective evaluation and, as a result, the test sets of
the challenge are reusable.
Question answering systems need a wide range of abil-
ities in order to participate in information access dia-
logues (Burger et al, 2001). First, the systems must re-
spond in real time to make interaction possible. They
must also properly interpret a given question within the
context of a specific dialogue, and also be cooperative
by adding appropriate information not mentioned explic-
itly by the user. Moreover, the systems should be able
to pose a question for clarification to resolve ambiguity
concerning the user?s goal and intentions, and to partici-
pate in mixed initiative dialogue by making suggestions
and leading the user toward solving the problem. Among
these various capabilities, QACIAD focuses on the most
fundamental aspect of dialogue, that is, interpreting a
given question within the context of a specific dialogue.
It measures context processing abilities of systems such
as anaphora resolution and ellipses handling.
This paper is organized as follows. The next chap-
ter explains the design of QACIAD. The following three
chapters discuss the reality of the challenge. First, we ex-
plain the process of constructing the test set of the chal-
lenge and introduce the results of a study conducted dur-
ing this process which show the validity of QACIAD.
That is, QACIAD measures valid abilities needed for
participating in information access dialogues. In other
words, the ability measured by the challenge is crucial
to the systems for realizing information access dialogues
for writing reports and summaries. Second, we show the
statistics of pragmatic phenomena in the constructed test
set, and demonstrate that the challenge covers a wide va-
riety of pragmatic phenomena observed in real dialogues.
Third, based on a preliminary analysis of the QACIAD
run, we show that the challenge has a proper difficulty
for evaluating the current state of open-domain question
answering technologies. In the last two chapters, we dis-
cuss problems identified while constructing the test set
and conducting the run, and draw some conclusions.
2 Design of QACIAD
2.1 History
The origin of QACIAD comes from QAC1 (Question
Answering Challenge), one of the tasks of the NTCIR3
workshop conducted from March 2001 through October
2002 (NTCIR, 2001). QACIAD was originally proposed
in March 2001 as the third subtask of QAC1, its formal
run was conducted in May 2002 (Fukumoto et al, 2001;
Fukumoto et al, 2002; Fukumoto et al, 2003), and the re-
sults were reported at the NTCIR3 workshop meeting in
October 2002. The current design of QACIAD reported
in this paper is based on that challenge and is the result
of extensive elaboration. The design of the challenge and
construction of the test set were performed from January
2003 through December 2003. The formal run was con-
ducted in December 2003, as a subtask of QAC2, which
in turn is a task of the NTCIR4 workshop (NTCIR, 2003).
2.2 QAC as a common ground
QAC is a challenge for evaluating question answering
technologies in Japanese. It consists of three subtasks
including QACIAD, and the common scope of those sub-
tasks covers factoid questions that have names as an-
swers. Here, names mean not only names of proper items
(named entities) including date expressions and monetary
values, but also common names such as names of species
and names of body parts. Although the syntactical range
of the names approximately corresponds to compound
nouns, some of them, such as the titles of novels and
movies, deviate from that range. The underlying docu-
ment set consists of two years of articles of two newspa-
pers in QAC2, and one newspaper in QAC1. Using those
documents as the data source, the systems answer various
open-domain questions.
From the outset, QAC has focused on question answer-
ing technologies that can be used as components of larger
intelligent systems and technologies that can handle re-
alistic problems. It persists in requesting exact answers
rather than the text snippets that contain them with the
cost of avoiding handling definition questions and why
questions, because such answers are crucial in order to be
used as inputs to other intelligent systems such as multi-
document summarization systems. Moreover, as such a
situation is considered to be more realistic, the systems
must collect all the possible correct answers and detect
the absence of an answer. Therefore two subtasks, one of
which is QACIAD, request systems to return one list of
answers that contains all and only correct answers, while
the other subtask requests systems to return a ranked list
of possible answers as in TREC-8. In both subtasks, the
presence of answers in the underlying documents is not
guaranteed and the number of answers is not specified, so
these subtasks are similar to the list question task in the
TREC-2003 style rather than the TREC-10 style (TREC,
2003).
2.3 Information access dialogue
Considering scenes in which those question answering
systems participate in a dialogue, we classified informa-
tion access dialogues into the following two categories.
As discussed later, dialogues in a real situation may have
different features in their different portions; the classifi-
cation just shows two extremes.
Gathering Type The user has a concrete objective such
as writing a report and summary on a specific topic,
and asks a system a series of questions all concern-
ing that topic. The dialogue has a common global
topic, and, as a result, each consecutive question
shares a local context.
Browsing Type The user does not have any fixed topic
of interest; the topic of interest varies as the dialogue
progresses. No global topic covers a whole dialogue
but each consecutive question shares a local context.
This paper proposes the design of the challenge, which
can measure the abilities of question answering systems
useful in such dialogues.
2.4 The setting
QACIAD requests participant systems to return all pos-
sible answers to a series of questions, each of which is a
factoid question that has names as answer. This series of
questions and the answers to those questions comprise an
information access dialogue. Two examples of the series
of questions are shown in Figure 1, which were picked
up from our test set discussed in the next chapter. Se-
ries 14 is a series of a typical gathering type, while series
22 of a typical browsing type. In QACIAD, a number of
series (in the case of our test set, 36 series) are given to
the system at once and systems are requested to answer
those series in a batch mode. One series consists of seven
questions on average. The systems must identify the type
to which a series belongs, as it is not given. The systems
need not identify the changes of series, as the boundary
of series is given. Those, however, must not look ahead
to the questions following the one currently being han-
dled. This restriction reflects the fact that QACIAD is a
simulation of interactive use of question answering sys-
tems in dialogues. This restriction, accompanied with the
existence of two types of series, increases the complexity
of the context processing that the systems must employ.
For example, the systems need to identify that series 22
is a browsing type and the focus of the second question is
Yankee stadium rather than New York Yankees without
looking ahead to the following questions. Especially in
Japanese, since anaphora are not realized often and the
definite and indefinite are not clearly distinguished, those
problems are more serious.
2.5 Evaluation measure
In QACIAD, as the systems are requested to return one
list consisting all and only correct answers and the num-
ber of correct answers differs for each question1, mod-
ified F measure is used for the evaluation, which takes
account of both precision and recall. Two modifications
were needed. The first is for the case where an answer
list returned by a system contains the same answer more
than once or answers in different expressions denoting
the same item. In that case, only one answer is regarded
as the correct one, and so the precision of such answer
list decreases. Cases regarded as different expressions
denoting the same item include a person?s name with and
without the position name, variations of foreign name no-
tation, differences of monetary units used, differences of
time zone referred to, and so on. The second modifica-
tion is for questions with no answer. For those questions,
modified F measure is 1.0 if a system returns an empty
list as the answer, and is 0.0 otherwise.
1It is a special case that the number of answers is just one
for all questions shown in Figure 1.
Series 14
When was Seiji Ozawa born?
Where was he born?
Which university did he graduate from?
Who did he study under?
Who recognized him?
Which orchestra was he conducting in 1998?
Which orchestra will he begin to conduct in 2002?
Series 22
Which stadium is home to the New York Yankees?
When was it built?
How many persons? monuments have been
displayed there?
Whose monument was displayed in 1999?
When did he come to Japan on honeymoon?
Who was the bride at that time?
Who often draws pop art using her as a motif?
What company?s can did he often draw also?
Figure 1: Examples of series of questions
The judgment as to whether a given answer is correct
or not takes into account not only an answer itself but
also the accompanying article from which the answer was
extracted. When the article does not validly support the
answer, that is, assessors cannot understand that the an-
swer is the correct one for a given question by reading
that article, it is regarded as incorrect even though the
answer itself is correct. The correctness of an answer
is determined according to the interpretation of a given
question done by human assessors within the given con-
text. The system?s answers to previous questions, and its
understanding of the context from which those answers
were derived, are irrelevant. For example, the correct an-
swer to the second question of series 22, namely when the
Yankee stadium was built, is 1923. If the system wrongly
answers the Shea stadium to the first question, and then
?correctly? answers the second question 1964, the year
when the Shea stadium was built, that answer to the sec-
ond question is not correct. On the other hand, if the
system answers 1923 to the second question with an ap-
propriate article supporting it, that answer is correct no
matter how the system answered the first question.
3 Constructing a Test Set and Usefulness
of the Challenge
We collected and analyzed questions for two purposes.
The first purpose was to establish a methodology for con-
structing a test set based on the design of QACIAD dis-
cussed in the previous chapter. The second purpose was
to confirm the reality of the challenge, that is, to deter-
mine whether it is useful for information access dialogues
to use question answering systems that can answer ques-
tions that have names as answers.
3.1 Collecting questions
Questions were collected as follows. Subjects were pre-
sented various topics, which included persons, organiza-
tions, and events selected from newspaper articles, and
were requested to make questions that ask for informa-
tion to be used in the report on that topic. The report is
supposed to describe facts on a given topic, rather than
contain opinions or prospects on the topic. The ques-
tions are restricted to wh-type questions, and natural se-
ries of questions containing anaphoric expressions and so
on were constructed. The topics were presented in three
different ways: only by a short description of the topic,
which corresponds to the title part of the TREC topic def-
inition; with a short article or the lead of a longer article,
which is representative of that topic and corresponds to
the narrative part of the TREC topic definition; and with
five articles concerning that topic. The number of top-
ics was 60, selected from two years of newspaper arti-
cles. Thirty subjects participated in the experiment. Each
subject made questions for ten topics for each topic pre-
sentation pattern, and was instructed to make around ten
questions for each topic. It is worth noting that the ques-
tions obtained were natural in both content and expres-
sion since in this experiment the subjects did not consider
whether the answers to their questions would be found in
the newspapers, and some subjects did not read the arti-
cles at all.
This time, for the test set construction and preliminary
analysis, 1,033 questions on 40 topics, made by three sub-
jects for each topic with different topic presentation pat-
terns, were used. All of the questions collected are now
being analyzed extensively, especially on the differences
among questions according to the topic presentation pat-
tern.
3.2 Analysis of the questions
Our main concern here is how many of the questions
collected fall into the category of questions that the cur-
rent question answering systems could answer. In other
words, how many of the questions can be answered by a
list of names? In the case the majority of them fall into
such a category, it is realistic to use question answering
systems for information access dialogues and the chal-
lenge on such abilities must be useful.
Table 1 shows the classification of questions according
to the subject asked. In the case where users ask ques-
tions to get information for a report, the number of why
questions is relatively small. Moreover, there were fewer
questions requesting an explanation or definition than ex-
Table 1: Categorization of questions by subject
Asking about
4W (Who, When, Where, What)
70%incl. several types of numerical values
Why 4%
How, for a procedure or method 10%
Definitions, descriptions or explanations 16%
Table 2: Categorization of questions by answer type
Answered in
Numerical values or date expressions 28%
Proper names 22%
Common names (in compound nouns) 8%
Names probably 14%
Clauses, sentences, or texts 28%
pected, probably because questions such as ?Who is Seiji
Ozawa? were decomposed into relatively concrete ques-
tions such as those asking for his birthday and birth place.
However, not all questions that were categorized as
4W questions could be answered by names. For exam-
ple, whereas questions asking where, such as ?Where was
Shakespeare born??, could be answered by a place name,
questions like ?Where do lobsters like to live?? need a
description and not a proper name as the answer. Table 2
shows the result of categorization according to this as-
pect. This categorization was conducted by inspecting
questions only, and some of the questions were hard to
determine decisively whether those could be answered
by names or not, and so were categorized as ?Names
probably?. For example, the question ?Where does the
name ?AIBO? come from?? could be answered by name
if AIBO is an acronym, but there may be a long story as
to its origin. Although such cases happened in other com-
binations of categories, those questions were categorized
into a more complex category as only the border of names
and descriptions are important in the current analysis.
As Table 2 shows, 58% to 72% of questions could be
answered by names. The amount of those questions is al-
most same as the amount of 4W questions, since while
some 4W questions could not be answered by names,
some definition and explanation questions might be able
to be answered by names. The fact that 58% to 72% of
questions for writing reports could be answered by names
demonstrates that question answering systems that an-
swer these questions are useful in such situations.
In addition, the answers to 84% of those 72% questions
could be found by humans from newspaper articles. This
indicates that the setting is realistic where users write re-
ports through interacting with a question answering sys-
tem that uses newspaper articles as its data source.
3.3 Constructing a test set
Using the questions collected, we constructed a test set
as follows. We selected 26 from 40 topics, and chose
appropriate questions and rearranged them for construct-
ing gathering type series. Some of the questions were
edited in order to resolve semantic or pragmatic ambigui-
ties, though we tried to use the questions without modifi-
cation where possible. The topics of the gathering series
consisted of 5 persons, 2 organizations, 11 events, 5 ar-
tifacts, and 3 animals and fishes, among which 4 topics
concerned sets of organizations and events, such as the
big three companies in the beer industry, simultaneous
terrorist attacks, and annual festival events.
Browsing type series were constructed by using some
of the remaining questions as seeds of a sequence and by
adding new questions to create a flow to/from those ques-
tions. For example, series 22 shown in Figure 1 was com-
posed by adding the last four newly created questions to
the first four questions which were collected for the Yan-
kee stadium2. For such seeds, we also used the collection
of questions for evaluating summarization constructed for
TSC (Text Summarization Challenge), another challenge
in the NTCIR workshop (TSC, 2003). Some topics used
for the question collection were the same as the topics
used in TSC also. We made 10 browsing series in this
way.
Finally, the test set constructed this time contained 36
series and 251 questions, with 26 series of the gather-
ing type and 10 series of the browsing type. The average
number of questions in one series was 6.92.
4 Characteristics of the Test Set
This chapter describes the pragmatic characteristics of
the constructed test set. Japanese has four major types
of anaphoric devices: pronouns, zero pronouns, definite
noun phrases, and ellipses. Zero pronouns are very com-
mon in Japanese in which pronouns are not realized on
the surface. As Japanese also has a completely different
determiner system from English, the difference between
definite and indefinite is not apparent on the surface,
and definite noun phrases usually have the same form
as generic noun phrases. Table 3 shows the summary
of such pragmatic phenomena observed in 215 questions
obtained by removing the first one of each series from
the 251 questions in the test set. The total number is
more than 215 as 12 questions contain more than one phe-
nomenon. The sixth question in series 22, ?Who was the
bride at that time?? is an example of such a question with
2The question focus of the first one was changed.
Table 3: Pragmatic phenomena observed in the test set
Type Occurence
Pronouns 76 (21)
Zero pronouns 134 (33)
Definite noun phrases 11 (4)
Ellipses 7
multiple anaphoric expressions. The numbers in paren-
theses show the number of cases in which the referenced
item is an event. As the table indicates, a wide range of
pragmatic phenomena is observed in the test set.
Precisely speaking, the series in the test set can be
characterized through the pragmatic phenomena that they
contain. Gathering type series consist of questions that
have a common referent in a broad sense, which is a
global topic mentioned in the first question of the series.
Strictly gathering type series can be distinguished as a
special case of gathering type series. In those series, all
questions refer exactly to the same item mentioned in the
first question and do not have any other anaphoric ex-
pression. In other words, questions about the common
topic introduced by the first question comprise a whole
sequence. Series 14 in Figure 1 is an example of the
strictly gathering type and all questions can be interpreted
by supplying Seiji Ozawa, who is introduced in the first
question. The test set has 5 series of the strictly gathering
type. Other gathering type series have other two types of
questions. The first type of questions not only has a ref-
erence to the global topic but also refers to other items or
has an ellipsis. The second type of questions has a refer-
ence to a complex item, such as an event that contains the
global topic as its component. Series 20 shown in Fig-
ure 2 is such a series. The third question refers not only
to the global topic, George Mallory, in this case, but also
to his famous phrase. The sixth one refers to an event
George Mallory was concerned in.
On the other hand, the questions of a browsing type
series do not have such a global topic. Sometimes the
referent is the answer of the immediately preceding ques-
tion, such as the fifth, seventh and eighth questions in
series 22 in Figure 1. No series, however, consists solely
of questions that have only a reference to the answer to
the immediately previous questions. All series contain
references to the answers to non-immediately previous
questions or items mentioned in the previous questions,
or more than one pragmatic phenomenon. In series 22,
the third, fourth and sixth questions belong to such a case.
In both types, therefore, the shifting pattern of the fo-
cus is not simple, and so a sophisticated way is needed to
track it. Such focus tracking is indispensable to get cor-
rect answers. Systems cannot even retrieve articles con-
Series 20
In which country was George Mallory born?
What was his famous phrase?
When did he say it?
How old was he when he started climbing
mountains?
On which expedition did he go missing near the
top of Everest?
When did it happen?
At what altitude on Everest was he seen last?
Who found his body?
Figure 2: Another example of series of questions
taining the answer just by accumulating keywords. This
is clear for the browsing type, as an article is unlikely to
mention both the New York Yankees and Campbell soup.
In the gathering type, since the topics mentioned in rela-
tively many articles were chosen, it is not easy to locate
the answer to a question from those articles retrieved us-
ing that topic as the keyword. For example, there are 155
articles mentioning Seiji Ozawa in our document sets, of
which 22 articles mention his move to the Vienna Phil-
harmonic Orchestra, and only two articles also mention
his birthday. An extensive, quantitative analysis is now
in progress.
5 Difficulty of the Challenge and the
Current State of Technologies
Seven teams and fourteen systems participated in the run
using the test set mentioned in the previous chapter con-
ducted in December 2003. In this chapter, based on a
preliminary analysis of the run, the difficulty of the chal-
lenge and the current state of technologies for addressing
the challenge are discussed. The techniques employed in
the participant systems have not yet been published, but
will be published by the NTCIR workshop 4 meeting at
the latest.
Figure 3 shows the mean modified F measures of the
top 10 participant systems. The chart shows the mean
modified F measure of three categories: all of the test set
questions, the questions of the first of each series, and
questions of the second and after. As anticipated, it is
more difficult to answer correctly the questions other than
the first question of each series. This indicates that more
sophisticated context processing is needed.
The mean modified F measure is not high even for the
top systems. This is probably because of not only the
difficulties of context processing but also the difficulties
of returning the list of all and only correct answers. It
is difficult to achieve high recall since some of the ques-
 
 
  
  
  
  
  	

 
    
    
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 9?16,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
WoZ Simulation of Interactive Question Answering
Tsuneaki Kato
The University of Tokyo
kato@boz.c.u-tokyo.ac.jp
Fumito Masui
Mie University
masui@ai.info.mie-u.ac.jp
Jun?ichi Fukumoto
Ritsumeikan University
fukumoto@media.ritsumei.ac.jp
Noriko Kando
National Institute of Informatics
kando@nii.ac.jp
Abstract
QACIAD (Question Answering Chal-
lenge for Information Access Dialogue)
is an evaluation framework for measur-
ing interactive question answering (QA)
technologies. It assumes that users inter-
actively collect information using a QA
system for writing a report on a given
topic and evaluates, among other things,
the capabilities needed under such cir-
cumstances. This paper reports an ex-
periment for examining the assumptions
made by QACIAD. In this experiment, di-
alogues under the situation that QACIAD
assumes are collected using WoZ (Wiz-
ard of Oz) simulating, which is frequently
used for collecting dialogue data for de-
signing speech dialogue systems, and then
analyzed. The results indicate that the set-
ting of QACIAD is real and appropriate
and that one of the important capabilities
for future interactive QA systems is pro-
viding cooperative and helpful responses.
1 Introduction
Open-domain question answering (QA) technolo-
gies allow users to ask a question using natural lan-
guage and obtain the answer itself rather than a list
of documents that contain the answer (Voorhees et
al.2000). While early research in this field concen-
trated on answering factoid questions one by one in
an isolated manner, recent research appears to be
moving in several new directions. Using QA sys-
tems in an interactive environment is one of those
directions. A context task was attempted in order
to evaluate the systems? ability to track context for
supporting interactive user sessions at TREC 2001
(Voorhees 2001). Since TREC 2004, questions in
the task have been given as collections of questions
related to common topics, rather than ones that are
isolated and independent of each other (Voorhees
2004). It is important for researchers to recognize
that such a cohesive manner is natural in QA, al-
though the task itself is not intended for evaluating
context processing abilities since, as it is given the
common topic, sophisticated context processing is
not needed.
Such a direction has also been envisaged as a re-
search roadmap, in which QA systems become more
sophisticated and can be used by professional re-
porters and information analysts (Burger et al2001).
At some stage of that sophistication, a young re-
porter writing an article on a specific topic will be
able to translate the main issue into a set of simpler
questions and pose those questions to the QA sys-
tem.
Another research trend in interactive QA has been
observed in several projects that are part of the
ARDA AQUAINT program. These studies concern
scenario-based QA, the aim of which is to handle
non-factoid, explanatory, analytical questions posed
by users with extensive background knowledge. Is-
sues include managing clarification dialogues in or-
der to disambiguate users? intentions and interests;
and question decomposition to obtain simpler and
more tractable questions (Small et al2003)(Hickl et
9
al.2004).
The nature of questions posed by users and pat-
terns of interaction vary depending on the users who
use a QA system and on the environments in which
it is used (Liddy 2002). The user may be a young re-
porter, a trained analyst, or a common man without
special training. Questions can be answered by sim-
ple names and facts, such as those handled in early
TREC conferences (Chai et al2004), or by short
passages retrieved like some systems developed in
the AQUAINT program do (Small et al2003). The
situation in which QA systems are supposed to be
used is an important factor of the system design and
the evaluation must take such a factor into account.
QACIAD (Question Answering Challenge for Infor-
mation Access Dialogue) is an objective and quan-
titative evaluation framework to measure the abil-
ities of QA systems used interactively to partici-
pate in dialogues for accessing information (Kato et
al.2004a)(Kato et al2006). It assumes the situation
in which users interactively collect information us-
ing a QA system for writing a report on a given topic
and evaluates, among other things, the capabilities
needed under such circumstances, i.e. proper inter-
pretation of questions under a given dialogue con-
text; in other words, context processing capabilities
such as anaphora resolution and ellipses handling.
We are interested in examining the assumptions
made by QACIAD, and conducted an experiment,
in which the dialogues under the situation QACIAD
assumes were simulated using the WoZ (Wizard of
Oz) technique (Fraser et al1991) and analyzed. In
WoZ simulation, which is frequently used for col-
lecting dialogue data for designing speech dialogue
systems, dialogues that become possible when a sys-
tem has been developed are simulated by a human, a
WoZ, who plays the role of the system, as well as a
subject who is not informed that a human is behav-
ing as the system and plays the role of its user. An-
alyzing the characteristics of language expressions
and pragmatic devices used by users, we confirm
whether QACIAD is a proper framework for eval-
uating QA systems used in the situation it assumes.
We also examine what functions will be needed for
such QA systems by analyzing intelligent behavior
of the WoZs.
2 QACIAD and the previous study
QACIAD was proposed by Kato et al as a task of
QAC, which is a series of challenges for evaluat-
ing QA technologies in Japanese (Kato et al2004b).
QAC covers factoid questions in the form of com-
plete sentences with interrogative pronouns. Any
answers to those questions should be names. Here,
?names? means not only names of proper items
including date expressions and monetary values
(called ?named entities?), but also common names
such as those of species and body parts. Although
the syntactical range of the names approximately
corresponds to compound nouns, some of them,
such as the titles of novels and movies, deviate from
that range. The underlying document set consists
of newspaper articles. Being given various open-
domain questions, systems are requested to extract
exact answers rather than text snippets that contain
the answers, and to return the answer along with the
newspaper article from which it was extracted. The
article should guarantee the legitimacy of the answer
to a given question.
In QACIAD, which assumes interactive use of
QA systems, systems are requested to answer series
of related questions. The series of questions and the
answers to those questions comprise an information
access dialogue. All questions except the first one of
each series have some anaphoric expressions, which
may be zero pronouns, while each question is in the
range of those handled in QAC. Although the sys-
tems are supposed to participate in dialogue inter-
actively, the interaction is only simulated; systems
answer a series of questions in batch mode. Such
a simulation may neglect the inherent dynamics of
dialogue, as the dialogue evolution is fixed before-
hand and therefore not something that the systems
can control. It is, however, a practical compromise
for an objective evaluation. Since all participants
must answer the same set of questions in the same
context, the results for the same test set are compa-
rable with each other, and the test sets of the task are
reusable by pooling the correct answers.
Systems are requested to return one list consisting
of all and only correct answers. Since the number of
correct answers differs for each question and is not
given, a modified F measure is used for the evalu-
ation, which takes into account both precision and
10
recall.
Two types of series were included in the QA-
CIAD, which correspond to two extremes of infor-
mation access dialogue: a gathering type in which
the user has a concrete objective such as writing a
report and summary on a specific topic, and asks
a system a series of questions related to that topic;
and a browsing type in which the user does not
have any fixed topic of interest. Although the QA-
CIAD assumes that users are interactively collect-
ing information on a given topic and the gathering-
type dialogue mainly occurs under such circum-
stances, browsing-type series are included in the task
based on the observation that even when focusing
on information access dialogue for writing reports,
the systems must handle focus shifts appearing in
browsing-type series. The systems must identify the
type of series, as it is not given, although they need
not identify changes of series, as the boundary is
given. The systems must not look ahead to questions
following the one currently being handled. This re-
striction reflects the fact that the QACIAD is a simu-
lation of interactive use of QA systems in dialogues.
Examples of series of QACIAD are shown in Fig-
ure 1. The original questions are in Japanese and the
figure shows their direct translations.
The evaluation of QA technologies based on QA-
CIAD were conducted twice in QAC2 and QAC3,
which are a part of the NTCIR-4 and NTCIR-5
workshops1, respectively (Kato et al2004b)(Kato et
al.2005). It was one of the three tasks of QAC2 and
the only task of QAC3. On each occasion, several
novel techniques were proposed for interactive QA.
Kato et al conducted an experiment for confirm-
ing the reality and appropriateness of QACIAD, in
which subjects were presented various topics and
were requested to write down series of questions
in Japanese to elicit information for a report on
that topic (Kato et al2004a)(Kato et al2006). The
report was supposed to describe facts on a given
topic, rather than state opinions or prospects on the
topic. The questions were restricted to wh-type
questions, and a natural series of questions that may
contain anaphoric expressions and ellipses was con-
1The NTCIR Workshop is a series of evaluation workshops
designed to enhance research in information access technolo-
gies including information retrieval, QA, text summarization,
extraction, and so on (NTCIR 2006).
Series 30002
What genre does the ?Harry Potter? series belong to?
Who is the author?
Who are the main characters in the series?
When was the first book published?
What was its title?
How many books had been published by 2001?
How many languages has it been translated into?
How many copies have been sold in Japan?
Series 30004
When did Asahi breweries Ltd. start selling their low-malt
beer?
What is the brand name?
How much did it cost?
What brands of low-malt beer were already on the
market at that time?
Which company had the largest share?
How much low-malt beer was sold compared to regular
beer?
Which company made it originally?
Series 30024
Where was Universal Studio Japan constructed?
What is the nearest train station?
Which actor attended the ribbon-cutting ceremony on the
opening day?
Which movie that he featured in was released in the New
Year season of 2001?
What movie starring Kevin Costner was released in the
same season?
What was the subject matter of that movie?
What role did Costner play in that movie?
Figure 1: Examples of Series in QACIAD
structed. Analysis of the question series collected
in such a manner showed that 58% to 75% of ques-
tions for writing reports could be answered by val-
ues or names; a wide range of reference expres-
sions is observed in questions in such a situation;
and sequences of questions are sometimes very com-
plicated and include subdialogues and focus shifts.
From these observations they concluded the reality
and appropriateness of the QACIAD, and validated
the needs of browsing-type series in the task.
One of the objectives of our experiment is to con-
firm these results in a more realistic situation. The
previous experiment setting is far from the actual
situations in which QA systems are used, in which
subjects have to write down their questions without
getting the answers. Using WoZ simulation, it is
confirmed whether or not this difference affected the
result. Moreover, observing the behavior of WoZs,
the capabilities and functions needed for QA sys-
11
tems used in such a situation are investigated.
3 Setting
Referring to the headlines in Mainichi and Yomi-
uri newspapers from 2000 and 2001, we selected
101 topics, which included events, persons, and or-
ganizations. On each of those topics, a summary
of between 800 and 1600 characters long and an
abstract of around 100 characters long were con-
structed using a full text search system on the news-
paper articles.2 Four experts shared this prepara-
tion work. Twenty topics were selected from among
the original 101 on the basis that enough informa-
tion was gathered and compiled into the summary.3
The topics consisted of 5 persons, 2 organizations,
7 events, 5 artifacts, and 1 syndrome, which include
Mr. Colin Powell, Charles, Prince of Wales, An ac-
cident of a Russian nuclear-powerd submarine, Con-
corde, ?Harry Potter?, and Windows XP.
The four experts who shared the preparation work
acted as a WoZ in the simulated QA systems, and
each expert participated in dialogues on the topic for
which she/he wrote the summary and abstract, and
tried to answer questions from users using the sum-
mary, a full text search system, and her/his memory.
Six subjects participated in the experiment and 10
topics were assigned to each subject. That is, each
topic was handled by three subjects. The subjects
behaved as users of simulated QA systems. Pre-
sented with the topic and abstract, the subjects were
directed to think about questions beforehand assum-
ing that they had to write a report on that topic, and
then they participated in an information access dia-
logue with the ?QA system.? The suggested number
of questions for each topic was around 10.
The dialogues were conducted through keyboards
and character displays. The subjects were told that
the system could answer only simple factoid ques-
tions, and the WoZs were instructed not to answer
complicated questions such as asking for reasons
and opinions. No restrictions on the usage of lan-
guage expressions were imposed but were left to
the discretion of the WoZs, though canned phrases
2Those lengths of Japanese texts roughly correspond to 300
to 600 words and 40 words of English text, respectively.
3We prepared 101 topics to meet the additional needs of
other experiments. It is absolutely not the case that only 20
out of 101 were acceptable for our experiments.
USR: What is the title of the first book of the ?Harry
Potter? series?
SYS: ?Harry Potter and the Sorcerer?s Stone.?
USR: How many books were sold?
SYS: The Japanese translation sold 600 thousand copies
in the first half year after its publication.
USR: Who is the author?
SYS: J. K. Rowling.
USR: Who are the main characters (in that series)?
SYS: Harry Potter, Ron Weasley, Hagrid and so on.
USR: What kind of person is Harry Potter?
SYS: A child with a wizard and a witch for parents.
USR: What is the plot?
SYS: It is an adventure story involving Harry Potter and his
friends who enter a school for witchcraft and wizardry.
USR: How many people went to the movie?
SYS: In the US, box-office sales of the movie ?Harry Potter
and the Sorcerer?s Stone? reached 188 million
dollars in the first 10 days after release.
Figure 2: Example of dialogues collected
such as ?Please wait a moment? and ?Sorry, the an-
swer could not be found? were prepared in advance.
The WoZs were also instructed that they could clar-
ify users? questions when they were ambiguous or
vague, and that their answers should be simple but
cooperative and helpful responses were not forbid-
den.
An example of the dialogues collected is shown in
Figure 2. In the figure, SYS stands for utterances of
the QA system simulated by a WoZ and USR repre-
sents that of the user, namely a subject. In the rest of
the paper, these are referred to as system?s utterances
and user?s utterances, respectively.
4 Coding and Results
Excluding meta-utterances for dialogue control such
as ?Please wait a moment? and ?That?s all,? 620
pairs of utterances were collected, of which 22 sys-
tem utterances were for clarification. Among the re-
maining 598 cases, the system gave some answers in
502 cases, and the other 94 utterances were negative
responses: 86 utterances said that the answer could
not found; 10 utterances said that the question was
too complicated or that they could not answer such
type of question.
4.1 Characteristics of questions and answers
The syntactic classification of user utterances and its
distribution is shown in Table 1. The numbers in
12
Table 1: Syntactic classification of user utterances
Syntactic form
Wh-type Question 87.7% (544)
Yes-no Question 9.5% (59)
Imperative (Information request) 2.6% (16)
Declarative (Answer to clarification) 0.2% (1)
Table 2: Categorization of user utterances by subject
Asking about
Who, Where, What 32.5% (201)
When 16.3% (101)
How much/many 16.8% (104)(for several types of numerical values)
Why 6.5% (40)
How (for procedures or situations) 17.0% (105)
Definitions, Descriptions, Explanations 10.8% (67)
Other (Multiple Whs) 0.2% (1)
parentheses are numbers of occurrences. In spite of
the direction of using wh-type questions, more than
10% of utterances are yes-no questions and impera-
tives for requesting information. Most of the user
responses to clarification questions from the sys-
tem are rephrasing of the question concerned; only
one response has a declarative form. Examples of
rephrasing will be shown in section 4.3.
The classification of user questions and requests
according to the subject asked or requested is shown
in Table 2; the classification of system answers ac-
cording to their syntactic and semantic categoriza-
tion is shown in Table 3. In Table 2, the classification
of yes-no questions was estimated based on the in-
formation provided in the helpful responses to those.
The classification in Table 3 was conducted based on
the syntactic and semantic form of the exact part of
the answer itself rather than on whole utterances of
the system. For example, the categorization of the
system utterance ?He was born on April 5, 1935,?
which is the answer to ?When was Mr. Colin Powell
born?? is not a sentence but a date expression.
4.2 Pragmatic phenomena
Japanese has four major types of anaphoric devices:
pronouns, zero pronouns, definite noun phrases,
Table 3: Categorization of user utterances by answer
type
Answered in
Numerical values 14.3% (72)
Date expressions 16.7% (84)
Proper names 22.1% (111)
Common names 8.8% (44)
Compound nouns except names 4.2% (21)
Noun phrases 6.2% (31)
Clauses, sentences, or texts 27.7% (139)
Table 4: Pragmatic phenomena observed
Type
No reference expression 203
Pronouns 14
Zero pronouns 317
Definite noun phrases 104
Ellipses 1
and ellipses. Zero pronouns are very common in
Japanese, in which pronouns are not apparent on the
surface. As Japanese also has a completely different
determiner system from English, the difference be-
tween definite and indefinite is not apparent on the
surface, and definite noun phrases usually have the
same form as generic noun phrases. Table 4 shows
a summary of such pragmatic phenomena observed.
The total number is more than 620 as some utter-
ances contain more than one anaphoric expression.
?How many crew members were in the submarine
when the accident happened?? is an example of such
a question with multiple anaphoric expressions.
Among 203 questions with no reference expres-
sion, 60 questions (30%) are the first one of a series
of utterances on the same topic. The others are di-
vided into two cases. In the first and major case, the
current foci appear literally rather than in pronouns
or definite noun phrases. The second case, which is
relatively minor, is the beginning of and the resume
from a subdialogue. For example, when a user him-
self resumed talking about Charles, Prince of Wales
after a few questions about his children, the user
used a literal expression like ?Who is the long-term
partner of Charles, Prince of Wales??; after talking
13
about the details of the accident, a user asked ?What
is the major characteristic of Concorde??
Out of 437 reference expressions excluding el-
lipses in Table 4, 94 expressions (22%) refer to items
other than the global topic. Moreover, 76 expres-
sions of those (81%) are a sole reference expression
in the utterance. The existence of questions with-
out referring to the global topic, accompanied with
the existence of some questions with no reference
expression, means that the focus shifts in those dia-
logues, and then the local focus can differ from the
global topic.
4.3 Clarifications
As mentioned above, only 22 clarification utterances
were observed in the experiment. They are catego-
rized into those asking about an ambiguous referent
(13 utterances) and those asking about a vague ques-
tion subject (9 utterances). Examples of each case
are shown below.
Clarification asking about an ambiguous referent
USR: When was he [Mr. Colin Powell] installed as head of
security of the United States?
SYS: December 6, 2000.
USR: Who was the president at that time?
SYS: Mr. George W. Bush.
USR: Which political party did he belong to?
SYS: Whose political party are you asking about?
USR: Which political party did Mr. Colin Powell belong to?
Clarification asking about a vague question subject
USR: Who developed Windows XP?
SYS: Are you asking for a company name?
USR: What company developed Windows XP?
4.4 Cooperative and helpful responses
Rather than simple and literal answers to the users?
questions and requests, cooperative and helpful re-
sponses, in which, for example, information not re-
quested directly by a user was additionally provided
(Kaplan 1983), were observed frequently. The re-
sults of analyzing such cooperative and helpful be-
havior in the system responses are shown in Table
5. Questions asking reasons, procedures, situations,
definitions, descriptions or explanations, which are
categorized into the last three categories except oth-
ers in Table 2, were excluded from this analysis,
since for those questions it is difficult to judge objec-
Table 5: Cooperative and helpful behavior observed
Type of behavior
Providing additional information 52 (15.6%)
Providing detailed information 38 (11.4%)
Elaboration of subject 38 (11.4%)
Providing alternative information 47 (14.1%)
Exemplification 28 (8.4%)
Providing speculations 3 (0.9%)
tively whether some information is additional or not
and, in addition, most of those are out of the scope
of QACIAD, which is our main interest. Table 5
shows the number of occurrences and its percentage
among positive responses. Sometimes, more than
one behavior was observed in one utterance. 148
utterances out of 334 positive responses (44%) con-
tained some helpful behavior. The definitions and
manually constructed examples of cooperative and
helpful behavior listed in Table 5 are shown below.
Providing additional information Information
not requested directly from a user is addition-
ally provided.
USR: What year was Mr. Colin Powell born?
SYS: He was born in 1935 in New York.
Providing detailed information More detailed in-
formation than that requested directly by a user
is provided.
USR: What year was Mr. Colin Powell born?
SYS: He was born on April 5, 1935.
Elaboration of subject A relatively vague subject
of a question is elaborated and made concrete.
In the following example, a question with a
vague expression, ?how well? is answered by
a value as a concrete statistic.
USR: How well did the movie do?
SYS: In the US, box-office sales reached 188 million
dollars in the first 10 days after release.
Providing alternative information Rather than
exact information a user requested, alternative
related information is provided. An alternative
may be proposed by relaxing the condition
imposed by the user. In the following example,
attendance figures were altered to box-office
sales.
USR: How many people went to the movie?
14
SYS: In the US, box-office sales reached 188 million
dollars in the first 10 days after release.
Exemplification Instead of a comprehensive list or
abstracted general description, some major in-
stances are listed.
USR: What kinds of character goods are on sale?
SYS: Stuffed owls, uniforms for schools of witchcraft and
wizardry, and so on.
Providing speculation Rather than fact, specula-
tions and opinions are provided, which can be
considered a special case of providing alterna-
tive information. Although there are few cases
of such behavior in Table 5, there were many
such answers in response to questions asking
for reasons and situations.
USR: What exploded?
SYS: It is surmised that torpedoes in the shed at the fore
exploded.
Detecting and correcting erroneous user assump-
tions is a famous helpful behavior discussed in the
literature (Pollack 1989) (Kaplan 1983). Only two
occurrences of such behavior were observed, one of
which is shown below.
USR: How long did it take from the explosion until the
shipwreck?
SYS: The explosion happened while the submarine was going
down, and ...
5 Discussion
The analysis of dialogue collected by the WoZ sim-
ulation showed very similar results to those obtained
in a previous study, in which sequences of questions
were written down by subjects without knowing the
answers to questions. That is, as shown in Table 2,
when users asked questions to get information for a
report, the number of why-questions was relatively
small. Moreover, there were fewer questions re-
questing an explanation or definition than expected,
probably because definition questions such as ?Who
is Mr. Colin Powell?? were decomposed into rela-
tively concrete questions such as those asking for his
birthday and birthplace. The remainder (65%) could
be answered in values and names. Table 3 indicates
that 62% of the questions in our experiments were
answered by values or names. If compound nouns
describing events or situations, which are usually
distinguished from names, are considered to be in
the range of answers, the percentage of answerable
questions reaches 68%. From these results, the set-
ting of QACIAD looks realistic where users write re-
ports interacting with a QA system handling factoid
questions that have values and names as answers.
A wide range of reference expressions is observed
in information access dialogues for writing reports.
Moreover, our study confirmed that those sequences
of questions were sometimes very complicated and
included subdialogues and focus shifts. It is ex-
pected that using an interactive QA system that can
manage those pragmatic phenomena will enable flu-
ent information access dialogue for writing reports.
In this sense, the objective of QACIAD is appropri-
ate.
It could be concluded from these results that the
reality and appropriateness of QACIAD was recon-
firmed in a more realistic situation. And yet suspi-
cion remains that even in our WoZ simulation, the
subjects were not motivated appropriately, as sug-
gested by the lack of dynamic dialogue development
in the example shown in Figure 2. Especially, the
users often gave up too easily when they did not
obtain answers to prepared questions.4 The truth,
however, may be that in the environment of gath-
ering information for writing reports, dynamic dia-
logue development is limited compared to the case
when trained analysts use QA systems for problem
solving. If so, research on this type of QA systems
represents a proper milestone toward interactive QA
systems in a broad sense.
Another finding of our experiment is the impor-
tance of cooperative and helpful responses. Nearly
half of WoZ utterances were not simple literal re-
sponses but included some cooperative and helpful
behavior. This situation contrasts with a relatively
small number of clarification dialogues. The im-
portance of this behavior, which was emphasized
in research on dialogues systems in the 80s and
90s, was reconfirmed in the latest research, although
question-answering technologies were redefined in
the late 90s. Some behavior such as providing alter-
native information could be viewed as a second-best
4It is understandable, however, that there were few rephras-
ing attempts since users were informed that paraphrasing such
as ?What is the population of the US?? to ?How many people
are living in the US?? are usually in vain.
15
strategy of resource-bounded human WoZs. Even
so, it is impossible to eliminate completely the need
for such a strategy by improving core QA technolo-
gies. In addition, intrinsic cooperative and helpful
behavior such as providing additional information
was also often observed. These facts, accompanied
by the fact that such dialogues are perceived as fluent
and felicitous, suggest that the capability to behave
cooperatively and helpfully is essential for interac-
tive QA technologies.
6 Conclusion
Through WoZ simulation, the capabilities and func-
tions needed for interactive QA systems used as a
participant in information access dialogues for writ-
ing reports were examined. The results are compati-
ble with those of previous research, and reconfirmed
the reality and appropriateness of QACIAD. A new
finding of our experiment is the importance of coop-
erative and helpful behavior of QA systems, which
was frequently observed in utterances of the WoZs
who simulated interactive QA systems. Designing
such cooperative functions is indispensable. While
this fact is well known in the context of past research
on dialogue systems, it has been reconfirmed in the
context of the latest interactive QA technologies.
References
Joyce Y. Chai and Rong Jin. 2004. Discource Struc-
ture for Context Question Answering. Proceedings of
HLT-NAACL2004 Workshop on Pragmatics of Ques-
tion Answering, pp. 23-30.
John Burger, Claire Cardie, Vinay Chaudhri, et al 2001.
Issues, Tasks and Program Structures to Roadmap Re-
search in Question & Answering (Q&A)
http://www-nlpir.nist.gov/projrcts/
duc/roadmpping.html.
Norma M. Fraser and G. Nigel Gilbert. 1991. Simulating
speech systems. Computer Speech and Language, Vol
5, No.1, pp. 81-99.
Andrew Hickl, Johm Lehmann, John Williams, and
Sanda Harabagiu. 2004. Experiments with Interactive
Question Answering in Complex Scenarios. Proceed-
ings of HLT-NAACL2004 Workshop on Pragmatics of
Question Answering, pp. 60-69.
Joerrold Kaplan. 1983. Cooperative Responses from a
Portable Natural Language Database Query System.
Michael Brady and Robert C. Berwick eds. Compu-
tational Models of Discourse, pp. 167?208, The MIT
Press.
Tsuneaki Kato, Jun?ichi Fukumoto, Fumito Masui and
Noriko Kando. 2004a. Handling Information Access
Dialogue through QA Technologies ? A novel chal-
lenge for open-domain question answering ?. Pro-
ceedings of HLT-NAACL2004 Workshop on Pragmat-
ics of Question Answering, pp. 70-77.
Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.
2004b. Question Answering Challenge for Informa-
tion Access Dialogue ? Overview of NTCIR4 QAC2
Subtask 3 ?. Proceedings of NTCIR-4 Workshop Meet-
ing.
Tsuneaki Kato, Jun?ici Fukumoto and Fumito Masui.
2005. An Overview of NTCIR-5 QAC3. Proceedings
of Fifth NTCIR Workshop Meeting, pp. 361?372.
Tsuneaki Kato, Jun?ici Fukumoto, Fumito Masui and
Noriko Kando. 2006. Are Open-domain Question
Answering Technologies Useful for Information Ac-
cess Dialogues? ? An empirical study and a proposal
of a novel challenge ? ACL Trans. of Asian Language
Information Processing, In Printing.
Elizabeth D. Liddy. 2002. Why are People Asking
these Questions? : A Call for Bringing Situation into
Question-Answering System Evaluation. LREC Work-
shop Proceedings on Question Answering ? Strategy
and Resources, pp. 5-8.
NTCIR Project Home Page. 2006.
http://research.nii.ac.jp/?ntcadm/
index-en.html
Martha E. Pollack. 1989. Plans as Complex Mental At-
titudes. Philip R. Cohen, Jerry Morgan and Martha E.
Pollack eds. Intentions in Communication, pp. 77?103,
The MIT Press.
Sharon Small, Nobuyuki Shimizu, Tomek Strzalkowski,
and Liu Ting 2003. HITIQA: A Data Driven Ap-
proach to Interactive Question Answering: A Prelim-
inary Report AAAI 2003 Spring Symposium New Di-
rections in Question Answering, pp. 94-104.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building a
Question Answering Test Collection the Proceedings
of the 23rd Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pp. 200 - 207.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. Proceedings of TREC
2001.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
Question Answering Track. Proceedings of TREC
2004.
16
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 107?115,
Beijing, August 2010
Towards an optimal weighting of context words based on distance
Bernard Brosseau-Villeneuve*#, Jian-Yun Nie*, Noriko Kando#
* Universit? de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
Abstract
Word Sense Disambiguation (WSD) of-
ten relies on a context model or vector
constructed from the words that co-occur
with the target word within the same text
windows. In most cases, a fixed-sized
window is used, which is determined by
trial and error. In addition, words within
the same window are weighted uniformly
regardless to their distance to the target
word. Intuitively, it seems more reason-
able to assign a stronger weight to con-
text words closer to the target word. How-
ever, it is difficult to manually define the
optimal weighting function based on dis-
tance. In this paper, we propose a unsu-
pervised method for determining the op-
timal weights for context words accord-
ing to their distance. The general idea is
that the optimal weights should maximize
the similarity of two context models of the
target word generated from two random
samples. This principle is applied to both
English and Japanese. The context mod-
els using the resulting weights are used
in WSD tasks on Semeval data. Our ex-
perimental results showed that substantial
improvements in WSD accuracy can be
obtained using the automatically defined
weighting schema.
1 Introduction
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and V?ronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
words assumption, this means building p(x|t) =
count(x,t)?
x? count(x?,t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and
t should co-occur within a window of up to k
words or sentences. The bounds are usually se-
lected in an ad-hoc fashion to maximize system
performance. Occurrences inside the window of-
ten weight the same without regard to their po-
sition. This is counterintuitive. Indeed, a word
closer to the target word generally has a greater
semantic constraint on the target word than a more
distant word. It is however difficult to define
the optimal weighting function manually. To get
around this, some systems add positional features
for very close words. In information retrieval, to
model the strength of word relations, some studies
have proposed non-uniform weighting methods of
context words, which decrease the importance of
more distant words in the context vector. How-
ever, the weighting functions are defined manu-
ally. It is unclear that these functions can best cap-
ture the impact of the context words on the target
word.
In this paper, we propose an unsupervised
method to automatically learn the optimal weight
of a word according to its distance to the target
word. The general principle used to determine
such weight is that, if we randomly determine
two sets of windows containing the target word
from the same corpus, the meaning ? or mixture
of meanings for polysemic words ? of the target
word in the two sets should be similar. As the con-
text model ? a probability distribution for the con-
text words ? determines the meaning of the target
word, the context models generated from the two
sets should also be similar. The weights of con-
text words at different distance are therefore de-
107
termined so as to maximize the similarity of con-
text models generated from the two sets of sam-
ples. In this paper, we propose a gradient descent
method to find the optimal weights. We will see
that the optimal weighting functions are different
from those used in previous studies. Experimenta-
tion on Semeval-2007 English and Semeval-2010
Japanese lexical sample task data shows that im-
provements can be attained using the resulting
weighting functions on simple Na?ve Bayes (NB)
systems in comparison to manually selected func-
tions. This result validates the general principle
we propose in this paper.
The remainder of this paper is organized as fol-
lows: typical uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 to 6,
we show experimental results on English and
Japanese WSD. We conclude in Section 7 with
discussion and further possible extensions.
2 Uses of text windows
Modeling the distribution of words around one
target word, which we call context model, has
many uses. For instance, one can use it to define
a co-occurrence-based stemmer (Xu and Croft,
1998), which uses window co-occurrence statis-
tics to calculate the best equivalence classes for a
group of word forms. In the study of Xu and Croft,
they suggest using windows of up to 100 words.
Context models are also widely used in WSD.
For example, top performing systems on English
WSD tasks in Semeval-2007, such as NUS-ML
(Cai et al, 2007), all made use of bag-of-words
features around the target word. In this case, they
found that the best results can be achieved using a
window size of 3.
Both systems limit the size of their windows for
different purposes. The former uses a large size in
order to model the topic of the documents contain-
ing the word rather than the word?s meaning. The
latter would limit the size because bag-of-words
features further from the target word would not be
sufficiently related to its meaning (Ide and V?ro-
nis, 1998). We see that there is a compromise be-
tween taking fewer, highly related words, or tak-
ing more, lower quality words. However, there is
no principled way to determine the optimal size
of windows. The size is determined by trial and
error.
A more questionable aspect in the above sys-
tems is that for bag-of-words features, all words
in a window are given equal weights. This is
counterintuitive. One can easily understand that
a context word closer to the target word gener-
ally imposes a stronger constraint on the meaning
of the latter, than a more distant context word. It
is then reasonable to define a weighting function
that decreases along with distance. Several studies
in information retrieval (IR) have proposed such
functions to model the strength of dependency be-
tween words. For instance, Gao et al (2002)
proposed an exponential decay function to capture
the strength of dependency between words. This
function turns out to work better than the uniform
weighting in the IR experiments.
Song and Bruza (2003) used a fixed-size slid-
ing window to determine word co-occurrences.
This is equivalent to define a linear decay func-
tion for context words. The context vectors de-
fined this way are used to estimate similarity be-
tween words. A use of the resulting similarity in
query expansion in IR turned out to be successful
(Bai et al, 2005).
In a more recent study, Lv and Zhai (2009) eval-
uated several kernel functions to determine the
weights of context words according to distance,
including Gaussian kernel, cosine kernel, and so
on. As for the exponential and linear decaying
functions, all these kernel functions have fixed
shapes, which are determined manually.
Notice that the above functions have only been
tested in IR experiments. It is not clear how
these functions perform in WSD. More impor-
tantly, all the previous studies have investigated
only a limited number of weighting functions for
context words. Although some improvements us-
ing these functions have been observed in IR, it
is not clear whether the functions can best capture
the true impact of the context words on the mean-
ing of the target word. Although the proposed
functions comply with the general principle that
closer words are more important than more dis-
tant words, no principled way has been proposed
to determine the particular shape of the function
for different languages and collections.
108
In this paper, we argue that there is indeed a hid-
den weighting function that best capture the im-
pact of context words, but the function cannot be
defined manually. Rather, the best function should
be the one that emerges naturally from the data.
Therefore, we propose an unsupervised method to
discover such a function based on the following
principle: the context models for a target word
generated from two random samples should be
similar. In the next section, we will define in detail
how this principle is used.
3 Computing weights for distances
In this section, we present our method for choos-
ing how much a word occurrence should count in
the context model according to its distance to the
target word. In this study, for simplicity, we as-
sume that all word occurrences at a given distance
count equally in the context model. That is, we
ignore other features such as POS-tags, which are
used in other studies on WSD.
Let C be a corpus, W a set of text windows for
the target word w, cW,i,x the count of occurrences
of word x at distance i in W , cW,i the sum of these
counts, and ?i the weight put on one word occur-
rence at distance i. Then,
PML,W (x) =
?
i ?icW,i,x?
i ?icW,i
(1)
is the maximum likelihood estimator for x in the
context model of w. To counter the zero probabil-
ity problem, we apply Dirichlet smoothing with
the collection language model as a prior:
PDir,W (x) =
?
i ?icW,i,x + ?WP (x|C)?
i ?icW,i + ?W
(2)
The pseudo-count ?W can be a constant, or can be
found by using Newton?s method, maximizing the
log likelihood via leave-one-out estimation:
L?1(?|W, C) =?
i
?
x?V ?icW,i,x log
?icW,i,x??i+?P (x|C)?
j ?jcW,j??i+?
The general process, which we call automatic
Dirichlet smoothing, is similar to that described
in (Zhai and Lafferty, 2002).
To find the best weights for our model we pro-
pose the following process:
? Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
? We want to find ?? that maximizes the sim-
ilarity of the models obtained from the two
sets, by minimizing their mutual cross en-
tropy:
l(?) = H(PML,A, PDir,B) + (3)
H(PML,B , PDir,A)
In other words, we want ?i to represent how much
an occurrence at distance i models the context
better than the collection language model, whose
counts are weighted by the Dirichlet parameter.
We hypothesize that target words occur in limited
contexts, and as we get farther from them, the pos-
sibilities become greater, resulting in sparse and
less related counts. Since two different sets of the
same word are essentially noisy samples of the
same distribution, the weights maximizing their
mutual generation probabilities should model this
phenomenon.
One may wonder why we do not use a distri-
bution similarity metric such as Kullback?Leibler
(KL) divergence or Information Radius (IRad).
The reason is that with enough word occurrences
(big windows or enough samples), the most sim-
ilar distributions are found with uniform weights,
when all word counts are used. KL divergence
is especially problematic as, since it requires
smoothing, the weights will converge to the de-
generate weights ? = 0, where only the identical
smoothing counts remain. Entropy minimization
is therefore needed in the objective function.
To determine the optimal weight of ?i, we pro-
pose a simple gradient descent minimizing (3)
over ?. The following are the necessary deriva-
tives:
?l
??i
= ?H(PML,A, PDir,B)??i
+
?H(PML,B , PDir,A)
??i
?H
(
PML,W , PDir,(T?W )
)
??i
=
109
?
?
x?V
[?PML,W (x)
??i
log PDir,(T?W )(x)+
?PDir,(T?W )(x)
??i
? PML,W (x)PDir,(T?W )(x)
]
?PML,W (x)
??i
= cW,i,x ? PML,W (x)cW,i?
j ?jcW,j
?PDir,W (x)
??i
= cW,i,x ? PDir,W (x)cW,i?
j ?jcW,j + ?W
We use stochastic gradient descent: one word is
selected randomly, it?s gradient is computed, a
small gradient step is done and the process is re-
peated. A pseudo-code of the process can be
found in Algorithm 1.
Algorithm 1 LearnWeight(C, ?, )
? ? 1k
repeat
T ?{Get windows for next word}
(A,B) ?RandomPartition(T )
for W in A,B do
PML,W ?MakeML(W ,?)
?W ?ComputePseudoCount(W ,C)
PDir,W ?MakeDir( PML,W , ?W , C)
end for
grad ? ?H(PML,A, PDir,B) +
?H(PML,B, PDir,A)
? ? ?? ? grad?grad?
until ??i < 
return ?/max{?i}
Now, as the objective function would eventu-
ally go towards putting nearly all weight on ?1,
we hypothesize that the farthest distances should
have a near-zero contribution, and determine the
stop criterion as having one weight go under a
small threshold. Alternatively, a control set of
held out words can be used to observe the progress
of the objective function or the gradient length.
When more and more weight is put on the few
closest positions, the objective function and gra-
dient depends on less counts and will become less
stable. This can be used as a stop criterion.
The above weight learning process is applied
on an English collection and a Japanese collection
with ? =  = 0.001, and ? = 1000. In the next
sections, we will describe both resulting weight-
ing functions in the context of WSD experiments.
4 Classifiers for supervised WSD tasks
Since we use the same systems for both English
and Japanese experiments, we will briefly discuss
the used classifiers in this section. In both tasks,
the objective is to maximize WSD accuracy on
held-out data, given that we have a set of training
text passages containing a sense-annotated target
word.
The first of our baselines, the Most Frequent
Sense (MFS) system always selects the most fre-
quent sense in the training set. It gives us a lower
bound on system accuracies.
Na?ve Bayes (NB) classifiers score classes us-
ing the Bayes formula under a feature indepen-
dence assumption. Let w be the target word in a
given window sample to be classified, the scoring
formula for sense class S is:
Score(w,S) = P (S)PTar(w|S)?Tar??
xi?context(w) PCon(xi|S)
?Con?dist(xi)
where dist(xi) is the distance between the context
word xi and the target word w. The target word
being an informative feature present in all sam-
ples, we use it in a target word language model
PTar . The surrounding words are summed in the
context model PCon as shown in equation (1). As
we can see with the presence of ? in the equation,
the scoring follows the same weighting scheme as
we do when accumulating counts, since the sam-
ples to classify follow the same distribution as the
training ones. Also, when a language model uses
automatic Dirichlet smoothing, the impact of the
features against the prior is controlled with the
manual parameters ?Tar or ?Con. When a man-
ual smoothing parameter is used, it also handles
impact control. Our systems use the following
weight functions:
Uniform: ?i = 11?i??, where ? is a window size
and 1 the indicator function.
Linear: ?i = max{0, 1 ? (i ? 1)?}, where ? is
the decay rate.
110
Exponential: ?i = e?(i?1)? , where ? is the ex-
ponential parameter.
Learned: ?i is the weight learned as shown pre-
viously.
The parameters for NB systems are identical for
all words of a task and were selected by exhaustive
search, maximizing leave-one-out accuracy on the
training set. For each language model, we tried
Laplace, manual Dirichlet and automatic Dirichlet
smoothing.
For the sake of comparison, also we provide a
Support Vector Machine (SVM) classifier, which
produces the best results in Semeval 2007. We
used libSVM with a linear kernel, and regular-
ization parameters were selected via grid search
maximizing leave-one-out accuracy on the train-
ing set. We tested the following windows limits:
all words in sample, current sentence, and various
fixed window sizes. We used the same features
as the NB systems, testing Boolean, raw count,
log-of-counts and counts from weight functions
representations. Although non-Boolean features
had good leave-one-out precision on the training
data, since SVM does not employ smoothing, only
Boolean features kept good results on test data, so
our SVM baseline uses Boolean features.
5 WSD experiments on Semeval-2007
English Lexical Sample
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al,
2007). The task is to maximize WSD accuracy on
a selected set of polysemous words, 65 verbs and
35 nouns, for which passages were taken from the
WSJ Tree corpus. Passages contain a couple of
sentences around the target word, which is manu-
ally annotated with a sense taken from OntoNotes
(Hovy et al, 2006). The sense inventory is quite
coarse, with an average of 3.6 senses per word.
Instances count are listed in Table 1.
Train Test Total
Verb 8988 2292 11280
Noun 13293 2559 15852
Total 22281 4851
Table 1: Number of instances in the ELS data
Figure 1: Weight curve for AP88-90
Since there are only 100 target words and in-
stances are limited in the Semeval collection, we
do not have sufficient samples to estimate the op-
timal weights for context words. Therefore, we
used the AP88-90 corpus of the TREC collection
(CD 1 & 2) in our training process. The AP col-
lection contains 242,918 documents. Since our
classifiers use word stems, the collection was also
stemmed with the Porter stemmer and sets of win-
dows were built for all word stems. To get near-
uniform counts in all distances, only full win-
dows with a size of 100, which was considered
big enough without any doubt, were kept. In order
to get more samples, windows to the right and to
the left were separated. For each target word, we
used 1000 windows. A stoplist of the top 10 fre-
quent words was used, but place holders were left
in the windows to preserve the distances. Mul-
tiple consecutive stop words (ex: ?of the?) were
merged, and the target word stem, being the same
for all samples of a set, was ignored in the con-
struction of context models. The AP collection re-
sults in 32,650 target words containing 5,870,604
windows. The training process described in Sec-
tion 3 is used to determine the best weights of con-
text words. Figure 1 shows the first 40 elements
of the resulting weighting function curve.
As we can see, the curve is neither exponen-
tial, linear, or any of the forms used by Lv and
Zhai. Its form is rather similar to x??, or rather
log?1(? + x) minus some constant. The decrease
111
System Cross-Val (%) Test set (%)
MFS 78.66 77.76
Uniform NB 86.04 84.52
SVM 85.53 85.03
Linear NB 86.89 85.71
Exp. NB 87.80 86.23
Learned NB 88.46 86.70
Table 2: WSD accuracy on Semeval-2007 ELC
rate is initially very high and then reduces as it
becomes closer to zero. This long tail is not
present in any of the previously suggested func-
tions. The large difference between the above op-
timal weighting function and the functions used
in previous studies would indicate that the latter
are suboptimal. Also, as we can see, the rela-
tion between context words and the target word
is mostly gone after a few words. This would
motivate the commonly used very small windows
when using a uniform weights, since using a big-
ger window would further widen the gap between
the used weight and the optimal ones.
Now for the system settings, the context words
were processed the same way as the external cor-
pus. The target word was used without stemming
but had the case stripped. The NB systems used
the concatenation of the AP collection and the
Semeval data for the collection language model.
This is motivated by the fact that the Semeval data
is not balanced: it contains only a small number of
passages containing the target words. This makes
words related to them unusually frequent. The
class priors used an absolute discounting of 0.5 on
class counts. Uniform NB uses a window of size 4,
a Laplace smoothing of 0.65 on PTar and an au-
tomatic Dirichlet with ?Con = 0.7 on PCon. Lin-
ear NB has ? = 0.135, uses a Laplace smoothing
of 0.85 on PTar and an automatic Dirichlet with
?Con = 0.985 on PCon. Exp NB has ? = 0.27,
uses a Laplace smoothing of 2.8 on PTar and an
automatic Dirichlet with ?Con = 1.01 on PCon.
The SVM system uses a window of size 3. Our
system, Learned NB uses a Laplace smoothing of
1.075 on PTar , and an automatic Dirichlet with
?Con = 1.025 on PCon. The results on WSD are
listed in Table 2. WSD accuracy is measured by
the proportion of correctly disambiguated words
among all the word samples. The cross-validation
is performed on the training data with leave-one-
out and is shown as a hint of the capacity of the
models. A randomization test comparing Expo-
nential NB and Learned NB gives a p-value of
0.0508, which is quite good considering the exten-
sive trials used to select the exponential parameter
in comparison to a single curve computed from a
different corpus. This performance is comparable
to the current state of the art. It outperforms most
of the systems participating in the task (Pradhan et
al., 2007). Out of 14 systems, the best results had
accuracies of 89.1*, 89.1*, 88.7, 86.9 and 86.4 (*
indicates post-competition submissions). Notice
that most previous systems used SVM with ad-
ditional features such as local collocations, posi-
tional word features and POS tags. Our approach
only uses bag-of-words in a Na?ve Bayes classi-
fier. Therefore, the performance of our method is
sub-optimal. With additional features and better
classification methods, we can expect that better
performance can be obtained. In future work, we
will investigate the applications of SVM with our
new term weighting scheme, together with addi-
tional types of features.
6 WSD experiments on Semeval-2010
Japanese Lexical Sample
The Semeval-2010 Japanese WSD task (Okumura
et al, 2010) consists of 50 polysemous words
for which examples were taken from the BCCWJ
corpus (Maekawa, 2008). It was manually seg-
mented, POS-tagged, and annotated with senses
taken from the Iwanami Kokugo dictionary. The
selected words have 50 samples for both the train-
ing and test set. The task is identical to the ELS
of the previous experiment.
Since the data was again insufficient to com-
pute the optimal weighting curve, we used the
Mainichi-2005 corpus of NTCIR-8. We tried to
reproduce the same kind of segmentation as the
training data by using the Chasen parser with Uni-
Dic, which nevertheless results in different word
segments as the training data. For the corpus and
Semeval data, conjugations (setsuzoku-to, jod?-
shi, etc.), particles (all jo-shi), symbols (blanks,
kig?, etc.), and numbers were stripped. When a
112
Figure 2: Weight curve for Mainichi 2005
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(Chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
As we can see, the general form of the curve
is similar to that of the English collection, but
is steeper. This suggests that the meaning of
Japanese words can be determined using only
the closest context words. Words further than a
few positions away have very small impact on
the target word. This can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-Verb-
Complement language, Japanese is considered
Subject-Complement-Verb. Verbs, mostly found
at the end of a sentence, can be far apart from their
subject, and vice versa. The window distance is
therefore less useful to capture the relatedness in
Japanese than in English since Japanese has more
non-local dependencies.
The Semeval Japanese test data being part of a
balanced corpus, untagged occurrences of the tar-
get words are plenty, so we can benefit from using
the collection-level counts for smoothing. Uni-
form NB uses a window of size 1, manual Dirich-
let smoothing of 4 for PTar and 90 for the PCon.
Linear NB has ? = 0.955, uses a manual Dirichlet
smoothing of 6.25 on PTar and manual Dirichlet
System Cross-Val (%) Test set (%)
MFS 75.23 68.96
SVM 82.55 74.92
Uniform NB 82.47 76.16
Linear NB 82.63 76.48
Exp. NB 82.68 76.44
Learned NB 82.67 76.52
Table 3: WSD accuracy on Semeval-2010 JWSD
smoothing with ?Con = 65 on PCon. Exp NB
has ? = 2.675, uses a manual Dirichlet smooth-
ing of 6.5 on PTar and a manual Dirichlet of 70
on PCon. The SVM system uses a window size of
1 and Boolean features. Learned NB used a man-
ual Dirichlet smoothing of 4 for PTar and auto-
matic Dirichlet smoothing with ?Con = 0.6 for
PCon. We believe this smoothing is beneficial
only on this system because it uses more words
(the long tail), that makes the estimation of the
pseudo-count more accurate. Results on WSD are
listed in Table 3. As we can see, the difference be-
tween the NB models is less substantial than for
English. This may be due to differences in the
segmentation parameters of our external corpus:
we used the human-checked segmentation found
in the Semeval data for classification, but used a
parser to segment our external corpus for weight
learning. We are positive that the Chasen parser
with the UniDic dictionary was used to create the
initial segmentation in the Semeval data, but there
may be differences in versions and the initial seg-
mentation results were further modified manually.
Another reason for the results could be that the
systems use almost the same weights: Uniform
NB and SVM both used windows of size 1, and
the Japanese curve is steeper than the English one,
making the context model account to almost only
immediately adjacent words. So, even if our con-
text model contains more context words at larger
distances, their weights are very low. This makes
all context model quite similar. Nevertheless, we
still observe some gain in WSD accuracy. These
results show that the curves work as expected even
in different languages. However, the weighting
curve is strongly language-dependent. It could
also be collection-dependent ? we will investigate
113
this aspect in the future, using different collec-
tions.
7 Conclusions
The definition of context vector and context model
is critical in WSD. In previous studies in IR, de-
caying weight along with distance within a text
window have been proposed. However, the de-
caying functions are defined manually. Although
some of the functions produced better results than
the uniform weighting, there is no evidence show-
ing that these functions best capture the impact
of the context words on the meaning of the tar-
get word. This paper proposed an unsupervised
method for finding optimal weights for context
words according to their distance to the target
word. The general idea was to find the weights
that best fit the data, in such a way that the context
models for the same target word generated from
two random windows samples become similar. It
is the first time that this general principle is used
for this purpose. Our experiments on WSD in En-
glish and Japanese suggest the validity of the prin-
ciple.
In this paper, we limited context models to bag-
of-words features, excluding additional features
such as POS-tags. Despite this simple type of fea-
ture and the use of a simple Na?ve Bayes classifier,
the WSD accuracy we obtained can rival the other
state-of-the-art systems with more sophisticated
features and classification algorithms. This result
indicates that a crucial aspect in WSD is the def-
inition of an appropriate context model, and our
weighting method can generate more reasonable
weights of context words than using a predefined
decaying function.
Our experiments also showed that the optimal
weighting function is language-dependent. We
obtained two different functions for English and
Japanese, although their general shapes are simi-
lar. In fact, the optimal weighting function reflects
the linguistic properties: as dependent words in
Japanese can be further away from the target word
due to its linguistic structure, the optimal weight-
ing quickly decays, meaning that we can rely less
on distant context words. This also shows a lim-
itation of this study: distance is not the sole cri-
terion to determine the impact of a context word.
Other factors, such as POS-tag and syntactic de-
pendency, can play an important role in the con-
text model. These additional factors are comple-
mentary to the distance criterion and our approach
can be extended to include such additional fea-
tures. This extension is part of our future work.
Another limitation of straight window distance
is that all words introduce the same distance, re-
gardless of their nature. In our experiments, to
make the distance a more sensible metric, we
merged consecutive stop words in one placeholder
token. The idea behind this it that some words,
such as stop words, should introduce less distance
than others. On the opposite, we can easily un-
derstand that tokens such as commas, full stops,
parentheses and paragraph should introduce a big-
ger distance than regular words. We could there-
fore use a congruence score for a word, an indi-
cator showing on average how much what comes
before is similar to what comes after the word.
Also, we have combined our weighting schema
with NB classifier. Other classifiers such as SVM
could lead to better results. The utilization of our
new weighting schema with SVM is another fu-
ture work.
Finally, the weights computed with our method
has been used in WSD tasks. The weights could
be seen as the expected strength of relation be-
tween two words in a document according to their
distance. The consideration of word relationships
in documents and queries is one of the endeav-
ors in current research in IR. The new weighting
schema could be easily integrated with a depen-
dency model in IR. We plan to perform such inte-
gration in the future.
Acknowledgments
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
114
References
Bai, Jing, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005. Query expansion using term
relationships in language models for information re-
trieval. In CIKM ?05 Proceedings, pages 688?695,
New York, NY, USA. ACM.
Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ?07 Proceedings,
pages 249?252, Morristown, NJ, USA. Association
for Computational Linguistics.
Cheung, Percy and Pascale Fung. 2004. Translation
disambiguation in mixed language queries. Ma-
chine Translation, 18(4):251?273.
Gao, Jianfeng, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ?02 Proceedings, pages 183?190, New York,
NY, USA. ACM.
Ide, Nancy and Jean V?ronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2?40.
Lv, Yuanhua and ChengXiang Zhai. 2009. Positional
language models for information retrieval. In SIGIR
?09 Proceedings, pages 299?306, New York, NY,
USA. ACM.
Maekawa, Kikuo. 2008. Compilation of the bal-
anced corpus of contemporary written japanese in
the kotonoha initiative (invited paper). In ISUC
?08 Proceedings, pages 169?172, Washington, DC,
USA. IEEE Computer Society.
Navigli, Roberto. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):1?69.
Okumura, Manabu, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ?10 Proceedings. Asso-
ciation for Computational Linguistics.
Pradhan, Sameer S., Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ?07 Proceedings, pages 87?92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Song, D. and P. D. Bruza. 2003. Towards context sen-
sitive information inference. Journal of the Amer-
ican Society for Information Science and Technol-
ogy, 54(4):321?334.
Xu, Jinxi and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61?81.
Zhai, ChengXiang and John Lafferty. 2002. Two-
stage language models for information retrieval. In
SIGIR ?02 Proceedings, pages 49?56, New York,
NY, USA. ACM.
115
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375?378,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
RALI: Automatic weighting of text window distances
Bernard Brosseau-Villeneuve*#, Noriko Kando#, Jian-Yun Nie*
* Universit? de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
Abstract
Systems using text windows to model
word contexts have mostly been using
fixed-sized windows and uniform weights.
The window size is often selected by trial
and error to maximize task results. We
propose a non-supervised method for se-
lecting weights for each window distance,
effectively removing the need to limit win-
dow sizes, by maximizing the mutual gen-
eration of two sets of samples of the same
word. Experiments on Semeval Word
Sense Disambiguation tasks showed con-
siderable improvements.
1 Introduction
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and V?ronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
word assumption, this means building p(x|t) =
count(x,t)
?
x
?
count(x
?
,t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and t
should co-occur within a window of up to k words
or sentences. The bounds are usually selected as
to maximize system performance. Occurrences in-
side the window usually weight the same with-
out regard to their position. This is counterintu-
itive. Indeed, a word closer to the target word usu-
ally has a greater semantic constraint on the tar-
get word than a more distant word. Some studies
have also proposed decaying factors to decrease
the importance of more distant words in the con-
text vector. However, the decaying functions are
defined manually. It is unclear that the functions
defined can capture the true impact of the con-
text words on the target word. In this paper, we
propose an unsupervised method to automatically
learn the optimal weight of a word according to its
distance to the target word. The general idea used
to determine such weight is that, if we randomly
determine two sets of texts containing the target
word, the resulting probability distributions for its
context words in the two sets should be similar.
Therefore, the weights of context words at differ-
ent distance are determined so as to maximize the
mutual generation probabilities of two sets of sam-
ples. Experimentation on Semeval-2007 English
and Semeval-2010 Japanese lexical sample task
data shows that improvements can automatically
be attained on simple Naive Bayes (NB) systems
in comparison to the best manually selected fixed
window system.
The remainder of this paper is organized as fol-
lows: example uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 and
5, we show experimental results on English and
Japanese WSD. We conclude in Section 6 with
discussion and further possible extensions.
2 Uses of text windows
Modeling the distribution of words around one
target word has many uses. For instance, the
Xu&Croft co-occurrence-based stemmer (Xu and
Croft, 1998) uses window co-occurrence statis-
tics to calculate the best equivalence classes for
a group of word forms. They suggest using win-
dows of up to 100 words. Another example can be
found in WSD systems, where a shorter window is
preferred. In Semeval-2007, top performing sys-
tems on WSD tasks, such as NUS-ML (Cai et al,
2007), made use of bag-of-word features around
the target word. In this case, they found that the
best results can be achieved using a window size
of 3.
375
Both these systems limit the size of their win-
dows for different purposes. The former aims to
model the topic of the documents containing the
word rather than the word?s meaning. The latter
limits the size because bag-of-word features fur-
ther from the target word would not be sufficiently
related to its meaning (Ide and V?ronis, 1998). We
see that because of sparsity issues, there is a com-
promise between taking few, highly related words,
or taking several, lower quality words.
In most current systems, all words in a window
are given equal weight, but we can easily under-
stand that the occurrences of words should gener-
ally count less as they become farther; they form
a long tail that we should use. Previous work pro-
posed using non-linear functions of the distance
to model the relation between two words. For in-
stance, improvements can be obtained by using an
exponential function (Gao et al, 2002). Yet, there
is no evidence that the exponential ? with its man-
ually selected parameter ? is the best function.
3 Computing weights for distances
In this section, we present our method for choos-
ing howmuch a word should count according to its
distance to the target word. First, for some defini-
tions, let C be a corpus, W a set of text windows,
c
W,i,x
the count of occurrences of word x at dis-
tance i in W , c
W,i
the sum of these counts, and ?
i
the weight put on one word at distance i. Then,
P
ML,W
(x) =
?
i
?
i
c
W,i,x
?
i
?
i
c
W,i
(1)
is the maximum likelihood estimator for x. To
counter the zero-probability problem, we apply
Dirichlet smoothing with the collection language
model as a prior:
P
Dir,W
(x) =
?
i
?
i
c
W,i,x
+ ?
W
P (x|C)
?
i
?
i
c
W,i
+ ?
W
(2)
The pseudo-count ?
W
is found by using Newton?s
method via leave-one-out estimation. We follow
the procedure shown in (Zhai and Lafferty, 2002),
but since occurrences have different weights, the
log-likelihood is changed to
L
?1
(?|W, C) = (3)
?
i
?
x?V
?
i
c
W,i,x
log
?
i
c
W,i,x
??
i
+?P (x|C)
?
j
?
j
c
W,j
??
i
+?
To find the best weights for our model we pro-
pose the following:
? Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
? We want to find ?
?
that maximizes the mu-
tual generation of the two sets, by minimizing
their cross-entropy:
l(?) = H(P
ML,A
, P
Dir,B
) + H(P
ML,B
, P
Dir,A
)
(4)
In other words, we want ?
i
to represent how
much an occurrence at distance i models the con-
text better than the collection language model,
whose counts are controlled by the Dirichlet
pseudo-count. We hypothesize that target words
occurs in limited contexts, and as we get farther
from them, the possibilities become greater, re-
sulting in sparse and less related counts.
3.1 Gradient descent
We propose a simple gradient descent minimiz-
ing (4) over ?. For the following experiments,
we used one single curve for all words in a task.
We used the mini-batch type of gradient descent:
the gradients of a fixed amount of target words are
summed, a gradient step is done, and the proces
is repeated while cycling the data. The starting
state was with all ?
i
to one, the batch size of 50
and a learning rate of 1. We notice that as the al-
gorithm progress, weights on close distances in-
crease and the farthest decrease. As further dis-
tances contribute less and less, middle distances
start to decay more and more, until at some point,
all distances but the closest start to decrease, head-
ing towards a degenerate solution. We therefore
suggest using the observation of several consecu-
tive decreases of all except ?
1
as an end criterion.
We used 10 consecutive steps for our experiments.
4 Experiments on Semeval-2007 English
Lexical Sample
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al,
2007). It consists of a selected set of polysemous
words, contained within passages where a sense
taken from a sense inventory is manually anno-
tated. The task is to create supervised classifiers
maximizing accuracy on test data.
Since there are only 50 words and instances are
few, we judged there was not enough data to com-
pute weights. Instead, we used the AP Newswire
corpus of the TREC collection (CD 1 & 2). Words
376
were stemmed with the Porter stemmer and text
windows were grouped for all words. For sim-
plicity and efficiency, windows to the right and to
the left were considered independent, and we only
kept words with between 30 and 1000 windows.
Also, only windows with a size of 100, which was
considered big enough without any doubt, were
kept. A stop list of the top 10 frequent words was
used, but place holders were left in the windows to
preserve the distances. Multiple consecutive stop
words (ex: ?of the?) were merged, and the tar-
get word, being the same for all samples of a set,
was ignored. This results in 32,650 sets contain-
ing 5,870,604 windows. In Figure 1, we can see
the resulting weight curve.
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 1: Weight curve for AP Newswire
Since the curve converges, words over the 100th
distance were assigned the minimumweight found
in the curve. From this we constructed NB models
whose class priors used an absolute discounting of
0.5. The collection language model used the con-
catenation of the AP collection and the Semeval
data. As the unstemmed target word is an impor-
tant feature it was added to the models. It?s weight
was chosen to be 0.7 by maximizing accuracy on
one-held-out cross-validation of the training data.
The results are listed in Table 1.
System Cross-Val (%) Test set (%)
Prior only 78.66 77.76
Best uniform 85.48 83.28
RALI-2 88.23 86.45
Table 1: WSD accuracy on Semeval-2007 ELC
We used two baselines: most frequent sense
(prior only), and the best uniform (except target
word) fixed size window found from extensive
search on the training data. The best settings were
a window of size 4, with a weight of 4.4 on the
target word and a Laplace smoothing of 2.9. The
improvements seen using our system are substan-
tial, beating most of the systems originally pro-
posed for the task (Pradhan et al, 2007). Out
of 15 systems, the best results had accuracies of
89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates post-
competition submissions). Notice that most were
using Support Vector Machine (SVM) with bag-
of-word features in a very small window, local col-
locations and POS tags. In our future work, we
will investigate the applications of SVM with our
new term weighting scheme.
5 Experiments on Semeval-2010
Japanese WSD
The Semeval-2010 Japanese WSD task (Okumura
et al, 2010) consists of 50 polysemous words
for which examples were taken from the BC-
CWJ tagged corpus. It was manually segmented,
tagged, and annotated with senses taken from the
Iwanami Kokugo dictionary. The task is identical
to the ELS of the previous experiment.
Since the data was again insufficient to com-
pute curves, we used the Mainichi-2005 corpus of
NTCIR-8. We tried to reproduce the same kind
of segmentation as the training data by using the
Chasen parser with UniDic. For the corpus and
Semeval data, conjugations (setsuzoku-to, jod?-
shi, etc.), particles (all jo-shi), symbols (blanks,
kig?, etc.), and numbers were stripped. When a
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
The NB models are the same as in the previous
experiments. Target words were again added the
same way as in the ELS task. The best fixed win-
dow model was found to have a window size of 1
with a target word weight of 0.6 and used manual
Dirichlet smoothing with a pseudo-count of 110.
We submited two systems with the following set-
tings: RALI-1 used manual Dirichlet smoothing
and 0.9 for the target word. RALI-2 used auto-
377
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 2: Weight curve for Mainichi Shinbun 2005
matic Dirichlet smoothing and 1.7 for the target
word weight. Results are listed in Table 2.
System Cross-Val (%) Test set (%)
prior only 75.23 68.96
Best uniform 82.29 76.12
RALI-1 82.77 75.92
RALI-2 83.05 76.36
Table 2: WSD accuracy on Semeval-2010 JWSD
As we can see, the results are not significantly
different from the best uniform model. This may
be due to differences in the segmentation parame-
ters of our external corpus. Another reason could
be that the systems use almost the same weights:
the best fixed window had size 1, and the Japanese
curve is steeper than the English one.
This steeper curve can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-
Verb-Complement language, Japanese is consid-
ered Subject-Complement-Verb. Verbs are mostly
found at the end of the sentence, far from their sub-
ject, and vice versa. The window distance is there-
fore less useful in Japanese than in English since
it has more non-local dependencies. These results
show that the curves work as expected even in dif-
ferent languages.
6 Conclusions
This paper proposed an unsupervised method for
finding weights for counts in text windows ac-
cording to their distance to the target word. Re-
sults from the Semeval-2007 English lexical sam-
ple showed a substantial improvement in preci-
sion. Yet, as we have seen with the Japanese task,
window distance is not always a good indicator of
word relatedness. Fortunately, we can easily imag-
ine extensions to the current scheme that bins word
counts by factors other than word distance. For in-
stance, we could also bin counts by parsing tree
distance, sentence distance or POS-tags.
Acknowledgments
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
References
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ?07 Proceedings,
pages 249?252, Morristown, NJ, USA. Association
for Computational Linguistics.
Jianfeng Gao, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ?02 Proceedings, pages 183?190, New York,
NY, USA. ACM.
Nancy Ide and Jean V?ronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2?40.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Comput. Surv., 41(2):1?69.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ?10 Proceedings. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ?07 Proceedings, pages 87?92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Jinxi Xu and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61?81.
ChengXiang Zhai and John Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR
?02 Proceedings, pages 49?56, NewYork, NY, USA.
ACM.
378
