Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 476?481,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Mutual Disambiguation for Entity Linking
Eric Charton
Polytechnique Montr?eal
Montr?eal, QC, Canada
eric.charton@polymtl.ca
Marie-Jean Meurs
Concordia University
Montr?eal, QC, Canada
marie-jean.meurs@concordia.ca
Ludovic Jean-Louis
Polytechnique Montr?eal
ludovic.jean-louis@polymtl.ca
Michel Gagnon
Polytechnique Montr?eal
michel.gagnon@polymtl.ca
Abstract
The disambiguation algorithm presented in
this paper is implemented in SemLinker, an
entity linking system. First, named entities
are linked to candidate Wikipedia pages by
a generic annotation engine. Then, the al-
gorithm re-ranks candidate links according to
mutual relations between all the named enti-
ties found in the document. The evaluation
is based on experiments conducted on the test
corpus of the TAC-KBP 2012 entity linking
task.
1 Introduction
The Entity Linking (EL) task consists in linking
name mentions of named entities (NEs) found in a
document to their corresponding entities in a ref-
erence Knowledge Base (KB). These NEs can be
of type person (PER), organization (ORG), etc.,
and they are usually represented in the KB by a
Uniform Resource Identifier (URI). Dealing with
ambiguity is one of the key difficulties in this task,
since mentions are often highly polysemous, and
potentially related to many different KB entries.
Various approaches have been proposed to solve
the named entity disambiguation (NED) problem.
Most of them involve the use of surface forms ex-
tracted from Wikipedia. Surface forms consist of
a word or a group of words that match lexical units
like Paris or New York City. They are used as
matching sequences to locate corresponding can-
didate entries in the KB, and then to disambiguate
those candidates using similarity measures.
The NED problem is related to the Word Sense
Disambiguation (WSD) problem (Navigli, 2009),
and is often more challenging since mentions of
NEs can be highly ambiguous. For instance,
names of places can be very common as is Paris,
which refers to 26 different places in Wikipedia.
Hence, systems that attempt to address the NED
problem must include disambiguation resources.
In the context of the Named Entity Recognition
(NER) task, such resources can be generic and
generative. This generative approach does not ap-
ply to the EL task where each entity to be linked to
a semantic description has a specific word context,
marker of its exact identity.
One of the classical approach to conduct the
disambiguation process in NED applications is to
consider the context of the mention to be mapped,
and compare this context with contextual informa-
tion about the potential target entities (see for in-
stance the KIM system (Popov et al, 2003)). This
is usually done using similarity measures (such as
cosine similarity, weighted Jaccard distance, KL
divergence...) that evaluate the distance between
a bag of words related to a candidate annotation,
and the words surrounding the entity to annotate
in the text.
In more recent approaches, it is suggested that
annotation processes based on similarity distance
measures can be improved by making use of other
annotations present in the same document. Such
techniques are referred to as semantic related-
ness (Strube and Ponzetto, 2006), collective dis-
ambiguation (Hoffart et al, 2011b), or joint dis-
ambiguation (Fahrni et al, 2012). The idea is to
evaluate in a set of candidate links which one is
the most likely to be correct by taking the other
links contained in the document into account. For
example, if a NE describes a city name like Paris,
it is more probable that the correct link for this
city name designates Paris (France) rather than
Paris (Texas) if a neighbor entity offers candidate
links semantically related to Paris (France) like
the Seine river or the Champs-Elys?ees. Such tech-
niques mostly involve exploration of graphs result-
ing of all the candidate annotations proposed for a
given document, and try to rank the best candi-
dates for each annotation using an ontology. The
ontology (like YAGO or DBPedia) provides a pre-
476
existing set of potential relations between the enti-
ties to link (like for instance, in our previous exam-
ple, Paris (France) has river Seine) that will
be used to rank the best candidates according to
their mutual presence in the document.
In this paper we explore the capabilities of a dis-
ambiguation algorithm using all the available an-
notation layers of NEs to improve their links. The
paper makes the following novel propositions: 1)
the ontology used to evaluate the relatedness of
candidates is replaced by internal links and cate-
gories from the Wikipedia corpus; 2) the coher-
ence of entities is improved prior to the calcula-
tion of semantic relatedness using a co-reference
resolution algorithm, and a NE label correction
method; 3) the proposed method is robust enough
to improve the performance of existing entity link-
ing annotation engines, which are capable of pro-
viding a set of ranked candidates for each annota-
tion in a document.
This paper is organized as follows. Section 2
describes related works. The proposed method is
presented in Section 3 where we explain how our
SemLinker system prepares documents that con-
tain mentions to disambiguate, then we detail the
disambiguation algorithm. The evaluation of the
complete system is provided in Section 4. Finally,
we discuss the obtained results, and conclude.
2 Related Work
Entity annotation and linking in natural language
text has been extensively studied in NLP research.
A strong effort has been conducted recently by the
TAC-KBP evaluation task (Ji et al, 2010) to cre-
ate standardized corpus, and annotation standards
based on Wikipedia for evaluation and comparison
of EL systems. In this paper, we consider the TAC-
KBP framework. We describe below some recent
approaches proposed for solving the EL task.
2.1 Wikipedia-based Disambiguation Methods
The use of Wikipedia for explicit disambiguation
dates back to (Bunescu and Pasca, 2006) who built
a system that compared the context of a mention
to the Wikipedia categories of an entity candidate.
Lately, (Cucerzan, 2007; Milne and Witten, 2008;
Nguyen and Cao, 2008) extended this framework
by using richer features for similarity comparison.
Some authors like Milne and Witten (2008) uti-
lized machine learning methods rather than a sim-
ilarity function to map mentions to entities. They
also introduced the notion of semantic relatedness.
Alternative propositions were suggested in other
works like (Han and Zhao, 2009) that considered
the relatedness of common noun phrases in a men-
tion context with Wikipedia article names. While
all these approaches focus on semantic relation be-
tween entities, their potential is limited by the sep-
arate mapping of candidate links for each mention.
2.2 Semantic Web Compliant Methods
More recently, several systems have been
launched as web services dedicated to EL tasks.
Most of them are compliant with new emergent
semantic web standards like LinkedData network.
DBPedia Spotlight (Mendes et al, 2011) is a
system that finds mentions of DBpedia (Auer
et al, 2007) resources in a textual document.
Wikimeta (Charton and Gagnon, 2012) is another
system relying on DBpedia. It uses bags of words
to disambiguate semantic entities according to
a cosine similarity algorithm. Those systems
have been compared with commercial ones
like AlchemyAPI, Zemanta, or Open Calais
in (Gangemi, 2013). The study showed that
they perform differently on various essential
aspects of EL tasks (mention detection, linking,
disambiguation). This suggests a wide range of
potential improvements on many aspects of the
EL task. Only some of these systems introduce
the semantic relatedness in their methods like
the AIDA (Hoffart et al, 2011b) system. It
proposes a disambiguation method that combines
popularity-based priors, similarity measures, and
coherence. It relies on the Wikipedia-derived
YAGO2 (Hoffart et al, 2011a) knowledge base.
3 Proposed Algorithm
We propose a mutual disambiguation algorithm
that improves the accuracy of entity links in a doc-
ument by using successive corrections applied to
an annotation object representing this document.
The annotation object is composed of information
extracted from the document along with linguistic
and semantic annotations as described hereafter.
3.1 Annotation Object
Documents are processed by an annotator capable
of producing POS tags for each word, as well as
spans, NE surface forms, NE labels and ranked
candidate Wikipedia URIs for each candidate NE.
For each document D, this knowledge is gathered
477
in an array called annotation object, which has ini-
tially one row per document lexical unit. Since the
system focuses on NEs, rows with lexical units
that do not belong to a NE SF are dropped from
the annotation object, and NE SF are refined as de-
scribed in (Charton et al, 2014). When NE SF are
spanned over several rows, these rows are merged
into a single one. Thus, we consider an annotation
object A
D
, which is an array with a row for each
NE, and columns storing related knowledge.
If n NEs were annotated in D, then A
D
has n
rows. If l candidate URIs are provided for each
NE, then A
D
has (l + 4) columns c
u,u?{1,l+4}
.
Columns c
1
to c
l
store Wikipedia URIs associated
with NEs, ordered by decreasing values of likeli-
hood. Column c
l+1
stores the offset of the NEs,
c
l+2
stores their surface forms, c
l+3
stores the NE
labels (PER, ORG, ...), and c
l+4
stores the (vec-
tors of) POS tags associated with the NE surface
forms. A
D
contains all the available knowledge
about the NEs found inD. Before being processed
by the disambiguation module,A
D
is dynamically
updated by correction processes.
3.2 Named Entity Label Correction
To support the correction process based on co-
reference chains, the system tries to correct NE
labels for all the NEs listed in the annotation ob-
ject. The NE label correction process assigns the
same NE label to all the NEs associated with the
same first rank URI. For all the rows inA
D
, sets of
rows with identical first rank URIs are considered.
Then, for each set, NE labels are counted per type,
and all the rows in a same set are updated with the
most frequent NE label found in the set, i.e. all the
NEs in this set are tagged with this label.
3.3 Correction Based on Co-reference Chains
First rank candidate URIs are corrected by a pro-
cess that relies on co-reference chains found in
the document. The co-reference detection is con-
ducted using the information recorded in the anno-
tation object. Among the NEs present in the docu-
ment, the ones that co-refer are identified and clus-
tered by logical rules applied to the content of the
annotation object. When a co-reference chain of
NEs is detected, the system assigns the same URI
to all the members of the chain. This URI is se-
lected through a decision process that gives more
weight to longer surface forms and frequent URIs.
The following example illustrates an application
of this correction process:
Three sentences are extracted from a document
about Paris, the French capital. NEs are indicated
in brackets, first rank URIs and surface forms are
added below the content of each sentence.
- [Paris] is famous around the world.
URI
1
: http://en.wikipedia.org/wiki/Paris Hilton
NE surface form: Paris
- The [city of Paris] attracts millions of tourists.
URI
1
: http://en.wikipedia.org/wiki/Paris
NE surface form: city of Paris
- The [capital of France] is easy to reach by train.
URI
1
: http://en.wikipedia.org/wiki/Paris
NE surface form: capital of France
The three NEs found in these sentences com-
pose a co-reference chain. The second NE has
a longer surface form than the first one, and
its associated first rank URI is the most fre-
quent. Hence, the co-reference correction pro-
cess will assign the right URI to the first NE
(URI
1
: http://en.wikipedia.org/wiki/Paris), which
was wrongly linked to the actress Paris Hilton.
3.4 Mutual Disambiguation Process
The extraction of an accurate link is a process oc-
curring after the URI annotation of NEs in the
whole document. The system makes use of all
the semantic content stored in A
D
to locally im-
prove the precision of each URI annotation in the
document. The Mutual Disambiguation Process
(MDP) relies on the graph of all the relations (in-
ternal links, categories) between Wikipedia con-
tent related to the document annotations.
A basic example of semantic relatedness that
should be captured is explained hereafter. Let us
consider the mention IBM in a given document.
Candidate NE annotations for this mention can be
International Business Machine or International
Brotherhood of Magicians. But if the IBM men-
tion co-occurs with a Thomas Watson, Jr mention
in the document, there will probably be more links
between the International Business Machine and
Thomas Watson, Jr related Wikipedia pages than
between the International Brotherhood of Magi-
cians and Thomas Watson, Jr related Wikipedia
pages. The purpose of the MDP is to capture this
semantic relatedness information contained in the
graph of links extracted from Wikipedia pages re-
lated to each candidate annotation.
In MDP, for each Wikipedia URI candidate an-
notation, all the internal links and categories con-
tained in the source Wikipedia document related
478
to this URI are collected. This information will be
used to calculate a weight for each of the l can-
didate URI annotations of each mention. For a
given NE, this weight is expected to measure the
mutual relations of a candidate annotation with all
the other candidate annotations of NEs in the doc-
ument. The input of the MDP is an annotation
object A
D
with n rows, obtained as explained in
Section 3.1. For all i ? [[1, n]], k ? [[1, l]], we build
the set S
k
i
, composed of the Wikipedia URIs and
categories contained in the source Wikipedia doc-
ument related to the URI stored in A
D
[i][k] that
we will refer to as URI
k
i
to ease the reading.
Scoring:
For all i, j ? [[1, n]], k ? [[1, l]], we want to cal-
culate the weight of mutual relations between the
candidate URI
k
i
and all the first rank candidates
URI
1
j
for j 6= i. The calculation combines two
scores that we called direct semantic relation score
(dsr score) and common semantic relation score
(csr score):
- the dsr score for URI
k
i
sums up the number of
occurrences of URI
k
i
in S
1
j
for all j ? [[1, n]]?{i}.
- the csr score for URI
k
i
sums up the number of
common URIs and categories between S
k
i
and S
1
j
for all j ? [[1, n]]? {i}.
We assumed the dsr score was much more
semantically significant than the csr score, and
translated this assumption in the weight calcula-
tion by introducing two correction parameters ?
and ? used in the final scoring calculation.
Re-ranking:
For all i ? [[1, n]], for each set of URIs {URI
k
i
, k ?
[[1, l]]}, the re-ranking process is conducted ac-
cording to the following steps:
For all i ? I ,
1. ?k ? [[1, l]], calculate dsr score(URI
k
i
)
2. ?k ? [[1, l]], calculate csr score(URI
k
i
)
3. ?k ? [[1, l]], calculate
mutual relation score(URI
k
i
) =
?.dsr score(URI
k
i
)+?.csr score(URI
k
i
)
4. re-order {URI
k
i
, k ? [[1, l]]}, by
decreasing order of mutual relation score.
In the following, we detail the MDP in the con-
text of a toy example to illustrate how it works.
The document contains two sentences, NE men-
tions are in bold:
IBM has 12 research laboratories
worldwide. Thomas J. Watson, Jr.
became president of the company.
For the first NE mention [IBM], A
D
contains
two candidate URIs identifying two different re-
sources:
[IBM] URI
1
1
? International Brotherhood of Magicians
URI
2
1
? International Business Machines Corporation
For the second NE mention [Thomas J.
Watson, Jr.], A
D
contains the following can-
didate URI, which is ranked first:
[Thomas J. Watson, Jr.] URI
1
2
? Thomas Watson, Jr.
S
1
1
gathers URIs and categories contained in the
International Brotherhood of Magicians Wikipedia
page. S
2
1
is associated to the International Business
Machines Corporation, and S
1
2
to the Thomas Watson,
Jr. page. dsr score(URI
1
1
) sums up the number of
occurrences of URI
1
1
in S
1
j
for all j ? [[1, n]]?{1}.
Hence, in the current example, dsr score(URI
1
1
) is
the number of occurrences of URI
1
1
in S
1
2
, namely
the number of times the International Brotherhood
of Magicians are cited in the Thomas Watson, Jr.
page. Similarly, dsr score(URI
2
1
) is equal to the
number of times the International Business Machines
Corporation is cited in the Thomas Watson, Jr. page.
csr score(URI
1
1
) sums up the number of common
URIs and categories between S
1
1
and S
1
2
, i.e. the
number of URIs and categories appearing in both
International Brotherhood of Magicians and Thomas
Watson, Jr. pages. csr score(URI
2
1
) counts the
number of URIs and categories appearing in both
International Business Machines Corporation and
Thomas Watson, Jr. pages.
After calculation, we have:
mutual relation score(URI
1
1
) < mutual relation score(URI
2
1
)
The candidate URIs for [IBM] are re-ranked
accordingly, and International Business Machines
Corporation becomes its first rank candidate.
4 Experiments and Results
SemLinker has been evaluated on the TAC-KBP
2012 EL task (Charton et al, 2013). In this task,
mentions of entities found in a document collec-
tion must be linked to entities in a reference KB, or
to new named entities discovered in the collection.
The document collection built for KBP 2012 con-
tains a combination of newswire articles (News),
479
SemLinker TAC-KBP2012 systems
modules no disambiguation MDP only all modules 1
st
2
nd
3
rd
median
Category B
3+
P B
3+
R B
3+
F1 B
3+
P B
3+
R B
3+
F1 B
3+
P B
3+
R B
3+
F1 B
3+
F1 B
3+
F1 B
3+
F1 B
3+
F1
Overall 0.620 0.633 0.626 0.675 0.681 0.678 0.694 0.695 0.695 0.730 0.699 0.689 0.536
PER 0.771 0.791 0.781 0.785 0.795 0.790 0.828 0.838 0.833 0.809 0.840 0.714 0.645
ORG 0.600 0.571 0.585 0.622 0.578 0.599 0.621 0.569 0.594 0.715 0.615 0.717 0.485
GPE 0.412 0.465 0.437 0.570 0.628 0.598 0.574 0.626 0.599 0.627 0.579 0.614 0.428
News 0.663 0.691 0.677 0.728 0.748 0.738 0.750 0.767 0.758 0.782 0.759 0.710 0.574
Web 0.536 0.520 0.528 0.572 0.550 0.561 0.585 0.556 0.570 0.630 0.580 0.508 0.491
Table 1: SemLinker results on the TAC-KBP 2012 test corpus with/out disambiguation modules, and
three best results and median from TAC-KBP 2012 systems.
posts to blogs and newsgroups (Web). Given a
query that consists of a document with a specified
name mention of an entity, the task is to determine
the correct node in the reference KB for the entity,
adding a new node for the entity if it is not already
in the reference KB. Entities can be of type person
(PER), organization (ORG), or geopolitical entity
(GPE). The reference knowledge base is derived
from an October 2008 dump of English Wikipedia,
which includes 818,741 nodes. Table 2 provides a
breakdown of the queries per categories of entities,
and per type of documents.
Category All PER ORG GPE News Web
# queries 2226 918 706 602 1471 755
Table 2: Breakdown of the TAC-KBP 2012 test
corpus queries according to entity types, and doc-
ument categories.
A complete description of these linguistic re-
sources can be found in (Ellis et al, 2011). For
the sake of reproducibility, we applied the KBP
scoring metric (B
3
+ F ) described in (TAC-KBP,
2012), and we used the KBP scorer
1
.
The evaluated system makes use of the
Wikimeta annotation engine. The maximum num-
ber of candidate URIs is l = 15. The MDP correc-
tion parameters ? and ? described in Section 3.4
have been experimentally set to ? = 10, ? = 2.
Table 1 presents the results obtained by the sys-
tem in three configurations. In the first column,
the system is evaluated without the disambigua-
tion module. In the second column, we applied
the MDP without correction processes. The sys-
tem with the complete disambiguation module ob-
tained the results provided in the third column.
The three best results and the median from TAC-
KBP 2012 systems are shown in the remaining
columns for the sake of comparison.
1
http://www.nist.gov/tac/2013/KBP/
EntityLinking/tools.html
We observe that the complete algorithm (co-
references, named entity labels and MDP) pro-
vides the best results on PER NE links. On GPE
and ORG entities, the simple application of MDP
without prior corrections obtains the best results.
A slight loss of accuracy is observed on ORG NEs
when the MDP is applied with corrections. For
those three categories of entities, we show that the
complete system improves the performance of a
simple algorithm using distance measures. Results
on categories News and Web show that the best
performance on the whole KBP corpus (without
distinction of NE categories) is obtained with the
complete algorithm.
5 Conclusion
The presented system provides a robust seman-
tic disambiguation method, based on mutual re-
lation of entities inside a document, using a stan-
dard annotation engine. It uses co-reference, NE
normalization methods, and Wikipedia internal
links as mutual disambiguation resource to im-
prove the annotations. We show that our propo-
sition improves the performance of a standard an-
notation engine applied to the TAC-KBP evalua-
tion framework. SemLinker is fully implemented,
and publicly released as an open source toolkit
(http://code.google.com/p/semlinker). It
has been deployed in the TAC-KBP 2013 evalu-
ation campaign. Our future work will integrate
other annotation engines in the system architecture
in a collaborative approach.
Acknowledgments
This research was supported as part of Dr Eric
Charton?s Mitacs Elevate Grant sponsored by
3CE. Participation of Dr Marie-Jean Meurs was
supported by the Genozymes Project funded by
Genome Canada & G?enome Qu?ebec. The Con-
cordia Tsang Lab provided computing resources.
480
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
In The semantic web, pages 722?735. Springer.
Razvan C. Bunescu and Marius Pasca. 2006. Us-
ing encyclopedic knowledge for named entity dis-
ambiguation. In Proceedings of the Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL). ACL.
Eric Charton and Michel Gagnon. 2012. A disam-
biguation resource extracted from Wikipedia for se-
mantic annotation. In Proceedings of LREC 2012.
Eric Charton, Marie-Jean Meurs, Ludovic Jean-Louis,
and Michel Gagnon. 2013. SemLinker system
for KBP2013: A disambiguation algorithm based
on mutual relations of semantic annotations inside
a document. In Text Analysis Conference KBP.
U.S. National Institute of Standards and Technology
(NIST).
Eric Charton, Marie-Jean Meurs, Ludovic Jean-Louis,
and Michel Gagnon. 2014. Improving Entity Link-
ing using Surface Form Refinement. In Proceedings
of LREC 2014.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing EMNLP-CoNLL. ACL.
Joe Ellis, Xuansong Li, Kira Griffitt, Stephanie M
Strassel, and Jonathan Wright. 2011. Linguistic re-
sources for 2012 knowledge base population evalu-
ations. In Proceedings of TAC-KBP 2012.
Angela Fahrni, Thierry G?ockel, and Michael Strube.
2012. Hitsmonolingual and cross-lingual entity
linking system at tac 2012: A joint approach. In
TAC (Text Analysis Conference) 2012 Workshop.
Aldo Gangemi. 2013. A Comparison of Knowledge
Extraction Tools for the Semantic Web. In The 10th
Extended Semantic Web Conference (ESWC) 2013.
Xianpei Han and Jun Zhao. 2009. Named entity
disambiguation by leveraging wikipedia semantic
knowledge. In Proceedings of the Conference on
Information and Knowledge Management (CIKM).
ACM.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011a. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World wide
web, pages 229?232. ACM.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011b. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 782?792. Association for Computational
Linguistics.
Heng Ji, Ralph Grishman, HT Dang, and K Griffitt.
2010. Overview of the TAC 2010 knowledge base
population track. Proceedings of TAC 2010.
Pablo N Mendes, Max Jakob, Andr?es Garc??a-Silva, and
Christian Bizer. 2011. DBpedia Spotlight: Shed-
ding Light on the Web of Documents. In The 7th
International Conference on Semantic Systems (I-
Semantics) 2011, pages 1?8.
David N. Milne and Ian H. Witten. 2008. Named en-
tity disambiguation by leveraging wikipedia seman-
tic knowledge. In Proceedings of the Conference on
Information and Knowledge Management (CIKM).
ACM.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Hien T. Nguyen and Tru H. Cao. 2008. Named
entity disambiguation on an ontology enriched by
wikipedia. In Research, Innovation and Vision for
the Future, 2008. RIVF 2008. IEEE International
Conference on, pages 247?254. IEEE.
Borislav Popov, Atanas Kiryakov, Angel Kirilov, Dimi-
tar Manov, Damyan Ognyanoff, and Miroslav Gora-
nov. 2003. KIM ? Semantic annotation platform.
Lecture Notes in Computer Science, pages 834?849.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! Computing Semantic Relatedness Us-
ing Wikipedia. In AAAI, volume 6, pages 1419?
1424.
TAC-KBP. 2012. Proposed Task Description for
Knowledge-Base Population at TAC 2012. In Pro-
ceedings of TAC-KBP 2012. National Institute of
Standards and Technology.
481
Poly-co : an unsupervised co-reference detection system
E?ric Charton, Michel Gagnon, Benoit Ozell
E?cole Polytechnique de Montre?al
2900 boulevard Edouard-Montpetit, Montreal, QC H3T 1J4, Canada.
{eric.charton, michel.gagnon, benoit.ozell}@polymtl.ca
Abstract
We describe our contribution to the Gen-
eration Challenge 2010 for the tasks
of Named Entity Recognition and co-
reference detection (GREC-NER). To ex-
tract the NE and the referring expressions,
we employ a combination of a Part of
Speech Tagger and the Conditional Ran-
dom Fields (CRF) learning technique. We
finally experiment an original algorithm
to detect co-references. We conclude
with discussion about our system perfor-
mances.
1 Introduction
Three submission tracks are proposed in Genera-
tion Challenges 2010. GREC-NEG, where partic-
ipating systems select a referring expression (RE)
from a given list. GREC-NER where partic-
ipating systems must recognize all mentions of
people in a text and identify which mentions co-
refer. And GREC-Full, end-to-end RE regener-
ation task; participating systems must identify all
mentions of people and then aim to generate im-
proved REs for the mentions. In this paper we
present an unsupervised CRF based Named Entity
Recognition (NER) system applied to the GREC-
NER Task.
2 System description
The proposed system follows a pipelined architec-
ture (each module processes the information pro-
vided by the previous one). First, a Part of Speech
(POS) tagger is applied to the corpus. Then, the
combination of words and POS tags are used by
a CRF classifier to detect Named Entities (NE).
Next, logical rules based on combination of POS
tags, words and NE labels are used to detect pro-
nouns related to persons. Finally, an algorithm
1This work is granted by Unima Inc and Prompt Que?bec
identifies, among the person entities that have
been detected, the ones that co-refer and cluster
them. At the end, all collected information is ag-
gregated in a XML file conform to GREC-NER
specifications.
2.1 Part of speech
The part of speech labeling is done with the En-
glish version of Treetagger1. It is completed by
a step where every NAM tag associated to a first
nname is replaced by a FNAME tag, using a lex-
ical resource of first names (see table 2, column
POS Tag). The first name tag improves the NE
detection model while it improves the estimation
of conditional probabilities for words describing a
person, encountered by a NER system.
Word from Corpus POS Tag NE Tag
Adrianne FNAM PERS
Calvo NAM PERS
enrolled VVD UNK
at IN UNK
Johnson NAM ORG
Wales NAM ORG
College NAM ORG
Table 2: Sample of word list with POS Tagging
and NE tagging
2.2 Named entity and pronoun labeling
The Named Entity Recognition (NER) system
is an implementation of the CRF based system
(Be?chet and Charton, 2010) that has been used
in the French NER evaluation campaign ESTER
2 (Galliano et al, 2009)2. For the present task,
training of the NER tool is fully unsupervised as
it does not use the GREC training corpus. It is
trained in English with an automatically NE an-
notated version of the Wikipedia Corpus (the full
system configuration is described in (Charton and
1The Tree-tagger is a tool for annotating text with part-
of-speech and lemma information. http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/
2Referenced in this paper as LIA
Poly-co Score B3 CEAF MUC
Set Precision Recall FScore Precision Recall FScore Precision Recall FScore
Full set 91.48 85.89 88,60 85.40 85.40 85.40 92.15 86.95 89.47
Chef 91.12 87.84 89.45 86.53 86.53 86.53 91.86 88.55 90.18
Composers 92.01 87.14 89.51 86.87 86.87 86.87 92.11 87.02 89.49
Inventors 91.27 82.63 86.74 82.73 82.73 82.73 92.48 85.29 88.74
Table 1: System results obtained on dev-set
Torres-Moreno, 2010)). It is able to label PERS3,
ORG, LOC, TIME, DATE. We measured a spe-
cific precision of 0,93 on PERS NE detection ap-
plied to the English ACE4 evaluation set.
Following the NE detection process, detection
rules are used to label each personal pronoun with
the PERS tag. Boolean AND rules are applied
to triples {word, POS tag, NE tag}, where word
= {he, him, she, her ...}, POS tag=NN, and NE
tag=UNK . This rule structure is adopted to avoid
the PERS labeling of pronouns included in an ex-
pression or in a previously tagged NE (i.e a music
album or a movie title, using word She, and pre-
viously labeled with PROD NE tag). Finally, each
PERS labeled entity is numbered by order of ap-
parition and is associated with the sentences refer-
ence number where it appears (consecutive PERS
labeled words, not separated by punctuation mark,
receive the same index number).
2.3 Entities clustering by unstacking
In the final stage, our system determines which
entities co-refer. First, a clustering process is
achieved. The principle of the algorithm is as
follows: entities characteristics (words, POS tags,
sentence position) are indexed in a stack, ordered
according to their chronological apparition in the
text (the entity at the top of the stack is the first one
that has been detected in the document). At the
beginning of the process, the entity that is at the
top of the stack is removed and constitutes the first
item of a cluster. This entity is compared sequen-
tially, by using similarity rules, with every other
entities contained in the stack. When there is a
match, entity is transfered to the currently instan-
tiated cluster and removed from the stack. When
the end of the stack is reached, remaining entities
are reordered and the process iterates form the be-
ginning. This operation is repeated until the stack
is empty.
Comparison of entities in the stack is done in
3PERS tag is commonly used in NER Task to describe
labels applied to people, ORG describe organisations, LOC
is for places.
4ACE is the former NIST NER evaluation campaign.
two ways according to the nature of the entity.
We consider a candidate entity Ec from stack
S. According to iteration k, the current clus-
ter is Ck. Each element of the sequence Ec (i.e
Chester FNAME Carton NAM) is compared to the
sequences previously transfered in Ck during the
exploration process of the stack. If Ec ?
?
Ck, it
is included in cluster Ck and removed from S. Fi-
nally inclusion of pronouns from S in Ec is done
by resolving the anaphora, according to the Hobbs
algorithm, as described in (Jurafsky et al, 2000)5.
3 Results and conclusions
Table 1 shows our results on dev-set. We ob-
tain good precision on the 3 subsets. Our system
slightly underperforms the recall. This can be ex-
plained by a good performance in the NE detection
process, but a difficulty in some cases for the clus-
tering algorithm to group entities. We have ob-
served in the Inventors dev-set some difficulties,
due to strong variation of surface forms for spe-
cific entities. We plan to experiment the use of
an external resource of surface forms for person
names extracted from Wikipedia to improve our
system in such specific case.
References
Fre?de?ric Be?chet and Eric Charton. 2010. Unsuper-
vised knowledge acquisition for extracting named
entities from speech. In ICASSP 2010, Dallas.
ICASSP.
Eric Charton and J.M. Torres-Moreno. 2010. NL-
GbAse: a free linguistic resource for Natural Lan-
guage Processing systems. In LREC 2010, editor,
English, number 1, Matla. Proceedings of LREC
2010.
S. Galliano, G. Gravier, and L. Chaubard. 2009. The
ESTER 2 Evaluation Campaign for the Rich Tran-
scription of French Radio Broadcasts. In Interna-
tional Speech Communication Association confer-
ence 2009, pages 2583?2586. Interspeech 2010.
D. Jurafsky, J.H. Martin, A. Kehler, K. Vander Linden,
and N. Ward. 2000. Speech and language process-
ing. Prentice Hall New York.
5p704, 21.6
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 97?101,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Poly-co: a multilayer perceptron approach for coreference detection
Eric Charton
E?cole Polytechnique de Montre?al
2500, chemin de Polytechnique
Montre?al (Que?bec), H3T 1J4
eric.charton@polymtl.ca
Michel Gagnon
E?cole Polytechnique de Montre?al
2500, chemin de Polytechnique
Montre?al (Que?bec), H3T 1J4
michel.gagnon@polymtl.ca
Abstract
This paper presents the coreference resolution
system Poly-co submitted to the closed track
of the CoNLL-2011 Shared Task. Our sys-
tem integrates a multilayer perceptron classi-
fier in a pipeline approach. We describe the
heuristic used to select the pairs of corefer-
ence candidates that are feeded to the network
for training, and our feature selection method.
The features used in our approach are based on
similarity and identity measures, filtering in-
formations, like gender and number, and other
syntactic information.
1 Introduction
Coreference resolution is the process of determining
whether two expressions in natural language refer to
the same entity in the world. It is an important sub-
task in natural language processing systems. In this
paper, we present a learning approach to coreference
resolution of named entities (NE), pronouns (PRP),
noun phrases (NP) in unrestricted text according to
the CoNLL-2011 shared task (Pradhan et al, 2011).
This system have been used in the context of closed
track.
2 Previous propositions
Many learning-based systems have been proposed to
solve coreference resolution task, and Soon?s (Soon
et al, 2001) architecture is one of the most pop-
ular ones. In this proposition, all possible men-
tions in a training document are determined by a
pipeline of natural language processing (NLP) mod-
ules. Then, training examples are generated as fea-
ture vectors. Each feature vector represents a pair
of mentions that can potentially corefer. Those vec-
tors are used as training examples given to build a
C5 classifier. To determine the coreference chains
in a new document, all potential pairs of corefer-
ring mentions are presented to the classifier, which
decides whether the two mentions actually core-
fer. Since then, this dominant architecture has been
widely implemented. As it is a very flexible propo-
sition, many families of classifiers have been used,
trained with various configurations of feature vec-
tors. Good results are obtained with SVM classi-
fiers, like described in (Versley et al, 2008). Some
propositions keep only the principle of feature vec-
tors, associated with more complex coreference de-
tection algorithms. A constraint-based graph parti-
tioning system has been experimented by (Sapena et
al., 2010) and a coreference detection system based
on Markov logic networks (MLNs) has been pro-
posed by (Poon and Domingos, 2008).
3 Architecture of the proposed system
A considerable engineering effort is needed to
achieve the coreference resolution task. A signif-
icant part of this effort concerns feature engineer-
ing. We decided to keep the well established archi-
tecture of (Soon et al, 2001) with a pre-processing
NLP pipeline used to prepare pairs of coreference
features. The features are then submitted to the clas-
sifier for pairing validation. We tested various clas-
sifiers on our feature model (see table 2) and fi-
nally selected a multilayer perceptron (MLP) clas-
sifier to make decision. Since the Ontonotes layers
provide syntactic information (Pradhan et al, 2007),
97
Gender and number detection
Training features vectors generationPerceptron training Features vectors generationPerceptron classification
Named entities alias detectionCandidate mentionsdetection moduleSimilarity measures
Model
Number and Genderdatas
Co-reference selection
Test Corpus
Candidate mentions extraction module
Training corpus
Labeled corpus
Figure 1: The pipeline architecture of the Poly-co system.
we could concentrate our efforts on the introduction
of some complementary high level properties (like
mention similarities or gender compatibility) used
in the feature vectors given to the classifiers. The
global architecture, presented in figure 1, includes
two pipelines. One configured for training purposes
and the other one for coreference resolution.
3.1 Architecture components
Ontonotes corpus includes part-of-speech tagging,
noun phrases identification and named entity labels.
We introduce complementary modules to detect gen-
der and number, and evaluate mentions aliasing and
similarity. The detection task is composed of 4 mod-
ules:
? Candidate mentions detection module, based
on extraction rules, using Ontonotes layers.
? Named entities alias detection module, based
on the previous version of Poly-co, described
in (Charton et al, 2010). The purpose of this
module is to identify variations in names of
the same entity by examination of their surface
form.
? Similarity calculation module, used to evalu-
ate the similarity of two mentions according to
a comparison of their string.
? Gender and number detection module,
which determines gender and number for any
candidate mention.
In the training pipeline, the candidate mentions
detection module and the alias detection module
are replaced by a unique candidate mentions ex-
traction module. This module collects from the
training corpus the labeled mentions and their refer-
ence numbers and use them to generate aliases and
mentions values required to build training features.
As we will see later, similarity calculation and
gender and number detection all result in a value that
is integrated to the feature vector used to train and
apply the classifier. We give below a more detailed
description of each module.
3.1.1 Candidate mentions detection module
It is mandatory for coreference resolution to first
get al the potential mentions from the input text.
To determine the mentions, this module explores the
text corpus and extracts a candidate mentions list.
This list includes, for each mention, its position in
the document, its word content and its syntactic cat-
egory. This module uses simple detection rules to
collect the mentions according to their part of speech
(POS) and their text content, their syntactic bound-
aries and their named entity type labels.
When used in classification mode, the detection
process is followed by a filtering process, where
rules are used to remove mentions that have a very
low probability of being involved in coreference.
These rules are based on simple word sequence pat-
terns. For example, pronoun it is filtered out when
immediately followed by verb to be and relative pro-
noun that within the next 6 following words.
3.1.2 Alias detection module
This module implements an algorithm that clus-
ters entities by comparing the form of their names.
Entities are put in a list, ordered according to their
chronological apparition in the text. At the begin-
ning of the process, the first entity in the list is re-
moved and constitutes the first item of a cluster. This
entity is compared sequentially, by using similarity
and logical rules (i.e, a PERSON can?t be an alias of
a LOC ), with every other entities contained in the
98
list. When there is a match, the entity is removed
from the list and transferred to the currently instan-
tiated cluster. This operation is repeated until the list
is empty.
At the end of this process, an entity in a cluster is
considered to be an alias of every other entity in the
same cluster.
The TIME and DATE alias detection is done
through a specific heuristic set. Each TIME entity
representation is converted in a standardized format
(Hour/Minutes). Dates are normalized as a relative
amount of days (?today? is 1, ?last month? is -30,
etc) or a formal date (Year/Month/Day).
3.1.3 Similarity calculation module
The similarity module is applied on named enti-
ties (excepted TIME and DATE ) and NP of the
candidate mentions list. It consists in a text com-
parison function which returns the number of com-
mon words between two mentions. After execution
of this module, we obtain a square matrix containing
a similarity measure for every pair of mentions.
3.1.4 Gender and number detection module
Gender and number are associated with each entry
of the candidate mentions list, including PRP and
NP. First, this module tries to detect the gender using
the gender data provided1. Then a set of less than
10 very simple rules is used to avoid anomaly (i.e a
PERSON entity associated with the neutral gender).
Another set of rules using plural markers of words
and POS is used to validate the number.
4 Features definition and production
The feature vector of the Poly-co system (see ta-
ble 1) consists of a 22 features set, described below.
This vector is based on two extracted mentions, A
and B, where B is the potential antecedent and A is
the anaphor.
Four features are common to A and B (section A
and B properties of table 1):
? IsAlias : this value is binary (yes or no) and
provided by the alias module. The value is yes
if A and B have been identified as describing
the same entity.
1The list allowed by the Shared Task definition and available
at http://www.clsp.jhu.edu/ sbergsma/Gender/
Feature Name Value value
A and B properties
IsAlias yes/no 1/0
IsSimilar real 0.00 /1.00
Distance int 0/const(b)
Sent int 0/x
Reference A
ISNE yes/no 1/0
ISPRP yes/no 1/0
ISNP yes/no 1/0
NE SEMANTIC TYPE null / EN 0 / 1-18
PRP NAME null / PRP 0 / 1-30
NP NAME null / DT 0 / 1-15
NP TYPE null / TYPE 0 / 1-3
GENDER M/F/N/U 1/2/3/0
NUMBER S/P/U 1/2/0
Reference B
Same as Reference A
Table 1: Feature parameters
? IsSimilar : this value is the similarity measure
provided by the similarity module.
? Distance : this indicates the offset distance (in
terms of number of items in the candidate men-
tions list) between A and B.
? Sent : this indicates the amount of sentences
marker (like . ! ?) separating the mentions A
and B.
For each candidate A and B, a set containing nine
features is added to the vector (in table 1, only prop-
erties for A are presented). First, 3 flags determine
if mention is a named entity (IsNE), a personal pro-
noun (IsPRP) or a noun phrase (IsNP). The next six
flags define the characteristics of the mention :
? NE SEMANTIC TYPE is one of the 18 available
NE types (PERSON, ORG, TIME, etc)
? PRP NAME is a value representing 30 possible
words (like my, she, it, etc) for a PRP.
? NP NAME is a value indicating the DT used by
a NP (like the, this, these, etc).
? NP TYPE specifies if NP is demonstrative, def-
inite, or a quantifier.
? GENDER and NUMBER flags indicate whether
the mention gender (Male, Female or Neutral)
99
Poly-co Score Mentions B3 CEAF MUC
R P F R P F R P F R P F
Multilayer perceptron (MLP) 65.91 64.84 65.37 66.61 62.09 64.27 50.18 50.18 50.18 54.47 50.86 52.60
SVM 65.06 66.11 65.58 65.28 57.68 61.24 46.31 46.31 46.31 53.30 50.00 51.60
Tree J48 66.06 64.57 65.31 66.53 62.27 64.33 50.59 50.59 50.59 54.24 50.60 52.36
Table 2: System results obtained with scorer v4 on gold dev-set applying various classifiers on same features vectors.
Poly-co Score Mentions B3 CEAF MUC
Multilayer perceptron (MLP) 64.53 63.42 63.97 66.07 61.65 63.79 49.12 49.12 49.12 52.70 49.22 50.90
Table 3: System results obtained with scorer v4 on predicted dev-set using our system.
and number (Singular or Plural) are known or
not (if not, U is the value for the flag).
A null value (0) is used when a flag doesn?t have
to be defined (i.e PRP flag if the mention is a NE).
5 Classifier training and use
For training, we use an algorithm that selects the
more relevant pairs or mentions. Suppose that
the candidate mentions list contains k mentions
M1,M2, . . . ,Mk, in this order in the document. The
algorithm starts with the last mention in the docu-
ment, that is, Mk. It compares Mk sequentially with
preceding mentions, going backward until a core-
ferring mention Mc is reached, or a maximum of n
mentions have been visited (the value of n is fixed to
10 in our experiments). When a coreferring mention
Mc has been found, a vector is constructed for every
pair of mentions ?Mk,Mi?, where Mi is a mention
that has been visited, including the coreferring one.
These vectors are added to the training set, Mc being
a positive instance, and all the others ones being neg-
ative instances. The process is repeated with Mk?1,
and so on, until every mention has been processed.
If none of the n precedent mentions are coreferent
to M1, all the n pairs are rejected and not used as
training instance.
During the coreference detection process, a sim-
ilar algorithm is used. Starting from mention Mk,
we compare it with n preceding mentions, until we
find one for which the multilayer perceptron classi-
fier gives a coreference probability higher than 0.52.
If none is found within the limit of n mentions, Mk
2Note that in comparison tests, displayed in table 2, SVM
provides a binary decision and J48 a probability value. They
are used as the multilayer perceptron ones.
is considered as a non coreferring mention. When
this has been done for every mention in the docu-
ment, the detected coreferences are used to construct
the coreference chains.
6 Results
The results presented on table 2 are obtained on the
dev-set of the Ontonotes corpus. To evaluate the po-
tential of our features model, we trained our sys-
tem with MLP, SVM and J48 Tree classifiers. We
finally chose the MLP models for the test evalua-
tion due to its better performance on the predicted
dev-set. However, according to the small difference
between MLP and J48 Tree, it?s difficult to define
clearly wich one is the best choice.
7 Conclusions
We presented Poly-co, a system for coreference res-
olution in English easy to adapt to other languages.
The first version of Poly-co was built to detect only
coreferences of persons. As the dataset provided for
CoNLL is much more complex, it was an intersting
opportunity to evaluate our mention detection algo-
rithms in the perspective of a full task, including dif-
ficult coreferences mentions beetween named enti-
ties, noun phrases and prepositions. Our comparison
of various classifier results on dev-sets have shown
that our proposition to use a multilayer perceptron as
coreference chain builder can be an intersting solu-
tion, but does not introduce an important difference
of performance with previously experimented clas-
sifiers.
100
References
Eric Charton, Michel Gagnon, and Benoit Ozell. 2010.
Poly-co : an unsupervised co-reference detection sys-
tem. In INLG 2010-GREC, Dublin. ACL SIGGEN.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing - EMNLP ?08, page
650, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In International Conference on Semantic
Computing, 2007. ICSC 2007., pages 446?453. IEEE.
Sameer Pradhan, Lance Ramshaw., Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Xue Nianwen.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon.
Emili Sapena, L. Padro?, and Jordi Turmo. 2010. Relax-
Cor: A global relaxation labeling approach to coref-
erence resolution. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, number July,
pages 88?91. Association for Computational Linguis-
tics.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27(4):521?544, December.
Yannick Versley, S.P. Ponzetto, Massimo Poesio,
Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng
Yang, and Alessandro Moschitti. 2008. BART:
A modular toolkit for coreference resolution. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), number 2006,
pages 9?12, Marrakech. European Language Re-
sources Association (ELRA).
101
