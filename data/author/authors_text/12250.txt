Proceedings of the Workshop on BioNLP, pages 108?116,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
User-Driven Development of Text Mining Resources for Cancer Risk
Assessment
Lin Sun, Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0GD, UK
ls418,alk23@cl.cam.ac.uk
Ilona Silins, Ulla Stenius
Institute of Environmental Medicine
Karolinska Institutet
S-17177, Stockholm
Sweden
ilona.silins,ulla.stenius@ki.se
Abstract
One of the most neglected areas of biomed-
ical Text Mining (TM) is the development
of systems based on carefully assessed user
needs. We investigate the needs of an im-
portant task yet to be tackled by TM ? Can-
cer Risk Assessment (CRA) ? and take the
first step towards the development of TM for
the task: identifying and organizing the sci-
entific evidence required for CRA in a taxon-
omy. The taxonomy is based on expert annota-
tion of 1297 MEDLINE abstracts. We report
promising results with inter-annotator agree-
ment tests and automatic classification experi-
ments, and a user test which demonstrates that
the resources we have built are well-defined,
accurate, and applicable to a real-world CRA
scenario. We discuss extending and refining
the taxonomy further via manual and machine
learning approaches, and the subsequent steps
required to develop TM for the needs of CRA.
1 Introduction
Biomedical Text Mining (TM) has become increas-
ingly popular due to the pressing need to provide
access to the tremendous body of texts available
in biomedical sciences. Considerable progress has
been made in the development of basic resources
(e.g. ontologies, annotated corpora) and techniques
(e.g. Information Retrieval (IR), Information Ex-
traction (IE)) in this area, and research has began
to focus on increasingly challenging tasks, e.g. sum-
marization and the discovery of novel information in
biomedical literature (Hunter and Cohen 2006, Ana-
niadou et al 2006, Zweigenbaum et al 2007).
In recent past, there has been an increasing de-
mand for research which is driven by actual user
needs rather than technical developments (Zweigen-
baum et al 2007). Shared tasks (e.g. BioCreative
and the TREC Genomics track) targeting the work-
flow of biomedical researchers have appeared along
with studies exploring the TM needs of specific tasks
(Karamanis et al 2008, Demaine et al 2006). How-
ever, the understanding of user needs is still one of
the neglected areas of BIO-TM, and further user-
centered evaluations and systems grounded in real-
life tasks are required to determine which tools and
services are useful (Cohen et al 2008).
We investigate the user needs of a challenging
task yet to be tackled by TM but identified as an
important potential application for it (Lewin et al
2008): Cancer Risk Assessment (CRA). Over the
past years, CRA has become increasingly important
as the link between environmental chemicals and
cancer has become evident. It involves examining
published evidence to determine the relationship be-
tween exposure to a chemical and the likelihood of
developing cancer from that exposure (EPA, 2005).
Performed manually by experts in health related in-
stitutions worldwide, CRA requires searching, lo-
cating and interpreting information in biomedical
journal articles. It can be extremely time-consuming
because the data for a single carcinogen may be scat-
tered across thousands of articles.
Given the exponentially growing volume of
biomedical literature and the rapid development of
molecular biology techniques, the task is now get-
ting too challenging to manage via manual means.
From the perspective of BIO-TM, CRA is an excel-
lent example of real-world task which could greatly
benefit from a dedicated TM tool. However, the de-
velopment of a truly useful tool requires careful in-
vestigation of risk assessors needs.
108
This paper reports our investigation of the user
needs of CRA and the creation of basic TM re-
sources for the task. Expanding on our preliminary
experiments (Lewin et al 2008), we present a taxon-
omy which specifies the scientific evidence needed
for CRA at the level of detail required for TM. The
taxonomy is based on expert annotation of a corpus
of 1297 MEDLINE abstracts. We report promis-
ing results with inter-annotator agreement tests, au-
tomatic classification of corpus data into taxonomy
classes, and a user test in a near real-world CRA
scenario which shows that the taxonomy is highly
accurate and useful for practical CRA. We discuss
refining and extending it further via manual and ma-
chine learning approaches, and the subsequent steps
required to develop TM for the needs of CRA.
2 User Needs of Cancer Risk Assessment
We interviewed 14 experienced risk assessors work-
ing for a number of authorities in Sweden1 asking
a range of questions related to different aspects of
their work. The risk assessors described the follow-
ing steps of CRA: (1) identifying the journal articles
relevant for CRA of the chemical in question, (2)
identifying the scientific evidence in these articles
which help to determine whether/how the chemical
causes cancer, (3) classifying and analysing the re-
sulting (partly conflicting) evidence to build the tox-
icological profile for the chemical, and (4) prepar-
ing the risk assessment report. These steps are con-
ducted manually, relying only on standard literature
search engines (e.g. PubMed) and word processors.
The average time required for CRA of a single
chemical was reported to be two years when done
(as usual) on a part time basis. Risk assessors were
unanimous about the need to increase productivity
to meet the current CRA demand. They reported
that locating and classifying the scientific evidence
in literature is the most time consuming part of their
work and that a tool capable of assisting it and ensur-
ing that all the potentially relevant evidence is found
would be particularly helpful.
It became clear that a prerequisite for the devel-
opment of such a tool would be an extensive spec-
ification of the scientific evidence used for CRA.
1Institute of Environmental Medicine at Karolinska Insti-
tutet, Swedish Chemical Inspectorate, Scientific Committee on
Occupational Exposure Limits (EU), Swedish Criteria Group.
This evidence ? which forms the basis of all the
subsequent steps of CRA ? is described in the
guideline documents of major international CRA
agencies, e.g. European Chemicals Agency (ECHA,
2008) and the United States Environmental Protec-
tion Agency (EPA, 2005). However, although these
documents constitute the main reference material in
CRA, they cover the main types of evidence only,
do not specify the evidence at the level of detail
required for comprehensive data gathering, and are
not updated regularly (i.e. do not incorporate the lat-
est developments in biomedical sciences). The risk
assessors admitted that rather than relying on these
documents, they rely on their experience and expert
knowledge when looking for the evidence. We de-
cided that our starting point should be to compose
a more adequate specification of the scientific evi-
dence needed for CRA.
3 Cancer Risk Assessment Taxonomy
We recruited three experienced risk assessors to help
construct the resources described in sections below:
(i) a representative corpus of CRA literature for
parts of hazard identification (i.e. the assessment of
whether a chemical is capable of causing cancer),
(ii) a tool for expert annotation of the corpus, (iii) an
annotated corpus, and (iv) a taxonomy which classi-
fies and organizes the scientific evidence discovered
in the corpus.
3.1 CRA corpus
Various human, animal (in vivo), cellular (in vitro)
and other mechanistic data provide evidence for haz-
ard identification and the assessment of the Mode of
Action (MOA) (i.e. the sequence of key events that
result in cancer formation, e.g. mutagenesis and in-
creased cell proliferation) in CRA. The experts se-
lected eight chemicals which are (i) well-researched
using a range of scientific tests and (ii) represent the
two most frequently used MOAs ? genotoxic and
non-genotoxic2 . 15 journals were identified which
are used frequently for CRA and jointly provide a
good coverage of relevant scientific evidence (e.g.
Cancer Research, Chemico-biological Interaction,
Mutagenesis, Toxicological Sciences). From these
2Chemicals acting by a genotoxic MOA interact with DNA,
while chemicals acting by a nongenotoxic MOA induce cancer
without interfering directly with DNA.
109
Figure 1: Screenshot of the annotation tool
journals, all the PubMed abstracts from 1998-2008
which include one of the 8 chemicals were down-
loaded. The resulting corpus of 1297 abstracts is
distributed per chemical as shown in Table 1.
3.2 Annotation tool
Risk assessors typically (i) read each abstract re-
trieved by PubMed to determine its relevance for
CRA, and (ii) classify each relevant abstract based
on the type of evidence it provides for CRA. We ex-
tended the tool designed for expert annotation of ab-
stracts in our earlier work (Lewin et al 2008) so that
imitates this process as closely as possible.
The tool provides two types of functionality. The
first enables the experts to classify abstracts as rele-
vant, irrelevant or unsure. The second enables them
to annotate such keywords (words or phrases) in ab-
stracts and their titles which indicate the scientific
evidence relevant for the task. Keyword annotation
was chosen because the experts found it intuitive, it
did not require linguistic training, and it specifies the
scientific evidence more precisely than larger spans
of text.
Initially a very shallow taxonomy (including only
human, animal, and cellular data) and the two types
of MOA was integrated inside the tool. This was
gradually extended as the annotation progressed.
The tool permits annotating any number of relevant
keywords in the abstracts, attaching them to any
class in the taxonomy, and classifying the same text
in more than one way. It was implemented inside the
familiar Mozilla Firefox browser using its extension
facility. A screenshot illustrating the tool is provided
in Figure 1.
3.3 Annotation
Given a set of initial guidelines agreed by the ex-
perts, one of the experts annotated a subset of the
corpus, the other two evaluated the result, disagree-
ments were then discussed, and the guidelines were
improved where needed. This process (crucial for
maintaining quality) was repeated several times.
The guidelines described below are the final result
of this work.
3.3.1 Relevance annotation
An abstract is classified as (i) relevant when it (or
its title) contains evidence relevant for CRA and (ii)
irrelevant when it (or its title) contains no evidence
or contains ?negative? evidence (e.g. diseases or
endpoints unrelated to cancer). Abstracts containing
vague, conflicting or complex evidence (e.g. stud-
ies on chemicals in complex mixtures) or evidence
whose association with cancer is currently unclear
were dealt on case by case basis. All the potentially
relevant abstracts were included for further assess-
ment as not to lose data valuable for CRA.
The experts annotated the 1297 abstracts in the
corpus. 89.4% were classified as relevant, 10.1% as
irrelevant, and 0.5% as unsure. We used the Kappa
statistics (Cohen 1960) to measure inter-annotator
agreement on unseen data which two experts an-
notated independently. 208 abstracts were selected
randomly from the 15 journals and from 16 jour-
nals likely to be irrelevant for CRA. The latter were
included to make the task harder as the proportion
of relevant abstracts was high in our corpus. Our
Kappa result is 0.68 ? a figure which indicates sub-
stantial agreement (Landis and G.Koch 1977).
The experts disagreed on 24 (11.5% of the) ab-
stracts. Half of the disagreements are due to one
of the annotators failing to notice relevant evidence.
Such cases are likely to decrease when annotators
gain more experience. The other half are caused by
vague or conflicting evidence. Many of these could
be addressed by further development of guidelines.
3.3.2 Keyword annotation
Keyword annotation focussed on the types of sci-
entific evidence experts typically look for in CRA:
carcinogenic activity (human, animal, cellular, and
other mechanistic data), Mode of Action (MOA)
(data for a specific MOA type ? genotoxic or non-
110
Chemical Retrieved Relevant
1,3-butadiene 195 187
phenobarbital 270 240
diethylnitrosamine 221 214
diethylstilbestrol 145 110
benzoapyrene 201 192
fumonisin 80 70
chloroform 96 84
styrene 162 132
Total 1297 1164
Table 1: Total of abstracts per chemical
genotoxic), and relevant parts of toxicokinetics (e.g.
metabolic activation). The experts annotated the
keywords which they considered as the most impor-
tant and which jointly identify the types of scientific
data offered by the abstract. They focussed on new
(rather than previously published) data on the chem-
ical in question.
All the 1164 abstracts deemed relevant were an-
notated. A total of 1742 unique keywords were
identified, both simple nouns and complex nomi-
nals / phrases. Figure 1 shows an example of an
annotated abstract where the keyword chromoso-
mal aberrations is identified as evidence for geno-
toxic MOA. Since the experts were not required to
annotate every relevant keyword, calculating inter-
annotator agreement was not meaningful. However,
the keyword annotation was evaluated jointly with
taxonomy classification (the following section).
3.4 The taxonomy and the resulting corpus
During keyword annotation, the initial taxonomy
was extended and refined with new classes and class
members. The resulting taxonomy relies solely on
expert knowledge. Experts were merely advised
on the main principles of taxonomy creation: the
classes should be conceptually coherent and their hi-
erarchical organization should be in terms of coher-
ent sub- and superordinate relations.
The taxonomy contains three top level classes:
1) Carcinogenic activity (CA), 2) Mode of Action
(MOA) and 3) Toxicokinetics (TOX). 1) and 2) are
organized by TYPE-OF relations (leukemia is a type
of carcinogenic evidence) and 3) by PART-OF rela-
tions (biodegradation is a part of Metabolism). Each
top level class divides into sub-classes. Figure 2
shows CA taxonomy with three keyword examples
per class. The taxonomy has 48 classes in total; half
of them under CA. Table 6 shows the total number
of abstracts and keywords per class: 82.4% of the
abstracts include keywords for CA, and 50.3% and
28.1% for MOA and TOX, respectively.
We calculated inter-annotator agreement for as-
signing abstracts to taxonomy classes. For each of
the 8 chemicals, 10 abstracts were randomly cho-
sen from the 15 journals. The average agreement
between two annotators is the highest with CA and
MOA (78%) and the lowest with TOX (62%). The
overall agreement is 76%. This result is good, par-
ticularly considering the high number of classes and
the chance agreement of 1.5%. The disagreements
are mostly due to one of the experts annotating as
many keywords as possible, and the other one an-
notating only the ones that classify each abstract as
precisely as possible. This was not a serious prob-
lem for us, but it demonstrates the importance of de-
tailed guidelines. Also, some of the classes were too
imprecise to yield unique distinctions. Future work
should focus on refining them further.
4 Automatic classification
To examine whether the classification created by ex-
perts provides a good representation of the corpus
data and is machine learnable, we conducted a se-
ries of abstract classification experiments.
4.1 Methods
4.1.1 Feature extraction
The first step of text categorization (TC) is to
transform documents into a feature vector represen-
tation. We experimented with two document rep-
resentation techniques. The first one is the sim-
ple ?bag of words? approach (BOW) which consid-
ers each word in the document as a separate feature.
BOW was evaluated using three methods which have
proved useful in previous TC work: (i) stemming
(using the Porter (1980) stemmer) which removes
affixes from words, (ii) the TFIDF weighting (Kib-
riya et al 2004), and (iii) stop word removal.
The second technique is the recent ?bag of sub-
strings? (BOS) method by (Wang et al 2008) which
considers the whole abstract as a string and extracts
from it all the length p substrings without affix re-
moval. BOS has proved promising in biomedical
TC (Han et al 2006, Wang et al 2008) and un-
like a traditional grammatical stemmer, does not re-
111
Figure 2: Taxonomy of Carcinogenic Activity
quire domain tuning for optimal performance. Be-
cause BOS generates substrings with fixed length p,
a word shorter than p?2 can get obscured by its con-
text3. For example, ?mice? would be transformed to
? mice a?, ? mice b?, . . . , which is less informative
than the original word form. Therefore, we enriched
BOS features with word forms shorter than p? 2.
4.1.2 Feature selection
We employed two feature selection methods for
dimensionality reduction. The first is Information
Gain (IG) which has proved useful in TC (Yang
and Pedersen 1997). Given a feature?s distribu-
tion X and class label distribution Y , IG(X) =
H(Y ) ? H(Y |X), H(X) is the entropy of X. The
second method fscore optimises the number of fea-
tures (N ). Features are first ranked using the simple
fscore criterion (Chen and Lin 2006), and N is se-
lected based on the performance of the SVM classi-
fier using the N features.
4.1.3 Classification
Three classifiers were used: Naive Multino-
mial Bayesian (NMB), Complement Naive Bayesian
(CNB) (Rennie and Karger 2003) and Linear Sup-
port Vector Machines (L-SVM) (Vapnik 1995).
NMB is a widely used classifier in TC (Kib-
riya et al 2004). It selects the class C with
the maximum probability given the document d:
argmaxc Pr(C)?w?d Pr(X = w|C). Pr(C) can
3Minus 2 because of space characters.
be estimated from the frequency of documents in C .
Pr(X = w|C) is estimated as the fraction of tokens
in documents of class C that contain w.
CNB extends NMB by addressing the problems
it has e.g. with imbalanced data and weight
magnitude error. The class c of a document
is: argmaxc[logp(?c)??i filogNc?i+?iNc?+? ]. Nc?i is the
number of times term i occurs in classes other than
c. ? and ?i are the smoothing parameters. p(?c) is
the prior distribution of class c.
L-SVM is the basic type of SVM which pro-
duces a hyperplane that separates two-class samples
with a maximum margin. It handles high dimen-
sional data efficiently, and has shown to perform
well in TC (Yang and Liu 1999). Given the data
set X = (x1, y1), . . . , (xn, yn) yi ? {?1,+1},
L-SVM requires a solution w to the following un-
constrained optimisation problem: min(12wTw +
C?ni=1 max(1 ? yiwTxi, 0)2. Cost parameter C
was estimated within range 22,. . . , 25 on training
data using cross validation. The C of the posi-
tive class was weighted by class population ratio
r = negative populationpositive population .
4.1.4 Evaluation
We used the standard measures of recall (R), pre-
cision (P) and F measure (F) for evaluation. These
are defined as follows:
R = TPTP+FN P = TPTP+FP F = 2?R?PR+P
Our random baseline is P+N+P+ .
112
P+/N : positive/negative population TP: truth positive; FN: false negative, FP: false positive
4.2 Experimental evaluation
4.2.1 Data
Our data was the expert annotated CRA corpus.
4.2.2 Document preprocessing
We first evaluated the BOW preprocessing tech-
nique with and without the use of (i) the Porter
(1980) stemmer, (ii) TFIDF, (iii) stop word removal,
and (iv) their combinations. The evaluation was
done in the context of the binary relevance classifica-
tion of abstracts (not in the context of the main tax-
onomic classification task to avoid overfitting pre-
processing techniques to the taxonomy). Only (iii)
improved all the classifiers and was thus adopted
for the main experiments. The poor performance
of (i) demonstrates that a standard stemmer is not
optimal for our data. As highlighted by (Han et al
2006, Wang et al 2008), semantically related bio-
logical terms sharing the same stem are not always
reducible to the stem form.
4.2.3 Feature selection
We evaluated the feature selection methods on
two taxonomy classes: the most balanced class ?An-
imal study? (positive/negative 1:1.4) and an imbal-
anced class ?Adducts? (positive/negative 1:6.5). IG
was used for the fixed N setting and fscore for the
dynamic N setting. Each combination of classifiers
(NMB/CNB/SVM), document representations (BOW,
BOS) and settings for N (dynamic, . . . , 83098) was
evaluated. The results show that the dynamic setting
yields consistent improvement on all the setups (al-
though the impact on SVM?s is not big). Also the
optimal N varies by the data and the classifier. Thus,
we used the dynamic feature selection in the taxo-
nomic classification.
4.2.4 Taxonomic classification
Experimental setup We ran two sets of experi-
ments on the corpus, using 1) BOW and 2) BOS for
feature extraction. Without feature selection, BOW
had c. 9000 features and BOS c. 83000. Features
were selected using fscore. For each class with
more than 20 abstracts (37 in total)4, three ?one
4The classes with less than 20 abstracts may have less than
2 positive abstracts in each fold of 10 fold CV, which is not
Method Feature Set P R F
NMB BOW 0.59 0.75 0.66
NMB BOS 0.62 0.82 0.70
CNB BOW 0.52 0.74 0.60
CNB BOS 0.57 0.76 0.64
SVM BOW 0.68 0.76 0.71
SVM BOS 0.71 0.77 0.74
Table 2: Performance of classifiers with BOS/BOW
Class Method P R F
CA NMB 0.94 0.89 0.91
CA CNB 0.92 0.94 0.93
CA SVM 0.93 0.93 0.93
MOA NMB 0.88 0.81 0.84
MOA CNB 0.84 0.82 0.83
MOA SVM 0.92 0.80 0.86
TOX NMB 0.66 0.83 0.74
TOX CNB 0.70 0.80 0.75
TOX SVM 0.76 0.79 0.78
Table 3: Result for the top level classes
against other? classifiers (NMB, CNB and L-SVM)
were trained and tested using 10-fold cross valida-
tion.
Results Table 2 shows the average performance
for the whole taxonomy. The performance of BOS
is better than that of BOW according to all the three
measures. On average, BOS outperforms BOW by
4% in P and F, and 3% in R. SVM yields the best
overall P and F (0.71 and 0.74) with BOS. Surpris-
ingly, NMB outperforms CNB with all the settings.
NMB yields the best overall R with BOS (0.82) but
its P is notably lower than that of SVM.
Table 3 shows the average P, R and F for the top
level classes using the best performing feature set
BOS with the three classifiers. CA has the best F
(0.93). Its positive population is the highest (posi-
tive/negative: 5:1). TOX with a lower positive pop-
ulation (1:2.6) has still good F (0.78). R and P are
balanced with an average difference of 0.06.
Table 4 shows the distribution of F across the
taxonomy. There is a clear correlation between
representative for the class population.
No. of abstracts(f) Classes F Random
f > 300 9 0.80 0.38
100 < f ? 300 12 0.73 0.13
20 < f ? 100 16 0.68 0.04
Table 4: Mean F and random baseline for taxonomic
classes in three frequency ranges.
113
frequency and performance: the average F de-
creases with descending frequency range, revealing
increased classification difficulty. Classes with more
than 300 abstracts have the highest average F (0.80
with standard deviation (SD) 0.08). Classes with
20-100 abstracts have the average F 0.68 (SD 0.11),
which is lower but still fairly good. No class has F
lower than 0.46, which is much higher than the av-
erage random baseline of 0.11.
5 User Test
A user test was carried out to examine the practical
usefulness of the automatic classification in a near
real-world scenario. The L-SVM+BOS classifier was
applied to the PubMed abstract data (from 1998-
2008) of five unseen chemicals representing geno-
toxic (geno) and non-genotoxic (non) MOAs (see
table 5). The results were displayed to two experts
in a friendly web interface. The experts were in-
vited to imagine that they have submitted a query to
a system, the system has returned the classification
of relevant abstracts for each chemical, and the task
is to judge whether it is correct. The top 500 BOS
features per class were shown to aid the judgement.
Results were evaluated using precision (P) (re-
call could not be calculated as not all of the positive
polulation was known). Table 5 shows the average
P for chemicals and top level classes. The results
are impressive: the only chemical with P lower than
0.90 is polychlorinated biphenyls (PCB). As PCB
has a well-known neuro-behavioural effect, the data
includes many abstracts irrelevant for CRA. Most
other errors are due to the lack of training data for
low frequency classes. For example, the CRA cor-
pus had only 27 abstracts in ?DNA repair (damage)?
class, while the new corpus has many abstracts on
DNA damage some of which are irrelevant for CRA.
The experts found the tool easy to use and felt
that if such a tool was available to support real-world
CRA, it could significantly increase their productiv-
ity and also lead to more consistent and thorough
CRA. Such a wide range of scientific evidence is dif-
ficult to gather via manual means, and chemical car-
cinogenesis is such a complex process that even the
most experienced risk assessor is incapable of mem-
orizing the full range of relevant evidence without
the support of a thorough specification / taxonomy.
Name MOA ? P
Aflatoxin B1 geno 189 0.95
Benzene geno 461 0.99
PCB non 761 0.89
Tamoxifen non 382 0.96
TCDD non 641 0.96
Class P
CA 0.94
MOA 0.95
TOX 0.99
Table 5: Chemicals and the results of the user test
6 Conclusion and Future Work
The results of our inter-annotator agreement tests,
automatic classification experiments and the user
test demonstrate that the taxonomy created by risk
assessors is accurate, well-defined, and can be use-
ful in a real-world CRA scenario. This is particu-
larly encouraging considering that the taxonomy is
based on biomedical annotation. As highlighted by
(Kim et al 2008), expert annotation is more chal-
lenging and prone to inter-annotator disagreement
than better-constrained linguistic annotation. We
believe that we obtained promising results because
we worked in collaboration with risk assessors and
developed technology which imitates their current
practices as closely as possible.
Most related work focuses on binary classifica-
tion, e.g. BioCreative II had a subtask (Krallinger
et al 2008) on the relevance classification of ab-
stracts for protein interactions. The few works
that have attempted multi-classification include e.g.
that of Aphinyanaphongs et al (2005) who applied
NMB, SVM and AdaBoost to classify abstracts of
internal medicine into four categories, and that of
Han et al (2006) who used BOS and NMB/L-SVM to
classify abstracts in five categories of protein post-
translational modifications.
In the future, we plan to refine the taxonomy fur-
ther by careful analysis of keyword types found in
the data and the taxonomic relationships defined by
experts. This will help to transform the taxonomy
into a better-developed knowledge resource. We
also need to extend the taxonomy. Although our
results show that the current taxonomy provides a
good basis for the classification of CRA literature,
it is not comprehensive: more data is required espe-
cially for low frequency classes, and the taxonomy
needs to be extended to cover more specific MOA
types (e.g. further subtypes of non-genotoxic chem-
icals).
The taxonomy can be extended by manual annota-
114
Change in F ? Classes Abstracts of class
20-100 100 - 200 200 - 1100
?F > 1% 16 (43%) 75% 33% 8%
|?F | ? 1% 15 (41%) 6% 44% 75%
?F < ?1% 6 (16%) 19% 33% 17%
Table 6: F gain(?F ) of MeSH compared to BOS
Class ? F
Carcinogenic activity 1068 92.8
Human study/epidemiology 190 77.7
Animal study 629 80.2
Cell experiments 319 78.5
Study on microorganisms 44 85.2
Mode of Action 653 85.5
Genotoxic 421 89.1
Nongenotoxic 324 76.3
Toxicokinetics 356 77.7
Absorption, . . . ,excretion 113 69.8
Metabolism 268 76.4
Toxicokinetic modeling 31 84.6
Table 7: ? abstracts and F of level 1,2 classes.
tion, supplementing it with additional information in
knowledge resources and/or by automatic methods.
One knowledge resource potentially useful is the
Medical Subject Headings (MeSH) taxonomy (Nel-
son et al 2002) which classifies PubMed abstracts
according to manually defined terms. We performed
a small experiment to investigate the usefulness of
MeSH for supplementing our current classification.
MeSH terms were first retrieved for each abstract us-
ing EFetch (NCBI 2005) and then appended to the
BOS feature vector. Best features were then selected
using fscore and classified using L-SVM. The fig-
ures in table 6 show that the results improved sig-
nificantly for 43% of the low frequency classes. Al-
though this demonstrates the potential usefulness of
additional resources, given the rapidly evolving na-
ture of CRA data, the best approach long term is
to develop technology for automatic updating of the
taxonomy from literature. Given the basic resources
we have constructed, the development of such tech-
nology is now realistic and can be done using unsu-
pervised or semi-supervised machine learning tech-
niques, e.g. (Cohen and Hersh 2005, Blaschko and
Gretton 2009).
The automatic classification could be improved
by the use of more sophisticated features extracted
using NLP tools that have been tuned for biomedi-
cal texts, such as parsers, e.g. (Tsuruoka et al 2005),
and named entity recognizers, e.g. (Corbett et al
2007), and exploiting resources such as the BioLex-
ion (Sasaki et al 2008).
Our long term goal is to develop a TM tool
specifically designed for CRA. Some tools have re-
cently been built to assist other critical activities of
biomedicine (e.g. literature curation for genetics).
A few of them have been evaluated for their practi-
cal usefulness in a real-world scenario (Karamanis
et al 2008, Demaine et al 2006). Such tools and
evaluations act as an important proof of concept for
biomedical TM and help to develop technology for
the needs of practical applications.
According to the interviews we conducted (Sec-
tion 2), a tool capable of identifying, ranking and
classifying articles based on the evidence they con-
tain, displaying the results to experts, and assisting
also in subsequent steps of CRA would be particu-
larly welcome. Such a tool, if developed in close
collaboration with users, could significantly increase
the productivity of CRA and enable risk assessors
to concentrate on what they are best at: the expert
judgement.
Acknowledgements Our work was funded by the
Royal Society (UK), the Medical Research Council
(G0601766) (UK) and the Swedish Council for Working
Life and Social Research (Sweden). LS was supported
by a Dorothy Hodgkin Postgraduate Award (UK). We
would like to thank Ian Lewin for his assistance at the
early stages of this work and for providing the first ver-
sion of the annotation tool. We are also grateful to Johan
Hogberg for supporting the annotation and the taxonomy
construction work.
References
Sophia Ananiadou, Douglas B. Kell, and Jun ichi Tsujii.
Text mining and its potential applications in systems
biology. Trends in Biotechnology, 24(12), 2006.
Y. Aphinyanaphongs, I. Tsamardinos, A. Statnikov,
D. Hardin, and C.F. Aliferis. Text categorization
models for high-quality article retrieval in internal
medicine. JAMIA, 12(2), 2005.
Matthew Blaschko and Arthur Gretton. Learning tax-
onomies by dependence maximization. In 22rd NIPS,
2009.
Yi-Wei Chen and Chih-Jen Lin. Combining SVMs with
various feature selection strategies. In Feature extrac-
tion, foundations and applications. 2006.
Aaron M. Cohen and William R. Hersh. A survey of
115
current work in biomedical text mining. Briefings in
Bioinformatics, 6(1), 2005.
Jacob Cohen. A coefficient of agreement for nominal
scales. Educ. Psychol. Meas., 20(1), 1960.
K. Bretonnel Cohen, Hong Yu, Philip E. Bourne, and
Lynette Hirschman. Translating biology:text mining
tools that work. In PSB, 2008.
Peter Corbett, Colin Batchelor, and Simone Teufel. An-
notation of chemical named entities. In Proceedings of
the ACL, 2007.
Jeffrey Demaine, Joel Martin, Lynn Wei, and Berry
de Bruijn. Litminer: integration of library services
within a bio-informatics application. Biomedical Dig-
ital Libraries, 3(1), 2006.
ECHA, 2008. Guidance on Information Requirements
and Chemical Safety Assessment. European Chemicals
Agency, 2008.
Bo Han, Zoran Obradovic, Zhang zhi Hu, Cathy H. Wu,
and Slobodan Vucetic. Substring selection for biomed-
ical document classification. Bioinformatics, 22, 2006.
Lawrence Hunter and K. Bretonnel Cohen. Biomedical
language processing: What?s beyond pubmed? Mol
Cell, 21(5), 2006.
N. Karamanis, R. Seal, I. Lewin, P. McQuilton, A. Vla-
chos, C. Gasperin, R. Drysdale, and T. Briscoe. Nat-
ural language processing in aid of flybase curators.
BMC Bioinformatics, 9(1), 2008.
Ashraf M. Kibriya, Eibe Frank, Bernhard Pfahringer, and
Geoffrey Holmes. Multinomial naive bayes for text
categorization revisited. In Australian Conference on
AI, volume 3339, 2004.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. Cor-
pus annotation for mining biomedical events from lter-
ature. BMC Bioinformatics, 9, 2008.
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. Overview of the
protein-protein interaction annotation extraction task
of biocreative ii. Genome Biology, 2008.
J.Richard Landis and Gary G.Koch. The measurement of
observer agreement for categorical data. Biometrics,
33(1), 1977.
Ian Lewin, Ilona Silins, Anna Korhonen, Johan Hogberg,
and Ulla Stenius. A new challenge for text mining:
Cancer risk assessment. In Proceedings of the ISMB
BioLINK Special Interest Group on Text Data Mining.,
2008.
NCBI. Efetch entrez utility, 2005. URL
http://www.ncbi.nlm.nih.gov/entrez/
query/static/efetch_help.html.
Sturart J. Nelson, Tammy Powell, and Besty L.
Humphreys. The Unified Medical Language System
(UMLS) Project. In Encyclopedia of Library and In-
formation Science, pages 369?378. Marcel Dekker,
2002.
M. F. Porter. An algorithm for suffix stripping. Program,
14(3):130?137, 1980.
Jason D. M. Rennie and David Karger. Tackling the poor
assumptions of naive bayes text classifiers. In In Pro-
ceedings of the 20th ICML, 2003.
Y. Sasaki, S. Montemagni, P. Pezik, D. Rebholz-
Schuhmann, J. McNaught, and S. Ananiadou. BioLex-
icon: A Lexical Resource for the Biology Domain.
2008.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. Developing a Robust Part-
of-Speech Tagger for Biomedical Text. 3746, 2005.
EPA, 2005. Guidelines for carcinogen risk as-
sessment. U.S. Environmental Protection Agency,
2005. URL http://www.epa.gov/iris/
cancer032505.pdf.
Vladimir N. Vapnik. The nature of statistical learning
theory. New York, NY, USA, 1995.
Hongning Wang, Minlie Huang, Shilin Ding, and Xi-
aoyan Zhu. Exploiting and integrating rich features
for biological literature classification. BMC Bioinfor-
matics, 9(Suppl 3), 2008.
Yiming Yang and Xin Liu. A re-examination of text cate-
gorization methods. In Proceedings of the 22nd SIGIR,
New York, NY, USA, 1999.
Yiming Yang and Jan O. Pedersen. A comparative study
on feature selection in text categorization. 1997.
Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,
and Kevin B. Cohen. Frontiers of biomedical text min-
ing: current progress. Brief Bioinform, 8(5), 2007.
116
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 76?80, Dublin, Ireland, August 23-29 2014.
CRAB 2.0: A text mining tool for supporting literature review in chemical
cancer risk assessment
Yufan Guo
1
, Diarmuid
?
O S
?
eaghdha
1
, Ilona Silins
2
, Lin Sun
1
,
Johan H
?
ogberg
2
, Ulla Stenius
2
, Anna Korhonen
1
1
Computer Laboratory, University of Cambridge, UK
2
Institute of Environmental Medicine, Karolinska Institutet, Stockholm, Sweden
Abstract
Chemical cancer risk assessment is a literature-dependent task which could greatly benefit from
text mining support. In this paper we describe CRAB ? the first publicly available tool for
supporting the risk assessment workflow. CRAB, currently at version 2.0, facilitates the gathering
of relevant literature via PubMed queries as well as semantic classification, statistical analysis and
efficient study of the literature. The tool is freely available as an in-browser application.
1 Introduction
Biomedical text mining addresses the great need to access information in the growing body of literature
in biomedical sciences. Prior research has produced useful tools for supporting practical tasks such as
literature curation and development of semantic databases, among others (Chapman and Cohen, 2009;
Harmston et al., 2010; Simpson and Demner-Fushman, 2012; McDonald and Kelly, 2012). In this paper
we describe a tool we have built to aid literature exploration for the task of chemical risk assessment
(CRA). The need for assessment of chemical hazards, exposures and their corresponding health risks is
growing, as many countries have tightened up their chemical safety rules. CRA work requires thorough
review of available scientific data for each chemical under inspection, much of which can be found in
scientific literature (EPA, 2005). Since the scientific data is highly varied and well-studied chemicals
may have tens of thousands of publications (e.g. to date PubMed contains 23,665 articles mentioning
phenobarbital), the task can be extremely time consuming when conducted via conventional means
(Korhonen et al., 2009). As a result, there is interest among the CRA community in text mining tools that
can aid and streamline the literature review process.
We have developed CRAB, an online system that supports the entire process of literature review for
cancer risk assessors. It is the first and only NLP system that serves this need. CRAB contains three main
components:
1. Literature search with PubMed integration
2. Semantic classification of abstracts with summary visualisation
3. Literature browsing with markup of information structure
These components are described further in Section 2 below. Version 2.0 of CRAB is freely available as an
in-browser application; see Section 4 for access information.
2 System description
2.1 Literature search
The first step for the user is to retrieve a collection of scientific articles relevant to their need, e.g., all
articles with abstracts that contain the name of a given chemical. The CRAB 2.0 search page (Figure
1) allows the user to directly query the MEDLINE database of biomedical abstracts. The search query
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
76
Figure 1: The CRAB 2.0 search interface
(a) Taxonomy view
gene
ral(wo
rds re
lated 
to RA
)
hum
an s
tudy
/epid
emio
logy
tumo
r rela
ted
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
rs
polym
orph
ism
anim
al st
udy
stud
y len
gth
2?ye
ar ca
ncer
 bioa
ssay
shor
t and
  me
dium tumo
rs
pren
eopl
astic
 lesio
ns
mo
rpho
logic
al ef
fect 
on ti
ssue
/orga
n
bioc
hem
ical/c
ellbio
logic
al ef
fects
biom
arke
r
type
 of a
nima
l
gene
ticall
y mo
dified
 anim
als
cell 
expe
rime
nts 
bioc
hem
ical/c
ellbio
logic
al ef
fects
subc
ellula
r sys
tems
 
stud
y on
 mic
roorg
anism
s
rev
iew 
artic
le, s
umm
ary
Scientific Evidence
% ab
strac
ts
0
20
40
60
80
100
(b) Histogram view
Figure 2: The CRAB 2.0 classification component
is sent, and the results received, using the E-Utilities web service provided by the National Center
for Biotechnology Information.
1
This query interface supports PubMed Advanced Search, facilitating
complex Boolean queries.
2.2 Semantic classification
The document collection returned by the PubMed web service is passed in XML format to a semantic
classifier that annotates each abstract with 42 binary labels indicating the presence/absence of concepts
relevant to CRA. These concepts are organised hierarchically in two main taxonomies: (1) kinds of
scientific evidence used for CRA (e.g., human studies, animal studies, cell experiments, biochemical/cell
biological effects); (2) the carcinogenic modes of action indicated by the evidence (e.g., genotoxic,
nongenotoxic/indirect genotoxic, cell death, inflammation, angiogenesis). The underlying classifier is a
support vector machine (SVM) trained on a dataset of 3,078 manually annotated abstracts. Features used
by the SVM include lexical n-grams, character n-grams and MeSH concepts. For more details on the
concept taxonomies, training corpus and classifier see Korhonen et al. (2012).
1
http://www.ncbi.nlm.nih.gov/books/NBK25501/
77
Figure 3: The CRAB 2.0 information structure component
Once each abstract in the retrieved collection has been classified, the user is presented with a summary
of counts for each concept (Figure 2a). In a user study, risk assessors found this summary very useful for
obtaining a broad overview of the literature, identifying groups of chemicals with similar toxicological
profiles and identifying data gaps (Korhonen et al., 2012). The user can also request a histogram
visualisation (Figure 2b), which is produced through a call to the statistical software R.
2
2.3 Literature browsing
The risk assessment workflow involves close reading of relevant abstracts to identify specific information
about methods, experimental details, results and conclusions. While it is not feasible to automate this
process, we have shown that automatic markup and visualisation of abstracts? information structure
can accelerate it considerably (Guo et al., 2011). The model of information structure incorporated in
CRAB 2.0 is based on argumentative zoning (Teufel and Moens, 2002; Mizuta et al., 2006; Teufel, 2010),
whereby the text of a scientific abstract (or article) is segmented into blocks of sentences that carry a
specific rhetorical function and combine to communicate the argument the authors wish to convey to
the reader. The markup scheme used in our system labels each sentence with one of seven categories:
background, objective, method, result, conclusion, related work and future work (Guo et al., 2010). The
CRAB system incorporates preprocessing (lemmatisation, POS tagging, parsing) with the C&C toolkit
3
and information structure markup with an SVM classifier that labels sentences according to a combination
of lexical, syntactic and discourse features (Guo et al., 2011). The classifier has been trained on an
annotated dataset of 1,000 CRA abstracts (Guo et al., 2010).
The automatic information structure markup is used to support browsing of the set of abstracts assigned
a label of interest by the semantic classifier; e.g., the user can inspect all abstracts labelled genotoxic
(Figure 3). Each information structure category is highlighted in a different colour and the user can select
a single category to focus on. To our knowledge, CRAB 2.0 is the first publicly available online tool that
provides information structure analysis of biomedical literature.
3 Evaluation
Intrinsic cross-validation evaluations of the semantic taxonomy classifier and information structure
classifier show high performance: 0.78 macro-averaged F-score (Korhonen et al., 2012) and 0.88 accuracy
(Guo et al., 2011), respectively. Furthermore, user-based evaluation in the context of real-life CRA has
2
http://www.r-project.org/
3
http://svn.ask.it.usyd.edu.au/trac/candc
78
produced promising results. (Korhonen et al., 2012) showed that the concept distributions produced by
our classifier confirmed known properties of chemicals without human input. Guo et al. (2011) found that
integrating information structure visualisation in abstract browsing helped risk assessors to find relevant
information in abstracts 7-8% more quickly.
4 Use
CRAB 2.0 is freely available as an in-browser application at http://omotesando-e.cl.cam.
ac.uk/CRAB/request.html. New users can register an id and password to allow them to store
and retrieve data from previous sessions. Alternatively, they can use an anonymous guest account (id
guest@coling, password guest@coling).
5 Conclusion
We have presented Version 2.0 of CRAB, the first NLP tool for supporting the workflow of literature
review for cancer risk assessment. CRAB meets a real, specialised need and is already being used to
improve the efficiency of CRA work. Although currently focused on cancer, CRAB can be easily adapted
to other health risks provided with the appropriate taxonomy and annotated data for machine learning. In
the future, the tool can be developed further in various ways, e.g. to support submissions in other formats
than PubMed XML; to take into account journal impact factors, number of citations and cross references
to better organize the literature; and to offer enriched statistical analysis of classified literature.
Acknowledgements
This work was supported by the Royal Society, Vinnova and the Swedish Research Council.
References
Wendy W. Chapman and K. Bretonnel Cohen. 2009. Current issues in biomedical text mining and natural language
processing. Journal of Biomedical Informatics, 42(5):757?759.
EPA. 2005. Guidelines for carcinogen risk assessment. US Environmental Protection Agency.
Yufan Guo, Anna Korhonen, Maria Liakata, Ilona Silins, Lin Sun, and Ulla Stenius. 2010. Identifying the informa-
tion structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP-10,
Uppsala, Sweden.
Yufan Guo, Anna Korhonen, Ilona Silins, and Ulla Stenius. 2011. Weakly supervised learning of information
structure of scientific abstracts: Is it accurate enough to benefit real-world tasks in biomedicine? Bioinformatics,
27(22):3179?3185.
Nathan Harmston, Wendy Filsell, and Michael P.H. Stumpf. 2010. What the papers say: Text mining for genomics
and systems biology. Human Genomics, 5(1):17?29.
Anna Korhonen, Ilona Silins, Lin Sun, and Ulla Stenius. 2009. The first step in the development of text min-
ing technology for cancer risk assessment: Identifying and organizing scientific evidence in risk assessment
literature. BMC Bioinformatics, 10:303.
Anna Korhonen, Diarmuid
?
O S?eaghdha, Ilona Silins, Lin Sun, Johan H?ogberg, and Ulla Stenius. 2012. Text
mining for literature review and knowledge discovery in cancer risk assessment and research. PLoS ONE,
7(4):e33427.
Diane McDonald and Ursula Kelly. 2012. The value and benefit of text mining to UK further and higher education.
Report 811, JISC.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel Collier. 2006. Zone analysis in biology articles as a basis
for information extraction. International Journal of Medical Informatics, 75(6):468?487.
Matthew S. Simpson and Dina Demner-Fushman. 2012. Biomedical text mining: A survey of recent progress. In
Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data. Springer.
79
Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
Simone Teufel. 2010. The Structure of Scientific Articles: Applications to Citation Indexing and Summarization.
CSLI Publications, Stanford, CA.
80
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 99?107,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Identifying the Information Structure of Scientific Abstracts: An
Investigation of Three Different Schemes
Yufan Guo
University of Cambridge, UK
yg244@cam.ac.uk
Anna Korhonen
University of Cambridge, UK
alk23@cam.ac.uk
Maria Liakata
Aberystwyth University, UK
mal@aber.ac.uk
Ilona Silins
Karolinska Institutet, SWEDEN
Ilona.Silins@ki.se
Lin Sun
University of Cambridge, UK
ls418@cam.ac.uk
Ulla Stenius
Karolinska Institutet, SWEDEN
Ulla.Stenius@ki.se
Abstract
Many practical tasks require accessing
specific types of information in scientific
literature; e.g. information about the ob-
jective, methods, results or conclusions of
the study in question. Several schemes
have been developed to characterize such
information in full journal papers. Yet
many tasks focus on abstracts instead. We
take three schemes of different type and
granularity (those based on section names,
argumentative zones and conceptual struc-
ture of documents) and investigate their
applicability to biomedical abstracts. We
show that even for the finest-grained of
these schemes, the majority of categories
appear in abstracts and can be identified
relatively reliably using machine learning.
We discuss the impact of our results and
the need for subsequent task-based evalu-
ation of the schemes.
1 Introduction
Scientific abstracts tend to be very similar in terms
of their information structure. For example, many
abstracts provide some background information
before defining the precise objective of the study,
and the conclusions are typically preceded by the
description of the results obtained.
Many readers of scientific abstracts are inter-
ested in specific types of information only, e.g.
the general background of the study, the methods
used in the study, or the results obtained. Accord-
ingly, many text mining tasks focus on the ex-
traction of information from certain parts of ab-
stracts only. Therefore classification of abstracts
(or full articles) according to the categories of in-
formation structure can support both the manual
study of scientific literature as well as its auto-
matic analysis, e.g. information extraction, sum-
marization and information retrieval (Teufel and
Moens, 2002; Mizuta et al, 2005; Tbahriti et al,
2006; Ruch et al, 2007).
To date, a number of different schemes and
techniques have been proposed for sentence-based
classification of scientific literature according to
information structure, e.g. (Teufel and Moens,
2002; Mizuta et al, 2005; Lin et al, 2006; Hi-
rohata et al, 2008; Teufel et al, 2009; Shatkay
et al, 2008; Liakata et al, 2010). Some of the
schemes are coarse-grained and merely classify
sentences according to typical section names seen
in scientific documents (Lin et al, 2006; Hirohata
et al, 2008). Others are finer-grained and based
e.g. on argumentative zones (Teufel and Moens,
2002; Mizuta et al, 2005; Teufel et al, 2009),
qualitative dimensions (Shatkay et al, 2008) or
conceptual structure (Liakata et al, 2010) of doc-
uments.
The majority of such schemes have been de-
veloped for full scientific journal articles which
are richer in information and also considered to
be more in need of the definition of information
structure (Lin, 2009). However, many practical
tasks currently focus on abstracts. As a distilled
summary of key information in full articles, ab-
stracts may exhibit an entirely different distribu-
tion of scheme categories than full articles. For
tasks involving abstracts, it would be useful to
know which schemes are applicable to abstracts
and which can be automatically identified in them
with reasonable accuracy.
In this paper, we will compare the applicabil-
ity of three different schemes ? those based on
section names, argumentative zones and concep-
tual structure of documents ? to a collection of
biomedical abstracts used for cancer risk assess-
ment (CRA). CRA is an example of a real-world
task which could greatly benefit from knowledge
about the information structure of abstracts since
cancer risk assessors look for a variety of infor-
mation in them ranging from specific methods to
99
results concerning different chemicals (Korhonen
et al, 2009). We report work on the annotation
of CRA abstracts according to each scheme and
investigate the schemes in terms of their distri-
bution, mutual overlap, and the success of iden-
tifying them automatically using machine learn-
ing. Our investigation provides an initial idea of
the practical usefulness of the schemes for tasks
involving abstracts. We discuss the impact of our
results and the further task-based evaluation which
we intend to conduct in the context of CRA.
2 The three schemes
We investigate three different schemes ? those
based on Section Names (S1), Argumentative
Zones (S2) and Core Scientific Concepts (S3):
S1: The first scheme differs from the others in the
sense that it is actually designed for abstracts. It
is based on section names found in some scientific
abstracts. We use the 4-way classification from
(Hirohata et al, 2008) where abstracts are divided
into objective, method, results and conclusions.
Table 1 provides a short description of each cate-
gory for this and other schemes (see also this table
for any category abbreviations used in this paper).
S2: The second scheme is based on Argumenta-
tive Zoning (AZ) of documents. The idea of AZ
is to follow the knowledge claims made by au-
thors. Teufel and Moens (2002) introduced AZ
and applied it to computational linguistics papers.
Mizuta et al (2005) modified the scheme for biol-
ogy papers. More recently, Teufel et al (2009) in-
troduced a refined version of AZ and applied it to
chemistry papers. As these schemes are too fine-
grained for abstracts (some of the categories do
not appear in abstracts at all), we adopt a reduced
version of AZ which integrates seven categories
from (Teufel and Moens, 2002) and (Mizuta et al,
2005) - those which actually appear in abstracts.
S3: The third scheme is concept-driven and
ontology-motivated (Liakata et al, 2010). It treats
scientific papers as humanly-readable representa-
tions of scientific investigations and seeks to re-
trieve the structure of the investigation from the
paper as generic high-level Core Scientific Con-
cepts (CoreSC). The CoreSC is a 3-layer annota-
tion scheme but we only consider the first layer
in the current work. The second layer pertains to
properties of the categories (e.g. ?advantage? vs.
?disadvantage? of METH, ?new? vs. ?old? METH
or OBJT). Such level of granularity is rare in ab-
stracts. The 3rd layer involves coreference iden-
tification between the same instances of each cat-
egory, which is also not of concern in abstracts.
With eleven categories, S3 is the most fine-grained
of our schemes. CoreSC has been previously ap-
plied to chemistry papers (Liakata et al, 2010,
2009).
3 Data: cancer risk assessment abstracts
We used as our data the corpus of CRA ab-
stracts described in (Korhonen et al, 2009) which
contains MedLine abstracts from different sub-
domains of biomedicine. The abstracts were se-
lected so that they provide rich information about
various scientific data (human, animal and cellu-
lar) used for CRA. We selected 1000 abstracts (in
random) from this corpus. The resulting data in-
cludes 7,985 sentences and 225,785 words in total.
4 Annotation of abstracts
Annotation guidelines. We used the guidelines of
Liakata for S3 (Liakata and Soldatova, 2008), and
developed the guidelines for S1 and S2 (15 pages
each). The guidelines define the unit (a sentence)
and the categories of annotation and provide ad-
vice for conflict resolution (e.g. which categories
to prefer when two or several are possible within
the same sentence), as well as examples of anno-
tated abstracts.
Annotation tool. We modified the annotation tool
of Korhonen et al (2009) so that it could be used to
annotate abstracts according to the schemes. This
tool was originally developed for the annotation of
CRA abstracts according to the scientific evidence
they contain. The tool works as a Firefox plug-in.
Figure 1 shows an example of an abstract anno-
tated according to the three schemes.
Description of annotation. Using the guidelines
and the tool, the CRA corpus was annotated ac-
cording to each of the schemes. The annotation
proceeded scheme by scheme, independently, so
that annotations of one scheme were not based on
any of the other two. One annotator (a computa-
tional linguist) annotated all the abstracts accord-
ing to the three schemes, starting from the coarse-
grained S1, then proceeding to S2 and finally to
the finest-grained S3. It took 45, 50 and 90 hours
in total for S1, S2 and S3, respectively.
The resulting corpus. Table 2 shows the distri-
bution of sentences per scheme category in the re-
sulting corpus.
100
Table 1: The Three Schemes
S1 Objective OBJ The background and the aim of the research
Method METH The way to achieve the goal
Result RES The principle findings
Conclusion CON Analysis, discussion and the main conclusions
S2 Background BKG The circumstances pertaining to the current work, situation, or its causes, history, etc.
Objective OBJ A thing aimed at or sought, a target or goal
Method METH A way of doing research, esp. according to a defined and regular plan; a special form of proce-
dure or characteristic set of procedures employed in a field of study as a mode of investigation
and inquiry
Result RES The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained
by calculation
Conclusion CON A judgment or statement arrived at by any reasoning process; an inference, deduction, induc-
tion; a proposition deduced by reasoning from other propositions; the result of a discussion,
or examination of a question, final determination, decision, resolution, final arrangement or
agreement
Related work REL A comparison between the current work and the related work
Future work FUT The work that needs to be done in the future
S3 Hypothesis HYP A statement that has not been yet confirmed rather than a factual statement
Motivation MOT The reason for carrying out the investigation
Background BKG Description of generally accepted background knowledge and previous work
Goal GOAL The target state of the investigation where intended discoveries are made
Object OBJT An entity which is a product or main theme of the investigation
Experiment EXP Experiment details
Model MOD A statement about a theoretical model or framework
Method METH The means by which the authors seek to achieve a goal of the investigation
Observation OBS The data/phenomena recorded within an investigation
Result RES Factual statements about the outputs of an investigation
Conclusion CON Statements inferred from observations and results, relating to research hypothesis
Inter-annotator agreement. We measured the
inter-annotator agreement on 300 abstracts (i.e. a
third of the corpus) using three annotators (one lin-
guist, one expert in CRA, and the computational
linguist who annotated all the corpus). Accord-
ing to Cohen?s Kappa (Cohen, 1960), the inter-
annotator agreement for S1, S2, and S3 was ? =
0.84, ? = 0.85, and ? = 0.50, respectively. Ac-
cording to (Landis and Koch, 1977), the agree-
ment 0.81-1.00 is perfect and 0.41-0.60 is mod-
erate. Our results indicate that S1 and S2 are
the easiest schemes for the annotators and S3 the
most challenging. This is not surprising as S3 is
the scheme with the finest granularity. Its reliable
identification may require a longer period of train-
ing and possibly improved guidelines. Moreover,
previous annotation efforts using S3 have used do-
main experts for annotation (Liakata et al, 2009,
2010). In our case the domain expert and the lin-
guist agreed the most on S3 (? = 0.60). For S1
and S2 the best agreement was between the lin-
guist and the computational linguist (? = 0.87 and
? = 0.88, respectively).
Table 2: Distribution of sentences in the scheme-
annotated CRA corpus
S1 OBJ METH RES CON
61483 39163 89575 35564 Words
2145 1396 3203 1241 Sentences
27% 17% 40% 16% Sentences
S2 BKG OBJ METH RES CON REL FUT
36828 23493 41544 89538 30752 2456 1174 Words
1429 674 1473 3185 1082 95 47 Sentences
18% 8% 18% 40% 14% 1% 1% Sentences
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
2676 4277 28028 10612 15894 22444 1157 17982 17402 75951 29362 Words
99 172 1088 294 474 805 41 637 744 2582 1049 Sentences
1% 2% 14% 4% 6% 10% 1% 8% 9% 32% 13% Sentences
5 Comparison of the schemes in terms of
annotations
The three schemes we have used to annotate ab-
stracts were developed independently and have
separate guidelines. Thus, even though they seem
to have some categories in common (e.g. METH,
RES, CON) this does not necessarily guarantee that
the latter cover the same information across all
three schemes. We therefore wanted to investigate
the relation between the schemes and the extent of
overlap or complementarity between them.
We used the annotations obtained with each
scheme to create three contingency matrices for
pairwise comparison. We calculated the chi-
squared Pearson statistic, the chi-squared like-
101
Figure 1: An example of an abstract annotated ac-
cording to the three schemes
S1
S2
S3
lihood ratio, the contingency coefficient and
Cramer?s V (Table 3)1, all of which showed a def-
inite correlation between rows and columns for the
pairwise comparison of all three schemes.
However, none of the above measures give an
indication of the differential association between
schemes, i.e. whether it goes both directions and
to what extent. For this reason we calculated the
Goodman-Kruskal lambda L statistic (Siegel and
Castellan, 1988), which gives us the reduction in
error for predicting the categories of one annota-
tion scheme, if we know the categories assigned
according to the other. When using the categories
of S1 as the independent variables, we obtained a
lambda of over 0.72 which suggests a 72% reduc-
tion in error in predicting S2 categories and 47% in
1These are association measures for r x c tables. We used
the implementation in the vcd package of R (http://www.r-
project.org/).
predicting S3 categories. With S2 categories being
the independent variables, we obtained a reduction
in error of 88% when predicting S1 and 55% when
predicting S3 categories. The lower lambdas for
predicting S3 are hardly surprising as S3 has 11
categories as opposed to 4 and 7 for S1 and S2 re-
spectively. S3 on the other hand has strong predic-
tive power in predicting the categories of S1 and
S2 with lambdas of 0.86 and 0.84 respectively. In
terms of association, S1 and S2 seem to be more
strongly associated, followed by S1 and S3 and
then S2 and S3.
We were then interested in the correspondence
between the actual categories of the three schemes,
which is visualized in Figure 2. Looking at the
categories of S1, OBJ maps mostly to BKG and OBJ
in S2 (with a small percentage in METH and REL).
S1 OBJ maps to BKG, GOAL, HYP, MOT and OBJT
in S3 (with a small percentage in METH and MOD).
S1 METH maps to METH in S2 (with a small per-
centage in S2 OBJ) while it maps to EXP, METH
and MOD in S3 (with a small percentage in GOAL
and OBJT). S1 RES covers S2 RES and 40% REL,
whereas in S3 it covers RES, OBS and 20% MOD.
S1 CON covers S2 CON, FUT, 45% REL and a small
percentage of RES. In terms of the S2 vs S3 com-
parison, S2 BKG maps to S3 BKG, HYP, MOT and a
small percentage of OBJT and MOD. S2 CON maps
to S3 CON, with a small percentage in RES, OBS
and HYP. S2 FUT maps entirely to S3 CON. S2
METH maps to S3 METH, EXP, MOD, 20% OBJT
and a small percentage of GOAL. S2 OBJ maps
to S3 GOAL and OBJT, with 15% HYP, MOD and
MOT and a small percentage in METH. S2 REL
spans across S3 CON, RES, MOT and OBJT, albeit
in very small percentages. Finally, S2 RES maps to
S3 RES and OBS, with 25% in MOD and small per-
centages in METH, CON, OBJT. Thus, it appears
that each category in S1 maps to a couple of cate-
gories in S2 and several in S3, which in turn seem
to elaborate on the S2 categories.
Based on the above analysis of the categories,
it is reasonable to assume a subsumption relation
between the categories of the type S1 > S2 >
S3, with REL cutting across several of the S3 cat-
egories and FUT branching off S3 CON. This is
an interesting and exciting outcome given that the
three different schemes have such a different ori-
gin.
102
Table 3: Association measures between schemes S1, S2, S3
S1 vs S2 S1 vs S3 S2 vs S3
X2 df P X2 df P X2 df P
Likelihood Ratio 5577.1 18 0 5363.6 30 0 6293.4 60 0
Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0
Contingency Coeff 0.842 0.837 0.871
Cramer?s V 0.901 0.885 0.725
Figure 2: Pairwise interpretation of categories of
one scheme in terms of the categories of the other.
6 Automatic identification of information
structure
6.1 Features
The first step in automatic identification of infor-
mation structure is feature extraction. We chose
a number of general purpose features suitable for
all the three schemes. With the exception of our
novel verb class feature, the features are similar to
those employed in related works, e.g. (Teufel and
Moens, 2002; Mullen et al, 2005; Hirohata et al,
2008):
History. There are typical patterns in the infor-
mation structure, e.g. RES tends to be followed
by CON rather than by BKG. Therefore, we used
the category assigned to the previous sentence as
a feature.
Location. Categories tend to appear in typical po-
sitions in a document, e.g. BKG occurs often in the
beginning and CON at the end of the abstract. We
divided each abstract into ten equal parts (1-10),
measured by the number of words, and defined the
location (of a sentence) feature by the parts where
the sentence begins and ends.
Word. Like many text classification tasks, we em-
ployed all the words in the corpus as features.
Bi-gram. We considered each bi-gram (combina-
tion of two word features) as a feature.
Verb. Verbs are central to the meaning of sen-
tences, and can vary from one category to another.
For example, experiment is frequent in METH and
conclude in CON. Previous works have used the
matrix verb of each sentence as a feature. Because
the matrix verb is not the only meaningful verb,
we used all the verbs instead.
Verb Class. Because individual verbs can result in
sparse data problems, we also experimented with a
novel feature: verb class (e.g. the class of EXPERI-
MENT verbs for verbs such as measure and inject).
We obtained 60 classes by clustering verbs appear-
ing in full cancer risk assessment articles using the
approach of Sun and Korhonen (2009).
POS. Tense tends to vary from one category to an-
other, e.g. past is common in RES and past partici-
103
ple in CON. We used the part-of-speech (POS) tag
of each verb assigned by the C&C tagger (Curran
et al, 2007) as a feature.
GR. Structural information about heads and de-
pendents has proved useful in text classification.
We used grammatical relations (GRs) returned by
the C&C parser as features. They consist of a
named relation, a head and a dependent, and pos-
sibly extra parameters depending on the relation
involved, e.g. (dobj investigate mouse). We cre-
ated features for each subject (ncsubj), direct ob-
ject (dobj), indirect object (iobj) and second object
(obj2) relation in the corpus.
Subj and Obj. As some GR features may suf-
fer from data sparsity, we collected all the subjects
and objects (appearing with any verbs) from GRs
and used them as features.
Voice. There may be a correspondence between
the active and passive voice and categories (e.g.
passive is frequent in METH). We therefore used
voice as a feature.
6.2 Methods
We used Naive Bayes (NB) and Support Vector
Machines (SVM) for classification. NB is a sim-
ple and fast method while SVM has yielded high
performance in many text classification tasks.
NB applies Bayes? rule and Maximum Like-
lihood estimation with strong independence as-
sumptions. It aims to select the class c with maxi-
mum probability given the feature set F :
argmaxc P (c|F )=argmaxc
P (c)?P (F |c)
P (F )
=argmaxc P (c)?P (F |c)
=argmaxc P (c)?
?
f?F P (f |c)
SVM constructs hyperplanes in a multidimen-
sional space that separates data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w ? x? b = 0, where w is the normal vector
to the hyperplane. We want to maximize the dis-
tance from the hyperplane to the data points, or the
distance between two parallel hyperplanes each of
which separates the data. The parallel hyperplanes
can be written as:
w?x?b = 1 andw?x?b = ?1, and the distance
between the two is 2|w| . The problem reduces to:
Minimize |w|
Subject to w ? xi ? b ? 1 for xi of one class,
and w ? xi ? b ? ?1 for xi of the other.
7 Experimental evaluation
7.1 Preprocessing
We developed a tokenizer to detect the bound-
aries of sentences and to perform basic tokenisa-
tion, such as separating punctuation from adjacent
words e.g. in tricky biomedical terms such as 2-
amino-3,8-diethylimidazo[4,5-f]quinoxaline. We
used the C&C tools (Curran et al, 2007) for POS
tagging, lemmatization and parsing. The lemma
output was used for extracting Word, Bi-gram and
Verb features. The parser produced GRs for each
sentence from which we extracted the GR, Subj,
Obj and Voice features. We only considered the
GRs relating to verbs. The ?obj? marker in a sub-
ject relation indicates a verb in passive voice (e.g.
(ncsubj observed 14 difference 5 obj)). To control
the number of features we removed the words and
GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences, and lemmatized the
lexical items for all the features.
7.2 Evaluation methods
We used Weka (Witten, 2008) for the classifica-
tion, employing its NB and SVM linear kernel. The
results were measured in terms of accuracy (the
percentage of correctly classified sentences), pre-
cision, recall, and F-Measure. We used 10-fold
cross validation to avoid the possible bias intro-
duced by relying on any one particular split of the
data. The data were randomly divided into ten
parts of approximately the same size. Each indi-
vidual part was retained as test data and the re-
maining nine parts were used as training data. The
process was repeated ten times with each part used
once as the test data. The resulting ten estimates
were then combined to give a final score. We
compare our classifiers against a baseline method
based on random sampling of category labels from
training data and their assignment to sentences on
the basis of their observed distribution.
7.3 Results
Table 4 shows F-measure results when using each
individual feature alone, and Table 5 when using
all the features but the individual feature in ques-
tion. In these two tables, we only report the results
for SVM which performed considerably better than
NB. Although we have results for most scheme
categories, the results for some are missing due to
the lack of sufficient training data (see Table 2), or
due to a small feature set (e.g. History alone).
104
Table 4: F-Measure results when using each in-
dividual feature alone
a b c d e f g h i j k
S1 OBJ .39 .83 .71 .69 .52 .45 .45 .45 .54 .39 -
METH - .47 .81 .74 .63 .49 - .46 .03 .42 .51
RES - .76 .85 .86 .76 .70 .72 .69 .70 .68 .54
CON - .72 .70 .65 .63 .53 .49 .57 .68 .20 -
S2 BKG .26 .73 .69 .67 .45 .38 .56 .33 .33 .29 -
OBJ - .13 .72 .68 .54 .63 - .49 .48 .20 -
METH - .50 .81 .72 .64 .47 - .47 .03 .42 .51
RES - .76 .85 .87 .76 .72 .72 .70 .69 .68 .54
CON - .70 .73 .71 .62 .51 .40 .61 .67 .23 -
REL - - - - - - - - - - -
FUT - - - - - - - - - - -
S3 HYP - - - - .67 - - - - - -
MOT .18 .57 .70 .49 .39 .13 .36 .33 .30 .40 -
BKG - - .54 .40 .21 - - .11 .06 .06 -
GOAL - - .53 .33 .22 - .19 .31 - .25 -
OBJT - - .73 .63 .60 .10 - .26 .32 - -
EXP - .22 .63 .46 .33 .30 - .31 .07 .44 .25
MOD - - - - - - - - - - -
METH - - .82 .61 .39 .39 - .50 - .37 -
OBS - .59 .75 .71 .63 .56 .56 .54 .48 .52 .47
RES - - .87 .73 .41 .34 - .38 .24 .35 -
CON - .74 .68 .65 .65 .50 .48 .49 .55 .21 -
a-k: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR,
Subj, Obj, Voice
Looking at individual features alone, Word,
Bi-gram and Verb perform the best for all the
schemes, and History and Voice perform the worst.
In fact History performs very well on the training
data, but for the test data we can only use esti-
mates rather than the actual labels. The Voice fea-
ture works only for RES and METH for S1 and S2,
and for OBS for S3. This feature is probably only
meaningful for some of the categories.
When using all but one of the features, S1 and
S2 suffer the most from the absence of Location,
while S3 from the absence of Word/POS. Verb
Class on its own performs worse than Verb, how-
ever when combined with other features it per-
forms better: leave-Verb-out outperforms leave-
Verb Class-out.
After comparing the various combinations of
features, we found that the best selection of fea-
tures was all but the Verb for all the schemes. Ta-
ble 6 shows the results for the baseline (BL), and
the best results for NB and SVM. NB and SVM per-
form clearly better than BL for all the schemes.
The results for SVM are the best. NB yields the
highest performance with S1. Being sensitive to
sparse data, it does not perform equally well on S2
and S3 which have a higher number of categories,
some of which are low in frequency (see Table 2).
For S1, SVM finds all the four scheme categories
with the accuracy of 89%. F-measure is 90 for
OBJ, RES and CON and 81 for METH. For S2,
the classifier finds six of the seven categories, with
the accuracy of 90% and the average F-measure of
Table 5: F-Measure results using all the features and
all but one of the features
ALL A B C D E F G H I J K
S1 OBJ .90 .89 .87 .92 .90 .90 .91 .91 .91 .92 .91 .88
METH .80 .81 .80 .80 .79 .81 .79 .80 .80 .80 .81 .81
RES .88 .90 .88 .90 .88 .90 .88 .88 .88 .89 .89 .90
CON .86 .85 .82 .87 .88 .90 .90 .88 .89 .88 .88 .90
S2 BKG .91 .94 .90 .90 .93 .94 .94 .91 .93 .94 .92 .94
OBJ .72 .78 .84 .78 .83 .88 .84 .81 .83 .84 .78 .83
METH .81 .83 .80 .81 .80 .85 .80 .78 .81 .81 .82 .83
RES .88 .90 .88 .89 .88 .91 .89 .89 .90 .90 .90 .89
CON .84 .83 .77 .83 .86 .88 .86 .87 .88 .89 .88 .81
REL - - - - - - - - - - - -
FUT - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
S3 HYP - - - - - - - - - - - -
MOT .82 .84 .80 .76 .82 .82 .83 .78 .83 .83 .83 .83
BKG .59 .60 .60 .54 .67 .62 .62 .59 .61 .61 .62 .61
GOAL .62 .67 .67 .62 .71 .62 .67 .43 .67 .67 .67 .62
OBJT .88 .85 .83 .74 .83 .85 .83 .74 .83 .83 .83 .85
EXP .72 .68 .72 .53 .65 .70 .72 .73 .74 .74 .72 .68
MOD - - - - - - - - - - - -
METH .87 .86 .87 .66 .85 .89 .87 .88 .86 .86 .87 .86
OBS .82 .81 .84 .72 .80 .82 .81 .80 .82 .82 .81 .81
RES .87 .87 .88 .74 .87 .86 .87 .86 .87 .87 .87 .88
CON .88 .88 .82 .88 .83 .87 .87 .84 .87 .88 .87 .86
A-K: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR, Subj,
Obj, Voice
We have 1.0 for FUT in S2 probably because the size of the training data is
just right, and the model doesn?t overfit the data. We make this assumption
because we have 1.0 for almost all the categories in the training data, but only
for FUT on the test data.
Table 6: Baseline and best NB and SVM results
Acc. F-Measure
S1 OBJ METH RES CON
BL .29 .23 .23 .39 .18
NB .82 .85 .75 .85 .71
SVM .89 .90 .81 .90 .90
Acc. F-Measure
S2 BKG OBJ METH RES CON REL FUT
BL .25 .13 .08 .22 .40 .13 - -
NB .76 .79 .25 .70 .83 .66 - -
SVM .90 .94 .88 .85 .91 .88 - 1.0
Acc. F-Measure
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
BL .15 - .10 .06 .04 .06 .11 - .13 .24 .15 .17
NB .53 - .56 - - - .30 - .32 .61 .59 .62
SVM .81 - .82 .62 .62 .85 .70 - .89 .82 .86 .87
91 for the six categories. As with S2, METH has
the lowest performance (at 85 F-measure). The
one missing category (REL) appears in our abstract
data with very low frequency (see Table 2).
For S3, SVM uncovers as many as nine of the
11 categories with accuracy of 81%. Six cate-
gories perform well, with F-measure higher than
80. EXP, BKG and GOAL have F-measure of 70,
62 and 62, respectively. Like the missing cate-
gories HYP and MOD, GOAL is very low in fre-
quency. The lower performance of the higher fre-
quency EXP and BKG is probably due to low pre-
cision in distinguishing between EXP and METH,
and BKG and other categories, respectively.
105
8 Discussion and conclusions
The results from our corpus annotation (see Ta-
ble 2) show that for the coarse-grained S1, all the
four categories appear frequently in biomedical
abstracts (this is not surprising because S1 was ac-
tually designed for abstracts). All of them can be
identified using machine learning. For S2 and S3,
the majority of categories appear in abstracts with
high enough frequency that we can conclude that
also these two schemes are applicable to abstracts.
For S2 we identified six categories using machine
learning, and for S3 as many as nine, indicating
that automatic identification of the schemes in ab-
stracts is realistic.
Our analysis in section 5 showed that there is
a subsumption relation between the categories of
the schemes. S2 and S3 provide finer-grained in-
formation about the information structure of ab-
stracts than S1, even with their 2-3 low frequency
(or missing) categories. They can be useful for
practical tasks requiring such information. For ex-
ample, considering S3, there may be tasks where
one needs to distinguish between EXP, MOD and
METH, between HYP, MOT and GOAL, or between
OBS and RES.
Ultimately, the optimal scheme will depend on
the level of detail required by the application at
hand. Therefore, in the future, we plan to conduct
task-based evaluation of the schemes in the con-
text of CRA and to evaluate the usefulness of S1-
S3 for tasks cancer risk assessors perform on ab-
stracts (Korhonen et al, 2009). Now that we have
annotated the CRA corpus for S1-S3 and have a
machine learning approach available, we are in an
excellent position to conduct this evaluation.
A key question for real-world tasks is the level
of machine learning performance required. We
plan to investigate this in the context of our task-
based evaluation. Although we employed fairly
standard text classification methodology in our ex-
periments, we obtained high performance for S1
and S2. Due to the higher number of categories
(and less training data for each of them), the over-
all performance was not equally impressive for S3
(although still quite high at 81% accuracy).
Hirohata et al (2008) have showed that the
amount of training data can have a big impact
on our task. They used c. 50,000 Medline ab-
stracts annotated (by the authors of the Medline
abstracts) as training data for S1. When using a
small set of standard text classification features
and Conditional Random Fields (CRF) (Lafferty
et al, 2001) for classification, they obtained 95.5%
per-sentence accuracy on 1000 abstracts. How-
ever, when only 1000 abstracts were used for train-
ing the accuracy was considerably worse; their re-
ported per-abstract accuracy dropped from 68.8%
to less than 50%. Although it would be difficult to
obtain similarly huge training data for S2 and S3,
this result suggests that one key to improved per-
formance is larger training data, and this is what
we plan to explore especially for S3.
In addition we plan to improve our method. We
showed that our schemes partly overlap and that
similar features and methods tend to perform the
best / worst for each of the schemes. It is therefore
unlikely that considerable scheme specific tuning
will be necessary. However, we plan to develop
our features further and to make better use of the
sequential nature of information structure. Cur-
rently this is only represented as the History fea-
ture, which provides a narrow window view to the
category of the previous sentence. Also we plan to
compare SVM against methods such as CRF and
Maximum Entropy which have proved successful
in recent related works (Hirohata et al, 2008; Mer-
ity et al, 2009). The resulting models will be eval-
uated both directly and in the context of CRA to
provide an indication of their practical usefulness
for real-world tasks.
Acknowledgments
The work reported in this paper was funded by the
Royal Society (UK), the Swedish Research Coun-
cil, FAS (Sweden), and JISC (UK) which is fund-
ing the SAPIENT Automation project. YG was
funded by the Cambridge International Scholar-
ship.
106
References
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&c and boxer. In
Proceedings of the ACL 2007 Demonstrations Ses-
sion, pages 33?36.
K. Hirohata, N. Okazaki, S. Ananiadou, and
M. Ishizuka. 2008. Identifying sections in scien-
tific abstracts using conditional random fields. In
Proc. of 3rd International Joint Conference on Nat-
ural Language Processing.
A. Korhonen, L. Sun, I. Silins, and U. Stenius. 2009.
The first step in the development of text mining tech-
nology for cancer risk assessment: Identifying and
organizing scientific evidence in risk assessment lit-
erature. BMC Bioinformatics, 10:303.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditionl random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. R. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33:159?174.
M. Liakata and L.N. Soldatova. 2008. Guide-
lines for the annotation of general scientific con-
cepts. Aberystwyth University, JISC Project Report
http://ie-repository.jisc.ac.uk/88/.
M. Liakata, Claire Q, and L.N. Soldatova. 2009. Se-
mantic annotation of papers: Interface & enrichment
tool (sapient). In Proceedings of BioNLP-09, pages
193?200, Boulder, Colorado.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batch-
elor. 2010. Corpora for the conceptualisation and
zoning of scientific papers. To appear in the 7th In-
ternational Conference on Language Resources and
Evaluation.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings
of BioNLP-06, pages 65?72, New York, USA.
J. Lin. 2009. Is searching full text more effective than
searching abstracts? BMC Bioinformatics, 10:46.
S. Merity, T. Murphy, and J. R. Curran. 2009. Ac-
curate argumentative zoning with maximum entropy
models. In Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital
Libraries, pages 19?26. Association for Computa-
tional Linguistics.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2005. Zone analysis in biology articles as a basis
for information extraction. International Journal of
Medical Informatics on Natural Language Process-
ing in Biomedicine and Its Applications.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A baseline
feature set for learning rhetorical zones using full ar-
ticles in the biomedical domain. Natural language
processing and text mining, 7:52?58.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007.
Using argumentation to extract key sentences from
biomedical abstracts. Int J Med Inform, 76:195?
200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur.
2008. Multi-dimensional classification of biomed-
ical text: Toward automated, practical provision of
high-utility text to diverse users. Bioinformatics,
18:2086?2093.
S. Siegel and N. J. Jr. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill, Berkeley, CA, 2nd edition.
L. Sun and A. Korhonen. 2009. Improving verb clus-
tering with automatically acquired selectional pref-
erence. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75:488?495.
S. Teufel and M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28:409?445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In Proc. of EMNLP.
I. H. Witten, 2008. Data mining: practical machine
learning tools and techniques with Java Implemen-
tations. http://www.cs.waikato.ac.nz/ml/weka/.
107
