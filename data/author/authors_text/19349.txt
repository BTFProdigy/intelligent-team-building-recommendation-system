Cross Language Text Categorization Using a Bilingual Lexicon
Ke Wu, Xiaolin Wang and Bao-Liang Lu?
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dong Chuan Rd., Shanghai 200240, China
{wuke,arthur general,bllu}@sjtu.edu.cn
Abstract
With the popularity of the Internet at a phe-
nomenal rate, an ever-increasing number of
documents in languages other than English
are available in the Internet. Cross lan-
guage text categorization has attracted more
and more attention for the organization of
these heterogeneous document collections.
In this paper, we focus on how to con-
duct effective cross language text catego-
rization. To this end, we propose a cross
language naive Bayes algorithm. The pre-
liminary experiments on collected document
collections show the effectiveness of the pro-
posed method and verify the feasibility of
achieving performance close to monolingual
text categorization, using a bilingual lexicon
alone. Also, our algorithm is more efficient
than our baselines.
1 Introduction
Due to the popularity of the Internet, an ever-
increasing number of documents in languages other
than English are available in the Internet. The or-
ganization of these heterogeneous document collec-
tions increases cost of human labor significantly. On
the one hand, experts who know different languages
are required to organize these collections. On the
other hand, maybe there exist a large amount of la-
belled documents in a language (e.g. English) which
are in the same class structure as the unlabelled doc-
uments in another language. As a result, how to ex-
?Corresponding author.
ploit the existing labelled documents in some lan-
guage (e.g. English) to classify the unlabelled doc-
uments other than the language in multilingual sce-
nario has attracted more and more attention (Bel et
al., 2003; Rigutini et al, 2005; Olsson et al, 2005;
Fortuna and Shawe-Taylor, 2005; Li and Shawe-
Taylor, 2006; Gliozzo and Strapparava, 2006). We
refer to this task as cross language text categoriza-
tion. It aims to extend the existing automated text
categorization system from one language to other
languages without additional intervention of human
experts. Formally, given two document collections
{De,Df} from two different languages e and f re-
spectively, we use the labelled document collection
De in the language e to deduce the labels of the doc-
ument collection Df in the language f via an algo-
rithm A and some external bilingual resources.
Typically, some external bilingual lexical re-
sources, such as machine translation system (MT),
large-scale parallel corpora and multilingual ontol-
ogy etc., are used to alleviate cross language text
categorization. However, it is hard to obtain them
for many language pairs. In this paper, we focus on
using a cheap bilingual resource, e.g. bilingual lexi-
con without any translation information, to conduct
cross language text categorization. To my knowl-
edge, there is little research on using a bilingual lex-
icon alone for cross language text categorization.
In this paper, we propose a novel approach for
cross language text categorization via a bilingual
lexicon alone. We call this approach as Cross Lan-
guage Naive Bayes Classifier (CLNBC). The pro-
posed approach consists of two main stages. The
first stage is to acquire a probabilistic bilingual lex-
165
icon. The second stage is to employ naive Bayes
method combined with Expectation Maximization
(EM) (Dempster et al, 1977) to conduct cross lan-
guage text categorization via the probabilistic bilin-
gual lexicon. For the first step, we propose two dif-
ferent methods. One is a naive and direct method,
that is, we convert a bilingual lexicon into a proba-
bilistic lexicon by simply assigning equal translation
probabilities to all translations of a word. Accord-
ingly, the approach in this case is named as CLNBC-
D. The other method is to employ an EM algorithm
to deduce the probabilistic lexicon. In this case, the
approach is called as CLNBC-EM. Our preliminary
experiments on our collected data have shown that
the proposed approach (CLNBC) significantly out-
performs the baselines in cross language case and is
close to the performance of monolingual text cate-
gorization.
The remainder of this paper is organized as fol-
lows. In Section 2, we introduce the naive Bayes
classifier briefly. In Section 3, we present our cross
language naive Bayes algorithm. In Section 4, eval-
uation over our proposed algorithm is performed.
Section 5 is conclusions and future work.
2 The Naive Bayes Classifier
The naive Bayes classifier is an effective known al-
gorithm for text categorization (Domingos and Paz-
zani, 1997). When it is used for text categorization
task, each document d ? D corresponds to an exam-
ple. The naive Bayes classifier estimates the prob-
ability of assigning a class c ? C to a document d
based on the following Bayes? theorem.
P (c|d) ? P (d|c)P (c) (1)
Then the naive Bayes classifier makes two as-
sumptions for text categorization. Firstly, each word
in a document occurs independently. Secondly, there
is no linear ordering of the word occurrences.
Therefore, the naive Bayes classifier can be fur-
ther formalized as follows:
P (c|d) ? P (c)
?
w?d
P (w|c) (2)
The estimates of P (c) and P (w|c) can be referred
to (McCallum and Nigam, 1998)
Some extensions to the naive Bayes classifier with
EM algorithm have been proposed for various text
categorization tasks. The naive Bayes classifier was
combined with EM algorithm to learn the class label
of the unlabelled documents by maximizing the like-
lihood of both labelled and unlabelled documents
(Nigam et al, 2000). In addition, the similar way
was adopted to handle the problem with the positive
samples alone (Liu et al, 2002). Recently, transfer
learning problem was tackled by applying EM algo-
rithm along with the naive Bayes classifier (Dai et
al., 2007). However, they all are monolingual text
categorization tasks. In this paper, we apply a simi-
lar method to cope with cross language text catego-
rization using bilingual lexicon alone.
3 Cross Language Naive Bayes Classifier
Algorithm
In this section, a novel cross language naive Bayes
classifier algorithm is presented. The algorithm con-
tains two main steps below. First, generate a prob-
abilistic bilingual lexicon; second, apply an EM-
based naive Bayes learning algorithm to deduce the
labels of documents in another language via the
probabilistic lexicon.
Table 1: Notations and explanations.
Notations Explanations
e Language of training set
f Language of test set
d Document
De Document collection in language e
Df Document collection in language f
Ve Vocabulary of language e
Vf Vocabulary of language f
L Bilingual lexicon
T ? Ve ? Vf Set of links in L
?? Set of words whose translation is ? in L
E ? Ve Set of words of language e in L
we ? E Word in E
F ? Vf Set of words of language f in L
wf ? F Word in F
|E| Number of distinct words in set E
|F | Number of distinct words in set F
N(we) Word frequency in De
N(wf , d) Word frequency in d in language f
De Data distribution in language e
166
For ease of description, we first define some nota-
tions in Table 1. In the next two sections, we detail
the mentioned-above two steps separately.
3.1 Generation of a probabilistic bilingual
lexicon
To fill the gap between different languages, there are
two different ways. One is to construct the multi-
lingual semantic space, and the other is to transform
documents in one language into ones in another lan-
guage. Since we concentrate on use of a bilingual
lexicon, we adopt the latter method. In this paper,
we focus on the probabilistic model instead of se-
lecting the best translation. That is, we need to cal-
culate the probability of the occurrence of word we
in language e given a document d in language f , i.e.
P (we|d). The estimation can be calculated as fol-
lows:
P (we|d) =
?
wf?d
P (we|wf , d)P (wf |d) (3)
Ignoring the context information in a document
d, the above probability can be approximately esti-
mated as follows:
P (we|d) '
?
wf?d
P (we|wf )P (wf |d) (4)
where P (wf |d) denotes the probability of occur-
rence of wf in d, which can be estimated by relative
frequency of wf in d.
In order to induce P (we|d), we have to know the
estimation of P (we|wf ). Typically, we can obtain a
probabilistic lexicon from a parallel corpus. In this
paper, we concentrate on using a bilingual lexicon
alone as our external bilingual resource. Therefore,
we propose two different methods for cross language
text categorization.
First, a naive and direct method is that we assume
a uniform distribution on a word?s distribution. For-
mally, P (we|wf ) = 1?wf , where (we, wf ) ? T ; oth-
erwise P (we|wf ) = 0.
Second, we can apply EM algorithm to deduce
the probabilistic bilingual lexicon via the bilingual
lexicon L and the training document collection at
hand. This idea is motivated by the work (Li and Li,
2002).
We can assume that each word we in language e
is independently generated by a finite mixture model
as follows:
P (we) =
?
wf?F
P (wf )P (we|wf ) (5)
Therefore we can use EM algorithm to estimate
the parameters of the model. Specifically speaking,
we can iterate the following two step for the purpose
above.
? E-step
P (wf |we) =
P (wf )P (we|wf )
?
w?F P (w)P (we|w)
(6)
? M-step
P (we|wf ) =
(N(we) + 1)P (wf |we)
?
w?E (N(w) + 1) P (wf |w)(7)
P (wf ) = ? ?
?
we?E
P (we)P (wf |we)
+ (1? ?) ? P ?(wf ) (8)
where 0 ? ? ? 1, and
P ?(wf ) =
?
d?Df N(wf , d) + 1
?
wf?F
?
d?Df N(wf , d) + |F |(9)
The detailed algorithm can be referred to Algorithm
1. Furthermore, the probability that each word in
language e occurs in a document d in language f ,
P (we|d), can be calculated according to Equation
(4).
3.2 EM-based Naive Bayes Algorithm for
Labelling Documents
In this sub-section, we present an EM-based semi-
supervised learning method for labelling documents
in different language from the language of train-
ing document collection. Its basic model is naive
Bayes model. This idea is motivated by the transfer
learning work (Dai et al, 2007). For simplicity of
description, we first formalize the problem. Given
the labelled document set De in the source language
and the unlabelled document set Df , the objective is
to find the maximum a posteriori hypothesis hMAP
167
Algorithm 1 EM-based Word Translation Probabil-
ity Algorithm
Input: Training document collectionD(l)e , bilingual
lexicon L and maximum times of iterations T
Output: Probabilistic bilingual lexicon P (we|wf )
1: Initialize P (0)(we|wf ) = 1|?wf | , where
(we, wf ) ? T ; otherwise P (0)(we|wf ) = 0
2: Initialize P (0)(wf ) = 1|F |
3: for t =1 to T do
4: Calculate P (t)(wf |we) based on
P (t?1)(we|wf ) and P (t?1)(wf ) accord-
ing to Equation (6)
5: Calculate P (t)(we|wf ) and P (t)(wf ) based
on P (t)(wf |we) according to Equation (7)
and Equation (8)
6: end for
7: return P (T )(we|wf )
from the hypothesis space H under the data distri-
bution of the language e, De, according to the fol-
lowing formula.
hMAP = arg max
h?H
PDe(h|De,Df ) (10)
Instead of trying to maximize PDe(h|De,Df ) in
Equation (10), we can work with `(h|De,Df ), that
is, log (PDe(h)P (De,Df |h)) . Then, using Equa-
tion (10), we can deduce the following equation.
`(h|De,Df ) ? log PDe(h)
+
?
d?De
log
?
c?C
PDe(d|c)PDe(c|h)
+
?
d?Df
log
?
c?C
PDe(d|c)PDe(c|h)
(11)
EM algorithm is applied to find a local maximum
of `(h|De,Df ) by iterating the following two steps:
? E-step:
PDe(c|d) ? PDe(c)PDe(d|c) (12)
? M-step:
PDe(c) =
?
k?{e,f}
PDe(Dk)PDe(c|Dk) (13)
PDe(we|c) =
?
k?{e,f}
PDe(Dk)PDe(we|c,Dk)
(14)
Algorithm 2 Cross Language Naive Bayes Algo-
rithm
Input: Labelled document collection De, unla-
belled document collection Df , a bilingual lexi-
con L from language e to language f and maxi-
mum times of iterations T .
Output: the class label of each document in Df
1: Generate a probabilistic bilingual lexicon;
2: Calculate P (we|d) according to Equation (4).
3: Initialize P (0)De (c|d) via the traditional naiveBayes model trained from the labelled collec-
tion D(l)e .
4: for t =1 to T do
5: for all c ? C do
6: Calculate P (t)De(c) based on P
(t?1)
De (c|d) ac-cording to Equation (13)
7: end for
8: for all we ? E do
9: Calculate P (t)De(we|c) based on P
(t?1)
De (c|d)and P (we|d) according to Equation (14)
10: end for
11: for all d ? Df do
12: Calculate P (t)De(c|d) based on P
(t)
De(c) and
P (t)De(we|c) according to Equation (12)
13: end for
14: end for
15: for all d ? Df do
16: c = arg max
c?C
P (T )De (c|d)
17: end for
For the ease of understanding, we directly put the
details of the algorithm in cross-language text cate-
gorization algorithmin which we ignore the detail of
the generation algorithm of a probabilistic lexicon.
In Equation (12), PDe(d|c) can be calculated by
PDe(d|c) =
?
{we|we??wf ?wf?d}
PDe(we|c)NDe (we,d)
(15)
where NDe(we, d) = |d|PDe(we|d).
168
In Equation (13), PDe(c|Dk) can be estimated as
follows:
PDe(c|Dk) =
?
d?Dk
PDe(c|d)PDe(d|Dk) (16)
In Equation (14), similar to section 2, we can es-
timate PDe(we|c,Dk) through Laplacian smoothing
as follows:
PDe(we|c,Dk) =
1 + NDe(we, c,Dk)
|Vk|+ NDe(c,Dk)
(17)
where
NDe(we, c,Dk) =
?
d?Dk
|d|PDe(we|d)PDe(c|d)
(18)
NDe(c,Dk) =
?
d?Dk
|d|PDe(c|d) (19)
In addition, in Equation (13) and (14), PDe(Dk)
can be actually viewed as the trade-off parame-
ter modulating the degree to which EM algorithm
weights the unlabelled documents translated from
the language f to the language e via a bilingual lex-
icon. In our experiments, we assume that the con-
straints are satisfied, i.e. PDe(De) + PDe(Df ) = 1
and PDe(d|Dk) = 1|Dk| .
4 Experiments
4.1 Data Preparation
We chose English and Chinese as our experimen-
tal languages, since we can easily setup our exper-
iments and they are rather different languages so
that we can easily extend our algorithm to other
language pairs. In addition, to evaluate the per-
formance of our algorithm, experiments were per-
formed over the collected data set. Standard evalu-
ation benchmark is not available and thus we devel-
oped a test data from the Internet, containing Chi-
nese Web pages and English Web pages. Specifi-
cally, we applied RSS reader1 to acquire the links
to the needed content and then downloaded the Web
pages. Although category information of the con-
tent can be obtained by RSS reader, we still used
three Chinese-English bilingual speakers to organize
these Web pages into the predefined categories. As
a result, the test data containing Chinese Web pages
1http://www.rssreader.com/
and English Web pages from various Web sites are
created. The data consists of news during Decem-
ber 2005. Also, 5462 English Web pages are from
18 different news Web sites and 6011 Chinese Web
pages are from 8 different news Web sites. Data dis-
tribution over categories is shown in Table 2. They
fall into five categories: Business, Education, Enter-
tainment, Science and Sports.
Some preprocessing steps are applied to Web
pages. First we extract the pure texts of all Web
pages, excluding anchor texts which introduce much
noise. Then for Chinese corpus, all Chinese charac-
ters with BIG5 encoding first were converted into
ones with GB2312 encoding, applied a Chinese seg-
menter tool2 by Zhibiao Wu from LDC to our Chi-
nese corpus and removed stop words and words
with one character and less than 4 occurrences; for
English corpus, we used the stop words list from
SMART system (Buckley, 1985) to eliminate com-
mon words. Finally, We randomly split both the En-
glish and Chinese document collection into 75% for
training and 25% for testing.
we compiled a large general-purpose English-
Chinese lexicon, which contains 276,889 translation
pairs, including 53,111 English entries and 38,517
Chinese entries. Actually we used a subset of the
lexicon including 20,754 English entries and 13,471
Chinese entries , which occur in our corpus.
Table 2: Distribution of documents over categories
Categories English Chinese
Sports 1797 2375
Business 951 1212
Science 843 1157
Education 546 692
Entertainment 1325 575
Total 5462 6011
4.2 Baseline Algorithms
To investigate the effectiveness of our algorithms
on cross-language text categorization, three baseline
methods are used for comparison. They are denoted
by ML, MT and LSI respectively.
ML (Monolingual). We conducted text catego-
rization by training and testing the text categoriza-
2http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
169
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 1: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
tion system on document collection in the same lan-
guage.
MT (Machine Translation). We used Systran
premium 5.0 to translate training data into the lan-
guage of test data, since the machine translation sys-
tem is one of the best machine translation systems.
Then use the translated data to learn a model for
classifying the test data.
LSI (Latent Semantic Indexing). We can use
the LSI or SVD technique to deduce language-
independent representations through a bilingual par-
allel corpus. In this paper, we use SVDS command
in MATLAB to acquire the eigenvectors with the
first K largest eigenvalues. We take K as 400 in our
experiments, where best performance is achieved.
In this paper, we use SVMs as the classifier of our
baselines, since SVMs has a solid theoretic founda-
tion based on structure risk minimization and thus
high generalization ability. The commonly used
one-vs-all framework is used for the multi-class
case. SVMs uses the SV M light software pack-
age(Joachims, 1998). In all experiments, the trade-
off parameter C is set to 1.
4.3 Results
In the experiments, all results are averaged on 5 runs.
Results are measured by accuracy, which is defined
as the ratio of the number of labelled correctly docu-
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 2: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
ments to the number of all documents. When inves-
tigating how different training data have effect on
performance, we randomly select the corresponding
number of training samples from the training set 5
times. The results are shown in Figure 1 and Fig-
ure 2. From the two figures, we can draw the fol-
lowing conclusions. First, CLNBC-EM has a stable
and good performance in almost all cases. Also, it
can achieve the best performance among cross lan-
guage methods. In addition, we notice that CLNBC-
D works surprisingly better than CLNBC-EM, when
there are enough test data and few training data. This
may be because the quality of the probabilistic bilin-
gual lexicon derived from CLNBC-EM method is
poor, since this bilingual lexicon is trained from in-
sufficient training data and thus may provide biased
translation probabilities.
To further investigate the effect of varying the
amount of test data, we randomly select the cor-
responding number of test samples from test set 5
times. The results are shown in Figure 3 and Fig-
ure 4, we can draw the following conclusions . First,
with the increasing test data, performance of our two
approaches is improved. Second, CLNBC-EM sta-
tistically significantly outperforms CLNBC-D.
From figures 1 through 4, we also notice that MT
and LSI always achieve some poor results. For MT,
170
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5
0.6
0.7
0.8
0.9
1
Ratio of test data
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 3: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
maybe it is due to the large difference of word usage
between original documents and the translated ones.
For example,   (Qi Shi) has two common trans-
lations, which are cavalier and knight. In sports do-
main, it often means a basketball team of National
Basketball Association (NBA) in U.S. and should
be translated into cavalier. However, the transla-
tion knight is provided by Systran translation system
we use in the experiment. In term of LSI method,
one possible reason is that the parallel corpus is too
limited. Another possible reason is that it is out-of-
domain compared with the domain of the used doc-
ument collections.
From Table 3, we can observe that our algorithm
is more efficient than three baselines. The spent time
are calculated on the machine, which has a 2.80GHz
Dual Pentium CPU.
5 Conclusions and Future Work
In this paper, we addressed the issue of how to con-
duct cross language text categorization using a bilin-
gual lexicon. To this end, we have developed a cross
language naive Bayes classifier, which contains two
main steps. In the first step, we deduce a proba-
bilistic bilingual lexicon. In the second step, we
adopt naive Bayes method combined with EM to
conduct cross language text categorization. We have
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Ratio of test data
Ac
cu
ra
cy
 
 
ML MT LSI CLNBC?D CLNBC?EM
Figure 4: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
proposed two different methods, namely CLNBC-D
and CLNBC-EM, for cross language text categoriza-
tion. The preliminary experiments on collected data
collections show the effectiveness of the proposed
two methods and verify the feasibility of achieving
performance near to monolingual text categorization
using a bilingual lexicon alone.
As further work, we will collect larger compara-
ble corpora to verify our algorithm. In addition, we
will investigate whether the algorithm can be scaled
to more fine-grained categories. Furthermore, we
will investigate how the coverage of bilingual lex-
icon have effect on performance of our algorithm.
Table 3: Comparison of average spent time by dif-
ferent methods, which are used to conduct cross-
language text categorization from English to Chi-
nese.
Methods Preparation Computation
CLNBC-D - ?1 Min
CLNBC-EM - ?2 Min
ML - ?10 Min
MT ?48 Hra ?14 Min
LSI ?90 Minb ?15 Min
aMachine Translation Cost
bSVD Decomposition Cost
171
Acknowledgements. The authors would like to
thank three anonymous reviewers for their valu-
able suggestions. This work was partially sup-
ported by the National Natural Science Founda-
tion of China under the grants NSFC 60375022 and
NSFC 60473040, and the Microsoft Laboratory for
Intelligent Computing and Intelligent Systems of
Shanghai Jiao Tong University.
References
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In ECDL,
pages 126?139.
Chris Buckley. 1985. Implementation of the SMART
information retrieval system. Technical report, Ithaca,
NY, USA.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classifiers for text
classification. In Proceedings of Twenty-Second AAAI
Conference on Artificial Intelligence (AAAI 2007),
pages 540?545, July.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple bayesian classifier under zero-
one loss. Machine Learning, 29(2-3):103?130.
Blaz? Fortuna and John Shawe-Taylor. 2005. The use
of machine translation tools for cross-lingual text min-
ing. In Learning With Multiple Views, Workshop at the
22nd International Conference on Machine Learning
(ICML).
Alfio Massimiliano Gliozzo and Carlo Strapparava.
2006. Exploiting comparable corpora and bilingual
dictionaries for cross-language text categorization. In
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics. The Association for
Computer Linguistics, July.
Thorsten Joachims. 1998. Making large-scale sup-
port vector machine learning practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 343?351.
Yaoyong Li and John Shawe-Taylor. 2006. Using KCCA
for Japanese-English cross-language information re-
trieval and document classification. Journal of Intel-
ligent Information Systems, 27(2):117?133.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.
2002. Partially supervised classification of text doc-
uments. In ICML ?02: Proceedings of the Nineteenth
International Conference on Machine Learning, pages
387?394, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classification.
In Proceedings of AAAI-98, Workshop on Learning for
Text Categorization.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 2000. Text classification from labeled
and unlabeled documents using EM. Machine Learn-
ing, 39(2/3):103?134.
J. Scott Olsson, Douglas W. Oard, and Jan Hajic?. 2005.
Cross-language text classification. In Proceedings of
the 28th Annual international ACM SIGIR Confer-
ence on Research and Development in information Re-
trieval, pages 645?646, New York, NY, August. ACM
Press.
Leonardo Rigutini, Marco Maggini, and Bing Liu. 2005.
An EM based training algorithm for cross-language
text categorization. In Proceedings of Web Intelligence
Conference (WI-2005), pages 529?535, Compie`gne,
France, September. IEEE Computer Society.
172
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1654?1664,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Refining Word Segmentation Using a Manually Aligned Corpus
for Statistical Machine Translation
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
National Institute of Information and Communications Technology
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
Abstract
Languages that have no explicit word de-
limiters often have to be segmented for sta-
tistical machine translation (SMT). This is
commonly performed by automated seg-
menters trained on manually annotated
corpora. However, the word segmentation
(WS) schemes of these annotated corpora
are handcrafted for general usage, and
may not be suitable for SMT. An analysis
was performed to test this hypothesis us-
ing a manually annotated word alignment
(WA) corpus for Chinese-English SMT.
An analysis revealed that 74.60% of the
sentences in the WA corpus if segmented
using an automated segmenter trained on
the Penn Chinese Treebank (CTB) will
contain conflicts with the gold WA an-
notations. We formulated an approach
based on word splitting with reference to
the annotated WA to alleviate these con-
flicts. Experimental results show that the
refined WS reduced word alignment error
rate by 6.82% and achieved the highest
BLEU improvement (0.63 on average) on
the Chinese-English open machine trans-
lation (OpenMT) corpora compared to re-
lated work.
1 Introduction
Word segmentation is a prerequisite for many
natural language processing (NLP) applications
on those languages that have no explicit space
between words, such as Arabic, Chinese and
Japanese. As the first processing step, WS affects
all successive steps, thus it has a large potential
impact on the final performance. For SMT, the
unsupervised WA, building translation models and
reordering models, and decoding are all based on
segmented words.
Automated word segmenters built through
supervised-learning methods, after decades of in-
tensive research, have emerged as effective so-
lutions to WS tasks and become widely used in
many NLP applications. For example, the Stan-
ford word segmenter (Xue et al., 2002)1 which is
based on conditional random field (CRF) is em-
ployed to prepare the official corpus for NTCIR-
9 Chinese-English patent translation task (Goto et
al., 2011).
However, one problem with applying these
supervised-learning word segmenters to SMT is
that the WS scheme of annotating the training cor-
pus may not be optimal for SMT. (Chang et al.,
2008) noticed that the words in CTB are often too
long for SMT. For example, a full Chinese per-
sonal name which consists of a family name and a
given name is always taken as a single word, but
its counterpart in English is usually two words.
Manually WA corpora are precious resources
for SMT research, but they used to be only avail-
able in small volumes due to the production cost.
For example, (Och and Ney, 2000) initially an-
notated 447 English-French sentence pairs, which
later became the test data set in ACL 2003 shared
task on word alignment (Mihalcea and Pedersen,
2003), and was used frequently thereafter (Liang
et al., 2006; DeNero and Klein, 2007; Haghighi et
al., 2009)
For Chinese and English, the shortage of man-
ually WA corpora has recently been relieved
by the linguistic data consortium (LDC) 2
GALE Chinese-English word alignment and tag-
ging training corpus (the GALE WA corpus)3.
The corpus is considerably large, containing 4,735
documents, 18,507 sentence pairs, 620,189 Chi-
nese tokens, 518,137 English words, and 421,763
1http://nlp.stanford.edu/software/
segmenter.shtml
2http://catalog.ldc.upenn.edu
3Catalog numbers: LDC2012T16, LDC2012T20,
LDC2012T24 and LDC2013T05.
1654
alignment annotations. The corpus carries no Chi-
nese WS annotation, and the WA annotation was
performed between Chinese characters and En-
glish words. The alignment identifies minimum
translation units and relations 4, referred as atomic
blocks and atomic edges, respectively, in this pa-
per. Figure 1 shows an example that contains six
atomic edges.
Visual inspection of the segmentation of an au-
tomatic segmenter with reference to a WA cor-
pus revealed a number of inconsistencies. For ex-
ample, consider the word ?bao fa? in Figure 1.
Empirically we observed that this word is seg-
mented as a single token by an automatic seg-
menter trained on the CTB, however, this segmen-
tation differs with the alignment in the WA cor-
pus, since its two components are aligned to two
different English words. Our hypothesis was that
the removal of these inconsistencies would benefit
machine translation performance (this is explained
further in Section 2.3), and we explored this idea
in this work.
This paper focuses on optimizing Chinese WS
for Chinese-English SMT, but both the research
method and the proposed solution are language-
independent. They can be applied to other lan-
guage pairs.
The major contributions of this paper include,
? analyze the CTB WS scheme for Chinese-
English SMT;
? propose a lexical word splitter to refine the
WS;
? achieve a BLEU improvement over a baseline
Stanford word segmenter, and a state-of-the-
art extension, on Chinese-English OpenMT
corpora.
The rest of this paper is organized as follows:
first, Section 2 analyzes WS using a WA corpus;
next, Section 3 proposes a lexical word splitter
to refine WS; then, Section 4 evaluates the pro-
posed method on end-to-end SMT as well as word
segmentation and alignment; after that, Section 5
compares this work to related work; finally, Sec-
tion 6 concludes this paper.
4Guidelines for Chinese-English Word Align-
ment(Version 4.0)
2 Analysis of a General-purpose
Automatic Word Segmenter
This section first briefly describes the GALE WA
corpus, then presents an analysis of the WS arising
from a CTB-standard word segmenter with refer-
ence to the segmentation of the atomic blocks in
the GALE WA corpus, finally the impact of the
findings on SMT is discussed.
2.1 GALE WA corpus
The GALE WA corpus was developed by the
LDC, and was used as training data in the DARPA
GALE global autonomous language exploitation
program 5. The corpus incorporates linguistic
knowledge into word aligned text to help improve
automatic WA and translation quality. It em-
ploys two annotation schemes: alignment and tag-
ging (Li et al., 2010). Alignment identifies min-
imum translation units and translation relations;
tagging adds contextual, syntactic and language-
specific features to the alignment annotation. For
example, the sample shown in Figure 1 carries tags
on both alignment edges and tokens.
The GALE WA corpus contains 18,057 man-
ually word aligned Chinese and English parallel
sentences which are extracted from newswire and
web blogs. Table 1 presents the statistics on the
corpus. One third of the sentences are approxi-
mately newswire text, and the remainder consists
of web blogs.
2.2 Analysis of WS
In order to produce a Chinese word segmenta-
tion consistent with the CTB standard we used the
Stanford Chinese word segmenter with a model
trained on the CTB corpus. We will refer to this
as the ?CTB segmenter? in the rest of this paper.
The Chinese sentences in the GALE WA cor-
pus were first segmented by the CTB segmenter,
and the predicted words were compared against
the atomic blocks with respect to the granularity of
segmentation. The analysis falls into the following
three categories, two of which may be potentially
harmful to SMT:
? Fully consistent: the word locates within the
block of one atomic alignment edge. For ex-
ample, in Figure 2(a), the Chinese text has
5https://catalog.ldc.upenn.edu/
LDC2012T16
1655
 	
 
 
        	 
  
     
	
 	
 
       	 
	  
Figure 1: Example from the GALE WA corpus. Each line arrow represents an atomic edge, and each box
represents an atomic block. SEM (semantic), GIS (grammatically inferred semantic) and FUN (function)
are tags of edges. INC (not translated), TOI (to-infinitive) and DET (determiner) are tags of tokens.
Genre # Files # Sentences? # CN tokens # EN tokens # Alignment edges
Newswire 2,175 6,218 246,371 205,281 164,033
Web blog 2,560 11,839 373,818 312,856 257,730
Total 4,735 18,057 620,189 518,137 421,763
Table 1: GALE WA corpus. ? Sentences rejected by the annotators are excluded.
four atomic blocks; the CTB segmenter pro-
duces five words which all locate within the
blocks, so they are all small enough.
? Alignment inconsistent: the word aligns to
more than one atomic block, but the target
expression is contiguous, allowing for cor-
rect phrase pair extraction (Zens et al., 2002).
For example, in Figure 2(b), the characters in
the word ?shuang fang?, which is produced
by the CTB segmenter, contains two atomic
blocks, but the span of the target ?to both
side? is continuous, therefore the phrase pair
?shuang fang ||| to both sides? can be ex-
tracted.
? Alignment inconsistent and extraction hin-
dered: the word aligned to more than one
atomic block, and the target expression is not
contiguous, which hinders correct phrase pair
extractions. For example, in Figure 2(c), the
word ?zeng chan? has to be split in order to
match the target language.
Table 2 shows the statistics of the three cat-
egories of CTB WS on the GALE WA corpus.
90.74% of the words are fully consistent, while the
remaining 9.26% of the words have inconsistent
alignments. 74.60% of the sentences contain this
problem. The category with inconsistent align-
ment and extraction hindered only accounts for
0.46% of the words, affecting 9.06% of the sen-
tences.
2.3 Impact of WS on SMT
The word alignment has a direct impact on the na-
ture of both the translation model, and lexical re-
ordering model in a phrase-base SMT system. The
words in last two categories are all longer than an
atomic block, which might lead to problems in the
word alignment in two ways:
? First, longer words tend to be more sparse in
the training corpus, thus the estimated distri-
bution of their target phrases are less accu-
rate.
? Second, the alignment from them to target
sides are one-to-many, which is much more
complicated and requires fertilized alignment
models such as IBM model 4 ? 6 (Och and
Ney, 2000).
The words in the category of ?fully consistent?
can be aligned using simple models, because the
alignment from them to the target side are one-to-
one or many-to-one, and simple alignment models
such as IBM model 1, IBM model 2 and HMM
model are sufficient (Och and Ney, 2000).
3 Refining the Word Segmentation
In the last subsection, it was shown that 74.60% of
parallel sentences were affected by issues related
to under-segmentation of the corpus. Our hypoth-
esis is that if these words are split into pieces that
match English words, the accuracy of the unsuper-
vised WA as well as the translation quality will be
improved. To achieve this, we adopt a splitting
1656
	
						
   	
 	  
(a)
	



	






    

  	 	    
(b)
	
 
 	 	
	
     	

	 	 	  
(c)
Figure 2: Examples of automated WS on manually WA corpus: (a) Fully consistent; (b) Alignment
inconsistent; (c) Alignment inconsistent and extraction hindered. The Chinese words separated by white
space are the output of the CTB segmenter. Arrows represent the alignment of atomic blocks. Note that
?shuang fang? and ?zeng chan? are words produced by the CTB segmenter, but consist of two atomic
blocks.
Category Count Word Ratio Sentence Ratio
Fully consistent 355,702 90.74% 25.40%?
Alignment inconsistent 34,464 8.81% 65.54%
Alignment inconsistent & extraction hindered 1,830 0.46% 9.06%
Sum of conflict ? 36,294 9.26% 74.60%
Table 2: CTB WS on GALE WA corpus: ? All words are fully consistent; ? Alignment inconsistent plus
alignment inconsistent & extraction hindered
strategy, based on a supervised learning approach,
to re-segment the corpus. This subsection first for-
malizes the task, and then presents the approach.
3.1 Word splitting task
The word splitting task is formalized as a sequence
labeling task as follows: each word (represented
by a sequence of characters x = x
1
. . . x
T
where
T is the length of sample) produced by the CTB
segmenter is a sample, and a corresponding se-
quence of binary boundary labels y = y
1
. . . y
T
is the learning target,
y
t
=
?
?
?
1 if there is a split point
between c
t
and c
t?1
;
0 otherwise.
(1)
The sequence of boundary labels is derived
from the gold WA annotation as follows: for a
sequence of two atomic blocks, where the first
character of the second block is x
t
, then the la-



	



	




 	
 
Figure 3: Samples of word splitting task
bel y
t
= 1. Figure 3 presents several samples ex-
tracted from the examples in Figure 2.
Each word sample may have no split point, one
split point or multiple split points, depending on
the gold WA annotation. Table 3 shows the statis-
tics of the word splitting data set which is built
from the GALE manual WA corpus and the CTB
segmenter?s output, where 2000 randomly sam-
pled sentences are taken as a held-out test set.
1657
Set # Sentences # Samples # Split points # Split points per sample
Train. 16,057 348,086 32,337 0.0929
Test 2,000 43,910 3,929 0.0895
Table 3: Data set for learning the word splitting
3.2 CRF approach
This paper employs a condition random field
(CRF) to solve this sequence labeling task (Laf-
ferty et al., 2001). A linear-chain CRF defines the
conditional probability of y given x as,
P
?
(y|x) =
1
Z
x
(
T
?
t=1
?
k
?
k
f
k
(y
t?1
, y
t
,x, t)),
(2)
where ? = {?
1
, . . .} are parameters, Z
x
is a per-
input normalization that makes the probability of
all state sequences sum to one; f
k
(y
t?1
, y
t
,x, t) is
a feature function which is often a binary-valued
sparse feature. The training of CRF model is to
maximize the likelihood of training data together
with a regularization penalty to avoid over-fitting
as (Peng et al., 2004; Peng and McCallum, 2006),
?
?
= argmax
?
(
?
i
logP
?
(y
i
|x
i
) ?
?
k
?
2
k
2?
2
k
),
(3)
where (x,y) are training samples; the hyperparam-
eter ?
k
can be understood as the variance of the
prior distribution of ?
k
. When predicting the la-
bels of test samples, the CRF decoder searches for
the optimal label sequence y? that maximizes the
conditional probability,
y
?
= argmax
y
P
?
(y|x). (4)
In (Chang et al., 2008) a method is proposed to
select an appropriate level of segmentation gran-
ularity (in practical terms, to encourage smaller
segments). We call their method ?length tuner?.
The following artificial feature is introduced into
the learned CRF model:
f
0
(x, y
t?1
, y
t
, 1) =
{
1 if y
t
= +1
0 otherwise
(5)
The weight ?
0
of this feature is set by hand to
bias the output of CRF model. By way of expla-
nation, a very large positive ?
0
will cause every
character to be segmented, or conversely a very
large negative ?
0
will inhibit the output of segmen-
tation boundaries. In their experiments, ?
0
= 2
was used to force a CRF segmenter to adopt an in-
termediate granularity between character and the
CTB WS scheme. Compared to the length tuner,
our proposed method exploits lexical knowledge
about word splitting, and we will therefore refer to
it as the ?lexical word splitter? or ?lexical splitter?
for short.
3.3 Feature Set
The features f
k
(y
t?1
, y
t
,x, t) we used include the
WS features from the Chinese Stanford word seg-
menter and a set of extended features described
below. The WS features are included because the
target split points may share some common char-
acteristics with the boundaries in the CTB WS
scheme.
The extended features consists of four types ?
named entities, word frequency, word length and
character-level unsupervised WA. For each type of
the feature, the value and value concatenated with
previous or current character are taken as sparse
features (see Table 4 for details). The real val-
ues of word frequency, word length and character-
level unsupervised WA are converted into sparse
features due to the routine of CRF model.
The character-level unsupervised alignment
feature is inspired by the related works of unsu-
pervised bilingual WS (Xu et al., 2008; Chung and
Gildea, 2009; Nguyen et al., 2010; Michael et al.,
2011). The idea is that the character-level WA can
approximately capture the counterpart English ex-
pression of each Chinese token, and source tokens
aligned to different target expressions should be
split into different words (see Figure 4 for an illus-
tration).
The values of the character-level alignment fea-
tures are obtained through building a dictionary.
First, unsupervised WA is performed on the SMT
training corpus where the Chinese sentences are
treated as sequences of characters; then, the Chi-
nese sentences are segmented by CTB segmenter
and a dictionary of segmented words are built; fi-
nally, for each word in the dictionary, the relative
frequency of being split at a certain position is cal-
1658
Feature Definition Example
NE NE tag of current word Geography:NE
NE-C
?1
NE concatenated with previous character Geo.-ding:NE-C
?1
NE-C
0
NE concatenated with current character Geo.-mei:NE-C
0
Frequency Nearest integer of negative logarithm of word frequency 5?:Freq
Freq.-C
?1
Frequency concatenated with previous character 5-ding:Freq-C
?1
Freq.-C
0
Frequency concatenated with current character 5-mei:Freq-C
0
Length Length of current word (1,2,3,4,5,6,7 or >7) 4:Len
Len.-Position Length concatenated with the position 4-2:Len-Pos
Len.-C
?1
Length concatenated with previous character 4-ding:Len-C
?1
Len.-C
0
Length concatenated with current character 4-mei:Len-C
0
Char. Align. Five-level relative frequency of being split 0.4?:CA
C.A.-C
?1
C.A. concatenated with previous character 0.4-ding:CA-C
?1
C.A.-C
0
C.A. concatenated with current character 0.4-mei:CA-C
0
Table 4: Extended features used in the CRF model for word splitting. The example shows the features
used in the decision whether to split the Chinese word ?la ding mei zhou? (Latin America, the first
four Chinese characters in Figure 4) after the second Chinese character. ? Round(-log
10
(0.00019)); ?
Round(0.43 ? 5 ) / 5
	

	
        	 

	      
Figure 4: Illustration of character-level unsuper-
vised alignment features. The dotted lines are
word boundaries suggested by the alignment.
culated as,
f
CA
(w, i) =
n
i
n
w
(6)
where w is a word, i is a splitting position (from
1 to the length of w minus 1); n
i
is the number of
times the words as split at position i according to
the character-level alignment, that is, the character
before and after i are aligned to different English
expressions; n
w
is occurrence count of word w in
the training corpus.
4 Experiments
In the last section we found that 9.26% of words
produced by the CTB segmenter have the poten-
tial to cause problems for SMT, and propose a
lexical word splitter to address this issue through
segmentation refinement. This section contains
experiments designed to empirically evaluate the
proposed lexical word splitter in three aspects:
first, whether the WS accuracy is improved; sec-
ond, whether the accuracy of the unsupervised WA
during training SMT systems is improved; third,
whether the end-to-end translation quality is im-
proved.
This section first describes the experimental
methodology, then presents the experimental re-
sults, and finally illustrates the operation of our
proposed method using a real example.
4.1 Experimental Methodology
4.1.1 Experimental Corpora
The GALE manual WA corpus and the Chinese to
English corpus from the shared task of the NIST
open machine translation (OpenMT) 2006 evalua-
tion 6 were employed as the experimental corpus
(Table 5).
The experimental corpus for WS was con-
structed by first segmenting 2000 held out sen-
tences from the GALE manual WA corpus with
the Stanford segmenter, and then refining the seg-
mentation with the gold alignment annotation. For
example, the gold segmentation for the examples
in Figure 2 is presented in Figure 5. Note that
this test corpus is intended to represent an oracle
segmentation for our proposed method, and serves
primarily to gauge the improvement of our method
over the baseline Stanford segmenter, relative to
an upper bound.
6http://www.itl.nist.gov/iad/mig/
tests/mt/2006/
1659
    	

     
   
 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752?758,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Empirical Study of Unsupervised Chinese Word Segmentation Methods
for SMT on Large-scale Corpora
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
National Institute of Information and Communications Technology
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
Abstract
Unsupervised word segmentation (UWS)
can provide domain-adaptive segmenta-
tion for statistical machine translation
(SMT) without annotated data, and bilin-
gual UWS can even optimize segmenta-
tion for alignment. Monolingual UWS ap-
proaches of explicitly modeling the proba-
bilities of words through Dirichlet process
(DP) models or Pitman-Yor process (PYP)
models have achieved high accuracy, but
their bilingual counterparts have only been
carried out on small corpora such as ba-
sic travel expression corpus (BTEC) due to
the computational complexity. This paper
proposes an efficient unified PYP-based
monolingual and bilingual UWS method.
Experimental results show that the pro-
posed method is comparable to super-
vised segmenters on the in-domain NIST
OpenMT corpus, and yields a 0.96 BLEU
relative increase on NTCIR PatentMT cor-
pus which is out-of-domain.
1 Introduction
Many languages, especially Asian languages such
as Chinese, Japanese and Myanmar, have no ex-
plicit word boundaries, thus word segmentation
(WS), that is, segmenting the continuous texts of
these languages into isolated words, is a prerequi-
site for many natural language processing applica-
tions including SMT.
Though supervised-learning approaches which
involve training segmenters on manually seg-
mented corpora are widely used (Chang et al,
2008), yet the criteria for manually annotat-
ing words are arbitrary, and the available anno-
tated corpora are limited in both quantity and
genre variety. For example, in machine transla-
tion, there are various parallel corpora such as
BTEC for tourism-related dialogues (Paul, 2008)
and PatentMT in the patent domain (Goto et
al., 2011)1, but researchers working on Chinese-
related tasks often use the Stanford Chinese seg-
menter (Tseng et al, 2005) which is trained on a
small amount of annotated news text.
In contrast, UWS, spurred by the findings that
infants are able to use statistical cues to determine
word boundaries (Saffran et al, 1996), relies on
statistical criteria instead of manually crafted stan-
dards. UWS learns from unsegmented raw text,
which are available in large quantities, and thus
it has the potential to provide more accurate and
adaptive segmentation than supervised approaches
with less development effort being required.
The approaches of explicitly modeling the
probability of words(Brent, 1999; Venkataraman,
2001; Goldwater et al, 2006; Goldwater et al,
2009; Mochihashi et al, 2009) significantly out-
performed a heuristic approach (Zhao and Kit,
2008) on the monolingual Chinese SIGHAN-MSR
corpus (Emerson, 2005), which inspired the work
of this paper.
However, bilingual approaches that model word
probabilities suffer from computational complex-
ity. Xu et al (2008) proposed a bilingual method
by adding alignment into the generative model, but
was only able to test it on small-scale BTEC data.
Nguyen et al (2010) used the local best alignment
to increase the speed of the Gibbs sampling in
training but the impact on accuracy was not ex-
plored.
This paper is dedicated to bilingual UWS on
large-scale corpora to support SMT. To this end,
we model bilingual UWS under a similar frame-
work with monolingual UWS in order to improve
efficiency, and replace Gibbs sampling with ex-
pectation maximization (EM) in training.
We aware that variational bayes (VB) may be
used for speeding up the training of DP-based
1http://ntcir.nii.ac.jp/PatentMT
752
or PYP-based bilingual UWS. However, VB re-
quires formulating the m expectations of (m?1)-
dimensional marginal distributions, where m is
the number of hidden variables. For UWS, the
hidden variables are indicators that identify sub-
strings of sentences in the corpus as words. These
variables are large in number and it is not clear
how to apply VB to UWS, and as far the authors
aware there is no previous work related to the ap-
plication of VB to monolingual UWS. Therefore,
we have not explored VB methods in this paper,
but we do show that our method is superior to the
existing methods.
The contributions of this paper include,
? state-of-the-art accuracy in monolingual
UWS;
? the first bilingual UWS method practical for
large corpora;
? improvement of BLEU scores compared
to supervised Stanford Chinese word seg-
menter.
2 Methods
This section describes our unified monolingual
and bilingual UWS scheme. Table 1 lists the main
notation. The set F is chosen to represent an un-
segmented foreign language sentence (a sequence
of characters), because an unsegmented sentence
can be seen as the set of all possible segmentations
of the sentence denoted F , i.e. F ? F .
Notation Meaning
F an unsegmented foreign sentence
F
k
?
k
unsegmented substring of the un-
derlying string of F from k to k?
F a segmented foreign sentence
f
j
the j-th foreign word
M monolingual segmentation model
P
M
(x) probability of x being a word ac-
cording to M
E a tokenized English sentence
e
i
the i-th English word
(F ,E) a bilingual sentence pair
B bilingual segmentation model
P
B
(x|e
i
) probability of x being a word ac-
cording to B given e
i
Table 1: Main Notation.
Monolingual and bilingual WS can be formu-
lated as follows, respectively,
?
F (F) = argmax
F?F
P (F |F ,M), (1)
?
F (F , E) = argmax
F?F
?
a
P (F, a|F , E,B), (2)
where a is an alignment between F and E. The
English sentence E is used in the generation of a
segmented sentence F .
UWS learns models by maximizing the likeli-
hood of the unsegmented corpus, formulated as,
?
M = argmax
M
?
F?F
(
?
F?F
P (F |M)
)
, (3)
?
B = argmax
B
?
(F ,E)?B
(
?
F?F
?
a
P (F, a|F , E,B)
)
.
(4)
Our method of learning M and B proceeds in a
similar manner to the EM algorithm. The follow-
ing two operations are performed iteratively for
each sentence (pair).
? Exclude the previous expected counts of the
current sentence (pair) from the model, and
then derive the current sentence in all pos-
sible ways, calculating the new expected
counts for the words (see Section 2.1), that
is, we calculate the expected probabilities of
the Fk?
k
being words given the data excluding
F , i.e. E
F/{F}
(P (F
k
?
k
|F)) = P (F
k
?
k
|F ,M)
in a similar manner to the marginalization in
the Gibbs sampling process which we are re-
placing;
? Update the respective model M or B accord-
ing to these expectations (see Section2.2).
2.1 Expectation
2.1.1 Monolingual Expectation
P (F
k
?
k
|F ,M) is the marginal probability of all
the possible F ? F that contain Fk?
k
as a word,
which can be calculated efficiently through dy-
namic programming (the process is similar to the
foreward-backward algorithm in training a hidden
Markov model (HMM) (Rabiner, 1989)):
P
a
(k) =
U
?
u=1
P
a
(k ? u)P
M
(F
k
k?u
)
P
b
(k
?
) =
U
?
u=1
P
b
(k
?
+ u)P
M
(F
k
?
+u
k
?
)
P (F
k
?
k
|F ,M) = P
a
(k)P
M
(F
k
?
k
)P
b
(k
?
), (5)
753
where U is the predefined maximum length of for-
eign language words, P
a
(k) and P
b
(k
?
) are the
forward and backward probabilities, respectively.
This section uses a unigram model for description
convenience, but the method can be extended to
n-gram models.
2.1.2 Bilingual Expectation
P (F
k
?
k
|F , E,B) is the marginal probability of all
the possible F ? F that contain Fk?
k
as a word and
are aligned with E, formulated as:
P (F
k
?
k
|F , E,B) =
?
F?F
F
k
?
k
?F
?
a
P (F, a|E,B)
?
?
F?F
F
j
k
=F
k
?
k
?
a
J
?
j=1
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
)
=
?
F?F
f
j
k
=F
k
?
k
J
?
j=1
?
a
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
),
(6)
where J and I are the number of foreign and En-
glish words, respectively, and a
j
is the position of
the English word that is aligned to f
j
in the align-
ment a. For the alignment we employ an approx-
imation to IBM model 2 (Brown et al, 1993; Och
and Ney, 2003) described below.
We define the conditional probability of f
j
given the corresponding English sentence E and
the model B as:
P
B
(f
j
|E) =
?
a
P (a
j
|j, I, J)P
B
(f
j
|e
a
j
) (7)
Then, the previous dynamic programming
method can be extended to the bilingual expecta-
tion
P
a
(k|E) =
U
?
u=1
P
a
(k ? u|E)P
B
(F
k
k?u
|E)
P
b
(k
?
|E) =
U
?
u=1
P
b
(k
?
+ u|E)P
B
(F
k
?
+u
k
?
|E)
P (F
k
?
k
|F , E,B) = P
a
(k|E)P
B
(F
k
?
k
|E)P
b
(k
?
|E).
(8)
Eq. 7 can be rewritten (as in IBM model 2):
P
B
(f
j
|E) =
I
?
i=1
P
?
(i|j, I, J)P
B
(f
j
|e
i
) (9)
P
?
(i|j, I, J) =
?
a:a
j
=i
P (a
j
|, j, I, J)
In order to maintain both speed and accuracy, the
following window function is adopted
P
?
(i|j, I, J) ? P
?
(i|k, I,K) =
?
?
?
e
?|i?kI/K|
/? |i? kI/K| 6 ?
b
/2
?
?
e
i
is empty word
0 otherwise
(10)
where K is the number of characters in F , and
the k-th character is the start of the word f
j
, since
j and J are unknown during the computation of
dynamic programming. ?
b
is the window size, ?
?
is the prior probability of an empty English word,
and ? ensures all the items sum to 1.
2.2 Maximization
Inspired by (Teh, 2006; Mochihashi et al, 2009;
Neubig et al, 2010; Teh and Jordan, 2010), we
employ a Pitman-Yor process model to build the
segmentation model M or B. The monolingual
model M is
P
M
(f
j
) =
max
(
n(f
j
)? d, 0
)
+ (? + d ? n
M
)G
0
(f
j
)
?
f
?
j
n(f
?
j
) + ?
n
M
=
?
?
{f
j
|n(f
j
) > d}
?
?
, (11)
where f
j
is a foreign language word, and n(f
j
) is
the observed counts of f
j
, ? is named the strength
parameter, G
0
(f
j
) is named the base distribution
of f
j
, and d is the discount.
The bilingual model is
P
B
(f
j
|e
i
) =
max
(
n(f
j
, e
i
)? d, 0
)
+ (? + d ? n
e
i
)G
0
(f
j
|e
i
)
?
f
?
j
n(f
?
j
, e
i
) + ?
n
e
i
=
?
?
{x |n(x, e
i
) > d}
?
?
. (12)
In Eqs. 11 and 12,
n(f
j
) =
?
F?F
P (f
j
|F ,M) (13)
n(f
j
, e
i
) =
?
(F ,E)?B
P (f
j
|F , E,B)
P
?
(i|j, I, J)P
B
(f
j
|e
i
)
?
I
i
?
=1
P
?
(i
?
|j, I, J)P
B
(f
j
|e
i
?
)
.
(14)
754
3 Complexity Analysis
The computational complexity of our method is
linear in the number of iterations, the size of the
corpus, and the complexity of calculating the ex-
pectations on each sentence or sentence pair. In
practical applications, the size of the corpus is
fixed, and we found empirically that the number
of iterations required by the proposed method for
convergence is usually small (less than five itera-
tions). We now look in more detail at the complex-
ity of the expectation calculation in monolingual
and bilingual models.
The monolingual expectation is calculated ac-
cording to Eq. 5; the complexity is linear in the
length of sentences and the square of the prede-
fined maximum length of words. Thus its overall
complexity is
O
unigram
monoling = O(Ni|F|KU
2
), (15)
where Ni is the number of iterations, K is the av-
erage number of characters per sentence, and U is
the predefined maximum length of words.
For the monolingual bigram model, the number
of states in the HMM is U times more than that
of the monolingual unigram model, as the states at
specific position of F are not only related to the
length of the current word, but also related to the
length of the word before it. Thus its complexity
is U2 times the unigram model?s complexity:
O
bigram
monoling = O(Ni|F|KU
4
). (16)
The bilingual expectation is given by Eq. 8,
whose complexity is the same as the monolingual
case. However, the complexity of calculating the
transition probability, in Eqs. 9 and 10, is O(?
b
).
Thus its overall complexity is:
O
unigram
biling = O(Ni|F|KU
2
?
b
). (17)
4 Experiments
In this section, the proposed method is first val-
idated on monolingual segmentation tasks, and
then evaluated in the context of SMT to study
whether the translation quality, measured by
BLEU, can be improved.
4.1 Experimental Settings
4.1.1 Experimental Corpora
Two monolingual corpora and two bilingual cor-
pora are used (Table 2). CHILDES (MacWhin-
ney and Snow, 1985) is the most common test
Corpus Type # Sentences # Characters
CHILDES Mono. 9,790 95,809
SIGHAN-MSR Mono. 90,903 4,234,824
OpenMT06 Biling. 437,004 19,692,605
PatentMT9 Biling. 1,004,000 63,130,757
Table 2: Experimental Corpora
corpus for UWS methods. The SIGHAN-MSR
corpus (Emerson, 2005) consists of manually seg-
mented simplified Chinese news text, released in
the SIGHAN bakeoff 2005 shared tasks.
The first bilingual corpus: OpenMT06 was used
in the NIST open machine translation 2006 Eval-
uation 2. We removed the United Nations cor-
pus and the traditional Chinese data sets from the
constraint training resources. The data sets of
NIST Eval 2002 to 2005 were used as the develop-
ment for MERT tuning (Och, 2003). This data set
mainly consists of news text 3. PatentMT9 is from
the shared task of NTCIR-9 patent machine trans-
lation . The training set consists of 1 million par-
allel sentences extracted from patent documents,
and the development set and test set both consist
of 2000 sentences.
4.1.2 Performance Measurement and
Baseline Methods
For the monolingual tasks, the F
1
score against
the gold annotation is adopted to measure the ac-
curacy. The results reported in related papers are
listed for comparison.
For the bilingual tasks, the publicly available
system of Moses (Koehn et al, 2007) with default
settings is employed to perform machine transla-
tion, and BLEU (Papineni et al, 2002) was used
to evaluate the quality. Character-based segmen-
tation, LDC segmenter and Stanford Chinese seg-
menters were used as the baseline methods.
4.1.3 Parameter settings
The parameters are tuned on held-out data sets.
The maximum length of foreign language words
is set to 4. For the PYP model, the base distri-
bution adopts the formula in (Chung and Gildea,
2009), and the strength parameter is set to 1.0, and
the discount is set to 1.0? 10?6.
For bilingual segmentation,the size of the align-
ment window is set to 6; the probability ?
?
of for-
eign language words being generated by an empty
2http://www.itl.nist.gov/iad/mig/
/tests/mt/2006/
3It also contains a small number of web blogs
755
Method Accuracy Time
CHILD. MSR CHILD. MSR
NPY(bigram)a 0.750 0.802 17 m ?
NPY(trigram)a 0.757 0.807 ? ?
HDP(bigram)b 0.723 ? 10 h ?
Fitnessc ? 0.667 ? ?
Prop.(unigram) 0.729 0.804 3 s 50 s
Prop.(bigram) 0.774 0.806 15 s 2530 s
a by (Mochihashi et al,2009);
b by (Goldwater et al,2009);
c by (Zhao and Kit, 2008).
Table 3: Results on Monolingual Corpora.
English word, was set to 0.3.
The training was started from assuming that
there was no previous segmentations on each sen-
tence (pair), and the number of iterations was
fixed. It was set to 3 for the monolingual unigram
model, and 2 for the bilingual unigram model,
which provided slightly higher BLEU scores on
the development set than the other settings. The
monolingual bigram model, however, was slower
to converge, so we started it from the segmenta-
tions of the unigram model, and using 10 itera-
tions.
4.2 Monolingual Segmentation Results
In monolingual segmentation, the proposed meth-
ods with both unigram and bigram models were
tested. Experimental results show that they are
competitive to state-of-the-art baselines in both ac-
curacy and speed (Table 3). Note that the com-
parison of speed is only for reference because the
times are obtained from their respective papers.
4.3 Bilingual Segmentation Results
Table 4 presents the BLEU scores for Moses using
different segmentation methods. Each experiment
was performed three times. The proposed method
with monolingual bigram model performed poorly
on the Chinese monolingual segmentation task;
thus, it was not tested. We intended to test (Mochi-
hashi et al, 2009), but found it impracticable on
large-scale corpora.
The experimental results show that the proposed
UWS methods are comparable to the Stanford seg-
menters on the OpenMT06 corpus, while achieves
a 0.96 BLEU increase on the PatentMT9 corpus.
This is because this corpus is out-of-domain for
the supervised segmenters. The CTB and PKU
Stanford segmenter were both trained on anno-
tated news text, which was the major domain of
OpenMT06.
Method BLEU
OpenMT06 PatentMT9
Character 29.50 ? 0.03 28.36 ? 0.09
LDC 31.33 ? 0.10 30.22 ? 0.14
Stanford(CTB) 31.68 ? 0.25 30.77 ? 0.13
Stanford(PKU) 31.54 ? 0.13 30.86 ? 0.04
Prop.(mono.) 31.47 ? 0.18 31.62 ? 0.06
Prop.(biling.) 31.61 ? 0.14 31.73 ? 0.05
Table 4: Results on Bilingual Corpora.
Method Time
OpenMT06 PatentMT9
Prop.(mono.) 28 m 1 h 01 m
Prop.(biling.) 2 h 25 m 5 h 02 m
Table 5: Time Costs on Bilingual Corpora.
Table 5 presents the run times of the proposed
methods on the bilingual corpora. The program
is single threaded and implemented in C++. The
time cost of the bilingual models is about 5 times
that of the monolingual model, which is consistent
with the complexity analysis in Section 3.
5 Conclusion
This paper is devoted to large-scale Chinese UWS
for SMT. An efficient unified monolingual and
bilingual UWS method is proposed and applied to
large-scale bilingual corpora.
Complexity analysis shows that our method is
capable of scaling to large-scale corpora. This was
verified by experiments on a corpus of 1-million
sentence pairs on which traditional MCMC ap-
proaches would struggle (Xu et al, 2008).
The proposed method does not require any
annotated data, but the SMT system with it
can achieve comparable performance compared
to state-of-the-art supervised word segmenters
trained on precious annotated data. Moreover,
the proposed method yields 0.96 BLEU improve-
ment relative to supervised word segmenters on
an out-of-domain corpus. Thus, we believe that
the proposed method would benefit SMT related to
low-resource languages where annotated data are
scare, and would also find application in domains
that differ too greatly from the domains on which
supervised word segmenters were trained.
In future research, we plan to improve the bilin-
gual UWS through applying VB and integrating
more accurate alignment models such as HMM
models and IBM model 4.
756
References
Michael R Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1-3):71?105.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational linguistics, 19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the 3rd Workshop on Statistical Machine
Translation, pages 224?232. Association for Com-
putational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 718?726. Association for Com-
putational Linguistics.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the 4th SIGHAN Workshop on Chinese Language
Processing, volume 133.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsu-
pervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 673?
680. Association for Computational Linguistics.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A Bayesian framework for word seg-
mentation: exploring the effects of context. Cogni-
tion, 112(1):21?54.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of NTCIR, volume 9, pages 559?578.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Journal of
child language, 12(2):271?296.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100?108.
Association for Computational Linguistics.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language
model from continuous speech. In InterSpeech,
pages 1053?1056.
ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.
2010. Nonparametric word segmentation for ma-
chine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 815?823. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318. Association
for Computational Linguistics.
Michael Paul. 2008. Overview of the IWSLT 2008
evaluation campaign. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 1?17.
Lawrence R Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Jenny R Saffran, Richard N Aslin, and Elissa L New-
port. 1996. Statistical learning by 8-month-old in-
fants. Science, 274(5294):1926?1928.
Yee Whye Teh and Michael I Jordan. 2010. Hierar-
chical Bayesian nonparametric models with appli-
cations. Bayesian Nonparametrics: Principles and
Practice, pages 158?207.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting on Association for Computational Linguis-
tics, pages 985?992. Association for Computational
Linguistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
Bakeoff 2005. In Proceedings of the 4th SIGHAN
Workshop on Chinese Language Processing, volume
171. Jeju Island, Korea.
757
Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3):351?372.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
Chinese word segmentation for statistical machine
translation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics-
Volume 1, pages 1017?1024. Association for Com-
putational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In Proceedings of the 3rd International Joint Con-
ference on Natural Language Processing, pages 9?
16.
758
