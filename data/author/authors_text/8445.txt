Proceedings of the Fourth International Natural Language Generation Conference, pages 130?132,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Building a semantically transparent corpus
for the generation of referring expressions
Kees van Deemter and Ielka van der Sluis and Albert Gatt
Department of Computing Science
University of Aberdeen
{kvdeemte,ivdsluis,agatt}@csd.abdn.ac.uk
Abstract
This paper discusses the construction of
a corpus for the evaluation of algorithms
that generate referring expressions. It is
argued that such an evaluation task re-
quires a semantically transparent corpus,
and controlled experiments are the best
way to create such a resource. We address
a number of issues that have arisen in an
ongoing evaluation study, among which is
the problem of judging the output of GRE
algorithms against a human gold standard.
1 Creating and using a corpus for GRE
A decade ago, Dale and Reiter (1995) published
a seminal paper in which they compared a num-
ber of GRE algorithms. These algorithms included
a Full Brevity (FB) algorithm which generates de-
scriptions of minimal length, a greedy algorithm
(GA), and an Incremental Algorithm (IA). The
authors argued that the latter was the best model
of human referential behaviour, and versions of
the IA have since come to represent the state
of the art in GRE. Dale and Reiter?s hypothe-
sis was motivated by psycholinguistic findings,
notably that speakers tend to initiate references
before they have completely scanned a domain.
However, this finding affords different algorithmic
interpretations. Similarly, the finding that basic-
level terms in referring expressions allow hearers
to form a psychological gestalt could be incorpo-
rated into practically any GRE algorithm.1
We decided to put Dale and Reiter?s hypothesis
to the test by an evaluation of the output of dif-
1A separate argument for IA involves tractability, but al-
though some alternatives (such as FB) are intractable, others
(such as GA) are only polynomial, and can therefore not eas-
ily be dismissed on purely computational grounds.
ferent GRE algorithms against human production.
However, it is notoriously difficult to obtain suit-
able corpora for a task that is as semantically in-
tensive as Content Determination (for GRE). Al-
though existing corpora are valuable resources,
NLG often requires information that is not avail-
able in text. Suppose, for example, that a corpus
contained articles about politics, how would the
output of a GRE algorithm be evaluated against the
corpus? It would be difficult to infer from an ar-
ticle exactly which representatives in the British
House of Commons are Liberal Democrats, or
Scottish. Combining multiple texts is hazardous,
since facts could alter across sources and time.
Moreover, the conditions under which such texts
were produced (e.g. fault-critical or not, as ex-
plained below) are hard to determine.
A recent GRE evaluation by Gupta and Stent
(2005) focused on dialogue corpora, using MAP-
TASK and COCONUT, both of which have an as-
sociated domain. Their results show that referent
identification in MAPTASK often requires no more
than a TYPE attribute, so that none of the algo-
rithms performed better than a baseline. In con-
trast to MAPTASK, COCONUT has a more elabo-
rate domain, but it is characterised by a collabora-
tive task, and references frequently go beyond the
identification criterion that is typically invoked in
GRE2. Mindful of the limitations of existing cor-
pora, and of the extent to which evaluation de-
pends on the corpus under study, we are using
controlled experiments to create a corpus whose
construction will ensure that existing algorithms
can be adequately differentiated on an identifica-
tion task.
2Jordan and Walker (2000) have demonstrated a signifi-
cantly better match to the human data when task-related con-
straints are taken into account.
130
2 Setup of the experiment
Like Dale and Reiter (1995), we focused on first-
mention descriptions. However, we decided to in-
clude simple ?disjunctive? references to sets (as
in ?the red chair and the black table?), in addi-
tion to conjunctions of atomic properties, since
these can be handled by essentially the same al-
gorithms (van Deemter, 2002). For generality, we
looked at two very different domains. One of these
involved artificially constructed pictures of furni-
ture, where the available attributes and values are
relatively easy to determine. The other involved
real photographs of individuals, which provide a
richer range of options to subjects. To date, data
has been collected from 19 participants, and anal-
ysis is in progress.
Our first challenge was to make the experiment
naturalistic. Subjects were shown 38 randomised
trials, each depicting a set of objects, one or two
of which were the targets, surrounded by 6 dis-
tractors (Figure 1). In each case, a minimal distin-
guishing description of the targets was available.
Subjects were led to believe that they would be
describing the targets for an interlocutor. Once a
description was typed, the system removed from
the screen what it took to be the referents.
Figure 1: A stimulus example from the furniture domain.
Three groups performed the task in different
conditions, namely: ??FaultCritical?, where
half the subjects in the ?+FaultCritical? case
could use location (?in the top left corner?). The
?+FaultCritical? group was told: ?Our program
will eventually be used in situations where it is
crucial that it understands descriptions accurately.
In these situations, there will often be no option to
correct mistakes. Therefore, (...) you will not get
the chance to revise (your description)?. By con-
trast, the ??FaultCritical? subjects were given
the opportunity to revise their description should
the system have got it wrong. Subjects in the
??Location? condition were told that their inter-
locutor could see exactly the same pictures as they
could, but these had been jumbled up; by con-
trast, ?+Location? subjects were led to believe
that their addressee could see the pictures in ex-
actly the same position.
The second main challenge was to create tri-
als that would distinguish between all the algo-
rithms. For instance, if trials involved only one at-
tribute, say an object?s TYPE (e.g., chair or table),
they would not allow us to distinguish IA from
FB, as both would always generate the shortest de-
scription. Subtler issues arise with local brevity
(Reiter, 1990), an optimisation strategy which re-
quires sufficiently complex trials to make a differ-
ence.
3 How to analyse the data?
Our semantically transparent corpus can be
used for testing various hypotheses, for in-
stance about when an algorithm should
overspecify descriptions (e.g. more in
?+FaultCritical,+Location? (Arts, 2004),
and/or when the target is a set). Here, we focus on
the issue raised in Section 1, namely, which of the
algorithms discussed in Dale and Reiter (1995)
matches human behaviour best.
The first problem is determining the relevant al-
gorithms. The IA comes in different flavours, be-
cause its output depends on the order in which
the different properties are attempted (commonly
called the preference order). It is possible to
consider all different IAs (trying every conceiv-
able preference order), but this would increase the
number of statistical hypotheses to be tested, im-
pacting the validity of the results and requiring a
Bonferroni correction. Instead, we are using a pre-
test to find the optimal version of IA, comparing
only that version to the other algorithms.
The second question is how to assess algorithm
performance. Since our production experiment
does not yield a single gold standard (GS), an al-
gorithm might match subjects better in one con-
dition (e.g. ?+FaultCritical), or perform bet-
ter in one domain (e.g. furniture). Moreover, it
might match subjects poorly overall due to sam-
ple variation, while evincing a perfect match with
a single individual. Using both a by-subjects and a
by-items analysis will partially control for sample
131
dispersion.
How should we calculate the match between an
algorithm and a GS? Once again, there are two
facets to this problem. Since we are focusing on
Content Determination, each human description
could be viewed as associating, with the relevant
trial, a set of properties. Our approach will be to
annotate each human description with the set of at-
tributes it contains. However, the real data is often
messy. For example, when one subject called an
object ?the non-coloured table?, and another called
it ?the grey desk?, both may be expressing the same
attributes (i.e. TYPE and COLOUR). Also, while it
is often assumed that the output of GRE is a def-
inite noun phrase, this is not always the case in
our corpus, which contains indefinite distinguish-
ing descriptions such as ?a red chair, facing to
the right?, and telegraphic messages such as ?red,
right-facing?.
The second aspect to the problem concerns the
actual human-algorithm comparison. Suppose the
GS equals the output of one subject, and we are
comparing two algorithms, x and y. Suppose our
subject produced ?the two huge red sofas?, which
the GS associates with the set {sofa, red, large}.
Suppose our algorithms describe the target as:
Output from x : {sofa, red, top}
Output from y : {sofa, red, large, top}
Which of these algorithms matches the GS best?
Algorithm y adds a property (perhaps overspecify-
ing even more than the GS). Algorithm x has the
same length as the GS, but replaces one property
by another. Several reasonable ways of assess-
ing the differences can be devised, one of which is
Levenshtein distance (which suggests preferring y
over x, since the latter involves a deletion and an
addition) (Levenshtein, 1966). We also intend to
examine how often the GS over- or underspecifies
where the algorithm does not.
4 Conclusion
Corpora can be an invaluable resource for NLG
as long as the necessary contextual information
and the conditions under which the texts in a cor-
pus were produced are known. We believe that
controlled and balanced experiments are needed
for building semantically transparent resources,
whose construction we have discussed. As shown
in this paper, evaluation of algorithms against the
number of gold standards obtained with such a
corpus needs careful consideration.
Evaluation of GRE ? and NLG systems more
generally ? would benefit from more investiga-
tion of the differences between readers and pro-
ducers. In future work, we intend to follow up
with a reader-oriented experiment in which we test
the speed and/or accuracy with which the output
of different GRE algorithms is understood by sub-
jects. The dependent variables here will be non-
linguistic (perhaps involving subjects clicking on
pictures of presumed target referents). This illus-
trates a more general issue in this area, namely
that corpora should, in our view, only be a start-
ing point, with which data of different kinds can
be associated.
5 Acknowledgments
Thanks to Ehud Reiter, Richard Power
and Emiel Krahmer for useful comments.
This work is part of the TUNA project
(http://www.csd.abdn.ac.uk/
research/tuna/), funded by the EPSRC
in the UK (GR/S13330/01).
References
[Arts2004] A. Arts. 2004. Overspecification in Instruc-
tive Texts. Ph.D. thesis, Tilburg University.
[Dale and Reiter1995] R. Dale and E. Reiter. 1995.
Computational interpretations of the Gricean max-
ims in the generation of referring expressions. Cog-
nitive Science, 18:233?263.
[van Deemter2002] K. van Deemter. 2002. Generat-
ing referring expressions: Boolean extensions of the
incremental algorithm. Computational Linguistics,
28(1):37?52.
[Gupta and Stent2005] S. Gupta and A. J. Stent. 2005.
Automatic evaluation of referring expression gener-
ation using corpora. In Proceedings of the 1st Work-
shop on Using Corpora in NLG, Birmingham, UK.
[Jordan and Walker2000] P. Jordan and M. Walker.
2000. Learning attribute selections for non-
pronominal expressions. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics.
[Levenshtein1966] V. Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and rever-
sals. Soviet Physics Doklady, 10(8):707?710.
[Reiter1990] E. Reiter. 1990. The computational com-
plexity of avoiding conversational implicatures. In
Proceedings of the 28th ACL Meeting, pages 97?
104. MIT Press.
132
Proceedings of the 12th European Workshop on Natural Language Generation, pages 146?153,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Towards Empirical Evaluation of Affective Tactical NLG
Ielka van der Sluis
Trinity College Dublin
Dublin
ielka.vandersluis@cs.tcd.ie
Chris Mellish
University of Aberdeen
Aberdeen
c.mellish@abdn.ac.uk
Abstract
One major aim of research in affective nat-
ural language generation is to be able to
use language intelligently to induce effects
on the emotions of the reader/ hearer. Al-
though varying the content of generated
language (?strategic? choices) might be
expected to change the effect on emotions,
it is not obvious that varying the form of
the language (?tactical? choices) can do
this. Indeed, previous experiments have
been unable to show emotional effects of
tactical variations. Building on what has
been discovered in previous experiments,
we present a new experiment which does
demonstrate such effects. This represents
an important step towards the empirical
evaluation of affective NLG systems.
1 Introduction
This paper is about developing techniques for the
empirical evaluation of affective natural language
generation (NLG). Affective NLG has been de-
fined as ?NLG that relates to, arises from or de-
liberately influences emotions or other non-strictly
rational aspects of the Hearer? (De Rosis and
Grasso, 2000). It currently covers two main
strands of work, the portrayal of non-rational as-
pects in an artificial speaker/writer (e.g. the work
of Mairesse and Walker (2008) on projecting per-
sonality) and the use of NLG in ways sensitive to
the non-rational aspects of the hearer/reader and
calculated to achieve effects on these aspects (e.g.
the work of De Rosis et al (1999) on generat-
ing instructions in an emotionally charged situa-
tion and that of Moore et al (2004) on producing
appropriate tutorial feedback). Although there has
been success in evaluating work of the first kind,
it remains more problematic to evaluate whether
work of the second type directly affects emotion or
mood, or whether it influences task performance
for other reasons.
Since the work of Thompson (1977), NLG tasks
have been considered to divide mainly into those
involving strategy (?deciding what to say?) and
tactics (?deciding how to say it?). It seems clear
that one can affect a reader?s emotion differently
by making different strategic decisions about con-
tent (e.g. telling someone that they have passed
an exam will make them happier than telling them
that they have failed), but it is less clear that tac-
tical alternations (e.g. involving ordering of ma-
terial, choice of words or syntactic constructions)
can have these kinds of effects. Unfortunately,
the exact dividing line between strategy and tac-
tics remains a matter of debate. For the purpose
of this paper, we take ?strategic? to cover matters
of basic propositional content (the basic informa-
tion to be communicated) and ?tactical? to include
most linguistic issues, including matters of em-
phasis and focus, inasmuch as they can be influ-
enced by linguistic formulation. It is important to
know whether tactical choices can influence emo-
tions because to a large extent NLG research con-
centrates on tactical issues (partly because strate-
gic NLG remains a rather domain-specific activ-
ity).
Some light on the effects of tactical variations
in text is shed by work in Psychology, where there
has been a great deal of work on the effects of the
?framing? of a text (Moxey and Sanford, 2000;
Teigen and Brun, 2003). Some of this has been
industrially funded, as there are considerable ap-
plications, for instance, in advertising. The alter-
native texts considered differ in ways that NLG re-
searchers would call tactical. For instance, a piece
of meat could be described as ?75% lean? or ?25%
fat?, and arguably these are alternative truthful de-
scriptions of the same situation. However, evalu-
ation of this work has been primarily in terms of
whether it affects people?s choices or evaluations
146
of options available (Levin et al, 1998), or other
aspects of task performance (O?Hara and Stern-
berg, 2001; Brown and Pinel, 2003; Cadinu et al,
2005). As far as we know it is unknown whether
emotions can be affected in this way. There is
therefore an open question about whether it is pos-
sible to detect the non-rational effects of differ-
ent tactical decisions on readers. We believe that
achieving this is important for the further scientific
development of affective NLG.
In the rest of this paper, we discuss previous
(unsuccessful) attempts to measure emotional ef-
fects of tactical decisions in texts (section 2), the
particular linguistic choices we have focussed on,
including a text validation experiment (section 3)
and our choice of a method for measuring emo-
tions (section 4). In section 5 we then present a
new study which for the first time demonstrates
significant differences in emotions evoked in read-
ers associated with tactical textual variations. We
then briefly reflect on this result in a concluding
section.
2 Background for the Present Study
In (van der Sluis and Mellish, 2008) we de-
scribed several experiments investigating differ-
ent methods of measuring the effects of texts on
emotions to demonstrate that tactical differences
would lead to differences in effects. Our method
was to present participants with texts about cancer-
causing chemicals in foods or unexpected health-
giving properties of drinking water and to attempt
to measure the emotions invoked by different vari-
ations of these texts. However, we were unable
to show statistically significant results of tactical
variations. We mentioned the following possible
explanations for this:
? We used methods where participants reported on their
own emotions. However, it could be that (in this con-
text) participants were unwilling or unable to report ac-
curately.
? The self-reporting methods used were perhaps not fine
grained enough to register the differences between the
effects of similar texts.
? The texts themselves were perhaps too subtly different
or not long enough to induce strong emotions.
? The participants were perhaps not involved enough in
the task to get strong emotions.
We believe that of these, the final reason is the
most compelling. The self-reporting methods used
had been validated and used in multiple previous
studies in Psychology, and so there was no rea-
son to suggest that they would fundamentally fail
in this new context. The granularity of the mea-
surement methods can be improved relatively sim-
ply (see section 4 below). But it is very believ-
able that the participants would fail to be really
concerned by the texts in the experiments reported
since the source was unclear, the message a gen-
eral one not addressed to them individually and the
topic (healthy and unhealthy food) one that occurs
often enough in newspapers to fail to overcome
natural boredom.
The main innovation of the experiment we de-
scribe below was in our method of seeking the
emotional involvement of the participants. The
texts that the participants read took the form of
?feedback? on a (fake) IQ test that they undertook
as part of the experiment. We selected university
students as the participants, as they would likely
be concerned about their intelligence, especially
as compared to their peers. The texts appeared to
be written individually for the participants and so
sought to engage them directly.
3 Linguistic Choice and Framing
As in (van der Sluis and Mellish, 2008), the study
we present here sought to evoke positive emotions
to differing extents in a reader by tactical manip-
ulations to ?slant? the tasks positively to varying
degrees. This section describes the text variations
used and their validation.
3.1 Tactical Methods
The two texts produced for this experiment were
written by hand, but used the following methods
to give a more ?positive slant? to a text. These are
all methods that could be implemented straight-
forwardly in an NLG system1. In the follow-
ing, the word ?positive polarity? is used to refer
to propositions giving good news to the reader or
attributes which give good news to the reader if
they have high values (such as the reader?s intel-
ligence). Similarly ?negative polarity? refers to
items that represent bad news, e.g. failing a test.
For ethical reasons, negative polarity items did not
arise in this experiment.
A. Sentence emphasis - include explicit emphasis in sen-
tences expressing positive polarity propositions (e.g.
exclamation marks and phrases such as ?on top of
this?).
1Though the choice about when to apply them might not
be so straightforward.
147
B. Choice of vague evaluative adjectives - when evaluat-
ing positive polarity attributes, choose vague evaluative
adjectives that are more positive over ones that are less
positive (e.g. ?excellent?, rather than ?ok?).
C. Choice of vague adverbs - provide explicit emphasis to
positive polarity propositions by including vague ad-
verbs expressing great extent (e.g. ?significantly?,
rather than ?to some extent? or no adverb).
D. Choice of verbs - for a positive polarity proposition,
choose a verb that emphasises the great extent of the
proposition (e.g. ?outperformed?, rather than ?did bet-
ter than?).
E. Choice of realisation of rhetorical relations - when re-
alising a concession/contrast relation between a pos-
itive polarity proposition and one that is negative or
neutral, word it so that the positive polarity proposi-
tion is in the nucleus (more emphasised) position (e.g.
say ?although you did badly on X, you did well on Y?
instead of ?although you did well on Y, you did badly
on X?).
The idea is that an NLG system would employ
methods of this kind in order to ?slant? a mes-
sage positively, rather that to present a message
in a more neutral way. This might be done, for
instance, to induce positive emotions in a reader
who needs encouragement.
We claim that these choices can be viewed as
tactical, i.e. that they are ?allowable? alterna-
tive realisations of the same underlying content.
For instance, we believe a teacher could use such
methods in giving feedback to a student need-
ing encouragement without fear of prosecution for
misrepresenting the same truth that would be ex-
pressed without the use of these methods.
Whenever one words a proposition in differ-
ent ways, it can be claimed that a (perhaps sub-
tle) change of meaning is involved. However, in
these cases we claim that it is the writer?s atti-
tudes that are being manipulated (and reflected in
the text). We can therefore choose between these
alternatives by varying the writer, not the under-
lying message. Our view is supported by a num-
ber of current accounts of the semantics of vague
adjectives (though this is not an area without con-
troversy). Many accounts of vagueness appeal to
the idea that there is a norm which an adjective
like ?tall? implicitly refers to, and some of these
argue both that the norm itself can be contextually
determined and also that the amount by which the
norm has to be exceeded has to be ?significant?
to a degree which is ?relativized to some agent?
(Kennedy, 2007). For instance, with the phrase
?John is tall?
?the property [...] attributed to John is not
an intrinsic property, but rather a relational one.
Moreover, it is not a property the possession of
which depends only on the difference between
Johns height and some norm, but also on whether
that difference is a significant one. I take it that
whether or not a difference is a significant differ-
ence does not depend only on its magnitude, but
also on what our interests are? (Graff, 2000)
It is compatible with these accounts that differ-
ent agents, with different interests and notions of
what is noteworthy, can use vague adjectives in
different ways2.
Another reason for considering these meth-
ods as tactical is that in an NLG system, they
would likely be implemented somewhere late in
the ?pipeline?.
Probably the best way to check that we are using
tactical alternations (according to our definition) is
via some kind of text validation experiment with
human participants. Section 3.3 below describes
such an experiment, which provides strong sup-
port for this position.
3.2 Test Texts
For the experiment, we produced two feedback
texts describing the same set of intelligence test
results, one relatively neutral and one ?positively
slanted? using the above methods. In the ex-
periment, they were given to participants in two
groups, named ?0? and ?+? respectively. Each text
consisted of 7 sentences, with a direct correspon-
dance between the sentences of the two texts. Fig-
ure 1 presents the variations used in the feedback
used in the experiment for group + (i.e. positively
slanted) and group 0 (i.e. neutrally slanted). Note
that the actual numbers are the same in both texts.
3.3 Text validation
A text validation study was conducted in which
15 colleagues participated. The participants were
asked to comment on 12 sentence pairs, the 7
shown in Figure 1 and 5 additional filler pairs. The
following analysis reports on our findings on the 7
sentence pairs shown in Figure 1 only.
In order that we could test our intuitions about
the tactical nature of the linguistic alternations
(discussed in section 3.1 above), the participants
were presented with a scenario where there were
two different teachers, Mary Jones and Gordon
2Though there are certainly some limits on the situations
where a word like ?tall? can be truthfully used to describe a
height
148
+1: Your Baumgartner score of 7.38 is excellent!
01: Your Baumgartner score of 7.38 is ok.
+2: You did distinctively better than the average score ob-
tained by other people in your age group.
02: You did somewhat better than the average score ob-
tained by other people in your age group.
+3: Especially your scores on Imagination/Creativity and
on Clarity of Thought were great and considerably
higher than average.
03: Your scores on Imagination/Creativity and on Clarity
of Thought were good and a little higher than average.
+4: A factor analyses of your Baumgartner score results in
an overall excellent performance.
04: A factor analyses of your Baumgartner score results in
an overall reasonable performance.
+5: Although, compared to your peers, you have only
slightly higher Spatial Intelligence (7.5 vs 7.0) and Vi-
sual Intelligence (7.2 vs 6.8) scores, your Clarity of
Thought Score is very much better (7.2 vs 6.3).
05: Compared to your peers, you have a somewhat better
Clarity of Thought Score (7.2 vs 6.3), but you have only
slightly higher Spatial Intelligence (7.5 vs 7.0) and Vi-
sual Intelligence (7.2 vs 6.8) scores.
+6: On top of this you also outperformed most people
in your age group with your exceptional scores for
Imagination and Creativity (7.9 vs 7.2) and Logical-
Mathematical Intelligence (7.1 vs. 6.5).
06: You did better than most people in your age group with
your scores for Imagination and Creativity (7.9 vs 7.2)
and Logical-Mathematical Intelligence (7.1 vs. 6.5).
+7: There is a lot of variation in your age group, but your
score is significantly higher than average.
07: Your score is higher than average, but there is a lot of
variation in your age group.
Figure 1: Linguistic variation used in the IQ test feedback
Smith, both completely honest but with very dif-
ferent ideas about teaching (Mary believing that
any pupil can succeed, given encouragement,
but Gordon believing that most pupils are lazy
and have overinflated ideas about their abilities).
Given a positively slanted sentence (e.g. +7) from
Mary and a corresponding more neutrally slanted
one (e.g. 07) from Gordon, addressed to one or
more pupils, participants were asked to indicate:
1. ?Is it possible that Mary and Gordon might actually be
(honestly) giving different feedback to the same pupil
on the same task??
2. ?If the two pieces of feedback were given to the same
pupil (for the same task) and the pupil?s parents found
out, do you think they would have grounds to make a
complaint that one of the teachers is lying??
The hypothesis was that (for the 7 pairs of
sentences from Figure 1) in general participants
would answer ?yes? to question 1 and ?no? to
question 2. Indeed, for 6 pairs at least 14 out of the
15 participants answered as we had predicted. For
the other pair (+4/04), 12 out of 15 agreed with
both predictions. We see this as very strong evi-
dence for our position (the participants gave dif-
ferent answers for the filler pairs, and so were not
just producing these answers blindly).
No alterations were made to the two feedback
texts on the basis of the text validation results.
4 Measuring Emotions
There are two broad ways of measuring the emo-
tions of human subjects ? physiological methods
and self-reporting. Physiological methods unfor-
tunately tend to have the problems of complex
setup and calibration, which mean that it is hard to
transport them between tasks or individuals. In ad-
dition, although emotional states are undoubtedly
connected to physiological variables, it is not al-
ways clear what is being measured by these meth-
ods (cf. (Lazarus et al, 1980); (Cacioppo et al,
2000) ).
Because of these problems, we have opted to in-
vestigate self-reporting methods, as validated and
used widely in psychological experiments. Three
well-established methods that are used frequently
in the field of psychology are the Russel Affect
Grid (Russell et al, 1989), the Self Assessment
Manikin (SAM) (Lang, 1980) and the Positive and
Negative Affect Scale (PANAS) (Watson et al,
1988). In our previous study (van der Sluis and
Mellish, 2008), we had problems with participants
understanding how to use the Russel Affect Grid
and SAM and so now we opted to use a version of
the PANAS test.
The PANAS test is a scale using affect terms
that describe positive and negative feelings and
emotions. Participants in the experiment read the
terms and indicate to what extent they experi-
ence(d) the emotions indicated by each of them
using a five point scale ranging from (1) very
slightly/not at all, (2) a little, (3) moderately, (4)
quite a bit to (5) extremely. A total score for
positive affect is calculated by simply adding the
scores for the positive terms, and similarly for neg-
ative affect.
As before, we used a simplified version of the
PANAS scale in order not to overburden the partic-
ipants with questions and to avoid bored answer-
ing. In this test, which has been fully validated
(Mackinnon et al, 1999), participants have to rate
only 10 instead of 20 terms: 5 for positive af-
149
fect (i.e. alert, determined, enthusiastic, excited,
inspired) and 5 for negative affect (i.e. afraid,
scared, nervous, upset, distressed).
Our use of the simplified PANAS in this study
differed from our previous study, however, by hav-
ing participants respond to the PANAS questions
using a slider, rather than a five point scale. This
means that only two terms were put at the extreme
ends of the slider (i.e. ?very slightly/not at all? and
?extremely? were presented but not ?a little?, ?mod-
erately? or ?quite a bit?). The change to use a slider
was because van der Sluis and Mellish (2008) ob-
served partipants only using a small part of the
possible scale for answers, and within this the five
point scale might have lost useful information.
Although our particular experiment focussed on
positive affect, we included the negative affect
terms partly so that we could detect outliers in
our participant set ? people who were perhaps ex-
tremely nervous about the test or sensitive about
their IQ. In fact, we did not find any such outliers.
5 Experiment to Measure Emotional
Effects of Positive Feedback
5.1 Set Up of the Study
As stated above, the texts that we presented to
our participants were portrayed as giving feedback
on an IQ test that the participants had just taken.
The IQ test was set up as a web experiment in
which participants could linearly traverse through
the various phases of the test. An outline of the
set up is given in Figure 2. In the general intro-
duction to the experiment, participants were told
that the experiment was ?an assessment of a new
kind of intelligence test which combines a number
of well-established methods that are used as indi-
cators of human brain power?. To make it more
difficult for the participant to keep track of how
well/poorly she performed over the course of the
test, it also said that the test consisted of open and
multiple choice questions that had different weight
factors in the calculation of the overall score and
that would assess various aspects of their intelli-
gence. Subsequently, the participant was asked
to tick a consent form to participate in the study.
Then a questionnaire followed in which the par-
ticipant was asked about her age, gender and the
quality of her English. She was also asked if she
had any experience with IQ tests and how she ex-
pected to score on this one. These questions were
interleaved with an emotion assessment test (re-
duced PANAS) in which the participant was asked
?how do you feel right now??.
After filling out the questionnaire, the partici-
pant could start the ?IQ test? whenever she was
ready. The ?IQ test? consisted of 30 questions
which she had to answer one at a time. The par-
ticipant could not skip a question and also had
to indicate for each of the questions how confi-
dent she was about her answer. The questions
that were used for the test were carefully collected
from the internet and included items from various
tests and games. Different types of questions were
used: questions about logical truths, mathematical
questions that required some calculations, ques-
tions about words and letter sequences, questions
including pictures and questions about the partic-
ipant?s personality. They were ordered randomly
(but with the same order for each participant).
When the participant had finished the test, she
was asked to wait patiently while the system cal-
culated the test scores. When enough calculation
time had passed the participant was presented with
the test feedback (one of the two texts, regardless
of their actual performance). This feedback first
explained the test and its type of scoring:
The Baumgartner test which you have just un-
dertaken tests various kinds of intelligence, for
instance, your visual intelligence, your logical-
mathematical intelligence and your spatial in-
telligence. These various aspects of your in-
telligence contribute to an overall Baumgartner
Score. The Baumgartner Score rates your intel-
ligence on a 10-point scale with 10 as the high-
est possible score. Note that your Baumgartner
Score can change over time dependent on expe-
rience and practice. Below your test score is pre-
sented in comparison with the average score in
your age group.
The introduction to the test was followed by ei-
ther the positively (+1..+7, Figure 1) or the rela-
tively neutrally (01..07, Figure 1) phrased test re-
sults. After the participant had processed the feed-
back, she was asked to fill out one more question-
naire to assess her emotions (i.e. ?How do you
feel right now knowing your scores on the test?).
This time the simplified PANAS test was inter-
leaved with questions about the participant?s re-
sults, (e.g. were they as expected and how did she
value them), the test (e.g. was it difficult, doable
or easy?) and space for comments on the test and
the experiment. Finally the participant was de-
briefed about the experiment and about the goal
of the study.
150
1. General introduction to the experiment;
2. Consent form;
3. Questionnaire on participant?s background and famil-
iarity with IQ-test interleaved with a PANAS test to as-
sess the participant?s current emotional state;
4. Message: ?Please press the next button at the bottom
of this page whenever you are ready to start the intelli-
gence test?;
5. IQ test questions;
6. Message: Please be patient while your answers are be-
ing processed and your test score is computed. After
the result page, you will be asked another set of ques-
tions about the test, your performance and the way you
feel about it. This information is very important for
this study, so please answer the questions as honestly
as possible.?;
7. Feedback + or 0;
8. Questionnaire: PANAS test to assess how the partic-
ipants felt after reading the test feedback interleaved
with questions about the test, their expectations and
space for comments;
9. Debriefing which informed participants about the
study?s purpose and stated that the IQ test was not real
and that their test results did not contain any truth.
Figure 2: Phases in the experiment set up
5.2 Pilot Experiment
A pilot of the experiment was carried out by ask-
ing a number of people to try the experiment via
the web interface. The main outcomes of this
study, in which 11 colleagues participated, was
that the experiment was too long. Accordingly, the
questionnaires before and after the IQ test (phase
3 and 8 in Figure 2) were shortened. Also the IQ
test itself was shortened from 40 to 30 questions.
5.3 Main Experiment: participants and
experimental setting
30 participants, all female university students,
took the IQ test. All participants except two were
in age band 18-24. The exceptions were in age
band 25-29 (group +) and 30-34 (group 0). The
participants were randomly distributed over group
+ and group 0 and (for ethical reasons) did the test
one by one in a one-person experiment room while
the experimenter was waiting outside the room.
As soon as the participant indicated that she had
finished the task (i.e. stepped out of the exper-
iment room), she was debriefed about the study
by the experimenter and was paid with a voucher
worth 5 pounds.
5.4 Hypotheses
Since the message of the feedback texts was rel-
atively positive and there is no necessary correla-
0-group +-group
Negative PANAS terms Before 1.60(.76) 1.58(.68)
Negative PANAS terms After 1.57(.68) 1.31(.45)
Positive PANAS terms Before 3.25(.78) 3.32(.55)
Positive PANAS terms After 3.13(.58) 3.75(.55)
Table 1: Means and Standard deviations (between brack-
ets) for the negative and positive PANAS terms as indicated
before and after the IQ test undertaken by participants that
received neutral and participants that received positive feed-
back on their performance.
tion between positive and negative PANAS scores
(Watson and Clark, 1999), we expected the main
effects of the texts to be on the average evaluation
of the positive PANAS terms. In order to cater for
the fact that individuals might differ in their initial
positive PANAS scores, we decided to look at the
difference of the scores (score after minus score
before). Therefore the hypothesis for this study
was that participants who received the positively
phrased feedback would show a larger change in
their positive emotions than the participants who
received the neutrally phrased feedback.
5.5 Results
Table 1 indicates that on average after they had re-
ceived their test results, participants in the +-group
were more positively tuned than participants in the
0-group. Participants in the +-group also rated the
positive emotion terms higher than they had done
before they undertook the IQ test. No such results
were found for the 0-group. In contrast, compared
to their responses before the IQ test, participants in
the 0-group rated the positive terms slightly lower
after they had processed their neutrally phrased
feedback. With respect to the negative PANAS
terms, participants in the +-group report slightly
less negative emotions after they read their test
scores, but none of the differences found in the
negative PANAS scores were significant.
A 2 (feedback type) * 2 (before/after) * 2 (pos-
itive/negative mean) repeated measures ANOVA
was carried out on the average PANAS scores.
This showed no main effect of feedback type
(+ vs 0) and no main effect of before/after on
average PANAS scores. However, there was a
highly significant interaction between feedback
type and before/after, which indicates that the
change in PANAS mean before and after the text
was strongly dependent on feedback type3 (F(1,
28) = 10.246, p < .003). We interpret this to mean
that the (after minus before) value is significantly
3An ANOVA test on the positive means only produces a
similar result.
151
0-group +-group
Alert Before 3.96(.80) 3.17(.99)
Alert After 3.45(.76) 3.65(.75)
Determined Before 3.49(1.02) 3.60(.50)
Determined After 3.50(1.13) 3.74(.61)
Enthusiastic Before 3.52(1.05 3.49(.72)
Enthusiastic After 2.97(.81) 3.84(.66)
Excited Before 2.74(.97) 3.28(.61)
Excited After 2.64(.75) 3.69(.83)
Inspired Before 2.56(1.21) 3.06(.77)
Inspired After 3.06(1.05) 3.81(.78)
Table 2: Means and Standard deviations (between brack-
ets) for the positive PANAS terms as indicated after the IQ
test undertaken by participants that received positive and par-
ticipants that received neutral feedback on their performance.
0-group +-group
ER
not disclosed 1 0
not so good 0 1
ok 9 4
well 4 10
extremely well 1 0
Table 3: Participant responses when questioned about the
results they expected (ER) .
greater for the +-group. A two-tailed, two sam-
ple t-test verifies this (t = 3.2, p < 0.004). We did
some post-hoc investigation in an attempt to un-
derstand the main result more fully. When look-
ing at the positive PANAS scores in more detail
(see Table 2), it turns out that only three of the
five positive PANAS terms included in the simpli-
fied PANAS test render promising results. Inter-
actions were found for the terms ?alert? (F(1, 28)
= 10.291, p < .003) and ?enthusiastic? (F(1, 28)
= 5.651, p < .025). No interactions were found
for the terms ?determined? and ?inspired?. For ?in-
spired? however, we found a main effect of feed-
back type : (F(1, 28) = 8.755, p < .006), which in-
dicates that participants in the +-group could have
been more inspired because of their test scores
than participants in the 0-group. Not all of these
results would be significant if Bonferroni correc-
tions were made.
5.6 The Role of Expectations
It is possible that this result could have been
caused by other (systematic but unanticipated) dif-
ferences between the two groups. In particular,
perhaps the result could be caused by a differ-
ence in how well the two groups of participants
expected to perform. As it happens, participants
were asked: ?How do you expect to score on an
intelligence test?? before they did the test. The
answers to this question are summarised in Ta-
ble 3. This data suggests that participants in the
+-group initially had higher expectations. It is
difficult to get a consensus from the psychologi-
cal literature about how this might have affected
the results. On the one hand, some studies have
shown that positive expectations can have an ac-
celerating effect on a person?s actual positive emo-
tional experience (Wilson et al, 2003; Wilson and
Klaaren, 1992). Such results might suggest an al-
ternative explanation of the fact that the +-group
showed a greater change in positive emotions. On
the other hand, it might be argued that subjects
with lower expectations would be more surprised
(since both texts presented good results) and so
their emotions would have been influenced more
significantly. That is, if a subject already expects
to do well then one would not expect that find-
ing that they actually did well would cause much
of a change in their emotions. This would predict
that it should be the 0-group that shows the great-
est emotion change. Overall, it is hard to know
whether the data about expectations should affect
our confidence in the experiment result, though it
would be worthwhile controlling for initial expec-
tations in further experiments of this kind.
6 Discussion and Future Directions
6.1 Discussion
Compared with the previous study of van der Sluis
and Mellish (2008), we expected participants to
indicate stronger emotional effects, because the
text participants were asked to read was about their
own capabilities instead of about something in the
world around them which they could think would
not affect them. Indeed, this seems to have been
the case. In van der Sluis and Mellish (2008), all
responses used the lower half of the scale, whereas
with the slider our participants indicated values up
to both extremes of the range available. Unfortu-
nately, the fact that one set of values is discrete and
the other continuous means that it is hard to carry
out a simple statistical comparison.
6.2 Future Work
In the study described in the paper, a number of
different techniques (e.g. emphasis, vague adjec-
tives and adverbs) were used to phrase the various
propositions in the feedback. In future work we
aim to identify the relative importance of the indi-
vidual techniques.
152
6.3 Conclusion
The fact that we have been able to show a signifi-
cant difference in the emotions induced by the two
texts is very encouraging. It suggests that there
is a possible methodology for directly evaluating
affective NLG and that the tactical concerns with
which much of NLG research is occupied are rel-
evant to affective NLG. A similar methodology
could perhaps now be used to determine the ef-
fectiveness of specific NLG methods and mecha-
nisms in terms of inducing emotions. Although we
have now shown that NLG tactical decisions can
affect emotions, it remains to be seen what kind of
changes in strategy, learning, motivation, etc., can
be induced by positive affect and thus how these
framing decisions would best be made by an NLG
system.
Acknowledgments
This work was supported by the EPSRC platfrom grant ?Af-
fecting people with natural language? (EP/E011764/1) and
also in part by Science Foundation Ireland under a CSET
grant (NGL/CSET). We would like to thank the people who
contributed to this study, most notably Judith Masthoff, Al-
bert Gatt and Kees van Deemter and Nikiforos Karamanis.
References
R. Brown and E. Pinel. 2003. Stigma on my mind: Indi-
vidual differences in the experience of stereotype threat.
Journal of Experimental Social Psychology, 39:626?633.
J. Cacioppo, G. Bernston, J. Larson, K. Poehlmann, and
T. Ito. 2000. The psychophysiology of emotion. In
M. Lewis and J. Haviland-Jones, editors, Handbook of
Emotions, pages 173?191. New York: Guilford Press.
M. Cadinu, A. Maass, A. Rosabianca, and J. Kiesner. 2005.
Why do women underperform under stereotype threat?
Psychological Science, 16(7):572?578.
D. Graff. 2000. Shifting sands: An interest-relative theory of
vagueness. Philosophical Topics, 20:45?81.
C. Kennedy. 2007. Vagueness and grammar: the semantics
of relative and absolute gradable adjectives. Linguistics
and Philosophy, 30:1?45.
P. Lang. 1980. Behavioral treatment and bio-behavioral
assessment: Computer applications. In J. Sidowske,
J. Johnson, and T. Williams, editors, Technology in Mental
Health Care Delivery Systems, pages 119?137. Norwood,
NJ: Ablex.
R. Lazarus, A. Kanner, and S. Folkman. 1980. Emotions: A
cognitive-phenomenological analysis. In R. Plutchik and
H. Kellerman, editors, Emotion, theory, research, and ex-
perience. New York: Academic Press.
I. Levin, S. Schneider, and G. Gaeth. 1998. All frames
are not created equal: A typology and critical analysis of
framing effects. Organizational behaviour and human de-
cision processes, 76(2):149?188.
A. Mackinnon, A. Jorm, H. Christensen, A. Korten, P. Ja-
comb, and B. Rodgers. 1999. A short form of the positive
and negative affect schedule: evaluation of factorial valid-
ity and invariance across demographic variables in a com-
munity sample. Personality and Individual Differences,
27(3):405?416.
F. Mairesse and M. Walker. 2008. Trainable generation of
big-five personality styles through data-driven parameter
estimation. In Proc. of the 46th Annual Meeting of the
ACL.
J. Moore, K. Porayska-Pomsta, S. Varges, and C. Zinn. 2004.
Generating tutorial feedback with affect. In Proceedings
of the 7th International Florida Artificial Intelligence Re-
search Symposium Conference (FLAIRS).
L. Moxey and A. Sanford. 2000. Communicating quantities:
A review of psycholinguistic evidence of how expressions
determine perspectives. Applied Cognitive Psychology,
14(3):237?255.
L. O?Hara and R. Sternberg. 2001. It doesn?t hurt to ask:
Effects of instructions to be creative, practical, or ana-
lytical on essay-writing performance and their interaction
with students? thinking styles. Creativity Research Jour-
nal, 13(2):197?210.
F. De Rosis and F. Grasso. 2000. Affective natural lan-
guage generation. In A. Paiva, editor, Affective Interac-
tions. Springer LNAI 1814.
F. De Rosis, F. Grasso, and D. Berry. 1999. Refining in-
structional text generation after evaluation. Artificial In-
telligence in Medicine, 17(1):1?36.
J. Russell, A. Weiss, and G. Mendelsohn. 1989. Affect grid:
A single-item scale of pleasure and arousal. Journal of
Personality and Social Psychology, 57:493?502.
K. Teigen and W. Brun. 2003. Verbal probabilities: A ques-
tion of frame. Journal of Behavioral Decision Making,
16:53?72.
H. Thompson. 1977. Strategy and tactics: A model for lan-
guage production. In Proceedings of the Chicago Linguis-
tics Society, Chicago.
I. van der Sluis and C. Mellish. 2008. Using tactical NLG to
induce affective states: Empirical investigations. In Pro-
ceedings of the fifth international natural language gener-
ation conference, pages 68?76.
D. Watson and L. Clark, 1999. Manual for the Positive and
Negative Affect Schedule - Expanded Form. The Univer-
sity of Iowa.
D. Watson, L. Clark, and A. Tellegen. 1988. Development
and validation of brief measures of positive and negative
affect: The PANAS scales. Journal of Personality and
Social Psychology, 54(1063-1070).
T. Wilson and K. Klaaren. 1992. The role of affective expec-
tations in affective experience. In M. Clark, editor, Review
of Personality and Social Psychology, volume 14: Emo-
tion and Social Behaviour, pages 1?31. Newbury Park,
CA: Sage.
T. Wilson, D Gilbert, and D. Centerbar. 2003. Making sense:
The causes of emotional evanescence. In I. Brocas and
J. Carrillo, editors, The Psychology of Economic Deci-
sions, volume 1: Rationality and Well Being, pages 209?
233. New York: Oxford University Press.
153
Using Tactical NLG to Induce Affective States: Empirical Investigations
Ielka van der Sluis
Computing Science,
University of Aberdeen
i.v.d.sluis@abdn.ac.uk
Chris Mellish
Computing Science
University of Aberdeen
c.mellish@abdn.ac.uk
Abstract
This paper reports on attempts at Aberdeen1
to measure the effects on readers? emotions of
positively and negatively ?slanted? texts with
the same basic message. The ?slanting? meth-
ods could be implemented in an (NLG) sys-
tem. We discuss a number of possible reasons
why the studies were unable to show clear, sta-
tistically significant differences between the
effects of the different texts.
1 Introduction: Affective NLG
?Affective NLG? has been defined as ?NLG that re-
lates to, arises from or deliberately influences emo-
tions or other non-strictly rational aspects of the
Hearer? (De Rosis and Grasso, 2000). Although this
term could cover a range of types of NLG, in prac-
tice, a lot of work on affective NLG emphasises the
depiction of emotional states/personalities (Oberlan-
der and Gill, 2004), rather than the induction of emo-
tional effects on readers. However, there are many
applications where the intention is, for instance, to
motivate or discourage, as well as to inform.
How can NLG influence the emotions of its read-
ers? It is apparent that strategical decisions (?what
to say?) can make a difference on how a reader re-
sponds emotionally to a text. If you tell someone
good news, they will be happier than if you tell them
bad news. On the other hand, much of NLG is con-
cerned with tactical decisions (?how to say it?), and
the affective relevance of these is less clear. Can tac-
tical NLG choices be used to achieve goals in terms
1Ielka van der Sluis is now at the Department of Computer
Science, Trinity College, Dublin
of the reader?s emotions? In the area of affective
computing, there has been some work on assess-
ing the effects of interfaces on the emotions of their
users, e.g. on their frustration levels (Prendinger et
al., 2006) or their feelings of support/trust (Lee et
al., 2007). In NLG there has been some work on
task-based evaluation cf. STOP (Reiter et al, 2003)
and SKILLSUM (Williams and Reiter, forthcoming).
However, to our knowledge, there has not yet been
any demonstration of tactical decisions making a
difference on a reader?s emotions.
The paper is organised as follows: Section 2 intro-
duces the tactical choices we are studying, our test
texts and a text validation study. Section 3 discusses
a pilot study that was conducted to try out poten-
tial psychological measurement methods. Section 4
presents a full study to measure the affect of text in-
voked in readers. The paper closes with a discussion
of the findings and their possible implications.
2 Tactical Choices
We decided that a safe way to start would be
to choose primitive positive versus negative emo-
tions (such as sadness, joy, disappointment, sur-
prise, anger), as opposed to more complex emo-
tions related to trust, persuasion, advice, reassur-
ance. Therefore we focus here on alternatives that
give a text a positive or negative ?slant?. These could
be applied by an NLG system whose message has
?positive? and ?negative? aspects, where ?positive?
information conjures up scenarios that are pleasant
and acceptable to the reader, makes them feel happy
and cooperative etc. and ?negative? information
conjures up unpleasant or threatening situations and
68
so makes them feel more unhappy, confused etc. For
instance, (DeRosis et al, 1999) discuss generating
instructions on how to take medication which have
to both address positive aspects (?this will make you
feel better if you do the following?) and also negative
ones (?this may produce side-effects, which I have to
tell you about by law?). An NLG system in such a
domain could make itself popular by only mention-
ing the positive information, but then it could leave
itself open to later criticism (or litigation) if by do-
ing so it clearly misrepresented the true situation.
Although it may be inappropriate grossly to misrep-
resent the provided message, there are more subtle
(tactical) ways to ?colour? or ?slant? the presenta-
tion of the message in order to emphasise either the
positive or the negative aspects.
We assume that the message to be conveyed is
a simple set of propositions, each classified in an
application-dependent way as having positive or
negative polarity according to whether the reader is
likely to welcome it or be unhappy about it in the
context of the current message.2 In general, this
classification could, for instance, be derived from
the information that a planning system has about
which propositions support which goals (e.g. to stay
healthy one needs to eat healthy food). We also as-
sume that a possible phrasing for a proposition has
a magnitude, which indicates the degree of impact it
has. This is independent of the polarity. We will not
need to actually measure magnitudes, but when we
make claims that one wording of a proposition has
a smaller magnitude than another we indicate this
with <. For instance, we would claim that usually:
?a few rats died? < ?many rats died?
Thus we claim that ?a few rats died? has less im-
pact than ?many rats died?, whether or not rats dy-
ing is considered a good thing (i.e. whether the po-
larity is positive or negative). In general, an NLG
system can manipulate the magnitude of wordings
of the propositions it expresses, to indicate its own
(subjective) view of their importance. In order to
slant a text positively, it can express positive polarity
propositions in ways that have high magnitudes and
negative polarity propositions in ways that have low
2Note that this sense of ?polarity? is not the same as the one
used to describe ?negative polarity items? in Linguistics
magnitudes. The opposite applies for negative slant-
ing. Thus, for instance, in an application where it
is bad for rats to die, expressing a given proposition
by ?a few rats died? would be giving more of a pos-
itive slant, whereas saying ?many rats died? would
be slanting it more negatively.
Whenever one words a proposition in different
ways, it can be claimed that a (perhaps subtle)
change of meaning is involved. In an example like
this, therefore, perhaps the content of the message
changes between the two wordings and so this is in
fact a strategic alternation. In this work, we take the
view that it is legal to make changes that relate to the
writer?s attitude to the material of the text. The dif-
ference between ?a few rats? and ?many rats? is (in
our view) that the number of rats is either less than
or more than the writer would have expected. We
can therefore choose between these alternatives by
varying the writer, not the underlying message. An-
other reason for considering this choice as tactical
is that in an NLG system, it would likely be imple-
mented somewhere late in the ?pipeline?. Our claim
that pairs such as this can appropriately describe the
same event is also supported by our text validation
experiments described below.
2.1 Test Texts
We started by composing by hand two messages
containing mainly negative and positive polarity
propositions respectively. The negative message
tells the reader that a cancer-causing colouring sub-
stance is found in some foods available in the su-
permarkets. The positive message tells the reader
that foods that contain Scottish water contain a min-
eral which helps to fight cancer. The first paragraph
of both texts states that there is a substance found
in consumer products that has an effect on people?s
health and it addresses the way in which this fact
is handled by the relevant authorities. The second
paragraph of the text elaborates on the products that
contain the substance and the third paragraph ex-
plains in what way the substance can affect people?s
health.
To study the effects of different wordings, for
each text a positive and a negative version was pro-
duced by slanting propositions in either a positive
or a negative way. This resulted in four texts in to-
tal, two texts with a negative message one positively
69
and one negatively phrased (NP and NN), and two
texts with a positive message one positively and one
negatively verbalised (PP and PN). To maximise the
impact aimed for, various slanting techniques were
used by hand as often as possible without loss of be-
lievability (this was assessed by the intuition of the
researchers). The positive and negative texts were
slanted in parallel as far as possible, that is in both
texts similar sentences were adapted so that they em-
phasised the positive or the negative aspects of the
message. The linguistic variation used in the texts
was algorithmically reproducible and the techniques
are illustrated below. A number of these were sug-
gested by work on ?framing? in Psychology (Moxey
and Sanford, 2000; Teigen and Brun, 2003). Indeed,
that work also suggests further variations that could
be manipulated, for instance, the choice between us-
ing numerical and non-numerical values for express-
ing quantities.
SLANTING EXAMPLES FOR THE NEGATIVE MESSAGE
Here it is assumed that recalls of products, risks
of danger etc. involve negative polarity proposi-
tions. Therefore negative slanting will amongst
other things choose high magnitude realisations for
these.
Techniques involving adjectives and adverbs:
- ?A recall? < ?A large-scale recall? of infected
merchandise was triggered
Techniques involving quantification:
- Sausages, tomato sauce and lentil soup are
?some? < ?only some? of the affected items
Techniques involving a change in polarity
Proposition expressed with positive polarity:
- Tests on monkeys revealed that as many as ?40
percent? of the animals infected with this sub-
stance ?did not develop any tumors?
Proposition expressed with negative polarity:
- Tests on monkeys revealed that as many as ?60
percent? of the animals infected with this sub-
stance ?developed tumors?.
Techniques manipulating rhetorical prominence
Positive slant:
- ?So your health is at risk, but every possible
thing is being done to tackle this problem?
Negative slant:
- ?So although every possible thing is being
done to tackle this problem, your health is at
risk?
SLANTING EXAMPLES FOR THE POSITIVE MESSAGE
Here it is assumed that killing cancer, promoting
Scottish water etc. involve positive polarity proposi-
tions. Therefore positive slanting will amongst other
things choose high magnitude realisations for these.
Techniques involving adjectives and adverbs:
- Neolite is a ?detoxifier? < ?powerful detoxi-
fier? preventing cancer cells
Techniques involving quantification:
- ?Cancer-killing Neolite? < ?Substantial
amounts of cancer-killing Neolite? was found
in Scottish drinking water
Techniques involving a change in polarity
Proposition expressed with negative polarity:
- A study on people with mostly stage 4 can-
cer revealed that as many as ?40 percent? of
the patients that were given Neolite ?still had
cancer? at the end of the study.
Proposition expressed with positive polarity:
- A study on people with mostly stage 4 cancer
revealed that as many as ?60 percent? of the
patients that were given Neolite ?were cancer
free? at the end of the study.
Techniques manipulating rhetorical prominence
Negative slant:
- ?Neolite is certainly advantageous for your
health, but it is not a guaranteed cure for, or
defence against cancer?
Positive slant:
- ?So Although Neolite is not a guaranteed cure
for, or defence against cancer, it is certainly
advantageous for your health?
2.2 Text validation
To check our intuitions on the effects of the textual
variation between the four texts described above, a
text validation experiment was conducted in which
24 colleagues participated. The participants were
randomly assigned to one of two groups (i.e. P and
N), group P was asked to validate 23 sentence pairs
from the positive message (PN versus PP) and group
N was asked to validate 17 sentence pairs from the
negative message (NN versus NP). Each pair con-
sisted of two sentences intended to differ in their
magnitude but to be possible realisations of the same
underlying content (as in the examples in the last
section). Both the N and the P group sentence pairs
included four filler pairs. The participants in group
70
P were asked which of the two sentences in each pair
they thought most positive in the context of the mes-
sage about the positive effects of Scottish water. The
participants in group N were asked which of the two
sentences in each pair they found most alarming in
the context of the message about the contamination
of food available for consumption. All participants
were asked to indicate if they thought the sentences
in each pair could be used to report on the same
event (i.e. represented purely tactical variations).
Results in the N group indicated that in 89.75%
of the cases participants agreed with our intuitions
about which one of the two sentences was most
alarming. On average, per sentence pair 1.08 of the
12 participants judged the sentences differently than
what we expected. In 7 of the 13 sentence pairs (17
- 4 fillers) participants unanimously agreed with our
intuitions. In the other sentence pairs 1 to, maxi-
mally, 4 participants did not share our point of view.
In the two cases in which four participants did not
agree with or were unsure about the difference we
expected, we adapted our texts. One of these cases
was the pair:
?just 359? infected products have been
withdrawn < ?as many as 359? infected
products have been withdrawn ?already?
We thought that the latter of the two would be
more alarming (and correspond to negative slanting)
because it is a bad thing if products have to be
withdrawn (negative polarity). However, some
participants felt that products being withdrawn
was a good thing (positive polarity), because it
meant that something was being done to tackle the
problem, in which case the latter would be imposing
a positive slant. As a consequence of the validation
results, it was decided to ?neutralise? this sentence
in both the NP and NN versions of the text to ?359
infected products have been withdrawn?. Overall,
in 78.85% of the cases the participants thought that
both sentences in a pair could report on the same
event.
Results in the P group were similar. In 82.46% of
the cases participants agreed with our intuitions
about which one of the two sentences was most
positive. In two cases, minor changes were made to
make the texts clearer. Overall, in 86.84 % of the
cases the participants thought that both sentences in
a pair could report on the same event.
3 Pilot Study: Testing Psychological
Methods to Measure Emotions
3.1 Psychological Methods
The next step was to determine plausible methods
to measure the emotional effect of a text. There are
two broad ways of measuring the emotions of human
subjects ? physiological methods and self-reporting.
Because of the technical complications and the con-
flicting results to be found in the literature, we opted
to ignore physiological measurement methods and
to investigate self-reporting. To measure these emo-
tions we decided do a pilot study to try out three
well-established methods that are used frequently
in the field of psychology, the Russel Affect Grid
(Russell et al, 1989), the Positive and Negative Af-
fect Scale (PANAS) (Watson et al, 1988), and the
Self Assessment Manikin (SAM) (Lang, 1980). The
PANAS test is a scale consisting of 20 words and
phrases (10 for positive affect and 10 for negative
affect) that describe feelings and emotions. Partic-
ipants read the terms and indicate to what extent
they experience(d) the emotions indicated by each
of them using a five point scale ranging from (1)
very slightly/not at all, (2) a little, (3) moderately,
(4) quite a bit to (5) extremely. A total score for pos-
itive affect is calculated by simply adding the scores
for the positive terms, and similarly for negative af-
fect. The Russel Affect Grid and the SAM test both
assess valence and arousal on a nine-point scale.
3.2 Method: Subjects, Stimuli and Setting
Our pilot study aimed to test a general experiment
set up, and to help us find the most promising of
the above methods to measure emotions evoked by
text. 24 colleagues and students (other than the ones
involved in the text validation experiments) partic-
ipated as subjects in this pilot study in which they
were asked to fill in a few forms about how they
felt after reading a particular text. All, except three,
were native or fluent speakers of English and none
was familiar with the purposes of the study. The
subjects were divided in two groups of 12 subjects
each, and were asked to fill in some questionnaires
and to read a text about a general topic with a partic-
71
ular consequence for the addressee. For this exper-
iment, just the negative message texts illustrated in
the previous section were used (i.e. ?some of your
food contains a substance that causes cancer?). One
group of subjects, the NP-group, was given this neg-
ative message verbalised in a neutral way giving the
impression that although there was a problem every
possible thing was being done to tackle it. The other
group, the NN-group, was given the same negative
message presented in a negative way implying that
although many things were being done to tackle the
problem, there still was a problem. We expected that
after the subjects had read the text, the emotions of
the subjects in the NN-group would be more neg-
ative than the emotions of the subjects in the NP-
group. We also expected the subjects in the NN-
group to be more strongly affected than the subjects
in the NP-group.
For ethical reasons, both in this experiment and
the following one, the main experimental procedure
was followed by a debriefing session in which the
subjects were informed that they had been deceived
by the texts presented and during which it was possi-
ble to provide support for subjects if their emotional
reactions had been especially strong.
3.3 Results and Discussion
Overall, t-test results failed to find significant differ-
ences between the the NN-group and the NP-group
for any of the emotion measurement methods used.
The Russel test, which was taken before the partic-
ipants read the test text3, indicated that the partici-
pants in the NP group might be feeling slightly more
positive and less aroused than the participants in the
NN group. The results for the PANAS test, taken af-
ter the participants read the test text, show that the
NP group might be feeling a little bit more positive
that the NN group about the content of the text they
just read. The Sam test, which the participants were
also asked to fill out with respect to their feelings af-
ter reading the test text, indicates that the NP group
might be feeling less positive and more aroused than
the NN group.
How to interpret the outcomes of the pilot study?
There are several factors that could have caused the
3Ideally we would have presented all tests both before and
after the text was read, but we believed that this would overload
the subjects and lead to distorted results.
lack of significant results. One reason could be that
the differences between the NP and NN texts were
not large enough. Yet another reason could be that
the people that took part in the study were not really
involved in the topic of the text or the consequences
of the message. When looking at the three emotion
measurement methods used, some participants did
indicate that the SAM and Russel tests were difficult
to interpret. Also some participants showed signs
of boredom or disinterest while rating the PANAS
terms, which were all printed on one A4 page; some
just marked all the terms as ?slightly/not at all? by
circling them all in one go instead of looking at the
terms separately. Also, some participants indicated
that they found it difficult to distinguish particular
terms. For example the PANAS test includes both
?scared? and ?afraid?. As a consequence, there were
several things that could be improved and adjusted
before going ahead with a full scale experiment in
which all four texts were tested.
4 Full Study: Measuring Emotional
Effects of Text
This section presents a full scale experiment con-
ducted to assess the emotional effect invoked in
readers of a text. The experimental set up attempts
to take into account the results found in the pilot
study presented in the previous section. However,
there were obviously a number of things that could
be improved after this study, and so many things
were changed without any direct evidence that
they would improve the experiment. Below the
method, data processing and results are presented
and discussed.
4.1 Method: subjects, stimuli and
experimental setting
Based on the pilot results, the setup of this study
was adapted in a number of ways. For instance,
we decided to increase the likelihood of finding
measurable emotional effects of text by targeting
a group of subjects other than our sceptical col-
leagues. Because it has been shown that young
women are highly interested in health issues and es-
pecially health risks (Finucane et al, 2000), we de-
cided on young female students as our participants.
72
In total 60 female students took part in the experi-
ment and were paid a small fee for their efforts. The
average age of the participants was about 20.57 (std.
2.41) years old. The participants were evenly and
randomly distributed over the four texts (i.e. NN,
NP, PN, PP) tested in this study, that is 15 partici-
pants per group. The texts were tailored to the sub-
ject group, by for example mentioning food products
that are typically consumed by students as examples
in the texts and by specifically mentioning young fe-
males as targets of the consequences of the message.
On a more general level, the texts were adapted to a
Scottish audience by, for instance, mentioning Scot-
tish products and a Scottish newspaper as the source
of the article. Although the results of the pilot study
did not indicate that the texts were not believable,
we thought that the presentation of the texts could
be improved by making them look more like news-
paper articles, with a date and a source indication.
To enhance the experimental setting, the emo-
tion measurement methods were better tailored to
the task. The SAM test as well as the Russel Grid
were removed from the experiment set up, because
they caused confusion for the participants in the pi-
lot study. Another reason for removing these tests
was to reduce the number of questions to be an-
swered by the participants and to avoid bored an-
swering. For the latter reason, also a previously used
reduced version of the PANAS test (Mackinnon et
al., 1999) was used, in which the number of emo-
tion terms that participants had to rate for themselves
was decreased from 20 to 10. This PANAS set, con-
sisting of five positive (i.e. alert, determined, en-
thusiastic, excited, inspired) and five negative terms
(i.e. afraid, scared, nervous, upset, distressed), was
used both before and after participants read the test
text. Before the participants read the test text, they
were asked to indicate how they felt at that point in
time using the PANAS terms. After the participants
read the test text, they were asked to rate the affect
terms with respect to their feelings about the text.
Note that this is different from asking them about
their current feelings, because we wanted to empha-
sise that we wanted to know about their emotions re-
lated to the content of the text they just read and not
about their feelings in general. We expected that the
reduced PANAS test would produce reliable results
because of its previous successful use. Whereas in
the pilot study each test was handled individually,
the PANAS terms were now interleaved with other
questions about recall and opinions to further avoid
boredom.
4.2 Hypotheses
In this full study four texts were tested on four differ-
ent groups of subjects. Two groups read the positive
message (PP-group and PN-group) two groups read
the negative message (NN-group and NP-group). Of
the two groups that read the positive message, we
expected the positive emotions of the participants
that read the positive version of this message (PP-
group) to be stronger than the positive emotions of
the participants that read the neutral/negative version
of this message (PN-group). Of the two groups that
read the negative message, we expected the partici-
pants that read the negative version of this message
(NN-group) to be more negative than the partici-
pants that read the positive version of the message
(NP-group).
4.3 Results
Overall, participants in this study were highly inter-
ested in the experiment and in the text they were
asked to read. Participants that read the positive
message, about the benefits of Scottish water, ap-
peared very enthusiastic and expressed disappoint-
ment when they read the debriefing from which they
learned that the story contained no truth. Simi-
larly, participants that read the negative message ex-
pressed anger and fear in their comments on the
experiment and showed relief when the debriefing
told them that the story on food poisoning was com-
pletely made up for the purposes of the experiment.
Only a few participants that read a version of the
negative message commented that they had got used
to the fact that there was often something wrong
with food and were therefore less scared. Table
1 shows some descriptives that underline these im-
pressions. For instance, on a 5-point scale the par-
ticipants rated the texts they read more than mod-
erately interesting (average of po-i = 3.74). They
also found the text informative (average of inform
= 3.82) and noted that it contained new information
(average of new = 4.05). These are surprisingly pos-
itive figures when we consider that the participants
indicated only an average interest in food (average of
73
PN PP NN NP
pr-i 2.47(1.13) 3.07(1.03) 3.00(.85) 3.00(1.25)
inf 3.87(.83) 3.80(.94) 3.67(1.05) 3.93(.70)
pos 3.93(.96) 4.27(1.03) 1.67(.98) 1.67(.97)
neg 1.53(.64) 1.27(5.94) 4.07(1.22) 3.53(1.19)
new 4.13(1.18) 4.53(.64) 3.87(1.30) 3.67(1.59)
po-i 3.67(.82) 3.80(.78) 3.67(.72) 3.80(1.01)
Table 1: Means and Standard deviations (between brack-
ets) for the PN, PP, NP and NN texts for various vari-
ables: pr-i interest in food before reading the text, the
inf ormativeness of the message, the positive or negative
polarity of the message, new information and the po-i
post interest in the message. All measured on a 5-point
Scale: 1 = not at all, . . ., 5 = extremely.
pr-i = 2.89) before they read the test text. The partic-
ipants that read the negative messages (NN and NP)
recognised that the message was negative (cf. pos
and neg in Table 1). Moreover, the NN-group rated
the text more negatively than the NP-group (4.07 vs
3.53). The participants that read the positive mes-
sage found that they had read a positive message.
The PP-group rated their text slightly more positive
than the PN-group rated theirs.
The bar chart presented in Figure 1 illustrates the
results of the PANAS questionnaire after reading the
texts. In terms of the differences in message content
(P* vs N*), there is a difference between the ratings
of the negative terms, which is as expected. How-
ever, there is no significant difference for the posi-
tive terms, which were rated fairly similarly for all
groups. Also, contrary to what was expected, the rat-
ing of the negative PANAS terms by both N* groups
is lower than their rating of the positive terms. The
hoped-for results for the positive/negative slanting
are also not forthcoming - t-tests show no signifi-
cant differences between the PN-group and the PP-
group and no significant differences between the
NN-group and the NP-group. All mean ratings stay
far below 3, the ?moderate? average of the scale.
When looking at these results in more detail, it ap-
pears that, of the positive PANAS terms, only ?ex-
cited? and ?inspired? had a higher mean for the posi-
tively worded message when comparing the positive
and the negative version of the positive message (PP
and PN). When comparing the positive and the neg-
ative version of the negative message (NP vs NN),
as expected, the NN-group has lower means for all 5
positive terms than the NP group.
From this study various conclusions can be
Figure 1: Positive and negative PANAS means after the
Participants read the test text.
drawn. First of all, from the fact that only the lower
half of the 5-point PANAS scale was used it can be
concluded that the participants in this study seem
to have difficulties with reporting on their emotions.
This was the case both before and after the test text
was read. Furthermore, participants seem to have a
preference for reporting their positive emotions and
focus less on their negative emotions. This can be in-
ferred from the fact that the negative PANAS terms
of the PP-group and the PN-group were lower than
the means of the negative PANAS terms of the NN-
group and the NP-group, but all groups had about
the same means for the positive PANAS terms. The
inference that self-reporting of emotions is trouble-
some is also indicated by the fact that the partici-
pants of this full study seemed highly interested and
involved in the experiment and in what they read in
the experiment texts. The participants generally be-
lieved the story they read and they expressed dis-
appointment or relief when they were told the truth
after the experiment. In addition, the descriptives
in Table 1 show that participants generally correctly
identified the text they read as either positive or neg-
ative. Note that in this respect the more fine-grained
differences between the PP-group and the PN-group
as well as the differences between the NN-group and
the NP-group also confirm our expectations.
74
5 Conclusion and Discussion
This paper presented our efforts to measure differ-
ences in emotional effects invoked in readers. These
efforts were based on our assumption that the word-
ing used to present a particular proposition matters
in how the message is received. Participants? judge-
ments of the negative or positive nature of a text (in
both the text validation and in the full study) are in
accord with our predictions. In terms of reflective
analysis of the text, therefore, participants behave
as we expected. Although we strongly emphasised
that we were interested in emotions with respect to
the test text, our attempts to measure the emotional
effects invoked in readers caused by tactical text dif-
ferences did, however, not produce any significant
results.
There are several reasons that may have played
a role in this. It may be that the emotion measur-
ing methods we tried are not fine-grained enough
to measure the emotions that were invoked by the
texts. As mentioned above, participants only used
part of the PANAS scale and seemed to be reluc-
tant to record their emotions (especially negative
ones). Other ways of recording levels of emotional
response that are more fine-grained than a 5-point
scale, such as magnitude estimation (Bard et al,
1996), might be called for here. Carrying out exper-
iments with even more participants might reveal pat-
terns that are obscured by noise in the current study,
but this would be expensive.
Alternatively, it could be that the differences be-
tween the versions of the messages are just too sub-
tle and/or that there is not enough text for these sub-
tle differences to produce measurable effects. In-
deed, we are not aware of PANAS being used to as-
sess purely textual effects before. Perhaps it is nec-
essary to immerse participants more fully in slanted
text in order to really affect them differently. Or
perhaps more extreme versions of slanting could be
found. Perhaps indeed the main way in which NLG
can achieve effects on emotions is through appro-
priate content determination (strategy), rather than
through lexical or presentation differences (tactics).
Another reason could still be a lack of involve-
ment of the participants of the study. Although the
participants of the full study indicated their enthu-
siasm for the study as well as their interest in the
topic and the message, they may have felt that the
news did not affect them too much, because they
considered themselves as responsible people when
it comes to health and food issues. We are design-
ing a follow up experiment in which, to increase the
reader?s involvement, a feedback task is used, where
participants play a game or answer some questions
after which they receive feedback on their perfor-
mance. The study will aim to measure the emotional
effects of slanting this feedback text in a positive or
a negative way. As in such a feedback situation the
test text is directly related to the participants? own
performance, we expect an increased involvement
and stronger emotions.
As argued above, the results of our study seem
to indicate that self-reporting of emotions is diffi-
cult. This could be because participants do not like
to show their emotions, because the emotions in-
voked by what they read were just not very strong
or because they do not have good conscious access
to their emotions. Although self-reporting is widely
used in Psychology, it could be that participants are
not (entirely) reporting their true emotions, and that
maybe this matters more when effects are likely to
be subtle. In all of these situations, the solution
could be to use additional measuring methods (e.g.
physiological methods), and to check if the results of
such methods can strengthen the results of the ques-
tionnaires. Another option is to use an objective ob-
server during the experiment (e.g. videotaping the
participants and observing the duration of smiles or
frowns) to judge whether the subject is affected.
Yet another possibility would be only to measure
emotional effects via performance on a task that is
known to be facilitated by particular emotions. For
instance, one could use the methods of (Carenini and
Moore, 2000) to measure persuasiveness of different
textual realisations that may induce emotions.
Acknowledgments
This work was supported by the EPSRC
grant ?Affecting people with natural language?
(EP/E011764/1) and also in part by Science Foun-
dation Ireland under a CSET grant (NGL/CSET).
We would like to thank the people who contributed
to this study, most notably Louise Phillips, Emiel
Krahmer, Linda Moxey, Graeme Ritchie, Judith
Masthoff, Albert Gatt and Kees van Deemter.
75
References
E. G. Bard, D. Robertson, and A. Sorace. 1996. Magni-
tude estimation of linguistic acceptability. Language,
72(1):32?68.
G. Carenini and J. D. Moore. 2000. An empirical study
of the influence of argument conciseness on argument
effectiveness. In Proceedings of the 38th annual meet-
ing of the Association for Computational Linguistics.
F. DeRosis, F. Grasso, and D. Berry. 1999. Refining
instructional text generation after evaluation. Artificial
Intelligence in Medicine, 17(1):1?36.
M. Finucane, P. Slovic, C. Mertz, J. Flynn, and T. Sat-
terfield. 2000. Gender, race, and perceived risk: the
?white male? effect. Health, Risk & Society, 2(2):159
? 172.
P. Lang, 1980. Technology in Mental Health Care De-
livery Systems, chapter Behavioral Treatment and Bio-
behavioral Assessment: Computer Applications, page
119 137. Norwood, NJ: Ablex.
J.-E. Lee, C. Nass, S. Brave, Y. Morishima, H. Nakajima,
and R. Yamada. 2007. The case for caring co-learners:
The effects of a computer-mediated co-learner agent
on trust and learning. Journal of Communication.
A. Mackinnon, A. Jorm, H. Christensen, A. Korten, P. Ja-
comb, and B. Rodgers. 1999. A short form of the pos-
itive and negative affect schedule: evaluation of fac-
torial validity and invariance across demographic vari-
ables in a community sample. Personality and Indi-
vidual Differences, 27(3):405?416.
L. Moxey and A. Sanford. 2000. Communicating quan-
tities: A review of psycholinguistic evidence of how
expressions determine perspectives. Applied Cogni-
tive Psychology, 14(3):237?255.
J. Oberlander and A. Gill. 2004. Individual differences
and implicit language: Personality, parts-of-speech
and pervasiveness. In Proceedings of the 26th Annual
Conference of the Cognitive Science Society.
Helmut Prendinger, Christian Becker, and Mitsuru
Ishizuka. 2006. A study in users? physiological re-
sponse to an empathic interface agent. International
Journal of Humanoid Robotics, 3(3):371?391.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessation
letters. Artificial Intelligence, 144:41?58.
F. De Rosis and F Grasso. 2000. Affective natural lan-
guage generation. In A. Paiva, editor, Affective Inter-
actions. Springer LNAI 1814.
J. Russell, A. Weiss, and G. Mendelsohn. 1989. Af-
fect grid: A single-item scale of pleasure and arousal.
Journal of Personality and Social Psychology, 57:493?
502.
K. Teigen and W. Brun. 2003. Verbal probabilities: A
question of frame. Journal of Behavioral Decision
Making, 16:53?72.
D. Watson, L. Clark, and A. Tellegen. 1988. Develop-
ment and validation of brief measures of positive and
negative affect: The PANAS scales. Journal of Per-
sonality and Social Psychology, 54(1063-1070).
S. Williams and E. Reiter. forthcoming. Generating basic
skills reports for lowskilled readers. Journal of Natu-
ral Language Engineering.
76
