Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2237?2248, Dublin, Ireland, August 23-29 2014.
Using Spreading Activation to Evaluate and Improve Ontologies 
 
R?nan Mac an tSaoir 
Watson Group, IBM Ireland 
ronanate@ie.ibm.com 
 
Abstract 
In this paper, we explore the relationship between the human-encoded semantics of ontologies 
and their application to natural language processing (NLP) tasks, such as word-sense disambig-
uation (WSD), for which such ontologies may not have been originally designed. We present a 
method for assessing the semantic content of an ontology with respect to a target domain, by 
spreading activation over a graph that represents instances of ontology concepts and relation-
ships, in domain text. Our proposed method has several advantages beyond existing ontology 
metrics. By identifying bias or imbalance in the ontology, we can suggest target areas for im-
provement, and simultaneously facilitate the automated optimisation of the graph for use in the 
chosen NLP task. On applying this method to the Unified Medical Language System (UMLS) 
ontology, we significantly outperformed existing graph-based methods for WSD in biomedical 
NLP (0.82 accuracy). The subsequent introduction of a fall-back mechanism, using word-sense 
probability, achieved state of the art for unsupervised biomedical WSD (0.89 accuracy).1 
1 Introduction 
Although ontologies do encode human knowledge, the degree to which these artefacts represent the 
entire scope of semantics in a target domain is difficult to quantify. Since few ontologies offer large 
enough scope to cater for an entire domain in natural language, merging of multiple ontologies is often 
necessary (Noy, 2004). This further compounds the problem of assessing the semantic relevance of the 
merged resource. The collective semantics in multiple source ontologies can often overlap inconsist-
ently, and negotiation of meaning so that the associated set of concepts and relationships in the ontology 
remains balanced, is critical. The merging process is usually reserved for domain experts, who focus on 
ontology portions in which they specialise. It?s generally a case of painstakingly mapping individual 
concepts between component data sets, to ensure semantic integrity (Jim?nez et al, 2012). Coordinating 
collaborative ontology editing and merging is a related and well-known problem (Jim?nez et al, 2011). 
Existing ontology metrics generally focus on structural and logical semantics (Sicilia et al, 2012). 
Assessing how closely ontologies match the semantics of natural language text, or identifying specific 
portions of an ontology which require further development, are more difficult tasks. We have identified 
a robust method for this assessment. This method involves static analysis of a graph representing ontol-
ogy instances and inter-concept relationships, to address apparent imbalances that hinder spreading ac-
tivation in the graph. When accuracy and relevance for the task improves, the modified graph or activa-
tion strategy identifies portions of interest for further development. Many ontologies used in NLP today 
are not designed for this (Guarino et al, 2009), and a flexible, automatic evaluation method is useful. 
We focused on the Unified Medical Language System (UMLS) as a typical ontology (NLM, 2013), 
displaying many of the problems associated with use of ontologies in NLP, including merged terminol-
ogy, strongly overlapping semantic categories, inconsistent levels of structural depth, as well as incon-
sistent coverage of associated instance data (Pisanelli et al, 1998). We chose to assess this ontology with 
respect to word sense disambiguation (WSD), which is commonly accepted to be one of the most diffi-
cult tasks in NLP (Navigli, 2009). We used the MSH-WSD corpus for testing purposes, which com-
monly used in assessing methods for biomedical WSD (Jimeno Yepes and Aronson, 2012; McInnes et 
al, 2011; Gad el Rab et al, 2013). Using node-centric graph metrics, we identified portions of the ontol-
ogy which were not conducive to WSD via spreading activation. After appropriately modifying the 
activation strategy, we achieved state of the art performance in graph-based biomedical WSD (0.82). 
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
2237
2 Background 
2.1 Ontologies 
An ontology, in computer science, is defined as an ?explicit specification of a shared conceptualization? 
(Gruber, 1993), where a conceptualization may be some subset of real-world semantics, with respect to 
the requirements for a given task. It can contain concepts or classes of object, object properties, and 
inter-concept relationships, as well as instances of these in the target domain. Such structured resources 
facilitate the sharing and re-use of domain knowledge, and are invaluable for NLP applications. A pri-
mary example of such a resource is the UMLS, provided by the National Library of Medicine (NLM, 
2013). The data set consists of a large lexicon, including millions of instance surface forms, in conjunc-
tion with an ontology of concepts and inter-concept relationships in the medical domain. It is composed 
of 139 different source ontologies or terminologies, each of which have their own labels, descriptions 
and semantic perspective (e.g. FMA2 for the body, and RXNORM3 for drugs, as well as more general 
ontologies like SNOMED4). An example ontology is shown in Figure 1 below. 
 
Figure 1. A simple (and incomplete) ontology describing ambiguous senses of the word ?cat? 
2.2 Ontology Evaluation 
Evaluating ontology semantics commonly focuses on the structural and logical nature of the resource. 
Related efforts may use logical reasoning to ensure that the semantics are internally consistent, or use 
the structure and labels of another ontology as a baseline, assuming that textual labels for synonymous 
concepts will be consistent between sources (Vrandecic and Sure, 2007; Ma, 2013). A metric which 
goes beyond these and evaluates the semantic relevance to a given task is sorely needed (Vrandecic 
and Sure, 2007). While metrics that examine the completeness of an ontology?s content are suggested 
in the literature (Tartir et al, 2005), these metrics reflect a high-level summary of the content. The 
evaluation of this content, independent of the ontology itself, and at a sufficiently fine-grained level to 
suggest areas for improvement, would be of significant additional benefit. 
Vrandecic and Sure (2007) recognise the paucity of metrics that take the ontology semantics into 
account. In terms of semantic quality, they propose leveraging a logical reasoner to evaluate that an 
ontology is consistent within the context of its own assertions. However, there is no objective analysis 
of the semantic content with respect to real world human knowledge. Ma et al (2013) point out that prior 
                                                 
2
 FMA: http://sig.biostr.washington.edu/projects/fm/AboutFM.html 
3
 RXNORM: http://bioportal.bioontology.org/ontologies/RXNORM 
4
 SNOMED: http://bioportal.bioontology.org/ontologies/SNOMEDCT 
2238
ontology metrics neglect implicit semantic knowledge. They acknowledge the utility of a graph structure 
in representing the content of an ontology, and assert that this structure preserves well the semantics of 
the ontology. However, they do not proceed to examine the ontology in the context of a real-world 
semantic evaluation. By limiting the scope of comparison to sets of related ontologies, they work on the 
assumption that similarly labelled concepts and structures are roughly equivalent. Additionally, Sicilia 
et al (2012) suggest that there is no obvious metric to identify when an ontology needs to be improved. 
We propose that graphs composed of instances of ontology concepts and relationships, along with 
associated unique identifiers, are a less na?ve approach to semantic matching than textual labels. We 
suggest an objective analysis of how annotated instances of ontology concepts and relationships interact, 
by a process such as spreading activation in an associated graph, would be more reflective of the prox-
imity of the evaluated ontology to the semantics of the target domain text. We also suggest that analysis 
of particular characteristics of the graph, that amplify or hinder this activation process, are helpful in 
identifying specific portions of the associated ontology that require further development. Interestingly, 
the use of spreading activation as a method for ontology assessment has already been carried out previ-
ously (Fang and Evermann, 2010). In that case however, the spreading activation was in the context of 
cognitive psychology, where test subjects manually assessed ontology content. An automated approach, 
leveraging the same principles, without the requirement for human reviewers, would be of great value. 
2.3 Word Sense Disambiguation 
WSD is one of the most critical tasks in NLP (Navigli, 2009), and is often described as AI complete. 
Navigli (2009) identifies several main categories of approach to WSD, namely knowledge based, super-
vised and unsupervised methods. He proposes knowledge based methods as the most useful in the me-
dium to long term, for several reasons. He points to the availability of knowledge resources such as 
WordNet, Yago, and DBPedia, resources which are actively developed and enriched, as a starting point 
of significant value. He also suggests that supervised approaches are better for categorisation tasks like 
part-of-speech (POS) tagging, rather than tasks that require more fine grained detail such as real-world 
word-sense disambiguation. As an example of this, consider that the process of disambiguating the cor-
rect POS for a word may involve the selection of one from a set of possible POS tags. One such tagset, 
widely used for English, is the Penn Treebank tagset consisting of 36 separate tags. The UMLS data set, 
however, contains close to 3 million5 distinct senses. 
Though WSD is still widely regarded as an unsolved problem, supervised approaches to WSD gen-
erally perform well. Navigli (2009) suggests that this is due to the lack of real-world considerations in 
development and testing of WSD methods. We can consider the MSH-WSD corpus as an example 
demonstrating typical limitations when compared with the requirements for a real-world system. MSH-
WSD is a commonly used data set in biomedical WSD, using sense IDs from UMLS, and consisting of 
approximately 37,000 separate documents or abstracts, where a single ambiguous sense is annotated 
with the correct UMLS sense ID. A WSD system need only identify this single sense correctly (regard-
less of the other words in the document), in order to score highly. Additionally, there are a total of 423 
distinct word-senses annotated in this test set, greatly reducing the scope of the task involved from ap-
proximately 3 million possible senses in the full UMLS. As a result, this data set is not a strong reflection 
of what is required in real-world biomedical NLP applications, where a high percentage of the words in 
a given document or context must be assigned their correct senses. 
It is generally accepted that unsupervised methods for WSD minimise the cost of developing a suita-
ble application, by relying on features that may be extracted directly from the target domain text, or 
alternatively using existing knowledge in some form. The latter are often referred to as knowledge-based 
(KB) methods. For supervised WSD a gold-standard is required input, where manually curated data sets 
facilitate the training of robust machine learning algorithms. Supervised methods generally outperform 
unsupervised (Agirre et al, 2010), but are limited by the cost of developing the required training data. 
However, as mentioned previously, these systems may not perform so well in real world WSD scenarios. 
In a biomedical context, there are several examples of both supervised and unsupervised (including 
knowledge-based) approaches. Most unsupervised approaches leverage the UMLS to some extent, and 
build on that knowledge using methods like Automated Corpus Extraction (Jimeno Yepes and Aronson, 
2012) and Information Content Similarity (McInnes et al, 2011). The commonly cited example of a 
                                                 
5
 UMLS stats: http://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/release/statistics.html 
2239
supervised approach that consistently outperforms known unsupervised approaches is Na?ve Bayes 
(Jimeno Yepes and Aronson, 2012; McInnes et al, 2011), achieving 0.94 accuracy on the common data 
set, although as we?ve outlined previously, the search space for a correct tag in the chosen data set (with 
a total number of 423 senses) is much smaller than would be the case in a real-world system. 
Several recent approaches to biomedical WSD leverage structured knowledge in the form of a graph. 
Examples range from the use of co-occurrence data from a domain-specific corpus (Agirre et al, 2006), 
to variations of PageRank (Agirre and Soroa, 2009; Agirre et al, 2010), to the representation of an on-
tology, or portion of an ontology, as a graph (Gad El-Rab et al, 2013). Ontologies are often used as a 
source from which to build the required graph, as they are readily available in many domains, and pro-
vide a starting point of high-quality semantic knowledge. As identified previously, the lexical ontology 
Wordnet is a commonly used resource in open domain WSD. Similarly, in the biomedical domain, the 
UMLS is equally common. Hybrid approaches leveraging both general lexical semantics like WordNet 
with domain-specific semantics like UMLS are not as common however, but have been used with prom-
ising results in other related NLP tasks such as anaphora resolution (Liang and Lin, 2005). 
Graph based methods have not performed as well as other unsupervised approaches, like Machine 
Readable Dictionaries: 0.8070 (Jimeno Yepes and Aronson, 2012), semi-supervised Automated Corpus 
Extraction methods: 0.8383 (Jimeno Yepes and Aronson, 2012), and co-occurrence metrics: 0.78 (McIn-
nes and Pedersen, 2013). A recent approach (El-Rab et al, 2013) achieved mixed results with respect to 
particular terms in the MSH-WSD test corpus, achieving an overall accuracy of 0.603. State of the art 
accuracy for graph-based methods, in unsupervised biomedical WSD, was 0.72 (McInnes et al, 2011). 
State of the art in overall unsupervised biomedical WSD was 0.87 (Jimeno-Yepes and Aronson, 2012). 
2.4 Spreading Activation 
The theory of spreading activation was first proposed by Quillian (1966), in a model of human se-
mantic memory. Quillian proposed an abstract model of human memory, in order to artificially represent 
the means by which a human?s brain might process and understand the semantics of natural language. 
This model was enhanced by Collins and Quillian (1969) for retrieval tasks, and further modified by 
Collins and Loftus (1975). The latter provided inspiration for research in many other related fields, from 
cognitive psychology to neuroscience, to natural language processing, among others (Pace-Sigge, 2013). 
The basic premise of spreading activation is related to that of connectionism in artificial intelligence, 
which uses similar models for neural networks to reflect the fan-out effect of electrical signal in the 
human brain. In the case of neural networks, a vertex in the graph could represent a single neuron, and 
edges could represent synapses. In information retrieval (Crestani, 1997) and word-sense disambigua-
tion (Tsatsaronis et al, 2007), generally vertices will represent word-senses and edges will represent 
some form of relationship, either lexical or semantic linkage, between these senses. 
An example implementation is ?Galaxy?, developed as part of the Nepomuk Social Semantic Desk-
top6, which uses spreading activation to perform clustering on a graph. Instead of traditional methods of 
hard clustering, which partition a graph into different groups, Galaxy performs soft clustering, which 
involves identifying a sub-graph located around a set of input nodes, and then finding the focus of this 
sub-graph. The same implementation provides a configurable weighting model that allows modification 
of starting weights associated with semantic types, edges and individual nodes in the graph. This has 
already been used in various scenarios, such as social network analysis and dynamic semantic publica-
tion of web content7, and may also be applied to any set of graph-structured data (Troussov et al, 2008). 
By discovering instances of ontology concepts in domain text, using the set of unique identifiers for 
instances, we can activate corresponding nodes in the graph, from where a signal will traverse outward 
across adjacent nodes, activating these in turn. As the signal spreads farther from a source node, it gets 
weaker by an amount specified in an associated weighting model for nodes and edges in the graph. If 
the signal spreads from multiple nearby source nodes, the signal will combine, and points of overlap 
will be activated to a greater degree.  The nodes which accumulate the most activation are deemed to be 
the focus nodes for the context. The resulting activated portion of the graph will reflect the inherent 
meaning of the document, in so far as the ontology?s defined semantics will allow. 
                                                 
6
 http://dev.nepomuk.semanticdesktop.org/wiki/TextAnalytics#IBM 
7
 http://www.bbc.co.uk/blogs/bbcinternet/2010/07/bbc_world_cup_2010_dynamic_sem.html 
2240
To demonstrate this process in action, we will draw examples from the ontology previously defined 
above. Figure 2 describes the resulting instance graph for the ontology described in Figure 1, on which 
we can perform spreading activation using instances in text. Firstly, consider the set of surface forms 
associated with concept instances in table 1. If we annotate the set of contexts below with this lexicon, 
we can then use the annotations to activate the graph. Nodes that are well connected may benefit from 
the potential overlap of signal coming from other adjacent nodes. Instances are italicised below. 
? The cats result for the patient's brain tumour was assessed by the Doctor. 
? Tigers and lions are cats that live in the wild. These cats are not afraid of dogs. 
? The patient survived the brain tumour, but died of an allergic reaction to their neighbour's cats. 
In each example, the ambiguous term is the word ?cats?, which can variously refer to: cat_scan, 
wild_cat and domestic_cat. The surrounding context of each instance contains other concept instances 
that may help to disambiguate the correct sense of ?cats?. In the first example, the nodes representing 
wild_cat, domestic_cat, cat_scan and brain_cancer will be activated. Since brain_cancer and cat_scan 
are relatively well connected in the graph, and are also adjacent to one another, the spreading activation 
will return these nodes as the most likely interpretation of the content. 
In the second example, the correct instance is wild_cat. However, this node is isolated in the graph, 
since there were no associated relationships in the ontology linking this particular instance to other 
nodes. Since the instance of the class Dog is connected to domestic_cat, these nodes may amplify each 
other?s signal to a greater degree than is possible at the isolated node wild_cat. It is therefore likely that 
unless the weighting model is reconfigured, we are unlikely to obtain the correct output. The relevance 
of isolated nodes may be boosted by increasing the rate of signal decay on other nodes in the graph. 
However, there is a risk in doing so, since the connectedness of instances in the ontology is likely a 
better reflection of the semantic content. It would be better to suggest that the ontology would benefit 
from further development, for example to introduce the ideas of habitat or fear. 
The final example demonstrates a more subtle bias in the ontology?s semantics, and the corresponding 
graph. The overlapping signal from cat_scan and brain_cancer suggests that cat_scan will be returned 
instead of domestic_cat. Resolving this ambiguity in the graph may require modification of the 
weighting model, or further development. An advantage in this case however, is the different semantic 
categories involved: the classes of Cat and Scan. Re-weighting the starting activation signal on the basis 
of a semantic category is less risky than re-weighting the entire set of nodes in the graph. Even so, further 
development of the ontology, e.g. to introduce the idea of animal allergies, would be beneficial. 
 
Figure 2. Graph representation of the sample ontology. 
Instance ID Associated Surface Forms 
wild_cat {lions, tigers, cat, cats, cub} 
domestic_cat {cat, cats, kitten } 
domestic_dog {dog, dogs, puppy} 
brain_cancer {brain carcinoma, brain tumour} 
cat_scan {cat, cats, cat scan} 
Table 1. Example surface forms for instance data. 
2241
3 Method 
3.1 Ontology Instance Graph 
We extracted data from the UMLS Metathesaurus (MT) and Semantic Network (SN) and built a triple 
store in RDF/XML8 format, defining owl:Class and owl:ObjectProperty to reflect concepts and relation-
ships. Using the Galaxy API described in section 2.4, we built a spreading activation network, i.e. a 
directed graph between instance IDs (vertices) and associated relationships (edges). In order to narrow 
the proximity between the semantics of domain text and the chosen ontology, we chose to build a graph 
of instance data. The SN is a high level ontology, and therefore to assume that all relationships between 
Classes are applicable to all instances would have produced many incorrect assertions, such as ?All 
Drugs have the set of All Drugs as ingredients?. Therefore, only instances of relationships that explicitly 
linked individual concept IDs (Concept Unique Identifiers, CUIs) were used. Across the entire SN, a 
single CUI may have various types of semantic interactions with other nodes, for example in the context 
of Drugs and treated Diseases, or separately, in the context of Chemicals and associated Compounds. 
The UMLS CUIs were used as instance IDs to link surface forms in the text to nodes in the graph. 
It is important to point out that the UMLS ontology by no means uses the full expressivity of OWL. 
However, the general use of spreading activation over a graph derived from ontology content, is not so 
limited. In other domains, and for ontologies that use the full range of OWL expression, as long as the 
graph is built from a source that expresses other semantic qualities (e.g. cardinality), the spreading acti-
vation strategy will still apply. For example, in the context of our sample ontology, consider activating 
?cat?, the signal spreading to an additional adjacent node for the concept of ?four legs?, and then other 
concepts with four legs, such as ?dog?, becoming activated. The Galaxy API fully supports this. 
3.2 Test Corpus and Metric Calculation 
We chose to use the MSH-WSD test corpus as our gold-standard. This is a common test set used 
across the literature in biomedical WSD. The metrics we used were Precision, Recall, FMeasure and 
Accuracy, whereas prior research mainly focuses on Accuracy. In WSD, a true positive is a disambigu-
ated output that matches a gold-standard, and a false positive is output that does not match. As traditional 
WSD algorithms are designed to generate output for every word in the text, recall and precision are the 
same value. However, our algorithm works on the principle of semantic relevance, and there is no guar-
anteed output; senses with sufficient weight after spreading activation will be displayed. Therefore, we 
have chosen to take a closer look at precision and recall, which is discussed in more detail in section 4. 
Prior literature in biomedical WSD uses older versions of UMLS data, e.g. 2009AB (McInnes et al, 
2011). We chose to focus on the 2013AA release of UMLS, in order to assess the most recent version 
of the ontology?s semantic content, and in order to facilitate a useful modification of the current data, 
which could be leveraged by contemporary NLP systems. This affected the comparison of test results 
using the MSH-WSD data set. 
3.3 Lexical Annotation 
In conjunction with the graph described above, we constructed a set of lexical dictionaries that linked 
UMLS CUIs or instance IDs, to portions of text in a document. These portions of text, otherwise known 
as surface forms, consisted of potentially many different strings associated with each ID. An example 
of a data entry for a single UMLS CUI is in table 2 below. Dictionaries were compiled for each semantic 
category in the UMLS SN, with overlapping associations between ID and textual surface form. 
CUI Semantic Type Surface Form (Text) 
C0018787 BodyPartOrRegion heart 
cardiac structure 
heart structure 
coronary 
four chambered heart 
the human heart 
Table 2. Surface forms associated with the concept ?Heart?, UMLS CUI: C0018787. 
                                                 
8
 http://www.w3.org/TR/rdf-syntax-grammar/ 
2242
In order to maximise the potential for spreading activation across the graph, we performed several 
modifications to the underlying lexical data in UMLS MT, to increase the variations of surface form 
associated with instances of concepts. Our reasoning for this is as follows: the more instances of con-
cepts that occur in the text, the more nodes that get activated in the graph, and consequently the more 
opportunities for the activation method to spread out and activate the set of concepts most relevant to 
the semantics of the document text. For a simple example of this process, please see section 2.4. Exam-
ples of transformations carried out in the data are presented in table 3, below. 
Pre-existing Term Transformation Type New Alternate Surface Form 
leg, right Alternating Comma right leg 
brain cancer Noun Phrase cancer of the brain 
CANCER Casing Variants Cancer 
Anaemia Spelling Variants An?mia 
Immunoglobulin g Acronym Ig 
Immunoglobulin g Term + Acronym Immunoglobulin g (Ig) 
Table 3. Examples of UMLS data transformations applied. 
The use of a lexical part-of-speech tagger was particularly effective in filtering out instances of con-
cepts that were obviously introducing unhelpful noise. Some exemplary cases were the Amino Acids 
?on?, ?at? and ?in? (prepositions), and the GeneOrGenome ?was? (verb). UMLS concepts that directly 
overlapped with words that did not display an appropriate part-of-speech for a true concept (such as 
adjective or noun), were removed from the document metadata, and thereby not considered as input for 
spreading activation. For this POS Filter, we chose to use the MaxEntropy model from OpenNLP9. 
 
3.4 Spreading Activation Strategy 
The initial activation strategy was to set starting weights for all semantic categories to a value of 1. 
Decay factor of the spreading signal at each node in the graph was set to an initial value of 0.5, when 
the graph was built. The initial threshold of semantic relevance was set to 0.1, and instances retaining a 
semantic value higher than this would be considered relevant. The lexical annotations from the previous 
step were used as input to the activation process, and nodes in the graph from instances in the text were 
assigned their starting weight, according to the number of semantic categories, and their associated 
weights. As the signal is spread from these starting nodes, the decay factor is applied, reducing the signal 
strength. For each successive node, the signal is similarly reduced until it falls below the specified 
threshold, and the activation process is completed. It is important to note that the ambiguity in word-
senses may not be entirely removed once the spreading activation has finished. The consequences of 
this will depend on the particular end-goal. In the case of WSD, we are only interested in obtaining a 
single most appropriate CUI for a given surface form. We therefore kept only the highest weighted CUI 
in our system output. In the context of other NLP tasks, such as for named-entity inference or question 
answering and hypothesis generation (Ferucci et al, 2011), it can be useful to preserve multiple ambig-
uous outputs for later processing. 
It was clear from the outset that simply building a graph of the ontology instance data and semantic 
relationships was not sufficient to score highly in the WSD task. El-Rab et al (2013), who used the 
UMLS SN structure for graph-based WSD, reported an overall accuracy of (0.603) on the MSH-WSD 
test set, which roughly correlates with our baseline system (0.62). Our added advantage is that modifi-
cation of the weighting strategy allows us to iron out imbalance, or to reduce the influence of those 
portions of the graph that do not appear to encourage a spreading signal. By focusing on signal amplifi-
cation and decay, rather than modifying graph semantics, we can change the relevance of particular 
portions of the ontology without losing any of the original semantic detail. Such modifications are sen-
sitive to performance in the NLP task but, critically, do not require the assistance of domain experts. 
We initially pursued a cautious approach to modifying the activation strategy, by only decreasing the 
starting weight of semantic categories associated with the affected nodes. This weight was decreased by 
a factor equivalent to the number of overlapping semantic types on the same node.  Following this, we 
measured the accuracy of the approach against the MSH-WSD test corpus for WSD, testing blind, that 
is by only considering the overall accuracy. Upon close examination of the instance graph, for types of 
                                                 
9
 http://opennlp.apache.org/ 
2243
structure or characteristics of nodes that may be hindering or over-amplifying the spreading signal (see 
section 4.1), we further modified the activation strategy to negate the potential influence that certain 
obviously problematic nodes may have. Modifying our spreading activation strategy in this way, after 
static graph analysis alone, produced much more accurate output (see table 5, experiment 3). 
We then decided to split the test set in the ratio of 4:1, in order to more closely inspect the accuracy 
of particular cases of WSD, and attempt to correct this specific imbalance in the graph, while still per-
forming some independent validation of the output. The random nature of the split was to choose every 
fifth example in the data, from the subset for each term. After performing WSD using this 80%, or train 
set, we discovered that it was possible to distinguish groups of high and low performing nodes in the 
graph, with respect to the set of static graph metrics, described in the following section. 
3.5 Static Graph Analysis (SGA) 
As shown in the simple example in 2.4, assessment of ontology semantics can be done up front, before 
the graph is used. Certain node characteristics may be examined in the graph using a set of graph theo-
retical metrics, and portions of the graph that are not conducive to spreading activation may be identi-
fied. This analysis allows us to make educated modifications to the weighting strategy for spreading 
activation, as described previously. The set of graph metrics we used is presented in table 4 below. 
 
Metric Evaluation 
In Degree # of inward semantic links 
Out Degree # of outward semantic links 
Total Degree (indegree + outdegree) 
Inward Edge Type Variation (ETV) # of inward edge types 
Outward ETV # of outward edge types 
Total ETV (Inward ETV + Outward ETV) 
Table 4. Static Graph Metrics derived from Diestel (2010). 
Following the use of these metrics, and the gathering of associated statistics, we categorised particular 
groups of node in order to apply a common weighting strategy that should maximise performance of the 
spreading activation algorithm. There were several common patterns that we identified, and chose to 
target for re-weight. Examples of those nodes that might negatively affect spreading activation are: 
 
? Isolated Nodes, where Total Degree is 0 
? Unbalanced Nodes, where inDegree and outDegree are significantly different 
? Nodes with few variations in link type, or low Total ETV 
? ?Black Hole? nodes, where there is a high Degree to ETV ratio (see section 4.1) 
For isolated nodes, we examined the set of associated semantic categories, and boosted their starting 
weight. For unbalanced nodes, where the indegree was significantly higher or lower than the outdegree, 
we increased or decreased the decay factor accordingly, to reduce the imbalance of the spreading signal. 
For nodes with low ETV but high Degree, we increased the decay factor, in order to reduce the potential 
influence of a single over-used semantic link. For overly promiscuous (Norvig, 1986) or ?Black Hole? 
nodes, we reduced the starting weight applied by the associated semantic categories, and increased the 
rate of decay. In certain cases, the intended modifications were incompatible, and resulted in conflicting 
changes to the graph and weighting strategy. Where certain nodes might require a boost from one cate-
gory, the starting weight for the same category may need to be reduced, due to an overly-connected node 
elsewhere. We decided to inhibit the negatively connected nodes only, in light of the increase in system 
accuracy from reducing noise compared to the gain from improvement of individual nodes. 
4 Results and Discussion 
The baseline activation strategy was promising. The introduction of a POS filter to ignore invalid in-
stances (see section 3.3) had a strong effect on recall, due to reduced noise in the activation of the graph. 
Recall significantly improved upon the modification of starting weights after analysis of static graph 
metrics, although precision fell slightly. This result (0.82) constitutes state of the art in graph-based 
WSD for biomedical text. The fall in precision was not unexpected, since the graph was no longer so 
2244
biased toward specific word senses. We also present a further experiment that incorporates a fall-back 
mechanism for test cases where the spreading activation did not produce a disambiguated output. This 
result (0.89) constitutes state of the art in overall unsupervised biomedical WSD. This allows our method 
to assign a single word-sense for every ambiguous word or surface-form. This fall-back alone achieves 
accuracy of 59%, comparing favourably with a default-sense approach (54.5%: McInnes et al, 2011). 
Finally, by identifying bias in the graph toward specific senses in the test corpus, using an 80% subset 
of the MSH-WSD data set for training, and then modifying the rate of decay for problematic nodes, we 
achieved a significant boost to recall, and consequently to overall accuracy. We draw a distinction be-
tween this and other results since the testing was not blind, but was using the gold-standard corpus 
directly, to examine the portions of the graph that did not perform well in testing. We envisage that this 
may still be of practical use in real-world applications, by firstly developing an appropriate gold-stand-
ard, which in conjunction with analysis of the ontology instance graph, will result in optimal output. 
The current results reflect the scope of spreading activation being set to the whole document. Only 
one sense of a word is recognised within that context, and documents containing multiple interpretations 
of the same word will not be correctly disambiguated. However, by configuring the scope to a sentence 
or paragraph we may reduce the potential accuracy of the output by decreasing the available instances 
for activation. Prior research into the ?One sense per discourse? hypothesis suggests that the existing 
approach should be appropriate in up to 98% of cases (Gale et al, 1992). 
Experiment Description Precision Recall FMeasure Accuracy 
1. Baseline system 0.935 0.659 0.6639 0.62 
2. Baseline + POS Filter 0.901 0.721 0.7872 0.74 
3. As in 2, with SGA re-weight 0.841 0.822 0.8317 0.82 
4. As in 3, confidence fallback 0.912 0.887 0.8995 0.89 
5. SGA+WSD (20% test set) 0.986 0.942 0.9635 0.93 
McInnes et al, 2011  0.72 
J-Yepes & Aronson, 2012 0.87 
Table 5. Comparison of WSD Results. 
4.1 Identifying and Resolving Graph Bias or Imbalance 
In experiment 5, having already identified specific cases that remained unbalanced, we attempted to 
rectify this by examining the graph in parallel with the WSD metric data. If a graph displays character-
istics indicating imbalance or bias, for example where a node is unreachable (isolated in the graph), or 
node degree and node edge-type variation are relatively low (see section 3.5), it is less likely that the 
spreading activation will reflect the meaning of the text. We made discoveries similar to the following: 
? 80% of nodes with Total ETV >15 had WSD precision of over 90% 
? 60% of nodes with Total ETV <5 had precision of less than 10% 
We also discovered cases in the graph where a node had very high Degree (> 100), and relatively low 
ETV. In terms of spreading activation, these nodes would be especially problematic. We have coined 
the term ?Black Hole Node? to describe this phenomenon. In psycholinguistic terms, this may be com-
parable to the notion of a Freudian slip, where a node in the graph which is not immediately relevant to 
the context of the document, has become over-stimulated by its connectivity, or as Norvig (1986) would 
suggest, its ?promiscuity?.  The signal will gravitate towards such an over-connected node during the 
process of spreading activation, affecting the relevance of other nodes in that context. An example black 
hole node is the UMLS CUI C0035298, representing a retina in a human eye, with 1636 edges and 19 
edge types. The extra noise in activating such a node can skew the signal across the entire graph. Word 
senses that compete for relevance with this or related nodes will have poorer accuracy. We modified the 
activation strategy to reflect this by increasing the rate of decay on such nodes from 0.5 to 0.99. 
By ensuring that only the graph weighting strategy is modified, we can keep all word-senses present 
in the graph, resolving the issue identified by Norvig (1986) where such graph content had to be re-
moved. Using the WSD metric output, we also modified the activation strategy to cope with bias toward 
particular senses in the test corpus. We reduced the starting weight for semantic categories for the high-
scoring sense, in order to potentially increase the relative semantic importance of the alternative senses. 
Table 6 demonstrates some of the improvements achieved with regard to specific ambiguous terms. 
 
2245
Term F-Measure Before F-Measure After 
Murine Sarcoma Virus 0 0.47 
Gamma-Interferon 0.013 0.28 
RA 0.021 0.59 
CCD 0.033 1 
AA 0.899 0.99 
Table 6. Examples of term-specific improvement using re-weighting strategy. 
4.2 MSH-WSD Data Set 
In working with the MSH-WSD data set, we came across many issues that Navigli (2009) previously 
identified.  The number of ambiguous senses (423) in the context of the full UMLS set of almost 3 
million, reduces the validity of this corpus for measuring real-world viability and accuracy. Further to 
this, our results with lexical analysis optimisation demonstrate that the test corpus ignored surrounding 
context for potentially overlapping terms, such as ?bat? and ?fruit bat?. In such cases, it would have 
been more accurate to use the CUI for ?fruit bat? as the specific type of ?bat?, but the test corpus does 
not reflect this. Our algorithm is sensitive to contextual semantics, so ensuring that all lexical matches 
of any length remain present, potentially reduces the accuracy of the algorithm?s output, as well as the 
real-world utility of the approach. In spite of the various data transformation techniques applied, our 
recall maximised at 96.4%. Critically, when we normalize our overall accuracy (0.89) to take this into 
account, we reach accuracy of 0.92, a significant achievement in unsupervised WSD. We are currently 
examining what may be required to achieve maximum recall of 100%. While such a result is not guar-
anteed, without full coverage of the test set, we have not yet measured the full potential of this method. 
4.3 Identifying Focus Areas for Ontology Improvement 
One of the primary outcomes of this research is a method for the identification of specific ontology 
portions that require further development. As we have seen in section 4.1, there are several candidates 
which stand out. Other issues pointing to required enhancements in the ontology were around the notion 
of isolated nodes in the graph. An example of this is "ADA", the American Dental Association. It is 
surprising to discover that although this term?s associated CUI (C0002456) is listed in 7 source ontolo-
gies of the UMLS SN, there are no semantic relationships in the source between this CUI and any others. 
Of the 203 ambiguous terms in the MSH-WSD data set, 5 of those terms had associated nodes that were 
similarly isolated in the graph. Without any semantic relationship to other concepts, it is reasonable to 
suggest that the ontology would benefit from focused development of these nodes? surrounding context. 
In terms of the variation of connectivity, we quickly discovered using our simple graph metrics that 
the ?SIB? or sibling relationship was extremely common. Consider the concept C0325089 representing 
the felidae family or the animal cat, which has 8 connections, but for which SIB is the only available 
link type. Hard-wiring siblings in this fashion, with no other link, is unhelpful since spreading activation 
can already identify siblings from common parent nodes. We contend that such concepts are not as well 
connected as they may first appear, and are therefore strong candidates for further development. This 
will not be apparent from the Degree metric alone, but by combining Degree and Edge Type Variation 
with node-specific accuracy in an NLP task, it becomes a straightforward process. Following this dis-
covery, we also suggest that an empirical analysis of link quality would be beneficial, although this 
would not be a trivial task given the size of the data set (~3 million senses and ~700 link types). 
5 Summary and Future Research 
We have presented a new method for evaluating ontology semantics which has several advantages 
over existing approaches. We have shown how the application of graph theoretical analysis to semantic 
structures like ontologies is a valid means by which to assess their semantic quality, while enabling the 
recommendation of specific focus areas for further development. We have additionally demonstrated 
that a graph-metric based weighting strategy for spreading activation can overcome an ontology?s in-
herent semantic inconsistencies, facilitating the optimisation of the ontology for a given NLP task. 
In the case of our UMLS prototype, we made significant improvements using this technique, achiev-
ing state of the art in unsupervised knowledge based WSD (0.82), as well as achieving state of the art in 
overall unsupervised WSD, with the use of a fall-back probability score (0.89). An additional semi-
2246
supervised approach, leveraging gold-standard data from a training portion of the MSH-WSD data set, 
had very promising performance (0.93). The amount of required input data to this method is relatively 
small when compared with fully supervised approaches, as a single gold-standard annotation in each 
target context is sufficient to evaluate the graph using our spreading activation algorithm. 
In future we would like to apply this technique to other ontologies, and associated test sets, for other 
domains in NLP. Merging of domain-specific ontologies with more general semantic resources like 
Yago or Wordnet may help to facilitate the activation of otherwise poorly connected or isolated nodes 
in the graph. We would like to investigate the automatic learning of an optimal spreading activation 
weighting strategy. An empirical study comparing data from human ontology reviewers with this 
spreading activation technique, would also be helpful. 
We would like to expand the set of metrics used, by adapting other existing graph theoretical metrics 
to suit the requirements of NLP. Some promising examples are ?Centrality? and ?Betweenness? outlined 
by Brandes and Erlebach (2005), which determine the relative importance of a node within a graph. In 
the case of UMLS, we can perform a comprehensive static analysis of all ambiguous CUIs within the 
data set, identifying competing senses which do not have sufficient separation in the graph. These senses 
could then be targeted in the configuration of the spreading activation strategy. 
As interest grows in the use of graph theoretical methods for the analysis of cognitive processes (Van 
Dijk et al, 2010; Bullmore and Sporns, 2009; Sporns, 2003), exploring the relationship between spread-
ing activation in a graph representing ontology semantics, as performed in this research, and in neural 
activity during psycholinguistic experimentation (Fang and Evermann, 2010), becomes an exciting pro-
spect that may lead to a better understanding of semantic processing in the human brain. 
Acknowledgements 
Sincere thanks to Mikhail Sogrin (IBM), the talented developer of both ?Galaxy? and the lexicon expan-
sion framework used here, without whom this research would not have been possible. 
References 
Agirre, E., Mart?nez, D., de Lacalle, O. L., & Soroa, A. (2006, July). Two graph-based algorithms for state-of-the-
art WSD. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (pp. 
585-593). Association for Computational Linguistics. 
Agirre, E., & Soroa, A. (2009, March). Personalizing PageRank for word sense disambiguation. In Proceedings 
of the 12th Conference of the European Chapter of the Association for Computational Linguistics. ACL 
Agirre, E., Soroa, A., & Stevenson, M. (2010). Graph-based Word Sense Disambiguation of biomedical docu-
ments. Bioinformatics, 26(22), 2889-2896. 
Brandes, U., & Erlebach, T. (Eds.). (2005). Network analysis: methodological foundations (Vol. 3418). Springer. 
Bullmore, E., & Sporns, O. (2009). Complex brain networks: graph theoretical analysis of structural and functional 
systems. Nature Reviews Neuroscience, 10 (3), 186-198. 
Collins, A. M., & Quillian, M. R. (1969). Retrieval time from semantic memory. Journal of verbal learning and 
verbal behavior, 8(2), 240-247. 
Collins, A. M., & Loftus, E. F. (1975). A spreading-activation theory of semantic processing. Psychological re-
view, 82(6), 407. 
Crestani, F. (1997). Application of spreading activation techniques in information retrieval. Artificial Intelligence 
Review, 11(6), 453-482. 
Diestel, R. (2005), Graph Theory (3rd ed.), Berlin, New York: Springer-Verlag, ISBN 978-3-540-26183-4. 
El-Rab, W. G., Za?ane, O. R., & El-Hajj, M. (2013, August). Biomedical text disambiguation using UMLS. In Pro-
ceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Min-
ing (pp. 943-947). ACM. 
Evermann, J., & Fang, J. (2010). Evaluating ontologies: Towards a cognitive measure of quality. Information Sys-
tems, 35(4), 391-403. 
Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A. A., & Welty, C. (2010). Building 
Watson: An overview of the DeepQA project. AI magazine, 31(3), 59-79. 
2247
Gale, W. A., Church, K. W., & Yarowsky, D. (1992, February). One sense per discourse. In Proceedings of the 
workshop on Speech and Natural Language (pp. 233-237). Association for Computational Linguistics. 
Gruber, T. R. (1993). A translation approach to portable ontology specifications. Knowledge acquisition, 5(2) 
Guarino, N., Oberle, D., & Staab, S. (2009). What is an Ontology? In Handbook on ontologies (pp. 1-17). Springer 
Jim?nez-Ruiz, E., Grau, B. C., & Horrocks, I. (2012). Exploiting the UMLS Metathesaurus in the Ontology Align-
ment Evaluation Initiative. In E-LKR Workshop (pp. 1-6). 
Jim?nez Ruiz, E., Grau, B. C., Horrocks, I., & Berlanga, R. (2011). Supporting concurrent ontology development: 
Framework, algorithms and tool. Data & Knowledge Engineering, 70(1), 146-164. 
Jimeno Yepes, A., & Aronson, A. R. (2012, January). Knowledge-based and knowledge-lean methods combined 
in unsupervised word sense disambiguation. In Proceedings of the 2nd ACM SIGHIT International Health In-
formatics Symposium (pp. 733-736). ACM. 
Liang, T., & Lin, Y. H. (2005). Anaphora resolution for biomedical literature by exploiting multiple resources. 
In Natural Language Processing?IJCNLP 2005(pp. 742-753). Springer Berlin Heidelberg 
Ma, Y., Jin, B., Liu, X., Liu, L., & Lu, K. (2013). A Graph Derivation Based Approach for Measuring and Com-
paring Structural Semantics of Ontologies. IEEE Transactions on Knowledge and Data Engineering, 1. 
McInnes, B. T., Pedersen, T., Liu, Y., Melton, G. B., & Pakhomov, S. V. (2011). Knowledge-based method for 
determining the meaning of ambiguous biomedical terms using information content measures of similarity. 
In AMIA Annual Symposium Proceedings (Vol. 2011, p. 895). American Medical Informatics Association. 
McInnes, B. T., & Pedersen, T. (2013). Evaluating measures of semantic similarity and relatedness to disambiguate 
terms in biomedical text. Journal of biomedical informatics, 46(6), 1116-1124. 
National Library of Medicine. 2013. Unified Medical Language System, version 2013AA. NLM 
Navigli, R. (2009). Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2), 10. 
Norvig, P. (1986). Unified theory of inference for text understanding. CALIFORNIA UNIV BERKELEY GRAD-
UATE DIV. 
Noy, N. F. (2004). Tools for mapping and merging ontologies. In Handbook on ontologies (pp. 365-384). Springer 
Pace-Sigge, M. (2013). Lexical Priming in Spoken English Usage. Palgrave Macmillan. 
Pisanelli, D. M., Gangemi, A., & Steve, G. (1998). An ontological analysis of the UMLS Metathesaurus. In Pro-
ceedings of the AMIA symposium (p. 810). American Medical Informatics Association. 
Plaza, L., Jimeno-Yepes, A. J., D?az, A., & Aronson, A. R. (2011). Studying the correlation between different 
word sense disambiguation methods and summarization effectiveness in biomedical texts. BMC bioinformatics 
Quillian, M.R. (1966). Semantic Memory. Unpublished doctoral dissertation, Carnegie Institute of Technology 
(Re-printed in part in M. Minsky (1968). Semantic Information Processing. Cambridge, Mass. MIT Press). 
Sicilia, M. A., Rodr?guez, D., Garc?a-Barriocanal, E., & S?nchez-Alonso, S. (2012). Empirical findings on ontol-
ogy metrics. Expert Systems with Applications, 39(8), 6706-6711. 
Sporns, O. (2003). Graph theory methods for the analysis of neural connectivity patterns. In Neuroscience Data-
bases (pp. 171-185). Springer US. 
Tartir, S., Arpinar, I. B., Moore, M., Sheth, A. P., & Aleman-Meza, B. (2005, November). OntoQA: Metric-based 
ontology quality analysis. In IEEE Workshop on Knowledge Acquisition from Distributed, Autonomous, Se-
mantically Heterogeneous Data and Knowledge Sources (Vol. 9). 
Troussov, A., Sogrin, M., Judge, J., & Botvich, D. (2008). Mining socio-semantic networks using spreading acti-
vation technique. In Proc. International Workshop on Knowledge Acquisition from the Social Web 
Tsatsaronis, G., Vazirgiannis, M., & Androutsopoulos, I. (2007, January). Word Sense Disambiguation with 
Spreading Activation Networks Generated from Thesauri. In IJCAI (Vol. 7, pp. 1725-1730). 
Van Dijk, K. R., Hedden, T., Venkataraman, A., Evans, K. C., Lazar, S. W., & Buckner, R. L. (2010). Intrinsic 
functional connectivity as a tool for human connectomics: theory, properties, and optimization. Journal of neu-
rophysiology, 103(1), 297. 
Vrande?i?, D., & Sure, Y. (2007). How to design better ontology metrics. In The Semantic Web: Research and 
Applications (pp. 311-325). Springer Berlin Heidelberg. 
2248
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 95?99, Dublin, Ireland, August 23-29 2014.
What or Who is Multilingual Watson?
Keith Cortis
IBM Ireland
keithcor@ie.ibm.com
Urvesh Bhowan
IBM Ireland
URVESHBH@ie.ibm.com
Ronan Mac an tSaoir
IBM Ireland
ronan.mcateer@ie.ibm.com
D.J. McCloskey
IBM Ireland
dj_mccloskey@ie.ibm.com
Mikhail Sogrin
IBM Ireland
SOGRIMIK@ie.ibm.com
Ross Cadogan
IBM Ireland
ROSSCADO@ie.ibm.com
Abstract
IBM Watson is an intelligent open-domain question answering system capable of an-
swering questions posed in natural language. However, the system originally developed
to compete against human players on Jeopardy! is heavily reliant on English language
components, such as the English Slot Grammar parser, which impacts multilingual ex-
tensibility and scalability. This paper presents a working prototype for a multilingual
Watson, introducing the major challenges encountered and their proposed solutions.
1 Introduction
IBMWatson is an intelligent open-domain question answering (QA) system capable of answering
questions posed in natural language (Ferrucci, 2012). The open-domain QA problem is one of
the most challenging in computer science and artificial intelligence, as it touches on aspects of
information retrieval, Natural Language Processing (NLP), knowledge representation, machine
learning and reasoning. Since Jeopardy! in 2011 (Ferrucci et al., 2010), Watson has been applied
successfully to other domains, such as healthcare, finance and customer engagement. However,
like Jeopardy!, these application areas deal exclusively with English data. The system is therefore
heavily reliant on English NLP components, in particular, the rule-based English Slot Grammar
(ESG) parser used throughout. While the ESG parser performs well and has limited multilingual
capabilities, the required grammar or slot rules are language-specific, impacting scalability and
deployment speed. This paper presents some of the major challenges and proposed solutions to
extend Watson to support any natural languag. We introduce a robust cross-lingual method for
identifying crucial characteristics in a question (such as the Lexical Answer Type), which shows
similar expressiveness to the hand-crafted English-based implementation. We outline a system
for detecting the same named entities across text in multiple languages, and our current effort
to train a multilingual Watson system using Wikipedia data for demonstration at Coling 2014.
The demo setup and a brief discussion of results is also presented.
2 Overview of Watson Architecture
This section presents an overview of the DeepQA architecture for multilingual Watson. InQues-
tion Analysis a detailed syntactic and semantic analysis is performed on the input question.
Unstructured text is converted to structured information for use in later components. This pro-
cess uses a series of NLP technologies, such as a statistically trained natural language parser, and
components for named entity recognition, anaphora resolution and relation extraction. The Lex-
ical Answer Type and Focus are important examples, described later. The Hypothesis Gen-
eration phase produces all possible candidate answers for a given question. Watson searches
its corpora for relevant content. Example sources include unstructured knowledge, such as
Wikipedia and structured resources including DBpedia and PRISMATIC. Potential answers to
the question are generated from the retrieved content as unscored hypotheses. In Supporting
Evidence Retrieval, the system gathers additional supporting evidence for each hypothesis
by searching for occurrences of the candidate answer in the context of analysed question data.
95
Hypothesis and Evidence Scoring uses many scoring algorithms to determine the relevance
of retrieved candidate answers. Each scorer, whether context dependent or independent, pro-
duces a measure of how well the evidence supports a candidate answer for a question. The
Final Merging and Ranking phase merges equivalent candidate answers and uses a machine
learning model to rank the final merged set of answers accordingly.
3 NLP and Parsing in Multilingual Watson
Syntactic parsing plays an important role throughout the major stages in Watson architecture,
from question analysis, to primary search and answer scoring. We aim to investigate the impact
of deep syntactic parsing compared to shallow methods for named entities, temporal and geo-
graphic references, etc. It is thought that accurate determination of the roles these aspects play
requires a deeper analysis of sentence structure. For example, in the original system, the rules
which detect the question focus and LAT are heavily dependent on the deep syntactical parse.
Our experiments have show comparable results in this task with shallow methods. However,
in corpus ingestion aspects such as building syntactic frames in order to learn axioms, such
as ?is _a(liquid, fluid)? and vice-versa, the subject-verb-object directionality of a statement is
paramount.
Watson uses the rule-based ESG parser (McCord et al., 2012), which performs exceptionally
well in terms of parse quality and throughput, and defines our parsing benchmark. The XSG
formalism underpinning ESG supports new languages through the generation of language-specific
grammar or slot rules. This activity requires significant effort from highly skilled linguists for
each new language. As the system further evolves for new domains and use-cases, such rules
must be revised and extended, resulting in a long term requirement for this specialised skillset.
To address this skill requirement and enable a more scalable approach, a move towards statistical
parsing methods is being investigated. We identified the following attributes of our ideal parsing
technology. It should be multilingually capable, fast (compared to XSG, currently 2 orders of
magnitude faster than current statistical parsers); highly accurate (comparable with XSG);
easily extensible to new languages with low effort (relative to XSG); easy (and fast) to train on
a new language or domain; memory efficient; robust to noisy/ungrammatical input; support a
rich set of annotation features (ESG has 70+); facilitate overriding of biases in training data.
Our investigations indicate that meeting all of these requirements will be a challenge in any
single parsing formalism. Statistical dependency parsing allowing for non-projective trees ap-
pears to be a good fit for the variation in language structure that we will need to support
(McDonald et al., 2013). Experiments with MSTParser and the Eisner algorithm in Italian have
shown promise from a quality point of view. We have also identified a language-independent for-
mal representation of a parse. McDonald et al. (2013) present a harmonized set of dependency
labels for multilingual parsing which has been adapted for other typologically diverse languages,
such as Chinese and Finnish and encourages convergence for reuse. Our initial investigations
using this set for English, Spanish, French, Brazilian Portugese and German text, suggest min-
imal modification is required to adapt for use in question answering. Modification of existing
pipeline components to a more streamlined dependency structure than the XSG formalism, will
form part of ongoing research.
Treebanks of parse data with part-of-speech and dependency labels will be required in order
to train the chosen parser. There are several examples of existing Treebanks for the chosen
languages, such as the IULA Spanish LSP Treebank
1
. However, the context of the data included
is very rarely representative of question-answering scenarios. For example, in the IULA corpus,
there are less than 10 questions. We will therefore be supplementing this training data with
our own hand-annotated corpora, in order to increase the validity of the trained parser for
use in question answering. As mentioned previously, we will also be adapting these resources
to use the UniPos and UniDep part-of-speech and dependency label sets. In parallel to the
1
http://www.iula.upf.edu/recurs01_tbk_uk.htm
96
investigations for multilingual parsing, we have also considered the requirements of a parser-
independent system which can leverage shallow parse data in order to perform reasonably well
at the same task. The multilingual LAT detection component which builds on part-of-speech
data to identify noun phrases and associated head words and modifiers, may be additionally
used to generate a simple linked parse structure without dependency labels. As the number of
supported languages in the system grows, it is important to have a meaningful baseline upon
which to build, even while a parser or appropriate training data for the new language is still
being prepared. The parser-independent capability will be particularly useful in this context.
4 Other Challenges Faced in Multilingual Watson
4.1 Multilingual Lexical Answer Type (LAT)
For the Jeopardy! challenge, one of the most critical elements in the system design was the
recognition of what is termed the LAT (Ferrucci et al., 2010). This is typically a noun or
noun-phrase in the question which identifies the type of answer required, without any attempt
to evaluate its semantics. Similarly influential, the Focus is the part of the sentence that, if
replaced with the answer, makes the question a standalone statement. For example, in the
question ?What countries share a border with China??, the LAT is ?countries?, and the Focus
is ?What countries?. Replacing this piece of text with the answer becomes a valid standalone
statement, such as ?Russia, Mongolia, India, . . . , share a border with China?. Ferrucci et al.
(2010) found that identifying a candidate answer as an instance of a LAT is an important part
of the answer scoring mechanism, and a common source of critical errors.
In the original system, a Prolog component was used to match specific English language
patterns for various purposes including LAT detection. In a multilingual context, we require a
more robust method that retains the same potential expressiveness and performance of a good
Prolog implementation, while facilitating cross-lingual pattern recognition. Our multilingual
prototype uses lexical features, such as part-of-speech and lemma. Pattern matching rules
over these features were developed using the IBM LanguageWare
2
rules engine which provides
a comparable level of expression with standard Prolog implementations. While maintaining
pipeline accuracy for English, this prototype was also 4 times faster than the original Prolog
modules. A statistical method that was originally used in the Jeopardy! pipeline is also being
adapted for use in a multilingual context. This approach will reduce the dependency on hard-
coded language-specific parsing rules. The use of harmonized Stanford dependencies (McDonald
et al., 2013) will further enhance these efforts.
4.2 Detecting Concepts across Multiple Languages
One of the most challenging aspects of multilingual NLP is the recognition of identical concepts
across text in multiple languages. Wikipedia and Wiktionary provide translations of words
from one language to another, however they do not establish language-independent identifiers
for concepts. Open Multilingual Wordnet (OMW) project
3
links WordNet style structured
resources, in up to 150 languages, to the Princeton Wordnet of English
4
. While Princeton
Wordnet is made specifically for English, its numeric ID system is in fact a set of language-
independent identifiers, and may be used to relate concepts and words. The OMW project
provides links to the same IDs from words in other languages. The Extended version of the
OMW dataset additionally links Wiktionary data with these WordNet structured resources,
thus greatly improving coverage of vocabulary.
In the multilingual Watson system, we can perform semantic analysis using domain knowledge
irrespective of the language of the question, or the domain. This is enabled by a process of
concept identification that maps instances of concepts in natural language text to a set of
2
http://www-01.ibm.com/software/globalization/topics/languageware/
3
http://compling.hss.ntu.edu.sg/omw/
4
http://wordnet.princeton.edu/
97
language-independent identifiers. Our implementation takes inspiration from efforts, such as
the OMW and the Unstructured Medical Language System
5
in the biomedical domain, which
use alphanumeric labels to identify individual semantic concepts, irrespective of the forms these
concepts take in any language. In addition to these unique identifiers, our design incorporates
fully qualified URI namespaces for these instances, as proposed by the W3C Semantic Web
Standard, in order to distinguish between instances of a concept in various contexts.
In parallel with this concept ID system, we have developed a lexicon expansion framework
that incorporates pluggable transformation modules to generate alternative forms for lexicon
entries. This facilitates the increased coverage of semantic concepts in the chosen domain text,
which remain linked to their respective namespace-qualified unique identifiers.
4.3 Machine Learning Challenges
The original Watson system uses a cascade of multiple trained machine-learning models to
decide if a candidate answer is correct. In each cascade, questions are categorised and routed to
models trained for different types of English Jeopardy! questions. Training these models requires
questions with known answers for the different question types. However, the same Jeopardy!
style question characteristics may not apply or be evident in different languages. To address
this, we simplified the hand-crafted model routing in the multilingual system to make no a
priori assumptions about the question type. While this requires good model generalisation over
a potentially broad range of questions, this is offset by the smaller but highly-focused feature set
used in this multilingual system. Initial features were chosen by ranking scorers whose output
showed the highest correlation to the correct class on experiments with English questions.
5 Multilingual Watson on Wikipedia-based Questions
5.1 Searching over Wikipedia
Ingestion is the process of transforming documents for use by Watson. Raw Wikipedia XML
documents
6
are transformed into the TREC standard
7
. These TREC files must conform to
the UTF-8 character encoding scheme. The TREC-formatted documents are then transformed
into Lucene
8
search indices. During the TREC transformation process, text normalisation and
character replacement was being conducted for English text. All corpora text in Unicode was
normalised to ASCII, e.g., for the term Jap?n, the accent was stripped from the ? character,
thus normalising to Japon. In addition, characters with particular ISO8859 codes, were replaced,
such as pi with the character n. Since the Jeopardy! pipeline handles only ASCII character
encoding, this prevents the system from generating correct answers which contain non-ASCII
characters, dramatically lowering recall on multilingual questions. These issues are resolved in
the multilingual Watson system, which uses Unicode in the Normalisation Form Compatibility
Composition.
5.2 Wikipedia-based Questions and Answers
We used 3732 English questions with known answers (originally gathered by the Watson team) as
our question base (split into 3359 training and 373 test questions). These questions were machine
translated to Spanish, French and Brazilian Portugese using IBM?s n.Fluent Translation service
9
. The test set was manually reviewed to correct any translation errors, and questions deemed
unanswerable (where Watson had no means of retrieving the correct answer from the source
Wikipedia corpus) were removed. To assist in identifying which questions are unanswerable,
the MediaWiki API
10
(an open web API service providing access to Wikipedia meta-data) was
5
http://www.nlm.nih.gov/research/umls/
6
Obtained from: http://dumps.wikimedia.org/
7
For the Text REtrieval Conference (TREC) standard see: http://trec.nist.gov/
8
http://lucene.apache.org/
9
http://www-03.ibm.com/press/us/en/pressrelease/28887.wss
10
http://www.mediawiki.org/wiki/API
98
used to filter questions whose answers could not be mapped to an article title in the Wikipedia
source corpus.The MediaWiki API also has a cross-language aspect which provides useful redirect
information between Wikipedia article titles, which we used to supplement our answers, e.g.,
?JFK? in English redirects to ?John F. Kennedy? in Spanish. The manual curation of the
translated English questions for English, Spanish, French and Brazilian Portuguese ensure that
the same question set is used across the different languages, and that these common questions
are all answerable with respect to their respective Wikipedia corpus.
6 Demo Setup and Discussion of Results
The setup of the demo will include a multilingual QA system (for several languages, such as
English, Spanish, French, Brazilian Portuguese, etc.). The participants attending the Coling
conference will be able to ask the multilingual Watson QA system a question via its web user
interface. The system will then attempt to answer the question in real-time, and will return a
list of the five top-ranked candidate answers and their confidence scores. The confidence score
for each candidate answer represents the likeliness that it is correct, based on the analysis of all
supporting evidence gathered by the system. Any supporting evidence can also be examined for
a given answer, such as the passage or document hits from the search process.
For disclosure purposes we are unable to provide details on our multilingual experimental
results. Therefore, we will briefly discuss our initial baseline results and the improvements
made in our current system. Our initial baseline is based on the results using the English test
questions, which achieved very high (near perfect) recall and high accuracy rates. In terms of
multilingual Watson, our initial recall results were very low compared to English. As a result,
recall became the primary focus of our multilingual investigation. Recall was improved by around
6% with the full Unicode text normalisation support and parser-independent changes (discussed
in Sections 3 and 5.1 respectively). In addition, the answer curation discussed in Section 5.2
improved recall by 9%. Other language specific improvements and customisation to the primary
search components in multilingual Watson, in particular, the Lucene analyser and search query,
resulted in a further 29% increase in recall. These combined efforts produced comparable recall
rates for Spanish, French and Brazilian Portuguese test questions to the English questions. The
next area of our investigation will be focused on accuracy improvements.
References
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur,
Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An Overview of the DeepQA Project. AI Magazine, 31(3):59?79.
David A. Ferrucci. 2012. Introduction to "this is watson". IBM Journal of Research and Development,
56(3):235?249.
Michael C. McCord, J. William Murdock, and Branimir Boguraev. 2012. Deep parsing in watson. IBM
Journal of Research and Development, 56(3):3.
Ryan T. McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith B. Hall, Slav Petrov, Hao Zhang, Oscar T?ckstr?m, Claudia Bedini, N?ria Bertomeu
Castell?, and Jungmee Lee. 2013. Universal dependency annotation for multilingual parsing. In ACL
(2), pages 92?97. The Association for Computer Linguistics.
99
