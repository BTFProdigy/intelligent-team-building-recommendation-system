Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 366?374,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Conditional Random Fields for Word Hyphenation
Nikolaos Trogkanis
Computer Science and Engineering
University of California, San Diego
La Jolla, California 92093-0404
tronikos@gmail.com
Charles Elkan
Computer Science and Engineering
University of California, San Diego
La Jolla, California 92093-0404
elkan@cs.ucsd.edu
Abstract
Finding allowable places in words to insert
hyphens is an important practical prob-
lem. The algorithm that is used most of-
ten nowadays has remained essentially un-
changed for 25 years. This method is the
TEX hyphenation algorithm of Knuth and
Liang. We present here a hyphenation
method that is clearly more accurate. The
new method is an application of condi-
tional random fields. We create new train-
ing sets for English and Dutch from the
CELEX European lexical resource, and
achieve error rates for English of less than
0.1% for correctly allowed hyphens, and
less than 0.01% for Dutch. Experiments
show that both the Knuth/Liang method
and a leading current commercial alterna-
tive have error rates several times higher
for both languages.
1 Introduction
The task that we investigate is learning to split
words into parts that are conventionally agreed to
be individual written units. In many languages, it
is acceptable to separate these units with hyphens,
but it is not acceptable to split words arbitrarily.
Another way of stating the task is that we want to
learn to predict for each letter in a word whether or
not it is permissible for the letter to be followed by
a hyphen. This means that we tag each letter with
either 1, for hyphen allowed following this letter,
or 0, for hyphen not allowed after this letter.
The hyphenation task is also called ortho-
graphic syllabification (Bartlett et al, 2008). It is
an important issue in real-world text processing,
as described further in Section 2 below. It is also
useful as a preprocessing step to improve letter-to-
phoneme conversion, and more generally for text-
to-speech conversion. In the well-known NETtalk
system, for example, syllable boundaries are an
input to the neural network in addition to letter
identities (Sejnowski and Rosenberg, 1988). Of
course, orthographic syllabification is not a fun-
damental scientific problem in linguistics. Nev-
ertheless, it is a difficult engineering task that is
worth studying for both practical and intellectual
reasons.
The goal in performing hyphenation is to pre-
dict a sequence of 0/1 values as a function of a se-
quence of input characters. This sequential predic-
tion task is significantly different from a standard
(non-sequential) supervised learning task. There
are at least three important differences that make
sequence prediction difficult. First, the set of all
possible sequences of labels is an exponentially
large set of possible outputs. Second, different in-
puts have different lengths, so it is not obvious
how to represent every input by a vector of the
same fixed length, as is almost universal in su-
pervised learning. Third and most important, too
much information is lost if we learn a traditional
classifier that makes a prediction for each letter
separately. Even if the traditional classifier is a
function of the whole input sequence, this remains
true. In order to achieve high accuracy, correla-
tions between neighboring predicted labels must
be taken into account.
Learning to predict a sequence of output labels,
given a sequence of input data items, is an instance
of a structured learning problem. In general, struc-
tured learning means learning to predict outputs
that have internal structure. This structure can
be modeled; to achieve high predictive accuracy,
when there are dependencies between parts of an
output, it must be modeled. Research on struc-
tured learning has been highly successful, with
sequence classification as its most important and
successful subfield, and with conditional random
fields (CRFs) as the most influential approach to
learning sequence classifiers. In the present paper,
366
we show that CRFs can achieve extremely good
performance on the hyphenation task.
2 History of automated hyphenation
The earliest software for automatic hyphenation
was implemented for RCA 301 computers, and
used by the Palm Beach Post-Tribune and Los An-
geles Times newspapers in 1962. These were two
different systems. The Florida system had a dic-
tionary of 30,000 words; words not in the dictio-
nary were hyphenated after the third, fifth, or sev-
enth letter, because the authors observed that this
was correct for many words. The California sys-
tem (Friedlander, 1968) used a collection of rules
based on the rules stated in a version of Webster?s
dictionary. The earliest hyphenation software for
a language other than English may have been a
rule-based program for Finnish first used in 1964
(Jarvi, 2009).
The first formal description of an algorithm for
hyphenation was in a patent application submit-
ted in 1964 (Damerau, 1964). Other early pub-
lications include (Ocker, 1971; Huyser, 1976).
The hyphenation algorithm that is by far the most
widely used now is due to Liang (Liang, 1983).
Although this method is well-known now as the
one used in TEX and its derivatives, the first ver-
sion of TEX used a different, simpler method.
Liang?s method was used also in troff and
groff, which were the main original competitors
of TEX, and is part of many contemporary software
products, supposedly including Microsoft Word.
Any major improvement over Liang?s method is
therefore of considerable practical and commer-
cial importance.
Over the years, various machine learning meth-
ods have been applied to the hyphenation task.
However, none have achieved high accuracy. One
paper that presents three different learning meth-
ods is (van den Bosch et al, 1995). The lowest
per-letter test error rate reported is about 2%. Neu-
ral networks have been used, but also without great
success. For example, the authors of (Kristensen
and Langmyhr, 2001) found that the TEX method
is a better choice for hyphenating Norwegian.
The highest accuracy achieved until now for the
hyphenation task is by (Bartlett et al, 2008), who
use a large-margin structured learning approach.
Our work is similar, but was done fully indepen-
dently. The accuracy we achieve is slightly higher:
word-level accuracy of 96.33% compared to their
95.65% for English. Moreover, (Bartlett et al,
2008) do not address the issue that false positive
hyphens are worse mistakes than false negative hy-
phens, which we address below. Also, they report
that training on 14,000 examples requires about an
hour, compared to 6.2 minutes for our method on
65,828 words. Perhaps more important for large-
scale publishing applications, our system is about
six times faster at syllabifying new text. The speed
comparison is fair because the computer we use is
slightly slower than the one they used.
Methods inspired by nonstatistical natural lan-
guage processing research have also been pro-
posed for the hyphenation task, in particular
(Bouma, 2003; Tsalidis et al, 2004; Woestenburg,
2006; Haralambous, 2006). However, the methods
for Dutch presented in (Bouma, 2003) were found
to have worse performance than TEX. Moreover,
our experimental results below show that the com-
mercial software of (Woestenburg, 2006) allows
hyphens incorrectly almost three times more often
than TEX.
In general, a dictionary based approach has zero
errors for words in the dictionary, but fails to work
for words not included in it. A rule-based ap-
proach requires an expert to define manually the
rules and exceptions for each language, which is
laborious work. Furthermore, for languages such
as English where hyphenation does not system-
atically follow general rules, such an approach
does not have good results. A pattern-learning ap-
proach, like that of TEX, infers patterns from a
training list of hyphenated words, and then uses
these patterns to hyphenate text. Although useful
patterns are learned automatically, both the TEX
learning algorithm and the learned patterns must
be hand-tuned to perform well (Liang, 1983).
Liang?s method is implemented in a program
named PATGEN, which takes as input a training
set of hyphenated words, and outputs a collection
of interacting hyphenation patterns. The standard
pattern collections are named hyphen.tex for
American English, ukhyphen.tex for British
English, and nehyph96.tex for Dutch. The
precise details of how different versions of TEX
and LATEX use these pattern collections to do hy-
phenation in practice are unclear. At a minimum,
current variants of TEX improve hyphenation ac-
curacy by disallowing hyphens in the first and last
two or three letters of every word, regardless of
what the PATGEN patterns recommend.
367
Despite the success of Liang?s method, incor-
rect hyphenations remain an issue with TEX and
its current variants and competitors. For instance,
incorrect hyphenations are common in the Wall
Street Journal, which has the highest circulation
of any newspaper in the U.S. An example is the
hyphenation of the word ?sudden? in this extract:
It is the case that most hyphenation mistakes in the
Wall Street Journal and other media are for proper
nouns such as ?Netflix? that do not appear in stan-
dard dictionaries, or in compound words such as
?sudden-acceleration? above.
3 Conditional random fields
A linear-chain conditional random field (Lafferty
et al, 2001) is a way to use a log-linear model
for the sequence prediction task. We use the bar
notation for sequences, so x? means a sequence of
variable length. Specifically, let x? be a sequence
of n letters and let y? be a corresponding sequence
of n tags. Define the log-linear model
p(y?|x?;w) =
1
Z(x?, w)
exp
?
j
wjFj(x?, y?).
The index j ranges over a large set of feature-
functions. Each such function Fj is a sum along
the output sequence for i = 1 to i = n:
Fj(x?, y?) =
n?
i=1
fj(yi?1, yi, x?, i)
where each function fj is a 0/1 indicator function
that picks out specific values for neighboring tags
yi?1 and yi and a particular substring of x?. The
denominator Z(x?, w) is a normalizing constant:
Z(x?, w) =
?
y?
exp
?
j
wjFj(x?, y?)
where the outer sum is over all possible labelings
y? of the input sequence x?. Training a CRF means
finding a weight vector w that gives the best pos-
sible predictions
y?? = arg max
y?
p(y?|x?;w)
for each training example x?.
The software we use as an implementation of
conditional random fields is named CRF++ (Kudo,
2007). This implementation offers fast training
since it uses L-BFGS (Nocedal and Wright, 1999),
a state-of-the-art quasi-Newton method for large
optimization problems. We adopt the default pa-
rameter settings of CRF++, so no development set
or tuning set is needed in our work.
We define indicator functions fj that depend on
substrings of the input word, and on whether or
not a hyphen is legal after the current and/or the
previous letter. The substrings are of length 2 to
5, covering up to 4 letters to the left and right of
the current letter. From all possible indicator func-
tions we use only those that involve a substring
that occurs at least once in the training data.
As an example, consider the word
hy-phen-ate. For this word x? = hyphenate
and y? = 010001000. Suppose i = 3 so p is the
current letter. Then exactly two functions fj that
depend on substrings of length 2 have value 1:
I(yi?1 = 1 and yi = 0 and x2x3 = yp) = 1,
I(yi?1 = 1 and yi = 0 and x3x4 = ph) = 1.
All other similar functions have value 0:
I(yi?1 = 1 and yi = 1 and x2x3 = yp) = 0,
I(yi?1 = 1 and yi = 0 and x2x3 = yq) = 0,
and so on. There are similar indicator functions for
substrings up to length 5. In total, 2,916,942 dif-
ferent indicator functions involve a substring that
appears at least once in the English dataset.
One finding of our work is that it is prefer-
able to use a large number of low-level features,
that is patterns of specific letters, rather than a
smaller number of higher-level features such as
consonant-vowel patterns. This finding is consis-
tent with an emerging general lesson about many
natural language processing tasks: the best perfor-
mance is achieved with models that are discrimi-
native, that are trained on as large a dataset as pos-
sible, and that have a very large number of param-
eters but are regularized (Halevy et al, 2009).
When evaluating the performance of a hyphen-
ation algorithm, one should not just count how
many words are hyphenated in exactly the same
way as in a reference dictionary. One should also
measure separately how many legal hyphens are
actually predicted, versus how many predicted hy-
phens are in fact not legal. Errors of the sec-
ond type are false positives. For any hyphenation
368
method, a false positive hyphen is a more serious
mistake than a false negative hyphen, i.e. a hyphen
allowed by the lexicon that the method fails to
identify. The standard Viterbi algorithm for mak-
ing predictions from a trained CRF is not tuned to
minimize false positives. To address this difficulty,
we use the forward-backward algorithm (Sha and
Pereira, 2003; Culotta and McCallum, 2004) to es-
timate separately for each position the probability
of a hyphen at that position. Then, we only allow a
hyphen if this probability is over a high threshold
such as 0.9.
Each hyphenation corresponds to one path
through a graph that defines all 2k?1 hyphenations
that are possible for a word of length k. The over-
all probability of a hyphen at any given location
is the sum of the weights of all paths that do have
a hyphen at this position, divided by the sum of
the weights of all paths. The forward-backward
algorithm uses the sum operator to compute the
weight of a set of paths, instead of the max op-
erator to compute the weight of a single highest-
weight path. In order to compute the weight of all
paths that contain a hyphen at a specific location,
weight 0 is assigned to all paths that do not have a
hyphen at this location.
4 Dataset creation
We start with the lexicon for English published
by the Dutch Centre for Lexical Information at
http://www.mpi.nl/world/celex. We
download all English word forms with legal hy-
phenation points indicated by hyphens. These
include plurals of nouns, conjugated forms of
verbs, and compound words such as ?off-line?.
We separate the components of compound words
and phrases, leading to 204,466 words, of which
68,744 are unique. In order to eliminate abbrevia-
tions and proper names which may not be English,
we remove all words that are not fully lower-case.
In particular, we exclude words that contain capi-
tal letters, apostrophes, and/or periods. This leaves
66,001 words.
Among these words, 86 have two different hy-
phenations, and one has three hyphenations. For
most of the 86 words with alternative hyphen-
ations, these alternatives exist because different
meanings of the words have different pronuncia-
tions, and the different pronunciations have differ-
ent boundaries between syllables. This fact im-
plies that no algorithm that operates on words in
isolation can be a complete solution for the hy-
phenation task.1
We exclude the few words that have two or more
different hyphenations from the dataset. Finally,
we obtain 65,828 spellings. These have 550,290
letters and 111,228 hyphens, so the average is 8.36
letters and 1.69 hyphens per word. Informal in-
spection suggests that the 65,828 spellings contain
no mistakes. However, about 1000 words follow
British as opposed to American spelling.
The Dutch dataset of 293,681 words is created
following the same procedure as for the English
dataset, except that all entries from CELEX that
are compound words containing dashes are dis-
carded instead of being split into parts, since many
of these are not in fact Dutch words.2
5 Experimental design
We use ten-fold cross validation for the experi-
ments. In order to measure accuracy, we com-
pute the confusion matrix for each method, and
from this we compute error rates. We report both
word-level and letter-level error rates. The word-
level error rate is the fraction of words on which
a method makes at least one mistake. The letter-
level error rate is the fraction of letters for which
the method predicts incorrectly whether or not a
hyphen is legal after this letter. Table 1 explains
the terminology that we use in presenting our re-
sults. Precision, recall, and F1 can be computed
easily from the reported confusion matrices.
As an implementation of Liang?s method we
use TEX Hyphenator in Java software available
at http://texhyphj.sourceforge.net.
We evaluate this algorithm on our entire English
and Dutch datasets using the appropriate language
pattern files, and not allowing a hyphen to be
placed between the first lefthyphenmin and
last righthyphenmin letters of each word. For
1The single word with more than two alternative
hyphenations is ?invalid? whose three hyphenations are
in-va-lid in-val-id and in-valid. Interest-
ingly, the Merriam?Webster online dictionary also gives
three hyphenations for this word, but not the same ones:
in-va-lid in-val-id invalid. The American
Heritage dictionary agrees with Merriam-Webster. The dis-
agreement illustrates that there is a certain irreducible ambi-
guity or subjectivity concerning the correctness of hyphen-
ations.
2Our English and Dutch datasets are available for other
researchers and practitioners to use at http://www.cs.
ucsd.edu/users/elkan/hyphenation. Previously
a similar but smaller CELEX-based English dataset was cre-
ated by (van den Bosch et al, 1995), but that dataset is not
available online currently.
369
Abbr Name Description
TP true positives #hyphens predicted correctly
FP false positives #hyphens predicted incorrectly
TN true negatives #hyphens correctly not predicted
FN false negatives #hyphens failed to be predicted
owe overall word-level errors #words with at least one FP or FN
swe serious word-level errors #words with at least one FP
ower overall word-level error rate owe / (total #words)
swer serious word-level error rate swe / (total #words)
oler overall letter-level error rate (FP+FN) / (TP+TN+FP+FN)
sler serious letter-level error rate FP / (TP+TN+FP+FN)
Table 1: Alternative measures of accuracy. TP, TN, FP, and FN are computed by summing over the test
sets of each fold of cross-validation.
English the default values are 2 and 3 respectively.
For Dutch the default values are both 2.
The hyphenation patterns used by TeXHyphen-
ator, which are those currently used by essentially
all variants of TEX, may not be optimal for our
new English and Dutch datasets. Therefore, we
also do experiments with the PATGEN tool (Liang
and Breitenlohner, 2008). These are learning ex-
periments so we also use ten-fold cross validation
in the same way as with CRF++. Specifically, we
create a pattern file from 90% of the dataset us-
ing PATGEN, and then hyphenate the remaining
10% of the dataset using Liang?s algorithm and the
learned pattern file.
The PATGEN tool has many user-settable pa-
rameters. As is the case with many machine learn-
ing methods, no strong guidance is available for
choosing values for these parameters. For En-
glish we use the parameters reported in (Liang,
1983). For Dutch we use the parameters reported
in (Tutelaers, 1999). Preliminary informal exper-
iments found that these parameters work better
than alternatives. We also disallow hyphens in the
first two letters of every word, and the last three
letters for English, or last two for Dutch.
We also evaluate the TALO commercial soft-
ware (Woestenburg, 2006). We know of one
other commercial hyphenation application, which
is named Dashes.3 Unfortunately we do not have
access to it for evaluation. We also cannot do a
precise comparison with the method of (Bartlett et
al., 2008). We do know that their training set was
also derived from CELEX, and their maximum
reported accuracy is slightly lower. Specifically,
for English our word-level accuracy (?ower?) is
96.33% while their best (?WA?) is 95.65%.
3http://www.circlenoetics.com/dashes.
aspx
6 Experimental results
In Table 2 and Table 3 we report the performance
of the different methods on the English and Dutch
datasets respectively. Figure 1 shows how the er-
ror rate is affected by increasing the CRF proba-
bility threshold for each language.
Figure 1 shows confidence intervals for the er-
ror rates. These are computed as follows. For a
single Bernoulli trial the mean is p and the vari-
ance is p(1 ? p). If N such trials are taken, then
the observed success rate f = S/N is a random
variable with mean p and variance p(1 ? p)/N .
For large N , the distribution of the random vari-
able f approaches the normal distribution. Hence
we can derive a confidence interval for p using the
formula
Pr[?z ?
f ? p
?
p(1? p)/N
? z] = c
where for a 95% confidence interval, i.e. for c =
0.95, we set z = 1.96. All differences between
rows in Table 2 are significant, with one exception:
the serious error rates for PATGEN and TALO are
not statistically significantly different. A similar
conclusion applies to Table 3.
For the English language, the CRF using the
Viterbi path has overall error rate of 0.84%, com-
pared to 6.81% for the TEX algorithm using Amer-
ican English patterns, which is eight times worse.
However, the serious error rate for the CRF is less
good: 0.41% compared to 0.24%. This weak-
ness is remedied by predicting that a hyphen is al-
lowable only if it has high probability. Figure 1
shows that the CRF can use a probability thresh-
old up to 0.99, and still have lower overall error
rate than the TEX algorithm. Fixing the probabil-
ity threshold at 0.99, the CRF serious error rate
is 0.04% (224 false positives) compared to 0.24%
(1343 false positives) for the TEX algorithm.
370
1
2
3
4
5
6
7
8
% 
ole
r
English
PATGEN
TeX
TALO
CRF
0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99
Probability threshold
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
% 
sle
r
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9 Dutch
0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99
Probability threshold
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Figure 1: Total letter-level error rate and serious letter-level error rate for different values of threshold for
the CRF. The left subfigures are for the English dataset, while the right ones are for the Dutch dataset.
The TALO and PATGEN lines are almost identical in the bottom left subfigure.
Method TP FP TN FN owe swe % ower % swer % oler % sler
Place no hyphen 0 0 439062 111228 57541 0 87.41 0.00 20.21 0.00
TEX (hyphen.tex) 75093 1343 437719 36135 30337 1311 46.09 1.99 6.81 0.24
TEX (ukhyphen.tex) 70307 13872 425190 40921 31337 11794 47.60 17.92 9.96 2.52
TALO 104266 3970 435092 6962 7213 3766 10.96 5.72 1.99 0.72
PATGEN 74397 3934 435128 36831 32348 3803 49.14 5.78 7.41 0.71
CRF 108859 2253 436809 2369 2413 2080 3.67 3.16 0.84 0.41
CRF (threshold = 0.99) 83021 224 438838 28207 22992 221 34.93 0.34 5.17 0.04
Table 2: Performance on the English dataset.
Method TP FP TN FN owe swe % ower % swer % oler % sler
Place no hyphen 0 0 2438913 742965 287484 0 97.89 0.00 23.35 0.00
TEX (nehyph96.tex) 722789 5580 2433333 20176 20730 5476 7.06 1.86 0.81 0.18
TALO 727145 3638 2435275 15820 16346 3596 5.57 1.22 0.61 0.11
PATGEN 730720 9660 2429253 12245 20318 9609 6.92 3.27 0.69 0.30
CRF 741796 1230 2437683 1169 1443 1207 0.49 0.41 0.08 0.04
CRF (threshold = 0.99) 719710 149 2438764 23255 22067 146 7.51 0.05 0.74 0.00
Table 3: Performance on the Dutch dataset.
Method TP FP TN FN owe swe % ower % swer % oler % sler
PATGEN 70357 6763 432299 40871 35013 6389 53.19 9.71 8.66 1.23
CRF 104487 6518 432544 6741 6527 5842 9.92 8.87 2.41 1.18
CRF (threshold = 0.99) 75651 654 438408 35577 27620 625 41.96 0.95 6.58 0.12
Table 4: Performance on the English dataset (10-fold cross validation dividing by stem).
Method TP FP TN FN owe swe % ower % swer % oler % sler
PATGEN 727306 13204 2425709 15659 25363 13030 8.64 4.44 0.91 0.41
CRF 740331 2670 2436243 2634 3066 2630 1.04 0.90 0.17 0.08
CRF (threshold = 0.99) 716596 383 2438530 26369 24934 373 8.49 0.13 0.84 0.01
Table 5: Performance on the Dutch dataset (10-fold cross validation dividing by stem).
Method TP FP TN FN owe swe % ower % swer % oler % sler
TEX 2711 43 21433 1420 1325 43 33.13 1.08 5.71 0.17
PATGEN 2590 113 21363 1541 1466 113 36.65 2.83 6.46 0.44
CRF 4129 2 21474 2 2 2 0.05 0.05 0.02 0.01
CRF (threshold = 0.9) 4065 0 21476 66 63 0 1.58 0.00 0.26 0.00
Table 6: Performance on the 4000 most frequent English words.
371
For the English language, TALO yields overall
error rate 1.99% with serious error rate 0.72%, so
the standard CRF using the Viterbi path is better
on both measures. The dominance of the CRF
method can be increased further by using a prob-
ability threshold. Figure 1 shows that the CRF
can use a probability threshold up to 0.94, and
still have lower overall error rate than TALO. Us-
ing this threshold, the CRF serious error rate is
0.12% (657 false positives) compared to 0.72%
(3970 false positives) for TALO.
For the Dutch language, the standard CRF us-
ing the Viterbi path has overall error rate 0.08%,
compared to 0.81% for the TEX algorithm. The
serious error rate for the CRF is 0.04% while for
TEX it is 0.18%. Figure 1 shows that any probabil-
ity threshold for the CRF of 0.99 or below yields
lower error rates than the TEX algorithm. Using
the threshold 0.99, the CRF has serious error rate
only 0.005%.
For the Dutch language, the TALO method has
overall error rate 0.61%. The serious error rate
for TALO is 0.11%. The CRF dominance can
again be increased via a high probability thresh-
old. Figure 1 shows that this threshold can range
up to 0.98, and still give lower overall error rate
than TALO. Using the 0.98 threshold, the CRF
has serious error rate 0.006% (206 false positives);
in comparison the serious error rate of TALO is
0.11% (3638 false positives).
For both languages, PATGEN has higher serious
letter-level and word-level error rates than TEX us-
ing the existing pattern files. This is expected since
the pattern collections included in TEX distribu-
tions have been tuned over the years to minimize
objectionable errors. The difference is especially
pronounced for American English, for which the
standard pattern collection has been manually im-
proved over more than two decades by many peo-
ple (Beeton, 2002). Initially, Liang optimized this
pattern collection extensively by upweighting the
most common words and by iteratively adding
exception words found by testing the algorithm
against a large dictionary from an unknown pub-
lisher (Liang, 1983).
One can tune PATGEN to yield either better
overall error rate, or better serious error rate, but
not both simultaneously, compared to the TEX al-
gorithm using the existing pattern files for both
languages. For the English dataset, if we use
Liang?s parameters for PATGEN as reported in
(Sojka and Sevecek, 1995), we obtain overall er-
ror rate of 6.05% and serious error rate of 0.85%.
It is possible that the specific patterns used in TEX
implementations today have been tuned by hand
to be better than anything the PATGEN software is
capable of.
7 Additional experiments
This section presents empirical results following
two experimental designs that are less standard,
but that may be more appropriate for the hyphen-
ation task.
First, the experimental design used above has
an issue shared by many CELEX-based tagging
or transduction evaluations: words are randomly
divided into training and test sets without be-
ing grouped by stem. This means that a method
can get credit for hyphenating ?accents? correctly,
when ?accent? appears in the training data. There-
fore, we do further experiments where the folds
for evaluation are divided by stem, and not by
word; that is, all versions of a base form of a
word appear in the same fold. Stemming uses
the English and Dutch versions of the Porter stem-
mer (Porter, 1980).4 The 65,828 English words in
our dictionary produce 27,100 unique stems, while
the 293,681 Dutch words produce 169,693 unique
stems. The results of these experiments are shown
in Tables 4 and 5.
The main evaluation in the previous section is
based on a list of unique words, which means that
in the results each word is equally weighted. Be-
cause cross validation is applied, errors are always
measured on testing subsets that are disjoint from
the corresponding training subsets. Hence, the
accuracy achieved can be interpreted as the per-
formance expected when hyphenating unknown
words, i.e. rare future words.
However, in real documents common words
appear repeatedly. Therefore, the second less-
standard experimental design for which we report
results restricts attention to the most common En-
glish words. Specifically, we consider the top
4000 words that make up about three quarters of
all word appearances in the American National
Corpus, which consists of 18,300,430 words from
written texts of all genres.5 From the 4,471 most
4Available at http://snowball.tartarus.org/.
A preferable alternative might be to use the information about
the lemmas of words available directly in CELEX.
5Available at americannationalcorpus.org/
SecondRelease/data/ANC-written-count.txt
372
frequent words in this list, if we omit the words
not in our dataset of 89,019 hyphenated English
words from CELEX, we get 4,000 words. The
words that are omitted are proper names, contrac-
tions, incomplete words containing apostrophes,
and abbreviations such as DNA. These 4,000 most
frequent words make up 74.93% of the whole cor-
pus.
We evaluate the following methods on the 4000
words: Liang?s method using the American pat-
terns file hyphen.tex, Liang?s method using
the patterns derived from PATGEN when trained
on the whole English dataset, our CRF trained on
the whole English dataset, and the same CRF with
a probability threshold of 0.9. Results are shown
in Table 6. In summary, TEX and PATGEN make
serious errors on 43 and 113 of the 4000 words,
respectively. With a threshold of 0.9, the CRF ap-
proach makes zero serious errors on these words.
8 Timings
Table 7 shows the speed of the alternative meth-
ods for the English dataset. The column ?Fea-
tures/Patterns? in the table reports the number of
feature-functions used for the CRF, or the number
of patterns used for the TEX algorithm. Overall,
the CRF approach is about ten times slower than
the TEX algorithm, but its performance is still ac-
ceptable on a standard personal computer. All ex-
periments use a machine having a Pentium 4 CPU
at 3.20GHz and 2GB memory. Moreover, infor-
mal experiments show that CRF training would be
about eight times faster if we used CRFSGD rather
than CRF++ (Bottou, 2008).
From a theoretical perspective, both methods
have almost-constant time complexity per word if
they are implemented using appropriate data struc-
tures. In TEX, hyphenation patterns are stored in
a data structure that is a variant of a trie. The
CRF software uses other data structures and op-
timizations that allow a word to be hyphenated in
time that is almost independent of the number of
feature-functions used.
9 Conclusions
Finding allowable places in words to insert hy-
phens is a real-world problem that is still not
fully solved in practice. The main contribu-
tion of this paper is a hyphenation method that
is clearly more accurate than the currently used
Knuth/Liang method. The new method is an ap-
Features/ Training Testing Speed
Method Patterns time (s) time (s) (ms/word)
CRF 2916942 372.67 25.386 0.386
TEX (us) 4447 - 2.749 0.042
PATGEN 4488 33.402 2.889 0.044
TALO - - 8.400 0.128
Table 7: Timings for the English dataset (training
and testing on the whole dataset that consists of
65,828 words).
plication of CRFs, which are a major advance of
recent years in machine learning. We hope that
the method proposed here is adopted in practice,
since the number of serious errors that it makes
is about a sixfold improvement over what is cur-
rently in use. A second contribution of this pa-
per is to provide training sets for hyphenation in
English and Dutch, so other researchers can, we
hope, soon invent even more accurate methods. A
third contribution of our work is a demonstration
that current CRF methods can be used straightfor-
wardly for an important application and outper-
form state-of-the-art commercial and open-source
software; we hope that this demonstration acceler-
ates the widespread use of CRFs.
References
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic syllabification with structured
SVMs for letter-to-phoneme conversion. Proceed-
ings of ACL-08: HLT, pages 568?576.
Barbara Beeton. 2002. Hyphenation exception log.
TUGboat, 23(3).
Le?on Bottou. 2008. Stochastic gradient CRF software
CRFSGD. Available at http://leon.bottou.
org/projects/sgd.
Gosse Bouma. 2003. Finite state methods for hyphen-
ation. Natural Language Engineering, 9(1):5?20,
March.
Aron Culotta and Andrew McCallum. 2004. Confi-
dence Estimation for Information Extraction. In Su-
san Dumais, Daniel Marcu, and Salim Roukos, edi-
tors, HLT-NAACL 2004: Short Papers, pages 109?
112, Boston, Massachusetts, USA, May. Associa-
tion for Computational Linguistics.
Fred J. Damerau. 1964. Automatic Hyphenation
Scheme. U.S. patent 3537076 filed June 17, 1964,
issued October 1970.
Gordon D. Friedlander. 1968. Automation comes to
the printing and publishing industry. IEEE Spec-
trum, 5:48?62, April.
373
Alon Halevy, Peter Norvig, and Fernando Pereira.
2009. The Unreasonable Effectiveness of Data.
IEEE Intelligent Systems, 24(2):8?12.
Yannis Haralambous. 2006. New hyphenation tech-
niques in ?2. TUGboat, 27:98?103.
Steven L. Huyser. 1976. AUTO-MA-TIC WORD DI-
VI-SION. SIGDOC Asterisk Journal of Computer
Documentation, 3(5):9?10.
Timo Jarvi. 2009. Computerized Typesetting and
Other New Applications in a Publishing House. In
History of Nordic Computing 2, pages 230?237.
Springer.
Terje Kristensen and Dag Langmyhr. 2001. Two
regimes of computer hyphenation?a comparison.
In Proceedings of the International Joint Confer-
ence on Neural Networks (IJCNN), volume 2, pages
1532?1535.
Taku Kudo, 2007. CRF++: Yet Another CRF
Toolkit. Version 0.5 available at http://crfpp.
sourceforge.net/.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning (ICML),
pages 282?289.
Franklin M. Liang and Peter Breitenlohner, 2008. PAT-
tern GENeration Program for the TEX82 Hyphen-
ator. Electronic documentation of PATGEN pro-
gram version 2.3 from web2c distribution on CTAN,
retrieved 2008.
Franklin M. Liang. 1983. Word Hy-phen-a-tion by
Com-put-er. Ph.D. thesis, Stanford University.
Jorge Nocedal and Stephen J. Wright. 1999. Limited
memory BFGS. In Numerical Optimization, pages
222?247. Springer.
Wolfgang A. Ocker. 1971. A program to hyphenate
English words. IEEE Transactions on Engineering,
Writing and Speech, 14(2):53?59, June.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Terrence J. Sejnowski and Charles R. Rosenberg, 1988.
NETtalk: A parallel network that learns to read
aloud, pages 661?672. MIT Press, Cambridge, MA,
USA.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 134?
141.
Petr Sojka and Pavel Sevecek. 1995. Hyphenation in
TEX?Quo Vadis? TUGboat, 16(3):280?289.
Christos Tsalidis, Giorgos Orphanos, Anna Iordanidou,
and Aristides Vagelatos. 2004. Proofing Tools
Technology at Neurosoft S.A. ArXiv Computer Sci-
ence e-prints, (cs/0408059), August.
P.T.H. Tutelaers, 1999. Afbreken in TEX, hoe werkt dat
nou? Available at ftp://ftp.tue.nl/pub/
tex/afbreken/.
Antal van den Bosch, Ton Weijters, Jaap Van Den
Herik, and Walter Daelemans. 1995. The profit
of learning exceptions. In Proceedings of the 5th
Belgian-Dutch Conference on Machine Learning
(BENELEARN), pages 118?126.
Jaap C. Woestenburg, 2006. *TALO?s Lan-
guage Technology, November. Available at
http://www.talo.nl/talo/download/
documents/Language_Book.pdf.
374
Proceedings of the SIGDIAL 2014 Conference, pages 218?227,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Learning to Re-rank for Interactive Problem Resolution and Query
Refinement
Rashmi Gangadharaiah
IBM Research,
India Research Lab,
Bangalore, KA, India
rashgang@in.ibm.com
Balakrishnan Narayanaswamy and Charles Elkan
Department of CSE,
University of California, San Diego
La Jolla, CA, USA
{muralib, elkan}@cs.ucsd.edu
Abstract
We study the design of an information re-
trieval (IR) system that assists customer
service agents while they interact with
end-users. The type of IR needed is
difficult because of the large lexical gap
between problems as described by cus-
tomers, and solutions. We describe an
approach that bridges this lexical gap by
learning semantic relatedness using tensor
representations. Queries that are short and
vague, which are common in practice, re-
sult in a large number of documents be-
ing retrieved, and a high cognitive load
for customer service agents. We show
how to reduce this burden by providing
suggestions that are selected based on the
learned measures of semantic relatedness.
Experiments show that the approach offers
substantial benefit compared to the use of
standard lexical similarity.
1 Introduction
Information retrieval systems help businesses and
individuals make decisions by automatically ex-
tracting actionable intelligence from large (un-
structured) data (Musen et al., 2006; Antonio
Palma-dos Reis, 1999). This paper focuses on the
application of retrieval systems in a contact cen-
ters where the system assists agents while they are
helping customers with problem resolution.
Currently, most contact center information re-
trieval use (web based) front-ends to search en-
gines indexed with knowledge sources (Holland,
2005). Agents enter queries to retrieve documents
related to the customer?s problem. These sources
are often incomplete as it is unlikely that all pos-
sible customer problems can be identified before
product release. This is particularly true for re-
cently released and frequently updated products.
One approach, which we build on here, is to mine
problems and resolutions from online discussion
forums Yahoo! Answers
1
Ubuntu Forums
2
and
Apple Support Communities
3
. While these often
provide useful solutions within hours or days of
a problem surfacing, they are semantically noisy
(Gangadharaiah and Narayanaswamy, 2013).
Most contact centers and agents are evaluated
based on the number of calls they handle over a
period (Pinedo et al., 2000). As a result, queries
entered by agents into the search engine are usu-
ally underspecified. This, together with noise in
the database, results in a large number of docu-
ments being retrieved as relevant documents. This
in turn, increases the cognitive load on agents, and
reduces the effectiveness of the search system and
the efficiency of the contact center. Our first task
in this paper is to automatically make candidate
suggestions that reduce the search space of rel-
evant documents in a contact center application.
The agent/user then interacts with the system by
selecting one of the suggestions. This is used to
expand the original query and the process can be
repeated. We show that even one round of inter-
action, with a small set of suggestions, can lead to
high quality solutions to user problems.
In query expansion, the classical approach is to
automatically find suggestions either in the form
of words, phrases or similar queries (Kelly et al.,
2009; Feuer et al., 2007; Leung et al., 2008).
These can be obtained either from query logs or
based on their representativeness of the initial re-
trieved documents (Guo et al., 2008; Baeza-yates
et al., 2004). The suggestions are then ranked ei-
ther based on their frequencies or based on their
similarity to the original query (Kelly et al., 2009;
Leung et al., 2008). For example, if suggestions
and queries are represented as term vectors (e.g.
1
http://answers.yahoo.com/
2
http://ubuntuforums.org/
3
https://discussions.apple.com/
218
term frequency-inverse document frequency or tf-
idf) their similarity may be determined using simi-
larity measures such as cosine similarity or inverse
of euclidean distance (Salton and McGill, 1983).
However, in question-answering and problem-
resolution domains, and in contrast to traditional
Information Retrieval, most often the query and
the suggestions do not have many overlapping
words. This leads to low similarity scores, even
when the suggestion is highly relevant. Consider
the representative example in Table 1, taken from
our crawled dataset. Although the suggestions,
?does not support file transfer?, ?connection not
stable?, ?pairing failed? are highly relevant for the
problem of ?Bluetooth not working?, their lexi-
cal similarity score is zero. The second task that
this paper addresses is how to bridge this lexical
chasm between the query and the suggestions. For
this, we learn a measure of semantic-relatedness
between the query and the suggestions rather than
defining closeness based on lexical similarity.
Query Bluetooth not working .
Suggestions devices not discovered,
bluetooth greyed out,
bluetooth device did not respond,
does not support file transfer,
connection not stable,
pairing failed
Table 1: Suggestions for the Query or customer?s
problem, ?Bluetooth not working?.
The primary contributions of this paper are that:
? We show how tensor methods can be used
to learn measures of question-answer or
problem-resolution similarity. In addition,
we show that these learned measures can
be used directly with well studied classifica-
tion techniques like Support Vector Machines
(SVMs) and Logistic Classifiers to classify
whether suggestions are relevant. This results
in substantially improved performance over
using conventional similarity metrics.
? We show that along with the learned similar-
ity metric, a data dependent Information Gain
(which incorporates knowledge about the set
of documents in the database) can be used as
a feature to further boost accuracy.
? We demonstrate the efficacy of our approach
on a complete end-to-end problem-resolution
system, which includes crawled data from
online forums and gold standard user inter-
action annotations.
2 System outline
As discussed in the Introduction, online discus-
sion forums form a rich source of problems and
their corresponding resolutions. Thread initiators
or users of a product facing problems with their
product post in these forums. Other users post
possible solutions to the problem. At the same
time, there is noise due to unstructured content,
off-topic replies and other factors. Our interac-
tion system has two phases, as shown in Figure
1. The offline phase attempts to reduce noise in
the database, while the online phase assists users
deal with the cognitive overload caused by a large
set of retrieved documents. In our paper, threads
form the documents indexed by the system.
The goals of the offline phase are two-fold.
First, to reduce the aforementioned noise in the
database, we succinctly represent each document
(i.e., a thread in online discussion forums) by its
signature, which is composed of units extracted
from the first post of the underlying thread that
best describe the problem discussed in the thread.
Second, the system makes use of click-through
data, where users clicked on relevant suggestions
for their queries to build a relevancy model. As
mentioned before, the primary challenge is to
build a model that can identify units that are se-
mantically similar to a given query.
In the online phase, the agent who acts as the
mediator between the user and the Search Engine
enters the user?s/customer?s query to retrieve rele-
vant documents. From these retrieved documents,
the system then obtains candidate suggestions and
ranks these suggestions using the relevancy model
built in the offline phase to further better under-
stand the query and thereby reduce the space of
documents retrieved. The user then selects the
suggestion that is most relevant to his query. The
retrieved documents are then filtered displaying
only those documents that contain the selected
suggestion in their signatures. The process con-
tinues until the user quits.
2.1 Signatures of documents
In the offline phase, every document (correspond-
ing to a thread in online discussion forums) is
represented by units that best describe a problem.
We adopt the approach suggested in (Gangadhara-
219
iah and Narayanaswamy, 2013) to automatically
generate these signatures from each discussion
thread. We assume that the first post describes
the user?s problem, something we have found to
be true in practice. From the dependency parse
trees of the first posts, we extract three types of
units (i) phrases (e.g., sync server), (ii) attribute-
values (e.g., iOS, 4) and (iii) action-attribute tuples
(e.g., sync server: failed). Phrases form good base
problem descriptors. Attribute-value pairs provide
configurational contexts to the problem. Action-
attribute tuples, as suggested in (Gangadharaiah
and Narayanaswamy, 2013), capture segments of
the first post that indicate user wanting to perform
an action (?I cannot hear notifications on blue-
tooth?) or the problems caused by a user?s action
(?working great before I updated?). These make
them particularly valuable features for problem-
resolution and question-answering.
2.2 Representation of Queries and
Suggestions
Queries are represented as term vectors using the
term frequency-inverse document frequency (tf-
idf) representation forming the query space. The
term frequency is defined as the frequency with
which word appears in the query and the inverse
document frequency for a word is defined as the
frequency of queries in which the word appeared.
Similarly, units are represented as tf-idf term vec-
tors from the suggestion space. Term frequency in
the unit space is defined as the number of times
a word appears in the unit and its inverse docu-
ment frequency is defined in terms of the number
of units in which the word appeared. Since the
vocabulary used in the queries and documents are
different, the representations for queries and units
belong to different spaces of different dimensions.
For every query-unit pair, we learn a measure
of similarity as explained in Section 4. Addi-
tionally, we use similarity features based on co-
sine similarity between the query and the unit un-
der consideration. We also consider an additional
feature based on information gain (Gangadhara-
iah and Narayanaswamy, 2013). In particular, if
S represents the set all retrieved documents, S
1
is
a subset of S (S
1
? S) containing a unit unit
i
and
S
2
is a subset of S that does not contain unit
i
,
information gain with unit
i
is,
Gain(S, unit
i
) = E(S)?
|S
1
|
|S|
E(S
1
)?
|S
2
|
|S|
E(S
2
) (1)
E(S) =
?
k=1,...|S|
?p(doc
k
)log
2
p(doc
k
). (2)
The probability for each document is based on its
rank in the retrieved of results,
p(doc
j
) =
1
rank(doc
j
)
?
k=1,...|S|
1
rank(doc
k
)
. (3)
We crawled posts and threads from online forums
for the products of interest, as detailed in Sec-
tion 5.1, and these form the documents. We used
trial interactions and retrievals to collect the click-
though data, which we used as labeled data for
similarity metric learning. In particular, labels in-
dicate which candidate units were selected as rel-
evant suggestions by a human annotator. We now
explain our training (offline) and testing (online)
phases that use this data in more detail.
2.3 Training
The labeled (click-through) data for training the
relevance model is collected as follows. Anno-
tators were given pairs of queries. Each pair is
composed of an underspecified query and a spe-
cific query (Section 5.1 provides more informa-
tion on the creation of these queries). An un-
derspecified query is a query that reflects what a
user/agent typically enters into the system, and the
corresponding specific query is full-specified ver-
sion of the underspecified query. Annotators were
first asked to query the search engine with each
underspecified query. We use the Lemur search
engine (Strohman et al., 2004). From the resulting
set of retrieved documents, the system uses the in-
formation gain criteria (as given in (1) below) to
rank and display to the annotators the candidate
suggestions (i.e., the units that appear in the signa-
tures of the retrieved documents). Thus, our sys-
tem is bootstrapped using the information gain cri-
terion. The annotators then selects the candidate
suggestion that is most relevant to the correspond-
ing specific query. The interaction with the system
continues until the annotators quit.
We then provide a class label for each unit based
on the collected click-through information. In par-
ticular, if a unit s ? S(x) was clicked by a user for
his query x, from the list S we provide a + la-
bel to indicate that the unit is relevant suggestion
for the query. Similarly, for all other units that are
never clicked by users for x are labeled as?. This
forms the training data for the system. Details on
220
Forum Discussion  Threads Unit Extraction Suggestion units  for first posts 
Search Engine query results Interaction Module Finds suggestions 
Candidate  Suggestions 
User clicks on  (units, query) Learn Relevance Model ! Offline 
Online 
Figure 1: Outline of our interactive query refine-
ment system for problem resolution
the feature extraction and how the model is created
is given in Section 3.
2.4 Testing
In the online phase, the search engine retrieves
documents for the user?s query x
?
. Signatures for
the retrieved documents form the initial space of
candidate units. As done in training, for every pair
of x
?
and unit the label is predicted using the model
built in the training phase. Units that are predicted
as + are shown to the user. When a user clicks
on his most relevant suggestion, the retrieved re-
sults are filtered to show only those documents that
contain the suggestion (i.e., in its signature). This
process continues until the user quits.
3 Model
We consider underspecified queries x ? R
x
d
and
units y ? R
y
d
. Given an underspecified query x
we pass it through a search engine, resulting in a
list of results S(x).
As explained in Section 2.3, our training data
consists of labels r(x, y) ? +1,?1 for each
under-specified query, y ? S(x). r(x, y) = +1
if the unit is labeled a relevant suggestion and
r(x, y) = ?1 if it is not labeled relevant. Units
are relevant or not based on the final query, and
not just y, a distinction we expand upon below.
At each time step, our system proposes a list
Z(x) of possible query refinement suggestions z
to the user. The user can select one or none of
these suggestions. If the user selects z, only those
documents that contain the suggestion (i.e., in its
signature) are shown to the user, resulting in a fil-
tered set of results, S(x+ z).
This process can be repeated until a stopping
criterion is reached. Stopping criterion include the
size of the returned list is smaller than some num-
ber |S(x + z)| < N , in which case all remain-
ing documents are returned. Special cases include
when only one document is returned N = 1. We
will design query suggestions so that |S(x+z)| >
0. Another criterion we use is to return all remain-
ing documents after a certain maximum number of
interactions or until the user quits.
4 Our Approach
We specify our algorithm using a tensor notation.
We do this since tensors appear to subsume most
of the methods applied in practice, where different
algorithms use slightly different costs, losses and
constraints. These ideas are strongly motivated by,
but generalize to some extent, suggestions for this
problem presented in (Elkan, 2010).
For our purposes, we consider tensors as multi-
dimensional arrays, with the number of dimen-
sions defined as the order of the tensor. An M
order tensor X ? R
I
1
?I
2
...I
M
. As such tensors
subsume vectors (1st order tensors) and matrices
(2nd order tensors). The vectorization of a ten-
sor of order M is obtained by stacking elements
from the M dimensions into a vector of length
I
1
? I
2
? . . .? I
M
in the natural way.
The inner product of two tensors is defined as
?X,W? =
I
1
?
i
1
I
2
?
i
2
. . .
I
M
?
i
M
x
i
1
w
i
1
x
i
2
w
i
2
. . . x
i
M
w
i
M
(4)
Analogous to the definition for vectors, the
(Kharti-Rao) outer product A = X ?W of two
tensors X and W has A
ij
= X
i
W
j
where i and j
run over all elements of X and W . Thus, if X is
of order M
X
and W of order M
W
, A is of order
M
A
= M
X
+M
W
.
The particular tensor we are interested in is a
2-D tensor (matrix) X which is the outer product
of query and unit pairs (Feats). In particular, for a
query x and unit y, X
i,j
= x
i
y
j
.
Given this representation, standard classifica-
tion and regression methods from the machine
learning literature can often be extended to deal
with tensors. In our work we consider two clas-
sifiers that have been successful in many applica-
tions, logistic regression and support vector ma-
chines (SVMs) (Bishop, 2006).
221
In the case of logistic regression, the conditional
probability of a reward signal r(X) = r(x, y) is,
p(r(X) = +1) =
1
1 + exp(??X,W?+ b)
(5)
The parameters W and b can be obtained by min-
imizing the log loss L
reg
on the training data D
L
reg
(W, b) = (6)
?
(X,r(X))?D
log(1 + exp(?r(X)?X,W?+ b)
For SVMs with the hinge loss we select param-
eters to minimize L
hinge
,
L
hinge
(W, b) = ||X||
2
F
+ (7)
?
?
(X,r(X))?D
max[0, 1? (r(X)?X,W?+ b)]
where ||X||
F
is the Frobenius norm of tensor X.
Given the number of parameters in our system
(W, b) to limit overfitting, we have to regularize
these parameters. We use regularizers of the form
?(W, b) = ?
W
||W||
F
(8)
such regularizes have been successful in many
large scale machine learning tasks including
learning of high dimensional graphical models
(Ravikumar et al., 2010) and link prediction
(Menon and Elkan, 2011).
Thus, the final optimization problem we are
faced with is of the form
min
W,b
L(W, b) + ?(W, b) (9)
where L is L
reg
or L
hinge
as appropriate. Other
losses, classifiers and regularizers may be used.
The advantage of tensors over their vectorized
counterparts, that may be lost in the notation, is
that they do not lose the information that the dif-
ferent dimensions can (and in our case do) lie in
different spaces. In particular, in our case we use
different features to represent queries and units (as
discussed in Section 2.2) which are not of the same
length, and as a result trivially do not lie in the
same space.
Tensor methods also allow us to regularize the
components of queries and units separately in dif-
ferent ways. This can be done for example by,
i) forcing W = Q
1
Q
2
, where Q
1
and Q
2
are
constrained to be of fixed rank s ii) using trace or
Frobenius norms on Q
1
and Q
2
for separate regu-
larization as proxies for the rank iii) using different
sparsity promoting norms on the rows of Q
1
and
Q
2
iv) weighing these penalties differently for the
two matrices in the final loss function. Note that
by analogy to the vector case, we directly obtain
generalization error guarantees for our methods.
We also discuss the advantage of the tensor
representation above over a natural representation
X = [x; y] i.e. X is the column vector obtained
by stacking the query and unit representations.
Note that in this representation, for logistic regres-
sion, while a change in the query x can change
the probability for a unit P (r(X) = 1) it can-
not change the relative probability of two different
units. Thus, the ordering of all unit remains the
same for all queries. This flaw has been pointed
out in the literature in (Vert and Jacob, 2008) and
(Bai et al., 2009), but was brought to our attention
by (Elkan, 2010).
Finally, we note that by normalizing the query
and unit vectors (x and y), and selecting W = I
(the identity matrix) we can recover the cosine
similarity metric (Elkan, 2010). Thus, our rep-
resentation is atleast as accurate and we show
that learning the diagonal and off-diagonal com-
ponents of W can substantially improve accuracy.
Additionally, for every (query,unit) we also
compute information gain (IG) as given in (1), and
the lexical similarity (Sim) in terms of cosine sim-
ilarity between the query and the unit as additional
features in the feature vectors.
5 Results and Discussion
To evaluate our system, we built and simulated
a contact center information retrieval system for
iPhone problem resolution.
5.1 Description of the Dataset
We collected data by crawling forum discussion
threads from the Apple Discussion Forum, created
during the period 2007-2011, resulting in about
147,000 discussion threads. The underspecified
queries and specific queries were created as fol-
lows. Discussion threads were first clustered treat-
ing each discussion thread as a data point using a
tf-idf representation. The thread nearest the cen-
troid of the 60 largest clusters were marked as the
?most common? problems.
The first post is used as a proxy for the problem
description. An annotator was asked to then create
222
Underspecified query ?Safari not working?
1. safari:crashes
2. safari:cannot find:server
3. server:stopped responding
4. phone:freezes
5. update:failed
Table 2: Specific Queries generated with the un-
derspecified Query, ?Safari not working?.
a short query (underspecified) from the first post
of each of the 60 selected threads. These queries
were given to the Lemur search engine (Strohman
et al., 2004) to retrieve the 50 most similar threads
from an index built on the entire set of 147,000
threads. The annotator manually analyzed the first
posts of the retrieved threads to create contexts,
resulting in a total 200 specific queries.
We give an example to illustrate the data cre-
ation in Table 2. From an under-specified query
?Safari not working?, the annotator found 5 spe-
cific queries. Two other annotators, were given
these specific queries with the search engine?s
results from the corresponding under-specified
query. They were asked to choose the most rel-
evant results for the specific queries. The intersec-
tion of the choices of the annotators formed our
?gold standard? of relevant documents.
5.2 Results
We simulated a contact center retrieval systems (as
in Figure 1) to evaluate the approach proposed in
this paper. To evaluate the generality of our ap-
proach we conduct experiments with both SVMs
and Logistic Regression. Due to lack of space we
illustrate each result for only one kind of classifier.
5.2.1 Evaluating the Relevance Model
To measure the performance of the relevance
model for predicting the class labels or for finding
the most relevant units towards making the user?s
underspecified query more specific, we performed
the following experiment. 4000 random query-
unit pairs were picked from the training data, col-
lected as explained in Section 2. Since most units
are not relevant for a query, 90% of the pairs be-
longed to the ? class. On average, every spe-
cific query gave rise to 2.4 suggestions. Hence,
predicting ? for all pairs still achieves an error
rate of 10%. This data was then split into vary-
ing sizes of training and test sets. The relevancy
model was then built on the training half and the
classifiers were used to predict labels on the test
0 500 1000 1500 2000 2500 3000 3500 40000
0.01
0.02
0.03
0.04
0.05
0.06
Number of training query?suggestion pairs
Erro
r ra
te
 
 
SimFeats?IG?SimFeats+IG+Sim
Figure 2: Performance with Logistic Regression
using different features and various sizes of Train-
ing and Test sets. Feats-IG-Sim does not use co-
sine similarity (Sim) and information gain (IG).
Feats+IG+Sim considers Sim and IG.
set. Figure 2 shows error rate obtained with logis-
tic regression (a similar trend was observed with
SVMs) on various sizes of the training data and
test data. The plot shows that the model (Feats-
IG-Sim and Feats+IG+Sim) performs significantly
better at predicting the relevancy of units for un-
derspecified queries when compared to just us-
ing cosine similarity (Sim) as a feature. Feats-
IG-Sim does not make use of cosine similarity
as a feature or the information gain feature while
Feats+IG+Sim uses both these features for train-
ing the relevancy model and for predicting the rel-
evancy of units. As expected the performance of
the classifier improves as the size of the training
data is increased.
5.2.2 Evaluating the Interaction Engine
We evaluate a complete system with both the user
(the agent) and the search engine in the loop. We
measure the value of the interactions by an analy-
sis of which results ?rise to the top?. Users were
given a specific query and its underspecified query
along with the results obtained when the under-
specified query was input to the search engine.
They were presented with suggestions that were
predicted + for the underspecified query using
SVMs. The user was asked to select the most ap-
propriate suggestion that made the underspecified
query more specific. This process continues until
the user quits either because he is satisfied with the
retrieved results or does not obtain relevant sug-
gestions from the system. For example, for the
underspecified query in Table 2, one of the pre-
dicted suggestions was, ?server:stopped respond-
223
1 2 3 4 5 6 7 8 9 100
0.5
1
1.5
2
2.5
Size of retrieved list
Me
an 
Ave
rag
e P
rec
isio
n
 
 
Baseline
Feats?IG?SimFeats+IG+Sim
Figure 3: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of MAP at N.
ing?. If the user finds the suggestion relevant, he
clicks on it. The selected suggestion then reduces
the number of retrieved results. We then measured
the relevance of the reduced result, with respect
to the gold standard for that specific query, using
metrics used in IR - MRR, Mean Average Preci-
sion (MAP) and Success at rank N.
Figures 3, 4 and Table 3 evaluate the results ob-
tained with the interaction engine using Feats-IG-
Sim and Feats+IG+Sim. We compared the per-
formance of our algorithms with a Baseline that
does not perform any interaction and is evaluated
based on the retrieved results obtained with the un-
derspecified queries. The models for each of the
systems were trained using query-suggestion pairs
collected from 100 specific queries (data collected
as explained in Section 2). The remaining 100 spe-
cific queries were used for testing. We see that the
suggestions predicted by the classifiers using the
relevancy model indeed improves the performance
of the baseline. Also, adding the IG and Sim fea-
ture further boosts the performance of the system.
Systems MRR
Baseline 0.4218
Feats-IG-Sim 0.9449
Feats+IG+Sim 0.9968
Table 3: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of MRR.
5.3 Related Work
Learning affinities between queries an documents
is a well studied area. (Liu, 2009) provides an ex-
cellent survey of these approaches. In these meth-
1 2 3 4 5 6 7 8 9 100.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Size of retrieved list
Suc
ces
s a
t
 
 
Baseline
Feats?IG?SimFeats+IG+Sim
Figure 4: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of Success at N.
ods, there is a fixed feature function ?(x, y) de-
fined between any query-document pair. These
features are then used, along with labeled train-
ing data, to learn the parameters of a model that
can then be used to predict the relevance r(x, y)
of a new query-document pair. The output of the
model can also be used to re-rank the results of a
search engine. In contrast to this class of methods,
we define and parameterize the ? function and
jointly optimize the parameters of the feature map-
ping and the machine learning re-ranking model.
Latent tensor methods for regression and clas-
sification have recently become popular in the im-
age and signal processing domain. Most of these
methods solve an optimization problem similar to
our own (9), but add additional constraints limit-
ing the rank of the learned matrix W either ex-
plicitly or implicit by defining W = Q
1
Q
T
2
, and
defining Q
1
? R
d
x
?d
and Q
2
? R
d
y
?d
. This ap-
proach is used for example in (Pirsiavash et al.,
2009) and more recently in (Tan et al., 2013) (Guo
et al., 2012). While this reduces the number of pa-
rameters to be learned from d
x
d
y
to d(d
x
+ d
y
) it
makes the problem non-convex and introduces an
additional parameter d that must be selected.
This approach of restricting the rank was re-
cently suggested for information retrieval in (Wu
et al., 2013). They look at a regression problem,
using click-through rates as the reward function
r(x, y). In addition, (Wu et al., 2013) does not
use an initial search engine and hence must learn
an affinity function between all query-document
pairs. In contrast to this, we learn a classification
function that discriminates between the true and
false positive documents that are deemed similar
224
by the search engine. This has three beneficial ef-
fects : (i) it reduces the amount of labeled training
data required and the imbalance between the posi-
tive and negative classes which can make learning
difficult (He and Garcia, 2009) and (ii) allows us
to build on the strengths of fast and strong existing
search engines increasing accuracy and decreas-
ing retrieval time and (iii) allows the learnt model
to focus learning on the query-document pairs that
are most problematic for the search engine.
Bilinear forms of tensor models without the
rank restriction have recently been studied for link
prediction (Menon and Elkan, 2011) and image
processing (Kobayashi and Otsu, 2012). Since
the applications are different, there is no prelimi-
nary search engine which retrieves results, making
them ranking methods and ours a re-ranking ap-
proach. Related work in text IR includes (Beefer-
man and Berger, 2000), where two queries are
considered semantically similar if their clicks lead
to the same page. However, the probability that
different queries lead to common clicks of the
same URLs is very small, again increasing the
training data required. Approaches in the past
have also proposed techniques to automatically
find suggestions either in the form of words,
phrases (Kelly et al., 2009; Feuer et al., 2007;
Baeza-yates et al., 2004) or similar queries (Leung
et al., 2008) from query logs (Guo et al., 2008;
Baeza-yates et al., 2004) or based on their prob-
ability of representing the initial retrieved doc-
uments (Kelly et al., 2009; Feuer et al., 2007).
These suggestions are then ranked either based on
their frequencies or based on their closeness to the
query. Closeness is defined in terms of lexical sim-
ilarity to the query. However, most often the query
and the suggestions do not have any co-occurring
words leading to low similarity scores, even when
the suggestion is relevant.
(Gangadharaiah and Narayanaswamy, 2013)
use information gain to rank candidate sugges-
tions. However, the relevancy of the suggestions
highly depends on the relevancy of the initial re-
trieved documents. Our work here addresses the
question of how to bridge this lexical chasm be-
tween the query and the suggestions. For this, we
use semantic-relatedness between the query and
the suggestions as a measure of closeness rather
than defining closeness based on lexical similar-
ity. A related approach to handle this lexical gap
by applying alignment techniques from Statistical
Machine translation (Brown et al., 1993), in par-
ticular by building translation models for infor-
mation retrieval (Berger and Lafferty, 1999; Rie-
zler et al., 2007). These approaches require train-
ing data in the form of question-answer pairs, are
again limited to words or phrases and are not in-
tended for understanding the user?s problem better
through interaction, which is our focus.
6 Conclusions, Discussions and Future
Work
We studied the problem of designing Information
Retrieval systems for interactive problem resolu-
tion. We developed a system for bridging the
large lexical gap between short, incomplete prob-
lem queries and documents in a database of reso-
lutions. We showed that tensor representations are
a useful tool to learn measures of semantic relat-
edness, beyond the cosine similarity metric. Our
results show that with interaction, suggestions can
be effective in pruning large sets of retrieved doc-
uments. We showed that our approach offers sub-
stantial improvement over systems that only use
lexical similarities for retrieval and re-ranking, in
an end-to-end problem-resolution domain.
In addition to the classification losses consid-
ered in this paper, we can also use another loss
term based on ideas from recommender systems,
in particular (Menon and Elkan, 2011). Consider
the matrix T with all training queries as rows and
all units as the columns. If we view the query
refinement problem as a matrix completion prob-
lem, it is natural to assume that this matrix has low
rank, so that T can be written as T = U?V
T
,
where ? is a diagonal matrix and parameter of our
optimization. These can then be incorporated into
the training process by appropriate changes to the
cost and regularization terms.
Another benefit of the tensor representation is
that it can easily be extended to incorporate other
meta-information that may be available. For ex-
ample, if context sensitive features, like the iden-
tity of the agent, are available these can be incor-
porated as another dimension in the tensor. While
optimization over these higher dimensional ten-
sors may be more computationally complex, the
problems are still convex and can be solved ef-
ficiently. This is a direction of future research
we are pursuing. Finally, exploring the power of
information gain type features in larger database
systems is of interest.
225
References
Fatemeh Zahedi Antonio Palma-dos Reis. 1999. De-
signing personalized intelligent financial decision
support systems.
Ricardo Baeza-yates, Carlos Hurtado, and Marcelo
Mendoza. 2004. Query recommendation us-
ing query logs in search engines. In In Interna-
tional Workshop on Clustering Information over the
Web (ClustWeb, in conjunction with EDBT), Creete,
pages 588?596. Springer.
Bing Bai, Jason Weston, David Grangier, Ronan Col-
lobert, Kunihiko Sadamasa, Yanjun Qi, Corinna
Cortes, and Mehryar Mohri. 2009. Polynomial se-
mantic indexing. In NIPS, pages 64?72.
Doug Beeferman and Adam Berger. 2000. Agglomer-
ative clustering of a search engine query log. In Pro-
ceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ?00, pages 407?416, New York, NY, USA.
ACM.
Adam Berger and John Lafferty. 1999. Information
retrieval as statistical translation. In Proceedings of
the 22Nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?99, pages 222?229, New York,
NY, USA. ACM.
Christopher M Bishop. 2006. Pattern recognition and
machine learning.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Charles Elkan. 2010. Learning affinity with biliear
models. Unpublished Notes.
Alan Feuer, Stefan Savev, and Javed A. Aslam. 2007.
Evaluation of phrasal query suggestions. In Pro-
ceedings of the Sixteenth ACM Conference on Con-
ference on Information and Knowledge Manage-
ment, CIKM ?07, pages 841?848, New York, NY,
USA. ACM.
Rashmi Gangadharaiah and Balakrishnan
Narayanaswamy. 2013. Natural language query
refinement for problem resolution from crowd-
sourced semi-structured data. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 243?251, Nagoya,
Japan, October. Asian Federation of Natural
Language Processing.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Sung-Hyon Myaeng, Douglas W. Oard,
Fabrizio Sebastiani, Tat-Seng Chua, and Mun-Kew
Leong, editors, SIGIR, pages 379?386. ACM.
Weiwei Guo, Irene Kotsia, and Ioannis Patras. 2012.
Tensor learning for regression. Image Processing,
IEEE Transactions on, 21(2):816?827.
Haibo He and Edwardo A Garcia. 2009. Learning
from imbalanced data. Knowledge and Data Engi-
neering, IEEE Transactions on, 21(9):1263?1284.
Alexander Holland. 2005. Modeling uncertainty in
decision support systems for customer call center.
In Computational Intelligence, Theory and Applica-
tions, pages 763?770. Springer.
Diane Kelly, Karl Gyllstrom, and Earl W. Bailey. 2009.
A comparison of query and term suggestion fea-
tures for interactive searching. In Proceedings of the
32Nd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
SIGIR ?09, pages 371?378, New York, NY, USA.
ACM.
Takumi Kobayashi and Nobuyuki Otsu. 2012. Effi-
cient optimization for low-rank integrated bilinear
classifiers. In Computer Vision?ECCV 2012, pages
474?487. Springer.
Kenneth Wai-Ting Leung, Wilfred Ng, and Dik Lun
Lee. 2008. Personalized concept-based clustering
of search engine queries. IEEE Trans. on Knowl.
and Data Eng., 20(11):1505?1518, November.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225?331.
Aditya Krishna Menon and Charles Elkan. 2011. Link
prediction via matrix factorization. In Machine
Learning and Knowledge Discovery in Databases,
pages 437?452. Springer.
Mark A Musen, Yuval Shahar, and Edward H Short-
liffe. 2006. Clinical decision-support systems.
Michael Pinedo, Sridhar Seshadri, and J George Shan-
thikumar. 2000. Call centers in financial services:
strategies, technologies, and operations. In Cre-
ating Value in Financial Services, pages 357?388.
Springer.
Hamed Pirsiavash, Deva Ramanan, and Charless
Fowlkes. 2009. Bilinear classifiers for visual recog-
nition. In NIPS, pages 1482?1490.
Pradeep Ravikumar, Martin J Wainwright, and John D
Lafferty. 2010. High-dimensional ising model se-
lection using 1-regularized logistic regression. The
Annals of Statistics, 38(3):1287?1319.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 464?471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
226
Gerard Salton and Michael J McGill. 1983. Introduc-
tion to modern information retrieval.
T. Strohman, D. Metzler, H. Turtle, and W. B. Croft.
2004. Indri: A language model-based search engine
for complex queries. Proceedings of the Interna-
tional Conference on Intelligence Analysis.
Xu Tan, Yin Zhang, Siliang Tang, Jian Shao, Fei Wu,
and Yueting Zhuang. 2013. Logistic tensor re-
gression for classification. In Intelligent Science
and Intelligent Data Engineering, pages 573?581.
Springer.
Jean-Philippe Vert and Laurent Jacob. 2008. Machine
learning for in silico virtual screening and chemical
genomics: new strategies. Combinatorial chemistry
& high throughput screening, 11(8):677.
Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learn-
ing bilinear model for matching queries and docu-
ments. The Journal of Machine Learning Research,
14(1):2519?2548.
227
