Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Anchor Text Extraction for Academic Search 
 
 
Shuming Shi1     Fei Xing2*     Mingjie Zhu3*     Zaiqing Nie1     Ji-Rong Wen1 
1Microsoft Research Asia 
2Alibaba Group, China 
3University of Science and Technology of China 
{shumings, znie, jrwen}@microsoft.com 
fei_c_xing@yahoo.com; mjzhu@ustc.edu 
 
 
 
Abstract* 
 
Anchor text plays a special important role in 
improving the performance of general Web 
search, due to the fact that it is relatively ob-
jective description for a Web page by poten-
tially a large number of other Web pages. 
Academic Search provides indexing and 
search functionality for academic articles. It 
may be desirable to utilize anchor text in aca-
demic search as well to improve the search re-
sults quality. The main challenge here is that 
no explicit URLs and anchor text is available 
for academic articles. In this paper we define 
and automatically assign a pseudo-URL for 
each academic article. And a machine learning 
approach is adopted to extract pseudo-anchor 
text for academic articles, by exploiting the ci-
tation relationship between them. The ex-
tracted pseudo-anchor text is then indexed and 
involved in the relevance score computation of 
academic articles. Experiments conducted on 
0.9 million research papers show that our ap-
proach is able to dramatically improve search 
performance. 
1 Introduction 
Anchor text is a piece of clickable text that links 
to a target Web page. In general Web search, 
anchor text plays an extremely important role in 
improving the search quality. The main reason 
for this is that anchor text actually aggregates the 
opinion (which is more comprehensive, accurate, 
and objective) of a potentially large number of 
people for a Web page. 
                                                 
* This work was performed when Fei Xing and Mingjie Zhu 
were interns at Microsoft Research Asia. 
In recent years, academic search (Giles et al, 
1998; Lawrence et al, 1999; Nie et al, 2005; 
Chakrabarti et al, 2006) has become an impor-
tant supplement to general web search for re-
trieving research articles. Several academic 
search systems (including Google Scholar?, Cite-
seer?, DBLP?, Libra**, ArnetMiner??, etc.) have 
been deployed. In order to improve the results 
quality of an academic search system, we may 
consider exploiting the techniques which are 
demonstrated to be quite useful and critical in 
general Web search. In this paper, we study the 
possibility of extracting anchor text for research 
papers and using them to improve the search per-
formance of an academic search system. 
 
 
Figure 1. An example of one paper citing other papers 
 
The basic search unit in most academic search 
systems is a research paper. Borrowing the con-
cepts of URL and anchor-text in general Web 
search, we may need to assign a pseudo-URL for 
one research paper as its identifier and to define 
the pseudo-anchor text for it by the contextual 
description when this paper is referenced (or 
mentioned). The pseudo-URL of a research pa-
per could be the combination of its title, authors 
and publication information. Figure-1 shows an 
excerpt where one paper cites a couple of other 
                                                 
? http://scholar.google.com/ 
? http://citeseerx.ist.psu.edu/ 
? http://www.informatik.uni-trier.de/~ley/db/ 
** http://libra.msra.cn/ 
?? http://www.arnetminer.org/ 
10
papers. The grayed text can be treated as the 
pseudo-anchor text of the papers being refe-
renced. Once the pseudo-anchor text of research 
papers is acquired, it can be indexed and utilized 
to help ranking, just as in general web search. 
However it remains a challenging task to cor-
rectly identify and extract these pseudo-URLs 
and pseudo-anchor texts. First, unlike the situa-
tion in general web search where one unique 
URL is assigned to each web page as a natural 
identifier, the information of research papers 
need to be extracted from web pages or PDF files. 
As a result, in constructing pseudo-URLs for 
research papers, we may face the problem of ex-
traction errors, typos, and the case of one re-
search paper having different expressions in dif-
ferent places. Second, in general Web search, 
anchor text is always explicitly specified by 
HTML tags (<a> and </a>). It is however much 
harder to perform anchor text extraction for re-
search papers. For example, human knowledge 
may be required in Figure-1 to accurately identi-
fy the description of every cited paper. 
To address the above challenges, we propose 
an approach for extracting and utilizing pseudo-
anchor text information in academic search to 
improve the search results quality. Our approach 
is composed of three phases. In the first phase, 
each time a paper is cited in another paper, we 
construct a tentative pseudo-URL for the cited 
paper and extract a candidate anchor block for it. 
The tentative pseudo-URL and the candidate 
anchor block are allowed to be inaccurate. In the 
second phase, we merge the tentative pseudo-
URLs that should represent the same paper. All 
candidate anchor blocks belong to the same pa-
per are grouped accordingly in this phase. In the 
third phase, the final pseudo-anchor text of each 
paper is generated from all its candidate blocks, 
by adopting a SVM-based machine learning me-
thodology. We conduct experiments upon a data-
set containing 0.9 million research papers. The 
experimental results show that lots of useful anc-
hor text can be successfully extracted and accu-
mulated using our approach, and the ultimate 
search performance is dramatically improved 
when anchor information is indexed and used for 
paper ranking. 
The remaining part of this paper is organized 
as follows. In Section 2, we describe in detail our 
approach for pseudo-anchor text extraction and 
accumulation. Experimental results are reported 
in Section 3. We discuss related work in Section 
4 and finally conclude the paper in Section 5. 
2 Our Approach 
2.1 Overview 
Before describing our approach in detail, we first 
recall how anchor text is processed in general 
Web search. Assume that there have been a col-
lection of documents being crawled and stored 
on local disk. In the first step, each web page is 
parsed and the out links (or forward links) within 
the page are extracted. Each link is comprised of 
a URL and its corresponding anchor text. In the 
second step, all links are accumulated according 
to their destination URLs (i.e. the anchor texts of 
all links pointed to the same URL are merged). 
Thus, we can get al anchor text corresponding to 
each web page. Figure-2 (a) demonstrates this 
process. 
 
 
Figure 2. The main process of extracting (a) anchor 
text in general web search and (b) pseudo-anchor text 
in academic search 
 
For academic search, we need to extract and 
parse the text content of papers. When a paper A 
mentions another paper B, it either explicitly or 
implicitly displays the key information of B to let 
the users know that it is referencing B instead of 
other papers. Such information can be extracted 
to construct the tentative pseudo-URL of B. The 
pseudo-URLs constructed in this phase are tenta-
tive because different tentative pseudo-URLs 
may be merged to generate the same final pseu-
do-URL. All information related to paper B in 
different papers can be accumulated and treated 
Web pages 
HTML parsing 
Links 
Anchor text 
for pages 
 
Group by link 
destination 
Papers 
Paper parsing 
Tentative pseudo-URLs 
Candidate anchor blocks 
Anchor block accumulation 
Papers with their  
candidate anchor blocks 
Papers with their  
pseudo-anchor text 
Anchor-text learning 
11
as the potential anchor text of B. Our goal is to 
get the anchor text related to each paper. 
Our approach for pseudo-anchor text extrac-
tion is shown in Figure-2 (b). The key process is 
similar to that in general Web search for accumu-
lating and utilizing page anchor text. One prima-
ry difference between Figure-2 (a) and (b) is the 
latter accumulates candidate anchor blocks rather 
than pieces of anchor text. A candidate anchor 
block is a piece of text that contains the descrip-
tion of one paper. The basic idea is: Instead of 
extracting the anchor text for a paper directly (a 
difficult task because of the lack of enough in-
formation), we first construct a candidate anchor 
block to contain the "possible" or "potential" de-
scription of the paper. After we accumulate all 
candidate anchor blocks, we have more informa-
tion to provide a better estimation about which 
pieces of texts are anchor texts. Following this 
idea, our proposed approach adopts a three-phase 
methodology to extract pseudo-anchor text. In 
the first phase, each time a paper B appearing in 
another paper A, a candidate anchor block is ex-
tracted for B. All candidate anchor blocks belong 
to the same paper are grouped in the second 
phase. In the third phase, the final pseudo-anchor 
text of each paper is selected among all candidate 
blocks. 
Extracting tentative pseudo-URLs and can-
didate anchor blocks: When one paper cites 
another paper, a piece of short text (e.g. "[1]" or 
?(xxx et al, 2008)?) is commonly inserted to 
represent the paper to be cited, and the detail in-
formation (key attributes) of it are typically put 
at the end of the document (in the references sec-
tion). We call each paper listed in the references 
section a reference item. The references section 
can be located by searching for the last occur-
rence of term 'reference' or 'references' in larger 
fonts. Then, we adopt a rule-based approach to 
divide the text in the references section into ref-
erence items. Another rule-based approach is 
used to extract paper attributes (title, authors, 
year, etc) from a reference item. We observed 
some errors in our resulting pseudo-URLs caused 
by the quality of HTML files converted from 
PDF format, reference item extraction errors, 
paper attribute extraction errors, and other fac-
tors. We also observed different reference item 
formats for the same paper. The pseudo-URL for 
a paper is defined according to its title, authors, 
publisher, and publication year, because these 
four kinds of information can readily be used to 
identify a paper. 
For each citation of a paper, we treat the sen-
tence containing the reference point (or citation 
point) as one candidate anchor block. When mul-
tiple papers are cited in one sentence, we treat 
the sentence as the candidate anchor block of 
every destination paper. 
Candidate Anchor Block Accumulation: 
This phase is in charge of merging all candidate 
blocks of the same pseudo-URL. As has been 
discussed, tentative pseudo-URLs are often inac-
curate; and different tentative pseudo-URLs may 
correspond to the same paper. The primary chal-
lenge here is perform the task in an efficient way 
and with high accuracy. We will address this 
problem in Subsection 2.2. 
Pseudo-Anchor Generation: In the previous 
phase, all candidate blocks of each paper have 
been accumulated. This phase is to generate the 
final anchor text for each paper from all its can-
didate blocks. Please refer to Subsection 2.3 for 
details. 
2.2 Candidate Anchor Block Accumulation 
via Multiple Feature-String Hashing 
Consider this problem: Given a potentially huge 
number of tentative pseudo-URLs for papers, we 
need to identify and merge the tentative pseudo-
URLs that represent the same paper. This is like 
the problems in the record linkage (Fellegi and 
Sunter, 1969), entity matching, and data integra-
tion which have been extensively studied in da-
tabase, AI, and other areas. In this sub-section, 
we will first show the major challenges and the 
previous similar work on this kind of problem. 
Then a possible approach is described to achieve 
a trade-off between accuracy and efficiency. 
 
 
Figure 3. Two tentative pseudo-URLs representing 
the same paper 
 
2.2.1 Challenges and candidate techniques 
Two issues should be addressed for this problem: 
similarity measurement, and the efficiency of the 
algorithm. On one hand, a proper similarity func-
tion is needed to identify two tentative pseudo-
URLs representing the same paper. Second, the 
12
integration process has to be accomplished effi-
ciently. 
We choose to compute the similarity between 
two papers to be a linear combination of the si-
milarities on the following fields: title, authors, 
venue (conference/journal name), and year. The 
similarity function on each field is carefully de-
signed. For paper title, we adopt a term-level edit 
distance to compute similarity. And for paper 
authors, person name abbreviation is considered. 
The similarity function we adopted is fairly well 
in accuracy (e.g., the similarity between the two 
pseudo-URLs in Figure-3 is high according to 
our function); but it is quite time-consuming to 
compute the similarity for each pair of papers 
(roughly 1012 similarity computation operations 
are needed for 1 million different tentative pseu-
do-URLs). 
Some existing methods are available for de-
creasing the times of similarity calculation opera-
tions. McCallum et al (2000) addresses this high 
dimensional data clustering problem by dividing 
data into overlapping subsets called canopies 
according to a cheap, approximate distance mea-
surement. Then the clustering process is per-
formed by measuring the exact distances only 
between objects from the same canopy. There are 
also other subspace methods (Parsons et al, 2004) 
in data clustering areas, where data are divided 
into subspaces of high dimensional spaces first 
and then processing is done in these subspaces. 
Also there are fast blocking approaches for 
record linkage in Baxter et al (2003). Though 
they may have different names, they hold similar 
ideas of dividing data into subsets to reduce the 
candidate comparison records. The size of data-
set used in the above papers is typically quite 
small (about thousands of data items). For effi-
ciency issue, Broder et al (1997) proposed a 
shingling approach to detect similar Web pages. 
They noticed that it is infeasible to compare 
sketches (which are generated by shingling) of 
all pairs of documents. So they built an inverted 
index that contains a list of shingle values and 
the documents they appearing in. With the in-
verted index, they can effectively generate a list 
of all the pairs of documents that share any shin-
gles, along with the number of shingles they 
have in common. They did experiments on a da-
taset containing 30 million documents. 
By adopting the main ideas of the above tech-
niques to our pseudo-URL matching problem, a 
possible approach can be as follows. 
 
 
Figure 4. The Multiple Feature-String Hashing algo-
rithm for candidate anchor block accumulation 
 
2.2.2 Method adopted 
The method utilized here for candidate anchor 
block accumulation is shown in Figure 4. The 
main idea is to construct a certain number of fea-
ture strings for a tentative pseudo-URL (abbre-
viated as TP-URL) and do hash for the feature 
strings. A feature string of a paper is a small 
piece of text which records a part of the paper?s 
key information, satisfying the following condi-
tions: First, multiple feature strings can typically 
be built from a TP-URL. Second, if two TP-
URLs are different representations of the same 
paper, then the probability that they have at least 
one common feature string is extremely high. We 
can choose the term-level n-grams of paper titles 
(referring to Section 3.4) as feature strings. 
The algorithm maintains an in-memory hash-
table which contains a lot of slots each of which 
is a list of TP-URLs belonging to this slot. For 
each TP-URL, feature strings are generated and 
hashed by a specified hash function. The TP-
URL is then added into some slots according to 
the hash values of its feature strings. Any two 
TP-URLs belonging to the same slot are further 
compared by utilizing our similarity function. If 
their similarity is larger than a threshold, the two 
TP-URLs are treated as being the same and 
therefore their corresponding candidate anchor 
blocks are merged. 
The above algorithm tries to achieve good bal-
ance between accuracy and performance. On one 
hand, compared with the na?ve algorithm of per-
forming one-one comparison between all pairs of 
TP-URLs, the algorithm needs only to compute 
Algorithm Multiple Feature-String Hashing for candidate anchor 
block accumulation 
Input: A list of papers (with their tentative pseudo-URLs 
and candidate anchor blocks) 
Output: Papers with all candidate anchor blocks of the 
same paper aggregated 
 
Initial: An empty hashtable h (each slot of h is a list of pa-
pers) 
For each paper A in the input list { 
For each feature-string of A { 
Lookup by the feature-string in h to get a slot s; 
Add A into s; 
} 
} 
For each slot s with size smaller than a threshold { 
For any two papers A1, A2 in s { 
float fSim = Similarity(A1, A2); 
if(fSim > the specified threshold) { 
Merge A1 and A2; 
} 
} 
} 
13
the similarity for the TP-URLs that share a 
common slot. On the other hand, because of the 
special property of feature strings, most TP-
URLs representing the same paper can be de-
tected and merged. 
The basic idea of dividing data into over-
lapped subsets is inherited from McCallum et al 
(2000), Broder et al (1997), and some subspace 
clustering approaches. Slightly different, we do 
not count the number of common feature strings 
between TP-URLs. Common bins (or inverted 
indices) between data points are calculated in 
McCallum et al (2000) as a ?cheap distance? for 
creating canopies. The number of common Shin-
gles between two Web documents is calculated 
(efficiently via inverted indices), such that Jac-
card similarity could be used to measure the si-
milarity between them. In our case, we simply 
compare any two TP-URLs in the same slot by 
using our similarity function directly. 
The effective and efficiency of this algorithm 
depend on the selection of feature strings. For a 
fixed feature string generation method, the per-
formance of this algorithm is affected by the size 
of each slot, especially the number and size of 
big slots (slots with size larger than a threshold). 
Big slots will be discarded in the algorithm to 
improve performance, just like removing com-
mon Shingles in Broder et al (1997). In Section 
4, we conduct experiments to test the perfor-
mance of the above algorithm with different fea-
ture string functions and different slot size thre-
sholds. 
2.3 Pseudo-Anchor Text Learning 
In this subsection, we address the problem of 
extracting the final pseudo-anchor text for a pa-
per, given all its candidate anchor blocks (see 
Figure 5 for an example). 
2.3.1 Problem definition 
A candidate anchor block is a piece of text with 
one or some reference points (a reference point is 
one occurrence of citation in a paper) specified, 
where a reference point is denoted by a 
<start_pos, end_pos> pair (means start position 
and end position respectively): ref = <start_pos, 
end_pos>. We represent a candidate anchor 
block to be the following format, 
AnchorBlock = (Text, ref1, ref2, ?) 
We define a block set to be a set of candidate 
anchor blocks for a paper, 
BlockSet = {AnchorBlock1, AnchorBlock2, ?} 
Now the problem is: Given a block set con-
taining N elements, extract some text excerpts 
from them as the anchor text of the paper. 
2.3.2 Learn term weights 
We adopt a machine-learning approach to assign, 
for each term in the anchor blocks, a discrete de-
gree of being anchor text. The main reasons for 
taking such an approach is twofold: First, we 
believe that assigning each term a fuzzy degree 
of being anchor text is more appropriate than a 
binary judgment as either an anchor-term or non-
anchor-term. Second, since the importance of a 
term for a ?link? may be determined by many 
factors in paper search, a machine-learning could 
be more flexible and general than the approaches 
that compute term degrees by a specially de-
signed formula. 
 
 
Figure 5. The candidate pseudo-anchor blocks of a 
paper 
 
The features used for learning are listed in Ta-
ble-1. 
We observed that it would be more effective if 
some of the above features are normalized before 
being used for learning. For a term in candidate 
anchor block B, its TF are normalized by the 
BM25 formula (Robertson et al, 1999), 
 
TFL
Bbbk
TFkTFnorm ?????
???
)||)1((
)1(
1
1
 
 
where L is average length of the candidate blocks, 
|B| is the length of B, and k1, b are parameters. 
DF is normalized by the following formula, 
 )1log( DF
NIDF ??
  
where N is the number of elements in the block 
set (i.e. total number of candidate anchor blocks 
for the current paper). 
Features RefPos and Dist are normalized as, 
 
RefPosnorm = RefPos / |B| 
Distnorm = (Dist-RefPos) / |B| 
 
And the feature BlockLen is normalized as, 
14
 BlockLennorm = log(1+BlockLen)  
 
Features Description 
DF 
Document frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of all papers. It is used to indicate whether the 
term is a stop word or not. 
BF 
Block frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of this paper. 
CTF 
Collection term frequency: Total number of times the 
term appearing in the blocks. For multiple times of 
occurrences in one block, all of them are counted. 
IsInURL 
Specify whether the term appears in the pseudo-URL 
of the paper. 
TF 
Term frequency: Number of times the terms appearing 
in the candidate block. 
Dist 
Directed distance from the nearest reference point to 
the term location 
RefPos 
Position of the nearest reference point in the candidate 
pseudo-anchor block. 
BlockLen Length of the candidate pseudo-anchor block 
Table 1. Features for learning 
 
We set four term importance levels, from 1 
(unrelated terms or stop words) to 4 (words par-
ticipating in describing the main ideas of the pa-
per). 
We choose support vector machine (SVM) for 
learning term weights here, because of its power-
ful classification ability and well generalization 
ability (Burges, 1998). We believe some other 
machine learning techniques should also work 
here. The input of the classifier is a feature vec-
tor of a term and the output is the importance 
level of the term. Given a set of training data 
? ?liii levelfeature 1, ?, a decision function f(x) can be 
acquired after training. Using the decision func-
tion, we can assign an importance level for each 
term automatically. 
 
3 Experiments 
3.1 Experimental Setup 
Our experimental dataset contains 0.9 million 
papers crawled from the web. All the papers are 
processed according to the process in Figure-2 
(b). We randomly select 300 queries from the 
query log of Libra (libra.msra.cn) and retrieve 
the results in our indexing and ranking system 
with/without the pseudo-anchors generated by 
our approach. Then the volunteer researchers and 
students in our group are involved to judge the 
search results. The top 30 results of different 
ranking algorithms for each query are labeled 
and assigned a relevance value from 1 (meaning 
'poor match') to 5 (meaning 'perfect match'). The 
search results quality is measured by NDCG 
(Jarvelin and Kekalainen, 2000). 
3.2 Overall Effect of our Approach 
Figure 6 shows the performance comparison be-
tween the results of two baseline paper ranking 
algorithms and the results of including pseudo-
anchor text in ranking. 
 
0.466
0.426
0.388
0.597
0.619
0.689
0.673 0.672
0.627
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
NDCG@1 NDCG@3 NDCG@10
Base(Without CitationCount)
Base
Pseudo-Anchor Included
 
Figure 6. Comparison between the baseline approach 
and our approach (measure: nDCG) 
 
The ?Base? algorithm considers the title, ab-
stract, full-text and static-rank (which is a func-
tion of the citation count) of a paper. In a bit 
more detail, for each paper, we adopt the BM25 
formula (Robertson et al, 1999) over its title, 
abstract, and full-text respectively. And then the 
resulting score is linearly combined with the stat-
ic-rank to get its final score. The static-rank is 
computed as follows, 
 StaticRank = log(1+CitationCount) (3.1) 
To test the performance of including pseudo-
anchor text in ranking, we compute an anchor 
score for each paper and linearly combine it with 
its baseline score (i.e. the score computed by the 
baseline algorithm). 
We tried two kinds of ways for anchor score 
computation. The first is to merge all pieces of 
anchor excerpts (extracted in the previous section) 
into a larger piece of anchor text, and use BM25 
to compute its relevance score. In another ap-
proach called homogeneous evidence combina-
tion (Shi et al, 2006), a relevance score is com-
puted for each anchor excerpt (still using BM25), 
and all the scores for the excerpts are sorted des-
cending and then combined by the following 
formula, 
 ?
?
?????
m
i
ianchor sicS 1 2))1(1(
1 (3.2) 
where si (i=1, ?, m) are scores for the m anchor 
excerpts, and c is a parameter. The primary idea 
15
here is to let larger scores to have relative greater 
weights. Please refer to Shi et al (2006) for a 
justification of this approach. As we get slightly 
better results with the latter way, we use it as our 
final choice for computing anchor scores. 
From Figure 6, we can see that the overall per-
formance is greatly improved by including pseu-
do-anchor information. Table 2 shows the t-test 
results, where a ?>? indicates that the algorithm 
in the row outperforms that in the column with a 
p-value of 0.05 or less, and a ?>>? means a p-
value of 0.01 or less. 
 
 
 
Base 
Base (without 
CitationCount) 
Our approach > >> 
Base  >> 
Base (without Cita-
tionCount) 
  
Table 2. Statistical significance tests (t-test over 
nDCG@3) 
 
Table 3 shows the performance comparison by 
using some traditional IR measures based on bi-
nary judgments. Since the results of not includ-
ing CitationCount are much worse than the other 
two, we omit it in the table. 
 
Measure 
Approach 
MAP MRR P@1 P@10 
Base (including 
CitationCount) 
0.364 0.727 0.613 0.501 
Our Approach 0.381 0.734 0.625 0.531 
Table 3. Performance compassion using binary judg-
ment measures 
 
3.3 Sample Query Analysis 
Here we analyze some sample queries to get 
some insights about why and how pseudo-anchor 
improves search performance. Figure-7 and Fig-
ure-8 show the top-3 results of two sample que-
ries: {TF-IDF} and {Page Rank}. 
For query "TF-IDF", the top results of the 
baseline approach have keyword "TF-IDF" ap-
peared in the title as well as in other places of the 
papers. Although the returned papers are relevant 
to the query, they are not excellent because typi-
cally users may want to get the first TF-IDF pa-
per or some papers introducing TF-IDF. When 
pseudo-anchor information is involved, some 
excellent results (B1, B2, B3) are generated. The 
main reason for getting the improved results is 
that these papers (or books) are described with 
"TF-IDF" when lots of other papers cite them. 
 
 
Figure 7. Top-3 results for query TF-IDF 
 
 
Figure 8. Top-3 results for query Page Rank 
 
Figure-8 shows another example about how 
pseudo-anchor helps to improve search results 
quality. For query "Page Rank" (note that there is 
a space in between), the results returned by the 
baseline approach are not satisfactory. In the pa-
pers returned by our approach, at least B1 and B2 
are very good results. Although they did not la-
bel themselves "Page Rank", other papers do so 
in citing them. Interestingly, although the result 
B3 is not about the "PageRank" algorithm, it de-
scribes another popular "Page Rank" algorithm 
in addition to PageRank. 
Another interesting observation from the two 
figures is that our approach retrieves older papers 
than the baseline method, because old papers 
tend to have more anchor text (due to more cita-
tions). So our approach may not be suitable for 
retrieve newer papers. To overcome this problem, 
maybe publication year should be considered in 
our ranking functions. 
3.4 Anchor Accumulation Experiments 
We conduct experiments to test the effectiveness 
and efficiency of the multiple-feature-string-
hashing algorithm presented in Section 2.2. The 
duplication detection quality of this algorithm is 
determined by the appropriate selection of fea-
A1. V Safronov, M Parashar, Y Wang et al Optimizing Web servers 
using Page rank prefetching for clustered accesses. Information 
Sciences. 2003. 
A2. AO Mendelzon, D Rafiei. An autonomous page ranking method for 
metasearch engines. WWW, 2002. 
A3. FB Kalhoff. On formally real Division Algebras and Quasifields of 
Rank two. 
(a) Without anchor 
B1. S Brin, L Page. The Anatomy of a Large-Scale Hypertextual Web 
Search Engine. WWW, 1998 
B2. L Page, S Brin, R Motwani, T Winograd. The pagerank citation 
ranking: Bringing order to the web. 1998. 
B3. JM Kleinberg. Authoritative sources in a hyperlinked environment. 
Journal of the ACM, 1999. 
(b) With anchor 
 
A1. K Sugiyama, K Hatano, M Yoshikawa, S Uemura. Refinement of TF-
IDF schemes for web pages using their hyperlinked neighboring pages. 
Hypertext?03 
A2. A Aizawa. An information-theoretic perspective of tf-idf measures. 
IPM?03. 
A3. N Oren. Reexamining tf.idf based information retrieval with Genet-
ic Programming. SAICSIT?02. 
(a) Without anchor 
B1. G Salton, MJ McGill. Introduction to Modern Information Retriev-
al. McGraw-Hill, 1983. 
B2. G Salton and C Buckley. Term weighting approaches in automatic 
text retrieval. IPM?98. 
B3. R Baeza-Yates, B Ribeiro-Neto. Modern Information Retrieval. 
Addison-Wesley, 1999 
(b) With anchor 
 
16
ture strings. When feature strings are fixed, the 
slot size threshold can be used to tune the tra-
deoff between accuracy and performance. 
 
Feature Strings 
Slot Distr. 
Ungram Bigram Trigram 4-gram 
# of Slots 1.4*105 1.2*106 2.8*106 3.4*106 
# of Slots with 
size > 100 
5240 6806 1541 253 
# of Slots with 
size > 1000 
998 363 50 5 
# of Slots with 
size > 10000 
59 11 0 0 
Table 4. Slot distribution with different feature strings 
 
We take all the papers extracted from PDF 
files as input to run the algorithm. Identical TP-
URLs are first eliminated (therefore their candi-
date anchor blocks are merged) by utilizing a 
hash table. This pre-process step results in about 
1.46 million distinct TP-URLs. The number is 
larger than our collection size (0.9 million), be-
cause some cited papers are not in our paper col-
lection. We tested four kinds of feature strings all 
of which are generated from paper title: uni-
grams, bigrams, trigrams, and 4-grams. Table-4 
shows the slot size distribution corresponding to 
each kind of feature strings. The performance 
comparison among different feature strings and 
slot size thresholds is shown in Table 5. It seems 
that bigrams achieve a good trade-off between 
accuracy and performance. 
 
Feature 
Strings 
Slot Size 
Threshold 
Dup. papers 
Detected 
Processing 
Time (sec) 
Unigram 
5000 529,717  119,739.0  
500 327,357 7,552.7  
Bigram 500 528,981 8,229.6  
Trigram 
Infinite 518,564 8,420.4  
500 516,369 2,654.9  
4-gram 500 482,299 1,138.2  
Table 5. Performance comparison between different 
feature strings and slot size thresholds 
 
4 Related Work 
There has been some work which uses anchor 
text or their surrounding text for various Web 
information retrieval tasks. It was known at the 
very beginning era of internet that anchor text 
was useful to Web search (McBryan, 1994). 
Most Web search engines now use anchor text as 
primary and power evidence for improving 
search performance. The idea of using contextual 
text in a certain vicinity of the anchor text was 
proposed in Chakrabarti et al (1998) to automat-
ically compile some lists of authoritative Web 
resources on a range of topics. An anchor win-
dow approach is proposed in Chakrabarti et al
(1998) to extract implicit anchor text. Following 
this work, anchor windows were considered in 
some other tasks (Amitay  et al, 1998; Haveli-
wala et al, 2002; Davison, 2002; Attardi et al, 
1999). Although we are inspired by these ideas, 
our work is different because research papers 
have many different properties from Web pages. 
From the viewpoint of implicit anchor extraction 
techniques, our approach is different from the 
anchor window approach. The anchor window 
approach is somewhat simpler and easy to im-
plement than ours. However, our method is more 
general and flexible. In our approach, the anchor 
text is not necessarily to be in a window. 
Citeseer (Giles et al, 1998; Lawrence  et al, 
1999) has been doing a lot of valuable work on 
citation recognition, reference matching, and pa-
per indexing. It has been displaying contextual 
information for cited papers. This feature has 
been shown to be helpful and useful for re-
searchers. Differently, we are using context de-
scription for improving ranking rather than dis-
play purpose. In addition to Citeseer, some other 
work (McCallum et al, 1999; Nanba and Oku-
mura, 1999; Nanba et al, 2004; Shi et al, 2006) 
is also available for extracting and accumulating 
reference information for research papers. 
5 Conclusions and Future Work 
In this paper, we propose to improve academic 
search by utilizing pseudo-anchor information. 
As pseudo-URL and pseudo-anchor text are not 
as explicit as in general web search, more efforts 
are needed for pseudo-anchor extraction. Our 
machine-learning approach has proven success-
ful in automatically extracting implicit anchor 
text. By using the pseudo-anchors in our academ-
ic search system, we see a significant perfor-
mance improvement over the basic approach. 
 
 
Acknowledgments 
We would like to thank Yunxiao Ma and Pu 
Wang for converting paper full-text from PDF to 
HTML format. Jian Shen has been helping us do 
some reference extraction and matching work. 
Special thanks are given to the researchers and 
students taking part in data labeling. 
 
 
 
 
17
References 
E. Amitay. 1998. Using common hypertext links to 
identify the best phrasal description of target web 
documents. In Proc. of the SIGIR'98 Post Confe-
rence Workshop on Hypertext Information Re-
trieval for the Web, Melbourne, Australia. 
G. Attardi, A. Gulli, and F. Sebastiani. 1999. Theseus: 
categorization by context. In Proceedings of the 8th 
International World Wide Web Conference. 
A. Baxter, P. Christen, T. Churches. 2003. A compar-
ison of fast blocking methods for record linkage. In 
ACM SIGKDD'03 Workshop on Data Cleaning, 
Record Linkage and Object consolidation. Wash-
ington DC. 
A. Broder, S. Glassman, M. Manasse, and G. Zweig. 
1997. Syntactic clustering of the Web. In Proceed-
ings of the Sixth International World Wide Web 
Conference, pp. 391-404. 
C.J.C. Burges. 1998. A tutorial on support vector ma-
chines for pattern recognition. Data Mining and 
Knowledge Discovery, 2, 121-167. 
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. 
Raghavan, and S. Rajagopalan. 1998. Automatic 
resource list compilation by analyzing hyperlink 
structure and associated text. In Proceedings of the 
7th International World Wide Web Conference. 
K. Chakrabarti, V. Ganti, J. Han, and D. Xin. 2006. 
Ranking objects based on relationships. In SIG-
MOD ?06: Proceedings of the 2006 ACM SIG-
MOD international conference on Management of 
data, pages 371?382, New York, NY, USA. ACM. 
B. Davison. 2000. Topical locality in the web. In SI-
GIR'00: Proceedings of the 23rd annual interna-
tional ACM SIGIR conference on Research and 
development in information retrieval, pages 272- 
279, New York, NY, USA. ACM. 
I.P. Fellegi, and A.B. Sunter. A Theory for Record 
Linkage, Journal of the American Statistical Asso-
ciation, 64, (1969), 1183-1210. 
C. L. Giles, K. Bollacker, and S. Lawrence. 1998. 
CiteSeer: An automatic citation indexing system. 
In IanWitten, Rob Akscyn, and Frank M. Shipman 
III, editors, Digital Libraries 98 - The Third ACM 
Conference on Digital Libraries, pages 89?98, 
Pittsburgh, PA, June 23?26. ACM Press. 
T.H. Haveliwala, A. Gionis, D. Klein, and P. Indyk. 
2002. Evaluating strategies for similarity search on 
the web. In WWW ?02: Proceedings of the 11th in-
ternational conference on World Wide Web, pages 
432?442, New York, NY, USA. ACM. 
K. Jarvelin, and J. Kekalainen. 2000. IR Evaluation 
Methods for Retrieving Highly Relevant Docu-
ments. In Proceedings of the 23rd Annual Interna-
tional ACM SIGIR Conference on Research and 
Development in Information Retrieval (SI-
GIR2000). 
S. Lawrence, C.L. Giles, and K. Bollacker. 1999. Dig-
ital libraries and Autonomous Citation Indexing. 
IEEE Computer, 32(6):67?71. 
A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 
1999. Building Domain-specific Search Engines 
with Machine Learning Techniques. In Proceed-
ings of the AAAI-99 Spring Symposium on Intelli-
gent Agents in Cyberspace. 
A. McCallum, K. Nigam, and L. Ungar. 2000. Effi-
cient clustering of high-dimensional data sets with 
application to reference matching. In Proc. 6th 
ACM SIGKDD Int. Conf. on Knowledge Discov-
ery and Data Mining. 
O.A. McBryan. 1994. Genvl and wwww: Tools for 
taming the web. In In Proceedings of the First In-
ternational World Wide Web Conference, pages 
79-90. 
H. Nanba, M. Okumura. 1999. Towards Multi-paper 
Summarization Using Reference Information. In 
Proc. of the 16th International Joint Conference on 
Artificial Intelligence, pp.926-931. 
H. Nanba, T. Abekawa, M. Okumura, and S. Saito. 
2004. Bilingual PRESRI: Integration of Multiple 
Research Paper Databases. In Proc. of RIAO 2004, 
195-211. 
L. Parsons, E. Haque, H. Liu. 2004. Subspace cluster-
ing for high dimensional data: a review. SIGKDD 
Explorations 6(1): 90-105. 
S.E. Robertson, S. Walker, and M. Beaulieu. 1999. 
Okapi at TREC-7: automatic ad hoc, filtering, VLC 
and filtering tracks. In Proceedings of TREC?99. 
S. Shi, R. Song, and J-R Wen. 2006. Latent Additivity: 
Combining Homogeneous Evidence. Technique 
report, MSR-TR-2006-110, Microsoft Research, 
August 2006. 
S. Shi, F. Xing, M. Zhu, Z.Nie, and J.-R. Wen. 2006. 
Pseudo-Anchor Extraction for Search Vertical Ob-
jects. In Proc. of the 2006 ACM 15th Conference 
on Information and Knowledge Management. Ar-
lington, USA. 
Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. 2005. 
Object-level ranking: bringing order to web objects. 
InWWW?05: Proceedings of the 14th international 
conference on World Wide Web, pages 567?574, 
New York, NY, USA. ACM. 
 
18
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 430?439,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Mining Name Translations from Entity Graph Mapping?
Gae-won You? Seung-won Hwang? Young-In Song? Long Jiang? Zaiqing Nie?
?Pohang University of Science and Technology, Pohang, Republic of Korea
{gwyou,swhwang}@postech.ac.kr
?Microsoft Research Asia, Beijing, China
{yosong,longj,znie}@microsoft.com
Abstract
This paper studies the problem of mining en-
tity translation, specifically, mining English
and Chinese name pairs. Existing efforts
can be categorized into (a) a transliteration-
based approach leveraging phonetic similar-
ity and (b) a corpus-based approach exploiting
bilingual co-occurrences, each of which suf-
fers from inaccuracy and scarcity respectively.
In clear contrast, we use unleveraged re-
sources of monolingual entity co-occurrences,
crawled from entity search engines, repre-
sented as two entity-relationship graphs ex-
tracted from two language corpora respec-
tively. Our problem is then abstracted as find-
ing correct mappings across two graphs. To
achieve this goal, we propose a holistic ap-
proach, of exploiting both transliteration sim-
ilarity and monolingual co-occurrences. This
approach, building upon monolingual corpora,
complements existing corpus-based work, re-
quiring scarce resources of parallel or compa-
rable corpus, while significantly boosting the
accuracy of transliteration-based work. We
validate our proposed system using real-life
datasets.
1 Introduction
Entity translation aims at mapping the entity names
(e.g., people, locations, and organizations) in source
language into their corresponding names in target
language. While high quality entity translation is es-
sential in cross-lingual information access and trans-
?This work was done when the first two authors visited Mi-
crosoft Research Asia.
lation, it is non-trivial to achieve, due to the chal-
lenge that entity translation, though typically bear-
ing pronunciation similarity, can also be arbitrary,
e.g., Jackie Chan and ? (pronounced Cheng
Long). Existing efforts to address these challenges
can be categorized into transliteration- and corpus-
based approaches. Transliteration-based approaches
(Wan and Verspoor, 1998; Knight and Graehl, 1998)
identify translations based on pronunciation similar-
ity, while corpus-based approaches mine bilingual
co-occurrences of translation pairs obtained from
parallel (Kupiec, 1993; Feng et al, 2004) or compa-
rable (Fung and Yee, 1998) corpora, or alternatively
mined from bilingual sentences (Lin et al, 2008;
Jiang et al, 2009). These two approaches have com-
plementary strength? transliteration-based similar-
ity can be computed for any name pair but cannot
mine translations of little (or none) phonetic simi-
larity. Corpus-based similarity can support arbitrary
translations, but require highly scarce resources of
bilingual co-occurrences, obtained from parallel or
comparable bilingual corpora.
In this paper, we propose a holistic approach,
leveraging both transliteration- and corpus-based
similarity. Our key contribution is to replace the
use of scarce resources of bilingual co-occurrences
with the use of untapped and significantly larger
resources of monolingual co-occurrences for trans-
lation. In particular, we extract monolingual co-
occurrences of entities from English and Chinese
Web corpora, which are readily available from en-
tity search engines such as PeopleEntityCube1, de-
ployed by Microsoft Research Asia. Such engine
1http://people.entitycube.com
430
automatically extracts people names from text and
their co-occurrences to retrieve related entities based
on co-occurrences. To illustrate, Figure 1(a) demon-
strates the query result for ?Bill Gates,? retrieving
and visualizing the ?entity-relationship graph? of re-
lated people names that frequently co-occur with
Bill in English corpus. Similarly, entity-relationship
graphs can be built over other language corpora, as
Figure 1(b) demonstrates the corresponding results
for the same query, from Renlifang2 on ChineseWeb
corpus. From this point on, for the sake of simplic-
ity, we refer to English and Chinese graphs, simply
asGe andGc respectively. Though we illustrate with
English-Chinese pairs in the paper, our method can
be easily adapted to other language pairs.
In particular, we propose a novel approach of ab-
stracting entity translation as a graph matching prob-
lem of two graphsGe andGc in Figures 1(a) and (b).
Specifically, the similarity between two nodes ve
and vc in Ge and Gc is initialized as their transliter-
ation similarity, which is iteratively refined based on
relational similarity obtained from monolingual co-
occurrences. To illustrate this, an English news ar-
ticle mentioning ?Bill Gates? and ?Melinda Gates?
evidences a relationship between the two entities,
which can be quantified from their co-occurrences
in the entire English Web corpus. Similarly, we
can mine Chinese news articles to obtain the re-
lationships between ???? and ???H??
?. Once these two bilingual graphs of people and
their relationships are harvested, entity translation
can leverage these parallel relationships to further
evidence the mapping between translation pairs, as
Figure 1(c) illustrates.
To highlight the advantage of our proposed ap-
proach, we compare our results with commercial
machine translators (1) Engkoo3 developed in Mi-
crosoft Research Asia and (2) Google Translator4.
In particular, Figure 2 reports the precision for two
groups? ?heads? that belong to top-100 popular peo-
ple (determined by the number of hits), among ran-
domly sampled 304 people names5 from six graph
pairs of size 1,000 each, and the remaining ?tails?.
Commercial translators such as Google, leveraging
2http://renlifang.msra.cn
3http://www.engkoo.com
4http://translate.google.com
5See Section 4 for the sampling process.
Ours Google Engkoo0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Pre
cis
ion
 
 
Tail
Head
Figure 2: Comparison for Head and Tail datasets
bilingual co-occurrences that are scarce for tails,
show significantly lower precision for tails. Mean-
while, our work, depending solely on monolin-
gual co-occurrences, shows high precision, for both
heads and tails.
Our focus is to boost translation accuracy for
long tails with non-trivial Web occurrences in each
monolingual corpus, but not with much bilingual co-
occurrences, e.g., researchers publishing actively in
two languages but not famous enough to be featured
in multi-lingual Wikipedia entries or news articles.
As existing translators are already highly accurate
for popular heads, this focus well addresses the re-
maining challenges for entity translation.
To summarize, we believe that this paper has the
following contributions:
? We abstract entity translation problem as
a graph mapping between entity-relationship
graphs in two languages.
? We develop an effective matching algo-
rithm leveraging both pronunciation and co-
occurrence similarity. This holistic approach
complements existing approaches and en-
hances the translation coverage and accuracy.
? We validate the effectiveness of our approach
using various real-life datasets.
The rest of this paper is organized as follows. Sec-
tion 2 reviews existing work. Section 3 then devel-
ops our framework. Section 4 reports experimental
results and Section 5 concludes our work.
431
(a) English PeopleEntityCube Ge (b) Chinese Renlifang Gc
(c) Abstracting translation as graph mapping
Figure 1: Illustration of entity-relationship graphs
2 Related Work
In this section, we first survey related efforts, cate-
gorized into transliteration-based and corpus-based
approaches. Our approach leveraging both is com-
plementary to these efforts.
2.1 Transliteration-based Approaches
Many name translations are loosely based on
phonetic similarity, which naturally inspires
transliteration-based translation of finding the
translation with the closest pronunciation similarity,
using either rule-based (Wan and Verspoor, 1998) or
statistical (Knight and Graehl, 1998; Li et al, 2004)
approaches. However, people are free to designate
arbitrary bilingual names of little (or none) pho-
netic similarity, for which the transliteration-based
approach is not effective.
2.2 Corpus-based Approaches
Corpus-based approach can mine arbitrary transla-
tion pairs, by mining bilingual co-occurrences from
parallel and comparable bilingual corpora. Using
parallel corpora (Kupiec, 1993; Feng et al, 2004),
e.g., bilingual Wikipedia entries on the same per-
son, renders high accuracy but suffers from high
scarcity. To alleviate such scarcity, (Fung and Yee,
432
1998; Shao and Ng, 2004) explore a more vast re-
source of comparable corpora, which share no par-
allel document- or sentence-alignments as in paral-
lel corpora but describe similar contents in two lan-
guages, e.g., news articles on the same event. Al-
ternatively, (Lin et al, 2008) extracts bilingual co-
occurrences from bilingual sentences, such as an-
notating terms with their corresponding translations
in English inside parentheses. Similarly, (Jiang et
al., 2009) identifies potential translation pairs from
bilingual sentences using lexical pattern analysis.
2.3 Holistic Approaches
The complementary strength of the above two ap-
proaches naturally calls for a holistic approach,
such as recent work combining transliteration-
and corpus-based similarity mining bilingual co-
occurrences using general search engines. Specifi-
cally, (Al-Onaizan and Knight, 2002) uses translit-
eration to generate candidates and then web corpora
to identify translations. Later, (Jiang et al, 2007)
enhances to use transliteration to guide web mining.
Our work is also a holistic approach, but leverag-
ing significantly larger corpora, specifically by ex-
ploiting monolingual co-occurrences. Such expan-
sion enables to translate ?long-tail? people entities
with non-trivial Web occurrences in each monolin-
gual corpus, but not much bilingual co-occurrences.
Specifically, we initialize name pair similarity using
transliteration-based approach, and iteratively rein-
forces base similarity using relational similarity.
3 Our Framework
Given two graphsGe = (Ve, Ee) andGc = (Vc, Ec)
harvested from English and Chinese corpora respec-
tively, our goal is to find translation pairs, or a set S
of matching node pairs such that S ? Ve ? Vc. Let
R be a |Ve|-by-|Vc| matrix where each Rij denotes
the similarity between two nodes i ? Ve and j ? Vc.
Overall, with the matrix R, our approach consists
of the following three steps, as we will discuss in the
following three sections respectively:
1. Initialization: computing base translation sim-
ilarities Rij between two entity nodes using
transliteration similarity
2. Reinforcement model: reinforcing the trans-
lation similarities Rij by exploiting the mono-
lingual co-occurrences
3. Matching extraction: extracting the matching
pairs from the final translation similarities Rij
3.1 Initialization with Transliteration
We initialize the translation similarity Rij as the
transliteration similarity. This section explains how
to get the transliteration similarity between English
and Chinese names using an unsupervised approach.
Formally, let an English name Ne =
(e1, e2, ? ? ? , en) and a Chinese name Nc =
(c1, c2, ? ? ? , cm) be given, where ei is an English
word and Ne is a sequence of the words, and ci
is a Chinese character and Nc is a sequence of
the characters. Our goal is to compute a score
indicating the similarity between the pronunciations
of the two names.
We first convert Nc into its Pinyin representation
PYc = (s1, s2, ? ? ? , sm), where si is the Pinyin rep-
resentation of ci. Pinyin is the romanization rep-
resentation of pronunciation of Chinese character.
For example, the Pinyin representation of Ne =
(?Barack?, ?Obama?) is PYc =(?ba?, ?la?, ?ke?,
?ao?, ?ba?, ?ma?). The Pinyin representations of
Chinese characters can be easily obtained from Chi-
nese character pronunciation dictionary. In our ex-
periments, we use an in-house dictionary, which
contains pronunciations of 20, 774 Chinese charac-
ters. For the Chinese characters having multiple pro-
nunciations, we only use the most popular one.
Calculation of transliteration similarity between
Ne and Nc is now transformed to calculation of pro-
nunciation similarity between Ne and PYc. Because
letters in Chinese Pinyins and English strings are
pronounced similarly, we can further approximate
pronunciation similarity between Ne and PYc us-
ing their spelling similarity. In this paper, we use
Edit Distance (ED) to measure the spelling similar-
ity. Moreover, since words in Ne are transliterated
into characters in PYc independently, it is more ac-
curate to compute the ED between Ne and PYc, i.e.,
EDname(Ne, PYc), as the sum of the EDs of all
component transliteration pairs, i.e., every ei in Ne
and its corresponding transliteration (si) in PYc. In
other words, we need to first align all sj?s in PYc
with corresponding ei in Ne based on whether they
433
are translations of each other. Then based on the
alignment, we can calculate EDname(Ne, PYc) us-
ing the following formula.
EDname(Ne, PYc) =
?
i
ED(ei, esi) (1)
where esi is a string generated by concatenating all
si?s that are aligned to ei and ED(ei, esi) is the
Edit Distance between ei and esi, i.e., the mini-
mum number of edit operations (including inser-
tion, deletion and substitution) needed to transform
ei into esi. Because an English word usually con-
sists of multiple syllables but every Chinese charac-
ter consists of only one syllable, when aligning ei?s
with sj?s, we add the constraint that each ei is al-
lowed to be aligned with 0 to 4 si?s but each si can
only be aligned with 0 to 1 ei. To get the align-
ment between PYc and Ne which has the minimal
EDname(Ne, PYc), we use a Dynamic Program-
ming based algorithm as defined in the following
formula:
EDname(N1,ie , PY 1,jc ) = min(
EDname(N1,i?1e , PY 1,jc ) + Len(ei),
EDname(N1,ie , PY 1,j?1c ) + Len(sj),
EDname(N1,i?1e , PY 1,j?1c ) + ED(ei, sj),
EDname(N1,i?1e , PY 1,j?2c ) + ED(ei, PY j?1,jc ),
EDname(N1,i?1e , PY 1,j?3c ) + ED(ei, PY j?2,jc ),
EDname(N1,i?1e , PY 1,j?4c ) + ED(ei, PY j?3,jc ))
where, given a sequence X = (x1, x2, ? ? ?)
such that xi is a word, X i,j is the subsequence
(xi, xi+1, ? ? ? , xj) of X and Len(X) is the number
of letters except spaces in the sequence X . For in-
stance, the minimal Edit Distance between the En-
glish name ?Barack Obama? and the Chinese Pinyin
representation ?ba la ke ao ba ma? is 4, as the
best alignment is: ?Barack? ? ?ba la ke? (ED: 3),
?Obama?? ?ao ba ma? (ED: 1). Finally the translit-
eration similarity between Nc and Ne is calculated
using the following formula.
Simtl(Nc, Ne) = 1?
EDname(Ne, PYc)
Len(PYc) + Len(Ne)
(2)
For example, Simtl(?Barack Obama?, ??n
.???j?) is 1? 411+12 = 0.826.
3.2 Reinforcement Model
From the initial similarity, we model our problem as
an iterative approach that iteratively reinforces the
similarityRij of the nodes i and j from the matching
similarities of their neighbor nodes u and v.
The basic intuition is built on exploiting the sim-
ilarity between monolingual co-occurrences of two
different languages. In particular, we assume two
entities with strong relationship co-occur frequently
in both corpora. In order to express this intuition, we
formally define an iterative reinforcement model as
follows. Let Rtij denote the similarity of nodes i and
j at t-th iteration:
Rt+1ij = ?
?
(u,v)k?Bt(i,j,?)
Rtuv
2k
+ (1? ?)R0ij (3)
The model is expressed as a linear combination
of (a) the relational similarity
?
Rtuv/2k and (b)
transliteration similarity R0ij . (? is the coefficient
for interpolating two similarities.)
In the relational similarity, Bt(i, j, ?) is an or-
dered set of the best matching pairs between neigh-
bor nodes of i and ones of j such that ?(u, v)k ?
Bt(i, j, ?), Rtuv ? ?, where (u, v)k is the match-
ing pair with k-th highest similarity score. We con-
sider (u, v) with similarity over some threshold ?,
or Rtuv ? ?, as a matching pair. In this neighbor
matching process, if many-to-many matches exist,
we select only one with the greatest matching score.
Figure 3 describes such matching process more for-
mally. N(i) andN(j) are the sets of neighbor nodes
of i and j, respectively, and H is a priority queue
sorting pairs in the decreasing order of similarity
scores.
Meanwhile, note that, in order to express that
the confidence for matching (i, j) progressively con-
verges as the number of matched neighbors in-
creases, we empirically use decaying coefficient
1/2k for Rtuv, because
??
k=1 1/2k = 1.
3.3 Matching Extraction
After the convergence of the above model, we get
the |Ve|-by-|Vc| similarity matrix R?. From this
matrix, we extract one-to-one matches maximizing
the overall similarity.
More formally, this problem can be stated as
the maximum weighted bipartite matching (West,
434
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
Bt(i, j, ?)? {}
?u ? N(i),?v ? N(j) : H.push(u, v;Rtuv)
while H is not empty do
(u, v; s)? H.pop()
if s < ? then
break
end if
if neither u nor v are matched yet then
Bt(i, j, ?)? Bt(i, j, ?) ? {(u, v)}
end if
end while
return Bt(i, j, ?)
Figure 3: How to get the ordered set Bt(i, j, ?)
2000)? Given two groups of entities Ve and Vc from
the two graphs Ge and Gc, we can build a weighted
bipartite graph is G = (V,E), where V = Ve ? Vc
and E is a set of edges (u, v) with weight R?uv. To
filter out null alignment, we construct only the edges
with weight R?uv ? ?. From this bipartite graph,
the maximum weighted bipartite matching problem
finds a set of pairwise non-adjacent edges S ? E
such that
?
(u,v)?S R?uv is the maximum. Well-
known algorithms include Hungarian algorithm with
time complexity of O(|V |2 log |V |+ |V ||E|) (West,
2000).
In this paper, to speed up processing, we consider
a greedy alternative with the following steps? (1)
choose the pair with the highest similarity score, (2)
remove the corresponding row and column from the
matrix, and (3) repeat (1) and (2) until their match-
ing scores are over a specific threshold ?.
4 Experiments
This section reports our experimental results to eval-
uate our proposed approach. First, we report our ex-
perimental setting in Section 4.1. Second, we vali-
date the effectiveness and the scalability of our ap-
proach over a real-life dataset in Section 4.2.
4.1 Experimental Settings
This section describes (1) how we collect the En-
glish and Chinese EntityCube datasets, (2) how to
build ground-truth test datasets for evaluating our
framework, and (3) how to set up three parameters
?, ?, and ?.
First, we crawled Ge = (Ve, Ee) and Gc =
(Vc, Ec) from English and Chinese EntityCubes.
Specifically, we built a graph pairs (Ge, Gc) expand-
ing from a ?seed pair? of nodes se ? Ve and sc ? Vc
until the number of nodes for each graph becomes
1,0006. More specifically, when we build a graph
Ge by expanding from se, we use a queue Q. We
first initialize Q by pushing the seed node se. We
then iteratively pop a node ve from Q, save ve into
Ve, and push its neighbor nodes in decreasing order
of co-occurrence scores with ve. Similarly, we can
get Gc from a counterpart seed node vc. By using
this procedure, we built six graph pairs from six dif-
ferent seed pairs. In particular, the six seed nodes
are English names and its corresponding Chinese
names representing a wide range of occupation do-
mains (e.g., ?Barack Obama,? ?Bill Gates,? ?Britney
Spears,? ?Bruno Senna,? ?Chris Paul,? and ?Eminem?)
as Table 1 depicts. Meanwhile, though we demon-
strate the effectiveness of the proposed method for
mining name translations in Chinese and English
languages, the method can be easily adapted to other
language pairs.
Table 1: Summary for graphs and test datasets obtained
from each seed pair
i |Ve|, |Vc| |Ti| English Name Chinese Name
1 1,000 51 Barack Obama ?n.???j
2 1,000 52 Bill Gates ??
3 1,000 40 Britney Spears Y}????
4 1,000 53 Bruno Senna Y0L??
5 1,000 51 Chris Paul .????[
6 1,000 57 Eminem ???
Second, we manually searched for about 50
?ground-truth? matched translations for each graph
pair to build test datasets Ti, by randomly selecting
nodes within two hops7 from the seed pair (se, sc),
since nodes outside two hops may include nodes
whose neighbors are not fully crawled. More specif-
ically, due to our crawling process expanding to add
neighbors from the seed, the nodes close to the seed
have all the neighbors they would have in the full
graph, while those far from the node may not. In or-
der to pick the nodes that well represent the actual
6Note, this is just a default setting, which we later increase
for scalability evaluation in Figure 6.
7Note that the numbers of nodes within two hops in Ge and
Gc are 327 and 399 on average respectively.
435
neighbors, we built test datasets among those within
two hops. However, this crawling is used for the
evaluation sake only, and thus does not suggest the
bias in our proposed framework. Table 1 describes
the size of such test dataset for each graph pair.
Lastly, we set up the three parameters ?, ?, and
? using 6-fold cross validation with 6 test datasets
Ti?s of the graphs. More specifically, for each
dataset Ti, we decide ?i and ?i such that average
MRR for the other 5 test datasets is maximized.
(About MRR, see more details of Equation (4) in
Section 4.2.) We then decide ?i such that average
F1-score is maximized. Figure 4 shows the average
MRR for ?i and ?i with default values ? = 0.66
and ? = 0.2. Based on these results, we set ?i with
values {0.2, 0.15, 0.2, 0.15, 0.2, 0.15} that optimize
MRR in datasets T1, . . . T6, and similarly ?i with
{0.67, 0.65, 0.67, 0.67, 0.65, 0.67}. We also set ?i
with values {0.63, 0.63, 0.61, 0.61, 0.61, 0.61} opti-
mizing F1-score with the same default values ? =
0.2 and ? = 0.66. We can observe the variances
of optimal parameter setting values are low, which
suggests the robustness of our framework.
4.2 Experimental Results
This section reports our experimental results using
the evaluation datasets explained in previous sec-
tion. For each graph pair, we evaluated the ef-
fectiveness of (1) reinforcement model using MRR
measure in Section 4.2.1 and (2) overall framework
using precision, recall, and F1 measures in Sec-
tion 4.2.2. We also validated (3) scalability of our
framework over larger scale of graphs (with up to
five thousand nodes) in Section 4.2.3. (In all experi-
mental results, Bold numbers indicate the best per-
formance for each metric.)
4.2.1 Effectiveness of reinforcement model
We evaluated the reinforcement model over
MRR (Voorhees, 2001), the average of the recipro-
cal ranks of the query results as the following for-
mula:
MRR = 1
|Q|
?
q?Q
1
rankq
(4)
Each q is a ground-truth matched pair (u, v) such
that u ? Ve and v ? Vc, and rankq is the rank of the
similarity score of Ruv among all Ruk?s such that
k ? Vc. Q is a set of such queries. By comparing
MRRs for two matricesR0 andR?, we can validate
the effectiveness of the reinforcement model.
? Baseline matrix (R0): using only the translit-
eration similarity score, i.e., without reinforce-
ment
? Reinforced matrix (R?): using the reinforced
similarity score obtained from Equation (3)
Table 2: MRR of baseline and reinforced matrices
Set MRRBaseline R0 Reinforced R?
T1 0.6964 0.8377
T2 0.6213 0.7581
T3 0.7095 0.7989
T4 0.8159 0.8378
T5 0.6984 0.8158
T6 0.5982 0.8011
Average 0.6900 0.8082
We empirically observed that the iterative model
converges within 5 iterations. In all experiments, we
used 5 iterations for the reinforcement.
Table 2 summarizes our experimental results. As
these figures show, MRR scores significantly in-
crease after applying our reinforcement model ex-
cept for the set T4 (on average from 69% to 81%),
which indirectly shows the effectiveness of our rein-
forcement model.
4.2.2 Effectiveness of overall framework
Based on the reinforced matrix, we evaluated
the effectiveness of our overall matching framework
using the following three measures?(1) precision:
how accurately the method returns matching pairs,
(2) recall: how many the method returns correct
matching pairs, and (3) F1-score: the harmonic
mean of precision and recall. We compared our ap-
proach with a baseline, mapping two graphs with
only transliteration similarity.
? Baseline: in matching extraction, using R0 as
the similarity matrix by bypassing the rein-
forcement step
? Ours: using R?, the similarity matrix con-
verged by Equation (3)
436
0.1 0.15 0.2 0.25 0.30.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
?(?=0.66)
AVG
(MR
R)
 
 
?1?2?3?4?5?6
0.61 0.63 0.65 0.67 0.690.74
0.76
0.78
0.8
0.82
0.84
? (?=0.2)
AVG
(MR
R)
 
 
?1
?2
?3
?4
?5
?6
0.57 0.59 0.61 0.63 0.650.68
0.69
0.7
0.71
0.72
0.73
0.74
?(?=0.2, ?=0.66)
AVG
(F1?
scor
e)
 
 
?1?2?3?4?5?6
Figure 4: Parameter setup for ?, ?, and ?
In addition, we compared ours with the machine
translators of Engkoo and Google. Table 3 summa-
rizes our experimental results.
As this table shows, our approach results in the
highest precision (about 80% on average) without
compromising the best recall of Google, i.e., 61%
of Google vs. 63% of ours. Overall, our approach
outperforms others in all three measures.
Meanwhile, in order to validate the translation ac-
curacy over popular head and long-tail, as discussed
in Section 1, we separated the test data into two
groups and analyzed the effectiveness separately.
Figure 5 plots the number of hits returned for the
names from Google search engine. According to the
distribution, we separate the test data into top-100
popular people with the highest hits and the remain-
ing, denoted head and tail, respectively.
0 50 100 150 200 250 300 35010
4
105
106
107
108
Number of names
Nu
mb
er o
f hi
ts i
n G
oog
le
Figure 5: Distribution over number of hits
Table 4 shows the effectiveness with both
datasets, respectively. As difference of the effective-
ness between tail and head (denoted diff ) with re-
spect to three measures shows, our approach shows
stably high precision, for both heads and tails.
4.2.3 Scalability
To validate the scalability of our approach, we
evaluated the effectiveness of our approach over the
number of nodes in two graphs. We built larger six
graph pairs (Ge, Gc) by expanding them from the
seed pairs further until the number of nodes reaches
5,000. Figure 6 shows the number of matched trans-
lations according to such increase. Overall, the num-
ber of matched pairs linearly increases as the num-
ber of nodes increases, which suggests scalability.
The ratio of node overlap in two graphs is about be-
tween 7% and 9% of total node size.
1000 2000 3000 4000 500050
100
150
200
250
300
350
|Ve| and |Vc|
# m
atc
hed
 tra
nsla
tion
s
Figure 6: Matched translations over |Ve| and |Vc|
5 Conclusion
This paper abstracted name translation problem as a
matching problem of two entity-relationship graphs.
This novel approach complements existing name
translation work, by not requiring rare resources
of parallel or comparable corpus yet outperforming
the state-of-the-art. More specifically, we combine
bilingual phonetic similarity and monolingual Web
co-occurrence similarity, to compute a holistic no-
tion of entity similarity. To achieve this goal, we de-
437
Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets Ti
Set Precision Recall F1-scoreEngkoo Google Baseline Ours Engkoo Google Baseline Ours Engkoo Google Baseline Ours
T1 0.5263 0.4510 0.5263 0.8974 0.3922 0.4510 0.1961 0.6863 0.4494 0.4510 0.2857 0.7778
T2 0.7551 0.75 0.7143 0.8056 0.7115 0.75 0.2885 0.5577 0.7327 0.75 0.4110 0.6591
T3 0.5833 0.7925 0.5556 0.7949 0.5283 0.7925 0.1887 0.5849 0.5545 0.7925 0.2817 0.6739
T4 0.5 0.45 0.7368 0.7353 0.425 0.45 0.35 0.625 0.4595 0.45 0.4746 0.6757
T5 0.6111 0.3137 0.5 0.7234 0.4314 0.3137 0.1765 0.6667 0.5057 0.3137 0.2609 0.6939
T6 0.5636 0.8947 0.6 0.8605 0.5438 0.8947 0.1053 0.6491 0.5536 0.8947 0.1791 0.74
AVG 0.5899 0.6086 0.6055 0.8028 0.5054 0.6086 0.2175 0.6283 0.5426 0.6086 0.3155 0.7034
Table 4: Precision, Recall, and F1-score of Engkoo, Google, and Ours with head and tail datasets
Method Precision Recall F1-scorehead tail diff head tail diff head tail diff
Engkoo 0.6082 0.5854 0.0229 0.59 0.4706 0.1194 0.5990 0.5217 0.0772
Google 0.75 0.5588 0.1912 0.75 0.5588 0.1912 0.75 0.5588 0.1912
Ours 0.8462 0.7812 0.0649 0.66 0.6127 0.0473 0.7416 0.6868 0.0548
veloped a graph alignment algorithm that iteratively
reinforces the matching similarity exploiting rela-
tional similarity and then extracts correct matches.
Our evaluation results empirically validated the ac-
curacy of our algorithm over real-life datasets, and
showed the effectiveness on our proposed perspec-
tive.
Acknowledgments
This work was supported by Microsoft Research
Asia NLP theme funding and MKE (Ministry of
Knowledge Economy), Korea, under the ITRC (In-
formation Technology Research Center) support
program supervised by the IITA (Institute for In-
formation Technology Advancement) (IITA-2009-
C1090-0902-0045).
References
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating Named Entities Using Monolingual and Bilin-
gual Resources. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics
(ACL?02), pages 400?408. Association for Computa-
tional Linguistics.
Donghui Feng, Yajuan Lu?, and Ming Zhou. 2004.
A New Approach for English-Chinese Named En-
tity Alignment. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?04), pages 372?379. Association for Com-
putational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR Ap-
proach for Translating New Words from Nonparal-
lel,Comparable Texts. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
(COLING?98), pages 414?420. Association for Com-
putational Linguistics.
Long Jiang, Ming Zhou, Lee feng Chien, and Cheng Niu.
2007. Named Entity Translation withWebMining and
Transliteration. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI?07), pages 1629?1634. Morgan Kaufmann Pub-
lishers Inc.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and
Qingsheng Zhu. 2009. Mining Bilingual Data from
the Web with Adaptively Learnt Patterns. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL?09), pages 870?878.
Association for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine Transliteration. Computational Linguistics,
24(4):599?612.
Julian Kupiec. 1993. An Algorithm for finding Noun
Phrase Correspondences in Bilingual Corpora. In Pro-
ceedings of the 31th Annual Meeting of the Association
for Computational Linguistics (ACL?93), pages 17?22.
Association for Computational Linguistics.
Haizhou Li, Zhang Min, and Su Jian. 2004. A Joint
Source-Channel Model for Machine Transliteration.
In Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics (ACL?04), pages
159?166. Association for Computational Linguistics.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining Parenthetical Transla-
438
tions from the Web by Word Alignment. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics (ACL?08), pages 994?
1002. Association for Computational Linguistics.
Li Shao and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Computa-
tional Linguistics (COLING?04), pages 618?624. As-
sociation for Computational Linguistics.
Ellen M. Voorhees. 2001. The trec question answering
track. Natural Language Engineering, 7(4):361?378.
Stephen Wan and Cornelia Maria Verspoor. 1998. Auto-
matic English-Chinese Name Transliteration for De-
velopment of Multilingual Resources. In Proceed-
ings of the 17th International Conference on Compu-
tational Linguistics (COLING?98), pages 1352?1356.
Association for Computational Linguistics.
Douglas Brent West. 2000. Introduction to Graph The-
ory. Prentice Hall, second edition.
439
